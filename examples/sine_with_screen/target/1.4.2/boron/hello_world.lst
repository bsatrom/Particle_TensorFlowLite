
/Users/bsatrom/Development/particle/libraries/Particle_TensorFlowLite_Examples/hello_world/target/1.4.2/boron/hello_world.elf:     file format elf32-littlearm

Sections:
Idx Name          Size      VMA       LMA       File off  Algn
  0 .module_info  00000018  000d4000  000d4000  00004000  2**2
                  CONTENTS, ALLOC, LOAD, READONLY, DATA
  1 .dynalib      00000004  000d4018  000d4018  00004018  2**2
                  CONTENTS, ALLOC, LOAD, READONLY, DATA
  2 .text         00017a00  000d4020  000d4020  00004020  2**3
                  CONTENTS, ALLOC, LOAD, READONLY, CODE
  3 .backup       00000000  2003f400  2003f400  0002bfe0  2**0
                  CONTENTS
  4 .data         00000594  2003bd40  000eba20  0001bd40  2**2
                  CONTENTS, ALLOC, LOAD, DATA
  5 .bss          0000251c  2003c2d4  2003c2d4  0002c2d4  2**2
                  ALLOC
  6 .module_info_suffix 00000028  000ebfb4  000ebfb4  0002bfb4  2**0
                  CONTENTS, ALLOC, LOAD, READONLY, DATA
  7 .module_info_crc 00000004  000ebfdc  000ebfdc  0002bfdc  2**0
                  CONTENTS, ALLOC, LOAD, READONLY, DATA
  8 .debug_info   0029414d  00000000  00000000  0002bfe0  2**0
                  CONTENTS, READONLY, DEBUGGING
  9 .debug_abbrev 0002688b  00000000  00000000  002c012d  2**0
                  CONTENTS, READONLY, DEBUGGING
 10 .debug_loc    00053eb3  00000000  00000000  002e69b8  2**0
                  CONTENTS, READONLY, DEBUGGING
 11 .debug_aranges 00003700  00000000  00000000  0033a86b  2**0
                  CONTENTS, READONLY, DEBUGGING
 12 .debug_ranges 00009838  00000000  00000000  0033df6b  2**0
                  CONTENTS, READONLY, DEBUGGING
 13 .debug_macro  0004ace8  00000000  00000000  003477a3  2**0
                  CONTENTS, READONLY, DEBUGGING
 14 .debug_line   0005dcf9  00000000  00000000  0039248b  2**0
                  CONTENTS, READONLY, DEBUGGING
 15 .debug_str    0018ef2a  00000000  00000000  003f0184  2**0
                  CONTENTS, READONLY, DEBUGGING
 16 .debug_frame  00010b30  00000000  00000000  0057f0b0  2**2
                  CONTENTS, READONLY, DEBUGGING

Disassembly of section .text:

000d4020 <module_user_pre_init>:
/**
 * Initializes this user module. Returns the start of the heap.
 */
void* module_user_pre_init() {

    if ( (&link_global_data_start!=&link_global_data_initial_values) && (link_global_data_size != 0))
   d4020:	4809      	ldr	r0, [pc, #36]	; (d4048 <module_user_pre_init+0x28>)
   d4022:	490a      	ldr	r1, [pc, #40]	; (d404c <module_user_pre_init+0x2c>)
   d4024:	4288      	cmp	r0, r1
extern constructor_ptr_t link_constructors_end;

/**
 * Initializes this user module. Returns the start of the heap.
 */
void* module_user_pre_init() {
   d4026:	b508      	push	{r3, lr}

    if ( (&link_global_data_start!=&link_global_data_initial_values) && (link_global_data_size != 0))
   d4028:	d005      	beq.n	d4036 <module_user_pre_init+0x16>
   d402a:	4a09      	ldr	r2, [pc, #36]	; (d4050 <module_user_pre_init+0x30>)
   d402c:	4282      	cmp	r2, r0
   d402e:	d002      	beq.n	d4036 <module_user_pre_init+0x16>
    {
        memcpy(&link_global_data_start, &link_global_data_initial_values, link_global_data_size);
   d4030:	1a12      	subs	r2, r2, r0
   d4032:	f013 fbd6 	bl	e77e2 <memcpy>
    }

    memset(&link_bss_location, 0, link_bss_size );
   d4036:	4807      	ldr	r0, [pc, #28]	; (d4054 <module_user_pre_init+0x34>)
   d4038:	4a07      	ldr	r2, [pc, #28]	; (d4058 <module_user_pre_init+0x38>)
   d403a:	2100      	movs	r1, #0
   d403c:	1a12      	subs	r2, r2, r0
   d403e:	f013 fbdb 	bl	e77f8 <memset>
    return &link_global_data_start;
}
   d4042:	4801      	ldr	r0, [pc, #4]	; (d4048 <module_user_pre_init+0x28>)
   d4044:	bd08      	pop	{r3, pc}
   d4046:	bf00      	nop
   d4048:	2003bd40 	.word	0x2003bd40
   d404c:	000eba20 	.word	0x000eba20
   d4050:	2003c2d4 	.word	0x2003c2d4
   d4054:	2003c2d4 	.word	0x2003c2d4
   d4058:	2003e7f0 	.word	0x2003e7f0

000d405c <module_user_init>:
extern constructor_ptr_t link_constructors_location[];
extern constructor_ptr_t link_constructors_end;
#define link_constructors_size   ((unsigned long)&link_constructors_end  -  (unsigned long)&link_constructors_location )

void module_user_init()
{
   d405c:	b570      	push	{r4, r5, r6, lr}
    module_user_init_hook();
   d405e:	f010 fe2b 	bl	e4cb8 <module_user_init_hook>
   d4062:	4c07      	ldr	r4, [pc, #28]	; (d4080 <module_user_init+0x24>)
   d4064:	4b07      	ldr	r3, [pc, #28]	; (d4084 <module_user_init+0x28>)
   d4066:	1ae4      	subs	r4, r4, r3
   d4068:	08a4      	lsrs	r4, r4, #2

    // invoke constructors
    int ctor_num;
    for (ctor_num=0; ctor_num < link_constructors_size/sizeof(constructor_ptr_t); ctor_num++ )
   d406a:	2500      	movs	r5, #0
   d406c:	461e      	mov	r6, r3
   d406e:	42a5      	cmp	r5, r4
   d4070:	d004      	beq.n	d407c <module_user_init+0x20>
    {
        link_constructors_location[ctor_num]();
   d4072:	f856 3025 	ldr.w	r3, [r6, r5, lsl #2]
   d4076:	4798      	blx	r3
{
    module_user_init_hook();

    // invoke constructors
    int ctor_num;
    for (ctor_num=0; ctor_num < link_constructors_size/sizeof(constructor_ptr_t); ctor_num++ )
   d4078:	3501      	adds	r5, #1
   d407a:	e7f8      	b.n	d406e <module_user_init+0x12>
    {
        link_constructors_location[ctor_num]();
    }
}
   d407c:	bd70      	pop	{r4, r5, r6, pc}
   d407e:	bf00      	nop
   d4080:	000eba20 	.word	0x000eba20
   d4084:	000eb9dc 	.word	0x000eb9dc

000d4088 <module_user_setup>:

/**
 * Export these functions with a fuller name so they don't clash with the setup/loop wrappers in the system module.
 */
void module_user_setup() {
    setup();
   d4088:	f000 b89a 	b.w	d41c0 <setup>

000d408c <module_user_loop>:
}

void module_user_loop() {
   d408c:	b508      	push	{r3, lr}
    loop();
   d408e:	f000 f933 	bl	d42f8 <loop>
    _post_loop();
}
   d4092:	e8bd 4008 	ldmia.w	sp!, {r3, lr}
    setup();
}

void module_user_loop() {
    loop();
    _post_loop();
   d4096:	f010 bddd 	b.w	e4c54 <_post_loop>

000d409a <_Znaj>:
	return malloc(size);
}

void *operator new[](size_t size)
{
	return malloc(size);
   d409a:	f010 b937 	b.w	e430c <malloc>

000d409e <_ZdlPv>:
   d409e:	f010 b93d 	b.w	e431c <free>

000d40a2 <_ZdaPv>:
	free(p);
}

void operator delete[](void *p)
{
	free(p);
   d40a2:	f010 b93b 	b.w	e431c <free>
	...

000d40a8 <_exit>:
int _getpid(void)
{
	return 1;
}

void _exit(int status) {
   d40a8:	b508      	push	{r3, lr}
    PANIC(Exit,"Exit Called");
   d40aa:	4a03      	ldr	r2, [pc, #12]	; (d40b8 <_exit+0x10>)
   d40ac:	2100      	movs	r1, #0
   d40ae:	2007      	movs	r0, #7
   d40b0:	f010 f8b0 	bl	e4214 <panic_>
   d40b4:	e7fe      	b.n	d40b4 <_exit+0xc>
   d40b6:	bf00      	nop
   d40b8:	000e3fe5 	.word	0x000e3fe5

000d40bc <__cxa_guard_acquire>:

/* Provide default implemenation for __cxa_guard_acquire() and
 * __cxa_guard_release(). Note: these must be revisited if a multitasking
 * OS is ported to this platform. */
__extension__ typedef int __guard __attribute__((mode (__DI__)));
int __cxa_guard_acquire(__guard *g) {return !*(char *)(g);};
   d40bc:	7800      	ldrb	r0, [r0, #0]
   d40be:	fab0 f080 	clz	r0, r0
   d40c2:	0940      	lsrs	r0, r0, #5
   d40c4:	4770      	bx	lr

000d40c6 <__cxa_guard_release>:
void __cxa_guard_release (__guard *g) {*(char *)g = 1;};
   d40c6:	2301      	movs	r3, #1
   d40c8:	7003      	strb	r3, [r0, #0]
   d40ca:	4770      	bx	lr

000d40cc <TfLiteIntArrayEqualsArray>:
  if (a == NULL || b == NULL) return 0;
  return TfLiteIntArrayEqualsArray(a, b->size, b->data);
}

int TfLiteIntArrayEqualsArray(const TfLiteIntArray* a, int b_size,
                              const int b_data[]) {
   d40cc:	b530      	push	{r4, r5, lr}
  if (a == NULL) return (b_size == 0);
   d40ce:	b918      	cbnz	r0, d40d8 <TfLiteIntArrayEqualsArray+0xc>
   d40d0:	fab1 f081 	clz	r0, r1
   d40d4:	0940      	lsrs	r0, r0, #5
   d40d6:	bd30      	pop	{r4, r5, pc}
  if (a->size != b_size) return 0;
   d40d8:	6803      	ldr	r3, [r0, #0]
   d40da:	4299      	cmp	r1, r3
   d40dc:	d10c      	bne.n	d40f8 <TfLiteIntArrayEqualsArray+0x2c>
   d40de:	2300      	movs	r3, #0
  int i = 0;
  for (; i < a->size; i++)
   d40e0:	428b      	cmp	r3, r1
   d40e2:	da07      	bge.n	d40f4 <TfLiteIntArrayEqualsArray+0x28>
    if (a->data[i] != b_data[i]) return 0;
   d40e4:	f850 5f04 	ldr.w	r5, [r0, #4]!
   d40e8:	f852 4023 	ldr.w	r4, [r2, r3, lsl #2]
   d40ec:	42a5      	cmp	r5, r4
   d40ee:	d103      	bne.n	d40f8 <TfLiteIntArrayEqualsArray+0x2c>
int TfLiteIntArrayEqualsArray(const TfLiteIntArray* a, int b_size,
                              const int b_data[]) {
  if (a == NULL) return (b_size == 0);
  if (a->size != b_size) return 0;
  int i = 0;
  for (; i < a->size; i++)
   d40f0:	3301      	adds	r3, #1
   d40f2:	e7f5      	b.n	d40e0 <TfLiteIntArrayEqualsArray+0x14>
    if (a->data[i] != b_data[i]) return 0;
  return 1;
   d40f4:	2001      	movs	r0, #1
   d40f6:	bd30      	pop	{r4, r5, pc}
}

int TfLiteIntArrayEqualsArray(const TfLiteIntArray* a, int b_size,
                              const int b_data[]) {
  if (a == NULL) return (b_size == 0);
  if (a->size != b_size) return 0;
   d40f8:	2000      	movs	r0, #0
  int i = 0;
  for (; i < a->size; i++)
    if (a->data[i] != b_data[i]) return 0;
  return 1;
}
   d40fa:	bd30      	pop	{r4, r5, pc}

000d40fc <TfLiteIntArrayEqual>:
  static TfLiteIntArray dummy;
  return sizeof(dummy) + sizeof(dummy.data[0]) * size;
}

int TfLiteIntArrayEqual(const TfLiteIntArray* a, const TfLiteIntArray* b) {
  if (a == b) return 1;
   d40fc:	4288      	cmp	r0, r1
   d40fe:	d005      	beq.n	d410c <TfLiteIntArrayEqual+0x10>
  if (a == NULL || b == NULL) return 0;
   d4100:	b130      	cbz	r0, d4110 <TfLiteIntArrayEqual+0x14>
   d4102:	b131      	cbz	r1, d4112 <TfLiteIntArrayEqual+0x16>
  return TfLiteIntArrayEqualsArray(a, b->size, b->data);
   d4104:	1d0a      	adds	r2, r1, #4
   d4106:	6809      	ldr	r1, [r1, #0]
   d4108:	f7ff bfe0 	b.w	d40cc <TfLiteIntArrayEqualsArray>
  static TfLiteIntArray dummy;
  return sizeof(dummy) + sizeof(dummy.data[0]) * size;
}

int TfLiteIntArrayEqual(const TfLiteIntArray* a, const TfLiteIntArray* b) {
  if (a == b) return 1;
   d410c:	2001      	movs	r0, #1
   d410e:	4770      	bx	lr
   d4110:	4770      	bx	lr
  if (a == NULL || b == NULL) return 0;
   d4112:	4608      	mov	r0, r1
  return TfLiteIntArrayEqualsArray(a, b->size, b->data);
}
   d4114:	4770      	bx	lr
	...

000d4118 <TfLiteTypeGetName>:
  }
  tensor->bytes = num_bytes;
}
#endif  // TF_LITE_STATIC_MEMORY

const char* TfLiteTypeGetName(TfLiteType type) {
   d4118:	280a      	cmp	r0, #10
   d411a:	bf9a      	itte	ls
   d411c:	4b02      	ldrls	r3, [pc, #8]	; (d4128 <TfLiteTypeGetName+0x10>)
   d411e:	f853 0020 	ldrls.w	r0, [r3, r0, lsl #2]
   d4122:	4802      	ldrhi	r0, [pc, #8]	; (d412c <TfLiteTypeGetName+0x14>)
      return "STRING";
    case kTfLiteFloat16:
      return "FLOAT16";
  }
  return "Unknown type";
}
   d4124:	4770      	bx	lr
   d4126:	bf00      	nop
   d4128:	000e78f4 	.word	0x000e78f4
   d412c:	000e78a0 	.word	0x000e78a0

000d4130 <_Z12HandleOutputPN6tflite13ErrorReporterEff>:
// Track whether the function has run at least once
bool initialized = false;

// Animates a dot across the screen to represent the current x and y values
void HandleOutput(tflite::ErrorReporter* error_reporter, float x_value,
                  float y_value) {
   d4130:	b570      	push	{r4, r5, r6, lr}
  // Do this only once
  if (!initialized) {
   d4132:	4d15      	ldr	r5, [pc, #84]	; (d4188 <_Z12HandleOutputPN6tflite13ErrorReporterEff+0x58>)
   d4134:	4c15      	ldr	r4, [pc, #84]	; (d418c <_Z12HandleOutputPN6tflite13ErrorReporterEff+0x5c>)
   d4136:	782b      	ldrb	r3, [r5, #0]
// Track whether the function has run at least once
bool initialized = false;

// Animates a dot across the screen to represent the current x and y values
void HandleOutput(tflite::ErrorReporter* error_reporter, float x_value,
                  float y_value) {
   d4138:	ed2d 8b02 	vpush	{d8}
   d413c:	4606      	mov	r6, r0
   d413e:	b082      	sub	sp, #8
   d4140:	eeb0 8a60 	vmov.f32	s16, s1
  // Do this only once
  if (!initialized) {
   d4144:	b92b      	cbnz	r3, d4152 <_Z12HandleOutputPN6tflite13ErrorReporterEff+0x22>
    // Set the LED pin to output
    pinMode(led, OUTPUT);
   d4146:	2101      	movs	r1, #1
   d4148:	8820      	ldrh	r0, [r4, #0]
   d414a:	f010 fdf3 	bl	e4d34 <pinMode>
    initialized = true;
   d414e:	2301      	movs	r3, #1
   d4150:	702b      	strb	r3, [r5, #0]
  }

  // Calculate the brightness of the LED such that y=-1 is fully off
  // and y=1 is fully on. The LED's brightness can range from 0-255.
  int brightness = (int)(127.5f * (y_value + 1));
   d4152:	eef7 0a00 	vmov.f32	s1, #112	; 0x3f800000  1.0
   d4156:	ee78 0a20 	vadd.f32	s1, s16, s1
   d415a:	eddf 7a0d 	vldr	s15, [pc, #52]	; d4190 <_Z12HandleOutputPN6tflite13ErrorReporterEff+0x60>

  // Set the brightness of the LED. If the specified pin does not support PWM,
  // this will result in the LED being on when y > 127, off otherwise.
  analogWrite(led, brightness);
   d415e:	8820      	ldrh	r0, [r4, #0]
    initialized = true;
  }

  // Calculate the brightness of the LED such that y=-1 is fully off
  // and y=1 is fully on. The LED's brightness can range from 0-255.
  int brightness = (int)(127.5f * (y_value + 1));
   d4160:	ee60 0aa7 	vmul.f32	s1, s1, s15
   d4164:	eefd 7ae0 	vcvt.s32.f32	s15, s1

  // Set the brightness of the LED. If the specified pin does not support PWM,
  // this will result in the LED being on when y > 127, off otherwise.
  analogWrite(led, brightness);
   d4168:	ee17 1a90 	vmov	r1, s15
    initialized = true;
  }

  // Calculate the brightness of the LED such that y=-1 is fully off
  // and y=1 is fully on. The LED's brightness can range from 0-255.
  int brightness = (int)(127.5f * (y_value + 1));
   d416c:	edcd 7a01 	vstr	s15, [sp, #4]

  // Set the brightness of the LED. If the specified pin does not support PWM,
  // this will result in the LED being on when y > 127, off otherwise.
  analogWrite(led, brightness);
   d4170:	f010 fdf1 	bl	e4d56 <_Z11analogWritetm>

  // Log the current brightness value for display in the Arduino plotter
  error_reporter->Report("%d\n", brightness);
   d4174:	9a01      	ldr	r2, [sp, #4]
   d4176:	4907      	ldr	r1, [pc, #28]	; (d4194 <_Z12HandleOutputPN6tflite13ErrorReporterEff+0x64>)
   d4178:	4630      	mov	r0, r6
}
   d417a:	b002      	add	sp, #8
   d417c:	ecbd 8b02 	vpop	{d8}
   d4180:	e8bd 4070 	ldmia.w	sp!, {r4, r5, r6, lr}
  // Set the brightness of the LED. If the specified pin does not support PWM,
  // this will result in the LED being on when y > 127, off otherwise.
  analogWrite(led, brightness);

  // Log the current brightness value for display in the Arduino plotter
  error_reporter->Report("%d\n", brightness);
   d4184:	f000 b920 	b.w	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
   d4188:	2003c2d8 	.word	0x2003c2d8
   d418c:	2003bd40 	.word	0x2003bd40
   d4190:	42ff0000 	.word	0x42ff0000
   d4194:	000e9595 	.word	0x000e9595

000d4198 <_GLOBAL__sub_I_led>:

inline void pinSetFast(pin_t _pin) __attribute__((always_inline));
inline void pinResetFast(pin_t _pin) __attribute__((always_inline));
inline int32_t pinReadFast(pin_t _pin) __attribute__((always_inline));

static Hal_Pin_Info* PIN_MAP = HAL_Pin_Map();
   d4198:	f00f bf34 	b.w	e4004 <HAL_Pin_Map>

000d419c <_ZN6tflite18MicroErrorReporterD1Ev>:

namespace tflite {

class MicroErrorReporter : public ErrorReporter {
 public:
  ~MicroErrorReporter() {}
   d419c:	4770      	bx	lr

000d419e <_ZN6tflite3ops5micro14AllOpsResolverD1Ev>:

namespace tflite {
namespace ops {
namespace micro {

class AllOpsResolver : public MicroMutableOpResolver {
   d419e:	4770      	bx	lr

000d41a0 <_ZN6tflite3ops5micro14AllOpsResolverD0Ev>:
   d41a0:	b510      	push	{r4, lr}
   d41a2:	f241 0108 	movw	r1, #4104	; 0x1008
   d41a6:	4604      	mov	r4, r0
   d41a8:	f010 fe43 	bl	e4e32 <_ZdlPvj>
   d41ac:	4620      	mov	r0, r4
   d41ae:	bd10      	pop	{r4, pc}

000d41b0 <_ZN6tflite18MicroErrorReporterD0Ev>:
   d41b0:	b510      	push	{r4, lr}
   d41b2:	2104      	movs	r1, #4
   d41b4:	4604      	mov	r4, r0
   d41b6:	f010 fe3c 	bl	e4e32 <_ZdlPvj>
   d41ba:	4620      	mov	r0, r4
   d41bc:	bd10      	pop	{r4, pc}
	...

000d41c0 <setup>:
uint8_t tensor_arena[kTensorArenaSize];
} // namespace

// The name of this function is important for Arduino compatibility.
void setup()
{
   d41c0:	b573      	push	{r0, r1, r4, r5, r6, lr}
  // Set up logging. Google style is to avoid globals or statics because of
  // lifetime uncertainty, but since this has a trivial destructor it's okay.
  // NOLINTNEXTLINE(runtime-global-variables)
  static tflite::MicroErrorReporter micro_error_reporter;
   d41c2:	4c3a      	ldr	r4, [pc, #232]	; (d42ac <setup+0xec>)
   d41c4:	6823      	ldr	r3, [r4, #0]
   d41c6:	07d9      	lsls	r1, r3, #31
   d41c8:	d40b      	bmi.n	d41e2 <setup+0x22>
   d41ca:	4620      	mov	r0, r4
   d41cc:	f7ff ff76 	bl	d40bc <__cxa_guard_acquire>
   d41d0:	b138      	cbz	r0, d41e2 <setup+0x22>
   d41d2:	4620      	mov	r0, r4
   d41d4:	f7ff ff77 	bl	d40c6 <__cxa_guard_release>
   d41d8:	4a35      	ldr	r2, [pc, #212]	; (d42b0 <setup+0xf0>)
   d41da:	4936      	ldr	r1, [pc, #216]	; (d42b4 <setup+0xf4>)
   d41dc:	4836      	ldr	r0, [pc, #216]	; (d42b8 <setup+0xf8>)
   d41de:	f010 fe23 	bl	e4e28 <__aeabi_atexit>
  error_reporter = &micro_error_reporter;
   d41e2:	4c36      	ldr	r4, [pc, #216]	; (d42bc <setup+0xfc>)
   d41e4:	4b34      	ldr	r3, [pc, #208]	; (d42b8 <setup+0xf8>)
   d41e6:	6023      	str	r3, [r4, #0]
// Helpers to get a typed pointer to the root object contained in the buffer.
template<typename T> T *GetMutableRoot(void *buf) {
  EndianCheck();
  return reinterpret_cast<T *>(
      reinterpret_cast<uint8_t *>(buf) +
      EndianScalar(*reinterpret_cast<uoffset_t *>(buf)));
   d41e8:	4b35      	ldr	r3, [pc, #212]	; (d42c0 <setup+0x100>)

  // Map the model into a usable data structure. This doesn't involve any
  // copying or parsing, it's a very lightweight operation.
  model = tflite::GetModel(g_sine_model_data);
   d41ea:	4a36      	ldr	r2, [pc, #216]	; (d42c4 <setup+0x104>)
   d41ec:	6818      	ldr	r0, [r3, #0]
   d41ee:	18c1      	adds	r1, r0, r3
// "tables" use an offset table (possibly shared) that allows fields to be
// omitted and added at will, but uses an extra indirection to read.
class Table {
 public:
  const uint8_t *GetVTable() const {
    return data_ - ReadScalar<soffset_t>(data_);
   d41f0:	58c3      	ldr	r3, [r0, r3]
   d41f2:	6011      	str	r1, [r2, #0]
   d41f4:	1acb      	subs	r3, r1, r3
   d41f6:	4616      	mov	r6, r2
    auto vtable = GetVTable();
    // The first element is the size of the vtable (fields + type id + itself).
    auto vtsize = ReadScalar<voffset_t>(vtable);
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
   d41f8:	8818      	ldrh	r0, [r3, #0]
   d41fa:	2804      	cmp	r0, #4
   d41fc:	d905      	bls.n	d420a <setup+0x4a>

template<typename T>
// UBSAN: C++ aliasing type rules, see std::bit_cast<> for details.
__supress_ubsan__("alignment")
T ReadScalar(const void *p) {
  return EndianScalar(*reinterpret_cast<const T *>(p));
   d41fe:	889a      	ldrh	r2, [r3, #4]
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d4200:	b122      	cbz	r2, d420c <setup+0x4c>
   d4202:	588a      	ldr	r2, [r1, r2]
  if (model->version() != TFLITE_SCHEMA_VERSION)
   d4204:	2a03      	cmp	r2, #3
   d4206:	d009      	beq.n	d421c <setup+0x5c>
   d4208:	e000      	b.n	d420c <setup+0x4c>
   d420a:	2200      	movs	r2, #0
  {
    error_reporter->Report(
        "Model provided is schema version %d not equal "
        "to supported version %d.",
        model->version(), TFLITE_SCHEMA_VERSION);
   d420c:	492e      	ldr	r1, [pc, #184]	; (d42c8 <setup+0x108>)
   d420e:	482a      	ldr	r0, [pc, #168]	; (d42b8 <setup+0xf8>)
   d4210:	2303      	movs	r3, #3
  input = interpreter->input(0);
  output = interpreter->output(0);

  // Keep track of how many inferences we have performed.
  inference_count = 0;
}
   d4212:	b002      	add	sp, #8
   d4214:	e8bd 4070 	ldmia.w	sp!, {r4, r5, r6, lr}
  if (model->version() != TFLITE_SCHEMA_VERSION)
  {
    error_reporter->Report(
        "Model provided is schema version %d not equal "
        "to supported version %d.",
        model->version(), TFLITE_SCHEMA_VERSION);
   d4218:	f000 b8d6 	b.w	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return;
  }

  // This pulls in all the operation implementations we need.
  // NOLINTNEXTLINE(runtime-global-variables)
  static tflite::ops::micro::AllOpsResolver resolver;
   d421c:	4d2b      	ldr	r5, [pc, #172]	; (d42cc <setup+0x10c>)
   d421e:	682b      	ldr	r3, [r5, #0]
   d4220:	07da      	lsls	r2, r3, #31
   d4222:	d40e      	bmi.n	d4242 <setup+0x82>
   d4224:	4628      	mov	r0, r5
   d4226:	f7ff ff49 	bl	d40bc <__cxa_guard_acquire>
   d422a:	b150      	cbz	r0, d4242 <setup+0x82>
   d422c:	4828      	ldr	r0, [pc, #160]	; (d42d0 <setup+0x110>)
   d422e:	f002 fff3 	bl	d7218 <_ZN6tflite3ops5micro14AllOpsResolverC1Ev>
   d4232:	4628      	mov	r0, r5
   d4234:	f7ff ff47 	bl	d40c6 <__cxa_guard_release>
   d4238:	4a1d      	ldr	r2, [pc, #116]	; (d42b0 <setup+0xf0>)
   d423a:	4926      	ldr	r1, [pc, #152]	; (d42d4 <setup+0x114>)
   d423c:	4824      	ldr	r0, [pc, #144]	; (d42d0 <setup+0x110>)
   d423e:	f010 fdf3 	bl	e4e28 <__aeabi_atexit>

  // Build an interpreter to run the model with.
  static tflite::MicroInterpreter static_interpreter(
      model, resolver, tensor_arena, kTensorArenaSize, error_reporter);
   d4242:	4d25      	ldr	r5, [pc, #148]	; (d42d8 <setup+0x118>)
   d4244:	682b      	ldr	r3, [r5, #0]
   d4246:	07db      	lsls	r3, r3, #31
   d4248:	d411      	bmi.n	d426e <setup+0xae>
   d424a:	4628      	mov	r0, r5
   d424c:	f7ff ff36 	bl	d40bc <__cxa_guard_acquire>
   d4250:	b168      	cbz	r0, d426e <setup+0xae>
   d4252:	6823      	ldr	r3, [r4, #0]
   d4254:	9301      	str	r3, [sp, #4]
   d4256:	f44f 6300 	mov.w	r3, #2048	; 0x800
   d425a:	9300      	str	r3, [sp, #0]
   d425c:	4a1c      	ldr	r2, [pc, #112]	; (d42d0 <setup+0x110>)
   d425e:	4b1f      	ldr	r3, [pc, #124]	; (d42dc <setup+0x11c>)
   d4260:	6831      	ldr	r1, [r6, #0]
   d4262:	481f      	ldr	r0, [pc, #124]	; (d42e0 <setup+0x120>)
   d4264:	f001 fe22 	bl	d5eac <_ZN6tflite16MicroInterpreterC1EPKNS_5ModelERKNS_10OpResolverEPhjPNS_13ErrorReporterE>
   d4268:	4628      	mov	r0, r5
   d426a:	f7ff ff2c 	bl	d40c6 <__cxa_guard_release>
  interpreter = &static_interpreter;
   d426e:	4e1d      	ldr	r6, [pc, #116]	; (d42e4 <setup+0x124>)
   d4270:	481b      	ldr	r0, [pc, #108]	; (d42e0 <setup+0x120>)
   d4272:	6030      	str	r0, [r6, #0]

  // Allocate memory from the tensor_arena for the model's tensors.
  TfLiteStatus allocate_status = interpreter->AllocateTensors();
   d4274:	f001 fe00 	bl	d5e78 <_ZN6tflite16MicroInterpreter15AllocateTensorsEv>
  if (allocate_status != kTfLiteOk)
   d4278:	4605      	mov	r5, r0
   d427a:	b130      	cbz	r0, d428a <setup+0xca>
  {
    error_reporter->Report("AllocateTensors() failed");
   d427c:	491a      	ldr	r1, [pc, #104]	; (d42e8 <setup+0x128>)
   d427e:	6820      	ldr	r0, [r4, #0]
  input = interpreter->input(0);
  output = interpreter->output(0);

  // Keep track of how many inferences we have performed.
  inference_count = 0;
}
   d4280:	b002      	add	sp, #8
   d4282:	e8bd 4070 	ldmia.w	sp!, {r4, r5, r6, lr}

  // Allocate memory from the tensor_arena for the model's tensors.
  TfLiteStatus allocate_status = interpreter->AllocateTensors();
  if (allocate_status != kTfLiteOk)
  {
    error_reporter->Report("AllocateTensors() failed");
   d4286:	f000 b89f 	b.w	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return;
  }

  // Obtain pointers to the model's input and output tensors.
  input = interpreter->input(0);
   d428a:	4601      	mov	r1, r0
   d428c:	6830      	ldr	r0, [r6, #0]
   d428e:	f001 fe71 	bl	d5f74 <_ZN6tflite16MicroInterpreter5inputEj>
   d4292:	4b16      	ldr	r3, [pc, #88]	; (d42ec <setup+0x12c>)
  output = interpreter->output(0);
   d4294:	4629      	mov	r1, r5
    error_reporter->Report("AllocateTensors() failed");
    return;
  }

  // Obtain pointers to the model's input and output tensors.
  input = interpreter->input(0);
   d4296:	6018      	str	r0, [r3, #0]
  output = interpreter->output(0);
   d4298:	6830      	ldr	r0, [r6, #0]
   d429a:	f001 fe4f 	bl	d5f3c <_ZN6tflite16MicroInterpreter6outputEj>
   d429e:	4b14      	ldr	r3, [pc, #80]	; (d42f0 <setup+0x130>)
   d42a0:	6018      	str	r0, [r3, #0]

  // Keep track of how many inferences we have performed.
  inference_count = 0;
   d42a2:	4b14      	ldr	r3, [pc, #80]	; (d42f4 <setup+0x134>)
   d42a4:	601d      	str	r5, [r3, #0]
}
   d42a6:	b002      	add	sp, #8
   d42a8:	bd70      	pop	{r4, r5, r6, pc}
   d42aa:	bf00      	nop
   d42ac:	2003db80 	.word	0x2003db80
   d42b0:	2003c2d4 	.word	0x2003c2d4
   d42b4:	000d419d 	.word	0x000d419d
   d42b8:	2003bd44 	.word	0x2003bd44
   d42bc:	2003caf0 	.word	0x2003caf0
   d42c0:	000e79b4 	.word	0x000e79b4
   d42c4:	2003caec 	.word	0x2003caec
   d42c8:	000e7924 	.word	0x000e7924
   d42cc:	2003c2e0 	.word	0x2003c2e0
   d42d0:	2003caf8 	.word	0x2003caf8
   d42d4:	000d419f 	.word	0x000d419f
   d42d8:	2003db88 	.word	0x2003db88
   d42dc:	2003c2e4 	.word	0x2003c2e4
   d42e0:	2003db00 	.word	0x2003db00
   d42e4:	2003caf4 	.word	0x2003caf4
   d42e8:	000e796b 	.word	0x000e796b
   d42ec:	2003c2dc 	.word	0x2003c2dc
   d42f0:	2003db84 	.word	0x2003db84
   d42f4:	2003db8c 	.word	0x2003db8c

000d42f8 <loop>:

// The name of this function is important for Arduino compatibility.
void loop()
{
   d42f8:	b530      	push	{r4, r5, lr}
  // Calculate an x value to feed into the model. We compare the current
  // inference_count to the number of inferences per cycle to determine
  // our position within the range of possible x values the model was
  // trained on, and use this to calculate a value.
  float position = static_cast<float>(inference_count) /
   d42fa:	4b22      	ldr	r3, [pc, #136]	; (d4384 <loop+0x8c>)
   d42fc:	4c22      	ldr	r4, [pc, #136]	; (d4388 <loop+0x90>)
   d42fe:	681b      	ldr	r3, [r3, #0]
                   static_cast<float>(kInferencesPerCycle);
  float x_val = position * kXrange;
   d4300:	edd4 7a00 	vldr	s15, [r4]
   d4304:	4d21      	ldr	r5, [pc, #132]	; (d438c <loop+0x94>)
   d4306:	eeb8 7ae7 	vcvt.f32.s32	s14, s15
   d430a:	ee07 3a90 	vmov	s15, r3
   d430e:	eef8 6ae7 	vcvt.f32.s32	s13, s15
  inference_count = 0;
}

// The name of this function is important for Arduino compatibility.
void loop()
{
   d4312:	ed2d 8b02 	vpush	{d8}
  // inference_count to the number of inferences per cycle to determine
  // our position within the range of possible x values the model was
  // trained on, and use this to calculate a value.
  float position = static_cast<float>(inference_count) /
                   static_cast<float>(kInferencesPerCycle);
  float x_val = position * kXrange;
   d4316:	eec7 7a26 	vdiv.f32	s15, s14, s13
  inference_count = 0;
}

// The name of this function is important for Arduino compatibility.
void loop()
{
   d431a:	b083      	sub	sp, #12
  // Calculate an x value to feed into the model. We compare the current
  // inference_count to the number of inferences per cycle to determine
  // our position within the range of possible x values the model was
  // trained on, and use this to calculate a value.
  float position = static_cast<float>(inference_count) /
   d431c:	9301      	str	r3, [sp, #4]
                   static_cast<float>(kInferencesPerCycle);
  float x_val = position * kXrange;

  // Place our calculated x value in the model's input tensor
  input->data.f[0] = x_val;
   d431e:	4b1c      	ldr	r3, [pc, #112]	; (d4390 <loop+0x98>)
   d4320:	681b      	ldr	r3, [r3, #0]
   d4322:	685b      	ldr	r3, [r3, #4]
  // inference_count to the number of inferences per cycle to determine
  // our position within the range of possible x values the model was
  // trained on, and use this to calculate a value.
  float position = static_cast<float>(inference_count) /
                   static_cast<float>(kInferencesPerCycle);
  float x_val = position * kXrange;
   d4324:	ed9f 8a1b 	vldr	s16, [pc, #108]	; d4394 <loop+0x9c>
   d4328:	ee27 8a88 	vmul.f32	s16, s15, s16

  // Place our calculated x value in the model's input tensor
  input->data.f[0] = x_val;
   d432c:	ed83 8a00 	vstr	s16, [r3]

  // Run inference, and report any error
  TfLiteStatus invoke_status = interpreter->Invoke();
   d4330:	4b19      	ldr	r3, [pc, #100]	; (d4398 <loop+0xa0>)
   d4332:	6818      	ldr	r0, [r3, #0]
   d4334:	f001 fe3a 	bl	d5fac <_ZN6tflite16MicroInterpreter6InvokeEv>
  if (invoke_status != kTfLiteOk)
   d4338:	b170      	cbz	r0, d4358 <loop+0x60>
  {
    error_reporter->Report("Invoke failed on x_val: %f\n",
                           static_cast<double>(x_val));
   d433a:	ee18 0a10 	vmov	r0, s16
   d433e:	f012 fe5d 	bl	e6ffc <__aeabi_f2d>
   d4342:	4602      	mov	r2, r0
   d4344:	460b      	mov	r3, r1
   d4346:	6828      	ldr	r0, [r5, #0]
   d4348:	4914      	ldr	r1, [pc, #80]	; (d439c <loop+0xa4>)
  // Increment the inference_counter, and reset it if we have reached
  // the total number per cycle
  inference_count += 1;
  if (inference_count >= kInferencesPerCycle)
    inference_count = 0;
}
   d434a:	b003      	add	sp, #12
   d434c:	ecbd 8b02 	vpop	{d8}
   d4350:	e8bd 4030 	ldmia.w	sp!, {r4, r5, lr}
  // Run inference, and report any error
  TfLiteStatus invoke_status = interpreter->Invoke();
  if (invoke_status != kTfLiteOk)
  {
    error_reporter->Report("Invoke failed on x_val: %f\n",
                           static_cast<double>(x_val));
   d4354:	f000 b838 	b.w	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return;
  }

  // Read the predicted y value from the model's output tensor
  float y_val = output->data.f[0];
   d4358:	4b11      	ldr	r3, [pc, #68]	; (d43a0 <loop+0xa8>)

  // Output the results. A custom HandleOutput function can be implemented
  // for each supported hardware target.
  HandleOutput(error_reporter, x_val, y_val);
   d435a:	6828      	ldr	r0, [r5, #0]
                           static_cast<double>(x_val));
    return;
  }

  // Read the predicted y value from the model's output tensor
  float y_val = output->data.f[0];
   d435c:	681b      	ldr	r3, [r3, #0]
   d435e:	685b      	ldr	r3, [r3, #4]

  // Output the results. A custom HandleOutput function can be implemented
  // for each supported hardware target.
  HandleOutput(error_reporter, x_val, y_val);
   d4360:	eeb0 0a48 	vmov.f32	s0, s16
   d4364:	edd3 0a00 	vldr	s1, [r3]
   d4368:	f7ff fee2 	bl	d4130 <_Z12HandleOutputPN6tflite13ErrorReporterEff>

  // Increment the inference_counter, and reset it if we have reached
  // the total number per cycle
  inference_count += 1;
   d436c:	6823      	ldr	r3, [r4, #0]
   d436e:	9a01      	ldr	r2, [sp, #4]
   d4370:	3301      	adds	r3, #1
   d4372:	429a      	cmp	r2, r3
   d4374:	bfd8      	it	le
   d4376:	2300      	movle	r3, #0
   d4378:	6023      	str	r3, [r4, #0]
  if (inference_count >= kInferencesPerCycle)
    inference_count = 0;
}
   d437a:	b003      	add	sp, #12
   d437c:	ecbd 8b02 	vpop	{d8}
   d4380:	bd30      	pop	{r4, r5, pc}
   d4382:	bf00      	nop
   d4384:	000e7920 	.word	0x000e7920
   d4388:	2003db8c 	.word	0x2003db8c
   d438c:	2003caf0 	.word	0x2003caf0
   d4390:	2003c2dc 	.word	0x2003c2dc
   d4394:	40c90fdb 	.word	0x40c90fdb
   d4398:	2003caf4 	.word	0x2003caf4
   d439c:	000e7984 	.word	0x000e7984
   d43a0:	2003db84 	.word	0x2003db84

000d43a4 <_GLOBAL__sub_I_SystemMode>:
   d43a4:	b508      	push	{r3, lr}
   d43a6:	f00f fe2d 	bl	e4004 <HAL_Pin_Map>
    WAKEUP_REASON_RTC = 2,
    WAKEUP_REASON_PIN_OR_RTC = 3
};

struct SleepResult {
    SleepResult() {}
   d43aa:	4b06      	ldr	r3, [pc, #24]	; (d43c4 <_GLOBAL__sub_I_SystemMode+0x20>)
   d43ac:	2200      	movs	r2, #0
   d43ae:	701a      	strb	r2, [r3, #0]
   d43b0:	805a      	strh	r2, [r3, #2]
   d43b2:	f64f 72ff 	movw	r2, #65535	; 0xffff
   d43b6:	809a      	strh	r2, [r3, #4]

class SystemClass {
public:

    SystemClass(System_Mode_TypeDef mode = DEFAULT) {
        set_system_mode(mode);
   d43b8:	2003      	movs	r0, #3
   d43ba:	e8bd 4008 	ldmia.w	sp!, {r3, lr}
   d43be:	f00f bf31 	b.w	e4224 <set_system_mode>
   d43c2:	bf00      	nop
   d43c4:	2003cae4 	.word	0x2003cae4

000d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>:
#include "tensorflow/lite/core/api/error_reporter.h"
#include <cstdarg>

namespace tflite {

int ErrorReporter::Report(const char* format, ...) {
   d43c8:	b40e      	push	{r1, r2, r3}
   d43ca:	b503      	push	{r0, r1, lr}
   d43cc:	aa03      	add	r2, sp, #12
  va_list args;
  va_start(args, format);
  int code = Report(format, args);
   d43ce:	6803      	ldr	r3, [r0, #0]
#include "tensorflow/lite/core/api/error_reporter.h"
#include <cstdarg>

namespace tflite {

int ErrorReporter::Report(const char* format, ...) {
   d43d0:	f852 1b04 	ldr.w	r1, [r2], #4
  va_list args;
  va_start(args, format);
   d43d4:	9201      	str	r2, [sp, #4]
  int code = Report(format, args);
   d43d6:	689b      	ldr	r3, [r3, #8]
   d43d8:	4798      	blx	r3
  va_end(args);
  return code;
}
   d43da:	b002      	add	sp, #8
   d43dc:	f85d eb04 	ldr.w	lr, [sp], #4
   d43e0:	b003      	add	sp, #12
   d43e2:	4770      	bx	lr

000d43e4 <_ZN6tflite12_GLOBAL__N_124SafeBuiltinDataAllocator18BuiltinDataDeleterclEPv>:
  class BuiltinDataDeleter {
   public:
    explicit BuiltinDataDeleter(BuiltinDataAllocator* allocator)
        : allocator_(allocator) {}

    void operator()(void* data) { allocator_->Deallocate(data); }
   d43e4:	6800      	ldr	r0, [r0, #0]
   d43e6:	6803      	ldr	r3, [r0, #0]
   d43e8:	685b      	ldr	r3, [r3, #4]
   d43ea:	4718      	bx	r3

000d43ec <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_7PaddingEE_clESA_.isra.0>:
// If it returns kTfLiteError, `builtin_data` will be `nullptr`.
TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,
                         ErrorReporter* error_reporter,
                         BuiltinDataAllocator* allocator, void** builtin_data) {
  auto parse_padding = [](Padding padding) {
    switch (padding) {
   d43ec:	b120      	cbz	r0, d43f8 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_7PaddingEE_clESA_.isra.0+0xc>
   d43ee:	2801      	cmp	r0, #1
      case Padding_SAME:
        return kTfLitePaddingSame;
      case Padding_VALID:
        return kTfLitePaddingValid;
    }
    return kTfLitePaddingUnknown;
   d43f0:	bf0c      	ite	eq
   d43f2:	2002      	moveq	r0, #2
   d43f4:	2000      	movne	r0, #0
   d43f6:	4770      	bx	lr
                         ErrorReporter* error_reporter,
                         BuiltinDataAllocator* allocator, void** builtin_data) {
  auto parse_padding = [](Padding padding) {
    switch (padding) {
      case Padding_SAME:
        return kTfLitePaddingSame;
   d43f8:	2001      	movs	r0, #1
      case Padding_VALID:
        return kTfLitePaddingValid;
    }
    return kTfLitePaddingUnknown;
  };
   d43fa:	4770      	bx	lr

000d43fc <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>:
  auto parse_activation = [](ActivationFunctionType activation) {
   d43fc:	3801      	subs	r0, #1
   d43fe:	b2c0      	uxtb	r0, r0
   d4400:	2804      	cmp	r0, #4
   d4402:	bf9a      	itte	ls
   d4404:	4b01      	ldrls	r3, [pc, #4]	; (d440c <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1+0x10>)
   d4406:	5c18      	ldrbls	r0, [r3, r0]
   d4408:	2000      	movhi	r0, #0
        return kTfLiteActTanh;
      case ActivationFunctionType_SIGN_BIT:
        return kTfLiteActSignBit;
    }
    return kTfLiteActNone;
  };
   d440a:	4770      	bx	lr
   d440c:	000e86bf 	.word	0x000e86bf

000d4410 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4>:
};

// Copies the contents from the flatbuffer int vector `flatbuffer` into the
// int array `buffer`. `flat_vector` and `buffer` represent the same
// configuration operation for a given operation.
TfLiteStatus FlatBufferIntVectorToArray(
   d4410:	b538      	push	{r3, r4, r5, lr}
   d4412:	4615      	mov	r5, r2
   d4414:	461a      	mov	r2, r3
    int max_size_of_buffer, const flatbuffers::Vector<int32_t>* flat_vector,
    int* buffer, ErrorReporter* error_reporter, const char* op_name) {
  if (!flat_vector) {
   d4416:	b908      	cbnz	r0, d441c <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0xc>
    error_reporter->Report("Input array not provided for operation '%s'.\n",
                           op_name);
   d4418:	490f      	ldr	r1, [pc, #60]	; (d4458 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x48>)
   d441a:	e003      	b.n	d4424 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x14>
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d441c:	6804      	ldr	r4, [r0, #0]
    return kTfLiteError;
  } else {
    int num_dimensions = flat_vector->size();
    if (num_dimensions > max_size_of_buffer / sizeof(int)) {
   d441e:	2c08      	cmp	r4, #8
   d4420:	d905      	bls.n	d442e <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x1e>
      error_reporter->Report(
          "Found too many dimensions in the input array of operation '%s'.\n",
          op_name);
   d4422:	490e      	ldr	r1, [pc, #56]	; (d445c <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x4c>)
   d4424:	4628      	mov	r0, r5
   d4426:	f7ff ffcf 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
      return kTfLiteError;
   d442a:	2001      	movs	r0, #1
   d442c:	bd38      	pop	{r3, r4, r5, pc}
   d442e:	4602      	mov	r2, r0
    error_reporter->Report("Input array not provided for operation '%s'.\n",
                           op_name);
    return kTfLiteError;
  } else {
    int num_dimensions = flat_vector->size();
    if (num_dimensions > max_size_of_buffer / sizeof(int)) {
   d4430:	2300      	movs	r3, #0
      error_reporter->Report(
          "Found too many dimensions in the input array of operation '%s'.\n",
          op_name);
      return kTfLiteError;
    } else {
      for (int i = 0; i < num_dimensions; ++i) {
   d4432:	429c      	cmp	r4, r3
   d4434:	d00e      	beq.n	d4454 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x44>

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
   d4436:	6805      	ldr	r5, [r0, #0]
   d4438:	42ab      	cmp	r3, r5
   d443a:	d305      	bcc.n	d4448 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x38>
   d443c:	4b08      	ldr	r3, [pc, #32]	; (d4460 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x50>)
   d443e:	4a09      	ldr	r2, [pc, #36]	; (d4464 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x54>)
   d4440:	4809      	ldr	r0, [pc, #36]	; (d4468 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x58>)
   d4442:	21ed      	movs	r1, #237	; 0xed
   d4444:	f00f ff82 	bl	e434c <__assert_func>
template<typename T> struct IndirectHelper {
  typedef T return_type;
  typedef T mutable_return_type;
  static const size_t element_stride = sizeof(T);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    return EndianScalar((reinterpret_cast<const T *>(p))[i]);
   d4448:	f852 5f04 	ldr.w	r5, [r2, #4]!
        buffer[i] = flat_vector->Get(i);
   d444c:	f841 5023 	str.w	r5, [r1, r3, lsl #2]
      error_reporter->Report(
          "Found too many dimensions in the input array of operation '%s'.\n",
          op_name);
      return kTfLiteError;
    } else {
      for (int i = 0; i < num_dimensions; ++i) {
   d4450:	3301      	adds	r3, #1
   d4452:	e7ee      	b.n	d4432 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x22>
        buffer[i] = flat_vector->Get(i);
      }
    }
  }
  return kTfLiteOk;
   d4454:	2000      	movs	r0, #0
}
   d4456:	bd38      	pop	{r3, r4, r5, pc}
   d4458:	000e84d2 	.word	0x000e84d2
   d445c:	000e8500 	.word	0x000e8500
   d4460:	000e8541 	.word	0x000e8541
   d4464:	000e8404 	.word	0x000e8404
   d4468:	000e854c 	.word	0x000e854c

000d446c <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>:
// "tables" use an offset table (possibly shared) that allows fields to be
// omitted and added at will, but uses an extra indirection to read.
class Table {
 public:
  const uint8_t *GetVTable() const {
    return data_ - ReadScalar<soffset_t>(data_);
   d446c:	6803      	ldr	r3, [r0, #0]
   d446e:	1ac0      	subs	r0, r0, r3
    auto vtable = GetVTable();
    // The first element is the size of the vtable (fields + type id + itself).
    auto vtsize = ReadScalar<voffset_t>(vtable);
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
   d4470:	8803      	ldrh	r3, [r0, #0]
   d4472:	428b      	cmp	r3, r1
   d4474:	bf8c      	ite	hi
   d4476:	5a40      	ldrhhi	r0, [r0, r1]
   d4478:	2000      	movls	r0, #0
  }
   d447a:	4770      	bx	lr

000d447c <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>:

}  // namespace

TfLiteStatus ConvertTensorType(TensorType tensor_type, TfLiteType* type,
                               ErrorReporter* error_reporter) {
   d447c:	b508      	push	{r3, lr}
   d447e:	4603      	mov	r3, r0
   d4480:	4610      	mov	r0, r2
  *type = kTfLiteNoType;
  switch (tensor_type) {
   d4482:	2b09      	cmp	r3, #9
   d4484:	d806      	bhi.n	d4494 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x18>
   d4486:	e8df f003 	tbb	[pc, r3]
   d448a:	0907      	.short	0x0907
   d448c:	15130f0d 	.word	0x15130f0d
   d4490:	11190b17 	.word	0x11190b17

}  // namespace

TfLiteStatus ConvertTensorType(TensorType tensor_type, TfLiteType* type,
                               ErrorReporter* error_reporter) {
  *type = kTfLiteNoType;
   d4494:	2200      	movs	r2, #0
   d4496:	e012      	b.n	d44be <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
  switch (tensor_type) {
    case TensorType_FLOAT32:
      *type = kTfLiteFloat32;
   d4498:	2201      	movs	r2, #1
   d449a:	e010      	b.n	d44be <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_FLOAT16:
      *type = kTfLiteFloat16;
   d449c:	220a      	movs	r2, #10
   d449e:	e00e      	b.n	d44be <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_INT16:
      *type = kTfLiteInt16;
   d44a0:	2207      	movs	r2, #7
   d44a2:	e00c      	b.n	d44be <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_INT32:
      *type = kTfLiteInt32;
   d44a4:	2202      	movs	r2, #2
   d44a6:	e00a      	b.n	d44be <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_UINT8:
      *type = kTfLiteUInt8;
   d44a8:	2203      	movs	r2, #3
   d44aa:	e008      	b.n	d44be <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_INT8:
      *type = kTfLiteInt8;
   d44ac:	2209      	movs	r2, #9
   d44ae:	e006      	b.n	d44be <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_INT64:
      *type = kTfLiteInt64;
   d44b0:	2204      	movs	r2, #4
   d44b2:	e004      	b.n	d44be <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_STRING:
      *type = kTfLiteString;
   d44b4:	2205      	movs	r2, #5
   d44b6:	e002      	b.n	d44be <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_BOOL:
      *type = kTfLiteBool;
   d44b8:	2206      	movs	r2, #6
   d44ba:	e000      	b.n	d44be <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_COMPLEX64:
      *type = kTfLiteComplex64;
   d44bc:	2208      	movs	r2, #8
   d44be:	700a      	strb	r2, [r1, #0]
      break;
  }
  if (*type == kTfLiteNoType) {
   d44c0:	780a      	ldrb	r2, [r1, #0]
   d44c2:	b92a      	cbnz	r2, d44d0 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x54>
    error_reporter->Report("Unsupported data type %d in tensor\n", tensor_type);
   d44c4:	461a      	mov	r2, r3
   d44c6:	4903      	ldr	r1, [pc, #12]	; (d44d4 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x58>)
   d44c8:	f7ff ff7e 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return kTfLiteError;
   d44cc:	2001      	movs	r0, #1
   d44ce:	bd08      	pop	{r3, pc}
  }
  return kTfLiteOk;
   d44d0:	2000      	movs	r0, #0
}
   d44d2:	bd08      	pop	{r3, pc}
   d44d4:	000e85f8 	.word	0x000e85f8

000d44d8 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>:

  template<typename T> T GetField(voffset_t field, T defaultval) const {
   d44d8:	b538      	push	{r3, r4, r5, lr}
   d44da:	4605      	mov	r5, r0
   d44dc:	4614      	mov	r4, r2
    auto field_offset = GetOptionalFieldOffset(field);
   d44de:	f7ff ffc5 	bl	d446c <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d44e2:	b108      	cbz	r0, d44e8 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_+0x10>
   d44e4:	5c28      	ldrb	r0, [r5, r0]
   d44e6:	bd38      	pop	{r3, r4, r5, pc}
   d44e8:	4620      	mov	r0, r4
  }
   d44ea:	bd38      	pop	{r3, r4, r5, pc}

000d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>:
    return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_INPUTS);
  }
  const flatbuffers::Vector<int32_t> *outputs() const {
    return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_OUTPUTS);
  }
  BuiltinOptions builtin_options_type() const {
   d44ec:	b508      	push	{r3, lr}
    return static_cast<BuiltinOptions>(GetField<uint8_t>(VT_BUILTIN_OPTIONS_TYPE, 0));
   d44ee:	2200      	movs	r2, #0
   d44f0:	210a      	movs	r1, #10
   d44f2:	f7ff fff1 	bl	d44d8 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
  }
   d44f6:	bd08      	pop	{r3, pc}

000d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>:
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
   d44f8:	b538      	push	{r3, r4, r5, lr}
   d44fa:	4605      	mov	r5, r0
   d44fc:	4614      	mov	r4, r2
    auto field_offset = GetOptionalFieldOffset(field);
   d44fe:	f7ff ffb5 	bl	d446c <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d4502:	b108      	cbz	r0, d4508 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_+0x10>
   d4504:	5828      	ldr	r0, [r5, r0]
   d4506:	bd38      	pop	{r3, r4, r5, pc}
   d4508:	4620      	mov	r0, r4
  }
   d450a:	bd38      	pop	{r3, r4, r5, pc}

000d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>:
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
   d450c:	b538      	push	{r3, r4, r5, lr}
   d450e:	4605      	mov	r5, r0
   d4510:	4614      	mov	r4, r2
    auto field_offset = GetOptionalFieldOffset(field);
   d4512:	f7ff ffab 	bl	d446c <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d4516:	b108      	cbz	r0, d451c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_+0x10>
   d4518:	5628      	ldrsb	r0, [r5, r0]
   d451a:	bd38      	pop	{r3, r4, r5, pc}
   d451c:	4620      	mov	r0, r4
  }
   d451e:	bd38      	pop	{r3, r4, r5, pc}

000d4520 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>:
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
   d4520:	b510      	push	{r4, lr}
   d4522:	ed2d 8b02 	vpush	{d8}
   d4526:	4604      	mov	r4, r0
   d4528:	eeb0 8a40 	vmov.f32	s16, s0
    auto field_offset = GetOptionalFieldOffset(field);
   d452c:	f7ff ff9e 	bl	d446c <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d4530:	b118      	cbz	r0, d453a <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_+0x1a>
   d4532:	4420      	add	r0, r4
   d4534:	ed90 0a00 	vldr	s0, [r0]
   d4538:	e001      	b.n	d453e <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_+0x1e>
   d453a:	eeb0 0a48 	vmov.f32	s0, s16
  }
   d453e:	ecbd 8b02 	vpop	{d8}
   d4542:	bd10      	pop	{r4, pc}

000d4544 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorIlEEEET_t>:

  template<typename P> P GetPointer(voffset_t field) {
   d4544:	b510      	push	{r4, lr}
   d4546:	4604      	mov	r4, r0
    auto field_offset = GetOptionalFieldOffset(field);
   d4548:	f7ff ff90 	bl	d446c <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    auto p = data_ + field_offset;
   d454c:	1822      	adds	r2, r4, r0
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
   d454e:	b108      	cbz	r0, d4554 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorIlEEEET_t+0x10>
   d4550:	5823      	ldr	r3, [r4, r0]
   d4552:	18d0      	adds	r0, r2, r3
  }
   d4554:	bd10      	pop	{r4, pc}

000d4556 <_ZNK6tflite8Operator15builtin_optionsEv>:
  const void *builtin_options() const {
   d4556:	b508      	push	{r3, lr}
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d4558:	210c      	movs	r1, #12
   d455a:	f7ff fff3 	bl	d4544 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorIlEEEET_t>
    return GetPointer<const void *>(VT_BUILTIN_OPTIONS);
  }
   d455e:	bd08      	pop	{r3, pc}

000d4560 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI23TfLiteSequenceRNNParamsEEPT_v>:
  // extension part will take ownership so destructors  will not be run during
  // deallocation.
  template <typename T>
  T* AllocatePOD() {
    static_assert(std::is_pod<T>::value, "Builtin data structure must be POD.");
    return static_cast<T*>(this->Allocate(sizeof(T)));
   d4560:	6803      	ldr	r3, [r0, #0]
   d4562:	2102      	movs	r1, #2
   d4564:	681b      	ldr	r3, [r3, #0]
   d4566:	4718      	bx	r3

000d4568 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI26TfLiteFullyConnectedParamsEEPT_v>:
   d4568:	6803      	ldr	r3, [r0, #0]
   d456a:	2103      	movs	r1, #3
   d456c:	681b      	ldr	r3, [r3, #0]
   d456e:	4718      	bx	r3

000d4570 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI19TfLiteSqueezeParamsEEPT_v>:
   d4570:	6803      	ldr	r3, [r0, #0]
   d4572:	2124      	movs	r1, #36	; 0x24
   d4574:	681b      	ldr	r3, [r3, #0]
   d4576:	4718      	bx	r3

000d4578 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI25TfLiteTransposeConvParamsEEPT_v>:
   d4578:	6803      	ldr	r3, [r0, #0]
   d457a:	210c      	movs	r1, #12
   d457c:	681b      	ldr	r3, [r3, #0]
   d457e:	4718      	bx	r3

000d4580 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteFakeQuantParamsEEPT_v>:
   d4580:	6803      	ldr	r3, [r0, #0]
   d4582:	2110      	movs	r1, #16
   d4584:	681b      	ldr	r3, [r3, #0]
   d4586:	4718      	bx	r3

000d4588 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>:
   d4588:	6803      	ldr	r3, [r0, #0]
   d458a:	2104      	movs	r1, #4
   d458c:	681b      	ldr	r3, [r3, #0]
   d458e:	4718      	bx	r3

000d4590 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>:
   d4590:	6803      	ldr	r3, [r0, #0]
   d4592:	2101      	movs	r1, #1
   d4594:	681b      	ldr	r3, [r3, #0]
   d4596:	4718      	bx	r3

000d4598 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>:
   d4598:	6803      	ldr	r3, [r0, #0]
   d459a:	2108      	movs	r1, #8
   d459c:	681b      	ldr	r3, [r3, #0]
   d459e:	4718      	bx	r3

000d45a0 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv>:
// If it returns kTfLiteOk, it passes the data out with `builtin_data`, which
// need to be released by calling `free`.`
// If it returns kTfLiteError, `builtin_data` will be `nullptr`.
TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,
                         ErrorReporter* error_reporter,
                         BuiltinDataAllocator* allocator, void** builtin_data) {
   d45a0:	e92d 41f3 	stmdb	sp!, {r0, r1, r4, r5, r6, r7, r8, lr}
   d45a4:	9e08      	ldr	r6, [sp, #32]
   d45a6:	461d      	mov	r5, r3
        return kTfLiteCombinerTypeSum;
    }
  };

  SafeBuiltinDataAllocator safe_allocator(allocator);
  *builtin_data = nullptr;
   d45a8:	2300      	movs	r3, #0
// If it returns kTfLiteOk, it passes the data out with `builtin_data`, which
// need to be released by calling `free`.`
// If it returns kTfLiteError, `builtin_data` will be `nullptr`.
TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,
                         ErrorReporter* error_reporter,
                         BuiltinDataAllocator* allocator, void** builtin_data) {
   d45aa:	4604      	mov	r4, r0
   d45ac:	4617      	mov	r7, r2
        return kTfLiteCombinerTypeSum;
    }
  };

  SafeBuiltinDataAllocator safe_allocator(allocator);
  *builtin_data = nullptr;
   d45ae:	6033      	str	r3, [r6, #0]
   d45b0:	4698      	mov	r8, r3
  switch (op_type) {
   d45b2:	2977      	cmp	r1, #119	; 0x77
   d45b4:	f200 870b 	bhi.w	d53ce <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2e>
   d45b8:	e8df f011 	tbh	[pc, r1, lsl #1]
   d45bc:	010402be 	.word	0x010402be
   d45c0:	00780283 	.word	0x00780283
   d45c4:	048f0142 	.word	0x048f0142
   d45c8:	07090709 	.word	0x07090709
   d45cc:	02300709 	.word	0x02300709
   d45d0:	030b0709 	.word	0x030b0709
   d45d4:	03240104 	.word	0x03240104
   d45d8:	00e60709 	.word	0x00e60709
   d45dc:	01040353 	.word	0x01040353
   d45e0:	070902a4 	.word	0x070902a4
   d45e4:	07090709 	.word	0x07090709
   d45e8:	0400043b 	.word	0x0400043b
   d45ec:	026a01f9 	.word	0x026a01f9
   d45f0:	01860479 	.word	0x01860479
   d45f4:	07090709 	.word	0x07090709
   d45f8:	07090453 	.word	0x07090453
   d45fc:	02130709 	.word	0x02130709
   d4600:	01a80709 	.word	0x01a80709
   d4604:	070904a5 	.word	0x070904a5
   d4608:	07090709 	.word	0x07090709
   d460c:	02f204bd 	.word	0x02f204bd
   d4610:	050202d8 	.word	0x050202d8
   d4614:	05280391 	.word	0x05280391
   d4618:	070901cc 	.word	0x070901cc
   d461c:	04d60709 	.word	0x04d60709
   d4620:	06070709 	.word	0x06070709
   d4624:	00b703c4 	.word	0x00b703c4
   d4628:	07090709 	.word	0x07090709
   d462c:	07090559 	.word	0x07090559
   d4630:	07090709 	.word	0x07090709
   d4634:	07090709 	.word	0x07090709
   d4638:	07090709 	.word	0x07090709
   d463c:	07090709 	.word	0x07090709
   d4640:	058d0709 	.word	0x058d0709
   d4644:	070905b3 	.word	0x070905b3
   d4648:	07090709 	.word	0x07090709
   d464c:	07090709 	.word	0x07090709
   d4650:	070904bd 	.word	0x070904bd
   d4654:	05cc0709 	.word	0x05cc0709
   d4658:	05730709 	.word	0x05730709
   d465c:	04bd060d 	.word	0x04bd060d
   d4660:	05ea04bd 	.word	0x05ea04bd
   d4664:	063d0709 	.word	0x063d0709
   d4668:	07090709 	.word	0x07090709
   d466c:	04bd0653 	.word	0x04bd0653
   d4670:	04bd0709 	.word	0x04bd0709
   d4674:	07090709 	.word	0x07090709
   d4678:	07090709 	.word	0x07090709
   d467c:	04220709 	.word	0x04220709
   d4680:	07090670 	.word	0x07090670
   d4684:	07090688 	.word	0x07090688
   d4688:	06a104ec 	.word	0x06a104ec
   d468c:	07090709 	.word	0x07090709
   d4690:	07090709 	.word	0x07090709
   d4694:	07090709 	.word	0x07090709
   d4698:	07090709 	.word	0x07090709
   d469c:	070906ba 	.word	0x070906ba
   d46a0:	07090709 	.word	0x07090709
   d46a4:	07090709 	.word	0x07090709
   d46a8:	06ef06d5 	.word	0x06ef06d5
   d46ac:	682b      	ldr	r3, [r5, #0]
   d46ae:	2118      	movs	r1, #24
   d46b0:	681b      	ldr	r3, [r3, #0]
   d46b2:	4628      	mov	r0, r5
   d46b4:	4798      	blx	r3
   d46b6:	4605      	mov	r5, r0
  template<typename T> const T *builtin_options_as() const;
  const Conv2DOptions *builtin_options_as_Conv2DOptions() const {
    return builtin_options_type() == BuiltinOptions_Conv2DOptions ? static_cast<const Conv2DOptions *>(builtin_options()) : nullptr;
   d46b8:	4620      	mov	r0, r4
   d46ba:	f7ff ff17 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d46be:	2801      	cmp	r0, #1
   d46c0:	4607      	mov	r7, r0
   d46c2:	f040 8683 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d46c6:	4620      	mov	r0, r4
   d46c8:	f7ff ff45 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
    case BuiltinOperator_CONV_2D: {
      auto params = safe_allocator.Allocate<TfLiteConvParams>();
      if (auto* conv_params = op->builtin_options_as_Conv2DOptions()) {
   d46cc:	4604      	mov	r4, r0
   d46ce:	2800      	cmp	r0, #0
   d46d0:	f000 867c 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_FUSED_ACTIVATION_FUNCTION = 10,
    VT_DILATION_W_FACTOR = 12,
    VT_DILATION_H_FACTOR = 14
  };
  Padding padding() const {
    return static_cast<Padding>(GetField<int8_t>(VT_PADDING, 0));
   d46d4:	2200      	movs	r2, #0
   d46d6:	2104      	movs	r1, #4
   d46d8:	f7ff ff18 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->padding = parse_padding(conv_params->padding());
   d46dc:	b2c0      	uxtb	r0, r0
   d46de:	f7ff fe85 	bl	d43ec <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_7PaddingEE_clESA_.isra.0>
  }
  int32_t stride_w() const {
    return GetField<int32_t>(VT_STRIDE_W, 0);
   d46e2:	2200      	movs	r2, #0
   d46e4:	7028      	strb	r0, [r5, #0]
   d46e6:	2106      	movs	r1, #6
   d46e8:	4620      	mov	r0, r4
   d46ea:	f7ff ff05 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t stride_h() const {
    return GetField<int32_t>(VT_STRIDE_H, 0);
   d46ee:	2200      	movs	r2, #0
        params->stride_width = conv_params->stride_w();
   d46f0:	6068      	str	r0, [r5, #4]
   d46f2:	2108      	movs	r1, #8
   d46f4:	4620      	mov	r0, r4
   d46f6:	f7ff feff 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d46fa:	2200      	movs	r2, #0
   d46fc:	210a      	movs	r1, #10
        params->stride_height = conv_params->stride_h();
   d46fe:	60a8      	str	r0, [r5, #8]
   d4700:	4620      	mov	r0, r4
   d4702:	f7ff ff03 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(conv_params->fused_activation_function());
   d4706:	b2c0      	uxtb	r0, r0
   d4708:	f7ff fe78 	bl	d43fc <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  }
  int32_t dilation_w_factor() const {
    return GetField<int32_t>(VT_DILATION_W_FACTOR, 1);
   d470c:	463a      	mov	r2, r7
   d470e:	7528      	strb	r0, [r5, #20]
   d4710:	210c      	movs	r1, #12
   d4712:	4620      	mov	r0, r4
   d4714:	f7ff fef0 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t dilation_h_factor() const {
    return GetField<int32_t>(VT_DILATION_H_FACTOR, 1);
   d4718:	463a      	mov	r2, r7

        params->dilation_width_factor = conv_params->dilation_w_factor();
   d471a:	60e8      	str	r0, [r5, #12]
   d471c:	210e      	movs	r1, #14
   d471e:	4620      	mov	r0, r4
   d4720:	f7ff feea 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->dilation_height_factor = conv_params->dilation_h_factor();
   d4724:	6128      	str	r0, [r5, #16]
   d4726:	f000 be51 	b.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d472a:	4628      	mov	r0, r5
   d472c:	f7ff ff18 	bl	d4560 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI23TfLiteSequenceRNNParamsEEPT_v>
   d4730:	4680      	mov	r8, r0
  }
  const LogSoftmaxOptions *builtin_options_as_LogSoftmaxOptions() const {
    return builtin_options_type() == BuiltinOptions_LogSoftmaxOptions ? static_cast<const LogSoftmaxOptions *>(builtin_options()) : nullptr;
  }
  const CastOptions *builtin_options_as_CastOptions() const {
    return builtin_options_type() == BuiltinOptions_CastOptions ? static_cast<const CastOptions *>(builtin_options()) : nullptr;
   d4732:	4620      	mov	r0, r4
      constexpr _Head_base(const _Head_base&) = default;
      constexpr _Head_base(_Head_base&&) = default;

      template<typename _UHead>
        constexpr _Head_base(_UHead&& __h)
	: _M_head_impl(std::forward<_UHead>(__h)) { }
   d4734:	e88d 0120 	stmia.w	sp, {r5, r8}
   d4738:	f7ff fed8 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d473c:	2825      	cmp	r0, #37	; 0x25
   d473e:	f040 8462 	bne.w	d5006 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
   d4742:	4620      	mov	r0, r4
   d4744:	f7ff ff07 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_CAST: {
      auto params = safe_allocator.Allocate<TfLiteCastParams>();
      if (const auto* schema_params = op->builtin_options_as_CastOptions()) {
   d4748:	4605      	mov	r5, r0
   d474a:	2800      	cmp	r0, #0
   d474c:	f000 845b 	beq.w	d5006 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_IN_DATA_TYPE = 4,
    VT_OUT_DATA_TYPE = 6
  };
  TensorType in_data_type() const {
    return static_cast<TensorType>(GetField<int8_t>(VT_IN_DATA_TYPE, 0));
   d4750:	2200      	movs	r2, #0
   d4752:	2104      	movs	r1, #4
   d4754:	f7ff feda 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        auto in_status =
            ConvertTensorType(schema_params->in_data_type(),
                              &params->in_data_type, error_reporter);
   d4758:	463a      	mov	r2, r7
   d475a:	4641      	mov	r1, r8
   d475c:	b2c0      	uxtb	r0, r0
   d475e:	f7ff fe8d 	bl	d447c <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
  }
  TensorType out_data_type() const {
    return static_cast<TensorType>(GetField<int8_t>(VT_OUT_DATA_TYPE, 0));
   d4762:	2200      	movs	r2, #0
   d4764:	4604      	mov	r4, r0
   d4766:	2106      	movs	r1, #6
   d4768:	4628      	mov	r0, r5
   d476a:	f7ff fecf 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        auto out_status =
            ConvertTensorType(schema_params->out_data_type(),
                              &params->out_data_type, error_reporter);
   d476e:	9901      	ldr	r1, [sp, #4]
   d4770:	463a      	mov	r2, r7
   d4772:	3101      	adds	r1, #1
   d4774:	b2c0      	uxtb	r0, r0
   d4776:	f7ff fe81 	bl	d447c <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
        if (in_status != kTfLiteOk || out_status != kTfLiteOk) {
   d477a:	2c00      	cmp	r4, #0
   d477c:	f040 8186 	bne.w	d4a8c <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x4ec>
   d4780:	2800      	cmp	r0, #0
   d4782:	f000 8440 	beq.w	d5006 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
   d4786:	e181      	b.n	d4a8c <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x4ec>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4788:	4628      	mov	r0, r5
   d478a:	f7ff ff01 	bl	d4590 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d478e:	4605      	mov	r5, r0
  }
  const ConcatEmbeddingsOptions *builtin_options_as_ConcatEmbeddingsOptions() const {
    return builtin_options_type() == BuiltinOptions_ConcatEmbeddingsOptions ? static_cast<const ConcatEmbeddingsOptions *>(builtin_options()) : nullptr;
  }
  const LSHProjectionOptions *builtin_options_as_LSHProjectionOptions() const {
    return builtin_options_type() == BuiltinOptions_LSHProjectionOptions ? static_cast<const LSHProjectionOptions *>(builtin_options()) : nullptr;
   d4790:	4620      	mov	r0, r4
   d4792:	f7ff feab 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4796:	2804      	cmp	r0, #4
   d4798:	4607      	mov	r7, r0
   d479a:	f040 8617 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d479e:	4620      	mov	r0, r4
   d47a0:	f7ff fed9 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_LSH_PROJECTION: {
      auto params = safe_allocator.Allocate<TfLiteLSHProjectionParams>();
      if (const auto* lshParams =
   d47a4:	2800      	cmp	r0, #0
   d47a6:	f000 8611 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef LSHProjectionOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TYPE = 4
  };
  LSHProjectionType type() const {
    return static_cast<LSHProjectionType>(GetField<int8_t>(VT_TYPE, 0));
   d47aa:	2200      	movs	r2, #0
   d47ac:	4639      	mov	r1, r7
   d47ae:	f7ff fead 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
      default:
        return kTfLiteLshProjectionUnknown;
    }
  };
  auto parseCombinerType = [](CombinerType type) {
    switch (type) {
   d47b2:	2801      	cmp	r0, #1
   d47b4:	d003      	beq.n	d47be <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x21e>
        return kTfLiteCombinerTypeMean;
      case CombinerType_SQRTN:
        return kTfLiteCombinerTypeSqrtn;
      case CombinerType_SUM:
      default:
        return kTfLiteCombinerTypeSum;
   d47b6:	2802      	cmp	r0, #2
   d47b8:	bf0c      	ite	eq
   d47ba:	2002      	moveq	r0, #2
   d47bc:	2000      	movne	r0, #0
    }
    case BuiltinOperator_LSH_PROJECTION: {
      auto params = safe_allocator.Allocate<TfLiteLSHProjectionParams>();
      if (const auto* lshParams =
              op->builtin_options_as_LSHProjectionOptions()) {
        params->type = parseLSHProjectionType(lshParams->type());
   d47be:	7028      	strb	r0, [r5, #0]
   d47c0:	f000 be04 	b.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d47c4:	682b      	ldr	r3, [r5, #0]
   d47c6:	2128      	movs	r1, #40	; 0x28
   d47c8:	681b      	ldr	r3, [r3, #0]
   d47ca:	4628      	mov	r0, r5
   d47cc:	4798      	blx	r3
   d47ce:	4605      	mov	r5, r0
  }
  const LSHProjectionOptions *builtin_options_as_LSHProjectionOptions() const {
    return builtin_options_type() == BuiltinOptions_LSHProjectionOptions ? static_cast<const LSHProjectionOptions *>(builtin_options()) : nullptr;
  }
  const Pool2DOptions *builtin_options_as_Pool2DOptions() const {
    return builtin_options_type() == BuiltinOptions_Pool2DOptions ? static_cast<const Pool2DOptions *>(builtin_options()) : nullptr;
   d47d0:	4620      	mov	r0, r4
   d47d2:	f7ff fe8b 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d47d6:	2805      	cmp	r0, #5
   d47d8:	f040 85f8 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d47dc:	4620      	mov	r0, r4
   d47de:	f7ff feba 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
    }
    case BuiltinOperator_AVERAGE_POOL_2D:
    case BuiltinOperator_MAX_POOL_2D:
    case BuiltinOperator_L2_POOL_2D: {
      auto params = safe_allocator.Allocate<TfLitePoolParams>();
      if (const auto* pool_params = op->builtin_options_as_Pool2DOptions()) {
   d47e2:	4604      	mov	r4, r0
   d47e4:	2800      	cmp	r0, #0
   d47e6:	f000 85f1 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_FILTER_WIDTH = 10,
    VT_FILTER_HEIGHT = 12,
    VT_FUSED_ACTIVATION_FUNCTION = 14
  };
  Padding padding() const {
    return static_cast<Padding>(GetField<int8_t>(VT_PADDING, 0));
   d47ea:	2200      	movs	r2, #0
   d47ec:	2104      	movs	r1, #4
   d47ee:	f7ff fe8d 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->padding = parse_padding(pool_params->padding());
   d47f2:	b2c0      	uxtb	r0, r0
   d47f4:	f7ff fdfa 	bl	d43ec <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_7PaddingEE_clESA_.isra.0>
  }
  int32_t stride_w() const {
    return GetField<int32_t>(VT_STRIDE_W, 0);
   d47f8:	2200      	movs	r2, #0
   d47fa:	7028      	strb	r0, [r5, #0]
   d47fc:	2106      	movs	r1, #6
   d47fe:	4620      	mov	r0, r4
   d4800:	f7ff fe7a 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t stride_h() const {
    return GetField<int32_t>(VT_STRIDE_H, 0);
   d4804:	2200      	movs	r2, #0
        params->stride_width = pool_params->stride_w();
   d4806:	6068      	str	r0, [r5, #4]
   d4808:	2108      	movs	r1, #8
   d480a:	4620      	mov	r0, r4
   d480c:	f7ff fe74 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t filter_width() const {
    return GetField<int32_t>(VT_FILTER_WIDTH, 0);
   d4810:	2200      	movs	r2, #0
        params->stride_height = pool_params->stride_h();
   d4812:	60a8      	str	r0, [r5, #8]
   d4814:	210a      	movs	r1, #10
   d4816:	4620      	mov	r0, r4
   d4818:	f7ff fe6e 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t filter_height() const {
    return GetField<int32_t>(VT_FILTER_HEIGHT, 0);
   d481c:	2200      	movs	r2, #0
        params->filter_width = pool_params->filter_width();
   d481e:	60e8      	str	r0, [r5, #12]
   d4820:	210c      	movs	r1, #12
   d4822:	4620      	mov	r0, r4
   d4824:	f7ff fe68 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4828:	2200      	movs	r2, #0
        params->filter_height = pool_params->filter_height();
   d482a:	6128      	str	r0, [r5, #16]
   d482c:	210e      	movs	r1, #14
   d482e:	4620      	mov	r0, r4
   d4830:	f7ff fe6c 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(pool_params->fused_activation_function());
   d4834:	b2c0      	uxtb	r0, r0
   d4836:	f7ff fde1 	bl	d43fc <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
   d483a:	7528      	strb	r0, [r5, #20]
   d483c:	f000 bdc6 	b.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4840:	682b      	ldr	r3, [r5, #0]
   d4842:	211c      	movs	r1, #28
   d4844:	681b      	ldr	r3, [r3, #0]
   d4846:	4628      	mov	r0, r5
   d4848:	4798      	blx	r3
   d484a:	4605      	mov	r5, r0
  template<typename T> const T *builtin_options_as() const;
  const Conv2DOptions *builtin_options_as_Conv2DOptions() const {
    return builtin_options_type() == BuiltinOptions_Conv2DOptions ? static_cast<const Conv2DOptions *>(builtin_options()) : nullptr;
  }
  const DepthwiseConv2DOptions *builtin_options_as_DepthwiseConv2DOptions() const {
    return builtin_options_type() == BuiltinOptions_DepthwiseConv2DOptions ? static_cast<const DepthwiseConv2DOptions *>(builtin_options()) : nullptr;
   d484c:	4620      	mov	r0, r4
   d484e:	f7ff fe4d 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4852:	2802      	cmp	r0, #2
   d4854:	f040 85ba 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4858:	4620      	mov	r0, r4
   d485a:	f7ff fe7c 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_DEPTHWISE_CONV_2D: {
      auto params = safe_allocator.Allocate<TfLiteDepthwiseConvParams>();
      if (const auto* conv_params =
   d485e:	4604      	mov	r4, r0
   d4860:	2800      	cmp	r0, #0
   d4862:	f000 85b3 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_FUSED_ACTIVATION_FUNCTION = 12,
    VT_DILATION_W_FACTOR = 14,
    VT_DILATION_H_FACTOR = 16
  };
  Padding padding() const {
    return static_cast<Padding>(GetField<int8_t>(VT_PADDING, 0));
   d4866:	2200      	movs	r2, #0
   d4868:	2104      	movs	r1, #4
   d486a:	f7ff fe4f 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_DepthwiseConv2DOptions()) {
        params->padding = parse_padding(conv_params->padding());
   d486e:	b2c0      	uxtb	r0, r0
   d4870:	f7ff fdbc 	bl	d43ec <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_7PaddingEE_clESA_.isra.0>
  }
  int32_t stride_w() const {
    return GetField<int32_t>(VT_STRIDE_W, 0);
   d4874:	2200      	movs	r2, #0
   d4876:	7028      	strb	r0, [r5, #0]
   d4878:	2106      	movs	r1, #6
   d487a:	4620      	mov	r0, r4
   d487c:	f7ff fe3c 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t stride_h() const {
    return GetField<int32_t>(VT_STRIDE_H, 0);
   d4880:	2200      	movs	r2, #0
        params->stride_width = conv_params->stride_w();
   d4882:	6068      	str	r0, [r5, #4]
   d4884:	2108      	movs	r1, #8
   d4886:	4620      	mov	r0, r4
   d4888:	f7ff fe36 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t depth_multiplier() const {
    return GetField<int32_t>(VT_DEPTH_MULTIPLIER, 0);
   d488c:	2200      	movs	r2, #0
        params->stride_height = conv_params->stride_h();
   d488e:	60a8      	str	r0, [r5, #8]
   d4890:	210a      	movs	r1, #10
   d4892:	4620      	mov	r0, r4
   d4894:	f7ff fe30 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4898:	2200      	movs	r2, #0
   d489a:	210c      	movs	r1, #12
        params->depth_multiplier = conv_params->depth_multiplier();
   d489c:	60e8      	str	r0, [r5, #12]
   d489e:	4620      	mov	r0, r4
   d48a0:	f7ff fe34 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(conv_params->fused_activation_function());
   d48a4:	b2c0      	uxtb	r0, r0
   d48a6:	f7ff fda9 	bl	d43fc <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  }
  int32_t dilation_w_factor() const {
    return GetField<int32_t>(VT_DILATION_W_FACTOR, 1);
   d48aa:	2201      	movs	r2, #1
   d48ac:	7428      	strb	r0, [r5, #16]
   d48ae:	210e      	movs	r1, #14
   d48b0:	4620      	mov	r0, r4
   d48b2:	f7ff fe21 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t dilation_h_factor() const {
    return GetField<int32_t>(VT_DILATION_H_FACTOR, 1);
   d48b6:	2201      	movs	r2, #1

        params->dilation_width_factor = conv_params->dilation_w_factor();
   d48b8:	6168      	str	r0, [r5, #20]
   d48ba:	2110      	movs	r1, #16
   d48bc:	4620      	mov	r0, r4
   d48be:	f7ff fe1b 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->dilation_height_factor = conv_params->dilation_h_factor();
   d48c2:	61a8      	str	r0, [r5, #24]
   d48c4:	f000 bd82 	b.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d48c8:	4628      	mov	r0, r5
   d48ca:	f7ff fe65 	bl	d4598 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
   d48ce:	4605      	mov	r5, r0
  }
  const Pool2DOptions *builtin_options_as_Pool2DOptions() const {
    return builtin_options_type() == BuiltinOptions_Pool2DOptions ? static_cast<const Pool2DOptions *>(builtin_options()) : nullptr;
  }
  const SVDFOptions *builtin_options_as_SVDFOptions() const {
    return builtin_options_type() == BuiltinOptions_SVDFOptions ? static_cast<const SVDFOptions *>(builtin_options()) : nullptr;
   d48d0:	4620      	mov	r0, r4
   d48d2:	f7ff fe0b 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d48d6:	2806      	cmp	r0, #6
   d48d8:	4607      	mov	r7, r0
   d48da:	f040 8577 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d48de:	4620      	mov	r0, r4
   d48e0:	f7ff fe39 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SVDF: {
      auto params = safe_allocator.Allocate<TfLiteSVDFParams>();
      if (const auto* svdf_params = op->builtin_options_as_SVDFOptions()) {
   d48e4:	4604      	mov	r4, r0
   d48e6:	2800      	cmp	r0, #0
   d48e8:	f000 8570 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_RANK = 4,
    VT_FUSED_ACTIVATION_FUNCTION = 6
  };
  int32_t rank() const {
    return GetField<int32_t>(VT_RANK, 0);
   d48ec:	2200      	movs	r2, #0
   d48ee:	2104      	movs	r1, #4
   d48f0:	f7ff fe02 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d48f4:	2200      	movs	r2, #0
        params->rank = svdf_params->rank();
   d48f6:	6028      	str	r0, [r5, #0]
   d48f8:	4639      	mov	r1, r7
   d48fa:	4620      	mov	r0, r4
   d48fc:	f7ff fe06 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(svdf_params->fused_activation_function());
   d4900:	b2c0      	uxtb	r0, r0
   d4902:	f7ff fd7b 	bl	d43fc <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
   d4906:	7128      	strb	r0, [r5, #4]
   d4908:	f000 bd60 	b.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d490c:	4628      	mov	r0, r5
   d490e:	f7ff fe27 	bl	d4560 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI23TfLiteSequenceRNNParamsEEPT_v>
   d4912:	4605      	mov	r5, r0
  }
  const SqueezeOptions *builtin_options_as_SqueezeOptions() const {
    return builtin_options_type() == BuiltinOptions_SqueezeOptions ? static_cast<const SqueezeOptions *>(builtin_options()) : nullptr;
  }
  const SequenceRNNOptions *builtin_options_as_SequenceRNNOptions() const {
    return builtin_options_type() == BuiltinOptions_SequenceRNNOptions ? static_cast<const SequenceRNNOptions *>(builtin_options()) : nullptr;
   d4914:	4620      	mov	r0, r4
   d4916:	f7ff fde9 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d491a:	281f      	cmp	r0, #31
   d491c:	f040 8556 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4920:	4620      	mov	r0, r4
   d4922:	f7ff fe18 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_UNIDIRECTIONAL_SEQUENCE_RNN: {
      auto params = safe_allocator.Allocate<TfLiteSequenceRNNParams>();
      if (const auto* sequence_rnn_params =
   d4926:	4604      	mov	r4, r0
   d4928:	2800      	cmp	r0, #0
   d492a:	f000 854f 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  };
  bool time_major() const {
    return GetField<uint8_t>(VT_TIME_MAJOR, 0) != 0;
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d492e:	2200      	movs	r2, #0
   d4930:	2106      	movs	r1, #6
   d4932:	f7ff fdeb 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_SequenceRNNOptions()) {
        params->activation =
            parse_activation(sequence_rnn_params->fused_activation_function());
   d4936:	b2c0      	uxtb	r0, r0
   d4938:	f7ff fd60 	bl	d43fc <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TIME_MAJOR = 4,
    VT_FUSED_ACTIVATION_FUNCTION = 6
  };
  bool time_major() const {
    return GetField<uint8_t>(VT_TIME_MAJOR, 0) != 0;
   d493c:	2200      	movs	r2, #0
   d493e:	7068      	strb	r0, [r5, #1]
   d4940:	2104      	movs	r1, #4
   d4942:	4620      	mov	r0, r4
   d4944:	f7ff fdc8 	bl	d44d8 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->time_major = sequence_rnn_params->time_major();
   d4948:	3000      	adds	r0, #0
   d494a:	bf18      	it	ne
   d494c:	2001      	movne	r0, #1
   d494e:	7028      	strb	r0, [r5, #0]
   d4950:	f000 bd3c 	b.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4954:	4628      	mov	r0, r5
   d4956:	f7ff fe07 	bl	d4568 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI26TfLiteFullyConnectedParamsEEPT_v>
   d495a:	4605      	mov	r5, r0
  }
  const BidirectionalSequenceLSTMOptions *builtin_options_as_BidirectionalSequenceLSTMOptions() const {
    return builtin_options_type() == BuiltinOptions_BidirectionalSequenceLSTMOptions ? static_cast<const BidirectionalSequenceLSTMOptions *>(builtin_options()) : nullptr;
  }
  const BidirectionalSequenceRNNOptions *builtin_options_as_BidirectionalSequenceRNNOptions() const {
    return builtin_options_type() == BuiltinOptions_BidirectionalSequenceRNNOptions ? static_cast<const BidirectionalSequenceRNNOptions *>(builtin_options()) : nullptr;
   d495c:	4620      	mov	r0, r4
   d495e:	f7ff fdc5 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4962:	2846      	cmp	r0, #70	; 0x46
   d4964:	f040 8532 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4968:	4620      	mov	r0, r4
   d496a:	f7ff fdf4 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_BIDIRECTIONAL_SEQUENCE_RNN: {
      auto params =
          safe_allocator.Allocate<TfLiteBidirectionalSequenceRNNParams>();
      if (const auto* bidi_sequence_rnn_params =
   d496e:	4604      	mov	r4, r0
   d4970:	2800      	cmp	r0, #0
   d4972:	f000 852b 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  };
  bool time_major() const {
    return GetField<uint8_t>(VT_TIME_MAJOR, 0) != 0;
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4976:	2200      	movs	r2, #0
   d4978:	2106      	movs	r1, #6
   d497a:	f7ff fdc7 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_BidirectionalSequenceRNNOptions()) {
        params->activation = parse_activation(
   d497e:	b2c0      	uxtb	r0, r0
   d4980:	f7ff fd3c 	bl	d43fc <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
    VT_TIME_MAJOR = 4,
    VT_FUSED_ACTIVATION_FUNCTION = 6,
    VT_MERGE_OUTPUTS = 8
  };
  bool time_major() const {
    return GetField<uint8_t>(VT_TIME_MAJOR, 0) != 0;
   d4984:	2200      	movs	r2, #0
            bidi_sequence_rnn_params->fused_activation_function());
   d4986:	7068      	strb	r0, [r5, #1]
   d4988:	2104      	movs	r1, #4
   d498a:	4620      	mov	r0, r4
   d498c:	f7ff fda4 	bl	d44d8 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->time_major = bidi_sequence_rnn_params->time_major();
   d4990:	3000      	adds	r0, #0
   d4992:	bf18      	it	ne
   d4994:	2001      	movne	r0, #1
   d4996:	7028      	strb	r0, [r5, #0]
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
  }
  bool merge_outputs() const {
    return GetField<uint8_t>(VT_MERGE_OUTPUTS, 0) != 0;
   d4998:	2200      	movs	r2, #0
   d499a:	2108      	movs	r1, #8
   d499c:	4620      	mov	r0, r4
   d499e:	f7ff fd9b 	bl	d44d8 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->merge_outputs = bidi_sequence_rnn_params->merge_outputs();
   d49a2:	3000      	adds	r0, #0
   d49a4:	bf18      	it	ne
   d49a6:	2001      	movne	r0, #1
   d49a8:	70a8      	strb	r0, [r5, #2]
   d49aa:	f000 bd0f 	b.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d49ae:	4628      	mov	r0, r5
   d49b0:	f7ff fdee 	bl	d4590 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d49b4:	4605      	mov	r5, r0
  }
  const SVDFOptions *builtin_options_as_SVDFOptions() const {
    return builtin_options_type() == BuiltinOptions_SVDFOptions ? static_cast<const SVDFOptions *>(builtin_options()) : nullptr;
  }
  const RNNOptions *builtin_options_as_RNNOptions() const {
    return builtin_options_type() == BuiltinOptions_RNNOptions ? static_cast<const RNNOptions *>(builtin_options()) : nullptr;
   d49b6:	4620      	mov	r0, r4
   d49b8:	f7ff fd98 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d49bc:	2807      	cmp	r0, #7
   d49be:	f040 8505 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d49c2:	4620      	mov	r0, r4
   d49c4:	f7ff fdc7 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_RNN: {
      auto params = safe_allocator.Allocate<TfLiteRNNParams>();
      if (const auto* rnn_params = op->builtin_options_as_RNNOptions()) {
   d49c8:	2800      	cmp	r0, #0
   d49ca:	f000 84ff 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef RNNOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSED_ACTIVATION_FUNCTION = 4
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d49ce:	2200      	movs	r2, #0
   d49d0:	2104      	movs	r1, #4
   d49d2:	f7ff fd9b 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(rnn_params->fused_activation_function());
   d49d6:	b2c0      	uxtb	r0, r0
   d49d8:	f7ff fd10 	bl	d43fc <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
   d49dc:	7028      	strb	r0, [r5, #0]
   d49de:	f000 bcf5 	b.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d49e2:	4628      	mov	r0, r5
   d49e4:	f7ff fdd4 	bl	d4590 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d49e8:	4605      	mov	r5, r0
  }
  const SpaceToDepthOptions *builtin_options_as_SpaceToDepthOptions() const {
    return builtin_options_type() == BuiltinOptions_SpaceToDepthOptions ? static_cast<const SpaceToDepthOptions *>(builtin_options()) : nullptr;
  }
  const EmbeddingLookupSparseOptions *builtin_options_as_EmbeddingLookupSparseOptions() const {
    return builtin_options_type() == BuiltinOptions_EmbeddingLookupSparseOptions ? static_cast<const EmbeddingLookupSparseOptions *>(builtin_options()) : nullptr;
   d49ea:	4620      	mov	r0, r4
   d49ec:	f7ff fd7e 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d49f0:	2814      	cmp	r0, #20
   d49f2:	f040 84eb 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d49f6:	4620      	mov	r0, r4
   d49f8:	f7ff fdad 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_EMBEDDING_LOOKUP_SPARSE: {
      auto params =
          safe_allocator.Allocate<TfLiteEmbeddingLookupSparseParams>();
      if (const auto* embedding_params =
   d49fc:	2800      	cmp	r0, #0
   d49fe:	f000 84e5 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef EmbeddingLookupSparseOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_COMBINER = 4
  };
  CombinerType combiner() const {
    return static_cast<CombinerType>(GetField<int8_t>(VT_COMBINER, 0));
   d4a02:	2200      	movs	r2, #0
   d4a04:	2104      	movs	r1, #4
   d4a06:	f7ff fd81 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
      default:
        return kTfLiteLshProjectionUnknown;
    }
  };
  auto parseCombinerType = [](CombinerType type) {
    switch (type) {
   d4a0a:	2801      	cmp	r0, #1
   d4a0c:	d003      	beq.n	d4a16 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x476>
        return kTfLiteCombinerTypeMean;
      case CombinerType_SQRTN:
        return kTfLiteCombinerTypeSqrtn;
      case CombinerType_SUM:
      default:
        return kTfLiteCombinerTypeSum;
   d4a0e:	2802      	cmp	r0, #2
   d4a10:	bf0c      	ite	eq
   d4a12:	2002      	moveq	r0, #2
   d4a14:	2000      	movne	r0, #0
    case BuiltinOperator_EMBEDDING_LOOKUP_SPARSE: {
      auto params =
          safe_allocator.Allocate<TfLiteEmbeddingLookupSparseParams>();
      if (const auto* embedding_params =
              op->builtin_options_as_EmbeddingLookupSparseOptions()) {
        params->combiner = parseCombinerType(embedding_params->combiner());
   d4a16:	7028      	strb	r0, [r5, #0]
   d4a18:	f000 bcd8 	b.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4a1c:	4628      	mov	r0, r5
   d4a1e:	f7ff fda3 	bl	d4568 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI26TfLiteFullyConnectedParamsEEPT_v>
   d4a22:	4680      	mov	r8, r0
  }
  const RNNOptions *builtin_options_as_RNNOptions() const {
    return builtin_options_type() == BuiltinOptions_RNNOptions ? static_cast<const RNNOptions *>(builtin_options()) : nullptr;
  }
  const FullyConnectedOptions *builtin_options_as_FullyConnectedOptions() const {
    return builtin_options_type() == BuiltinOptions_FullyConnectedOptions ? static_cast<const FullyConnectedOptions *>(builtin_options()) : nullptr;
   d4a24:	4620      	mov	r0, r4
   d4a26:	e88d 0120 	stmia.w	sp, {r5, r8}
   d4a2a:	f7ff fd5f 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4a2e:	2808      	cmp	r0, #8
   d4a30:	4605      	mov	r5, r0
   d4a32:	f040 82e8 	bne.w	d5006 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
   d4a36:	4620      	mov	r0, r4
   d4a38:	f7ff fd8d 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_FULLY_CONNECTED: {
      auto params = safe_allocator.Allocate<TfLiteFullyConnectedParams>();
      if (const auto* fully_connected_params =
   d4a3c:	4604      	mov	r4, r0
   d4a3e:	2800      	cmp	r0, #0
   d4a40:	f000 82e1 	beq.w	d5006 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
    VT_FUSED_ACTIVATION_FUNCTION = 4,
    VT_WEIGHTS_FORMAT = 6,
    VT_KEEP_NUM_DIMS = 8
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4a44:	2200      	movs	r2, #0
   d4a46:	2104      	movs	r1, #4
   d4a48:	f7ff fd60 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_FullyConnectedOptions()) {
        params->activation = parse_activation(
   d4a4c:	b2c0      	uxtb	r0, r0
   d4a4e:	f7ff fcd5 	bl	d43fc <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  }
  FullyConnectedOptionsWeightsFormat weights_format() const {
    return static_cast<FullyConnectedOptionsWeightsFormat>(GetField<int8_t>(VT_WEIGHTS_FORMAT, 0));
  }
  bool keep_num_dims() const {
    return GetField<uint8_t>(VT_KEEP_NUM_DIMS, 0) != 0;
   d4a52:	2200      	movs	r2, #0
            fully_connected_params->fused_activation_function());
   d4a54:	f888 0000 	strb.w	r0, [r8]
   d4a58:	4629      	mov	r1, r5
   d4a5a:	4620      	mov	r0, r4
   d4a5c:	f7ff fd3c 	bl	d44d8 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
      }

      /// Return the stored pointer.
      pointer
      get() const noexcept
      { return std::get<0>(_M_t); }
   d4a60:	f8dd 8004 	ldr.w	r8, [sp, #4]
        params->keep_num_dims = fully_connected_params->keep_num_dims();
   d4a64:	3000      	adds	r0, #0
   d4a66:	bf18      	it	ne
   d4a68:	2001      	movne	r0, #1
   d4a6a:	f888 0002 	strb.w	r0, [r8, #2]
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
  }
  FullyConnectedOptionsWeightsFormat weights_format() const {
    return static_cast<FullyConnectedOptionsWeightsFormat>(GetField<int8_t>(VT_WEIGHTS_FORMAT, 0));
   d4a6e:	2200      	movs	r2, #0
   d4a70:	2106      	movs	r1, #6
   d4a72:	4620      	mov	r0, r4
   d4a74:	f7ff fd4a 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        switch (fully_connected_params->weights_format()) {
   d4a78:	b108      	cbz	r0, d4a7e <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x4de>
   d4a7a:	2801      	cmp	r0, #1
   d4a7c:	d102      	bne.n	d4a84 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x4e4>
   d4a7e:	9b01      	ldr	r3, [sp, #4]
          case FullyConnectedOptionsWeightsFormat_DEFAULT:
            params->weights_format = kTfLiteFullyConnectedWeightsFormatDefault;
            break;
          case FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8:
            params->weights_format =
                kTfLiteFullyConnectedWeightsFormatShuffled4x16Int8;
   d4a80:	7058      	strb	r0, [r3, #1]
            break;
   d4a82:	e2c0      	b.n	d5006 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
          default:
            error_reporter->Report("Unhandled fully-connected weights format.");
   d4a84:	49da      	ldr	r1, [pc, #872]	; (d4df0 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x850>)
   d4a86:	4638      	mov	r0, r7
   d4a88:	f7ff fc9e 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>

      /// Destructor, invokes the deleter if the stored pointer is not null.
      ~unique_ptr() noexcept
      {
	auto& __ptr = std::get<0>(_M_t);
	if (__ptr != nullptr)
   d4a8c:	9901      	ldr	r1, [sp, #4]
   d4a8e:	e2b1      	b.n	d4ff4 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa54>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4a90:	4628      	mov	r0, r5
   d4a92:	f7ff fd79 	bl	d4588 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
   d4a96:	4605      	mov	r5, r0
  }
  const FullyConnectedOptions *builtin_options_as_FullyConnectedOptions() const {
    return builtin_options_type() == BuiltinOptions_FullyConnectedOptions ? static_cast<const FullyConnectedOptions *>(builtin_options()) : nullptr;
  }
  const SoftmaxOptions *builtin_options_as_SoftmaxOptions() const {
    return builtin_options_type() == BuiltinOptions_SoftmaxOptions ? static_cast<const SoftmaxOptions *>(builtin_options()) : nullptr;
   d4a98:	4620      	mov	r0, r4
   d4a9a:	f7ff fd27 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4a9e:	2809      	cmp	r0, #9
   d4aa0:	f040 8494 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4aa4:	4620      	mov	r0, r4
   d4aa6:	f7ff fd56 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
    case BuiltinOperator_HASHTABLE_LOOKUP:
      // no-op.
      break;
    case BuiltinOperator_SOFTMAX: {
      auto params = safe_allocator.Allocate<TfLiteSoftmaxParams>();
      if (const auto* softmax_params =
   d4aaa:	2800      	cmp	r0, #0
   d4aac:	f000 848e 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef SoftmaxOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BETA = 4
  };
  float beta() const {
    return GetField<float>(VT_BETA, 0.0f);
   d4ab0:	ed9f 0ad0 	vldr	s0, [pc, #832]	; d4df4 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
   d4ab4:	2104      	movs	r1, #4
   d4ab6:	f7ff fd33 	bl	d4520 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
              op->builtin_options_as_SoftmaxOptions()) {
        params->beta = softmax_params->beta();
   d4aba:	ed85 0a00 	vstr	s0, [r5]
   d4abe:	f000 bc85 	b.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4ac2:	4628      	mov	r0, r5
   d4ac4:	f7ff fd68 	bl	d4598 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
   d4ac8:	4605      	mov	r5, r0
  }
  const SoftmaxOptions *builtin_options_as_SoftmaxOptions() const {
    return builtin_options_type() == BuiltinOptions_SoftmaxOptions ? static_cast<const SoftmaxOptions *>(builtin_options()) : nullptr;
  }
  const ConcatenationOptions *builtin_options_as_ConcatenationOptions() const {
    return builtin_options_type() == BuiltinOptions_ConcatenationOptions ? static_cast<const ConcatenationOptions *>(builtin_options()) : nullptr;
   d4aca:	4620      	mov	r0, r4
   d4acc:	f7ff fd0e 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4ad0:	280a      	cmp	r0, #10
   d4ad2:	f040 847b 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4ad6:	4620      	mov	r0, r4
   d4ad8:	f7ff fd3d 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_CONCATENATION: {
      auto params = safe_allocator.Allocate<TfLiteConcatenationParams>();
      if (const auto* concatenation_params =
   d4adc:	4604      	mov	r4, r0
   d4ade:	2800      	cmp	r0, #0
   d4ae0:	f000 8474 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  };
  int32_t axis() const {
    return GetField<int32_t>(VT_AXIS, 0);
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4ae4:	2200      	movs	r2, #0
   d4ae6:	2106      	movs	r1, #6
   d4ae8:	f7ff fd10 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_ConcatenationOptions()) {
        params->activation =
            parse_activation(concatenation_params->fused_activation_function());
   d4aec:	b2c0      	uxtb	r0, r0
   d4aee:	f7ff fc85 	bl	d43fc <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXIS = 4,
    VT_FUSED_ACTIVATION_FUNCTION = 6
  };
  int32_t axis() const {
    return GetField<int32_t>(VT_AXIS, 0);
   d4af2:	2200      	movs	r2, #0
   d4af4:	7128      	strb	r0, [r5, #4]
   d4af6:	2104      	movs	r1, #4
   d4af8:	4620      	mov	r0, r4
   d4afa:	f7ff fcfd 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->axis = concatenation_params->axis();
   d4afe:	6028      	str	r0, [r5, #0]
   d4b00:	f000 bc64 	b.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4b04:	4628      	mov	r0, r5
   d4b06:	f7ff fd43 	bl	d4590 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d4b0a:	4605      	mov	r5, r0
  }
  const EmbeddingLookupSparseOptions *builtin_options_as_EmbeddingLookupSparseOptions() const {
    return builtin_options_type() == BuiltinOptions_EmbeddingLookupSparseOptions ? static_cast<const EmbeddingLookupSparseOptions *>(builtin_options()) : nullptr;
  }
  const MulOptions *builtin_options_as_MulOptions() const {
    return builtin_options_type() == BuiltinOptions_MulOptions ? static_cast<const MulOptions *>(builtin_options()) : nullptr;
   d4b0c:	4620      	mov	r0, r4
   d4b0e:	f7ff fced 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4b12:	2815      	cmp	r0, #21
   d4b14:	f040 845a 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4b18:	4620      	mov	r0, r4
   d4b1a:	f7ff fd1c 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_MUL: {
      auto params = safe_allocator.Allocate<TfLiteMulParams>();
      if (const auto* schema_params = op->builtin_options_as_MulOptions()) {
   d4b1e:	2800      	cmp	r0, #0
   d4b20:	f000 8454 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef MulOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSED_ACTIVATION_FUNCTION = 4
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4b24:	2200      	movs	r2, #0
   d4b26:	2104      	movs	r1, #4
   d4b28:	f7ff fcf0 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(schema_params->fused_activation_function());
   d4b2c:	b2c0      	uxtb	r0, r0
   d4b2e:	f7ff fc65 	bl	d43fc <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
   d4b32:	7028      	strb	r0, [r5, #0]
   d4b34:	f000 bc4a 	b.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4b38:	4628      	mov	r0, r5
   d4b3a:	f7ff fd29 	bl	d4590 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d4b3e:	4605      	mov	r5, r0
  }
  const ConcatenationOptions *builtin_options_as_ConcatenationOptions() const {
    return builtin_options_type() == BuiltinOptions_ConcatenationOptions ? static_cast<const ConcatenationOptions *>(builtin_options()) : nullptr;
  }
  const AddOptions *builtin_options_as_AddOptions() const {
    return builtin_options_type() == BuiltinOptions_AddOptions ? static_cast<const AddOptions *>(builtin_options()) : nullptr;
   d4b40:	4620      	mov	r0, r4
   d4b42:	f7ff fcd3 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4b46:	280b      	cmp	r0, #11
   d4b48:	f040 8440 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4b4c:	4620      	mov	r0, r4
   d4b4e:	f7ff fd02 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_ADD: {
      auto params = safe_allocator.Allocate<TfLiteAddParams>();
      if (const auto* schema_params = op->builtin_options_as_AddOptions()) {
   d4b52:	2800      	cmp	r0, #0
   d4b54:	f000 843a 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef AddOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSED_ACTIVATION_FUNCTION = 4
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4b58:	2200      	movs	r2, #0
   d4b5a:	2104      	movs	r1, #4
   d4b5c:	f7ff fcd6 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(schema_params->fused_activation_function());
   d4b60:	b2c0      	uxtb	r0, r0
   d4b62:	f7ff fc4b 	bl	d43fc <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
   d4b66:	7028      	strb	r0, [r5, #0]
   d4b68:	f000 bc30 	b.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4b6c:	4628      	mov	r0, r5
   d4b6e:	f7ff fd0f 	bl	d4590 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d4b72:	4605      	mov	r5, r0
  }
  const SubOptions *builtin_options_as_SubOptions() const {
    return builtin_options_type() == BuiltinOptions_SubOptions ? static_cast<const SubOptions *>(builtin_options()) : nullptr;
  }
  const DivOptions *builtin_options_as_DivOptions() const {
    return builtin_options_type() == BuiltinOptions_DivOptions ? static_cast<const DivOptions *>(builtin_options()) : nullptr;
   d4b74:	4620      	mov	r0, r4
   d4b76:	f7ff fcb9 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4b7a:	281d      	cmp	r0, #29
   d4b7c:	f040 8426 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4b80:	4620      	mov	r0, r4
   d4b82:	f7ff fce8 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_DIV: {
      auto params = safe_allocator.Allocate<TfLiteDivParams>();
      if (const auto* schema_params = op->builtin_options_as_DivOptions()) {
   d4b86:	2800      	cmp	r0, #0
   d4b88:	f000 8420 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef DivOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSED_ACTIVATION_FUNCTION = 4
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4b8c:	2200      	movs	r2, #0
   d4b8e:	2104      	movs	r1, #4
   d4b90:	f7ff fcbc 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(schema_params->fused_activation_function());
   d4b94:	b2c0      	uxtb	r0, r0
   d4b96:	f7ff fc31 	bl	d43fc <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
   d4b9a:	7028      	strb	r0, [r5, #0]
   d4b9c:	f000 bc16 	b.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4ba0:	4628      	mov	r0, r5
   d4ba2:	f7ff fcf5 	bl	d4590 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d4ba6:	4605      	mov	r5, r0
  }
  const ReducerOptions *builtin_options_as_ReducerOptions() const {
    return builtin_options_type() == BuiltinOptions_ReducerOptions ? static_cast<const ReducerOptions *>(builtin_options()) : nullptr;
  }
  const SubOptions *builtin_options_as_SubOptions() const {
    return builtin_options_type() == BuiltinOptions_SubOptions ? static_cast<const SubOptions *>(builtin_options()) : nullptr;
   d4ba8:	4620      	mov	r0, r4
   d4baa:	f7ff fc9f 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4bae:	281c      	cmp	r0, #28
   d4bb0:	f040 840c 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4bb4:	4620      	mov	r0, r4
   d4bb6:	f7ff fcce 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SUB: {
      auto params = safe_allocator.Allocate<TfLiteSubParams>();
      if (const auto* schema_params = op->builtin_options_as_SubOptions()) {
   d4bba:	2800      	cmp	r0, #0
   d4bbc:	f000 8406 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef SubOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSED_ACTIVATION_FUNCTION = 4
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4bc0:	2200      	movs	r2, #0
   d4bc2:	2104      	movs	r1, #4
   d4bc4:	f7ff fca2 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(schema_params->fused_activation_function());
   d4bc8:	b2c0      	uxtb	r0, r0
   d4bca:	f7ff fc17 	bl	d43fc <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
   d4bce:	7028      	strb	r0, [r5, #0]
   d4bd0:	e3fc      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4bd2:	4628      	mov	r0, r5
   d4bd4:	f7ff fcdc 	bl	d4590 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d4bd8:	4605      	mov	r5, r0
  }
  const AddOptions *builtin_options_as_AddOptions() const {
    return builtin_options_type() == BuiltinOptions_AddOptions ? static_cast<const AddOptions *>(builtin_options()) : nullptr;
  }
  const L2NormOptions *builtin_options_as_L2NormOptions() const {
    return builtin_options_type() == BuiltinOptions_L2NormOptions ? static_cast<const L2NormOptions *>(builtin_options()) : nullptr;
   d4bda:	4620      	mov	r0, r4
   d4bdc:	f7ff fc86 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4be0:	280c      	cmp	r0, #12
   d4be2:	f040 83f3 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4be6:	4620      	mov	r0, r4
   d4be8:	f7ff fcb5 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_L2_NORMALIZATION: {
      auto params = safe_allocator.Allocate<TfLiteL2NormParams>();
      if (const auto* schema_params = op->builtin_options_as_L2NormOptions()) {
   d4bec:	2800      	cmp	r0, #0
   d4bee:	f000 83ed 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef L2NormOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSED_ACTIVATION_FUNCTION = 4
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4bf2:	2200      	movs	r2, #0
   d4bf4:	2104      	movs	r1, #4
   d4bf6:	f7ff fc89 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(schema_params->fused_activation_function());
   d4bfa:	b2c0      	uxtb	r0, r0
   d4bfc:	f7ff fbfe 	bl	d43fc <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
   d4c00:	7028      	strb	r0, [r5, #0]
   d4c02:	e3e3      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4c04:	4628      	mov	r0, r5
   d4c06:	f7ff fcbb 	bl	d4580 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteFakeQuantParamsEEPT_v>
   d4c0a:	4605      	mov	r5, r0
  }
  const L2NormOptions *builtin_options_as_L2NormOptions() const {
    return builtin_options_type() == BuiltinOptions_L2NormOptions ? static_cast<const L2NormOptions *>(builtin_options()) : nullptr;
  }
  const LocalResponseNormalizationOptions *builtin_options_as_LocalResponseNormalizationOptions() const {
    return builtin_options_type() == BuiltinOptions_LocalResponseNormalizationOptions ? static_cast<const LocalResponseNormalizationOptions *>(builtin_options()) : nullptr;
   d4c0c:	4620      	mov	r0, r4
   d4c0e:	f7ff fc6d 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4c12:	280d      	cmp	r0, #13
   d4c14:	f040 83da 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4c18:	4620      	mov	r0, r4
   d4c1a:	f7ff fc9c 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_LOCAL_RESPONSE_NORMALIZATION: {
      auto params = safe_allocator.Allocate<TfLiteLocalResponseNormParams>();
      if (const auto* schema_params =
   d4c1e:	4604      	mov	r4, r0
   d4c20:	2800      	cmp	r0, #0
   d4c22:	f000 83d3 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_BIAS = 6,
    VT_ALPHA = 8,
    VT_BETA = 10
  };
  int32_t radius() const {
    return GetField<int32_t>(VT_RADIUS, 0);
   d4c26:	2200      	movs	r2, #0
   d4c28:	2104      	movs	r1, #4
   d4c2a:	f7ff fc65 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  float bias() const {
    return GetField<float>(VT_BIAS, 0.0f);
   d4c2e:	2106      	movs	r1, #6
              op->builtin_options_as_LocalResponseNormalizationOptions()) {
        params->radius = schema_params->radius();
   d4c30:	6028      	str	r0, [r5, #0]
   d4c32:	ed9f 0a70 	vldr	s0, [pc, #448]	; d4df4 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
   d4c36:	4620      	mov	r0, r4
   d4c38:	f7ff fc72 	bl	d4520 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  float alpha() const {
    return GetField<float>(VT_ALPHA, 0.0f);
   d4c3c:	2108      	movs	r1, #8
        params->bias = schema_params->bias();
   d4c3e:	ed85 0a01 	vstr	s0, [r5, #4]
   d4c42:	4620      	mov	r0, r4
   d4c44:	ed9f 0a6b 	vldr	s0, [pc, #428]	; d4df4 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
   d4c48:	f7ff fc6a 	bl	d4520 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  float beta() const {
    return GetField<float>(VT_BETA, 0.0f);
   d4c4c:	210a      	movs	r1, #10
        params->alpha = schema_params->alpha();
   d4c4e:	ed85 0a02 	vstr	s0, [r5, #8]
   d4c52:	4620      	mov	r0, r4
   d4c54:	ed9f 0a67 	vldr	s0, [pc, #412]	; d4df4 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
   d4c58:	f7ff fc62 	bl	d4520 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
        params->beta = schema_params->beta();
   d4c5c:	ed85 0a03 	vstr	s0, [r5, #12]
   d4c60:	e3b4      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4c62:	4628      	mov	r0, r5
   d4c64:	f7ff fc8c 	bl	d4580 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteFakeQuantParamsEEPT_v>
   d4c68:	4680      	mov	r8, r0
  }
  const LocalResponseNormalizationOptions *builtin_options_as_LocalResponseNormalizationOptions() const {
    return builtin_options_type() == BuiltinOptions_LocalResponseNormalizationOptions ? static_cast<const LocalResponseNormalizationOptions *>(builtin_options()) : nullptr;
  }
  const LSTMOptions *builtin_options_as_LSTMOptions() const {
    return builtin_options_type() == BuiltinOptions_LSTMOptions ? static_cast<const LSTMOptions *>(builtin_options()) : nullptr;
   d4c6a:	4620      	mov	r0, r4
   d4c6c:	e88d 0120 	stmia.w	sp, {r5, r8}
   d4c70:	f7ff fc3c 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4c74:	280e      	cmp	r0, #14
   d4c76:	d130      	bne.n	d4cda <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x73a>
   d4c78:	4620      	mov	r0, r4
   d4c7a:	f7ff fc6c 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_LSTM: {
      auto params = safe_allocator.Allocate<TfLiteLSTMParams>();
      if (const auto* lstm_params = op->builtin_options_as_LSTMOptions()) {
   d4c7e:	4605      	mov	r5, r0
   d4c80:	b358      	cbz	r0, d4cda <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x73a>
    VT_CELL_CLIP = 6,
    VT_PROJ_CLIP = 8,
    VT_KERNEL_TYPE = 10
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4c82:	2200      	movs	r2, #0
   d4c84:	2104      	movs	r1, #4
   d4c86:	f7ff fc41 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(lstm_params->fused_activation_function());
   d4c8a:	b2c0      	uxtb	r0, r0
   d4c8c:	f7ff fbb6 	bl	d43fc <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  }
  float cell_clip() const {
    return GetField<float>(VT_CELL_CLIP, 0.0f);
   d4c90:	2106      	movs	r1, #6
   d4c92:	f888 0000 	strb.w	r0, [r8]
   d4c96:	ed9f 0a57 	vldr	s0, [pc, #348]	; d4df4 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
      }

      /// Return the stored pointer.
      pointer
      get() const noexcept
      { return std::get<0>(_M_t); }
   d4c9a:	9c01      	ldr	r4, [sp, #4]
   d4c9c:	4628      	mov	r0, r5
   d4c9e:	f7ff fc3f 	bl	d4520 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  float proj_clip() const {
    return GetField<float>(VT_PROJ_CLIP, 0.0f);
   d4ca2:	2108      	movs	r1, #8
        params->cell_clip = lstm_params->cell_clip();
   d4ca4:	ed84 0a01 	vstr	s0, [r4, #4]
   d4ca8:	4628      	mov	r0, r5
   d4caa:	ed9f 0a52 	vldr	s0, [pc, #328]	; d4df4 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
   d4cae:	9c01      	ldr	r4, [sp, #4]
   d4cb0:	f7ff fc36 	bl	d4520 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  LSTMKernelType kernel_type() const {
    return static_cast<LSTMKernelType>(GetField<int8_t>(VT_KERNEL_TYPE, 0));
   d4cb4:	2200      	movs	r2, #0
        params->proj_clip = lstm_params->proj_clip();
   d4cb6:	ed84 0a02 	vstr	s0, [r4, #8]
   d4cba:	210a      	movs	r1, #10
   d4cbc:	4628      	mov	r0, r5
   d4cbe:	f7ff fc25 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        switch (lstm_params->kernel_type()) {
   d4cc2:	b108      	cbz	r0, d4cc8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x728>
   d4cc4:	2801      	cmp	r0, #1
   d4cc6:	d102      	bne.n	d4cce <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x72e>
          case LSTMKernelType_FULL:
            params->kernel_type = kTfLiteLSTMFullKernel;
            break;
          case LSTMKernelType_BASIC:
            params->kernel_type = kTfLiteLSTMBasicKernel;
   d4cc8:	7320      	strb	r0, [r4, #12]
        }
      } else {
        error_reporter->Report("No valid LSTM builtin options exist");
        return kTfLiteError;
      }
      *builtin_data = reinterpret_cast<void*>(params.release());
   d4cca:	6034      	str	r4, [r6, #0]
   d4ccc:	e37f      	b.n	d53ce <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2e>
          case LSTMKernelType_BASIC:
            params->kernel_type = kTfLiteLSTMBasicKernel;
            break;
          default:
            error_reporter->Report("Unhandled LSTM kernel type: %d",
                                   lstm_params->kernel_type());
   d4cce:	b2c2      	uxtb	r2, r0
   d4cd0:	4949      	ldr	r1, [pc, #292]	; (d4df8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x858>)
   d4cd2:	4638      	mov	r0, r7
   d4cd4:	f7ff fb78 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
            return kTfLiteError;
   d4cd8:	e6d8      	b.n	d4a8c <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x4ec>
        }
      } else {
        error_reporter->Report("No valid LSTM builtin options exist");
   d4cda:	4948      	ldr	r1, [pc, #288]	; (d4dfc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x85c>)
   d4cdc:	e6d3      	b.n	d4a86 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x4e6>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4cde:	4628      	mov	r0, r5
   d4ce0:	f7ff fc4e 	bl	d4580 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteFakeQuantParamsEEPT_v>
   d4ce4:	4605      	mov	r5, r0
  }
  const BidirectionalSequenceRNNOptions *builtin_options_as_BidirectionalSequenceRNNOptions() const {
    return builtin_options_type() == BuiltinOptions_BidirectionalSequenceRNNOptions ? static_cast<const BidirectionalSequenceRNNOptions *>(builtin_options()) : nullptr;
  }
  const UnidirectionalSequenceLSTMOptions *builtin_options_as_UnidirectionalSequenceLSTMOptions() const {
    return builtin_options_type() == BuiltinOptions_UnidirectionalSequenceLSTMOptions ? static_cast<const UnidirectionalSequenceLSTMOptions *>(builtin_options()) : nullptr;
   d4ce6:	4620      	mov	r0, r4
   d4ce8:	f7ff fc00 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4cec:	2847      	cmp	r0, #71	; 0x47
   d4cee:	f040 836d 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4cf2:	4620      	mov	r0, r4
   d4cf4:	f7ff fc2f 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_UNIDIRECTIONAL_SEQUENCE_LSTM: {
      auto params =
          safe_allocator.Allocate<TfLiteUnidirectionalSequenceLSTMParams>();
      if (const auto* seq_lstm_params =
   d4cf8:	4604      	mov	r4, r0
   d4cfa:	2800      	cmp	r0, #0
   d4cfc:	f000 8366 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_CELL_CLIP = 6,
    VT_PROJ_CLIP = 8,
    VT_TIME_MAJOR = 10
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4d00:	2200      	movs	r2, #0
   d4d02:	2104      	movs	r1, #4
   d4d04:	f7ff fc02 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_UnidirectionalSequenceLSTMOptions()) {
        params->activation =
            parse_activation(seq_lstm_params->fused_activation_function());
   d4d08:	b2c0      	uxtb	r0, r0
   d4d0a:	f7ff fb77 	bl	d43fc <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  }
  float cell_clip() const {
    return GetField<float>(VT_CELL_CLIP, 0.0f);
   d4d0e:	2106      	movs	r1, #6
   d4d10:	7028      	strb	r0, [r5, #0]
   d4d12:	ed9f 0a38 	vldr	s0, [pc, #224]	; d4df4 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
   d4d16:	4620      	mov	r0, r4
   d4d18:	f7ff fc02 	bl	d4520 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  float proj_clip() const {
    return GetField<float>(VT_PROJ_CLIP, 0.0f);
   d4d1c:	2108      	movs	r1, #8
        params->cell_clip = seq_lstm_params->cell_clip();
   d4d1e:	ed85 0a01 	vstr	s0, [r5, #4]
   d4d22:	4620      	mov	r0, r4
   d4d24:	ed9f 0a33 	vldr	s0, [pc, #204]	; d4df4 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
   d4d28:	f7ff fbfa 	bl	d4520 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  bool time_major() const {
    return GetField<uint8_t>(VT_TIME_MAJOR, 0) != 0;
   d4d2c:	2200      	movs	r2, #0
        params->proj_clip = seq_lstm_params->proj_clip();
   d4d2e:	ed85 0a02 	vstr	s0, [r5, #8]
   d4d32:	210a      	movs	r1, #10
   d4d34:	4620      	mov	r0, r4
   d4d36:	f7ff fbcf 	bl	d44d8 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->time_major = seq_lstm_params->time_major();
   d4d3a:	3000      	adds	r0, #0
   d4d3c:	bf18      	it	ne
   d4d3e:	2001      	movne	r0, #1
   d4d40:	7328      	strb	r0, [r5, #12]
   d4d42:	e343      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4d44:	4628      	mov	r0, r5
   d4d46:	f7ff fc1b 	bl	d4580 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteFakeQuantParamsEEPT_v>
   d4d4a:	4605      	mov	r5, r0
  }
  const FillOptions *builtin_options_as_FillOptions() const {
    return builtin_options_type() == BuiltinOptions_FillOptions ? static_cast<const FillOptions *>(builtin_options()) : nullptr;
  }
  const BidirectionalSequenceLSTMOptions *builtin_options_as_BidirectionalSequenceLSTMOptions() const {
    return builtin_options_type() == BuiltinOptions_BidirectionalSequenceLSTMOptions ? static_cast<const BidirectionalSequenceLSTMOptions *>(builtin_options()) : nullptr;
   d4d4c:	4620      	mov	r0, r4
   d4d4e:	f7ff fbcd 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4d52:	2845      	cmp	r0, #69	; 0x45
   d4d54:	f040 833a 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4d58:	4620      	mov	r0, r4
   d4d5a:	f7ff fbfc 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_BIDIRECTIONAL_SEQUENCE_LSTM: {
      auto params =
          safe_allocator.Allocate<TfLiteBidirectionalSequenceLSTMParams>();
      if (const auto* bidi_lstm_params =
   d4d5e:	4604      	mov	r4, r0
   d4d60:	2800      	cmp	r0, #0
   d4d62:	f000 8333 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_PROJ_CLIP = 8,
    VT_MERGE_OUTPUTS = 10,
    VT_TIME_MAJOR = 12
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4d66:	2200      	movs	r2, #0
   d4d68:	2104      	movs	r1, #4
   d4d6a:	f7ff fbcf 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_BidirectionalSequenceLSTMOptions()) {
        params->activation =
            parse_activation(bidi_lstm_params->fused_activation_function());
   d4d6e:	b2c0      	uxtb	r0, r0
   d4d70:	f7ff fb44 	bl	d43fc <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  }
  float cell_clip() const {
    return GetField<float>(VT_CELL_CLIP, 0.0f);
   d4d74:	2106      	movs	r1, #6
   d4d76:	7028      	strb	r0, [r5, #0]
   d4d78:	ed9f 0a1e 	vldr	s0, [pc, #120]	; d4df4 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
   d4d7c:	4620      	mov	r0, r4
   d4d7e:	f7ff fbcf 	bl	d4520 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  float proj_clip() const {
    return GetField<float>(VT_PROJ_CLIP, 0.0f);
   d4d82:	2108      	movs	r1, #8
        params->cell_clip = bidi_lstm_params->cell_clip();
   d4d84:	ed85 0a01 	vstr	s0, [r5, #4]
   d4d88:	4620      	mov	r0, r4
   d4d8a:	ed9f 0a1a 	vldr	s0, [pc, #104]	; d4df4 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
   d4d8e:	f7ff fbc7 	bl	d4520 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  bool merge_outputs() const {
    return GetField<uint8_t>(VT_MERGE_OUTPUTS, 0) != 0;
   d4d92:	2200      	movs	r2, #0
        params->proj_clip = bidi_lstm_params->proj_clip();
   d4d94:	ed85 0a02 	vstr	s0, [r5, #8]
   d4d98:	210a      	movs	r1, #10
   d4d9a:	4620      	mov	r0, r4
   d4d9c:	f7ff fb9c 	bl	d44d8 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->merge_outputs = bidi_lstm_params->merge_outputs();
   d4da0:	3000      	adds	r0, #0
   d4da2:	bf18      	it	ne
   d4da4:	2001      	movne	r0, #1
   d4da6:	7328      	strb	r0, [r5, #12]
  }
  bool time_major() const {
    return GetField<uint8_t>(VT_TIME_MAJOR, 1) != 0;
   d4da8:	2201      	movs	r2, #1
   d4daa:	210c      	movs	r1, #12
   d4dac:	4620      	mov	r0, r4
   d4dae:	f7ff fb93 	bl	d44d8 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->time_major = bidi_lstm_params->time_major();
   d4db2:	3000      	adds	r0, #0
   d4db4:	bf18      	it	ne
   d4db6:	2001      	movne	r0, #1
   d4db8:	7368      	strb	r0, [r5, #13]
   d4dba:	e307      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4dbc:	4628      	mov	r0, r5
   d4dbe:	f7ff fbe7 	bl	d4590 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d4dc2:	4605      	mov	r5, r0
  }
  const LSTMOptions *builtin_options_as_LSTMOptions() const {
    return builtin_options_type() == BuiltinOptions_LSTMOptions ? static_cast<const LSTMOptions *>(builtin_options()) : nullptr;
  }
  const ResizeBilinearOptions *builtin_options_as_ResizeBilinearOptions() const {
    return builtin_options_type() == BuiltinOptions_ResizeBilinearOptions ? static_cast<const ResizeBilinearOptions *>(builtin_options()) : nullptr;
   d4dc4:	4620      	mov	r0, r4
   d4dc6:	f7ff fb91 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4dca:	280f      	cmp	r0, #15
   d4dcc:	f040 82fe 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4dd0:	4620      	mov	r0, r4
   d4dd2:	f7ff fbc0 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_RESIZE_BILINEAR: {
      auto params = safe_allocator.Allocate<TfLiteResizeBilinearParams>();
      if (const auto* schema_params =
   d4dd6:	2800      	cmp	r0, #0
   d4dd8:	f000 82f8 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef ResizeBilinearOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ALIGN_CORNERS = 8
  };
  bool align_corners() const {
    return GetField<uint8_t>(VT_ALIGN_CORNERS, 0) != 0;
   d4ddc:	2200      	movs	r2, #0
   d4dde:	2108      	movs	r1, #8
   d4de0:	f7ff fb7a 	bl	d44d8 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
              op->builtin_options_as_ResizeBilinearOptions()) {
        params->align_corners = schema_params->align_corners();
   d4de4:	3000      	adds	r0, #0
   d4de6:	bf18      	it	ne
   d4de8:	2001      	movne	r0, #1
   d4dea:	7028      	strb	r0, [r5, #0]
   d4dec:	e2ee      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4dee:	bf00      	nop
   d4df0:	000e861c 	.word	0x000e861c
   d4df4:	00000000 	.word	0x00000000
   d4df8:	000e8646 	.word	0x000e8646
   d4dfc:	000e8665 	.word	0x000e8665
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4e00:	4628      	mov	r0, r5
   d4e02:	f7ff fbc5 	bl	d4590 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d4e06:	4605      	mov	r5, r0
  }
  const RangeOptions *builtin_options_as_RangeOptions() const {
    return builtin_options_type() == BuiltinOptions_RangeOptions ? static_cast<const RangeOptions *>(builtin_options()) : nullptr;
  }
  const ResizeNearestNeighborOptions *builtin_options_as_ResizeNearestNeighborOptions() const {
    return builtin_options_type() == BuiltinOptions_ResizeNearestNeighborOptions ? static_cast<const ResizeNearestNeighborOptions *>(builtin_options()) : nullptr;
   d4e08:	4620      	mov	r0, r4
   d4e0a:	f7ff fb6f 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4e0e:	284a      	cmp	r0, #74	; 0x4a
   d4e10:	f040 82dc 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4e14:	4620      	mov	r0, r4
   d4e16:	f7ff fb9e 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      // required to minimize function size. TODO(b/118447267): Simplify
      // ParseOpData function and reduce its length.
      [&]() {
        auto params =
            safe_allocator.Allocate<TfLiteResizeNearestNeighborParams>();
        if (const auto* schema_params =
   d4e1a:	2800      	cmp	r0, #0
   d4e1c:	f000 82d6 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef ResizeNearestNeighborOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ALIGN_CORNERS = 4
  };
  bool align_corners() const {
    return GetField<uint8_t>(VT_ALIGN_CORNERS, 0) != 0;
   d4e20:	2200      	movs	r2, #0
   d4e22:	2104      	movs	r1, #4
   d4e24:	f7ff fb58 	bl	d44d8 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
                op->builtin_options_as_ResizeNearestNeighborOptions()) {
          params->align_corners = schema_params->align_corners();
   d4e28:	3000      	adds	r0, #0
   d4e2a:	bf18      	it	ne
   d4e2c:	2001      	movne	r0, #1
   d4e2e:	7028      	strb	r0, [r5, #0]
   d4e30:	e2cc      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4e32:	4628      	mov	r0, r5
   d4e34:	f7ff fb9c 	bl	d4570 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI19TfLiteSqueezeParamsEEPT_v>
   d4e38:	4680      	mov	r8, r0
  }
  const CallOptions *builtin_options_as_CallOptions() const {
    return builtin_options_type() == BuiltinOptions_CallOptions ? static_cast<const CallOptions *>(builtin_options()) : nullptr;
  }
  const ReshapeOptions *builtin_options_as_ReshapeOptions() const {
    return builtin_options_type() == BuiltinOptions_ReshapeOptions ? static_cast<const ReshapeOptions *>(builtin_options()) : nullptr;
   d4e3a:	4620      	mov	r0, r4
   d4e3c:	e88d 0120 	stmia.w	sp, {r5, r8}
   d4e40:	f7ff fb54 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4e44:	2811      	cmp	r0, #17
   d4e46:	f040 80de 	bne.w	d5006 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
   d4e4a:	4620      	mov	r0, r4
   d4e4c:	f7ff fb83 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      }();
      break;
    }
    case BuiltinOperator_RESHAPE: {
      auto params = safe_allocator.Allocate<TfLiteReshapeParams>();
      if (const auto* schema_params = op->builtin_options_as_ReshapeOptions()) {
   d4e50:	2800      	cmp	r0, #0
   d4e52:	f000 80d8 	beq.w	d5006 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
   d4e56:	2104      	movs	r1, #4
   d4e58:	f7ff fb74 	bl	d4544 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorIlEEEET_t>
        auto* new_shape = schema_params->new_shape();
        TF_LITE_ENSURE_STATUS(FlatBufferIntVectorToArray(
   d4e5c:	4bca      	ldr	r3, [pc, #808]	; (d5188 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xbe8>)
   d4e5e:	4604      	mov	r4, r0
   d4e60:	e0c2      	b.n	d4fe8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa48>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4e62:	4628      	mov	r0, r5
   d4e64:	f7ff fb88 	bl	d4578 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI25TfLiteTransposeConvParamsEEPT_v>
   d4e68:	4605      	mov	r5, r0
  }
  const SkipGramOptions *builtin_options_as_SkipGramOptions() const {
    return builtin_options_type() == BuiltinOptions_SkipGramOptions ? static_cast<const SkipGramOptions *>(builtin_options()) : nullptr;
   d4e6a:	4620      	mov	r0, r4
   d4e6c:	f7ff fb3e 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4e70:	2812      	cmp	r0, #18
   d4e72:	f040 82ab 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4e76:	4620      	mov	r0, r4
   d4e78:	f7ff fb6d 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SKIP_GRAM: {
      auto params = safe_allocator.Allocate<TfLiteSkipGramParams>();
      if (const auto* skip_gram_params =
   d4e7c:	4604      	mov	r4, r0
   d4e7e:	2800      	cmp	r0, #0
   d4e80:	f000 82a4 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_NGRAM_SIZE = 4,
    VT_MAX_SKIP_SIZE = 6,
    VT_INCLUDE_ALL_NGRAMS = 8
  };
  int32_t ngram_size() const {
    return GetField<int32_t>(VT_NGRAM_SIZE, 0);
   d4e84:	2200      	movs	r2, #0
   d4e86:	2104      	movs	r1, #4
   d4e88:	f7ff fb36 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t max_skip_size() const {
    return GetField<int32_t>(VT_MAX_SKIP_SIZE, 0);
   d4e8c:	2200      	movs	r2, #0
              op->builtin_options_as_SkipGramOptions()) {
        params->ngram_size = skip_gram_params->ngram_size();
   d4e8e:	6028      	str	r0, [r5, #0]
   d4e90:	2106      	movs	r1, #6
   d4e92:	4620      	mov	r0, r4
   d4e94:	f7ff fb30 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  bool include_all_ngrams() const {
    return GetField<uint8_t>(VT_INCLUDE_ALL_NGRAMS, 0) != 0;
   d4e98:	2200      	movs	r2, #0
        params->max_skip_size = skip_gram_params->max_skip_size();
   d4e9a:	6068      	str	r0, [r5, #4]
   d4e9c:	2108      	movs	r1, #8
   d4e9e:	4620      	mov	r0, r4
   d4ea0:	f7ff fb1a 	bl	d44d8 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->include_all_ngrams = skip_gram_params->include_all_ngrams();
   d4ea4:	3000      	adds	r0, #0
   d4ea6:	bf18      	it	ne
   d4ea8:	2001      	movne	r0, #1
   d4eaa:	7228      	strb	r0, [r5, #8]
   d4eac:	e28e      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4eae:	4628      	mov	r0, r5
   d4eb0:	f7ff fb6a 	bl	d4588 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
   d4eb4:	4605      	mov	r5, r0
  }
  const SkipGramOptions *builtin_options_as_SkipGramOptions() const {
    return builtin_options_type() == BuiltinOptions_SkipGramOptions ? static_cast<const SkipGramOptions *>(builtin_options()) : nullptr;
  }
  const SpaceToDepthOptions *builtin_options_as_SpaceToDepthOptions() const {
    return builtin_options_type() == BuiltinOptions_SpaceToDepthOptions ? static_cast<const SpaceToDepthOptions *>(builtin_options()) : nullptr;
   d4eb6:	4620      	mov	r0, r4
   d4eb8:	f7ff fb18 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4ebc:	2813      	cmp	r0, #19
   d4ebe:	f040 8285 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4ec2:	4620      	mov	r0, r4
   d4ec4:	f7ff fb47 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SPACE_TO_DEPTH: {
      auto params = safe_allocator.Allocate<TfLiteSpaceToDepthParams>();
      if (const auto* schema_params =
   d4ec8:	2800      	cmp	r0, #0
   d4eca:	f000 827f 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef SpaceToDepthOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BLOCK_SIZE = 4
  };
  int32_t block_size() const {
    return GetField<int32_t>(VT_BLOCK_SIZE, 0);
   d4ece:	2200      	movs	r2, #0
   d4ed0:	2104      	movs	r1, #4
   d4ed2:	f7ff fb11 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
              op->builtin_options_as_SpaceToDepthOptions()) {
        params->block_size = schema_params->block_size();
   d4ed6:	6028      	str	r0, [r5, #0]
   d4ed8:	e278      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4eda:	4628      	mov	r0, r5
   d4edc:	f7ff fb54 	bl	d4588 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
   d4ee0:	4605      	mov	r5, r0
  }
  const WhileOptions *builtin_options_as_WhileOptions() const {
    return builtin_options_type() == BuiltinOptions_WhileOptions ? static_cast<const WhileOptions *>(builtin_options()) : nullptr;
  }
  const DepthToSpaceOptions *builtin_options_as_DepthToSpaceOptions() const {
    return builtin_options_type() == BuiltinOptions_DepthToSpaceOptions ? static_cast<const DepthToSpaceOptions *>(builtin_options()) : nullptr;
   d4ee2:	4620      	mov	r0, r4
   d4ee4:	f7ff fb02 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4ee8:	285e      	cmp	r0, #94	; 0x5e
   d4eea:	f040 826f 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4eee:	4620      	mov	r0, r4
   d4ef0:	f7ff fb31 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_DEPTH_TO_SPACE: {
      auto params = safe_allocator.Allocate<TfLiteDepthToSpaceParams>();
      if (const auto* schema_params =
   d4ef4:	2800      	cmp	r0, #0
   d4ef6:	f000 8269 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef DepthToSpaceOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BLOCK_SIZE = 4
  };
  int32_t block_size() const {
    return GetField<int32_t>(VT_BLOCK_SIZE, 0);
   d4efa:	2200      	movs	r2, #0
   d4efc:	2104      	movs	r1, #4
   d4efe:	f7ff fafb 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
              op->builtin_options_as_DepthToSpaceOptions()) {
        params->block_size = schema_params->block_size();
   d4f02:	6028      	str	r0, [r5, #0]
   d4f04:	e262      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4f06:	4628      	mov	r0, r5
   d4f08:	f7ff fb3e 	bl	d4588 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_GATHER: {
      auto params = safe_allocator.Allocate<TfLiteGatherParams>();
      params->axis = 0;
   d4f0c:	f8c0 8000 	str.w	r8, [r0]
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4f10:	4605      	mov	r5, r0
  }
  const PadOptions *builtin_options_as_PadOptions() const {
    return builtin_options_type() == BuiltinOptions_PadOptions ? static_cast<const PadOptions *>(builtin_options()) : nullptr;
  }
  const GatherOptions *builtin_options_as_GatherOptions() const {
    return builtin_options_type() == BuiltinOptions_GatherOptions ? static_cast<const GatherOptions *>(builtin_options()) : nullptr;
   d4f12:	4620      	mov	r0, r4
   d4f14:	f7ff faea 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4f18:	2817      	cmp	r0, #23
   d4f1a:	f040 8257 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4f1e:	4620      	mov	r0, r4
   d4f20:	f7ff fb19 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_GATHER: {
      auto params = safe_allocator.Allocate<TfLiteGatherParams>();
      params->axis = 0;
      if (const auto* gather_params = op->builtin_options_as_GatherOptions()) {
   d4f24:	2800      	cmp	r0, #0
   d4f26:	f000 8251 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef GatherOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXIS = 4
  };
  int32_t axis() const {
    return GetField<int32_t>(VT_AXIS, 0);
   d4f2a:	2200      	movs	r2, #0
   d4f2c:	2104      	movs	r1, #4
   d4f2e:	f7ff fae3 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->axis = gather_params->axis();
   d4f32:	6028      	str	r0, [r5, #0]
   d4f34:	e24a      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4f36:	4628      	mov	r0, r5
   d4f38:	f7ff fb2a 	bl	d4590 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d4f3c:	4605      	mov	r5, r0
  }
  const TransposeOptions *builtin_options_as_TransposeOptions() const {
    return builtin_options_type() == BuiltinOptions_TransposeOptions ? static_cast<const TransposeOptions *>(builtin_options()) : nullptr;
  }
  const ReducerOptions *builtin_options_as_ReducerOptions() const {
    return builtin_options_type() == BuiltinOptions_ReducerOptions ? static_cast<const ReducerOptions *>(builtin_options()) : nullptr;
   d4f3e:	4620      	mov	r0, r4
   d4f40:	f7ff fad4 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4f44:	281b      	cmp	r0, #27
   d4f46:	f040 8241 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4f4a:	4620      	mov	r0, r4
   d4f4c:	f7ff fb03 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
    case BuiltinOperator_REDUCE_MIN:
    case BuiltinOperator_REDUCE_PROD:
    case BuiltinOperator_REDUCE_ANY:
    case BuiltinOperator_SUM: {
      auto params = safe_allocator.Allocate<TfLiteReducerParams>();
      if (const auto* schema_params = op->builtin_options_as_ReducerOptions()) {
   d4f50:	2800      	cmp	r0, #0
   d4f52:	f000 823b 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef ReducerOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_KEEP_DIMS = 4
  };
  bool keep_dims() const {
    return GetField<uint8_t>(VT_KEEP_DIMS, 0) != 0;
   d4f56:	2200      	movs	r2, #0
   d4f58:	2104      	movs	r1, #4
   d4f5a:	f7ff fabd 	bl	d44d8 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->keep_dims = schema_params->keep_dims();
   d4f5e:	3000      	adds	r0, #0
   d4f60:	bf18      	it	ne
   d4f62:	2001      	movne	r0, #1
   d4f64:	7028      	strb	r0, [r5, #0]
   d4f66:	e231      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4f68:	4628      	mov	r0, r5
   d4f6a:	f7ff fb0d 	bl	d4588 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
   d4f6e:	4605      	mov	r5, r0
  }
  const TopKV2Options *builtin_options_as_TopKV2Options() const {
    return builtin_options_type() == BuiltinOptions_TopKV2Options ? static_cast<const TopKV2Options *>(builtin_options()) : nullptr;
  }
  const SplitOptions *builtin_options_as_SplitOptions() const {
    return builtin_options_type() == BuiltinOptions_SplitOptions ? static_cast<const SplitOptions *>(builtin_options()) : nullptr;
   d4f70:	4620      	mov	r0, r4
   d4f72:	f7ff fabb 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4f76:	2823      	cmp	r0, #35	; 0x23
   d4f78:	f040 8228 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4f7c:	4620      	mov	r0, r4
   d4f7e:	f7ff faea 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SPLIT: {
      auto params = safe_allocator.Allocate<TfLiteSplitParams>();
      if (const auto* schema_params = op->builtin_options_as_SplitOptions()) {
   d4f82:	2800      	cmp	r0, #0
   d4f84:	f000 8222 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef SplitOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_NUM_SPLITS = 4
  };
  int32_t num_splits() const {
    return GetField<int32_t>(VT_NUM_SPLITS, 0);
   d4f88:	2200      	movs	r2, #0
   d4f8a:	2104      	movs	r1, #4
   d4f8c:	f7ff fab4 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->num_splits = schema_params->num_splits();
   d4f90:	6028      	str	r0, [r5, #0]
   d4f92:	e21b      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4f94:	4628      	mov	r0, r5
   d4f96:	f7ff faf7 	bl	d4588 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
   d4f9a:	4605      	mov	r5, r0
  }
  const AbsOptions *builtin_options_as_AbsOptions() const {
    return builtin_options_type() == BuiltinOptions_AbsOptions ? static_cast<const AbsOptions *>(builtin_options()) : nullptr;
  }
  const SplitVOptions *builtin_options_as_SplitVOptions() const {
    return builtin_options_type() == BuiltinOptions_SplitVOptions ? static_cast<const SplitVOptions *>(builtin_options()) : nullptr;
   d4f9c:	4620      	mov	r0, r4
   d4f9e:	f7ff faa5 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4fa2:	284f      	cmp	r0, #79	; 0x4f
   d4fa4:	f040 8212 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4fa8:	4620      	mov	r0, r4
   d4faa:	f7ff fad4 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SPLIT_V: {
      auto params = safe_allocator.Allocate<TfLiteSplitParams>();
      if (const auto* schema_params = op->builtin_options_as_SplitVOptions()) {
   d4fae:	2800      	cmp	r0, #0
   d4fb0:	f000 820c 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef SplitVOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_NUM_SPLITS = 4
  };
  int32_t num_splits() const {
    return GetField<int32_t>(VT_NUM_SPLITS, 0);
   d4fb4:	2200      	movs	r2, #0
   d4fb6:	2104      	movs	r1, #4
   d4fb8:	f7ff fa9e 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->num_splits = schema_params->num_splits();
   d4fbc:	6028      	str	r0, [r5, #0]
   d4fbe:	e205      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4fc0:	4628      	mov	r0, r5
   d4fc2:	f7ff fad5 	bl	d4570 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI19TfLiteSqueezeParamsEEPT_v>
   d4fc6:	4680      	mov	r8, r0
  }
  const DivOptions *builtin_options_as_DivOptions() const {
    return builtin_options_type() == BuiltinOptions_DivOptions ? static_cast<const DivOptions *>(builtin_options()) : nullptr;
  }
  const SqueezeOptions *builtin_options_as_SqueezeOptions() const {
    return builtin_options_type() == BuiltinOptions_SqueezeOptions ? static_cast<const SqueezeOptions *>(builtin_options()) : nullptr;
   d4fc8:	4620      	mov	r0, r4
   d4fca:	e88d 0120 	stmia.w	sp, {r5, r8}
   d4fce:	f7ff fa8d 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4fd2:	281e      	cmp	r0, #30
   d4fd4:	d117      	bne.n	d5006 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
   d4fd6:	4620      	mov	r0, r4
   d4fd8:	f7ff fabd 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SQUEEZE: {
      auto params = safe_allocator.Allocate<TfLiteSqueezeParams>();
      if (const auto* schema_params = op->builtin_options_as_SqueezeOptions()) {
   d4fdc:	b198      	cbz	r0, d5006 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
   d4fde:	2104      	movs	r1, #4
   d4fe0:	f7ff fab0 	bl	d4544 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorIlEEEET_t>
        const auto& squeeze_dims = schema_params->squeeze_dims();
        TF_LITE_ENSURE_STATUS(FlatBufferIntVectorToArray(
   d4fe4:	4b69      	ldr	r3, [pc, #420]	; (d518c <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xbec>)
   d4fe6:	4604      	mov	r4, r0
   d4fe8:	4641      	mov	r1, r8
   d4fea:	463a      	mov	r2, r7
   d4fec:	f7ff fa10 	bl	d4410 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4>
   d4ff0:	9901      	ldr	r1, [sp, #4]
   d4ff2:	b130      	cbz	r0, d5002 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa62>

      /// Destructor, invokes the deleter if the stored pointer is not null.
      ~unique_ptr() noexcept
      {
	auto& __ptr = std::get<0>(_M_t);
	if (__ptr != nullptr)
   d4ff4:	2900      	cmp	r1, #0
   d4ff6:	f000 80ec 	beq.w	d51d2 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xc32>
	  get_deleter()(__ptr);
   d4ffa:	4668      	mov	r0, sp
   d4ffc:	f7ff f9f2 	bl	d43e4 <_ZN6tflite12_GLOBAL__N_124SafeBuiltinDataAllocator18BuiltinDataDeleterclEPv>
   d5000:	e0e7      	b.n	d51d2 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xc32>
            sizeof(params->squeeze_dims), squeeze_dims, params->squeeze_dims,
            error_reporter, "squeeze"));
        params->num_squeeze_dims = squeeze_dims->size();
   d5002:	6823      	ldr	r3, [r4, #0]
   d5004:	620b      	str	r3, [r1, #32]
      }
      *builtin_data = reinterpret_cast<void*>(params.release());
   d5006:	9b01      	ldr	r3, [sp, #4]
   d5008:	6033      	str	r3, [r6, #0]
   d500a:	e1e0      	b.n	d53ce <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2e>
   d500c:	682b      	ldr	r3, [r5, #0]
   d500e:	2114      	movs	r1, #20
   d5010:	681b      	ldr	r3, [r3, #0]
   d5012:	4628      	mov	r0, r5
   d5014:	4798      	blx	r3
   d5016:	4605      	mov	r5, r0
  }
  const SequenceRNNOptions *builtin_options_as_SequenceRNNOptions() const {
    return builtin_options_type() == BuiltinOptions_SequenceRNNOptions ? static_cast<const SequenceRNNOptions *>(builtin_options()) : nullptr;
  }
  const StridedSliceOptions *builtin_options_as_StridedSliceOptions() const {
    return builtin_options_type() == BuiltinOptions_StridedSliceOptions ? static_cast<const StridedSliceOptions *>(builtin_options()) : nullptr;
   d5018:	4620      	mov	r0, r4
   d501a:	f7ff fa67 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d501e:	2820      	cmp	r0, #32
   d5020:	f040 81d4 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d5024:	4620      	mov	r0, r4
   d5026:	f7ff fa96 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_STRIDED_SLICE: {
      auto params = safe_allocator.Allocate<TfLiteStridedSliceParams>();
      if (const auto* schema_params =
   d502a:	4604      	mov	r4, r0
   d502c:	2800      	cmp	r0, #0
   d502e:	f000 81cd 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_ELLIPSIS_MASK = 8,
    VT_NEW_AXIS_MASK = 10,
    VT_SHRINK_AXIS_MASK = 12
  };
  int32_t begin_mask() const {
    return GetField<int32_t>(VT_BEGIN_MASK, 0);
   d5032:	2200      	movs	r2, #0
   d5034:	2104      	movs	r1, #4
   d5036:	f7ff fa5f 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t end_mask() const {
    return GetField<int32_t>(VT_END_MASK, 0);
   d503a:	2200      	movs	r2, #0
              op->builtin_options_as_StridedSliceOptions()) {
        params->begin_mask = schema_params->begin_mask();
   d503c:	6028      	str	r0, [r5, #0]
   d503e:	2106      	movs	r1, #6
   d5040:	4620      	mov	r0, r4
   d5042:	f7ff fa59 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t ellipsis_mask() const {
    return GetField<int32_t>(VT_ELLIPSIS_MASK, 0);
   d5046:	2200      	movs	r2, #0
        params->end_mask = schema_params->end_mask();
   d5048:	6068      	str	r0, [r5, #4]
   d504a:	2108      	movs	r1, #8
   d504c:	4620      	mov	r0, r4
   d504e:	f7ff fa53 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t new_axis_mask() const {
    return GetField<int32_t>(VT_NEW_AXIS_MASK, 0);
   d5052:	2200      	movs	r2, #0
        params->ellipsis_mask = schema_params->ellipsis_mask();
   d5054:	60a8      	str	r0, [r5, #8]
   d5056:	210a      	movs	r1, #10
   d5058:	4620      	mov	r0, r4
   d505a:	f7ff fa4d 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t shrink_axis_mask() const {
    return GetField<int32_t>(VT_SHRINK_AXIS_MASK, 0);
   d505e:	2200      	movs	r2, #0
        params->new_axis_mask = schema_params->new_axis_mask();
   d5060:	60e8      	str	r0, [r5, #12]
   d5062:	210c      	movs	r1, #12
   d5064:	4620      	mov	r0, r4
   d5066:	f7ff fa47 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->shrink_axis_mask = schema_params->shrink_axis_mask();
   d506a:	6128      	str	r0, [r5, #16]
   d506c:	e1ae      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d506e:	4628      	mov	r0, r5
   d5070:	f7ff fa8e 	bl	d4590 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d5074:	4605      	mov	r5, r0
  }
  const MaximumMinimumOptions *builtin_options_as_MaximumMinimumOptions() const {
    return builtin_options_type() == BuiltinOptions_MaximumMinimumOptions ? static_cast<const MaximumMinimumOptions *>(builtin_options()) : nullptr;
  }
  const ArgMaxOptions *builtin_options_as_ArgMaxOptions() const {
    return builtin_options_type() == BuiltinOptions_ArgMaxOptions ? static_cast<const ArgMaxOptions *>(builtin_options()) : nullptr;
   d5076:	4620      	mov	r0, r4
   d5078:	f7ff fa38 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d507c:	2828      	cmp	r0, #40	; 0x28
   d507e:	f040 81a5 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d5082:	4620      	mov	r0, r4
   d5084:	f7ff fa67 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_ARG_MAX: {
      auto params = safe_allocator.Allocate<TfLiteArgMaxParams>();
      if (const auto* schema_params = op->builtin_options_as_ArgMaxOptions()) {
   d5088:	2800      	cmp	r0, #0
   d508a:	f000 819f 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef ArgMaxOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OUTPUT_TYPE = 4
  };
  TensorType output_type() const {
    return static_cast<TensorType>(GetField<int8_t>(VT_OUTPUT_TYPE, 0));
   d508e:	2200      	movs	r2, #0
   d5090:	2104      	movs	r1, #4
   d5092:	f7ff fa3b 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        ConvertTensorType(schema_params->output_type(), &params->output_type,
                          error_reporter);
   d5096:	463a      	mov	r2, r7
   d5098:	4629      	mov	r1, r5
   d509a:	b2c0      	uxtb	r0, r0
   d509c:	f7ff f9ee 	bl	d447c <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
   d50a0:	e194      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d50a2:	4628      	mov	r0, r5
   d50a4:	f7ff fa74 	bl	d4590 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d50a8:	4605      	mov	r5, r0
  }
  const PowOptions *builtin_options_as_PowOptions() const {
    return builtin_options_type() == BuiltinOptions_PowOptions ? static_cast<const PowOptions *>(builtin_options()) : nullptr;
  }
  const ArgMinOptions *builtin_options_as_ArgMinOptions() const {
    return builtin_options_type() == BuiltinOptions_ArgMinOptions ? static_cast<const ArgMinOptions *>(builtin_options()) : nullptr;
   d50aa:	4620      	mov	r0, r4
   d50ac:	f7ff fa1e 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d50b0:	2839      	cmp	r0, #57	; 0x39
   d50b2:	f040 818b 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d50b6:	4620      	mov	r0, r4
   d50b8:	f7ff fa4d 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_ARG_MIN: {
      auto params = safe_allocator.Allocate<TfLiteArgMinParams>();
      if (const auto* schema_params = op->builtin_options_as_ArgMinOptions()) {
   d50bc:	2800      	cmp	r0, #0
   d50be:	f000 8185 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef ArgMinOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OUTPUT_TYPE = 4
  };
  TensorType output_type() const {
    return static_cast<TensorType>(GetField<int8_t>(VT_OUTPUT_TYPE, 0));
   d50c2:	2200      	movs	r2, #0
   d50c4:	2104      	movs	r1, #4
   d50c6:	f7ff fa21 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        ConvertTensorType(schema_params->output_type(), &params->output_type,
                          error_reporter);
   d50ca:	463a      	mov	r2, r7
   d50cc:	4629      	mov	r1, r5
   d50ce:	b2c0      	uxtb	r0, r0
   d50d0:	f7ff f9d4 	bl	d447c <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
   d50d4:	e17a      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d50d6:	4628      	mov	r0, r5
   d50d8:	f7ff fa4e 	bl	d4578 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI25TfLiteTransposeConvParamsEEPT_v>
   d50dc:	4605      	mov	r5, r0
  }
  const SliceOptions *builtin_options_as_SliceOptions() const {
    return builtin_options_type() == BuiltinOptions_SliceOptions ? static_cast<const SliceOptions *>(builtin_options()) : nullptr;
  }
  const TransposeConvOptions *builtin_options_as_TransposeConvOptions() const {
    return builtin_options_type() == BuiltinOptions_TransposeConvOptions ? static_cast<const TransposeConvOptions *>(builtin_options()) : nullptr;
   d50de:	4620      	mov	r0, r4
   d50e0:	f7ff fa04 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d50e4:	2831      	cmp	r0, #49	; 0x31
   d50e6:	f040 8171 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d50ea:	4620      	mov	r0, r4
   d50ec:	f7ff fa33 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_TRANSPOSE_CONV: {
      auto params = safe_allocator.Allocate<TfLiteTransposeConvParams>();
      if (const auto* transpose_conv_params =
   d50f0:	4604      	mov	r4, r0
   d50f2:	2800      	cmp	r0, #0
   d50f4:	f000 816a 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_PADDING = 4,
    VT_STRIDE_W = 6,
    VT_STRIDE_H = 8
  };
  Padding padding() const {
    return static_cast<Padding>(GetField<int8_t>(VT_PADDING, 0));
   d50f8:	2200      	movs	r2, #0
   d50fa:	2104      	movs	r1, #4
   d50fc:	f7ff fa06 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_TransposeConvOptions()) {
        params->padding = parse_padding(transpose_conv_params->padding());
   d5100:	b2c0      	uxtb	r0, r0
   d5102:	f7ff f973 	bl	d43ec <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_7PaddingEE_clESA_.isra.0>
  }
  int32_t stride_w() const {
    return GetField<int32_t>(VT_STRIDE_W, 0);
   d5106:	2200      	movs	r2, #0
   d5108:	7028      	strb	r0, [r5, #0]
   d510a:	2106      	movs	r1, #6
   d510c:	4620      	mov	r0, r4
   d510e:	f7ff f9f3 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t stride_h() const {
    return GetField<int32_t>(VT_STRIDE_H, 0);
   d5112:	2200      	movs	r2, #0
        params->stride_width = transpose_conv_params->stride_w();
   d5114:	6068      	str	r0, [r5, #4]
   d5116:	2108      	movs	r1, #8
   d5118:	4620      	mov	r0, r4
   d511a:	f7ff f9ed 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->stride_height = transpose_conv_params->stride_h();
   d511e:	60a8      	str	r0, [r5, #8]
   d5120:	e154      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d5122:	4628      	mov	r0, r5
   d5124:	f7ff fa34 	bl	d4590 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d5128:	4605      	mov	r5, r0
  }
  const TransposeConvOptions *builtin_options_as_TransposeConvOptions() const {
    return builtin_options_type() == BuiltinOptions_TransposeConvOptions ? static_cast<const TransposeConvOptions *>(builtin_options()) : nullptr;
  }
  const SparseToDenseOptions *builtin_options_as_SparseToDenseOptions() const {
    return builtin_options_type() == BuiltinOptions_SparseToDenseOptions ? static_cast<const SparseToDenseOptions *>(builtin_options()) : nullptr;
   d512a:	4620      	mov	r0, r4
   d512c:	f7ff f9de 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d5130:	2832      	cmp	r0, #50	; 0x32
   d5132:	f040 814b 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d5136:	4620      	mov	r0, r4
   d5138:	f7ff fa0d 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SPARSE_TO_DENSE: {
      auto params = safe_allocator.Allocate<TfLiteSparseToDenseParams>();
      if (const auto* sparse_to_dense_params =
   d513c:	2800      	cmp	r0, #0
   d513e:	f000 8145 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef SparseToDenseOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_VALIDATE_INDICES = 4
  };
  bool validate_indices() const {
    return GetField<uint8_t>(VT_VALIDATE_INDICES, 0) != 0;
   d5142:	2200      	movs	r2, #0
   d5144:	2104      	movs	r1, #4
   d5146:	f7ff f9c7 	bl	d44d8 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
              op->builtin_options_as_SparseToDenseOptions()) {
        params->validate_indices = sparse_to_dense_params->validate_indices();
   d514a:	3000      	adds	r0, #0
   d514c:	bf18      	it	ne
   d514e:	2001      	movne	r0, #1
   d5150:	7028      	strb	r0, [r5, #0]
   d5152:	e13b      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d5154:	4628      	mov	r0, r5
   d5156:	f7ff fa1b 	bl	d4590 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d515a:	4605      	mov	r5, r0
  }
  const NotEqualOptions *builtin_options_as_NotEqualOptions() const {
    return builtin_options_type() == BuiltinOptions_NotEqualOptions ? static_cast<const NotEqualOptions *>(builtin_options()) : nullptr;
  }
  const ShapeOptions *builtin_options_as_ShapeOptions() const {
    return builtin_options_type() == BuiltinOptions_ShapeOptions ? static_cast<const ShapeOptions *>(builtin_options()) : nullptr;
   d515c:	4620      	mov	r0, r4
   d515e:	f7ff f9c5 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d5162:	2837      	cmp	r0, #55	; 0x37
   d5164:	f040 8132 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d5168:	4620      	mov	r0, r4
   d516a:	f7ff f9f4 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SHAPE: {
      auto params = safe_allocator.Allocate<TfLiteShapeParams>();
      if (const auto* schema_params = op->builtin_options_as_ShapeOptions()) {
   d516e:	2800      	cmp	r0, #0
   d5170:	f000 812c 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef ShapeOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OUT_TYPE = 4
  };
  TensorType out_type() const {
    return static_cast<TensorType>(GetField<int8_t>(VT_OUT_TYPE, 0));
   d5174:	2200      	movs	r2, #0
   d5176:	2104      	movs	r1, #4
   d5178:	f7ff f9c8 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        ConvertTensorType(schema_params->out_type(), &params->out_type,
                          error_reporter);
   d517c:	463a      	mov	r2, r7
   d517e:	4629      	mov	r1, r5
   d5180:	b2c0      	uxtb	r0, r0
   d5182:	f7ff f97b 	bl	d447c <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
   d5186:	e121      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d5188:	000e8689 	.word	0x000e8689
   d518c:	000e8691 	.word	0x000e8691
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d5190:	4628      	mov	r0, r5
   d5192:	f7ff fa01 	bl	d4598 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
   d5196:	4605      	mov	r5, r0
  }
  const FakeQuantOptions *builtin_options_as_FakeQuantOptions() const {
    return builtin_options_type() == BuiltinOptions_FakeQuantOptions ? static_cast<const FakeQuantOptions *>(builtin_options()) : nullptr;
  }
  const PackOptions *builtin_options_as_PackOptions() const {
    return builtin_options_type() == BuiltinOptions_PackOptions ? static_cast<const PackOptions *>(builtin_options()) : nullptr;
   d5198:	4620      	mov	r0, r4
   d519a:	f7ff f9a7 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d519e:	283b      	cmp	r0, #59	; 0x3b
   d51a0:	f040 8114 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d51a4:	4620      	mov	r0, r4
   d51a6:	f7ff f9d6 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = static_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_PACK: {
      auto params = safe_allocator.Allocate<TfLitePackParams>();
      if (const auto* pack_params = op->builtin_options_as_PackOptions()) {
   d51aa:	4604      	mov	r4, r0
   d51ac:	2800      	cmp	r0, #0
   d51ae:	f000 810d 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_VALUES_COUNT = 4,
    VT_AXIS = 6
  };
  int32_t values_count() const {
    return GetField<int32_t>(VT_VALUES_COUNT, 0);
   d51b2:	2200      	movs	r2, #0
   d51b4:	2104      	movs	r1, #4
   d51b6:	f7ff f99f 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t axis() const {
    return GetField<int32_t>(VT_AXIS, 0);
   d51ba:	2200      	movs	r2, #0
        params->values_count = pack_params->values_count();
   d51bc:	6028      	str	r0, [r5, #0]
   d51be:	2106      	movs	r1, #6
   d51c0:	4620      	mov	r0, r4
   d51c2:	f7ff f999 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->axis = pack_params->axis();
   d51c6:	6068      	str	r0, [r5, #4]
   d51c8:	e100      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_DELEGATE: {
      // TODO(ycling): Revisit when supporting saving delegated models.
      error_reporter->Report("DELEGATE op shouldn't exist in model.");
   d51ca:	4983      	ldr	r1, [pc, #524]	; (d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe38>)
   d51cc:	4610      	mov	r0, r2
   d51ce:	f7ff f8fb 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
      return kTfLiteError;
   d51d2:	2001      	movs	r0, #1
   d51d4:	e0fc      	b.n	d53d0 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe30>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d51d6:	4628      	mov	r0, r5
   d51d8:	f7ff f9d2 	bl	d4580 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteFakeQuantParamsEEPT_v>
   d51dc:	4605      	mov	r5, r0
  }
  const ArgMinOptions *builtin_options_as_ArgMinOptions() const {
    return builtin_options_type() == BuiltinOptions_ArgMinOptions ? static_cast<const ArgMinOptions *>(builtin_options()) : nullptr;
  }
  const FakeQuantOptions *builtin_options_as_FakeQuantOptions() const {
    return builtin_options_type() == BuiltinOptions_FakeQuantOptions ? static_cast<const FakeQuantOptions *>(builtin_options()) : nullptr;
   d51de:	4620      	mov	r0, r4
   d51e0:	f7ff f984 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d51e4:	283a      	cmp	r0, #58	; 0x3a
   d51e6:	f040 80f1 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d51ea:	4620      	mov	r0, r4
   d51ec:	f7ff f9b3 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      error_reporter->Report("DELEGATE op shouldn't exist in model.");
      return kTfLiteError;
    }
    case BuiltinOperator_FAKE_QUANT: {
      auto params = safe_allocator.Allocate<TfLiteFakeQuantParams>();
      if (const auto* schema_params =
   d51f0:	4604      	mov	r4, r0
   d51f2:	2800      	cmp	r0, #0
   d51f4:	f000 80ea 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_MAX = 6,
    VT_NUM_BITS = 8,
    VT_NARROW_RANGE = 10
  };
  float min() const {
    return GetField<float>(VT_MIN, 0.0f);
   d51f8:	2104      	movs	r1, #4
   d51fa:	ed9f 0a78 	vldr	s0, [pc, #480]	; d53dc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe3c>
   d51fe:	f7ff f98f 	bl	d4520 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  float max() const {
    return GetField<float>(VT_MAX, 0.0f);
   d5202:	2106      	movs	r1, #6
              op->builtin_options_as_FakeQuantOptions()) {
        params->min = schema_params->min();
   d5204:	ed85 0a00 	vstr	s0, [r5]
   d5208:	4620      	mov	r0, r4
   d520a:	ed9f 0a74 	vldr	s0, [pc, #464]	; d53dc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe3c>
   d520e:	f7ff f987 	bl	d4520 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  int32_t num_bits() const {
    return GetField<int32_t>(VT_NUM_BITS, 0);
   d5212:	2200      	movs	r2, #0
        params->max = schema_params->max();
   d5214:	ed85 0a01 	vstr	s0, [r5, #4]
   d5218:	2108      	movs	r1, #8
   d521a:	4620      	mov	r0, r4
   d521c:	f7ff f96c 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  bool narrow_range() const {
    return GetField<uint8_t>(VT_NARROW_RANGE, 0) != 0;
   d5220:	2200      	movs	r2, #0
        params->num_bits = schema_params->num_bits();
   d5222:	60a8      	str	r0, [r5, #8]
   d5224:	210a      	movs	r1, #10
   d5226:	4620      	mov	r0, r4
   d5228:	f7ff f956 	bl	d44d8 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->narrow_range = schema_params->narrow_range();
   d522c:	3000      	adds	r0, #0
   d522e:	bf18      	it	ne
   d5230:	2001      	movne	r0, #1
   d5232:	7328      	strb	r0, [r5, #12]
   d5234:	e0ca      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d5236:	4628      	mov	r0, r5
   d5238:	f7ff f9a6 	bl	d4588 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
   d523c:	4605      	mov	r5, r0
  }
  const LogicalOrOptions *builtin_options_as_LogicalOrOptions() const {
    return builtin_options_type() == BuiltinOptions_LogicalOrOptions ? static_cast<const LogicalOrOptions *>(builtin_options()) : nullptr;
  }
  const OneHotOptions *builtin_options_as_OneHotOptions() const {
    return builtin_options_type() == BuiltinOptions_OneHotOptions ? static_cast<const OneHotOptions *>(builtin_options()) : nullptr;
   d523e:	4620      	mov	r0, r4
   d5240:	f7ff f954 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d5244:	283d      	cmp	r0, #61	; 0x3d
   d5246:	f040 80c1 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d524a:	4620      	mov	r0, r4
   d524c:	f7ff f983 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = static_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_ONE_HOT: {
      auto params = safe_allocator.Allocate<TfLiteOneHotParams>();
      if (const auto* schema_params = op->builtin_options_as_OneHotOptions()) {
   d5250:	2800      	cmp	r0, #0
   d5252:	f000 80bb 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef OneHotOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXIS = 4
  };
  int32_t axis() const {
    return GetField<int32_t>(VT_AXIS, 0);
   d5256:	2200      	movs	r2, #0
   d5258:	2104      	movs	r1, #4
   d525a:	f7ff f94d 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->axis = schema_params->axis();
   d525e:	6028      	str	r0, [r5, #0]
   d5260:	e0b4      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d5262:	4628      	mov	r0, r5
   d5264:	f7ff f998 	bl	d4598 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
   d5268:	4605      	mov	r5, r0
  }
  const LogicalNotOptions *builtin_options_as_LogicalNotOptions() const {
    return builtin_options_type() == BuiltinOptions_LogicalNotOptions ? static_cast<const LogicalNotOptions *>(builtin_options()) : nullptr;
  }
  const UnpackOptions *builtin_options_as_UnpackOptions() const {
    return builtin_options_type() == BuiltinOptions_UnpackOptions ? static_cast<const UnpackOptions *>(builtin_options()) : nullptr;
   d526a:	4620      	mov	r0, r4
   d526c:	f7ff f93e 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d5270:	2840      	cmp	r0, #64	; 0x40
   d5272:	f040 80ab 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d5276:	4620      	mov	r0, r4
   d5278:	f7ff f96d 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = static_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_UNPACK: {
      auto params = safe_allocator.Allocate<TfLiteUnpackParams>();
      if (const auto* unpack_params = op->builtin_options_as_UnpackOptions()) {
   d527c:	4604      	mov	r4, r0
   d527e:	2800      	cmp	r0, #0
   d5280:	f000 80a4 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_NUM = 4,
    VT_AXIS = 6
  };
  int32_t num() const {
    return GetField<int32_t>(VT_NUM, 0);
   d5284:	2200      	movs	r2, #0
   d5286:	2104      	movs	r1, #4
   d5288:	f7ff f936 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t axis() const {
    return GetField<int32_t>(VT_AXIS, 0);
   d528c:	2200      	movs	r2, #0
        params->num = unpack_params->num();
   d528e:	6028      	str	r0, [r5, #0]
   d5290:	2106      	movs	r1, #6
   d5292:	4620      	mov	r0, r4
   d5294:	f7ff f930 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->axis = unpack_params->axis();
   d5298:	6068      	str	r0, [r5, #4]
   d529a:	e097      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d529c:	4628      	mov	r0, r5
   d529e:	f7ff f973 	bl	d4588 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
   d52a2:	4605      	mov	r5, r0
  }
  const ResizeNearestNeighborOptions *builtin_options_as_ResizeNearestNeighborOptions() const {
    return builtin_options_type() == BuiltinOptions_ResizeNearestNeighborOptions ? static_cast<const ResizeNearestNeighborOptions *>(builtin_options()) : nullptr;
  }
  const LeakyReluOptions *builtin_options_as_LeakyReluOptions() const {
    return builtin_options_type() == BuiltinOptions_LeakyReluOptions ? static_cast<const LeakyReluOptions *>(builtin_options()) : nullptr;
   d52a4:	4620      	mov	r0, r4
   d52a6:	f7ff f921 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d52aa:	284b      	cmp	r0, #75	; 0x4b
   d52ac:	f040 808e 	bne.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d52b0:	4620      	mov	r0, r4
   d52b2:	f7ff f950 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_LEAKY_RELU: {
      auto params = safe_allocator.Allocate<TfLiteLeakyReluParams>();
      if (const auto* leaky_relu_params =
   d52b6:	2800      	cmp	r0, #0
   d52b8:	f000 8088 	beq.w	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef LeakyReluOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ALPHA = 4
  };
  float alpha() const {
    return GetField<float>(VT_ALPHA, 0.0f);
   d52bc:	ed9f 0a47 	vldr	s0, [pc, #284]	; d53dc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe3c>
   d52c0:	2104      	movs	r1, #4
   d52c2:	f7ff f92d 	bl	d4520 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
              op->builtin_options_as_LeakyReluOptions()) {
        params->alpha = leaky_relu_params->alpha();
   d52c6:	ed85 0a00 	vstr	s0, [r5]
   d52ca:	e07f      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d52cc:	4628      	mov	r0, r5
   d52ce:	f7ff f95f 	bl	d4590 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d52d2:	4605      	mov	r5, r0
  }
  const SquaredDifferenceOptions *builtin_options_as_SquaredDifferenceOptions() const {
    return builtin_options_type() == BuiltinOptions_SquaredDifferenceOptions ? static_cast<const SquaredDifferenceOptions *>(builtin_options()) : nullptr;
  }
  const MirrorPadOptions *builtin_options_as_MirrorPadOptions() const {
    return builtin_options_type() == BuiltinOptions_MirrorPadOptions ? static_cast<const MirrorPadOptions *>(builtin_options()) : nullptr;
   d52d4:	4620      	mov	r0, r4
   d52d6:	f7ff f909 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d52da:	284d      	cmp	r0, #77	; 0x4d
   d52dc:	d176      	bne.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d52de:	4620      	mov	r0, r4
   d52e0:	f7ff f939 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_MIRROR_PAD: {
      auto params = safe_allocator.Allocate<TfLiteMirrorPaddingParams>();
      const auto* mirror_pad_params = op->builtin_options_as_MirrorPadOptions();
      if (mirror_pad_params != nullptr) {
   d52e4:	2800      	cmp	r0, #0
   d52e6:	d071      	beq.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef MirrorPadOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_MODE = 4
  };
  MirrorPadMode mode() const {
    return static_cast<MirrorPadMode>(GetField<int8_t>(VT_MODE, 0));
   d52e8:	2200      	movs	r2, #0
   d52ea:	2104      	movs	r1, #4
   d52ec:	f7ff f90e 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->mode =
            mirror_pad_params->mode() == tflite::MirrorPadMode_REFLECT
                ? TfLiteMirrorPaddingMode::kTfLiteMirrorPaddingReflect
                : TfLiteMirrorPaddingMode::kTfLiteMirrorPaddingSymmetric;
   d52f0:	b2c0      	uxtb	r0, r0
   d52f2:	2800      	cmp	r0, #0
   d52f4:	bf0c      	ite	eq
   d52f6:	2301      	moveq	r3, #1
   d52f8:	2302      	movne	r3, #2
   d52fa:	702b      	strb	r3, [r5, #0]
   d52fc:	e066      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d52fe:	4628      	mov	r0, r5
   d5300:	f7ff f946 	bl	d4590 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d5304:	4605      	mov	r5, r0
  }
  const SplitVOptions *builtin_options_as_SplitVOptions() const {
    return builtin_options_type() == BuiltinOptions_SplitVOptions ? static_cast<const SplitVOptions *>(builtin_options()) : nullptr;
  }
  const UniqueOptions *builtin_options_as_UniqueOptions() const {
    return builtin_options_type() == BuiltinOptions_UniqueOptions ? static_cast<const UniqueOptions *>(builtin_options()) : nullptr;
   d5306:	4620      	mov	r0, r4
   d5308:	f7ff f8f0 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d530c:	2850      	cmp	r0, #80	; 0x50
   d530e:	d15d      	bne.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d5310:	4620      	mov	r0, r4
   d5312:	f7ff f920 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_UNIQUE: {
      auto params = safe_allocator.Allocate<TfLiteUniqueParams>();
      const auto* unique_params = op->builtin_options_as_UniqueOptions();
      if (unique_params != nullptr) {
   d5316:	2800      	cmp	r0, #0
   d5318:	d058      	beq.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef UniqueOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_IDX_OUT_TYPE = 4
  };
  TensorType idx_out_type() const {
    return static_cast<TensorType>(GetField<int8_t>(VT_IDX_OUT_TYPE, 2));
   d531a:	2202      	movs	r2, #2
   d531c:	2104      	movs	r1, #4
   d531e:	f7ff f8f5 	bl	d450c <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->index_out_type =
            unique_params->idx_out_type() == tflite::TensorType_INT64
                ? TfLiteType::kTfLiteInt64
                : TfLiteType::kTfLiteInt32;
   d5322:	b2c0      	uxtb	r0, r0
   d5324:	2804      	cmp	r0, #4
   d5326:	bf0c      	ite	eq
   d5328:	2304      	moveq	r3, #4
   d532a:	2302      	movne	r3, #2
   d532c:	702b      	strb	r3, [r5, #0]
   d532e:	e04d      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d5330:	4628      	mov	r0, r5
   d5332:	f7ff f931 	bl	d4598 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
   d5336:	4605      	mov	r5, r0
  }
  const RankOptions *builtin_options_as_RankOptions() const {
    return builtin_options_type() == BuiltinOptions_RankOptions ? static_cast<const RankOptions *>(builtin_options()) : nullptr;
  }
  const ReverseSequenceOptions *builtin_options_as_ReverseSequenceOptions() const {
    return builtin_options_type() == BuiltinOptions_ReverseSequenceOptions ? static_cast<const ReverseSequenceOptions *>(builtin_options()) : nullptr;
   d5338:	4620      	mov	r0, r4
   d533a:	f7ff f8d7 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d533e:	2857      	cmp	r0, #87	; 0x57
   d5340:	d144      	bne.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d5342:	4620      	mov	r0, r4
   d5344:	f7ff f907 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_REVERSE_SEQUENCE: {
      auto params = safe_allocator.Allocate<TfLiteReverseSequenceParams>();
      if (const auto* reverse_seq_params =
   d5348:	4604      	mov	r4, r0
   d534a:	2800      	cmp	r0, #0
   d534c:	d03e      	beq.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SEQ_DIM = 4,
    VT_BATCH_DIM = 6
  };
  int32_t seq_dim() const {
    return GetField<int32_t>(VT_SEQ_DIM, 0);
   d534e:	2200      	movs	r2, #0
   d5350:	2104      	movs	r1, #4
   d5352:	f7ff f8d1 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t batch_dim() const {
    return GetField<int32_t>(VT_BATCH_DIM, 0);
   d5356:	2200      	movs	r2, #0
              op->builtin_options_as_ReverseSequenceOptions()) {
        params->seq_dim = reverse_seq_params->seq_dim();
   d5358:	6028      	str	r0, [r5, #0]
   d535a:	2106      	movs	r1, #6
   d535c:	4620      	mov	r0, r4
   d535e:	f7ff f8cb 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->batch_dim = reverse_seq_params->batch_dim();
   d5362:	6068      	str	r0, [r5, #4]
   d5364:	e032      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      }
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_IF: {
      TfLiteIfParams* params = allocator->AllocatePOD<TfLiteIfParams>();
   d5366:	4628      	mov	r0, r5
   d5368:	f7ff f916 	bl	d4598 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
   d536c:	4605      	mov	r5, r0
  }
  const HardSwishOptions *builtin_options_as_HardSwishOptions() const {
    return builtin_options_type() == BuiltinOptions_HardSwishOptions ? static_cast<const HardSwishOptions *>(builtin_options()) : nullptr;
  }
  const IfOptions *builtin_options_as_IfOptions() const {
    return builtin_options_type() == BuiltinOptions_IfOptions ? static_cast<const IfOptions *>(builtin_options()) : nullptr;
   d536e:	4620      	mov	r0, r4
   d5370:	f7ff f8bc 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d5374:	285c      	cmp	r0, #92	; 0x5c
   d5376:	d129      	bne.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d5378:	4620      	mov	r0, r4
   d537a:	f7ff f8ec 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      if (const auto* if_params = op->builtin_options_as_IfOptions()) {
   d537e:	4604      	mov	r4, r0
   d5380:	b320      	cbz	r0, d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_THEN_SUBGRAPH_INDEX = 4,
    VT_ELSE_SUBGRAPH_INDEX = 6
  };
  int32_t then_subgraph_index() const {
    return GetField<int32_t>(VT_THEN_SUBGRAPH_INDEX, 0);
   d5382:	2200      	movs	r2, #0
   d5384:	2104      	movs	r1, #4
   d5386:	f7ff f8b7 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t else_subgraph_index() const {
    return GetField<int32_t>(VT_ELSE_SUBGRAPH_INDEX, 0);
   d538a:	2200      	movs	r2, #0
        params->then_subgraph_index = if_params->then_subgraph_index();
   d538c:	6028      	str	r0, [r5, #0]
   d538e:	2106      	movs	r1, #6
   d5390:	4620      	mov	r0, r4
   d5392:	f7ff f8b1 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->else_subgraph_index = if_params->else_subgraph_index();
   d5396:	6068      	str	r0, [r5, #4]
   d5398:	e018      	b.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      }
      *builtin_data = reinterpret_cast<void*>(params);
      break;
    }
    case BuiltinOperator_WHILE: {
      TfLiteWhileParams* params = allocator->AllocatePOD<TfLiteWhileParams>();
   d539a:	4628      	mov	r0, r5
   d539c:	f7ff f8fc 	bl	d4598 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
   d53a0:	4605      	mov	r5, r0
  }
  const IfOptions *builtin_options_as_IfOptions() const {
    return builtin_options_type() == BuiltinOptions_IfOptions ? static_cast<const IfOptions *>(builtin_options()) : nullptr;
  }
  const WhileOptions *builtin_options_as_WhileOptions() const {
    return builtin_options_type() == BuiltinOptions_WhileOptions ? static_cast<const WhileOptions *>(builtin_options()) : nullptr;
   d53a2:	4620      	mov	r0, r4
   d53a4:	f7ff f8a2 	bl	d44ec <_ZNK6tflite8Operator20builtin_options_typeEv>
   d53a8:	285d      	cmp	r0, #93	; 0x5d
   d53aa:	d10f      	bne.n	d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d53ac:	4620      	mov	r0, r4
   d53ae:	f7ff f8d2 	bl	d4556 <_ZNK6tflite8Operator15builtin_optionsEv>
      if (const auto* while_params = op->builtin_options_as_WhileOptions()) {
   d53b2:	4604      	mov	r4, r0
   d53b4:	b150      	cbz	r0, d53cc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_COND_SUBGRAPH_INDEX = 4,
    VT_BODY_SUBGRAPH_INDEX = 6
  };
  int32_t cond_subgraph_index() const {
    return GetField<int32_t>(VT_COND_SUBGRAPH_INDEX, 0);
   d53b6:	2200      	movs	r2, #0
   d53b8:	2104      	movs	r1, #4
   d53ba:	f7ff f89d 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t body_subgraph_index() const {
    return GetField<int32_t>(VT_BODY_SUBGRAPH_INDEX, 0);
   d53be:	2200      	movs	r2, #0
        params->cond_subgraph_index = while_params->cond_subgraph_index();
   d53c0:	6028      	str	r0, [r5, #0]
   d53c2:	2106      	movs	r1, #6
   d53c4:	4620      	mov	r0, r4
   d53c6:	f7ff f897 	bl	d44f8 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->body_subgraph_index = while_params->body_subgraph_index();
   d53ca:	6068      	str	r0, [r5, #4]
      }
      *builtin_data = reinterpret_cast<void*>(params);
   d53cc:	6035      	str	r5, [r6, #0]
    case BuiltinOperator_QUANTIZE:
    case BuiltinOperator_NON_MAX_SUPPRESSION_V4:
    case BuiltinOperator_NON_MAX_SUPPRESSION_V5:
      break;
  }
  return kTfLiteOk;
   d53ce:	2000      	movs	r0, #0
}  // NOLINT[readability/fn_size]
   d53d0:	b002      	add	sp, #8
   d53d2:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   d53d6:	bf00      	nop
   d53d8:	000e8699 	.word	0x000e8699
   d53dc:	00000000 	.word	0x00000000

000d53e0 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration>:

namespace tflite {

TfLiteStatus GetRegistrationFromOpCode(
    const OperatorCode* opcode, const OpResolver& op_resolver,
    ErrorReporter* error_reporter, const TfLiteRegistration** registration) {
   d53e0:	e92d 43f8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, lr}
   d53e4:	461f      	mov	r7, r3
  TfLiteStatus status = kTfLiteOk;
  *registration = nullptr;
   d53e6:	2300      	movs	r3, #0

namespace tflite {

TfLiteStatus GetRegistrationFromOpCode(
    const OperatorCode* opcode, const OpResolver& op_resolver,
    ErrorReporter* error_reporter, const TfLiteRegistration** registration) {
   d53e8:	460e      	mov	r6, r1
  TfLiteStatus status = kTfLiteOk;
  *registration = nullptr;
   d53ea:	603b      	str	r3, [r7, #0]
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
   d53ec:	2104      	movs	r1, #4

namespace tflite {

TfLiteStatus GetRegistrationFromOpCode(
    const OperatorCode* opcode, const OpResolver& op_resolver,
    ErrorReporter* error_reporter, const TfLiteRegistration** registration) {
   d53ee:	4605      	mov	r5, r0
   d53f0:	4690      	mov	r8, r2
   d53f2:	f7ff f83b 	bl	d446c <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d53f6:	b100      	cbz	r0, d53fa <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x1a>
   d53f8:	5628      	ldrsb	r0, [r5, r0]
    VT_BUILTIN_CODE = 4,
    VT_CUSTOM_CODE = 6,
    VT_VERSION = 8
  };
  BuiltinOperator builtin_code() const {
    return static_cast<BuiltinOperator>(GetField<int8_t>(VT_BUILTIN_CODE, 0));
   d53fa:	b2c4      	uxtb	r4, r0
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
   d53fc:	2108      	movs	r1, #8
   d53fe:	4628      	mov	r0, r5
   d5400:	f7ff f834 	bl	d446c <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d5404:	b110      	cbz	r0, d540c <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x2c>
   d5406:	f855 9000 	ldr.w	r9, [r5, r0]
   d540a:	e001      	b.n	d5410 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x30>
   d540c:	f04f 0901 	mov.w	r9, #1
  TfLiteStatus status = kTfLiteOk;
  *registration = nullptr;
  auto builtin_code = opcode->builtin_code();
  int version = opcode->version();

  if (builtin_code > BuiltinOperator_MAX ||
   d5410:	2c79      	cmp	r4, #121	; 0x79
   d5412:	d905      	bls.n	d5420 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x40>
      builtin_code < BuiltinOperator_MIN) {
    error_reporter->Report(
        "Op builtin_code out of range: %d. Are you using old TFLite binary "
        "with newer model?",
        builtin_code);
   d5414:	4622      	mov	r2, r4
   d5416:	491b      	ldr	r1, [pc, #108]	; (d5484 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0xa4>)
   d5418:	4640      	mov	r0, r8
   d541a:	f7fe ffd5 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
   d541e:	e01f      	b.n	d5460 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x80>
    status = kTfLiteError;
  } else if (builtin_code != BuiltinOperator_CUSTOM) {
   d5420:	2c20      	cmp	r4, #32
   d5422:	d010      	beq.n	d5446 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x66>
    *registration = op_resolver.FindOp(builtin_code, version);
   d5424:	6833      	ldr	r3, [r6, #0]
   d5426:	464a      	mov	r2, r9
   d5428:	681b      	ldr	r3, [r3, #0]
   d542a:	4621      	mov	r1, r4
   d542c:	4630      	mov	r0, r6
   d542e:	4798      	blx	r3
   d5430:	6038      	str	r0, [r7, #0]
    if (*registration == nullptr) {
   d5432:	bb20      	cbnz	r0, d547e <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x9e>
      error_reporter->Report(
          "Didn't find op for builtin opcode '%s' version '%d'\n",
          EnumNameBuiltinOperator(builtin_code), version);
   d5434:	4a14      	ldr	r2, [pc, #80]	; (d5488 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0xa8>)
   d5436:	4915      	ldr	r1, [pc, #84]	; (d548c <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0xac>)
   d5438:	f852 2024 	ldr.w	r2, [r2, r4, lsl #2]
   d543c:	464b      	mov	r3, r9
   d543e:	4640      	mov	r0, r8
   d5440:	f7fe ffc2 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
   d5444:	e00c      	b.n	d5460 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x80>
  }

  template<typename P> P GetPointer(voffset_t field) {
    auto field_offset = GetOptionalFieldOffset(field);
   d5446:	2106      	movs	r1, #6
   d5448:	4628      	mov	r0, r5
   d544a:	f7ff f80f 	bl	d446c <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    auto p = data_ + field_offset;
   d544e:	182a      	adds	r2, r5, r0
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
   d5450:	b110      	cbz	r0, d5458 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x78>
   d5452:	5829      	ldr	r1, [r5, r0]
      status = kTfLiteError;
    }
  } else if (!opcode->custom_code()) {
   d5454:	1851      	adds	r1, r2, r1
   d5456:	d106      	bne.n	d5466 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x86>
    error_reporter->Report(
        "Operator with CUSTOM builtin_code has no custom_code.\n");
   d5458:	490d      	ldr	r1, [pc, #52]	; (d5490 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0xb0>)
   d545a:	4640      	mov	r0, r8
   d545c:	f7fe ffb4 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    status = kTfLiteError;
   d5460:	2001      	movs	r0, #1
   d5462:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
  } else {
    const char* name = opcode->custom_code()->c_str();
    *registration = op_resolver.FindOp(name, version);
   d5466:	6833      	ldr	r3, [r6, #0]
   d5468:	464a      	mov	r2, r9
   d546a:	685b      	ldr	r3, [r3, #4]
   d546c:	3104      	adds	r1, #4
   d546e:	4630      	mov	r0, r6
   d5470:	4798      	blx	r3
   d5472:	6038      	str	r0, [r7, #0]
      builtin_code < BuiltinOperator_MIN) {
    error_reporter->Report(
        "Op builtin_code out of range: %d. Are you using old TFLite binary "
        "with newer model?",
        builtin_code);
    status = kTfLiteError;
   d5474:	fab0 f080 	clz	r0, r0
   d5478:	0940      	lsrs	r0, r0, #5
   d547a:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
namespace tflite {

TfLiteStatus GetRegistrationFromOpCode(
    const OperatorCode* opcode, const OpResolver& op_resolver,
    ErrorReporter* error_reporter, const TfLiteRegistration** registration) {
  TfLiteStatus status = kTfLiteOk;
   d547e:	2000      	movs	r0, #0
      // while preparing ops.
      status = kTfLiteError;
    }
  }
  return status;
}
   d5480:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
   d5484:	000e88b0 	.word	0x000e88b0
   d5488:	000e86c4 	.word	0x000e86c4
   d548c:	000e8904 	.word	0x000e8904
   d5490:	000e8939 	.word	0x000e8939

000d5494 <_ZN6tflite19ResetVariableTensorEP12TfLiteTensor>:

#include <string.h>

namespace tflite {

TfLiteStatus ResetVariableTensor(TfLiteTensor* tensor) {
   d5494:	b530      	push	{r4, r5, lr}
  if (!tensor->is_variable) {
   d5496:	f890 302d 	ldrb.w	r3, [r0, #45]	; 0x2d
   d549a:	b16b      	cbz	r3, d54b8 <_ZN6tflite19ResetVariableTensorEP12TfLiteTensor+0x24>
    return kTfLiteOk;
  }
  // TODO(b/115961645): Implement - If a variable tensor has a buffer, reset it
  // to the value of the buffer.
  int value = 0;
  if (tensor->type == kTfLiteInt8) {
   d549c:	7803      	ldrb	r3, [r0, #0]
#if __ANDROID__ || defined(__x86_64__) || defined(__i386__) || \
    defined(__i386) || defined(__x86__) || defined(__X86__) || \
    defined(_X86_) || defined(_M_IX86) || defined(_M_X64)
  memset(tensor->data.raw, value, tensor->bytes);
#else
  char* raw_ptr = tensor->data.raw;
   d549e:	6844      	ldr	r4, [r0, #4]
    return kTfLiteOk;
  }
  // TODO(b/115961645): Implement - If a variable tensor has a buffer, reset it
  // to the value of the buffer.
  int value = 0;
  if (tensor->type == kTfLiteInt8) {
   d54a0:	2b09      	cmp	r3, #9
    value = tensor->params.zero_point;
   d54a2:	bf0c      	ite	eq
   d54a4:	6901      	ldreq	r1, [r0, #16]
  if (!tensor->is_variable) {
    return kTfLiteOk;
  }
  // TODO(b/115961645): Implement - If a variable tensor has a buffer, reset it
  // to the value of the buffer.
  int value = 0;
   d54a6:	2100      	movne	r1, #0
#if __ANDROID__ || defined(__x86_64__) || defined(__i386__) || \
    defined(__i386) || defined(__x86__) || defined(__X86__) || \
    defined(_X86_) || defined(_M_IX86) || defined(_M_X64)
  memset(tensor->data.raw, value, tensor->bytes);
#else
  char* raw_ptr = tensor->data.raw;
   d54a8:	4623      	mov	r3, r4
  for (int i = 0; i < tensor->bytes; ++i) {
   d54aa:	6985      	ldr	r5, [r0, #24]
   d54ac:	1b1a      	subs	r2, r3, r4
   d54ae:	4295      	cmp	r5, r2
   d54b0:	d902      	bls.n	d54b8 <_ZN6tflite19ResetVariableTensorEP12TfLiteTensor+0x24>
    *raw_ptr = value;
   d54b2:	f803 1b01 	strb.w	r1, [r3], #1
    defined(__i386) || defined(__x86__) || defined(__X86__) || \
    defined(_X86_) || defined(_M_IX86) || defined(_M_X64)
  memset(tensor->data.raw, value, tensor->bytes);
#else
  char* raw_ptr = tensor->data.raw;
  for (int i = 0; i < tensor->bytes; ++i) {
   d54b6:	e7f8      	b.n	d54aa <_ZN6tflite19ResetVariableTensorEP12TfLiteTensor+0x16>
    *raw_ptr = value;
    raw_ptr++;
  }
#endif
  return kTfLiteOk;
}
   d54b8:	2000      	movs	r0, #0
   d54ba:	bd30      	pop	{r4, r5, pc}

000d54bc <_ZN12_GLOBAL__N_19StrCatStrEPciPKc>:
  return start;
}

// Appends a string to a string, in-place. You need to pass in the maximum
// string length as the second argument.
char* StrCatStr(char* main, int main_max_length, const char* to_append) {
   d54bc:	b530      	push	{r4, r5, lr}
   d54be:	4604      	mov	r4, r0
   d54c0:	4623      	mov	r3, r4
   d54c2:	3401      	adds	r4, #1
  char* current = main;
  while (*current != 0) {
   d54c4:	781d      	ldrb	r5, [r3, #0]
   d54c6:	2d00      	cmp	r5, #0
   d54c8:	d1fa      	bne.n	d54c0 <_ZN12_GLOBAL__N_19StrCatStrEPciPKc+0x4>
    ++current;
  }
  char* current_end = main + (main_max_length - 1);
   d54ca:	3901      	subs	r1, #1
   d54cc:	4408      	add	r0, r1
   d54ce:	3a01      	subs	r2, #1
  while ((*to_append != 0) && (current < current_end)) {
   d54d0:	f812 1f01 	ldrb.w	r1, [r2, #1]!
   d54d4:	b121      	cbz	r1, d54e0 <_ZN12_GLOBAL__N_19StrCatStrEPciPKc+0x24>
   d54d6:	4283      	cmp	r3, r0
   d54d8:	d202      	bcs.n	d54e0 <_ZN12_GLOBAL__N_19StrCatStrEPciPKc+0x24>
    *current = *to_append;
   d54da:	f803 1b01 	strb.w	r1, [r3], #1
  char* current = main;
  while (*current != 0) {
    ++current;
  }
  char* current_end = main + (main_max_length - 1);
  while ((*to_append != 0) && (current < current_end)) {
   d54de:	e7f7      	b.n	d54d0 <_ZN12_GLOBAL__N_19StrCatStrEPciPKc+0x14>
    *current = *to_append;
    ++current;
    ++to_append;
  }
  *current = 0;
   d54e0:	2200      	movs	r2, #0
   d54e2:	701a      	strb	r2, [r3, #0]
  return current;
}
   d54e4:	4618      	mov	r0, r3
   d54e6:	bd30      	pop	{r4, r5, pc}

000d54e8 <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci>:

// Populates the provided buffer with an ASCII representation of the number.
char* FastUInt32ToBufferLeft(uint32_t i, char* buffer, int base) {
   d54e8:	b530      	push	{r4, r5, lr}
   d54ea:	4603      	mov	r3, r0
   d54ec:	460c      	mov	r4, r1
  char* start = buffer;
  do {
    int32_t digit = i % base;
   d54ee:	fbb3 f5f2 	udiv	r5, r3, r2
   d54f2:	fb02 3315 	mls	r3, r2, r5, r3
    char character;
    if (digit < 10) {
   d54f6:	2b09      	cmp	r3, #9
      character = '0' + digit;
   d54f8:	bfd4      	ite	le
   d54fa:	3330      	addle	r3, #48	; 0x30
    } else {
      character = 'a' + (digit - 10);
   d54fc:	3357      	addgt	r3, #87	; 0x57
    }
    *buffer++ = character;
   d54fe:	4620      	mov	r0, r4
    int32_t digit = i % base;
    char character;
    if (digit < 10) {
      character = '0' + digit;
    } else {
      character = 'a' + (digit - 10);
   d5500:	b2db      	uxtb	r3, r3
    }
    *buffer++ = character;
   d5502:	f800 3b01 	strb.w	r3, [r0], #1
    i /= base;
   d5506:	462b      	mov	r3, r5
  } while (i > 0);
   d5508:	b10d      	cbz	r5, d550e <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci+0x26>
   d550a:	4604      	mov	r4, r0
   d550c:	e7ef      	b.n	d54ee <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci+0x6>
  *buffer = 0;
   d550e:	7065      	strb	r5, [r4, #1]

// Reverses a zero-terminated string in-place.
char* ReverseStringInPlace(char* start, char* end) {
  char* p1 = start;
  char* p2 = end - 1;
  while (p1 < p2) {
   d5510:	42a1      	cmp	r1, r4
   d5512:	d206      	bcs.n	d5522 <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci+0x3a>
    char tmp = *p1;
   d5514:	780b      	ldrb	r3, [r1, #0]
    *p1++ = *p2;
   d5516:	7822      	ldrb	r2, [r4, #0]
   d5518:	f801 2b01 	strb.w	r2, [r1], #1
    *p2-- = tmp;
   d551c:	f804 3901 	strb.w	r3, [r4], #-1
   d5520:	e7f6      	b.n	d5510 <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci+0x28>
    i /= base;
  } while (i > 0);
  *buffer = 0;
  ReverseStringInPlace(start, buffer);
  return buffer;
}
   d5522:	bd30      	pop	{r4, r5, pc}

000d5524 <DebugLogInt32>:
  return current;
}

}  // namespace

extern "C" void DebugLogInt32(int32_t i) {
   d5524:	b500      	push	{lr}
}

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
   d5526:	2800      	cmp	r0, #0
  return current;
}

}  // namespace

extern "C" void DebugLogInt32(int32_t i) {
   d5528:	b08d      	sub	sp, #52	; 0x34

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
    *buffer++ = '-';
   d552a:	bfbd      	ittte	lt
   d552c:	232d      	movlt	r3, #45	; 0x2d
    u = -u;
   d552e:	4240      	neglt	r0, r0

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
    *buffer++ = '-';
   d5530:	f10d 0101 	addlt.w	r1, sp, #1
}

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
   d5534:	4669      	movge	r1, sp
    *buffer++ = '-';
    u = -u;
  }
  return FastUInt32ToBufferLeft(u, buffer, 10);
   d5536:	f04f 020a 	mov.w	r2, #10

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
    *buffer++ = '-';
   d553a:	bfb8      	it	lt
   d553c:	f88d 3000 	strblt.w	r3, [sp]
    u = -u;
  }
  return FastUInt32ToBufferLeft(u, buffer, 10);
   d5540:	f7ff ffd2 	bl	d54e8 <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci>
}  // namespace

extern "C" void DebugLogInt32(int32_t i) {
  char number_string[kFastToBufferSize];
  FastInt32ToBufferLeft(i, number_string);
  DebugLog(number_string);
   d5544:	4668      	mov	r0, sp
   d5546:	f000 ff61 	bl	d640c <DebugLog>
}
   d554a:	b00d      	add	sp, #52	; 0x34
   d554c:	f85d fb04 	ldr.w	pc, [sp], #4

000d5550 <DebugLogFloat>:
  char number_string[kFastToBufferSize];
  FastUInt32ToBufferLeft(i, number_string, 16);
  DebugLog(number_string);
}

extern "C" void DebugLogFloat(float i) {
   d5550:	b5f0      	push	{r4, r5, r6, r7, lr}
  const uint32_t sign_mask = 0x80000000;
  const uint32_t exponent_mask = 0x7f800000;
  const int32_t exponent_shift = 23;
  const int32_t exponent_bias = 127;
  const uint32_t fraction_mask = 0x007fffff;
  const uint32_t u = *reinterpret_cast<uint32_t*>(&f);
   d5552:	ee10 3a10 	vmov	r3, s0
  char number_string[kFastToBufferSize];
  FastUInt32ToBufferLeft(i, number_string, 16);
  DebugLog(number_string);
}

extern "C" void DebugLogFloat(float i) {
   d5556:	b09d      	sub	sp, #116	; 0x74
  const uint32_t u = *reinterpret_cast<uint32_t*>(&f);
  const int32_t exponent =
      ((u & exponent_mask) >> exponent_shift) - exponent_bias;
  const uint32_t fraction = (u & fraction_mask);
  // Expect ~0x2B1B9D3 for fraction.
  if (u & sign_mask) {
   d5558:	2b00      	cmp	r3, #0
  const int32_t exponent_shift = 23;
  const int32_t exponent_bias = 127;
  const uint32_t fraction_mask = 0x007fffff;
  const uint32_t u = *reinterpret_cast<uint32_t*>(&f);
  const int32_t exponent =
      ((u & exponent_mask) >> exponent_shift) - exponent_bias;
   d555a:	f3c3 54c7 	ubfx	r4, r3, #23, #8
  const uint32_t fraction = (u & fraction_mask);
   d555e:	f3c3 0716 	ubfx	r7, r3, #0, #23
  // Expect ~0x2B1B9D3 for fraction.
  if (u & sign_mask) {
    *current = '-';
   d5562:	bfbb      	ittet	lt
   d5564:	232d      	movlt	r3, #45	; 0x2d
   d5566:	f88d 3010 	strblt.w	r3, [sp, #16]
// Populates the provided buffer with ASCII representation of the float number.
// Avoids the use of any floating point instructions (since these aren't
// supported on many microcontrollers) and as a consequence prints values with
// power-of-two exponents.
char* FastFloatToBufferLeft(float f, char* buffer) {
  char* current = buffer;
   d556a:	ab04      	addge	r3, sp, #16
      ((u & exponent_mask) >> exponent_shift) - exponent_bias;
  const uint32_t fraction = (u & fraction_mask);
  // Expect ~0x2B1B9D3 for fraction.
  if (u & sign_mask) {
    *current = '-';
    current += 1;
   d556c:	f10d 0311 	addlt.w	r3, sp, #17
  const int32_t exponent_shift = 23;
  const int32_t exponent_bias = 127;
  const uint32_t fraction_mask = 0x007fffff;
  const uint32_t u = *reinterpret_cast<uint32_t*>(&f);
  const int32_t exponent =
      ((u & exponent_mask) >> exponent_shift) - exponent_bias;
   d5570:	3c7f      	subs	r4, #127	; 0x7f
  // Expect ~0x2B1B9D3 for fraction.
  if (u & sign_mask) {
    *current = '-';
    current += 1;
  }
  *current = 0;
   d5572:	2200      	movs	r2, #0
  // These are special cases for infinities and not-a-numbers.
  if (exponent == 128) {
   d5574:	2c80      	cmp	r4, #128	; 0x80
  // Expect ~0x2B1B9D3 for fraction.
  if (u & sign_mask) {
    *current = '-';
    current += 1;
  }
  *current = 0;
   d5576:	701a      	strb	r2, [r3, #0]
  // These are special cases for infinities and not-a-numbers.
  if (exponent == 128) {
   d5578:	d108      	bne.n	d558c <DebugLogFloat+0x3c>
   d557a:	f10d 013f 	add.w	r1, sp, #63	; 0x3f
    if (fraction == 0) {
   d557e:	b90f      	cbnz	r7, d5584 <DebugLogFloat+0x34>
      current = StrCatStr(current, (current_end - current), "Inf");
   d5580:	4a2a      	ldr	r2, [pc, #168]	; (d562c <DebugLogFloat+0xdc>)
   d5582:	e000      	b.n	d5586 <DebugLogFloat+0x36>
      return current;
    } else {
      current = StrCatStr(current, (current_end - current), "NaN");
   d5584:	4a2a      	ldr	r2, [pc, #168]	; (d5630 <DebugLogFloat+0xe0>)
   d5586:	1ac9      	subs	r1, r1, r3
   d5588:	4618      	mov	r0, r3
   d558a:	e047      	b.n	d561c <DebugLogFloat+0xcc>
  // We can approximate this using multiply-adds and right-shifts using the
  // values in this array. The 1. portion of the number string is printed out
  // in a fixed way before the fraction, below.
  const int32_t scale_shifts_size = 13;
  const int8_t scale_shifts[13] = {3,  4,  8,  11, 13, 14, 17,
                                   18, 19, 20, 21, 22, 23};
   d558c:	4a29      	ldr	r2, [pc, #164]	; (d5634 <DebugLogFloat+0xe4>)
   d558e:	466d      	mov	r5, sp
   d5590:	f102 0c08 	add.w	ip, r2, #8
   d5594:	46ee      	mov	lr, sp
   d5596:	6810      	ldr	r0, [r2, #0]
   d5598:	6851      	ldr	r1, [r2, #4]
   d559a:	462e      	mov	r6, r5
   d559c:	c603      	stmia	r6!, {r0, r1}
   d559e:	3208      	adds	r2, #8
   d55a0:	4562      	cmp	r2, ip
   d55a2:	4635      	mov	r5, r6
   d55a4:	d1f7      	bne.n	d5596 <DebugLogFloat+0x46>
   d55a6:	6810      	ldr	r0, [r2, #0]
   d55a8:	7912      	ldrb	r2, [r2, #4]
   d55aa:	6030      	str	r0, [r6, #0]
   d55ac:	7132      	strb	r2, [r6, #4]
  uint32_t scaled_fraction = fraction;
   d55ae:	4638      	mov	r0, r7
  for (int i = 0; i < scale_shifts_size; ++i) {
   d55b0:	2200      	movs	r2, #0
    scaled_fraction += (fraction >> scale_shifts[i]);
   d55b2:	f91e 1002 	ldrsb.w	r1, [lr, r2]
  // in a fixed way before the fraction, below.
  const int32_t scale_shifts_size = 13;
  const int8_t scale_shifts[13] = {3,  4,  8,  11, 13, 14, 17,
                                   18, 19, 20, 21, 22, 23};
  uint32_t scaled_fraction = fraction;
  for (int i = 0; i < scale_shifts_size; ++i) {
   d55b6:	3201      	adds	r2, #1
    scaled_fraction += (fraction >> scale_shifts[i]);
   d55b8:	fa27 f101 	lsr.w	r1, r7, r1
  // in a fixed way before the fraction, below.
  const int32_t scale_shifts_size = 13;
  const int8_t scale_shifts[13] = {3,  4,  8,  11, 13, 14, 17,
                                   18, 19, 20, 21, 22, 23};
  uint32_t scaled_fraction = fraction;
  for (int i = 0; i < scale_shifts_size; ++i) {
   d55bc:	2a0d      	cmp	r2, #13
    scaled_fraction += (fraction >> scale_shifts[i]);
   d55be:	4408      	add	r0, r1
  // in a fixed way before the fraction, below.
  const int32_t scale_shifts_size = 13;
  const int8_t scale_shifts[13] = {3,  4,  8,  11, 13, 14, 17,
                                   18, 19, 20, 21, 22, 23};
  uint32_t scaled_fraction = fraction;
  for (int i = 0; i < scale_shifts_size; ++i) {
   d55c0:	d1f7      	bne.n	d55b2 <DebugLogFloat+0x62>
    scaled_fraction += (fraction >> scale_shifts[i]);
  }
  *current = '1';
   d55c2:	2231      	movs	r2, #49	; 0x31
   d55c4:	701a      	strb	r2, [r3, #0]
  current += 1;
  *current = '.';
   d55c6:	222e      	movs	r2, #46	; 0x2e
  current += 1;
   d55c8:	1c9e      	adds	r6, r3, #2
  for (int i = 0; i < scale_shifts_size; ++i) {
    scaled_fraction += (fraction >> scale_shifts[i]);
  }
  *current = '1';
  current += 1;
  *current = '.';
   d55ca:	705a      	strb	r2, [r3, #1]
  current += 1;
  *current = 0;
  current = StrCatUInt32(current, (current_end - current), scaled_fraction, 10);
   d55cc:	f10d 053f 	add.w	r5, sp, #63	; 0x3f
  }
  *current = '1';
  current += 1;
  *current = '.';
  current += 1;
  *current = 0;
   d55d0:	2200      	movs	r2, #0
   d55d2:	709a      	strb	r2, [r3, #2]
  current = StrCatUInt32(current, (current_end - current), scaled_fraction, 10);
   d55d4:	1baf      	subs	r7, r5, r6
}

// Converts a number to a string and appends it to another.
char* StrCatUInt32(char* main, int main_max_length, uint32_t number, int base) {
  char number_string[kFastToBufferSize];
  FastUInt32ToBufferLeft(number, number_string, base);
   d55d6:	220a      	movs	r2, #10
   d55d8:	a910      	add	r1, sp, #64	; 0x40
   d55da:	f7ff ff85 	bl	d54e8 <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci>
  return StrCatStr(main, main_max_length, number_string);
   d55de:	aa10      	add	r2, sp, #64	; 0x40
   d55e0:	4639      	mov	r1, r7
   d55e2:	4630      	mov	r0, r6
   d55e4:	f7ff ff6a 	bl	d54bc <_ZN12_GLOBAL__N_19StrCatStrEPciPKc>
  current += 1;
  *current = '.';
  current += 1;
  *current = 0;
  current = StrCatUInt32(current, (current_end - current), scaled_fraction, 10);
  current = StrCatStr(current, (current_end - current), "*2^");
   d55e8:	4a13      	ldr	r2, [pc, #76]	; (d5638 <DebugLogFloat+0xe8>)
   d55ea:	1a29      	subs	r1, r5, r0
   d55ec:	f7ff ff66 	bl	d54bc <_ZN12_GLOBAL__N_19StrCatStrEPciPKc>
}

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
   d55f0:	2c00      	cmp	r4, #0
    *buffer++ = '-';
    u = -u;
   d55f2:	bfb8      	it	lt
   d55f4:	4264      	neglt	r4, r4
  current += 1;
  *current = '.';
  current += 1;
  *current = 0;
  current = StrCatUInt32(current, (current_end - current), scaled_fraction, 10);
  current = StrCatStr(current, (current_end - current), "*2^");
   d55f6:	4606      	mov	r6, r0
  current = StrCatInt32(current, (current_end - current), exponent);
   d55f8:	eba5 0500 	sub.w	r5, r5, r0

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
    *buffer++ = '-';
   d55fc:	bfba      	itte	lt
   d55fe:	232d      	movlt	r3, #45	; 0x2d
   d5600:	f10d 0141 	addlt.w	r1, sp, #65	; 0x41
}

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
   d5604:	a910      	addge	r1, sp, #64	; 0x40
    *buffer++ = '-';
    u = -u;
  }
  return FastUInt32ToBufferLeft(u, buffer, 10);
   d5606:	f04f 020a 	mov.w	r2, #10
   d560a:	4620      	mov	r0, r4

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
    *buffer++ = '-';
   d560c:	bfb8      	it	lt
   d560e:	f88d 3040 	strblt.w	r3, [sp, #64]	; 0x40
    u = -u;
  }
  return FastUInt32ToBufferLeft(u, buffer, 10);
   d5612:	f7ff ff69 	bl	d54e8 <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci>

// Converts a number to a string and appends it to another.
char* StrCatInt32(char* main, int main_max_length, int32_t number) {
  char number_string[kFastToBufferSize];
  FastInt32ToBufferLeft(number, number_string);
  return StrCatStr(main, main_max_length, number_string);
   d5616:	aa10      	add	r2, sp, #64	; 0x40
   d5618:	4629      	mov	r1, r5
   d561a:	4630      	mov	r0, r6
   d561c:	f7ff ff4e 	bl	d54bc <_ZN12_GLOBAL__N_19StrCatStrEPciPKc>
}

extern "C" void DebugLogFloat(float i) {
  char number_string[kFastToBufferSize];
  FastFloatToBufferLeft(i, number_string);
  DebugLog(number_string);
   d5620:	a804      	add	r0, sp, #16
   d5622:	f000 fef3 	bl	d640c <DebugLog>
}
   d5626:	b01d      	add	sp, #116	; 0x74
   d5628:	bdf0      	pop	{r4, r5, r6, r7, pc}
   d562a:	bf00      	nop
   d562c:	000e8e18 	.word	0x000e8e18
   d5630:	000e8e1c 	.word	0x000e8e1c
   d5634:	000e8e0b 	.word	0x000e8e0b
   d5638:	000e8e20 	.word	0x000e8e20

000d563c <_ZN6tflite14AlignPointerUpEPhj>:

uint8_t* AlignPointerUp(uint8_t* data, size_t alignment) {
  size_t data_as_size_t = reinterpret_cast<size_t>(data);
  uint8_t* aligned_result = reinterpret_cast<uint8_t*>(
      ((data_as_size_t + (alignment - 1)) / alignment) * alignment);
  return aligned_result;
   d563c:	1e4b      	subs	r3, r1, #1
   d563e:	4418      	add	r0, r3
   d5640:	fbb0 f0f1 	udiv	r0, r0, r1
}
   d5644:	4348      	muls	r0, r1
   d5646:	4770      	bx	lr

000d5648 <_ZN6tflite16AlignPointerDownEPhj>:

uint8_t* AlignPointerDown(uint8_t* data, size_t alignment) {
  size_t data_as_size_t = reinterpret_cast<size_t>(data);
  uint8_t* aligned_result =
      reinterpret_cast<uint8_t*>((data_as_size_t / alignment) * alignment);
  return aligned_result;
   d5648:	fbb0 f0f1 	udiv	r0, r0, r1
}
   d564c:	4348      	muls	r0, r1
   d564e:	4770      	bx	lr

000d5650 <_ZN6tflite11AlignSizeUpEjj>:

size_t AlignSizeUp(size_t size, size_t alignment) {
  size_t aligned_size = (((size + (alignment - 1)) / alignment) * alignment);
  return aligned_size;
   d5650:	3801      	subs	r0, #1
   d5652:	4408      	add	r0, r1
   d5654:	fbb0 f0f1 	udiv	r0, r0, r1
}
   d5658:	4348      	muls	r0, r1
   d565a:	4770      	bx	lr

000d565c <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE>:

TfLiteStatus TfLiteTypeSizeOf(TfLiteType type, size_t* size,
                              ErrorReporter* reporter) {
   d565c:	b538      	push	{r3, r4, r5, lr}
  switch (type) {
   d565e:	1e43      	subs	r3, r0, #1
  size_t aligned_size = (((size + (alignment - 1)) / alignment) * alignment);
  return aligned_size;
}

TfLiteStatus TfLiteTypeSizeOf(TfLiteType type, size_t* size,
                              ErrorReporter* reporter) {
   d5660:	4604      	mov	r4, r0
   d5662:	4615      	mov	r5, r2
  switch (type) {
   d5664:	2b08      	cmp	r3, #8
   d5666:	d810      	bhi.n	d568a <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE+0x2e>
   d5668:	e8df f003 	tbb	[pc, r3]
   d566c:	0d0b0909 	.word	0x0d0b0909
   d5670:	0d050b0f 	.word	0x0d050b0f
   d5674:	0b          	.byte	0x0b
   d5675:	00          	.byte	0x00
    case kTfLiteFloat32:
      *size = sizeof(float);
      break;
    case kTfLiteInt16:
      *size = sizeof(int16_t);
   d5676:	2302      	movs	r3, #2
   d5678:	600b      	str	r3, [r1, #0]
    default:
      reporter->Report("Type %s (%d) not is not supported",
                       TfLiteTypeGetName(type), type);
      return kTfLiteError;
  }
  return kTfLiteOk;
   d567a:	2000      	movs	r0, #0
    case kTfLiteFloat32:
      *size = sizeof(float);
      break;
    case kTfLiteInt16:
      *size = sizeof(int16_t);
      break;
   d567c:	bd38      	pop	{r3, r4, r5, pc}
    case kTfLiteInt32:
      *size = sizeof(int32_t);
   d567e:	2304      	movs	r3, #4
   d5680:	e7fa      	b.n	d5678 <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE+0x1c>
      break;
    case kTfLiteInt64:
      *size = sizeof(int64_t);
      break;
    case kTfLiteBool:
      *size = sizeof(bool);
   d5682:	2301      	movs	r3, #1
   d5684:	e7f8      	b.n	d5678 <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE+0x1c>
      break;
    case kTfLiteComplex64:
      *size = sizeof(float) * 2;
   d5686:	2308      	movs	r3, #8
   d5688:	e7f6      	b.n	d5678 <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE+0x1c>
      break;
    default:
      reporter->Report("Type %s (%d) not is not supported",
   d568a:	f7fe fd45 	bl	d4118 <TfLiteTypeGetName>
                       TfLiteTypeGetName(type), type);
   d568e:	4623      	mov	r3, r4
   d5690:	4602      	mov	r2, r0
   d5692:	4903      	ldr	r1, [pc, #12]	; (d56a0 <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE+0x44>)
   d5694:	4628      	mov	r0, r5
   d5696:	f7fe fe97 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
   d569a:	2001      	movs	r0, #1
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   d569c:	bd38      	pop	{r3, r4, r5, pc}
   d569e:	bf00      	nop
   d56a0:	000e8e24 	.word	0x000e8e24

000d56a4 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE>:

TfLiteStatus BytesRequiredForTensor(const tflite::Tensor& flatbuffer_tensor,
                                    size_t* bytes, size_t* type_size,
                                    ErrorReporter* error_reporter) {
   d56a4:	e92d 41f3 	stmdb	sp!, {r0, r1, r4, r5, r6, r7, r8, lr}
   d56a8:	460d      	mov	r5, r1
// "tables" use an offset table (possibly shared) that allows fields to be
// omitted and added at will, but uses an extra indirection to read.
class Table {
 public:
  const uint8_t *GetVTable() const {
    return data_ - ReadScalar<soffset_t>(data_);
   d56aa:	6801      	ldr	r1, [r0, #0]
   d56ac:	1a41      	subs	r1, r0, r1
   d56ae:	461f      	mov	r7, r3
   d56b0:	f8b1 e000 	ldrh.w	lr, [r1]
   d56b4:	4616      	mov	r6, r2
  int element_count = 1;
  for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
   d56b6:	2300      	movs	r3, #0
}

TfLiteStatus BytesRequiredForTensor(const tflite::Tensor& flatbuffer_tensor,
                                    size_t* bytes, size_t* type_size,
                                    ErrorReporter* error_reporter) {
  int element_count = 1;
   d56b8:	2401      	movs	r4, #1
    auto vtable = GetVTable();
    // The first element is the size of the vtable (fields + type id + itself).
    auto vtsize = ReadScalar<voffset_t>(vtable);
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
   d56ba:	f1be 0f04 	cmp.w	lr, #4
   d56be:	bf8c      	ite	hi
   d56c0:	888a      	ldrhhi	r2, [r1, #4]
   d56c2:	2200      	movls	r2, #0
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
  }

  template<typename P> P GetPointer(voffset_t field) {
    auto field_offset = GetOptionalFieldOffset(field);
    auto p = data_ + field_offset;
   d56c4:	eb00 0802 	add.w	r8, r0, r2
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
   d56c8:	b362      	cbz	r2, d5724 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x80>
   d56ca:	f850 c002 	ldr.w	ip, [r0, r2]
   d56ce:	eb08 020c 	add.w	r2, r8, ip
  for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
   d56d2:	f858 c00c 	ldr.w	ip, [r8, ip]
   d56d6:	4563      	cmp	r3, ip
   d56d8:	d205      	bcs.n	d56e6 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x42>
template<typename T> struct IndirectHelper {
  typedef T return_type;
  typedef T mutable_return_type;
  static const size_t element_stride = sizeof(T);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    return EndianScalar((reinterpret_cast<const T *>(p))[i]);
   d56da:	eb02 0283 	add.w	r2, r2, r3, lsl #2
   d56de:	3301      	adds	r3, #1
    element_count *= flatbuffer_tensor.shape()->Get(n);
   d56e0:	6852      	ldr	r2, [r2, #4]
   d56e2:	4354      	muls	r4, r2

TfLiteStatus BytesRequiredForTensor(const tflite::Tensor& flatbuffer_tensor,
                                    size_t* bytes, size_t* type_size,
                                    ErrorReporter* error_reporter) {
  int element_count = 1;
  for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
   d56e4:	e7e9      	b.n	d56ba <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x16>
    auto vtable = GetVTable();
    // The first element is the size of the vtable (fields + type id + itself).
    auto vtsize = ReadScalar<voffset_t>(vtable);
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
   d56e6:	f1be 0f06 	cmp.w	lr, #6
   d56ea:	d903      	bls.n	d56f4 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x50>
   d56ec:	88ca      	ldrh	r2, [r1, #6]
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d56ee:	b11a      	cbz	r2, d56f8 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x54>
   d56f0:	5680      	ldrsb	r0, [r0, r2]
   d56f2:	e002      	b.n	d56fa <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x56>
   d56f4:	2000      	movs	r0, #0
   d56f6:	e000      	b.n	d56fa <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x56>
   d56f8:	4610      	mov	r0, r2
    element_count *= flatbuffer_tensor.shape()->Get(n);
  }

  TfLiteType tf_lite_type;
  TF_LITE_ENSURE_STATUS(ConvertTensorType(flatbuffer_tensor.type(),
   d56fa:	463a      	mov	r2, r7
   d56fc:	f10d 0107 	add.w	r1, sp, #7
   d5700:	b2c0      	uxtb	r0, r0
   d5702:	f7fe febb 	bl	d447c <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
   d5706:	b108      	cbz	r0, d570c <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x68>
   d5708:	2001      	movs	r0, #1
   d570a:	e00d      	b.n	d5728 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x84>
                                          &tf_lite_type, error_reporter));
  TF_LITE_ENSURE_STATUS(
   d570c:	463a      	mov	r2, r7
   d570e:	4631      	mov	r1, r6
   d5710:	f89d 0007 	ldrb.w	r0, [sp, #7]
   d5714:	f7ff ffa2 	bl	d565c <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE>
   d5718:	2800      	cmp	r0, #0
   d571a:	d1f5      	bne.n	d5708 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x64>
      TfLiteTypeSizeOf(tf_lite_type, type_size, error_reporter));
  *bytes = element_count * (*type_size);
   d571c:	6833      	ldr	r3, [r6, #0]
   d571e:	435c      	muls	r4, r3
   d5720:	602c      	str	r4, [r5, #0]
  return kTfLiteOk;
   d5722:	e001      	b.n	d5728 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x84>
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d5724:	6813      	ldr	r3, [r2, #0]
   d5726:	deff      	udf	#255	; 0xff
}
   d5728:	b002      	add	sp, #8
   d572a:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}

000d572e <_ZNK6tflite6Tensor11is_variableEv>:
    return GetPointer<const flatbuffers::String *>(VT_NAME);
  }
  const QuantizationParameters *quantization() const {
    return GetPointer<const QuantizationParameters *>(VT_QUANTIZATION);
  }
  bool is_variable() const {
   d572e:	b510      	push	{r4, lr}
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
   d5730:	210e      	movs	r1, #14
   d5732:	4604      	mov	r4, r0
   d5734:	f7fe fe9a 	bl	d446c <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d5738:	b100      	cbz	r0, d573c <_ZNK6tflite6Tensor11is_variableEv+0xe>
   d573a:	5c20      	ldrb	r0, [r4, r0]
    return GetField<uint8_t>(VT_IS_VARIABLE, 0) != 0;
  }
   d573c:	3000      	adds	r0, #0
   d573e:	bf18      	it	ne
   d5740:	2001      	movne	r0, #1
   d5742:	bd10      	pop	{r4, pc}

000d5744 <_ZNK11flatbuffers6VectorIfE3GetEm>:
  uoffset_t Length() const { return size(); }

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
   d5744:	b508      	push	{r3, lr}
    FLATBUFFERS_ASSERT(i < size());
   d5746:	6803      	ldr	r3, [r0, #0]
   d5748:	4299      	cmp	r1, r3
   d574a:	d305      	bcc.n	d5758 <_ZNK11flatbuffers6VectorIfE3GetEm+0x14>
   d574c:	4b05      	ldr	r3, [pc, #20]	; (d5764 <_ZNK11flatbuffers6VectorIfE3GetEm+0x20>)
   d574e:	4a06      	ldr	r2, [pc, #24]	; (d5768 <_ZNK11flatbuffers6VectorIfE3GetEm+0x24>)
   d5750:	4806      	ldr	r0, [pc, #24]	; (d576c <_ZNK11flatbuffers6VectorIfE3GetEm+0x28>)
   d5752:	21ed      	movs	r1, #237	; 0xed
   d5754:	f00e fdfa 	bl	e434c <__assert_func>
template<typename T> struct IndirectHelper {
  typedef T return_type;
  typedef T mutable_return_type;
  static const size_t element_stride = sizeof(T);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    return EndianScalar((reinterpret_cast<const T *>(p))[i]);
   d5758:	eb00 0081 	add.w	r0, r0, r1, lsl #2
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
    return IndirectHelper<T>::Read(Data(), i);
  }
   d575c:	ed90 0a01 	vldr	s0, [r0, #4]
   d5760:	bd08      	pop	{r3, pc}
   d5762:	bf00      	nop
   d5764:	000e8541 	.word	0x000e8541
   d5768:	000e9210 	.word	0x000e9210
   d576c:	000e854c 	.word	0x000e854c

000d5770 <_ZNK11flatbuffers6VectorIlE3GetEm>:
  uoffset_t Length() const { return size(); }

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
   d5770:	b508      	push	{r3, lr}
    FLATBUFFERS_ASSERT(i < size());
   d5772:	6803      	ldr	r3, [r0, #0]
   d5774:	4299      	cmp	r1, r3
   d5776:	d305      	bcc.n	d5784 <_ZNK11flatbuffers6VectorIlE3GetEm+0x14>
   d5778:	4b04      	ldr	r3, [pc, #16]	; (d578c <_ZNK11flatbuffers6VectorIlE3GetEm+0x1c>)
   d577a:	4a05      	ldr	r2, [pc, #20]	; (d5790 <_ZNK11flatbuffers6VectorIlE3GetEm+0x20>)
   d577c:	4805      	ldr	r0, [pc, #20]	; (d5794 <_ZNK11flatbuffers6VectorIlE3GetEm+0x24>)
   d577e:	21ed      	movs	r1, #237	; 0xed
   d5780:	f00e fde4 	bl	e434c <__assert_func>
template<typename T> struct IndirectHelper {
  typedef T return_type;
  typedef T mutable_return_type;
  static const size_t element_stride = sizeof(T);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    return EndianScalar((reinterpret_cast<const T *>(p))[i]);
   d5784:	eb00 0081 	add.w	r0, r0, r1, lsl #2
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
    return IndirectHelper<T>::Read(Data(), i);
  }
   d5788:	6840      	ldr	r0, [r0, #4]
   d578a:	bd08      	pop	{r3, pc}
   d578c:	000e8541 	.word	0x000e8541
   d5790:	000e8e46 	.word	0x000e8e46
   d5794:	000e854c 	.word	0x000e854c

000d5798 <_ZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEm>:
  uoffset_t Length() const { return size(); }

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
   d5798:	b508      	push	{r3, lr}
    FLATBUFFERS_ASSERT(i < size());
   d579a:	6803      	ldr	r3, [r0, #0]
   d579c:	4299      	cmp	r1, r3
   d579e:	d305      	bcc.n	d57ac <_ZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEm+0x14>
   d57a0:	4b06      	ldr	r3, [pc, #24]	; (d57bc <_ZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEm+0x24>)
   d57a2:	4a07      	ldr	r2, [pc, #28]	; (d57c0 <_ZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEm+0x28>)
   d57a4:	4807      	ldr	r0, [pc, #28]	; (d57c4 <_ZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEm+0x2c>)
   d57a6:	21ed      	movs	r1, #237	; 0xed
   d57a8:	f00e fdd0 	bl	e434c <__assert_func>
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d57ac:	3004      	adds	r0, #4
template<typename T> struct IndirectHelper<Offset<T>> {
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
   d57ae:	eb00 0281 	add.w	r2, r0, r1, lsl #2
  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
    return IndirectHelper<T>::Read(Data(), i);
   d57b2:	f850 0021 	ldr.w	r0, [r0, r1, lsl #2]
  }
   d57b6:	4410      	add	r0, r2
   d57b8:	bd08      	pop	{r3, pc}
   d57ba:	bf00      	nop
   d57bc:	000e8541 	.word	0x000e8541
   d57c0:	000e900e 	.word	0x000e900e
   d57c4:	000e854c 	.word	0x000e854c

000d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>:
  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
  }

  template<typename P> P GetPointer(voffset_t field) {
   d57c8:	b510      	push	{r4, lr}
   d57ca:	4604      	mov	r4, r0
    auto field_offset = GetOptionalFieldOffset(field);
   d57cc:	f7fe fe4e 	bl	d446c <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    auto p = data_ + field_offset;
   d57d0:	1822      	adds	r2, r4, r0
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
   d57d2:	b108      	cbz	r0, d57d8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t+0x10>
   d57d4:	5823      	ldr	r3, [r4, r0]
   d57d6:	18d0      	adds	r0, r2, r3
  }
   d57d8:	bd10      	pop	{r4, pc}
	...

000d57dc <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE>:
// requirement for SIMD extensions.
constexpr int kBufferAlignment = 16;

}  // namespace

MicroAllocator::MicroAllocator(TfLiteContext* context, const Model* model,
   d57dc:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
   d57e0:	9e07      	ldr	r6, [sp, #28]
// though we have enough information about lifetimes of the tensors to do so.
// This makes it pretty wasteful, so we should use a more intelligent method.
class SimpleMemoryAllocator {
 public:
  SimpleMemoryAllocator(uint8_t* buffer, size_t buffer_size)
      : data_size_(0), data_size_max_(buffer_size), data_(buffer) {}
   d57e2:	60c3      	str	r3, [r0, #12]
   d57e4:	460f      	mov	r7, r1
   d57e6:	2500      	movs	r5, #0
   d57e8:	9906      	ldr	r1, [sp, #24]
   d57ea:	6081      	str	r1, [r0, #8]
    : model_(model),
      memory_allocator_(tensor_arena, arena_size),
      error_reporter_(error_reporter),
      context_(context),
      arena_(tensor_arena),
      arena_size_(arena_size) {
   d57ec:	6183      	str	r3, [r0, #24]
   d57ee:	61c1      	str	r1, [r0, #28]
   d57f0:	6002      	str	r2, [r0, #0]
   d57f2:	6045      	str	r5, [r0, #4]
   d57f4:	6106      	str	r6, [r0, #16]
   d57f6:	6147      	str	r7, [r0, #20]
// requirement for SIMD extensions.
constexpr int kBufferAlignment = 16;

}  // namespace

MicroAllocator::MicroAllocator(TfLiteContext* context, const Model* model,
   d57f8:	4604      	mov	r4, r0
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d57fa:	2108      	movs	r1, #8
   d57fc:	4610      	mov	r0, r2
   d57fe:	f7ff ffe3 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
      error_reporter_(error_reporter),
      context_(context),
      arena_(tensor_arena),
      arena_size_(arena_size) {
  auto* subgraphs = model->subgraphs();
  if (subgraphs->size() != 1) {
   d5802:	6803      	ldr	r3, [r0, #0]
   d5804:	2b01      	cmp	r3, #1
   d5806:	d004      	beq.n	d5812 <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE+0x36>
    error_reporter->Report("Only 1 subgraph is currently supported.\n");
   d5808:	4917      	ldr	r1, [pc, #92]	; (d5868 <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE+0x8c>)
   d580a:	4630      	mov	r0, r6
   d580c:	f7fe fddc 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return;
   d5810:	e026      	b.n	d5860 <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE+0x84>
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
   d5812:	6843      	ldr	r3, [r0, #4]
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d5814:	1d06      	adds	r6, r0, #4
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
   d5816:	441e      	add	r6, r3
  }
  subgraph_ = (*subgraphs)[0];
   d5818:	6226      	str	r6, [r4, #32]
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d581a:	2104      	movs	r1, #4
   d581c:	4630      	mov	r0, r6
   d581e:	f7ff ffd3 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
   d5822:	210a      	movs	r1, #10
   d5824:	4680      	mov	r8, r0
  tensors_ = subgraph_->tensors();
   d5826:	62a0      	str	r0, [r4, #40]	; 0x28
   d5828:	4630      	mov	r0, r6
   d582a:	f7ff ffcd 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
  operators_ = subgraph_->operators();
   d582e:	6260      	str	r0, [r4, #36]	; 0x24
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d5830:	f8d8 3000 	ldr.w	r3, [r8]

  context_->tensors_size = tensors_->size();
   d5834:	603b      	str	r3, [r7, #0]
  context_->tensors =
      reinterpret_cast<TfLiteTensor*>(memory_allocator_.AllocateFromTail(
          sizeof(TfLiteTensor) * context_->tensors_size, 4));
   d5836:	6967      	ldr	r7, [r4, #20]
  tensors_ = subgraph_->tensors();
  operators_ = subgraph_->operators();

  context_->tensors_size = tensors_->size();
  context_->tensors =
      reinterpret_cast<TfLiteTensor*>(memory_allocator_.AllocateFromTail(
   d5838:	6839      	ldr	r1, [r7, #0]
   d583a:	2204      	movs	r2, #4
   d583c:	2638      	movs	r6, #56	; 0x38
   d583e:	4371      	muls	r1, r6
   d5840:	18a0      	adds	r0, r4, r2
   d5842:	f000 fdce 	bl	d63e2 <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>
          sizeof(TfLiteTensor) * context_->tensors_size, 4));

  // Null all inputs so we can later perform a null check to avoid re-allocating
  // registered pre-allocated inputs.
  for (size_t i = 0; i < context_->tensors_size; ++i) {
   d5846:	462b      	mov	r3, r5
  operators_ = subgraph_->operators();

  context_->tensors_size = tensors_->size();
  context_->tensors =
      reinterpret_cast<TfLiteTensor*>(memory_allocator_.AllocateFromTail(
          sizeof(TfLiteTensor) * context_->tensors_size, 4));
   d5848:	60b8      	str	r0, [r7, #8]

  // Null all inputs so we can later perform a null check to avoid re-allocating
  // registered pre-allocated inputs.
  for (size_t i = 0; i < context_->tensors_size; ++i) {
    context_->tensors[i].data.raw = nullptr;
   d584a:	4629      	mov	r1, r5
      reinterpret_cast<TfLiteTensor*>(memory_allocator_.AllocateFromTail(
          sizeof(TfLiteTensor) * context_->tensors_size, 4));

  // Null all inputs so we can later perform a null check to avoid re-allocating
  // registered pre-allocated inputs.
  for (size_t i = 0; i < context_->tensors_size; ++i) {
   d584c:	6962      	ldr	r2, [r4, #20]
   d584e:	6810      	ldr	r0, [r2, #0]
   d5850:	4283      	cmp	r3, r0
   d5852:	d205      	bcs.n	d5860 <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE+0x84>
    context_->tensors[i].data.raw = nullptr;
   d5854:	6892      	ldr	r2, [r2, #8]
   d5856:	fb06 2203 	mla	r2, r6, r3, r2
      reinterpret_cast<TfLiteTensor*>(memory_allocator_.AllocateFromTail(
          sizeof(TfLiteTensor) * context_->tensors_size, 4));

  // Null all inputs so we can later perform a null check to avoid re-allocating
  // registered pre-allocated inputs.
  for (size_t i = 0; i < context_->tensors_size; ++i) {
   d585a:	3301      	adds	r3, #1
    context_->tensors[i].data.raw = nullptr;
   d585c:	6051      	str	r1, [r2, #4]
      reinterpret_cast<TfLiteTensor*>(memory_allocator_.AllocateFromTail(
          sizeof(TfLiteTensor) * context_->tensors_size, 4));

  // Null all inputs so we can later perform a null check to avoid re-allocating
  // registered pre-allocated inputs.
  for (size_t i = 0; i < context_->tensors_size; ++i) {
   d585e:	e7f5      	b.n	d584c <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE+0x70>
    context_->tensors[i].data.raw = nullptr;
  }
}
   d5860:	4620      	mov	r0, r4
   d5862:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   d5866:	bf00      	nop
   d5868:	000e9104 	.word	0x000e9104

000d586c <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh>:

TfLiteStatus MicroAllocator::InitializeRuntimeTensor(
    const tflite::Tensor& flatbuffer_tensor,
    const flatbuffers::Vector<flatbuffers::Offset<Buffer>>* buffers,
    ErrorReporter* error_reporter, TfLiteTensor* result,
    uint8_t* preallocated_buffer) {
   d586c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d5870:	460d      	mov	r5, r1
   d5872:	b087      	sub	sp, #28
   d5874:	4607      	mov	r7, r0
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
   d5876:	2106      	movs	r1, #6
   d5878:	4628      	mov	r0, r5
   d587a:	4616      	mov	r6, r2
   d587c:	4698      	mov	r8, r3
   d587e:	9c10      	ldr	r4, [sp, #64]	; 0x40
   d5880:	f8dd 9044 	ldr.w	r9, [sp, #68]	; 0x44
   d5884:	f7fe fdf2 	bl	d446c <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d5888:	b100      	cbz	r0, d588c <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x20>
   d588a:	5628      	ldrsb	r0, [r5, r0]
  // Make sure the serialized type is one we know how to deal with, and convert
  // it from a flatbuffer enum into a constant used by the kernel C API.
  TF_LITE_ENSURE_STATUS(ConvertTensorType(flatbuffer_tensor.type(),
   d588c:	4642      	mov	r2, r8
   d588e:	4621      	mov	r1, r4
   d5890:	b2c0      	uxtb	r0, r0
   d5892:	f7fe fdf3 	bl	d447c <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
   d5896:	4682      	mov	sl, r0
   d5898:	2800      	cmp	r0, #0
   d589a:	f040 80e2 	bne.w	d5a62 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1f6>
                                          &result->type, error_reporter));
  // Make sure we remember if the serialized tensor is designated as a variable.
  result->is_variable = flatbuffer_tensor.is_variable();
   d589e:	4628      	mov	r0, r5
   d58a0:	f7ff ff45 	bl	d572e <_ZNK6tflite6Tensor11is_variableEv>
  // We need to figure out where the actual contents of this tensor are stored
  // in memory. We'll check to see if there's a serialized buffer (pretty much
  // the same as a constant op in TensorFlow) associated with this tensor first,
  // and if there is update the runtime structure to point to its location in
  // memory.
  result->data.raw = nullptr;
   d58a4:	f8c4 a004 	str.w	sl, [r4, #4]
  // Make sure the serialized type is one we know how to deal with, and convert
  // it from a flatbuffer enum into a constant used by the kernel C API.
  TF_LITE_ENSURE_STATUS(ConvertTensorType(flatbuffer_tensor.type(),
                                          &result->type, error_reporter));
  // Make sure we remember if the serialized tensor is designated as a variable.
  result->is_variable = flatbuffer_tensor.is_variable();
   d58a8:	f884 002d 	strb.w	r0, [r4, #45]	; 0x2d
  // in memory. We'll check to see if there's a serialized buffer (pretty much
  // the same as a constant op in TensorFlow) associated with this tensor first,
  // and if there is update the runtime structure to point to its location in
  // memory.
  result->data.raw = nullptr;
  result->bytes = 0;
   d58ac:	f8c4 a018 	str.w	sl, [r4, #24]
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
   d58b0:	2108      	movs	r1, #8
   d58b2:	4628      	mov	r0, r5
   d58b4:	f7fe fdda 	bl	d446c <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d58b8:	b100      	cbz	r0, d58bc <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x50>
   d58ba:	5828      	ldr	r0, [r5, r0]

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
   d58bc:	6833      	ldr	r3, [r6, #0]
   d58be:	4283      	cmp	r3, r0
   d58c0:	d802      	bhi.n	d58c8 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x5c>
   d58c2:	4b74      	ldr	r3, [pc, #464]	; (d5a94 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x228>)
   d58c4:	4a74      	ldr	r2, [pc, #464]	; (d5a98 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x22c>)
   d58c6:	e0ac      	b.n	d5a22 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1b6>
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d58c8:	3604      	adds	r6, #4
template<typename T> struct IndirectHelper<Offset<T>> {
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
   d58ca:	eb06 0380 	add.w	r3, r6, r0, lsl #2
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
   d58ce:	f856 0020 	ldr.w	r0, [r6, r0, lsl #2]
  // First see if there's any buffer information in the serialized tensor.
  if (auto* buffer = (*buffers)[flatbuffer_tensor.buffer()]) {
   d58d2:	1818      	adds	r0, r3, r0
   d58d4:	d009      	beq.n	d58ea <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x7e>
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d58d6:	2104      	movs	r1, #4
   d58d8:	f7ff ff76 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
    // If we've found a buffer, does it have any data?
    if (auto* array = buffer->data()) {
   d58dc:	b128      	cbz	r0, d58ea <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x7e>
      // If it has any data, is the data size larger than zero?
      if (size_t array_size = array->size()) {
   d58de:	6803      	ldr	r3, [r0, #0]
   d58e0:	b11b      	cbz	r3, d58ea <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x7e>
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d58e2:	3004      	adds	r0, #4
        // We've found a buffer with valid data, so update the runtime tensor
        // data structure to point to it.
        result->data.raw =
            const_cast<char*>(reinterpret_cast<const char*>(array->data()));
        // We set the data from a serialized buffer, so record tha.
        result->allocation_type = kTfLiteMmapRo;
   d58e4:	2301      	movs	r3, #1
      // If it has any data, is the data size larger than zero?
      if (size_t array_size = array->size()) {
        // We've found a buffer with valid data, so update the runtime tensor
        // data structure to point to it.
        result->data.raw =
            const_cast<char*>(reinterpret_cast<const char*>(array->data()));
   d58e6:	6060      	str	r0, [r4, #4]
        // We set the data from a serialized buffer, so record tha.
        result->allocation_type = kTfLiteMmapRo;
   d58e8:	7523      	strb	r3, [r4, #20]
    // it less ambiguous.
  }

  // TODO(petewarden): Some of these paths aren't getting enough testing
  // coverage, so we should figure out some tests that exercise them.
  if (!result->data.raw) {
   d58ea:	6863      	ldr	r3, [r4, #4]
   d58ec:	b933      	cbnz	r3, d58fc <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x90>
    // The tensor contents haven't been set from a serialized buffer, so
    // make a note that they will be allocated from memory. The actual
    // allocation won't happen until later.
    result->allocation_type = kTfLiteArenaRw;
   d58ee:	2302      	movs	r3, #2
   d58f0:	7523      	strb	r3, [r4, #20]
    if (preallocated_buffer != nullptr) {
   d58f2:	f1b9 0f00 	cmp.w	r9, #0
   d58f6:	d001      	beq.n	d58fc <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x90>
      // If the client is supplying memory for the contents of the tensor
      // themselves, use it.
      // TODO(petewarden): Should we store the fact this is a client-allocated
      // buffer?
      result->data.raw = reinterpret_cast<char*>(preallocated_buffer);
   d58f8:	f8c4 9004 	str.w	r9, [r4, #4]
    }
  }

  // Figure out what the size in bytes of the buffer is and store it.
  size_t type_size;
  TF_LITE_ENSURE_STATUS(BytesRequiredForTensor(
   d58fc:	4643      	mov	r3, r8
   d58fe:	aa05      	add	r2, sp, #20
   d5900:	f104 0118 	add.w	r1, r4, #24
   d5904:	4628      	mov	r0, r5
   d5906:	f7ff fecd 	bl	d56a4 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE>
   d590a:	4606      	mov	r6, r0
   d590c:	2800      	cmp	r0, #0
   d590e:	f040 80a8 	bne.w	d5a62 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1f6>
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5912:	2104      	movs	r1, #4
   d5914:	4628      	mov	r0, r5
   d5916:	f7ff ff57 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
      flatbuffer_tensor, &result->bytes, &type_size, error_reporter));
  // Copy the shape of the tensor from the serialized data into the runtime
  // form. We have to allocate memory for this.
  result->dims =
      reinterpret_cast<TfLiteIntArray*>(memory_allocator_.AllocateFromTail(
   d591a:	6801      	ldr	r1, [r0, #0]
   d591c:	f107 0b04 	add.w	fp, r7, #4
   d5920:	3101      	adds	r1, #1
   d5922:	2204      	movs	r2, #4
   d5924:	0089      	lsls	r1, r1, #2
   d5926:	4658      	mov	r0, fp
   d5928:	f000 fd5b 	bl	d63e2 <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>
   d592c:	2104      	movs	r1, #4
   d592e:	4607      	mov	r7, r0
          sizeof(int) * (flatbuffer_tensor.shape()->Length() + 1),
          sizeof(int)));
   d5930:	60a0      	str	r0, [r4, #8]
   d5932:	4628      	mov	r0, r5
   d5934:	f7ff ff48 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
  result->dims->size = flatbuffer_tensor.shape()->Length();
   d5938:	6803      	ldr	r3, [r0, #0]
   d593a:	603b      	str	r3, [r7, #0]
  for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
   d593c:	4637      	mov	r7, r6
   d593e:	2104      	movs	r1, #4
   d5940:	4628      	mov	r0, r5
   d5942:	f7ff ff41 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
   d5946:	6803      	ldr	r3, [r0, #0]
   d5948:	42bb      	cmp	r3, r7
   d594a:	d90a      	bls.n	d5962 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0xf6>
    result->dims->data[n] = flatbuffer_tensor.shape()->Get(n);
   d594c:	f8d4 8008 	ldr.w	r8, [r4, #8]
   d5950:	4639      	mov	r1, r7
   d5952:	f7ff ff0d 	bl	d5770 <_ZNK11flatbuffers6VectorIlE3GetEm>
   d5956:	eb08 0887 	add.w	r8, r8, r7, lsl #2
  result->dims =
      reinterpret_cast<TfLiteIntArray*>(memory_allocator_.AllocateFromTail(
          sizeof(int) * (flatbuffer_tensor.shape()->Length() + 1),
          sizeof(int)));
  result->dims->size = flatbuffer_tensor.shape()->Length();
  for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
   d595a:	3701      	adds	r7, #1
    result->dims->data[n] = flatbuffer_tensor.shape()->Get(n);
   d595c:	f8c8 0004 	str.w	r0, [r8, #4]
   d5960:	e7ed      	b.n	d593e <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0xd2>
   d5962:	210c      	movs	r1, #12
   d5964:	4628      	mov	r0, r5
   d5966:	f7ff ff2f 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
  }
  // Copy the quantization information from the serialized data.
  const auto* src_quantization = flatbuffer_tensor.quantization();
  if (src_quantization && src_quantization->scale() &&
      (src_quantization->scale()->size() > 0) &&
      src_quantization->zero_point() &&
   d596a:	4607      	mov	r7, r0
   d596c:	2800      	cmp	r0, #0
   d596e:	d066      	beq.n	d5a3e <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1d2>
   d5970:	2108      	movs	r1, #8
   d5972:	f7ff ff29 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
  for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
    result->dims->data[n] = flatbuffer_tensor.shape()->Get(n);
  }
  // Copy the quantization information from the serialized data.
  const auto* src_quantization = flatbuffer_tensor.quantization();
  if (src_quantization && src_quantization->scale() &&
   d5976:	4680      	mov	r8, r0
   d5978:	2800      	cmp	r0, #0
   d597a:	d060      	beq.n	d5a3e <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1d2>
   d597c:	6803      	ldr	r3, [r0, #0]
   d597e:	2b00      	cmp	r3, #0
   d5980:	d05d      	beq.n	d5a3e <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1d2>
   d5982:	210a      	movs	r1, #10
   d5984:	4638      	mov	r0, r7
   d5986:	f7ff ff1f 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
      (src_quantization->scale()->size() > 0) &&
   d598a:	2800      	cmp	r0, #0
   d598c:	d057      	beq.n	d5a3e <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1d2>
      src_quantization->zero_point() &&
   d598e:	6803      	ldr	r3, [r0, #0]
   d5990:	2b00      	cmp	r3, #0
   d5992:	d054      	beq.n	d5a3e <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1d2>
      (src_quantization->zero_point()->size() > 0)) {
    result->params.scale = src_quantization->scale()->Get(0);
   d5994:	4640      	mov	r0, r8
   d5996:	2100      	movs	r1, #0
   d5998:	f7ff fed4 	bl	d5744 <_ZNK11flatbuffers6VectorIfE3GetEm>
   d599c:	f104 090f 	add.w	r9, r4, #15
   d59a0:	ed84 0a03 	vstr	s0, [r4, #12]
    // This magic handles issues with little-endianness.
    for (unsigned int b = 0; b < sizeof(int64_t); ++b)
   d59a4:	f04f 0800 	mov.w	r8, #0
   d59a8:	210a      	movs	r1, #10
   d59aa:	4638      	mov	r0, r7
   d59ac:	f7ff ff0c 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
      *(reinterpret_cast<char*>(&result->params.zero_point) + b) =
          *(reinterpret_cast<const char*>(
                src_quantization->zero_point()->Data()) +
            b);
   d59b0:	4440      	add	r0, r8
      (src_quantization->scale()->size() > 0) &&
      src_quantization->zero_point() &&
      (src_quantization->zero_point()->size() > 0)) {
    result->params.scale = src_quantization->scale()->Get(0);
    // This magic handles issues with little-endianness.
    for (unsigned int b = 0; b < sizeof(int64_t); ++b)
   d59b2:	f108 0801 	add.w	r8, r8, #1
      *(reinterpret_cast<char*>(&result->params.zero_point) + b) =
          *(reinterpret_cast<const char*>(
                src_quantization->zero_point()->Data()) +
            b);
   d59b6:	7903      	ldrb	r3, [r0, #4]
   d59b8:	f809 3f01 	strb.w	r3, [r9, #1]!
      (src_quantization->scale()->size() > 0) &&
      src_quantization->zero_point() &&
      (src_quantization->zero_point()->size() > 0)) {
    result->params.scale = src_quantization->scale()->Get(0);
    // This magic handles issues with little-endianness.
    for (unsigned int b = 0; b < sizeof(int64_t); ++b)
   d59bc:	f1b8 0f08 	cmp.w	r8, #8
   d59c0:	d1f2      	bne.n	d59a8 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x13c>
   d59c2:	4641      	mov	r1, r8
   d59c4:	4638      	mov	r0, r7
   d59c6:	f7ff feff 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
    // Populate per-channel quantization params.
    int channels = src_quantization->scale()->size();
    TfLiteAffineQuantization* quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            memory_allocator_.AllocateFromTail(sizeof(TfLiteAffineQuantization),
                                               sizeof(int)));
   d59ca:	2204      	movs	r2, #4
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d59cc:	f8d0 8000 	ldr.w	r8, [r0]
   d59d0:	210c      	movs	r1, #12
   d59d2:	4658      	mov	r0, fp
   d59d4:	f000 fd05 	bl	d63e2 <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>
    int* zero_point_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
   d59d8:	ea4f 0388 	mov.w	r3, r8, lsl #2
   d59dc:	f103 0904 	add.w	r9, r3, #4
            channels * sizeof(int) + sizeof(int), sizeof(int)));
   d59e0:	4649      	mov	r1, r9
    // Populate per-channel quantization params.
    int channels = src_quantization->scale()->size();
    TfLiteAffineQuantization* quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            memory_allocator_.AllocateFromTail(sizeof(TfLiteAffineQuantization),
                                               sizeof(int)));
   d59e2:	9001      	str	r0, [sp, #4]
    int* zero_point_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(int) + sizeof(int), sizeof(int)));
   d59e4:	2204      	movs	r2, #4
   d59e6:	4658      	mov	r0, fp
   d59e8:	f000 fcfb 	bl	d63e2 <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>
    int* scale_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(float) + sizeof(int), sizeof(int)));
   d59ec:	4649      	mov	r1, r9
        reinterpret_cast<TfLiteAffineQuantization*>(
            memory_allocator_.AllocateFromTail(sizeof(TfLiteAffineQuantization),
                                               sizeof(int)));
    int* zero_point_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(int) + sizeof(int), sizeof(int)));
   d59ee:	4682      	mov	sl, r0
    int* scale_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(float) + sizeof(int), sizeof(int)));
   d59f0:	2204      	movs	r2, #4
   d59f2:	4658      	mov	r0, fp
   d59f4:	f000 fcf5 	bl	d63e2 <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>
    zero_point_array[0] = channels;
    scale_array[0] = channels;
   d59f8:	4681      	mov	r9, r0
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(int) + sizeof(int), sizeof(int)));
    int* scale_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(float) + sizeof(int), sizeof(int)));
    zero_point_array[0] = channels;
   d59fa:	f8ca 8000 	str.w	r8, [sl]
    scale_array[0] = channels;
   d59fe:	f849 8b04 	str.w	r8, [r9], #4
    int* zero_point_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(int) + sizeof(int), sizeof(int)));
    int* scale_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(float) + sizeof(int), sizeof(int)));
   d5a02:	9002      	str	r0, [sp, #8]
   d5a04:	f8cd a00c 	str.w	sl, [sp, #12]
    zero_point_array[0] = channels;
    scale_array[0] = channels;
    int* zero_point_data = &zero_point_array[1];
    float* scale_data = reinterpret_cast<float*>(&scale_array[1]);
    for (int i = 0; i < channels; i++) {
   d5a08:	f04f 0b00 	mov.w	fp, #0
   d5a0c:	45d8      	cmp	r8, fp
   d5a0e:	dd0c      	ble.n	d5a2a <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1be>
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5a10:	210a      	movs	r1, #10
   d5a12:	4638      	mov	r0, r7
   d5a14:	f7ff fed8 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
   d5a18:	6801      	ldr	r1, [r0, #0]
   d5a1a:	458b      	cmp	fp, r1
   d5a1c:	d323      	bcc.n	d5a66 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1fa>
   d5a1e:	4b1d      	ldr	r3, [pc, #116]	; (d5a94 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x228>)
   d5a20:	4a1e      	ldr	r2, [pc, #120]	; (d5a9c <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x230>)
   d5a22:	481f      	ldr	r0, [pc, #124]	; (d5aa0 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x234>)
   d5a24:	21ed      	movs	r1, #237	; 0xed
   d5a26:	f00e fc91 	bl	e434c <__assert_func>
      zero_point_data[i] = src_quantization->zero_point()->Get(i);
      scale_data[i] = src_quantization->scale()->Get(i);
    }
    quantization->scale = reinterpret_cast<TfLiteFloatArray*>(scale_array);
   d5a2a:	9b01      	ldr	r3, [sp, #4]
   d5a2c:	461a      	mov	r2, r3
   d5a2e:	9b02      	ldr	r3, [sp, #8]
   d5a30:	6013      	str	r3, [r2, #0]
    quantization->zero_point =
        reinterpret_cast<TfLiteIntArray*>(zero_point_array);

    result->quantization = {kTfLiteAffineQuantization, quantization};
   d5a32:	2301      	movs	r3, #1
      zero_point_data[i] = src_quantization->zero_point()->Get(i);
      scale_data[i] = src_quantization->scale()->Get(i);
    }
    quantization->scale = reinterpret_cast<TfLiteFloatArray*>(scale_array);
    quantization->zero_point =
        reinterpret_cast<TfLiteIntArray*>(zero_point_array);
   d5a34:	f8c2 a004 	str.w	sl, [r2, #4]

    result->quantization = {kTfLiteAffineQuantization, quantization};
   d5a38:	f884 3030 	strb.w	r3, [r4, #48]	; 0x30
   d5a3c:	6362      	str	r2, [r4, #52]	; 0x34
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5a3e:	210a      	movs	r1, #10
   d5a40:	4628      	mov	r0, r5
   d5a42:	f7ff fec1 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
  }
  // Copy the name, if there is one.
  if (flatbuffer_tensor.name()->c_str() != nullptr) {
   d5a46:	3004      	adds	r0, #4
    result->name = flatbuffer_tensor.name()->c_str();
  } else {
    result->name = "<No name>";
   d5a48:	bf04      	itt	eq
   d5a4a:	4b16      	ldreq	r3, [pc, #88]	; (d5aa4 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x238>)
   d5a4c:	6223      	streq	r3, [r4, #32]
  }
  // These aren't used by the micro flavor of TFL, so set them to defaults.
  result->allocation = nullptr;
   d5a4e:	f04f 0300 	mov.w	r3, #0

    result->quantization = {kTfLiteAffineQuantization, quantization};
  }
  // Copy the name, if there is one.
  if (flatbuffer_tensor.name()->c_str() != nullptr) {
    result->name = flatbuffer_tensor.name()->c_str();
   d5a52:	bf18      	it	ne
   d5a54:	6220      	strne	r0, [r4, #32]
  } else {
    result->name = "<No name>";
  }
  // These aren't used by the micro flavor of TFL, so set them to defaults.
  result->allocation = nullptr;
   d5a56:	61e3      	str	r3, [r4, #28]
  result->delegate = nullptr;
   d5a58:	6263      	str	r3, [r4, #36]	; 0x24
  result->buffer_handle = 0;
   d5a5a:	62a3      	str	r3, [r4, #40]	; 0x28
  result->data_is_stale = false;
   d5a5c:	f884 302c 	strb.w	r3, [r4, #44]	; 0x2c
   d5a60:	e014      	b.n	d5a8c <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x220>
    const flatbuffers::Vector<flatbuffers::Offset<Buffer>>* buffers,
    ErrorReporter* error_reporter, TfLiteTensor* result,
    uint8_t* preallocated_buffer) {
  // Make sure the serialized type is one we know how to deal with, and convert
  // it from a flatbuffer enum into a constant used by the kernel C API.
  TF_LITE_ENSURE_STATUS(ConvertTensorType(flatbuffer_tensor.type(),
   d5a62:	2601      	movs	r6, #1
   d5a64:	e012      	b.n	d5a8c <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x220>
template<typename T> struct IndirectHelper {
  typedef T return_type;
  typedef T mutable_return_type;
  static const size_t element_stride = sizeof(T);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    return EndianScalar((reinterpret_cast<const T *>(p))[i]);
   d5a66:	eb00 00cb 	add.w	r0, r0, fp, lsl #3
    zero_point_array[0] = channels;
    scale_array[0] = channels;
    int* zero_point_data = &zero_point_array[1];
    float* scale_data = reinterpret_cast<float*>(&scale_array[1]);
    for (int i = 0; i < channels; i++) {
      zero_point_data[i] = src_quantization->zero_point()->Get(i);
   d5a6a:	9b03      	ldr	r3, [sp, #12]
   d5a6c:	6841      	ldr	r1, [r0, #4]
   d5a6e:	f843 1f04 	str.w	r1, [r3, #4]!
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5a72:	4638      	mov	r0, r7
   d5a74:	2108      	movs	r1, #8
   d5a76:	9303      	str	r3, [sp, #12]
   d5a78:	f7ff fea6 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
      scale_data[i] = src_quantization->scale()->Get(i);
   d5a7c:	4659      	mov	r1, fp
   d5a7e:	f7ff fe61 	bl	d5744 <_ZNK11flatbuffers6VectorIfE3GetEm>
            channels * sizeof(float) + sizeof(int), sizeof(int)));
    zero_point_array[0] = channels;
    scale_array[0] = channels;
    int* zero_point_data = &zero_point_array[1];
    float* scale_data = reinterpret_cast<float*>(&scale_array[1]);
    for (int i = 0; i < channels; i++) {
   d5a82:	f10b 0b01 	add.w	fp, fp, #1
      zero_point_data[i] = src_quantization->zero_point()->Get(i);
      scale_data[i] = src_quantization->scale()->Get(i);
   d5a86:	eca9 0a01 	vstmia	r9!, {s0}
   d5a8a:	e7bf      	b.n	d5a0c <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1a0>
  result->allocation = nullptr;
  result->delegate = nullptr;
  result->buffer_handle = 0;
  result->data_is_stale = false;
  return kTfLiteOk;
}
   d5a8c:	4630      	mov	r0, r6
   d5a8e:	b007      	add	sp, #28
   d5a90:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   d5a94:	000e8541 	.word	0x000e8541
   d5a98:	000e92d8 	.word	0x000e92d8
   d5a9c:	000e93ce 	.word	0x000e93ce
   d5aa0:	000e854c 	.word	0x000e854c
   d5aa4:	000e912d 	.word	0x000e912d

000d5aa8 <_ZN6tflite14MicroAllocator15AllocateTensorsEv>:
  const auto* tensor = tensors_->Get(tensor_index);
  return InitializeRuntimeTensor(*tensor, buffers, error_reporter_,
                                 &context_->tensors[tensor_index], buffer);
}

TfLiteStatus MicroAllocator::AllocateTensors() {
   d5aa8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d5aac:	6a83      	ldr	r3, [r0, #40]	; 0x28
   d5aae:	f8d3 9000 	ldr.w	r9, [r3]

  // It would be better not to allocate this memory for the lifetime of the
  // model, but we don't have a straightforward way to avoid it.
  TensorInfo* tensor_info =
      reinterpret_cast<TensorInfo*>(memory_allocator_.AllocateFromTail(
          sizeof(TensorInfo) * tensors_size, sizeof(TensorInfo)));
   d5ab2:	2214      	movs	r2, #20
  const auto* tensor = tensors_->Get(tensor_index);
  return InitializeRuntimeTensor(*tensor, buffers, error_reporter_,
                                 &context_->tensors[tensor_index], buffer);
}

TfLiteStatus MicroAllocator::AllocateTensors() {
   d5ab4:	b091      	sub	sp, #68	; 0x44

  // It would be better not to allocate this memory for the lifetime of the
  // model, but we don't have a straightforward way to avoid it.
  TensorInfo* tensor_info =
      reinterpret_cast<TensorInfo*>(memory_allocator_.AllocateFromTail(
          sizeof(TensorInfo) * tensors_size, sizeof(TensorInfo)));
   d5ab6:	fb02 f109 	mul.w	r1, r2, r9
  const auto* tensor = tensors_->Get(tensor_index);
  return InitializeRuntimeTensor(*tensor, buffers, error_reporter_,
                                 &context_->tensors[tensor_index], buffer);
}

TfLiteStatus MicroAllocator::AllocateTensors() {
   d5aba:	4604      	mov	r4, r0

  // It would be better not to allocate this memory for the lifetime of the
  // model, but we don't have a straightforward way to avoid it.
  TensorInfo* tensor_info =
      reinterpret_cast<TensorInfo*>(memory_allocator_.AllocateFromTail(
          sizeof(TensorInfo) * tensors_size, sizeof(TensorInfo)));
   d5abc:	3004      	adds	r0, #4
   d5abe:	f000 fc90 	bl	d63e2 <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5ac2:	210c      	movs	r1, #12
   d5ac4:	4605      	mov	r5, r0
   d5ac6:	6820      	ldr	r0, [r4, #0]
   d5ac8:	f7ff fe7e 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
   d5acc:	f105 0710 	add.w	r7, r5, #16

  const flatbuffers::Vector<flatbuffers::Offset<Buffer>>* buffers =
      model_->buffers();

  // Set up the runtime data structures for all tensors.
  for (size_t i = 0; i < tensors_size; ++i) {
   d5ad0:	f04f 0800 	mov.w	r8, #0
   d5ad4:	9002      	str	r0, [sp, #8]
   d5ad6:	463e      	mov	r6, r7
    const bool is_variable = current->flatbuffer_tensor->is_variable();
    if (is_variable) {
      current->first_created = 0;
      current->last_used = operators_->size();
    } else {
      current->first_created = -1;
   d5ad8:	f04f 3aff 	mov.w	sl, #4294967295	; 0xffffffff
    TensorInfo* current = &tensor_info[i];
    current->flatbuffer_tensor = &(*(tensors_->Get(i)));
    current->runtime_tensor = &context_->tensors[i];
    const bool is_variable = current->flatbuffer_tensor->is_variable();
    if (is_variable) {
      current->first_created = 0;
   d5adc:	46c3      	mov	fp, r8

  const flatbuffers::Vector<flatbuffers::Offset<Buffer>>* buffers =
      model_->buffers();

  // Set up the runtime data structures for all tensors.
  for (size_t i = 0; i < tensors_size; ++i) {
   d5ade:	45c8      	cmp	r8, r9
   d5ae0:	d104      	bne.n	d5aec <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x44>
   d5ae2:	2600      	movs	r6, #0
  }

  // First go through the inputs and figure out if they need to be allocated.
  for (size_t i = 0; i < subgraph_->inputs()->size(); ++i) {
    const int tensor_index = subgraph_->inputs()->Get(i);
    TensorInfo* current = &tensor_info[tensor_index];
   d5ae4:	f04f 0914 	mov.w	r9, #20
    // Check for pre-allocated inputs.
    current->needs_allocating = (current->runtime_tensor->data.raw == nullptr);
    current->first_created = 0;
   d5ae8:	46b0      	mov	r8, r6
   d5aea:	e03e      	b.n	d5b6a <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0xc2>
      model_->buffers();

  // Set up the runtime data structures for all tensors.
  for (size_t i = 0; i < tensors_size; ++i) {
    TensorInfo* current = &tensor_info[i];
    current->flatbuffer_tensor = &(*(tensors_->Get(i)));
   d5aec:	6aa0      	ldr	r0, [r4, #40]	; 0x28
   d5aee:	4641      	mov	r1, r8
   d5af0:	f7ff fe52 	bl	d5798 <_ZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEm>
   d5af4:	f846 0c10 	str.w	r0, [r6, #-16]
    current->runtime_tensor = &context_->tensors[i];
   d5af8:	6963      	ldr	r3, [r4, #20]
   d5afa:	689b      	ldr	r3, [r3, #8]
   d5afc:	2238      	movs	r2, #56	; 0x38
   d5afe:	fb02 3308 	mla	r3, r2, r8, r3
   d5b02:	f846 3c0c 	str.w	r3, [r6, #-12]
   d5b06:	9303      	str	r3, [sp, #12]
    const bool is_variable = current->flatbuffer_tensor->is_variable();
   d5b08:	f7ff fe11 	bl	d572e <_ZNK6tflite6Tensor11is_variableEv>
    if (is_variable) {
   d5b0c:	9b03      	ldr	r3, [sp, #12]
   d5b0e:	b130      	cbz	r0, d5b1e <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x76>
      current->first_created = 0;
   d5b10:	f846 bc08 	str.w	fp, [r6, #-8]
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d5b14:	6a62      	ldr	r2, [r4, #36]	; 0x24
      current->last_used = operators_->size();
   d5b16:	6812      	ldr	r2, [r2, #0]
   d5b18:	f846 2c04 	str.w	r2, [r6, #-4]
   d5b1c:	e003      	b.n	d5b26 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x7e>
    } else {
      current->first_created = -1;
   d5b1e:	f846 ac08 	str.w	sl, [r6, #-8]
      current->last_used = -1;
   d5b22:	f846 ac04 	str.w	sl, [r6, #-4]
    }
    current->needs_allocating = false;
   d5b26:	f886 b000 	strb.w	fp, [r6]
    // Preallocated inputs have already been set up earlier, so skip them.
    const bool is_preallocated_input =
        (current->runtime_tensor->data.raw != nullptr);
    if (!is_preallocated_input) {
   d5b2a:	685a      	ldr	r2, [r3, #4]
   d5b2c:	b11a      	cbz	r2, d5b36 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x8e>

  const flatbuffers::Vector<flatbuffers::Offset<Buffer>>* buffers =
      model_->buffers();

  // Set up the runtime data structures for all tensors.
  for (size_t i = 0; i < tensors_size; ++i) {
   d5b2e:	f108 0801 	add.w	r8, r8, #1
   d5b32:	3614      	adds	r6, #20
   d5b34:	e7d3      	b.n	d5ade <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x36>
    current->needs_allocating = false;
    // Preallocated inputs have already been set up earlier, so skip them.
    const bool is_preallocated_input =
        (current->runtime_tensor->data.raw != nullptr);
    if (!is_preallocated_input) {
      TF_LITE_ENSURE_STATUS(InitializeRuntimeTensor(
   d5b36:	9201      	str	r2, [sp, #4]
   d5b38:	9300      	str	r3, [sp, #0]
   d5b3a:	6923      	ldr	r3, [r4, #16]
   d5b3c:	9a02      	ldr	r2, [sp, #8]
   d5b3e:	f856 1c10 	ldr.w	r1, [r6, #-16]
   d5b42:	4620      	mov	r0, r4
   d5b44:	f7ff fe92 	bl	d586c <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh>
   d5b48:	2800      	cmp	r0, #0
   d5b4a:	d0f0      	beq.n	d5b2e <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x86>
   d5b4c:	e101      	b.n	d5d52 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2aa>
    }
  }

  // First go through the inputs and figure out if they need to be allocated.
  for (size_t i = 0; i < subgraph_->inputs()->size(); ++i) {
    const int tensor_index = subgraph_->inputs()->Get(i);
   d5b4e:	4631      	mov	r1, r6
   d5b50:	f7ff fe0e 	bl	d5770 <_ZNK11flatbuffers6VectorIlE3GetEm>
    TensorInfo* current = &tensor_info[tensor_index];
   d5b54:	fb09 5000 	mla	r0, r9, r0, r5
          current->runtime_tensor, nullptr));
    }
  }

  // First go through the inputs and figure out if they need to be allocated.
  for (size_t i = 0; i < subgraph_->inputs()->size(); ++i) {
   d5b58:	3601      	adds	r6, #1
    const int tensor_index = subgraph_->inputs()->Get(i);
    TensorInfo* current = &tensor_info[tensor_index];
    // Check for pre-allocated inputs.
    current->needs_allocating = (current->runtime_tensor->data.raw == nullptr);
   d5b5a:	6843      	ldr	r3, [r0, #4]
   d5b5c:	685b      	ldr	r3, [r3, #4]
    current->first_created = 0;
   d5b5e:	f8c0 8008 	str.w	r8, [r0, #8]
  // First go through the inputs and figure out if they need to be allocated.
  for (size_t i = 0; i < subgraph_->inputs()->size(); ++i) {
    const int tensor_index = subgraph_->inputs()->Get(i);
    TensorInfo* current = &tensor_info[tensor_index];
    // Check for pre-allocated inputs.
    current->needs_allocating = (current->runtime_tensor->data.raw == nullptr);
   d5b62:	fab3 f383 	clz	r3, r3
   d5b66:	095b      	lsrs	r3, r3, #5
   d5b68:	7403      	strb	r3, [r0, #16]
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5b6a:	2106      	movs	r1, #6
   d5b6c:	6a20      	ldr	r0, [r4, #32]
   d5b6e:	f7ff fe2b 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
          current->runtime_tensor, nullptr));
    }
  }

  // First go through the inputs and figure out if they need to be allocated.
  for (size_t i = 0; i < subgraph_->inputs()->size(); ++i) {
   d5b72:	6803      	ldr	r3, [r0, #0]
   d5b74:	429e      	cmp	r6, r3
   d5b76:	d3ea      	bcc.n	d5b4e <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0xa6>
   d5b78:	2600      	movs	r6, #0

  // Mark all outputs as persistent to the end of the invocation.
  for (size_t i = 0; i < subgraph_->outputs()->size(); ++i) {
    const int tensor_index = subgraph_->outputs()->Get(i);
    TensorInfo* current = &tensor_info[tensor_index];
    current->last_used = operators_->size() - 1;
   d5b7a:	f04f 0914 	mov.w	r9, #20
   d5b7e:	2108      	movs	r1, #8
   d5b80:	6a20      	ldr	r0, [r4, #32]
   d5b82:	f7ff fe21 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
    current->needs_allocating = (current->runtime_tensor->data.raw == nullptr);
    current->first_created = 0;
  }

  // Mark all outputs as persistent to the end of the invocation.
  for (size_t i = 0; i < subgraph_->outputs()->size(); ++i) {
   d5b86:	6803      	ldr	r3, [r0, #0]
   d5b88:	f8d4 8024 	ldr.w	r8, [r4, #36]	; 0x24
   d5b8c:	429e      	cmp	r6, r3
   d5b8e:	d20a      	bcs.n	d5ba6 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0xfe>
    const int tensor_index = subgraph_->outputs()->Get(i);
   d5b90:	4631      	mov	r1, r6
   d5b92:	f7ff fded 	bl	d5770 <_ZNK11flatbuffers6VectorIlE3GetEm>
    TensorInfo* current = &tensor_info[tensor_index];
    current->last_used = operators_->size() - 1;
   d5b96:	f8d8 3000 	ldr.w	r3, [r8]
   d5b9a:	fb09 5000 	mla	r0, r9, r0, r5
   d5b9e:	3b01      	subs	r3, #1
   d5ba0:	60c3      	str	r3, [r0, #12]
    current->needs_allocating = (current->runtime_tensor->data.raw == nullptr);
    current->first_created = 0;
  }

  // Mark all outputs as persistent to the end of the invocation.
  for (size_t i = 0; i < subgraph_->outputs()->size(); ++i) {
   d5ba2:	3601      	adds	r6, #1
   d5ba4:	e7eb      	b.n	d5b7e <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0xd6>
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d5ba6:	f8d8 6000 	ldr.w	r6, [r8]
    TensorInfo* current = &tensor_info[tensor_index];
    current->last_used = operators_->size() - 1;
  }

  // Figure out when the first and last use of each tensor is.
  for (int i = (operators_->size() - 1); i >= 0; --i) {
   d5baa:	f106 38ff 	add.w	r8, r6, #4294967295	; 0xffffffff
   d5bae:	f106 4680 	add.w	r6, r6, #1073741824	; 0x40000000
   d5bb2:	3e01      	subs	r6, #1
   d5bb4:	00b6      	lsls	r6, r6, #2
    const auto* op = operators_->Get(i);
    for (size_t n = 0; n < op->inputs()->size(); ++n) {
      const int tensor_index = op->inputs()->Get(n);
      TensorInfo* current = &tensor_info[tensor_index];
   d5bb6:	f04f 0914 	mov.w	r9, #20
    TensorInfo* current = &tensor_info[tensor_index];
    current->last_used = operators_->size() - 1;
  }

  // Figure out when the first and last use of each tensor is.
  for (int i = (operators_->size() - 1); i >= 0; --i) {
   d5bba:	f1b8 0f00 	cmp.w	r8, #0
   d5bbe:	da03      	bge.n	d5bc8 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x120>
   d5bc0:	462b      	mov	r3, r5
   d5bc2:	2200      	movs	r2, #0
          "Logic error in memory planner, tensor %d has an invalid lifetime",
          i);
      return kTfLiteError;
    }
    if (!is_read_only && !is_preallocated_input) {
      current->needs_allocating = true;
   d5bc4:	2001      	movs	r0, #1
   d5bc6:	e04c      	b.n	d5c62 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x1ba>
    current->last_used = operators_->size() - 1;
  }

  // Figure out when the first and last use of each tensor is.
  for (int i = (operators_->size() - 1); i >= 0; --i) {
    const auto* op = operators_->Get(i);
   d5bc8:	6a63      	ldr	r3, [r4, #36]	; 0x24

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
   d5bca:	681a      	ldr	r2, [r3, #0]
   d5bcc:	4590      	cmp	r8, r2
   d5bce:	d305      	bcc.n	d5bdc <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x134>
   d5bd0:	4b66      	ldr	r3, [pc, #408]	; (d5d6c <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2c4>)
   d5bd2:	4a67      	ldr	r2, [pc, #412]	; (d5d70 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2c8>)
   d5bd4:	4867      	ldr	r0, [pc, #412]	; (d5d74 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2cc>)
   d5bd6:	21ed      	movs	r1, #237	; 0xed
   d5bd8:	f00e fbb8 	bl	e434c <__assert_func>
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d5bdc:	3304      	adds	r3, #4
template<typename T> struct IndirectHelper<Offset<T>> {
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
   d5bde:	eb03 0b06 	add.w	fp, r3, r6
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
   d5be2:	599b      	ldr	r3, [r3, r6]
    for (size_t n = 0; n < op->inputs()->size(); ++n) {
   d5be4:	f04f 0a00 	mov.w	sl, #0
   d5be8:	449b      	add	fp, r3
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5bea:	2106      	movs	r1, #6
   d5bec:	4658      	mov	r0, fp
   d5bee:	f7ff fdeb 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
   d5bf2:	6803      	ldr	r3, [r0, #0]
   d5bf4:	459a      	cmp	sl, r3
   d5bf6:	d302      	bcc.n	d5bfe <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x156>
   d5bf8:	f04f 0a00 	mov.w	sl, #0
   d5bfc:	e01a      	b.n	d5c34 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x18c>
      const int tensor_index = op->inputs()->Get(n);
   d5bfe:	4651      	mov	r1, sl
   d5c00:	f7ff fdb6 	bl	d5770 <_ZNK11flatbuffers6VectorIlE3GetEm>
      TensorInfo* current = &tensor_info[tensor_index];
   d5c04:	fb09 5000 	mla	r0, r9, r0, r5
      if ((current->last_used == -1) || (current->last_used > i)) {
   d5c08:	68c3      	ldr	r3, [r0, #12]
   d5c0a:	1c59      	adds	r1, r3, #1
   d5c0c:	d001      	beq.n	d5c12 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x16a>
   d5c0e:	4598      	cmp	r8, r3
   d5c10:	da01      	bge.n	d5c16 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x16e>
        current->last_used = i;
   d5c12:	f8c0 800c 	str.w	r8, [r0, #12]
  }

  // Figure out when the first and last use of each tensor is.
  for (int i = (operators_->size() - 1); i >= 0; --i) {
    const auto* op = operators_->Get(i);
    for (size_t n = 0; n < op->inputs()->size(); ++n) {
   d5c16:	f10a 0a01 	add.w	sl, sl, #1
   d5c1a:	e7e6      	b.n	d5bea <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x142>
      if ((current->last_used == -1) || (current->last_used > i)) {
        current->last_used = i;
      }
    }
    for (size_t n = 0; n < op->outputs()->size(); ++n) {
      const int tensor_index = op->outputs()->Get(n);
   d5c1c:	4651      	mov	r1, sl
   d5c1e:	f7ff fda7 	bl	d5770 <_ZNK11flatbuffers6VectorIlE3GetEm>
      TensorInfo* current = &tensor_info[tensor_index];
   d5c22:	fb09 5000 	mla	r0, r9, r0, r5
      if ((current->first_created == -1) || (current->first_created < i)) {
   d5c26:	6883      	ldr	r3, [r0, #8]
   d5c28:	1c5a      	adds	r2, r3, #1
   d5c2a:	d00b      	beq.n	d5c44 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x19c>
   d5c2c:	4598      	cmp	r8, r3
   d5c2e:	dc09      	bgt.n	d5c44 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x19c>
      TensorInfo* current = &tensor_info[tensor_index];
      if ((current->last_used == -1) || (current->last_used > i)) {
        current->last_used = i;
      }
    }
    for (size_t n = 0; n < op->outputs()->size(); ++n) {
   d5c30:	f10a 0a01 	add.w	sl, sl, #1
   d5c34:	2108      	movs	r1, #8
   d5c36:	4658      	mov	r0, fp
   d5c38:	f7ff fdc6 	bl	d57c8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
   d5c3c:	6803      	ldr	r3, [r0, #0]
   d5c3e:	459a      	cmp	sl, r3
   d5c40:	d3ec      	bcc.n	d5c1c <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x174>
   d5c42:	e002      	b.n	d5c4a <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x1a2>
      const int tensor_index = op->outputs()->Get(n);
      TensorInfo* current = &tensor_info[tensor_index];
      if ((current->first_created == -1) || (current->first_created < i)) {
        current->first_created = i;
   d5c44:	f8c0 8008 	str.w	r8, [r0, #8]
   d5c48:	e7f2      	b.n	d5c30 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x188>
    TensorInfo* current = &tensor_info[tensor_index];
    current->last_used = operators_->size() - 1;
  }

  // Figure out when the first and last use of each tensor is.
  for (int i = (operators_->size() - 1); i >= 0; --i) {
   d5c4a:	f108 38ff 	add.w	r8, r8, #4294967295	; 0xffffffff
   d5c4e:	3e04      	subs	r6, #4
   d5c50:	e7b3      	b.n	d5bba <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x112>

  // Work out which tensors need to be allocated.
  for (size_t i = 0; i < tensors_->size(); ++i) {
    TensorInfo* current = &tensor_info[i];
    const bool is_read_only =
        (current->first_created == -1) && (current->last_used != -1);
   d5c52:	6899      	ldr	r1, [r3, #8]
   d5c54:	3101      	adds	r1, #1
   d5c56:	68d9      	ldr	r1, [r3, #12]
   d5c58:	d175      	bne.n	d5d46 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x29e>
   d5c5a:	3101      	adds	r1, #1
   d5c5c:	d075      	beq.n	d5d4a <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2a2>
      }
    }
  }

  // Work out which tensors need to be allocated.
  for (size_t i = 0; i < tensors_->size(); ++i) {
   d5c5e:	3201      	adds	r2, #1
   d5c60:	3314      	adds	r3, #20
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d5c62:	6aa1      	ldr	r1, [r4, #40]	; 0x28
   d5c64:	6809      	ldr	r1, [r1, #0]
   d5c66:	428a      	cmp	r2, r1
   d5c68:	d3f3      	bcc.n	d5c52 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x1aa>
    if (!is_read_only && !is_preallocated_input) {
      current->needs_allocating = true;
    }
  }

  uint8_t* aligned_arena = AlignPointerUp(arena_, kBufferAlignment);
   d5c6a:	2110      	movs	r1, #16
   d5c6c:	69a0      	ldr	r0, [r4, #24]
   d5c6e:	f7ff fce5 	bl	d563c <_ZN6tflite14AlignPointerUpEPhj>
  const size_t alignment_loss = (aligned_arena - arena_);

  int remaining_arena_size =
      arena_size_ - (memory_allocator_.GetDataSize() + alignment_loss);
   d5c72:	69e3      	ldr	r3, [r4, #28]
   d5c74:	6866      	ldr	r6, [r4, #4]
   d5c76:	1b9e      	subs	r6, r3, r6
   d5c78:	69a3      	ldr	r3, [r4, #24]
   d5c7a:	1ac3      	subs	r3, r0, r3
   d5c7c:	1af6      	subs	r6, r6, r3
  GreedyMemoryPlanner planner(aligned_arena, remaining_arena_size);
   d5c7e:	4601      	mov	r1, r0
    if (!is_read_only && !is_preallocated_input) {
      current->needs_allocating = true;
    }
  }

  uint8_t* aligned_arena = AlignPointerUp(arena_, kBufferAlignment);
   d5c80:	4680      	mov	r8, r0
  const size_t alignment_loss = (aligned_arena - arena_);

  int remaining_arena_size =
      arena_size_ - (memory_allocator_.GetDataSize() + alignment_loss);
  GreedyMemoryPlanner planner(aligned_arena, remaining_arena_size);
   d5c82:	4632      	mov	r2, r6
   d5c84:	a806      	add	r0, sp, #24
   d5c86:	f00d fd3d 	bl	e3704 <_ZN6tflite19GreedyMemoryPlannerC1EPhi>

  // Add the tensors to our allocation plan.
  for (size_t i = 0; i < tensors_->size(); ++i) {
   d5c8a:	f04f 0900 	mov.w	r9, #0
   d5c8e:	6aa3      	ldr	r3, [r4, #40]	; 0x28
   d5c90:	681b      	ldr	r3, [r3, #0]
   d5c92:	4599      	cmp	r9, r3
   d5c94:	d21b      	bcs.n	d5cce <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x226>
    TensorInfo* current = &tensor_info[i];
    if (current->needs_allocating) {
   d5c96:	783b      	ldrb	r3, [r7, #0]
   d5c98:	b1ab      	cbz	r3, d5cc6 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x21e>
      size_t bytes_required;
      size_t type_size;
      TF_LITE_ENSURE_STATUS(BytesRequiredForTensor(*current->flatbuffer_tensor,
   d5c9a:	6923      	ldr	r3, [r4, #16]
   d5c9c:	f857 0c10 	ldr.w	r0, [r7, #-16]
   d5ca0:	aa05      	add	r2, sp, #20
   d5ca2:	a904      	add	r1, sp, #16
   d5ca4:	f7ff fcfe 	bl	d56a4 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE>
   d5ca8:	bb00      	cbnz	r0, d5cec <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x244>
                                                   &bytes_required, &type_size,
                                                   error_reporter_));
      size_t aligned_bytes_required =
          AlignSizeUp(bytes_required, kBufferAlignment);
   d5caa:	2110      	movs	r1, #16
   d5cac:	9804      	ldr	r0, [sp, #16]
   d5cae:	f7ff fccf 	bl	d5650 <_ZN6tflite11AlignSizeUpEjj>
      planner.AddBuffer(error_reporter_, aligned_bytes_required,
                        current->first_created, current->last_used);
   d5cb2:	f857 3c04 	ldr.w	r3, [r7, #-4]
   d5cb6:	9300      	str	r3, [sp, #0]
   d5cb8:	4602      	mov	r2, r0
   d5cba:	f857 3c08 	ldr.w	r3, [r7, #-8]
   d5cbe:	6921      	ldr	r1, [r4, #16]
   d5cc0:	a806      	add	r0, sp, #24
   d5cc2:	f00d fce3 	bl	e368c <_ZN6tflite19GreedyMemoryPlanner9AddBufferEPNS_13ErrorReporterEiii>
  int remaining_arena_size =
      arena_size_ - (memory_allocator_.GetDataSize() + alignment_loss);
  GreedyMemoryPlanner planner(aligned_arena, remaining_arena_size);

  // Add the tensors to our allocation plan.
  for (size_t i = 0; i < tensors_->size(); ++i) {
   d5cc6:	f109 0901 	add.w	r9, r9, #1
   d5cca:	3714      	adds	r7, #20
   d5ccc:	e7df      	b.n	d5c8e <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x1e6>
                        current->first_created, current->last_used);
    }
  }

  // Make sure we have enough room.
  if (planner.GetMaximumMemorySize() > remaining_arena_size) {
   d5cce:	a806      	add	r0, sp, #24
   d5cd0:	f00d fe01 	bl	e38d6 <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv>
   d5cd4:	4286      	cmp	r6, r0
   d5cd6:	da0b      	bge.n	d5cf0 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x248>
    error_reporter_->Report(
   d5cd8:	a806      	add	r0, sp, #24
   d5cda:	6924      	ldr	r4, [r4, #16]
   d5cdc:	f00d fdfb 	bl	e38d6 <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv>
        "Arena size is too small for activation buffers. Needed %d but only %d "
        "was available.",
        planner.GetMaximumMemorySize(), remaining_arena_size);
   d5ce0:	4633      	mov	r3, r6
   d5ce2:	4602      	mov	r2, r0
   d5ce4:	4924      	ldr	r1, [pc, #144]	; (d5d78 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2d0>)
   d5ce6:	4620      	mov	r0, r4
   d5ce8:	f7fe fb6e 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return kTfLiteError;
   d5cec:	2401      	movs	r4, #1
   d5cee:	e026      	b.n	d5d3e <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x296>
   d5cf0:	2600      	movs	r6, #0
   d5cf2:	4637      	mov	r7, r6
   d5cf4:	6aa3      	ldr	r3, [r4, #40]	; 0x28
  }

  // Figure out the actual memory addresses for each buffer, based on the plan.
  int planner_index = 0;
  for (size_t i = 0; i < tensors_->size(); ++i) {
   d5cf6:	681b      	ldr	r3, [r3, #0]
   d5cf8:	429e      	cmp	r6, r3
   d5cfa:	d21f      	bcs.n	d5d3c <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x294>
    TensorInfo* current = &tensor_info[i];
    if (current->needs_allocating) {
   d5cfc:	7c2b      	ldrb	r3, [r5, #16]
   d5cfe:	b163      	cbz	r3, d5d1a <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x272>
      int offset;
      TF_LITE_ENSURE_STATUS(
   d5d00:	ab05      	add	r3, sp, #20
   d5d02:	463a      	mov	r2, r7
   d5d04:	6921      	ldr	r1, [r4, #16]
   d5d06:	a806      	add	r0, sp, #24
   d5d08:	f00d fe00 	bl	e390c <_ZN6tflite19GreedyMemoryPlanner18GetOffsetForBufferEPNS_13ErrorReporterEiPi>
   d5d0c:	2800      	cmp	r0, #0
   d5d0e:	d1ed      	bne.n	d5cec <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x244>
          planner.GetOffsetForBuffer(error_reporter_, planner_index, &offset));
      current->runtime_tensor->data.uint8 = aligned_arena + offset;
   d5d10:	9b05      	ldr	r3, [sp, #20]
   d5d12:	686a      	ldr	r2, [r5, #4]
   d5d14:	4443      	add	r3, r8
   d5d16:	6053      	str	r3, [r2, #4]
      ++planner_index;
   d5d18:	3701      	adds	r7, #1
    }
    // Set default value for variable tensors:
    if (current->flatbuffer_tensor->is_variable()) {
   d5d1a:	6828      	ldr	r0, [r5, #0]
   d5d1c:	f7ff fd07 	bl	d572e <_ZNK6tflite6Tensor11is_variableEv>
   d5d20:	b148      	cbz	r0, d5d36 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x28e>
      if (current->runtime_tensor->data.uint8 == nullptr) {
   d5d22:	6868      	ldr	r0, [r5, #4]
   d5d24:	6843      	ldr	r3, [r0, #4]
   d5d26:	b923      	cbnz	r3, d5d32 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x28a>
        error_reporter_->Report("Variable is not allocated");
   d5d28:	4914      	ldr	r1, [pc, #80]	; (d5d7c <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2d4>)
   d5d2a:	6920      	ldr	r0, [r4, #16]
   d5d2c:	f7fe fb4c 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
   d5d30:	e7dc      	b.n	d5cec <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x244>
        return kTfLiteError;
      }
      tflite::ResetVariableTensor(current->runtime_tensor);
   d5d32:	f7ff fbaf 	bl	d5494 <_ZN6tflite19ResetVariableTensorEP12TfLiteTensor>
    return kTfLiteError;
  }

  // Figure out the actual memory addresses for each buffer, based on the plan.
  int planner_index = 0;
  for (size_t i = 0; i < tensors_->size(); ++i) {
   d5d36:	3601      	adds	r6, #1
   d5d38:	3514      	adds	r5, #20
   d5d3a:	e7db      	b.n	d5cf4 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x24c>
      }
      tflite::ResetVariableTensor(current->runtime_tensor);
    }
  }

  return kTfLiteOk;
   d5d3c:	2400      	movs	r4, #0
  uint8_t* aligned_arena = AlignPointerUp(arena_, kBufferAlignment);
  const size_t alignment_loss = (aligned_arena - arena_);

  int remaining_arena_size =
      arena_size_ - (memory_allocator_.GetDataSize() + alignment_loss);
  GreedyMemoryPlanner planner(aligned_arena, remaining_arena_size);
   d5d3e:	a806      	add	r0, sp, #24
   d5d40:	f00d fc9a 	bl	e3678 <_ZN6tflite19GreedyMemoryPlannerD1Ev>
   d5d44:	e00e      	b.n	d5d64 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2bc>
        (current->first_created == -1) && (current->last_used != -1);
    const bool is_preallocated_input =
        (current->runtime_tensor->data.raw != nullptr);
    const bool has_partial_lifetime =
        !is_read_only &&
        ((current->first_created == -1) || (current->last_used == -1));
   d5d46:	3101      	adds	r1, #1
   d5d48:	d105      	bne.n	d5d56 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2ae>
    if (has_partial_lifetime) {
      error_reporter_->Report(
          "Logic error in memory planner, tensor %d has an invalid lifetime",
          i);
   d5d4a:	490d      	ldr	r1, [pc, #52]	; (d5d80 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2d8>)
   d5d4c:	6920      	ldr	r0, [r4, #16]
   d5d4e:	f7fe fb3b 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
      return kTfLiteError;
   d5d52:	2401      	movs	r4, #1
   d5d54:	e006      	b.n	d5d64 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2bc>
  for (size_t i = 0; i < tensors_->size(); ++i) {
    TensorInfo* current = &tensor_info[i];
    const bool is_read_only =
        (current->first_created == -1) && (current->last_used != -1);
    const bool is_preallocated_input =
        (current->runtime_tensor->data.raw != nullptr);
   d5d56:	6859      	ldr	r1, [r3, #4]
      error_reporter_->Report(
          "Logic error in memory planner, tensor %d has an invalid lifetime",
          i);
      return kTfLiteError;
    }
    if (!is_read_only && !is_preallocated_input) {
   d5d58:	6849      	ldr	r1, [r1, #4]
   d5d5a:	2900      	cmp	r1, #0
   d5d5c:	f47f af7f 	bne.w	d5c5e <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x1b6>
      current->needs_allocating = true;
   d5d60:	7418      	strb	r0, [r3, #16]
   d5d62:	e77c      	b.n	d5c5e <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x1b6>
      tflite::ResetVariableTensor(current->runtime_tensor);
    }
  }

  return kTfLiteOk;
}
   d5d64:	4620      	mov	r0, r4
   d5d66:	b011      	add	sp, #68	; 0x44
   d5d68:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   d5d6c:	000e8541 	.word	0x000e8541
   d5d70:	000e8f14 	.word	0x000e8f14
   d5d74:	000e854c 	.word	0x000e854c
   d5d78:	000e9160 	.word	0x000e9160
   d5d7c:	000e91b5 	.word	0x000e91b5
   d5d80:	000e91cf 	.word	0x000e91cf

000d5d84 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list>:
  }
  DebugLog("\r\n");
}
}  // namespace

int MicroErrorReporter::Report(const char* format, va_list args) {
   d5d84:	b5f0      	push	{r4, r5, r6, r7, lr}
namespace tflite {
namespace {
void DebugLogPrintf(const char* format, va_list args) {
  const int output_cache_size = 64;
  char output_cache[output_cache_size + 1];
  int output_cache_index = 0;
   d5d86:	2300      	movs	r3, #0
  }
  DebugLog("\r\n");
}
}  // namespace

int MicroErrorReporter::Report(const char* format, va_list args) {
   d5d88:	b093      	sub	sp, #76	; 0x4c
   d5d8a:	460d      	mov	r5, r1
   d5d8c:	4614      	mov	r4, r2
    } else {
      output_cache[output_cache_index] = *current;
      output_cache_index += 1;
    }
    if (output_cache_index >= output_cache_size) {
      output_cache[output_cache_index] = 0;
   d5d8e:	461f      	mov	r7, r3
void DebugLogPrintf(const char* format, va_list args) {
  const int output_cache_size = 64;
  char output_cache[output_cache_size + 1];
  int output_cache_index = 0;
  const char* current = format;
  while (*current != 0) {
   d5d90:	782a      	ldrb	r2, [r5, #0]
   d5d92:	2a00      	cmp	r2, #0
   d5d94:	d041      	beq.n	d5e1a <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x96>
    if (*current == '%') {
   d5d96:	2a25      	cmp	r2, #37	; 0x25
   d5d98:	d12e      	bne.n	d5df8 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x74>
      const char next = *(current + 1);
   d5d9a:	786e      	ldrb	r6, [r5, #1]
      if ((next == 'd') || (next == 's') || (next == 'f')) {
   d5d9c:	f006 02fd 	and.w	r2, r6, #253	; 0xfd
   d5da0:	2a64      	cmp	r2, #100	; 0x64
   d5da2:	d001      	beq.n	d5da8 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x24>
   d5da4:	2e73      	cmp	r6, #115	; 0x73
   d5da6:	d12c      	bne.n	d5e02 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x7e>
        current += 1;
   d5da8:	3501      	adds	r5, #1
        if (output_cache_index > 0) {
   d5daa:	b133      	cbz	r3, d5dba <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x36>
          output_cache[output_cache_index] = 0;
   d5dac:	aa12      	add	r2, sp, #72	; 0x48
   d5dae:	4413      	add	r3, r2
          DebugLog(output_cache);
   d5db0:	a801      	add	r0, sp, #4
    if (*current == '%') {
      const char next = *(current + 1);
      if ((next == 'd') || (next == 's') || (next == 'f')) {
        current += 1;
        if (output_cache_index > 0) {
          output_cache[output_cache_index] = 0;
   d5db2:	f803 7c44 	strb.w	r7, [r3, #-68]
          DebugLog(output_cache);
   d5db6:	f000 fb29 	bl	d640c <DebugLog>
          output_cache_index = 0;
        }
        if (next == 'd') {
   d5dba:	2e64      	cmp	r6, #100	; 0x64
   d5dbc:	d104      	bne.n	d5dc8 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x44>
          DebugLogInt32(va_arg(args, int));
   d5dbe:	6820      	ldr	r0, [r4, #0]
   d5dc0:	1d26      	adds	r6, r4, #4
   d5dc2:	f7ff fbaf 	bl	d5524 <DebugLogInt32>
   d5dc6:	e005      	b.n	d5dd4 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x50>
        } else if (next == 's') {
   d5dc8:	2e73      	cmp	r6, #115	; 0x73
   d5dca:	d105      	bne.n	d5dd8 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x54>
          DebugLog(va_arg(args, char*));
   d5dcc:	6820      	ldr	r0, [r4, #0]
   d5dce:	1d26      	adds	r6, r4, #4
   d5dd0:	f000 fb1c 	bl	d640c <DebugLog>
   d5dd4:	4634      	mov	r4, r6
   d5dd6:	e01d      	b.n	d5e14 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x90>
        } else if (next == 'f') {
   d5dd8:	2e66      	cmp	r6, #102	; 0x66
   d5dda:	d11b      	bne.n	d5e14 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x90>
          DebugLogFloat(va_arg(args, double));
   d5ddc:	1de2      	adds	r2, r4, #7
   d5dde:	f022 0207 	bic.w	r2, r2, #7
   d5de2:	e9d2 0100 	ldrd	r0, r1, [r2]
   d5de6:	f102 0408 	add.w	r4, r2, #8
   d5dea:	f011 fc3d 	bl	e7668 <__aeabi_d2f>
   d5dee:	ee00 0a10 	vmov	s0, r0
   d5df2:	f7ff fbad 	bl	d5550 <DebugLogFloat>
   d5df6:	e00d      	b.n	d5e14 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x90>
        }
      }
    } else {
      output_cache[output_cache_index] = *current;
   d5df8:	a912      	add	r1, sp, #72	; 0x48
   d5dfa:	4419      	add	r1, r3
      output_cache_index += 1;
   d5dfc:	3301      	adds	r3, #1
        } else if (next == 'f') {
          DebugLogFloat(va_arg(args, double));
        }
      }
    } else {
      output_cache[output_cache_index] = *current;
   d5dfe:	f801 2c44 	strb.w	r2, [r1, #-68]
      output_cache_index += 1;
    }
    if (output_cache_index >= output_cache_size) {
   d5e02:	2b3f      	cmp	r3, #63	; 0x3f
   d5e04:	dd07      	ble.n	d5e16 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x92>
      output_cache[output_cache_index] = 0;
   d5e06:	aa12      	add	r2, sp, #72	; 0x48
   d5e08:	4413      	add	r3, r2
      DebugLog(output_cache);
   d5e0a:	a801      	add	r0, sp, #4
    } else {
      output_cache[output_cache_index] = *current;
      output_cache_index += 1;
    }
    if (output_cache_index >= output_cache_size) {
      output_cache[output_cache_index] = 0;
   d5e0c:	f803 7c44 	strb.w	r7, [r3, #-68]
      DebugLog(output_cache);
   d5e10:	f000 fafc 	bl	d640c <DebugLog>
      output_cache_index = 0;
   d5e14:	2300      	movs	r3, #0
    }
    current += 1;
   d5e16:	3501      	adds	r5, #1
   d5e18:	e7ba      	b.n	d5d90 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0xc>
  }
  if (output_cache_index > 0) {
   d5e1a:	b133      	cbz	r3, d5e2a <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0xa6>
    output_cache[output_cache_index] = 0;
   d5e1c:	a912      	add	r1, sp, #72	; 0x48
   d5e1e:	440b      	add	r3, r1
    DebugLog(output_cache);
   d5e20:	a801      	add	r0, sp, #4
      output_cache_index = 0;
    }
    current += 1;
  }
  if (output_cache_index > 0) {
    output_cache[output_cache_index] = 0;
   d5e22:	f803 2c44 	strb.w	r2, [r3, #-68]
    DebugLog(output_cache);
   d5e26:	f000 faf1 	bl	d640c <DebugLog>
    output_cache_index = 0;
  }
  DebugLog("\r\n");
   d5e2a:	4803      	ldr	r0, [pc, #12]	; (d5e38 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0xb4>)
   d5e2c:	f000 faee 	bl	d640c <DebugLog>
}  // namespace

int MicroErrorReporter::Report(const char* format, va_list args) {
  DebugLogPrintf(format, args);
  return 0;
}
   d5e30:	2000      	movs	r0, #0
   d5e32:	b013      	add	sp, #76	; 0x4c
   d5e34:	bdf0      	pop	{r4, r5, r6, r7, pc}
   d5e36:	bf00      	nop
   d5e38:	000e94bc 	.word	0x000e94bc

000d5e3c <_ZN6tflite12_GLOBAL__N_118StackDataAllocator8AllocateEj>:
namespace {
const int kStackDataAllocatorSize = 128;
class StackDataAllocator : public BuiltinDataAllocator {
 public:
  void* Allocate(size_t size) override {
    if (size > kStackDataAllocatorSize) {
   d5e3c:	2980      	cmp	r1, #128	; 0x80
      return nullptr;
    } else {
      return data_;
   d5e3e:	bf94      	ite	ls
   d5e40:	3004      	addls	r0, #4
const int kStackDataAllocatorSize = 128;
class StackDataAllocator : public BuiltinDataAllocator {
 public:
  void* Allocate(size_t size) override {
    if (size > kStackDataAllocatorSize) {
      return nullptr;
   d5e42:	2000      	movhi	r0, #0
    } else {
      return data_;
    }
  }
   d5e44:	4770      	bx	lr

000d5e46 <_ZN6tflite12_GLOBAL__N_118StackDataAllocator10DeallocateEPv>:
  void Deallocate(void* data) override {
   d5e46:	4770      	bx	lr

000d5e48 <_ZN6tflite12_GLOBAL__N_118StackDataAllocatorD1Ev>:
#include "tensorflow/lite/experimental/micro/compatibility.h"

namespace tflite {
namespace {
const int kStackDataAllocatorSize = 128;
class StackDataAllocator : public BuiltinDataAllocator {
   d5e48:	4770      	bx	lr

000d5e4a <_ZN6tflite12_GLOBAL__N_113ReportOpErrorEP13TfLiteContextPKcz>:
  } else {
    return EnumNameBuiltinOperator(BuiltinOperator(registration->builtin_code));
  }
}

void ReportOpError(struct TfLiteContext* context, const char* format, ...) {
   d5e4a:	b40e      	push	{r1, r2, r3}
   d5e4c:	b503      	push	{r0, r1, lr}
  MicroInterpreter* interpreter =
      static_cast<MicroInterpreter*>(context->impl_);
   d5e4e:	68c3      	ldr	r3, [r0, #12]
   d5e50:	6898      	ldr	r0, [r3, #8]
  } else {
    return EnumNameBuiltinOperator(BuiltinOperator(registration->builtin_code));
  }
}

void ReportOpError(struct TfLiteContext* context, const char* format, ...) {
   d5e52:	aa03      	add	r2, sp, #12
  MicroInterpreter* interpreter =
      static_cast<MicroInterpreter*>(context->impl_);
  va_list args;
  va_start(args, format);
  interpreter->error_reporter()->Report(format, args);
   d5e54:	6803      	ldr	r3, [r0, #0]
  } else {
    return EnumNameBuiltinOperator(BuiltinOperator(registration->builtin_code));
  }
}

void ReportOpError(struct TfLiteContext* context, const char* format, ...) {
   d5e56:	f852 1b04 	ldr.w	r1, [r2], #4
  MicroInterpreter* interpreter =
      static_cast<MicroInterpreter*>(context->impl_);
  va_list args;
  va_start(args, format);
   d5e5a:	9201      	str	r2, [sp, #4]
  interpreter->error_reporter()->Report(format, args);
   d5e5c:	689b      	ldr	r3, [r3, #8]
   d5e5e:	4798      	blx	r3
  va_end(args);
}
   d5e60:	b002      	add	sp, #8
   d5e62:	f85d eb04 	ldr.w	lr, [sp], #4
   d5e66:	b003      	add	sp, #12
   d5e68:	4770      	bx	lr

000d5e6a <_ZN6tflite12_GLOBAL__N_118StackDataAllocatorD0Ev>:
#include "tensorflow/lite/experimental/micro/compatibility.h"

namespace tflite {
namespace {
const int kStackDataAllocatorSize = 128;
class StackDataAllocator : public BuiltinDataAllocator {
   d5e6a:	b510      	push	{r4, lr}
   d5e6c:	2184      	movs	r1, #132	; 0x84
   d5e6e:	4604      	mov	r4, r0
   d5e70:	f00e ffdf 	bl	e4e32 <_ZdlPvj>
   d5e74:	4620      	mov	r0, r4
   d5e76:	bd10      	pop	{r4, pc}

000d5e78 <_ZN6tflite16MicroInterpreter15AllocateTensorsEv>:
TfLiteStatus MicroInterpreter::RegisterPreallocatedInput(uint8_t* buffer,
                                                         size_t input_index) {
  return allocator_.RegisterPreallocatedInput(buffer, input_index);
}

TfLiteStatus MicroInterpreter::AllocateTensors() {
   d5e78:	b510      	push	{r4, lr}
   d5e7a:	4604      	mov	r4, r0
  TfLiteStatus status = allocator_.AllocateTensors();
   d5e7c:	3044      	adds	r0, #68	; 0x44
   d5e7e:	f7ff fe13 	bl	d5aa8 <_ZN6tflite14MicroAllocator15AllocateTensorsEv>
   d5e82:	2301      	movs	r3, #1
  TF_LITE_ENSURE_OK(&context_, status);
   d5e84:	b910      	cbnz	r0, d5e8c <_ZN6tflite16MicroInterpreter15AllocateTensorsEv+0x14>
  tensors_allocated_ = true;
   d5e86:	f884 3070 	strb.w	r3, [r4, #112]	; 0x70
  return kTfLiteOk;
   d5e8a:	bd10      	pop	{r4, pc}
  return allocator_.RegisterPreallocatedInput(buffer, input_index);
}

TfLiteStatus MicroInterpreter::AllocateTensors() {
  TfLiteStatus status = allocator_.AllocateTensors();
  TF_LITE_ENSURE_OK(&context_, status);
   d5e8c:	4618      	mov	r0, r3
  tensors_allocated_ = true;
  return kTfLiteOk;
}
   d5e8e:	bd10      	pop	{r4, pc}

000d5e90 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>:
// "tables" use an offset table (possibly shared) that allows fields to be
// omitted and added at will, but uses an extra indirection to read.
class Table {
 public:
  const uint8_t *GetVTable() const {
    return data_ - ReadScalar<soffset_t>(data_);
   d5e90:	6803      	ldr	r3, [r0, #0]
   d5e92:	1ac3      	subs	r3, r0, r3
    auto vtable = GetVTable();
    // The first element is the size of the vtable (fields + type id + itself).
    auto vtsize = ReadScalar<voffset_t>(vtable);
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
   d5e94:	881a      	ldrh	r2, [r3, #0]
   d5e96:	428a      	cmp	r2, r1
   d5e98:	bf8c      	ite	hi
   d5e9a:	5a5b      	ldrhhi	r3, [r3, r1]
   d5e9c:	2300      	movls	r3, #0
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
  }

  template<typename P> P GetPointer(voffset_t field) {
    auto field_offset = GetOptionalFieldOffset(field);
    auto p = data_ + field_offset;
   d5e9e:	18c2      	adds	r2, r0, r3
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
   d5ea0:	b113      	cbz	r3, d5ea8 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t+0x18>
   d5ea2:	58c3      	ldr	r3, [r0, r3]
   d5ea4:	18d0      	adds	r0, r2, r3
   d5ea6:	4770      	bx	lr
   d5ea8:	4618      	mov	r0, r3
  }
   d5eaa:	4770      	bx	lr

000d5eac <_ZN6tflite16MicroInterpreterC1EPKNS_5ModelERKNS_10OpResolverEPhjPNS_13ErrorReporterE>:
  va_end(args);
}

}  // namespace

MicroInterpreter::MicroInterpreter(const Model* model,
   d5eac:	e92d 41f3 	stmdb	sp!, {r0, r1, r4, r5, r6, r7, r8, lr}
   d5eb0:	4604      	mov	r4, r0
   d5eb2:	9d09      	ldr	r5, [sp, #36]	; 0x24
      op_resolver_(op_resolver),
      error_reporter_(error_reporter),
      context_(),
      allocator_(&context_, model_, tensor_arena, tensor_arena_size,
                 error_reporter_),
      tensors_allocated_(false) {
   d5eb4:	6021      	str	r1, [r4, #0]
   d5eb6:	f100 070c 	add.w	r7, r0, #12
  va_end(args);
}

}  // namespace

MicroInterpreter::MicroInterpreter(const Model* model,
   d5eba:	460e      	mov	r6, r1
      op_resolver_(op_resolver),
      error_reporter_(error_reporter),
      context_(),
      allocator_(&context_, model_, tensor_arena, tensor_arena_size,
                 error_reporter_),
      tensors_allocated_(false) {
   d5ebc:	6042      	str	r2, [r0, #4]
   d5ebe:	6085      	str	r5, [r0, #8]
   d5ec0:	2238      	movs	r2, #56	; 0x38
   d5ec2:	2100      	movs	r1, #0
   d5ec4:	4638      	mov	r0, r7
  va_end(args);
}

}  // namespace

MicroInterpreter::MicroInterpreter(const Model* model,
   d5ec6:	4698      	mov	r8, r3
      op_resolver_(op_resolver),
      error_reporter_(error_reporter),
      context_(),
      allocator_(&context_, model_, tensor_arena, tensor_arena_size,
                 error_reporter_),
      tensors_allocated_(false) {
   d5ec8:	f011 fc96 	bl	e77f8 <memset>
   d5ecc:	9b08      	ldr	r3, [sp, #32]
   d5ece:	9300      	str	r3, [sp, #0]
   d5ed0:	4632      	mov	r2, r6
   d5ed2:	4639      	mov	r1, r7
   d5ed4:	4643      	mov	r3, r8
   d5ed6:	9501      	str	r5, [sp, #4]
   d5ed8:	f104 0044 	add.w	r0, r4, #68	; 0x44
   d5edc:	2700      	movs	r7, #0
   d5ede:	f7ff fc7d 	bl	d57dc <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE>
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5ee2:	4630      	mov	r0, r6
   d5ee4:	f884 7070 	strb.w	r7, [r4, #112]	; 0x70
   d5ee8:	2108      	movs	r1, #8
   d5eea:	f7ff ffd1 	bl	d5e90 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
  auto* subgraphs = model->subgraphs();
  if (subgraphs->size() != 1) {
   d5eee:	6806      	ldr	r6, [r0, #0]
   d5ef0:	2e01      	cmp	r6, #1
   d5ef2:	d007      	beq.n	d5f04 <_ZN6tflite16MicroInterpreterC1EPKNS_5ModelERKNS_10OpResolverEPhjPNS_13ErrorReporterE+0x58>
    error_reporter->Report("Only 1 subgraph is currently supported.\n");
   d5ef4:	490f      	ldr	r1, [pc, #60]	; (d5f34 <_ZN6tflite16MicroInterpreterC1EPKNS_5ModelERKNS_10OpResolverEPhjPNS_13ErrorReporterE+0x88>)
   d5ef6:	4628      	mov	r0, r5
   d5ef8:	f7fe fa66 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    initialization_status_ = kTfLiteError;
   d5efc:	2301      	movs	r3, #1
   d5efe:	f884 3071 	strb.w	r3, [r4, #113]	; 0x71
    return;
   d5f02:	e013      	b.n	d5f2c <_ZN6tflite16MicroInterpreterC1EPKNS_5ModelERKNS_10OpResolverEPhjPNS_13ErrorReporterE+0x80>
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
   d5f04:	6843      	ldr	r3, [r0, #4]
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d5f06:	1d05      	adds	r5, r0, #4
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
   d5f08:	441d      	add	r5, r3
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5f0a:	2104      	movs	r1, #4
  }
  subgraph_ = (*subgraphs)[0];
   d5f0c:	67e5      	str	r5, [r4, #124]	; 0x7c
   d5f0e:	4628      	mov	r0, r5
   d5f10:	f7ff ffbe 	bl	d5e90 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
   d5f14:	210a      	movs	r1, #10
  tensors_ = subgraph_->tensors();
   d5f16:	6760      	str	r0, [r4, #116]	; 0x74
   d5f18:	4628      	mov	r0, r5
   d5f1a:	f7ff ffb9 	bl	d5e90 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
  operators_ = subgraph_->operators();

  context_.impl_ = static_cast<void*>(this);
  context_.ReportError = ReportOpError;
   d5f1e:	4b06      	ldr	r3, [pc, #24]	; (d5f38 <_ZN6tflite16MicroInterpreterC1EPKNS_5ModelERKNS_10OpResolverEPhjPNS_13ErrorReporterE+0x8c>)
    initialization_status_ = kTfLiteError;
    return;
  }
  subgraph_ = (*subgraphs)[0];
  tensors_ = subgraph_->tensors();
  operators_ = subgraph_->operators();
   d5f20:	67a0      	str	r0, [r4, #120]	; 0x78

  context_.impl_ = static_cast<void*>(this);
   d5f22:	61a4      	str	r4, [r4, #24]
  context_.ReportError = ReportOpError;
   d5f24:	6223      	str	r3, [r4, #32]
  context_.recommended_num_threads = 1;
   d5f26:	6326      	str	r6, [r4, #48]	; 0x30
      if (thisTensor->allocation_type == kTfLiteMmapRo)
        CorrectTensorEndianness(thisTensor);
    }
  }

  initialization_status_ = kTfLiteOk;
   d5f28:	f884 7071 	strb.w	r7, [r4, #113]	; 0x71
}
   d5f2c:	4620      	mov	r0, r4
   d5f2e:	b002      	add	sp, #8
   d5f30:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   d5f34:	000e9104 	.word	0x000e9104
   d5f38:	000d5e4b 	.word	0x000d5e4b

000d5f3c <_ZN6tflite16MicroInterpreter6outputEj>:
    return nullptr;
  }
  return &(context_.tensors[inputs->Get(index)]);
}

TfLiteTensor* MicroInterpreter::output(size_t index) {
   d5f3c:	b538      	push	{r3, r4, r5, lr}
   d5f3e:	460d      	mov	r5, r1
   d5f40:	4604      	mov	r4, r0
   d5f42:	2108      	movs	r1, #8
   d5f44:	6fc0      	ldr	r0, [r0, #124]	; 0x7c
   d5f46:	f7ff ffa3 	bl	d5e90 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d5f4a:	6803      	ldr	r3, [r0, #0]
  const flatbuffers::Vector<int32_t>* outputs = subgraph_->outputs();
  const size_t length = outputs->size();
  if ((index < 0) || (index >= outputs->size())) {
   d5f4c:	42ab      	cmp	r3, r5
   d5f4e:	d806      	bhi.n	d5f5e <_ZN6tflite16MicroInterpreter6outputEj+0x22>
    error_reporter_->Report("Output index %d out of range (length is %d)",
                            index, length);
   d5f50:	462a      	mov	r2, r5
   d5f52:	4907      	ldr	r1, [pc, #28]	; (d5f70 <_ZN6tflite16MicroInterpreter6outputEj+0x34>)
   d5f54:	68a0      	ldr	r0, [r4, #8]
   d5f56:	f7fe fa37 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return nullptr;
   d5f5a:	2000      	movs	r0, #0
   d5f5c:	bd38      	pop	{r3, r4, r5, pc}
  }
  return &(context_.tensors[outputs->Get(index)]);
   d5f5e:	4629      	mov	r1, r5
   d5f60:	f7ff fc06 	bl	d5770 <_ZNK11flatbuffers6VectorIlE3GetEm>
   d5f64:	6962      	ldr	r2, [r4, #20]
   d5f66:	2338      	movs	r3, #56	; 0x38
   d5f68:	fb03 2000 	mla	r0, r3, r0, r2
}
   d5f6c:	bd38      	pop	{r3, r4, r5, pc}
   d5f6e:	bf00      	nop
   d5f70:	000e94eb 	.word	0x000e94eb

000d5f74 <_ZN6tflite16MicroInterpreter5inputEj>:
    }
  }
  return status;
}

TfLiteTensor* MicroInterpreter::input(size_t index) {
   d5f74:	b538      	push	{r3, r4, r5, lr}
   d5f76:	460d      	mov	r5, r1
   d5f78:	4604      	mov	r4, r0
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5f7a:	2106      	movs	r1, #6
   d5f7c:	6fc0      	ldr	r0, [r0, #124]	; 0x7c
   d5f7e:	f7ff ff87 	bl	d5e90 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d5f82:	6803      	ldr	r3, [r0, #0]
  const flatbuffers::Vector<int32_t>* inputs = subgraph_->inputs();
  const size_t length = inputs->size();
  if ((index < 0) || (index >= length)) {
   d5f84:	429d      	cmp	r5, r3
   d5f86:	d306      	bcc.n	d5f96 <_ZN6tflite16MicroInterpreter5inputEj+0x22>
    error_reporter_->Report("Input index %d out of range (length is %d)", index,
                            length);
   d5f88:	462a      	mov	r2, r5
   d5f8a:	4907      	ldr	r1, [pc, #28]	; (d5fa8 <_ZN6tflite16MicroInterpreter5inputEj+0x34>)
   d5f8c:	68a0      	ldr	r0, [r4, #8]
   d5f8e:	f7fe fa1b 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return nullptr;
   d5f92:	2000      	movs	r0, #0
   d5f94:	bd38      	pop	{r3, r4, r5, pc}
  }
  return &(context_.tensors[inputs->Get(index)]);
   d5f96:	4629      	mov	r1, r5
   d5f98:	f7ff fbea 	bl	d5770 <_ZNK11flatbuffers6VectorIlE3GetEm>
   d5f9c:	6962      	ldr	r2, [r4, #20]
   d5f9e:	2338      	movs	r3, #56	; 0x38
   d5fa0:	fb03 2000 	mla	r0, r3, r0, r2
}
   d5fa4:	bd38      	pop	{r3, r4, r5, pc}
   d5fa6:	bf00      	nop
   d5fa8:	000e9517 	.word	0x000e9517

000d5fac <_ZN6tflite16MicroInterpreter6InvokeEv>:
  TF_LITE_ENSURE_OK(&context_, status);
  tensors_allocated_ = true;
  return kTfLiteOk;
}

TfLiteStatus MicroInterpreter::Invoke() {
   d5fac:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  if (initialization_status_ != kTfLiteOk) {
   d5fb0:	f890 5071 	ldrb.w	r5, [r0, #113]	; 0x71
  TF_LITE_ENSURE_OK(&context_, status);
  tensors_allocated_ = true;
  return kTfLiteOk;
}

TfLiteStatus MicroInterpreter::Invoke() {
   d5fb4:	b0c5      	sub	sp, #276	; 0x114
   d5fb6:	4604      	mov	r4, r0
  if (initialization_status_ != kTfLiteOk) {
   d5fb8:	b125      	cbz	r5, d5fc4 <_ZN6tflite16MicroInterpreter6InvokeEv+0x18>
    error_reporter_->Report("Invoke() called after initialization failed\n");
   d5fba:	4976      	ldr	r1, [pc, #472]	; (d6194 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1e8>)
   d5fbc:	6880      	ldr	r0, [r0, #8]
   d5fbe:	f7fe fa03 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
   d5fc2:	e0a2      	b.n	d610a <_ZN6tflite16MicroInterpreter6InvokeEv+0x15e>
    return kTfLiteError;
  }

  // Ensure tensors are allocated before the interpreter is invoked to avoid
  // difficult to debug segfaults.
  if (!tensors_allocated_) {
   d5fc4:	f890 3070 	ldrb.w	r3, [r0, #112]	; 0x70
   d5fc8:	b90b      	cbnz	r3, d5fce <_ZN6tflite16MicroInterpreter6InvokeEv+0x22>
    AllocateTensors();
   d5fca:	f7ff ff55 	bl	d5e78 <_ZN6tflite16MicroInterpreter15AllocateTensorsEv>
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5fce:	2106      	movs	r1, #6
   d5fd0:	6820      	ldr	r0, [r4, #0]
   d5fd2:	f7ff ff5d 	bl	d5e90 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
  }
  TfLiteStatus status = kTfLiteOk;
  auto opcodes = model_->operator_codes();
  for (size_t i = 0; i < operators_->size(); ++i) {
   d5fd6:	2600      	movs	r6, #0
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d5fd8:	1d03      	adds	r3, r0, #4
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5fda:	4683      	mov	fp, r0
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d5fdc:	46b2      	mov	sl, r6
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d5fde:	9304      	str	r3, [sp, #16]
   d5fe0:	6fa3      	ldr	r3, [r4, #120]	; 0x78
   d5fe2:	681a      	ldr	r2, [r3, #0]
   d5fe4:	4296      	cmp	r6, r2
   d5fe6:	f080 80d1 	bcs.w	d618c <_ZN6tflite16MicroInterpreter6InvokeEv+0x1e0>
   d5fea:	3304      	adds	r3, #4
template<typename T> struct IndirectHelper<Offset<T>> {
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
   d5fec:	eb03 0286 	add.w	r2, r3, r6, lsl #2
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
   d5ff0:	f853 3026 	ldr.w	r3, [r3, r6, lsl #2]
   d5ff4:	18d7      	adds	r7, r2, r3
// "tables" use an offset table (possibly shared) that allows fields to be
// omitted and added at will, but uses an extra indirection to read.
class Table {
 public:
  const uint8_t *GetVTable() const {
    return data_ - ReadScalar<soffset_t>(data_);
   d5ff6:	58d3      	ldr	r3, [r2, r3]
   d5ff8:	1afb      	subs	r3, r7, r3
    auto vtable = GetVTable();
    // The first element is the size of the vtable (fields + type id + itself).
    auto vtsize = ReadScalar<voffset_t>(vtable);
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
   d5ffa:	881a      	ldrh	r2, [r3, #0]
   d5ffc:	2a04      	cmp	r2, #4
   d5ffe:	d904      	bls.n	d600a <_ZN6tflite16MicroInterpreter6InvokeEv+0x5e>
   d6000:	889b      	ldrh	r3, [r3, #4]
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d6002:	b12b      	cbz	r3, d6010 <_ZN6tflite16MicroInterpreter6InvokeEv+0x64>
   d6004:	f857 8003 	ldr.w	r8, [r7, r3]
   d6008:	e003      	b.n	d6012 <_ZN6tflite16MicroInterpreter6InvokeEv+0x66>
   d600a:	f04f 0800 	mov.w	r8, #0
   d600e:	e000      	b.n	d6012 <_ZN6tflite16MicroInterpreter6InvokeEv+0x66>
   d6010:	4698      	mov	r8, r3
    const auto* op = operators_->Get(i);
    size_t index = op->opcode_index();
    if (index < 0 || index >= opcodes->size()) {
   d6012:	f8db 3000 	ldr.w	r3, [fp]
   d6016:	4543      	cmp	r3, r8
   d6018:	d802      	bhi.n	d6020 <_ZN6tflite16MicroInterpreter6InvokeEv+0x74>
      error_reporter_->Report("Missing registration for opcode_index %d\n",
                              index);
   d601a:	4642      	mov	r2, r8
   d601c:	495e      	ldr	r1, [pc, #376]	; (d6198 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1ec>)
   d601e:	e025      	b.n	d606c <_ZN6tflite16MicroInterpreter6InvokeEv+0xc0>
template<typename T> struct IndirectHelper<Offset<T>> {
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
   d6020:	9b04      	ldr	r3, [sp, #16]
      return kTfLiteError;
    }
    auto opcode = (*opcodes)[index];
    const TfLiteRegistration* registration = nullptr;
    status = GetRegistrationFromOpCode(opcode, op_resolver_, error_reporter_,
                                       &registration);
   d6022:	68a2      	ldr	r2, [r4, #8]
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
   d6024:	f853 e028 	ldr.w	lr, [r3, r8, lsl #2]
   d6028:	6861      	ldr	r1, [r4, #4]
template<typename T> struct IndirectHelper<Offset<T>> {
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
   d602a:	eb03 0088 	add.w	r0, r3, r8, lsl #2
      error_reporter_->Report("Missing registration for opcode_index %d\n",
                              index);
      return kTfLiteError;
    }
    auto opcode = (*opcodes)[index];
    const TfLiteRegistration* registration = nullptr;
   d602e:	ab44      	add	r3, sp, #272	; 0x110
    status = GetRegistrationFromOpCode(opcode, op_resolver_, error_reporter_,
                                       &registration);
   d6030:	4470      	add	r0, lr
      error_reporter_->Report("Missing registration for opcode_index %d\n",
                              index);
      return kTfLiteError;
    }
    auto opcode = (*opcodes)[index];
    const TfLiteRegistration* registration = nullptr;
   d6032:	f843 adf4 	str.w	sl, [r3, #-244]!
    status = GetRegistrationFromOpCode(opcode, op_resolver_, error_reporter_,
                                       &registration);
   d6036:	f7ff f9d3 	bl	d53e0 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration>
    if (status != kTfLiteOk) {
   d603a:	2800      	cmp	r0, #0
   d603c:	d167      	bne.n	d610e <_ZN6tflite16MicroInterpreter6InvokeEv+0x162>
      return status;
    }
    if (registration == nullptr) {
   d603e:	9b07      	ldr	r3, [sp, #28]
   d6040:	b913      	cbnz	r3, d6048 <_ZN6tflite16MicroInterpreter6InvokeEv+0x9c>
      error_reporter_->Report("Skipping op for opcode_index %d\n", index);
   d6042:	4642      	mov	r2, r8
   d6044:	4955      	ldr	r1, [pc, #340]	; (d619c <_ZN6tflite16MicroInterpreter6InvokeEv+0x1f0>)
   d6046:	e011      	b.n	d606c <_ZN6tflite16MicroInterpreter6InvokeEv+0xc0>
      return kTfLiteError;
    }
    BuiltinOperator op_type =
        static_cast<BuiltinOperator>(registration->builtin_code);
   d6048:	f893 8014 	ldrb.w	r8, [r3, #20]

    if (op_type != BuiltinOperator_CUSTOM && op->custom_options()) {
   d604c:	f1b8 0f20 	cmp.w	r8, #32
   d6050:	d010      	beq.n	d6074 <_ZN6tflite16MicroInterpreter6InvokeEv+0xc8>
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d6052:	210e      	movs	r1, #14
   d6054:	4638      	mov	r0, r7
   d6056:	f7ff ff1b 	bl	d5e90 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
   d605a:	b158      	cbz	r0, d6074 <_ZN6tflite16MicroInterpreter6InvokeEv+0xc8>
  };
  return names;
}

inline const char *EnumNameBuiltinOperator(BuiltinOperator e) {
  if (e < BuiltinOperator_ADD || e > BuiltinOperator_NON_MAX_SUPPRESSION_V5) return "";
   d605c:	f1b8 0f79 	cmp.w	r8, #121	; 0x79
   d6060:	f200 8092 	bhi.w	d6188 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1dc>
  const size_t index = static_cast<size_t>(e);
  return EnumNamesBuiltinOperator()[index];
   d6064:	4b4e      	ldr	r3, [pc, #312]	; (d61a0 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1f4>)
   d6066:	f853 2028 	ldr.w	r2, [r3, r8, lsl #2]
      error_reporter_->Report(
          "Unsupported behavior: found builtin operator %s with custom "
          "options.\n",
          EnumNameBuiltinOperator(op_type));
   d606a:	494e      	ldr	r1, [pc, #312]	; (d61a4 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1f8>)
   d606c:	68a0      	ldr	r0, [r4, #8]
   d606e:	f7fe f9ab 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
   d6072:	e04a      	b.n	d610a <_ZN6tflite16MicroInterpreter6InvokeEv+0x15e>
#include "tensorflow/lite/experimental/micro/compatibility.h"

namespace tflite {
namespace {
const int kStackDataAllocatorSize = 128;
class StackDataAllocator : public BuiltinDataAllocator {
   d6074:	4b4c      	ldr	r3, [pc, #304]	; (d61a8 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1fc>)
   d6076:	9323      	str	r3, [sp, #140]	; 0x8c
   d6078:	210e      	movs	r1, #14
   d607a:	4638      	mov	r0, r7
      return kTfLiteError;
    }
    StackDataAllocator stack_data_allocator;
    const char* custom_data = nullptr;
    size_t custom_data_size = 0;
    unsigned char* builtin_data = nullptr;
   d607c:	f8cd a020 	str.w	sl, [sp, #32]
   d6080:	f7ff ff06 	bl	d5e90 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
    if (op->custom_options()) {
   d6084:	2800      	cmp	r0, #0
   d6086:	d044      	beq.n	d6112 <_ZN6tflite16MicroInterpreter6InvokeEv+0x166>
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d6088:	1d03      	adds	r3, r0, #4
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d608a:	f8d0 9000 	ldr.w	r9, [r0]
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d608e:	9303      	str	r3, [sp, #12]
                                        (void**)(&builtin_data)));
    }

    const char* init_data;
    size_t init_data_size;
    if (registration->builtin_code == BuiltinOperator_CUSTOM) {
   d6090:	9807      	ldr	r0, [sp, #28]
   d6092:	6942      	ldr	r2, [r0, #20]
    } else {
      init_data = reinterpret_cast<const char*>(builtin_data);
      init_data_size = 0;
    }
    void* user_data = nullptr;
    if (registration->init) {
   d6094:	6803      	ldr	r3, [r0, #0]
                                        (void**)(&builtin_data)));
    }

    const char* init_data;
    size_t init_data_size;
    if (registration->builtin_code == BuiltinOperator_CUSTOM) {
   d6096:	2a20      	cmp	r2, #32
      init_data = custom_data;
      init_data_size = custom_data_size;
    } else {
      init_data = reinterpret_cast<const char*>(builtin_data);
   d6098:	bf15      	itete	ne
   d609a:	9908      	ldrne	r1, [sp, #32]
    }

    const char* init_data;
    size_t init_data_size;
    if (registration->builtin_code == BuiltinOperator_CUSTOM) {
      init_data = custom_data;
   d609c:	9903      	ldreq	r1, [sp, #12]
      init_data_size = custom_data_size;
    } else {
      init_data = reinterpret_cast<const char*>(builtin_data);
      init_data_size = 0;
   d609e:	2200      	movne	r2, #0

    const char* init_data;
    size_t init_data_size;
    if (registration->builtin_code == BuiltinOperator_CUSTOM) {
      init_data = custom_data;
      init_data_size = custom_data_size;
   d60a0:	464a      	moveq	r2, r9
    } else {
      init_data = reinterpret_cast<const char*>(builtin_data);
      init_data_size = 0;
    }
    void* user_data = nullptr;
    if (registration->init) {
   d60a2:	2b00      	cmp	r3, #0
   d60a4:	d042      	beq.n	d612c <_ZN6tflite16MicroInterpreter6InvokeEv+0x180>
      user_data = registration->init(&context_, init_data, init_data_size);
   d60a6:	f104 000c 	add.w	r0, r4, #12
   d60aa:	4798      	blx	r3
   d60ac:	4680      	mov	r8, r0
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d60ae:	2106      	movs	r1, #6
   d60b0:	4638      	mov	r0, r7
   d60b2:	f7ff feed 	bl	d5e90 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
   d60b6:	2108      	movs	r1, #8
   d60b8:	9005      	str	r0, [sp, #20]
   d60ba:	4638      	mov	r0, r7
   d60bc:	f7ff fee8 	bl	d5e90 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
    TfLiteIntArray* temporaries_array =
        reinterpret_cast<TfLiteIntArray*>(temporaries_data);
    temporaries_array->size = 0;

    TfLiteNode node;
    node.inputs = inputs_array;
   d60c0:	9a05      	ldr	r2, [sp, #20]
    node.outputs = outputs_array;
    node.temporaries = temporaries_array;
    node.user_data = user_data;
    node.builtin_data = reinterpret_cast<void*>(builtin_data);
    node.custom_initial_data = custom_data;
   d60c2:	9b03      	ldr	r3, [sp, #12]
    TfLiteIntArray* temporaries_array =
        reinterpret_cast<TfLiteIntArray*>(temporaries_data);
    temporaries_array->size = 0;

    TfLiteNode node;
    node.inputs = inputs_array;
   d60c4:	9209      	str	r2, [sp, #36]	; 0x24
    node.outputs = outputs_array;
    node.temporaries = temporaries_array;
   d60c6:	aa12      	add	r2, sp, #72	; 0x48
   d60c8:	920c      	str	r2, [sp, #48]	; 0x30
    node.user_data = user_data;
    node.builtin_data = reinterpret_cast<void*>(builtin_data);
    node.custom_initial_data = custom_data;
   d60ca:	930f      	str	r3, [sp, #60]	; 0x3c
    TfLiteNode node;
    node.inputs = inputs_array;
    node.outputs = outputs_array;
    node.temporaries = temporaries_array;
    node.user_data = user_data;
    node.builtin_data = reinterpret_cast<void*>(builtin_data);
   d60cc:	9a08      	ldr	r2, [sp, #32]
    node.custom_initial_data = custom_data;
    node.custom_initial_data_size = custom_data_size;
    node.delegate = nullptr;
    if (registration->prepare) {
   d60ce:	9b07      	ldr	r3, [sp, #28]

    TfLiteNode node;
    node.inputs = inputs_array;
    node.outputs = outputs_array;
    node.temporaries = temporaries_array;
    node.user_data = user_data;
   d60d0:	f8cd 8034 	str.w	r8, [sp, #52]	; 0x34
    node.builtin_data = reinterpret_cast<void*>(builtin_data);
   d60d4:	920e      	str	r2, [sp, #56]	; 0x38
    node.custom_initial_data = custom_data;
    node.custom_initial_data_size = custom_data_size;
   d60d6:	f8cd 9040 	str.w	r9, [sp, #64]	; 0x40
    node.delegate = nullptr;
   d60da:	f8cd a044 	str.w	sl, [sp, #68]	; 0x44
    if (registration->prepare) {
   d60de:	689b      	ldr	r3, [r3, #8]

    const int kMaxTemporaries = 16;
    int temporaries_data[kMaxTemporaries + 1];
    TfLiteIntArray* temporaries_array =
        reinterpret_cast<TfLiteIntArray*>(temporaries_data);
    temporaries_array->size = 0;
   d60e0:	f8cd a048 	str.w	sl, [sp, #72]	; 0x48

    TfLiteNode node;
    node.inputs = inputs_array;
    node.outputs = outputs_array;
   d60e4:	900a      	str	r0, [sp, #40]	; 0x28
    node.user_data = user_data;
    node.builtin_data = reinterpret_cast<void*>(builtin_data);
    node.custom_initial_data = custom_data;
    node.custom_initial_data_size = custom_data_size;
    node.delegate = nullptr;
    if (registration->prepare) {
   d60e6:	b35b      	cbz	r3, d6140 <_ZN6tflite16MicroInterpreter6InvokeEv+0x194>
      TfLiteStatus prepare_status = registration->prepare(&context_, &node);
   d60e8:	a909      	add	r1, sp, #36	; 0x24
   d60ea:	f104 000c 	add.w	r0, r4, #12
   d60ee:	4798      	blx	r3
      if (prepare_status != kTfLiteOk) {
   d60f0:	4601      	mov	r1, r0
   d60f2:	b328      	cbz	r0, d6140 <_ZN6tflite16MicroInterpreter6InvokeEv+0x194>
        error_reporter_->Report(
   d60f4:	9a07      	ldr	r2, [sp, #28]
   d60f6:	68a0      	ldr	r0, [r4, #8]

  TF_LITE_REMOVE_VIRTUAL_DELETE
};

const char* OpNameFromRegistration(const TfLiteRegistration* registration) {
  if (registration->builtin_code == BuiltinOperator_CUSTOM) {
   d60f8:	6953      	ldr	r3, [r2, #20]
   d60fa:	2b20      	cmp	r3, #32
   d60fc:	d118      	bne.n	d6130 <_ZN6tflite16MicroInterpreter6InvokeEv+0x184>
    return registration->custom_name;
   d60fe:	6992      	ldr	r2, [r2, #24]
    if (registration->prepare) {
      TfLiteStatus prepare_status = registration->prepare(&context_, &node);
      if (prepare_status != kTfLiteOk) {
        error_reporter_->Report(
            "Node %s (number %d) failed to prepare with status %d",
            OpNameFromRegistration(registration), i, prepare_status);
   d6100:	9100      	str	r1, [sp, #0]
   d6102:	492a      	ldr	r1, [pc, #168]	; (d61ac <_ZN6tflite16MicroInterpreter6InvokeEv+0x200>)
   d6104:	4633      	mov	r3, r6
    if (registration->invoke) {
      TfLiteStatus invoke_status = registration->invoke(&context_, &node);
      if (invoke_status != kTfLiteOk) {
        error_reporter_->Report(
            "Node %s (number %d) failed to invoke with status %d",
            OpNameFromRegistration(registration), i, invoke_status);
   d6106:	f7fe f95f 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    if (op_type != BuiltinOperator_CUSTOM && op->custom_options()) {
      error_reporter_->Report(
          "Unsupported behavior: found builtin operator %s with custom "
          "options.\n",
          EnumNameBuiltinOperator(op_type));
      return kTfLiteError;
   d610a:	2501      	movs	r5, #1
   d610c:	e03e      	b.n	d618c <_ZN6tflite16MicroInterpreter6InvokeEv+0x1e0>
      return kTfLiteError;
    }
    auto opcode = (*opcodes)[index];
    const TfLiteRegistration* registration = nullptr;
    status = GetRegistrationFromOpCode(opcode, op_resolver_, error_reporter_,
                                       &registration);
   d610e:	4605      	mov	r5, r0
   d6110:	e03c      	b.n	d618c <_ZN6tflite16MicroInterpreter6InvokeEv+0x1e0>
    unsigned char* builtin_data = nullptr;
    if (op->custom_options()) {
      custom_data = reinterpret_cast<const char*>(op->custom_options()->data());
      custom_data_size = op->custom_options()->size();
    } else {
      TF_LITE_ENSURE_STATUS(ParseOpData(op, op_type, error_reporter_,
   d6112:	ab08      	add	r3, sp, #32
   d6114:	9300      	str	r3, [sp, #0]
   d6116:	68a2      	ldr	r2, [r4, #8]
   d6118:	ab23      	add	r3, sp, #140	; 0x8c
   d611a:	4641      	mov	r1, r8
   d611c:	4638      	mov	r0, r7
   d611e:	f7fe fa3f 	bl	d45a0 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv>
   d6122:	2800      	cmp	r0, #0
   d6124:	d1f1      	bne.n	d610a <_ZN6tflite16MicroInterpreter6InvokeEv+0x15e>
          EnumNameBuiltinOperator(op_type));
      return kTfLiteError;
    }
    StackDataAllocator stack_data_allocator;
    const char* custom_data = nullptr;
    size_t custom_data_size = 0;
   d6126:	4681      	mov	r9, r0
          "options.\n",
          EnumNameBuiltinOperator(op_type));
      return kTfLiteError;
    }
    StackDataAllocator stack_data_allocator;
    const char* custom_data = nullptr;
   d6128:	9003      	str	r0, [sp, #12]
   d612a:	e7b1      	b.n	d6090 <_ZN6tflite16MicroInterpreter6InvokeEv+0xe4>
      init_data_size = custom_data_size;
    } else {
      init_data = reinterpret_cast<const char*>(builtin_data);
      init_data_size = 0;
    }
    void* user_data = nullptr;
   d612c:	4698      	mov	r8, r3
   d612e:	e7be      	b.n	d60ae <_ZN6tflite16MicroInterpreter6InvokeEv+0x102>
  };
  return names;
}

inline const char *EnumNameBuiltinOperator(BuiltinOperator e) {
  if (e < BuiltinOperator_ADD || e > BuiltinOperator_NON_MAX_SUPPRESSION_V5) return "";
   d6130:	b2db      	uxtb	r3, r3
   d6132:	2b79      	cmp	r3, #121	; 0x79
  const size_t index = static_cast<size_t>(e);
  return EnumNamesBuiltinOperator()[index];
   d6134:	bf96      	itet	ls
   d6136:	4a1a      	ldrls	r2, [pc, #104]	; (d61a0 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1f4>)
  };
  return names;
}

inline const char *EnumNameBuiltinOperator(BuiltinOperator e) {
  if (e < BuiltinOperator_ADD || e > BuiltinOperator_NON_MAX_SUPPRESSION_V5) return "";
   d6138:	4a1d      	ldrhi	r2, [pc, #116]	; (d61b0 <_ZN6tflite16MicroInterpreter6InvokeEv+0x204>)
  const size_t index = static_cast<size_t>(e);
  return EnumNamesBuiltinOperator()[index];
   d613a:	f852 2023 	ldrls.w	r2, [r2, r3, lsl #2]
   d613e:	e7df      	b.n	d6100 <_ZN6tflite16MicroInterpreter6InvokeEv+0x154>
            OpNameFromRegistration(registration), i, prepare_status);
        return kTfLiteError;
      }
    }

    if (registration->invoke) {
   d6140:	9b07      	ldr	r3, [sp, #28]
   d6142:	68db      	ldr	r3, [r3, #12]
   d6144:	b1bb      	cbz	r3, d6176 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1ca>
      TfLiteStatus invoke_status = registration->invoke(&context_, &node);
   d6146:	a909      	add	r1, sp, #36	; 0x24
   d6148:	f104 000c 	add.w	r0, r4, #12
   d614c:	4798      	blx	r3
      if (invoke_status != kTfLiteOk) {
   d614e:	4601      	mov	r1, r0
   d6150:	b188      	cbz	r0, d6176 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1ca>
        error_reporter_->Report(
   d6152:	9a07      	ldr	r2, [sp, #28]
   d6154:	68a0      	ldr	r0, [r4, #8]

  TF_LITE_REMOVE_VIRTUAL_DELETE
};

const char* OpNameFromRegistration(const TfLiteRegistration* registration) {
  if (registration->builtin_code == BuiltinOperator_CUSTOM) {
   d6156:	6953      	ldr	r3, [r2, #20]
   d6158:	2b20      	cmp	r3, #32
   d615a:	d101      	bne.n	d6160 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1b4>
    return registration->custom_name;
   d615c:	6992      	ldr	r2, [r2, #24]
   d615e:	e006      	b.n	d616e <_ZN6tflite16MicroInterpreter6InvokeEv+0x1c2>
  };
  return names;
}

inline const char *EnumNameBuiltinOperator(BuiltinOperator e) {
  if (e < BuiltinOperator_ADD || e > BuiltinOperator_NON_MAX_SUPPRESSION_V5) return "";
   d6160:	b2db      	uxtb	r3, r3
   d6162:	2b79      	cmp	r3, #121	; 0x79
  const size_t index = static_cast<size_t>(e);
  return EnumNamesBuiltinOperator()[index];
   d6164:	bf96      	itet	ls
   d6166:	4a0e      	ldrls	r2, [pc, #56]	; (d61a0 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1f4>)
  };
  return names;
}

inline const char *EnumNameBuiltinOperator(BuiltinOperator e) {
  if (e < BuiltinOperator_ADD || e > BuiltinOperator_NON_MAX_SUPPRESSION_V5) return "";
   d6168:	4a11      	ldrhi	r2, [pc, #68]	; (d61b0 <_ZN6tflite16MicroInterpreter6InvokeEv+0x204>)
  const size_t index = static_cast<size_t>(e);
  return EnumNamesBuiltinOperator()[index];
   d616a:	f852 2023 	ldrls.w	r2, [r2, r3, lsl #2]
    if (registration->invoke) {
      TfLiteStatus invoke_status = registration->invoke(&context_, &node);
      if (invoke_status != kTfLiteOk) {
        error_reporter_->Report(
            "Node %s (number %d) failed to invoke with status %d",
            OpNameFromRegistration(registration), i, invoke_status);
   d616e:	9100      	str	r1, [sp, #0]
   d6170:	4633      	mov	r3, r6
   d6172:	4910      	ldr	r1, [pc, #64]	; (d61b4 <_ZN6tflite16MicroInterpreter6InvokeEv+0x208>)
   d6174:	e7c7      	b.n	d6106 <_ZN6tflite16MicroInterpreter6InvokeEv+0x15a>
        return kTfLiteError;
      }
    }

    if (registration->free) {
   d6176:	9b07      	ldr	r3, [sp, #28]
   d6178:	685b      	ldr	r3, [r3, #4]
   d617a:	b11b      	cbz	r3, d6184 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1d8>
      registration->free(&context_, user_data);
   d617c:	4641      	mov	r1, r8
   d617e:	f104 000c 	add.w	r0, r4, #12
   d6182:	4798      	blx	r3
  if (!tensors_allocated_) {
    AllocateTensors();
  }
  TfLiteStatus status = kTfLiteOk;
  auto opcodes = model_->operator_codes();
  for (size_t i = 0; i < operators_->size(); ++i) {
   d6184:	3601      	adds	r6, #1
   d6186:	e72b      	b.n	d5fe0 <_ZN6tflite16MicroInterpreter6InvokeEv+0x34>
  };
  return names;
}

inline const char *EnumNameBuiltinOperator(BuiltinOperator e) {
  if (e < BuiltinOperator_ADD || e > BuiltinOperator_NON_MAX_SUPPRESSION_V5) return "";
   d6188:	4a09      	ldr	r2, [pc, #36]	; (d61b0 <_ZN6tflite16MicroInterpreter6InvokeEv+0x204>)
   d618a:	e76e      	b.n	d606a <_ZN6tflite16MicroInterpreter6InvokeEv+0xbe>
    if (registration->free) {
      registration->free(&context_, user_data);
    }
  }
  return status;
}
   d618c:	4628      	mov	r0, r5
   d618e:	b045      	add	sp, #276	; 0x114
   d6190:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   d6194:	000e9542 	.word	0x000e9542
   d6198:	000e956f 	.word	0x000e956f
   d619c:	000e9599 	.word	0x000e9599
   d61a0:	000e86c4 	.word	0x000e86c4
   d61a4:	000e95ba 	.word	0x000e95ba
   d61a8:	000e9674 	.word	0x000e9674
   d61ac:	000e9600 	.word	0x000e9600
   d61b0:	000e94be 	.word	0x000e94be
   d61b4:	000e9635 	.word	0x000e9635

000d61b8 <_ZNK6tflite22MicroMutableOpResolver6FindOpENS_15BuiltinOperatorEi>:
#include "tensorflow/lite/experimental/micro/micro_mutable_op_resolver.h"

namespace tflite {

const TfLiteRegistration* MicroMutableOpResolver::FindOp(
    tflite::BuiltinOperator op, int version) const {
   d61b8:	b570      	push	{r4, r5, r6, lr}
  for (int i = 0; i < registrations_len_; ++i) {
   d61ba:	f241 0304 	movw	r3, #4100	; 0x1004
   d61be:	2400      	movs	r4, #0
   d61c0:	58c5      	ldr	r5, [r0, r3]
   d61c2:	4603      	mov	r3, r0
   d61c4:	42ac      	cmp	r4, r5
   d61c6:	da0c      	bge.n	d61e2 <_ZNK6tflite22MicroMutableOpResolver6FindOpENS_15BuiltinOperatorEi+0x2a>
    const TfLiteRegistration& registration = registrations_[i];
    if ((registration.builtin_code == op) &&
   d61c8:	699e      	ldr	r6, [r3, #24]
   d61ca:	428e      	cmp	r6, r1
   d61cc:	d106      	bne.n	d61dc <_ZNK6tflite22MicroMutableOpResolver6FindOpENS_15BuiltinOperatorEi+0x24>
   d61ce:	6a1e      	ldr	r6, [r3, #32]
   d61d0:	4296      	cmp	r6, r2
   d61d2:	d103      	bne.n	d61dc <_ZNK6tflite22MicroMutableOpResolver6FindOpENS_15BuiltinOperatorEi+0x24>
namespace tflite {

const TfLiteRegistration* MicroMutableOpResolver::FindOp(
    tflite::BuiltinOperator op, int version) const {
  for (int i = 0; i < registrations_len_; ++i) {
    const TfLiteRegistration& registration = registrations_[i];
   d61d4:	eb00 1044 	add.w	r0, r0, r4, lsl #5
   d61d8:	3004      	adds	r0, #4
   d61da:	bd70      	pop	{r4, r5, r6, pc}

namespace tflite {

const TfLiteRegistration* MicroMutableOpResolver::FindOp(
    tflite::BuiltinOperator op, int version) const {
  for (int i = 0; i < registrations_len_; ++i) {
   d61dc:	3401      	adds	r4, #1
   d61de:	3320      	adds	r3, #32
   d61e0:	e7f0      	b.n	d61c4 <_ZNK6tflite22MicroMutableOpResolver6FindOpENS_15BuiltinOperatorEi+0xc>
    if ((registration.builtin_code == op) &&
        (registration.version == version)) {
      return &registration;
    }
  }
  return nullptr;
   d61e2:	2000      	movs	r0, #0
}
   d61e4:	bd70      	pop	{r4, r5, r6, pc}

000d61e6 <_ZNK6tflite22MicroMutableOpResolver6FindOpEPKci>:

const TfLiteRegistration* MicroMutableOpResolver::FindOp(const char* op,
                                                         int version) const {
   d61e6:	e92d 43f8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, lr}
  for (int i = 0; i < registrations_len_; ++i) {
   d61ea:	f241 0304 	movw	r3, #4100	; 0x1004
  }
  return nullptr;
}

const TfLiteRegistration* MicroMutableOpResolver::FindOp(const char* op,
                                                         int version) const {
   d61ee:	4604      	mov	r4, r0
  for (int i = 0; i < registrations_len_; ++i) {
   d61f0:	58c7      	ldr	r7, [r0, r3]
  }
  return nullptr;
}

const TfLiteRegistration* MicroMutableOpResolver::FindOp(const char* op,
                                                         int version) const {
   d61f2:	4688      	mov	r8, r1
   d61f4:	4691      	mov	r9, r2
   d61f6:	4605      	mov	r5, r0
  for (int i = 0; i < registrations_len_; ++i) {
   d61f8:	2600      	movs	r6, #0
   d61fa:	42be      	cmp	r6, r7
   d61fc:	da12      	bge.n	d6224 <_ZNK6tflite22MicroMutableOpResolver6FindOpEPKci+0x3e>
    const TfLiteRegistration& registration = registrations_[i];
    if ((registration.builtin_code == BuiltinOperator_CUSTOM) &&
   d61fe:	69ab      	ldr	r3, [r5, #24]
   d6200:	2b20      	cmp	r3, #32
   d6202:	d10c      	bne.n	d621e <_ZNK6tflite22MicroMutableOpResolver6FindOpEPKci+0x38>
        (strcmp(registration.custom_name, op) == 0) &&
   d6204:	4641      	mov	r1, r8
   d6206:	69e8      	ldr	r0, [r5, #28]
   d6208:	f011 fb26 	bl	e7858 <strcmp>

const TfLiteRegistration* MicroMutableOpResolver::FindOp(const char* op,
                                                         int version) const {
  for (int i = 0; i < registrations_len_; ++i) {
    const TfLiteRegistration& registration = registrations_[i];
    if ((registration.builtin_code == BuiltinOperator_CUSTOM) &&
   d620c:	b938      	cbnz	r0, d621e <_ZNK6tflite22MicroMutableOpResolver6FindOpEPKci+0x38>
        (strcmp(registration.custom_name, op) == 0) &&
   d620e:	6a2b      	ldr	r3, [r5, #32]
   d6210:	454b      	cmp	r3, r9
   d6212:	d104      	bne.n	d621e <_ZNK6tflite22MicroMutableOpResolver6FindOpEPKci+0x38>
}

const TfLiteRegistration* MicroMutableOpResolver::FindOp(const char* op,
                                                         int version) const {
  for (int i = 0; i < registrations_len_; ++i) {
    const TfLiteRegistration& registration = registrations_[i];
   d6214:	eb04 1046 	add.w	r0, r4, r6, lsl #5
   d6218:	3004      	adds	r0, #4
   d621a:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
  return nullptr;
}

const TfLiteRegistration* MicroMutableOpResolver::FindOp(const char* op,
                                                         int version) const {
  for (int i = 0; i < registrations_len_; ++i) {
   d621e:	3601      	adds	r6, #1
   d6220:	3520      	adds	r5, #32
   d6222:	e7ea      	b.n	d61fa <_ZNK6tflite22MicroMutableOpResolver6FindOpEPKci+0x14>
        (strcmp(registration.custom_name, op) == 0) &&
        (registration.version == version)) {
      return &registration;
    }
  }
  return nullptr;
   d6224:	2000      	movs	r0, #0
}
   d6226:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}

000d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>:

void MicroMutableOpResolver::AddBuiltin(tflite::BuiltinOperator op,
                                        TfLiteRegistration* registration,
                                        int min_version, int max_version) {
  for (int version = min_version; version <= max_version; ++version) {
    if (registrations_len_ >= TFLITE_REGISTRATIONS_MAX) {
   d622a:	f500 5c80 	add.w	ip, r0, #4096	; 0x1000
  return nullptr;
}

void MicroMutableOpResolver::AddBuiltin(tflite::BuiltinOperator op,
                                        TfLiteRegistration* registration,
                                        int min_version, int max_version) {
   d622e:	e92d 47f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, lr}
  for (int version = min_version; version <= max_version; ++version) {
    if (registrations_len_ >= TFLITE_REGISTRATIONS_MAX) {
   d6232:	f10c 0c04 	add.w	ip, ip, #4
  return nullptr;
}

void MicroMutableOpResolver::AddBuiltin(tflite::BuiltinOperator op,
                                        TfLiteRegistration* registration,
                                        int min_version, int max_version) {
   d6236:	4680      	mov	r8, r0
   d6238:	4689      	mov	r9, r1
   d623a:	4692      	mov	sl, r2
   d623c:	461f      	mov	r7, r3
  for (int version = min_version; version <= max_version; ++version) {
   d623e:	9b08      	ldr	r3, [sp, #32]
   d6240:	429f      	cmp	r7, r3
   d6242:	dc15      	bgt.n	d6270 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii+0x46>
    if (registrations_len_ >= TFLITE_REGISTRATIONS_MAX) {
   d6244:	f8dc 6000 	ldr.w	r6, [ip]
   d6248:	2e7f      	cmp	r6, #127	; 0x7f
   d624a:	dc11      	bgt.n	d6270 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii+0x46>
      // TODO(petewarden) - Add error reporting hooks so we can report this!
      return;
    }
    TfLiteRegistration* new_registration = &registrations_[registrations_len_];
    registrations_len_ += 1;
   d624c:	1c73      	adds	r3, r6, #1
   d624e:	f8cc 3000 	str.w	r3, [ip]

    *new_registration = *registration;
   d6252:	4655      	mov	r5, sl
   d6254:	cd0f      	ldmia	r5!, {r0, r1, r2, r3}
   d6256:	eb08 1646 	add.w	r6, r8, r6, lsl #5
   d625a:	1d34      	adds	r4, r6, #4
   d625c:	c40f      	stmia	r4!, {r0, r1, r2, r3}
   d625e:	e895 000f 	ldmia.w	r5, {r0, r1, r2, r3}
   d6262:	e884 000f 	stmia.w	r4, {r0, r1, r2, r3}
    new_registration->builtin_code = op;
    new_registration->version = version;
   d6266:	6237      	str	r7, [r6, #32]
    }
    TfLiteRegistration* new_registration = &registrations_[registrations_len_];
    registrations_len_ += 1;

    *new_registration = *registration;
    new_registration->builtin_code = op;
   d6268:	f8c6 9018 	str.w	r9, [r6, #24]
}

void MicroMutableOpResolver::AddBuiltin(tflite::BuiltinOperator op,
                                        TfLiteRegistration* registration,
                                        int min_version, int max_version) {
  for (int version = min_version; version <= max_version; ++version) {
   d626c:	3701      	adds	r7, #1
   d626e:	e7e6      	b.n	d623e <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii+0x14>
   d6270:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}

000d6274 <_ZN6tflite12ElementCountERK14TfLiteIntArray>:
static const int8_t kAsymmetricInt8Max = 127;
static const int kSymmetricInt8Scale = kAsymmetricInt8Max;

}  // namespace

int ElementCount(const TfLiteIntArray& dims) {
   d6274:	b510      	push	{r4, lr}
   d6276:	4603      	mov	r3, r0
  int result = 1;
  for (int i = 0; i < dims.size; ++i) {
   d6278:	6804      	ldr	r4, [r0, #0]
   d627a:	2200      	movs	r2, #0
static const int kSymmetricInt8Scale = kAsymmetricInt8Max;

}  // namespace

int ElementCount(const TfLiteIntArray& dims) {
  int result = 1;
   d627c:	2001      	movs	r0, #1
  for (int i = 0; i < dims.size; ++i) {
   d627e:	42a2      	cmp	r2, r4
   d6280:	da04      	bge.n	d628c <_ZN6tflite12ElementCountERK14TfLiteIntArray+0x18>
    result *= dims.data[i];
   d6282:	f853 1f04 	ldr.w	r1, [r3, #4]!

}  // namespace

int ElementCount(const TfLiteIntArray& dims) {
  int result = 1;
  for (int i = 0; i < dims.size; ++i) {
   d6286:	3201      	adds	r2, #1
    result *= dims.data[i];
   d6288:	4348      	muls	r0, r1

}  // namespace

int ElementCount(const TfLiteIntArray& dims) {
  int result = 1;
  for (int i = 0; i < dims.size; ++i) {
   d628a:	e7f8      	b.n	d627e <_ZN6tflite12ElementCountERK14TfLiteIntArray+0xa>
    result *= dims.data[i];
  }
  return result;
}
   d628c:	bd10      	pop	{r4, pc}
	...

000d6290 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf>:

void SignedSymmetricPerChannelQuantize(const float* values,
                                       TfLiteIntArray* dims,
                                       int quantized_dimension,
                                       int8_t* quantized_values,
                                       float* scaling_factors) {
   d6290:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d6294:	ed2d 8b04 	vpush	{d8-d9}
   d6298:	b087      	sub	sp, #28
   d629a:	4688      	mov	r8, r1
   d629c:	4693      	mov	fp, r2
   d629e:	4682      	mov	sl, r0
  int input_size = ElementCount(*dims);
   d62a0:	4608      	mov	r0, r1

void SignedSymmetricPerChannelQuantize(const float* values,
                                       TfLiteIntArray* dims,
                                       int quantized_dimension,
                                       int8_t* quantized_values,
                                       float* scaling_factors) {
   d62a2:	9302      	str	r3, [sp, #8]
  int input_size = ElementCount(*dims);
   d62a4:	f7ff ffe6 	bl	d6274 <_ZN6tflite12ElementCountERK14TfLiteIntArray>
  int channel_count = dims->data[quantized_dimension];
   d62a8:	eb08 038b 	add.w	r3, r8, fp, lsl #2
   d62ac:	9f14      	ldr	r7, [sp, #80]	; 0x50
   d62ae:	685b      	ldr	r3, [r3, #4]
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
   d62b0:	eddf 9a40 	vldr	s19, [pc, #256]	; d63b4 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x124>
                                       TfLiteIntArray* dims,
                                       int quantized_dimension,
                                       int8_t* quantized_values,
                                       float* scaling_factors) {
  int input_size = ElementCount(*dims);
  int channel_count = dims->data[quantized_dimension];
   d62b4:	9301      	str	r3, [sp, #4]
  int per_channel_size = input_size / channel_count;
  for (int channel = 0; channel < channel_count; channel++) {
   d62b6:	2500      	movs	r5, #0
                                       int quantized_dimension,
                                       int8_t* quantized_values,
                                       float* scaling_factors) {
  int input_size = ElementCount(*dims);
  int channel_count = dims->data[quantized_dimension];
  int per_channel_size = input_size / channel_count;
   d62b8:	fb90 f6f3 	sdiv	r6, r0, r3
  for (int channel = 0; channel < channel_count; channel++) {
   d62bc:	9b01      	ldr	r3, [sp, #4]
   d62be:	429d      	cmp	r5, r3
   d62c0:	da73      	bge.n	d63aa <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x11a>
   d62c2:	4641      	mov	r1, r8
   d62c4:	2200      	movs	r2, #0
   d62c6:	f04f 0901 	mov.w	r9, #1
    float min = 0;
    float max = 0;
    int stride = 1;
    for (int i = 0; i < quantized_dimension; i++) {
   d62ca:	455a      	cmp	r2, fp
   d62cc:	da05      	bge.n	d62da <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x4a>
      stride *= dims->data[i];
   d62ce:	f851 0f04 	ldr.w	r0, [r1, #4]!
  int per_channel_size = input_size / channel_count;
  for (int channel = 0; channel < channel_count; channel++) {
    float min = 0;
    float max = 0;
    int stride = 1;
    for (int i = 0; i < quantized_dimension; i++) {
   d62d2:	3201      	adds	r2, #1
      stride *= dims->data[i];
   d62d4:	fb00 f909 	mul.w	r9, r0, r9
  int per_channel_size = input_size / channel_count;
  for (int channel = 0; channel < channel_count; channel++) {
    float min = 0;
    float max = 0;
    int stride = 1;
    for (int i = 0; i < quantized_dimension; i++) {
   d62d8:	e7f7      	b.n	d62ca <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x3a>
  int input_size = ElementCount(*dims);
  int channel_count = dims->data[quantized_dimension];
  int per_channel_size = input_size / channel_count;
  for (int channel = 0; channel < channel_count; channel++) {
    float min = 0;
    float max = 0;
   d62da:	ed9f 8a37 	vldr	s16, [pc, #220]	; d63b8 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x128>
      stride *= dims->data[i];
    }
    int channel_stride = per_channel_size / stride;
    // Calculate scales for each channel.
    for (int i = 0; i < per_channel_size; i++) {
      int idx = channel * channel_stride + i * stride;
   d62de:	fb96 f4f9 	sdiv	r4, r6, r9
   d62e2:	436c      	muls	r4, r5
   d62e4:	ea4f 0089 	mov.w	r0, r9, lsl #2
   d62e8:	eb0a 0284 	add.w	r2, sl, r4, lsl #2
    for (int i = 0; i < quantized_dimension; i++) {
      stride *= dims->data[i];
    }
    int channel_stride = per_channel_size / stride;
    // Calculate scales for each channel.
    for (int i = 0; i < per_channel_size; i++) {
   d62ec:	2100      	movs	r1, #0
                                       float* scaling_factors) {
  int input_size = ElementCount(*dims);
  int channel_count = dims->data[quantized_dimension];
  int per_channel_size = input_size / channel_count;
  for (int channel = 0; channel < channel_count; channel++) {
    float min = 0;
   d62ee:	eef0 8a48 	vmov.f32	s17, s16
    for (int i = 0; i < quantized_dimension; i++) {
      stride *= dims->data[i];
    }
    int channel_stride = per_channel_size / stride;
    // Calculate scales for each channel.
    for (int i = 0; i < per_channel_size; i++) {
   d62f2:	42b1      	cmp	r1, r6
   d62f4:	9005      	str	r0, [sp, #20]
   d62f6:	9104      	str	r1, [sp, #16]
   d62f8:	da18      	bge.n	d632c <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x9c>
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
   d62fa:	ed92 9a00 	vldr	s18, [r2]
   d62fe:	9203      	str	r2, [sp, #12]
   d6300:	eef0 0a49 	vmov.f32	s1, s18
   d6304:	eeb0 0a68 	vmov.f32	s0, s17
   d6308:	f00f f8f0 	bl	e54ec <fminf>
      max = fmaxf(max, values[idx]);
   d630c:	eef0 0a49 	vmov.f32	s1, s18
    }
    int channel_stride = per_channel_size / stride;
    // Calculate scales for each channel.
    for (int i = 0; i < per_channel_size; i++) {
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
   d6310:	eef0 8a40 	vmov.f32	s17, s0
      max = fmaxf(max, values[idx]);
   d6314:	eeb0 0a48 	vmov.f32	s0, s16
   d6318:	f00f f8ca 	bl	e54b0 <fmaxf>
    for (int i = 0; i < quantized_dimension; i++) {
      stride *= dims->data[i];
    }
    int channel_stride = per_channel_size / stride;
    // Calculate scales for each channel.
    for (int i = 0; i < per_channel_size; i++) {
   d631c:	9904      	ldr	r1, [sp, #16]
   d631e:	9a03      	ldr	r2, [sp, #12]
   d6320:	9805      	ldr	r0, [sp, #20]
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
   d6322:	eeb0 8a40 	vmov.f32	s16, s0
    for (int i = 0; i < quantized_dimension; i++) {
      stride *= dims->data[i];
    }
    int channel_stride = per_channel_size / stride;
    // Calculate scales for each channel.
    for (int i = 0; i < per_channel_size; i++) {
   d6326:	3101      	adds	r1, #1
   d6328:	4402      	add	r2, r0
   d632a:	e7e2      	b.n	d62f2 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x62>
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
   d632c:	eef0 0ac8 	vabs.f32	s1, s16
   d6330:	eeb0 0ae8 	vabs.f32	s0, s17
   d6334:	f00f f8bc 	bl	e54b0 <fmaxf>
   d6338:	ee80 0a29 	vdiv.f32	s0, s0, s19
      int idx = channel * channel_stride + i * stride;
      const int32_t quantized_value =
          static_cast<int32_t>(roundf(values[idx] / scaling_factors[channel]));
      // Clamp: just in case some odd numeric offset.
      quantized_values[idx] = fminf(
          kSymmetricInt8Scale, fmaxf(-kSymmetricInt8Scale, quantized_value));
   d633c:	9b02      	ldr	r3, [sp, #8]
   d633e:	ebc9 0004 	rsb	r0, r9, r4
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
   d6342:	2200      	movs	r2, #0
      int idx = channel * channel_stride + i * stride;
      const int32_t quantized_value =
          static_cast<int32_t>(roundf(values[idx] / scaling_factors[channel]));
      // Clamp: just in case some odd numeric offset.
      quantized_values[idx] = fminf(
          kSymmetricInt8Scale, fmaxf(-kSymmetricInt8Scale, quantized_value));
   d6344:	ebc9 0404 	rsb	r4, r9, r4
   d6348:	0080      	lsls	r0, r0, #2
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
    for (int i = 0; i < per_channel_size; i++) {
   d634a:	4611      	mov	r1, r2
      int idx = channel * channel_stride + i * stride;
      const int32_t quantized_value =
          static_cast<int32_t>(roundf(values[idx] / scaling_factors[channel]));
      // Clamp: just in case some odd numeric offset.
      quantized_values[idx] = fminf(
          kSymmetricInt8Scale, fmaxf(-kSymmetricInt8Scale, quantized_value));
   d634c:	441c      	add	r4, r3
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
   d634e:	eca7 0a01 	vstmia	r7!, {s0}
    for (int i = 0; i < per_channel_size; i++) {
   d6352:	42b1      	cmp	r1, r6
   d6354:	444a      	add	r2, r9
   d6356:	9105      	str	r1, [sp, #20]
   d6358:	da25      	bge.n	d63a6 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x116>
      int idx = channel * channel_stride + i * stride;
      const int32_t quantized_value =
          static_cast<int32_t>(roundf(values[idx] / scaling_factors[channel]));
   d635a:	eb00 0e82 	add.w	lr, r0, r2, lsl #2
   d635e:	44d6      	add	lr, sl
   d6360:	ed9e 0a00 	vldr	s0, [lr]
   d6364:	ed57 7a01 	vldr	s15, [r7, #-4]
   d6368:	9204      	str	r2, [sp, #16]
   d636a:	ee80 0a27 	vdiv.f32	s0, s0, s15
   d636e:	9003      	str	r0, [sp, #12]
   d6370:	f00f f8f6 	bl	e5560 <roundf>
      // Clamp: just in case some odd numeric offset.
      quantized_values[idx] = fminf(
   d6374:	eefd 0ac0 	vcvt.s32.f32	s1, s0
   d6378:	ed9f 0a10 	vldr	s0, [pc, #64]	; d63bc <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x12c>
   d637c:	eef8 0ae0 	vcvt.f32.s32	s1, s1
   d6380:	f00f f896 	bl	e54b0 <fmaxf>
   d6384:	eef0 0a40 	vmov.f32	s1, s0
   d6388:	ed9f 0a0a 	vldr	s0, [pc, #40]	; d63b4 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x124>
   d638c:	f00f f8ae 	bl	e54ec <fminf>
          kSymmetricInt8Scale, fmaxf(-kSymmetricInt8Scale, quantized_value));
   d6390:	eefd 7ac0 	vcvt.s32.f32	s15, s0
   d6394:	9a04      	ldr	r2, [sp, #16]
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
    for (int i = 0; i < per_channel_size; i++) {
   d6396:	9905      	ldr	r1, [sp, #20]
   d6398:	9803      	ldr	r0, [sp, #12]
      int idx = channel * channel_stride + i * stride;
      const int32_t quantized_value =
          static_cast<int32_t>(roundf(values[idx] / scaling_factors[channel]));
      // Clamp: just in case some odd numeric offset.
      quantized_values[idx] = fminf(
          kSymmetricInt8Scale, fmaxf(-kSymmetricInt8Scale, quantized_value));
   d639a:	ee17 ea90 	vmov	lr, s15
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
    for (int i = 0; i < per_channel_size; i++) {
   d639e:	3101      	adds	r1, #1
      int idx = channel * channel_stride + i * stride;
      const int32_t quantized_value =
          static_cast<int32_t>(roundf(values[idx] / scaling_factors[channel]));
      // Clamp: just in case some odd numeric offset.
      quantized_values[idx] = fminf(
          kSymmetricInt8Scale, fmaxf(-kSymmetricInt8Scale, quantized_value));
   d63a0:	f804 e002 	strb.w	lr, [r4, r2]
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
    for (int i = 0; i < per_channel_size; i++) {
   d63a4:	e7d5      	b.n	d6352 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0xc2>
                                       int8_t* quantized_values,
                                       float* scaling_factors) {
  int input_size = ElementCount(*dims);
  int channel_count = dims->data[quantized_dimension];
  int per_channel_size = input_size / channel_count;
  for (int channel = 0; channel < channel_count; channel++) {
   d63a6:	3501      	adds	r5, #1
   d63a8:	e788      	b.n	d62bc <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x2c>
      // Clamp: just in case some odd numeric offset.
      quantized_values[idx] = fminf(
          kSymmetricInt8Scale, fmaxf(-kSymmetricInt8Scale, quantized_value));
    }
  }
}
   d63aa:	b007      	add	sp, #28
   d63ac:	ecbd 8b04 	vpop	{d8-d9}
   d63b0:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   d63b4:	42fe0000 	.word	0x42fe0000
   d63b8:	00000000 	.word	0x00000000
   d63bc:	c2fe0000 	.word	0xc2fe0000

000d63c0 <_ZN6tflite19SymmetricDequantizeEPKaifPf>:
                          scaling_factor);
}

void SymmetricDequantize(const int8_t* values, const int size,
                         const float dequantization_scale,
                         float* dequantized_values) {
   d63c0:	b510      	push	{r4, lr}
   d63c2:	4603      	mov	r3, r0
  for (int i = 0; i < size; ++i) {
   d63c4:	1a1c      	subs	r4, r3, r0
   d63c6:	42a1      	cmp	r1, r4
   d63c8:	dd0a      	ble.n	d63e0 <_ZN6tflite19SymmetricDequantizeEPKaifPf+0x20>
    dequantized_values[i] = values[i] * dequantization_scale;
   d63ca:	f913 4b01 	ldrsb.w	r4, [r3], #1
   d63ce:	ee07 4a90 	vmov	s15, r4
   d63d2:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   d63d6:	ee67 7a80 	vmul.f32	s15, s15, s0
   d63da:	ece2 7a01 	vstmia	r2!, {s15}
}

void SymmetricDequantize(const int8_t* values, const int size,
                         const float dequantization_scale,
                         float* dequantized_values) {
  for (int i = 0; i < size; ++i) {
   d63de:	e7f1      	b.n	d63c4 <_ZN6tflite19SymmetricDequantizeEPKaifPf+0x4>
   d63e0:	bd10      	pop	{r4, pc}

000d63e2 <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>:
#include "tensorflow/lite/experimental/micro/memory_helpers.h"

namespace tflite {

uint8_t* SimpleMemoryAllocator::AllocateFromTail(size_t size,
                                                 size_t alignment) {
   d63e2:	b538      	push	{r3, r4, r5, lr}
  uint8_t* previous_free = (data_ + data_size_max_) - data_size_;
   d63e4:	6804      	ldr	r4, [r0, #0]
   d63e6:	6843      	ldr	r3, [r0, #4]
   d63e8:	1b1b      	subs	r3, r3, r4
   d63ea:	6884      	ldr	r4, [r0, #8]
   d63ec:	441c      	add	r4, r3
#include "tensorflow/lite/experimental/micro/memory_helpers.h"

namespace tflite {

uint8_t* SimpleMemoryAllocator::AllocateFromTail(size_t size,
                                                 size_t alignment) {
   d63ee:	4605      	mov	r5, r0
  uint8_t* previous_free = (data_ + data_size_max_) - data_size_;
  uint8_t* current_data = previous_free - size;
  uint8_t* aligned_result = AlignPointerDown(current_data, alignment);
   d63f0:	1a60      	subs	r0, r4, r1
   d63f2:	4611      	mov	r1, r2
   d63f4:	f7ff f928 	bl	d5648 <_ZN6tflite16AlignPointerDownEPhj>
  size_t aligned_size = (previous_free - aligned_result);
  if ((data_size_ + aligned_size) > data_size_max_) {
   d63f8:	682b      	ldr	r3, [r5, #0]
   d63fa:	1a24      	subs	r4, r4, r0
   d63fc:	441c      	add	r4, r3
   d63fe:	686b      	ldr	r3, [r5, #4]
   d6400:	429c      	cmp	r4, r3
    // TODO(petewarden): Add error reporting beyond returning null!
    return nullptr;
  }
  data_size_ += aligned_size;
   d6402:	bf94      	ite	ls
   d6404:	602c      	strls	r4, [r5, #0]
  uint8_t* current_data = previous_free - size;
  uint8_t* aligned_result = AlignPointerDown(current_data, alignment);
  size_t aligned_size = (previous_free - aligned_result);
  if ((data_size_ + aligned_size) > data_size_max_) {
    // TODO(petewarden): Add error reporting beyond returning null!
    return nullptr;
   d6406:	2000      	movhi	r0, #0
  }
  data_size_ += aligned_size;
  return aligned_result;
}
   d6408:	bd38      	pop	{r3, r4, r5, pc}
	...

000d640c <DebugLog>:
#define DEBUG_SERIAL_OBJECT (Serial)
#endif

// On Arduino platforms, we set up a serial port and write to it for debug
// logging.
extern "C" void DebugLog(const char* s) {
   d640c:	b538      	push	{r3, r4, r5, lr}
  static bool is_initialized = false;
  if (!is_initialized) {
   d640e:	4c09      	ldr	r4, [pc, #36]	; (d6434 <DebugLog+0x28>)
   d6410:	7823      	ldrb	r3, [r4, #0]
#define DEBUG_SERIAL_OBJECT (Serial)
#endif

// On Arduino platforms, we set up a serial port and write to it for debug
// logging.
extern "C" void DebugLog(const char* s) {
   d6412:	4605      	mov	r5, r0
  static bool is_initialized = false;
  if (!is_initialized) {
   d6414:	b93b      	cbnz	r3, d6426 <DebugLog+0x1a>
    DEBUG_SERIAL_OBJECT.begin(9600);
   d6416:	f00e fbd9 	bl	e4bcc <_Z16_fetch_usbserialv>
   d641a:	f44f 5116 	mov.w	r1, #9600	; 0x2580
   d641e:	f00e fbc9 	bl	e4bb4 <_ZN9USBSerial5beginEl>
    is_initialized = true;
   d6422:	2301      	movs	r3, #1
   d6424:	7023      	strb	r3, [r4, #0]
  }
  DEBUG_SERIAL_OBJECT.print(s);
   d6426:	f00e fbd1 	bl	e4bcc <_Z16_fetch_usbserialv>
   d642a:	4629      	mov	r1, r5
}
   d642c:	e8bd 4038 	ldmia.w	sp!, {r3, r4, r5, lr}
  static bool is_initialized = false;
  if (!is_initialized) {
    DEBUG_SERIAL_OBJECT.begin(9600);
    is_initialized = true;
  }
  DEBUG_SERIAL_OBJECT.print(s);
   d6430:	f00e ba68 	b.w	e4904 <_ZN5Print5printEPKc>
   d6434:	2003db90 	.word	0x2003db90

000d6438 <_GLOBAL__sub_I_DebugLog>:
   d6438:	f00d bde4 	b.w	e4004 <HAL_Pin_Map>

000d643c <_ZN6tflite3ops5micro3add4InitEP13TfLiteContextPKcj>:
  int32 output_offset;
};

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
   d643c:	2000      	movs	r0, #0
   d643e:	4770      	bx	lr

000d6440 <_ZN6tflite3ops5micro3add4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
   d6440:	4770      	bx	lr

000d6442 <_ZN6tflite3ops5micro3add7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   d6442:	2000      	movs	r0, #0
   d6444:	4770      	bx	lr

000d6446 <_ZN6tflite12RuntimeShapeD1Ev>:
  bool operator==(const RuntimeShape& comp) const {
    return this->size_ == comp.size_ &&
           std::memcmp(DimsData(), comp.DimsData(), size_ * sizeof(int32)) == 0;
  }

  ~RuntimeShape() {
   d6446:	b510      	push	{r4, lr}
    if (size_ > kMaxSmallSize) {
   d6448:	6803      	ldr	r3, [r0, #0]
   d644a:	2b04      	cmp	r3, #4
  bool operator==(const RuntimeShape& comp) const {
    return this->size_ == comp.size_ &&
           std::memcmp(DimsData(), comp.DimsData(), size_ * sizeof(int32)) == 0;
  }

  ~RuntimeShape() {
   d644c:	4604      	mov	r4, r0
    if (size_ > kMaxSmallSize) {
   d644e:	dd03      	ble.n	d6458 <_ZN6tflite12RuntimeShapeD1Ev+0x12>
#ifdef TF_LITE_STATIC_MEMORY
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
   d6450:	6840      	ldr	r0, [r0, #4]
   d6452:	b108      	cbz	r0, d6458 <_ZN6tflite12RuntimeShapeD1Ev+0x12>
   d6454:	f7fd fe25 	bl	d40a2 <_ZdaPv>
#endif  // TF_LITE_STATIC_MEMORY
    }
  }
   d6458:	4620      	mov	r0, r4
   d645a:	bd10      	pop	{r4, pc}

000d645c <_ZNK6tflite12RuntimeShape4DimsEi>:

  inline int32 DimensionsCount() const { return size_; }
  inline int32 Dims(int i) const {
    TFLITE_DCHECK_GE(i, 0);
   d645c:	2900      	cmp	r1, #0
#endif  // TF_LITE_STATIC_MEMORY
    }
  }

  inline int32 DimensionsCount() const { return size_; }
  inline int32 Dims(int i) const {
   d645e:	b508      	push	{r3, lr}
    TFLITE_DCHECK_GE(i, 0);
   d6460:	da01      	bge.n	d6466 <_ZNK6tflite12RuntimeShape4DimsEi+0xa>
   d6462:	f00d ff63 	bl	e432c <abort>
    TFLITE_DCHECK_LT(i, size_);
   d6466:	6803      	ldr	r3, [r0, #0]
   d6468:	428b      	cmp	r3, r1
   d646a:	ddfa      	ble.n	d6462 <_ZNK6tflite12RuntimeShape4DimsEi+0x6>
    return size_ > kMaxSmallSize ? dims_pointer_[i] : dims_[i];
   d646c:	2b04      	cmp	r3, #4
   d646e:	bfc7      	ittee	gt
   d6470:	6843      	ldrgt	r3, [r0, #4]
   d6472:	f853 0021 	ldrgt.w	r0, [r3, r1, lsl #2]
   d6476:	eb00 0081 	addle.w	r0, r0, r1, lsl #2
   d647a:	6840      	ldrle	r0, [r0, #4]
  }
   d647c:	bd08      	pop	{r3, pc}

000d647e <_ZN6tflite12RuntimeShape6SetDimEil>:
  inline void SetDim(int i, int32 val) {
    TFLITE_DCHECK_GE(i, 0);
   d647e:	2900      	cmp	r1, #0
  inline int32 Dims(int i) const {
    TFLITE_DCHECK_GE(i, 0);
    TFLITE_DCHECK_LT(i, size_);
    return size_ > kMaxSmallSize ? dims_pointer_[i] : dims_[i];
  }
  inline void SetDim(int i, int32 val) {
   d6480:	b508      	push	{r3, lr}
    TFLITE_DCHECK_GE(i, 0);
   d6482:	da01      	bge.n	d6488 <_ZN6tflite12RuntimeShape6SetDimEil+0xa>
   d6484:	f00d ff52 	bl	e432c <abort>
    TFLITE_DCHECK_LT(i, size_);
   d6488:	6803      	ldr	r3, [r0, #0]
   d648a:	428b      	cmp	r3, r1
   d648c:	ddfa      	ble.n	d6484 <_ZN6tflite12RuntimeShape6SetDimEil+0x6>
    if (size_ > kMaxSmallSize) {
   d648e:	2b04      	cmp	r3, #4
      dims_pointer_[i] = val;
   d6490:	bfcb      	itete	gt
   d6492:	6843      	ldrgt	r3, [r0, #4]
    } else {
      dims_[i] = val;
   d6494:	eb00 0081 	addle.w	r0, r0, r1, lsl #2
  }
  inline void SetDim(int i, int32 val) {
    TFLITE_DCHECK_GE(i, 0);
    TFLITE_DCHECK_LT(i, size_);
    if (size_ > kMaxSmallSize) {
      dims_pointer_[i] = val;
   d6498:	f843 2021 	strgt.w	r2, [r3, r1, lsl #2]
    } else {
      dims_[i] = val;
   d649c:	6042      	strle	r2, [r0, #4]
   d649e:	bd08      	pop	{r3, pc}

000d64a0 <_ZN6tflite12RuntimeShape6ResizeEi>:
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  // The caller must ensure that the shape is no bigger than 4-D.
  inline const int32* DimsDataUpTo4D() const { return dims_; }

  inline void Resize(int dimensions_count) {
   d64a0:	b538      	push	{r3, r4, r5, lr}
    if (size_ > kMaxSmallSize) {
   d64a2:	6803      	ldr	r3, [r0, #0]
   d64a4:	2b04      	cmp	r3, #4
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  // The caller must ensure that the shape is no bigger than 4-D.
  inline const int32* DimsDataUpTo4D() const { return dims_; }

  inline void Resize(int dimensions_count) {
   d64a6:	4605      	mov	r5, r0
   d64a8:	460c      	mov	r4, r1
    if (size_ > kMaxSmallSize) {
   d64aa:	dd03      	ble.n	d64b4 <_ZN6tflite12RuntimeShape6ResizeEi+0x14>
#ifdef TF_LITE_STATIC_MEMORY
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
   d64ac:	6840      	ldr	r0, [r0, #4]
   d64ae:	b108      	cbz	r0, d64b4 <_ZN6tflite12RuntimeShape6ResizeEi+0x14>
   d64b0:	f7fd fdf7 	bl	d40a2 <_ZdaPv>
#endif  // TF_LITE_STATIC_MEMORY
    }
    size_ = dimensions_count;
    if (dimensions_count > kMaxSmallSize) {
   d64b4:	2c04      	cmp	r4, #4
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
#endif  // TF_LITE_STATIC_MEMORY
    }
    size_ = dimensions_count;
   d64b6:	602c      	str	r4, [r5, #0]
    if (dimensions_count > kMaxSmallSize) {
   d64b8:	dd08      	ble.n	d64cc <_ZN6tflite12RuntimeShape6ResizeEi+0x2c>
#ifdef TF_LITE_STATIC_MEMORY
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      dims_pointer_ = new int32[dimensions_count];
   d64ba:	f1b4 5ffe 	cmp.w	r4, #532676608	; 0x1fc00000
   d64be:	bfd4      	ite	le
   d64c0:	00a0      	lslle	r0, r4, #2
   d64c2:	f04f 30ff 	movgt.w	r0, #4294967295	; 0xffffffff
   d64c6:	f7fd fde8 	bl	d409a <_Znaj>
   d64ca:	6068      	str	r0, [r5, #4]
   d64cc:	bd38      	pop	{r3, r4, r5, pc}

000d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>:

 private:
  // For use only by ExtendedShape(), written to guarantee (return-value) copy
  // elision in C++17.
  // This creates a shape padded to the desired size with the specified value.
  RuntimeShape(int new_shape_size, const RuntimeShape& shape, int pad_value)
   d64ce:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
      : size_(0) {
   d64d2:	2500      	movs	r5, #0
   d64d4:	6005      	str	r5, [r0, #0]

 private:
  // For use only by ExtendedShape(), written to guarantee (return-value) copy
  // elision in C++17.
  // This creates a shape padded to the desired size with the specified value.
  RuntimeShape(int new_shape_size, const RuntimeShape& shape, int pad_value)
   d64d6:	4698      	mov	r8, r3
      : size_(0) {
    // If the following check fails, it is likely because a 4D-only kernel is
    // being used with an array of larger dimension count.
    TFLITE_CHECK_GE(new_shape_size, shape.DimensionsCount());
   d64d8:	6813      	ldr	r3, [r2, #0]
   d64da:	428b      	cmp	r3, r1

 private:
  // For use only by ExtendedShape(), written to guarantee (return-value) copy
  // elision in C++17.
  // This creates a shape padded to the desired size with the specified value.
  RuntimeShape(int new_shape_size, const RuntimeShape& shape, int pad_value)
   d64dc:	4606      	mov	r6, r0
   d64de:	460f      	mov	r7, r1
   d64e0:	4614      	mov	r4, r2
      : size_(0) {
    // If the following check fails, it is likely because a 4D-only kernel is
    // being used with an array of larger dimension count.
    TFLITE_CHECK_GE(new_shape_size, shape.DimensionsCount());
   d64e2:	dd01      	ble.n	d64e8 <_ZN6tflite12RuntimeShapeC1EiRKS0_i+0x1a>
   d64e4:	f00d ff22 	bl	e432c <abort>
    Resize(new_shape_size);
   d64e8:	f7ff ffda 	bl	d64a0 <_ZN6tflite12RuntimeShape6ResizeEi>
    const int size_increase = new_shape_size - shape.DimensionsCount();
   d64ec:	6820      	ldr	r0, [r4, #0]
   d64ee:	1a3f      	subs	r7, r7, r0
    for (int i = 0; i < size_increase; ++i) {
   d64f0:	42bd      	cmp	r5, r7
   d64f2:	da06      	bge.n	d6502 <_ZN6tflite12RuntimeShapeC1EiRKS0_i+0x34>
      SetDim(i, pad_value);
   d64f4:	4629      	mov	r1, r5
   d64f6:	4642      	mov	r2, r8
   d64f8:	4630      	mov	r0, r6
   d64fa:	f7ff ffc0 	bl	d647e <_ZN6tflite12RuntimeShape6SetDimEil>
    // If the following check fails, it is likely because a 4D-only kernel is
    // being used with an array of larger dimension count.
    TFLITE_CHECK_GE(new_shape_size, shape.DimensionsCount());
    Resize(new_shape_size);
    const int size_increase = new_shape_size - shape.DimensionsCount();
    for (int i = 0; i < size_increase; ++i) {
   d64fe:	3501      	adds	r5, #1
   d6500:	e7f6      	b.n	d64f0 <_ZN6tflite12RuntimeShapeC1EiRKS0_i+0x22>
      dims_[i] = val;
    }
  }

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d6502:	6833      	ldr	r3, [r6, #0]
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d6504:	6822      	ldr	r2, [r4, #0]
      dims_[i] = val;
    }
  }

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d6506:	2b04      	cmp	r3, #4
   d6508:	bfcc      	ite	gt
   d650a:	6870      	ldrgt	r0, [r6, #4]
   d650c:	1d30      	addle	r0, r6, #4
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d650e:	2a04      	cmp	r2, #4
    Resize(new_shape_size);
    const int size_increase = new_shape_size - shape.DimensionsCount();
    for (int i = 0; i < size_increase; ++i) {
      SetDim(i, pad_value);
    }
    std::memcpy(DimsData() + size_increase, shape.DimsData(),
   d6510:	eb00 0087 	add.w	r0, r0, r7, lsl #2

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d6514:	bfcc      	ite	gt
   d6516:	6861      	ldrgt	r1, [r4, #4]
   d6518:	1d21      	addle	r1, r4, #4
    const int size_increase = new_shape_size - shape.DimensionsCount();
    for (int i = 0; i < size_increase; ++i) {
      SetDim(i, pad_value);
    }
    std::memcpy(DimsData() + size_increase, shape.DimsData(),
                sizeof(int32) * shape.DimensionsCount());
   d651a:	0092      	lsls	r2, r2, #2
   d651c:	f011 f961 	bl	e77e2 <memcpy>
  }
   d6520:	4630      	mov	r0, r6
   d6522:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}

000d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>:
    }
  }
  return offset;
}

inline int Offset(const RuntimeShape& shape, int i0, int i1, int i2, int i3) {
   d6526:	b570      	push	{r4, r5, r6, lr}
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), 4);
   d6528:	6805      	ldr	r5, [r0, #0]
    }
  }
  return offset;
}

inline int Offset(const RuntimeShape& shape, int i0, int i1, int i2, int i3) {
   d652a:	9c04      	ldr	r4, [sp, #16]
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), 4);
   d652c:	2d04      	cmp	r5, #4
   d652e:	d001      	beq.n	d6534 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xe>
   d6530:	f00d fefc 	bl	e432c <abort>
  const int* dims_data = reinterpret_cast<const int*>(shape.DimsDataUpTo4D());
  TFLITE_DCHECK(i0 >= 0 && i0 < dims_data[0]);
   d6534:	2900      	cmp	r1, #0
   d6536:	dbfb      	blt.n	d6530 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
   d6538:	6845      	ldr	r5, [r0, #4]
   d653a:	42a9      	cmp	r1, r5
   d653c:	daf8      	bge.n	d6530 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
  TFLITE_DCHECK(i1 >= 0 && i1 < dims_data[1]);
   d653e:	2a00      	cmp	r2, #0
   d6540:	dbf6      	blt.n	d6530 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
   d6542:	6886      	ldr	r6, [r0, #8]
   d6544:	42b2      	cmp	r2, r6
   d6546:	daf3      	bge.n	d6530 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
  TFLITE_DCHECK(i2 >= 0 && i2 < dims_data[2]);
   d6548:	2b00      	cmp	r3, #0
   d654a:	dbf1      	blt.n	d6530 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
   d654c:	68c5      	ldr	r5, [r0, #12]
   d654e:	42ab      	cmp	r3, r5
   d6550:	daee      	bge.n	d6530 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
  TFLITE_DCHECK(i3 >= 0 && i3 < dims_data[3]);
   d6552:	2c00      	cmp	r4, #0
   d6554:	dbec      	blt.n	d6530 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
   d6556:	6900      	ldr	r0, [r0, #16]
   d6558:	4284      	cmp	r4, r0
   d655a:	dae9      	bge.n	d6530 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
  return ((i0 * dims_data[1] + i1) * dims_data[2] + i2) * dims_data[3] + i3;
   d655c:	fb06 2101 	mla	r1, r6, r1, r2
   d6560:	fb05 3301 	mla	r3, r5, r1, r3
}
   d6564:	fb00 4003 	mla	r0, r0, r3, r4
   d6568:	bd70      	pop	{r4, r5, r6, pc}

000d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>:
  return shape.FlatSize();
}

inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0,
                            const RuntimeShape& check_shape_1) {
   d656a:	e92d 43f8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, lr}
   d656e:	6805      	ldr	r5, [r0, #0]
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   d6570:	680b      	ldr	r3, [r1, #0]
   d6572:	429d      	cmp	r5, r3
  return shape.FlatSize();
}

inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0,
                            const RuntimeShape& check_shape_1) {
   d6574:	4604      	mov	r4, r0
   d6576:	4688      	mov	r8, r1
   d6578:	4617      	mov	r7, r2
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   d657a:	d101      	bne.n	d6580 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x16>
   d657c:	2600      	movs	r6, #0
   d657e:	e00d      	b.n	d659c <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x32>
   d6580:	f00d fed4 	bl	e432c <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   d6584:	4631      	mov	r1, r6
   d6586:	4620      	mov	r0, r4
   d6588:	f7ff ff68 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d658c:	4631      	mov	r1, r6
   d658e:	4681      	mov	r9, r0
   d6590:	4640      	mov	r0, r8
   d6592:	f7ff ff63 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d6596:	4581      	cmp	r9, r0
   d6598:	d1f2      	bne.n	d6580 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x16>
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0,
                            const RuntimeShape& check_shape_1) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   d659a:	3601      	adds	r6, #1
   d659c:	42ae      	cmp	r6, r5
   d659e:	dbf1      	blt.n	d6584 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x1a>

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   d65a0:	683b      	ldr	r3, [r7, #0]
   d65a2:	429d      	cmp	r5, r3
   d65a4:	d1ec      	bne.n	d6580 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x16>
   d65a6:	2600      	movs	r6, #0
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   d65a8:	42b5      	cmp	r5, r6
   d65aa:	dd0c      	ble.n	d65c6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x5c>
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   d65ac:	4631      	mov	r1, r6
   d65ae:	4620      	mov	r0, r4
   d65b0:	f7ff ff54 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d65b4:	4631      	mov	r1, r6
   d65b6:	4680      	mov	r8, r0
   d65b8:	4638      	mov	r0, r7
   d65ba:	f7ff ff4f 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d65be:	4580      	cmp	r8, r0
   d65c0:	d1de      	bne.n	d6580 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x16>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   d65c2:	3601      	adds	r6, #1
   d65c4:	e7f0      	b.n	d65a8 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x3e>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d65c6:	2d04      	cmp	r5, #4
   d65c8:	bfcc      	ite	gt
   d65ca:	6864      	ldrgt	r4, [r4, #4]
   d65cc:	3404      	addle	r4, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   d65ce:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
   d65d0:	2001      	movs	r0, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   d65d2:	429d      	cmp	r5, r3
   d65d4:	dd04      	ble.n	d65e0 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x76>
      buffer_size *= dims_data[i];
   d65d6:	f854 2023 	ldr.w	r2, [r4, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   d65da:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
   d65dc:	4350      	muls	r0, r2
   d65de:	e7f8      	b.n	d65d2 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x68>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
  }
  return MatchingFlatSize(shape, check_shape_1);
}
   d65e0:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}

000d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>:
// This function implements the same computation as the ARMv7 NEON VQRDMULH
// instruction.
template <>
inline std::int32_t SaturatingRoundingDoublingHighMul(std::int32_t a,
                                                      std::int32_t b) {
  bool overflow = a == b && a == std::numeric_limits<std::int32_t>::min();
   d65e4:	4288      	cmp	r0, r1
  }
#endif
}

inline int32 MultiplyByQuantizedMultiplierSmallerThanOneExp(
    int32 x, int32 quantized_multiplier, int left_shift) {
   d65e6:	b570      	push	{r4, r5, r6, lr}
   d65e8:	d104      	bne.n	d65f4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x10>
   d65ea:	f100 4300 	add.w	r3, r0, #2147483648	; 0x80000000
   d65ee:	425e      	negs	r6, r3
   d65f0:	415e      	adcs	r6, r3
   d65f2:	e000      	b.n	d65f6 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x12>
   d65f4:	2600      	movs	r6, #0
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
   d65f6:	fb80 4501 	smull	r4, r5, r0, r1
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
   d65fa:	2c00      	cmp	r4, #0
   d65fc:	f175 0300 	sbcs.w	r3, r5, #0
   d6600:	4b1c      	ldr	r3, [pc, #112]	; (d6674 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x90>)
   d6602:	bfa8      	it	ge
   d6604:	f04f 4380 	movge.w	r3, #1073741824	; 0x40000000
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
   d6608:	b97e      	cbnz	r6, d662a <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x46>
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
   d660a:	18e4      	adds	r4, r4, r3
   d660c:	eb45 75e3 	adc.w	r5, r5, r3, asr #31
   d6610:	2c00      	cmp	r4, #0
   d6612:	f175 0300 	sbcs.w	r3, r5, #0
   d6616:	da04      	bge.n	d6622 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x3e>
   d6618:	f06f 4000 	mvn.w	r0, #2147483648	; 0x80000000
   d661c:	2100      	movs	r1, #0
   d661e:	1824      	adds	r4, r4, r0
   d6620:	414d      	adcs	r5, r1
   d6622:	0fe4      	lsrs	r4, r4, #31
   d6624:	ea44 0445 	orr.w	r4, r4, r5, lsl #1
   d6628:	e001      	b.n	d662e <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x4a>
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
   d662a:	f06f 4400 	mvn.w	r4, #2147483648	; 0x80000000
  using gemmlowp::RoundingDivideByPOT;
  using gemmlowp::SaturatingRoundingDoublingHighMul;
  return RoundingDivideByPOT(
   d662e:	4255      	negs	r5, r2

// Correctly-rounded-to-nearest division by a power-of-two.
// Also known as a rounding arithmetic right shift.
template <typename IntegerType>
inline IntegerType RoundingDivideByPOT(IntegerType x, int exponent) {
  assert(exponent >= 0);
   d6630:	2d00      	cmp	r5, #0
   d6632:	da04      	bge.n	d663e <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x5a>
   d6634:	4b10      	ldr	r3, [pc, #64]	; (d6678 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x94>)
   d6636:	4a11      	ldr	r2, [pc, #68]	; (d667c <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x98>)
   d6638:	f44f 71b3 	mov.w	r1, #358	; 0x166
   d663c:	e005      	b.n	d664a <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x66>
  assert(exponent <= 31);
   d663e:	2d1f      	cmp	r5, #31
   d6640:	dd06      	ble.n	d6650 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x6c>
   d6642:	4b0f      	ldr	r3, [pc, #60]	; (d6680 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x9c>)
   d6644:	4a0d      	ldr	r2, [pc, #52]	; (d667c <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x98>)
   d6646:	f240 1167 	movw	r1, #359	; 0x167
   d664a:	480e      	ldr	r0, [pc, #56]	; (d6684 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0xa0>)
   d664c:	f00d fe7e 	bl	e434c <__assert_func>
  const IntegerType mask = Dup<IntegerType>((1ll << exponent) - 1);
   d6650:	462a      	mov	r2, r5
   d6652:	2001      	movs	r0, #1
   d6654:	2100      	movs	r1, #0
   d6656:	f010 fb61 	bl	e6d1c <__aeabi_llsl>
   d665a:	3801      	subs	r0, #1
      SaturatingRoundingDoublingHighMul(x, quantized_multiplier), -left_shift);
   d665c:	ea00 0304 	and.w	r3, r0, r4
   d6660:	1040      	asrs	r0, r0, #1
   d6662:	eb00 70d4 	add.w	r0, r0, r4, lsr #31
   d6666:	412c      	asrs	r4, r5
}
   d6668:	4283      	cmp	r3, r0
   d666a:	bfd4      	ite	le
   d666c:	4620      	movle	r0, r4
   d666e:	1c60      	addgt	r0, r4, #1
   d6670:	bd70      	pop	{r4, r5, r6, pc}
   d6672:	bf00      	nop
   d6674:	c0000001 	.word	0xc0000001
   d6678:	000e9684 	.word	0x000e9684
   d667c:	000e9773 	.word	0x000e9773
   d6680:	000e9731 	.word	0x000e9731
   d6684:	000e9692 	.word	0x000e9692

000d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>:
// DO NOT USE THIS FUNCTION FOR NEW FUNCTIONALITY BEYOND IMPLEMENTING
// BROADCASTING.
//
// Same as Offset(), except takes as NdArrayDesc<N> instead of Dims<N>.
inline int SubscriptToIndex(const NdArrayDesc<4>& desc, int i0, int i1, int i2,
                            int i3) {
   d6688:	b570      	push	{r4, r5, r6, lr}
  TFLITE_DCHECK(i0 >= 0 && i0 < desc.extents[0]);
   d668a:	2900      	cmp	r1, #0
// DO NOT USE THIS FUNCTION FOR NEW FUNCTIONALITY BEYOND IMPLEMENTING
// BROADCASTING.
//
// Same as Offset(), except takes as NdArrayDesc<N> instead of Dims<N>.
inline int SubscriptToIndex(const NdArrayDesc<4>& desc, int i0, int i1, int i2,
                            int i3) {
   d668c:	9c04      	ldr	r4, [sp, #16]
  TFLITE_DCHECK(i0 >= 0 && i0 < desc.extents[0]);
   d668e:	db02      	blt.n	d6696 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
   d6690:	6805      	ldr	r5, [r0, #0]
   d6692:	42a9      	cmp	r1, r5
   d6694:	db01      	blt.n	d669a <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0x12>
   d6696:	f00d fe49 	bl	e432c <abort>
  TFLITE_DCHECK(i1 >= 0 && i1 < desc.extents[1]);
   d669a:	2a00      	cmp	r2, #0
   d669c:	dbfb      	blt.n	d6696 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
   d669e:	6845      	ldr	r5, [r0, #4]
   d66a0:	42aa      	cmp	r2, r5
   d66a2:	daf8      	bge.n	d6696 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
  TFLITE_DCHECK(i2 >= 0 && i2 < desc.extents[2]);
   d66a4:	2b00      	cmp	r3, #0
   d66a6:	dbf6      	blt.n	d6696 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
   d66a8:	6885      	ldr	r5, [r0, #8]
   d66aa:	42ab      	cmp	r3, r5
   d66ac:	daf3      	bge.n	d6696 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
  TFLITE_DCHECK(i3 >= 0 && i3 < desc.extents[3]);
   d66ae:	2c00      	cmp	r4, #0
   d66b0:	dbf1      	blt.n	d6696 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
   d66b2:	68c5      	ldr	r5, [r0, #12]
   d66b4:	42ac      	cmp	r4, r5
   d66b6:	daee      	bge.n	d6696 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
  return i0 * desc.strides[0] + i1 * desc.strides[1] + i2 * desc.strides[2] +
         i3 * desc.strides[3];
   d66b8:	6945      	ldr	r5, [r0, #20]
   d66ba:	6906      	ldr	r6, [r0, #16]
   d66bc:	4355      	muls	r5, r2
   d66be:	6982      	ldr	r2, [r0, #24]
   d66c0:	69c0      	ldr	r0, [r0, #28]
   d66c2:	fb06 5101 	mla	r1, r6, r1, r5
   d66c6:	fb02 1303 	mla	r3, r2, r3, r1
}
   d66ca:	fb00 3004 	mla	r0, r0, r4, r3
   d66ce:	bd70      	pop	{r4, r5, r6, pc}

000d66d0 <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph>:
}

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const uint8* input1_data,
                const RuntimeShape& input2_shape, const uint8* input2_data,
                const RuntimeShape& output_shape, uint8* output_data) {
   d66d0:	e92d 4ff8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d66d4:	4604      	mov	r4, r0
   d66d6:	4690      	mov	r8, r2
   d66d8:	4608      	mov	r0, r1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
   d66da:	6b22      	ldr	r2, [r4, #48]	; 0x30
   d66dc:	6ae1      	ldr	r1, [r4, #44]	; 0x2c
}

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const uint8* input1_data,
                const RuntimeShape& input2_shape, const uint8* input2_data,
                const RuntimeShape& output_shape, uint8* output_data) {
   d66de:	9f0a      	ldr	r7, [sp, #40]	; 0x28
   d66e0:	9e0c      	ldr	r6, [sp, #48]	; 0x30
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
   d66e2:	4291      	cmp	r1, r2
   d66e4:	dd01      	ble.n	d66ea <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1a>

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const uint8* input1_data,
                const RuntimeShape& input2_shape, const uint8* input2_data,
                const RuntimeShape& output_shape, uint8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
   d66e6:	f00d fe21 	bl	e432c <abort>
                   params.quantized_activation_max);
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d66ea:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   d66ec:	4619      	mov	r1, r3
   d66ee:	f7ff ff3c 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>

  TFLITE_DCHECK_GT(params.input1_offset, -256);
   d66f2:	6862      	ldr	r2, [r4, #4]
   d66f4:	f112 0fff 	cmn.w	r2, #255	; 0xff
                const RuntimeShape& input2_shape, const uint8* input2_data,
                const RuntimeShape& output_shape, uint8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d66f8:	4681      	mov	r9, r0

  TFLITE_DCHECK_GT(params.input1_offset, -256);
   d66fa:	dbf4      	blt.n	d66e6 <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x16>
  TFLITE_DCHECK_GT(params.input2_offset, -256);
  TFLITE_DCHECK_LT(params.input1_offset, 256);
   d66fc:	2aff      	cmp	r2, #255	; 0xff
                   params.quantized_activation_max);
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);

  TFLITE_DCHECK_GT(params.input1_offset, -256);
  TFLITE_DCHECK_GT(params.input2_offset, -256);
   d66fe:	68a3      	ldr	r3, [r4, #8]
  TFLITE_DCHECK_LT(params.input1_offset, 256);
   d6700:	dcf1      	bgt.n	d66e6 <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x16>
  TFLITE_DCHECK_LT(params.input2_offset, 256);
   d6702:	33ff      	adds	r3, #255	; 0xff
   d6704:	f5b3 7fff 	cmp.w	r3, #510	; 0x1fe
   d6708:	d8ed      	bhi.n	d66e6 <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x16>
   d670a:	2500      	movs	r5, #0
  TFLITE_DCHECK_GT(params.input1_offset, -256);
  TFLITE_DCHECK_GT(params.input2_offset, -256);
  TFLITE_DCHECK_LT(params.input1_offset, 256);
  TFLITE_DCHECK_LT(params.input2_offset, 256);

  for (int i = 0; i < size; ++i) {
   d670c:	45a9      	cmp	r9, r5
   d670e:	dd28      	ble.n	d6762 <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x92>
    const int32 input1_val = params.input1_offset + input1_data[i];
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
   d6710:	f817 a005 	ldrb.w	sl, [r7, r5]
   d6714:	68a3      	ldr	r3, [r4, #8]
  TFLITE_DCHECK_LT(params.input2_offset, 256);

  for (int i = 0; i < size; ++i) {
    const int32 input1_val = params.input1_offset + input1_data[i];
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
   d6716:	69a0      	ldr	r0, [r4, #24]
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, params.input1_multiplier, params.input1_shift);
   d6718:	f818 2005 	ldrb.w	r2, [r8, r5]
   d671c:	69e1      	ldr	r1, [r4, #28]

  for (int i = 0; i < size; ++i) {
    const int32 input1_val = params.input1_offset + input1_data[i];
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
   d671e:	4453      	add	r3, sl
   d6720:	fa03 fa00 	lsl.w	sl, r3, r0
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, params.input1_multiplier, params.input1_shift);
   d6724:	6863      	ldr	r3, [r4, #4]
   d6726:	4413      	add	r3, r2
   d6728:	fa03 f000 	lsl.w	r0, r3, r0
   d672c:	6a22      	ldr	r2, [r4, #32]
   d672e:	f7ff ff59 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, params.input2_multiplier, params.input2_shift);
   d6732:	6aa2      	ldr	r2, [r4, #40]	; 0x28
   d6734:	6a61      	ldr	r1, [r4, #36]	; 0x24
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, params.input1_multiplier, params.input1_shift);
   d6736:	4683      	mov	fp, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, params.input2_multiplier, params.input2_shift);
   d6738:	4650      	mov	r0, sl
   d673a:	f7ff ff53 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 raw_sum = scaled_input1_val + scaled_input2_val;
    const int32 raw_output =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
   d673e:	6962      	ldr	r2, [r4, #20]
   d6740:	6921      	ldr	r1, [r4, #16]
   d6742:	4458      	add	r0, fp
   d6744:	f7ff ff4e 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
            raw_sum, params.output_multiplier, params.output_shift) +
        params.output_offset;
    const int32 clamped_output =
        std::min(params.quantized_activation_max,
                 std::max(params.quantized_activation_min, raw_output));
    output_data[i] = static_cast<uint8>(clamped_output);
   d6748:	68e3      	ldr	r3, [r4, #12]
   d674a:	4418      	add	r0, r3
   d674c:	6ae3      	ldr	r3, [r4, #44]	; 0x2c
   d674e:	4298      	cmp	r0, r3
   d6750:	bfb8      	it	lt
   d6752:	4618      	movlt	r0, r3
   d6754:	6b23      	ldr	r3, [r4, #48]	; 0x30
   d6756:	4283      	cmp	r3, r0
   d6758:	bfa8      	it	ge
   d675a:	4603      	movge	r3, r0
   d675c:	5573      	strb	r3, [r6, r5]
  TFLITE_DCHECK_GT(params.input1_offset, -256);
  TFLITE_DCHECK_GT(params.input2_offset, -256);
  TFLITE_DCHECK_LT(params.input1_offset, 256);
  TFLITE_DCHECK_LT(params.input2_offset, 256);

  for (int i = 0; i < size; ++i) {
   d675e:	3501      	adds	r5, #1
   d6760:	e7d4      	b.n	d670c <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x3c>
   d6762:	e8bd 8ff8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d6766 <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa>:
}

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const int8_t* input1_data,
                const RuntimeShape& input2_shape, const int8_t* input2_data,
                const RuntimeShape& output_shape, int8_t* output_data) {
   d6766:	e92d 4ff8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d676a:	4604      	mov	r4, r0
   d676c:	4690      	mov	r8, r2
   d676e:	4608      	mov	r0, r1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
   d6770:	6b22      	ldr	r2, [r4, #48]	; 0x30
   d6772:	6ae1      	ldr	r1, [r4, #44]	; 0x2c
}

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const int8_t* input1_data,
                const RuntimeShape& input2_shape, const int8_t* input2_data,
                const RuntimeShape& output_shape, int8_t* output_data) {
   d6774:	9f0a      	ldr	r7, [sp, #40]	; 0x28
   d6776:	9e0c      	ldr	r6, [sp, #48]	; 0x30
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
   d6778:	4291      	cmp	r1, r2
   d677a:	dd01      	ble.n	d6780 <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x1a>

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const int8_t* input1_data,
                const RuntimeShape& input2_shape, const int8_t* input2_data,
                const RuntimeShape& output_shape, int8_t* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
   d677c:	f00d fdd6 	bl	e432c <abort>
                   params.quantized_activation_max);
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d6780:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   d6782:	4619      	mov	r1, r3
   d6784:	f7ff fef1 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>

  const int32_t int8_max_value = std::numeric_limits<int8_t>::max();
  TFLITE_DCHECK_GE(params.input1_offset, -1 * int8_max_value);
   d6788:	6862      	ldr	r2, [r4, #4]
   d678a:	f112 0f7f 	cmn.w	r2, #127	; 0x7f
                const RuntimeShape& input2_shape, const int8_t* input2_data,
                const RuntimeShape& output_shape, int8_t* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d678e:	4681      	mov	r9, r0

  const int32_t int8_max_value = std::numeric_limits<int8_t>::max();
  TFLITE_DCHECK_GE(params.input1_offset, -1 * int8_max_value);
   d6790:	dbf4      	blt.n	d677c <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x16>
  TFLITE_DCHECK_GE(params.input2_offset, -1 * int8_max_value);
  TFLITE_DCHECK_LE(params.input1_offset, int8_max_value);
   d6792:	2a7f      	cmp	r2, #127	; 0x7f
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);

  const int32_t int8_max_value = std::numeric_limits<int8_t>::max();
  TFLITE_DCHECK_GE(params.input1_offset, -1 * int8_max_value);
  TFLITE_DCHECK_GE(params.input2_offset, -1 * int8_max_value);
   d6794:	68a3      	ldr	r3, [r4, #8]
  TFLITE_DCHECK_LE(params.input1_offset, int8_max_value);
   d6796:	dcf1      	bgt.n	d677c <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x16>
  TFLITE_DCHECK_LE(params.input2_offset, int8_max_value);
   d6798:	337f      	adds	r3, #127	; 0x7f
   d679a:	2bfe      	cmp	r3, #254	; 0xfe
   d679c:	d8ee      	bhi.n	d677c <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x16>
   d679e:	2500      	movs	r5, #0
  TFLITE_DCHECK_GE(params.input1_offset, -1 * int8_max_value);
  TFLITE_DCHECK_GE(params.input2_offset, -1 * int8_max_value);
  TFLITE_DCHECK_LE(params.input1_offset, int8_max_value);
  TFLITE_DCHECK_LE(params.input2_offset, int8_max_value);

  for (int i = 0; i < size; ++i) {
   d67a0:	45a9      	cmp	r9, r5
   d67a2:	dd28      	ble.n	d67f6 <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x90>
    const int32 input1_val = params.input1_offset + input1_data[i];
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
   d67a4:	f917 a005 	ldrsb.w	sl, [r7, r5]
   d67a8:	68a3      	ldr	r3, [r4, #8]
  TFLITE_DCHECK_LE(params.input2_offset, int8_max_value);

  for (int i = 0; i < size; ++i) {
    const int32 input1_val = params.input1_offset + input1_data[i];
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
   d67aa:	69a0      	ldr	r0, [r4, #24]
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, params.input1_multiplier, params.input1_shift);
   d67ac:	f918 2005 	ldrsb.w	r2, [r8, r5]
   d67b0:	69e1      	ldr	r1, [r4, #28]

  for (int i = 0; i < size; ++i) {
    const int32 input1_val = params.input1_offset + input1_data[i];
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
   d67b2:	4453      	add	r3, sl
   d67b4:	fa03 fa00 	lsl.w	sl, r3, r0
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, params.input1_multiplier, params.input1_shift);
   d67b8:	6863      	ldr	r3, [r4, #4]
   d67ba:	4413      	add	r3, r2
   d67bc:	fa03 f000 	lsl.w	r0, r3, r0
   d67c0:	6a22      	ldr	r2, [r4, #32]
   d67c2:	f7ff ff0f 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, params.input2_multiplier, params.input2_shift);
   d67c6:	6aa2      	ldr	r2, [r4, #40]	; 0x28
   d67c8:	6a61      	ldr	r1, [r4, #36]	; 0x24
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, params.input1_multiplier, params.input1_shift);
   d67ca:	4683      	mov	fp, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, params.input2_multiplier, params.input2_shift);
   d67cc:	4650      	mov	r0, sl
   d67ce:	f7ff ff09 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 raw_sum = scaled_input1_val + scaled_input2_val;
    const int32 raw_output =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
   d67d2:	6962      	ldr	r2, [r4, #20]
   d67d4:	6921      	ldr	r1, [r4, #16]
   d67d6:	4458      	add	r0, fp
   d67d8:	f7ff ff04 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
            raw_sum, params.output_multiplier, params.output_shift) +
        params.output_offset;
    const int32 clamped_output =
        std::min(params.quantized_activation_max,
                 std::max(params.quantized_activation_min, raw_output));
    output_data[i] = static_cast<int8_t>(clamped_output);
   d67dc:	68e3      	ldr	r3, [r4, #12]
   d67de:	4418      	add	r0, r3
   d67e0:	6ae3      	ldr	r3, [r4, #44]	; 0x2c
   d67e2:	4298      	cmp	r0, r3
   d67e4:	bfb8      	it	lt
   d67e6:	4618      	movlt	r0, r3
   d67e8:	6b23      	ldr	r3, [r4, #48]	; 0x30
   d67ea:	4283      	cmp	r3, r0
   d67ec:	bfa8      	it	ge
   d67ee:	4603      	movge	r3, r0
   d67f0:	5573      	strb	r3, [r6, r5]
  TFLITE_DCHECK_GE(params.input1_offset, -1 * int8_max_value);
  TFLITE_DCHECK_GE(params.input2_offset, -1 * int8_max_value);
  TFLITE_DCHECK_LE(params.input1_offset, int8_max_value);
  TFLITE_DCHECK_LE(params.input2_offset, int8_max_value);

  for (int i = 0; i < size; ++i) {
   d67f2:	3501      	adds	r5, #1
   d67f4:	e7d4      	b.n	d67a0 <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x3a>
   d67f6:	e8bd 8ff8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d67fa <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE>:
//
// Returns true iff there is some sort of broadcast, which includes five-fold
// patterns and falling back to generic broadcast.
inline bool ProcessBroadcastShapes(const RuntimeShape& shape0,
                                   const RuntimeShape& shape1,
                                   tflite::ArithmeticParams* params) {
   d67fa:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
   d67fe:	b091      	sub	sp, #68	; 0x44
   d6800:	6804      	ldr	r4, [r0, #0]
   d6802:	680b      	ldr	r3, [r1, #0]
      dims_pointer_ = new int32[dimensions_count];
#endif  // TF_LITE_STATIC_MEMORY
    }
  }

  RuntimeShape(int shape_size, int32 value) : size_(0) {
   d6804:	af10      	add	r7, sp, #64	; 0x40
   d6806:	2600      	movs	r6, #0
   d6808:	429c      	cmp	r4, r3
   d680a:	f847 6d3c 	str.w	r6, [r7, #-60]!
   d680e:	bfb8      	it	lt
   d6810:	461c      	movlt	r4, r3
  const int dims_count =
      std::max(shape0.DimensionsCount(), shape1.DimensionsCount());

  params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
   d6812:	2304      	movs	r3, #4
//
// Returns true iff there is some sort of broadcast, which includes five-fold
// patterns and falling back to generic broadcast.
inline bool ProcessBroadcastShapes(const RuntimeShape& shape0,
                                   const RuntimeShape& shape1,
                                   tflite::ArithmeticParams* params) {
   d6814:	4681      	mov	r9, r0
   d6816:	4688      	mov	r8, r1
  const int dims_count =
      std::max(shape0.DimensionsCount(), shape1.DimensionsCount());

  params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
   d6818:	7013      	strb	r3, [r2, #0]
    Resize(shape_size);
   d681a:	4621      	mov	r1, r4
   d681c:	4638      	mov	r0, r7
//
// Returns true iff there is some sort of broadcast, which includes five-fold
// patterns and falling back to generic broadcast.
inline bool ProcessBroadcastShapes(const RuntimeShape& shape0,
                                   const RuntimeShape& shape1,
                                   tflite::ArithmeticParams* params) {
   d681e:	4615      	mov	r5, r2
   d6820:	f7ff fe3e 	bl	d64a0 <_ZN6tflite12RuntimeShape6ResizeEi>
    for (int i = 0; i < shape_size; ++i) {
   d6824:	42a6      	cmp	r6, r4
   d6826:	da06      	bge.n	d6836 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x3c>
      SetDim(i, value);
   d6828:	4631      	mov	r1, r6
   d682a:	2201      	movs	r2, #1
   d682c:	4638      	mov	r0, r7
   d682e:	f7ff fe26 	bl	d647e <_ZN6tflite12RuntimeShape6SetDimEil>
    }
  }

  RuntimeShape(int shape_size, int32 value) : size_(0) {
    Resize(shape_size);
    for (int i = 0; i < shape_size; ++i) {
   d6832:	3601      	adds	r6, #1
   d6834:	e7f6      	b.n	d6824 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x2a>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
   d6836:	2301      	movs	r3, #1
   d6838:	464a      	mov	r2, r9
   d683a:	4621      	mov	r1, r4
   d683c:	a806      	add	r0, sp, #24
   d683e:	f7ff fe46 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d6842:	2301      	movs	r3, #1
   d6844:	4642      	mov	r2, r8
   d6846:	4621      	mov	r1, r4
   d6848:	a80b      	add	r0, sp, #44	; 0x2c
   d684a:	f7ff fe40 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
    }
    std::memcpy(DimsData(), other.DimsData(), sizeof(int32) * size_);
  }

  bool operator==(const RuntimeShape& comp) const {
    return this->size_ == comp.size_ &&
   d684e:	9a06      	ldr	r2, [sp, #24]
   d6850:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   d6852:	429a      	cmp	r2, r3
   d6854:	d10d      	bne.n	d6872 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x78>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d6856:	2a04      	cmp	r2, #4
   d6858:	bfc7      	ittee	gt
   d685a:	9807      	ldrgt	r0, [sp, #28]
   d685c:	990c      	ldrgt	r1, [sp, #48]	; 0x30
   d685e:	a807      	addle	r0, sp, #28
   d6860:	a90c      	addle	r1, sp, #48	; 0x30
    std::memcpy(DimsData(), other.DimsData(), sizeof(int32) * size_);
  }

  bool operator==(const RuntimeShape& comp) const {
    return this->size_ == comp.size_ &&
           std::memcmp(DimsData(), comp.DimsData(), size_ * sizeof(int32)) == 0;
   d6862:	0092      	lsls	r2, r2, #2
   d6864:	f010 ffae 	bl	e77c4 <memcmp>
    }
    std::memcpy(DimsData(), other.DimsData(), sizeof(int32) * size_);
  }

  bool operator==(const RuntimeShape& comp) const {
    return this->size_ == comp.size_ &&
   d6868:	b918      	cbnz	r0, d6872 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x78>
  auto extended_shape0 = RuntimeShape::ExtendedShape(dims_count, shape0);
  auto extended_shape1 = RuntimeShape::ExtendedShape(dims_count, shape1);

  // Check for "exact" match, implicitly accepting any scalar shapes.
  if (extended_shape0 == extended_shape1) {
    params->broadcast_category = BroadcastableOpCategory::kNonBroadcast;
   d686a:	2301      	movs	r3, #1
   d686c:	702b      	strb	r3, [r5, #0]

  if (params->broadcast_category !=
          BroadcastableOpCategory::kFirstInputBroadcastsFast &&
      params->broadcast_category !=
          BroadcastableOpCategory::kSecondInputBroadcastsFast) {
    return false;
   d686e:	2400      	movs	r4, #0
   d6870:	e08c      	b.n	d698c <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x192>
  if (extended_shape0 == extended_shape1) {
    params->broadcast_category = BroadcastableOpCategory::kNonBroadcast;
    return false;
  }

  for (int i = dims_count - 1; i >= 0; --i) {
   d6872:	3c01      	subs	r4, #1
   d6874:	4626      	mov	r6, r4
   d6876:	2e00      	cmp	r6, #0
   d6878:	db13      	blt.n	d68a2 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0xa8>
    if (extended_shape0.Dims(i) == extended_shape1.Dims(i)) {
   d687a:	4631      	mov	r1, r6
   d687c:	a806      	add	r0, sp, #24
   d687e:	f7ff fded 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d6882:	4631      	mov	r1, r6
   d6884:	4680      	mov	r8, r0
   d6886:	a80b      	add	r0, sp, #44	; 0x2c
   d6888:	f7ff fde8 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d688c:	4580      	cmp	r8, r0
   d688e:	d04d      	beq.n	d692c <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x132>
      continue;
    } else if (extended_shape0.Dims(i) == 1) {
   d6890:	f1b8 0f01 	cmp.w	r8, #1
   d6894:	d101      	bne.n	d689a <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0xa0>
      params->broadcast_category =
          BroadcastableOpCategory::kFirstInputBroadcastsFast;
   d6896:	2302      	movs	r3, #2
   d6898:	e002      	b.n	d68a0 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0xa6>
      break;
    } else if (extended_shape1.Dims(i) == 1) {
   d689a:	2801      	cmp	r0, #1
   d689c:	d143      	bne.n	d6926 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x12c>
      params->broadcast_category =
          BroadcastableOpCategory::kSecondInputBroadcastsFast;
   d689e:	2303      	movs	r3, #3
   d68a0:	702b      	strb	r3, [r5, #0]
      params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
      return true;
    }
  }

  if (params->broadcast_category !=
   d68a2:	782b      	ldrb	r3, [r5, #0]
   d68a4:	1e9a      	subs	r2, r3, #2
   d68a6:	2a01      	cmp	r2, #1
   d68a8:	d8e1      	bhi.n	d686e <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x74>
  // From this point it is assumed contractually that corresponding dimensions
  // in shape0 and shape1 are either (a) equal or (b) one or other equals 1.
  const bool swap_inputs = params->broadcast_category ==
                           BroadcastableOpCategory::kSecondInputBroadcastsFast;
  const RuntimeShape* shape_a =
      swap_inputs ? &extended_shape1 : &extended_shape0;
   d68aa:	2b03      	cmp	r3, #3
  const RuntimeShape* shape_b =
      swap_inputs ? &extended_shape0 : &extended_shape1;

  int i = dims_count - 1;
  params->broadcast_shape[0] = 1;
   d68ac:	f04f 0301 	mov.w	r3, #1
  // From this point it is assumed contractually that corresponding dimensions
  // in shape0 and shape1 are either (a) equal or (b) one or other equals 1.
  const bool swap_inputs = params->broadcast_category ==
                           BroadcastableOpCategory::kSecondInputBroadcastsFast;
  const RuntimeShape* shape_a =
      swap_inputs ? &extended_shape1 : &extended_shape0;
   d68b0:	bf07      	ittee	eq
   d68b2:	f10d 082c 	addeq.w	r8, sp, #44	; 0x2c
  const RuntimeShape* shape_b =
      swap_inputs ? &extended_shape0 : &extended_shape1;
   d68b6:	ae06      	addeq	r6, sp, #24
  // From this point it is assumed contractually that corresponding dimensions
  // in shape0 and shape1 are either (a) equal or (b) one or other equals 1.
  const bool swap_inputs = params->broadcast_category ==
                           BroadcastableOpCategory::kSecondInputBroadcastsFast;
  const RuntimeShape* shape_a =
      swap_inputs ? &extended_shape1 : &extended_shape0;
   d68b8:	f10d 0818 	addne.w	r8, sp, #24
  const RuntimeShape* shape_b =
      swap_inputs ? &extended_shape0 : &extended_shape1;
   d68bc:	ae0b      	addne	r6, sp, #44	; 0x2c

  int i = dims_count - 1;
  params->broadcast_shape[0] = 1;
   d68be:	63eb      	str	r3, [r5, #60]	; 0x3c
  params->broadcast_shape[1] = 1;
   d68c0:	642b      	str	r3, [r5, #64]	; 0x40
  params->broadcast_shape[2] = 1;
   d68c2:	646b      	str	r3, [r5, #68]	; 0x44
  params->broadcast_shape[3] = 1;
   d68c4:	64ab      	str	r3, [r5, #72]	; 0x48
  params->broadcast_shape[4] = 1;
   d68c6:	64eb      	str	r3, [r5, #76]	; 0x4c
  // y_0 is greedy: include dims if both or neither equal 1: in other words,
  // test for equality rather than (shape_a->Dims(i) != 1).
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
   d68c8:	2c00      	cmp	r4, #0
   d68ca:	db5e      	blt.n	d698a <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x190>
   d68cc:	4621      	mov	r1, r4
   d68ce:	4640      	mov	r0, r8
   d68d0:	f7ff fdc4 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d68d4:	4621      	mov	r1, r4
   d68d6:	4681      	mov	r9, r0
   d68d8:	4630      	mov	r0, r6
   d68da:	f7ff fdbf 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d68de:	4581      	cmp	r9, r0
   d68e0:	d026      	beq.n	d6930 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x136>
    params->broadcast_shape[4] *= shape_b->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).  If it is input_b
  // that has the unit dimension, the next two loops are not entered.
  while (i >= 0 && shape_a->Dims(i) == 1) {
   d68e2:	4621      	mov	r1, r4
   d68e4:	4640      	mov	r0, r8
   d68e6:	f7ff fdb9 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d68ea:	2801      	cmp	r0, #1
   d68ec:	d026      	beq.n	d693c <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x142>
    params->broadcast_shape[3] *= shape_b->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
   d68ee:	4621      	mov	r1, r4
   d68f0:	4640      	mov	r0, r8
   d68f2:	f7ff fdb3 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d68f6:	4621      	mov	r1, r4
   d68f8:	4681      	mov	r9, r0
   d68fa:	4630      	mov	r0, r6
   d68fc:	f7ff fdae 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d6900:	4581      	cmp	r9, r0
   d6902:	d027      	beq.n	d6954 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x15a>
    params->broadcast_shape[2] *= shape_a->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).
  while (i >= 0 && shape_b->Dims(i) == 1) {
   d6904:	4621      	mov	r1, r4
   d6906:	4630      	mov	r0, r6
   d6908:	f7ff fda8 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d690c:	2801      	cmp	r0, #1
   d690e:	d029      	beq.n	d6964 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x16a>
    params->broadcast_shape[1] *= shape_a->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
   d6910:	4621      	mov	r1, r4
   d6912:	4640      	mov	r0, r8
   d6914:	f7ff fda2 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d6918:	4621      	mov	r1, r4
   d691a:	4681      	mov	r9, r0
   d691c:	4630      	mov	r0, r6
   d691e:	f7ff fd9d 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d6922:	4581      	cmp	r9, r0
   d6924:	d02a      	beq.n	d697c <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x182>
          BroadcastableOpCategory::kSecondInputBroadcastsFast;
      break;
    } else {
      // This case is erroneous: there is a dimension that does not match and
      // is not a broadcast from one shape to the other.
      params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
   d6926:	2304      	movs	r3, #4
   d6928:	702b      	strb	r3, [r5, #0]
   d692a:	e02e      	b.n	d698a <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x190>
  if (extended_shape0 == extended_shape1) {
    params->broadcast_category = BroadcastableOpCategory::kNonBroadcast;
    return false;
  }

  for (int i = dims_count - 1; i >= 0; --i) {
   d692c:	3e01      	subs	r6, #1
   d692e:	e7a2      	b.n	d6876 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x7c>
  params->broadcast_shape[3] = 1;
  params->broadcast_shape[4] = 1;
  // y_0 is greedy: include dims if both or neither equal 1: in other words,
  // test for equality rather than (shape_a->Dims(i) != 1).
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
    params->broadcast_shape[4] *= shape_b->Dims(i);
   d6930:	6ceb      	ldr	r3, [r5, #76]	; 0x4c
   d6932:	fb09 f303 	mul.w	r3, r9, r3
   d6936:	64eb      	str	r3, [r5, #76]	; 0x4c
    --i;
   d6938:	3c01      	subs	r4, #1
  params->broadcast_shape[2] = 1;
  params->broadcast_shape[3] = 1;
  params->broadcast_shape[4] = 1;
  // y_0 is greedy: include dims if both or neither equal 1: in other words,
  // test for equality rather than (shape_a->Dims(i) != 1).
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
   d693a:	e7c5      	b.n	d68c8 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0xce>
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).  If it is input_b
  // that has the unit dimension, the next two loops are not entered.
  while (i >= 0 && shape_a->Dims(i) == 1) {
    params->broadcast_shape[3] *= shape_b->Dims(i);
   d693c:	4621      	mov	r1, r4
   d693e:	4630      	mov	r0, r6
   d6940:	f7ff fd8c 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d6944:	6cab      	ldr	r3, [r5, #72]	; 0x48
    params->broadcast_shape[4] *= shape_b->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).  If it is input_b
  // that has the unit dimension, the next two loops are not entered.
  while (i >= 0 && shape_a->Dims(i) == 1) {
   d6946:	f114 34ff 	adds.w	r4, r4, #4294967295	; 0xffffffff
    params->broadcast_shape[3] *= shape_b->Dims(i);
   d694a:	fb00 f003 	mul.w	r0, r0, r3
   d694e:	64a8      	str	r0, [r5, #72]	; 0x48
    params->broadcast_shape[4] *= shape_b->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).  If it is input_b
  // that has the unit dimension, the next two loops are not entered.
  while (i >= 0 && shape_a->Dims(i) == 1) {
   d6950:	d2c7      	bcs.n	d68e2 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0xe8>
   d6952:	e01a      	b.n	d698a <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x190>
    params->broadcast_shape[3] *= shape_b->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
    params->broadcast_shape[2] *= shape_a->Dims(i);
   d6954:	6c6b      	ldr	r3, [r5, #68]	; 0x44
  // that has the unit dimension, the next two loops are not entered.
  while (i >= 0 && shape_a->Dims(i) == 1) {
    params->broadcast_shape[3] *= shape_b->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
   d6956:	f114 34ff 	adds.w	r4, r4, #4294967295	; 0xffffffff
    params->broadcast_shape[2] *= shape_a->Dims(i);
   d695a:	fb09 f303 	mul.w	r3, r9, r3
   d695e:	646b      	str	r3, [r5, #68]	; 0x44
  // that has the unit dimension, the next two loops are not entered.
  while (i >= 0 && shape_a->Dims(i) == 1) {
    params->broadcast_shape[3] *= shape_b->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
   d6960:	d2c5      	bcs.n	d68ee <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0xf4>
   d6962:	e012      	b.n	d698a <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x190>
    params->broadcast_shape[2] *= shape_a->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).
  while (i >= 0 && shape_b->Dims(i) == 1) {
    params->broadcast_shape[1] *= shape_a->Dims(i);
   d6964:	4621      	mov	r1, r4
   d6966:	4640      	mov	r0, r8
   d6968:	f7ff fd78 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d696c:	6c2b      	ldr	r3, [r5, #64]	; 0x40
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
    params->broadcast_shape[2] *= shape_a->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).
  while (i >= 0 && shape_b->Dims(i) == 1) {
   d696e:	f114 34ff 	adds.w	r4, r4, #4294967295	; 0xffffffff
    params->broadcast_shape[1] *= shape_a->Dims(i);
   d6972:	fb00 f003 	mul.w	r0, r0, r3
   d6976:	6428      	str	r0, [r5, #64]	; 0x40
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
    params->broadcast_shape[2] *= shape_a->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).
  while (i >= 0 && shape_b->Dims(i) == 1) {
   d6978:	d2c4      	bcs.n	d6904 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x10a>
   d697a:	e006      	b.n	d698a <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x190>
    params->broadcast_shape[1] *= shape_a->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
    params->broadcast_shape[0] *= shape_b->Dims(i);
   d697c:	6beb      	ldr	r3, [r5, #60]	; 0x3c
  // Here either input_a or input_b has dim of 1 (if i >= 0).
  while (i >= 0 && shape_b->Dims(i) == 1) {
    params->broadcast_shape[1] *= shape_a->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
   d697e:	f114 34ff 	adds.w	r4, r4, #4294967295	; 0xffffffff
    params->broadcast_shape[0] *= shape_b->Dims(i);
   d6982:	fb09 f303 	mul.w	r3, r9, r3
   d6986:	63eb      	str	r3, [r5, #60]	; 0x3c
  // Here either input_a or input_b has dim of 1 (if i >= 0).
  while (i >= 0 && shape_b->Dims(i) == 1) {
    params->broadcast_shape[1] *= shape_a->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
   d6988:	d2c2      	bcs.n	d6910 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x116>
  // Rarer case is when the broadcast dimensions cannot be handled by a fivefold
  // loop.
  if (i >= 0) {
    params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
  }
  return true;
   d698a:	2401      	movs	r4, #1

  params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
  RuntimeShape scalar_shape(dims_count, 1);

  auto extended_shape0 = RuntimeShape::ExtendedShape(dims_count, shape0);
  auto extended_shape1 = RuntimeShape::ExtendedShape(dims_count, shape1);
   d698c:	a80b      	add	r0, sp, #44	; 0x2c
   d698e:	f7ff fd5a 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
      std::max(shape0.DimensionsCount(), shape1.DimensionsCount());

  params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
  RuntimeShape scalar_shape(dims_count, 1);

  auto extended_shape0 = RuntimeShape::ExtendedShape(dims_count, shape0);
   d6992:	a806      	add	r0, sp, #24
   d6994:	f7ff fd57 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                                   tflite::ArithmeticParams* params) {
  const int dims_count =
      std::max(shape0.DimensionsCount(), shape1.DimensionsCount());

  params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
  RuntimeShape scalar_shape(dims_count, 1);
   d6998:	4638      	mov	r0, r7
   d699a:	f7ff fd54 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  // loop.
  if (i >= 0) {
    params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
  }
  return true;
}
   d699e:	4620      	mov	r0, r4
   d69a0:	b011      	add	sp, #68	; 0x44
   d69a2:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}

000d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>:
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
}

inline RuntimeShape GetTensorShape(const TfLiteTensor* tensor) {
   d69a6:	b570      	push	{r4, r5, r6, lr}
   d69a8:	4604      	mov	r4, r0
  if (tensor == nullptr) {
   d69aa:	b909      	cbnz	r1, d69b0 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor+0xa>
  // larger shapes are separately allocated.
  static constexpr int kMaxSmallSize = 4;

  RuntimeShape& operator=(RuntimeShape const&) = delete;

  RuntimeShape() : size_(0) {}
   d69ac:	6001      	str	r1, [r0, #0]
   d69ae:	e010      	b.n	d69d2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor+0x2c>
    return RuntimeShape();
  }

  TfLiteIntArray* dims = tensor->dims;
  const int dims_size = dims->size;
   d69b0:	688d      	ldr	r5, [r1, #8]
   d69b2:	f855 6b04 	ldr.w	r6, [r5], #4
    for (int i = 0; i < shape_size; ++i) {
      SetDim(i, value);
    }
  }

  RuntimeShape(int dimensions_count, const int32* dims_data) : size_(0) {
   d69b6:	2300      	movs	r3, #0
   d69b8:	6003      	str	r3, [r0, #0]
#endif  // TF_LITE_STATIC_MEMORY
    }
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
    Resize(dimensions_count);
   d69ba:	4631      	mov	r1, r6
   d69bc:	f7ff fd70 	bl	d64a0 <_ZN6tflite12RuntimeShape6ResizeEi>
      dims_[i] = val;
    }
  }

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d69c0:	6823      	ldr	r3, [r4, #0]
   d69c2:	2b04      	cmp	r3, #4
   d69c4:	bfcc      	ite	gt
   d69c6:	6860      	ldrgt	r0, [r4, #4]
   d69c8:	1d20      	addle	r0, r4, #4
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
    Resize(dimensions_count);
    int32* dst_dims = DimsData();
    std::memcpy(dst_dims, dims_data, dimensions_count * sizeof(int32));
   d69ca:	00b2      	lsls	r2, r6, #2
   d69cc:	4629      	mov	r1, r5
   d69ce:	f010 ff08 	bl	e77e2 <memcpy>
  const int32_t* dims_data = reinterpret_cast<const int32_t*>(dims->data);
  return RuntimeShape(dims_size, dims_data);
}
   d69d2:	4620      	mov	r0, r4
   d69d4:	bd70      	pop	{r4, r5, r6, pc}
	...

000d69d8 <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE>:

TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteAddParams* params,
                             const TfLiteTensor* input1,
                             const TfLiteTensor* input2, TfLiteTensor* output,
                             OpData* data) {
   d69d8:	e92d 47f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, lr}
  data->requires_broadcast = !HaveSameShapes(input1, input2);
   d69dc:	4610      	mov	r0, r2
}

TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteAddParams* params,
                             const TfLiteTensor* input1,
                             const TfLiteTensor* input2, TfLiteTensor* output,
                             OpData* data) {
   d69de:	ed2d 8b06 	vpush	{d8-d10}
   d69e2:	4688      	mov	r8, r1
  data->requires_broadcast = !HaveSameShapes(input1, input2);
   d69e4:	4619      	mov	r1, r3
}

TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteAddParams* params,
                             const TfLiteTensor* input1,
                             const TfLiteTensor* input2, TfLiteTensor* output,
                             OpData* data) {
   d69e6:	461e      	mov	r6, r3
   d69e8:	9d0e      	ldr	r5, [sp, #56]	; 0x38
   d69ea:	9c0f      	ldr	r4, [sp, #60]	; 0x3c
   d69ec:	4617      	mov	r7, r2
  data->requires_broadcast = !HaveSameShapes(input1, input2);
   d69ee:	f00d f9bf 	bl	e3d70 <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
   d69f2:	f080 0001 	eor.w	r0, r0, #1
   d69f6:	7020      	strb	r0, [r4, #0]

  if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt8) {
   d69f8:	782b      	ldrb	r3, [r5, #0]
   d69fa:	2b03      	cmp	r3, #3
   d69fc:	d001      	beq.n	d6a02 <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE+0x2a>
   d69fe:	2b09      	cmp	r3, #9
   d6a00:	d16f      	bne.n	d6ae2 <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE+0x10a>
    // 8bit -> 8bit general quantized path, with general rescalings
    data->input1_offset = -input1->params.zero_point;
   d6a02:	693b      	ldr	r3, [r7, #16]
   d6a04:	425b      	negs	r3, r3
   d6a06:	62a3      	str	r3, [r4, #40]	; 0x28
    data->input2_offset = -input2->params.zero_point;
   d6a08:	6933      	ldr	r3, [r6, #16]
   d6a0a:	425b      	negs	r3, r3
   d6a0c:	62e3      	str	r3, [r4, #44]	; 0x2c
    data->output_offset = output->params.zero_point;
   d6a0e:	692b      	ldr	r3, [r5, #16]
   d6a10:	6323      	str	r3, [r4, #48]	; 0x30
    data->left_shift = 20;
   d6a12:	2314      	movs	r3, #20
   d6a14:	6263      	str	r3, [r4, #36]	; 0x24
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   d6a16:	ed97 8a03 	vldr	s16, [r7, #12]
   d6a1a:	edd6 8a03 	vldr	s17, [r6, #12]
    const double twice_max_input_scale =
        2 * std::max(input1->params.scale, input2->params.scale);
   d6a1e:	eeb4 8ae8 	vcmpe.f32	s16, s17
   d6a22:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d6a26:	bf54      	ite	pl
   d6a28:	eef0 7a48 	vmovpl.f32	s15, s16
   d6a2c:	eef0 7a68 	vmovmi.f32	s15, s17
   d6a30:	ee77 7aa7 	vadd.f32	s15, s15, s15
        input2->params.scale / twice_max_input_scale;
    const double real_output_multiplier =
        twice_max_input_scale /
        ((1 << data->left_shift) * output->params.scale);

    QuantizeMultiplierSmallerThanOneExp(
   d6a34:	f104 0a04 	add.w	sl, r4, #4
    data->input1_offset = -input1->params.zero_point;
    data->input2_offset = -input2->params.zero_point;
    data->output_offset = output->params.zero_point;
    data->left_shift = 20;
    const double twice_max_input_scale =
        2 * std::max(input1->params.scale, input2->params.scale);
   d6a38:	ee17 0a90 	vmov	r0, s15
   d6a3c:	f010 fade 	bl	e6ffc <__aeabi_f2d>
   d6a40:	4606      	mov	r6, r0
   d6a42:	460f      	mov	r7, r1
    const double real_input1_multiplier =
        input1->params.scale / twice_max_input_scale;
    const double real_input2_multiplier =
        input2->params.scale / twice_max_input_scale;
   d6a44:	ee18 0a90 	vmov	r0, s17
   d6a48:	f010 fad8 	bl	e6ffc <__aeabi_f2d>
   d6a4c:	4632      	mov	r2, r6
   d6a4e:	463b      	mov	r3, r7
   d6a50:	f010 fc52 	bl	e72f8 <__aeabi_ddiv>
    const double real_output_multiplier =
        twice_max_input_scale /
        ((1 << data->left_shift) * output->params.scale);
   d6a54:	ed95 7a03 	vldr	s14, [r5, #12]
   d6a58:	eddf 7a24 	vldr	s15, [pc, #144]	; d6aec <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE+0x114>
   d6a5c:	ee67 7a27 	vmul.f32	s15, s14, s15
    const double twice_max_input_scale =
        2 * std::max(input1->params.scale, input2->params.scale);
    const double real_input1_multiplier =
        input1->params.scale / twice_max_input_scale;
    const double real_input2_multiplier =
        input2->params.scale / twice_max_input_scale;
   d6a60:	ec41 0b1a 	vmov	d10, r0, r1
    const double real_output_multiplier =
        twice_max_input_scale /
        ((1 << data->left_shift) * output->params.scale);
   d6a64:	ee17 0a90 	vmov	r0, s15
   d6a68:	f010 fac8 	bl	e6ffc <__aeabi_f2d>
   d6a6c:	4602      	mov	r2, r0
   d6a6e:	460b      	mov	r3, r1
   d6a70:	4630      	mov	r0, r6
   d6a72:	4639      	mov	r1, r7
   d6a74:	f010 fc40 	bl	e72f8 <__aeabi_ddiv>
   d6a78:	ec41 0b19 	vmov	d9, r0, r1

    QuantizeMultiplierSmallerThanOneExp(
        real_input1_multiplier, &data->input1_multiplier, &data->input1_shift);
   d6a7c:	ee18 0a10 	vmov	r0, s16
   d6a80:	f010 fabc 	bl	e6ffc <__aeabi_f2d>
   d6a84:	4632      	mov	r2, r6
   d6a86:	463b      	mov	r3, r7
   d6a88:	f010 fc36 	bl	e72f8 <__aeabi_ddiv>
        input2->params.scale / twice_max_input_scale;
    const double real_output_multiplier =
        twice_max_input_scale /
        ((1 << data->left_shift) * output->params.scale);

    QuantizeMultiplierSmallerThanOneExp(
   d6a8c:	f104 0914 	add.w	r9, r4, #20
        real_input1_multiplier, &data->input1_multiplier, &data->input1_shift);
   d6a90:	ec41 0b10 	vmov	d0, r0, r1
   d6a94:	4651      	mov	r1, sl
   d6a96:	4648      	mov	r0, r9
   d6a98:	f00d f9de 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>

    QuantizeMultiplierSmallerThanOneExp(
        real_input2_multiplier, &data->input2_multiplier, &data->input2_shift);
   d6a9c:	eeb0 0a4a 	vmov.f32	s0, s20
   d6aa0:	eef0 0a6a 	vmov.f32	s1, s21
   d6aa4:	f104 0108 	add.w	r1, r4, #8
   d6aa8:	f104 0018 	add.w	r0, r4, #24
   d6aac:	f00d f9d4 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>

    QuantizeMultiplierSmallerThanOneExp(
        real_output_multiplier, &data->output_multiplier, &data->output_shift);
   d6ab0:	eeb0 0a49 	vmov.f32	s0, s18
   d6ab4:	eef0 0a69 	vmov.f32	s1, s19
   d6ab8:	f104 0120 	add.w	r1, r4, #32
   d6abc:	f104 001c 	add.w	r0, r4, #28
   d6ac0:	f00d f9ca 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>

    if (output->type == kTfLiteUInt8) {
   d6ac4:	782b      	ldrb	r3, [r5, #0]
   d6ac6:	f898 0000 	ldrb.w	r0, [r8]
   d6aca:	2b03      	cmp	r3, #3
      CalculateActivationRangeUint8(params->activation, output,
                                    &data->output_activation_min,
                                    &data->output_activation_max);
   d6acc:	4629      	mov	r1, r5
   d6ace:	f104 0310 	add.w	r3, r4, #16
   d6ad2:	f104 020c 	add.w	r2, r4, #12
        real_input2_multiplier, &data->input2_multiplier, &data->input2_shift);

    QuantizeMultiplierSmallerThanOneExp(
        real_output_multiplier, &data->output_multiplier, &data->output_shift);

    if (output->type == kTfLiteUInt8) {
   d6ad6:	d102      	bne.n	d6ade <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE+0x106>
      CalculateActivationRangeUint8(params->activation, output,
                                    &data->output_activation_min,
                                    &data->output_activation_max);
   d6ad8:	f00d f844 	bl	e3b64 <_ZN6tflite29CalculateActivationRangeUint8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>
   d6adc:	e001      	b.n	d6ae2 <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE+0x10a>
    } else {
      CalculateActivationRangeInt8(params->activation, output,
                                   &data->output_activation_min,
                                   &data->output_activation_max);
   d6ade:	f00d f93b 	bl	e3d58 <_ZN6tflite28CalculateActivationRangeInt8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>
    }
  }

  return kTfLiteOk;
}
   d6ae2:	ecbd 8b06 	vpop	{d8-d10}
   d6ae6:	2000      	movs	r0, #0
   d6ae8:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}
   d6aec:	49800000 	.word	0x49800000

000d6af0 <_ZN6tflite3ops5micro12Register_ADDEv>:
}  // namespace add

TfLiteRegistration* Register_ADD() {
  static TfLiteRegistration r = {add::Init, add::Free, add::Prepare, add::Eval};
  return &r;
}
   d6af0:	4800      	ldr	r0, [pc, #0]	; (d6af4 <_ZN6tflite3ops5micro12Register_ADDEv+0x4>)
   d6af2:	4770      	bx	lr
   d6af4:	2003bd48 	.word	0x2003bd48

000d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>:
    }
  }
}

template <int N>
inline void NdArrayDescsForElementwiseBroadcast(
   d6af8:	e92d 47f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, lr}
   d6afc:	460e      	mov	r6, r1
   d6afe:	b08a      	sub	sp, #40	; 0x28
   d6b00:	461d      	mov	r5, r3
    const RuntimeShape& input0_shape, const RuntimeShape& input1_shape,
    NdArrayDesc<N>* desc0_out, NdArrayDesc<N>* desc1_out) {
  TFLITE_DCHECK(desc0_out != nullptr);
   d6b02:	4614      	mov	r4, r2
   d6b04:	b90a      	cbnz	r2, d6b0a <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0x12>
   d6b06:	f00d fc11 	bl	e432c <abort>
  TFLITE_DCHECK(desc1_out != nullptr);
   d6b0a:	2b00      	cmp	r3, #0
   d6b0c:	d0fb      	beq.n	d6b06 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0xe>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
   d6b0e:	4602      	mov	r2, r0
   d6b10:	2301      	movs	r3, #1
   d6b12:	2104      	movs	r1, #4
   d6b14:	4668      	mov	r0, sp
   d6b16:	f7ff fcda 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d6b1a:	4632      	mov	r2, r6
   d6b1c:	2301      	movs	r3, #1
   d6b1e:	2104      	movs	r1, #4
   d6b20:	a805      	add	r0, sp, #20
   d6b22:	f7ff fcd4 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
   d6b26:	f04f 0901 	mov.w	r9, #1
   d6b2a:	46a0      	mov	r8, r4
   d6b2c:	462f      	mov	r7, r5
  for (int i = N - 1; i >= 0; --i) {
   d6b2e:	2603      	movs	r6, #3

  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
   d6b30:	46ca      	mov	sl, r9
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
   d6b32:	4631      	mov	r1, r6
   d6b34:	4668      	mov	r0, sp
   d6b36:	f7ff fc91 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
    desc0_out->strides[i] = desc0_stride;
   d6b3a:	f8c8 a01c 	str.w	sl, [r8, #28]

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
   d6b3e:	f8c8 000c 	str.w	r0, [r8, #12]
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
   d6b42:	4631      	mov	r1, r6
   d6b44:	4668      	mov	r0, sp
   d6b46:	f7ff fc89 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
   d6b4a:	4631      	mov	r1, r6
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
   d6b4c:	fb00 fa0a 	mul.w	sl, r0, sl
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
   d6b50:	a805      	add	r0, sp, #20
   d6b52:	f7ff fc83 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
    desc1_out->strides[i] = desc1_stride;
   d6b56:	f8c7 901c 	str.w	r9, [r7, #28]
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
   d6b5a:	60f8      	str	r0, [r7, #12]
    desc1_out->strides[i] = desc1_stride;
    desc1_stride *= extended_input1_shape.Dims(i);
   d6b5c:	4631      	mov	r1, r6
   d6b5e:	a805      	add	r0, sp, #20
   d6b60:	f7ff fc7c 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
   d6b64:	3e01      	subs	r6, #1
   d6b66:	1c73      	adds	r3, r6, #1
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
    desc1_out->strides[i] = desc1_stride;
    desc1_stride *= extended_input1_shape.Dims(i);
   d6b68:	fb00 f909 	mul.w	r9, r0, r9
   d6b6c:	f1a8 0804 	sub.w	r8, r8, #4
   d6b70:	f1a7 0704 	sub.w	r7, r7, #4
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
   d6b74:	d1dd      	bne.n	d6b32 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0x3a>
   d6b76:	2600      	movs	r6, #0
   d6b78:	3510      	adds	r5, #16
   d6b7a:	3410      	adds	r4, #16
   d6b7c:	46b0      	mov	r8, r6

  // Walk over each dimension. If the extents are equal do nothing.
  // Otherwise, set the desc with extent 1 to have extent equal to the other and
  // stride 0.
  for (int i = 0; i < N; ++i) {
    const int extent0 = extended_input0_shape.Dims(i);
   d6b7e:	4631      	mov	r1, r6
   d6b80:	4668      	mov	r0, sp
   d6b82:	f7ff fc6b 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
    const int extent1 = extended_input1_shape.Dims(i);
   d6b86:	4631      	mov	r1, r6

  // Walk over each dimension. If the extents are equal do nothing.
  // Otherwise, set the desc with extent 1 to have extent equal to the other and
  // stride 0.
  for (int i = 0; i < N; ++i) {
    const int extent0 = extended_input0_shape.Dims(i);
   d6b88:	4607      	mov	r7, r0
    const int extent1 = extended_input1_shape.Dims(i);
   d6b8a:	a805      	add	r0, sp, #20
   d6b8c:	f7ff fc66 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
    if (extent0 != extent1) {
   d6b90:	4287      	cmp	r7, r0
   d6b92:	d00c      	beq.n	d6bae <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0xb6>
      if (extent0 == 1) {
   d6b94:	2f01      	cmp	r7, #1
   d6b96:	d104      	bne.n	d6ba2 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0xaa>
        desc0_out->strides[i] = 0;
   d6b98:	f8c4 8000 	str.w	r8, [r4]
        desc0_out->extents[i] = extent1;
   d6b9c:	f844 0c10 	str.w	r0, [r4, #-16]
   d6ba0:	e005      	b.n	d6bae <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0xb6>
      } else {
        TFLITE_DCHECK_EQ(extent1, 1);
   d6ba2:	2801      	cmp	r0, #1
   d6ba4:	d1af      	bne.n	d6b06 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0xe>
        desc1_out->strides[i] = 0;
   d6ba6:	f8c5 8000 	str.w	r8, [r5]
        desc1_out->extents[i] = extent0;
   d6baa:	f845 7c10 	str.w	r7, [r5, #-16]
  }

  // Walk over each dimension. If the extents are equal do nothing.
  // Otherwise, set the desc with extent 1 to have extent equal to the other and
  // stride 0.
  for (int i = 0; i < N; ++i) {
   d6bae:	3601      	adds	r6, #1
   d6bb0:	2e04      	cmp	r6, #4
   d6bb2:	f105 0504 	add.w	r5, r5, #4
   d6bb6:	f104 0404 	add.w	r4, r4, #4
   d6bba:	d1e0      	bne.n	d6b7e <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0x86>
    NdArrayDesc<N>* desc0_out, NdArrayDesc<N>* desc1_out) {
  TFLITE_DCHECK(desc0_out != nullptr);
  TFLITE_DCHECK(desc1_out != nullptr);

  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);
   d6bbc:	a805      	add	r0, sp, #20
   d6bbe:	f7ff fc42 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
    const RuntimeShape& input0_shape, const RuntimeShape& input1_shape,
    NdArrayDesc<N>* desc0_out, NdArrayDesc<N>* desc1_out) {
  TFLITE_DCHECK(desc0_out != nullptr);
  TFLITE_DCHECK(desc1_out != nullptr);

  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
   d6bc2:	4668      	mov	r0, sp
   d6bc4:	f7ff fc3f 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
        desc1_out->strides[i] = 0;
        desc1_out->extents[i] = extent0;
      }
    }
  }
}
   d6bc8:	b00a      	add	sp, #40	; 0x28
   d6bca:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}

000d6bce <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf>:
                               const RuntimeShape& input1_shape,
                               const float* input1_data,
                               const RuntimeShape& input2_shape,
                               const float* input2_data,
                               const RuntimeShape& output_shape,
                               float* output_data) {
   d6bce:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d6bd2:	b09b      	sub	sp, #108	; 0x6c
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
   d6bd4:	ad0a      	add	r5, sp, #40	; 0x28
                               const RuntimeShape& input1_shape,
                               const float* input1_data,
                               const RuntimeShape& input2_shape,
                               const float* input2_data,
                               const RuntimeShape& output_shape,
                               float* output_data) {
   d6bd6:	9203      	str	r2, [sp, #12]
   d6bd8:	4683      	mov	fp, r0
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
   d6bda:	462a      	mov	r2, r5
                               const RuntimeShape& input1_shape,
                               const float* input1_data,
                               const RuntimeShape& input2_shape,
                               const float* input2_data,
                               const RuntimeShape& output_shape,
                               float* output_data) {
   d6bdc:	4608      	mov	r0, r1
   d6bde:	4619      	mov	r1, r3
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
   d6be0:	ab12      	add	r3, sp, #72	; 0x48
   d6be2:	f7ff ff89 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
   d6be6:	2301      	movs	r3, #1
   d6be8:	9a25      	ldr	r2, [sp, #148]	; 0x94
   d6bea:	2104      	movs	r1, #4
   d6bec:	a805      	add	r0, sp, #20
   d6bee:	f7ff fc6e 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   d6bf2:	2400      	movs	r4, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              ActivationFunctionWithMinMax(
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
   d6bf4:	462f      	mov	r7, r5
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   d6bf6:	2100      	movs	r1, #0
   d6bf8:	a805      	add	r0, sp, #20
   d6bfa:	f7ff fc2f 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d6bfe:	4284      	cmp	r4, r0
   d6c00:	da61      	bge.n	d6cc6 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0xf8>
   d6c02:	2500      	movs	r5, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
   d6c04:	f10d 0914 	add.w	r9, sp, #20
   d6c08:	2101      	movs	r1, #1
   d6c0a:	4648      	mov	r0, r9
   d6c0c:	f7ff fc26 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d6c10:	4285      	cmp	r5, r0
   d6c12:	da56      	bge.n	d6cc2 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0xf4>
   d6c14:	f04f 0800 	mov.w	r8, #0
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
   d6c18:	464e      	mov	r6, r9
   d6c1a:	2102      	movs	r1, #2
   d6c1c:	4630      	mov	r0, r6
   d6c1e:	f7ff fc1d 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d6c22:	4580      	cmp	r8, r0
   d6c24:	da4b      	bge.n	d6cbe <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0xf0>
   d6c26:	f04f 0900 	mov.w	r9, #0
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   d6c2a:	2103      	movs	r1, #3
   d6c2c:	4630      	mov	r0, r6
   d6c2e:	f7ff fc15 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d6c32:	4581      	cmp	r9, r0
   d6c34:	da40      	bge.n	d6cb8 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0xea>
          output_data[Offset(extended_output_shape, b, y, x, c)] =
   d6c36:	f8cd 9000 	str.w	r9, [sp]
   d6c3a:	4643      	mov	r3, r8
   d6c3c:	462a      	mov	r2, r5
   d6c3e:	4621      	mov	r1, r4
   d6c40:	4630      	mov	r0, r6
   d6c42:	f7ff fc70 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   d6c46:	9b26      	ldr	r3, [sp, #152]	; 0x98
              ActivationFunctionWithMinMax(
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
   d6c48:	f8cd 9000 	str.w	r9, [sp]
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          output_data[Offset(extended_output_shape, b, y, x, c)] =
   d6c4c:	eb03 0380 	add.w	r3, r3, r0, lsl #2
   d6c50:	9302      	str	r3, [sp, #8]
              ActivationFunctionWithMinMax(
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
   d6c52:	462a      	mov	r2, r5
   d6c54:	4643      	mov	r3, r8
   d6c56:	4621      	mov	r1, r4
   d6c58:	4638      	mov	r0, r7
   d6c5a:	f7ff fd15 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                      input2_data[SubscriptToIndex(desc2, b, y, x, c)],
   d6c5e:	f8cd 9000 	str.w	r9, [sp]
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              ActivationFunctionWithMinMax(
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
   d6c62:	4682      	mov	sl, r0
                      input2_data[SubscriptToIndex(desc2, b, y, x, c)],
   d6c64:	4643      	mov	r3, r8
   d6c66:	462a      	mov	r2, r5
   d6c68:	4621      	mov	r1, r4
   d6c6a:	a812      	add	r0, sp, #72	; 0x48
   d6c6c:	f7ff fd0c 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              ActivationFunctionWithMinMax(
   d6c70:	9b03      	ldr	r3, [sp, #12]
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
                      input2_data[SubscriptToIndex(desc2, b, y, x, c)],
                  params.float_activation_min, params.float_activation_max);
   d6c72:	eddb 6a0e 	vldr	s13, [fp, #56]	; 0x38
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              ActivationFunctionWithMinMax(
   d6c76:	eb03 0a8a 	add.w	sl, r3, sl, lsl #2
   d6c7a:	9b24      	ldr	r3, [sp, #144]	; 0x90
   d6c7c:	edda 7a00 	vldr	s15, [sl]
   d6c80:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   d6c84:	ed90 7a00 	vldr	s14, [r0]
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
                      input2_data[SubscriptToIndex(desc2, b, y, x, c)],
                  params.float_activation_min, params.float_activation_max);
   d6c88:	9b02      	ldr	r3, [sp, #8]
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              ActivationFunctionWithMinMax(
   d6c8a:	ee37 7a87 	vadd.f32	s14, s15, s14
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
                      input2_data[SubscriptToIndex(desc2, b, y, x, c)],
                  params.float_activation_min, params.float_activation_max);
   d6c8e:	eddb 7a0d 	vldr	s15, [fp, #52]	; 0x34
	return __b;
      return __a;
   d6c92:	eeb4 7ae7 	vcmpe.f32	s14, s15
   d6c96:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d6c9a:	bf58      	it	pl
   d6c9c:	eef0 7a47 	vmovpl.f32	s15, s14
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   d6ca0:	eef4 6a67 	vcmp.f32	s13, s15
   d6ca4:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d6ca8:	bf48      	it	mi
   d6caa:	eef0 7a66 	vmovmi.f32	s15, s13
   d6cae:	edc3 7a00 	vstr	s15, [r3]
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   d6cb2:	f109 0901 	add.w	r9, r9, #1
   d6cb6:	e7b8      	b.n	d6c2a <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0x5c>
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
   d6cb8:	f108 0801 	add.w	r8, r8, #1
   d6cbc:	e7ad      	b.n	d6c1a <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0x4c>
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
   d6cbe:	3501      	adds	r5, #1
   d6cc0:	e7a0      	b.n	d6c04 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0x36>
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   d6cc2:	3401      	adds	r4, #1
   d6cc4:	e797      	b.n	d6bf6 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0x28>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
  const RuntimeShape extended_output_shape =
      RuntimeShape::ExtendedShape(4, output_shape);
   d6cc6:	a805      	add	r0, sp, #20
   d6cc8:	f7ff fbbd 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                  params.float_activation_min, params.float_activation_max);
        }
      }
    }
  }
}
   d6ccc:	b01b      	add	sp, #108	; 0x6c
   d6cce:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

000d6cd4 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_>:
  return kTfLiteOk;
}

void EvalAdd(TfLiteContext* context, TfLiteNode* node, TfLiteAddParams* params,
             const OpData* data, const TfLiteTensor* input1,
             const TfLiteTensor* input2, TfLiteTensor* output) {
   d6cd4:	b5f0      	push	{r4, r5, r6, r7, lr}
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
   d6cd6:	7812      	ldrb	r2, [r2, #0]
  return kTfLiteOk;
}

void EvalAdd(TfLiteContext* context, TfLiteNode* node, TfLiteAddParams* params,
             const OpData* data, const TfLiteTensor* input1,
             const TfLiteTensor* input2, TfLiteTensor* output) {
   d6cd8:	b0a9      	sub	sp, #164	; 0xa4
// Calculates the useful range of an activation layer given its activation
// tensor.a
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
   d6cda:	2a01      	cmp	r2, #1
   d6cdc:	9e2e      	ldr	r6, [sp, #184]	; 0xb8
   d6cde:	9d2f      	ldr	r5, [sp, #188]	; 0xbc
   d6ce0:	9c30      	ldr	r4, [sp, #192]	; 0xc0
   d6ce2:	d011      	beq.n	d6d08 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x34>
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
   d6ce4:	2a03      	cmp	r2, #3
   d6ce6:	d012      	beq.n	d6d0e <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x3a>
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
    *activation_max = std::numeric_limits<T>::max();
   d6ce8:	ed9f 7a3c 	vldr	s14, [pc, #240]	; d6ddc <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x108>
   d6cec:	eddf 6a3c 	vldr	s13, [pc, #240]	; d6de0 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x10c>
   d6cf0:	2a02      	cmp	r2, #2
   d6cf2:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   d6cf6:	bf18      	it	ne
   d6cf8:	eef0 7a47 	vmovne.f32	s15, s14
   d6cfc:	eebf 7a00 	vmov.f32	s14, #240	; 0xbf800000 -1.0
   d6d00:	bf18      	it	ne
   d6d02:	eeb0 7a66 	vmovne.f32	s14, s13
   d6d06:	e006      	b.n	d6d16 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x42>
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
   d6d08:	eddf 7a34 	vldr	s15, [pc, #208]	; d6ddc <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x108>
   d6d0c:	e001      	b.n	d6d12 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x3e>
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
    *activation_max = 6;
   d6d0e:	eef1 7a08 	vmov.f32	s15, #24	; 0x40c00000  6.0
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
   d6d12:	ed9f 7a34 	vldr	s14, [pc, #208]	; d6de4 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x110>
#define TF_LITE_ADD(opname)                                                   \
  reference_ops::opname(op_params, GetTensorShape(input1),                    \
                        GetTensorData<float>(input1), GetTensorShape(input2), \
                        GetTensorData<float>(input2), GetTensorShape(output), \
                        GetTensorData<float>(output))
  if (data->requires_broadcast) {
   d6d16:	781b      	ldrb	r3, [r3, #0]
  int output_shift;
};

template <typename P>
inline void SetActivationParams(float min, float max, P* params) {
  params->float_activation_min = min;
   d6d18:	ed8d 7a21 	vstr	s14, [sp, #132]	; 0x84
  params->float_activation_max = max;
   d6d1c:	edcd 7a22 	vstr	s15, [sp, #136]	; 0x88
   d6d20:	af0f      	add	r7, sp, #60	; 0x3c
    TF_LITE_ADD(BroadcastAdd4DSlow);
   d6d22:	4631      	mov	r1, r6
   d6d24:	a805      	add	r0, sp, #20
#define TF_LITE_ADD(opname)                                                   \
  reference_ops::opname(op_params, GetTensorShape(input1),                    \
                        GetTensorData<float>(input1), GetTensorShape(input2), \
                        GetTensorData<float>(input2), GetTensorShape(output), \
                        GetTensorData<float>(output))
  if (data->requires_broadcast) {
   d6d26:	b1cb      	cbz	r3, d6d5c <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x88>
    TF_LITE_ADD(BroadcastAdd4DSlow);
   d6d28:	f7ff fe3d 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d6d2c:	b106      	cbz	r6, d6d30 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x5c>
   d6d2e:	6876      	ldr	r6, [r6, #4]
   d6d30:	4629      	mov	r1, r5
   d6d32:	a80a      	add	r0, sp, #40	; 0x28
   d6d34:	f7ff fe37 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d6d38:	b105      	cbz	r5, d6d3c <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x68>
   d6d3a:	686d      	ldr	r5, [r5, #4]
   d6d3c:	4621      	mov	r1, r4
   d6d3e:	4638      	mov	r0, r7
   d6d40:	f7ff fe31 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d6d44:	b104      	cbz	r4, d6d48 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x74>
   d6d46:	6864      	ldr	r4, [r4, #4]
   d6d48:	9402      	str	r4, [sp, #8]
   d6d4a:	e88d 00a0 	stmia.w	sp, {r5, r7}
   d6d4e:	ab0a      	add	r3, sp, #40	; 0x28
   d6d50:	4632      	mov	r2, r6
   d6d52:	a905      	add	r1, sp, #20
   d6d54:	a814      	add	r0, sp, #80	; 0x50
   d6d56:	f7ff ff3a 	bl	d6bce <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf>
   d6d5a:	e033      	b.n	d6dc4 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0xf0>
  } else {
    TF_LITE_ADD(Add);
   d6d5c:	f7ff fe23 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d6d60:	b106      	cbz	r6, d6d64 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x90>
   d6d62:	6876      	ldr	r6, [r6, #4]
   d6d64:	4629      	mov	r1, r5
   d6d66:	a80a      	add	r0, sp, #40	; 0x28
   d6d68:	f7ff fe1d 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d6d6c:	b105      	cbz	r5, d6d70 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x9c>
   d6d6e:	686d      	ldr	r5, [r5, #4]
   d6d70:	4621      	mov	r1, r4
   d6d72:	4638      	mov	r0, r7
   d6d74:	f7ff fe17 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d6d78:	b104      	cbz	r4, d6d7c <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0xa8>
   d6d7a:	6864      	ldr	r4, [r4, #4]

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const float* input1_data,
                const RuntimeShape& input2_shape, const float* input2_data,
                const RuntimeShape& output_shape, float* output_data) {
  const int size = MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d6d7c:	463a      	mov	r2, r7
   d6d7e:	a90a      	add	r1, sp, #40	; 0x28
   d6d80:	a805      	add	r0, sp, #20
   d6d82:	f7ff fbf2 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
  for (int i = 0; i < size; i++) {
   d6d86:	2300      	movs	r3, #0
   d6d88:	4298      	cmp	r0, r3
   d6d8a:	dd1b      	ble.n	d6dc4 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0xf0>
    auto x = input1_data[i] + input2_data[i];
   d6d8c:	ecf6 7a01 	vldmia	r6!, {s15}
   d6d90:	ecb5 7a01 	vldmia	r5!, {s14}
    output_data[i] = ActivationFunctionWithMinMax(
        x, params.float_activation_min, params.float_activation_max);
   d6d94:	eddd 6a22 	vldr	s13, [sp, #136]	; 0x88
                const RuntimeShape& input1_shape, const float* input1_data,
                const RuntimeShape& input2_shape, const float* input2_data,
                const RuntimeShape& output_shape, float* output_data) {
  const int size = MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int i = 0; i < size; i++) {
    auto x = input1_data[i] + input2_data[i];
   d6d98:	ee37 7a87 	vadd.f32	s14, s15, s14
    output_data[i] = ActivationFunctionWithMinMax(
        x, params.float_activation_min, params.float_activation_max);
   d6d9c:	eddd 7a21 	vldr	s15, [sp, #132]	; 0x84
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
	return __b;
      return __a;
   d6da0:	eeb4 7ae7 	vcmpe.f32	s14, s15
   d6da4:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d6da8:	bf58      	it	pl
   d6daa:	eef0 7a47 	vmovpl.f32	s15, s14
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   d6dae:	eef4 6a67 	vcmp.f32	s13, s15
   d6db2:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d6db6:	bf48      	it	mi
   d6db8:	eef0 7a66 	vmovmi.f32	s15, s13
   d6dbc:	ece4 7a01 	vstmia	r4!, {s15}
inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const float* input1_data,
                const RuntimeShape& input2_shape, const float* input2_data,
                const RuntimeShape& output_shape, float* output_data) {
  const int size = MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int i = 0; i < size; i++) {
   d6dc0:	3301      	adds	r3, #1
   d6dc2:	e7e1      	b.n	d6d88 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0xb4>
   d6dc4:	4638      	mov	r0, r7
   d6dc6:	f7ff fb3e 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   d6dca:	a80a      	add	r0, sp, #40	; 0x28
   d6dcc:	f7ff fb3b 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   d6dd0:	a805      	add	r0, sp, #20
   d6dd2:	f7ff fb38 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  }
#undef TF_LITE_ADD
}
   d6dd6:	b029      	add	sp, #164	; 0xa4
   d6dd8:	bdf0      	pop	{r4, r5, r6, r7, pc}
   d6dda:	bf00      	nop
   d6ddc:	7f7fffff 	.word	0x7f7fffff
   d6de0:	ff7fffff 	.word	0xff7fffff
   d6de4:	00000000 	.word	0x00000000

000d6de8 <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa>:
                               const RuntimeShape& input1_shape,
                               const int8_t* input1_data,
                               const RuntimeShape& input2_shape,
                               const int8_t* input2_data,
                               const RuntimeShape& output_shape,
                               int8_t* output_data) {
   d6de8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d6dec:	b09b      	sub	sp, #108	; 0x6c
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
   d6dee:	ae0a      	add	r6, sp, #40	; 0x28
                               const RuntimeShape& input1_shape,
                               const int8_t* input1_data,
                               const RuntimeShape& input2_shape,
                               const int8_t* input2_data,
                               const RuntimeShape& output_shape,
                               int8_t* output_data) {
   d6df0:	4604      	mov	r4, r0
   d6df2:	4693      	mov	fp, r2
   d6df4:	4608      	mov	r0, r1
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
   d6df6:	4632      	mov	r2, r6
                               const RuntimeShape& input1_shape,
                               const int8_t* input1_data,
                               const RuntimeShape& input2_shape,
                               const int8_t* input2_data,
                               const RuntimeShape& output_shape,
                               int8_t* output_data) {
   d6df8:	4619      	mov	r1, r3
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
   d6dfa:	ab12      	add	r3, sp, #72	; 0x48
   d6dfc:	f7ff fe7c 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
   d6e00:	2301      	movs	r3, #1
   d6e02:	9a25      	ldr	r2, [sp, #148]	; 0x94
   d6e04:	2104      	movs	r1, #4
   d6e06:	a805      	add	r0, sp, #20
   d6e08:	f7ff fb61 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   d6e0c:	2500      	movs	r5, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          const int32_t input1_val =
              params.input1_offset +
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d6e0e:	9602      	str	r6, [sp, #8]
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   d6e10:	2100      	movs	r1, #0
   d6e12:	a805      	add	r0, sp, #20
   d6e14:	f7ff fb22 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d6e18:	4285      	cmp	r5, r0
   d6e1a:	da65      	bge.n	d6ee8 <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x100>
   d6e1c:	2600      	movs	r6, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
   d6e1e:	f10d 0814 	add.w	r8, sp, #20
   d6e22:	2101      	movs	r1, #1
   d6e24:	4640      	mov	r0, r8
   d6e26:	f7ff fb19 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d6e2a:	4286      	cmp	r6, r0
   d6e2c:	da5a      	bge.n	d6ee4 <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0xfc>
   d6e2e:	2700      	movs	r7, #0
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
   d6e30:	2102      	movs	r1, #2
   d6e32:	4640      	mov	r0, r8
   d6e34:	f7ff fb12 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d6e38:	4287      	cmp	r7, r0
   d6e3a:	da51      	bge.n	d6ee0 <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0xf8>
   d6e3c:	f04f 0900 	mov.w	r9, #0
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   d6e40:	2103      	movs	r1, #3
   d6e42:	4640      	mov	r0, r8
   d6e44:	f7ff fb0a 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d6e48:	4581      	cmp	r9, r0
   d6e4a:	da47      	bge.n	d6edc <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0xf4>
          const int32_t input1_val =
              params.input1_offset +
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d6e4c:	f8cd 9000 	str.w	r9, [sp]
   d6e50:	463b      	mov	r3, r7
   d6e52:	4632      	mov	r2, r6
   d6e54:	4629      	mov	r1, r5
   d6e56:	9802      	ldr	r0, [sp, #8]
   d6e58:	f7ff fc16 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   d6e5c:	6863      	ldr	r3, [r4, #4]
   d6e5e:	f91b a000 	ldrsb.w	sl, [fp, r0]
          const int32_t input2_val =
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d6e62:	f8cd 9000 	str.w	r9, [sp]
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          const int32_t input1_val =
              params.input1_offset +
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d6e66:	449a      	add	sl, r3
          const int32_t input2_val =
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d6e68:	4632      	mov	r2, r6
   d6e6a:	463b      	mov	r3, r7
   d6e6c:	4629      	mov	r1, r5
   d6e6e:	a812      	add	r0, sp, #72	; 0x48
   d6e70:	f7ff fc0a 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32_t shifted_input1_val =
              input1_val * (1 << params.left_shift);
          const int32_t shifted_input2_val =
              input2_val * (1 << params.left_shift);
   d6e74:	9b24      	ldr	r3, [sp, #144]	; 0x90
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32_t input2_val =
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32_t shifted_input1_val =
              input1_val * (1 << params.left_shift);
   d6e76:	f8d4 e018 	ldr.w	lr, [r4, #24]
          const int32_t shifted_input2_val =
              input2_val * (1 << params.left_shift);
   d6e7a:	561a      	ldrsb	r2, [r3, r0]
   d6e7c:	68a3      	ldr	r3, [r4, #8]
          const int32_t scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
   d6e7e:	69e1      	ldr	r1, [r4, #28]
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32_t shifted_input1_val =
              input1_val * (1 << params.left_shift);
          const int32_t shifted_input2_val =
              input2_val * (1 << params.left_shift);
   d6e80:	4413      	add	r3, r2
   d6e82:	fa03 f30e 	lsl.w	r3, r3, lr
          const int32_t scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
   d6e86:	fa0a f00e 	lsl.w	r0, sl, lr
   d6e8a:	6a22      	ldr	r2, [r4, #32]
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32_t shifted_input1_val =
              input1_val * (1 << params.left_shift);
          const int32_t shifted_input2_val =
              input2_val * (1 << params.left_shift);
   d6e8c:	9303      	str	r3, [sp, #12]
          const int32_t scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
   d6e8e:	f7ff fba9 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32_t scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, params.input2_multiplier,
                  params.input2_shift);
   d6e92:	9b03      	ldr	r3, [sp, #12]
   d6e94:	6aa2      	ldr	r2, [r4, #40]	; 0x28
   d6e96:	6a61      	ldr	r1, [r4, #36]	; 0x24
          const int32_t shifted_input2_val =
              input2_val * (1 << params.left_shift);
          const int32_t scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
   d6e98:	4682      	mov	sl, r0
          const int32_t scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, params.input2_multiplier,
                  params.input2_shift);
   d6e9a:	4618      	mov	r0, r3
   d6e9c:	f7ff fba2 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32_t raw_sum = scaled_input1_val + scaled_input2_val;
          const int32_t raw_output =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
   d6ea0:	6962      	ldr	r2, [r4, #20]
   d6ea2:	6921      	ldr	r1, [r4, #16]
   d6ea4:	4450      	add	r0, sl
   d6ea6:	f7ff fb9d 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
   d6eaa:	68e3      	ldr	r3, [r4, #12]
                  raw_sum, params.output_multiplier, params.output_shift) +
              params.output_offset;
          const int32_t clamped_output =
              std::min(params.quantized_activation_max,
                       std::max(params.quantized_activation_min, raw_output));
          output_data[Offset(extended_output_shape, b, y, x, c)] =
   d6eac:	f8cd 9000 	str.w	r9, [sp]
   d6eb0:	4418      	add	r0, r3
   d6eb2:	6ae3      	ldr	r3, [r4, #44]	; 0x2c
   d6eb4:	4283      	cmp	r3, r0
   d6eb6:	bfb8      	it	lt
   d6eb8:	4603      	movlt	r3, r0
   d6eba:	6b20      	ldr	r0, [r4, #48]	; 0x30
   d6ebc:	4283      	cmp	r3, r0
   d6ebe:	bfa8      	it	ge
   d6ec0:	4603      	movge	r3, r0
   d6ec2:	469a      	mov	sl, r3
   d6ec4:	4632      	mov	r2, r6
   d6ec6:	463b      	mov	r3, r7
   d6ec8:	4629      	mov	r1, r5
   d6eca:	4640      	mov	r0, r8
   d6ecc:	f7ff fb2b 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<int8_t>(clamped_output);
   d6ed0:	9b26      	ldr	r3, [sp, #152]	; 0x98
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   d6ed2:	f109 0901 	add.w	r9, r9, #1
              params.output_offset;
          const int32_t clamped_output =
              std::min(params.quantized_activation_max,
                       std::max(params.quantized_activation_min, raw_output));
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              static_cast<int8_t>(clamped_output);
   d6ed6:	f803 a000 	strb.w	sl, [r3, r0]
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   d6eda:	e7b1      	b.n	d6e40 <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x58>
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
   d6edc:	3701      	adds	r7, #1
   d6ede:	e7a7      	b.n	d6e30 <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x48>
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
   d6ee0:	3601      	adds	r6, #1
   d6ee2:	e79c      	b.n	d6e1e <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x36>
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   d6ee4:	3501      	adds	r5, #1
   d6ee6:	e793      	b.n	d6e10 <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x28>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
  const RuntimeShape extended_output_shape =
      RuntimeShape::ExtendedShape(4, output_shape);
   d6ee8:	a805      	add	r0, sp, #20
   d6eea:	f7ff faac 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
              static_cast<int8_t>(clamped_output);
        }
      }
    }
  }
}
   d6eee:	b01b      	add	sp, #108	; 0x6c
   d6ef0:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d6ef4 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph>:
                               const RuntimeShape& input1_shape,
                               const uint8* input1_data,
                               const RuntimeShape& input2_shape,
                               const uint8* input2_data,
                               const RuntimeShape& output_shape,
                               uint8* output_data) {
   d6ef4:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d6ef8:	b09b      	sub	sp, #108	; 0x6c
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
   d6efa:	ae0a      	add	r6, sp, #40	; 0x28
                               const RuntimeShape& input1_shape,
                               const uint8* input1_data,
                               const RuntimeShape& input2_shape,
                               const uint8* input2_data,
                               const RuntimeShape& output_shape,
                               uint8* output_data) {
   d6efc:	4604      	mov	r4, r0
   d6efe:	4693      	mov	fp, r2
   d6f00:	4608      	mov	r0, r1
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
   d6f02:	4632      	mov	r2, r6
                               const RuntimeShape& input1_shape,
                               const uint8* input1_data,
                               const RuntimeShape& input2_shape,
                               const uint8* input2_data,
                               const RuntimeShape& output_shape,
                               uint8* output_data) {
   d6f04:	4619      	mov	r1, r3
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
   d6f06:	ab12      	add	r3, sp, #72	; 0x48
   d6f08:	f7ff fdf6 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
   d6f0c:	2301      	movs	r3, #1
   d6f0e:	9a25      	ldr	r2, [sp, #148]	; 0x94
   d6f10:	2104      	movs	r1, #4
   d6f12:	a805      	add	r0, sp, #20
   d6f14:	f7ff fadb 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   d6f18:	2500      	movs	r5, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          const int32 input1_val =
              params.input1_offset +
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d6f1a:	9602      	str	r6, [sp, #8]
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   d6f1c:	2100      	movs	r1, #0
   d6f1e:	a805      	add	r0, sp, #20
   d6f20:	f7ff fa9c 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d6f24:	4285      	cmp	r5, r0
   d6f26:	da65      	bge.n	d6ff4 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x100>
   d6f28:	2600      	movs	r6, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
   d6f2a:	f10d 0814 	add.w	r8, sp, #20
   d6f2e:	2101      	movs	r1, #1
   d6f30:	4640      	mov	r0, r8
   d6f32:	f7ff fa93 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d6f36:	4286      	cmp	r6, r0
   d6f38:	da5a      	bge.n	d6ff0 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0xfc>
   d6f3a:	2700      	movs	r7, #0
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
   d6f3c:	2102      	movs	r1, #2
   d6f3e:	4640      	mov	r0, r8
   d6f40:	f7ff fa8c 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d6f44:	4287      	cmp	r7, r0
   d6f46:	da51      	bge.n	d6fec <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0xf8>
   d6f48:	f04f 0900 	mov.w	r9, #0
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   d6f4c:	2103      	movs	r1, #3
   d6f4e:	4640      	mov	r0, r8
   d6f50:	f7ff fa84 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d6f54:	4581      	cmp	r9, r0
   d6f56:	da47      	bge.n	d6fe8 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0xf4>
          const int32 input1_val =
              params.input1_offset +
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d6f58:	f8cd 9000 	str.w	r9, [sp]
   d6f5c:	463b      	mov	r3, r7
   d6f5e:	4632      	mov	r2, r6
   d6f60:	4629      	mov	r1, r5
   d6f62:	9802      	ldr	r0, [sp, #8]
   d6f64:	f7ff fb90 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   d6f68:	6863      	ldr	r3, [r4, #4]
   d6f6a:	f81b a000 	ldrb.w	sl, [fp, r0]
          const int32 input2_val =
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d6f6e:	f8cd 9000 	str.w	r9, [sp]
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          const int32 input1_val =
              params.input1_offset +
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d6f72:	449a      	add	sl, r3
          const int32 input2_val =
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d6f74:	4632      	mov	r2, r6
   d6f76:	463b      	mov	r3, r7
   d6f78:	4629      	mov	r1, r5
   d6f7a:	a812      	add	r0, sp, #72	; 0x48
   d6f7c:	f7ff fb84 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val =
              input1_val * (1 << params.left_shift);
          const int32 shifted_input2_val =
              input2_val * (1 << params.left_shift);
   d6f80:	9b24      	ldr	r3, [sp, #144]	; 0x90
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val =
              input1_val * (1 << params.left_shift);
   d6f82:	f8d4 e018 	ldr.w	lr, [r4, #24]
          const int32 shifted_input2_val =
              input2_val * (1 << params.left_shift);
   d6f86:	5c1a      	ldrb	r2, [r3, r0]
   d6f88:	68a3      	ldr	r3, [r4, #8]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
   d6f8a:	69e1      	ldr	r1, [r4, #28]
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val =
              input1_val * (1 << params.left_shift);
          const int32 shifted_input2_val =
              input2_val * (1 << params.left_shift);
   d6f8c:	4413      	add	r3, r2
   d6f8e:	fa03 f30e 	lsl.w	r3, r3, lr
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
   d6f92:	fa0a f00e 	lsl.w	r0, sl, lr
   d6f96:	6a22      	ldr	r2, [r4, #32]
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val =
              input1_val * (1 << params.left_shift);
          const int32 shifted_input2_val =
              input2_val * (1 << params.left_shift);
   d6f98:	9303      	str	r3, [sp, #12]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
   d6f9a:	f7ff fb23 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, params.input2_multiplier,
                  params.input2_shift);
   d6f9e:	9b03      	ldr	r3, [sp, #12]
   d6fa0:	6aa2      	ldr	r2, [r4, #40]	; 0x28
   d6fa2:	6a61      	ldr	r1, [r4, #36]	; 0x24
          const int32 shifted_input2_val =
              input2_val * (1 << params.left_shift);
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
   d6fa4:	4682      	mov	sl, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, params.input2_multiplier,
                  params.input2_shift);
   d6fa6:	4618      	mov	r0, r3
   d6fa8:	f7ff fb1c 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 raw_sum = scaled_input1_val + scaled_input2_val;
          const int32 raw_output =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
   d6fac:	6962      	ldr	r2, [r4, #20]
   d6fae:	6921      	ldr	r1, [r4, #16]
   d6fb0:	4450      	add	r0, sl
   d6fb2:	f7ff fb17 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
   d6fb6:	68e3      	ldr	r3, [r4, #12]
                  raw_sum, params.output_multiplier, params.output_shift) +
              params.output_offset;
          const int32 clamped_output =
              std::min(params.quantized_activation_max,
                       std::max(params.quantized_activation_min, raw_output));
          output_data[Offset(extended_output_shape, b, y, x, c)] =
   d6fb8:	f8cd 9000 	str.w	r9, [sp]
   d6fbc:	4418      	add	r0, r3
   d6fbe:	6ae3      	ldr	r3, [r4, #44]	; 0x2c
   d6fc0:	4283      	cmp	r3, r0
   d6fc2:	bfb8      	it	lt
   d6fc4:	4603      	movlt	r3, r0
   d6fc6:	6b20      	ldr	r0, [r4, #48]	; 0x30
   d6fc8:	4283      	cmp	r3, r0
   d6fca:	bfa8      	it	ge
   d6fcc:	4603      	movge	r3, r0
   d6fce:	469a      	mov	sl, r3
   d6fd0:	4632      	mov	r2, r6
   d6fd2:	463b      	mov	r3, r7
   d6fd4:	4629      	mov	r1, r5
   d6fd6:	4640      	mov	r0, r8
   d6fd8:	f7ff faa5 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<uint8>(clamped_output);
   d6fdc:	9b26      	ldr	r3, [sp, #152]	; 0x98
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   d6fde:	f109 0901 	add.w	r9, r9, #1
              params.output_offset;
          const int32 clamped_output =
              std::min(params.quantized_activation_max,
                       std::max(params.quantized_activation_min, raw_output));
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              static_cast<uint8>(clamped_output);
   d6fe2:	f803 a000 	strb.w	sl, [r3, r0]
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   d6fe6:	e7b1      	b.n	d6f4c <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x58>
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
   d6fe8:	3701      	adds	r7, #1
   d6fea:	e7a7      	b.n	d6f3c <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x48>
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
   d6fec:	3601      	adds	r6, #1
   d6fee:	e79c      	b.n	d6f2a <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x36>
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   d6ff0:	3501      	adds	r5, #1
   d6ff2:	e793      	b.n	d6f1c <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x28>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
  const RuntimeShape extended_output_shape =
      RuntimeShape::ExtendedShape(4, output_shape);
   d6ff4:	a805      	add	r0, sp, #20
   d6ff6:	f7ff fa26 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
              static_cast<uint8>(clamped_output);
        }
      }
    }
  }
}
   d6ffa:	b01b      	add	sp, #108	; 0x6c
   d6ffc:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d7000 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4>:

TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
   d7000:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
   d7004:	b0a8      	sub	sp, #160	; 0xa0
   d7006:	461e      	mov	r6, r3
                              const TfLiteTensor* input1,
                              const TfLiteTensor* input2,
                              TfLiteTensor* output) {
  if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt8) {
    tflite::ArithmeticParams op_params;
    op_params.left_shift = data->left_shift;
   d7008:	6a43      	ldr	r3, [r0, #36]	; 0x24
   d700a:	931a      	str	r3, [sp, #104]	; 0x68
    op_params.input1_offset = data->input1_offset;
   d700c:	6a83      	ldr	r3, [r0, #40]	; 0x28
   d700e:	9315      	str	r3, [sp, #84]	; 0x54
    op_params.input1_multiplier = data->input1_multiplier;
   d7010:	6943      	ldr	r3, [r0, #20]
   d7012:	931b      	str	r3, [sp, #108]	; 0x6c
    op_params.input1_shift = data->input1_shift;
   d7014:	6843      	ldr	r3, [r0, #4]
   d7016:	931c      	str	r3, [sp, #112]	; 0x70
    op_params.input2_offset = data->input2_offset;
   d7018:	6ac3      	ldr	r3, [r0, #44]	; 0x2c
   d701a:	9316      	str	r3, [sp, #88]	; 0x58
    op_params.input2_multiplier = data->input2_multiplier;
   d701c:	6983      	ldr	r3, [r0, #24]
   d701e:	931d      	str	r3, [sp, #116]	; 0x74
    op_params.input2_shift = data->input2_shift;
   d7020:	6883      	ldr	r3, [r0, #8]
   d7022:	931e      	str	r3, [sp, #120]	; 0x78
    op_params.output_offset = data->output_offset;
   d7024:	6b03      	ldr	r3, [r0, #48]	; 0x30
   d7026:	9317      	str	r3, [sp, #92]	; 0x5c
    op_params.output_multiplier = data->output_multiplier;
   d7028:	69c3      	ldr	r3, [r0, #28]
   d702a:	9318      	str	r3, [sp, #96]	; 0x60
    op_params.output_shift = data->output_shift;
   d702c:	6a03      	ldr	r3, [r0, #32]
   d702e:	9319      	str	r3, [sp, #100]	; 0x64
    TF_LITE_ADD(Add);
  }
#undef TF_LITE_ADD
}

TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
   d7030:	4614      	mov	r4, r2
    op_params.input2_shift = data->input2_shift;
    op_params.output_offset = data->output_offset;
    op_params.output_multiplier = data->output_multiplier;
    op_params.output_shift = data->output_shift;
    SetActivationParams(data->output_activation_min,
                        data->output_activation_max, &op_params);
   d7032:	6903      	ldr	r3, [r0, #16]
  params->float_activation_max = max;
}

template <typename P>
inline void SetActivationParams(int32 min, int32 max, P* params) {
  params->quantized_activation_min = min;
   d7034:	68c2      	ldr	r2, [r0, #12]
  params->quantized_activation_max = max;
   d7036:	9320      	str	r3, [sp, #128]	; 0x80
    bool need_broadcast = reference_ops::ProcessBroadcastShapes(
        GetTensorShape(input1), GetTensorShape(input2), &op_params);
   d7038:	a80f      	add	r0, sp, #60	; 0x3c
    TF_LITE_ADD(Add);
  }
#undef TF_LITE_ADD
}

TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
   d703a:	460d      	mov	r5, r1
  params->float_activation_max = max;
}

template <typename P>
inline void SetActivationParams(int32 min, int32 max, P* params) {
  params->quantized_activation_min = min;
   d703c:	921f      	str	r2, [sp, #124]	; 0x7c
    op_params.output_multiplier = data->output_multiplier;
    op_params.output_shift = data->output_shift;
    SetActivationParams(data->output_activation_min,
                        data->output_activation_max, &op_params);
    bool need_broadcast = reference_ops::ProcessBroadcastShapes(
        GetTensorShape(input1), GetTensorShape(input2), &op_params);
   d703e:	f7ff fcb2 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d7042:	4621      	mov	r1, r4
   d7044:	a80a      	add	r0, sp, #40	; 0x28
   d7046:	f7ff fcae 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d704a:	a90a      	add	r1, sp, #40	; 0x28
   d704c:	aa14      	add	r2, sp, #80	; 0x50
   d704e:	a80f      	add	r0, sp, #60	; 0x3c
   d7050:	f7ff fbd3 	bl	d67fa <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE>
   d7054:	4680      	mov	r8, r0
   d7056:	a80a      	add	r0, sp, #40	; 0x28
   d7058:	f7ff f9f5 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   d705c:	a80f      	add	r0, sp, #60	; 0x3c
   d705e:	f7ff f9f2 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
#define TF_LITE_ADD(type, opname, dtype)                             \
  type::opname(op_params, GetTensorShape(input1),                    \
               GetTensorData<dtype>(input1), GetTensorShape(input2), \
               GetTensorData<dtype>(input2), GetTensorShape(output), \
               GetTensorData<dtype>(output));
    if (output->type == kTfLiteInt8) {
   d7062:	7833      	ldrb	r3, [r6, #0]
   d7064:	2b09      	cmp	r3, #9
      if (need_broadcast) {
        TF_LITE_ADD(reference_integer_ops, BroadcastAdd4DSlow, int8_t);
   d7066:	4629      	mov	r1, r5
   d7068:	a80f      	add	r0, sp, #60	; 0x3c
   d706a:	af05      	add	r7, sp, #20
#define TF_LITE_ADD(type, opname, dtype)                             \
  type::opname(op_params, GetTensorShape(input1),                    \
               GetTensorData<dtype>(input1), GetTensorShape(input2), \
               GetTensorData<dtype>(input2), GetTensorShape(output), \
               GetTensorData<dtype>(output));
    if (output->type == kTfLiteInt8) {
   d706c:	d134      	bne.n	d70d8 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0xd8>
      if (need_broadcast) {
   d706e:	f1b8 0f00 	cmp.w	r8, #0
   d7072:	d018      	beq.n	d70a6 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0xa6>
        TF_LITE_ADD(reference_integer_ops, BroadcastAdd4DSlow, int8_t);
   d7074:	f7ff fc97 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d7078:	b105      	cbz	r5, d707c <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x7c>
   d707a:	686d      	ldr	r5, [r5, #4]
   d707c:	4621      	mov	r1, r4
   d707e:	a80a      	add	r0, sp, #40	; 0x28
   d7080:	f7ff fc91 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d7084:	b104      	cbz	r4, d7088 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x88>
   d7086:	6864      	ldr	r4, [r4, #4]
   d7088:	4631      	mov	r1, r6
   d708a:	4638      	mov	r0, r7
   d708c:	f7ff fc8b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d7090:	6873      	ldr	r3, [r6, #4]
   d7092:	9302      	str	r3, [sp, #8]
   d7094:	e88d 0090 	stmia.w	sp, {r4, r7}
   d7098:	ab0a      	add	r3, sp, #40	; 0x28
   d709a:	462a      	mov	r2, r5
   d709c:	a90f      	add	r1, sp, #60	; 0x3c
   d709e:	a814      	add	r0, sp, #80	; 0x50
   d70a0:	f7ff fea2 	bl	d6de8 <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa>
   d70a4:	e04c      	b.n	d7140 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x140>
      } else {
        TF_LITE_ADD(reference_integer_ops, Add, int8_t);
   d70a6:	f7ff fc7e 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d70aa:	b105      	cbz	r5, d70ae <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0xae>
   d70ac:	686d      	ldr	r5, [r5, #4]
   d70ae:	4621      	mov	r1, r4
   d70b0:	a80a      	add	r0, sp, #40	; 0x28
   d70b2:	f7ff fc78 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d70b6:	b104      	cbz	r4, d70ba <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0xba>
   d70b8:	6864      	ldr	r4, [r4, #4]
   d70ba:	4631      	mov	r1, r6
   d70bc:	4638      	mov	r0, r7
   d70be:	f7ff fc72 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d70c2:	6873      	ldr	r3, [r6, #4]
   d70c4:	9302      	str	r3, [sp, #8]
   d70c6:	e88d 0090 	stmia.w	sp, {r4, r7}
   d70ca:	ab0a      	add	r3, sp, #40	; 0x28
   d70cc:	462a      	mov	r2, r5
   d70ce:	a90f      	add	r1, sp, #60	; 0x3c
   d70d0:	a814      	add	r0, sp, #80	; 0x50
   d70d2:	f7ff fb48 	bl	d6766 <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa>
   d70d6:	e033      	b.n	d7140 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x140>
      }
    } else {
      if (need_broadcast) {
   d70d8:	f1b8 0f00 	cmp.w	r8, #0
   d70dc:	d018      	beq.n	d7110 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x110>
        TF_LITE_ADD(reference_ops, BroadcastAdd4DSlow, uint8_t);
   d70de:	f7ff fc62 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d70e2:	b105      	cbz	r5, d70e6 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0xe6>
   d70e4:	686d      	ldr	r5, [r5, #4]
   d70e6:	4621      	mov	r1, r4
   d70e8:	a80a      	add	r0, sp, #40	; 0x28
   d70ea:	f7ff fc5c 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d70ee:	b104      	cbz	r4, d70f2 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0xf2>
   d70f0:	6864      	ldr	r4, [r4, #4]
   d70f2:	4631      	mov	r1, r6
   d70f4:	4638      	mov	r0, r7
   d70f6:	f7ff fc56 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d70fa:	6873      	ldr	r3, [r6, #4]
   d70fc:	9302      	str	r3, [sp, #8]
   d70fe:	e88d 0090 	stmia.w	sp, {r4, r7}
   d7102:	ab0a      	add	r3, sp, #40	; 0x28
   d7104:	462a      	mov	r2, r5
   d7106:	a90f      	add	r1, sp, #60	; 0x3c
   d7108:	a814      	add	r0, sp, #80	; 0x50
   d710a:	f7ff fef3 	bl	d6ef4 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph>
   d710e:	e017      	b.n	d7140 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x140>
      } else {
        TF_LITE_ADD(reference_ops, Add, uint8_t);
   d7110:	f7ff fc49 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d7114:	b105      	cbz	r5, d7118 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x118>
   d7116:	686d      	ldr	r5, [r5, #4]
   d7118:	4621      	mov	r1, r4
   d711a:	a80a      	add	r0, sp, #40	; 0x28
   d711c:	f7ff fc43 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d7120:	b104      	cbz	r4, d7124 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x124>
   d7122:	6864      	ldr	r4, [r4, #4]
   d7124:	4631      	mov	r1, r6
   d7126:	4638      	mov	r0, r7
   d7128:	f7ff fc3d 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d712c:	6873      	ldr	r3, [r6, #4]
   d712e:	9302      	str	r3, [sp, #8]
   d7130:	e88d 0090 	stmia.w	sp, {r4, r7}
   d7134:	ab0a      	add	r3, sp, #40	; 0x28
   d7136:	462a      	mov	r2, r5
   d7138:	a90f      	add	r1, sp, #60	; 0x3c
   d713a:	a814      	add	r0, sp, #80	; 0x50
   d713c:	f7ff fac8 	bl	d66d0 <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph>
   d7140:	4638      	mov	r0, r7
   d7142:	f7ff f980 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   d7146:	a80a      	add	r0, sp, #40	; 0x28
   d7148:	f7ff f97d 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   d714c:	a80f      	add	r0, sp, #60	; 0x3c
   d714e:	f7ff f97a 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
    }
#undef TF_LITE_ADD
  }

  return kTfLiteOk;
}
   d7152:	b028      	add	sp, #160	; 0xa0
   d7154:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}

000d7158 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_>:

TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
                              TfLiteAddParams* params, const OpData* data,
                              const TfLiteTensor* input1,
                              const TfLiteTensor* input2,
                              TfLiteTensor* output) {
   d7158:	b508      	push	{r3, lr}
   d715a:	4618      	mov	r0, r3
   d715c:	9b04      	ldr	r3, [sp, #16]
  if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt8) {
   d715e:	781a      	ldrb	r2, [r3, #0]
   d7160:	2a03      	cmp	r2, #3
   d7162:	d001      	beq.n	d7168 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x10>
   d7164:	2a09      	cmp	r2, #9
   d7166:	d103      	bne.n	d7170 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x18>
   d7168:	9a03      	ldr	r2, [sp, #12]
   d716a:	9902      	ldr	r1, [sp, #8]
   d716c:	f7ff ff48 	bl	d7000 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4>
    }
#undef TF_LITE_ADD
  }

  return kTfLiteOk;
}
   d7170:	2000      	movs	r0, #0
   d7172:	bd08      	pop	{r3, pc}

000d7174 <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   d7174:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d7178:	680a      	ldr	r2, [r1, #0]
   d717a:	f8d0 8008 	ldr.w	r8, [r0, #8]
  auto* params = reinterpret_cast<TfLiteAddParams*>(node->builtin_data);
   d717e:	f8d1 9014 	ldr.w	r9, [r1, #20]
  }

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   d7182:	460f      	mov	r7, r1
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d7184:	6851      	ldr	r1, [r2, #4]
   d7186:	6892      	ldr	r2, [r2, #8]
   d7188:	b095      	sub	sp, #84	; 0x54
   d718a:	2338      	movs	r3, #56	; 0x38
   d718c:	fb03 8202 	mla	r2, r3, r2, r8
   d7190:	9204      	str	r2, [sp, #16]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d7192:	687a      	ldr	r2, [r7, #4]
   d7194:	6854      	ldr	r4, [r2, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d7196:	fb03 8b01 	mla	fp, r3, r1, r8
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d719a:	4363      	muls	r3, r4
   d719c:	eb08 0403 	add.w	r4, r8, r3
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  OpData data;
  TF_LITE_ENSURE_STATUS(
   d71a0:	f10d 0a1c 	add.w	sl, sp, #28
   d71a4:	9305      	str	r3, [sp, #20]
   d71a6:	e88d 0410 	stmia.w	sp, {r4, sl}
   d71aa:	9b04      	ldr	r3, [sp, #16]
   d71ac:	465a      	mov	r2, fp
   d71ae:	4649      	mov	r1, r9
  }

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   d71b0:	4606      	mov	r6, r0
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  OpData data;
  TF_LITE_ENSURE_STATUS(
   d71b2:	f7ff fc11 	bl	d69d8 <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE>
   d71b6:	4605      	mov	r5, r0
   d71b8:	bb38      	cbnz	r0, d720a <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0x96>
      CalculateOpData(context, params, input1, input2, output, &data));

  if (output->type == kTfLiteFloat32) {
   d71ba:	9b05      	ldr	r3, [sp, #20]
   d71bc:	f818 3003 	ldrb.w	r3, [r8, r3]
   d71c0:	2b01      	cmp	r3, #1
   d71c2:	d10b      	bne.n	d71dc <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0x68>
    EvalAdd(context, node, params, &data, input1, input2, output);
   d71c4:	9b04      	ldr	r3, [sp, #16]
   d71c6:	9301      	str	r3, [sp, #4]
   d71c8:	9402      	str	r4, [sp, #8]
   d71ca:	f8cd b000 	str.w	fp, [sp]
   d71ce:	4653      	mov	r3, sl
   d71d0:	464a      	mov	r2, r9
   d71d2:	4639      	mov	r1, r7
   d71d4:	4630      	mov	r0, r6
   d71d6:	f7ff fd7d 	bl	d6cd4 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_>
   d71da:	e017      	b.n	d720c <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0x98>
  } else if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt8) {
   d71dc:	2b03      	cmp	r3, #3
   d71de:	d001      	beq.n	d71e4 <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0x70>
   d71e0:	2b09      	cmp	r3, #9
   d71e2:	d10e      	bne.n	d7202 <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0x8e>
    TF_LITE_ENSURE_OK(context, EvalAddQuantized(context, node, params, &data,
   d71e4:	9b04      	ldr	r3, [sp, #16]
   d71e6:	9301      	str	r3, [sp, #4]
   d71e8:	9402      	str	r4, [sp, #8]
   d71ea:	f8cd b000 	str.w	fp, [sp]
   d71ee:	4653      	mov	r3, sl
   d71f0:	464a      	mov	r2, r9
   d71f2:	4639      	mov	r1, r7
   d71f4:	4630      	mov	r0, r6
   d71f6:	f7ff ffaf 	bl	d7158 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_>
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  OpData data;
  TF_LITE_ENSURE_STATUS(
   d71fa:	1c05      	adds	r5, r0, #0
   d71fc:	bf18      	it	ne
   d71fe:	2501      	movne	r5, #1
   d7200:	e004      	b.n	d720c <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0x98>
  } else if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt8) {
    TF_LITE_ENSURE_OK(context, EvalAddQuantized(context, node, params, &data,
                                                input1, input2, output));
  } else {
    context->ReportError(context,
                         "Inputs and outputs not all float|uint8|int8 types.");
   d7202:	6973      	ldr	r3, [r6, #20]
   d7204:	4903      	ldr	r1, [pc, #12]	; (d7214 <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0xa0>)
   d7206:	4630      	mov	r0, r6
   d7208:	4798      	blx	r3
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  OpData data;
  TF_LITE_ENSURE_STATUS(
   d720a:	2501      	movs	r5, #1
                         "Inputs and outputs not all float|uint8|int8 types.");
    return kTfLiteError;
  }

  return kTfLiteOk;
}
   d720c:	4628      	mov	r0, r5
   d720e:	b015      	add	sp, #84	; 0x54
   d7210:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   d7214:	000e9740 	.word	0x000e9740

000d7218 <_ZN6tflite3ops5micro14AllOpsResolverC1Ev>:
#define TFLITE_REGISTRATIONS_MAX (128)
#endif

namespace tflite {

class MicroMutableOpResolver : public OpResolver {
   d7218:	f241 0304 	movw	r3, #4100	; 0x1004
TfLiteRegistration* Register_UNPACK();
TfLiteRegistration* Register_NEG();
TfLiteRegistration* Register_ADD();
TfLiteRegistration* Register_QUANTIZE();
TfLiteRegistration* Register_DEQUANTIZE();
AllOpsResolver::AllOpsResolver() {
   d721c:	b5f7      	push	{r0, r1, r2, r4, r5, r6, r7, lr}
   d721e:	2700      	movs	r7, #0
   d7220:	50c7      	str	r7, [r0, r3]
   d7222:	4bbd      	ldr	r3, [pc, #756]	; (d7518 <_ZN6tflite3ops5micro14AllOpsResolverC1Ev+0x300>)
   d7224:	6003      	str	r3, [r0, #0]
   d7226:	4605      	mov	r5, r0
  AddBuiltin(BuiltinOperator_DEPTHWISE_CONV_2D, Register_DEPTHWISE_CONV_2D());
   d7228:	f00b fc1a 	bl	e2a60 <_ZN6tflite3ops5micro26Register_DEPTHWISE_CONV_2DEv>
   d722c:	2401      	movs	r4, #1
   d722e:	4623      	mov	r3, r4
   d7230:	4602      	mov	r2, r0
   d7232:	2104      	movs	r1, #4
   d7234:	4628      	mov	r0, r5
   d7236:	9400      	str	r4, [sp, #0]
   d7238:	f7fe fff7 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_FULLY_CONNECTED, Register_FULLY_CONNECTED(),
   d723c:	f006 f948 	bl	dd4d0 <_ZN6tflite3ops5micro24Register_FULLY_CONNECTEDEv>
             /* min_version */ 1,
             /* max_version */ 4);
   d7240:	2604      	movs	r6, #4
   d7242:	4623      	mov	r3, r4
   d7244:	4602      	mov	r2, r0
   d7246:	2109      	movs	r1, #9
   d7248:	4628      	mov	r0, r5
   d724a:	9600      	str	r6, [sp, #0]
   d724c:	f7fe ffed 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_MAX_POOL_2D, Register_MAX_POOL_2D());
   d7250:	f008 f840 	bl	df2d4 <_ZN6tflite3ops5micro20Register_MAX_POOL_2DEv>
   d7254:	4623      	mov	r3, r4
   d7256:	4602      	mov	r2, r0
   d7258:	2111      	movs	r1, #17
   d725a:	4628      	mov	r0, r5
   d725c:	9400      	str	r4, [sp, #0]
   d725e:	f7fe ffe4 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_SOFTMAX, Register_SOFTMAX());
   d7262:	f008 fef9 	bl	e0058 <_ZN6tflite3ops5micro16Register_SOFTMAXEv>
   d7266:	4623      	mov	r3, r4
   d7268:	4602      	mov	r2, r0
   d726a:	2119      	movs	r1, #25
   d726c:	4628      	mov	r0, r5
   d726e:	9400      	str	r4, [sp, #0]
   d7270:	f7fe ffdb 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LOGISTIC, Register_LOGISTIC());
   d7274:	f006 fb66 	bl	dd944 <_ZN6tflite3ops5micro17Register_LOGISTICEv>
   d7278:	4623      	mov	r3, r4
   d727a:	4602      	mov	r2, r0
   d727c:	210e      	movs	r1, #14
   d727e:	4628      	mov	r0, r5
   d7280:	9400      	str	r4, [sp, #0]
   d7282:	f7fe ffd2 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_SVDF, Register_SVDF());
   d7286:	f00a ffad 	bl	e21e4 <_ZN6tflite3ops5micro13Register_SVDFEv>
   d728a:	4623      	mov	r3, r4
   d728c:	4602      	mov	r2, r0
   d728e:	211b      	movs	r1, #27
   d7290:	4628      	mov	r0, r5
   d7292:	9400      	str	r4, [sp, #0]
   d7294:	f7fe ffc9 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_CONV_2D, Register_CONV_2D());
   d7298:	f005 fb08 	bl	dc8ac <_ZN6tflite3ops5micro16Register_CONV_2DEv>
   d729c:	4623      	mov	r3, r4
   d729e:	4602      	mov	r2, r0
   d72a0:	2103      	movs	r1, #3
   d72a2:	4628      	mov	r0, r5
   d72a4:	9400      	str	r4, [sp, #0]
   d72a6:	f7fe ffc0 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_AVERAGE_POOL_2D, Register_AVERAGE_POOL_2D());
   d72aa:	f008 f80f 	bl	df2cc <_ZN6tflite3ops5micro24Register_AVERAGE_POOL_2DEv>
   d72ae:	4623      	mov	r3, r4
   d72b0:	4602      	mov	r2, r0
   d72b2:	4621      	mov	r1, r4
   d72b4:	4628      	mov	r0, r5
   d72b6:	9400      	str	r4, [sp, #0]
   d72b8:	f7fe ffb7 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_ABS, Register_ABS());
   d72bc:	f005 fd4a 	bl	dcd54 <_ZN6tflite3ops5micro12Register_ABSEv>
   d72c0:	4623      	mov	r3, r4
   d72c2:	4602      	mov	r2, r0
   d72c4:	2165      	movs	r1, #101	; 0x65
   d72c6:	4628      	mov	r0, r5
   d72c8:	9400      	str	r4, [sp, #0]
   d72ca:	f7fe ffae 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_SIN, Register_SIN());
   d72ce:	f005 fd45 	bl	dcd5c <_ZN6tflite3ops5micro12Register_SINEv>
   d72d2:	4623      	mov	r3, r4
   d72d4:	4602      	mov	r2, r0
   d72d6:	2142      	movs	r1, #66	; 0x42
   d72d8:	4628      	mov	r0, r5
   d72da:	9400      	str	r4, [sp, #0]
   d72dc:	f7fe ffa5 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_COS, Register_COS());
   d72e0:	f005 fd40 	bl	dcd64 <_ZN6tflite3ops5micro12Register_COSEv>
   d72e4:	4623      	mov	r3, r4
   d72e6:	4602      	mov	r2, r0
   d72e8:	216c      	movs	r1, #108	; 0x6c
   d72ea:	4628      	mov	r0, r5
   d72ec:	9400      	str	r4, [sp, #0]
   d72ee:	f7fe ff9c 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LOG, Register_LOG());
   d72f2:	f005 fd3b 	bl	dcd6c <_ZN6tflite3ops5micro12Register_LOGEv>
   d72f6:	4623      	mov	r3, r4
   d72f8:	4602      	mov	r2, r0
   d72fa:	2149      	movs	r1, #73	; 0x49
   d72fc:	4628      	mov	r0, r5
   d72fe:	9400      	str	r4, [sp, #0]
   d7300:	f7fe ff93 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_SQRT, Register_SQRT());
   d7304:	f005 fd36 	bl	dcd74 <_ZN6tflite3ops5micro13Register_SQRTEv>
   d7308:	4623      	mov	r3, r4
   d730a:	4602      	mov	r2, r0
   d730c:	214b      	movs	r1, #75	; 0x4b
   d730e:	4628      	mov	r0, r5
   d7310:	9400      	str	r4, [sp, #0]
   d7312:	f7fe ff8a 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_RSQRT, Register_RSQRT());
   d7316:	f005 fd31 	bl	dcd7c <_ZN6tflite3ops5micro14Register_RSQRTEv>
   d731a:	4623      	mov	r3, r4
   d731c:	4602      	mov	r2, r0
   d731e:	214c      	movs	r1, #76	; 0x4c
   d7320:	4628      	mov	r0, r5
   d7322:	9400      	str	r4, [sp, #0]
   d7324:	f7fe ff81 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_SQUARE, Register_SQUARE());
   d7328:	f005 fd2c 	bl	dcd84 <_ZN6tflite3ops5micro15Register_SQUAREEv>
   d732c:	4623      	mov	r3, r4
   d732e:	4602      	mov	r2, r0
   d7330:	215c      	movs	r1, #92	; 0x5c
   d7332:	4628      	mov	r0, r5
   d7334:	9400      	str	r4, [sp, #0]
   d7336:	f7fe ff78 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_PRELU, Register_PRELU());
   d733a:	f007 ffd1 	bl	df2e0 <_ZN6tflite3ops5micro14Register_PRELUEv>
   d733e:	4623      	mov	r3, r4
   d7340:	4602      	mov	r2, r0
   d7342:	2136      	movs	r1, #54	; 0x36
   d7344:	4628      	mov	r0, r5
   d7346:	9400      	str	r4, [sp, #0]
   d7348:	f7fe ff6f 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_FLOOR, Register_FLOOR());
   d734c:	f005 fd8e 	bl	dce6c <_ZN6tflite3ops5micro14Register_FLOOREv>
   d7350:	4623      	mov	r3, r4
   d7352:	4602      	mov	r2, r0
   d7354:	2108      	movs	r1, #8
   d7356:	4628      	mov	r0, r5
   d7358:	9400      	str	r4, [sp, #0]
   d735a:	f7fe ff66 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_MAXIMUM, Register_MAXIMUM());
   d735e:	f006 fb2f 	bl	dd9c0 <_ZN6tflite3ops5micro16Register_MAXIMUMEv>
   d7362:	4623      	mov	r3, r4
   d7364:	4602      	mov	r2, r0
   d7366:	2137      	movs	r1, #55	; 0x37
   d7368:	4628      	mov	r0, r5
   d736a:	9400      	str	r4, [sp, #0]
   d736c:	f7fe ff5d 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_MINIMUM, Register_MINIMUM());
   d7370:	f006 fb2a 	bl	dd9c8 <_ZN6tflite3ops5micro16Register_MINIMUMEv>
   d7374:	4623      	mov	r3, r4
   d7376:	4602      	mov	r2, r0
   d7378:	2139      	movs	r1, #57	; 0x39
   d737a:	4628      	mov	r0, r5
   d737c:	9400      	str	r4, [sp, #0]
   d737e:	f7fe ff54 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_ARG_MAX, Register_ARG_MAX());
   d7382:	f000 f8cd 	bl	d7520 <_ZN6tflite3ops5micro16Register_ARG_MAXEv>
   d7386:	4623      	mov	r3, r4
   d7388:	4602      	mov	r2, r0
   d738a:	2138      	movs	r1, #56	; 0x38
   d738c:	4628      	mov	r0, r5
   d738e:	9400      	str	r4, [sp, #0]
   d7390:	f7fe ff4b 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_ARG_MIN, Register_ARG_MIN());
   d7394:	f000 f8c8 	bl	d7528 <_ZN6tflite3ops5micro16Register_ARG_MINEv>
   d7398:	4623      	mov	r3, r4
   d739a:	4602      	mov	r2, r0
   d739c:	214f      	movs	r1, #79	; 0x4f
   d739e:	4628      	mov	r0, r5
   d73a0:	9400      	str	r4, [sp, #0]
   d73a2:	f7fe ff42 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LOGICAL_OR, Register_LOGICAL_OR());
   d73a6:	f006 f8a1 	bl	dd4ec <_ZN6tflite3ops5micro19Register_LOGICAL_OREv>
   d73aa:	4623      	mov	r3, r4
   d73ac:	4602      	mov	r2, r0
   d73ae:	2154      	movs	r1, #84	; 0x54
   d73b0:	4628      	mov	r0, r5
   d73b2:	9400      	str	r4, [sp, #0]
   d73b4:	f7fe ff39 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LOGICAL_AND, Register_LOGICAL_AND());
   d73b8:	f006 f89c 	bl	dd4f4 <_ZN6tflite3ops5micro20Register_LOGICAL_ANDEv>
   d73bc:	4623      	mov	r3, r4
   d73be:	4602      	mov	r2, r0
   d73c0:	2156      	movs	r1, #86	; 0x56
   d73c2:	4628      	mov	r0, r5
   d73c4:	9400      	str	r4, [sp, #0]
   d73c6:	f7fe ff30 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LOGICAL_NOT, Register_LOGICAL_NOT());
   d73ca:	f005 fcdf 	bl	dcd8c <_ZN6tflite3ops5micro20Register_LOGICAL_NOTEv>
   d73ce:	4623      	mov	r3, r4
   d73d0:	4602      	mov	r2, r0
   d73d2:	2157      	movs	r1, #87	; 0x57
   d73d4:	4628      	mov	r0, r5
   d73d6:	9400      	str	r4, [sp, #0]
   d73d8:	f7fe ff27 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_RESHAPE, Register_RESHAPE());
   d73dc:	f008 fb7a 	bl	dfad4 <_ZN6tflite3ops5micro16Register_RESHAPEEv>
   d73e0:	4623      	mov	r3, r4
   d73e2:	4602      	mov	r2, r0
   d73e4:	2116      	movs	r1, #22
   d73e6:	4628      	mov	r0, r5
   d73e8:	9400      	str	r4, [sp, #0]
   d73ea:	f7fe ff1e 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_EQUAL, Register_EQUAL());
   d73ee:	f000 fd53 	bl	d7e98 <_ZN6tflite3ops5micro14Register_EQUALEv>
   d73f2:	4623      	mov	r3, r4
   d73f4:	4602      	mov	r2, r0
   d73f6:	2147      	movs	r1, #71	; 0x47
   d73f8:	4628      	mov	r0, r5
   d73fa:	9400      	str	r4, [sp, #0]
   d73fc:	f7fe ff15 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_NOT_EQUAL, Register_NOT_EQUAL());
   d7400:	f000 fd4e 	bl	d7ea0 <_ZN6tflite3ops5micro18Register_NOT_EQUALEv>
   d7404:	4623      	mov	r3, r4
   d7406:	4602      	mov	r2, r0
   d7408:	2148      	movs	r1, #72	; 0x48
   d740a:	4628      	mov	r0, r5
   d740c:	9400      	str	r4, [sp, #0]
   d740e:	f7fe ff0c 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_GREATER, Register_GREATER());
   d7412:	f000 fd49 	bl	d7ea8 <_ZN6tflite3ops5micro16Register_GREATEREv>
   d7416:	4623      	mov	r3, r4
   d7418:	4602      	mov	r2, r0
   d741a:	213d      	movs	r1, #61	; 0x3d
   d741c:	4628      	mov	r0, r5
   d741e:	9400      	str	r4, [sp, #0]
   d7420:	f7fe ff03 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_GREATER_EQUAL, Register_GREATER_EQUAL());
   d7424:	f000 fd44 	bl	d7eb0 <_ZN6tflite3ops5micro22Register_GREATER_EQUALEv>
   d7428:	4623      	mov	r3, r4
   d742a:	4602      	mov	r2, r0
   d742c:	213e      	movs	r1, #62	; 0x3e
   d742e:	4628      	mov	r0, r5
   d7430:	9400      	str	r4, [sp, #0]
   d7432:	f7fe fefa 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LESS, Register_LESS());
   d7436:	f000 fd3f 	bl	d7eb8 <_ZN6tflite3ops5micro13Register_LESSEv>
   d743a:	4623      	mov	r3, r4
   d743c:	4602      	mov	r2, r0
   d743e:	213a      	movs	r1, #58	; 0x3a
   d7440:	4628      	mov	r0, r5
   d7442:	9400      	str	r4, [sp, #0]
   d7444:	f7fe fef1 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LESS_EQUAL, Register_LESS_EQUAL());
   d7448:	f000 fd3a 	bl	d7ec0 <_ZN6tflite3ops5micro19Register_LESS_EQUALEv>
   d744c:	4623      	mov	r3, r4
   d744e:	4602      	mov	r2, r0
   d7450:	213f      	movs	r1, #63	; 0x3f
   d7452:	4628      	mov	r0, r5
   d7454:	9400      	str	r4, [sp, #0]
   d7456:	f7fe fee8 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_CEIL, Register_CEIL());
   d745a:	f000 fd19 	bl	d7e90 <_ZN6tflite3ops5micro13Register_CEILEv>
   d745e:	4623      	mov	r3, r4
   d7460:	4602      	mov	r2, r0
   d7462:	2168      	movs	r1, #104	; 0x68
   d7464:	4628      	mov	r0, r5
   d7466:	9400      	str	r4, [sp, #0]
   d7468:	f7fe fedf 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_ROUND, Register_ROUND());
   d746c:	f008 fc5e 	bl	dfd2c <_ZN6tflite3ops5micro14Register_ROUNDEv>
   d7470:	4623      	mov	r3, r4
   d7472:	4602      	mov	r2, r0
   d7474:	2174      	movs	r1, #116	; 0x74
   d7476:	4628      	mov	r0, r5
   d7478:	9400      	str	r4, [sp, #0]
   d747a:	f7fe fed6 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_STRIDED_SLICE, Register_STRIDED_SLICE());
   d747e:	f009 fe37 	bl	e10f0 <_ZN6tflite3ops5micro22Register_STRIDED_SLICEEv>
   d7482:	4623      	mov	r3, r4
   d7484:	4602      	mov	r2, r0
   d7486:	212d      	movs	r1, #45	; 0x2d
   d7488:	4628      	mov	r0, r5
   d748a:	9400      	str	r4, [sp, #0]
   d748c:	f7fe fecd 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_PACK, Register_PACK());
   d7490:	f007 f926 	bl	de6e0 <_ZN6tflite3ops5micro13Register_PACKEv>
   d7494:	4623      	mov	r3, r4
   d7496:	4602      	mov	r2, r0
   d7498:	2153      	movs	r1, #83	; 0x53
   d749a:	4628      	mov	r0, r5
   d749c:	9400      	str	r4, [sp, #0]
   d749e:	f7fe fec4 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_SPLIT, Register_SPLIT(),
   d74a2:	f009 f913 	bl	e06cc <_ZN6tflite3ops5micro14Register_SPLITEv>
             /* min_version */ 1,
             /* max_version */ 3);
   d74a6:	2303      	movs	r3, #3
   d74a8:	4602      	mov	r2, r0
   d74aa:	9300      	str	r3, [sp, #0]
   d74ac:	2131      	movs	r1, #49	; 0x31
   d74ae:	4623      	mov	r3, r4
   d74b0:	4628      	mov	r0, r5
   d74b2:	f7fe feba 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_UNPACK, Register_UNPACK());
   d74b6:	f00b f87d 	bl	e25b4 <_ZN6tflite3ops5micro15Register_UNPACKEv>
   d74ba:	4623      	mov	r3, r4
   d74bc:	4602      	mov	r2, r0
   d74be:	2158      	movs	r1, #88	; 0x58
   d74c0:	4628      	mov	r0, r5
   d74c2:	9400      	str	r4, [sp, #0]
   d74c4:	f7fe feb1 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_NEG, Register_NEG());
   d74c8:	f006 fecc 	bl	de264 <_ZN6tflite3ops5micro12Register_NEGEv>
   d74cc:	4623      	mov	r3, r4
   d74ce:	4602      	mov	r2, r0
   d74d0:	213b      	movs	r1, #59	; 0x3b
   d74d2:	4628      	mov	r0, r5
   d74d4:	9400      	str	r4, [sp, #0]
   d74d6:	f7fe fea8 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_ADD, Register_ADD());
   d74da:	f7ff fb09 	bl	d6af0 <_ZN6tflite3ops5micro12Register_ADDEv>
   d74de:	4623      	mov	r3, r4
   d74e0:	4602      	mov	r2, r0
   d74e2:	4639      	mov	r1, r7
   d74e4:	4628      	mov	r0, r5
   d74e6:	9400      	str	r4, [sp, #0]
   d74e8:	f7fe fe9f 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_QUANTIZE, Register_QUANTIZE(), 1, 4);
   d74ec:	f008 fa0e 	bl	df90c <_ZN6tflite3ops5micro17Register_QUANTIZEEv>
   d74f0:	4623      	mov	r3, r4
   d74f2:	4602      	mov	r2, r0
   d74f4:	2172      	movs	r1, #114	; 0x72
   d74f6:	4628      	mov	r0, r5
   d74f8:	9600      	str	r6, [sp, #0]
   d74fa:	f7fe fe96 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_DEQUANTIZE, Register_DEQUANTIZE(), 1, 4);
   d74fe:	f005 fadd 	bl	dcabc <_ZN6tflite3ops5micro19Register_DEQUANTIZEEv>
   d7502:	9600      	str	r6, [sp, #0]
   d7504:	4602      	mov	r2, r0
   d7506:	4623      	mov	r3, r4
   d7508:	4628      	mov	r0, r5
   d750a:	2106      	movs	r1, #6
   d750c:	f7fe fe8d 	bl	d622a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
}
   d7510:	4628      	mov	r0, r5
   d7512:	b003      	add	sp, #12
   d7514:	bdf0      	pop	{r4, r5, r6, r7, pc}
   d7516:	bf00      	nop
   d7518:	000e97d8 	.word	0x000e97d8

000d751c <_ZN6tflite3ops5micro11arg_min_max7PrepareEP13TfLiteContextP10TfLiteNode>:
constexpr int kAxis = 1;
constexpr int kOutputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   d751c:	2000      	movs	r0, #0
   d751e:	4770      	bx	lr

000d7520 <_ZN6tflite3ops5micro16Register_ARG_MAXEv>:

TfLiteRegistration* Register_ARG_MAX() {
  static TfLiteRegistration r = {nullptr, nullptr, arg_min_max::Prepare,
                                 arg_min_max::ArgMaxEval};
  return &r;
}
   d7520:	4800      	ldr	r0, [pc, #0]	; (d7524 <_ZN6tflite3ops5micro16Register_ARG_MAXEv+0x4>)
   d7522:	4770      	bx	lr
   d7524:	2003bd68 	.word	0x2003bd68

000d7528 <_ZN6tflite3ops5micro16Register_ARG_MINEv>:

TfLiteRegistration* Register_ARG_MIN() {
  static TfLiteRegistration r = {nullptr, nullptr, arg_min_max::Prepare,
                                 arg_min_max::ArgMinEval};
  return &r;
}
   d7528:	4800      	ldr	r0, [pc, #0]	; (d752c <_ZN6tflite3ops5micro16Register_ARG_MINEv+0x4>)
   d752a:	4770      	bx	lr
   d752c:	2003bd88 	.word	0x2003bd88

000d7530 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>:
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d7530:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d7534:	6805      	ldr	r5, [r0, #0]
   d7536:	b087      	sub	sp, #28
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d7538:	2d00      	cmp	r5, #0
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d753a:	4606      	mov	r6, r0
   d753c:	9105      	str	r1, [sp, #20]
   d753e:	461f      	mov	r7, r3
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d7540:	dc01      	bgt.n	d7546 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x16>
   d7542:	f00c fef3 	bl	e432c <abort>
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
   d7546:	6839      	ldr	r1, [r7, #0]
   d7548:	1e6b      	subs	r3, r5, #1
   d754a:	428b      	cmp	r3, r1
   d754c:	d1f9      	bne.n	d7542 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
   d754e:	6814      	ldr	r4, [r2, #0]
  if (axis < 0) {
   d7550:	2c00      	cmp	r4, #0
    axis += input1_shape.DimensionsCount();
   d7552:	bfb8      	it	lt
   d7554:	1964      	addlt	r4, r4, r5
  }
  const int axis_size = input1_shape.Dims(axis);
   d7556:	4621      	mov	r1, r4
   d7558:	f7fe ff80 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d755c:	f04f 0a00 	mov.w	sl, #0
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
  if (axis < 0) {
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);
   d7560:	4680      	mov	r8, r0

  int outer_size = 1;
   d7562:	f04f 0901 	mov.w	r9, #1
  for (int i = 0; i < axis; ++i) {
   d7566:	4554      	cmp	r4, sl
   d7568:	dd0f      	ble.n	d758a <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x5a>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
   d756a:	4651      	mov	r1, sl
   d756c:	4630      	mov	r0, r6
   d756e:	f7fe ff75 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d7572:	4651      	mov	r1, sl
   d7574:	4683      	mov	fp, r0
   d7576:	4638      	mov	r0, r7
   d7578:	f7fe ff70 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d757c:	4583      	cmp	fp, r0
   d757e:	d1e0      	bne.n	d7542 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    outer_size *= input1_shape.Dims(i);
   d7580:	fb0b f909 	mul.w	r9, fp, r9
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d7584:	f10a 0a01 	add.w	sl, sl, #1
   d7588:	e7ed      	b.n	d7566 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x36>
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d758a:	f104 0a01 	add.w	sl, r4, #1
  for (int i = 0; i < axis; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
   d758e:	2401      	movs	r4, #1
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d7590:	45aa      	cmp	sl, r5
   d7592:	da10      	bge.n	d75b6 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x86>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
   d7594:	4651      	mov	r1, sl
   d7596:	4630      	mov	r0, r6
   d7598:	f7fe ff60 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d759c:	f10a 31ff 	add.w	r1, sl, #4294967295	; 0xffffffff
   d75a0:	4683      	mov	fp, r0
   d75a2:	4638      	mov	r0, r7
   d75a4:	f7fe ff5a 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d75a8:	4583      	cmp	fp, r0
   d75aa:	d1ca      	bne.n	d7542 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    inner_size *= input1_shape.Dims(i);
   d75ac:	fb0b f404 	mul.w	r4, fp, r4
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d75b0:	f10a 0a01 	add.w	sl, sl, #1
   d75b4:	e7ec      	b.n	d7590 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x60>
   d75b6:	fb04 f308 	mul.w	r3, r4, r8
   d75ba:	9304      	str	r3, [sp, #16]
   d75bc:	f8dd e040 	ldr.w	lr, [sp, #64]	; 0x40
   d75c0:	2300      	movs	r3, #0
   d75c2:	ea4f 0a84 	mov.w	sl, r4, lsl #2
   d75c6:	461f      	mov	r7, r3
   d75c8:	469c      	mov	ip, r3
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d75ca:	45e1      	cmp	r9, ip
   d75cc:	dd31      	ble.n	d7632 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x102>
   d75ce:	9a05      	ldr	r2, [sp, #20]
   d75d0:	eb02 0183 	add.w	r1, r2, r3, lsl #2
   d75d4:	fb07 4204 	mla	r2, r7, r4, r4
   d75d8:	1ad2      	subs	r2, r2, r3
   d75da:	0092      	lsls	r2, r2, #2
   d75dc:	9202      	str	r2, [sp, #8]
   d75de:	2000      	movs	r0, #0
    for (int inner = 0; inner < inner_size; ++inner) {
   d75e0:	4284      	cmp	r4, r0
   d75e2:	dd1f      	ble.n	d7624 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xf4>
   d75e4:	9a02      	ldr	r2, [sp, #8]
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
   d75e6:	edd1 7a00 	vldr	s15, [r1]
   d75ea:	188a      	adds	r2, r1, r2
   d75ec:	2500      	movs	r5, #0
   d75ee:	9203      	str	r2, [sp, #12]
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d75f0:	2601      	movs	r6, #1
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
   d75f2:	9501      	str	r5, [sp, #4]
      for (int i = 1; i < axis_size; ++i) {
   d75f4:	4546      	cmp	r6, r8
   d75f6:	da0f      	bge.n	d7618 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xe8>
   d75f8:	9a03      	ldr	r2, [sp, #12]
   d75fa:	eb02 0b05 	add.w	fp, r2, r5
   d75fe:	ed9b 7a00 	vldr	s14, [fp]
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
   d7602:	eef4 7ac7 	vcmpe.f32	s15, s14
   d7606:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d760a:	bf44      	itt	mi
   d760c:	9601      	strmi	r6, [sp, #4]
          min_max_value = curr_value;
   d760e:	eef0 7a47 	vmovmi.f32	s15, s14
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d7612:	3601      	adds	r6, #1
   d7614:	4455      	add	r5, sl
   d7616:	e7ed      	b.n	d75f4 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xc4>
        if (cmp(curr_value, min_max_value)) {
          min_max_value = curr_value;
          min_max_index = static_cast<T2>(i);
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
   d7618:	9a01      	ldr	r2, [sp, #4]
   d761a:	f84e 2020 	str.w	r2, [lr, r0, lsl #2]
   d761e:	3104      	adds	r1, #4
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
   d7620:	3001      	adds	r0, #1
   d7622:	e7dd      	b.n	d75e0 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xb0>
   d7624:	9a04      	ldr	r2, [sp, #16]
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d7626:	f10c 0c01 	add.w	ip, ip, #1
   d762a:	44d6      	add	lr, sl
   d762c:	4447      	add	r7, r8
   d762e:	4413      	add	r3, r2
   d7630:	e7cb      	b.n	d75ca <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x9a>
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
    }
  }
}
   d7632:	b007      	add	sp, #28
   d7634:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d7638 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>:
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d7638:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d763c:	6805      	ldr	r5, [r0, #0]
   d763e:	b087      	sub	sp, #28
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d7640:	2d00      	cmp	r5, #0
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d7642:	4606      	mov	r6, r0
   d7644:	9105      	str	r1, [sp, #20]
   d7646:	461f      	mov	r7, r3
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d7648:	dc01      	bgt.n	d764e <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x16>
   d764a:	f00c fe6f 	bl	e432c <abort>
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
   d764e:	6839      	ldr	r1, [r7, #0]
   d7650:	1e6b      	subs	r3, r5, #1
   d7652:	428b      	cmp	r3, r1
   d7654:	d1f9      	bne.n	d764a <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
   d7656:	6814      	ldr	r4, [r2, #0]
  if (axis < 0) {
   d7658:	2c00      	cmp	r4, #0
    axis += input1_shape.DimensionsCount();
   d765a:	bfb8      	it	lt
   d765c:	1964      	addlt	r4, r4, r5
  }
  const int axis_size = input1_shape.Dims(axis);
   d765e:	4621      	mov	r1, r4
   d7660:	f7fe fefc 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d7664:	f04f 0a00 	mov.w	sl, #0
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
  if (axis < 0) {
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);
   d7668:	4680      	mov	r8, r0

  int outer_size = 1;
   d766a:	f04f 0901 	mov.w	r9, #1
  for (int i = 0; i < axis; ++i) {
   d766e:	4554      	cmp	r4, sl
   d7670:	dd0f      	ble.n	d7692 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x5a>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
   d7672:	4651      	mov	r1, sl
   d7674:	4630      	mov	r0, r6
   d7676:	f7fe fef1 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d767a:	4651      	mov	r1, sl
   d767c:	4683      	mov	fp, r0
   d767e:	4638      	mov	r0, r7
   d7680:	f7fe feec 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d7684:	4583      	cmp	fp, r0
   d7686:	d1e0      	bne.n	d764a <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    outer_size *= input1_shape.Dims(i);
   d7688:	fb0b f909 	mul.w	r9, fp, r9
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d768c:	f10a 0a01 	add.w	sl, sl, #1
   d7690:	e7ed      	b.n	d766e <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x36>
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d7692:	f104 0a01 	add.w	sl, r4, #1
  for (int i = 0; i < axis; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
   d7696:	2401      	movs	r4, #1
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d7698:	45aa      	cmp	sl, r5
   d769a:	da10      	bge.n	d76be <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x86>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
   d769c:	4651      	mov	r1, sl
   d769e:	4630      	mov	r0, r6
   d76a0:	f7fe fedc 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d76a4:	f10a 31ff 	add.w	r1, sl, #4294967295	; 0xffffffff
   d76a8:	4683      	mov	fp, r0
   d76aa:	4638      	mov	r0, r7
   d76ac:	f7fe fed6 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d76b0:	4583      	cmp	fp, r0
   d76b2:	d1ca      	bne.n	d764a <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    inner_size *= input1_shape.Dims(i);
   d76b4:	fb0b f404 	mul.w	r4, fp, r4
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d76b8:	f10a 0a01 	add.w	sl, sl, #1
   d76bc:	e7ec      	b.n	d7698 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x60>
   d76be:	fb04 f308 	mul.w	r3, r4, r8
   d76c2:	9304      	str	r3, [sp, #16]
   d76c4:	f8dd e040 	ldr.w	lr, [sp, #64]	; 0x40
   d76c8:	2300      	movs	r3, #0
   d76ca:	ea4f 0a84 	mov.w	sl, r4, lsl #2
   d76ce:	461f      	mov	r7, r3
   d76d0:	469c      	mov	ip, r3
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d76d2:	45e1      	cmp	r9, ip
   d76d4:	dd31      	ble.n	d773a <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x102>
   d76d6:	9a05      	ldr	r2, [sp, #20]
   d76d8:	eb02 0183 	add.w	r1, r2, r3, lsl #2
   d76dc:	fb07 4204 	mla	r2, r7, r4, r4
   d76e0:	1ad2      	subs	r2, r2, r3
   d76e2:	0092      	lsls	r2, r2, #2
   d76e4:	9202      	str	r2, [sp, #8]
   d76e6:	2000      	movs	r0, #0
    for (int inner = 0; inner < inner_size; ++inner) {
   d76e8:	4284      	cmp	r4, r0
   d76ea:	dd1f      	ble.n	d772c <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xf4>
   d76ec:	9a02      	ldr	r2, [sp, #8]
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
   d76ee:	edd1 7a00 	vldr	s15, [r1]
   d76f2:	188a      	adds	r2, r1, r2
   d76f4:	2500      	movs	r5, #0
   d76f6:	9203      	str	r2, [sp, #12]
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d76f8:	2601      	movs	r6, #1
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
   d76fa:	9501      	str	r5, [sp, #4]
      for (int i = 1; i < axis_size; ++i) {
   d76fc:	4546      	cmp	r6, r8
   d76fe:	da0f      	bge.n	d7720 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xe8>
   d7700:	9a03      	ldr	r2, [sp, #12]
   d7702:	eb02 0b05 	add.w	fp, r2, r5
   d7706:	ed9b 7a00 	vldr	s14, [fp]
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
   d770a:	eef4 7ac7 	vcmpe.f32	s15, s14
   d770e:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d7712:	bfc4      	itt	gt
   d7714:	9601      	strgt	r6, [sp, #4]
          min_max_value = curr_value;
   d7716:	eef0 7a47 	vmovgt.f32	s15, s14
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d771a:	3601      	adds	r6, #1
   d771c:	4455      	add	r5, sl
   d771e:	e7ed      	b.n	d76fc <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xc4>
        if (cmp(curr_value, min_max_value)) {
          min_max_value = curr_value;
          min_max_index = static_cast<T2>(i);
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
   d7720:	9a01      	ldr	r2, [sp, #4]
   d7722:	f84e 2020 	str.w	r2, [lr, r0, lsl #2]
   d7726:	3104      	adds	r1, #4
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
   d7728:	3001      	adds	r0, #1
   d772a:	e7dd      	b.n	d76e8 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xb0>
   d772c:	9a04      	ldr	r2, [sp, #16]
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d772e:	f10c 0c01 	add.w	ip, ip, #1
   d7732:	44d6      	add	lr, sl
   d7734:	4447      	add	r7, r8
   d7736:	4413      	add	r3, r2
   d7738:	e7cb      	b.n	d76d2 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x9a>
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
    }
  }
}
   d773a:	b007      	add	sp, #28
   d773c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d7740 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>:
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d7740:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d7744:	6806      	ldr	r6, [r0, #0]
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d7746:	2e00      	cmp	r6, #0
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d7748:	b087      	sub	sp, #28
   d774a:	4681      	mov	r9, r0
   d774c:	460f      	mov	r7, r1
   d774e:	469a      	mov	sl, r3
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d7750:	dc01      	bgt.n	d7756 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x16>
   d7752:	f00c fdeb 	bl	e432c <abort>
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
   d7756:	f8da 1000 	ldr.w	r1, [sl]
   d775a:	1e73      	subs	r3, r6, #1
   d775c:	428b      	cmp	r3, r1
   d775e:	d1f8      	bne.n	d7752 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
   d7760:	6814      	ldr	r4, [r2, #0]
  if (axis < 0) {
   d7762:	2c00      	cmp	r4, #0
    axis += input1_shape.DimensionsCount();
   d7764:	bfb8      	it	lt
   d7766:	19a4      	addlt	r4, r4, r6
  }
  const int axis_size = input1_shape.Dims(axis);
   d7768:	4621      	mov	r1, r4
   d776a:	f7fe fe77 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d776e:	f04f 0b00 	mov.w	fp, #0
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
  if (axis < 0) {
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);
   d7772:	4605      	mov	r5, r0

  int outer_size = 1;
   d7774:	f04f 0801 	mov.w	r8, #1
  for (int i = 0; i < axis; ++i) {
   d7778:	455c      	cmp	r4, fp
   d777a:	dd10      	ble.n	d779e <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x5e>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
   d777c:	4659      	mov	r1, fp
   d777e:	4648      	mov	r0, r9
   d7780:	f7fe fe6c 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d7784:	4659      	mov	r1, fp
   d7786:	9001      	str	r0, [sp, #4]
   d7788:	4650      	mov	r0, sl
   d778a:	f7fe fe67 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d778e:	9b01      	ldr	r3, [sp, #4]
   d7790:	4283      	cmp	r3, r0
   d7792:	d1de      	bne.n	d7752 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    outer_size *= input1_shape.Dims(i);
   d7794:	fb03 f808 	mul.w	r8, r3, r8
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d7798:	f10b 0b01 	add.w	fp, fp, #1
   d779c:	e7ec      	b.n	d7778 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x38>
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d779e:	f104 0b01 	add.w	fp, r4, #1
  for (int i = 0; i < axis; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
   d77a2:	2401      	movs	r4, #1
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d77a4:	45b3      	cmp	fp, r6
   d77a6:	da10      	bge.n	d77ca <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x8a>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
   d77a8:	4659      	mov	r1, fp
   d77aa:	4648      	mov	r0, r9
   d77ac:	f7fe fe56 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d77b0:	f10b 31ff 	add.w	r1, fp, #4294967295	; 0xffffffff
   d77b4:	9001      	str	r0, [sp, #4]
   d77b6:	4650      	mov	r0, sl
   d77b8:	f7fe fe50 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d77bc:	9b01      	ldr	r3, [sp, #4]
   d77be:	4283      	cmp	r3, r0
   d77c0:	d1c7      	bne.n	d7752 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    inner_size *= input1_shape.Dims(i);
   d77c2:	435c      	muls	r4, r3
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d77c4:	f10b 0b01 	add.w	fp, fp, #1
   d77c8:	e7ec      	b.n	d77a4 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x64>
   d77ca:	00a3      	lsls	r3, r4, #2
   d77cc:	9302      	str	r3, [sp, #8]
   d77ce:	2200      	movs	r2, #0
   d77d0:	fb05 f304 	mul.w	r3, r5, r4
   d77d4:	f8dd e040 	ldr.w	lr, [sp, #64]	; 0x40
   d77d8:	9304      	str	r3, [sp, #16]
   d77da:	9701      	str	r7, [sp, #4]
   d77dc:	4694      	mov	ip, r2
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d77de:	45e0      	cmp	r8, ip
   d77e0:	dd29      	ble.n	d7836 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xf6>
   d77e2:	fb02 4304 	mla	r3, r2, r4, r4
   d77e6:	9305      	str	r3, [sp, #20]
   d77e8:	2300      	movs	r3, #0
    for (int inner = 0; inner < inner_size; ++inner) {
   d77ea:	429c      	cmp	r4, r3
   d77ec:	dd19      	ble.n	d7822 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xe2>
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
   d77ee:	9901      	ldr	r1, [sp, #4]
   d77f0:	f811 a003 	ldrb.w	sl, [r1, r3]
   d77f4:	9905      	ldr	r1, [sp, #20]
   d77f6:	1859      	adds	r1, r3, r1
   d77f8:	1879      	adds	r1, r7, r1
   d77fa:	9103      	str	r1, [sp, #12]
   d77fc:	2100      	movs	r1, #0
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d77fe:	2001      	movs	r0, #1
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
   d7800:	4689      	mov	r9, r1
      for (int i = 1; i < axis_size; ++i) {
   d7802:	42a8      	cmp	r0, r5
   d7804:	da09      	bge.n	d781a <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xda>
   d7806:	9e03      	ldr	r6, [sp, #12]
   d7808:	f816 b001 	ldrb.w	fp, [r6, r1]
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
   d780c:	45da      	cmp	sl, fp
   d780e:	bf3c      	itt	cc
   d7810:	4681      	movcc	r9, r0
   d7812:	46da      	movcc	sl, fp
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d7814:	3001      	adds	r0, #1
   d7816:	4421      	add	r1, r4
   d7818:	e7f3      	b.n	d7802 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xc2>
        if (cmp(curr_value, min_max_value)) {
          min_max_value = curr_value;
          min_max_index = static_cast<T2>(i);
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
   d781a:	f84e 9023 	str.w	r9, [lr, r3, lsl #2]
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
   d781e:	3301      	adds	r3, #1
   d7820:	e7e3      	b.n	d77ea <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xaa>
   d7822:	9b02      	ldr	r3, [sp, #8]
   d7824:	9901      	ldr	r1, [sp, #4]
   d7826:	449e      	add	lr, r3
   d7828:	9b04      	ldr	r3, [sp, #16]
   d782a:	4419      	add	r1, r3
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d782c:	f10c 0c01 	add.w	ip, ip, #1
   d7830:	9101      	str	r1, [sp, #4]
   d7832:	442a      	add	r2, r5
   d7834:	e7d3      	b.n	d77de <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x9e>
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
    }
  }
}
   d7836:	b007      	add	sp, #28
   d7838:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d783c <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>:
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d783c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d7840:	6806      	ldr	r6, [r0, #0]
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d7842:	2e00      	cmp	r6, #0
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d7844:	b087      	sub	sp, #28
   d7846:	4681      	mov	r9, r0
   d7848:	460f      	mov	r7, r1
   d784a:	469a      	mov	sl, r3
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d784c:	dc01      	bgt.n	d7852 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x16>
   d784e:	f00c fd6d 	bl	e432c <abort>
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
   d7852:	f8da 1000 	ldr.w	r1, [sl]
   d7856:	1e73      	subs	r3, r6, #1
   d7858:	428b      	cmp	r3, r1
   d785a:	d1f8      	bne.n	d784e <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
   d785c:	6814      	ldr	r4, [r2, #0]
  if (axis < 0) {
   d785e:	2c00      	cmp	r4, #0
    axis += input1_shape.DimensionsCount();
   d7860:	bfb8      	it	lt
   d7862:	19a4      	addlt	r4, r4, r6
  }
  const int axis_size = input1_shape.Dims(axis);
   d7864:	4621      	mov	r1, r4
   d7866:	f7fe fdf9 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d786a:	f04f 0b00 	mov.w	fp, #0
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
  if (axis < 0) {
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);
   d786e:	4605      	mov	r5, r0

  int outer_size = 1;
   d7870:	f04f 0801 	mov.w	r8, #1
  for (int i = 0; i < axis; ++i) {
   d7874:	455c      	cmp	r4, fp
   d7876:	dd10      	ble.n	d789a <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x5e>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
   d7878:	4659      	mov	r1, fp
   d787a:	4648      	mov	r0, r9
   d787c:	f7fe fdee 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d7880:	4659      	mov	r1, fp
   d7882:	9001      	str	r0, [sp, #4]
   d7884:	4650      	mov	r0, sl
   d7886:	f7fe fde9 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d788a:	9b01      	ldr	r3, [sp, #4]
   d788c:	4283      	cmp	r3, r0
   d788e:	d1de      	bne.n	d784e <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    outer_size *= input1_shape.Dims(i);
   d7890:	fb03 f808 	mul.w	r8, r3, r8
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d7894:	f10b 0b01 	add.w	fp, fp, #1
   d7898:	e7ec      	b.n	d7874 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x38>
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d789a:	f104 0b01 	add.w	fp, r4, #1
  for (int i = 0; i < axis; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
   d789e:	2401      	movs	r4, #1
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d78a0:	45b3      	cmp	fp, r6
   d78a2:	da10      	bge.n	d78c6 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x8a>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
   d78a4:	4659      	mov	r1, fp
   d78a6:	4648      	mov	r0, r9
   d78a8:	f7fe fdd8 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d78ac:	f10b 31ff 	add.w	r1, fp, #4294967295	; 0xffffffff
   d78b0:	9001      	str	r0, [sp, #4]
   d78b2:	4650      	mov	r0, sl
   d78b4:	f7fe fdd2 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d78b8:	9b01      	ldr	r3, [sp, #4]
   d78ba:	4283      	cmp	r3, r0
   d78bc:	d1c7      	bne.n	d784e <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    inner_size *= input1_shape.Dims(i);
   d78be:	435c      	muls	r4, r3
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d78c0:	f10b 0b01 	add.w	fp, fp, #1
   d78c4:	e7ec      	b.n	d78a0 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x64>
   d78c6:	00a3      	lsls	r3, r4, #2
   d78c8:	9302      	str	r3, [sp, #8]
   d78ca:	2200      	movs	r2, #0
   d78cc:	fb05 f304 	mul.w	r3, r5, r4
   d78d0:	f8dd e040 	ldr.w	lr, [sp, #64]	; 0x40
   d78d4:	9304      	str	r3, [sp, #16]
   d78d6:	9701      	str	r7, [sp, #4]
   d78d8:	4694      	mov	ip, r2
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d78da:	45e0      	cmp	r8, ip
   d78dc:	dd29      	ble.n	d7932 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xf6>
   d78de:	fb02 4304 	mla	r3, r2, r4, r4
   d78e2:	9305      	str	r3, [sp, #20]
   d78e4:	2300      	movs	r3, #0
    for (int inner = 0; inner < inner_size; ++inner) {
   d78e6:	429c      	cmp	r4, r3
   d78e8:	dd19      	ble.n	d791e <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xe2>
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
   d78ea:	9901      	ldr	r1, [sp, #4]
   d78ec:	f811 a003 	ldrb.w	sl, [r1, r3]
   d78f0:	9905      	ldr	r1, [sp, #20]
   d78f2:	1859      	adds	r1, r3, r1
   d78f4:	1879      	adds	r1, r7, r1
   d78f6:	9103      	str	r1, [sp, #12]
   d78f8:	2100      	movs	r1, #0
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d78fa:	2001      	movs	r0, #1
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
   d78fc:	4689      	mov	r9, r1
      for (int i = 1; i < axis_size; ++i) {
   d78fe:	42a8      	cmp	r0, r5
   d7900:	da09      	bge.n	d7916 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xda>
   d7902:	9e03      	ldr	r6, [sp, #12]
   d7904:	f816 b001 	ldrb.w	fp, [r6, r1]
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
   d7908:	45da      	cmp	sl, fp
   d790a:	bf84      	itt	hi
   d790c:	4681      	movhi	r9, r0
   d790e:	46da      	movhi	sl, fp
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d7910:	3001      	adds	r0, #1
   d7912:	4421      	add	r1, r4
   d7914:	e7f3      	b.n	d78fe <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xc2>
        if (cmp(curr_value, min_max_value)) {
          min_max_value = curr_value;
          min_max_index = static_cast<T2>(i);
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
   d7916:	f84e 9023 	str.w	r9, [lr, r3, lsl #2]
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
   d791a:	3301      	adds	r3, #1
   d791c:	e7e3      	b.n	d78e6 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xaa>
   d791e:	9b02      	ldr	r3, [sp, #8]
   d7920:	9901      	ldr	r1, [sp, #4]
   d7922:	449e      	add	lr, r3
   d7924:	9b04      	ldr	r3, [sp, #16]
   d7926:	4419      	add	r1, r3
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d7928:	f10c 0c01 	add.w	ip, ip, #1
   d792c:	9101      	str	r1, [sp, #4]
   d792e:	442a      	add	r2, r5
   d7930:	e7d3      	b.n	d78da <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x9e>
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
    }
  }
}
   d7932:	b007      	add	sp, #28
   d7934:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d7938 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>:
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d7938:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d793c:	6806      	ldr	r6, [r0, #0]
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d793e:	2e00      	cmp	r6, #0
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d7940:	b087      	sub	sp, #28
   d7942:	4681      	mov	r9, r0
   d7944:	460f      	mov	r7, r1
   d7946:	469a      	mov	sl, r3
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d7948:	dc01      	bgt.n	d794e <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x16>
   d794a:	f00c fcef 	bl	e432c <abort>
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
   d794e:	f8da 1000 	ldr.w	r1, [sl]
   d7952:	1e73      	subs	r3, r6, #1
   d7954:	428b      	cmp	r3, r1
   d7956:	d1f8      	bne.n	d794a <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
   d7958:	6814      	ldr	r4, [r2, #0]
  if (axis < 0) {
   d795a:	2c00      	cmp	r4, #0
    axis += input1_shape.DimensionsCount();
   d795c:	bfb8      	it	lt
   d795e:	19a4      	addlt	r4, r4, r6
  }
  const int axis_size = input1_shape.Dims(axis);
   d7960:	4621      	mov	r1, r4
   d7962:	f7fe fd7b 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d7966:	f04f 0b00 	mov.w	fp, #0
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
  if (axis < 0) {
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);
   d796a:	4605      	mov	r5, r0

  int outer_size = 1;
   d796c:	f04f 0801 	mov.w	r8, #1
  for (int i = 0; i < axis; ++i) {
   d7970:	455c      	cmp	r4, fp
   d7972:	dd10      	ble.n	d7996 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x5e>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
   d7974:	4659      	mov	r1, fp
   d7976:	4648      	mov	r0, r9
   d7978:	f7fe fd70 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d797c:	4659      	mov	r1, fp
   d797e:	9001      	str	r0, [sp, #4]
   d7980:	4650      	mov	r0, sl
   d7982:	f7fe fd6b 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d7986:	9b01      	ldr	r3, [sp, #4]
   d7988:	4283      	cmp	r3, r0
   d798a:	d1de      	bne.n	d794a <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    outer_size *= input1_shape.Dims(i);
   d798c:	fb03 f808 	mul.w	r8, r3, r8
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d7990:	f10b 0b01 	add.w	fp, fp, #1
   d7994:	e7ec      	b.n	d7970 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x38>
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d7996:	f104 0b01 	add.w	fp, r4, #1
  for (int i = 0; i < axis; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
   d799a:	2401      	movs	r4, #1
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d799c:	45b3      	cmp	fp, r6
   d799e:	da10      	bge.n	d79c2 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x8a>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
   d79a0:	4659      	mov	r1, fp
   d79a2:	4648      	mov	r0, r9
   d79a4:	f7fe fd5a 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d79a8:	f10b 31ff 	add.w	r1, fp, #4294967295	; 0xffffffff
   d79ac:	9001      	str	r0, [sp, #4]
   d79ae:	4650      	mov	r0, sl
   d79b0:	f7fe fd54 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d79b4:	9b01      	ldr	r3, [sp, #4]
   d79b6:	4283      	cmp	r3, r0
   d79b8:	d1c7      	bne.n	d794a <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    inner_size *= input1_shape.Dims(i);
   d79ba:	435c      	muls	r4, r3
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d79bc:	f10b 0b01 	add.w	fp, fp, #1
   d79c0:	e7ec      	b.n	d799c <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x64>
   d79c2:	00a3      	lsls	r3, r4, #2
   d79c4:	9302      	str	r3, [sp, #8]
   d79c6:	2200      	movs	r2, #0
   d79c8:	fb05 f304 	mul.w	r3, r5, r4
   d79cc:	f8dd e040 	ldr.w	lr, [sp, #64]	; 0x40
   d79d0:	9304      	str	r3, [sp, #16]
   d79d2:	9701      	str	r7, [sp, #4]
   d79d4:	4694      	mov	ip, r2
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d79d6:	45e0      	cmp	r8, ip
   d79d8:	dd29      	ble.n	d7a2e <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xf6>
   d79da:	fb02 4304 	mla	r3, r2, r4, r4
   d79de:	9305      	str	r3, [sp, #20]
   d79e0:	2300      	movs	r3, #0
    for (int inner = 0; inner < inner_size; ++inner) {
   d79e2:	429c      	cmp	r4, r3
   d79e4:	dd19      	ble.n	d7a1a <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xe2>
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
   d79e6:	9901      	ldr	r1, [sp, #4]
   d79e8:	f911 a003 	ldrsb.w	sl, [r1, r3]
   d79ec:	9905      	ldr	r1, [sp, #20]
   d79ee:	1859      	adds	r1, r3, r1
   d79f0:	1879      	adds	r1, r7, r1
   d79f2:	9103      	str	r1, [sp, #12]
   d79f4:	2100      	movs	r1, #0
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d79f6:	2001      	movs	r0, #1
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
   d79f8:	4689      	mov	r9, r1
      for (int i = 1; i < axis_size; ++i) {
   d79fa:	42a8      	cmp	r0, r5
   d79fc:	da09      	bge.n	d7a12 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xda>
   d79fe:	9e03      	ldr	r6, [sp, #12]
   d7a00:	f916 b001 	ldrsb.w	fp, [r6, r1]
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
   d7a04:	45da      	cmp	sl, fp
   d7a06:	bfbc      	itt	lt
   d7a08:	4681      	movlt	r9, r0
   d7a0a:	46da      	movlt	sl, fp
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d7a0c:	3001      	adds	r0, #1
   d7a0e:	4421      	add	r1, r4
   d7a10:	e7f3      	b.n	d79fa <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xc2>
        if (cmp(curr_value, min_max_value)) {
          min_max_value = curr_value;
          min_max_index = static_cast<T2>(i);
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
   d7a12:	f84e 9023 	str.w	r9, [lr, r3, lsl #2]
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
   d7a16:	3301      	adds	r3, #1
   d7a18:	e7e3      	b.n	d79e2 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xaa>
   d7a1a:	9b02      	ldr	r3, [sp, #8]
   d7a1c:	9901      	ldr	r1, [sp, #4]
   d7a1e:	449e      	add	lr, r3
   d7a20:	9b04      	ldr	r3, [sp, #16]
   d7a22:	4419      	add	r1, r3
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d7a24:	f10c 0c01 	add.w	ip, ip, #1
   d7a28:	9101      	str	r1, [sp, #4]
   d7a2a:	442a      	add	r2, r5
   d7a2c:	e7d3      	b.n	d79d6 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x9e>
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
    }
  }
}
   d7a2e:	b007      	add	sp, #28
   d7a30:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d7a34 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>:
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d7a34:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d7a38:	6806      	ldr	r6, [r0, #0]
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d7a3a:	2e00      	cmp	r6, #0
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d7a3c:	b087      	sub	sp, #28
   d7a3e:	4681      	mov	r9, r0
   d7a40:	460f      	mov	r7, r1
   d7a42:	469a      	mov	sl, r3
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d7a44:	dc01      	bgt.n	d7a4a <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x16>
   d7a46:	f00c fc71 	bl	e432c <abort>
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
   d7a4a:	f8da 1000 	ldr.w	r1, [sl]
   d7a4e:	1e73      	subs	r3, r6, #1
   d7a50:	428b      	cmp	r3, r1
   d7a52:	d1f8      	bne.n	d7a46 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
   d7a54:	6814      	ldr	r4, [r2, #0]
  if (axis < 0) {
   d7a56:	2c00      	cmp	r4, #0
    axis += input1_shape.DimensionsCount();
   d7a58:	bfb8      	it	lt
   d7a5a:	19a4      	addlt	r4, r4, r6
  }
  const int axis_size = input1_shape.Dims(axis);
   d7a5c:	4621      	mov	r1, r4
   d7a5e:	f7fe fcfd 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d7a62:	f04f 0b00 	mov.w	fp, #0
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
  if (axis < 0) {
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);
   d7a66:	4605      	mov	r5, r0

  int outer_size = 1;
   d7a68:	f04f 0801 	mov.w	r8, #1
  for (int i = 0; i < axis; ++i) {
   d7a6c:	455c      	cmp	r4, fp
   d7a6e:	dd10      	ble.n	d7a92 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x5e>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
   d7a70:	4659      	mov	r1, fp
   d7a72:	4648      	mov	r0, r9
   d7a74:	f7fe fcf2 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d7a78:	4659      	mov	r1, fp
   d7a7a:	9001      	str	r0, [sp, #4]
   d7a7c:	4650      	mov	r0, sl
   d7a7e:	f7fe fced 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d7a82:	9b01      	ldr	r3, [sp, #4]
   d7a84:	4283      	cmp	r3, r0
   d7a86:	d1de      	bne.n	d7a46 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    outer_size *= input1_shape.Dims(i);
   d7a88:	fb03 f808 	mul.w	r8, r3, r8
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d7a8c:	f10b 0b01 	add.w	fp, fp, #1
   d7a90:	e7ec      	b.n	d7a6c <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x38>
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d7a92:	f104 0b01 	add.w	fp, r4, #1
  for (int i = 0; i < axis; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
   d7a96:	2401      	movs	r4, #1
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d7a98:	45b3      	cmp	fp, r6
   d7a9a:	da10      	bge.n	d7abe <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x8a>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
   d7a9c:	4659      	mov	r1, fp
   d7a9e:	4648      	mov	r0, r9
   d7aa0:	f7fe fcdc 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d7aa4:	f10b 31ff 	add.w	r1, fp, #4294967295	; 0xffffffff
   d7aa8:	9001      	str	r0, [sp, #4]
   d7aaa:	4650      	mov	r0, sl
   d7aac:	f7fe fcd6 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d7ab0:	9b01      	ldr	r3, [sp, #4]
   d7ab2:	4283      	cmp	r3, r0
   d7ab4:	d1c7      	bne.n	d7a46 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    inner_size *= input1_shape.Dims(i);
   d7ab6:	435c      	muls	r4, r3
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d7ab8:	f10b 0b01 	add.w	fp, fp, #1
   d7abc:	e7ec      	b.n	d7a98 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x64>
   d7abe:	00a3      	lsls	r3, r4, #2
   d7ac0:	9302      	str	r3, [sp, #8]
   d7ac2:	2200      	movs	r2, #0
   d7ac4:	fb05 f304 	mul.w	r3, r5, r4
   d7ac8:	f8dd e040 	ldr.w	lr, [sp, #64]	; 0x40
   d7acc:	9304      	str	r3, [sp, #16]
   d7ace:	9701      	str	r7, [sp, #4]
   d7ad0:	4694      	mov	ip, r2
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d7ad2:	45e0      	cmp	r8, ip
   d7ad4:	dd29      	ble.n	d7b2a <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xf6>
   d7ad6:	fb02 4304 	mla	r3, r2, r4, r4
   d7ada:	9305      	str	r3, [sp, #20]
   d7adc:	2300      	movs	r3, #0
    for (int inner = 0; inner < inner_size; ++inner) {
   d7ade:	429c      	cmp	r4, r3
   d7ae0:	dd19      	ble.n	d7b16 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xe2>
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
   d7ae2:	9901      	ldr	r1, [sp, #4]
   d7ae4:	f911 a003 	ldrsb.w	sl, [r1, r3]
   d7ae8:	9905      	ldr	r1, [sp, #20]
   d7aea:	1859      	adds	r1, r3, r1
   d7aec:	1879      	adds	r1, r7, r1
   d7aee:	9103      	str	r1, [sp, #12]
   d7af0:	2100      	movs	r1, #0
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d7af2:	2001      	movs	r0, #1
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
   d7af4:	4689      	mov	r9, r1
      for (int i = 1; i < axis_size; ++i) {
   d7af6:	42a8      	cmp	r0, r5
   d7af8:	da09      	bge.n	d7b0e <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xda>
   d7afa:	9e03      	ldr	r6, [sp, #12]
   d7afc:	f916 b001 	ldrsb.w	fp, [r6, r1]
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
   d7b00:	45da      	cmp	sl, fp
   d7b02:	bfc4      	itt	gt
   d7b04:	4681      	movgt	r9, r0
   d7b06:	46da      	movgt	sl, fp
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d7b08:	3001      	adds	r0, #1
   d7b0a:	4421      	add	r1, r4
   d7b0c:	e7f3      	b.n	d7af6 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xc2>
        if (cmp(curr_value, min_max_value)) {
          min_max_value = curr_value;
          min_max_index = static_cast<T2>(i);
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
   d7b0e:	f84e 9023 	str.w	r9, [lr, r3, lsl #2]
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
   d7b12:	3301      	adds	r3, #1
   d7b14:	e7e3      	b.n	d7ade <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xaa>
   d7b16:	9b02      	ldr	r3, [sp, #8]
   d7b18:	9901      	ldr	r1, [sp, #4]
   d7b1a:	449e      	add	lr, r3
   d7b1c:	9b04      	ldr	r3, [sp, #16]
   d7b1e:	4419      	add	r1, r3
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d7b20:	f10c 0c01 	add.w	ip, ip, #1
   d7b24:	9101      	str	r1, [sp, #4]
   d7b26:	442a      	add	r2, r5
   d7b28:	e7d3      	b.n	d7ad2 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x9e>
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
    }
  }
}
   d7b2a:	b007      	add	sp, #28
   d7b2c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d7b30 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb>:
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
                             output_shape, output_data, micro::Less());
  }
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node, bool is_arg_max) {
   d7b30:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
   d7b34:	680e      	ldr	r6, [r1, #0]
   d7b36:	4604      	mov	r4, r0
   d7b38:	4617      	mov	r7, r2
   d7b3a:	6882      	ldr	r2, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d7b3c:	68b0      	ldr	r0, [r6, #8]
   d7b3e:	2338      	movs	r3, #56	; 0x38
   d7b40:	4358      	muls	r0, r3
   d7b42:	eb02 0800 	add.w	r8, r2, r0

#define TF_LITE_ARG_MIN_MAX(data_type, axis_type, output_type)            \
  ArgMinMaxHelper(GetTensorShape(input), GetTensorData<data_type>(input), \
                  GetTensorData<axis_type>(axis), GetTensorShape(output), \
                  GetTensorData<output_type>(output), is_arg_max)
  if (axis->type == kTfLiteInt32) {
   d7b46:	5c10      	ldrb	r0, [r2, r0]
   d7b48:	2802      	cmp	r0, #2
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
                             output_shape, output_data, micro::Less());
  }
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node, bool is_arg_max) {
   d7b4a:	b08e      	sub	sp, #56	; 0x38

#define TF_LITE_ARG_MIN_MAX(data_type, axis_type, output_type)            \
  ArgMinMaxHelper(GetTensorShape(input), GetTensorData<data_type>(input), \
                  GetTensorData<axis_type>(axis), GetTensorShape(output), \
                  GetTensorData<output_type>(output), is_arg_max)
  if (axis->type == kTfLiteInt32) {
   d7b4c:	d16b      	bne.n	d7c26 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xf6>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d7b4e:	6849      	ldr	r1, [r1, #4]
   d7b50:	6849      	ldr	r1, [r1, #4]
   d7b52:	4359      	muls	r1, r3
   d7b54:	1855      	adds	r5, r2, r1
    if (output->type == kTfLiteInt32) {
   d7b56:	5c50      	ldrb	r0, [r2, r1]
   d7b58:	2802      	cmp	r0, #2
   d7b5a:	d164      	bne.n	d7c26 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xf6>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d7b5c:	6871      	ldr	r1, [r6, #4]
   d7b5e:	434b      	muls	r3, r1
   d7b60:	18d6      	adds	r6, r2, r3
      switch (input->type) {
   d7b62:	5cd0      	ldrb	r0, [r2, r3]
   d7b64:	2803      	cmp	r0, #3
   d7b66:	d01d      	beq.n	d7ba4 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0x74>
   d7b68:	2809      	cmp	r0, #9
   d7b6a:	d035      	beq.n	d7bd8 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xa8>
   d7b6c:	2801      	cmp	r0, #1
   d7b6e:	d154      	bne.n	d7c1a <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xea>
        case kTfLiteFloat32:
          TF_LITE_ARG_MIN_MAX(float, int32_t, int32_t);
   d7b70:	4631      	mov	r1, r6
   d7b72:	a804      	add	r0, sp, #16
   d7b74:	f7fe ff17 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d7b78:	6874      	ldr	r4, [r6, #4]
   d7b7a:	f8d8 6004 	ldr.w	r6, [r8, #4]
   d7b7e:	4629      	mov	r1, r5
   d7b80:	a809      	add	r0, sp, #36	; 0x24
   d7b82:	f7fe ff10 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d7b86:	686b      	ldr	r3, [r5, #4]
inline void ArgMinMaxHelper(const RuntimeShape& input1_shape,
                            const T1* input1_data, const T3* input2_data,
                            const RuntimeShape& output_shape, T2* output_data,
                            bool is_arg_max) {
  if (is_arg_max) {
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
   d7b88:	9300      	str	r3, [sp, #0]
   d7b8a:	aa03      	add	r2, sp, #12
   d7b8c:	9201      	str	r2, [sp, #4]
   d7b8e:	ab09      	add	r3, sp, #36	; 0x24
   d7b90:	4632      	mov	r2, r6
   d7b92:	4621      	mov	r1, r4
   d7b94:	a804      	add	r0, sp, #16
template <typename T1, typename T2, typename T3>
inline void ArgMinMaxHelper(const RuntimeShape& input1_shape,
                            const T1* input1_data, const T3* input2_data,
                            const RuntimeShape& output_shape, T2* output_data,
                            bool is_arg_max) {
  if (is_arg_max) {
   d7b96:	b117      	cbz	r7, d7b9e <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0x6e>
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
   d7b98:	f7ff fcca 	bl	d7530 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>
   d7b9c:	e035      	b.n	d7c0a <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xda>
                             output_shape, output_data, micro::Greater());
  } else {
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
   d7b9e:	f7ff fd4b 	bl	d7638 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>
   d7ba2:	e032      	b.n	d7c0a <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xda>
      switch (input->type) {
        case kTfLiteFloat32:
          TF_LITE_ARG_MIN_MAX(float, int32_t, int32_t);
          break;
        case kTfLiteUInt8:
          TF_LITE_ARG_MIN_MAX(uint8_t, int32_t, int32_t);
   d7ba4:	4631      	mov	r1, r6
   d7ba6:	a804      	add	r0, sp, #16
   d7ba8:	f7fe fefd 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d7bac:	6874      	ldr	r4, [r6, #4]
   d7bae:	f8d8 6004 	ldr.w	r6, [r8, #4]
   d7bb2:	4629      	mov	r1, r5
   d7bb4:	a809      	add	r0, sp, #36	; 0x24
   d7bb6:	f7fe fef6 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d7bba:	686b      	ldr	r3, [r5, #4]
inline void ArgMinMaxHelper(const RuntimeShape& input1_shape,
                            const T1* input1_data, const T3* input2_data,
                            const RuntimeShape& output_shape, T2* output_data,
                            bool is_arg_max) {
  if (is_arg_max) {
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
   d7bbc:	9300      	str	r3, [sp, #0]
   d7bbe:	aa03      	add	r2, sp, #12
   d7bc0:	9201      	str	r2, [sp, #4]
   d7bc2:	ab09      	add	r3, sp, #36	; 0x24
   d7bc4:	4632      	mov	r2, r6
   d7bc6:	4621      	mov	r1, r4
   d7bc8:	a804      	add	r0, sp, #16
template <typename T1, typename T2, typename T3>
inline void ArgMinMaxHelper(const RuntimeShape& input1_shape,
                            const T1* input1_data, const T3* input2_data,
                            const RuntimeShape& output_shape, T2* output_data,
                            bool is_arg_max) {
  if (is_arg_max) {
   d7bca:	b117      	cbz	r7, d7bd2 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xa2>
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
   d7bcc:	f7ff fdb8 	bl	d7740 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>
   d7bd0:	e01b      	b.n	d7c0a <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xda>
                             output_shape, output_data, micro::Greater());
  } else {
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
   d7bd2:	f7ff fe33 	bl	d783c <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>
   d7bd6:	e018      	b.n	d7c0a <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xda>
          break;
        case kTfLiteUInt8:
          TF_LITE_ARG_MIN_MAX(uint8_t, int32_t, int32_t);
          break;
        case kTfLiteInt8:
          TF_LITE_ARG_MIN_MAX(int8_t, int32_t, int32_t);
   d7bd8:	4631      	mov	r1, r6
   d7bda:	a804      	add	r0, sp, #16
   d7bdc:	f7fe fee3 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d7be0:	6874      	ldr	r4, [r6, #4]
   d7be2:	f8d8 6004 	ldr.w	r6, [r8, #4]
   d7be6:	4629      	mov	r1, r5
   d7be8:	a809      	add	r0, sp, #36	; 0x24
   d7bea:	f7fe fedc 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d7bee:	686b      	ldr	r3, [r5, #4]
inline void ArgMinMaxHelper(const RuntimeShape& input1_shape,
                            const T1* input1_data, const T3* input2_data,
                            const RuntimeShape& output_shape, T2* output_data,
                            bool is_arg_max) {
  if (is_arg_max) {
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
   d7bf0:	9300      	str	r3, [sp, #0]
   d7bf2:	aa03      	add	r2, sp, #12
   d7bf4:	9201      	str	r2, [sp, #4]
   d7bf6:	ab09      	add	r3, sp, #36	; 0x24
   d7bf8:	4632      	mov	r2, r6
   d7bfa:	4621      	mov	r1, r4
   d7bfc:	a804      	add	r0, sp, #16
template <typename T1, typename T2, typename T3>
inline void ArgMinMaxHelper(const RuntimeShape& input1_shape,
                            const T1* input1_data, const T3* input2_data,
                            const RuntimeShape& output_shape, T2* output_data,
                            bool is_arg_max) {
  if (is_arg_max) {
   d7bfe:	b117      	cbz	r7, d7c06 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xd6>
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
   d7c00:	f7ff fe9a 	bl	d7938 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>
   d7c04:	e001      	b.n	d7c0a <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xda>
                             output_shape, output_data, micro::Greater());
  } else {
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
   d7c06:	f7ff ff15 	bl	d7a34 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>
          break;
        case kTfLiteUInt8:
          TF_LITE_ARG_MIN_MAX(uint8_t, int32_t, int32_t);
          break;
        case kTfLiteInt8:
          TF_LITE_ARG_MIN_MAX(int8_t, int32_t, int32_t);
   d7c0a:	a809      	add	r0, sp, #36	; 0x24
   d7c0c:	f7fe fc1b 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   d7c10:	a804      	add	r0, sp, #16
   d7c12:	f7fe fc18 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
    return kTfLiteError;
  }

#undef TF_LITE_ARG_MIN_MAX

  return kTfLiteOk;
   d7c16:	2000      	movs	r0, #0
        case kTfLiteUInt8:
          TF_LITE_ARG_MIN_MAX(uint8_t, int32_t, int32_t);
          break;
        case kTfLiteInt8:
          TF_LITE_ARG_MIN_MAX(int8_t, int32_t, int32_t);
          break;
   d7c18:	e00d      	b.n	d7c36 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0x106>
        default:
          context->ReportError(context,
   d7c1a:	6965      	ldr	r5, [r4, #20]
   d7c1c:	f7fc fa7c 	bl	d4118 <TfLiteTypeGetName>
                               "Only float32, uint8 and int8 are "
                               "supported currently, got %s.",
                               TfLiteTypeGetName(input->type));
   d7c20:	4906      	ldr	r1, [pc, #24]	; (d7c3c <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0x10c>)
   d7c22:	4602      	mov	r2, r0
   d7c24:	e004      	b.n	d7c30 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0x100>
                           "Only int32 are supported currently, got %s.",
                           TfLiteTypeGetName(output->type));
      return kTfLiteError;
    }
  } else {
    context->ReportError(context, "Only int32 are supported currently, got %s.",
   d7c26:	6965      	ldr	r5, [r4, #20]
   d7c28:	f7fc fa76 	bl	d4118 <TfLiteTypeGetName>
                         TfLiteTypeGetName(axis->type));
   d7c2c:	4904      	ldr	r1, [pc, #16]	; (d7c40 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0x110>)
   d7c2e:	4602      	mov	r2, r0
   d7c30:	4620      	mov	r0, r4
   d7c32:	47a8      	blx	r5
    return kTfLiteError;
   d7c34:	2001      	movs	r0, #1
  }

#undef TF_LITE_ARG_MIN_MAX

  return kTfLiteOk;
}
   d7c36:	b00e      	add	sp, #56	; 0x38
   d7c38:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   d7c3c:	000e97e8 	.word	0x000e97e8
   d7c40:	000e9826 	.word	0x000e9826

000d7c44 <_ZN6tflite3ops5micro11arg_min_max10ArgMinEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus ArgMinEval(TfLiteContext* context, TfLiteNode* node) {
  return Eval(context, node, false);
   d7c44:	2200      	movs	r2, #0
   d7c46:	f7ff bf73 	b.w	d7b30 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb>

000d7c4a <_ZN6tflite3ops5micro11arg_min_max10ArgMaxEvalEP13TfLiteContextP10TfLiteNode>:
}

TfLiteStatus ArgMaxEval(TfLiteContext* context, TfLiteNode* node) {
  return Eval(context, node, true);
   d7c4a:	2201      	movs	r2, #1
   d7c4c:	f7ff bf70 	b.w	d7b30 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb>

000d7c50 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode>:
namespace ceil {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   d7c50:	b5f0      	push	{r4, r5, r6, r7, lr}
   d7c52:	680b      	ldr	r3, [r1, #0]
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
   d7c54:	681e      	ldr	r6, [r3, #0]
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   d7c56:	2e01      	cmp	r6, #1
namespace ceil {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   d7c58:	b085      	sub	sp, #20
   d7c5a:	4605      	mov	r5, r0
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   d7c5c:	d009      	beq.n	d7c72 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x22>
   d7c5e:	4b3b      	ldr	r3, [pc, #236]	; (d7d4c <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xfc>)
   d7c60:	9301      	str	r3, [sp, #4]
   d7c62:	2401      	movs	r4, #1
   d7c64:	4b3a      	ldr	r3, [pc, #232]	; (d7d50 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x100>)
   d7c66:	9300      	str	r3, [sp, #0]
   d7c68:	9403      	str	r4, [sp, #12]
   d7c6a:	9602      	str	r6, [sp, #8]
   d7c6c:	6945      	ldr	r5, [r0, #20]
   d7c6e:	2321      	movs	r3, #33	; 0x21
   d7c70:	e01e      	b.n	d7cb0 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
   d7c72:	f8d1 e004 	ldr.w	lr, [r1, #4]
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
   d7c76:	f8de 4000 	ldr.w	r4, [lr]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
   d7c7a:	2c01      	cmp	r4, #1
   d7c7c:	d008      	beq.n	d7c90 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x40>
   d7c7e:	4b33      	ldr	r3, [pc, #204]	; (d7d4c <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xfc>)
   d7c80:	9301      	str	r3, [sp, #4]
   d7c82:	4b34      	ldr	r3, [pc, #208]	; (d7d54 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x104>)
   d7c84:	9300      	str	r3, [sp, #0]
   d7c86:	9603      	str	r6, [sp, #12]
   d7c88:	9402      	str	r4, [sp, #8]
   d7c8a:	6944      	ldr	r4, [r0, #20]
   d7c8c:	2322      	movs	r3, #34	; 0x22
   d7c8e:	e022      	b.n	d7cd6 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x86>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d7c90:	6859      	ldr	r1, [r3, #4]
   d7c92:	6882      	ldr	r2, [r0, #8]
   d7c94:	2338      	movs	r3, #56	; 0x38
   d7c96:	4359      	muls	r1, r3
   d7c98:	1857      	adds	r7, r2, r1
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
   d7c9a:	5c56      	ldrb	r6, [r2, r1]
   d7c9c:	2e01      	cmp	r6, #1
   d7c9e:	d00b      	beq.n	d7cb8 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x68>
   d7ca0:	4b2d      	ldr	r3, [pc, #180]	; (d7d58 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x108>)
   d7ca2:	9301      	str	r3, [sp, #4]
   d7ca4:	4b2d      	ldr	r3, [pc, #180]	; (d7d5c <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x10c>)
   d7ca6:	9300      	str	r3, [sp, #0]
   d7ca8:	9403      	str	r4, [sp, #12]
   d7caa:	9602      	str	r6, [sp, #8]
   d7cac:	6945      	ldr	r5, [r0, #20]
   d7cae:	2323      	movs	r3, #35	; 0x23
   d7cb0:	4a2b      	ldr	r2, [pc, #172]	; (d7d60 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x110>)
   d7cb2:	492c      	ldr	r1, [pc, #176]	; (d7d64 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x114>)
   d7cb4:	47a8      	blx	r5
   d7cb6:	e042      	b.n	d7d3e <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xee>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d7cb8:	f8de 1004 	ldr.w	r1, [lr, #4]
   d7cbc:	434b      	muls	r3, r1
   d7cbe:	18d1      	adds	r1, r2, r3
  TF_LITE_ENSURE_EQ(context, output->type, input->type);
   d7cc0:	5cd4      	ldrb	r4, [r2, r3]
   d7cc2:	2c01      	cmp	r4, #1
   d7cc4:	d00a      	beq.n	d7cdc <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x8c>
   d7cc6:	4b25      	ldr	r3, [pc, #148]	; (d7d5c <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x10c>)
   d7cc8:	9301      	str	r3, [sp, #4]
   d7cca:	4b27      	ldr	r3, [pc, #156]	; (d7d68 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x118>)
   d7ccc:	9300      	str	r3, [sp, #0]
   d7cce:	9603      	str	r6, [sp, #12]
   d7cd0:	9402      	str	r4, [sp, #8]
   d7cd2:	6944      	ldr	r4, [r0, #20]
   d7cd4:	2324      	movs	r3, #36	; 0x24
   d7cd6:	4a22      	ldr	r2, [pc, #136]	; (d7d60 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x110>)
   d7cd8:	4922      	ldr	r1, [pc, #136]	; (d7d64 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x114>)
   d7cda:	e02f      	b.n	d7d3c <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xec>
  TF_LITE_ENSURE_EQ(context, output->bytes, input->bytes);
   d7cdc:	698b      	ldr	r3, [r1, #24]
   d7cde:	69ba      	ldr	r2, [r7, #24]
   d7ce0:	4293      	cmp	r3, r2
   d7ce2:	d008      	beq.n	d7cf6 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xa6>
   d7ce4:	9302      	str	r3, [sp, #8]
   d7ce6:	4b21      	ldr	r3, [pc, #132]	; (d7d6c <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x11c>)
   d7ce8:	9301      	str	r3, [sp, #4]
   d7cea:	4b21      	ldr	r3, [pc, #132]	; (d7d70 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x120>)
   d7cec:	9300      	str	r3, [sp, #0]
   d7cee:	9203      	str	r2, [sp, #12]
   d7cf0:	6945      	ldr	r5, [r0, #20]
   d7cf2:	2325      	movs	r3, #37	; 0x25
   d7cf4:	e7dc      	b.n	d7cb0 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
  TF_LITE_ENSURE_EQ(context, output->dims->size, input->dims->size);
   d7cf6:	688b      	ldr	r3, [r1, #8]
   d7cf8:	68ba      	ldr	r2, [r7, #8]
   d7cfa:	681e      	ldr	r6, [r3, #0]
   d7cfc:	6811      	ldr	r1, [r2, #0]
   d7cfe:	428e      	cmp	r6, r1
   d7d00:	d008      	beq.n	d7d14 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xc4>
   d7d02:	4b1c      	ldr	r3, [pc, #112]	; (d7d74 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x124>)
   d7d04:	9301      	str	r3, [sp, #4]
   d7d06:	4b1c      	ldr	r3, [pc, #112]	; (d7d78 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x128>)
   d7d08:	9300      	str	r3, [sp, #0]
   d7d0a:	9103      	str	r1, [sp, #12]
   d7d0c:	9602      	str	r6, [sp, #8]
   d7d0e:	6945      	ldr	r5, [r0, #20]
   d7d10:	2326      	movs	r3, #38	; 0x26
   d7d12:	e7cd      	b.n	d7cb0 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
   d7d14:	2100      	movs	r1, #0
  for (int i = 0; i < output->dims->size; ++i) {
   d7d16:	42b1      	cmp	r1, r6
   d7d18:	da15      	bge.n	d7d46 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xf6>
    TF_LITE_ENSURE_EQ(context, output->dims->data[i], input->dims->data[i]);
   d7d1a:	f853 0f04 	ldr.w	r0, [r3, #4]!
   d7d1e:	f852 4f04 	ldr.w	r4, [r2, #4]!
   d7d22:	42a0      	cmp	r0, r4
   d7d24:	d00d      	beq.n	d7d42 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xf2>
   d7d26:	4b15      	ldr	r3, [pc, #84]	; (d7d7c <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x12c>)
   d7d28:	9301      	str	r3, [sp, #4]
   d7d2a:	4b15      	ldr	r3, [pc, #84]	; (d7d80 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x130>)
   d7d2c:	9002      	str	r0, [sp, #8]
   d7d2e:	9300      	str	r3, [sp, #0]
   d7d30:	9403      	str	r4, [sp, #12]
   d7d32:	696c      	ldr	r4, [r5, #20]
   d7d34:	4a0a      	ldr	r2, [pc, #40]	; (d7d60 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x110>)
   d7d36:	490b      	ldr	r1, [pc, #44]	; (d7d64 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x114>)
   d7d38:	2328      	movs	r3, #40	; 0x28
   d7d3a:	4628      	mov	r0, r5
   d7d3c:	47a0      	blx	r4
   d7d3e:	2001      	movs	r0, #1
   d7d40:	e002      	b.n	d7d48 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xf8>
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
  TF_LITE_ENSURE_EQ(context, output->type, input->type);
  TF_LITE_ENSURE_EQ(context, output->bytes, input->bytes);
  TF_LITE_ENSURE_EQ(context, output->dims->size, input->dims->size);
  for (int i = 0; i < output->dims->size; ++i) {
   d7d42:	3101      	adds	r1, #1
   d7d44:	e7e7      	b.n	d7d16 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xc6>
    TF_LITE_ENSURE_EQ(context, output->dims->data[i], input->dims->data[i]);
  }
  return kTfLiteOk;
   d7d46:	2000      	movs	r0, #0
}
   d7d48:	b005      	add	sp, #20
   d7d4a:	bdf0      	pop	{r4, r5, r6, r7, pc}
   d7d4c:	000eb2c5 	.word	0x000eb2c5
   d7d50:	000e9912 	.word	0x000e9912
   d7d54:	000e9922 	.word	0x000e9922
   d7d58:	000ea167 	.word	0x000ea167
   d7d5c:	000e9933 	.word	0x000e9933
   d7d60:	000e9852 	.word	0x000e9852
   d7d64:	000e98f8 	.word	0x000e98f8
   d7d68:	000e993f 	.word	0x000e993f
   d7d6c:	000e994c 	.word	0x000e994c
   d7d70:	000e9959 	.word	0x000e9959
   d7d74:	000e9967 	.word	0x000e9967
   d7d78:	000e9979 	.word	0x000e9979
   d7d7c:	000e998c 	.word	0x000e998c
   d7d80:	000e99a1 	.word	0x000e99a1

000d7d84 <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl>:
      dims_pointer_ = new int32[dimensions_count];
#endif  // TF_LITE_STATIC_MEMORY
    }
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
   d7d84:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
  }
  // The caller must ensure that the shape is no bigger than 4-D.
  inline const int32* DimsDataUpTo4D() const { return dims_; }

  inline void Resize(int dimensions_count) {
    if (size_ > kMaxSmallSize) {
   d7d86:	6803      	ldr	r3, [r0, #0]
   d7d88:	2b04      	cmp	r3, #4
      dims_pointer_ = new int32[dimensions_count];
#endif  // TF_LITE_STATIC_MEMORY
    }
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
   d7d8a:	4604      	mov	r4, r0
   d7d8c:	460d      	mov	r5, r1
   d7d8e:	4617      	mov	r7, r2
  }
  // The caller must ensure that the shape is no bigger than 4-D.
  inline const int32* DimsDataUpTo4D() const { return dims_; }

  inline void Resize(int dimensions_count) {
    if (size_ > kMaxSmallSize) {
   d7d90:	dd03      	ble.n	d7d9a <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl+0x16>
#ifdef TF_LITE_STATIC_MEMORY
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
   d7d92:	6840      	ldr	r0, [r0, #4]
   d7d94:	b108      	cbz	r0, d7d9a <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl+0x16>
   d7d96:	f7fc f984 	bl	d40a2 <_ZdaPv>
#endif  // TF_LITE_STATIC_MEMORY
    }
    size_ = dimensions_count;
    if (dimensions_count > kMaxSmallSize) {
   d7d9a:	2d04      	cmp	r5, #4
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
#endif  // TF_LITE_STATIC_MEMORY
    }
    size_ = dimensions_count;
   d7d9c:	6025      	str	r5, [r4, #0]
   d7d9e:	ea4f 0685 	mov.w	r6, r5, lsl #2
    if (dimensions_count > kMaxSmallSize) {
   d7da2:	dd08      	ble.n	d7db6 <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl+0x32>
#ifdef TF_LITE_STATIC_MEMORY
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      dims_pointer_ = new int32[dimensions_count];
   d7da4:	f1b5 5ffe 	cmp.w	r5, #532676608	; 0x1fc00000
   d7da8:	bfd4      	ite	le
   d7daa:	4630      	movle	r0, r6
   d7dac:	f04f 30ff 	movgt.w	r0, #4294967295	; 0xffffffff
   d7db0:	f7fc f973 	bl	d409a <_Znaj>
   d7db4:	6060      	str	r0, [r4, #4]
      dims_[i] = val;
    }
  }

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d7db6:	6823      	ldr	r3, [r4, #0]
   d7db8:	2b04      	cmp	r3, #4
   d7dba:	bfcc      	ite	gt
   d7dbc:	6860      	ldrgt	r0, [r4, #4]
   d7dbe:	1d20      	addle	r0, r4, #4
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
    Resize(dimensions_count);
    int32* dst_dims = DimsData();
    std::memcpy(dst_dims, dims_data, dimensions_count * sizeof(int32));
   d7dc0:	4632      	mov	r2, r6
   d7dc2:	4639      	mov	r1, r7
  }
   d7dc4:	e8bd 40f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, lr}
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
    Resize(dimensions_count);
    int32* dst_dims = DimsData();
    std::memcpy(dst_dims, dims_data, dimensions_count * sizeof(int32));
   d7dc8:	f00f bd0b 	b.w	e77e2 <memcpy>

000d7dcc <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   d7dcc:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d7dd0:	680a      	ldr	r2, [r1, #0]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d7dd2:	6849      	ldr	r1, [r1, #4]
   d7dd4:	6883      	ldr	r3, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d7dd6:	6855      	ldr	r5, [r2, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d7dd8:	684c      	ldr	r4, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d7dda:	2238      	movs	r2, #56	; 0x38
   d7ddc:	fb02 3505 	mla	r5, r2, r5, r3
   d7de0:	b08a      	sub	sp, #40	; 0x28
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d7de2:	fb02 3404 	mla	r4, r2, r4, r3
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
}

inline RuntimeShape GetTensorShape(const TfLiteTensor* tensor) {
  if (tensor == nullptr) {
   d7de6:	b90d      	cbnz	r5, d7dec <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x20>
  // larger shapes are separately allocated.
  static constexpr int kMaxSmallSize = 4;

  RuntimeShape& operator=(RuntimeShape const&) = delete;

  RuntimeShape() : size_(0) {}
   d7de8:	9500      	str	r5, [sp, #0]
   d7dea:	e009      	b.n	d7e00 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x34>
    return RuntimeShape();
  }

  TfLiteIntArray* dims = tensor->dims;
  const int dims_size = dims->size;
   d7dec:	68aa      	ldr	r2, [r5, #8]
    for (int i = 0; i < shape_size; ++i) {
      SetDim(i, value);
    }
  }

  RuntimeShape(int dimensions_count, const int32* dims_data) : size_(0) {
   d7dee:	a80a      	add	r0, sp, #40	; 0x28
   d7df0:	2300      	movs	r3, #0
   d7df2:	f852 1b04 	ldr.w	r1, [r2], #4
   d7df6:	f840 3d28 	str.w	r3, [r0, #-40]!
    ReplaceWith(dimensions_count, dims_data);
   d7dfa:	f7ff ffc3 	bl	d7d84 <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d7dfe:	686d      	ldr	r5, [r5, #4]
}

inline RuntimeShape GetTensorShape(const TfLiteTensor* tensor) {
  if (tensor == nullptr) {
   d7e00:	b90c      	cbnz	r4, d7e06 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x3a>
  // larger shapes are separately allocated.
  static constexpr int kMaxSmallSize = 4;

  RuntimeShape& operator=(RuntimeShape const&) = delete;

  RuntimeShape() : size_(0) {}
   d7e02:	9405      	str	r4, [sp, #20]
   d7e04:	e009      	b.n	d7e1a <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x4e>
    return RuntimeShape();
  }

  TfLiteIntArray* dims = tensor->dims;
  const int dims_size = dims->size;
   d7e06:	68a2      	ldr	r2, [r4, #8]
    for (int i = 0; i < shape_size; ++i) {
      SetDim(i, value);
    }
  }

  RuntimeShape(int dimensions_count, const int32* dims_data) : size_(0) {
   d7e08:	a80a      	add	r0, sp, #40	; 0x28
   d7e0a:	2300      	movs	r3, #0
   d7e0c:	f852 1b04 	ldr.w	r1, [r2], #4
   d7e10:	f840 3d14 	str.w	r3, [r0, #-20]!
    ReplaceWith(dimensions_count, dims_data);
   d7e14:	f7ff ffb6 	bl	d7d84 <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d7e18:	6864      	ldr	r4, [r4, #4]
   d7e1a:	9f00      	ldr	r7, [sp, #0]

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   d7e1c:	9b05      	ldr	r3, [sp, #20]
   d7e1e:	429f      	cmp	r7, r3
   d7e20:	d101      	bne.n	d7e26 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x5a>
   d7e22:	2600      	movs	r6, #0
   d7e24:	e00d      	b.n	d7e42 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x76>
   d7e26:	f00c fa81 	bl	e432c <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   d7e2a:	4631      	mov	r1, r6
   d7e2c:	4668      	mov	r0, sp
   d7e2e:	f7fe fb15 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d7e32:	4631      	mov	r1, r6
   d7e34:	4680      	mov	r8, r0
   d7e36:	a805      	add	r0, sp, #20
   d7e38:	f7fe fb10 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d7e3c:	4580      	cmp	r8, r0
   d7e3e:	d1f2      	bne.n	d7e26 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x5a>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   d7e40:	3601      	adds	r6, #1
   d7e42:	42b7      	cmp	r7, r6
   d7e44:	dcf1      	bgt.n	d7e2a <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x5e>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d7e46:	2f04      	cmp	r7, #4
   d7e48:	bfcc      	ite	gt
   d7e4a:	9a01      	ldrgt	r2, [sp, #4]
   d7e4c:	aa01      	addle	r2, sp, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   d7e4e:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
   d7e50:	f04f 0801 	mov.w	r8, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   d7e54:	429f      	cmp	r7, r3
   d7e56:	dd05      	ble.n	d7e64 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x98>
      buffer_size *= dims_data[i];
   d7e58:	f852 1023 	ldr.w	r1, [r2, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   d7e5c:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
   d7e5e:	fb01 f808 	mul.w	r8, r1, r8
   d7e62:	e7f7      	b.n	d7e54 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x88>
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   d7e64:	2600      	movs	r6, #0

inline void Ceil(const RuntimeShape& input_shape, const float* input_data,
                 const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; ++i) {
   d7e66:	4546      	cmp	r6, r8
   d7e68:	da07      	bge.n	d7e7a <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0xae>
  using ::ceil;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  ceil(float __x)
  { return __builtin_ceilf(__x); }
   d7e6a:	ecb5 0a01 	vldmia	r5!, {s0}
   d7e6e:	f00d fa49 	bl	e5304 <ceilf>
   d7e72:	3601      	adds	r6, #1
    output_data[i] = std::ceil(input_data[i]);
   d7e74:	eca4 0a01 	vstmia	r4!, {s0}
   d7e78:	e7f5      	b.n	d7e66 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x9a>
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  reference_ops::Ceil(GetTensorShape(input), GetTensorData<float>(input),
                      GetTensorShape(output), GetTensorData<float>(output));
   d7e7a:	a805      	add	r0, sp, #20
   d7e7c:	f7fe fae3 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  reference_ops::Ceil(GetTensorShape(input), GetTensorData<float>(input),
   d7e80:	4668      	mov	r0, sp
   d7e82:	f7fe fae0 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                      GetTensorShape(output), GetTensorData<float>(output));

  return kTfLiteOk;
}
   d7e86:	2000      	movs	r0, #0
   d7e88:	b00a      	add	sp, #40	; 0x28
   d7e8a:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
	...

000d7e90 <_ZN6tflite3ops5micro13Register_CEILEv>:

TfLiteRegistration* Register_CEIL() {
  static TfLiteRegistration r = {/*init=*/nullptr,
                                 /*free=*/nullptr, ceil::Prepare, ceil::Eval};
  return &r;
}
   d7e90:	4800      	ldr	r0, [pc, #0]	; (d7e94 <_ZN6tflite3ops5micro13Register_CEILEv+0x4>)
   d7e92:	4770      	bx	lr
   d7e94:	2003bda8 	.word	0x2003bda8

000d7e98 <_ZN6tflite3ops5micro14Register_EQUALEv>:

TfLiteRegistration* Register_EQUAL() {
  static TfLiteRegistration r = {nullptr, nullptr, nullptr,
                                 comparisons::EqualEval};
  return &r;
}
   d7e98:	4800      	ldr	r0, [pc, #0]	; (d7e9c <_ZN6tflite3ops5micro14Register_EQUALEv+0x4>)
   d7e9a:	4770      	bx	lr
   d7e9c:	2003bde8 	.word	0x2003bde8

000d7ea0 <_ZN6tflite3ops5micro18Register_NOT_EQUALEv>:

TfLiteRegistration* Register_NOT_EQUAL() {
  static TfLiteRegistration r = {nullptr, nullptr, nullptr,
                                 comparisons::NotEqualEval};
  return &r;
}
   d7ea0:	4800      	ldr	r0, [pc, #0]	; (d7ea4 <_ZN6tflite3ops5micro18Register_NOT_EQUALEv+0x4>)
   d7ea2:	4770      	bx	lr
   d7ea4:	2003be28 	.word	0x2003be28

000d7ea8 <_ZN6tflite3ops5micro16Register_GREATEREv>:

TfLiteRegistration* Register_GREATER() {
  static TfLiteRegistration r = {nullptr, nullptr, nullptr,
                                 comparisons::GreaterEval};
  return &r;
}
   d7ea8:	4800      	ldr	r0, [pc, #0]	; (d7eac <_ZN6tflite3ops5micro16Register_GREATEREv+0x4>)
   d7eaa:	4770      	bx	lr
   d7eac:	2003bdc8 	.word	0x2003bdc8

000d7eb0 <_ZN6tflite3ops5micro22Register_GREATER_EQUALEv>:

TfLiteRegistration* Register_GREATER_EQUAL() {
  static TfLiteRegistration r = {nullptr, nullptr, nullptr,
                                 comparisons::GreaterEqualEval};
  return &r;
}
   d7eb0:	4800      	ldr	r0, [pc, #0]	; (d7eb4 <_ZN6tflite3ops5micro22Register_GREATER_EQUALEv+0x4>)
   d7eb2:	4770      	bx	lr
   d7eb4:	2003be48 	.word	0x2003be48

000d7eb8 <_ZN6tflite3ops5micro13Register_LESSEv>:

TfLiteRegistration* Register_LESS() {
  static TfLiteRegistration r = {nullptr, nullptr, nullptr,
                                 comparisons::LessEval};
  return &r;
}
   d7eb8:	4800      	ldr	r0, [pc, #0]	; (d7ebc <_ZN6tflite3ops5micro13Register_LESSEv+0x4>)
   d7eba:	4770      	bx	lr
   d7ebc:	2003be08 	.word	0x2003be08

000d7ec0 <_ZN6tflite3ops5micro19Register_LESS_EQUALEv>:

TfLiteRegistration* Register_LESS_EQUAL() {
  static TfLiteRegistration r = {nullptr, nullptr, nullptr,
                                 comparisons::LessEqualEval};
  return &r;
}
   d7ec0:	4800      	ldr	r0, [pc, #0]	; (d7ec4 <_ZN6tflite3ops5micro19Register_LESS_EQUALEv+0x4>)
   d7ec2:	4770      	bx	lr
   d7ec4:	2003be68 	.word	0x2003be68

000d7ec8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d7ec8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d7ecc:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d7ece:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d7ed0:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d7ed2:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d7ed4:	4691      	mov	r9, r2
   d7ed6:	460c      	mov	r4, r1
   d7ed8:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d7eda:	dd01      	ble.n	d7ee0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d7edc:	f00c fa26 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d7ee0:	682b      	ldr	r3, [r5, #0]
   d7ee2:	2b04      	cmp	r3, #4
   d7ee4:	dcfa      	bgt.n	d7edc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d7ee6:	6813      	ldr	r3, [r2, #0]
   d7ee8:	2b04      	cmp	r3, #4
   d7eea:	dcf7      	bgt.n	d7edc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
   d7eec:	2301      	movs	r3, #1
   d7eee:	2104      	movs	r1, #4
   d7ef0:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d7ef2:	f10d 0820 	add.w	r8, sp, #32
   d7ef6:	f7fe faea 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d7efa:	4620      	mov	r0, r4
   d7efc:	ab10      	add	r3, sp, #64	; 0x40
   d7efe:	4642      	mov	r2, r8
   d7f00:	4629      	mov	r1, r5
   d7f02:	f7fe fdf9 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d7f06:	2400      	movs	r4, #0
   d7f08:	2100      	movs	r1, #0
   d7f0a:	a803      	add	r0, sp, #12
   d7f0c:	f7fe faa6 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d7f10:	4284      	cmp	r4, r0
   d7f12:	da3d      	bge.n	d7f90 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
   d7f14:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d7f16:	2101      	movs	r1, #1
   d7f18:	a803      	add	r0, sp, #12
   d7f1a:	f7fe fa9f 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d7f1e:	4285      	cmp	r5, r0
   d7f20:	da34      	bge.n	d7f8c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
   d7f22:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d7f24:	2102      	movs	r1, #2
   d7f26:	a803      	add	r0, sp, #12
   d7f28:	f7fe fa98 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d7f2c:	4286      	cmp	r6, r0
   d7f2e:	da2b      	bge.n	d7f88 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
   d7f30:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d7f32:	2103      	movs	r1, #3
   d7f34:	a803      	add	r0, sp, #12
   d7f36:	f7fe fa91 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d7f3a:	4287      	cmp	r7, r0
   d7f3c:	da22      	bge.n	d7f84 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xbc>
          output_data[Offset(output_shape, b, y, x, c)] =
   d7f3e:	9700      	str	r7, [sp, #0]
   d7f40:	4633      	mov	r3, r6
   d7f42:	462a      	mov	r2, r5
   d7f44:	4621      	mov	r1, r4
   d7f46:	a803      	add	r0, sp, #12
   d7f48:	f7fe faed 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d7f4c:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d7f4e:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d7f50:	4633      	mov	r3, r6
   d7f52:	462a      	mov	r2, r5
   d7f54:	4621      	mov	r1, r4
   d7f56:	4640      	mov	r0, r8
   d7f58:	f7fe fb96 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d7f5c:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d7f5e:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d7f60:	4633      	mov	r3, r6
   d7f62:	462a      	mov	r2, r5
   d7f64:	4621      	mov	r1, r4
   d7f66:	a810      	add	r0, sp, #64	; 0x40
   d7f68:	f7fe fb8e 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d7f6c:	9a22      	ldr	r2, [sp, #136]	; 0x88
   d7f6e:	f819 300b 	ldrb.w	r3, [r9, fp]
   d7f72:	5c12      	ldrb	r2, [r2, r0]
   d7f74:	1a9a      	subs	r2, r3, r2
   d7f76:	4253      	negs	r3, r2
   d7f78:	4153      	adcs	r3, r2
   d7f7a:	9a24      	ldr	r2, [sp, #144]	; 0x90
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d7f7c:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
   d7f7e:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d7f82:	e7d6      	b.n	d7f32 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d7f84:	3601      	adds	r6, #1
   d7f86:	e7cd      	b.n	d7f24 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d7f88:	3501      	adds	r5, #1
   d7f8a:	e7c4      	b.n	d7f16 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d7f8c:	3401      	adds	r4, #1
   d7f8e:	e7bb      	b.n	d7f08 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d7f90:	a803      	add	r0, sp, #12
   d7f92:	f7fe fa58 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d7f96:	b019      	add	sp, #100	; 0x64
   d7f98:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d7f9c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d7f9c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d7fa0:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d7fa2:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d7fa4:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d7fa6:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d7fa8:	4692      	mov	sl, r2
   d7faa:	460c      	mov	r4, r1
   d7fac:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d7fae:	dd01      	ble.n	d7fb4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d7fb0:	f00c f9bc 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d7fb4:	682b      	ldr	r3, [r5, #0]
   d7fb6:	2b04      	cmp	r3, #4
   d7fb8:	dcfa      	bgt.n	d7fb0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d7fba:	6813      	ldr	r3, [r2, #0]
   d7fbc:	2b04      	cmp	r3, #4
   d7fbe:	dcf7      	bgt.n	d7fb0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d7fc0:	2301      	movs	r3, #1
   d7fc2:	2104      	movs	r1, #4
   d7fc4:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d7fc6:	f10d 0820 	add.w	r8, sp, #32
   d7fca:	f7fe fa80 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d7fce:	4620      	mov	r0, r4
   d7fd0:	ab10      	add	r3, sp, #64	; 0x40
   d7fd2:	4642      	mov	r2, r8
   d7fd4:	4629      	mov	r1, r5
   d7fd6:	f7fe fd8f 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d7fda:	2400      	movs	r4, #0
   d7fdc:	2100      	movs	r1, #0
   d7fde:	a803      	add	r0, sp, #12
   d7fe0:	f7fe fa3c 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d7fe4:	4284      	cmp	r4, r0
   d7fe6:	da46      	bge.n	d8076 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xda>
   d7fe8:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d7fea:	2101      	movs	r1, #1
   d7fec:	a803      	add	r0, sp, #12
   d7fee:	f7fe fa35 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d7ff2:	4285      	cmp	r5, r0
   d7ff4:	da3d      	bge.n	d8072 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd6>
   d7ff6:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d7ff8:	2102      	movs	r1, #2
   d7ffa:	a803      	add	r0, sp, #12
   d7ffc:	f7fe fa2e 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8000:	4286      	cmp	r6, r0
   d8002:	da34      	bge.n	d806e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd2>
   d8004:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8006:	2103      	movs	r1, #3
   d8008:	a803      	add	r0, sp, #12
   d800a:	f7fe fa27 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d800e:	4287      	cmp	r7, r0
   d8010:	da2b      	bge.n	d806a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xce>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8012:	9700      	str	r7, [sp, #0]
   d8014:	4633      	mov	r3, r6
   d8016:	462a      	mov	r2, r5
   d8018:	4621      	mov	r1, r4
   d801a:	a803      	add	r0, sp, #12
   d801c:	f7fe fa83 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8020:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8022:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8024:	4633      	mov	r3, r6
   d8026:	462a      	mov	r2, r5
   d8028:	4621      	mov	r1, r4
   d802a:	4640      	mov	r0, r8
   d802c:	f7fe fb2c 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8030:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8032:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8034:	4633      	mov	r3, r6
   d8036:	462a      	mov	r2, r5
   d8038:	4621      	mov	r1, r4
   d803a:	a810      	add	r0, sp, #64	; 0x40
   d803c:	f7fe fb24 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8040:	9b22      	ldr	r3, [sp, #136]	; 0x88

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8042:	9a24      	ldr	r2, [sp, #144]	; 0x90
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8044:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   d8048:	eb0a 0989 	add.w	r9, sl, r9, lsl #2

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d804c:	ed99 7a00 	vldr	s14, [r9]
   d8050:	edd0 7a00 	vldr	s15, [r0]
   d8054:	eeb4 7a67 	vcmp.f32	s14, s15
   d8058:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d805c:	bf0c      	ite	eq
   d805e:	2301      	moveq	r3, #1
   d8060:	2300      	movne	r3, #0
   d8062:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8066:	3701      	adds	r7, #1
   d8068:	e7cd      	b.n	d8006 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d806a:	3601      	adds	r6, #1
   d806c:	e7c4      	b.n	d7ff8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d806e:	3501      	adds	r5, #1
   d8070:	e7bb      	b.n	d7fea <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8072:	3401      	adds	r4, #1
   d8074:	e7b2      	b.n	d7fdc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8076:	a803      	add	r0, sp, #12
   d8078:	f7fe f9e5 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d807c:	b019      	add	sp, #100	; 0x64
   d807e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8082 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8082:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8086:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8088:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d808a:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d808c:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d808e:	4691      	mov	r9, r2
   d8090:	460c      	mov	r4, r1
   d8092:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8094:	dd01      	ble.n	d809a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8096:	f00c f949 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d809a:	682b      	ldr	r3, [r5, #0]
   d809c:	2b04      	cmp	r3, #4
   d809e:	dcfa      	bgt.n	d8096 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d80a0:	6813      	ldr	r3, [r2, #0]
   d80a2:	2b04      	cmp	r3, #4
   d80a4:	dcf7      	bgt.n	d8096 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d80a6:	2301      	movs	r3, #1
   d80a8:	2104      	movs	r1, #4
   d80aa:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d80ac:	f10d 0820 	add.w	r8, sp, #32
   d80b0:	f7fe fa0d 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d80b4:	4620      	mov	r0, r4
   d80b6:	ab10      	add	r3, sp, #64	; 0x40
   d80b8:	4642      	mov	r2, r8
   d80ba:	4629      	mov	r1, r5
   d80bc:	f7fe fd1c 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d80c0:	2400      	movs	r4, #0
   d80c2:	2100      	movs	r1, #0
   d80c4:	a803      	add	r0, sp, #12
   d80c6:	f7fe f9c9 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d80ca:	4284      	cmp	r4, r0
   d80cc:	da3e      	bge.n	d814c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xca>
   d80ce:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d80d0:	2101      	movs	r1, #1
   d80d2:	a803      	add	r0, sp, #12
   d80d4:	f7fe f9c2 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d80d8:	4285      	cmp	r5, r0
   d80da:	da35      	bge.n	d8148 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc6>
   d80dc:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d80de:	2102      	movs	r1, #2
   d80e0:	a803      	add	r0, sp, #12
   d80e2:	f7fe f9bb 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d80e6:	4286      	cmp	r6, r0
   d80e8:	da2c      	bge.n	d8144 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc2>
   d80ea:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d80ec:	2103      	movs	r1, #3
   d80ee:	a803      	add	r0, sp, #12
   d80f0:	f7fe f9b4 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d80f4:	4287      	cmp	r7, r0
   d80f6:	da23      	bge.n	d8140 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xbe>
          output_data[Offset(output_shape, b, y, x, c)] =
   d80f8:	9700      	str	r7, [sp, #0]
   d80fa:	4633      	mov	r3, r6
   d80fc:	462a      	mov	r2, r5
   d80fe:	4621      	mov	r1, r4
   d8100:	a803      	add	r0, sp, #12
   d8102:	f7fe fa10 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8106:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8108:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d810a:	4633      	mov	r3, r6
   d810c:	462a      	mov	r2, r5
   d810e:	4621      	mov	r1, r4
   d8110:	4640      	mov	r0, r8
   d8112:	f7fe fab9 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8116:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8118:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d811a:	4633      	mov	r3, r6
   d811c:	462a      	mov	r2, r5
   d811e:	4621      	mov	r1, r4
   d8120:	a810      	add	r0, sp, #64	; 0x40
   d8122:	f7fe fab1 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8126:	9a22      	ldr	r2, [sp, #136]	; 0x88
   d8128:	f859 302b 	ldr.w	r3, [r9, fp, lsl #2]
   d812c:	f852 2020 	ldr.w	r2, [r2, r0, lsl #2]
   d8130:	1a9a      	subs	r2, r3, r2
   d8132:	4253      	negs	r3, r2
   d8134:	4153      	adcs	r3, r2
   d8136:	9a24      	ldr	r2, [sp, #144]	; 0x90
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8138:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
   d813a:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d813e:	e7d5      	b.n	d80ec <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8140:	3601      	adds	r6, #1
   d8142:	e7cc      	b.n	d80de <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8144:	3501      	adds	r5, #1
   d8146:	e7c3      	b.n	d80d0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8148:	3401      	adds	r4, #1
   d814a:	e7ba      	b.n	d80c2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d814c:	a803      	add	r0, sp, #12
   d814e:	f7fe f97a 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8152:	b019      	add	sp, #100	; 0x64
   d8154:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8158 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8158:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d815c:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d815e:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8160:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8162:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8164:	4692      	mov	sl, r2
   d8166:	460c      	mov	r4, r1
   d8168:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d816a:	dd01      	ble.n	d8170 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d816c:	f00c f8de 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8170:	682b      	ldr	r3, [r5, #0]
   d8172:	2b04      	cmp	r3, #4
   d8174:	dcfa      	bgt.n	d816c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8176:	6813      	ldr	r3, [r2, #0]
   d8178:	2b04      	cmp	r3, #4
   d817a:	dcf7      	bgt.n	d816c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d817c:	2301      	movs	r3, #1
   d817e:	2104      	movs	r1, #4
   d8180:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8182:	f10d 0820 	add.w	r8, sp, #32
   d8186:	f7fe f9a2 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d818a:	4620      	mov	r0, r4
   d818c:	ab10      	add	r3, sp, #64	; 0x40
   d818e:	4642      	mov	r2, r8
   d8190:	4629      	mov	r1, r5
   d8192:	f7fe fcb1 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8196:	2400      	movs	r4, #0
   d8198:	2100      	movs	r1, #0
   d819a:	a803      	add	r0, sp, #12
   d819c:	f7fe f95e 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d81a0:	4284      	cmp	r4, r0
   d81a2:	da45      	bge.n	d8230 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd8>
   d81a4:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d81a6:	2101      	movs	r1, #1
   d81a8:	a803      	add	r0, sp, #12
   d81aa:	f7fe f957 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d81ae:	4285      	cmp	r5, r0
   d81b0:	da3c      	bge.n	d822c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd4>
   d81b2:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d81b4:	2102      	movs	r1, #2
   d81b6:	a803      	add	r0, sp, #12
   d81b8:	f7fe f950 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d81bc:	4286      	cmp	r6, r0
   d81be:	da33      	bge.n	d8228 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd0>
   d81c0:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d81c2:	2103      	movs	r1, #3
   d81c4:	a803      	add	r0, sp, #12
   d81c6:	f7fe f949 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d81ca:	4287      	cmp	r7, r0
   d81cc:	da2a      	bge.n	d8224 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
          output_data[Offset(output_shape, b, y, x, c)] =
   d81ce:	9700      	str	r7, [sp, #0]
   d81d0:	4633      	mov	r3, r6
   d81d2:	462a      	mov	r2, r5
   d81d4:	4621      	mov	r1, r4
   d81d6:	a803      	add	r0, sp, #12
   d81d8:	f7fe f9a5 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d81dc:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d81de:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d81e0:	4633      	mov	r3, r6
   d81e2:	462a      	mov	r2, r5
   d81e4:	4621      	mov	r1, r4
   d81e6:	4640      	mov	r0, r8
   d81e8:	f7fe fa4e 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d81ec:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d81ee:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d81f0:	4633      	mov	r3, r6
   d81f2:	462a      	mov	r2, r5
   d81f4:	4621      	mov	r1, r4
   d81f6:	a810      	add	r0, sp, #64	; 0x40
   d81f8:	f7fe fa46 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d81fc:	9b22      	ldr	r3, [sp, #136]	; 0x88
   d81fe:	eb0a 09c9 	add.w	r9, sl, r9, lsl #3
   d8202:	eb03 03c0 	add.w	r3, r3, r0, lsl #3

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8206:	e9d3 2300 	ldrd	r2, r3, [r3]
   d820a:	e9d9 0100 	ldrd	r0, r1, [r9]
   d820e:	4299      	cmp	r1, r3
   d8210:	bf08      	it	eq
   d8212:	4290      	cmpeq	r0, r2
   d8214:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d8216:	bf0c      	ite	eq
   d8218:	2301      	moveq	r3, #1
   d821a:	2300      	movne	r3, #0
   d821c:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8220:	3701      	adds	r7, #1
   d8222:	e7ce      	b.n	d81c2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8224:	3601      	adds	r6, #1
   d8226:	e7c5      	b.n	d81b4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8228:	3501      	adds	r5, #1
   d822a:	e7bc      	b.n	d81a6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d822c:	3401      	adds	r4, #1
   d822e:	e7b3      	b.n	d8198 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8230:	a803      	add	r0, sp, #12
   d8232:	f7fe f908 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8236:	b019      	add	sp, #100	; 0x64
   d8238:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d823c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d823c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8240:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8242:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8244:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8246:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8248:	4691      	mov	r9, r2
   d824a:	460c      	mov	r4, r1
   d824c:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d824e:	dd01      	ble.n	d8254 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8250:	f00c f86c 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8254:	682b      	ldr	r3, [r5, #0]
   d8256:	2b04      	cmp	r3, #4
   d8258:	dcfa      	bgt.n	d8250 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d825a:	6813      	ldr	r3, [r2, #0]
   d825c:	2b04      	cmp	r3, #4
   d825e:	dcf7      	bgt.n	d8250 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8260:	2301      	movs	r3, #1
   d8262:	2104      	movs	r1, #4
   d8264:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8266:	f10d 0820 	add.w	r8, sp, #32
   d826a:	f7fe f930 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d826e:	4620      	mov	r0, r4
   d8270:	ab10      	add	r3, sp, #64	; 0x40
   d8272:	4642      	mov	r2, r8
   d8274:	4629      	mov	r1, r5
   d8276:	f7fe fc3f 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d827a:	2400      	movs	r4, #0
   d827c:	2100      	movs	r1, #0
   d827e:	a803      	add	r0, sp, #12
   d8280:	f7fe f8ec 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8284:	4284      	cmp	r4, r0
   d8286:	da3b      	bge.n	d8300 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
   d8288:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d828a:	2101      	movs	r1, #1
   d828c:	a803      	add	r0, sp, #12
   d828e:	f7fe f8e5 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8292:	4285      	cmp	r5, r0
   d8294:	da32      	bge.n	d82fc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
   d8296:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8298:	2102      	movs	r1, #2
   d829a:	a803      	add	r0, sp, #12
   d829c:	f7fe f8de 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d82a0:	4286      	cmp	r6, r0
   d82a2:	da29      	bge.n	d82f8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xbc>
   d82a4:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d82a6:	2103      	movs	r1, #3
   d82a8:	a803      	add	r0, sp, #12
   d82aa:	f7fe f8d7 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d82ae:	4287      	cmp	r7, r0
   d82b0:	da20      	bge.n	d82f4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xb8>
          output_data[Offset(output_shape, b, y, x, c)] =
   d82b2:	9700      	str	r7, [sp, #0]
   d82b4:	4633      	mov	r3, r6
   d82b6:	462a      	mov	r2, r5
   d82b8:	4621      	mov	r1, r4
   d82ba:	a803      	add	r0, sp, #12
   d82bc:	f7fe f933 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d82c0:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d82c2:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d82c4:	4633      	mov	r3, r6
   d82c6:	462a      	mov	r2, r5
   d82c8:	4621      	mov	r1, r4
   d82ca:	4640      	mov	r0, r8
   d82cc:	f7fe f9dc 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d82d0:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d82d2:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d82d4:	4633      	mov	r3, r6
   d82d6:	462a      	mov	r2, r5
   d82d8:	4621      	mov	r1, r4
   d82da:	a810      	add	r0, sp, #64	; 0x40
   d82dc:	f7fe f9d4 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d82e0:	9b22      	ldr	r3, [sp, #136]	; 0x88
   d82e2:	f819 200b 	ldrb.w	r2, [r9, fp]
   d82e6:	5c1b      	ldrb	r3, [r3, r0]
   d82e8:	4053      	eors	r3, r2
   d82ea:	9a24      	ldr	r2, [sp, #144]	; 0x90
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d82ec:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
   d82ee:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d82f2:	e7d8      	b.n	d82a6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d82f4:	3601      	adds	r6, #1
   d82f6:	e7cf      	b.n	d8298 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d82f8:	3501      	adds	r5, #1
   d82fa:	e7c6      	b.n	d828a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d82fc:	3401      	adds	r4, #1
   d82fe:	e7bd      	b.n	d827c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8300:	a803      	add	r0, sp, #12
   d8302:	f7fe f8a0 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8306:	b019      	add	sp, #100	; 0x64
   d8308:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d830c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d830c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8310:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8312:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8314:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8316:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8318:	4692      	mov	sl, r2
   d831a:	460c      	mov	r4, r1
   d831c:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d831e:	dd01      	ble.n	d8324 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8320:	f00c f804 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8324:	682b      	ldr	r3, [r5, #0]
   d8326:	2b04      	cmp	r3, #4
   d8328:	dcfa      	bgt.n	d8320 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d832a:	6813      	ldr	r3, [r2, #0]
   d832c:	2b04      	cmp	r3, #4
   d832e:	dcf7      	bgt.n	d8320 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8330:	2301      	movs	r3, #1
   d8332:	2104      	movs	r1, #4
   d8334:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8336:	f10d 0820 	add.w	r8, sp, #32
   d833a:	f7fe f8c8 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d833e:	4620      	mov	r0, r4
   d8340:	ab10      	add	r3, sp, #64	; 0x40
   d8342:	4642      	mov	r2, r8
   d8344:	4629      	mov	r1, r5
   d8346:	f7fe fbd7 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d834a:	2400      	movs	r4, #0
   d834c:	2100      	movs	r1, #0
   d834e:	a803      	add	r0, sp, #12
   d8350:	f7fe f884 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8354:	4284      	cmp	r4, r0
   d8356:	da46      	bge.n	d83e6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xda>
   d8358:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d835a:	2101      	movs	r1, #1
   d835c:	a803      	add	r0, sp, #12
   d835e:	f7fe f87d 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8362:	4285      	cmp	r5, r0
   d8364:	da3d      	bge.n	d83e2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd6>
   d8366:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8368:	2102      	movs	r1, #2
   d836a:	a803      	add	r0, sp, #12
   d836c:	f7fe f876 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8370:	4286      	cmp	r6, r0
   d8372:	da34      	bge.n	d83de <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd2>
   d8374:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8376:	2103      	movs	r1, #3
   d8378:	a803      	add	r0, sp, #12
   d837a:	f7fe f86f 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d837e:	4287      	cmp	r7, r0
   d8380:	da2b      	bge.n	d83da <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xce>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8382:	9700      	str	r7, [sp, #0]
   d8384:	4633      	mov	r3, r6
   d8386:	462a      	mov	r2, r5
   d8388:	4621      	mov	r1, r4
   d838a:	a803      	add	r0, sp, #12
   d838c:	f7fe f8cb 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8390:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8392:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8394:	4633      	mov	r3, r6
   d8396:	462a      	mov	r2, r5
   d8398:	4621      	mov	r1, r4
   d839a:	4640      	mov	r0, r8
   d839c:	f7fe f974 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d83a0:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d83a2:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d83a4:	4633      	mov	r3, r6
   d83a6:	462a      	mov	r2, r5
   d83a8:	4621      	mov	r1, r4
   d83aa:	a810      	add	r0, sp, #64	; 0x40
   d83ac:	f7fe f96c 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d83b0:	9b22      	ldr	r3, [sp, #136]	; 0x88

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d83b2:	9a24      	ldr	r2, [sp, #144]	; 0x90
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d83b4:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   d83b8:	eb0a 0989 	add.w	r9, sl, r9, lsl #2

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d83bc:	ed99 7a00 	vldr	s14, [r9]
   d83c0:	edd0 7a00 	vldr	s15, [r0]
   d83c4:	eeb4 7a67 	vcmp.f32	s14, s15
   d83c8:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d83cc:	bf14      	ite	ne
   d83ce:	2301      	movne	r3, #1
   d83d0:	2300      	moveq	r3, #0
   d83d2:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d83d6:	3701      	adds	r7, #1
   d83d8:	e7cd      	b.n	d8376 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d83da:	3601      	adds	r6, #1
   d83dc:	e7c4      	b.n	d8368 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d83de:	3501      	adds	r5, #1
   d83e0:	e7bb      	b.n	d835a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d83e2:	3401      	adds	r4, #1
   d83e4:	e7b2      	b.n	d834c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d83e6:	a803      	add	r0, sp, #12
   d83e8:	f7fe f82d 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d83ec:	b019      	add	sp, #100	; 0x64
   d83ee:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d83f2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d83f2:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d83f6:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d83f8:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d83fa:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d83fc:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d83fe:	4691      	mov	r9, r2
   d8400:	460c      	mov	r4, r1
   d8402:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8404:	dd01      	ble.n	d840a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8406:	f00b ff91 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d840a:	682b      	ldr	r3, [r5, #0]
   d840c:	2b04      	cmp	r3, #4
   d840e:	dcfa      	bgt.n	d8406 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8410:	6813      	ldr	r3, [r2, #0]
   d8412:	2b04      	cmp	r3, #4
   d8414:	dcf7      	bgt.n	d8406 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8416:	2301      	movs	r3, #1
   d8418:	2104      	movs	r1, #4
   d841a:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d841c:	f10d 0820 	add.w	r8, sp, #32
   d8420:	f7fe f855 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8424:	4620      	mov	r0, r4
   d8426:	ab10      	add	r3, sp, #64	; 0x40
   d8428:	4642      	mov	r2, r8
   d842a:	4629      	mov	r1, r5
   d842c:	f7fe fb64 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8430:	2400      	movs	r4, #0
   d8432:	2100      	movs	r1, #0
   d8434:	a803      	add	r0, sp, #12
   d8436:	f7fe f811 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d843a:	4284      	cmp	r4, r0
   d843c:	da3e      	bge.n	d84bc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xca>
   d843e:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8440:	2101      	movs	r1, #1
   d8442:	a803      	add	r0, sp, #12
   d8444:	f7fe f80a 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8448:	4285      	cmp	r5, r0
   d844a:	da35      	bge.n	d84b8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc6>
   d844c:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d844e:	2102      	movs	r1, #2
   d8450:	a803      	add	r0, sp, #12
   d8452:	f7fe f803 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8456:	4286      	cmp	r6, r0
   d8458:	da2c      	bge.n	d84b4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc2>
   d845a:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d845c:	2103      	movs	r1, #3
   d845e:	a803      	add	r0, sp, #12
   d8460:	f7fd fffc 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8464:	4287      	cmp	r7, r0
   d8466:	da23      	bge.n	d84b0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xbe>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8468:	9700      	str	r7, [sp, #0]
   d846a:	4633      	mov	r3, r6
   d846c:	462a      	mov	r2, r5
   d846e:	4621      	mov	r1, r4
   d8470:	a803      	add	r0, sp, #12
   d8472:	f7fe f858 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8476:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8478:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d847a:	4633      	mov	r3, r6
   d847c:	462a      	mov	r2, r5
   d847e:	4621      	mov	r1, r4
   d8480:	4640      	mov	r0, r8
   d8482:	f7fe f901 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8486:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8488:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d848a:	4633      	mov	r3, r6
   d848c:	462a      	mov	r2, r5
   d848e:	4621      	mov	r1, r4
   d8490:	a810      	add	r0, sp, #64	; 0x40
   d8492:	f7fe f8f9 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8496:	9a22      	ldr	r2, [sp, #136]	; 0x88
   d8498:	f859 302b 	ldr.w	r3, [r9, fp, lsl #2]
   d849c:	f852 2020 	ldr.w	r2, [r2, r0, lsl #2]
   d84a0:	1a9b      	subs	r3, r3, r2
   d84a2:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d84a4:	bf18      	it	ne
   d84a6:	2301      	movne	r3, #1
   d84a8:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d84ac:	3701      	adds	r7, #1
   d84ae:	e7d5      	b.n	d845c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d84b0:	3601      	adds	r6, #1
   d84b2:	e7cc      	b.n	d844e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d84b4:	3501      	adds	r5, #1
   d84b6:	e7c3      	b.n	d8440 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d84b8:	3401      	adds	r4, #1
   d84ba:	e7ba      	b.n	d8432 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d84bc:	a803      	add	r0, sp, #12
   d84be:	f7fd ffc2 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d84c2:	b019      	add	sp, #100	; 0x64
   d84c4:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d84c8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d84c8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d84cc:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d84ce:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d84d0:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d84d2:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d84d4:	4692      	mov	sl, r2
   d84d6:	460c      	mov	r4, r1
   d84d8:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d84da:	dd01      	ble.n	d84e0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d84dc:	f00b ff26 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d84e0:	682b      	ldr	r3, [r5, #0]
   d84e2:	2b04      	cmp	r3, #4
   d84e4:	dcfa      	bgt.n	d84dc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d84e6:	6813      	ldr	r3, [r2, #0]
   d84e8:	2b04      	cmp	r3, #4
   d84ea:	dcf7      	bgt.n	d84dc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d84ec:	2301      	movs	r3, #1
   d84ee:	2104      	movs	r1, #4
   d84f0:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d84f2:	f10d 0820 	add.w	r8, sp, #32
   d84f6:	f7fd ffea 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d84fa:	4620      	mov	r0, r4
   d84fc:	ab10      	add	r3, sp, #64	; 0x40
   d84fe:	4642      	mov	r2, r8
   d8500:	4629      	mov	r1, r5
   d8502:	f7fe faf9 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8506:	2400      	movs	r4, #0
   d8508:	2100      	movs	r1, #0
   d850a:	a803      	add	r0, sp, #12
   d850c:	f7fd ffa6 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8510:	4284      	cmp	r4, r0
   d8512:	da45      	bge.n	d85a0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd8>
   d8514:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8516:	2101      	movs	r1, #1
   d8518:	a803      	add	r0, sp, #12
   d851a:	f7fd ff9f 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d851e:	4285      	cmp	r5, r0
   d8520:	da3c      	bge.n	d859c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd4>
   d8522:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8524:	2102      	movs	r1, #2
   d8526:	a803      	add	r0, sp, #12
   d8528:	f7fd ff98 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d852c:	4286      	cmp	r6, r0
   d852e:	da33      	bge.n	d8598 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd0>
   d8530:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8532:	2103      	movs	r1, #3
   d8534:	a803      	add	r0, sp, #12
   d8536:	f7fd ff91 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d853a:	4287      	cmp	r7, r0
   d853c:	da2a      	bge.n	d8594 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
          output_data[Offset(output_shape, b, y, x, c)] =
   d853e:	9700      	str	r7, [sp, #0]
   d8540:	4633      	mov	r3, r6
   d8542:	462a      	mov	r2, r5
   d8544:	4621      	mov	r1, r4
   d8546:	a803      	add	r0, sp, #12
   d8548:	f7fd ffed 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d854c:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d854e:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8550:	4633      	mov	r3, r6
   d8552:	462a      	mov	r2, r5
   d8554:	4621      	mov	r1, r4
   d8556:	4640      	mov	r0, r8
   d8558:	f7fe f896 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d855c:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d855e:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8560:	4633      	mov	r3, r6
   d8562:	462a      	mov	r2, r5
   d8564:	4621      	mov	r1, r4
   d8566:	a810      	add	r0, sp, #64	; 0x40
   d8568:	f7fe f88e 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d856c:	9b22      	ldr	r3, [sp, #136]	; 0x88
   d856e:	eb0a 09c9 	add.w	r9, sl, r9, lsl #3
   d8572:	eb03 03c0 	add.w	r3, r3, r0, lsl #3

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8576:	e9d3 2300 	ldrd	r2, r3, [r3]
   d857a:	e9d9 0100 	ldrd	r0, r1, [r9]
   d857e:	4299      	cmp	r1, r3
   d8580:	bf08      	it	eq
   d8582:	4290      	cmpeq	r0, r2
   d8584:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d8586:	bf14      	ite	ne
   d8588:	2301      	movne	r3, #1
   d858a:	2300      	moveq	r3, #0
   d858c:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8590:	3701      	adds	r7, #1
   d8592:	e7ce      	b.n	d8532 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8594:	3601      	adds	r6, #1
   d8596:	e7c5      	b.n	d8524 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8598:	3501      	adds	r5, #1
   d859a:	e7bc      	b.n	d8516 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d859c:	3401      	adds	r4, #1
   d859e:	e7b3      	b.n	d8508 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d85a0:	a803      	add	r0, sp, #12
   d85a2:	f7fd ff50 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d85a6:	b019      	add	sp, #100	; 0x64
   d85a8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d85ac <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d85ac:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d85b0:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d85b2:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d85b4:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d85b6:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d85b8:	4692      	mov	sl, r2
   d85ba:	460c      	mov	r4, r1
   d85bc:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d85be:	dd01      	ble.n	d85c4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d85c0:	f00b feb4 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d85c4:	682b      	ldr	r3, [r5, #0]
   d85c6:	2b04      	cmp	r3, #4
   d85c8:	dcfa      	bgt.n	d85c0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d85ca:	6813      	ldr	r3, [r2, #0]
   d85cc:	2b04      	cmp	r3, #4
   d85ce:	dcf7      	bgt.n	d85c0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d85d0:	2301      	movs	r3, #1
   d85d2:	2104      	movs	r1, #4
   d85d4:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d85d6:	f10d 0820 	add.w	r8, sp, #32
   d85da:	f7fd ff78 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d85de:	4620      	mov	r0, r4
   d85e0:	ab10      	add	r3, sp, #64	; 0x40
   d85e2:	4642      	mov	r2, r8
   d85e4:	4629      	mov	r1, r5
   d85e6:	f7fe fa87 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d85ea:	2400      	movs	r4, #0
   d85ec:	2100      	movs	r1, #0
   d85ee:	a803      	add	r0, sp, #12
   d85f0:	f7fd ff34 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d85f4:	4284      	cmp	r4, r0
   d85f6:	da46      	bge.n	d8686 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xda>
   d85f8:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d85fa:	2101      	movs	r1, #1
   d85fc:	a803      	add	r0, sp, #12
   d85fe:	f7fd ff2d 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8602:	4285      	cmp	r5, r0
   d8604:	da3d      	bge.n	d8682 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd6>
   d8606:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8608:	2102      	movs	r1, #2
   d860a:	a803      	add	r0, sp, #12
   d860c:	f7fd ff26 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8610:	4286      	cmp	r6, r0
   d8612:	da34      	bge.n	d867e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd2>
   d8614:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8616:	2103      	movs	r1, #3
   d8618:	a803      	add	r0, sp, #12
   d861a:	f7fd ff1f 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d861e:	4287      	cmp	r7, r0
   d8620:	da2b      	bge.n	d867a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xce>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8622:	9700      	str	r7, [sp, #0]
   d8624:	4633      	mov	r3, r6
   d8626:	462a      	mov	r2, r5
   d8628:	4621      	mov	r1, r4
   d862a:	a803      	add	r0, sp, #12
   d862c:	f7fd ff7b 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8630:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8632:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8634:	4633      	mov	r3, r6
   d8636:	462a      	mov	r2, r5
   d8638:	4621      	mov	r1, r4
   d863a:	4640      	mov	r0, r8
   d863c:	f7fe f824 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8640:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8642:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8644:	4633      	mov	r3, r6
   d8646:	462a      	mov	r2, r5
   d8648:	4621      	mov	r1, r4
   d864a:	a810      	add	r0, sp, #64	; 0x40
   d864c:	f7fe f81c 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8650:	9b22      	ldr	r3, [sp, #136]	; 0x88

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8652:	9a24      	ldr	r2, [sp, #144]	; 0x90
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8654:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   d8658:	eb0a 0989 	add.w	r9, sl, r9, lsl #2

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d865c:	ed99 7a00 	vldr	s14, [r9]
   d8660:	edd0 7a00 	vldr	s15, [r0]
   d8664:	eeb4 7ae7 	vcmpe.f32	s14, s15
   d8668:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d866c:	bfcc      	ite	gt
   d866e:	2301      	movgt	r3, #1
   d8670:	2300      	movle	r3, #0
   d8672:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8676:	3701      	adds	r7, #1
   d8678:	e7cd      	b.n	d8616 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d867a:	3601      	adds	r6, #1
   d867c:	e7c4      	b.n	d8608 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d867e:	3501      	adds	r5, #1
   d8680:	e7bb      	b.n	d85fa <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8682:	3401      	adds	r4, #1
   d8684:	e7b2      	b.n	d85ec <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8686:	a803      	add	r0, sp, #12
   d8688:	f7fd fedd 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d868c:	b019      	add	sp, #100	; 0x64
   d868e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8692 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8692:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8696:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8698:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d869a:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d869c:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d869e:	4691      	mov	r9, r2
   d86a0:	460c      	mov	r4, r1
   d86a2:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d86a4:	dd01      	ble.n	d86aa <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d86a6:	f00b fe41 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d86aa:	682b      	ldr	r3, [r5, #0]
   d86ac:	2b04      	cmp	r3, #4
   d86ae:	dcfa      	bgt.n	d86a6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d86b0:	6813      	ldr	r3, [r2, #0]
   d86b2:	2b04      	cmp	r3, #4
   d86b4:	dcf7      	bgt.n	d86a6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d86b6:	2301      	movs	r3, #1
   d86b8:	2104      	movs	r1, #4
   d86ba:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d86bc:	f10d 0820 	add.w	r8, sp, #32
   d86c0:	f7fd ff05 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d86c4:	4620      	mov	r0, r4
   d86c6:	ab10      	add	r3, sp, #64	; 0x40
   d86c8:	4642      	mov	r2, r8
   d86ca:	4629      	mov	r1, r5
   d86cc:	f7fe fa14 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d86d0:	2400      	movs	r4, #0
   d86d2:	2100      	movs	r1, #0
   d86d4:	a803      	add	r0, sp, #12
   d86d6:	f7fd fec1 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d86da:	4284      	cmp	r4, r0
   d86dc:	da3f      	bge.n	d875e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
   d86de:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d86e0:	2101      	movs	r1, #1
   d86e2:	a803      	add	r0, sp, #12
   d86e4:	f7fd feba 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d86e8:	4285      	cmp	r5, r0
   d86ea:	da36      	bge.n	d875a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
   d86ec:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d86ee:	2102      	movs	r1, #2
   d86f0:	a803      	add	r0, sp, #12
   d86f2:	f7fd feb3 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d86f6:	4286      	cmp	r6, r0
   d86f8:	da2d      	bge.n	d8756 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
   d86fa:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d86fc:	2103      	movs	r1, #3
   d86fe:	a803      	add	r0, sp, #12
   d8700:	f7fd feac 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8704:	4287      	cmp	r7, r0
   d8706:	da24      	bge.n	d8752 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8708:	9700      	str	r7, [sp, #0]
   d870a:	4633      	mov	r3, r6
   d870c:	462a      	mov	r2, r5
   d870e:	4621      	mov	r1, r4
   d8710:	a803      	add	r0, sp, #12
   d8712:	f7fd ff08 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8716:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8718:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d871a:	4633      	mov	r3, r6
   d871c:	462a      	mov	r2, r5
   d871e:	4621      	mov	r1, r4
   d8720:	4640      	mov	r0, r8
   d8722:	f7fd ffb1 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8726:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8728:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d872a:	4633      	mov	r3, r6
   d872c:	462a      	mov	r2, r5
   d872e:	4621      	mov	r1, r4
   d8730:	a810      	add	r0, sp, #64	; 0x40
   d8732:	f7fd ffa9 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8736:	9a22      	ldr	r2, [sp, #136]	; 0x88
   d8738:	f859 302b 	ldr.w	r3, [r9, fp, lsl #2]
   d873c:	f852 2020 	ldr.w	r2, [r2, r0, lsl #2]
   d8740:	4293      	cmp	r3, r2
   d8742:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d8744:	bfd4      	ite	le
   d8746:	2300      	movle	r3, #0
   d8748:	2301      	movgt	r3, #1
   d874a:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d874e:	3701      	adds	r7, #1
   d8750:	e7d4      	b.n	d86fc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8752:	3601      	adds	r6, #1
   d8754:	e7cb      	b.n	d86ee <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8756:	3501      	adds	r5, #1
   d8758:	e7c2      	b.n	d86e0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d875a:	3401      	adds	r4, #1
   d875c:	e7b9      	b.n	d86d2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d875e:	a803      	add	r0, sp, #12
   d8760:	f7fd fe71 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8764:	b019      	add	sp, #100	; 0x64
   d8766:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d876a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d876a:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d876e:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8770:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8772:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8774:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8776:	4692      	mov	sl, r2
   d8778:	460c      	mov	r4, r1
   d877a:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d877c:	dd01      	ble.n	d8782 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d877e:	f00b fdd5 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8782:	682b      	ldr	r3, [r5, #0]
   d8784:	2b04      	cmp	r3, #4
   d8786:	dcfa      	bgt.n	d877e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8788:	6813      	ldr	r3, [r2, #0]
   d878a:	2b04      	cmp	r3, #4
   d878c:	dcf7      	bgt.n	d877e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d878e:	2301      	movs	r3, #1
   d8790:	2104      	movs	r1, #4
   d8792:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8794:	f10d 0820 	add.w	r8, sp, #32
   d8798:	f7fd fe99 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d879c:	4620      	mov	r0, r4
   d879e:	ab10      	add	r3, sp, #64	; 0x40
   d87a0:	4642      	mov	r2, r8
   d87a2:	4629      	mov	r1, r5
   d87a4:	f7fe f9a8 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d87a8:	2400      	movs	r4, #0
   d87aa:	2100      	movs	r1, #0
   d87ac:	a803      	add	r0, sp, #12
   d87ae:	f7fd fe55 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d87b2:	4284      	cmp	r4, r0
   d87b4:	da45      	bge.n	d8842 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd8>
   d87b6:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d87b8:	2101      	movs	r1, #1
   d87ba:	a803      	add	r0, sp, #12
   d87bc:	f7fd fe4e 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d87c0:	4285      	cmp	r5, r0
   d87c2:	da3c      	bge.n	d883e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd4>
   d87c4:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d87c6:	2102      	movs	r1, #2
   d87c8:	a803      	add	r0, sp, #12
   d87ca:	f7fd fe47 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d87ce:	4286      	cmp	r6, r0
   d87d0:	da33      	bge.n	d883a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd0>
   d87d2:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d87d4:	2103      	movs	r1, #3
   d87d6:	a803      	add	r0, sp, #12
   d87d8:	f7fd fe40 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d87dc:	4287      	cmp	r7, r0
   d87de:	da2a      	bge.n	d8836 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
          output_data[Offset(output_shape, b, y, x, c)] =
   d87e0:	9700      	str	r7, [sp, #0]
   d87e2:	4633      	mov	r3, r6
   d87e4:	462a      	mov	r2, r5
   d87e6:	4621      	mov	r1, r4
   d87e8:	a803      	add	r0, sp, #12
   d87ea:	f7fd fe9c 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d87ee:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d87f0:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d87f2:	4633      	mov	r3, r6
   d87f4:	462a      	mov	r2, r5
   d87f6:	4621      	mov	r1, r4
   d87f8:	4640      	mov	r0, r8
   d87fa:	f7fd ff45 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d87fe:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8800:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8802:	4633      	mov	r3, r6
   d8804:	462a      	mov	r2, r5
   d8806:	4621      	mov	r1, r4
   d8808:	a810      	add	r0, sp, #64	; 0x40
   d880a:	f7fd ff3d 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d880e:	9b22      	ldr	r3, [sp, #136]	; 0x88
   d8810:	eb0a 09c9 	add.w	r9, sl, r9, lsl #3
   d8814:	eb03 00c0 	add.w	r0, r3, r0, lsl #3

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8818:	e9d0 0100 	ldrd	r0, r1, [r0]
   d881c:	e9d9 2300 	ldrd	r2, r3, [r9]
   d8820:	4290      	cmp	r0, r2
   d8822:	eb71 0303 	sbcs.w	r3, r1, r3
   d8826:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d8828:	bfb4      	ite	lt
   d882a:	2301      	movlt	r3, #1
   d882c:	2300      	movge	r3, #0
   d882e:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8832:	3701      	adds	r7, #1
   d8834:	e7ce      	b.n	d87d4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8836:	3601      	adds	r6, #1
   d8838:	e7c5      	b.n	d87c6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d883a:	3501      	adds	r5, #1
   d883c:	e7bc      	b.n	d87b8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d883e:	3401      	adds	r4, #1
   d8840:	e7b3      	b.n	d87aa <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8842:	a803      	add	r0, sp, #12
   d8844:	f7fd fdff 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8848:	b019      	add	sp, #100	; 0x64
   d884a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d884e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d884e:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8852:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8854:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8856:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8858:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d885a:	4692      	mov	sl, r2
   d885c:	460c      	mov	r4, r1
   d885e:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8860:	dd01      	ble.n	d8866 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8862:	f00b fd63 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8866:	682b      	ldr	r3, [r5, #0]
   d8868:	2b04      	cmp	r3, #4
   d886a:	dcfa      	bgt.n	d8862 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d886c:	6813      	ldr	r3, [r2, #0]
   d886e:	2b04      	cmp	r3, #4
   d8870:	dcf7      	bgt.n	d8862 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8872:	2301      	movs	r3, #1
   d8874:	2104      	movs	r1, #4
   d8876:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8878:	f10d 0820 	add.w	r8, sp, #32
   d887c:	f7fd fe27 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8880:	4620      	mov	r0, r4
   d8882:	ab10      	add	r3, sp, #64	; 0x40
   d8884:	4642      	mov	r2, r8
   d8886:	4629      	mov	r1, r5
   d8888:	f7fe f936 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d888c:	2400      	movs	r4, #0
   d888e:	2100      	movs	r1, #0
   d8890:	a803      	add	r0, sp, #12
   d8892:	f7fd fde3 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8896:	4284      	cmp	r4, r0
   d8898:	da46      	bge.n	d8928 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xda>
   d889a:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d889c:	2101      	movs	r1, #1
   d889e:	a803      	add	r0, sp, #12
   d88a0:	f7fd fddc 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d88a4:	4285      	cmp	r5, r0
   d88a6:	da3d      	bge.n	d8924 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd6>
   d88a8:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d88aa:	2102      	movs	r1, #2
   d88ac:	a803      	add	r0, sp, #12
   d88ae:	f7fd fdd5 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d88b2:	4286      	cmp	r6, r0
   d88b4:	da34      	bge.n	d8920 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd2>
   d88b6:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d88b8:	2103      	movs	r1, #3
   d88ba:	a803      	add	r0, sp, #12
   d88bc:	f7fd fdce 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d88c0:	4287      	cmp	r7, r0
   d88c2:	da2b      	bge.n	d891c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xce>
          output_data[Offset(output_shape, b, y, x, c)] =
   d88c4:	9700      	str	r7, [sp, #0]
   d88c6:	4633      	mov	r3, r6
   d88c8:	462a      	mov	r2, r5
   d88ca:	4621      	mov	r1, r4
   d88cc:	a803      	add	r0, sp, #12
   d88ce:	f7fd fe2a 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d88d2:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d88d4:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d88d6:	4633      	mov	r3, r6
   d88d8:	462a      	mov	r2, r5
   d88da:	4621      	mov	r1, r4
   d88dc:	4640      	mov	r0, r8
   d88de:	f7fd fed3 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d88e2:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d88e4:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d88e6:	4633      	mov	r3, r6
   d88e8:	462a      	mov	r2, r5
   d88ea:	4621      	mov	r1, r4
   d88ec:	a810      	add	r0, sp, #64	; 0x40
   d88ee:	f7fd fecb 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d88f2:	9b22      	ldr	r3, [sp, #136]	; 0x88

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d88f4:	9a24      	ldr	r2, [sp, #144]	; 0x90
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d88f6:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   d88fa:	eb0a 0989 	add.w	r9, sl, r9, lsl #2

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d88fe:	ed99 7a00 	vldr	s14, [r9]
   d8902:	edd0 7a00 	vldr	s15, [r0]
   d8906:	eeb4 7ae7 	vcmpe.f32	s14, s15
   d890a:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d890e:	bfac      	ite	ge
   d8910:	2301      	movge	r3, #1
   d8912:	2300      	movlt	r3, #0
   d8914:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8918:	3701      	adds	r7, #1
   d891a:	e7cd      	b.n	d88b8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d891c:	3601      	adds	r6, #1
   d891e:	e7c4      	b.n	d88aa <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8920:	3501      	adds	r5, #1
   d8922:	e7bb      	b.n	d889c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8924:	3401      	adds	r4, #1
   d8926:	e7b2      	b.n	d888e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8928:	a803      	add	r0, sp, #12
   d892a:	f7fd fd8c 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d892e:	b019      	add	sp, #100	; 0x64
   d8930:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8934 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8934:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8938:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d893a:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d893c:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d893e:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8940:	4691      	mov	r9, r2
   d8942:	460c      	mov	r4, r1
   d8944:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8946:	dd01      	ble.n	d894c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8948:	f00b fcf0 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d894c:	682b      	ldr	r3, [r5, #0]
   d894e:	2b04      	cmp	r3, #4
   d8950:	dcfa      	bgt.n	d8948 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8952:	6813      	ldr	r3, [r2, #0]
   d8954:	2b04      	cmp	r3, #4
   d8956:	dcf7      	bgt.n	d8948 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8958:	2301      	movs	r3, #1
   d895a:	2104      	movs	r1, #4
   d895c:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d895e:	f10d 0820 	add.w	r8, sp, #32
   d8962:	f7fd fdb4 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8966:	4620      	mov	r0, r4
   d8968:	ab10      	add	r3, sp, #64	; 0x40
   d896a:	4642      	mov	r2, r8
   d896c:	4629      	mov	r1, r5
   d896e:	f7fe f8c3 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8972:	2400      	movs	r4, #0
   d8974:	2100      	movs	r1, #0
   d8976:	a803      	add	r0, sp, #12
   d8978:	f7fd fd70 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d897c:	4284      	cmp	r4, r0
   d897e:	da3f      	bge.n	d8a00 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
   d8980:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8982:	2101      	movs	r1, #1
   d8984:	a803      	add	r0, sp, #12
   d8986:	f7fd fd69 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d898a:	4285      	cmp	r5, r0
   d898c:	da36      	bge.n	d89fc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
   d898e:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8990:	2102      	movs	r1, #2
   d8992:	a803      	add	r0, sp, #12
   d8994:	f7fd fd62 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8998:	4286      	cmp	r6, r0
   d899a:	da2d      	bge.n	d89f8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
   d899c:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d899e:	2103      	movs	r1, #3
   d89a0:	a803      	add	r0, sp, #12
   d89a2:	f7fd fd5b 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d89a6:	4287      	cmp	r7, r0
   d89a8:	da24      	bge.n	d89f4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
          output_data[Offset(output_shape, b, y, x, c)] =
   d89aa:	9700      	str	r7, [sp, #0]
   d89ac:	4633      	mov	r3, r6
   d89ae:	462a      	mov	r2, r5
   d89b0:	4621      	mov	r1, r4
   d89b2:	a803      	add	r0, sp, #12
   d89b4:	f7fd fdb7 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d89b8:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d89ba:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d89bc:	4633      	mov	r3, r6
   d89be:	462a      	mov	r2, r5
   d89c0:	4621      	mov	r1, r4
   d89c2:	4640      	mov	r0, r8
   d89c4:	f7fd fe60 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d89c8:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d89ca:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d89cc:	4633      	mov	r3, r6
   d89ce:	462a      	mov	r2, r5
   d89d0:	4621      	mov	r1, r4
   d89d2:	a810      	add	r0, sp, #64	; 0x40
   d89d4:	f7fd fe58 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d89d8:	9a22      	ldr	r2, [sp, #136]	; 0x88
   d89da:	f859 302b 	ldr.w	r3, [r9, fp, lsl #2]
   d89de:	f852 2020 	ldr.w	r2, [r2, r0, lsl #2]
   d89e2:	4293      	cmp	r3, r2
   d89e4:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d89e6:	bfb4      	ite	lt
   d89e8:	2300      	movlt	r3, #0
   d89ea:	2301      	movge	r3, #1
   d89ec:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d89f0:	3701      	adds	r7, #1
   d89f2:	e7d4      	b.n	d899e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d89f4:	3601      	adds	r6, #1
   d89f6:	e7cb      	b.n	d8990 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d89f8:	3501      	adds	r5, #1
   d89fa:	e7c2      	b.n	d8982 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d89fc:	3401      	adds	r4, #1
   d89fe:	e7b9      	b.n	d8974 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8a00:	a803      	add	r0, sp, #12
   d8a02:	f7fd fd20 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8a06:	b019      	add	sp, #100	; 0x64
   d8a08:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8a0c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8a0c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8a10:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8a12:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8a14:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8a16:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8a18:	4692      	mov	sl, r2
   d8a1a:	460c      	mov	r4, r1
   d8a1c:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8a1e:	dd01      	ble.n	d8a24 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8a20:	f00b fc84 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8a24:	682b      	ldr	r3, [r5, #0]
   d8a26:	2b04      	cmp	r3, #4
   d8a28:	dcfa      	bgt.n	d8a20 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8a2a:	6813      	ldr	r3, [r2, #0]
   d8a2c:	2b04      	cmp	r3, #4
   d8a2e:	dcf7      	bgt.n	d8a20 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8a30:	2301      	movs	r3, #1
   d8a32:	2104      	movs	r1, #4
   d8a34:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8a36:	f10d 0820 	add.w	r8, sp, #32
   d8a3a:	f7fd fd48 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8a3e:	4620      	mov	r0, r4
   d8a40:	ab10      	add	r3, sp, #64	; 0x40
   d8a42:	4642      	mov	r2, r8
   d8a44:	4629      	mov	r1, r5
   d8a46:	f7fe f857 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8a4a:	2400      	movs	r4, #0
   d8a4c:	2100      	movs	r1, #0
   d8a4e:	a803      	add	r0, sp, #12
   d8a50:	f7fd fd04 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8a54:	4284      	cmp	r4, r0
   d8a56:	da45      	bge.n	d8ae4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd8>
   d8a58:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8a5a:	2101      	movs	r1, #1
   d8a5c:	a803      	add	r0, sp, #12
   d8a5e:	f7fd fcfd 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8a62:	4285      	cmp	r5, r0
   d8a64:	da3c      	bge.n	d8ae0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd4>
   d8a66:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8a68:	2102      	movs	r1, #2
   d8a6a:	a803      	add	r0, sp, #12
   d8a6c:	f7fd fcf6 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8a70:	4286      	cmp	r6, r0
   d8a72:	da33      	bge.n	d8adc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd0>
   d8a74:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8a76:	2103      	movs	r1, #3
   d8a78:	a803      	add	r0, sp, #12
   d8a7a:	f7fd fcef 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8a7e:	4287      	cmp	r7, r0
   d8a80:	da2a      	bge.n	d8ad8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8a82:	9700      	str	r7, [sp, #0]
   d8a84:	4633      	mov	r3, r6
   d8a86:	462a      	mov	r2, r5
   d8a88:	4621      	mov	r1, r4
   d8a8a:	a803      	add	r0, sp, #12
   d8a8c:	f7fd fd4b 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8a90:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8a92:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8a94:	4633      	mov	r3, r6
   d8a96:	462a      	mov	r2, r5
   d8a98:	4621      	mov	r1, r4
   d8a9a:	4640      	mov	r0, r8
   d8a9c:	f7fd fdf4 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8aa0:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8aa2:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8aa4:	4633      	mov	r3, r6
   d8aa6:	462a      	mov	r2, r5
   d8aa8:	4621      	mov	r1, r4
   d8aaa:	a810      	add	r0, sp, #64	; 0x40
   d8aac:	f7fd fdec 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8ab0:	9b22      	ldr	r3, [sp, #136]	; 0x88
   d8ab2:	eb0a 09c9 	add.w	r9, sl, r9, lsl #3
   d8ab6:	eb03 03c0 	add.w	r3, r3, r0, lsl #3

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8aba:	e9d3 2300 	ldrd	r2, r3, [r3]
   d8abe:	e9d9 0100 	ldrd	r0, r1, [r9]
   d8ac2:	4290      	cmp	r0, r2
   d8ac4:	eb71 0303 	sbcs.w	r3, r1, r3
   d8ac8:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d8aca:	bfac      	ite	ge
   d8acc:	2301      	movge	r3, #1
   d8ace:	2300      	movlt	r3, #0
   d8ad0:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8ad4:	3701      	adds	r7, #1
   d8ad6:	e7ce      	b.n	d8a76 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8ad8:	3601      	adds	r6, #1
   d8ada:	e7c5      	b.n	d8a68 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8adc:	3501      	adds	r5, #1
   d8ade:	e7bc      	b.n	d8a5a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8ae0:	3401      	adds	r4, #1
   d8ae2:	e7b3      	b.n	d8a4c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8ae4:	a803      	add	r0, sp, #12
   d8ae6:	f7fd fcae 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8aea:	b019      	add	sp, #100	; 0x64
   d8aec:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8af0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8af0:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8af4:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8af6:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8af8:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8afa:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8afc:	4692      	mov	sl, r2
   d8afe:	460c      	mov	r4, r1
   d8b00:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8b02:	dd01      	ble.n	d8b08 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8b04:	f00b fc12 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8b08:	682b      	ldr	r3, [r5, #0]
   d8b0a:	2b04      	cmp	r3, #4
   d8b0c:	dcfa      	bgt.n	d8b04 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8b0e:	6813      	ldr	r3, [r2, #0]
   d8b10:	2b04      	cmp	r3, #4
   d8b12:	dcf7      	bgt.n	d8b04 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8b14:	2301      	movs	r3, #1
   d8b16:	2104      	movs	r1, #4
   d8b18:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8b1a:	f10d 0820 	add.w	r8, sp, #32
   d8b1e:	f7fd fcd6 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8b22:	4620      	mov	r0, r4
   d8b24:	ab10      	add	r3, sp, #64	; 0x40
   d8b26:	4642      	mov	r2, r8
   d8b28:	4629      	mov	r1, r5
   d8b2a:	f7fd ffe5 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8b2e:	2400      	movs	r4, #0
   d8b30:	2100      	movs	r1, #0
   d8b32:	a803      	add	r0, sp, #12
   d8b34:	f7fd fc92 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8b38:	4284      	cmp	r4, r0
   d8b3a:	da46      	bge.n	d8bca <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xda>
   d8b3c:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8b3e:	2101      	movs	r1, #1
   d8b40:	a803      	add	r0, sp, #12
   d8b42:	f7fd fc8b 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8b46:	4285      	cmp	r5, r0
   d8b48:	da3d      	bge.n	d8bc6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd6>
   d8b4a:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8b4c:	2102      	movs	r1, #2
   d8b4e:	a803      	add	r0, sp, #12
   d8b50:	f7fd fc84 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8b54:	4286      	cmp	r6, r0
   d8b56:	da34      	bge.n	d8bc2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd2>
   d8b58:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8b5a:	2103      	movs	r1, #3
   d8b5c:	a803      	add	r0, sp, #12
   d8b5e:	f7fd fc7d 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8b62:	4287      	cmp	r7, r0
   d8b64:	da2b      	bge.n	d8bbe <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xce>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8b66:	9700      	str	r7, [sp, #0]
   d8b68:	4633      	mov	r3, r6
   d8b6a:	462a      	mov	r2, r5
   d8b6c:	4621      	mov	r1, r4
   d8b6e:	a803      	add	r0, sp, #12
   d8b70:	f7fd fcd9 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8b74:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8b76:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8b78:	4633      	mov	r3, r6
   d8b7a:	462a      	mov	r2, r5
   d8b7c:	4621      	mov	r1, r4
   d8b7e:	4640      	mov	r0, r8
   d8b80:	f7fd fd82 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8b84:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8b86:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8b88:	4633      	mov	r3, r6
   d8b8a:	462a      	mov	r2, r5
   d8b8c:	4621      	mov	r1, r4
   d8b8e:	a810      	add	r0, sp, #64	; 0x40
   d8b90:	f7fd fd7a 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8b94:	9b22      	ldr	r3, [sp, #136]	; 0x88

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8b96:	9a24      	ldr	r2, [sp, #144]	; 0x90
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8b98:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   d8b9c:	eb0a 0989 	add.w	r9, sl, r9, lsl #2

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8ba0:	ed99 7a00 	vldr	s14, [r9]
   d8ba4:	edd0 7a00 	vldr	s15, [r0]
   d8ba8:	eeb4 7ae7 	vcmpe.f32	s14, s15
   d8bac:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d8bb0:	bf4c      	ite	mi
   d8bb2:	2301      	movmi	r3, #1
   d8bb4:	2300      	movpl	r3, #0
   d8bb6:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8bba:	3701      	adds	r7, #1
   d8bbc:	e7cd      	b.n	d8b5a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8bbe:	3601      	adds	r6, #1
   d8bc0:	e7c4      	b.n	d8b4c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8bc2:	3501      	adds	r5, #1
   d8bc4:	e7bb      	b.n	d8b3e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8bc6:	3401      	adds	r4, #1
   d8bc8:	e7b2      	b.n	d8b30 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8bca:	a803      	add	r0, sp, #12
   d8bcc:	f7fd fc3b 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8bd0:	b019      	add	sp, #100	; 0x64
   d8bd2:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8bd6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8bd6:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8bda:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8bdc:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8bde:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8be0:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8be2:	4691      	mov	r9, r2
   d8be4:	460c      	mov	r4, r1
   d8be6:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8be8:	dd01      	ble.n	d8bee <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8bea:	f00b fb9f 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8bee:	682b      	ldr	r3, [r5, #0]
   d8bf0:	2b04      	cmp	r3, #4
   d8bf2:	dcfa      	bgt.n	d8bea <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8bf4:	6813      	ldr	r3, [r2, #0]
   d8bf6:	2b04      	cmp	r3, #4
   d8bf8:	dcf7      	bgt.n	d8bea <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8bfa:	2301      	movs	r3, #1
   d8bfc:	2104      	movs	r1, #4
   d8bfe:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8c00:	f10d 0820 	add.w	r8, sp, #32
   d8c04:	f7fd fc63 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8c08:	4620      	mov	r0, r4
   d8c0a:	ab10      	add	r3, sp, #64	; 0x40
   d8c0c:	4642      	mov	r2, r8
   d8c0e:	4629      	mov	r1, r5
   d8c10:	f7fd ff72 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8c14:	2400      	movs	r4, #0
   d8c16:	2100      	movs	r1, #0
   d8c18:	a803      	add	r0, sp, #12
   d8c1a:	f7fd fc1f 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8c1e:	4284      	cmp	r4, r0
   d8c20:	da3f      	bge.n	d8ca2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
   d8c22:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8c24:	2101      	movs	r1, #1
   d8c26:	a803      	add	r0, sp, #12
   d8c28:	f7fd fc18 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8c2c:	4285      	cmp	r5, r0
   d8c2e:	da36      	bge.n	d8c9e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
   d8c30:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8c32:	2102      	movs	r1, #2
   d8c34:	a803      	add	r0, sp, #12
   d8c36:	f7fd fc11 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8c3a:	4286      	cmp	r6, r0
   d8c3c:	da2d      	bge.n	d8c9a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
   d8c3e:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8c40:	2103      	movs	r1, #3
   d8c42:	a803      	add	r0, sp, #12
   d8c44:	f7fd fc0a 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8c48:	4287      	cmp	r7, r0
   d8c4a:	da24      	bge.n	d8c96 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8c4c:	9700      	str	r7, [sp, #0]
   d8c4e:	4633      	mov	r3, r6
   d8c50:	462a      	mov	r2, r5
   d8c52:	4621      	mov	r1, r4
   d8c54:	a803      	add	r0, sp, #12
   d8c56:	f7fd fc66 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8c5a:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8c5c:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8c5e:	4633      	mov	r3, r6
   d8c60:	462a      	mov	r2, r5
   d8c62:	4621      	mov	r1, r4
   d8c64:	4640      	mov	r0, r8
   d8c66:	f7fd fd0f 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8c6a:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8c6c:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8c6e:	4633      	mov	r3, r6
   d8c70:	462a      	mov	r2, r5
   d8c72:	4621      	mov	r1, r4
   d8c74:	a810      	add	r0, sp, #64	; 0x40
   d8c76:	f7fd fd07 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8c7a:	9a22      	ldr	r2, [sp, #136]	; 0x88
   d8c7c:	f859 302b 	ldr.w	r3, [r9, fp, lsl #2]
   d8c80:	f852 2020 	ldr.w	r2, [r2, r0, lsl #2]
   d8c84:	4293      	cmp	r3, r2
   d8c86:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d8c88:	bfac      	ite	ge
   d8c8a:	2300      	movge	r3, #0
   d8c8c:	2301      	movlt	r3, #1
   d8c8e:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8c92:	3701      	adds	r7, #1
   d8c94:	e7d4      	b.n	d8c40 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8c96:	3601      	adds	r6, #1
   d8c98:	e7cb      	b.n	d8c32 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8c9a:	3501      	adds	r5, #1
   d8c9c:	e7c2      	b.n	d8c24 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8c9e:	3401      	adds	r4, #1
   d8ca0:	e7b9      	b.n	d8c16 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8ca2:	a803      	add	r0, sp, #12
   d8ca4:	f7fd fbcf 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8ca8:	b019      	add	sp, #100	; 0x64
   d8caa:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8cae <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8cae:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8cb2:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8cb4:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8cb6:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8cb8:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8cba:	4692      	mov	sl, r2
   d8cbc:	460c      	mov	r4, r1
   d8cbe:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8cc0:	dd01      	ble.n	d8cc6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8cc2:	f00b fb33 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8cc6:	682b      	ldr	r3, [r5, #0]
   d8cc8:	2b04      	cmp	r3, #4
   d8cca:	dcfa      	bgt.n	d8cc2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8ccc:	6813      	ldr	r3, [r2, #0]
   d8cce:	2b04      	cmp	r3, #4
   d8cd0:	dcf7      	bgt.n	d8cc2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8cd2:	2301      	movs	r3, #1
   d8cd4:	2104      	movs	r1, #4
   d8cd6:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8cd8:	f10d 0820 	add.w	r8, sp, #32
   d8cdc:	f7fd fbf7 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8ce0:	4620      	mov	r0, r4
   d8ce2:	ab10      	add	r3, sp, #64	; 0x40
   d8ce4:	4642      	mov	r2, r8
   d8ce6:	4629      	mov	r1, r5
   d8ce8:	f7fd ff06 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8cec:	2400      	movs	r4, #0
   d8cee:	2100      	movs	r1, #0
   d8cf0:	a803      	add	r0, sp, #12
   d8cf2:	f7fd fbb3 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8cf6:	4284      	cmp	r4, r0
   d8cf8:	da45      	bge.n	d8d86 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd8>
   d8cfa:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8cfc:	2101      	movs	r1, #1
   d8cfe:	a803      	add	r0, sp, #12
   d8d00:	f7fd fbac 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8d04:	4285      	cmp	r5, r0
   d8d06:	da3c      	bge.n	d8d82 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd4>
   d8d08:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8d0a:	2102      	movs	r1, #2
   d8d0c:	a803      	add	r0, sp, #12
   d8d0e:	f7fd fba5 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8d12:	4286      	cmp	r6, r0
   d8d14:	da33      	bge.n	d8d7e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd0>
   d8d16:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8d18:	2103      	movs	r1, #3
   d8d1a:	a803      	add	r0, sp, #12
   d8d1c:	f7fd fb9e 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8d20:	4287      	cmp	r7, r0
   d8d22:	da2a      	bge.n	d8d7a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8d24:	9700      	str	r7, [sp, #0]
   d8d26:	4633      	mov	r3, r6
   d8d28:	462a      	mov	r2, r5
   d8d2a:	4621      	mov	r1, r4
   d8d2c:	a803      	add	r0, sp, #12
   d8d2e:	f7fd fbfa 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8d32:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8d34:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8d36:	4633      	mov	r3, r6
   d8d38:	462a      	mov	r2, r5
   d8d3a:	4621      	mov	r1, r4
   d8d3c:	4640      	mov	r0, r8
   d8d3e:	f7fd fca3 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8d42:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8d44:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8d46:	4633      	mov	r3, r6
   d8d48:	462a      	mov	r2, r5
   d8d4a:	4621      	mov	r1, r4
   d8d4c:	a810      	add	r0, sp, #64	; 0x40
   d8d4e:	f7fd fc9b 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8d52:	9b22      	ldr	r3, [sp, #136]	; 0x88
   d8d54:	eb0a 09c9 	add.w	r9, sl, r9, lsl #3
   d8d58:	eb03 03c0 	add.w	r3, r3, r0, lsl #3

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8d5c:	e9d3 2300 	ldrd	r2, r3, [r3]
   d8d60:	e9d9 0100 	ldrd	r0, r1, [r9]
   d8d64:	4290      	cmp	r0, r2
   d8d66:	eb71 0303 	sbcs.w	r3, r1, r3
   d8d6a:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d8d6c:	bfb4      	ite	lt
   d8d6e:	2301      	movlt	r3, #1
   d8d70:	2300      	movge	r3, #0
   d8d72:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8d76:	3701      	adds	r7, #1
   d8d78:	e7ce      	b.n	d8d18 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8d7a:	3601      	adds	r6, #1
   d8d7c:	e7c5      	b.n	d8d0a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8d7e:	3501      	adds	r5, #1
   d8d80:	e7bc      	b.n	d8cfc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8d82:	3401      	adds	r4, #1
   d8d84:	e7b3      	b.n	d8cee <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8d86:	a803      	add	r0, sp, #12
   d8d88:	f7fd fb5d 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8d8c:	b019      	add	sp, #100	; 0x64
   d8d8e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8d92 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8d92:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8d96:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8d98:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8d9a:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8d9c:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8d9e:	4692      	mov	sl, r2
   d8da0:	460c      	mov	r4, r1
   d8da2:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8da4:	dd01      	ble.n	d8daa <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8da6:	f00b fac1 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8daa:	682b      	ldr	r3, [r5, #0]
   d8dac:	2b04      	cmp	r3, #4
   d8dae:	dcfa      	bgt.n	d8da6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8db0:	6813      	ldr	r3, [r2, #0]
   d8db2:	2b04      	cmp	r3, #4
   d8db4:	dcf7      	bgt.n	d8da6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8db6:	2301      	movs	r3, #1
   d8db8:	2104      	movs	r1, #4
   d8dba:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8dbc:	f10d 0820 	add.w	r8, sp, #32
   d8dc0:	f7fd fb85 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8dc4:	4620      	mov	r0, r4
   d8dc6:	ab10      	add	r3, sp, #64	; 0x40
   d8dc8:	4642      	mov	r2, r8
   d8dca:	4629      	mov	r1, r5
   d8dcc:	f7fd fe94 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8dd0:	2400      	movs	r4, #0
   d8dd2:	2100      	movs	r1, #0
   d8dd4:	a803      	add	r0, sp, #12
   d8dd6:	f7fd fb41 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8dda:	4284      	cmp	r4, r0
   d8ddc:	da46      	bge.n	d8e6c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xda>
   d8dde:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8de0:	2101      	movs	r1, #1
   d8de2:	a803      	add	r0, sp, #12
   d8de4:	f7fd fb3a 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8de8:	4285      	cmp	r5, r0
   d8dea:	da3d      	bge.n	d8e68 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd6>
   d8dec:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8dee:	2102      	movs	r1, #2
   d8df0:	a803      	add	r0, sp, #12
   d8df2:	f7fd fb33 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8df6:	4286      	cmp	r6, r0
   d8df8:	da34      	bge.n	d8e64 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd2>
   d8dfa:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8dfc:	2103      	movs	r1, #3
   d8dfe:	a803      	add	r0, sp, #12
   d8e00:	f7fd fb2c 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8e04:	4287      	cmp	r7, r0
   d8e06:	da2b      	bge.n	d8e60 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xce>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8e08:	9700      	str	r7, [sp, #0]
   d8e0a:	4633      	mov	r3, r6
   d8e0c:	462a      	mov	r2, r5
   d8e0e:	4621      	mov	r1, r4
   d8e10:	a803      	add	r0, sp, #12
   d8e12:	f7fd fb88 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8e16:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8e18:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8e1a:	4633      	mov	r3, r6
   d8e1c:	462a      	mov	r2, r5
   d8e1e:	4621      	mov	r1, r4
   d8e20:	4640      	mov	r0, r8
   d8e22:	f7fd fc31 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8e26:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8e28:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8e2a:	4633      	mov	r3, r6
   d8e2c:	462a      	mov	r2, r5
   d8e2e:	4621      	mov	r1, r4
   d8e30:	a810      	add	r0, sp, #64	; 0x40
   d8e32:	f7fd fc29 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8e36:	9b22      	ldr	r3, [sp, #136]	; 0x88

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8e38:	9a24      	ldr	r2, [sp, #144]	; 0x90
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8e3a:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   d8e3e:	eb0a 0989 	add.w	r9, sl, r9, lsl #2

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8e42:	ed99 7a00 	vldr	s14, [r9]
   d8e46:	edd0 7a00 	vldr	s15, [r0]
   d8e4a:	eeb4 7ae7 	vcmpe.f32	s14, s15
   d8e4e:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d8e52:	bf94      	ite	ls
   d8e54:	2301      	movls	r3, #1
   d8e56:	2300      	movhi	r3, #0
   d8e58:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8e5c:	3701      	adds	r7, #1
   d8e5e:	e7cd      	b.n	d8dfc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8e60:	3601      	adds	r6, #1
   d8e62:	e7c4      	b.n	d8dee <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8e64:	3501      	adds	r5, #1
   d8e66:	e7bb      	b.n	d8de0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8e68:	3401      	adds	r4, #1
   d8e6a:	e7b2      	b.n	d8dd2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8e6c:	a803      	add	r0, sp, #12
   d8e6e:	f7fd faea 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8e72:	b019      	add	sp, #100	; 0x64
   d8e74:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8e78 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8e78:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8e7c:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8e7e:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8e80:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8e82:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8e84:	4691      	mov	r9, r2
   d8e86:	460c      	mov	r4, r1
   d8e88:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8e8a:	dd01      	ble.n	d8e90 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8e8c:	f00b fa4e 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8e90:	682b      	ldr	r3, [r5, #0]
   d8e92:	2b04      	cmp	r3, #4
   d8e94:	dcfa      	bgt.n	d8e8c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8e96:	6813      	ldr	r3, [r2, #0]
   d8e98:	2b04      	cmp	r3, #4
   d8e9a:	dcf7      	bgt.n	d8e8c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8e9c:	2301      	movs	r3, #1
   d8e9e:	2104      	movs	r1, #4
   d8ea0:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8ea2:	f10d 0820 	add.w	r8, sp, #32
   d8ea6:	f7fd fb12 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8eaa:	4620      	mov	r0, r4
   d8eac:	ab10      	add	r3, sp, #64	; 0x40
   d8eae:	4642      	mov	r2, r8
   d8eb0:	4629      	mov	r1, r5
   d8eb2:	f7fd fe21 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8eb6:	2400      	movs	r4, #0
   d8eb8:	2100      	movs	r1, #0
   d8eba:	a803      	add	r0, sp, #12
   d8ebc:	f7fd face 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8ec0:	4284      	cmp	r4, r0
   d8ec2:	da3f      	bge.n	d8f44 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
   d8ec4:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8ec6:	2101      	movs	r1, #1
   d8ec8:	a803      	add	r0, sp, #12
   d8eca:	f7fd fac7 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8ece:	4285      	cmp	r5, r0
   d8ed0:	da36      	bge.n	d8f40 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
   d8ed2:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8ed4:	2102      	movs	r1, #2
   d8ed6:	a803      	add	r0, sp, #12
   d8ed8:	f7fd fac0 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8edc:	4286      	cmp	r6, r0
   d8ede:	da2d      	bge.n	d8f3c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
   d8ee0:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8ee2:	2103      	movs	r1, #3
   d8ee4:	a803      	add	r0, sp, #12
   d8ee6:	f7fd fab9 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8eea:	4287      	cmp	r7, r0
   d8eec:	da24      	bge.n	d8f38 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8eee:	9700      	str	r7, [sp, #0]
   d8ef0:	4633      	mov	r3, r6
   d8ef2:	462a      	mov	r2, r5
   d8ef4:	4621      	mov	r1, r4
   d8ef6:	a803      	add	r0, sp, #12
   d8ef8:	f7fd fb15 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8efc:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8efe:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8f00:	4633      	mov	r3, r6
   d8f02:	462a      	mov	r2, r5
   d8f04:	4621      	mov	r1, r4
   d8f06:	4640      	mov	r0, r8
   d8f08:	f7fd fbbe 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8f0c:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8f0e:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8f10:	4633      	mov	r3, r6
   d8f12:	462a      	mov	r2, r5
   d8f14:	4621      	mov	r1, r4
   d8f16:	a810      	add	r0, sp, #64	; 0x40
   d8f18:	f7fd fbb6 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8f1c:	9a22      	ldr	r2, [sp, #136]	; 0x88
   d8f1e:	f859 302b 	ldr.w	r3, [r9, fp, lsl #2]
   d8f22:	f852 2020 	ldr.w	r2, [r2, r0, lsl #2]
   d8f26:	4293      	cmp	r3, r2
   d8f28:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d8f2a:	bfcc      	ite	gt
   d8f2c:	2300      	movgt	r3, #0
   d8f2e:	2301      	movle	r3, #1
   d8f30:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8f34:	3701      	adds	r7, #1
   d8f36:	e7d4      	b.n	d8ee2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8f38:	3601      	adds	r6, #1
   d8f3a:	e7cb      	b.n	d8ed4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8f3c:	3501      	adds	r5, #1
   d8f3e:	e7c2      	b.n	d8ec6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8f40:	3401      	adds	r4, #1
   d8f42:	e7b9      	b.n	d8eb8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8f44:	a803      	add	r0, sp, #12
   d8f46:	f7fd fa7e 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8f4a:	b019      	add	sp, #100	; 0x64
   d8f4c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8f50 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8f50:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8f54:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8f56:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8f58:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8f5a:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8f5c:	4692      	mov	sl, r2
   d8f5e:	460c      	mov	r4, r1
   d8f60:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8f62:	dd01      	ble.n	d8f68 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8f64:	f00b f9e2 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8f68:	682b      	ldr	r3, [r5, #0]
   d8f6a:	2b04      	cmp	r3, #4
   d8f6c:	dcfa      	bgt.n	d8f64 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8f6e:	6813      	ldr	r3, [r2, #0]
   d8f70:	2b04      	cmp	r3, #4
   d8f72:	dcf7      	bgt.n	d8f64 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8f74:	2301      	movs	r3, #1
   d8f76:	2104      	movs	r1, #4
   d8f78:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8f7a:	f10d 0820 	add.w	r8, sp, #32
   d8f7e:	f7fd faa6 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8f82:	4620      	mov	r0, r4
   d8f84:	ab10      	add	r3, sp, #64	; 0x40
   d8f86:	4642      	mov	r2, r8
   d8f88:	4629      	mov	r1, r5
   d8f8a:	f7fd fdb5 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8f8e:	2400      	movs	r4, #0
   d8f90:	2100      	movs	r1, #0
   d8f92:	a803      	add	r0, sp, #12
   d8f94:	f7fd fa62 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8f98:	4284      	cmp	r4, r0
   d8f9a:	da45      	bge.n	d9028 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd8>
   d8f9c:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8f9e:	2101      	movs	r1, #1
   d8fa0:	a803      	add	r0, sp, #12
   d8fa2:	f7fd fa5b 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8fa6:	4285      	cmp	r5, r0
   d8fa8:	da3c      	bge.n	d9024 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd4>
   d8faa:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8fac:	2102      	movs	r1, #2
   d8fae:	a803      	add	r0, sp, #12
   d8fb0:	f7fd fa54 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8fb4:	4286      	cmp	r6, r0
   d8fb6:	da33      	bge.n	d9020 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd0>
   d8fb8:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8fba:	2103      	movs	r1, #3
   d8fbc:	a803      	add	r0, sp, #12
   d8fbe:	f7fd fa4d 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d8fc2:	4287      	cmp	r7, r0
   d8fc4:	da2a      	bge.n	d901c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8fc6:	9700      	str	r7, [sp, #0]
   d8fc8:	4633      	mov	r3, r6
   d8fca:	462a      	mov	r2, r5
   d8fcc:	4621      	mov	r1, r4
   d8fce:	a803      	add	r0, sp, #12
   d8fd0:	f7fd faa9 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8fd4:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8fd6:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8fd8:	4633      	mov	r3, r6
   d8fda:	462a      	mov	r2, r5
   d8fdc:	4621      	mov	r1, r4
   d8fde:	4640      	mov	r0, r8
   d8fe0:	f7fd fb52 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8fe4:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8fe6:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8fe8:	4633      	mov	r3, r6
   d8fea:	462a      	mov	r2, r5
   d8fec:	4621      	mov	r1, r4
   d8fee:	a810      	add	r0, sp, #64	; 0x40
   d8ff0:	f7fd fb4a 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8ff4:	9b22      	ldr	r3, [sp, #136]	; 0x88
   d8ff6:	eb0a 09c9 	add.w	r9, sl, r9, lsl #3
   d8ffa:	eb03 00c0 	add.w	r0, r3, r0, lsl #3

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8ffe:	e9d0 0100 	ldrd	r0, r1, [r0]
   d9002:	e9d9 2300 	ldrd	r2, r3, [r9]
   d9006:	4290      	cmp	r0, r2
   d9008:	eb71 0303 	sbcs.w	r3, r1, r3
   d900c:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d900e:	bfac      	ite	ge
   d9010:	2301      	movge	r3, #1
   d9012:	2300      	movlt	r3, #0
   d9014:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d9018:	3701      	adds	r7, #1
   d901a:	e7ce      	b.n	d8fba <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d901c:	3601      	adds	r6, #1
   d901e:	e7c5      	b.n	d8fac <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d9020:	3501      	adds	r5, #1
   d9022:	e7bc      	b.n	d8f9e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d9024:	3401      	adds	r4, #1
   d9026:	e7b3      	b.n	d8f90 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d9028:	a803      	add	r0, sp, #12
   d902a:	f7fd fa0c 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d902e:	b019      	add	sp, #100	; 0x64
   d9030:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d9034 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d9034:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d9038:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d903a:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d903c:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d903e:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d9040:	9208      	str	r2, [sp, #32]
   d9042:	4604      	mov	r4, r0
   d9044:	460e      	mov	r6, r1
   d9046:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d9048:	dd01      	ble.n	d904e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   d904a:	f00b f96f 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d904e:	683b      	ldr	r3, [r7, #0]
   d9050:	2b04      	cmp	r3, #4
   d9052:	dcfa      	bgt.n	d904a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d9054:	6813      	ldr	r3, [r2, #0]
   d9056:	2b04      	cmp	r3, #4
   d9058:	dcf7      	bgt.n	d904a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   d905a:	2301      	movs	r3, #1
   d905c:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d905e:	ad10      	add	r5, sp, #64	; 0x40
   d9060:	a80b      	add	r0, sp, #44	; 0x2c
   d9062:	f7fd fa34 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d9066:	ab18      	add	r3, sp, #96	; 0x60
   d9068:	462a      	mov	r2, r5
   d906a:	4639      	mov	r1, r7
   d906c:	4630      	mov	r0, r6
   d906e:	f7fd fd43 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   d9072:	6863      	ldr	r3, [r4, #4]
   d9074:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   d9076:	68a3      	ldr	r3, [r4, #8]
   d9078:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   d907a:	68e3      	ldr	r3, [r4, #12]
   d907c:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   d907e:	6923      	ldr	r3, [r4, #16]
   d9080:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   d9082:	6963      	ldr	r3, [r4, #20]
   d9084:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   d9086:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   d9088:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   d908c:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d908e:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d9090:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d9092:	2100      	movs	r1, #0
   d9094:	a80b      	add	r0, sp, #44	; 0x2c
   d9096:	f7fd f9e1 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d909a:	4284      	cmp	r4, r0
   d909c:	da59      	bge.n	d9152 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   d909e:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d90a0:	af0b      	add	r7, sp, #44	; 0x2c
   d90a2:	2101      	movs	r1, #1
   d90a4:	4638      	mov	r0, r7
   d90a6:	f7fd f9d9 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d90aa:	4285      	cmp	r5, r0
   d90ac:	da4f      	bge.n	d914e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   d90ae:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d90b0:	2102      	movs	r1, #2
   d90b2:	4638      	mov	r0, r7
   d90b4:	f7fd f9d2 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d90b8:	4286      	cmp	r6, r0
   d90ba:	da46      	bge.n	d914a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   d90bc:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d90c0:	2103      	movs	r1, #3
   d90c2:	4638      	mov	r0, r7
   d90c4:	f7fd f9ca 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d90c8:	4580      	cmp	r8, r0
   d90ca:	da3c      	bge.n	d9146 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d90cc:	f8cd 8000 	str.w	r8, [sp]
   d90d0:	4633      	mov	r3, r6
   d90d2:	462a      	mov	r2, r5
   d90d4:	4621      	mov	r1, r4
   d90d6:	9809      	ldr	r0, [sp, #36]	; 0x24
   d90d8:	f7fd fad6 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   d90dc:	9b08      	ldr	r3, [sp, #32]
   d90de:	f813 9000 	ldrb.w	r9, [r3, r0]
   d90e2:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d90e4:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d90e8:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d90ea:	462a      	mov	r2, r5
   d90ec:	4633      	mov	r3, r6
   d90ee:	4621      	mov	r1, r4
   d90f0:	a818      	add	r0, sp, #96	; 0x60
   d90f2:	f7fd fac9 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d90f6:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d90f8:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d90fa:	f813 b000 	ldrb.w	fp, [r3, r0]
   d90fe:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d9100:	9903      	ldr	r1, [sp, #12]
   d9102:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9106:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d9108:	f7fd fa6c 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d910c:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d9110:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   d9112:	9a07      	ldr	r2, [sp, #28]
   d9114:	9906      	ldr	r1, [sp, #24]
   d9116:	4658      	mov	r0, fp
   d9118:	f7fd fa64 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   d911c:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   d9120:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   d9122:	4633      	mov	r3, r6
   d9124:	462a      	mov	r2, r5
   d9126:	4621      	mov	r1, r4
   d9128:	4638      	mov	r0, r7
   d912a:	f7fd f9fc 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   d912e:	ebcb 0309 	rsb	r3, fp, r9
   d9132:	f1d3 0900 	rsbs	r9, r3, #0
   d9136:	eb49 0903 	adc.w	r9, r9, r3
   d913a:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d913c:	f108 0801 	add.w	r8, r8, #1
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
          output_data[Offset(output_shape, b, y, x, c)] =
   d9140:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d9144:	e7bc      	b.n	d90c0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d9146:	3601      	adds	r6, #1
   d9148:	e7b2      	b.n	d90b0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d914a:	3501      	adds	r5, #1
   d914c:	e7a8      	b.n	d90a0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d914e:	3401      	adds	r4, #1
   d9150:	e79f      	b.n	d9092 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d9152:	a80b      	add	r0, sp, #44	; 0x2c
   d9154:	f7fd f977 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   d9158:	b021      	add	sp, #132	; 0x84
   d915a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d915e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d915e:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d9162:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d9164:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d9166:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d9168:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d916a:	9208      	str	r2, [sp, #32]
   d916c:	4604      	mov	r4, r0
   d916e:	460e      	mov	r6, r1
   d9170:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d9172:	dd01      	ble.n	d9178 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   d9174:	f00b f8da 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d9178:	683b      	ldr	r3, [r7, #0]
   d917a:	2b04      	cmp	r3, #4
   d917c:	dcfa      	bgt.n	d9174 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d917e:	6813      	ldr	r3, [r2, #0]
   d9180:	2b04      	cmp	r3, #4
   d9182:	dcf7      	bgt.n	d9174 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   d9184:	2301      	movs	r3, #1
   d9186:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d9188:	ad10      	add	r5, sp, #64	; 0x40
   d918a:	a80b      	add	r0, sp, #44	; 0x2c
   d918c:	f7fd f99f 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d9190:	ab18      	add	r3, sp, #96	; 0x60
   d9192:	462a      	mov	r2, r5
   d9194:	4639      	mov	r1, r7
   d9196:	4630      	mov	r0, r6
   d9198:	f7fd fcae 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   d919c:	6863      	ldr	r3, [r4, #4]
   d919e:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   d91a0:	68a3      	ldr	r3, [r4, #8]
   d91a2:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   d91a4:	68e3      	ldr	r3, [r4, #12]
   d91a6:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   d91a8:	6923      	ldr	r3, [r4, #16]
   d91aa:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   d91ac:	6963      	ldr	r3, [r4, #20]
   d91ae:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   d91b0:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   d91b2:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   d91b6:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d91b8:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d91ba:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d91bc:	2100      	movs	r1, #0
   d91be:	a80b      	add	r0, sp, #44	; 0x2c
   d91c0:	f7fd f94c 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d91c4:	4284      	cmp	r4, r0
   d91c6:	da59      	bge.n	d927c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   d91c8:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d91ca:	af0b      	add	r7, sp, #44	; 0x2c
   d91cc:	2101      	movs	r1, #1
   d91ce:	4638      	mov	r0, r7
   d91d0:	f7fd f944 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d91d4:	4285      	cmp	r5, r0
   d91d6:	da4f      	bge.n	d9278 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   d91d8:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d91da:	2102      	movs	r1, #2
   d91dc:	4638      	mov	r0, r7
   d91de:	f7fd f93d 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d91e2:	4286      	cmp	r6, r0
   d91e4:	da46      	bge.n	d9274 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   d91e6:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d91ea:	2103      	movs	r1, #3
   d91ec:	4638      	mov	r0, r7
   d91ee:	f7fd f935 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d91f2:	4580      	cmp	r8, r0
   d91f4:	da3c      	bge.n	d9270 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d91f6:	f8cd 8000 	str.w	r8, [sp]
   d91fa:	4633      	mov	r3, r6
   d91fc:	462a      	mov	r2, r5
   d91fe:	4621      	mov	r1, r4
   d9200:	9809      	ldr	r0, [sp, #36]	; 0x24
   d9202:	f7fd fa41 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   d9206:	9b08      	ldr	r3, [sp, #32]
   d9208:	f913 9000 	ldrsb.w	r9, [r3, r0]
   d920c:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d920e:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d9212:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d9214:	462a      	mov	r2, r5
   d9216:	4633      	mov	r3, r6
   d9218:	4621      	mov	r1, r4
   d921a:	a818      	add	r0, sp, #96	; 0x60
   d921c:	f7fd fa34 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9220:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d9222:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9224:	f913 b000 	ldrsb.w	fp, [r3, r0]
   d9228:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d922a:	9903      	ldr	r1, [sp, #12]
   d922c:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9230:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d9232:	f7fd f9d7 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9236:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d923a:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   d923c:	9a07      	ldr	r2, [sp, #28]
   d923e:	9906      	ldr	r1, [sp, #24]
   d9240:	4658      	mov	r0, fp
   d9242:	f7fd f9cf 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   d9246:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   d924a:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   d924c:	4633      	mov	r3, r6
   d924e:	462a      	mov	r2, r5
   d9250:	4621      	mov	r1, r4
   d9252:	4638      	mov	r0, r7
   d9254:	f7fd f967 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   d9258:	ebcb 0309 	rsb	r3, fp, r9
   d925c:	f1d3 0900 	rsbs	r9, r3, #0
   d9260:	eb49 0903 	adc.w	r9, r9, r3
   d9264:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d9266:	f108 0801 	add.w	r8, r8, #1
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
          output_data[Offset(output_shape, b, y, x, c)] =
   d926a:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d926e:	e7bc      	b.n	d91ea <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d9270:	3601      	adds	r6, #1
   d9272:	e7b2      	b.n	d91da <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d9274:	3501      	adds	r5, #1
   d9276:	e7a8      	b.n	d91ca <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d9278:	3401      	adds	r4, #1
   d927a:	e79f      	b.n	d91bc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d927c:	a80b      	add	r0, sp, #44	; 0x2c
   d927e:	f7fd f8e2 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   d9282:	b021      	add	sp, #132	; 0x84
   d9284:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d9288 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode>:
              op_params, GetTensorShape(input1), GetTensorData<type>(input1), \
              GetTensorShape(input2), GetTensorData<type>(input2),            \
              GetTensorShape(output), GetTensorData<bool>(output));           \
  }

TfLiteStatus EqualEval(TfLiteContext* context, TfLiteNode* node) {
   d9288:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d928c:	680a      	ldr	r2, [r1, #0]
   d928e:	f8d0 9008 	ldr.w	r9, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d9292:	6895      	ldr	r5, [r2, #8]
   d9294:	4682      	mov	sl, r0
   d9296:	6850      	ldr	r0, [r2, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d9298:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d929a:	2338      	movs	r3, #56	; 0x38
   d929c:	fb03 f800 	mul.w	r8, r3, r0
   d92a0:	fb03 9505 	mla	r5, r3, r5, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d92a4:	6854      	ldr	r4, [r2, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d92a6:	eb09 0608 	add.w	r6, r9, r8
   d92aa:	b0ab      	sub	sp, #172	; 0xac
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   d92ac:	4629      	mov	r1, r5
   d92ae:	4630      	mov	r0, r6
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d92b0:	fb03 9404 	mla	r4, r3, r4, r9
   d92b4:	f00a fd5c 	bl	e3d70 <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
  switch (input1->type) {
   d92b8:	f819 2008 	ldrb.w	r2, [r9, r8]

TfLiteStatus EqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   d92bc:	f080 0001 	eor.w	r0, r0, #1
  switch (input1->type) {
   d92c0:	1e53      	subs	r3, r2, #1

TfLiteStatus EqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   d92c2:	b2c7      	uxtb	r7, r0
  switch (input1->type) {
   d92c4:	2b08      	cmp	r3, #8
   d92c6:	f200 8274 	bhi.w	d97b2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x52a>
   d92ca:	e8df f013 	tbh	[pc, r3, lsl #1]
   d92ce:	0057      	.short	0x0057
   d92d0:	013f00a5 	.word	0x013f00a5
   d92d4:	027200f1 	.word	0x027200f1
   d92d8:	02720009 	.word	0x02720009
   d92dc:	01d30272 	.word	0x01d30272
   d92e0:	f10d 0874 	add.w	r8, sp, #116	; 0x74
    case kTfLiteBool:
      TF_LITE_COMPARISON(bool, Equal, requires_broadcast);
   d92e4:	4631      	mov	r1, r6
   d92e6:	b1cf      	cbz	r7, d931c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x94>
   d92e8:	a813      	add	r0, sp, #76	; 0x4c
   d92ea:	f7fd fb5c 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d92ee:	4629      	mov	r1, r5
   d92f0:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d92f2:	6876      	ldr	r6, [r6, #4]
   d92f4:	f7fd fb57 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d92f8:	b105      	cbz	r5, d92fc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x74>
   d92fa:	686d      	ldr	r5, [r5, #4]
   d92fc:	4621      	mov	r1, r4
   d92fe:	4640      	mov	r0, r8
   d9300:	f7fd fb51 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9304:	b104      	cbz	r4, d9308 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x80>
   d9306:	6864      	ldr	r4, [r4, #4]
      bool* output_data) {                                                     \
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
   d9308:	9402      	str	r4, [sp, #8]
   d930a:	e88d 0120 	stmia.w	sp, {r5, r8}
   d930e:	ab18      	add	r3, sp, #96	; 0x60
   d9310:	4632      	mov	r2, r6
   d9312:	a913      	add	r1, sp, #76	; 0x4c
   d9314:	a822      	add	r0, sp, #136	; 0x88
   d9316:	f7fe fdd7 	bl	d7ec8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d931a:	e1ed      	b.n	d96f8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x470>
   d931c:	a818      	add	r0, sp, #96	; 0x60
   d931e:	f7fd fb42 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9322:	4629      	mov	r1, r5
   d9324:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9326:	6876      	ldr	r6, [r6, #4]
   d9328:	f7fd fb3d 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d932c:	b105      	cbz	r5, d9330 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0xa8>
   d932e:	686d      	ldr	r5, [r5, #4]
   d9330:	4621      	mov	r1, r4
   d9332:	a822      	add	r0, sp, #136	; 0x88
   d9334:	f7fd fb37 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9338:	b104      	cbz	r4, d933c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0xb4>
   d933a:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d933c:	4641      	mov	r1, r8
   d933e:	aa22      	add	r2, sp, #136	; 0x88
   d9340:	a818      	add	r0, sp, #96	; 0x60
   d9342:	f7fd f912 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d9346:	3d01      	subs	r5, #1
   d9348:	1e73      	subs	r3, r6, #1
   d934a:	17c1      	asrs	r1, r0, #31
   d934c:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
   d934e:	2600      	movs	r6, #0
   d9350:	2700      	movs	r7, #0
   d9352:	4286      	cmp	r6, r0
   d9354:	eb77 0201 	sbcs.w	r2, r7, r1
   d9358:	f280 8232 	bge.w	d97c0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x538>
    output_data[i] = F(input1_data[i], input2_data[i]);
   d935c:	f813 2f01 	ldrb.w	r2, [r3, #1]!
   d9360:	f815 ef01 	ldrb.w	lr, [r5, #1]!
   d9364:	ebce 0c02 	rsb	ip, lr, r2
   d9368:	f1dc 0200 	rsbs	r2, ip, #0
   d936c:	eb42 020c 	adc.w	r2, r2, ip
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9370:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9372:	f804 2f01 	strb.w	r2, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9376:	f147 0700 	adc.w	r7, r7, #0
   d937a:	e7ea      	b.n	d9352 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0xca>
   d937c:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteFloat32:
      TF_LITE_COMPARISON(float, Equal, requires_broadcast);
   d9380:	4631      	mov	r1, r6
   d9382:	b1cf      	cbz	r7, d93b8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x130>
   d9384:	a813      	add	r0, sp, #76	; 0x4c
   d9386:	f7fd fb0e 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d938a:	4629      	mov	r1, r5
   d938c:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d938e:	6876      	ldr	r6, [r6, #4]
   d9390:	f7fd fb09 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9394:	b105      	cbz	r5, d9398 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x110>
   d9396:	686d      	ldr	r5, [r5, #4]
   d9398:	4621      	mov	r1, r4
   d939a:	4640      	mov	r0, r8
   d939c:	f7fd fb03 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d93a0:	b104      	cbz	r4, d93a4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x11c>
   d93a2:	6864      	ldr	r4, [r4, #4]
      bool* output_data) {                                                     \
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
   d93a4:	9402      	str	r4, [sp, #8]
   d93a6:	e88d 0120 	stmia.w	sp, {r5, r8}
   d93aa:	ab18      	add	r3, sp, #96	; 0x60
   d93ac:	4632      	mov	r2, r6
   d93ae:	a913      	add	r1, sp, #76	; 0x4c
   d93b0:	a822      	add	r0, sp, #136	; 0x88
   d93b2:	f7fe fdf3 	bl	d7f9c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d93b6:	e19f      	b.n	d96f8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x470>
   d93b8:	a818      	add	r0, sp, #96	; 0x60
   d93ba:	f7fd faf4 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d93be:	4629      	mov	r1, r5
   d93c0:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d93c2:	6876      	ldr	r6, [r6, #4]
   d93c4:	f7fd faef 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d93c8:	b105      	cbz	r5, d93cc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x144>
   d93ca:	686d      	ldr	r5, [r5, #4]
   d93cc:	4621      	mov	r1, r4
   d93ce:	a822      	add	r0, sp, #136	; 0x88
   d93d0:	f7fd fae9 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d93d4:	b104      	cbz	r4, d93d8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x150>
   d93d6:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d93d8:	4641      	mov	r1, r8
   d93da:	aa22      	add	r2, sp, #136	; 0x88
   d93dc:	a818      	add	r0, sp, #96	; 0x60
   d93de:	f7fd f8c4 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d93e2:	3c01      	subs	r4, #1
   d93e4:	4633      	mov	r3, r6
   d93e6:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   d93e8:	2600      	movs	r6, #0
   d93ea:	2700      	movs	r7, #0
   d93ec:	4286      	cmp	r6, r0
   d93ee:	eb77 0201 	sbcs.w	r2, r7, r1
   d93f2:	f280 81e5 	bge.w	d97c0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x538>
    output_data[i] = F(input1_data[i], input2_data[i]);
   d93f6:	ecb3 7a01 	vldmia	r3!, {s14}
   d93fa:	ecf5 7a01 	vldmia	r5!, {s15}
   d93fe:	eeb4 7a67 	vcmp.f32	s14, s15
   d9402:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d9406:	bf0c      	ite	eq
   d9408:	2201      	moveq	r2, #1
   d940a:	2200      	movne	r2, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d940c:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   d940e:	f804 2f01 	strb.w	r2, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9412:	f147 0700 	adc.w	r7, r7, #0
   d9416:	e7e9      	b.n	d93ec <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x164>
   d9418:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, Equal, requires_broadcast);
   d941c:	4631      	mov	r1, r6
   d941e:	b1cf      	cbz	r7, d9454 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x1cc>
   d9420:	a813      	add	r0, sp, #76	; 0x4c
   d9422:	f7fd fac0 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9426:	4629      	mov	r1, r5
   d9428:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d942a:	6876      	ldr	r6, [r6, #4]
   d942c:	f7fd fabb 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9430:	b105      	cbz	r5, d9434 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x1ac>
   d9432:	686d      	ldr	r5, [r5, #4]
   d9434:	4621      	mov	r1, r4
   d9436:	4640      	mov	r0, r8
   d9438:	f7fd fab5 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d943c:	b104      	cbz	r4, d9440 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x1b8>
   d943e:	6864      	ldr	r4, [r4, #4]
      bool* output_data) {                                                     \
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
   d9440:	9402      	str	r4, [sp, #8]
   d9442:	e88d 0120 	stmia.w	sp, {r5, r8}
   d9446:	ab18      	add	r3, sp, #96	; 0x60
   d9448:	4632      	mov	r2, r6
   d944a:	a913      	add	r1, sp, #76	; 0x4c
   d944c:	a822      	add	r0, sp, #136	; 0x88
   d944e:	f7fe fe18 	bl	d8082 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d9452:	e151      	b.n	d96f8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x470>
   d9454:	a818      	add	r0, sp, #96	; 0x60
   d9456:	f7fd faa6 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d945a:	4629      	mov	r1, r5
   d945c:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d945e:	6876      	ldr	r6, [r6, #4]
   d9460:	f7fd faa1 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9464:	b105      	cbz	r5, d9468 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x1e0>
   d9466:	686d      	ldr	r5, [r5, #4]
   d9468:	4621      	mov	r1, r4
   d946a:	a822      	add	r0, sp, #136	; 0x88
   d946c:	f7fd fa9b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9470:	b104      	cbz	r4, d9474 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x1ec>
   d9472:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9474:	aa22      	add	r2, sp, #136	; 0x88
   d9476:	4641      	mov	r1, r8
   d9478:	a818      	add	r0, sp, #96	; 0x60
   d947a:	f7fd f876 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d947e:	3c01      	subs	r4, #1
   d9480:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   d9482:	2200      	movs	r2, #0
   d9484:	2300      	movs	r3, #0
   d9486:	4282      	cmp	r2, r0
   d9488:	eb73 0701 	sbcs.w	r7, r3, r1
   d948c:	f280 8198 	bge.w	d97c0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x538>
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9490:	f856 7022 	ldr.w	r7, [r6, r2, lsl #2]
   d9494:	f855 e022 	ldr.w	lr, [r5, r2, lsl #2]
   d9498:	ebce 0e07 	rsb	lr, lr, r7
   d949c:	f1de 0700 	rsbs	r7, lr, #0
   d94a0:	eb47 070e 	adc.w	r7, r7, lr
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d94a4:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   d94a6:	f804 7f01 	strb.w	r7, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d94aa:	f143 0300 	adc.w	r3, r3, #0
   d94ae:	e7ea      	b.n	d9486 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x1fe>
   d94b0:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, Equal, requires_broadcast);
   d94b4:	4631      	mov	r1, r6
   d94b6:	b1cf      	cbz	r7, d94ec <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x264>
   d94b8:	a813      	add	r0, sp, #76	; 0x4c
   d94ba:	f7fd fa74 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d94be:	4629      	mov	r1, r5
   d94c0:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d94c2:	6876      	ldr	r6, [r6, #4]
   d94c4:	f7fd fa6f 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d94c8:	b105      	cbz	r5, d94cc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x244>
   d94ca:	686d      	ldr	r5, [r5, #4]
   d94cc:	4621      	mov	r1, r4
   d94ce:	4640      	mov	r0, r8
   d94d0:	f7fd fa69 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d94d4:	b104      	cbz	r4, d94d8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x250>
   d94d6:	6864      	ldr	r4, [r4, #4]
      bool* output_data) {                                                     \
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
   d94d8:	9402      	str	r4, [sp, #8]
   d94da:	e88d 0120 	stmia.w	sp, {r5, r8}
   d94de:	ab18      	add	r3, sp, #96	; 0x60
   d94e0:	4632      	mov	r2, r6
   d94e2:	a913      	add	r1, sp, #76	; 0x4c
   d94e4:	a822      	add	r0, sp, #136	; 0x88
   d94e6:	f7fe fe37 	bl	d8158 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d94ea:	e105      	b.n	d96f8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x470>
   d94ec:	a818      	add	r0, sp, #96	; 0x60
   d94ee:	f7fd fa5a 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d94f2:	4629      	mov	r1, r5
   d94f4:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d94f6:	6876      	ldr	r6, [r6, #4]
   d94f8:	f7fd fa55 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d94fc:	b105      	cbz	r5, d9500 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x278>
   d94fe:	686d      	ldr	r5, [r5, #4]
   d9500:	4621      	mov	r1, r4
   d9502:	a822      	add	r0, sp, #136	; 0x88
   d9504:	f7fd fa4f 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9508:	b104      	cbz	r4, d950c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x284>
   d950a:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d950c:	aa22      	add	r2, sp, #136	; 0x88
   d950e:	4641      	mov	r1, r8
   d9510:	a818      	add	r0, sp, #96	; 0x60
   d9512:	f7fd f82a 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d9516:	3d08      	subs	r5, #8
   d9518:	17c1      	asrs	r1, r0, #31
   d951a:	f1a6 0e08 	sub.w	lr, r6, #8
   d951e:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
   d9520:	2200      	movs	r2, #0
   d9522:	2300      	movs	r3, #0
   d9524:	4282      	cmp	r2, r0
   d9526:	eb73 0601 	sbcs.w	r6, r3, r1
   d952a:	f280 8149 	bge.w	d97c0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x538>
    output_data[i] = F(input1_data[i], input2_data[i]);
   d952e:	e9f5 6702 	ldrd	r6, r7, [r5, #8]!
   d9532:	e9fe ab02 	ldrd	sl, fp, [lr, #8]!
   d9536:	45bb      	cmp	fp, r7
   d9538:	bf06      	itte	eq
   d953a:	45b2      	cmpeq	sl, r6
   d953c:	2601      	moveq	r6, #1
   d953e:	2600      	movne	r6, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9540:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9542:	f804 6f01 	strb.w	r6, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9546:	f143 0300 	adc.w	r3, r3, #0
   d954a:	e7eb      	b.n	d9524 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x29c>
            GetTensorData<input_dtype>(input2), GetTensorShape(output),        \
            GetTensorData<bool>(output));                                      \
      }                                                                        \
    }                                                                          \
  }
TF_LITE_QUANTIZE_COMPARISON(Equal);
   d954c:	6933      	ldr	r3, [r6, #16]
   d954e:	68f0      	ldr	r0, [r6, #12]
   d9550:	f1c3 0900 	rsb	r9, r3, #0
   d9554:	692b      	ldr	r3, [r5, #16]
   d9556:	f1c3 0800 	rsb	r8, r3, #0
   d955a:	f00d fd4f 	bl	e6ffc <__aeabi_f2d>
   d955e:	ec41 0b10 	vmov	d0, r0, r1
   d9562:	a910      	add	r1, sp, #64	; 0x40
   d9564:	a80f      	add	r0, sp, #60	; 0x3c
   d9566:	f00a fc77 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   d956a:	68e8      	ldr	r0, [r5, #12]
   d956c:	f00d fd46 	bl	e6ffc <__aeabi_f2d>
   d9570:	ec41 0b10 	vmov	d0, r0, r1
   d9574:	a912      	add	r1, sp, #72	; 0x48
   d9576:	a811      	add	r0, sp, #68	; 0x44
   d9578:	f00a fc6e 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   d957c:	2308      	movs	r3, #8
   d957e:	9322      	str	r3, [sp, #136]	; 0x88
   d9580:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   d9582:	9324      	str	r3, [sp, #144]	; 0x90
   d9584:	9b10      	ldr	r3, [sp, #64]	; 0x40
   d9586:	9325      	str	r3, [sp, #148]	; 0x94
   d9588:	9b11      	ldr	r3, [sp, #68]	; 0x44
   d958a:	9327      	str	r3, [sp, #156]	; 0x9c
   d958c:	9b12      	ldr	r3, [sp, #72]	; 0x48
   d958e:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   d9592:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   d9596:	9328      	str	r3, [sp, #160]	; 0xa0
   d9598:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   d959c:	4631      	mov	r1, r6
   d959e:	a813      	add	r0, sp, #76	; 0x4c
   d95a0:	b1bf      	cbz	r7, d95d2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x34a>
   d95a2:	f7fd fa00 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d95a6:	4629      	mov	r1, r5
   d95a8:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d95aa:	6876      	ldr	r6, [r6, #4]
   d95ac:	f7fd f9fb 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d95b0:	4621      	mov	r1, r4
   d95b2:	4640      	mov	r0, r8
   d95b4:	686d      	ldr	r5, [r5, #4]
   d95b6:	f7fd f9f6 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d95ba:	b104      	cbz	r4, d95be <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x336>
   d95bc:	6864      	ldr	r4, [r4, #4]
      bool* output_data) {                                                     \
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
   d95be:	9402      	str	r4, [sp, #8]
   d95c0:	e88d 0120 	stmia.w	sp, {r5, r8}
   d95c4:	ab18      	add	r3, sp, #96	; 0x60
   d95c6:	4632      	mov	r2, r6
   d95c8:	a913      	add	r1, sp, #76	; 0x4c
   d95ca:	a822      	add	r0, sp, #136	; 0x88
   d95cc:	f7ff fd32 	bl	d9034 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d95d0:	e092      	b.n	d96f8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x470>
   d95d2:	f7fd f9e8 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d95d6:	6873      	ldr	r3, [r6, #4]
   d95d8:	9305      	str	r3, [sp, #20]
   d95da:	4629      	mov	r1, r5
   d95dc:	a818      	add	r0, sp, #96	; 0x60
   d95de:	f7fd f9e2 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d95e2:	4621      	mov	r1, r4
   d95e4:	4640      	mov	r0, r8
   d95e6:	f8d5 b004 	ldr.w	fp, [r5, #4]
   d95ea:	f7fd f9dc 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d95ee:	b104      	cbz	r4, d95f2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x36a>
   d95f0:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   d95f2:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   d95f4:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   d95f6:	9b24      	ldr	r3, [sp, #144]	; 0x90
   d95f8:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   d95fa:	9b25      	ldr	r3, [sp, #148]	; 0x94
   d95fc:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   d95fe:	9b26      	ldr	r3, [sp, #152]	; 0x98
   d9600:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   d9602:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   d9604:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9606:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   d9608:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   d960a:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d960c:	a918      	add	r1, sp, #96	; 0x60
   d960e:	a813      	add	r0, sp, #76	; 0x4c
   d9610:	f7fc ffab 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d9614:	4602      	mov	r2, r0
   d9616:	17c3      	asrs	r3, r0, #31
   d9618:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   d961c:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d961e:	f04f 0800 	mov.w	r8, #0
   d9622:	f04f 0900 	mov.w	r9, #0
   d9626:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   d962a:	4590      	cmp	r8, r2
   d962c:	eb79 0303 	sbcs.w	r3, r9, r3
   d9630:	f280 80b4 	bge.w	d979c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x514>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9634:	f81b 5008 	ldrb.w	r5, [fp, r8]
   d9638:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d963a:	9a08      	ldr	r2, [sp, #32]
   d963c:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d963e:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d9640:	9b05      	ldr	r3, [sp, #20]
   d9642:	f813 0008 	ldrb.w	r0, [r3, r8]
   d9646:	9b06      	ldr	r3, [sp, #24]
   d9648:	4418      	add	r0, r3
   d964a:	40b8      	lsls	r0, r7
   d964c:	f7fc ffca 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9650:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d9652:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   d9654:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   d9656:	990a      	ldr	r1, [sp, #40]	; 0x28
   d9658:	4628      	mov	r0, r5
   d965a:	f7fc ffc3 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   d965e:	ebc0 020a 	rsb	r2, r0, sl
   d9662:	4250      	negs	r0, r2
   d9664:	4150      	adcs	r0, r2
   d9666:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d966a:	f118 0801 	adds.w	r8, r8, #1
   d966e:	f149 0900 	adc.w	r9, r9, #0
   d9672:	e7d8      	b.n	d9626 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x39e>
   d9674:	6933      	ldr	r3, [r6, #16]
   d9676:	68f0      	ldr	r0, [r6, #12]
   d9678:	f1c3 0900 	rsb	r9, r3, #0
   d967c:	692b      	ldr	r3, [r5, #16]
   d967e:	f1c3 0800 	rsb	r8, r3, #0
   d9682:	f00d fcbb 	bl	e6ffc <__aeabi_f2d>
   d9686:	ec41 0b10 	vmov	d0, r0, r1
   d968a:	a910      	add	r1, sp, #64	; 0x40
   d968c:	a80f      	add	r0, sp, #60	; 0x3c
   d968e:	f00a fbe3 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   d9692:	68e8      	ldr	r0, [r5, #12]
   d9694:	f00d fcb2 	bl	e6ffc <__aeabi_f2d>
   d9698:	ec41 0b10 	vmov	d0, r0, r1
   d969c:	a912      	add	r1, sp, #72	; 0x48
   d969e:	a811      	add	r0, sp, #68	; 0x44
   d96a0:	f00a fbda 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   d96a4:	2308      	movs	r3, #8
   d96a6:	9322      	str	r3, [sp, #136]	; 0x88
   d96a8:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   d96aa:	9324      	str	r3, [sp, #144]	; 0x90
   d96ac:	9b10      	ldr	r3, [sp, #64]	; 0x40
   d96ae:	9325      	str	r3, [sp, #148]	; 0x94
   d96b0:	9b11      	ldr	r3, [sp, #68]	; 0x44
   d96b2:	9327      	str	r3, [sp, #156]	; 0x9c
   d96b4:	9b12      	ldr	r3, [sp, #72]	; 0x48
   d96b6:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   d96ba:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   d96be:	9328      	str	r3, [sp, #160]	; 0xa0
   d96c0:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   d96c4:	4631      	mov	r1, r6
   d96c6:	a813      	add	r0, sp, #76	; 0x4c
   d96c8:	b1c7      	cbz	r7, d96fc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x474>
   d96ca:	f7fd f96c 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d96ce:	4629      	mov	r1, r5
   d96d0:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d96d2:	6876      	ldr	r6, [r6, #4]
   d96d4:	f7fd f967 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d96d8:	4621      	mov	r1, r4
   d96da:	4640      	mov	r0, r8
   d96dc:	686d      	ldr	r5, [r5, #4]
   d96de:	f7fd f962 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d96e2:	b104      	cbz	r4, d96e6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x45e>
   d96e4:	6864      	ldr	r4, [r4, #4]
      bool* output_data) {                                                     \
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
   d96e6:	9402      	str	r4, [sp, #8]
   d96e8:	e88d 0120 	stmia.w	sp, {r5, r8}
   d96ec:	ab18      	add	r3, sp, #96	; 0x60
   d96ee:	4632      	mov	r2, r6
   d96f0:	a913      	add	r1, sp, #76	; 0x4c
   d96f2:	a822      	add	r0, sp, #136	; 0x88
   d96f4:	f7ff fd33 	bl	d915e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d96f8:	4640      	mov	r0, r8
   d96fa:	e050      	b.n	d979e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x516>
   d96fc:	f7fd f953 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9700:	6873      	ldr	r3, [r6, #4]
   d9702:	9305      	str	r3, [sp, #20]
   d9704:	4629      	mov	r1, r5
   d9706:	a818      	add	r0, sp, #96	; 0x60
   d9708:	f7fd f94d 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d970c:	4621      	mov	r1, r4
   d970e:	4640      	mov	r0, r8
   d9710:	f8d5 b004 	ldr.w	fp, [r5, #4]
   d9714:	f7fd f947 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9718:	b104      	cbz	r4, d971c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x494>
   d971a:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   d971c:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   d971e:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   d9720:	9b24      	ldr	r3, [sp, #144]	; 0x90
   d9722:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   d9724:	9b25      	ldr	r3, [sp, #148]	; 0x94
   d9726:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   d9728:	9b26      	ldr	r3, [sp, #152]	; 0x98
   d972a:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   d972c:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   d972e:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9730:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   d9732:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   d9734:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9736:	a918      	add	r1, sp, #96	; 0x60
   d9738:	a813      	add	r0, sp, #76	; 0x4c
   d973a:	f7fc ff16 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d973e:	4602      	mov	r2, r0
   d9740:	17c3      	asrs	r3, r0, #31
   d9742:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   d9746:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9748:	f04f 0800 	mov.w	r8, #0
   d974c:	f04f 0900 	mov.w	r9, #0
   d9750:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   d9754:	4590      	cmp	r8, r2
   d9756:	eb79 0303 	sbcs.w	r3, r9, r3
   d975a:	da1f      	bge.n	d979c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x514>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d975c:	f91b 5008 	ldrsb.w	r5, [fp, r8]
   d9760:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d9762:	9a08      	ldr	r2, [sp, #32]
   d9764:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9766:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d9768:	9b05      	ldr	r3, [sp, #20]
   d976a:	f913 0008 	ldrsb.w	r0, [r3, r8]
   d976e:	9b06      	ldr	r3, [sp, #24]
   d9770:	4418      	add	r0, r3
   d9772:	40b8      	lsls	r0, r7
   d9774:	f7fc ff36 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9778:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d977a:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   d977c:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   d977e:	990a      	ldr	r1, [sp, #40]	; 0x28
   d9780:	4628      	mov	r0, r5
   d9782:	f7fc ff2f 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   d9786:	ebc0 030a 	rsb	r3, r0, sl
   d978a:	4258      	negs	r0, r3
   d978c:	4158      	adcs	r0, r3
   d978e:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9792:	f118 0801 	adds.w	r8, r8, #1
   d9796:	f149 0900 	adc.w	r9, r9, #0
   d979a:	e7d9      	b.n	d9750 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x4c8>
   d979c:	a81d      	add	r0, sp, #116	; 0x74
   d979e:	f7fc fe52 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   d97a2:	a818      	add	r0, sp, #96	; 0x60
   d97a4:	f7fc fe4f 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   d97a8:	a813      	add	r0, sp, #76	; 0x4c
   d97aa:	f7fc fe4c 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(
          context, "Does not support type %d, requires bool|float|int|uint8",
          input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
   d97ae:	2000      	movs	r0, #0
   d97b0:	e00e      	b.n	d97d0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x548>
                                 requires_broadcast);
      break;
    default:
      context->ReportError(
          context, "Does not support type %d, requires bool|float|int|uint8",
          input1->type);
   d97b2:	4650      	mov	r0, sl
   d97b4:	f8da 3014 	ldr.w	r3, [sl, #20]
   d97b8:	4907      	ldr	r1, [pc, #28]	; (d97d8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x550>)
   d97ba:	4798      	blx	r3
      return kTfLiteError;
   d97bc:	2001      	movs	r0, #1
   d97be:	e007      	b.n	d97d0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x548>
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, Equal, requires_broadcast);
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, Equal, requires_broadcast);
   d97c0:	a822      	add	r0, sp, #136	; 0x88
   d97c2:	f7fc fe40 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   d97c6:	4640      	mov	r0, r8
   d97c8:	f7fc fe3d 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   d97cc:	a818      	add	r0, sp, #96	; 0x60
   d97ce:	e7ec      	b.n	d97aa <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x522>
          context, "Does not support type %d, requires bool|float|int|uint8",
          input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   d97d0:	b02b      	add	sp, #172	; 0xac
   d97d2:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   d97d6:	bf00      	nop
   d97d8:	000e99b7 	.word	0x000e99b7

000d97dc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d97dc:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d97e0:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d97e2:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d97e4:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d97e6:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d97e8:	9208      	str	r2, [sp, #32]
   d97ea:	4604      	mov	r4, r0
   d97ec:	460e      	mov	r6, r1
   d97ee:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d97f0:	dd01      	ble.n	d97f6 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   d97f2:	f00a fd9b 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d97f6:	683b      	ldr	r3, [r7, #0]
   d97f8:	2b04      	cmp	r3, #4
   d97fa:	dcfa      	bgt.n	d97f2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d97fc:	6813      	ldr	r3, [r2, #0]
   d97fe:	2b04      	cmp	r3, #4
   d9800:	dcf7      	bgt.n	d97f2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   d9802:	2301      	movs	r3, #1
   d9804:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d9806:	ad10      	add	r5, sp, #64	; 0x40
   d9808:	a80b      	add	r0, sp, #44	; 0x2c
   d980a:	f7fc fe60 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d980e:	ab18      	add	r3, sp, #96	; 0x60
   d9810:	462a      	mov	r2, r5
   d9812:	4639      	mov	r1, r7
   d9814:	4630      	mov	r0, r6
   d9816:	f7fd f96f 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   d981a:	6863      	ldr	r3, [r4, #4]
   d981c:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   d981e:	68a3      	ldr	r3, [r4, #8]
   d9820:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   d9822:	68e3      	ldr	r3, [r4, #12]
   d9824:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   d9826:	6923      	ldr	r3, [r4, #16]
   d9828:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   d982a:	6963      	ldr	r3, [r4, #20]
   d982c:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   d982e:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   d9830:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   d9834:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d9836:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d9838:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d983a:	2100      	movs	r1, #0
   d983c:	a80b      	add	r0, sp, #44	; 0x2c
   d983e:	f7fc fe0d 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d9842:	4284      	cmp	r4, r0
   d9844:	da58      	bge.n	d98f8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11c>
   d9846:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d9848:	af0b      	add	r7, sp, #44	; 0x2c
   d984a:	2101      	movs	r1, #1
   d984c:	4638      	mov	r0, r7
   d984e:	f7fc fe05 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d9852:	4285      	cmp	r5, r0
   d9854:	da4e      	bge.n	d98f4 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x118>
   d9856:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d9858:	2102      	movs	r1, #2
   d985a:	4638      	mov	r0, r7
   d985c:	f7fc fdfe 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d9860:	4286      	cmp	r6, r0
   d9862:	da45      	bge.n	d98f0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x114>
   d9864:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d9868:	2103      	movs	r1, #3
   d986a:	4638      	mov	r0, r7
   d986c:	f7fc fdf6 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d9870:	4580      	cmp	r8, r0
   d9872:	da3b      	bge.n	d98ec <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x110>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d9874:	f8cd 8000 	str.w	r8, [sp]
   d9878:	4633      	mov	r3, r6
   d987a:	462a      	mov	r2, r5
   d987c:	4621      	mov	r1, r4
   d987e:	9809      	ldr	r0, [sp, #36]	; 0x24
   d9880:	f7fc ff02 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   d9884:	9b08      	ldr	r3, [sp, #32]
   d9886:	f813 9000 	ldrb.w	r9, [r3, r0]
   d988a:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d988c:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d9890:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d9892:	462a      	mov	r2, r5
   d9894:	4633      	mov	r3, r6
   d9896:	4621      	mov	r1, r4
   d9898:	a818      	add	r0, sp, #96	; 0x60
   d989a:	f7fc fef5 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d989e:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d98a0:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d98a2:	f813 b000 	ldrb.w	fp, [r3, r0]
   d98a6:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d98a8:	9903      	ldr	r1, [sp, #12]
   d98aa:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d98ae:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d98b0:	f7fc fe98 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d98b4:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d98b8:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   d98ba:	9a07      	ldr	r2, [sp, #28]
   d98bc:	9906      	ldr	r1, [sp, #24]
   d98be:	4658      	mov	r0, fp
   d98c0:	f7fc fe90 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   d98c4:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   d98c8:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   d98ca:	4633      	mov	r3, r6
   d98cc:	462a      	mov	r2, r5
   d98ce:	4621      	mov	r1, r4
   d98d0:	4638      	mov	r0, r7
   d98d2:	f7fc fe28 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   d98d6:	ebb9 090b 	subs.w	r9, r9, fp
   d98da:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   d98dc:	bf18      	it	ne
   d98de:	f04f 0901 	movne.w	r9, #1
   d98e2:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d98e6:	f108 0801 	add.w	r8, r8, #1
   d98ea:	e7bd      	b.n	d9868 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d98ec:	3601      	adds	r6, #1
   d98ee:	e7b3      	b.n	d9858 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d98f0:	3501      	adds	r5, #1
   d98f2:	e7a9      	b.n	d9848 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d98f4:	3401      	adds	r4, #1
   d98f6:	e7a0      	b.n	d983a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d98f8:	a80b      	add	r0, sp, #44	; 0x2c
   d98fa:	f7fc fda4 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   d98fe:	b021      	add	sp, #132	; 0x84
   d9900:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d9904 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d9904:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d9908:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d990a:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d990c:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d990e:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d9910:	9208      	str	r2, [sp, #32]
   d9912:	4604      	mov	r4, r0
   d9914:	460e      	mov	r6, r1
   d9916:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d9918:	dd01      	ble.n	d991e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   d991a:	f00a fd07 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d991e:	683b      	ldr	r3, [r7, #0]
   d9920:	2b04      	cmp	r3, #4
   d9922:	dcfa      	bgt.n	d991a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d9924:	6813      	ldr	r3, [r2, #0]
   d9926:	2b04      	cmp	r3, #4
   d9928:	dcf7      	bgt.n	d991a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   d992a:	2301      	movs	r3, #1
   d992c:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d992e:	ad10      	add	r5, sp, #64	; 0x40
   d9930:	a80b      	add	r0, sp, #44	; 0x2c
   d9932:	f7fc fdcc 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d9936:	ab18      	add	r3, sp, #96	; 0x60
   d9938:	462a      	mov	r2, r5
   d993a:	4639      	mov	r1, r7
   d993c:	4630      	mov	r0, r6
   d993e:	f7fd f8db 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   d9942:	6863      	ldr	r3, [r4, #4]
   d9944:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   d9946:	68a3      	ldr	r3, [r4, #8]
   d9948:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   d994a:	68e3      	ldr	r3, [r4, #12]
   d994c:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   d994e:	6923      	ldr	r3, [r4, #16]
   d9950:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   d9952:	6963      	ldr	r3, [r4, #20]
   d9954:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   d9956:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   d9958:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   d995c:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d995e:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d9960:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d9962:	2100      	movs	r1, #0
   d9964:	a80b      	add	r0, sp, #44	; 0x2c
   d9966:	f7fc fd79 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d996a:	4284      	cmp	r4, r0
   d996c:	da58      	bge.n	d9a20 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11c>
   d996e:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d9970:	af0b      	add	r7, sp, #44	; 0x2c
   d9972:	2101      	movs	r1, #1
   d9974:	4638      	mov	r0, r7
   d9976:	f7fc fd71 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d997a:	4285      	cmp	r5, r0
   d997c:	da4e      	bge.n	d9a1c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x118>
   d997e:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d9980:	2102      	movs	r1, #2
   d9982:	4638      	mov	r0, r7
   d9984:	f7fc fd6a 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d9988:	4286      	cmp	r6, r0
   d998a:	da45      	bge.n	d9a18 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x114>
   d998c:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d9990:	2103      	movs	r1, #3
   d9992:	4638      	mov	r0, r7
   d9994:	f7fc fd62 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d9998:	4580      	cmp	r8, r0
   d999a:	da3b      	bge.n	d9a14 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x110>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d999c:	f8cd 8000 	str.w	r8, [sp]
   d99a0:	4633      	mov	r3, r6
   d99a2:	462a      	mov	r2, r5
   d99a4:	4621      	mov	r1, r4
   d99a6:	9809      	ldr	r0, [sp, #36]	; 0x24
   d99a8:	f7fc fe6e 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   d99ac:	9b08      	ldr	r3, [sp, #32]
   d99ae:	f913 9000 	ldrsb.w	r9, [r3, r0]
   d99b2:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d99b4:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d99b8:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d99ba:	462a      	mov	r2, r5
   d99bc:	4633      	mov	r3, r6
   d99be:	4621      	mov	r1, r4
   d99c0:	a818      	add	r0, sp, #96	; 0x60
   d99c2:	f7fc fe61 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d99c6:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d99c8:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d99ca:	f913 b000 	ldrsb.w	fp, [r3, r0]
   d99ce:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d99d0:	9903      	ldr	r1, [sp, #12]
   d99d2:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d99d6:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d99d8:	f7fc fe04 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d99dc:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d99e0:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   d99e2:	9a07      	ldr	r2, [sp, #28]
   d99e4:	9906      	ldr	r1, [sp, #24]
   d99e6:	4658      	mov	r0, fp
   d99e8:	f7fc fdfc 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   d99ec:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   d99f0:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   d99f2:	4633      	mov	r3, r6
   d99f4:	462a      	mov	r2, r5
   d99f6:	4621      	mov	r1, r4
   d99f8:	4638      	mov	r0, r7
   d99fa:	f7fc fd94 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   d99fe:	ebb9 090b 	subs.w	r9, r9, fp
   d9a02:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   d9a04:	bf18      	it	ne
   d9a06:	f04f 0901 	movne.w	r9, #1
   d9a0a:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d9a0e:	f108 0801 	add.w	r8, r8, #1
   d9a12:	e7bd      	b.n	d9990 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d9a14:	3601      	adds	r6, #1
   d9a16:	e7b3      	b.n	d9980 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d9a18:	3501      	adds	r5, #1
   d9a1a:	e7a9      	b.n	d9970 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d9a1c:	3401      	adds	r4, #1
   d9a1e:	e7a0      	b.n	d9962 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d9a20:	a80b      	add	r0, sp, #44	; 0x2c
   d9a22:	f7fc fd10 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   d9a26:	b021      	add	sp, #132	; 0x84
   d9a28:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d9a2c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode>:

// TODO(renjieliu): Refactor the logic to avoid duplications.
TfLiteStatus NotEqualEval(TfLiteContext* context, TfLiteNode* node) {
   d9a2c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d9a30:	680a      	ldr	r2, [r1, #0]
   d9a32:	f8d0 9008 	ldr.w	r9, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d9a36:	6895      	ldr	r5, [r2, #8]
   d9a38:	4682      	mov	sl, r0
   d9a3a:	6850      	ldr	r0, [r2, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d9a3c:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d9a3e:	2338      	movs	r3, #56	; 0x38
   d9a40:	fb03 f800 	mul.w	r8, r3, r0
   d9a44:	fb03 9505 	mla	r5, r3, r5, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d9a48:	6854      	ldr	r4, [r2, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d9a4a:	eb09 0608 	add.w	r6, r9, r8
   d9a4e:	b0ab      	sub	sp, #172	; 0xac
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   d9a50:	4629      	mov	r1, r5
   d9a52:	4630      	mov	r0, r6
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d9a54:	fb03 9404 	mla	r4, r3, r4, r9
   d9a58:	f00a f98a 	bl	e3d70 <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
  switch (input1->type) {
   d9a5c:	f819 2008 	ldrb.w	r2, [r9, r8]
// TODO(renjieliu): Refactor the logic to avoid duplications.
TfLiteStatus NotEqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   d9a60:	f080 0001 	eor.w	r0, r0, #1
  switch (input1->type) {
   d9a64:	1e53      	subs	r3, r2, #1
// TODO(renjieliu): Refactor the logic to avoid duplications.
TfLiteStatus NotEqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   d9a66:	b2c7      	uxtb	r7, r0
  switch (input1->type) {
   d9a68:	2b08      	cmp	r3, #8
   d9a6a:	f200 826e 	bhi.w	d9f4a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x51e>
   d9a6e:	e8df f013 	tbh	[pc, r3, lsl #1]
   d9a72:	0053      	.short	0x0053
   d9a74:	013900a1 	.word	0x013900a1
   d9a78:	026c00eb 	.word	0x026c00eb
   d9a7c:	026c0009 	.word	0x026c0009
   d9a80:	01cd026c 	.word	0x01cd026c
   d9a84:	f10d 0874 	add.w	r8, sp, #116	; 0x74
    case kTfLiteBool:
      TF_LITE_COMPARISON(bool, NotEqual, requires_broadcast);
   d9a88:	4631      	mov	r1, r6
   d9a8a:	b1cf      	cbz	r7, d9ac0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x94>
   d9a8c:	a813      	add	r0, sp, #76	; 0x4c
   d9a8e:	f7fc ff8a 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9a92:	4629      	mov	r1, r5
   d9a94:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9a96:	6876      	ldr	r6, [r6, #4]
   d9a98:	f7fc ff85 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9a9c:	b105      	cbz	r5, d9aa0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x74>
   d9a9e:	686d      	ldr	r5, [r5, #4]
   d9aa0:	4621      	mov	r1, r4
   d9aa2:	4640      	mov	r0, r8
   d9aa4:	f7fc ff7f 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9aa8:	b104      	cbz	r4, d9aac <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x80>
   d9aaa:	6864      	ldr	r4, [r4, #4]
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
   d9aac:	9402      	str	r4, [sp, #8]
   d9aae:	e88d 0120 	stmia.w	sp, {r5, r8}
   d9ab2:	ab18      	add	r3, sp, #96	; 0x60
   d9ab4:	4632      	mov	r2, r6
   d9ab6:	a913      	add	r1, sp, #76	; 0x4c
   d9ab8:	a822      	add	r0, sp, #136	; 0x88
   d9aba:	f7fe fbbf 	bl	d823c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d9abe:	e1e7      	b.n	d9e90 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x464>
   d9ac0:	a818      	add	r0, sp, #96	; 0x60
   d9ac2:	f7fc ff70 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9ac6:	4629      	mov	r1, r5
   d9ac8:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9aca:	6876      	ldr	r6, [r6, #4]
   d9acc:	f7fc ff6b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9ad0:	b105      	cbz	r5, d9ad4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0xa8>
   d9ad2:	686d      	ldr	r5, [r5, #4]
   d9ad4:	4621      	mov	r1, r4
   d9ad6:	a822      	add	r0, sp, #136	; 0x88
   d9ad8:	f7fc ff65 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9adc:	b104      	cbz	r4, d9ae0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0xb4>
   d9ade:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9ae0:	4641      	mov	r1, r8
   d9ae2:	aa22      	add	r2, sp, #136	; 0x88
   d9ae4:	a818      	add	r0, sp, #96	; 0x60
   d9ae6:	f7fc fd40 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d9aea:	3d01      	subs	r5, #1
   d9aec:	1e73      	subs	r3, r6, #1
   d9aee:	17c1      	asrs	r1, r0, #31
   d9af0:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
   d9af2:	2600      	movs	r6, #0
   d9af4:	2700      	movs	r7, #0
   d9af6:	4286      	cmp	r6, r0
   d9af8:	eb77 0201 	sbcs.w	r2, r7, r1
   d9afc:	f280 822c 	bge.w	d9f58 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x52c>
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9b00:	f813 ef01 	ldrb.w	lr, [r3, #1]!
   d9b04:	f815 2f01 	ldrb.w	r2, [r5, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9b08:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9b0a:	ea8e 0202 	eor.w	r2, lr, r2
   d9b0e:	f804 2f01 	strb.w	r2, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9b12:	f147 0700 	adc.w	r7, r7, #0
   d9b16:	e7ee      	b.n	d9af6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0xca>
   d9b18:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteFloat32:
      TF_LITE_COMPARISON(float, NotEqual, requires_broadcast);
   d9b1c:	4631      	mov	r1, r6
   d9b1e:	b1cf      	cbz	r7, d9b54 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x128>
   d9b20:	a813      	add	r0, sp, #76	; 0x4c
   d9b22:	f7fc ff40 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9b26:	4629      	mov	r1, r5
   d9b28:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9b2a:	6876      	ldr	r6, [r6, #4]
   d9b2c:	f7fc ff3b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9b30:	b105      	cbz	r5, d9b34 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x108>
   d9b32:	686d      	ldr	r5, [r5, #4]
   d9b34:	4621      	mov	r1, r4
   d9b36:	4640      	mov	r0, r8
   d9b38:	f7fc ff35 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9b3c:	b104      	cbz	r4, d9b40 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x114>
   d9b3e:	6864      	ldr	r4, [r4, #4]
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
   d9b40:	9402      	str	r4, [sp, #8]
   d9b42:	e88d 0120 	stmia.w	sp, {r5, r8}
   d9b46:	ab18      	add	r3, sp, #96	; 0x60
   d9b48:	4632      	mov	r2, r6
   d9b4a:	a913      	add	r1, sp, #76	; 0x4c
   d9b4c:	a822      	add	r0, sp, #136	; 0x88
   d9b4e:	f7fe fbdd 	bl	d830c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d9b52:	e19d      	b.n	d9e90 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x464>
   d9b54:	a818      	add	r0, sp, #96	; 0x60
   d9b56:	f7fc ff26 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9b5a:	4629      	mov	r1, r5
   d9b5c:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9b5e:	6876      	ldr	r6, [r6, #4]
   d9b60:	f7fc ff21 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9b64:	b105      	cbz	r5, d9b68 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x13c>
   d9b66:	686d      	ldr	r5, [r5, #4]
   d9b68:	4621      	mov	r1, r4
   d9b6a:	a822      	add	r0, sp, #136	; 0x88
   d9b6c:	f7fc ff1b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9b70:	b104      	cbz	r4, d9b74 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x148>
   d9b72:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9b74:	4641      	mov	r1, r8
   d9b76:	aa22      	add	r2, sp, #136	; 0x88
   d9b78:	a818      	add	r0, sp, #96	; 0x60
   d9b7a:	f7fc fcf6 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d9b7e:	3c01      	subs	r4, #1
   d9b80:	4633      	mov	r3, r6
   d9b82:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   d9b84:	2600      	movs	r6, #0
   d9b86:	2700      	movs	r7, #0
   d9b88:	4286      	cmp	r6, r0
   d9b8a:	eb77 0201 	sbcs.w	r2, r7, r1
   d9b8e:	f280 81e3 	bge.w	d9f58 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x52c>
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9b92:	ecb3 7a01 	vldmia	r3!, {s14}
   d9b96:	ecf5 7a01 	vldmia	r5!, {s15}
   d9b9a:	eeb4 7a67 	vcmp.f32	s14, s15
   d9b9e:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d9ba2:	bf14      	ite	ne
   d9ba4:	2201      	movne	r2, #1
   d9ba6:	2200      	moveq	r2, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9ba8:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9baa:	f804 2f01 	strb.w	r2, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9bae:	f147 0700 	adc.w	r7, r7, #0
   d9bb2:	e7e9      	b.n	d9b88 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x15c>
   d9bb4:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, NotEqual, requires_broadcast);
   d9bb8:	4631      	mov	r1, r6
   d9bba:	b1cf      	cbz	r7, d9bf0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x1c4>
   d9bbc:	a813      	add	r0, sp, #76	; 0x4c
   d9bbe:	f7fc fef2 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9bc2:	4629      	mov	r1, r5
   d9bc4:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9bc6:	6876      	ldr	r6, [r6, #4]
   d9bc8:	f7fc feed 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9bcc:	b105      	cbz	r5, d9bd0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x1a4>
   d9bce:	686d      	ldr	r5, [r5, #4]
   d9bd0:	4621      	mov	r1, r4
   d9bd2:	4640      	mov	r0, r8
   d9bd4:	f7fc fee7 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9bd8:	b104      	cbz	r4, d9bdc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x1b0>
   d9bda:	6864      	ldr	r4, [r4, #4]
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
   d9bdc:	9402      	str	r4, [sp, #8]
   d9bde:	e88d 0120 	stmia.w	sp, {r5, r8}
   d9be2:	ab18      	add	r3, sp, #96	; 0x60
   d9be4:	4632      	mov	r2, r6
   d9be6:	a913      	add	r1, sp, #76	; 0x4c
   d9be8:	a822      	add	r0, sp, #136	; 0x88
   d9bea:	f7fe fc02 	bl	d83f2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d9bee:	e14f      	b.n	d9e90 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x464>
   d9bf0:	a818      	add	r0, sp, #96	; 0x60
   d9bf2:	f7fc fed8 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9bf6:	4629      	mov	r1, r5
   d9bf8:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9bfa:	6876      	ldr	r6, [r6, #4]
   d9bfc:	f7fc fed3 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9c00:	b105      	cbz	r5, d9c04 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x1d8>
   d9c02:	686d      	ldr	r5, [r5, #4]
   d9c04:	4621      	mov	r1, r4
   d9c06:	a822      	add	r0, sp, #136	; 0x88
   d9c08:	f7fc fecd 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9c0c:	b104      	cbz	r4, d9c10 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x1e4>
   d9c0e:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9c10:	aa22      	add	r2, sp, #136	; 0x88
   d9c12:	4641      	mov	r1, r8
   d9c14:	a818      	add	r0, sp, #96	; 0x60
   d9c16:	f7fc fca8 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d9c1a:	3c01      	subs	r4, #1
   d9c1c:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   d9c1e:	2200      	movs	r2, #0
   d9c20:	2300      	movs	r3, #0
   d9c22:	4282      	cmp	r2, r0
   d9c24:	eb73 0701 	sbcs.w	r7, r3, r1
   d9c28:	f280 8196 	bge.w	d9f58 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x52c>
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9c2c:	f856 7022 	ldr.w	r7, [r6, r2, lsl #2]
   d9c30:	f855 e022 	ldr.w	lr, [r5, r2, lsl #2]
   d9c34:	ebb7 070e 	subs.w	r7, r7, lr
   d9c38:	bf18      	it	ne
   d9c3a:	2701      	movne	r7, #1
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9c3c:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9c3e:	f804 7f01 	strb.w	r7, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9c42:	f143 0300 	adc.w	r3, r3, #0
   d9c46:	e7ec      	b.n	d9c22 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x1f6>
   d9c48:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, NotEqual, requires_broadcast);
   d9c4c:	4631      	mov	r1, r6
   d9c4e:	b1cf      	cbz	r7, d9c84 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x258>
   d9c50:	a813      	add	r0, sp, #76	; 0x4c
   d9c52:	f7fc fea8 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9c56:	4629      	mov	r1, r5
   d9c58:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9c5a:	6876      	ldr	r6, [r6, #4]
   d9c5c:	f7fc fea3 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9c60:	b105      	cbz	r5, d9c64 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x238>
   d9c62:	686d      	ldr	r5, [r5, #4]
   d9c64:	4621      	mov	r1, r4
   d9c66:	4640      	mov	r0, r8
   d9c68:	f7fc fe9d 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9c6c:	b104      	cbz	r4, d9c70 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x244>
   d9c6e:	6864      	ldr	r4, [r4, #4]
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
   d9c70:	9402      	str	r4, [sp, #8]
   d9c72:	e88d 0120 	stmia.w	sp, {r5, r8}
   d9c76:	ab18      	add	r3, sp, #96	; 0x60
   d9c78:	4632      	mov	r2, r6
   d9c7a:	a913      	add	r1, sp, #76	; 0x4c
   d9c7c:	a822      	add	r0, sp, #136	; 0x88
   d9c7e:	f7fe fc23 	bl	d84c8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d9c82:	e105      	b.n	d9e90 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x464>
   d9c84:	a818      	add	r0, sp, #96	; 0x60
   d9c86:	f7fc fe8e 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9c8a:	4629      	mov	r1, r5
   d9c8c:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9c8e:	6876      	ldr	r6, [r6, #4]
   d9c90:	f7fc fe89 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9c94:	b105      	cbz	r5, d9c98 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x26c>
   d9c96:	686d      	ldr	r5, [r5, #4]
   d9c98:	4621      	mov	r1, r4
   d9c9a:	a822      	add	r0, sp, #136	; 0x88
   d9c9c:	f7fc fe83 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9ca0:	b104      	cbz	r4, d9ca4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x278>
   d9ca2:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9ca4:	aa22      	add	r2, sp, #136	; 0x88
   d9ca6:	4641      	mov	r1, r8
   d9ca8:	a818      	add	r0, sp, #96	; 0x60
   d9caa:	f7fc fc5e 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d9cae:	3d08      	subs	r5, #8
   d9cb0:	17c1      	asrs	r1, r0, #31
   d9cb2:	f1a6 0e08 	sub.w	lr, r6, #8
   d9cb6:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
   d9cb8:	2200      	movs	r2, #0
   d9cba:	2300      	movs	r3, #0
   d9cbc:	4282      	cmp	r2, r0
   d9cbe:	eb73 0601 	sbcs.w	r6, r3, r1
   d9cc2:	f280 8149 	bge.w	d9f58 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x52c>
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9cc6:	e9f5 6702 	ldrd	r6, r7, [r5, #8]!
   d9cca:	e9fe ab02 	ldrd	sl, fp, [lr, #8]!
   d9cce:	45bb      	cmp	fp, r7
   d9cd0:	bf0a      	itet	eq
   d9cd2:	45b2      	cmpeq	sl, r6
   d9cd4:	2601      	movne	r6, #1
   d9cd6:	2600      	moveq	r6, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9cd8:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9cda:	f804 6f01 	strb.w	r6, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9cde:	f143 0300 	adc.w	r3, r3, #0
   d9ce2:	e7eb      	b.n	d9cbc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x290>
            GetTensorData<bool>(output));                                      \
      }                                                                        \
    }                                                                          \
  }
TF_LITE_QUANTIZE_COMPARISON(Equal);
TF_LITE_QUANTIZE_COMPARISON(NotEqual);
   d9ce4:	6933      	ldr	r3, [r6, #16]
   d9ce6:	68f0      	ldr	r0, [r6, #12]
   d9ce8:	f1c3 0900 	rsb	r9, r3, #0
   d9cec:	692b      	ldr	r3, [r5, #16]
   d9cee:	f1c3 0800 	rsb	r8, r3, #0
   d9cf2:	f00d f983 	bl	e6ffc <__aeabi_f2d>
   d9cf6:	ec41 0b10 	vmov	d0, r0, r1
   d9cfa:	a910      	add	r1, sp, #64	; 0x40
   d9cfc:	a80f      	add	r0, sp, #60	; 0x3c
   d9cfe:	f00a f8ab 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   d9d02:	68e8      	ldr	r0, [r5, #12]
   d9d04:	f00d f97a 	bl	e6ffc <__aeabi_f2d>
   d9d08:	ec41 0b10 	vmov	d0, r0, r1
   d9d0c:	a912      	add	r1, sp, #72	; 0x48
   d9d0e:	a811      	add	r0, sp, #68	; 0x44
   d9d10:	f00a f8a2 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   d9d14:	2308      	movs	r3, #8
   d9d16:	9322      	str	r3, [sp, #136]	; 0x88
   d9d18:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   d9d1a:	9324      	str	r3, [sp, #144]	; 0x90
   d9d1c:	9b10      	ldr	r3, [sp, #64]	; 0x40
   d9d1e:	9325      	str	r3, [sp, #148]	; 0x94
   d9d20:	9b11      	ldr	r3, [sp, #68]	; 0x44
   d9d22:	9327      	str	r3, [sp, #156]	; 0x9c
   d9d24:	9b12      	ldr	r3, [sp, #72]	; 0x48
   d9d26:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   d9d2a:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   d9d2e:	9328      	str	r3, [sp, #160]	; 0xa0
   d9d30:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   d9d34:	4631      	mov	r1, r6
   d9d36:	a813      	add	r0, sp, #76	; 0x4c
   d9d38:	b1bf      	cbz	r7, d9d6a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x33e>
   d9d3a:	f7fc fe34 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9d3e:	4629      	mov	r1, r5
   d9d40:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9d42:	6876      	ldr	r6, [r6, #4]
   d9d44:	f7fc fe2f 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9d48:	4621      	mov	r1, r4
   d9d4a:	4640      	mov	r0, r8
   d9d4c:	686d      	ldr	r5, [r5, #4]
   d9d4e:	f7fc fe2a 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9d52:	b104      	cbz	r4, d9d56 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x32a>
   d9d54:	6864      	ldr	r4, [r4, #4]
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
   d9d56:	9402      	str	r4, [sp, #8]
   d9d58:	e88d 0120 	stmia.w	sp, {r5, r8}
   d9d5c:	ab18      	add	r3, sp, #96	; 0x60
   d9d5e:	4632      	mov	r2, r6
   d9d60:	a913      	add	r1, sp, #76	; 0x4c
   d9d62:	a822      	add	r0, sp, #136	; 0x88
   d9d64:	f7ff fd3a 	bl	d97dc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d9d68:	e092      	b.n	d9e90 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x464>
   d9d6a:	f7fc fe1c 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9d6e:	6873      	ldr	r3, [r6, #4]
   d9d70:	9305      	str	r3, [sp, #20]
   d9d72:	4629      	mov	r1, r5
   d9d74:	a818      	add	r0, sp, #96	; 0x60
   d9d76:	f7fc fe16 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9d7a:	4621      	mov	r1, r4
   d9d7c:	4640      	mov	r0, r8
   d9d7e:	f8d5 b004 	ldr.w	fp, [r5, #4]
   d9d82:	f7fc fe10 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9d86:	b104      	cbz	r4, d9d8a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x35e>
   d9d88:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   d9d8a:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   d9d8c:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   d9d8e:	9b24      	ldr	r3, [sp, #144]	; 0x90
   d9d90:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   d9d92:	9b25      	ldr	r3, [sp, #148]	; 0x94
   d9d94:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   d9d96:	9b26      	ldr	r3, [sp, #152]	; 0x98
   d9d98:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   d9d9a:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   d9d9c:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9d9e:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   d9da0:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   d9da2:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9da4:	a918      	add	r1, sp, #96	; 0x60
   d9da6:	a813      	add	r0, sp, #76	; 0x4c
   d9da8:	f7fc fbdf 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d9dac:	4602      	mov	r2, r0
   d9dae:	17c3      	asrs	r3, r0, #31
   d9db0:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   d9db4:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9db6:	f04f 0800 	mov.w	r8, #0
   d9dba:	f04f 0900 	mov.w	r9, #0
   d9dbe:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   d9dc2:	4590      	cmp	r8, r2
   d9dc4:	eb79 0303 	sbcs.w	r3, r9, r3
   d9dc8:	f280 80b4 	bge.w	d9f34 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x508>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9dcc:	f81b 5008 	ldrb.w	r5, [fp, r8]
   d9dd0:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d9dd2:	9a08      	ldr	r2, [sp, #32]
   d9dd4:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9dd6:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d9dd8:	9b05      	ldr	r3, [sp, #20]
   d9dda:	f813 0008 	ldrb.w	r0, [r3, r8]
   d9dde:	9b06      	ldr	r3, [sp, #24]
   d9de0:	4418      	add	r0, r3
   d9de2:	40b8      	lsls	r0, r7
   d9de4:	f7fc fbfe 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9de8:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d9dea:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   d9dec:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   d9dee:	990a      	ldr	r1, [sp, #40]	; 0x28
   d9df0:	4628      	mov	r0, r5
   d9df2:	f7fc fbf7 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   d9df6:	ebba 0000 	subs.w	r0, sl, r0
   d9dfa:	bf18      	it	ne
   d9dfc:	2001      	movne	r0, #1
   d9dfe:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9e02:	f118 0801 	adds.w	r8, r8, #1
   d9e06:	f149 0900 	adc.w	r9, r9, #0
   d9e0a:	e7d8      	b.n	d9dbe <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x392>
   d9e0c:	6933      	ldr	r3, [r6, #16]
   d9e0e:	68f0      	ldr	r0, [r6, #12]
   d9e10:	f1c3 0900 	rsb	r9, r3, #0
   d9e14:	692b      	ldr	r3, [r5, #16]
   d9e16:	f1c3 0800 	rsb	r8, r3, #0
   d9e1a:	f00d f8ef 	bl	e6ffc <__aeabi_f2d>
   d9e1e:	ec41 0b10 	vmov	d0, r0, r1
   d9e22:	a910      	add	r1, sp, #64	; 0x40
   d9e24:	a80f      	add	r0, sp, #60	; 0x3c
   d9e26:	f00a f817 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   d9e2a:	68e8      	ldr	r0, [r5, #12]
   d9e2c:	f00d f8e6 	bl	e6ffc <__aeabi_f2d>
   d9e30:	ec41 0b10 	vmov	d0, r0, r1
   d9e34:	a912      	add	r1, sp, #72	; 0x48
   d9e36:	a811      	add	r0, sp, #68	; 0x44
   d9e38:	f00a f80e 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   d9e3c:	2308      	movs	r3, #8
   d9e3e:	9322      	str	r3, [sp, #136]	; 0x88
   d9e40:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   d9e42:	9324      	str	r3, [sp, #144]	; 0x90
   d9e44:	9b10      	ldr	r3, [sp, #64]	; 0x40
   d9e46:	9325      	str	r3, [sp, #148]	; 0x94
   d9e48:	9b11      	ldr	r3, [sp, #68]	; 0x44
   d9e4a:	9327      	str	r3, [sp, #156]	; 0x9c
   d9e4c:	9b12      	ldr	r3, [sp, #72]	; 0x48
   d9e4e:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   d9e52:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   d9e56:	9328      	str	r3, [sp, #160]	; 0xa0
   d9e58:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   d9e5c:	4631      	mov	r1, r6
   d9e5e:	a813      	add	r0, sp, #76	; 0x4c
   d9e60:	b1c7      	cbz	r7, d9e94 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x468>
   d9e62:	f7fc fda0 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9e66:	4629      	mov	r1, r5
   d9e68:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9e6a:	6876      	ldr	r6, [r6, #4]
   d9e6c:	f7fc fd9b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9e70:	4621      	mov	r1, r4
   d9e72:	4640      	mov	r0, r8
   d9e74:	686d      	ldr	r5, [r5, #4]
   d9e76:	f7fc fd96 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9e7a:	b104      	cbz	r4, d9e7e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x452>
   d9e7c:	6864      	ldr	r4, [r4, #4]
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
   d9e7e:	9402      	str	r4, [sp, #8]
   d9e80:	e88d 0120 	stmia.w	sp, {r5, r8}
   d9e84:	ab18      	add	r3, sp, #96	; 0x60
   d9e86:	4632      	mov	r2, r6
   d9e88:	a913      	add	r1, sp, #76	; 0x4c
   d9e8a:	a822      	add	r0, sp, #136	; 0x88
   d9e8c:	f7ff fd3a 	bl	d9904 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d9e90:	4640      	mov	r0, r8
   d9e92:	e050      	b.n	d9f36 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x50a>
   d9e94:	f7fc fd87 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9e98:	6873      	ldr	r3, [r6, #4]
   d9e9a:	9305      	str	r3, [sp, #20]
   d9e9c:	4629      	mov	r1, r5
   d9e9e:	a818      	add	r0, sp, #96	; 0x60
   d9ea0:	f7fc fd81 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9ea4:	4621      	mov	r1, r4
   d9ea6:	4640      	mov	r0, r8
   d9ea8:	f8d5 b004 	ldr.w	fp, [r5, #4]
   d9eac:	f7fc fd7b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9eb0:	b104      	cbz	r4, d9eb4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x488>
   d9eb2:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   d9eb4:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   d9eb6:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   d9eb8:	9b24      	ldr	r3, [sp, #144]	; 0x90
   d9eba:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   d9ebc:	9b25      	ldr	r3, [sp, #148]	; 0x94
   d9ebe:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   d9ec0:	9b26      	ldr	r3, [sp, #152]	; 0x98
   d9ec2:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   d9ec4:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   d9ec6:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9ec8:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   d9eca:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   d9ecc:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9ece:	a918      	add	r1, sp, #96	; 0x60
   d9ed0:	a813      	add	r0, sp, #76	; 0x4c
   d9ed2:	f7fc fb4a 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d9ed6:	4602      	mov	r2, r0
   d9ed8:	17c3      	asrs	r3, r0, #31
   d9eda:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   d9ede:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9ee0:	f04f 0800 	mov.w	r8, #0
   d9ee4:	f04f 0900 	mov.w	r9, #0
   d9ee8:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   d9eec:	4590      	cmp	r8, r2
   d9eee:	eb79 0303 	sbcs.w	r3, r9, r3
   d9ef2:	da1f      	bge.n	d9f34 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x508>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9ef4:	f91b 5008 	ldrsb.w	r5, [fp, r8]
   d9ef8:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d9efa:	9a08      	ldr	r2, [sp, #32]
   d9efc:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9efe:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d9f00:	9b05      	ldr	r3, [sp, #20]
   d9f02:	f913 0008 	ldrsb.w	r0, [r3, r8]
   d9f06:	9b06      	ldr	r3, [sp, #24]
   d9f08:	4418      	add	r0, r3
   d9f0a:	40b8      	lsls	r0, r7
   d9f0c:	f7fc fb6a 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9f10:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d9f12:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   d9f14:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   d9f16:	990a      	ldr	r1, [sp, #40]	; 0x28
   d9f18:	4628      	mov	r0, r5
   d9f1a:	f7fc fb63 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   d9f1e:	ebba 0000 	subs.w	r0, sl, r0
   d9f22:	bf18      	it	ne
   d9f24:	2001      	movne	r0, #1
   d9f26:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9f2a:	f118 0801 	adds.w	r8, r8, #1
   d9f2e:	f149 0900 	adc.w	r9, r9, #0
   d9f32:	e7d9      	b.n	d9ee8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x4bc>
   d9f34:	a81d      	add	r0, sp, #116	; 0x74
   d9f36:	f7fc fa86 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   d9f3a:	a818      	add	r0, sp, #96	; 0x60
   d9f3c:	f7fc fa83 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   d9f40:	a813      	add	r0, sp, #76	; 0x4c
   d9f42:	f7fc fa80 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(
          context, "Does not support type %d, requires bool|float|int|uint8",
          input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
   d9f46:	2000      	movs	r0, #0
   d9f48:	e00e      	b.n	d9f68 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x53c>
                                    requires_broadcast);
      break;
    default:
      context->ReportError(
          context, "Does not support type %d, requires bool|float|int|uint8",
          input1->type);
   d9f4a:	4650      	mov	r0, sl
   d9f4c:	f8da 3014 	ldr.w	r3, [sl, #20]
   d9f50:	4907      	ldr	r1, [pc, #28]	; (d9f70 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x544>)
   d9f52:	4798      	blx	r3
      return kTfLiteError;
   d9f54:	2001      	movs	r0, #1
   d9f56:	e007      	b.n	d9f68 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x53c>
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, NotEqual, requires_broadcast);
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, NotEqual, requires_broadcast);
   d9f58:	a822      	add	r0, sp, #136	; 0x88
   d9f5a:	f7fc fa74 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   d9f5e:	4640      	mov	r0, r8
   d9f60:	f7fc fa71 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   d9f64:	a818      	add	r0, sp, #96	; 0x60
   d9f66:	e7ec      	b.n	d9f42 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x516>
          context, "Does not support type %d, requires bool|float|int|uint8",
          input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   d9f68:	b02b      	add	sp, #172	; 0xac
   d9f6a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   d9f6e:	bf00      	nop
   d9f70:	000e99b7 	.word	0x000e99b7

000d9f74 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d9f74:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d9f78:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d9f7a:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d9f7c:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d9f7e:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d9f80:	9208      	str	r2, [sp, #32]
   d9f82:	4604      	mov	r4, r0
   d9f84:	460e      	mov	r6, r1
   d9f86:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d9f88:	dd01      	ble.n	d9f8e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   d9f8a:	f00a f9cf 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d9f8e:	683b      	ldr	r3, [r7, #0]
   d9f90:	2b04      	cmp	r3, #4
   d9f92:	dcfa      	bgt.n	d9f8a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d9f94:	6813      	ldr	r3, [r2, #0]
   d9f96:	2b04      	cmp	r3, #4
   d9f98:	dcf7      	bgt.n	d9f8a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   d9f9a:	2301      	movs	r3, #1
   d9f9c:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d9f9e:	ad10      	add	r5, sp, #64	; 0x40
   d9fa0:	a80b      	add	r0, sp, #44	; 0x2c
   d9fa2:	f7fc fa94 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d9fa6:	ab18      	add	r3, sp, #96	; 0x60
   d9fa8:	462a      	mov	r2, r5
   d9faa:	4639      	mov	r1, r7
   d9fac:	4630      	mov	r0, r6
   d9fae:	f7fc fda3 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   d9fb2:	6863      	ldr	r3, [r4, #4]
   d9fb4:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   d9fb6:	68a3      	ldr	r3, [r4, #8]
   d9fb8:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   d9fba:	68e3      	ldr	r3, [r4, #12]
   d9fbc:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   d9fbe:	6923      	ldr	r3, [r4, #16]
   d9fc0:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   d9fc2:	6963      	ldr	r3, [r4, #20]
   d9fc4:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   d9fc6:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   d9fc8:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   d9fcc:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d9fce:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d9fd0:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d9fd2:	2100      	movs	r1, #0
   d9fd4:	a80b      	add	r0, sp, #44	; 0x2c
   d9fd6:	f7fc fa41 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d9fda:	4284      	cmp	r4, r0
   d9fdc:	da59      	bge.n	da092 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   d9fde:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d9fe0:	af0b      	add	r7, sp, #44	; 0x2c
   d9fe2:	2101      	movs	r1, #1
   d9fe4:	4638      	mov	r0, r7
   d9fe6:	f7fc fa39 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d9fea:	4285      	cmp	r5, r0
   d9fec:	da4f      	bge.n	da08e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   d9fee:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d9ff0:	2102      	movs	r1, #2
   d9ff2:	4638      	mov	r0, r7
   d9ff4:	f7fc fa32 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   d9ff8:	4286      	cmp	r6, r0
   d9ffa:	da46      	bge.n	da08a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   d9ffc:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   da000:	2103      	movs	r1, #3
   da002:	4638      	mov	r0, r7
   da004:	f7fc fa2a 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   da008:	4580      	cmp	r8, r0
   da00a:	da3c      	bge.n	da086 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da00c:	f8cd 8000 	str.w	r8, [sp]
   da010:	4633      	mov	r3, r6
   da012:	462a      	mov	r2, r5
   da014:	4621      	mov	r1, r4
   da016:	9809      	ldr	r0, [sp, #36]	; 0x24
   da018:	f7fc fb36 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   da01c:	9b08      	ldr	r3, [sp, #32]
   da01e:	f813 9000 	ldrb.w	r9, [r3, r0]
   da022:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   da024:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da028:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   da02a:	462a      	mov	r2, r5
   da02c:	4633      	mov	r3, r6
   da02e:	4621      	mov	r1, r4
   da030:	a818      	add	r0, sp, #96	; 0x60
   da032:	f7fc fb29 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da036:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da038:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da03a:	f813 b000 	ldrb.w	fp, [r3, r0]
   da03e:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da040:	9903      	ldr	r1, [sp, #12]
   da042:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da046:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da048:	f7fc facc 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da04c:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da050:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   da052:	9a07      	ldr	r2, [sp, #28]
   da054:	9906      	ldr	r1, [sp, #24]
   da056:	4658      	mov	r0, fp
   da058:	f7fc fac4 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   da05c:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   da060:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   da062:	4633      	mov	r3, r6
   da064:	462a      	mov	r2, r5
   da066:	4621      	mov	r1, r4
   da068:	4638      	mov	r0, r7
   da06a:	f7fc fa5c 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   da06e:	45d9      	cmp	r9, fp
   da070:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   da072:	bfd4      	ite	le
   da074:	f04f 0900 	movle.w	r9, #0
   da078:	f04f 0901 	movgt.w	r9, #1
   da07c:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   da080:	f108 0801 	add.w	r8, r8, #1
   da084:	e7bc      	b.n	da000 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   da086:	3601      	adds	r6, #1
   da088:	e7b2      	b.n	d9ff0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   da08a:	3501      	adds	r5, #1
   da08c:	e7a8      	b.n	d9fe0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da08e:	3401      	adds	r4, #1
   da090:	e79f      	b.n	d9fd2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   da092:	a80b      	add	r0, sp, #44	; 0x2c
   da094:	f7fc f9d7 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   da098:	b021      	add	sp, #132	; 0x84
   da09a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000da09e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   da09e:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   da0a2:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   da0a4:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   da0a6:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   da0a8:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   da0aa:	9208      	str	r2, [sp, #32]
   da0ac:	4604      	mov	r4, r0
   da0ae:	460e      	mov	r6, r1
   da0b0:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   da0b2:	dd01      	ble.n	da0b8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   da0b4:	f00a f93a 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   da0b8:	683b      	ldr	r3, [r7, #0]
   da0ba:	2b04      	cmp	r3, #4
   da0bc:	dcfa      	bgt.n	da0b4 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   da0be:	6813      	ldr	r3, [r2, #0]
   da0c0:	2b04      	cmp	r3, #4
   da0c2:	dcf7      	bgt.n	da0b4 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   da0c4:	2301      	movs	r3, #1
   da0c6:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   da0c8:	ad10      	add	r5, sp, #64	; 0x40
   da0ca:	a80b      	add	r0, sp, #44	; 0x2c
   da0cc:	f7fc f9ff 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   da0d0:	ab18      	add	r3, sp, #96	; 0x60
   da0d2:	462a      	mov	r2, r5
   da0d4:	4639      	mov	r1, r7
   da0d6:	4630      	mov	r0, r6
   da0d8:	f7fc fd0e 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   da0dc:	6863      	ldr	r3, [r4, #4]
   da0de:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   da0e0:	68a3      	ldr	r3, [r4, #8]
   da0e2:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   da0e4:	68e3      	ldr	r3, [r4, #12]
   da0e6:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   da0e8:	6923      	ldr	r3, [r4, #16]
   da0ea:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   da0ec:	6963      	ldr	r3, [r4, #20]
   da0ee:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   da0f0:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   da0f2:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   da0f6:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da0f8:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da0fa:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da0fc:	2100      	movs	r1, #0
   da0fe:	a80b      	add	r0, sp, #44	; 0x2c
   da100:	f7fc f9ac 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   da104:	4284      	cmp	r4, r0
   da106:	da59      	bge.n	da1bc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   da108:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   da10a:	af0b      	add	r7, sp, #44	; 0x2c
   da10c:	2101      	movs	r1, #1
   da10e:	4638      	mov	r0, r7
   da110:	f7fc f9a4 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   da114:	4285      	cmp	r5, r0
   da116:	da4f      	bge.n	da1b8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   da118:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   da11a:	2102      	movs	r1, #2
   da11c:	4638      	mov	r0, r7
   da11e:	f7fc f99d 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   da122:	4286      	cmp	r6, r0
   da124:	da46      	bge.n	da1b4 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   da126:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   da12a:	2103      	movs	r1, #3
   da12c:	4638      	mov	r0, r7
   da12e:	f7fc f995 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   da132:	4580      	cmp	r8, r0
   da134:	da3c      	bge.n	da1b0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da136:	f8cd 8000 	str.w	r8, [sp]
   da13a:	4633      	mov	r3, r6
   da13c:	462a      	mov	r2, r5
   da13e:	4621      	mov	r1, r4
   da140:	9809      	ldr	r0, [sp, #36]	; 0x24
   da142:	f7fc faa1 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   da146:	9b08      	ldr	r3, [sp, #32]
   da148:	f913 9000 	ldrsb.w	r9, [r3, r0]
   da14c:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   da14e:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da152:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   da154:	462a      	mov	r2, r5
   da156:	4633      	mov	r3, r6
   da158:	4621      	mov	r1, r4
   da15a:	a818      	add	r0, sp, #96	; 0x60
   da15c:	f7fc fa94 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da160:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da162:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da164:	f913 b000 	ldrsb.w	fp, [r3, r0]
   da168:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da16a:	9903      	ldr	r1, [sp, #12]
   da16c:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da170:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da172:	f7fc fa37 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da176:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da17a:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   da17c:	9a07      	ldr	r2, [sp, #28]
   da17e:	9906      	ldr	r1, [sp, #24]
   da180:	4658      	mov	r0, fp
   da182:	f7fc fa2f 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   da186:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   da18a:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   da18c:	4633      	mov	r3, r6
   da18e:	462a      	mov	r2, r5
   da190:	4621      	mov	r1, r4
   da192:	4638      	mov	r0, r7
   da194:	f7fc f9c7 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   da198:	45d9      	cmp	r9, fp
   da19a:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   da19c:	bfd4      	ite	le
   da19e:	f04f 0900 	movle.w	r9, #0
   da1a2:	f04f 0901 	movgt.w	r9, #1
   da1a6:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   da1aa:	f108 0801 	add.w	r8, r8, #1
   da1ae:	e7bc      	b.n	da12a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   da1b0:	3601      	adds	r6, #1
   da1b2:	e7b2      	b.n	da11a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   da1b4:	3501      	adds	r5, #1
   da1b6:	e7a8      	b.n	da10a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da1b8:	3401      	adds	r4, #1
   da1ba:	e79f      	b.n	da0fc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   da1bc:	a80b      	add	r0, sp, #44	; 0x2c
   da1be:	f7fc f942 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   da1c2:	b021      	add	sp, #132	; 0x84
   da1c4:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000da1c8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus GreaterEval(TfLiteContext* context, TfLiteNode* node) {
   da1c8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   da1cc:	680a      	ldr	r2, [r1, #0]
   da1ce:	f8d0 9008 	ldr.w	r9, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   da1d2:	6895      	ldr	r5, [r2, #8]
   da1d4:	4682      	mov	sl, r0
   da1d6:	6850      	ldr	r0, [r2, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   da1d8:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   da1da:	2338      	movs	r3, #56	; 0x38
   da1dc:	fb03 f800 	mul.w	r8, r3, r0
   da1e0:	fb03 9505 	mla	r5, r3, r5, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   da1e4:	6854      	ldr	r4, [r2, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   da1e6:	eb09 0608 	add.w	r6, r9, r8
   da1ea:	b0ab      	sub	sp, #172	; 0xac
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   da1ec:	4629      	mov	r1, r5
   da1ee:	4630      	mov	r0, r6
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   da1f0:	fb03 9404 	mla	r4, r3, r4, r9
   da1f4:	f009 fdbc 	bl	e3d70 <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
  switch (input1->type) {
   da1f8:	f819 2008 	ldrb.w	r2, [r9, r8]

TfLiteStatus GreaterEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   da1fc:	f080 0001 	eor.w	r0, r0, #1
  switch (input1->type) {
   da200:	1e53      	subs	r3, r2, #1

TfLiteStatus GreaterEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   da202:	b2c7      	uxtb	r7, r0
  switch (input1->type) {
   da204:	2b08      	cmp	r3, #8
   da206:	f200 8225 	bhi.w	da654 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x48c>
   da20a:	e8df f013 	tbh	[pc, r3, lsl #1]
   da20e:	0009      	.short	0x0009
   da210:	00f00057 	.word	0x00f00057
   da214:	022300a1 	.word	0x022300a1
   da218:	02230223 	.word	0x02230223
   da21c:	01840223 	.word	0x01840223
   da220:	f10d 0874 	add.w	r8, sp, #116	; 0x74
    case kTfLiteFloat32:
      TF_LITE_COMPARISON(float, Greater, requires_broadcast);
   da224:	4631      	mov	r1, r6
   da226:	b1cf      	cbz	r7, da25c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x94>
   da228:	a813      	add	r0, sp, #76	; 0x4c
   da22a:	f7fc fbbc 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da22e:	4629      	mov	r1, r5
   da230:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da232:	6876      	ldr	r6, [r6, #4]
   da234:	f7fc fbb7 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da238:	b105      	cbz	r5, da23c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x74>
   da23a:	686d      	ldr	r5, [r5, #4]
   da23c:	4621      	mov	r1, r4
   da23e:	4640      	mov	r0, r8
   da240:	f7fc fbb1 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da244:	b104      	cbz	r4, da248 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x80>
   da246:	6864      	ldr	r4, [r4, #4]
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
   da248:	9402      	str	r4, [sp, #8]
   da24a:	e88d 0120 	stmia.w	sp, {r5, r8}
   da24e:	ab18      	add	r3, sp, #96	; 0x60
   da250:	4632      	mov	r2, r6
   da252:	a913      	add	r1, sp, #76	; 0x4c
   da254:	a822      	add	r0, sp, #136	; 0x88
   da256:	f7fe f9a9 	bl	d85ac <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   da25a:	e19e      	b.n	da59a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   da25c:	a818      	add	r0, sp, #96	; 0x60
   da25e:	f7fc fba2 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da262:	4629      	mov	r1, r5
   da264:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da266:	6876      	ldr	r6, [r6, #4]
   da268:	f7fc fb9d 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da26c:	b105      	cbz	r5, da270 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0xa8>
   da26e:	686d      	ldr	r5, [r5, #4]
   da270:	4621      	mov	r1, r4
   da272:	a822      	add	r0, sp, #136	; 0x88
   da274:	f7fc fb97 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da278:	b104      	cbz	r4, da27c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0xb4>
   da27a:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   da27c:	4641      	mov	r1, r8
   da27e:	aa22      	add	r2, sp, #136	; 0x88
   da280:	a818      	add	r0, sp, #96	; 0x60
   da282:	f7fc f972 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   da286:	3c01      	subs	r4, #1
   da288:	4633      	mov	r3, r6
   da28a:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   da28c:	2600      	movs	r6, #0
   da28e:	2700      	movs	r7, #0
   da290:	4286      	cmp	r6, r0
   da292:	eb77 0201 	sbcs.w	r2, r7, r1
   da296:	f280 81e4 	bge.w	da662 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   da29a:	ecb3 7a01 	vldmia	r3!, {s14}
   da29e:	ecf5 7a01 	vldmia	r5!, {s15}
   da2a2:	eeb4 7ae7 	vcmpe.f32	s14, s15
   da2a6:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   da2aa:	bfcc      	ite	gt
   da2ac:	2201      	movgt	r2, #1
   da2ae:	2200      	movle	r2, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da2b0:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   da2b2:	f804 2f01 	strb.w	r2, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da2b6:	f147 0700 	adc.w	r7, r7, #0
   da2ba:	e7e9      	b.n	da290 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0xc8>
   da2bc:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, Greater, requires_broadcast);
   da2c0:	4631      	mov	r1, r6
   da2c2:	b1cf      	cbz	r7, da2f8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x130>
   da2c4:	a813      	add	r0, sp, #76	; 0x4c
   da2c6:	f7fc fb6e 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da2ca:	4629      	mov	r1, r5
   da2cc:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da2ce:	6876      	ldr	r6, [r6, #4]
   da2d0:	f7fc fb69 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da2d4:	b105      	cbz	r5, da2d8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x110>
   da2d6:	686d      	ldr	r5, [r5, #4]
   da2d8:	4621      	mov	r1, r4
   da2da:	4640      	mov	r0, r8
   da2dc:	f7fc fb63 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da2e0:	b104      	cbz	r4, da2e4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x11c>
   da2e2:	6864      	ldr	r4, [r4, #4]
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
   da2e4:	9402      	str	r4, [sp, #8]
   da2e6:	e88d 0120 	stmia.w	sp, {r5, r8}
   da2ea:	ab18      	add	r3, sp, #96	; 0x60
   da2ec:	4632      	mov	r2, r6
   da2ee:	a913      	add	r1, sp, #76	; 0x4c
   da2f0:	a822      	add	r0, sp, #136	; 0x88
   da2f2:	f7fe f9ce 	bl	d8692 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   da2f6:	e150      	b.n	da59a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   da2f8:	a818      	add	r0, sp, #96	; 0x60
   da2fa:	f7fc fb54 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da2fe:	4629      	mov	r1, r5
   da300:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da302:	6876      	ldr	r6, [r6, #4]
   da304:	f7fc fb4f 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da308:	b105      	cbz	r5, da30c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x144>
   da30a:	686d      	ldr	r5, [r5, #4]
   da30c:	4621      	mov	r1, r4
   da30e:	a822      	add	r0, sp, #136	; 0x88
   da310:	f7fc fb49 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da314:	b104      	cbz	r4, da318 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x150>
   da316:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   da318:	aa22      	add	r2, sp, #136	; 0x88
   da31a:	4641      	mov	r1, r8
   da31c:	a818      	add	r0, sp, #96	; 0x60
   da31e:	f7fc f924 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   da322:	3c01      	subs	r4, #1
   da324:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   da326:	2200      	movs	r2, #0
   da328:	2300      	movs	r3, #0
   da32a:	4282      	cmp	r2, r0
   da32c:	eb73 0701 	sbcs.w	r7, r3, r1
   da330:	f280 8197 	bge.w	da662 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   da334:	f856 7022 	ldr.w	r7, [r6, r2, lsl #2]
   da338:	f855 e022 	ldr.w	lr, [r5, r2, lsl #2]
   da33c:	4577      	cmp	r7, lr
   da33e:	bfd4      	ite	le
   da340:	2700      	movle	r7, #0
   da342:	2701      	movgt	r7, #1
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da344:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   da346:	f804 7f01 	strb.w	r7, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da34a:	f143 0300 	adc.w	r3, r3, #0
   da34e:	e7ec      	b.n	da32a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x162>
   da350:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, Greater, requires_broadcast);
   da354:	4631      	mov	r1, r6
   da356:	b1cf      	cbz	r7, da38c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x1c4>
   da358:	a813      	add	r0, sp, #76	; 0x4c
   da35a:	f7fc fb24 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da35e:	4629      	mov	r1, r5
   da360:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da362:	6876      	ldr	r6, [r6, #4]
   da364:	f7fc fb1f 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da368:	b105      	cbz	r5, da36c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x1a4>
   da36a:	686d      	ldr	r5, [r5, #4]
   da36c:	4621      	mov	r1, r4
   da36e:	4640      	mov	r0, r8
   da370:	f7fc fb19 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da374:	b104      	cbz	r4, da378 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x1b0>
   da376:	6864      	ldr	r4, [r4, #4]
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
   da378:	9402      	str	r4, [sp, #8]
   da37a:	e88d 0120 	stmia.w	sp, {r5, r8}
   da37e:	ab18      	add	r3, sp, #96	; 0x60
   da380:	4632      	mov	r2, r6
   da382:	a913      	add	r1, sp, #76	; 0x4c
   da384:	a822      	add	r0, sp, #136	; 0x88
   da386:	f7fe f9f0 	bl	d876a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   da38a:	e106      	b.n	da59a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   da38c:	a818      	add	r0, sp, #96	; 0x60
   da38e:	f7fc fb0a 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da392:	4629      	mov	r1, r5
   da394:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da396:	6876      	ldr	r6, [r6, #4]
   da398:	f7fc fb05 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da39c:	b105      	cbz	r5, da3a0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x1d8>
   da39e:	686d      	ldr	r5, [r5, #4]
   da3a0:	4621      	mov	r1, r4
   da3a2:	a822      	add	r0, sp, #136	; 0x88
   da3a4:	f7fc faff 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da3a8:	b104      	cbz	r4, da3ac <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x1e4>
   da3aa:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   da3ac:	aa22      	add	r2, sp, #136	; 0x88
   da3ae:	4641      	mov	r1, r8
   da3b0:	a818      	add	r0, sp, #96	; 0x60
   da3b2:	f7fc f8da 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   da3b6:	3d08      	subs	r5, #8
   da3b8:	17c1      	asrs	r1, r0, #31
   da3ba:	f1a6 0e08 	sub.w	lr, r6, #8
   da3be:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
   da3c0:	2200      	movs	r2, #0
   da3c2:	2300      	movs	r3, #0
   da3c4:	4282      	cmp	r2, r0
   da3c6:	eb73 0601 	sbcs.w	r6, r3, r1
   da3ca:	f280 814a 	bge.w	da662 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   da3ce:	e9fe 6702 	ldrd	r6, r7, [lr, #8]!
   da3d2:	e9f5 ab02 	ldrd	sl, fp, [r5, #8]!
   da3d6:	45b2      	cmp	sl, r6
   da3d8:	eb7b 0607 	sbcs.w	r6, fp, r7
   da3dc:	bfb4      	ite	lt
   da3de:	2601      	movlt	r6, #1
   da3e0:	2600      	movge	r6, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da3e2:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   da3e4:	f804 6f01 	strb.w	r6, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da3e8:	f143 0300 	adc.w	r3, r3, #0
   da3ec:	e7ea      	b.n	da3c4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x1fc>
      }                                                                        \
    }                                                                          \
  }
TF_LITE_QUANTIZE_COMPARISON(Equal);
TF_LITE_QUANTIZE_COMPARISON(NotEqual);
TF_LITE_QUANTIZE_COMPARISON(Greater);
   da3ee:	6933      	ldr	r3, [r6, #16]
   da3f0:	68f0      	ldr	r0, [r6, #12]
   da3f2:	f1c3 0900 	rsb	r9, r3, #0
   da3f6:	692b      	ldr	r3, [r5, #16]
   da3f8:	f1c3 0800 	rsb	r8, r3, #0
   da3fc:	f00c fdfe 	bl	e6ffc <__aeabi_f2d>
   da400:	ec41 0b10 	vmov	d0, r0, r1
   da404:	a910      	add	r1, sp, #64	; 0x40
   da406:	a80f      	add	r0, sp, #60	; 0x3c
   da408:	f009 fd26 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   da40c:	68e8      	ldr	r0, [r5, #12]
   da40e:	f00c fdf5 	bl	e6ffc <__aeabi_f2d>
   da412:	ec41 0b10 	vmov	d0, r0, r1
   da416:	a912      	add	r1, sp, #72	; 0x48
   da418:	a811      	add	r0, sp, #68	; 0x44
   da41a:	f009 fd1d 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   da41e:	2308      	movs	r3, #8
   da420:	9322      	str	r3, [sp, #136]	; 0x88
   da422:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   da424:	9324      	str	r3, [sp, #144]	; 0x90
   da426:	9b10      	ldr	r3, [sp, #64]	; 0x40
   da428:	9325      	str	r3, [sp, #148]	; 0x94
   da42a:	9b11      	ldr	r3, [sp, #68]	; 0x44
   da42c:	9327      	str	r3, [sp, #156]	; 0x9c
   da42e:	9b12      	ldr	r3, [sp, #72]	; 0x48
   da430:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   da434:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   da438:	9328      	str	r3, [sp, #160]	; 0xa0
   da43a:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   da43e:	4631      	mov	r1, r6
   da440:	a813      	add	r0, sp, #76	; 0x4c
   da442:	b1bf      	cbz	r7, da474 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x2ac>
   da444:	f7fc faaf 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da448:	4629      	mov	r1, r5
   da44a:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da44c:	6876      	ldr	r6, [r6, #4]
   da44e:	f7fc faaa 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da452:	4621      	mov	r1, r4
   da454:	4640      	mov	r0, r8
   da456:	686d      	ldr	r5, [r5, #4]
   da458:	f7fc faa5 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da45c:	b104      	cbz	r4, da460 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x298>
   da45e:	6864      	ldr	r4, [r4, #4]
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
   da460:	9402      	str	r4, [sp, #8]
   da462:	e88d 0120 	stmia.w	sp, {r5, r8}
   da466:	ab18      	add	r3, sp, #96	; 0x60
   da468:	4632      	mov	r2, r6
   da46a:	a913      	add	r1, sp, #76	; 0x4c
   da46c:	a822      	add	r0, sp, #136	; 0x88
   da46e:	f7ff fd81 	bl	d9f74 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   da472:	e092      	b.n	da59a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   da474:	f7fc fa97 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da478:	6873      	ldr	r3, [r6, #4]
   da47a:	9305      	str	r3, [sp, #20]
   da47c:	4629      	mov	r1, r5
   da47e:	a818      	add	r0, sp, #96	; 0x60
   da480:	f7fc fa91 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da484:	4621      	mov	r1, r4
   da486:	4640      	mov	r0, r8
   da488:	f8d5 b004 	ldr.w	fp, [r5, #4]
   da48c:	f7fc fa8b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da490:	b104      	cbz	r4, da494 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x2cc>
   da492:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   da494:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   da496:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   da498:	9b24      	ldr	r3, [sp, #144]	; 0x90
   da49a:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   da49c:	9b25      	ldr	r3, [sp, #148]	; 0x94
   da49e:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   da4a0:	9b26      	ldr	r3, [sp, #152]	; 0x98
   da4a2:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   da4a4:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   da4a6:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   da4a8:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   da4aa:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   da4ac:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   da4ae:	a918      	add	r1, sp, #96	; 0x60
   da4b0:	a813      	add	r0, sp, #76	; 0x4c
   da4b2:	f7fc f85a 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   da4b6:	4602      	mov	r2, r0
   da4b8:	17c3      	asrs	r3, r0, #31
   da4ba:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   da4be:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da4c0:	f04f 0800 	mov.w	r8, #0
   da4c4:	f04f 0900 	mov.w	r9, #0
   da4c8:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   da4cc:	4590      	cmp	r8, r2
   da4ce:	eb79 0303 	sbcs.w	r3, r9, r3
   da4d2:	f280 80b4 	bge.w	da63e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x476>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da4d6:	f81b 5008 	ldrb.w	r5, [fp, r8]
   da4da:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   da4dc:	9a08      	ldr	r2, [sp, #32]
   da4de:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da4e0:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   da4e2:	9b05      	ldr	r3, [sp, #20]
   da4e4:	f813 0008 	ldrb.w	r0, [r3, r8]
   da4e8:	9b06      	ldr	r3, [sp, #24]
   da4ea:	4418      	add	r0, r3
   da4ec:	40b8      	lsls	r0, r7
   da4ee:	f7fc f879 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da4f2:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   da4f4:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   da4f6:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   da4f8:	990a      	ldr	r1, [sp, #40]	; 0x28
   da4fa:	4628      	mov	r0, r5
   da4fc:	f7fc f872 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   da500:	4582      	cmp	sl, r0
   da502:	bfd4      	ite	le
   da504:	2000      	movle	r0, #0
   da506:	2001      	movgt	r0, #1
   da508:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da50c:	f118 0801 	adds.w	r8, r8, #1
   da510:	f149 0900 	adc.w	r9, r9, #0
   da514:	e7d8      	b.n	da4c8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x300>
   da516:	6933      	ldr	r3, [r6, #16]
   da518:	68f0      	ldr	r0, [r6, #12]
   da51a:	f1c3 0900 	rsb	r9, r3, #0
   da51e:	692b      	ldr	r3, [r5, #16]
   da520:	f1c3 0800 	rsb	r8, r3, #0
   da524:	f00c fd6a 	bl	e6ffc <__aeabi_f2d>
   da528:	ec41 0b10 	vmov	d0, r0, r1
   da52c:	a910      	add	r1, sp, #64	; 0x40
   da52e:	a80f      	add	r0, sp, #60	; 0x3c
   da530:	f009 fc92 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   da534:	68e8      	ldr	r0, [r5, #12]
   da536:	f00c fd61 	bl	e6ffc <__aeabi_f2d>
   da53a:	ec41 0b10 	vmov	d0, r0, r1
   da53e:	a912      	add	r1, sp, #72	; 0x48
   da540:	a811      	add	r0, sp, #68	; 0x44
   da542:	f009 fc89 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   da546:	2308      	movs	r3, #8
   da548:	9322      	str	r3, [sp, #136]	; 0x88
   da54a:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   da54c:	9324      	str	r3, [sp, #144]	; 0x90
   da54e:	9b10      	ldr	r3, [sp, #64]	; 0x40
   da550:	9325      	str	r3, [sp, #148]	; 0x94
   da552:	9b11      	ldr	r3, [sp, #68]	; 0x44
   da554:	9327      	str	r3, [sp, #156]	; 0x9c
   da556:	9b12      	ldr	r3, [sp, #72]	; 0x48
   da558:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   da55c:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   da560:	9328      	str	r3, [sp, #160]	; 0xa0
   da562:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   da566:	4631      	mov	r1, r6
   da568:	a813      	add	r0, sp, #76	; 0x4c
   da56a:	b1c7      	cbz	r7, da59e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3d6>
   da56c:	f7fc fa1b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da570:	4629      	mov	r1, r5
   da572:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da574:	6876      	ldr	r6, [r6, #4]
   da576:	f7fc fa16 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da57a:	4621      	mov	r1, r4
   da57c:	4640      	mov	r0, r8
   da57e:	686d      	ldr	r5, [r5, #4]
   da580:	f7fc fa11 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da584:	b104      	cbz	r4, da588 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3c0>
   da586:	6864      	ldr	r4, [r4, #4]
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
   da588:	9402      	str	r4, [sp, #8]
   da58a:	e88d 0120 	stmia.w	sp, {r5, r8}
   da58e:	ab18      	add	r3, sp, #96	; 0x60
   da590:	4632      	mov	r2, r6
   da592:	a913      	add	r1, sp, #76	; 0x4c
   da594:	a822      	add	r0, sp, #136	; 0x88
   da596:	f7ff fd82 	bl	da09e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   da59a:	4640      	mov	r0, r8
   da59c:	e050      	b.n	da640 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x478>
   da59e:	f7fc fa02 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da5a2:	6873      	ldr	r3, [r6, #4]
   da5a4:	9305      	str	r3, [sp, #20]
   da5a6:	4629      	mov	r1, r5
   da5a8:	a818      	add	r0, sp, #96	; 0x60
   da5aa:	f7fc f9fc 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da5ae:	4621      	mov	r1, r4
   da5b0:	4640      	mov	r0, r8
   da5b2:	f8d5 b004 	ldr.w	fp, [r5, #4]
   da5b6:	f7fc f9f6 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da5ba:	b104      	cbz	r4, da5be <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3f6>
   da5bc:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   da5be:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   da5c0:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   da5c2:	9b24      	ldr	r3, [sp, #144]	; 0x90
   da5c4:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   da5c6:	9b25      	ldr	r3, [sp, #148]	; 0x94
   da5c8:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   da5ca:	9b26      	ldr	r3, [sp, #152]	; 0x98
   da5cc:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   da5ce:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   da5d0:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   da5d2:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   da5d4:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   da5d6:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   da5d8:	a918      	add	r1, sp, #96	; 0x60
   da5da:	a813      	add	r0, sp, #76	; 0x4c
   da5dc:	f7fb ffc5 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   da5e0:	4602      	mov	r2, r0
   da5e2:	17c3      	asrs	r3, r0, #31
   da5e4:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   da5e8:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da5ea:	f04f 0800 	mov.w	r8, #0
   da5ee:	f04f 0900 	mov.w	r9, #0
   da5f2:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   da5f6:	4590      	cmp	r8, r2
   da5f8:	eb79 0303 	sbcs.w	r3, r9, r3
   da5fc:	da1f      	bge.n	da63e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x476>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da5fe:	f91b 5008 	ldrsb.w	r5, [fp, r8]
   da602:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   da604:	9a08      	ldr	r2, [sp, #32]
   da606:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da608:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   da60a:	9b05      	ldr	r3, [sp, #20]
   da60c:	f913 0008 	ldrsb.w	r0, [r3, r8]
   da610:	9b06      	ldr	r3, [sp, #24]
   da612:	4418      	add	r0, r3
   da614:	40b8      	lsls	r0, r7
   da616:	f7fb ffe5 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da61a:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   da61c:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   da61e:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   da620:	990a      	ldr	r1, [sp, #40]	; 0x28
   da622:	4628      	mov	r0, r5
   da624:	f7fb ffde 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   da628:	4582      	cmp	sl, r0
   da62a:	bfd4      	ite	le
   da62c:	2000      	movle	r0, #0
   da62e:	2001      	movgt	r0, #1
   da630:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da634:	f118 0801 	adds.w	r8, r8, #1
   da638:	f149 0900 	adc.w	r9, r9, #0
   da63c:	e7d9      	b.n	da5f2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x42a>
   da63e:	a81d      	add	r0, sp, #116	; 0x74
   da640:	f7fb ff01 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   da644:	a818      	add	r0, sp, #96	; 0x60
   da646:	f7fb fefe 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   da64a:	a813      	add	r0, sp, #76	; 0x4c
   da64c:	f7fb fefb 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
   da650:	2000      	movs	r0, #0
   da652:	e00e      	b.n	da672 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x4aa>
                                   requires_broadcast);
      break;
    default:
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
   da654:	4650      	mov	r0, sl
   da656:	f8da 3014 	ldr.w	r3, [sl, #20]
   da65a:	4907      	ldr	r1, [pc, #28]	; (da678 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x4b0>)
   da65c:	4798      	blx	r3
      return kTfLiteError;
   da65e:	2001      	movs	r0, #1
   da660:	e007      	b.n	da672 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x4aa>
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, Greater, requires_broadcast);
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, Greater, requires_broadcast);
   da662:	a822      	add	r0, sp, #136	; 0x88
   da664:	f7fb feef 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   da668:	4640      	mov	r0, r8
   da66a:	f7fb feec 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   da66e:	a818      	add	r0, sp, #96	; 0x60
   da670:	e7ec      	b.n	da64c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x484>
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   da672:	b02b      	add	sp, #172	; 0xac
   da674:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   da678:	000e99ef 	.word	0x000e99ef

000da67c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   da67c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   da680:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   da682:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   da684:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   da686:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   da688:	9208      	str	r2, [sp, #32]
   da68a:	4604      	mov	r4, r0
   da68c:	460e      	mov	r6, r1
   da68e:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   da690:	dd01      	ble.n	da696 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   da692:	f009 fe4b 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   da696:	683b      	ldr	r3, [r7, #0]
   da698:	2b04      	cmp	r3, #4
   da69a:	dcfa      	bgt.n	da692 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   da69c:	6813      	ldr	r3, [r2, #0]
   da69e:	2b04      	cmp	r3, #4
   da6a0:	dcf7      	bgt.n	da692 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   da6a2:	2301      	movs	r3, #1
   da6a4:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   da6a6:	ad10      	add	r5, sp, #64	; 0x40
   da6a8:	a80b      	add	r0, sp, #44	; 0x2c
   da6aa:	f7fb ff10 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   da6ae:	ab18      	add	r3, sp, #96	; 0x60
   da6b0:	462a      	mov	r2, r5
   da6b2:	4639      	mov	r1, r7
   da6b4:	4630      	mov	r0, r6
   da6b6:	f7fc fa1f 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   da6ba:	6863      	ldr	r3, [r4, #4]
   da6bc:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   da6be:	68a3      	ldr	r3, [r4, #8]
   da6c0:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   da6c2:	68e3      	ldr	r3, [r4, #12]
   da6c4:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   da6c6:	6923      	ldr	r3, [r4, #16]
   da6c8:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   da6ca:	6963      	ldr	r3, [r4, #20]
   da6cc:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   da6ce:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   da6d0:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   da6d4:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da6d6:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da6d8:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da6da:	2100      	movs	r1, #0
   da6dc:	a80b      	add	r0, sp, #44	; 0x2c
   da6de:	f7fb febd 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   da6e2:	4284      	cmp	r4, r0
   da6e4:	da59      	bge.n	da79a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   da6e6:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   da6e8:	af0b      	add	r7, sp, #44	; 0x2c
   da6ea:	2101      	movs	r1, #1
   da6ec:	4638      	mov	r0, r7
   da6ee:	f7fb feb5 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   da6f2:	4285      	cmp	r5, r0
   da6f4:	da4f      	bge.n	da796 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   da6f6:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   da6f8:	2102      	movs	r1, #2
   da6fa:	4638      	mov	r0, r7
   da6fc:	f7fb feae 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   da700:	4286      	cmp	r6, r0
   da702:	da46      	bge.n	da792 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   da704:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   da708:	2103      	movs	r1, #3
   da70a:	4638      	mov	r0, r7
   da70c:	f7fb fea6 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   da710:	4580      	cmp	r8, r0
   da712:	da3c      	bge.n	da78e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da714:	f8cd 8000 	str.w	r8, [sp]
   da718:	4633      	mov	r3, r6
   da71a:	462a      	mov	r2, r5
   da71c:	4621      	mov	r1, r4
   da71e:	9809      	ldr	r0, [sp, #36]	; 0x24
   da720:	f7fb ffb2 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   da724:	9b08      	ldr	r3, [sp, #32]
   da726:	f813 9000 	ldrb.w	r9, [r3, r0]
   da72a:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   da72c:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da730:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   da732:	462a      	mov	r2, r5
   da734:	4633      	mov	r3, r6
   da736:	4621      	mov	r1, r4
   da738:	a818      	add	r0, sp, #96	; 0x60
   da73a:	f7fb ffa5 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da73e:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da740:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da742:	f813 b000 	ldrb.w	fp, [r3, r0]
   da746:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da748:	9903      	ldr	r1, [sp, #12]
   da74a:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da74e:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da750:	f7fb ff48 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da754:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da758:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   da75a:	9a07      	ldr	r2, [sp, #28]
   da75c:	9906      	ldr	r1, [sp, #24]
   da75e:	4658      	mov	r0, fp
   da760:	f7fb ff40 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   da764:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   da768:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   da76a:	4633      	mov	r3, r6
   da76c:	462a      	mov	r2, r5
   da76e:	4621      	mov	r1, r4
   da770:	4638      	mov	r0, r7
   da772:	f7fb fed8 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   da776:	45d9      	cmp	r9, fp
   da778:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   da77a:	bfb4      	ite	lt
   da77c:	f04f 0900 	movlt.w	r9, #0
   da780:	f04f 0901 	movge.w	r9, #1
   da784:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   da788:	f108 0801 	add.w	r8, r8, #1
   da78c:	e7bc      	b.n	da708 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   da78e:	3601      	adds	r6, #1
   da790:	e7b2      	b.n	da6f8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   da792:	3501      	adds	r5, #1
   da794:	e7a8      	b.n	da6e8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da796:	3401      	adds	r4, #1
   da798:	e79f      	b.n	da6da <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   da79a:	a80b      	add	r0, sp, #44	; 0x2c
   da79c:	f7fb fe53 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   da7a0:	b021      	add	sp, #132	; 0x84
   da7a2:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000da7a6 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   da7a6:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   da7aa:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   da7ac:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   da7ae:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   da7b0:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   da7b2:	9208      	str	r2, [sp, #32]
   da7b4:	4604      	mov	r4, r0
   da7b6:	460e      	mov	r6, r1
   da7b8:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   da7ba:	dd01      	ble.n	da7c0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   da7bc:	f009 fdb6 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   da7c0:	683b      	ldr	r3, [r7, #0]
   da7c2:	2b04      	cmp	r3, #4
   da7c4:	dcfa      	bgt.n	da7bc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   da7c6:	6813      	ldr	r3, [r2, #0]
   da7c8:	2b04      	cmp	r3, #4
   da7ca:	dcf7      	bgt.n	da7bc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   da7cc:	2301      	movs	r3, #1
   da7ce:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   da7d0:	ad10      	add	r5, sp, #64	; 0x40
   da7d2:	a80b      	add	r0, sp, #44	; 0x2c
   da7d4:	f7fb fe7b 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   da7d8:	ab18      	add	r3, sp, #96	; 0x60
   da7da:	462a      	mov	r2, r5
   da7dc:	4639      	mov	r1, r7
   da7de:	4630      	mov	r0, r6
   da7e0:	f7fc f98a 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   da7e4:	6863      	ldr	r3, [r4, #4]
   da7e6:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   da7e8:	68a3      	ldr	r3, [r4, #8]
   da7ea:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   da7ec:	68e3      	ldr	r3, [r4, #12]
   da7ee:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   da7f0:	6923      	ldr	r3, [r4, #16]
   da7f2:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   da7f4:	6963      	ldr	r3, [r4, #20]
   da7f6:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   da7f8:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   da7fa:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   da7fe:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da800:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da802:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da804:	2100      	movs	r1, #0
   da806:	a80b      	add	r0, sp, #44	; 0x2c
   da808:	f7fb fe28 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   da80c:	4284      	cmp	r4, r0
   da80e:	da59      	bge.n	da8c4 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   da810:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   da812:	af0b      	add	r7, sp, #44	; 0x2c
   da814:	2101      	movs	r1, #1
   da816:	4638      	mov	r0, r7
   da818:	f7fb fe20 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   da81c:	4285      	cmp	r5, r0
   da81e:	da4f      	bge.n	da8c0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   da820:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   da822:	2102      	movs	r1, #2
   da824:	4638      	mov	r0, r7
   da826:	f7fb fe19 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   da82a:	4286      	cmp	r6, r0
   da82c:	da46      	bge.n	da8bc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   da82e:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   da832:	2103      	movs	r1, #3
   da834:	4638      	mov	r0, r7
   da836:	f7fb fe11 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   da83a:	4580      	cmp	r8, r0
   da83c:	da3c      	bge.n	da8b8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da83e:	f8cd 8000 	str.w	r8, [sp]
   da842:	4633      	mov	r3, r6
   da844:	462a      	mov	r2, r5
   da846:	4621      	mov	r1, r4
   da848:	9809      	ldr	r0, [sp, #36]	; 0x24
   da84a:	f7fb ff1d 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   da84e:	9b08      	ldr	r3, [sp, #32]
   da850:	f913 9000 	ldrsb.w	r9, [r3, r0]
   da854:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   da856:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da85a:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   da85c:	462a      	mov	r2, r5
   da85e:	4633      	mov	r3, r6
   da860:	4621      	mov	r1, r4
   da862:	a818      	add	r0, sp, #96	; 0x60
   da864:	f7fb ff10 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da868:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da86a:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da86c:	f913 b000 	ldrsb.w	fp, [r3, r0]
   da870:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da872:	9903      	ldr	r1, [sp, #12]
   da874:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da878:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da87a:	f7fb feb3 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da87e:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da882:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   da884:	9a07      	ldr	r2, [sp, #28]
   da886:	9906      	ldr	r1, [sp, #24]
   da888:	4658      	mov	r0, fp
   da88a:	f7fb feab 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   da88e:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   da892:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   da894:	4633      	mov	r3, r6
   da896:	462a      	mov	r2, r5
   da898:	4621      	mov	r1, r4
   da89a:	4638      	mov	r0, r7
   da89c:	f7fb fe43 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   da8a0:	45d9      	cmp	r9, fp
   da8a2:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   da8a4:	bfb4      	ite	lt
   da8a6:	f04f 0900 	movlt.w	r9, #0
   da8aa:	f04f 0901 	movge.w	r9, #1
   da8ae:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   da8b2:	f108 0801 	add.w	r8, r8, #1
   da8b6:	e7bc      	b.n	da832 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   da8b8:	3601      	adds	r6, #1
   da8ba:	e7b2      	b.n	da822 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   da8bc:	3501      	adds	r5, #1
   da8be:	e7a8      	b.n	da812 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da8c0:	3401      	adds	r4, #1
   da8c2:	e79f      	b.n	da804 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   da8c4:	a80b      	add	r0, sp, #44	; 0x2c
   da8c6:	f7fb fdbe 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   da8ca:	b021      	add	sp, #132	; 0x84
   da8cc:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000da8d0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus GreaterEqualEval(TfLiteContext* context, TfLiteNode* node) {
   da8d0:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   da8d4:	680a      	ldr	r2, [r1, #0]
   da8d6:	f8d0 9008 	ldr.w	r9, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   da8da:	6895      	ldr	r5, [r2, #8]
   da8dc:	4682      	mov	sl, r0
   da8de:	6850      	ldr	r0, [r2, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   da8e0:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   da8e2:	2338      	movs	r3, #56	; 0x38
   da8e4:	fb03 f800 	mul.w	r8, r3, r0
   da8e8:	fb03 9505 	mla	r5, r3, r5, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   da8ec:	6854      	ldr	r4, [r2, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   da8ee:	eb09 0608 	add.w	r6, r9, r8
   da8f2:	b0ab      	sub	sp, #172	; 0xac
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   da8f4:	4629      	mov	r1, r5
   da8f6:	4630      	mov	r0, r6
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   da8f8:	fb03 9404 	mla	r4, r3, r4, r9
   da8fc:	f009 fa38 	bl	e3d70 <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
  switch (input1->type) {
   da900:	f819 2008 	ldrb.w	r2, [r9, r8]

TfLiteStatus GreaterEqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   da904:	f080 0001 	eor.w	r0, r0, #1
  switch (input1->type) {
   da908:	1e53      	subs	r3, r2, #1

TfLiteStatus GreaterEqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   da90a:	b2c7      	uxtb	r7, r0
  switch (input1->type) {
   da90c:	2b08      	cmp	r3, #8
   da90e:	f200 8225 	bhi.w	dad5c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x48c>
   da912:	e8df f013 	tbh	[pc, r3, lsl #1]
   da916:	0009      	.short	0x0009
   da918:	00f00057 	.word	0x00f00057
   da91c:	022300a1 	.word	0x022300a1
   da920:	02230223 	.word	0x02230223
   da924:	01840223 	.word	0x01840223
   da928:	f10d 0874 	add.w	r8, sp, #116	; 0x74
    case kTfLiteFloat32:
      TF_LITE_COMPARISON(float, GreaterEqual, requires_broadcast);
   da92c:	4631      	mov	r1, r6
   da92e:	b1cf      	cbz	r7, da964 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x94>
   da930:	a813      	add	r0, sp, #76	; 0x4c
   da932:	f7fc f838 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da936:	4629      	mov	r1, r5
   da938:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da93a:	6876      	ldr	r6, [r6, #4]
   da93c:	f7fc f833 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da940:	b105      	cbz	r5, da944 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x74>
   da942:	686d      	ldr	r5, [r5, #4]
   da944:	4621      	mov	r1, r4
   da946:	4640      	mov	r0, r8
   da948:	f7fc f82d 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da94c:	b104      	cbz	r4, da950 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x80>
   da94e:	6864      	ldr	r4, [r4, #4]
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
   da950:	9402      	str	r4, [sp, #8]
   da952:	e88d 0120 	stmia.w	sp, {r5, r8}
   da956:	ab18      	add	r3, sp, #96	; 0x60
   da958:	4632      	mov	r2, r6
   da95a:	a913      	add	r1, sp, #76	; 0x4c
   da95c:	a822      	add	r0, sp, #136	; 0x88
   da95e:	f7fd ff76 	bl	d884e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   da962:	e19e      	b.n	daca2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   da964:	a818      	add	r0, sp, #96	; 0x60
   da966:	f7fc f81e 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da96a:	4629      	mov	r1, r5
   da96c:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da96e:	6876      	ldr	r6, [r6, #4]
   da970:	f7fc f819 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da974:	b105      	cbz	r5, da978 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0xa8>
   da976:	686d      	ldr	r5, [r5, #4]
   da978:	4621      	mov	r1, r4
   da97a:	a822      	add	r0, sp, #136	; 0x88
   da97c:	f7fc f813 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da980:	b104      	cbz	r4, da984 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0xb4>
   da982:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   da984:	4641      	mov	r1, r8
   da986:	aa22      	add	r2, sp, #136	; 0x88
   da988:	a818      	add	r0, sp, #96	; 0x60
   da98a:	f7fb fdee 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   da98e:	3c01      	subs	r4, #1
   da990:	4633      	mov	r3, r6
   da992:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   da994:	2600      	movs	r6, #0
   da996:	2700      	movs	r7, #0
   da998:	4286      	cmp	r6, r0
   da99a:	eb77 0201 	sbcs.w	r2, r7, r1
   da99e:	f280 81e4 	bge.w	dad6a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   da9a2:	ecb3 7a01 	vldmia	r3!, {s14}
   da9a6:	ecf5 7a01 	vldmia	r5!, {s15}
   da9aa:	eeb4 7ae7 	vcmpe.f32	s14, s15
   da9ae:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   da9b2:	bfac      	ite	ge
   da9b4:	2201      	movge	r2, #1
   da9b6:	2200      	movlt	r2, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da9b8:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   da9ba:	f804 2f01 	strb.w	r2, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da9be:	f147 0700 	adc.w	r7, r7, #0
   da9c2:	e7e9      	b.n	da998 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0xc8>
   da9c4:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, GreaterEqual, requires_broadcast);
   da9c8:	4631      	mov	r1, r6
   da9ca:	b1cf      	cbz	r7, daa00 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x130>
   da9cc:	a813      	add	r0, sp, #76	; 0x4c
   da9ce:	f7fb ffea 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da9d2:	4629      	mov	r1, r5
   da9d4:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da9d6:	6876      	ldr	r6, [r6, #4]
   da9d8:	f7fb ffe5 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da9dc:	b105      	cbz	r5, da9e0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x110>
   da9de:	686d      	ldr	r5, [r5, #4]
   da9e0:	4621      	mov	r1, r4
   da9e2:	4640      	mov	r0, r8
   da9e4:	f7fb ffdf 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da9e8:	b104      	cbz	r4, da9ec <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x11c>
   da9ea:	6864      	ldr	r4, [r4, #4]
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
   da9ec:	9402      	str	r4, [sp, #8]
   da9ee:	e88d 0120 	stmia.w	sp, {r5, r8}
   da9f2:	ab18      	add	r3, sp, #96	; 0x60
   da9f4:	4632      	mov	r2, r6
   da9f6:	a913      	add	r1, sp, #76	; 0x4c
   da9f8:	a822      	add	r0, sp, #136	; 0x88
   da9fa:	f7fd ff9b 	bl	d8934 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   da9fe:	e150      	b.n	daca2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   daa00:	a818      	add	r0, sp, #96	; 0x60
   daa02:	f7fb ffd0 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   daa06:	4629      	mov	r1, r5
   daa08:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   daa0a:	6876      	ldr	r6, [r6, #4]
   daa0c:	f7fb ffcb 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   daa10:	b105      	cbz	r5, daa14 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x144>
   daa12:	686d      	ldr	r5, [r5, #4]
   daa14:	4621      	mov	r1, r4
   daa16:	a822      	add	r0, sp, #136	; 0x88
   daa18:	f7fb ffc5 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   daa1c:	b104      	cbz	r4, daa20 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x150>
   daa1e:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   daa20:	aa22      	add	r2, sp, #136	; 0x88
   daa22:	4641      	mov	r1, r8
   daa24:	a818      	add	r0, sp, #96	; 0x60
   daa26:	f7fb fda0 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   daa2a:	3c01      	subs	r4, #1
   daa2c:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   daa2e:	2200      	movs	r2, #0
   daa30:	2300      	movs	r3, #0
   daa32:	4282      	cmp	r2, r0
   daa34:	eb73 0701 	sbcs.w	r7, r3, r1
   daa38:	f280 8197 	bge.w	dad6a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   daa3c:	f856 7022 	ldr.w	r7, [r6, r2, lsl #2]
   daa40:	f855 e022 	ldr.w	lr, [r5, r2, lsl #2]
   daa44:	4577      	cmp	r7, lr
   daa46:	bfb4      	ite	lt
   daa48:	2700      	movlt	r7, #0
   daa4a:	2701      	movge	r7, #1
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   daa4c:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   daa4e:	f804 7f01 	strb.w	r7, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   daa52:	f143 0300 	adc.w	r3, r3, #0
   daa56:	e7ec      	b.n	daa32 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x162>
   daa58:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, GreaterEqual, requires_broadcast);
   daa5c:	4631      	mov	r1, r6
   daa5e:	b1cf      	cbz	r7, daa94 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x1c4>
   daa60:	a813      	add	r0, sp, #76	; 0x4c
   daa62:	f7fb ffa0 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   daa66:	4629      	mov	r1, r5
   daa68:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   daa6a:	6876      	ldr	r6, [r6, #4]
   daa6c:	f7fb ff9b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   daa70:	b105      	cbz	r5, daa74 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x1a4>
   daa72:	686d      	ldr	r5, [r5, #4]
   daa74:	4621      	mov	r1, r4
   daa76:	4640      	mov	r0, r8
   daa78:	f7fb ff95 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   daa7c:	b104      	cbz	r4, daa80 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x1b0>
   daa7e:	6864      	ldr	r4, [r4, #4]
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
   daa80:	9402      	str	r4, [sp, #8]
   daa82:	e88d 0120 	stmia.w	sp, {r5, r8}
   daa86:	ab18      	add	r3, sp, #96	; 0x60
   daa88:	4632      	mov	r2, r6
   daa8a:	a913      	add	r1, sp, #76	; 0x4c
   daa8c:	a822      	add	r0, sp, #136	; 0x88
   daa8e:	f7fd ffbd 	bl	d8a0c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   daa92:	e106      	b.n	daca2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   daa94:	a818      	add	r0, sp, #96	; 0x60
   daa96:	f7fb ff86 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   daa9a:	4629      	mov	r1, r5
   daa9c:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   daa9e:	6876      	ldr	r6, [r6, #4]
   daaa0:	f7fb ff81 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   daaa4:	b105      	cbz	r5, daaa8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x1d8>
   daaa6:	686d      	ldr	r5, [r5, #4]
   daaa8:	4621      	mov	r1, r4
   daaaa:	a822      	add	r0, sp, #136	; 0x88
   daaac:	f7fb ff7b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   daab0:	b104      	cbz	r4, daab4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x1e4>
   daab2:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   daab4:	aa22      	add	r2, sp, #136	; 0x88
   daab6:	4641      	mov	r1, r8
   daab8:	a818      	add	r0, sp, #96	; 0x60
   daaba:	f7fb fd56 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   daabe:	3d08      	subs	r5, #8
   daac0:	17c1      	asrs	r1, r0, #31
   daac2:	f1a6 0e08 	sub.w	lr, r6, #8
   daac6:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
   daac8:	2200      	movs	r2, #0
   daaca:	2300      	movs	r3, #0
   daacc:	4282      	cmp	r2, r0
   daace:	eb73 0601 	sbcs.w	r6, r3, r1
   daad2:	f280 814a 	bge.w	dad6a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   daad6:	e9fe ab02 	ldrd	sl, fp, [lr, #8]!
   daada:	e9f5 6702 	ldrd	r6, r7, [r5, #8]!
   daade:	45b2      	cmp	sl, r6
   daae0:	eb7b 0607 	sbcs.w	r6, fp, r7
   daae4:	bfac      	ite	ge
   daae6:	2601      	movge	r6, #1
   daae8:	2600      	movlt	r6, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   daaea:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   daaec:	f804 6f01 	strb.w	r6, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   daaf0:	f143 0300 	adc.w	r3, r3, #0
   daaf4:	e7ea      	b.n	daacc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x1fc>
    }                                                                          \
  }
TF_LITE_QUANTIZE_COMPARISON(Equal);
TF_LITE_QUANTIZE_COMPARISON(NotEqual);
TF_LITE_QUANTIZE_COMPARISON(Greater);
TF_LITE_QUANTIZE_COMPARISON(GreaterEqual);
   daaf6:	6933      	ldr	r3, [r6, #16]
   daaf8:	68f0      	ldr	r0, [r6, #12]
   daafa:	f1c3 0900 	rsb	r9, r3, #0
   daafe:	692b      	ldr	r3, [r5, #16]
   dab00:	f1c3 0800 	rsb	r8, r3, #0
   dab04:	f00c fa7a 	bl	e6ffc <__aeabi_f2d>
   dab08:	ec41 0b10 	vmov	d0, r0, r1
   dab0c:	a910      	add	r1, sp, #64	; 0x40
   dab0e:	a80f      	add	r0, sp, #60	; 0x3c
   dab10:	f009 f9a2 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   dab14:	68e8      	ldr	r0, [r5, #12]
   dab16:	f00c fa71 	bl	e6ffc <__aeabi_f2d>
   dab1a:	ec41 0b10 	vmov	d0, r0, r1
   dab1e:	a912      	add	r1, sp, #72	; 0x48
   dab20:	a811      	add	r0, sp, #68	; 0x44
   dab22:	f009 f999 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   dab26:	2308      	movs	r3, #8
   dab28:	9322      	str	r3, [sp, #136]	; 0x88
   dab2a:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   dab2c:	9324      	str	r3, [sp, #144]	; 0x90
   dab2e:	9b10      	ldr	r3, [sp, #64]	; 0x40
   dab30:	9325      	str	r3, [sp, #148]	; 0x94
   dab32:	9b11      	ldr	r3, [sp, #68]	; 0x44
   dab34:	9327      	str	r3, [sp, #156]	; 0x9c
   dab36:	9b12      	ldr	r3, [sp, #72]	; 0x48
   dab38:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   dab3c:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   dab40:	9328      	str	r3, [sp, #160]	; 0xa0
   dab42:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   dab46:	4631      	mov	r1, r6
   dab48:	a813      	add	r0, sp, #76	; 0x4c
   dab4a:	b1bf      	cbz	r7, dab7c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x2ac>
   dab4c:	f7fb ff2b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dab50:	4629      	mov	r1, r5
   dab52:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dab54:	6876      	ldr	r6, [r6, #4]
   dab56:	f7fb ff26 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dab5a:	4621      	mov	r1, r4
   dab5c:	4640      	mov	r0, r8
   dab5e:	686d      	ldr	r5, [r5, #4]
   dab60:	f7fb ff21 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dab64:	b104      	cbz	r4, dab68 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x298>
   dab66:	6864      	ldr	r4, [r4, #4]
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
   dab68:	9402      	str	r4, [sp, #8]
   dab6a:	e88d 0120 	stmia.w	sp, {r5, r8}
   dab6e:	ab18      	add	r3, sp, #96	; 0x60
   dab70:	4632      	mov	r2, r6
   dab72:	a913      	add	r1, sp, #76	; 0x4c
   dab74:	a822      	add	r0, sp, #136	; 0x88
   dab76:	f7ff fd81 	bl	da67c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   dab7a:	e092      	b.n	daca2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   dab7c:	f7fb ff13 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dab80:	6873      	ldr	r3, [r6, #4]
   dab82:	9305      	str	r3, [sp, #20]
   dab84:	4629      	mov	r1, r5
   dab86:	a818      	add	r0, sp, #96	; 0x60
   dab88:	f7fb ff0d 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dab8c:	4621      	mov	r1, r4
   dab8e:	4640      	mov	r0, r8
   dab90:	f8d5 b004 	ldr.w	fp, [r5, #4]
   dab94:	f7fb ff07 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dab98:	b104      	cbz	r4, dab9c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x2cc>
   dab9a:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   dab9c:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   dab9e:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   daba0:	9b24      	ldr	r3, [sp, #144]	; 0x90
   daba2:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   daba4:	9b25      	ldr	r3, [sp, #148]	; 0x94
   daba6:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   daba8:	9b26      	ldr	r3, [sp, #152]	; 0x98
   dabaa:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   dabac:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   dabae:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   dabb0:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   dabb2:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   dabb4:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   dabb6:	a918      	add	r1, sp, #96	; 0x60
   dabb8:	a813      	add	r0, sp, #76	; 0x4c
   dabba:	f7fb fcd6 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   dabbe:	4602      	mov	r2, r0
   dabc0:	17c3      	asrs	r3, r0, #31
   dabc2:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   dabc6:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   dabc8:	f04f 0800 	mov.w	r8, #0
   dabcc:	f04f 0900 	mov.w	r9, #0
   dabd0:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   dabd4:	4590      	cmp	r8, r2
   dabd6:	eb79 0303 	sbcs.w	r3, r9, r3
   dabda:	f280 80b4 	bge.w	dad46 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x476>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dabde:	f81b 5008 	ldrb.w	r5, [fp, r8]
   dabe2:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dabe4:	9a08      	ldr	r2, [sp, #32]
   dabe6:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dabe8:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dabea:	9b05      	ldr	r3, [sp, #20]
   dabec:	f813 0008 	ldrb.w	r0, [r3, r8]
   dabf0:	9b06      	ldr	r3, [sp, #24]
   dabf2:	4418      	add	r0, r3
   dabf4:	40b8      	lsls	r0, r7
   dabf6:	f7fb fcf5 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dabfa:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dabfc:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   dabfe:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   dac00:	990a      	ldr	r1, [sp, #40]	; 0x28
   dac02:	4628      	mov	r0, r5
   dac04:	f7fb fcee 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   dac08:	4582      	cmp	sl, r0
   dac0a:	bfb4      	ite	lt
   dac0c:	2000      	movlt	r0, #0
   dac0e:	2001      	movge	r0, #1
   dac10:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   dac14:	f118 0801 	adds.w	r8, r8, #1
   dac18:	f149 0900 	adc.w	r9, r9, #0
   dac1c:	e7d8      	b.n	dabd0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x300>
   dac1e:	6933      	ldr	r3, [r6, #16]
   dac20:	68f0      	ldr	r0, [r6, #12]
   dac22:	f1c3 0900 	rsb	r9, r3, #0
   dac26:	692b      	ldr	r3, [r5, #16]
   dac28:	f1c3 0800 	rsb	r8, r3, #0
   dac2c:	f00c f9e6 	bl	e6ffc <__aeabi_f2d>
   dac30:	ec41 0b10 	vmov	d0, r0, r1
   dac34:	a910      	add	r1, sp, #64	; 0x40
   dac36:	a80f      	add	r0, sp, #60	; 0x3c
   dac38:	f009 f90e 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   dac3c:	68e8      	ldr	r0, [r5, #12]
   dac3e:	f00c f9dd 	bl	e6ffc <__aeabi_f2d>
   dac42:	ec41 0b10 	vmov	d0, r0, r1
   dac46:	a912      	add	r1, sp, #72	; 0x48
   dac48:	a811      	add	r0, sp, #68	; 0x44
   dac4a:	f009 f905 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   dac4e:	2308      	movs	r3, #8
   dac50:	9322      	str	r3, [sp, #136]	; 0x88
   dac52:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   dac54:	9324      	str	r3, [sp, #144]	; 0x90
   dac56:	9b10      	ldr	r3, [sp, #64]	; 0x40
   dac58:	9325      	str	r3, [sp, #148]	; 0x94
   dac5a:	9b11      	ldr	r3, [sp, #68]	; 0x44
   dac5c:	9327      	str	r3, [sp, #156]	; 0x9c
   dac5e:	9b12      	ldr	r3, [sp, #72]	; 0x48
   dac60:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   dac64:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   dac68:	9328      	str	r3, [sp, #160]	; 0xa0
   dac6a:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   dac6e:	4631      	mov	r1, r6
   dac70:	a813      	add	r0, sp, #76	; 0x4c
   dac72:	b1c7      	cbz	r7, daca6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d6>
   dac74:	f7fb fe97 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dac78:	4629      	mov	r1, r5
   dac7a:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dac7c:	6876      	ldr	r6, [r6, #4]
   dac7e:	f7fb fe92 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dac82:	4621      	mov	r1, r4
   dac84:	4640      	mov	r0, r8
   dac86:	686d      	ldr	r5, [r5, #4]
   dac88:	f7fb fe8d 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dac8c:	b104      	cbz	r4, dac90 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3c0>
   dac8e:	6864      	ldr	r4, [r4, #4]
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
   dac90:	9402      	str	r4, [sp, #8]
   dac92:	e88d 0120 	stmia.w	sp, {r5, r8}
   dac96:	ab18      	add	r3, sp, #96	; 0x60
   dac98:	4632      	mov	r2, r6
   dac9a:	a913      	add	r1, sp, #76	; 0x4c
   dac9c:	a822      	add	r0, sp, #136	; 0x88
   dac9e:	f7ff fd82 	bl	da7a6 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   daca2:	4640      	mov	r0, r8
   daca4:	e050      	b.n	dad48 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x478>
   daca6:	f7fb fe7e 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dacaa:	6873      	ldr	r3, [r6, #4]
   dacac:	9305      	str	r3, [sp, #20]
   dacae:	4629      	mov	r1, r5
   dacb0:	a818      	add	r0, sp, #96	; 0x60
   dacb2:	f7fb fe78 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dacb6:	4621      	mov	r1, r4
   dacb8:	4640      	mov	r0, r8
   dacba:	f8d5 b004 	ldr.w	fp, [r5, #4]
   dacbe:	f7fb fe72 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dacc2:	b104      	cbz	r4, dacc6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3f6>
   dacc4:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   dacc6:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   dacc8:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   dacca:	9b24      	ldr	r3, [sp, #144]	; 0x90
   daccc:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   dacce:	9b25      	ldr	r3, [sp, #148]	; 0x94
   dacd0:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   dacd2:	9b26      	ldr	r3, [sp, #152]	; 0x98
   dacd4:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   dacd6:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   dacd8:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   dacda:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   dacdc:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   dacde:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   dace0:	a918      	add	r1, sp, #96	; 0x60
   dace2:	a813      	add	r0, sp, #76	; 0x4c
   dace4:	f7fb fc41 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   dace8:	4602      	mov	r2, r0
   dacea:	17c3      	asrs	r3, r0, #31
   dacec:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   dacf0:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   dacf2:	f04f 0800 	mov.w	r8, #0
   dacf6:	f04f 0900 	mov.w	r9, #0
   dacfa:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   dacfe:	4590      	cmp	r8, r2
   dad00:	eb79 0303 	sbcs.w	r3, r9, r3
   dad04:	da1f      	bge.n	dad46 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x476>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dad06:	f91b 5008 	ldrsb.w	r5, [fp, r8]
   dad0a:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dad0c:	9a08      	ldr	r2, [sp, #32]
   dad0e:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dad10:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dad12:	9b05      	ldr	r3, [sp, #20]
   dad14:	f913 0008 	ldrsb.w	r0, [r3, r8]
   dad18:	9b06      	ldr	r3, [sp, #24]
   dad1a:	4418      	add	r0, r3
   dad1c:	40b8      	lsls	r0, r7
   dad1e:	f7fb fc61 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dad22:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dad24:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   dad26:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   dad28:	990a      	ldr	r1, [sp, #40]	; 0x28
   dad2a:	4628      	mov	r0, r5
   dad2c:	f7fb fc5a 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   dad30:	4582      	cmp	sl, r0
   dad32:	bfb4      	ite	lt
   dad34:	2000      	movlt	r0, #0
   dad36:	2001      	movge	r0, #1
   dad38:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   dad3c:	f118 0801 	adds.w	r8, r8, #1
   dad40:	f149 0900 	adc.w	r9, r9, #0
   dad44:	e7d9      	b.n	dacfa <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x42a>
   dad46:	a81d      	add	r0, sp, #116	; 0x74
   dad48:	f7fb fb7d 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   dad4c:	a818      	add	r0, sp, #96	; 0x60
   dad4e:	f7fb fb7a 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   dad52:	a813      	add	r0, sp, #76	; 0x4c
   dad54:	f7fb fb77 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
   dad58:	2000      	movs	r0, #0
   dad5a:	e00e      	b.n	dad7a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x4aa>
                                        requires_broadcast);
      break;
    default:
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
   dad5c:	4650      	mov	r0, sl
   dad5e:	f8da 3014 	ldr.w	r3, [sl, #20]
   dad62:	4907      	ldr	r1, [pc, #28]	; (dad80 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x4b0>)
   dad64:	4798      	blx	r3
      return kTfLiteError;
   dad66:	2001      	movs	r0, #1
   dad68:	e007      	b.n	dad7a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x4aa>
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, GreaterEqual, requires_broadcast);
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, GreaterEqual, requires_broadcast);
   dad6a:	a822      	add	r0, sp, #136	; 0x88
   dad6c:	f7fb fb6b 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   dad70:	4640      	mov	r0, r8
   dad72:	f7fb fb68 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   dad76:	a818      	add	r0, sp, #96	; 0x60
   dad78:	e7ec      	b.n	dad54 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x484>
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   dad7a:	b02b      	add	sp, #172	; 0xac
   dad7c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dad80:	000e99ef 	.word	0x000e99ef

000dad84 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   dad84:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dad88:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   dad8a:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   dad8c:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   dad8e:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   dad90:	9208      	str	r2, [sp, #32]
   dad92:	4604      	mov	r4, r0
   dad94:	460e      	mov	r6, r1
   dad96:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   dad98:	dd01      	ble.n	dad9e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   dad9a:	f009 fac7 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   dad9e:	683b      	ldr	r3, [r7, #0]
   dada0:	2b04      	cmp	r3, #4
   dada2:	dcfa      	bgt.n	dad9a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   dada4:	6813      	ldr	r3, [r2, #0]
   dada6:	2b04      	cmp	r3, #4
   dada8:	dcf7      	bgt.n	dad9a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   dadaa:	2301      	movs	r3, #1
   dadac:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   dadae:	ad10      	add	r5, sp, #64	; 0x40
   dadb0:	a80b      	add	r0, sp, #44	; 0x2c
   dadb2:	f7fb fb8c 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   dadb6:	ab18      	add	r3, sp, #96	; 0x60
   dadb8:	462a      	mov	r2, r5
   dadba:	4639      	mov	r1, r7
   dadbc:	4630      	mov	r0, r6
   dadbe:	f7fb fe9b 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   dadc2:	6863      	ldr	r3, [r4, #4]
   dadc4:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   dadc6:	68a3      	ldr	r3, [r4, #8]
   dadc8:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   dadca:	68e3      	ldr	r3, [r4, #12]
   dadcc:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   dadce:	6923      	ldr	r3, [r4, #16]
   dadd0:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   dadd2:	6963      	ldr	r3, [r4, #20]
   dadd4:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   dadd6:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   dadd8:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   daddc:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   dadde:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   dade0:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   dade2:	2100      	movs	r1, #0
   dade4:	a80b      	add	r0, sp, #44	; 0x2c
   dade6:	f7fb fb39 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dadea:	4284      	cmp	r4, r0
   dadec:	da59      	bge.n	daea2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   dadee:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   dadf0:	af0b      	add	r7, sp, #44	; 0x2c
   dadf2:	2101      	movs	r1, #1
   dadf4:	4638      	mov	r0, r7
   dadf6:	f7fb fb31 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dadfa:	4285      	cmp	r5, r0
   dadfc:	da4f      	bge.n	dae9e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   dadfe:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   dae00:	2102      	movs	r1, #2
   dae02:	4638      	mov	r0, r7
   dae04:	f7fb fb2a 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dae08:	4286      	cmp	r6, r0
   dae0a:	da46      	bge.n	dae9a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   dae0c:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dae10:	2103      	movs	r1, #3
   dae12:	4638      	mov	r0, r7
   dae14:	f7fb fb22 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dae18:	4580      	cmp	r8, r0
   dae1a:	da3c      	bge.n	dae96 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   dae1c:	f8cd 8000 	str.w	r8, [sp]
   dae20:	4633      	mov	r3, r6
   dae22:	462a      	mov	r2, r5
   dae24:	4621      	mov	r1, r4
   dae26:	9809      	ldr	r0, [sp, #36]	; 0x24
   dae28:	f7fb fc2e 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   dae2c:	9b08      	ldr	r3, [sp, #32]
   dae2e:	f813 9000 	ldrb.w	r9, [r3, r0]
   dae32:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   dae34:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   dae38:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   dae3a:	462a      	mov	r2, r5
   dae3c:	4633      	mov	r3, r6
   dae3e:	4621      	mov	r1, r4
   dae40:	a818      	add	r0, sp, #96	; 0x60
   dae42:	f7fb fc21 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dae46:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   dae48:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dae4a:	f813 b000 	ldrb.w	fp, [r3, r0]
   dae4e:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   dae50:	9903      	ldr	r1, [sp, #12]
   dae52:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dae56:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   dae58:	f7fb fbc4 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dae5c:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   dae60:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   dae62:	9a07      	ldr	r2, [sp, #28]
   dae64:	9906      	ldr	r1, [sp, #24]
   dae66:	4658      	mov	r0, fp
   dae68:	f7fb fbbc 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   dae6c:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   dae70:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   dae72:	4633      	mov	r3, r6
   dae74:	462a      	mov	r2, r5
   dae76:	4621      	mov	r1, r4
   dae78:	4638      	mov	r0, r7
   dae7a:	f7fb fb54 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   dae7e:	45d9      	cmp	r9, fp
   dae80:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   dae82:	bfac      	ite	ge
   dae84:	f04f 0900 	movge.w	r9, #0
   dae88:	f04f 0901 	movlt.w	r9, #1
   dae8c:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dae90:	f108 0801 	add.w	r8, r8, #1
   dae94:	e7bc      	b.n	dae10 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   dae96:	3601      	adds	r6, #1
   dae98:	e7b2      	b.n	dae00 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   dae9a:	3501      	adds	r5, #1
   dae9c:	e7a8      	b.n	dadf0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   dae9e:	3401      	adds	r4, #1
   daea0:	e79f      	b.n	dade2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   daea2:	a80b      	add	r0, sp, #44	; 0x2c
   daea4:	f7fb facf 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   daea8:	b021      	add	sp, #132	; 0x84
   daeaa:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000daeae <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   daeae:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   daeb2:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   daeb4:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   daeb6:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   daeb8:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   daeba:	9208      	str	r2, [sp, #32]
   daebc:	4604      	mov	r4, r0
   daebe:	460e      	mov	r6, r1
   daec0:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   daec2:	dd01      	ble.n	daec8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   daec4:	f009 fa32 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   daec8:	683b      	ldr	r3, [r7, #0]
   daeca:	2b04      	cmp	r3, #4
   daecc:	dcfa      	bgt.n	daec4 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   daece:	6813      	ldr	r3, [r2, #0]
   daed0:	2b04      	cmp	r3, #4
   daed2:	dcf7      	bgt.n	daec4 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   daed4:	2301      	movs	r3, #1
   daed6:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   daed8:	ad10      	add	r5, sp, #64	; 0x40
   daeda:	a80b      	add	r0, sp, #44	; 0x2c
   daedc:	f7fb faf7 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   daee0:	ab18      	add	r3, sp, #96	; 0x60
   daee2:	462a      	mov	r2, r5
   daee4:	4639      	mov	r1, r7
   daee6:	4630      	mov	r0, r6
   daee8:	f7fb fe06 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   daeec:	6863      	ldr	r3, [r4, #4]
   daeee:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   daef0:	68a3      	ldr	r3, [r4, #8]
   daef2:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   daef4:	68e3      	ldr	r3, [r4, #12]
   daef6:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   daef8:	6923      	ldr	r3, [r4, #16]
   daefa:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   daefc:	6963      	ldr	r3, [r4, #20]
   daefe:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   daf00:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   daf02:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   daf06:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   daf08:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   daf0a:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   daf0c:	2100      	movs	r1, #0
   daf0e:	a80b      	add	r0, sp, #44	; 0x2c
   daf10:	f7fb faa4 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   daf14:	4284      	cmp	r4, r0
   daf16:	da59      	bge.n	dafcc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   daf18:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   daf1a:	af0b      	add	r7, sp, #44	; 0x2c
   daf1c:	2101      	movs	r1, #1
   daf1e:	4638      	mov	r0, r7
   daf20:	f7fb fa9c 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   daf24:	4285      	cmp	r5, r0
   daf26:	da4f      	bge.n	dafc8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   daf28:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   daf2a:	2102      	movs	r1, #2
   daf2c:	4638      	mov	r0, r7
   daf2e:	f7fb fa95 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   daf32:	4286      	cmp	r6, r0
   daf34:	da46      	bge.n	dafc4 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   daf36:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   daf3a:	2103      	movs	r1, #3
   daf3c:	4638      	mov	r0, r7
   daf3e:	f7fb fa8d 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   daf42:	4580      	cmp	r8, r0
   daf44:	da3c      	bge.n	dafc0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   daf46:	f8cd 8000 	str.w	r8, [sp]
   daf4a:	4633      	mov	r3, r6
   daf4c:	462a      	mov	r2, r5
   daf4e:	4621      	mov	r1, r4
   daf50:	9809      	ldr	r0, [sp, #36]	; 0x24
   daf52:	f7fb fb99 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   daf56:	9b08      	ldr	r3, [sp, #32]
   daf58:	f913 9000 	ldrsb.w	r9, [r3, r0]
   daf5c:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   daf5e:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   daf62:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   daf64:	462a      	mov	r2, r5
   daf66:	4633      	mov	r3, r6
   daf68:	4621      	mov	r1, r4
   daf6a:	a818      	add	r0, sp, #96	; 0x60
   daf6c:	f7fb fb8c 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   daf70:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   daf72:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   daf74:	f913 b000 	ldrsb.w	fp, [r3, r0]
   daf78:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   daf7a:	9903      	ldr	r1, [sp, #12]
   daf7c:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   daf80:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   daf82:	f7fb fb2f 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   daf86:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   daf8a:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   daf8c:	9a07      	ldr	r2, [sp, #28]
   daf8e:	9906      	ldr	r1, [sp, #24]
   daf90:	4658      	mov	r0, fp
   daf92:	f7fb fb27 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   daf96:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   daf9a:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   daf9c:	4633      	mov	r3, r6
   daf9e:	462a      	mov	r2, r5
   dafa0:	4621      	mov	r1, r4
   dafa2:	4638      	mov	r0, r7
   dafa4:	f7fb fabf 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   dafa8:	45d9      	cmp	r9, fp
   dafaa:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   dafac:	bfac      	ite	ge
   dafae:	f04f 0900 	movge.w	r9, #0
   dafb2:	f04f 0901 	movlt.w	r9, #1
   dafb6:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dafba:	f108 0801 	add.w	r8, r8, #1
   dafbe:	e7bc      	b.n	daf3a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   dafc0:	3601      	adds	r6, #1
   dafc2:	e7b2      	b.n	daf2a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   dafc4:	3501      	adds	r5, #1
   dafc6:	e7a8      	b.n	daf1a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   dafc8:	3401      	adds	r4, #1
   dafca:	e79f      	b.n	daf0c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   dafcc:	a80b      	add	r0, sp, #44	; 0x2c
   dafce:	f7fb fa3a 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   dafd2:	b021      	add	sp, #132	; 0x84
   dafd4:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dafd8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus LessEval(TfLiteContext* context, TfLiteNode* node) {
   dafd8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dafdc:	680a      	ldr	r2, [r1, #0]
   dafde:	f8d0 9008 	ldr.w	r9, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dafe2:	6895      	ldr	r5, [r2, #8]
   dafe4:	4682      	mov	sl, r0
   dafe6:	6850      	ldr	r0, [r2, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dafe8:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dafea:	2338      	movs	r3, #56	; 0x38
   dafec:	fb03 f800 	mul.w	r8, r3, r0
   daff0:	fb03 9505 	mla	r5, r3, r5, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   daff4:	6854      	ldr	r4, [r2, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   daff6:	eb09 0608 	add.w	r6, r9, r8
   daffa:	b0ab      	sub	sp, #172	; 0xac
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   daffc:	4629      	mov	r1, r5
   daffe:	4630      	mov	r0, r6
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   db000:	fb03 9404 	mla	r4, r3, r4, r9
   db004:	f008 feb4 	bl	e3d70 <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
  switch (input1->type) {
   db008:	f819 2008 	ldrb.w	r2, [r9, r8]

TfLiteStatus LessEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   db00c:	f080 0001 	eor.w	r0, r0, #1
  switch (input1->type) {
   db010:	1e53      	subs	r3, r2, #1

TfLiteStatus LessEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   db012:	b2c7      	uxtb	r7, r0
  switch (input1->type) {
   db014:	2b08      	cmp	r3, #8
   db016:	f200 8225 	bhi.w	db464 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x48c>
   db01a:	e8df f013 	tbh	[pc, r3, lsl #1]
   db01e:	0009      	.short	0x0009
   db020:	00f00057 	.word	0x00f00057
   db024:	022300a1 	.word	0x022300a1
   db028:	02230223 	.word	0x02230223
   db02c:	01840223 	.word	0x01840223
   db030:	f10d 0874 	add.w	r8, sp, #116	; 0x74
    case kTfLiteFloat32:
      TF_LITE_COMPARISON(float, Less, requires_broadcast);
   db034:	4631      	mov	r1, r6
   db036:	b1cf      	cbz	r7, db06c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x94>
   db038:	a813      	add	r0, sp, #76	; 0x4c
   db03a:	f7fb fcb4 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db03e:	4629      	mov	r1, r5
   db040:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db042:	6876      	ldr	r6, [r6, #4]
   db044:	f7fb fcaf 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db048:	b105      	cbz	r5, db04c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x74>
   db04a:	686d      	ldr	r5, [r5, #4]
   db04c:	4621      	mov	r1, r4
   db04e:	4640      	mov	r0, r8
   db050:	f7fb fca9 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db054:	b104      	cbz	r4, db058 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x80>
   db056:	6864      	ldr	r4, [r4, #4]
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
   db058:	9402      	str	r4, [sp, #8]
   db05a:	e88d 0120 	stmia.w	sp, {r5, r8}
   db05e:	ab18      	add	r3, sp, #96	; 0x60
   db060:	4632      	mov	r2, r6
   db062:	a913      	add	r1, sp, #76	; 0x4c
   db064:	a822      	add	r0, sp, #136	; 0x88
   db066:	f7fd fd43 	bl	d8af0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   db06a:	e19e      	b.n	db3aa <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   db06c:	a818      	add	r0, sp, #96	; 0x60
   db06e:	f7fb fc9a 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db072:	4629      	mov	r1, r5
   db074:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db076:	6876      	ldr	r6, [r6, #4]
   db078:	f7fb fc95 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db07c:	b105      	cbz	r5, db080 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0xa8>
   db07e:	686d      	ldr	r5, [r5, #4]
   db080:	4621      	mov	r1, r4
   db082:	a822      	add	r0, sp, #136	; 0x88
   db084:	f7fb fc8f 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db088:	b104      	cbz	r4, db08c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0xb4>
   db08a:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db08c:	4641      	mov	r1, r8
   db08e:	aa22      	add	r2, sp, #136	; 0x88
   db090:	a818      	add	r0, sp, #96	; 0x60
   db092:	f7fb fa6a 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   db096:	3c01      	subs	r4, #1
   db098:	4633      	mov	r3, r6
   db09a:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   db09c:	2600      	movs	r6, #0
   db09e:	2700      	movs	r7, #0
   db0a0:	4286      	cmp	r6, r0
   db0a2:	eb77 0201 	sbcs.w	r2, r7, r1
   db0a6:	f280 81e4 	bge.w	db472 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   db0aa:	ecb3 7a01 	vldmia	r3!, {s14}
   db0ae:	ecf5 7a01 	vldmia	r5!, {s15}
   db0b2:	eeb4 7ae7 	vcmpe.f32	s14, s15
   db0b6:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   db0ba:	bf4c      	ite	mi
   db0bc:	2201      	movmi	r2, #1
   db0be:	2200      	movpl	r2, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db0c0:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   db0c2:	f804 2f01 	strb.w	r2, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db0c6:	f147 0700 	adc.w	r7, r7, #0
   db0ca:	e7e9      	b.n	db0a0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0xc8>
   db0cc:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, Less, requires_broadcast);
   db0d0:	4631      	mov	r1, r6
   db0d2:	b1cf      	cbz	r7, db108 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x130>
   db0d4:	a813      	add	r0, sp, #76	; 0x4c
   db0d6:	f7fb fc66 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db0da:	4629      	mov	r1, r5
   db0dc:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db0de:	6876      	ldr	r6, [r6, #4]
   db0e0:	f7fb fc61 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db0e4:	b105      	cbz	r5, db0e8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x110>
   db0e6:	686d      	ldr	r5, [r5, #4]
   db0e8:	4621      	mov	r1, r4
   db0ea:	4640      	mov	r0, r8
   db0ec:	f7fb fc5b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db0f0:	b104      	cbz	r4, db0f4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x11c>
   db0f2:	6864      	ldr	r4, [r4, #4]
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
   db0f4:	9402      	str	r4, [sp, #8]
   db0f6:	e88d 0120 	stmia.w	sp, {r5, r8}
   db0fa:	ab18      	add	r3, sp, #96	; 0x60
   db0fc:	4632      	mov	r2, r6
   db0fe:	a913      	add	r1, sp, #76	; 0x4c
   db100:	a822      	add	r0, sp, #136	; 0x88
   db102:	f7fd fd68 	bl	d8bd6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   db106:	e150      	b.n	db3aa <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   db108:	a818      	add	r0, sp, #96	; 0x60
   db10a:	f7fb fc4c 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db10e:	4629      	mov	r1, r5
   db110:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db112:	6876      	ldr	r6, [r6, #4]
   db114:	f7fb fc47 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db118:	b105      	cbz	r5, db11c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x144>
   db11a:	686d      	ldr	r5, [r5, #4]
   db11c:	4621      	mov	r1, r4
   db11e:	a822      	add	r0, sp, #136	; 0x88
   db120:	f7fb fc41 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db124:	b104      	cbz	r4, db128 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x150>
   db126:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db128:	aa22      	add	r2, sp, #136	; 0x88
   db12a:	4641      	mov	r1, r8
   db12c:	a818      	add	r0, sp, #96	; 0x60
   db12e:	f7fb fa1c 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   db132:	3c01      	subs	r4, #1
   db134:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   db136:	2200      	movs	r2, #0
   db138:	2300      	movs	r3, #0
   db13a:	4282      	cmp	r2, r0
   db13c:	eb73 0701 	sbcs.w	r7, r3, r1
   db140:	f280 8197 	bge.w	db472 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   db144:	f856 7022 	ldr.w	r7, [r6, r2, lsl #2]
   db148:	f855 e022 	ldr.w	lr, [r5, r2, lsl #2]
   db14c:	4577      	cmp	r7, lr
   db14e:	bfac      	ite	ge
   db150:	2700      	movge	r7, #0
   db152:	2701      	movlt	r7, #1
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db154:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   db156:	f804 7f01 	strb.w	r7, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db15a:	f143 0300 	adc.w	r3, r3, #0
   db15e:	e7ec      	b.n	db13a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x162>
   db160:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, Less, requires_broadcast);
   db164:	4631      	mov	r1, r6
   db166:	b1cf      	cbz	r7, db19c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x1c4>
   db168:	a813      	add	r0, sp, #76	; 0x4c
   db16a:	f7fb fc1c 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db16e:	4629      	mov	r1, r5
   db170:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db172:	6876      	ldr	r6, [r6, #4]
   db174:	f7fb fc17 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db178:	b105      	cbz	r5, db17c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x1a4>
   db17a:	686d      	ldr	r5, [r5, #4]
   db17c:	4621      	mov	r1, r4
   db17e:	4640      	mov	r0, r8
   db180:	f7fb fc11 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db184:	b104      	cbz	r4, db188 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x1b0>
   db186:	6864      	ldr	r4, [r4, #4]
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
   db188:	9402      	str	r4, [sp, #8]
   db18a:	e88d 0120 	stmia.w	sp, {r5, r8}
   db18e:	ab18      	add	r3, sp, #96	; 0x60
   db190:	4632      	mov	r2, r6
   db192:	a913      	add	r1, sp, #76	; 0x4c
   db194:	a822      	add	r0, sp, #136	; 0x88
   db196:	f7fd fd8a 	bl	d8cae <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   db19a:	e106      	b.n	db3aa <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   db19c:	a818      	add	r0, sp, #96	; 0x60
   db19e:	f7fb fc02 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db1a2:	4629      	mov	r1, r5
   db1a4:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db1a6:	6876      	ldr	r6, [r6, #4]
   db1a8:	f7fb fbfd 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db1ac:	b105      	cbz	r5, db1b0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x1d8>
   db1ae:	686d      	ldr	r5, [r5, #4]
   db1b0:	4621      	mov	r1, r4
   db1b2:	a822      	add	r0, sp, #136	; 0x88
   db1b4:	f7fb fbf7 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db1b8:	b104      	cbz	r4, db1bc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x1e4>
   db1ba:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db1bc:	aa22      	add	r2, sp, #136	; 0x88
   db1be:	4641      	mov	r1, r8
   db1c0:	a818      	add	r0, sp, #96	; 0x60
   db1c2:	f7fb f9d2 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   db1c6:	3d08      	subs	r5, #8
   db1c8:	17c1      	asrs	r1, r0, #31
   db1ca:	f1a6 0e08 	sub.w	lr, r6, #8
   db1ce:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
   db1d0:	2200      	movs	r2, #0
   db1d2:	2300      	movs	r3, #0
   db1d4:	4282      	cmp	r2, r0
   db1d6:	eb73 0601 	sbcs.w	r6, r3, r1
   db1da:	f280 814a 	bge.w	db472 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   db1de:	e9fe ab02 	ldrd	sl, fp, [lr, #8]!
   db1e2:	e9f5 6702 	ldrd	r6, r7, [r5, #8]!
   db1e6:	45b2      	cmp	sl, r6
   db1e8:	eb7b 0607 	sbcs.w	r6, fp, r7
   db1ec:	bfb4      	ite	lt
   db1ee:	2601      	movlt	r6, #1
   db1f0:	2600      	movge	r6, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db1f2:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   db1f4:	f804 6f01 	strb.w	r6, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db1f8:	f143 0300 	adc.w	r3, r3, #0
   db1fc:	e7ea      	b.n	db1d4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x1fc>
  }
TF_LITE_QUANTIZE_COMPARISON(Equal);
TF_LITE_QUANTIZE_COMPARISON(NotEqual);
TF_LITE_QUANTIZE_COMPARISON(Greater);
TF_LITE_QUANTIZE_COMPARISON(GreaterEqual);
TF_LITE_QUANTIZE_COMPARISON(Less);
   db1fe:	6933      	ldr	r3, [r6, #16]
   db200:	68f0      	ldr	r0, [r6, #12]
   db202:	f1c3 0900 	rsb	r9, r3, #0
   db206:	692b      	ldr	r3, [r5, #16]
   db208:	f1c3 0800 	rsb	r8, r3, #0
   db20c:	f00b fef6 	bl	e6ffc <__aeabi_f2d>
   db210:	ec41 0b10 	vmov	d0, r0, r1
   db214:	a910      	add	r1, sp, #64	; 0x40
   db216:	a80f      	add	r0, sp, #60	; 0x3c
   db218:	f008 fe1e 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   db21c:	68e8      	ldr	r0, [r5, #12]
   db21e:	f00b feed 	bl	e6ffc <__aeabi_f2d>
   db222:	ec41 0b10 	vmov	d0, r0, r1
   db226:	a912      	add	r1, sp, #72	; 0x48
   db228:	a811      	add	r0, sp, #68	; 0x44
   db22a:	f008 fe15 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   db22e:	2308      	movs	r3, #8
   db230:	9322      	str	r3, [sp, #136]	; 0x88
   db232:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   db234:	9324      	str	r3, [sp, #144]	; 0x90
   db236:	9b10      	ldr	r3, [sp, #64]	; 0x40
   db238:	9325      	str	r3, [sp, #148]	; 0x94
   db23a:	9b11      	ldr	r3, [sp, #68]	; 0x44
   db23c:	9327      	str	r3, [sp, #156]	; 0x9c
   db23e:	9b12      	ldr	r3, [sp, #72]	; 0x48
   db240:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   db244:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   db248:	9328      	str	r3, [sp, #160]	; 0xa0
   db24a:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   db24e:	4631      	mov	r1, r6
   db250:	a813      	add	r0, sp, #76	; 0x4c
   db252:	b1bf      	cbz	r7, db284 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x2ac>
   db254:	f7fb fba7 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db258:	4629      	mov	r1, r5
   db25a:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db25c:	6876      	ldr	r6, [r6, #4]
   db25e:	f7fb fba2 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db262:	4621      	mov	r1, r4
   db264:	4640      	mov	r0, r8
   db266:	686d      	ldr	r5, [r5, #4]
   db268:	f7fb fb9d 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db26c:	b104      	cbz	r4, db270 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x298>
   db26e:	6864      	ldr	r4, [r4, #4]
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
   db270:	9402      	str	r4, [sp, #8]
   db272:	e88d 0120 	stmia.w	sp, {r5, r8}
   db276:	ab18      	add	r3, sp, #96	; 0x60
   db278:	4632      	mov	r2, r6
   db27a:	a913      	add	r1, sp, #76	; 0x4c
   db27c:	a822      	add	r0, sp, #136	; 0x88
   db27e:	f7ff fd81 	bl	dad84 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   db282:	e092      	b.n	db3aa <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   db284:	f7fb fb8f 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db288:	6873      	ldr	r3, [r6, #4]
   db28a:	9305      	str	r3, [sp, #20]
   db28c:	4629      	mov	r1, r5
   db28e:	a818      	add	r0, sp, #96	; 0x60
   db290:	f7fb fb89 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db294:	4621      	mov	r1, r4
   db296:	4640      	mov	r0, r8
   db298:	f8d5 b004 	ldr.w	fp, [r5, #4]
   db29c:	f7fb fb83 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db2a0:	b104      	cbz	r4, db2a4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x2cc>
   db2a2:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   db2a4:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   db2a6:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   db2a8:	9b24      	ldr	r3, [sp, #144]	; 0x90
   db2aa:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   db2ac:	9b25      	ldr	r3, [sp, #148]	; 0x94
   db2ae:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   db2b0:	9b26      	ldr	r3, [sp, #152]	; 0x98
   db2b2:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   db2b4:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   db2b6:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db2b8:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   db2ba:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   db2bc:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db2be:	a918      	add	r1, sp, #96	; 0x60
   db2c0:	a813      	add	r0, sp, #76	; 0x4c
   db2c2:	f7fb f952 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   db2c6:	4602      	mov	r2, r0
   db2c8:	17c3      	asrs	r3, r0, #31
   db2ca:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   db2ce:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db2d0:	f04f 0800 	mov.w	r8, #0
   db2d4:	f04f 0900 	mov.w	r9, #0
   db2d8:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   db2dc:	4590      	cmp	r8, r2
   db2de:	eb79 0303 	sbcs.w	r3, r9, r3
   db2e2:	f280 80b4 	bge.w	db44e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x476>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db2e6:	f81b 5008 	ldrb.w	r5, [fp, r8]
   db2ea:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   db2ec:	9a08      	ldr	r2, [sp, #32]
   db2ee:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db2f0:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   db2f2:	9b05      	ldr	r3, [sp, #20]
   db2f4:	f813 0008 	ldrb.w	r0, [r3, r8]
   db2f8:	9b06      	ldr	r3, [sp, #24]
   db2fa:	4418      	add	r0, r3
   db2fc:	40b8      	lsls	r0, r7
   db2fe:	f7fb f971 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db302:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   db304:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   db306:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   db308:	990a      	ldr	r1, [sp, #40]	; 0x28
   db30a:	4628      	mov	r0, r5
   db30c:	f7fb f96a 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   db310:	4582      	cmp	sl, r0
   db312:	bfac      	ite	ge
   db314:	2000      	movge	r0, #0
   db316:	2001      	movlt	r0, #1
   db318:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db31c:	f118 0801 	adds.w	r8, r8, #1
   db320:	f149 0900 	adc.w	r9, r9, #0
   db324:	e7d8      	b.n	db2d8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x300>
   db326:	6933      	ldr	r3, [r6, #16]
   db328:	68f0      	ldr	r0, [r6, #12]
   db32a:	f1c3 0900 	rsb	r9, r3, #0
   db32e:	692b      	ldr	r3, [r5, #16]
   db330:	f1c3 0800 	rsb	r8, r3, #0
   db334:	f00b fe62 	bl	e6ffc <__aeabi_f2d>
   db338:	ec41 0b10 	vmov	d0, r0, r1
   db33c:	a910      	add	r1, sp, #64	; 0x40
   db33e:	a80f      	add	r0, sp, #60	; 0x3c
   db340:	f008 fd8a 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   db344:	68e8      	ldr	r0, [r5, #12]
   db346:	f00b fe59 	bl	e6ffc <__aeabi_f2d>
   db34a:	ec41 0b10 	vmov	d0, r0, r1
   db34e:	a912      	add	r1, sp, #72	; 0x48
   db350:	a811      	add	r0, sp, #68	; 0x44
   db352:	f008 fd81 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   db356:	2308      	movs	r3, #8
   db358:	9322      	str	r3, [sp, #136]	; 0x88
   db35a:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   db35c:	9324      	str	r3, [sp, #144]	; 0x90
   db35e:	9b10      	ldr	r3, [sp, #64]	; 0x40
   db360:	9325      	str	r3, [sp, #148]	; 0x94
   db362:	9b11      	ldr	r3, [sp, #68]	; 0x44
   db364:	9327      	str	r3, [sp, #156]	; 0x9c
   db366:	9b12      	ldr	r3, [sp, #72]	; 0x48
   db368:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   db36c:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   db370:	9328      	str	r3, [sp, #160]	; 0xa0
   db372:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   db376:	4631      	mov	r1, r6
   db378:	a813      	add	r0, sp, #76	; 0x4c
   db37a:	b1c7      	cbz	r7, db3ae <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3d6>
   db37c:	f7fb fb13 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db380:	4629      	mov	r1, r5
   db382:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db384:	6876      	ldr	r6, [r6, #4]
   db386:	f7fb fb0e 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db38a:	4621      	mov	r1, r4
   db38c:	4640      	mov	r0, r8
   db38e:	686d      	ldr	r5, [r5, #4]
   db390:	f7fb fb09 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db394:	b104      	cbz	r4, db398 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3c0>
   db396:	6864      	ldr	r4, [r4, #4]
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
   db398:	9402      	str	r4, [sp, #8]
   db39a:	e88d 0120 	stmia.w	sp, {r5, r8}
   db39e:	ab18      	add	r3, sp, #96	; 0x60
   db3a0:	4632      	mov	r2, r6
   db3a2:	a913      	add	r1, sp, #76	; 0x4c
   db3a4:	a822      	add	r0, sp, #136	; 0x88
   db3a6:	f7ff fd82 	bl	daeae <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   db3aa:	4640      	mov	r0, r8
   db3ac:	e050      	b.n	db450 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x478>
   db3ae:	f7fb fafa 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db3b2:	6873      	ldr	r3, [r6, #4]
   db3b4:	9305      	str	r3, [sp, #20]
   db3b6:	4629      	mov	r1, r5
   db3b8:	a818      	add	r0, sp, #96	; 0x60
   db3ba:	f7fb faf4 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db3be:	4621      	mov	r1, r4
   db3c0:	4640      	mov	r0, r8
   db3c2:	f8d5 b004 	ldr.w	fp, [r5, #4]
   db3c6:	f7fb faee 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db3ca:	b104      	cbz	r4, db3ce <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3f6>
   db3cc:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   db3ce:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   db3d0:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   db3d2:	9b24      	ldr	r3, [sp, #144]	; 0x90
   db3d4:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   db3d6:	9b25      	ldr	r3, [sp, #148]	; 0x94
   db3d8:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   db3da:	9b26      	ldr	r3, [sp, #152]	; 0x98
   db3dc:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   db3de:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   db3e0:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db3e2:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   db3e4:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   db3e6:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db3e8:	a918      	add	r1, sp, #96	; 0x60
   db3ea:	a813      	add	r0, sp, #76	; 0x4c
   db3ec:	f7fb f8bd 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   db3f0:	4602      	mov	r2, r0
   db3f2:	17c3      	asrs	r3, r0, #31
   db3f4:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   db3f8:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db3fa:	f04f 0800 	mov.w	r8, #0
   db3fe:	f04f 0900 	mov.w	r9, #0
   db402:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   db406:	4590      	cmp	r8, r2
   db408:	eb79 0303 	sbcs.w	r3, r9, r3
   db40c:	da1f      	bge.n	db44e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x476>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db40e:	f91b 5008 	ldrsb.w	r5, [fp, r8]
   db412:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   db414:	9a08      	ldr	r2, [sp, #32]
   db416:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db418:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   db41a:	9b05      	ldr	r3, [sp, #20]
   db41c:	f913 0008 	ldrsb.w	r0, [r3, r8]
   db420:	9b06      	ldr	r3, [sp, #24]
   db422:	4418      	add	r0, r3
   db424:	40b8      	lsls	r0, r7
   db426:	f7fb f8dd 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db42a:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   db42c:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   db42e:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   db430:	990a      	ldr	r1, [sp, #40]	; 0x28
   db432:	4628      	mov	r0, r5
   db434:	f7fb f8d6 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   db438:	4582      	cmp	sl, r0
   db43a:	bfac      	ite	ge
   db43c:	2000      	movge	r0, #0
   db43e:	2001      	movlt	r0, #1
   db440:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db444:	f118 0801 	adds.w	r8, r8, #1
   db448:	f149 0900 	adc.w	r9, r9, #0
   db44c:	e7d9      	b.n	db402 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x42a>
   db44e:	a81d      	add	r0, sp, #116	; 0x74
   db450:	f7fa fff9 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   db454:	a818      	add	r0, sp, #96	; 0x60
   db456:	f7fa fff6 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   db45a:	a813      	add	r0, sp, #76	; 0x4c
   db45c:	f7fa fff3 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
   db460:	2000      	movs	r0, #0
   db462:	e00e      	b.n	db482 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x4aa>
                                requires_broadcast);
      break;
    default:
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
   db464:	4650      	mov	r0, sl
   db466:	f8da 3014 	ldr.w	r3, [sl, #20]
   db46a:	4907      	ldr	r1, [pc, #28]	; (db488 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x4b0>)
   db46c:	4798      	blx	r3
      return kTfLiteError;
   db46e:	2001      	movs	r0, #1
   db470:	e007      	b.n	db482 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x4aa>
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, Less, requires_broadcast);
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, Less, requires_broadcast);
   db472:	a822      	add	r0, sp, #136	; 0x88
   db474:	f7fa ffe7 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   db478:	4640      	mov	r0, r8
   db47a:	f7fa ffe4 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   db47e:	a818      	add	r0, sp, #96	; 0x60
   db480:	e7ec      	b.n	db45c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x484>
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   db482:	b02b      	add	sp, #172	; 0xac
   db484:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   db488:	000e99ef 	.word	0x000e99ef

000db48c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   db48c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   db490:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   db492:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   db494:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   db496:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   db498:	9208      	str	r2, [sp, #32]
   db49a:	4604      	mov	r4, r0
   db49c:	460e      	mov	r6, r1
   db49e:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   db4a0:	dd01      	ble.n	db4a6 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   db4a2:	f008 ff43 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   db4a6:	683b      	ldr	r3, [r7, #0]
   db4a8:	2b04      	cmp	r3, #4
   db4aa:	dcfa      	bgt.n	db4a2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   db4ac:	6813      	ldr	r3, [r2, #0]
   db4ae:	2b04      	cmp	r3, #4
   db4b0:	dcf7      	bgt.n	db4a2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   db4b2:	2301      	movs	r3, #1
   db4b4:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   db4b6:	ad10      	add	r5, sp, #64	; 0x40
   db4b8:	a80b      	add	r0, sp, #44	; 0x2c
   db4ba:	f7fb f808 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   db4be:	ab18      	add	r3, sp, #96	; 0x60
   db4c0:	462a      	mov	r2, r5
   db4c2:	4639      	mov	r1, r7
   db4c4:	4630      	mov	r0, r6
   db4c6:	f7fb fb17 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   db4ca:	6863      	ldr	r3, [r4, #4]
   db4cc:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   db4ce:	68a3      	ldr	r3, [r4, #8]
   db4d0:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   db4d2:	68e3      	ldr	r3, [r4, #12]
   db4d4:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   db4d6:	6923      	ldr	r3, [r4, #16]
   db4d8:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   db4da:	6963      	ldr	r3, [r4, #20]
   db4dc:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   db4de:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   db4e0:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   db4e4:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   db4e6:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   db4e8:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   db4ea:	2100      	movs	r1, #0
   db4ec:	a80b      	add	r0, sp, #44	; 0x2c
   db4ee:	f7fa ffb5 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   db4f2:	4284      	cmp	r4, r0
   db4f4:	da59      	bge.n	db5aa <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   db4f6:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   db4f8:	af0b      	add	r7, sp, #44	; 0x2c
   db4fa:	2101      	movs	r1, #1
   db4fc:	4638      	mov	r0, r7
   db4fe:	f7fa ffad 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   db502:	4285      	cmp	r5, r0
   db504:	da4f      	bge.n	db5a6 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   db506:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   db508:	2102      	movs	r1, #2
   db50a:	4638      	mov	r0, r7
   db50c:	f7fa ffa6 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   db510:	4286      	cmp	r6, r0
   db512:	da46      	bge.n	db5a2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   db514:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   db518:	2103      	movs	r1, #3
   db51a:	4638      	mov	r0, r7
   db51c:	f7fa ff9e 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   db520:	4580      	cmp	r8, r0
   db522:	da3c      	bge.n	db59e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   db524:	f8cd 8000 	str.w	r8, [sp]
   db528:	4633      	mov	r3, r6
   db52a:	462a      	mov	r2, r5
   db52c:	4621      	mov	r1, r4
   db52e:	9809      	ldr	r0, [sp, #36]	; 0x24
   db530:	f7fb f8aa 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   db534:	9b08      	ldr	r3, [sp, #32]
   db536:	f813 9000 	ldrb.w	r9, [r3, r0]
   db53a:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   db53c:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   db540:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   db542:	462a      	mov	r2, r5
   db544:	4633      	mov	r3, r6
   db546:	4621      	mov	r1, r4
   db548:	a818      	add	r0, sp, #96	; 0x60
   db54a:	f7fb f89d 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db54e:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   db550:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db552:	f813 b000 	ldrb.w	fp, [r3, r0]
   db556:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   db558:	9903      	ldr	r1, [sp, #12]
   db55a:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db55e:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   db560:	f7fb f840 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db564:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   db568:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   db56a:	9a07      	ldr	r2, [sp, #28]
   db56c:	9906      	ldr	r1, [sp, #24]
   db56e:	4658      	mov	r0, fp
   db570:	f7fb f838 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   db574:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   db578:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   db57a:	4633      	mov	r3, r6
   db57c:	462a      	mov	r2, r5
   db57e:	4621      	mov	r1, r4
   db580:	4638      	mov	r0, r7
   db582:	f7fa ffd0 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   db586:	45d9      	cmp	r9, fp
   db588:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   db58a:	bfcc      	ite	gt
   db58c:	f04f 0900 	movgt.w	r9, #0
   db590:	f04f 0901 	movle.w	r9, #1
   db594:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   db598:	f108 0801 	add.w	r8, r8, #1
   db59c:	e7bc      	b.n	db518 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   db59e:	3601      	adds	r6, #1
   db5a0:	e7b2      	b.n	db508 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   db5a2:	3501      	adds	r5, #1
   db5a4:	e7a8      	b.n	db4f8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   db5a6:	3401      	adds	r4, #1
   db5a8:	e79f      	b.n	db4ea <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   db5aa:	a80b      	add	r0, sp, #44	; 0x2c
   db5ac:	f7fa ff4b 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   db5b0:	b021      	add	sp, #132	; 0x84
   db5b2:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000db5b6 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   db5b6:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   db5ba:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   db5bc:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   db5be:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   db5c0:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   db5c2:	9208      	str	r2, [sp, #32]
   db5c4:	4604      	mov	r4, r0
   db5c6:	460e      	mov	r6, r1
   db5c8:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   db5ca:	dd01      	ble.n	db5d0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   db5cc:	f008 feae 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   db5d0:	683b      	ldr	r3, [r7, #0]
   db5d2:	2b04      	cmp	r3, #4
   db5d4:	dcfa      	bgt.n	db5cc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   db5d6:	6813      	ldr	r3, [r2, #0]
   db5d8:	2b04      	cmp	r3, #4
   db5da:	dcf7      	bgt.n	db5cc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   db5dc:	2301      	movs	r3, #1
   db5de:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   db5e0:	ad10      	add	r5, sp, #64	; 0x40
   db5e2:	a80b      	add	r0, sp, #44	; 0x2c
   db5e4:	f7fa ff73 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   db5e8:	ab18      	add	r3, sp, #96	; 0x60
   db5ea:	462a      	mov	r2, r5
   db5ec:	4639      	mov	r1, r7
   db5ee:	4630      	mov	r0, r6
   db5f0:	f7fb fa82 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   db5f4:	6863      	ldr	r3, [r4, #4]
   db5f6:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   db5f8:	68a3      	ldr	r3, [r4, #8]
   db5fa:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   db5fc:	68e3      	ldr	r3, [r4, #12]
   db5fe:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   db600:	6923      	ldr	r3, [r4, #16]
   db602:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   db604:	6963      	ldr	r3, [r4, #20]
   db606:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   db608:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   db60a:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   db60e:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   db610:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   db612:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   db614:	2100      	movs	r1, #0
   db616:	a80b      	add	r0, sp, #44	; 0x2c
   db618:	f7fa ff20 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   db61c:	4284      	cmp	r4, r0
   db61e:	da59      	bge.n	db6d4 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   db620:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   db622:	af0b      	add	r7, sp, #44	; 0x2c
   db624:	2101      	movs	r1, #1
   db626:	4638      	mov	r0, r7
   db628:	f7fa ff18 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   db62c:	4285      	cmp	r5, r0
   db62e:	da4f      	bge.n	db6d0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   db630:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   db632:	2102      	movs	r1, #2
   db634:	4638      	mov	r0, r7
   db636:	f7fa ff11 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   db63a:	4286      	cmp	r6, r0
   db63c:	da46      	bge.n	db6cc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   db63e:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   db642:	2103      	movs	r1, #3
   db644:	4638      	mov	r0, r7
   db646:	f7fa ff09 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   db64a:	4580      	cmp	r8, r0
   db64c:	da3c      	bge.n	db6c8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   db64e:	f8cd 8000 	str.w	r8, [sp]
   db652:	4633      	mov	r3, r6
   db654:	462a      	mov	r2, r5
   db656:	4621      	mov	r1, r4
   db658:	9809      	ldr	r0, [sp, #36]	; 0x24
   db65a:	f7fb f815 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   db65e:	9b08      	ldr	r3, [sp, #32]
   db660:	f913 9000 	ldrsb.w	r9, [r3, r0]
   db664:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   db666:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   db66a:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   db66c:	462a      	mov	r2, r5
   db66e:	4633      	mov	r3, r6
   db670:	4621      	mov	r1, r4
   db672:	a818      	add	r0, sp, #96	; 0x60
   db674:	f7fb f808 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db678:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   db67a:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db67c:	f913 b000 	ldrsb.w	fp, [r3, r0]
   db680:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   db682:	9903      	ldr	r1, [sp, #12]
   db684:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db688:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   db68a:	f7fa ffab 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db68e:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   db692:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   db694:	9a07      	ldr	r2, [sp, #28]
   db696:	9906      	ldr	r1, [sp, #24]
   db698:	4658      	mov	r0, fp
   db69a:	f7fa ffa3 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   db69e:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   db6a2:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   db6a4:	4633      	mov	r3, r6
   db6a6:	462a      	mov	r2, r5
   db6a8:	4621      	mov	r1, r4
   db6aa:	4638      	mov	r0, r7
   db6ac:	f7fa ff3b 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   db6b0:	45d9      	cmp	r9, fp
   db6b2:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   db6b4:	bfcc      	ite	gt
   db6b6:	f04f 0900 	movgt.w	r9, #0
   db6ba:	f04f 0901 	movle.w	r9, #1
   db6be:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   db6c2:	f108 0801 	add.w	r8, r8, #1
   db6c6:	e7bc      	b.n	db642 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   db6c8:	3601      	adds	r6, #1
   db6ca:	e7b2      	b.n	db632 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   db6cc:	3501      	adds	r5, #1
   db6ce:	e7a8      	b.n	db622 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   db6d0:	3401      	adds	r4, #1
   db6d2:	e79f      	b.n	db614 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   db6d4:	a80b      	add	r0, sp, #44	; 0x2c
   db6d6:	f7fa feb6 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   db6da:	b021      	add	sp, #132	; 0x84
   db6dc:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000db6e0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus LessEqualEval(TfLiteContext* context, TfLiteNode* node) {
   db6e0:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   db6e4:	680a      	ldr	r2, [r1, #0]
   db6e6:	f8d0 9008 	ldr.w	r9, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   db6ea:	6895      	ldr	r5, [r2, #8]
   db6ec:	4682      	mov	sl, r0
   db6ee:	6850      	ldr	r0, [r2, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   db6f0:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   db6f2:	2338      	movs	r3, #56	; 0x38
   db6f4:	fb03 f800 	mul.w	r8, r3, r0
   db6f8:	fb03 9505 	mla	r5, r3, r5, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   db6fc:	6854      	ldr	r4, [r2, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   db6fe:	eb09 0608 	add.w	r6, r9, r8
   db702:	b0ab      	sub	sp, #172	; 0xac
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   db704:	4629      	mov	r1, r5
   db706:	4630      	mov	r0, r6
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   db708:	fb03 9404 	mla	r4, r3, r4, r9
   db70c:	f008 fb30 	bl	e3d70 <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
  switch (input1->type) {
   db710:	f819 2008 	ldrb.w	r2, [r9, r8]

TfLiteStatus LessEqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   db714:	f080 0001 	eor.w	r0, r0, #1
  switch (input1->type) {
   db718:	1e53      	subs	r3, r2, #1

TfLiteStatus LessEqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   db71a:	b2c7      	uxtb	r7, r0
  switch (input1->type) {
   db71c:	2b08      	cmp	r3, #8
   db71e:	f200 8225 	bhi.w	dbb6c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x48c>
   db722:	e8df f013 	tbh	[pc, r3, lsl #1]
   db726:	0009      	.short	0x0009
   db728:	00f00057 	.word	0x00f00057
   db72c:	022300a1 	.word	0x022300a1
   db730:	02230223 	.word	0x02230223
   db734:	01840223 	.word	0x01840223
   db738:	f10d 0874 	add.w	r8, sp, #116	; 0x74
    case kTfLiteFloat32:
      TF_LITE_COMPARISON(float, LessEqual, requires_broadcast);
   db73c:	4631      	mov	r1, r6
   db73e:	b1cf      	cbz	r7, db774 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x94>
   db740:	a813      	add	r0, sp, #76	; 0x4c
   db742:	f7fb f930 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db746:	4629      	mov	r1, r5
   db748:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db74a:	6876      	ldr	r6, [r6, #4]
   db74c:	f7fb f92b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db750:	b105      	cbz	r5, db754 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x74>
   db752:	686d      	ldr	r5, [r5, #4]
   db754:	4621      	mov	r1, r4
   db756:	4640      	mov	r0, r8
   db758:	f7fb f925 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db75c:	b104      	cbz	r4, db760 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x80>
   db75e:	6864      	ldr	r4, [r4, #4]
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
TFLITE_COMPARISON_OP(LessEqual);
   db760:	9402      	str	r4, [sp, #8]
   db762:	e88d 0120 	stmia.w	sp, {r5, r8}
   db766:	ab18      	add	r3, sp, #96	; 0x60
   db768:	4632      	mov	r2, r6
   db76a:	a913      	add	r1, sp, #76	; 0x4c
   db76c:	a822      	add	r0, sp, #136	; 0x88
   db76e:	f7fd fb10 	bl	d8d92 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   db772:	e19e      	b.n	dbab2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   db774:	a818      	add	r0, sp, #96	; 0x60
   db776:	f7fb f916 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db77a:	4629      	mov	r1, r5
   db77c:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db77e:	6876      	ldr	r6, [r6, #4]
   db780:	f7fb f911 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db784:	b105      	cbz	r5, db788 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0xa8>
   db786:	686d      	ldr	r5, [r5, #4]
   db788:	4621      	mov	r1, r4
   db78a:	a822      	add	r0, sp, #136	; 0x88
   db78c:	f7fb f90b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db790:	b104      	cbz	r4, db794 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0xb4>
   db792:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db794:	4641      	mov	r1, r8
   db796:	aa22      	add	r2, sp, #136	; 0x88
   db798:	a818      	add	r0, sp, #96	; 0x60
   db79a:	f7fa fee6 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   db79e:	3c01      	subs	r4, #1
   db7a0:	4633      	mov	r3, r6
   db7a2:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   db7a4:	2600      	movs	r6, #0
   db7a6:	2700      	movs	r7, #0
   db7a8:	4286      	cmp	r6, r0
   db7aa:	eb77 0201 	sbcs.w	r2, r7, r1
   db7ae:	f280 81e4 	bge.w	dbb7a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   db7b2:	ecb3 7a01 	vldmia	r3!, {s14}
   db7b6:	ecf5 7a01 	vldmia	r5!, {s15}
   db7ba:	eeb4 7ae7 	vcmpe.f32	s14, s15
   db7be:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   db7c2:	bf94      	ite	ls
   db7c4:	2201      	movls	r2, #1
   db7c6:	2200      	movhi	r2, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db7c8:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   db7ca:	f804 2f01 	strb.w	r2, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db7ce:	f147 0700 	adc.w	r7, r7, #0
   db7d2:	e7e9      	b.n	db7a8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0xc8>
   db7d4:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, LessEqual, requires_broadcast);
   db7d8:	4631      	mov	r1, r6
   db7da:	b1cf      	cbz	r7, db810 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x130>
   db7dc:	a813      	add	r0, sp, #76	; 0x4c
   db7de:	f7fb f8e2 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db7e2:	4629      	mov	r1, r5
   db7e4:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db7e6:	6876      	ldr	r6, [r6, #4]
   db7e8:	f7fb f8dd 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db7ec:	b105      	cbz	r5, db7f0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x110>
   db7ee:	686d      	ldr	r5, [r5, #4]
   db7f0:	4621      	mov	r1, r4
   db7f2:	4640      	mov	r0, r8
   db7f4:	f7fb f8d7 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db7f8:	b104      	cbz	r4, db7fc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x11c>
   db7fa:	6864      	ldr	r4, [r4, #4]
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
TFLITE_COMPARISON_OP(LessEqual);
   db7fc:	9402      	str	r4, [sp, #8]
   db7fe:	e88d 0120 	stmia.w	sp, {r5, r8}
   db802:	ab18      	add	r3, sp, #96	; 0x60
   db804:	4632      	mov	r2, r6
   db806:	a913      	add	r1, sp, #76	; 0x4c
   db808:	a822      	add	r0, sp, #136	; 0x88
   db80a:	f7fd fb35 	bl	d8e78 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   db80e:	e150      	b.n	dbab2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   db810:	a818      	add	r0, sp, #96	; 0x60
   db812:	f7fb f8c8 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db816:	4629      	mov	r1, r5
   db818:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db81a:	6876      	ldr	r6, [r6, #4]
   db81c:	f7fb f8c3 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db820:	b105      	cbz	r5, db824 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x144>
   db822:	686d      	ldr	r5, [r5, #4]
   db824:	4621      	mov	r1, r4
   db826:	a822      	add	r0, sp, #136	; 0x88
   db828:	f7fb f8bd 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db82c:	b104      	cbz	r4, db830 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x150>
   db82e:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db830:	aa22      	add	r2, sp, #136	; 0x88
   db832:	4641      	mov	r1, r8
   db834:	a818      	add	r0, sp, #96	; 0x60
   db836:	f7fa fe98 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   db83a:	3c01      	subs	r4, #1
   db83c:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   db83e:	2200      	movs	r2, #0
   db840:	2300      	movs	r3, #0
   db842:	4282      	cmp	r2, r0
   db844:	eb73 0701 	sbcs.w	r7, r3, r1
   db848:	f280 8197 	bge.w	dbb7a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   db84c:	f856 7022 	ldr.w	r7, [r6, r2, lsl #2]
   db850:	f855 e022 	ldr.w	lr, [r5, r2, lsl #2]
   db854:	4577      	cmp	r7, lr
   db856:	bfcc      	ite	gt
   db858:	2700      	movgt	r7, #0
   db85a:	2701      	movle	r7, #1
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db85c:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   db85e:	f804 7f01 	strb.w	r7, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db862:	f143 0300 	adc.w	r3, r3, #0
   db866:	e7ec      	b.n	db842 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x162>
   db868:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, LessEqual, requires_broadcast);
   db86c:	4631      	mov	r1, r6
   db86e:	b1cf      	cbz	r7, db8a4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x1c4>
   db870:	a813      	add	r0, sp, #76	; 0x4c
   db872:	f7fb f898 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db876:	4629      	mov	r1, r5
   db878:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db87a:	6876      	ldr	r6, [r6, #4]
   db87c:	f7fb f893 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db880:	b105      	cbz	r5, db884 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x1a4>
   db882:	686d      	ldr	r5, [r5, #4]
   db884:	4621      	mov	r1, r4
   db886:	4640      	mov	r0, r8
   db888:	f7fb f88d 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db88c:	b104      	cbz	r4, db890 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x1b0>
   db88e:	6864      	ldr	r4, [r4, #4]
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
TFLITE_COMPARISON_OP(LessEqual);
   db890:	9402      	str	r4, [sp, #8]
   db892:	e88d 0120 	stmia.w	sp, {r5, r8}
   db896:	ab18      	add	r3, sp, #96	; 0x60
   db898:	4632      	mov	r2, r6
   db89a:	a913      	add	r1, sp, #76	; 0x4c
   db89c:	a822      	add	r0, sp, #136	; 0x88
   db89e:	f7fd fb57 	bl	d8f50 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   db8a2:	e106      	b.n	dbab2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   db8a4:	a818      	add	r0, sp, #96	; 0x60
   db8a6:	f7fb f87e 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db8aa:	4629      	mov	r1, r5
   db8ac:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db8ae:	6876      	ldr	r6, [r6, #4]
   db8b0:	f7fb f879 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db8b4:	b105      	cbz	r5, db8b8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x1d8>
   db8b6:	686d      	ldr	r5, [r5, #4]
   db8b8:	4621      	mov	r1, r4
   db8ba:	a822      	add	r0, sp, #136	; 0x88
   db8bc:	f7fb f873 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db8c0:	b104      	cbz	r4, db8c4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x1e4>
   db8c2:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db8c4:	aa22      	add	r2, sp, #136	; 0x88
   db8c6:	4641      	mov	r1, r8
   db8c8:	a818      	add	r0, sp, #96	; 0x60
   db8ca:	f7fa fe4e 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   db8ce:	3d08      	subs	r5, #8
   db8d0:	17c1      	asrs	r1, r0, #31
   db8d2:	f1a6 0e08 	sub.w	lr, r6, #8
   db8d6:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
   db8d8:	2200      	movs	r2, #0
   db8da:	2300      	movs	r3, #0
   db8dc:	4282      	cmp	r2, r0
   db8de:	eb73 0601 	sbcs.w	r6, r3, r1
   db8e2:	f280 814a 	bge.w	dbb7a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   db8e6:	e9fe 6702 	ldrd	r6, r7, [lr, #8]!
   db8ea:	e9f5 ab02 	ldrd	sl, fp, [r5, #8]!
   db8ee:	45b2      	cmp	sl, r6
   db8f0:	eb7b 0607 	sbcs.w	r6, fp, r7
   db8f4:	bfac      	ite	ge
   db8f6:	2601      	movge	r6, #1
   db8f8:	2600      	movlt	r6, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db8fa:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   db8fc:	f804 6f01 	strb.w	r6, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db900:	f143 0300 	adc.w	r3, r3, #0
   db904:	e7ea      	b.n	db8dc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x1fc>
TF_LITE_QUANTIZE_COMPARISON(Equal);
TF_LITE_QUANTIZE_COMPARISON(NotEqual);
TF_LITE_QUANTIZE_COMPARISON(Greater);
TF_LITE_QUANTIZE_COMPARISON(GreaterEqual);
TF_LITE_QUANTIZE_COMPARISON(Less);
TF_LITE_QUANTIZE_COMPARISON(LessEqual);
   db906:	6933      	ldr	r3, [r6, #16]
   db908:	68f0      	ldr	r0, [r6, #12]
   db90a:	f1c3 0900 	rsb	r9, r3, #0
   db90e:	692b      	ldr	r3, [r5, #16]
   db910:	f1c3 0800 	rsb	r8, r3, #0
   db914:	f00b fb72 	bl	e6ffc <__aeabi_f2d>
   db918:	ec41 0b10 	vmov	d0, r0, r1
   db91c:	a910      	add	r1, sp, #64	; 0x40
   db91e:	a80f      	add	r0, sp, #60	; 0x3c
   db920:	f008 fa9a 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   db924:	68e8      	ldr	r0, [r5, #12]
   db926:	f00b fb69 	bl	e6ffc <__aeabi_f2d>
   db92a:	ec41 0b10 	vmov	d0, r0, r1
   db92e:	a912      	add	r1, sp, #72	; 0x48
   db930:	a811      	add	r0, sp, #68	; 0x44
   db932:	f008 fa91 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   db936:	2308      	movs	r3, #8
   db938:	9322      	str	r3, [sp, #136]	; 0x88
   db93a:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   db93c:	9324      	str	r3, [sp, #144]	; 0x90
   db93e:	9b10      	ldr	r3, [sp, #64]	; 0x40
   db940:	9325      	str	r3, [sp, #148]	; 0x94
   db942:	9b11      	ldr	r3, [sp, #68]	; 0x44
   db944:	9327      	str	r3, [sp, #156]	; 0x9c
   db946:	9b12      	ldr	r3, [sp, #72]	; 0x48
   db948:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   db94c:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   db950:	9328      	str	r3, [sp, #160]	; 0xa0
   db952:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   db956:	4631      	mov	r1, r6
   db958:	a813      	add	r0, sp, #76	; 0x4c
   db95a:	b1bf      	cbz	r7, db98c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x2ac>
   db95c:	f7fb f823 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db960:	4629      	mov	r1, r5
   db962:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db964:	6876      	ldr	r6, [r6, #4]
   db966:	f7fb f81e 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db96a:	4621      	mov	r1, r4
   db96c:	4640      	mov	r0, r8
   db96e:	686d      	ldr	r5, [r5, #4]
   db970:	f7fb f819 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db974:	b104      	cbz	r4, db978 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x298>
   db976:	6864      	ldr	r4, [r4, #4]
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
TFLITE_COMPARISON_OP(LessEqual);
   db978:	9402      	str	r4, [sp, #8]
   db97a:	e88d 0120 	stmia.w	sp, {r5, r8}
   db97e:	ab18      	add	r3, sp, #96	; 0x60
   db980:	4632      	mov	r2, r6
   db982:	a913      	add	r1, sp, #76	; 0x4c
   db984:	a822      	add	r0, sp, #136	; 0x88
   db986:	f7ff fd81 	bl	db48c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   db98a:	e092      	b.n	dbab2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   db98c:	f7fb f80b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db990:	6873      	ldr	r3, [r6, #4]
   db992:	9305      	str	r3, [sp, #20]
   db994:	4629      	mov	r1, r5
   db996:	a818      	add	r0, sp, #96	; 0x60
   db998:	f7fb f805 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db99c:	4621      	mov	r1, r4
   db99e:	4640      	mov	r0, r8
   db9a0:	f8d5 b004 	ldr.w	fp, [r5, #4]
   db9a4:	f7fa ffff 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db9a8:	b104      	cbz	r4, db9ac <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x2cc>
   db9aa:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   db9ac:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   db9ae:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   db9b0:	9b24      	ldr	r3, [sp, #144]	; 0x90
   db9b2:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   db9b4:	9b25      	ldr	r3, [sp, #148]	; 0x94
   db9b6:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   db9b8:	9b26      	ldr	r3, [sp, #152]	; 0x98
   db9ba:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   db9bc:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   db9be:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db9c0:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   db9c2:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   db9c4:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db9c6:	a918      	add	r1, sp, #96	; 0x60
   db9c8:	a813      	add	r0, sp, #76	; 0x4c
   db9ca:	f7fa fdce 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   db9ce:	4602      	mov	r2, r0
   db9d0:	17c3      	asrs	r3, r0, #31
   db9d2:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   db9d6:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db9d8:	f04f 0800 	mov.w	r8, #0
   db9dc:	f04f 0900 	mov.w	r9, #0
   db9e0:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   db9e4:	4590      	cmp	r8, r2
   db9e6:	eb79 0303 	sbcs.w	r3, r9, r3
   db9ea:	f280 80b4 	bge.w	dbb56 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x476>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db9ee:	f81b 5008 	ldrb.w	r5, [fp, r8]
   db9f2:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   db9f4:	9a08      	ldr	r2, [sp, #32]
   db9f6:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db9f8:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   db9fa:	9b05      	ldr	r3, [sp, #20]
   db9fc:	f813 0008 	ldrb.w	r0, [r3, r8]
   dba00:	9b06      	ldr	r3, [sp, #24]
   dba02:	4418      	add	r0, r3
   dba04:	40b8      	lsls	r0, r7
   dba06:	f7fa fded 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dba0a:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dba0c:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   dba0e:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   dba10:	990a      	ldr	r1, [sp, #40]	; 0x28
   dba12:	4628      	mov	r0, r5
   dba14:	f7fa fde6 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   dba18:	4582      	cmp	sl, r0
   dba1a:	bfcc      	ite	gt
   dba1c:	2000      	movgt	r0, #0
   dba1e:	2001      	movle	r0, #1
   dba20:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   dba24:	f118 0801 	adds.w	r8, r8, #1
   dba28:	f149 0900 	adc.w	r9, r9, #0
   dba2c:	e7d8      	b.n	db9e0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x300>
   dba2e:	6933      	ldr	r3, [r6, #16]
   dba30:	68f0      	ldr	r0, [r6, #12]
   dba32:	f1c3 0900 	rsb	r9, r3, #0
   dba36:	692b      	ldr	r3, [r5, #16]
   dba38:	f1c3 0800 	rsb	r8, r3, #0
   dba3c:	f00b fade 	bl	e6ffc <__aeabi_f2d>
   dba40:	ec41 0b10 	vmov	d0, r0, r1
   dba44:	a910      	add	r1, sp, #64	; 0x40
   dba46:	a80f      	add	r0, sp, #60	; 0x3c
   dba48:	f008 fa06 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   dba4c:	68e8      	ldr	r0, [r5, #12]
   dba4e:	f00b fad5 	bl	e6ffc <__aeabi_f2d>
   dba52:	ec41 0b10 	vmov	d0, r0, r1
   dba56:	a912      	add	r1, sp, #72	; 0x48
   dba58:	a811      	add	r0, sp, #68	; 0x44
   dba5a:	f008 f9fd 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   dba5e:	2308      	movs	r3, #8
   dba60:	9322      	str	r3, [sp, #136]	; 0x88
   dba62:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   dba64:	9324      	str	r3, [sp, #144]	; 0x90
   dba66:	9b10      	ldr	r3, [sp, #64]	; 0x40
   dba68:	9325      	str	r3, [sp, #148]	; 0x94
   dba6a:	9b11      	ldr	r3, [sp, #68]	; 0x44
   dba6c:	9327      	str	r3, [sp, #156]	; 0x9c
   dba6e:	9b12      	ldr	r3, [sp, #72]	; 0x48
   dba70:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   dba74:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   dba78:	9328      	str	r3, [sp, #160]	; 0xa0
   dba7a:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   dba7e:	4631      	mov	r1, r6
   dba80:	a813      	add	r0, sp, #76	; 0x4c
   dba82:	b1c7      	cbz	r7, dbab6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d6>
   dba84:	f7fa ff8f 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dba88:	4629      	mov	r1, r5
   dba8a:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dba8c:	6876      	ldr	r6, [r6, #4]
   dba8e:	f7fa ff8a 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dba92:	4621      	mov	r1, r4
   dba94:	4640      	mov	r0, r8
   dba96:	686d      	ldr	r5, [r5, #4]
   dba98:	f7fa ff85 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dba9c:	b104      	cbz	r4, dbaa0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3c0>
   dba9e:	6864      	ldr	r4, [r4, #4]
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
TFLITE_COMPARISON_OP(LessEqual);
   dbaa0:	9402      	str	r4, [sp, #8]
   dbaa2:	e88d 0120 	stmia.w	sp, {r5, r8}
   dbaa6:	ab18      	add	r3, sp, #96	; 0x60
   dbaa8:	4632      	mov	r2, r6
   dbaaa:	a913      	add	r1, sp, #76	; 0x4c
   dbaac:	a822      	add	r0, sp, #136	; 0x88
   dbaae:	f7ff fd82 	bl	db5b6 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   dbab2:	4640      	mov	r0, r8
   dbab4:	e050      	b.n	dbb58 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x478>
   dbab6:	f7fa ff76 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dbaba:	6873      	ldr	r3, [r6, #4]
   dbabc:	9305      	str	r3, [sp, #20]
   dbabe:	4629      	mov	r1, r5
   dbac0:	a818      	add	r0, sp, #96	; 0x60
   dbac2:	f7fa ff70 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dbac6:	4621      	mov	r1, r4
   dbac8:	4640      	mov	r0, r8
   dbaca:	f8d5 b004 	ldr.w	fp, [r5, #4]
   dbace:	f7fa ff6a 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dbad2:	b104      	cbz	r4, dbad6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3f6>
   dbad4:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   dbad6:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   dbad8:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   dbada:	9b24      	ldr	r3, [sp, #144]	; 0x90
   dbadc:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   dbade:	9b25      	ldr	r3, [sp, #148]	; 0x94
   dbae0:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   dbae2:	9b26      	ldr	r3, [sp, #152]	; 0x98
   dbae4:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   dbae6:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   dbae8:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   dbaea:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   dbaec:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   dbaee:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   dbaf0:	a918      	add	r1, sp, #96	; 0x60
   dbaf2:	a813      	add	r0, sp, #76	; 0x4c
   dbaf4:	f7fa fd39 	bl	d656a <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   dbaf8:	4602      	mov	r2, r0
   dbafa:	17c3      	asrs	r3, r0, #31
   dbafc:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   dbb00:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   dbb02:	f04f 0800 	mov.w	r8, #0
   dbb06:	f04f 0900 	mov.w	r9, #0
   dbb0a:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   dbb0e:	4590      	cmp	r8, r2
   dbb10:	eb79 0303 	sbcs.w	r3, r9, r3
   dbb14:	da1f      	bge.n	dbb56 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x476>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dbb16:	f91b 5008 	ldrsb.w	r5, [fp, r8]
   dbb1a:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dbb1c:	9a08      	ldr	r2, [sp, #32]
   dbb1e:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dbb20:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dbb22:	9b05      	ldr	r3, [sp, #20]
   dbb24:	f913 0008 	ldrsb.w	r0, [r3, r8]
   dbb28:	9b06      	ldr	r3, [sp, #24]
   dbb2a:	4418      	add	r0, r3
   dbb2c:	40b8      	lsls	r0, r7
   dbb2e:	f7fa fd59 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dbb32:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dbb34:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   dbb36:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   dbb38:	990a      	ldr	r1, [sp, #40]	; 0x28
   dbb3a:	4628      	mov	r0, r5
   dbb3c:	f7fa fd52 	bl	d65e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   dbb40:	4582      	cmp	sl, r0
   dbb42:	bfcc      	ite	gt
   dbb44:	2000      	movgt	r0, #0
   dbb46:	2001      	movle	r0, #1
   dbb48:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   dbb4c:	f118 0801 	adds.w	r8, r8, #1
   dbb50:	f149 0900 	adc.w	r9, r9, #0
   dbb54:	e7d9      	b.n	dbb0a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x42a>
   dbb56:	a81d      	add	r0, sp, #116	; 0x74
   dbb58:	f7fa fc75 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   dbb5c:	a818      	add	r0, sp, #96	; 0x60
   dbb5e:	f7fa fc72 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   dbb62:	a813      	add	r0, sp, #76	; 0x4c
   dbb64:	f7fa fc6f 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
   dbb68:	2000      	movs	r0, #0
   dbb6a:	e00e      	b.n	dbb8a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x4aa>
                                     requires_broadcast);
      break;
    default:
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
   dbb6c:	4650      	mov	r0, sl
   dbb6e:	f8da 3014 	ldr.w	r3, [sl, #20]
   dbb72:	4907      	ldr	r1, [pc, #28]	; (dbb90 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x4b0>)
   dbb74:	4798      	blx	r3
      return kTfLiteError;
   dbb76:	2001      	movs	r0, #1
   dbb78:	e007      	b.n	dbb8a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x4aa>
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, LessEqual, requires_broadcast);
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, LessEqual, requires_broadcast);
   dbb7a:	a822      	add	r0, sp, #136	; 0x88
   dbb7c:	f7fa fc63 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   dbb80:	4640      	mov	r0, r8
   dbb82:	f7fa fc60 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   dbb86:	a818      	add	r0, sp, #96	; 0x60
   dbb88:	e7ec      	b.n	dbb64 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x484>
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   dbb8a:	b02b      	add	sp, #172	; 0xac
   dbb8c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dbb90:	000e99ef 	.word	0x000e99ef

000dbb94 <_ZN6tflite3ops5micro4conv4InitEP13TfLiteContextPKcj>:
  return kTfLiteOk;
}

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
   dbb94:	2000      	movs	r0, #0
   dbb96:	4770      	bx	lr

000dbb98 <_ZN6tflite3ops5micro4conv4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
   dbb98:	4770      	bx	lr

000dbb9a <_ZN6tflite3ops5micro4conv7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   dbb9a:	2000      	movs	r0, #0
   dbb9c:	4770      	bx	lr

000dbb9e <_ZNK6tflite12RuntimeShape8FlatSizeEv>:
    BuildFrom<const std::initializer_list<int>>(init_list);
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
   dbb9e:	b510      	push	{r4, lr}

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   dbba0:	6801      	ldr	r1, [r0, #0]
   dbba2:	2904      	cmp	r1, #4
   dbba4:	bfcc      	ite	gt
   dbba6:	6843      	ldrgt	r3, [r0, #4]
   dbba8:	1d03      	addle	r3, r0, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dbbaa:	2200      	movs	r2, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
   dbbac:	2001      	movs	r0, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dbbae:	428a      	cmp	r2, r1
   dbbb0:	da04      	bge.n	dbbbc <_ZNK6tflite12RuntimeShape8FlatSizeEv+0x1e>
      buffer_size *= dims_data[i];
   dbbb2:	f853 4022 	ldr.w	r4, [r3, r2, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dbbb6:	3201      	adds	r2, #1
      buffer_size *= dims_data[i];
   dbbb8:	4360      	muls	r0, r4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dbbba:	e7f8      	b.n	dbbae <_ZNK6tflite12RuntimeShape8FlatSizeEv+0x10>
      buffer_size *= dims_data[i];
    }
    return buffer_size;
  }
   dbbbc:	bd10      	pop	{r4, pc}

000dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>:
  return MatchingArraySize(array1, index1, args...);
}

// Get common shape dim, DCHECKing that they all agree.
inline int MatchingDim(const RuntimeShape& shape1, int index1,
                       const RuntimeShape& shape2, int index2) {
   dbbbe:	b570      	push	{r4, r5, r6, lr}
   dbbc0:	4615      	mov	r5, r2
   dbbc2:	461e      	mov	r6, r3
  TFLITE_DCHECK_EQ(shape1.Dims(index1), shape2.Dims(index2));
   dbbc4:	f7fa fc4a 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dbbc8:	4631      	mov	r1, r6
   dbbca:	4604      	mov	r4, r0
   dbbcc:	4628      	mov	r0, r5
   dbbce:	f7fa fc45 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dbbd2:	4284      	cmp	r4, r0
   dbbd4:	d001      	beq.n	dbbda <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i+0x1c>
   dbbd6:	f008 fba9 	bl	e432c <abort>
  return shape1.Dims(index1);
}
   dbbda:	bd70      	pop	{r4, r5, r6, pc}

000dbbdc <_ZN6tflite29MultiplyByQuantizedMultiplierElli>:
  return SaturatingRoundingDoublingHighMul(x * (1 << left_shift),
                                           quantized_multiplier);
}

inline int32 MultiplyByQuantizedMultiplier(int32 x, int32 quantized_multiplier,
                                           int shift) {
   dbbdc:	b570      	push	{r4, r5, r6, lr}
  using gemmlowp::RoundingDivideByPOT;
  using gemmlowp::SaturatingRoundingDoublingHighMul;
  int left_shift = shift > 0 ? shift : 0;
   dbbde:	ea22 74e2 	bic.w	r4, r2, r2, asr #31
  int right_shift = shift > 0 ? 0 : -shift;
   dbbe2:	2a00      	cmp	r2, #0
  return RoundingDivideByPOT(SaturatingRoundingDoublingHighMul(
   dbbe4:	fa00 f004 	lsl.w	r0, r0, r4
inline int32 MultiplyByQuantizedMultiplier(int32 x, int32 quantized_multiplier,
                                           int shift) {
  using gemmlowp::RoundingDivideByPOT;
  using gemmlowp::SaturatingRoundingDoublingHighMul;
  int left_shift = shift > 0 ? shift : 0;
  int right_shift = shift > 0 ? 0 : -shift;
   dbbe8:	bfd4      	ite	le
   dbbea:	4256      	negle	r6, r2
   dbbec:	2600      	movgt	r6, #0
// This function implements the same computation as the ARMv7 NEON VQRDMULH
// instruction.
template <>
inline std::int32_t SaturatingRoundingDoublingHighMul(std::int32_t a,
                                                      std::int32_t b) {
  bool overflow = a == b && a == std::numeric_limits<std::int32_t>::min();
   dbbee:	4288      	cmp	r0, r1
   dbbf0:	d104      	bne.n	dbbfc <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x20>
   dbbf2:	f101 4300 	add.w	r3, r1, #2147483648	; 0x80000000
   dbbf6:	425a      	negs	r2, r3
   dbbf8:	415a      	adcs	r2, r3
   dbbfa:	e000      	b.n	dbbfe <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x22>
   dbbfc:	2200      	movs	r2, #0
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
   dbbfe:	fb80 4501 	smull	r4, r5, r0, r1
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
   dbc02:	2c00      	cmp	r4, #0
   dbc04:	f175 0300 	sbcs.w	r3, r5, #0
   dbc08:	4b18      	ldr	r3, [pc, #96]	; (dbc6c <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x90>)
   dbc0a:	bfa8      	it	ge
   dbc0c:	f04f 4380 	movge.w	r3, #1073741824	; 0x40000000
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
   dbc10:	b97a      	cbnz	r2, dbc32 <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x56>
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
   dbc12:	18e4      	adds	r4, r4, r3
   dbc14:	eb45 75e3 	adc.w	r5, r5, r3, asr #31
   dbc18:	2c00      	cmp	r4, #0
   dbc1a:	f175 0300 	sbcs.w	r3, r5, #0
   dbc1e:	da04      	bge.n	dbc2a <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x4e>
   dbc20:	f06f 4200 	mvn.w	r2, #2147483648	; 0x80000000
   dbc24:	2300      	movs	r3, #0
   dbc26:	18a4      	adds	r4, r4, r2
   dbc28:	415d      	adcs	r5, r3
   dbc2a:	0fe0      	lsrs	r0, r4, #31
   dbc2c:	ea40 0445 	orr.w	r4, r0, r5, lsl #1
   dbc30:	e001      	b.n	dbc36 <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x5a>
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
   dbc32:	f06f 4400 	mvn.w	r4, #2147483648	; 0x80000000
// Correctly-rounded-to-nearest division by a power-of-two.
// Also known as a rounding arithmetic right shift.
template <typename IntegerType>
inline IntegerType RoundingDivideByPOT(IntegerType x, int exponent) {
  assert(exponent >= 0);
  assert(exponent <= 31);
   dbc36:	2e1f      	cmp	r6, #31
   dbc38:	dd06      	ble.n	dbc48 <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x6c>
   dbc3a:	4b0d      	ldr	r3, [pc, #52]	; (dbc70 <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x94>)
   dbc3c:	4a0d      	ldr	r2, [pc, #52]	; (dbc74 <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x98>)
   dbc3e:	480e      	ldr	r0, [pc, #56]	; (dbc78 <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x9c>)
   dbc40:	f240 1167 	movw	r1, #359	; 0x167
   dbc44:	f008 fb82 	bl	e434c <__assert_func>
  const IntegerType mask = Dup<IntegerType>((1ll << exponent) - 1);
   dbc48:	4632      	mov	r2, r6
   dbc4a:	2001      	movs	r0, #1
   dbc4c:	2100      	movs	r1, #0
   dbc4e:	f00b f865 	bl	e6d1c <__aeabi_llsl>
   dbc52:	3801      	subs	r0, #1
  return RoundingDivideByPOT(SaturatingRoundingDoublingHighMul(
                                 x * (1 << left_shift), quantized_multiplier),
                             right_shift);
   dbc54:	ea00 0304 	and.w	r3, r0, r4
   dbc58:	1040      	asrs	r0, r0, #1
   dbc5a:	eb00 70d4 	add.w	r0, r0, r4, lsr #31
   dbc5e:	4134      	asrs	r4, r6
}
   dbc60:	4283      	cmp	r3, r0
   dbc62:	bfd4      	ite	le
   dbc64:	4620      	movle	r0, r4
   dbc66:	1c60      	addgt	r0, r4, #1
   dbc68:	bd70      	pop	{r4, r5, r6, pc}
   dbc6a:	bf00      	nop
   dbc6c:	c0000001 	.word	0xc0000001
   dbc70:	000e9731 	.word	0x000e9731
   dbc74:	000e9b96 	.word	0x000e9b96
   dbc78:	000e9692 	.word	0x000e9692

000dbc7c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_>:
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const float* input_data, const RuntimeShape& filter_shape,
                 const float* filter_data, const RuntimeShape& bias_shape,
                 const float* bias_data, const RuntimeShape& output_shape,
                 float* output_data, const RuntimeShape& im2col_shape,
                 float* im2col_data) {
   dbc7c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dbc80:	ed2d 8b04 	vpush	{d8-d9}
   dbc84:	b09b      	sub	sp, #108	; 0x6c
   dbc86:	4699      	mov	r9, r3
  const int stride_width = params.stride_width;
   dbc88:	f9b0 300a 	ldrsh.w	r3, [r0, #10]
   dbc8c:	930a      	str	r3, [sp, #40]	; 0x28
  const int stride_height = params.stride_height;
   dbc8e:	f9b0 300c 	ldrsh.w	r3, [r0, #12]
   dbc92:	930b      	str	r3, [sp, #44]	; 0x2c
  const int dilation_width_factor = params.dilation_width_factor;
   dbc94:	f9b0 300e 	ldrsh.w	r3, [r0, #14]
   dbc98:	930c      	str	r3, [sp, #48]	; 0x30
  const int dilation_height_factor = params.dilation_height_factor;
   dbc9a:	f9b0 3010 	ldrsh.w	r3, [r0, #16]
   dbc9e:	930d      	str	r3, [sp, #52]	; 0x34
  const int pad_width = params.padding_values.width;
   dbca0:	f9b0 3002 	ldrsh.w	r3, [r0, #2]
   dbca4:	930e      	str	r3, [sp, #56]	; 0x38
  const int pad_height = params.padding_values.height;
   dbca6:	f9b0 3004 	ldrsh.w	r3, [r0, #4]
   dbcaa:	930f      	str	r3, [sp, #60]	; 0x3c
  const float output_activation_min = params.float_activation_min;
  const float output_activation_max = params.float_activation_max;
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dbcac:	680b      	ldr	r3, [r1, #0]
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const float* input_data, const RuntimeShape& filter_shape,
                 const float* filter_data, const RuntimeShape& bias_shape,
                 const float* bias_data, const RuntimeShape& output_shape,
                 float* output_data, const RuntimeShape& im2col_shape,
                 float* im2col_data) {
   dbcae:	9219      	str	r2, [sp, #100]	; 0x64
  const int dilation_height_factor = params.dilation_height_factor;
  const int pad_width = params.padding_values.width;
  const int pad_height = params.padding_values.height;
  const float output_activation_min = params.float_activation_min;
  const float output_activation_max = params.float_activation_max;
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dbcb0:	2b04      	cmp	r3, #4
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const float* input_data, const RuntimeShape& filter_shape,
                 const float* filter_data, const RuntimeShape& bias_shape,
                 const float* bias_data, const RuntimeShape& output_shape,
                 float* output_data, const RuntimeShape& im2col_shape,
                 float* im2col_data) {
   dbcb2:	468b      	mov	fp, r1
   dbcb4:	f8dd a0ac 	ldr.w	sl, [sp, #172]	; 0xac
  const int stride_height = params.stride_height;
  const int dilation_width_factor = params.dilation_width_factor;
  const int dilation_height_factor = params.dilation_height_factor;
  const int pad_width = params.padding_values.width;
  const int pad_height = params.padding_values.height;
  const float output_activation_min = params.float_activation_min;
   dbcb8:	edd0 8a0c 	vldr	s17, [r0, #48]	; 0x30
  const float output_activation_max = params.float_activation_max;
   dbcbc:	ed90 9a0d 	vldr	s18, [r0, #52]	; 0x34
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dbcc0:	d001      	beq.n	dbcc6 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x4a>
   dbcc2:	f008 fb33 	bl	e432c <abort>
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
   dbcc6:	f8d9 3000 	ldr.w	r3, [r9]
   dbcca:	2b04      	cmp	r3, #4
   dbccc:	d1f9      	bne.n	dbcc2 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x46>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   dbcce:	f8da 3000 	ldr.w	r3, [sl]
   dbcd2:	2b04      	cmp	r3, #4
   dbcd4:	d1f5      	bne.n	dbcc2 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x46>

  (void)im2col_data;   // only used in optimized code.
  (void)im2col_shape;  // only used in optimized code.
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dbcd6:	2300      	movs	r3, #0
   dbcd8:	4619      	mov	r1, r3
   dbcda:	4652      	mov	r2, sl
   dbcdc:	4658      	mov	r0, fp
   dbcde:	f7ff ff6e 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   dbce2:	2303      	movs	r3, #3
   dbce4:	4619      	mov	r1, r3
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  (void)im2col_data;   // only used in optimized code.
  (void)im2col_shape;  // only used in optimized code.
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dbce6:	9010      	str	r0, [sp, #64]	; 0x40
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   dbce8:	464a      	mov	r2, r9
   dbcea:	4658      	mov	r0, fp
   dbcec:	f7ff ff67 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
   dbcf0:	2303      	movs	r3, #3
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  (void)im2col_data;   // only used in optimized code.
  (void)im2col_shape;  // only used in optimized code.
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   dbcf2:	9011      	str	r0, [sp, #68]	; 0x44
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
   dbcf4:	4652      	mov	r2, sl
   dbcf6:	2100      	movs	r1, #0
   dbcf8:	4648      	mov	r0, r9
   dbcfa:	f7ff ff60 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  if (bias_data) {
   dbcfe:	9b2a      	ldr	r3, [sp, #168]	; 0xa8

  (void)im2col_data;   // only used in optimized code.
  (void)im2col_shape;  // only used in optimized code.
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
   dbd00:	9009      	str	r0, [sp, #36]	; 0x24
  if (bias_data) {
   dbd02:	b12b      	cbz	r3, dbd10 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x94>
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
   dbd04:	9829      	ldr	r0, [sp, #164]	; 0xa4
   dbd06:	f7ff ff4a 	bl	dbb9e <_ZNK6tflite12RuntimeShape8FlatSizeEv>
   dbd0a:	9b09      	ldr	r3, [sp, #36]	; 0x24
   dbd0c:	4283      	cmp	r3, r0
   dbd0e:	d1d8      	bne.n	dbcc2 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x46>
  }
  const int input_height = input_shape.Dims(1);
   dbd10:	2101      	movs	r1, #1
   dbd12:	4658      	mov	r0, fp
   dbd14:	f7fa fba2 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   dbd18:	2102      	movs	r1, #2
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
   dbd1a:	9012      	str	r0, [sp, #72]	; 0x48
  const int input_width = input_shape.Dims(2);
   dbd1c:	4658      	mov	r0, fp
   dbd1e:	f7fa fb9d 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_height = filter_shape.Dims(1);
   dbd22:	2101      	movs	r1, #1
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   dbd24:	9013      	str	r0, [sp, #76]	; 0x4c
  const int filter_height = filter_shape.Dims(1);
   dbd26:	4648      	mov	r0, r9
   dbd28:	f7fa fb98 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_width = filter_shape.Dims(2);
   dbd2c:	2102      	movs	r1, #2
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
   dbd2e:	9014      	str	r0, [sp, #80]	; 0x50
  const int filter_width = filter_shape.Dims(2);
   dbd30:	4648      	mov	r0, r9
   dbd32:	f7fa fb93 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   dbd36:	2101      	movs	r1, #1
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
   dbd38:	9015      	str	r0, [sp, #84]	; 0x54
  const int output_height = output_shape.Dims(1);
   dbd3a:	4650      	mov	r0, sl
   dbd3c:	f7fa fb8e 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   dbd40:	2102      	movs	r1, #2
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   dbd42:	9016      	str	r0, [sp, #88]	; 0x58
  const int output_width = output_shape.Dims(2);
   dbd44:	4650      	mov	r0, sl
   dbd46:	f7fa fb89 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  for (int batch = 0; batch < batches; ++batch) {
   dbd4a:	2500      	movs	r5, #0
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   dbd4c:	9017      	str	r0, [sp, #92]	; 0x5c
  for (int batch = 0; batch < batches; ++batch) {
   dbd4e:	9b10      	ldr	r3, [sp, #64]	; 0x40
   dbd50:	429d      	cmp	r5, r3
   dbd52:	f280 809c 	bge.w	dbe8e <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x212>
   dbd56:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   dbd58:	425b      	negs	r3, r3
   dbd5a:	9308      	str	r3, [sp, #32]
   dbd5c:	2300      	movs	r3, #0
   dbd5e:	9303      	str	r3, [sp, #12]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   dbd60:	9b03      	ldr	r3, [sp, #12]
   dbd62:	9a16      	ldr	r2, [sp, #88]	; 0x58
   dbd64:	4293      	cmp	r3, r2
   dbd66:	f280 8090 	bge.w	dbe8a <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x20e>
   dbd6a:	9b0e      	ldr	r3, [sp, #56]	; 0x38
   dbd6c:	425b      	negs	r3, r3
   dbd6e:	9307      	str	r3, [sp, #28]
   dbd70:	2300      	movs	r3, #0
   dbd72:	9304      	str	r3, [sp, #16]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   dbd74:	9b04      	ldr	r3, [sp, #16]
   dbd76:	9a17      	ldr	r2, [sp, #92]	; 0x5c
   dbd78:	4293      	cmp	r3, r2
   dbd7a:	da7e      	bge.n	dbe7a <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x1fe>
   dbd7c:	2400      	movs	r4, #0
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dbd7e:	9b09      	ldr	r3, [sp, #36]	; 0x24
   dbd80:	429c      	cmp	r4, r3
   dbd82:	da72      	bge.n	dbe6a <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x1ee>
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          float total = 0.f;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   dbd84:	2300      	movs	r3, #0
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dbd86:	9e08      	ldr	r6, [sp, #32]
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          float total = 0.f;
   dbd88:	ed9f 8a43 	vldr	s16, [pc, #268]	; dbe98 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x21c>
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   dbd8c:	9305      	str	r3, [sp, #20]
   dbd8e:	9b05      	ldr	r3, [sp, #20]
   dbd90:	9a14      	ldr	r2, [sp, #80]	; 0x50
   dbd92:	4293      	cmp	r3, r2
   dbd94:	da42      	bge.n	dbe1c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x1a0>
   dbd96:	2300      	movs	r3, #0
   dbd98:	9f07      	ldr	r7, [sp, #28]
   dbd9a:	9306      	str	r3, [sp, #24]
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   dbd9c:	9b06      	ldr	r3, [sp, #24]
   dbd9e:	9a15      	ldr	r2, [sp, #84]	; 0x54
   dbda0:	4293      	cmp	r3, r2
   dbda2:	da35      	bge.n	dbe10 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x194>
   dbda4:	f04f 0800 	mov.w	r8, #0
              for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
   dbda8:	9b11      	ldr	r3, [sp, #68]	; 0x44
   dbdaa:	4598      	cmp	r8, r3
   dbdac:	da2a      	bge.n	dbe04 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x188>
                const int in_x = in_x_origin + dilation_width_factor * filter_x;
                const int in_y =
                    in_y_origin + dilation_height_factor * filter_y;
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
   dbdae:	2f00      	cmp	r7, #0
   dbdb0:	db25      	blt.n	dbdfe <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x182>
   dbdb2:	9b13      	ldr	r3, [sp, #76]	; 0x4c
   dbdb4:	42bb      	cmp	r3, r7
   dbdb6:	dd22      	ble.n	dbdfe <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x182>
   dbdb8:	2e00      	cmp	r6, #0
   dbdba:	db20      	blt.n	dbdfe <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x182>
   dbdbc:	9b12      	ldr	r3, [sp, #72]	; 0x48
   dbdbe:	42b3      	cmp	r3, r6
   dbdc0:	dd1d      	ble.n	dbdfe <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x182>
                    (in_y < input_height)) {
                  float input_value = input_data[Offset(
   dbdc2:	463b      	mov	r3, r7
   dbdc4:	4632      	mov	r2, r6
   dbdc6:	4629      	mov	r1, r5
   dbdc8:	f8cd 8000 	str.w	r8, [sp]
   dbdcc:	4658      	mov	r0, fp
   dbdce:	f7fa fbaa 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                      input_shape, batch, in_y, in_x, in_channel)];
                  float filter_value =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
   dbdd2:	9b06      	ldr	r3, [sp, #24]
                    in_y_origin + dilation_height_factor * filter_y;
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height)) {
                  float input_value = input_data[Offset(
   dbdd4:	9018      	str	r0, [sp, #96]	; 0x60
                      input_shape, batch, in_y, in_x, in_channel)];
                  float filter_value =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
   dbdd6:	9a05      	ldr	r2, [sp, #20]
   dbdd8:	f8cd 8000 	str.w	r8, [sp]
   dbddc:	4621      	mov	r1, r4
   dbdde:	4648      	mov	r0, r9
   dbde0:	f7fa fba1 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height)) {
                  float input_value = input_data[Offset(
                      input_shape, batch, in_y, in_x, in_channel)];
   dbde4:	9a18      	ldr	r2, [sp, #96]	; 0x60
   dbde6:	9b19      	ldr	r3, [sp, #100]	; 0x64
   dbde8:	eb03 0382 	add.w	r3, r3, r2, lsl #2
                  float filter_value =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
                                         filter_x, in_channel)];
   dbdec:	9a28      	ldr	r2, [sp, #160]	; 0xa0
                  total += (input_value * filter_value);
   dbdee:	ed93 7a00 	vldr	s14, [r3]
                    (in_y < input_height)) {
                  float input_value = input_data[Offset(
                      input_shape, batch, in_y, in_x, in_channel)];
                  float filter_value =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
                                         filter_x, in_channel)];
   dbdf2:	eb02 0080 	add.w	r0, r2, r0, lsl #2
                  total += (input_value * filter_value);
   dbdf6:	edd0 7a00 	vldr	s15, [r0]
   dbdfa:	eea7 8a27 	vfma.f32	s16, s14, s15
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          float total = 0.f;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
              for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
   dbdfe:	f108 0801 	add.w	r8, r8, #1
   dbe02:	e7d1      	b.n	dbda8 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x12c>
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          float total = 0.f;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   dbe04:	9b06      	ldr	r3, [sp, #24]
   dbe06:	3301      	adds	r3, #1
   dbe08:	9306      	str	r3, [sp, #24]
   dbe0a:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   dbe0c:	441f      	add	r7, r3
   dbe0e:	e7c5      	b.n	dbd9c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x120>
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          float total = 0.f;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   dbe10:	9b05      	ldr	r3, [sp, #20]
   dbe12:	3301      	adds	r3, #1
   dbe14:	9305      	str	r3, [sp, #20]
   dbe16:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   dbe18:	441e      	add	r6, r3
   dbe1a:	e7b8      	b.n	dbd8e <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x112>
                }
              }
            }
          }
          float bias_value = 0.0f;
          if (bias_data) {
   dbe1c:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
   dbe1e:	b123      	cbz	r3, dbe2a <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x1ae>
            bias_value = bias_data[out_channel];
   dbe20:	eb03 0384 	add.w	r3, r3, r4, lsl #2
   dbe24:	edd3 9a00 	vldr	s19, [r3]
   dbe28:	e001      	b.n	dbe2e <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x1b2>
                  total += (input_value * filter_value);
                }
              }
            }
          }
          float bias_value = 0.0f;
   dbe2a:	eddf 9a1b 	vldr	s19, [pc, #108]	; dbe98 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x21c>
          if (bias_data) {
            bias_value = bias_data[out_channel];
          }
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
   dbe2e:	9400      	str	r4, [sp, #0]
   dbe30:	9b04      	ldr	r3, [sp, #16]
   dbe32:	9a03      	ldr	r2, [sp, #12]
   dbe34:	4629      	mov	r1, r5
   dbe36:	4650      	mov	r0, sl
   dbe38:	f7fa fb75 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              ActivationFunctionWithMinMax(total + bias_value,
   dbe3c:	ee78 7a29 	vadd.f32	s15, s16, s19
          }
          float bias_value = 0.0f;
          if (bias_data) {
            bias_value = bias_data[out_channel];
          }
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
   dbe40:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
	return __b;
      return __a;
   dbe42:	eef4 8ae7 	vcmpe.f32	s17, s15
   dbe46:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   dbe4a:	bfc8      	it	gt
   dbe4c:	eef0 7a68 	vmovgt.f32	s15, s17
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   dbe50:	eeb4 9a67 	vcmp.f32	s18, s15
   dbe54:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   dbe58:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   dbe5c:	bf48      	it	mi
   dbe5e:	eef0 7a49 	vmovmi.f32	s15, s18
              ActivationFunctionWithMinMax(total + bias_value,
                                           output_activation_min,
                                           output_activation_max);
   dbe62:	edc0 7a00 	vstr	s15, [r0]
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dbe66:	3401      	adds	r4, #1
   dbe68:	e789      	b.n	dbd7e <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x102>
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   dbe6a:	9b04      	ldr	r3, [sp, #16]
   dbe6c:	9a0a      	ldr	r2, [sp, #40]	; 0x28
   dbe6e:	3301      	adds	r3, #1
   dbe70:	9304      	str	r3, [sp, #16]
   dbe72:	9b07      	ldr	r3, [sp, #28]
   dbe74:	4413      	add	r3, r2
   dbe76:	9307      	str	r3, [sp, #28]
   dbe78:	e77c      	b.n	dbd74 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0xf8>
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   dbe7a:	9b03      	ldr	r3, [sp, #12]
   dbe7c:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   dbe7e:	3301      	adds	r3, #1
   dbe80:	9303      	str	r3, [sp, #12]
   dbe82:	9b08      	ldr	r3, [sp, #32]
   dbe84:	4413      	add	r3, r2
   dbe86:	9308      	str	r3, [sp, #32]
   dbe88:	e76a      	b.n	dbd60 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0xe4>
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
   dbe8a:	3501      	adds	r5, #1
   dbe8c:	e75f      	b.n	dbd4e <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0xd2>
                                           output_activation_max);
        }
      }
    }
  }
}
   dbe8e:	b01b      	add	sp, #108	; 0x6c
   dbe90:	ecbd 8b04 	vpop	{d8-d9}
   dbe94:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dbe98:	00000000 	.word	0x00000000

000dbe9c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv>:
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const uint8* input_data, const RuntimeShape& filter_shape,
                 const uint8* filter_data, const RuntimeShape& bias_shape,
                 const int32* bias_data, const RuntimeShape& output_shape,
                 uint8* output_data, const RuntimeShape& im2col_shape,
                 uint8* im2col_data, void* cpu_backend_context) {
   dbe9c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dbea0:	b0a3      	sub	sp, #140	; 0x8c
   dbea2:	469a      	mov	sl, r3
  (void)cpu_backend_context;  // only used in optimized code.
  (void)im2col_data;   // only used in optimized code.
  (void)im2col_shape;  // only used in optimized code.
  const int stride_width = params.stride_width;
   dbea4:	f9b0 300a 	ldrsh.w	r3, [r0, #10]
   dbea8:	930d      	str	r3, [sp, #52]	; 0x34
  const int stride_height = params.stride_height;
   dbeaa:	f9b0 300c 	ldrsh.w	r3, [r0, #12]
   dbeae:	930e      	str	r3, [sp, #56]	; 0x38
  const int dilation_width_factor = params.dilation_width_factor;
   dbeb0:	f9b0 300e 	ldrsh.w	r3, [r0, #14]
   dbeb4:	930f      	str	r3, [sp, #60]	; 0x3c
  const int dilation_height_factor = params.dilation_height_factor;
   dbeb6:	f9b0 3010 	ldrsh.w	r3, [r0, #16]
   dbeba:	9310      	str	r3, [sp, #64]	; 0x40
  const int pad_width = params.padding_values.width;
   dbebc:	f9b0 3002 	ldrsh.w	r3, [r0, #2]
   dbec0:	9311      	str	r3, [sp, #68]	; 0x44
  const int pad_height = params.padding_values.height;
   dbec2:	f9b0 3004 	ldrsh.w	r3, [r0, #4]
   dbec6:	9312      	str	r3, [sp, #72]	; 0x48
  const int32 input_offset = params.input_offset;
   dbec8:	6943      	ldr	r3, [r0, #20]
   dbeca:	9313      	str	r3, [sp, #76]	; 0x4c
  const int32 filter_offset = params.weights_offset;
   dbecc:	6983      	ldr	r3, [r0, #24]
   dbece:	9314      	str	r3, [sp, #80]	; 0x50
  const int32 output_offset = params.output_offset;
   dbed0:	69c3      	ldr	r3, [r0, #28]
   dbed2:	9315      	str	r3, [sp, #84]	; 0x54
  const int32 output_multiplier = params.output_multiplier;
   dbed4:	6a03      	ldr	r3, [r0, #32]
   dbed6:	9316      	str	r3, [sp, #88]	; 0x58
  const int output_shift = params.output_shift;
   dbed8:	6a43      	ldr	r3, [r0, #36]	; 0x24
   dbeda:	9317      	str	r3, [sp, #92]	; 0x5c
  const int32 output_activation_min = params.quantized_activation_min;
   dbedc:	6a83      	ldr	r3, [r0, #40]	; 0x28
   dbede:	930a      	str	r3, [sp, #40]	; 0x28
  const int32 output_activation_max = params.quantized_activation_max;
   dbee0:	6ac3      	ldr	r3, [r0, #44]	; 0x2c
   dbee2:	930b      	str	r3, [sp, #44]	; 0x2c
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const uint8* input_data, const RuntimeShape& filter_shape,
                 const uint8* filter_data, const RuntimeShape& bias_shape,
                 const int32* bias_data, const RuntimeShape& output_shape,
                 uint8* output_data, const RuntimeShape& im2col_shape,
                 uint8* im2col_data, void* cpu_backend_context) {
   dbee4:	9221      	str	r2, [sp, #132]	; 0x84
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   dbee6:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   dbee8:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const uint8* input_data, const RuntimeShape& filter_shape,
                 const uint8* filter_data, const RuntimeShape& bias_shape,
                 const int32* bias_data, const RuntimeShape& output_shape,
                 uint8* output_data, const RuntimeShape& im2col_shape,
                 uint8* im2col_data, void* cpu_backend_context) {
   dbeea:	f8dd 80bc 	ldr.w	r8, [sp, #188]	; 0xbc
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   dbeee:	4293      	cmp	r3, r2
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const uint8* input_data, const RuntimeShape& filter_shape,
                 const uint8* filter_data, const RuntimeShape& bias_shape,
                 const int32* bias_data, const RuntimeShape& output_shape,
                 uint8* output_data, const RuntimeShape& im2col_shape,
                 uint8* im2col_data, void* cpu_backend_context) {
   dbef0:	4689      	mov	r9, r1
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   dbef2:	dd01      	ble.n	dbef8 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x5c>
   dbef4:	f008 fa1a 	bl	e432c <abort>

  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dbef8:	680b      	ldr	r3, [r1, #0]
   dbefa:	2b04      	cmp	r3, #4
   dbefc:	d1fa      	bne.n	dbef4 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x58>
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
   dbefe:	f8da 3000 	ldr.w	r3, [sl]
   dbf02:	2b04      	cmp	r3, #4
   dbf04:	d1f6      	bne.n	dbef4 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x58>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   dbf06:	f8d8 3000 	ldr.w	r3, [r8]
   dbf0a:	2b04      	cmp	r3, #4
   dbf0c:	d1f2      	bne.n	dbef4 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x58>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dbf0e:	2300      	movs	r3, #0
   dbf10:	4619      	mov	r1, r3
   dbf12:	4642      	mov	r2, r8
   dbf14:	4648      	mov	r0, r9
   dbf16:	f7ff fe52 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   dbf1a:	2303      	movs	r3, #3
   dbf1c:	4619      	mov	r1, r3
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);

  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dbf1e:	9018      	str	r0, [sp, #96]	; 0x60
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   dbf20:	4652      	mov	r2, sl
   dbf22:	4648      	mov	r0, r9
   dbf24:	f7ff fe4b 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
   dbf28:	2303      	movs	r3, #3

  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   dbf2a:	9019      	str	r0, [sp, #100]	; 0x64
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
   dbf2c:	4642      	mov	r2, r8
   dbf2e:	2100      	movs	r1, #0
   dbf30:	4650      	mov	r0, sl
   dbf32:	f7ff fe44 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  if (bias_data) {
   dbf36:	9b2e      	ldr	r3, [sp, #184]	; 0xb8
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
   dbf38:	900c      	str	r0, [sp, #48]	; 0x30
  if (bias_data) {
   dbf3a:	b12b      	cbz	r3, dbf48 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0xac>
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
   dbf3c:	982d      	ldr	r0, [sp, #180]	; 0xb4
   dbf3e:	f7ff fe2e 	bl	dbb9e <_ZNK6tflite12RuntimeShape8FlatSizeEv>
   dbf42:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   dbf44:	4283      	cmp	r3, r0
   dbf46:	d1d5      	bne.n	dbef4 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x58>
  }
  const int input_height = input_shape.Dims(1);
   dbf48:	2101      	movs	r1, #1
   dbf4a:	4648      	mov	r0, r9
   dbf4c:	f7fa fa86 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   dbf50:	2102      	movs	r1, #2
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
   dbf52:	901a      	str	r0, [sp, #104]	; 0x68
  const int input_width = input_shape.Dims(2);
   dbf54:	4648      	mov	r0, r9
   dbf56:	f7fa fa81 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_height = filter_shape.Dims(1);
   dbf5a:	2101      	movs	r1, #1
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   dbf5c:	901b      	str	r0, [sp, #108]	; 0x6c
  const int filter_height = filter_shape.Dims(1);
   dbf5e:	4650      	mov	r0, sl
   dbf60:	f7fa fa7c 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_width = filter_shape.Dims(2);
   dbf64:	2102      	movs	r1, #2
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
   dbf66:	901c      	str	r0, [sp, #112]	; 0x70
  const int filter_width = filter_shape.Dims(2);
   dbf68:	4650      	mov	r0, sl
   dbf6a:	f7fa fa77 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   dbf6e:	2101      	movs	r1, #1
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
   dbf70:	901d      	str	r0, [sp, #116]	; 0x74
  const int output_height = output_shape.Dims(1);
   dbf72:	4640      	mov	r0, r8
   dbf74:	f7fa fa72 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   dbf78:	2102      	movs	r1, #2
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   dbf7a:	901e      	str	r0, [sp, #120]	; 0x78
  const int output_width = output_shape.Dims(2);
   dbf7c:	4640      	mov	r0, r8
   dbf7e:	f7fa fa6d 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  for (int batch = 0; batch < batches; ++batch) {
   dbf82:	f04f 0b00 	mov.w	fp, #0
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   dbf86:	901f      	str	r0, [sp, #124]	; 0x7c
  for (int batch = 0; batch < batches; ++batch) {
   dbf88:	9b18      	ldr	r3, [sp, #96]	; 0x60
   dbf8a:	459b      	cmp	fp, r3
   dbf8c:	f280 8093 	bge.w	dc0b6 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x21a>
   dbf90:	9b12      	ldr	r3, [sp, #72]	; 0x48
   dbf92:	425b      	negs	r3, r3
   dbf94:	9309      	str	r3, [sp, #36]	; 0x24
   dbf96:	2300      	movs	r3, #0
   dbf98:	9304      	str	r3, [sp, #16]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   dbf9a:	9b04      	ldr	r3, [sp, #16]
   dbf9c:	9a1e      	ldr	r2, [sp, #120]	; 0x78
   dbf9e:	4293      	cmp	r3, r2
   dbfa0:	f280 8086 	bge.w	dc0b0 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x214>
   dbfa4:	9b11      	ldr	r3, [sp, #68]	; 0x44
   dbfa6:	425b      	negs	r3, r3
   dbfa8:	9308      	str	r3, [sp, #32]
   dbfaa:	2300      	movs	r3, #0
   dbfac:	9305      	str	r3, [sp, #20]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   dbfae:	9b05      	ldr	r3, [sp, #20]
   dbfb0:	9a1f      	ldr	r2, [sp, #124]	; 0x7c
   dbfb2:	4293      	cmp	r3, r2
   dbfb4:	da74      	bge.n	dc0a0 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x204>
   dbfb6:	2400      	movs	r4, #0
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dbfb8:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   dbfba:	429c      	cmp	r4, r3
   dbfbc:	da68      	bge.n	dc090 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x1f4>
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
   dbfbe:	2500      	movs	r5, #0
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dbfc0:	9e09      	ldr	r6, [sp, #36]	; 0x24
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   dbfc2:	9506      	str	r5, [sp, #24]
   dbfc4:	9b06      	ldr	r3, [sp, #24]
   dbfc6:	9a1c      	ldr	r2, [sp, #112]	; 0x70
   dbfc8:	4293      	cmp	r3, r2
   dbfca:	da41      	bge.n	dc050 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x1b4>
   dbfcc:	2300      	movs	r3, #0
   dbfce:	9f08      	ldr	r7, [sp, #32]
   dbfd0:	9307      	str	r3, [sp, #28]
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   dbfd2:	9b07      	ldr	r3, [sp, #28]
   dbfd4:	9a1d      	ldr	r2, [sp, #116]	; 0x74
   dbfd6:	4293      	cmp	r3, r2
   dbfd8:	da34      	bge.n	dc044 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x1a8>
   dbfda:	2300      	movs	r3, #0
   dbfdc:	9303      	str	r3, [sp, #12]
              for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
   dbfde:	9b03      	ldr	r3, [sp, #12]
   dbfe0:	9a19      	ldr	r2, [sp, #100]	; 0x64
   dbfe2:	4293      	cmp	r3, r2
   dbfe4:	da28      	bge.n	dc038 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x19c>
                const int in_x = in_x_origin + dilation_width_factor * filter_x;
                const int in_y =
                    in_y_origin + dilation_height_factor * filter_y;
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
   dbfe6:	2f00      	cmp	r7, #0
   dbfe8:	db23      	blt.n	dc032 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x196>
   dbfea:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
   dbfec:	42bb      	cmp	r3, r7
   dbfee:	dd20      	ble.n	dc032 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x196>
   dbff0:	2e00      	cmp	r6, #0
   dbff2:	db1e      	blt.n	dc032 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x196>
   dbff4:	9b1a      	ldr	r3, [sp, #104]	; 0x68
   dbff6:	42b3      	cmp	r3, r6
   dbff8:	dd1b      	ble.n	dc032 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x196>
                    (in_y < input_height)) {
                  int32 input_val = input_data[Offset(input_shape, batch, in_y,
   dbffa:	9b03      	ldr	r3, [sp, #12]
   dbffc:	9300      	str	r3, [sp, #0]
   dbffe:	4632      	mov	r2, r6
   dc000:	463b      	mov	r3, r7
   dc002:	4659      	mov	r1, fp
   dc004:	4648      	mov	r0, r9
   dc006:	f7fa fa8e 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                                                      in_x, in_channel)];
                  int32 filter_val =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
   dc00a:	9b03      	ldr	r3, [sp, #12]
                    in_y_origin + dilation_height_factor * filter_y;
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height)) {
                  int32 input_val = input_data[Offset(input_shape, batch, in_y,
   dc00c:	9020      	str	r0, [sp, #128]	; 0x80
                                                      in_x, in_channel)];
                  int32 filter_val =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
   dc00e:	9300      	str	r3, [sp, #0]
   dc010:	9a06      	ldr	r2, [sp, #24]
   dc012:	9b07      	ldr	r3, [sp, #28]
   dc014:	4621      	mov	r1, r4
   dc016:	4650      	mov	r0, sl
   dc018:	f7fa fa85 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                                         filter_x, in_channel)];
                  acc +=
                      (filter_val + filter_offset) * (input_val + input_offset);
   dc01c:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   dc01e:	9a14      	ldr	r2, [sp, #80]	; 0x50
   dc020:	5c1b      	ldrb	r3, [r3, r0]
   dc022:	9920      	ldr	r1, [sp, #128]	; 0x80
   dc024:	4413      	add	r3, r2
   dc026:	9a21      	ldr	r2, [sp, #132]	; 0x84
   dc028:	5c52      	ldrb	r2, [r2, r1]
   dc02a:	9913      	ldr	r1, [sp, #76]	; 0x4c
   dc02c:	440a      	add	r2, r1
   dc02e:	fb02 5503 	mla	r5, r2, r3, r5
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
              for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
   dc032:	9b03      	ldr	r3, [sp, #12]
   dc034:	3301      	adds	r3, #1
   dc036:	e7d1      	b.n	dbfdc <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x140>
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   dc038:	9b07      	ldr	r3, [sp, #28]
   dc03a:	3301      	adds	r3, #1
   dc03c:	9307      	str	r3, [sp, #28]
   dc03e:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   dc040:	441f      	add	r7, r3
   dc042:	e7c6      	b.n	dbfd2 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x136>
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   dc044:	9b06      	ldr	r3, [sp, #24]
   dc046:	3301      	adds	r3, #1
   dc048:	9306      	str	r3, [sp, #24]
   dc04a:	9b10      	ldr	r3, [sp, #64]	; 0x40
   dc04c:	441e      	add	r6, r3
   dc04e:	e7b9      	b.n	dbfc4 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x128>
                      (filter_val + filter_offset) * (input_val + input_offset);
                }
              }
            }
          }
          if (bias_data) {
   dc050:	9b2e      	ldr	r3, [sp, #184]	; 0xb8
   dc052:	b113      	cbz	r3, dc05a <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x1be>
            acc += bias_data[out_channel];
   dc054:	f853 3024 	ldr.w	r3, [r3, r4, lsl #2]
   dc058:	441d      	add	r5, r3
          }
          acc = MultiplyByQuantizedMultiplier(acc, output_multiplier,
   dc05a:	9a17      	ldr	r2, [sp, #92]	; 0x5c
   dc05c:	9916      	ldr	r1, [sp, #88]	; 0x58
   dc05e:	4628      	mov	r0, r5
   dc060:	f7ff fdbc 	bl	dbbdc <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
                                              output_shift);
          acc += output_offset;
   dc064:	9b15      	ldr	r3, [sp, #84]	; 0x54
          acc = std::max(acc, output_activation_min);
          acc = std::min(acc, output_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
   dc066:	9400      	str	r4, [sp, #0]
          if (bias_data) {
            acc += bias_data[out_channel];
          }
          acc = MultiplyByQuantizedMultiplier(acc, output_multiplier,
                                              output_shift);
          acc += output_offset;
   dc068:	4418      	add	r0, r3
   dc06a:	9b0a      	ldr	r3, [sp, #40]	; 0x28
          acc = std::max(acc, output_activation_min);
          acc = std::min(acc, output_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
   dc06c:	9a04      	ldr	r2, [sp, #16]
   dc06e:	4283      	cmp	r3, r0
   dc070:	bfb8      	it	lt
   dc072:	4603      	movlt	r3, r0
   dc074:	461d      	mov	r5, r3
   dc076:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   dc078:	429d      	cmp	r5, r3
   dc07a:	bfa8      	it	ge
   dc07c:	461d      	movge	r5, r3
   dc07e:	4659      	mov	r1, fp
   dc080:	9b05      	ldr	r3, [sp, #20]
   dc082:	4640      	mov	r0, r8
   dc084:	f7fa fa4f 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<uint8>(acc);
   dc088:	9b30      	ldr	r3, [sp, #192]	; 0xc0
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dc08a:	3401      	adds	r4, #1
                                              output_shift);
          acc += output_offset;
          acc = std::max(acc, output_activation_min);
          acc = std::min(acc, output_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
              static_cast<uint8>(acc);
   dc08c:	541d      	strb	r5, [r3, r0]
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dc08e:	e793      	b.n	dbfb8 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x11c>
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   dc090:	9b05      	ldr	r3, [sp, #20]
   dc092:	9a0d      	ldr	r2, [sp, #52]	; 0x34
   dc094:	3301      	adds	r3, #1
   dc096:	9305      	str	r3, [sp, #20]
   dc098:	9b08      	ldr	r3, [sp, #32]
   dc09a:	4413      	add	r3, r2
   dc09c:	9308      	str	r3, [sp, #32]
   dc09e:	e786      	b.n	dbfae <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x112>
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   dc0a0:	9b04      	ldr	r3, [sp, #16]
   dc0a2:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   dc0a4:	3301      	adds	r3, #1
   dc0a6:	9304      	str	r3, [sp, #16]
   dc0a8:	9b09      	ldr	r3, [sp, #36]	; 0x24
   dc0aa:	4413      	add	r3, r2
   dc0ac:	9309      	str	r3, [sp, #36]	; 0x24
   dc0ae:	e774      	b.n	dbf9a <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0xfe>
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
   dc0b0:	f10b 0b01 	add.w	fp, fp, #1
   dc0b4:	e768      	b.n	dbf88 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0xec>
              static_cast<uint8>(acc);
        }
      }
    }
  }
}
   dc0b6:	b023      	add	sp, #140	; 0x8c
   dc0b8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dc0bc <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa>:
    const ConvParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
   dc0bc:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dc0c0:	b09d      	sub	sp, #116	; 0x74
   dc0c2:	4699      	mov	r9, r3
  // Get parameters.
  const int32 input_offset = params.input_offset;  // r = s(q - Z)
   dc0c4:	6943      	ldr	r3, [r0, #20]
   dc0c6:	9309      	str	r3, [sp, #36]	; 0x24
  const int stride_width = params.stride_width;
   dc0c8:	f9b0 300a 	ldrsh.w	r3, [r0, #10]
   dc0cc:	930a      	str	r3, [sp, #40]	; 0x28
  const int stride_height = params.stride_height;
   dc0ce:	f9b0 300c 	ldrsh.w	r3, [r0, #12]
   dc0d2:	930b      	str	r3, [sp, #44]	; 0x2c
  const int dilation_width_factor = params.dilation_width_factor;
   dc0d4:	f9b0 300e 	ldrsh.w	r3, [r0, #14]
   dc0d8:	930c      	str	r3, [sp, #48]	; 0x30
  const int dilation_height_factor = params.dilation_height_factor;
   dc0da:	f9b0 3010 	ldrsh.w	r3, [r0, #16]
   dc0de:	930d      	str	r3, [sp, #52]	; 0x34
  const int pad_width = params.padding_values.width;
   dc0e0:	f9b0 3002 	ldrsh.w	r3, [r0, #2]
   dc0e4:	930e      	str	r3, [sp, #56]	; 0x38
  const int pad_height = params.padding_values.height;
   dc0e6:	f9b0 3004 	ldrsh.w	r3, [r0, #4]
   dc0ea:	930f      	str	r3, [sp, #60]	; 0x3c
  const int32 output_offset = params.output_offset;
   dc0ec:	69c3      	ldr	r3, [r0, #28]
   dc0ee:	9310      	str	r3, [sp, #64]	; 0x40
  const int32 output_activation_min = std::numeric_limits<int8_t>::min();
  const int32 output_activation_max = std::numeric_limits<int8_t>::max();

  // Sanity check.
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dc0f0:	f8d9 3000 	ldr.w	r3, [r9]
    const ConvParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
   dc0f4:	911a      	str	r1, [sp, #104]	; 0x68
  const int32 output_activation_min = std::numeric_limits<int8_t>::min();
  const int32 output_activation_max = std::numeric_limits<int8_t>::max();

  // Sanity check.
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dc0f6:	2b04      	cmp	r3, #4
    const ConvParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
   dc0f8:	921b      	str	r2, [sp, #108]	; 0x6c
   dc0fa:	f8dd b09c 	ldr.w	fp, [sp, #156]	; 0x9c
  const int32 output_activation_min = std::numeric_limits<int8_t>::min();
  const int32 output_activation_max = std::numeric_limits<int8_t>::max();

  // Sanity check.
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dc0fe:	d001      	beq.n	dc104 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x48>
   dc100:	f008 f914 	bl	e432c <abort>
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
   dc104:	f8db 3000 	ldr.w	r3, [fp]
   dc108:	2b04      	cmp	r3, #4
   dc10a:	d1f9      	bne.n	dc100 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x44>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   dc10c:	9b2b      	ldr	r3, [sp, #172]	; 0xac
   dc10e:	681b      	ldr	r3, [r3, #0]
   dc110:	2b04      	cmp	r3, #4
   dc112:	d1f5      	bne.n	dc100 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x44>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dc114:	2300      	movs	r3, #0
   dc116:	4619      	mov	r1, r3
   dc118:	9a2b      	ldr	r2, [sp, #172]	; 0xac
   dc11a:	4648      	mov	r0, r9
   dc11c:	f7ff fd4f 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   dc120:	2303      	movs	r3, #3
   dc122:	4619      	mov	r1, r3
  // Sanity check.
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dc124:	9011      	str	r0, [sp, #68]	; 0x44
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   dc126:	465a      	mov	r2, fp
   dc128:	4648      	mov	r0, r9
   dc12a:	f7ff fd48 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
   dc12e:	2303      	movs	r3, #3
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   dc130:	9012      	str	r0, [sp, #72]	; 0x48
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
   dc132:	9a2b      	ldr	r2, [sp, #172]	; 0xac
   dc134:	2100      	movs	r1, #0
   dc136:	4658      	mov	r0, fp
   dc138:	f7ff fd41 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  if (bias_data) {
   dc13c:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
   dc13e:	9008      	str	r0, [sp, #32]
  if (bias_data) {
   dc140:	b12b      	cbz	r3, dc14e <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x92>
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
   dc142:	9829      	ldr	r0, [sp, #164]	; 0xa4
   dc144:	f7ff fd2b 	bl	dbb9e <_ZNK6tflite12RuntimeShape8FlatSizeEv>
   dc148:	9b08      	ldr	r3, [sp, #32]
   dc14a:	4283      	cmp	r3, r0
   dc14c:	d1d8      	bne.n	dc100 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x44>
  }

  // Check dimensions of the tensors.
  const int input_height = input_shape.Dims(1);
   dc14e:	2101      	movs	r1, #1
   dc150:	4648      	mov	r0, r9
   dc152:	f7fa f983 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   dc156:	2102      	movs	r1, #2
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }

  // Check dimensions of the tensors.
  const int input_height = input_shape.Dims(1);
   dc158:	9013      	str	r0, [sp, #76]	; 0x4c
  const int input_width = input_shape.Dims(2);
   dc15a:	4648      	mov	r0, r9
   dc15c:	f7fa f97e 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_height = filter_shape.Dims(1);
   dc160:	2101      	movs	r1, #1
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }

  // Check dimensions of the tensors.
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   dc162:	9014      	str	r0, [sp, #80]	; 0x50
  const int filter_height = filter_shape.Dims(1);
   dc164:	4658      	mov	r0, fp
   dc166:	f7fa f979 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_width = filter_shape.Dims(2);
   dc16a:	2102      	movs	r1, #2
  }

  // Check dimensions of the tensors.
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
   dc16c:	9015      	str	r0, [sp, #84]	; 0x54
  const int filter_width = filter_shape.Dims(2);
   dc16e:	4658      	mov	r0, fp
   dc170:	f7fa f974 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   dc174:	2101      	movs	r1, #1

  // Check dimensions of the tensors.
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
   dc176:	9016      	str	r0, [sp, #88]	; 0x58
  const int output_height = output_shape.Dims(1);
   dc178:	982b      	ldr	r0, [sp, #172]	; 0xac
   dc17a:	f7fa f96f 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   dc17e:	2102      	movs	r1, #2
  // Check dimensions of the tensors.
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   dc180:	9017      	str	r0, [sp, #92]	; 0x5c
  const int output_width = output_shape.Dims(2);
   dc182:	982b      	ldr	r0, [sp, #172]	; 0xac
   dc184:	f7fa f96a 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  for (int batch = 0; batch < batches; ++batch) {
   dc188:	f04f 0a00 	mov.w	sl, #0
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   dc18c:	9018      	str	r0, [sp, #96]	; 0x60
  for (int batch = 0; batch < batches; ++batch) {
   dc18e:	9b11      	ldr	r3, [sp, #68]	; 0x44
   dc190:	459a      	cmp	sl, r3
   dc192:	f280 8091 	bge.w	dc2b8 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1fc>
   dc196:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   dc198:	425b      	negs	r3, r3
   dc19a:	9307      	str	r3, [sp, #28]
   dc19c:	2300      	movs	r3, #0
   dc19e:	9302      	str	r3, [sp, #8]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   dc1a0:	9b02      	ldr	r3, [sp, #8]
   dc1a2:	9a17      	ldr	r2, [sp, #92]	; 0x5c
   dc1a4:	4293      	cmp	r3, r2
   dc1a6:	da6b      	bge.n	dc280 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1c4>
   dc1a8:	9b0e      	ldr	r3, [sp, #56]	; 0x38
   dc1aa:	425b      	negs	r3, r3
   dc1ac:	9306      	str	r3, [sp, #24]
   dc1ae:	2300      	movs	r3, #0
   dc1b0:	9303      	str	r3, [sp, #12]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   dc1b2:	9b03      	ldr	r3, [sp, #12]
   dc1b4:	9a18      	ldr	r2, [sp, #96]	; 0x60
   dc1b6:	4293      	cmp	r3, r2
   dc1b8:	da5a      	bge.n	dc270 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1b4>
   dc1ba:	2400      	movs	r4, #0
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dc1bc:	9b08      	ldr	r3, [sp, #32]
   dc1be:	429c      	cmp	r4, r3
   dc1c0:	da4e      	bge.n	dc260 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1a4>
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
   dc1c2:	2500      	movs	r5, #0
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dc1c4:	9f07      	ldr	r7, [sp, #28]
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   dc1c6:	9504      	str	r5, [sp, #16]
   dc1c8:	9b04      	ldr	r3, [sp, #16]
   dc1ca:	9a15      	ldr	r2, [sp, #84]	; 0x54
   dc1cc:	4293      	cmp	r3, r2
   dc1ce:	da24      	bge.n	dc21a <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x15e>
   dc1d0:	2300      	movs	r3, #0
   dc1d2:	f8dd 8018 	ldr.w	r8, [sp, #24]
   dc1d6:	9305      	str	r3, [sp, #20]
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   dc1d8:	9b05      	ldr	r3, [sp, #20]
   dc1da:	9a16      	ldr	r2, [sp, #88]	; 0x58
   dc1dc:	4293      	cmp	r3, r2
   dc1de:	da16      	bge.n	dc20e <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x152>
   dc1e0:	2600      	movs	r6, #0
              for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
   dc1e2:	9b12      	ldr	r3, [sp, #72]	; 0x48
   dc1e4:	429e      	cmp	r6, r3
   dc1e6:	da0c      	bge.n	dc202 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x146>
                const int in_x = in_x_origin + dilation_width_factor * filter_x;
                const int in_y =
                    in_y_origin + dilation_height_factor * filter_y;
                // Zero padding by omitting the areas outside the image.
                const bool is_point_inside_image =
                    (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
   dc1e8:	f1b8 0f00 	cmp.w	r8, #0
   dc1ec:	db07      	blt.n	dc1fe <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x142>
   dc1ee:	9b14      	ldr	r3, [sp, #80]	; 0x50
   dc1f0:	4543      	cmp	r3, r8
   dc1f2:	dd04      	ble.n	dc1fe <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x142>
   dc1f4:	2f00      	cmp	r7, #0
   dc1f6:	db02      	blt.n	dc1fe <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x142>
   dc1f8:	9b13      	ldr	r3, [sp, #76]	; 0x4c
   dc1fa:	42bb      	cmp	r3, r7
   dc1fc:	dc43      	bgt.n	dc286 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1ca>
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
              for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
   dc1fe:	3601      	adds	r6, #1
   dc200:	e7ef      	b.n	dc1e2 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x126>
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   dc202:	9b05      	ldr	r3, [sp, #20]
   dc204:	3301      	adds	r3, #1
   dc206:	9305      	str	r3, [sp, #20]
   dc208:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   dc20a:	4498      	add	r8, r3
   dc20c:	e7e4      	b.n	dc1d8 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x11c>
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   dc20e:	9b04      	ldr	r3, [sp, #16]
   dc210:	3301      	adds	r3, #1
   dc212:	9304      	str	r3, [sp, #16]
   dc214:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   dc216:	441f      	add	r7, r3
   dc218:	e7d6      	b.n	dc1c8 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x10c>
                }
              }
            }
          }

          if (bias_data) {
   dc21a:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
   dc21c:	b113      	cbz	r3, dc224 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x168>
            acc += bias_data[out_channel];
   dc21e:	f853 3024 	ldr.w	r3, [r3, r4, lsl #2]
   dc222:	441d      	add	r5, r3
          }
          acc = MultiplyByQuantizedMultiplier(
   dc224:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
   dc226:	f853 2024 	ldr.w	r2, [r3, r4, lsl #2]
   dc22a:	9b1a      	ldr	r3, [sp, #104]	; 0x68
   dc22c:	4628      	mov	r0, r5
   dc22e:	f853 1024 	ldr.w	r1, [r3, r4, lsl #2]
   dc232:	f7ff fcd3 	bl	dbbdc <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
              acc, output_multiplier[out_channel], output_shift[out_channel]);
          acc += output_offset;
   dc236:	9b10      	ldr	r3, [sp, #64]	; 0x40
          acc = std::max(acc, output_activation_min);
          acc = std::min(acc, output_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
   dc238:	9400      	str	r4, [sp, #0]
   dc23a:	f06f 057f 	mvn.w	r5, #127	; 0x7f
          if (bias_data) {
            acc += bias_data[out_channel];
          }
          acc = MultiplyByQuantizedMultiplier(
              acc, output_multiplier[out_channel], output_shift[out_channel]);
          acc += output_offset;
   dc23e:	4418      	add	r0, r3
   dc240:	4285      	cmp	r5, r0
   dc242:	bfb8      	it	lt
   dc244:	4605      	movlt	r5, r0
          acc = std::max(acc, output_activation_min);
          acc = std::min(acc, output_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
   dc246:	9b03      	ldr	r3, [sp, #12]
   dc248:	9a02      	ldr	r2, [sp, #8]
   dc24a:	982b      	ldr	r0, [sp, #172]	; 0xac
   dc24c:	4651      	mov	r1, sl
   dc24e:	f7fa f96a 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<int8_t>(acc);
   dc252:	2d7f      	cmp	r5, #127	; 0x7f
   dc254:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   dc256:	bfa8      	it	ge
   dc258:	257f      	movge	r5, #127	; 0x7f
   dc25a:	541d      	strb	r5, [r3, r0]
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dc25c:	3401      	adds	r4, #1
   dc25e:	e7ad      	b.n	dc1bc <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x100>
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   dc260:	9b03      	ldr	r3, [sp, #12]
   dc262:	9a0a      	ldr	r2, [sp, #40]	; 0x28
   dc264:	3301      	adds	r3, #1
   dc266:	9303      	str	r3, [sp, #12]
   dc268:	9b06      	ldr	r3, [sp, #24]
   dc26a:	4413      	add	r3, r2
   dc26c:	9306      	str	r3, [sp, #24]
   dc26e:	e7a0      	b.n	dc1b2 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0xf6>
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   dc270:	9b02      	ldr	r3, [sp, #8]
   dc272:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   dc274:	3301      	adds	r3, #1
   dc276:	9302      	str	r3, [sp, #8]
   dc278:	9b07      	ldr	r3, [sp, #28]
   dc27a:	4413      	add	r3, r2
   dc27c:	9307      	str	r3, [sp, #28]
   dc27e:	e78f      	b.n	dc1a0 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0xe4>
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
   dc280:	f10a 0a01 	add.w	sl, sl, #1
   dc284:	e783      	b.n	dc18e <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0xd2>
                // Zero padding by omitting the areas outside the image.
                const bool is_point_inside_image =
                    (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height);
                if (is_point_inside_image) {
                  int32 input_val = input_data[Offset(input_shape, batch, in_y,
   dc286:	4643      	mov	r3, r8
   dc288:	463a      	mov	r2, r7
   dc28a:	4651      	mov	r1, sl
   dc28c:	9600      	str	r6, [sp, #0]
   dc28e:	4648      	mov	r0, r9
   dc290:	f7fa f949 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                                                      in_x, in_channel)];
                  int32 filter_val =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
   dc294:	9b05      	ldr	r3, [sp, #20]
                // Zero padding by omitting the areas outside the image.
                const bool is_point_inside_image =
                    (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height);
                if (is_point_inside_image) {
                  int32 input_val = input_data[Offset(input_shape, batch, in_y,
   dc296:	9019      	str	r0, [sp, #100]	; 0x64
                                                      in_x, in_channel)];
                  int32 filter_val =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
   dc298:	9a04      	ldr	r2, [sp, #16]
   dc29a:	9600      	str	r6, [sp, #0]
   dc29c:	4621      	mov	r1, r4
   dc29e:	4658      	mov	r0, fp
   dc2a0:	f7fa f941 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                  // long as the filter size (filter_y * filter_x * in_channel)
                  // does not exceed 2^16, which is the case in all the models
                  // we have seen so far.
                  // TODO(jianlijianli): Add a check to make sure the
                  // accumulator depth is smaller than 2^16.
                  acc += filter_val * (input_val + input_offset);
   dc2a4:	9a19      	ldr	r2, [sp, #100]	; 0x64
   dc2a6:	9b26      	ldr	r3, [sp, #152]	; 0x98
   dc2a8:	569b      	ldrsb	r3, [r3, r2]
   dc2aa:	9a09      	ldr	r2, [sp, #36]	; 0x24
   dc2ac:	4413      	add	r3, r2
   dc2ae:	9a28      	ldr	r2, [sp, #160]	; 0xa0
   dc2b0:	5612      	ldrsb	r2, [r2, r0]
   dc2b2:	fb02 5503 	mla	r5, r2, r3, r5
   dc2b6:	e7a2      	b.n	dc1fe <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x142>
              static_cast<int8_t>(acc);
        }
      }
    }
  }
}
   dc2b8:	b01d      	add	sp, #116	; 0x74
   dc2ba:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dc2be <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>:

// Matching GetWindowedOutputSize in TensorFlow.
inline int ComputeOutSize(TfLitePadding padding, int image_size,
                          int filter_size, int stride, int dilation_rate = 1) {
  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;
  switch (padding) {
   dc2be:	2801      	cmp	r0, #1
   dc2c0:	d008      	beq.n	dc2d4 <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii+0x16>
   dc2c2:	2802      	cmp	r0, #2
   dc2c4:	d10b      	bne.n	dc2de <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii+0x20>
    case kTfLitePaddingSame:
      return (image_size + stride - 1) / stride;
    case kTfLitePaddingValid:
      return (image_size + stride - effective_filter_size) / stride;
   dc2c6:	9800      	ldr	r0, [sp, #0]
   dc2c8:	3a01      	subs	r2, #1
   dc2ca:	4350      	muls	r0, r2
   dc2cc:	4419      	add	r1, r3
   dc2ce:	3001      	adds	r0, #1
   dc2d0:	1a09      	subs	r1, r1, r0
   dc2d2:	e001      	b.n	dc2d8 <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii+0x1a>
inline int ComputeOutSize(TfLitePadding padding, int image_size,
                          int filter_size, int stride, int dilation_rate = 1) {
  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;
  switch (padding) {
    case kTfLitePaddingSame:
      return (image_size + stride - 1) / stride;
   dc2d4:	4419      	add	r1, r3
   dc2d6:	3901      	subs	r1, #1
    case kTfLitePaddingValid:
      return (image_size + stride - effective_filter_size) / stride;
   dc2d8:	fb91 f0f3 	sdiv	r0, r1, r3
   dc2dc:	4770      	bx	lr
    default:
      return 0;
   dc2de:	2000      	movs	r0, #0
  }
}
   dc2e0:	4770      	bx	lr
	...

000dc2e4 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE>:

TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, int width, int height,
                             int filter_width, int filter_height, int out_width,
                             int out_height, const TfLiteType data_type,
                             OpData* data) {
   dc2e4:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dc2e8:	469a      	mov	sl, r3
  bool has_bias = node->inputs->size == 3;
   dc2ea:	680b      	ldr	r3, [r1, #0]
  // Check number of inputs/outputs
  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
   dc2ec:	681b      	ldr	r3, [r3, #0]

TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, int width, int height,
                             int filter_width, int filter_height, int out_width,
                             int out_height, const TfLiteType data_type,
                             OpData* data) {
   dc2ee:	b08d      	sub	sp, #52	; 0x34
  bool has_bias = node->inputs->size == 3;
  // Check number of inputs/outputs
  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
   dc2f0:	3b02      	subs	r3, #2
   dc2f2:	2b01      	cmp	r3, #1

TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, int width, int height,
                             int filter_width, int filter_height, int out_width,
                             int out_height, const TfLiteType data_type,
                             OpData* data) {
   dc2f4:	4680      	mov	r8, r0
   dc2f6:	4689      	mov	r9, r1
   dc2f8:	4616      	mov	r6, r2
   dc2fa:	9c1c      	ldr	r4, [sp, #112]	; 0x70
  bool has_bias = node->inputs->size == 3;
  // Check number of inputs/outputs
  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
   dc2fc:	d908      	bls.n	dc310 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x2c>
   dc2fe:	4b49      	ldr	r3, [pc, #292]	; (dc424 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x140>)
   dc300:	9300      	str	r3, [sp, #0]
   dc302:	6944      	ldr	r4, [r0, #20]
   dc304:	4a48      	ldr	r2, [pc, #288]	; (dc428 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x144>)
   dc306:	4949      	ldr	r1, [pc, #292]	; (dc42c <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x148>)
   dc308:	234f      	movs	r3, #79	; 0x4f
   dc30a:	47a0      	blx	r4
   dc30c:	2001      	movs	r0, #1
   dc30e:	e085      	b.n	dc41c <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x138>
  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);
   dc310:	684b      	ldr	r3, [r1, #4]
   dc312:	681b      	ldr	r3, [r3, #0]
   dc314:	2b01      	cmp	r3, #1
   dc316:	d00c      	beq.n	dc332 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x4e>
   dc318:	9302      	str	r3, [sp, #8]
   dc31a:	4b45      	ldr	r3, [pc, #276]	; (dc430 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x14c>)
   dc31c:	9301      	str	r3, [sp, #4]
   dc31e:	2401      	movs	r4, #1
   dc320:	4b44      	ldr	r3, [pc, #272]	; (dc434 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x150>)
   dc322:	9300      	str	r3, [sp, #0]
   dc324:	9403      	str	r4, [sp, #12]
   dc326:	6945      	ldr	r5, [r0, #20]
   dc328:	4a3f      	ldr	r2, [pc, #252]	; (dc428 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x144>)
   dc32a:	4943      	ldr	r1, [pc, #268]	; (dc438 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x154>)
   dc32c:	2350      	movs	r3, #80	; 0x50
   dc32e:	47a8      	blx	r5
   dc330:	e7ec      	b.n	dc30c <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x28>

  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
   dc332:	f892 b000 	ldrb.w	fp, [r2]
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
   dc336:	6893      	ldr	r3, [r2, #8]
   dc338:	68d5      	ldr	r5, [r2, #12]
   dc33a:	6917      	ldr	r7, [r2, #16]
   dc33c:	9309      	str	r3, [sp, #36]	; 0x24

inline TfLitePaddingValues ComputePaddingHeightWidth(
    int stride_height, int stride_width, int dilation_rate_height,
    int dilation_rate_width, int in_height, int in_width, int filter_height,
    int filter_width, TfLitePadding padding, int* out_height, int* out_width) {
  *out_width = ComputeOutSize(padding, in_width, filter_width, stride_width,
   dc33e:	4651      	mov	r1, sl
   dc340:	6853      	ldr	r3, [r2, #4]
   dc342:	9500      	str	r5, [sp, #0]
   dc344:	9a17      	ldr	r2, [sp, #92]	; 0x5c
   dc346:	930a      	str	r3, [sp, #40]	; 0x28
   dc348:	4658      	mov	r0, fp
   dc34a:	f7ff ffb8 	bl	dc2be <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>
                              dilation_rate_width);
  *out_height = ComputeOutSize(padding, in_height, filter_height, stride_height,
   dc34e:	9700      	str	r7, [sp, #0]

inline TfLitePaddingValues ComputePaddingHeightWidth(
    int stride_height, int stride_width, int dilation_rate_height,
    int dilation_rate_width, int in_height, int in_width, int filter_height,
    int filter_width, TfLitePadding padding, int* out_height, int* out_width) {
  *out_width = ComputeOutSize(padding, in_width, filter_width, stride_width,
   dc350:	900b      	str	r0, [sp, #44]	; 0x2c
                              dilation_rate_width);
  *out_height = ComputeOutSize(padding, in_height, filter_height, stride_height,
   dc352:	9b09      	ldr	r3, [sp, #36]	; 0x24
   dc354:	9a18      	ldr	r2, [sp, #96]	; 0x60
   dc356:	9916      	ldr	r1, [sp, #88]	; 0x58
   dc358:	4658      	mov	r0, fp
   dc35a:	f7ff ffb0 	bl	dc2be <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>
inline int ComputePaddingWithOffset(int stride, int dilation_rate, int in_size,
                                    int filter_size, int out_size,
                                    int* offset) {
  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;
  int total_padding =
      ((out_size - 1) * stride + effective_filter_size - in_size);
   dc35e:	9b18      	ldr	r3, [sp, #96]	; 0x60
   dc360:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   dc362:	3b01      	subs	r3, #1
   dc364:	435f      	muls	r7, r3
   dc366:	9b09      	ldr	r3, [sp, #36]	; 0x24
   dc368:	3701      	adds	r7, #1
   dc36a:	3801      	subs	r0, #1
   dc36c:	fb03 7000 	mla	r0, r3, r0, r7
   dc370:	9b16      	ldr	r3, [sp, #88]	; 0x58
   dc372:	1ac0      	subs	r0, r0, r3
   dc374:	9b17      	ldr	r3, [sp, #92]	; 0x5c
   dc376:	3b01      	subs	r3, #1
   dc378:	436b      	muls	r3, r5
   dc37a:	1e55      	subs	r5, r2, #1
   dc37c:	9a0a      	ldr	r2, [sp, #40]	; 0x28
   dc37e:	3301      	adds	r3, #1
   dc380:	fb02 3505 	mla	r5, r2, r5, r3
   dc384:	ebca 0a05 	rsb	sl, sl, r5
  total_padding = total_padding > 0 ? total_padding : 0;
   dc388:	ea2a 7aea 	bic.w	sl, sl, sl, asr #31
   dc38c:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
   dc390:	ea4f 036a 	mov.w	r3, sl, asr #1
   dc394:	6023      	str	r3, [r4, #0]
   dc396:	1043      	asrs	r3, r0, #1
   dc398:	6063      	str	r3, [r4, #4]

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training.
  if (data_type != kTfLiteFloat32) {
   dc39a:	f89d 306c 	ldrb.w	r3, [sp, #108]	; 0x6c
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
   dc39e:	f00a 0501 	and.w	r5, sl, #1
   dc3a2:	f000 0001 	and.w	r0, r0, #1

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training.
  if (data_type != kTfLiteFloat32) {
   dc3a6:	2b01      	cmp	r3, #1
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
   dc3a8:	60a5      	str	r5, [r4, #8]
   dc3aa:	60e0      	str	r0, [r4, #12]

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training.
  if (data_type != kTfLiteFloat32) {
   dc3ac:	d035      	beq.n	dc41a <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x136>
   dc3ae:	f8d9 5000 	ldr.w	r5, [r9]
   dc3b2:	f8d8 0008 	ldr.w	r0, [r8, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dc3b6:	6869      	ldr	r1, [r5, #4]
   dc3b8:	68aa      	ldr	r2, [r5, #8]
}

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
   dc3ba:	68ed      	ldr	r5, [r5, #12]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dc3bc:	2338      	movs	r3, #56	; 0x38

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
   dc3be:	1c6f      	adds	r7, r5, #1
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dc3c0:	fb03 0101 	mla	r1, r3, r1, r0
   dc3c4:	fb03 0202 	mla	r2, r3, r2, r0
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dc3c8:	bf18      	it	ne
   dc3ca:	fb03 0305 	mlane	r3, r3, r5, r0
    const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
    const TfLiteTensor* bias =
        GetOptionalInputTensor(context, node, kBiasTensor);
    TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
   dc3ce:	f504 758c 	add.w	r5, r4, #280	; 0x118
   dc3d2:	9507      	str	r5, [sp, #28]
   dc3d4:	f104 0518 	add.w	r5, r4, #24
   dc3d8:	9506      	str	r5, [sp, #24]
   dc3da:	f504 7507 	add.w	r5, r4, #540	; 0x21c
   dc3de:	9505      	str	r5, [sp, #20]
   dc3e0:	f504 7506 	add.w	r5, r4, #536	; 0x218
   dc3e4:	9504      	str	r5, [sp, #16]
   dc3e6:	f104 0514 	add.w	r5, r4, #20
   dc3ea:	f104 0410 	add.w	r4, r4, #16
   dc3ee:	9402      	str	r4, [sp, #8]
   dc3f0:	f106 0614 	add.w	r6, r6, #20
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dc3f4:	f8d9 4004 	ldr.w	r4, [r9, #4]
   dc3f8:	9503      	str	r5, [sp, #12]
   dc3fa:	9601      	str	r6, [sp, #4]
   dc3fc:	6864      	ldr	r4, [r4, #4]
   dc3fe:	f04f 0538 	mov.w	r5, #56	; 0x38
   dc402:	fb05 0004 	mla	r0, r5, r4, r0
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
  }
  return nullptr;
   dc406:	bf08      	it	eq
   dc408:	2300      	moveq	r3, #0
   dc40a:	9000      	str	r0, [sp, #0]
   dc40c:	4640      	mov	r0, r8
   dc40e:	f007 fbb5 	bl	e3b7c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_>
                             int filter_width, int filter_height, int out_width,
                             int out_height, const TfLiteType data_type,
                             OpData* data) {
  bool has_bias = node->inputs->size == 3;
  // Check number of inputs/outputs
  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
   dc412:	3000      	adds	r0, #0
   dc414:	bf18      	it	ne
   dc416:	2001      	movne	r0, #1
   dc418:	e000      	b.n	dc41c <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x138>
        &data->output_multiplier, &data->output_shift,
        &data->output_activation_min, &data->output_activation_max,
        data->per_channel_output_multiplier,
        reinterpret_cast<int*>(data->per_channel_output_shift)));
  }
  return kTfLiteOk;
   dc41a:	2000      	movs	r0, #0
}
   dc41c:	b00d      	add	sp, #52	; 0x34
   dc41e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dc422:	bf00      	nop
   dc424:	000e9adf 	.word	0x000e9adf
   dc428:	000e9a22 	.word	0x000e9a22
   dc42c:	000e9ac8 	.word	0x000e9ac8
   dc430:	000eb2c5 	.word	0x000eb2c5
   dc434:	000e9b03 	.word	0x000e9b03
   dc438:	000e98f8 	.word	0x000e98f8

000dc43c <_ZN6tflite3ops5micro4conv13EvalQuantizedEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_>:

void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
   dc43c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dc440:	b0b1      	sub	sp, #196	; 0xc4
  int32_t output_activation_min;
  int32_t output_activation_max;
};

inline PaddingType RuntimePaddingType(TfLitePadding padding) {
  switch (padding) {
   dc442:	7810      	ldrb	r0, [r2, #0]

void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
   dc444:	f8dd 80e8 	ldr.w	r8, [sp, #232]	; 0xe8
   dc448:	9f3b      	ldr	r7, [sp, #236]	; 0xec
  const int32_t input_offset = -input->params.zero_point;
   dc44a:	f8d8 1010 	ldr.w	r1, [r8, #16]

void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
   dc44e:	9e3f      	ldr	r6, [sp, #252]	; 0xfc
   dc450:	9c3c      	ldr	r4, [sp, #240]	; 0xf0
   dc452:	9d3d      	ldr	r5, [sp, #244]	; 0xf4
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;
   dc454:	f8d6 c010 	ldr.w	ip, [r6, #16]
void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
   dc458:	f1c1 0e00 	rsb	lr, r1, #0
  const int32_t filter_offset = -filter->params.zero_point;
   dc45c:	6939      	ldr	r1, [r7, #16]
  int32_t output_activation_min;
  int32_t output_activation_max;
};

inline PaddingType RuntimePaddingType(TfLitePadding padding) {
  switch (padding) {
   dc45e:	2801      	cmp	r0, #1
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
   dc460:	f1c1 0100 	rsb	r1, r1, #0
  int32_t output_activation_min;
  int32_t output_activation_max;
};

inline PaddingType RuntimePaddingType(TfLitePadding padding) {
  switch (padding) {
   dc464:	d003      	beq.n	dc46e <_ZN6tflite3ops5micro4conv13EvalQuantizedEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x32>
      return PaddingType::kSame;
    case TfLitePadding::kTfLitePaddingValid:
      return PaddingType::kValid;
    case TfLitePadding::kTfLitePaddingUnknown:
    default:
      return PaddingType::kNone;
   dc466:	2802      	cmp	r0, #2
   dc468:	bf0c      	ite	eq
   dc46a:	2002      	moveq	r0, #2
   dc46c:	2000      	movne	r0, #0
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(params->padding);
   dc46e:	f88d 0088 	strb.w	r0, [sp, #136]	; 0x88
  op_params.padding_values.width = data->padding.width;
   dc472:	6818      	ldr	r0, [r3, #0]
   dc474:	f8ad 008a 	strh.w	r0, [sp, #138]	; 0x8a
  op_params.padding_values.height = data->padding.height;
   dc478:	6858      	ldr	r0, [r3, #4]
   dc47a:	f8ad 008c 	strh.w	r0, [sp, #140]	; 0x8c
  op_params.stride_width = params->stride_width;
   dc47e:	6850      	ldr	r0, [r2, #4]
   dc480:	f8ad 0092 	strh.w	r0, [sp, #146]	; 0x92
  op_params.stride_height = params->stride_height;
   dc484:	6890      	ldr	r0, [r2, #8]
   dc486:	f8ad 0094 	strh.w	r0, [sp, #148]	; 0x94
  op_params.dilation_width_factor = params->dilation_width_factor;
   dc48a:	68d0      	ldr	r0, [r2, #12]
  op_params.dilation_height_factor = params->dilation_height_factor;
   dc48c:	6912      	ldr	r2, [r2, #16]
   dc48e:	f8ad 2098 	strh.w	r2, [sp, #152]	; 0x98
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
   dc492:	691a      	ldr	r2, [r3, #16]
   dc494:	922a      	str	r2, [sp, #168]	; 0xa8
  op_params.output_shift = -data->output_shift;
   dc496:	695a      	ldr	r2, [r3, #20]
  op_params.padding_type = RuntimePaddingType(params->padding);
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
   dc498:	f8ad 0096 	strh.w	r0, [sp, #150]	; 0x96
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
   dc49c:	4252      	negs	r2, r2
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
   dc49e:	9128      	str	r1, [sp, #160]	; 0xa0
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
   dc4a0:	922b      	str	r2, [sp, #172]	; 0xac
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
   dc4a2:	4641      	mov	r1, r8
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
   dc4a4:	f8d3 2218 	ldr.w	r2, [r3, #536]	; 0x218
  op_params.quantized_activation_max = data->output_activation_max;
   dc4a8:	f8d3 321c 	ldr.w	r3, [r3, #540]	; 0x21c
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = input_offset;
   dc4ac:	f8cd e09c 	str.w	lr, [sp, #156]	; 0x9c
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
   dc4b0:	a809      	add	r0, sp, #36	; 0x24
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
   dc4b2:	f8cd c0a4 	str.w	ip, [sp, #164]	; 0xa4
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
   dc4b6:	922c      	str	r2, [sp, #176]	; 0xb0
  op_params.quantized_activation_max = data->output_activation_max;
   dc4b8:	932d      	str	r3, [sp, #180]	; 0xb4
  reference_ops::Conv(op_params, GetTensorShape(input),
   dc4ba:	f7fa fa74 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                      GetTensorData<uint8_t>(input), GetTensorShape(filter),
   dc4be:	4639      	mov	r1, r7
   dc4c0:	a80e      	add	r0, sp, #56	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dc4c2:	f8d8 8004 	ldr.w	r8, [r8, #4]
   dc4c6:	f7fa fa6e 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dc4ca:	f8d7 9004 	ldr.w	r9, [r7, #4]
                      GetTensorData<uint8_t>(filter), GetTensorShape(bias),
   dc4ce:	af13      	add	r7, sp, #76	; 0x4c
   dc4d0:	4621      	mov	r1, r4
   dc4d2:	4638      	mov	r0, r7
   dc4d4:	f7fa fa67 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dc4d8:	b114      	cbz	r4, dc4e0 <_ZN6tflite3ops5micro4conv13EvalQuantizedEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xa4>
   dc4da:	f8d4 a004 	ldr.w	sl, [r4, #4]
   dc4de:	e000      	b.n	dc4e2 <_ZN6tflite3ops5micro4conv13EvalQuantizedEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xa6>
   dc4e0:	46a2      	mov	sl, r4
                      GetTensorData<int32_t>(bias), GetTensorShape(output),
   dc4e2:	ac18      	add	r4, sp, #96	; 0x60
   dc4e4:	4631      	mov	r1, r6
   dc4e6:	4620      	mov	r0, r4
   dc4e8:	f7fa fa5d 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dc4ec:	f8d6 b004 	ldr.w	fp, [r6, #4]
                      GetTensorData<uint8_t>(output), GetTensorShape(im2col),
   dc4f0:	ae1d      	add	r6, sp, #116	; 0x74
   dc4f2:	4629      	mov	r1, r5
   dc4f4:	4630      	mov	r0, r6
   dc4f6:	f7fa fa56 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dc4fa:	b105      	cbz	r5, dc4fe <_ZN6tflite3ops5micro4conv13EvalQuantizedEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xc2>
   dc4fc:	686d      	ldr	r5, [r5, #4]
                      GetTensorData<uint8_t>(im2col), nullptr);
   dc4fe:	9506      	str	r5, [sp, #24]
   dc500:	2300      	movs	r3, #0
   dc502:	4642      	mov	r2, r8
   dc504:	a909      	add	r1, sp, #36	; 0x24
   dc506:	9307      	str	r3, [sp, #28]
   dc508:	a822      	add	r0, sp, #136	; 0x88
   dc50a:	ab0e      	add	r3, sp, #56	; 0x38
   dc50c:	9605      	str	r6, [sp, #20]
   dc50e:	f8cd b010 	str.w	fp, [sp, #16]
   dc512:	9403      	str	r4, [sp, #12]
   dc514:	f8cd a008 	str.w	sl, [sp, #8]
   dc518:	9701      	str	r7, [sp, #4]
   dc51a:	f8cd 9000 	str.w	r9, [sp]
   dc51e:	f7ff fcbd 	bl	dbe9c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv>
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<uint8_t>(input), GetTensorShape(filter),
                      GetTensorData<uint8_t>(filter), GetTensorShape(bias),
                      GetTensorData<int32_t>(bias), GetTensorShape(output),
                      GetTensorData<uint8_t>(output), GetTensorShape(im2col),
   dc522:	4630      	mov	r0, r6
   dc524:	f7f9 ff8f 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<uint8_t>(input), GetTensorShape(filter),
                      GetTensorData<uint8_t>(filter), GetTensorShape(bias),
                      GetTensorData<int32_t>(bias), GetTensorShape(output),
   dc528:	4620      	mov	r0, r4
   dc52a:	f7f9 ff8c 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<uint8_t>(input), GetTensorShape(filter),
                      GetTensorData<uint8_t>(filter), GetTensorShape(bias),
   dc52e:	4638      	mov	r0, r7
   dc530:	f7f9 ff89 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<uint8_t>(input), GetTensorShape(filter),
   dc534:	a80e      	add	r0, sp, #56	; 0x38
   dc536:	f7f9 ff86 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
   dc53a:	a809      	add	r0, sp, #36	; 0x24
   dc53c:	f7f9 ff83 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                      GetTensorData<uint8_t>(input), GetTensorShape(filter),
                      GetTensorData<uint8_t>(filter), GetTensorShape(bias),
                      GetTensorData<int32_t>(bias), GetTensorShape(output),
                      GetTensorData<uint8_t>(output), GetTensorShape(im2col),
                      GetTensorData<uint8_t>(im2col), nullptr);
}
   dc540:	b031      	add	sp, #196	; 0xc4
   dc542:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dc546 <_ZN6tflite3ops5micro4conv23EvalQuantizedPerChannelEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_>:
void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, OpData* data,
                             const TfLiteTensor* input,
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output,
                             TfLiteTensor* im2col) {
   dc546:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dc54a:	b0ad      	sub	sp, #180	; 0xb4
   dc54c:	ac37      	add	r4, sp, #220	; 0xdc
   dc54e:	9f36      	ldr	r7, [sp, #216]	; 0xd8
   dc550:	e894 0430 	ldmia.w	r4, {r4, r5, sl}
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
   dc554:	6939      	ldr	r1, [r7, #16]
   dc556:	4249      	negs	r1, r1
   dc558:	9123      	str	r1, [sp, #140]	; 0x8c
  op_params.output_offset = output->params.zero_point;
   dc55a:	f8da 1010 	ldr.w	r1, [sl, #16]
   dc55e:	9125      	str	r1, [sp, #148]	; 0x94
  op_params.stride_height = params->stride_height;
   dc560:	6891      	ldr	r1, [r2, #8]
   dc562:	f8ad 1084 	strh.w	r1, [sp, #132]	; 0x84
  op_params.stride_width = params->stride_width;
   dc566:	6851      	ldr	r1, [r2, #4]
   dc568:	f8ad 1082 	strh.w	r1, [sp, #130]	; 0x82
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
   dc56c:	4699      	mov	r9, r3
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.dilation_height_factor = params->dilation_height_factor;
   dc56e:	6911      	ldr	r1, [r2, #16]
  op_params.dilation_width_factor = params->dilation_width_factor;
   dc570:	68d2      	ldr	r2, [r2, #12]
   dc572:	f8ad 2086 	strh.w	r2, [sp, #134]	; 0x86
  op_params.padding_values.height = data->padding.height;
   dc576:	685a      	ldr	r2, [r3, #4]
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.dilation_height_factor = params->dilation_height_factor;
   dc578:	f8ad 1088 	strh.w	r1, [sp, #136]	; 0x88
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
   dc57c:	f8ad 207c 	strh.w	r2, [sp, #124]	; 0x7c
  op_params.padding_values.width = data->padding.width;

  reference_integer_ops::ConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
   dc580:	4639      	mov	r1, r7
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
   dc582:	f859 2b18 	ldr.w	r2, [r9], #24
   dc586:	f8ad 207a 	strh.w	r2, [sp, #122]	; 0x7a

  reference_integer_ops::ConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
   dc58a:	a80a      	add	r0, sp, #40	; 0x28
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;

  reference_integer_ops::ConvPerChannel(
   dc58c:	f503 768c 	add.w	r6, r3, #280	; 0x118
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
   dc590:	f7fa fa09 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dc594:	f8d7 b004 	ldr.w	fp, [r7, #4]
      GetTensorData<int8>(input), GetTensorShape(filter),
   dc598:	af0f      	add	r7, sp, #60	; 0x3c
   dc59a:	4621      	mov	r1, r4
   dc59c:	4638      	mov	r0, r7
   dc59e:	f7fa fa02 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dc5a2:	b104      	cbz	r4, dc5a6 <_ZN6tflite3ops5micro4conv23EvalQuantizedPerChannelEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_+0x60>
   dc5a4:	6864      	ldr	r4, [r4, #4]
      GetTensorData<int8>(filter), GetTensorShape(bias),
   dc5a6:	f10d 0850 	add.w	r8, sp, #80	; 0x50
   dc5aa:	4629      	mov	r1, r5
   dc5ac:	4640      	mov	r0, r8
   dc5ae:	f7fa f9fa 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dc5b2:	b10d      	cbz	r5, dc5b8 <_ZN6tflite3ops5micro4conv23EvalQuantizedPerChannelEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_+0x72>
   dc5b4:	686b      	ldr	r3, [r5, #4]
   dc5b6:	e000      	b.n	dc5ba <_ZN6tflite3ops5micro4conv23EvalQuantizedPerChannelEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_+0x74>
   dc5b8:	462b      	mov	r3, r5
      GetTensorData<int32>(bias), GetTensorShape(output),
   dc5ba:	ad19      	add	r5, sp, #100	; 0x64
   dc5bc:	4651      	mov	r1, sl
   dc5be:	4628      	mov	r0, r5
   dc5c0:	9309      	str	r3, [sp, #36]	; 0x24
   dc5c2:	f7fa f9f0 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorData<int8>(output));
   dc5c6:	f8da 2004 	ldr.w	r2, [sl, #4]
   dc5ca:	9b09      	ldr	r3, [sp, #36]	; 0x24
   dc5cc:	9206      	str	r2, [sp, #24]
   dc5ce:	4649      	mov	r1, r9
   dc5d0:	4632      	mov	r2, r6
   dc5d2:	9304      	str	r3, [sp, #16]
   dc5d4:	a81e      	add	r0, sp, #120	; 0x78
   dc5d6:	ab0a      	add	r3, sp, #40	; 0x28
   dc5d8:	9505      	str	r5, [sp, #20]
   dc5da:	f8cd 800c 	str.w	r8, [sp, #12]
   dc5de:	9402      	str	r4, [sp, #8]
   dc5e0:	9701      	str	r7, [sp, #4]
   dc5e2:	f8cd b000 	str.w	fp, [sp]
   dc5e6:	f7ff fd69 	bl	dc0bc <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa>
  reference_integer_ops::ConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
      GetTensorData<int8>(filter), GetTensorShape(bias),
      GetTensorData<int32>(bias), GetTensorShape(output),
   dc5ea:	4628      	mov	r0, r5
   dc5ec:	f7f9 ff2b 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>

  reference_integer_ops::ConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
      GetTensorData<int8>(filter), GetTensorShape(bias),
   dc5f0:	4640      	mov	r0, r8
   dc5f2:	f7f9 ff28 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.padding_values.width = data->padding.width;

  reference_integer_ops::ConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
   dc5f6:	4638      	mov	r0, r7
   dc5f8:	f7f9 ff25 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;

  reference_integer_ops::ConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
   dc5fc:	a80a      	add	r0, sp, #40	; 0x28
   dc5fe:	f7f9 ff22 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
      GetTensorData<int8>(input), GetTensorShape(filter),
      GetTensorData<int8>(filter), GetTensorShape(bias),
      GetTensorData<int32>(bias), GetTensorShape(output),
      GetTensorData<int8>(output));
}
   dc602:	b02d      	add	sp, #180	; 0xb4
   dc604:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dc608 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_>:

void EvalFloat(TfLiteContext* context, TfLiteNode* node,
               TfLiteConvParams* params, OpData* data,
               const TfLiteTensor* input, const TfLiteTensor* filter,
               const TfLiteTensor* bias, TfLiteTensor* im2col,
               TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
   dc608:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
   dc60c:	7d11      	ldrb	r1, [r2, #20]

void EvalFloat(TfLiteContext* context, TfLiteNode* node,
               TfLiteConvParams* params, OpData* data,
               const TfLiteTensor* input, const TfLiteTensor* filter,
               const TfLiteTensor* bias, TfLiteTensor* im2col,
               TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
   dc60e:	b0b1      	sub	sp, #196	; 0xc4
   dc610:	ac3a      	add	r4, sp, #232	; 0xe8
// Calculates the useful range of an activation layer given its activation
// tensor.a
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
   dc612:	2901      	cmp	r1, #1
   dc614:	e894 0170 	ldmia.w	r4, {r4, r5, r6, r8}
   dc618:	9f3f      	ldr	r7, [sp, #252]	; 0xfc
   dc61a:	d011      	beq.n	dc640 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x38>
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
   dc61c:	2903      	cmp	r1, #3
   dc61e:	d012      	beq.n	dc646 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x3e>
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
    *activation_max = std::numeric_limits<T>::max();
   dc620:	ed9f 7a3f 	vldr	s14, [pc, #252]	; dc720 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x118>
   dc624:	eddf 6a3f 	vldr	s13, [pc, #252]	; dc724 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x11c>
   dc628:	2902      	cmp	r1, #2
   dc62a:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   dc62e:	bf18      	it	ne
   dc630:	eef0 7a47 	vmovne.f32	s15, s14
   dc634:	eebf 7a00 	vmov.f32	s14, #240	; 0xbf800000 -1.0
   dc638:	bf18      	it	ne
   dc63a:	eeb0 7a66 	vmovne.f32	s14, s13
   dc63e:	e006      	b.n	dc64e <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x46>
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
   dc640:	eddf 7a37 	vldr	s15, [pc, #220]	; dc720 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x118>
   dc644:	e001      	b.n	dc64a <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x42>
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
    *activation_max = 6;
   dc646:	eef1 7a08 	vmov.f32	s15, #24	; 0x40c00000  6.0
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
   dc64a:	ed9f 7a37 	vldr	s14, [pc, #220]	; dc728 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x120>
  int32_t output_activation_min;
  int32_t output_activation_max;
};

inline PaddingType RuntimePaddingType(TfLitePadding padding) {
  switch (padding) {
   dc64e:	7811      	ldrb	r1, [r2, #0]
   dc650:	2901      	cmp	r1, #1
   dc652:	d003      	beq.n	dc65c <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x54>
      return PaddingType::kSame;
    case TfLitePadding::kTfLitePaddingValid:
      return PaddingType::kValid;
    case TfLitePadding::kTfLitePaddingUnknown:
    default:
      return PaddingType::kNone;
   dc654:	2902      	cmp	r1, #2
   dc656:	bf0c      	ite	eq
   dc658:	2102      	moveq	r1, #2
   dc65a:	2100      	movne	r1, #0
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(params->padding);
   dc65c:	f88d 1088 	strb.w	r1, [sp, #136]	; 0x88
  op_params.padding_values.width = data->padding.width;
   dc660:	6819      	ldr	r1, [r3, #0]
  op_params.padding_values.height = data->padding.height;
   dc662:	685b      	ldr	r3, [r3, #4]
   dc664:	f8ad 308c 	strh.w	r3, [sp, #140]	; 0x8c
  op_params.stride_width = params->stride_width;
   dc668:	6853      	ldr	r3, [r2, #4]
   dc66a:	f8ad 3092 	strh.w	r3, [sp, #146]	; 0x92
  op_params.stride_height = params->stride_height;
   dc66e:	6893      	ldr	r3, [r2, #8]
   dc670:	f8ad 3094 	strh.w	r3, [sp, #148]	; 0x94
  op_params.dilation_width_factor = params->dilation_width_factor;
   dc674:	68d3      	ldr	r3, [r2, #12]
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(params->padding);
  op_params.padding_values.width = data->padding.width;
   dc676:	f8ad 108a 	strh.w	r1, [sp, #138]	; 0x8a
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
   dc67a:	f8ad 3096 	strh.w	r3, [sp, #150]	; 0x96
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  reference_ops::Conv(op_params, GetTensorShape(input),
   dc67e:	4621      	mov	r1, r4
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
   dc680:	6913      	ldr	r3, [r2, #16]
   dc682:	f8ad 3098 	strh.w	r3, [sp, #152]	; 0x98
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  reference_ops::Conv(op_params, GetTensorShape(input),
   dc686:	a809      	add	r0, sp, #36	; 0x24
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
   dc688:	ed8d 7a2e 	vstr	s14, [sp, #184]	; 0xb8
  op_params.float_activation_max = output_activation_max;
   dc68c:	edcd 7a2f 	vstr	s15, [sp, #188]	; 0xbc

  reference_ops::Conv(op_params, GetTensorShape(input),
   dc690:	f7fa f989 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dc694:	b104      	cbz	r4, dc698 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x90>
   dc696:	6864      	ldr	r4, [r4, #4]
                      GetTensorData<float>(input), GetTensorShape(filter),
   dc698:	4629      	mov	r1, r5
   dc69a:	a80e      	add	r0, sp, #56	; 0x38
   dc69c:	f7fa f983 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dc6a0:	b105      	cbz	r5, dc6a4 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x9c>
   dc6a2:	686d      	ldr	r5, [r5, #4]
                      GetTensorData<float>(filter), GetTensorShape(bias),
   dc6a4:	f10d 094c 	add.w	r9, sp, #76	; 0x4c
   dc6a8:	4631      	mov	r1, r6
   dc6aa:	4648      	mov	r0, r9
   dc6ac:	f7fa f97b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dc6b0:	b106      	cbz	r6, dc6b4 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xac>
   dc6b2:	6876      	ldr	r6, [r6, #4]
                      GetTensorData<float>(bias), GetTensorShape(output),
   dc6b4:	f10d 0a60 	add.w	sl, sp, #96	; 0x60
   dc6b8:	4639      	mov	r1, r7
   dc6ba:	4650      	mov	r0, sl
   dc6bc:	f7fa f973 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dc6c0:	b107      	cbz	r7, dc6c4 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xbc>
   dc6c2:	687f      	ldr	r7, [r7, #4]
                      GetTensorData<float>(output), GetTensorShape(im2col),
   dc6c4:	f10d 0b74 	add.w	fp, sp, #116	; 0x74
   dc6c8:	4641      	mov	r1, r8
   dc6ca:	4658      	mov	r0, fp
   dc6cc:	f7fa f96b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dc6d0:	f1b8 0f00 	cmp.w	r8, #0
   dc6d4:	d002      	beq.n	dc6dc <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xd4>
   dc6d6:	f8d8 3004 	ldr.w	r3, [r8, #4]
   dc6da:	e000      	b.n	dc6de <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xd6>
   dc6dc:	4643      	mov	r3, r8
                      GetTensorData<float>(im2col));
   dc6de:	4622      	mov	r2, r4
   dc6e0:	a909      	add	r1, sp, #36	; 0x24
   dc6e2:	9306      	str	r3, [sp, #24]
   dc6e4:	a822      	add	r0, sp, #136	; 0x88
   dc6e6:	ab0e      	add	r3, sp, #56	; 0x38
   dc6e8:	f8cd b014 	str.w	fp, [sp, #20]
   dc6ec:	9704      	str	r7, [sp, #16]
   dc6ee:	f8cd a00c 	str.w	sl, [sp, #12]
   dc6f2:	9602      	str	r6, [sp, #8]
   dc6f4:	e88d 0220 	stmia.w	sp, {r5, r9}
   dc6f8:	f7ff fac0 	bl	dbc7c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_>

  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<float>(input), GetTensorShape(filter),
                      GetTensorData<float>(filter), GetTensorShape(bias),
                      GetTensorData<float>(bias), GetTensorShape(output),
                      GetTensorData<float>(output), GetTensorShape(im2col),
   dc6fc:	4658      	mov	r0, fp
   dc6fe:	f7f9 fea2 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.float_activation_max = output_activation_max;

  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<float>(input), GetTensorShape(filter),
                      GetTensorData<float>(filter), GetTensorShape(bias),
                      GetTensorData<float>(bias), GetTensorShape(output),
   dc702:	4650      	mov	r0, sl
   dc704:	f7f9 fe9f 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<float>(input), GetTensorShape(filter),
                      GetTensorData<float>(filter), GetTensorShape(bias),
   dc708:	4648      	mov	r0, r9
   dc70a:	f7f9 fe9c 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<float>(input), GetTensorShape(filter),
   dc70e:	a80e      	add	r0, sp, #56	; 0x38
   dc710:	f7f9 fe99 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  reference_ops::Conv(op_params, GetTensorShape(input),
   dc714:	a809      	add	r0, sp, #36	; 0x24
   dc716:	f7f9 fe96 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                      GetTensorData<float>(input), GetTensorShape(filter),
                      GetTensorData<float>(filter), GetTensorShape(bias),
                      GetTensorData<float>(bias), GetTensorShape(output),
                      GetTensorData<float>(output), GetTensorShape(im2col),
                      GetTensorData<float>(im2col));
}
   dc71a:	b031      	add	sp, #196	; 0xc4
   dc71c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dc720:	7f7fffff 	.word	0x7f7fffff
   dc724:	ff7fffff 	.word	0xff7fffff
   dc728:	00000000 	.word	0x00000000

000dc72c <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dc72c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dc730:	680a      	ldr	r2, [r1, #0]
   dc732:	6887      	ldr	r7, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dc734:	6896      	ldr	r6, [r2, #8]
   dc736:	4688      	mov	r8, r1
   dc738:	6851      	ldr	r1, [r2, #4]
}

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
   dc73a:	68d2      	ldr	r2, [r2, #12]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dc73c:	2338      	movs	r3, #56	; 0x38
   dc73e:	fb03 fa01 	mul.w	sl, r3, r1
   dc742:	f5ad 7d15 	sub.w	sp, sp, #596	; 0x254
   dc746:	eb07 010a 	add.w	r1, r7, sl
   dc74a:	4605      	mov	r5, r0
  int filter_height = filter->dims->data[1];
  int output_width = output->dims->data[2];
  int output_height = output->dims->data[1];

  OpData data;
  if (input->type != kTfLiteFloat32) {
   dc74c:	f817 000a 	ldrb.w	r0, [r7, sl]
   dc750:	910b      	str	r1, [sp, #44]	; 0x2c

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
   dc752:	1c51      	adds	r1, r2, #1
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dc754:	fb03 7606 	mla	r6, r3, r6, r7
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dc758:	bf14      	ite	ne
   dc75a:	fb03 7302 	mlane	r3, r3, r2, r7
  }
  return nullptr;
   dc75e:	2300      	moveq	r3, #0
   dc760:	2801      	cmp	r0, #1
   dc762:	930a      	str	r3, [sp, #40]	; 0x28
   dc764:	d024      	beq.n	dc7b0 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x84>
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
   dc766:	f896 4030 	ldrb.w	r4, [r6, #48]	; 0x30
   dc76a:	2c01      	cmp	r4, #1
   dc76c:	d00e      	beq.n	dc78c <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x60>
   dc76e:	4b47      	ldr	r3, [pc, #284]	; (dc88c <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x160>)
   dc770:	9301      	str	r3, [sp, #4]
   dc772:	2601      	movs	r6, #1
   dc774:	4b46      	ldr	r3, [pc, #280]	; (dc890 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x164>)
   dc776:	9402      	str	r4, [sp, #8]
   dc778:	9300      	str	r3, [sp, #0]
   dc77a:	696c      	ldr	r4, [r5, #20]
   dc77c:	9603      	str	r6, [sp, #12]
   dc77e:	23dd      	movs	r3, #221	; 0xdd
   dc780:	4a44      	ldr	r2, [pc, #272]	; (dc894 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x168>)
   dc782:	4945      	ldr	r1, [pc, #276]	; (dc898 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x16c>)
   dc784:	4628      	mov	r0, r5
   dc786:	47a0      	blx	r4
   dc788:	4634      	mov	r4, r6
   dc78a:	e079      	b.n	dc880 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x154>
                      kTfLiteAffineQuantization);

    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
   dc78c:	6b73      	ldr	r3, [r6, #52]	; 0x34
    TF_LITE_ENSURE(context, affine_quantization);
   dc78e:	b923      	cbnz	r3, dc79a <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x6e>
   dc790:	4b42      	ldr	r3, [pc, #264]	; (dc89c <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x170>)
   dc792:	9300      	str	r3, [sp, #0]
   dc794:	696e      	ldr	r6, [r5, #20]
   dc796:	23e2      	movs	r3, #226	; 0xe2
   dc798:	e005      	b.n	dc7a6 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x7a>
    TF_LITE_ENSURE(context, affine_quantization->scale);
   dc79a:	681b      	ldr	r3, [r3, #0]
   dc79c:	b943      	cbnz	r3, dc7b0 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x84>
   dc79e:	4b40      	ldr	r3, [pc, #256]	; (dc8a0 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x174>)
   dc7a0:	696e      	ldr	r6, [r5, #20]
   dc7a2:	9300      	str	r3, [sp, #0]
   dc7a4:	23e3      	movs	r3, #227	; 0xe3
   dc7a6:	4a3b      	ldr	r2, [pc, #236]	; (dc894 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x168>)
   dc7a8:	493e      	ldr	r1, [pc, #248]	; (dc8a4 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x178>)
   dc7aa:	4628      	mov	r0, r5
   dc7ac:	47b0      	blx	r6
   dc7ae:	e067      	b.n	dc880 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x154>
                      GetTensorData<float>(output), GetTensorShape(im2col),
                      GetTensorData<float>(im2col));
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
   dc7b0:	f8d8 3014 	ldr.w	r3, [r8, #20]
   dc7b4:	9309      	str	r3, [sp, #36]	; 0x24
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dc7b6:	f8d8 3004 	ldr.w	r3, [r8, #4]
  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);

  int input_width = input->dims->data[2];
  int input_height = input->dims->data[1];
  int filter_width = filter->dims->data[2];
   dc7ba:	68b2      	ldr	r2, [r6, #8]
   dc7bc:	685b      	ldr	r3, [r3, #4]
   dc7be:	f04f 0938 	mov.w	r9, #56	; 0x38
   dc7c2:	fb09 7903 	mla	r9, r9, r3, r7
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);

  int input_width = input->dims->data[2];
   dc7c6:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
  int input_height = input->dims->data[1];
  int filter_width = filter->dims->data[2];
  int filter_height = filter->dims->data[1];
  int output_width = output->dims->data[2];
   dc7c8:	f8d9 1008 	ldr.w	r1, [r9, #8]
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);

  int input_width = input->dims->data[2];
   dc7cc:	689b      	ldr	r3, [r3, #8]
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
  }

  TF_LITE_ENSURE_STATUS(CalculateOpData(
   dc7ce:	9005      	str	r0, [sp, #20]
   dc7d0:	f10d 0b30 	add.w	fp, sp, #48	; 0x30
   dc7d4:	f8cd b018 	str.w	fp, [sp, #24]
   dc7d8:	6888      	ldr	r0, [r1, #8]
   dc7da:	9004      	str	r0, [sp, #16]
   dc7dc:	68c9      	ldr	r1, [r1, #12]
   dc7de:	9103      	str	r1, [sp, #12]
   dc7e0:	6891      	ldr	r1, [r2, #8]
   dc7e2:	9102      	str	r1, [sp, #8]
   dc7e4:	68d2      	ldr	r2, [r2, #12]
   dc7e6:	9201      	str	r2, [sp, #4]
   dc7e8:	689a      	ldr	r2, [r3, #8]
   dc7ea:	9200      	str	r2, [sp, #0]
   dc7ec:	68db      	ldr	r3, [r3, #12]
   dc7ee:	9a09      	ldr	r2, [sp, #36]	; 0x24
   dc7f0:	4641      	mov	r1, r8
   dc7f2:	4628      	mov	r0, r5
   dc7f4:	f7ff fd76 	bl	dc2e4 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE>
   dc7f8:	4604      	mov	r4, r0
   dc7fa:	2800      	cmp	r0, #0
   dc7fc:	d13f      	bne.n	dc87e <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x152>
      context, node, params, input_width, input_height, filter_width,
      filter_height, output_width, output_height, input->type, &data));

  switch (input->type) {  // Already know in/out types are same.
   dc7fe:	f817 000a 	ldrb.w	r0, [r7, sl]
   dc802:	2803      	cmp	r0, #3
   dc804:	d022      	beq.n	dc84c <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x120>
   dc806:	2809      	cmp	r0, #9
   dc808:	d011      	beq.n	dc82e <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x102>
   dc80a:	2801      	cmp	r0, #1
   dc80c:	d12e      	bne.n	dc86c <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x140>
    case kTfLiteFloat32:
      EvalFloat(context, node, params, &data, input, filter, bias, nullptr,
                nullptr, output);
   dc80e:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   dc810:	9302      	str	r3, [sp, #8]
   dc812:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   dc814:	9300      	str	r3, [sp, #0]
   dc816:	f8cd 9014 	str.w	r9, [sp, #20]
   dc81a:	9404      	str	r4, [sp, #16]
   dc81c:	9403      	str	r4, [sp, #12]
   dc81e:	9601      	str	r6, [sp, #4]
   dc820:	465b      	mov	r3, fp
   dc822:	9a09      	ldr	r2, [sp, #36]	; 0x24
   dc824:	4641      	mov	r1, r8
   dc826:	4628      	mov	r0, r5
   dc828:	f7ff feee 	bl	dc608 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_>
      break;
   dc82c:	e028      	b.n	dc880 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x154>
    case kTfLiteInt8:
      EvalQuantizedPerChannel(context, node, params, &data, input, filter, bias,
                              output, nullptr);
   dc82e:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   dc830:	9302      	str	r3, [sp, #8]
   dc832:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   dc834:	9300      	str	r3, [sp, #0]
   dc836:	9404      	str	r4, [sp, #16]
   dc838:	f8cd 900c 	str.w	r9, [sp, #12]
   dc83c:	9601      	str	r6, [sp, #4]
   dc83e:	465b      	mov	r3, fp
   dc840:	9a09      	ldr	r2, [sp, #36]	; 0x24
   dc842:	4641      	mov	r1, r8
   dc844:	4628      	mov	r0, r5
   dc846:	f7ff fe7e 	bl	dc546 <_ZN6tflite3ops5micro4conv23EvalQuantizedPerChannelEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_>
      break;
   dc84a:	e019      	b.n	dc880 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x154>
    case kTfLiteUInt8:
      EvalQuantized(context, node, params, &data, input, filter, bias, nullptr,
                    nullptr, output);
   dc84c:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   dc84e:	9302      	str	r3, [sp, #8]
   dc850:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   dc852:	9300      	str	r3, [sp, #0]
   dc854:	f8cd 9014 	str.w	r9, [sp, #20]
   dc858:	9404      	str	r4, [sp, #16]
   dc85a:	9403      	str	r4, [sp, #12]
   dc85c:	9601      	str	r6, [sp, #4]
   dc85e:	465b      	mov	r3, fp
   dc860:	9a09      	ldr	r2, [sp, #36]	; 0x24
   dc862:	4641      	mov	r1, r8
   dc864:	4628      	mov	r0, r5
   dc866:	f7ff fde9 	bl	dc43c <_ZN6tflite3ops5micro4conv13EvalQuantizedEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_>
      break;
   dc86a:	e009      	b.n	dc880 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x154>
    default:
      context->ReportError(context, "Type %s (%d) not supported.",
   dc86c:	696c      	ldr	r4, [r5, #20]
   dc86e:	f7f7 fc53 	bl	d4118 <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type), input->type);
   dc872:	f817 300a 	ldrb.w	r3, [r7, sl]
   dc876:	490c      	ldr	r1, [pc, #48]	; (dc8a8 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x17c>)
   dc878:	4602      	mov	r2, r0
   dc87a:	4628      	mov	r0, r5
   dc87c:	47a0      	blx	r4
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
  }

  TF_LITE_ENSURE_STATUS(CalculateOpData(
   dc87e:	2401      	movs	r4, #1
      context->ReportError(context, "Type %s (%d) not supported.",
                           TfLiteTypeGetName(input->type), input->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   dc880:	4620      	mov	r0, r4
   dc882:	f50d 7d15 	add.w	sp, sp, #596	; 0x254
   dc886:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dc88a:	bf00      	nop
   dc88c:	000e9b17 	.word	0x000e9b17
   dc890:	000e9b31 	.word	0x000e9b31
   dc894:	000e9a22 	.word	0x000e9a22
   dc898:	000e98f8 	.word	0x000e98f8
   dc89c:	000e9b4b 	.word	0x000e9b4b
   dc8a0:	000e9b5f 	.word	0x000e9b5f
   dc8a4:	000e9ac8 	.word	0x000e9ac8
   dc8a8:	000e9b7a 	.word	0x000e9b7a

000dc8ac <_ZN6tflite3ops5micro16Register_CONV_2DEv>:

TfLiteRegistration* Register_CONV_2D() {
  static TfLiteRegistration r = {conv::Init, conv::Free, conv::Prepare,
                                 conv::Eval};
  return &r;
}
   dc8ac:	4800      	ldr	r0, [pc, #0]	; (dc8b0 <_ZN6tflite3ops5micro16Register_CONV_2DEv+0x4>)
   dc8ae:	4770      	bx	lr
   dc8b0:	2003be88 	.word	0x2003be88

000dc8b4 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode>:
namespace tflite {
namespace ops {
namespace micro {
namespace dequantize {

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   dc8b4:	b5f0      	push	{r4, r5, r6, r7, lr}
   dc8b6:	680b      	ldr	r3, [r1, #0]
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
   dc8b8:	681e      	ldr	r6, [r3, #0]
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   dc8ba:	2e01      	cmp	r6, #1
namespace tflite {
namespace ops {
namespace micro {
namespace dequantize {

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   dc8bc:	b085      	sub	sp, #20
   dc8be:	4605      	mov	r5, r0
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   dc8c0:	d00c      	beq.n	dc8dc <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x28>
   dc8c2:	4b20      	ldr	r3, [pc, #128]	; (dc944 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x90>)
   dc8c4:	9301      	str	r3, [sp, #4]
   dc8c6:	2401      	movs	r4, #1
   dc8c8:	4b1f      	ldr	r3, [pc, #124]	; (dc948 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x94>)
   dc8ca:	9300      	str	r3, [sp, #0]
   dc8cc:	9403      	str	r4, [sp, #12]
   dc8ce:	9602      	str	r6, [sp, #8]
   dc8d0:	6945      	ldr	r5, [r0, #20]
   dc8d2:	4a1e      	ldr	r2, [pc, #120]	; (dc94c <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x98>)
   dc8d4:	491e      	ldr	r1, [pc, #120]	; (dc950 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x9c>)
   dc8d6:	231d      	movs	r3, #29
   dc8d8:	47a8      	blx	r5
   dc8da:	e02d      	b.n	dc938 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x84>
   dc8dc:	684f      	ldr	r7, [r1, #4]
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
   dc8de:	683c      	ldr	r4, [r7, #0]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
   dc8e0:	2c01      	cmp	r4, #1
   dc8e2:	d00b      	beq.n	dc8fc <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x48>
   dc8e4:	4b17      	ldr	r3, [pc, #92]	; (dc944 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x90>)
   dc8e6:	9301      	str	r3, [sp, #4]
   dc8e8:	4b1a      	ldr	r3, [pc, #104]	; (dc954 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0xa0>)
   dc8ea:	9300      	str	r3, [sp, #0]
   dc8ec:	9603      	str	r6, [sp, #12]
   dc8ee:	9402      	str	r4, [sp, #8]
   dc8f0:	6944      	ldr	r4, [r0, #20]
   dc8f2:	4a16      	ldr	r2, [pc, #88]	; (dc94c <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x98>)
   dc8f4:	4916      	ldr	r1, [pc, #88]	; (dc950 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x9c>)
   dc8f6:	231e      	movs	r3, #30
   dc8f8:	47a0      	blx	r4
   dc8fa:	e01d      	b.n	dc938 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x84>

  // TODO(b/140515557): Add cached dequant to improve hybrid model performance.
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  TF_LITE_ENSURE(context,
   dc8fc:	685a      	ldr	r2, [r3, #4]
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);

  // TODO(b/140515557): Add cached dequant to improve hybrid model performance.
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
   dc8fe:	6881      	ldr	r1, [r0, #8]
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  TF_LITE_ENSURE(context,
   dc900:	2338      	movs	r3, #56	; 0x38
   dc902:	435a      	muls	r2, r3
   dc904:	5c8a      	ldrb	r2, [r1, r2]
   dc906:	2a03      	cmp	r2, #3
   dc908:	d009      	beq.n	dc91e <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x6a>
   dc90a:	2a09      	cmp	r2, #9
   dc90c:	d007      	beq.n	dc91e <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x6a>
   dc90e:	4b12      	ldr	r3, [pc, #72]	; (dc958 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0xa4>)
   dc910:	9300      	str	r3, [sp, #0]
   dc912:	6945      	ldr	r5, [r0, #20]
   dc914:	4a0d      	ldr	r2, [pc, #52]	; (dc94c <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x98>)
   dc916:	4911      	ldr	r1, [pc, #68]	; (dc95c <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0xa8>)
   dc918:	2325      	movs	r3, #37	; 0x25
   dc91a:	47a8      	blx	r5
   dc91c:	e00c      	b.n	dc938 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x84>
                 input->type == kTfLiteUInt8 || input->type == kTfLiteInt8);
  TF_LITE_ENSURE(context, output->type == kTfLiteFloat32);
   dc91e:	687a      	ldr	r2, [r7, #4]
   dc920:	4353      	muls	r3, r2
   dc922:	5ccb      	ldrb	r3, [r1, r3]
   dc924:	2b01      	cmp	r3, #1
   dc926:	d009      	beq.n	dc93c <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x88>
   dc928:	4b0d      	ldr	r3, [pc, #52]	; (dc960 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0xac>)
   dc92a:	9300      	str	r3, [sp, #0]
   dc92c:	696c      	ldr	r4, [r5, #20]
   dc92e:	4a07      	ldr	r2, [pc, #28]	; (dc94c <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x98>)
   dc930:	490a      	ldr	r1, [pc, #40]	; (dc95c <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0xa8>)
   dc932:	2326      	movs	r3, #38	; 0x26
   dc934:	4628      	mov	r0, r5
   dc936:	47a0      	blx	r4
   dc938:	2001      	movs	r0, #1
   dc93a:	e000      	b.n	dc93e <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x8a>

  return kTfLiteOk;
   dc93c:	2000      	movs	r0, #0
}
   dc93e:	b005      	add	sp, #20
   dc940:	bdf0      	pop	{r4, r5, r6, r7, pc}
   dc942:	bf00      	nop
   dc944:	000eb2c5 	.word	0x000eb2c5
   dc948:	000e9912 	.word	0x000e9912
   dc94c:	000e9bf0 	.word	0x000e9bf0
   dc950:	000e98f8 	.word	0x000e98f8
   dc954:	000e9922 	.word	0x000e9922
   dc958:	000e9c9c 	.word	0x000e9c9c
   dc95c:	000e9ac8 	.word	0x000e9ac8
   dc960:	000e9cd6 	.word	0x000e9cd6

000dc964 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_>:
}

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
   dc964:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
   dc968:	6806      	ldr	r6, [r0, #0]
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   dc96a:	680b      	ldr	r3, [r1, #0]
   dc96c:	429e      	cmp	r6, r3
}

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
   dc96e:	4604      	mov	r4, r0
   dc970:	460f      	mov	r7, r1
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   dc972:	d101      	bne.n	dc978 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_+0x14>
   dc974:	2500      	movs	r5, #0
   dc976:	e00d      	b.n	dc994 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_+0x30>
   dc978:	f007 fcd8 	bl	e432c <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   dc97c:	4629      	mov	r1, r5
   dc97e:	4620      	mov	r0, r4
   dc980:	f7f9 fd6c 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dc984:	4629      	mov	r1, r5
   dc986:	4680      	mov	r8, r0
   dc988:	4638      	mov	r0, r7
   dc98a:	f7f9 fd67 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dc98e:	4580      	cmp	r8, r0
   dc990:	d1f2      	bne.n	dc978 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_+0x14>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   dc992:	3501      	adds	r5, #1
   dc994:	42b5      	cmp	r5, r6
   dc996:	dbf1      	blt.n	dc97c <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_+0x18>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   dc998:	2e04      	cmp	r6, #4
   dc99a:	bfcc      	ite	gt
   dc99c:	6864      	ldrgt	r4, [r4, #4]
   dc99e:	3404      	addle	r4, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dc9a0:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
   dc9a2:	2001      	movs	r0, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dc9a4:	429e      	cmp	r6, r3
   dc9a6:	dd04      	ble.n	dc9b2 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_+0x4e>
      buffer_size *= dims_data[i];
   dc9a8:	f854 2023 	ldr.w	r2, [r4, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dc9ac:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
   dc9ae:	4350      	muls	r0, r2
   dc9b0:	e7f8      	b.n	dc9a4 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_+0x40>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
  }
  return shape.FlatSize();
}
   dc9b2:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
	...

000dc9b8 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dc9b8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
   dc9bc:	680b      	ldr	r3, [r1, #0]
   dc9be:	f8d0 b008 	ldr.w	fp, [r0, #8]
   dc9c2:	685a      	ldr	r2, [r3, #4]
   dc9c4:	2338      	movs	r3, #56	; 0x38
   dc9c6:	fb03 f802 	mul.w	r8, r3, r2
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
   dc9ca:	684a      	ldr	r2, [r1, #4]

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
   dc9cc:	eb0b 0508 	add.w	r5, fp, r8
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
   dc9d0:	6854      	ldr	r4, [r2, #4]

  tflite::DequantizationParams op_params;
  op_params.zero_point = input->params.zero_point;
   dc9d2:	f8d5 9010 	ldr.w	r9, [r5, #16]
  TF_LITE_ENSURE(context, output->type == kTfLiteFloat32);

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dc9d6:	b08b      	sub	sp, #44	; 0x2c
   dc9d8:	4682      	mov	sl, r0
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  tflite::DequantizationParams op_params;
  op_params.zero_point = input->params.zero_point;
  op_params.scale = input->params.scale;
   dc9da:	68e8      	ldr	r0, [r5, #12]
  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
   dc9dc:	fb03 b404 	mla	r4, r3, r4, fp

  tflite::DequantizationParams op_params;
  op_params.zero_point = input->params.zero_point;
  op_params.scale = input->params.scale;
   dc9e0:	f00a fb0c 	bl	e6ffc <__aeabi_f2d>
   dc9e4:	4606      	mov	r6, r0
  switch (input->type) {
   dc9e6:	f81b 0008 	ldrb.w	r0, [fp, r8]
   dc9ea:	2803      	cmp	r0, #3
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  tflite::DequantizationParams op_params;
  op_params.zero_point = input->params.zero_point;
  op_params.scale = input->params.scale;
   dc9ec:	460f      	mov	r7, r1
  switch (input->type) {
   dc9ee:	d002      	beq.n	dc9f6 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0x3e>
   dc9f0:	2809      	cmp	r0, #9
   dc9f2:	d025      	beq.n	dca40 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0x88>
   dc9f4:	e051      	b.n	dca9a <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0xe2>
    case kTfLiteUInt8:
      reference_ops::Dequantize(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   dc9f6:	4629      	mov	r1, r5
   dc9f8:	4668      	mov	r0, sp
   dc9fa:	f7f9 ffd4 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
          GetTensorShape(output), GetTensorData<float>(output));
   dc9fe:	4621      	mov	r1, r4
   dca00:	a805      	add	r0, sp, #20
   dca02:	f8d5 8004 	ldr.w	r8, [r5, #4]
   dca06:	f7f9 ffce 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dca0a:	b104      	cbz	r4, dca0e <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0x56>
   dca0c:	6864      	ldr	r4, [r4, #4]
inline void Dequantize(const tflite::DequantizationParams& op_params,
                       const RuntimeShape& input_shape, const T* input_data,
                       const RuntimeShape& output_shape, float* output_data) {
  int32 zero_point = op_params.zero_point;
  const double scale = op_params.scale;
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
   dca0e:	a905      	add	r1, sp, #20
   dca10:	4668      	mov	r0, sp
   dca12:	f7ff ffa7 	bl	dc964 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_>
   dca16:	4645      	mov	r5, r8
   dca18:	4682      	mov	sl, r0

  for (int i = 0; i < flat_size; i++) {
   dca1a:	ebc8 0305 	rsb	r3, r8, r5
   dca1e:	4553      	cmp	r3, sl
   dca20:	da33      	bge.n	dca8a <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0xd2>
    const int32 val = input_data[i];
   dca22:	f815 0b01 	ldrb.w	r0, [r5], #1
    const float result = static_cast<float>(scale * (val - zero_point));
    output_data[i] = result;
   dca26:	ebc9 0000 	rsb	r0, r9, r0
   dca2a:	f00a fad5 	bl	e6fd8 <__aeabi_i2d>
   dca2e:	4632      	mov	r2, r6
   dca30:	463b      	mov	r3, r7
   dca32:	f00a fb37 	bl	e70a4 <__aeabi_dmul>
   dca36:	f00a fe17 	bl	e7668 <__aeabi_d2f>
   dca3a:	f844 0b04 	str.w	r0, [r4], #4
   dca3e:	e7ec      	b.n	dca1a <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0x62>
      break;
    case kTfLiteInt8:
      reference_ops::Dequantize(
          op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
   dca40:	4629      	mov	r1, r5
   dca42:	4668      	mov	r0, sp
   dca44:	f7f9 ffaf 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
          GetTensorShape(output), GetTensorData<float>(output));
   dca48:	4621      	mov	r1, r4
   dca4a:	a805      	add	r0, sp, #20
   dca4c:	f8d5 8004 	ldr.w	r8, [r5, #4]
   dca50:	f7f9 ffa9 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dca54:	b104      	cbz	r4, dca58 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0xa0>
   dca56:	6864      	ldr	r4, [r4, #4]
inline void Dequantize(const tflite::DequantizationParams& op_params,
                       const RuntimeShape& input_shape, const T* input_data,
                       const RuntimeShape& output_shape, float* output_data) {
  int32 zero_point = op_params.zero_point;
  const double scale = op_params.scale;
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
   dca58:	a905      	add	r1, sp, #20
   dca5a:	4668      	mov	r0, sp
   dca5c:	f7ff ff82 	bl	dc964 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_>
   dca60:	4645      	mov	r5, r8
   dca62:	4682      	mov	sl, r0

  for (int i = 0; i < flat_size; i++) {
   dca64:	ebc8 0305 	rsb	r3, r8, r5
   dca68:	4553      	cmp	r3, sl
   dca6a:	da0e      	bge.n	dca8a <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0xd2>
    const int32 val = input_data[i];
   dca6c:	f915 0b01 	ldrsb.w	r0, [r5], #1
    const float result = static_cast<float>(scale * (val - zero_point));
    output_data[i] = result;
   dca70:	ebc9 0000 	rsb	r0, r9, r0
   dca74:	f00a fab0 	bl	e6fd8 <__aeabi_i2d>
   dca78:	4632      	mov	r2, r6
   dca7a:	463b      	mov	r3, r7
   dca7c:	f00a fb12 	bl	e70a4 <__aeabi_dmul>
   dca80:	f00a fdf2 	bl	e7668 <__aeabi_d2f>
   dca84:	f844 0b04 	str.w	r0, [r4], #4
   dca88:	e7ec      	b.n	dca64 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0xac>
   dca8a:	a805      	add	r0, sp, #20
   dca8c:	f7f9 fcdb 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(output), GetTensorData<float>(output));
      break;
    case kTfLiteInt8:
      reference_ops::Dequantize(
          op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
   dca90:	4668      	mov	r0, sp
   dca92:	f7f9 fcd8 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(context, "Type %s (%d) not supported.",
                           TfLiteTypeGetName(input->type), input->type);
      return kTfLiteError;
  }

  return kTfLiteOk;
   dca96:	2000      	movs	r0, #0
      break;
    case kTfLiteInt8:
      reference_ops::Dequantize(
          op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
          GetTensorShape(output), GetTensorData<float>(output));
      break;
   dca98:	e00a      	b.n	dcab0 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0xf8>
    default:
      context->ReportError(context, "Type %s (%d) not supported.",
   dca9a:	f8da 4014 	ldr.w	r4, [sl, #20]
   dca9e:	f7f7 fb3b 	bl	d4118 <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type), input->type);
   dcaa2:	f81b 3008 	ldrb.w	r3, [fp, r8]
   dcaa6:	4904      	ldr	r1, [pc, #16]	; (dcab8 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0x100>)
   dcaa8:	4602      	mov	r2, r0
   dcaaa:	4650      	mov	r0, sl
   dcaac:	47a0      	blx	r4
      return kTfLiteError;
   dcaae:	2001      	movs	r0, #1
  }

  return kTfLiteOk;
}
   dcab0:	b00b      	add	sp, #44	; 0x2c
   dcab2:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dcab6:	bf00      	nop
   dcab8:	000e9b7a 	.word	0x000e9b7a

000dcabc <_ZN6tflite3ops5micro19Register_DEQUANTIZEEv>:

TfLiteRegistration* Register_DEQUANTIZE() {
  static TfLiteRegistration r = {nullptr, nullptr, dequantize::Prepare,
                                 dequantize::Eval};
  return &r;
}
   dcabc:	4800      	ldr	r0, [pc, #0]	; (dcac0 <_ZN6tflite3ops5micro19Register_DEQUANTIZEEv+0x4>)
   dcabe:	4770      	bx	lr
   dcac0:	2003bea8 	.word	0x2003bea8

000dcac4 <_ZSt3absf>:
#endif

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  abs(float __x)
  { return __builtin_fabsf(__x); }
   dcac4:	eeb0 0ac0 	vabs.f32	s0, s0
   dcac8:	4770      	bx	lr

000dcaca <_ZZN6tflite3ops5micro11elementwise12_GLOBAL__N_110SquareEvalEP13TfLiteContextP10TfLiteNodeENUlfE_4_FUNEf>:
TfLiteStatus RsqrtEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalNumeric(context, node, [](float f) { return 1.f / std::sqrt(f); });
}

TfLiteStatus SquareEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalNumeric(context, node, [](float f) { return f * f; });
   dcaca:	ee20 0a00 	vmul.f32	s0, s0, s0
   dcace:	4770      	bx	lr

000dcad0 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode>:
  return type == kTfLiteBool;
}

typedef bool (*IsSupportedType)(TfLiteType);
template <IsSupportedType>
TfLiteStatus GenericPrepare(TfLiteContext* context, TfLiteNode* node) {
   dcad0:	e92d 41ff 	stmdb	sp!, {r0, r1, r2, r3, r4, r5, r6, r7, r8, lr}
   dcad4:	680b      	ldr	r3, [r1, #0]
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
   dcad6:	681e      	ldr	r6, [r3, #0]
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   dcad8:	2e01      	cmp	r6, #1
  return type == kTfLiteBool;
}

typedef bool (*IsSupportedType)(TfLiteType);
template <IsSupportedType>
TfLiteStatus GenericPrepare(TfLiteContext* context, TfLiteNode* node) {
   dcada:	4605      	mov	r5, r0
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   dcadc:	d009      	beq.n	dcaf2 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x22>
   dcade:	4b21      	ldr	r3, [pc, #132]	; (dcb64 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x94>)
   dcae0:	9301      	str	r3, [sp, #4]
   dcae2:	2401      	movs	r4, #1
   dcae4:	4b20      	ldr	r3, [pc, #128]	; (dcb68 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x98>)
   dcae6:	9300      	str	r3, [sp, #0]
   dcae8:	9403      	str	r4, [sp, #12]
   dcaea:	9602      	str	r6, [sp, #8]
   dcaec:	6945      	ldr	r5, [r0, #20]
   dcaee:	2327      	movs	r3, #39	; 0x27
   dcaf0:	e022      	b.n	dcb38 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x68>
   dcaf2:	6849      	ldr	r1, [r1, #4]
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
   dcaf4:	680c      	ldr	r4, [r1, #0]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
   dcaf6:	2c01      	cmp	r4, #1
   dcaf8:	d00c      	beq.n	dcb14 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x44>
   dcafa:	4b1a      	ldr	r3, [pc, #104]	; (dcb64 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x94>)
   dcafc:	9301      	str	r3, [sp, #4]
   dcafe:	4b1b      	ldr	r3, [pc, #108]	; (dcb6c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x9c>)
   dcb00:	9300      	str	r3, [sp, #0]
   dcb02:	9603      	str	r6, [sp, #12]
   dcb04:	9402      	str	r4, [sp, #8]
   dcb06:	6944      	ldr	r4, [r0, #20]
   dcb08:	4a19      	ldr	r2, [pc, #100]	; (dcb70 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa0>)
   dcb0a:	491a      	ldr	r1, [pc, #104]	; (dcb74 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa4>)
   dcb0c:	2328      	movs	r3, #40	; 0x28
   dcb0e:	47a0      	blx	r4
   dcb10:	4630      	mov	r0, r6
   dcb12:	e023      	b.n	dcb5c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x8c>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dcb14:	685e      	ldr	r6, [r3, #4]
  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, output->type);
   dcb16:	6849      	ldr	r1, [r1, #4]
   dcb18:	6887      	ldr	r7, [r0, #8]
   dcb1a:	2238      	movs	r2, #56	; 0x38
   dcb1c:	4356      	muls	r6, r2
   dcb1e:	434a      	muls	r2, r1
   dcb20:	5dbb      	ldrb	r3, [r7, r6]
   dcb22:	5cba      	ldrb	r2, [r7, r2]
   dcb24:	4293      	cmp	r3, r2
   dcb26:	d00b      	beq.n	dcb40 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x70>
   dcb28:	9302      	str	r3, [sp, #8]
   dcb2a:	4b13      	ldr	r3, [pc, #76]	; (dcb78 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa8>)
   dcb2c:	9301      	str	r3, [sp, #4]
   dcb2e:	4b13      	ldr	r3, [pc, #76]	; (dcb7c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xac>)
   dcb30:	9300      	str	r3, [sp, #0]
   dcb32:	9203      	str	r2, [sp, #12]
   dcb34:	6945      	ldr	r5, [r0, #20]
   dcb36:	232b      	movs	r3, #43	; 0x2b
   dcb38:	4a0d      	ldr	r2, [pc, #52]	; (dcb70 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa0>)
   dcb3a:	490e      	ldr	r1, [pc, #56]	; (dcb74 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa4>)
   dcb3c:	47a8      	blx	r5
   dcb3e:	e00a      	b.n	dcb56 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x86>
  if (!IsSupportedType(input->type)) {
   dcb40:	b95b      	cbnz	r3, dcb5a <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x8a>
    context->ReportError(context, "Input data type %s (%d) is not supported.",
   dcb42:	f8d0 8014 	ldr.w	r8, [r0, #20]
   dcb46:	4618      	mov	r0, r3
   dcb48:	f7f7 fae6 	bl	d4118 <TfLiteTypeGetName>
   dcb4c:	5dbb      	ldrb	r3, [r7, r6]
   dcb4e:	490c      	ldr	r1, [pc, #48]	; (dcb80 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xb0>)
   dcb50:	4602      	mov	r2, r0
   dcb52:	4628      	mov	r0, r5
   dcb54:	47c0      	blx	r8
                         TfLiteTypeGetName(input->type), input->type);
    return kTfLiteError;
   dcb56:	4620      	mov	r0, r4
   dcb58:	e000      	b.n	dcb5c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x8c>
  }
  return kTfLiteOk;
   dcb5a:	2000      	movs	r0, #0
}
   dcb5c:	b004      	add	sp, #16
   dcb5e:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   dcb62:	bf00      	nop
   dcb64:	000eb2c5 	.word	0x000eb2c5
   dcb68:	000e9912 	.word	0x000e9912
   dcb6c:	000e9922 	.word	0x000e9922
   dcb70:	000e9cf5 	.word	0x000e9cf5
   dcb74:	000e98f8 	.word	0x000e98f8
   dcb78:	000e993f 	.word	0x000e993f
   dcb7c:	000e9933 	.word	0x000e9933
   dcb80:	000e9da2 	.word	0x000e9da2

000dcb84 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsNumericSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode>:
   dcb84:	f7ff bfa4 	b.w	dcad0 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode>

000dcb88 <_ZSt3sinf>:
  using ::sin;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  sin(float __x)
  { return __builtin_sinf(__x); }
   dcb88:	f008 bd1a 	b.w	e55c0 <sinf>

000dcb8c <_ZSt3cosf>:
  using ::cos;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  cos(float __x)
  { return __builtin_cosf(__x); }
   dcb8c:	f008 bc00 	b.w	e5390 <cosf>

000dcb90 <_ZSt3logf>:
  using ::log;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  log(float __x)
  { return __builtin_logf(__x); }
   dcb90:	f008 be68 	b.w	e5864 <logf>

000dcb94 <_ZSt4sqrtf>:
  using ::sqrt;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  sqrt(float __x)
  { return __builtin_sqrtf(__x); }
   dcb94:	f008 bee2 	b.w	e595c <sqrtf>

000dcb98 <_ZZN6tflite3ops5micro11elementwise12_GLOBAL__N_19RsqrtEvalEP13TfLiteContextP10TfLiteNodeENUlfE_4_FUNEf>:
TfLiteStatus SqrtEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalNumeric(context, node, std::sqrt);
}

TfLiteStatus RsqrtEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalNumeric(context, node, [](float f) { return 1.f / std::sqrt(f); });
   dcb98:	b508      	push	{r3, lr}
   dcb9a:	f008 fedf 	bl	e595c <sqrtf>
   dcb9e:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   dcba2:	ee87 0a80 	vdiv.f32	s0, s15, s0
   dcba6:	bd08      	pop	{r3, pc}

000dcba8 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>:
  }
  return kTfLiteOk;
}

template <typename T>
inline TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node,
   dcba8:	e92d 4dff 	stmdb	sp!, {r0, r1, r2, r3, r4, r5, r6, r7, r8, sl, fp, lr}
   dcbac:	4690      	mov	r8, r2
   dcbae:	680a      	ldr	r2, [r1, #0]
   dcbb0:	6883      	ldr	r3, [r0, #8]
   dcbb2:	6854      	ldr	r4, [r2, #4]
   dcbb4:	2238      	movs	r2, #56	; 0x38
   dcbb6:	4362      	muls	r2, r4
   dcbb8:	189c      	adds	r4, r3, r2
                             T func(T), TfLiteType expected_type) {
  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
   dcbba:	5c9a      	ldrb	r2, [r3, r2]
   dcbbc:	2a01      	cmp	r2, #1
   dcbbe:	d00d      	beq.n	dcbdc <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x34>
   dcbc0:	4b20      	ldr	r3, [pc, #128]	; (dcc44 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x9c>)
   dcbc2:	9301      	str	r3, [sp, #4]
   dcbc4:	2401      	movs	r4, #1
   dcbc6:	4b20      	ldr	r3, [pc, #128]	; (dcc48 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0xa0>)
   dcbc8:	9202      	str	r2, [sp, #8]
   dcbca:	9300      	str	r3, [sp, #0]
   dcbcc:	9403      	str	r4, [sp, #12]
   dcbce:	6945      	ldr	r5, [r0, #20]
   dcbd0:	4a1e      	ldr	r2, [pc, #120]	; (dcc4c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0xa4>)
   dcbd2:	491f      	ldr	r1, [pc, #124]	; (dcc50 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0xa8>)
   dcbd4:	2339      	movs	r3, #57	; 0x39
   dcbd6:	47a8      	blx	r5
   dcbd8:	4620      	mov	r0, r4
   dcbda:	e030      	b.n	dcc3e <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x96>
   dcbdc:	68a0      	ldr	r0, [r4, #8]
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
   dcbde:	f8d0 e000 	ldr.w	lr, [r0]
   dcbe2:	2200      	movs	r2, #0
inline int NumIntermediates(const TfLiteNode* node) {
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
   dcbe4:	2601      	movs	r6, #1
   dcbe6:	2700      	movs	r7, #0
  for (int i = 0; i < dims->size; ++i) {
   dcbe8:	4596      	cmp	lr, r2
   dcbea:	dd0c      	ble.n	dcc06 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x5e>
    count *= dims->data[i];
   dcbec:	f850 cf04 	ldr.w	ip, [r0, #4]!
   dcbf0:	ea4f 7bec 	mov.w	fp, ip, asr #31
   dcbf4:	fb06 f50b 	mul.w	r5, r6, fp
   dcbf8:	fb0c 5507 	mla	r5, ip, r7, r5
   dcbfc:	fba6 670c 	umull	r6, r7, r6, ip
   dcc00:	442f      	add	r7, r5
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
   dcc02:	3201      	adds	r2, #1
   dcc04:	e7f0      	b.n	dcbe8 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x40>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dcc06:	684a      	ldr	r2, [r1, #4]
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dcc08:	6865      	ldr	r5, [r4, #4]
   dcc0a:	6851      	ldr	r1, [r2, #4]
   dcc0c:	2238      	movs	r2, #56	; 0x38
   dcc0e:	fb02 3301 	mla	r3, r2, r1, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dcc12:	b103      	cbz	r3, dcc16 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x6e>
   dcc14:	685b      	ldr	r3, [r3, #4]
   dcc16:	461c      	mov	r4, r3
  const int64_t num_elements = NumElements(input);
  const T* in_data = GetTensorData<T>(input);
  T* out_data = GetTensorData<T>(output);
  for (int64_t i = 0; i < num_elements; ++i) {
   dcc18:	f04f 0a00 	mov.w	sl, #0
   dcc1c:	f04f 0b00 	mov.w	fp, #0
   dcc20:	45b2      	cmp	sl, r6
   dcc22:	eb7b 0307 	sbcs.w	r3, fp, r7
   dcc26:	da09      	bge.n	dcc3c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x94>
    out_data[i] = func(in_data[i]);
   dcc28:	ecb5 0a01 	vldmia	r5!, {s0}
   dcc2c:	47c0      	blx	r8
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
  const int64_t num_elements = NumElements(input);
  const T* in_data = GetTensorData<T>(input);
  T* out_data = GetTensorData<T>(output);
  for (int64_t i = 0; i < num_elements; ++i) {
   dcc2e:	f11a 0a01 	adds.w	sl, sl, #1
    out_data[i] = func(in_data[i]);
   dcc32:	eca4 0a01 	vstmia	r4!, {s0}
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
  const int64_t num_elements = NumElements(input);
  const T* in_data = GetTensorData<T>(input);
  T* out_data = GetTensorData<T>(output);
  for (int64_t i = 0; i < num_elements; ++i) {
   dcc36:	f14b 0b00 	adc.w	fp, fp, #0
   dcc3a:	e7f1      	b.n	dcc20 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x78>
    out_data[i] = func(in_data[i]);
  }
  return kTfLiteOk;
   dcc3c:	2000      	movs	r0, #0
}
   dcc3e:	b004      	add	sp, #16
   dcc40:	e8bd 8df0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, sl, fp, pc}
   dcc44:	000e9dcc 	.word	0x000e9dcc
   dcc48:	000e9933 	.word	0x000e9933
   dcc4c:	000e9cf5 	.word	0x000e9cf5
   dcc50:	000e98f8 	.word	0x000e98f8

000dcc54 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_110SquareEvalEP13TfLiteContextP10TfLiteNode>:

inline TfLiteStatus EvalNumeric(TfLiteContext* context, TfLiteNode* node,
                                float float_func(float)) {
  return EvalImpl<float>(context, node, float_func, kTfLiteFloat32);
   dcc54:	4a01      	ldr	r2, [pc, #4]	; (dcc5c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_110SquareEvalEP13TfLiteContextP10TfLiteNode+0x8>)
   dcc56:	f7ff bfa7 	b.w	dcba8 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
   dcc5a:	bf00      	nop
   dcc5c:	000dcacb 	.word	0x000dcacb

000dcc60 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17AbsEvalEP13TfLiteContextP10TfLiteNode>:
   dcc60:	4a01      	ldr	r2, [pc, #4]	; (dcc68 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17AbsEvalEP13TfLiteContextP10TfLiteNode+0x8>)
   dcc62:	f7ff bfa1 	b.w	dcba8 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
   dcc66:	bf00      	nop
   dcc68:	000dcac5 	.word	0x000dcac5

000dcc6c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17SinEvalEP13TfLiteContextP10TfLiteNode>:
   dcc6c:	4a01      	ldr	r2, [pc, #4]	; (dcc74 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17SinEvalEP13TfLiteContextP10TfLiteNode+0x8>)
   dcc6e:	f7ff bf9b 	b.w	dcba8 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
   dcc72:	bf00      	nop
   dcc74:	000dcb89 	.word	0x000dcb89

000dcc78 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17CosEvalEP13TfLiteContextP10TfLiteNode>:
   dcc78:	4a01      	ldr	r2, [pc, #4]	; (dcc80 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17CosEvalEP13TfLiteContextP10TfLiteNode+0x8>)
   dcc7a:	f7ff bf95 	b.w	dcba8 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
   dcc7e:	bf00      	nop
   dcc80:	000dcb8d 	.word	0x000dcb8d

000dcc84 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_19RsqrtEvalEP13TfLiteContextP10TfLiteNode>:
   dcc84:	4a01      	ldr	r2, [pc, #4]	; (dcc8c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_19RsqrtEvalEP13TfLiteContextP10TfLiteNode+0x8>)
   dcc86:	f7ff bf8f 	b.w	dcba8 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
   dcc8a:	bf00      	nop
   dcc8c:	000dcb99 	.word	0x000dcb99

000dcc90 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17LogEvalEP13TfLiteContextP10TfLiteNode>:
   dcc90:	4a01      	ldr	r2, [pc, #4]	; (dcc98 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17LogEvalEP13TfLiteContextP10TfLiteNode+0x8>)
   dcc92:	f7ff bf89 	b.w	dcba8 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
   dcc96:	bf00      	nop
   dcc98:	000dcb91 	.word	0x000dcb91

000dcc9c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18SqrtEvalEP13TfLiteContextP10TfLiteNode>:
   dcc9c:	4a01      	ldr	r2, [pc, #4]	; (dcca4 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18SqrtEvalEP13TfLiteContextP10TfLiteNode+0x8>)
   dcc9e:	f7ff bf83 	b.w	dcba8 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
   dcca2:	bf00      	nop
   dcca4:	000dcb95 	.word	0x000dcb95

000dcca8 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus SquareEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalNumeric(context, node, [](float f) { return f * f; });
}

TfLiteStatus LogicalNotEval(TfLiteContext* context, TfLiteNode* node) {
   dcca8:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dccac:	680a      	ldr	r2, [r1, #0]
   dccae:	6883      	ldr	r3, [r0, #8]
   dccb0:	460f      	mov	r7, r1
   dccb2:	6851      	ldr	r1, [r2, #4]
   dccb4:	2238      	movs	r2, #56	; 0x38
   dccb6:	434a      	muls	r2, r1
   dccb8:	189e      	adds	r6, r3, r2
template <typename T>
inline TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node,
                             T func(T), TfLiteType expected_type) {
  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
   dccba:	5c9a      	ldrb	r2, [r3, r2]
   dccbc:	2a06      	cmp	r2, #6

TfLiteStatus SquareEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalNumeric(context, node, [](float f) { return f * f; });
}

TfLiteStatus LogicalNotEval(TfLiteContext* context, TfLiteNode* node) {
   dccbe:	b085      	sub	sp, #20
template <typename T>
inline TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node,
                             T func(T), TfLiteType expected_type) {
  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
   dccc0:	d00d      	beq.n	dccde <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x36>
   dccc2:	2306      	movs	r3, #6
   dccc4:	9303      	str	r3, [sp, #12]
   dccc6:	4b1f      	ldr	r3, [pc, #124]	; (dcd44 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x9c>)
   dccc8:	9301      	str	r3, [sp, #4]
   dccca:	4b1f      	ldr	r3, [pc, #124]	; (dcd48 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0xa0>)
   dcccc:	9202      	str	r2, [sp, #8]
   dccce:	9300      	str	r3, [sp, #0]
   dccd0:	6944      	ldr	r4, [r0, #20]
   dccd2:	4a1e      	ldr	r2, [pc, #120]	; (dcd4c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0xa4>)
   dccd4:	491e      	ldr	r1, [pc, #120]	; (dcd50 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0xa8>)
   dccd6:	2339      	movs	r3, #57	; 0x39
   dccd8:	47a0      	blx	r4
   dccda:	2001      	movs	r0, #1
   dccdc:	e02f      	b.n	dcd3e <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x96>
   dccde:	68b2      	ldr	r2, [r6, #8]
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
   dcce0:	f8d2 c000 	ldr.w	ip, [r2]
   dcce4:	2400      	movs	r4, #0
inline int NumIntermediates(const TfLiteNode* node) {
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
   dcce6:	2001      	movs	r0, #1
   dcce8:	2100      	movs	r1, #0
  for (int i = 0; i < dims->size; ++i) {
   dccea:	45a4      	cmp	ip, r4
   dccec:	dd0c      	ble.n	dcd08 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x60>
    count *= dims->data[i];
   dccee:	f852 ef04 	ldr.w	lr, [r2, #4]!
   dccf2:	ea4f 79ee 	mov.w	r9, lr, asr #31
   dccf6:	fb00 f509 	mul.w	r5, r0, r9
   dccfa:	fb0e 5501 	mla	r5, lr, r1, r5
   dccfe:	fba0 010e 	umull	r0, r1, r0, lr
   dcd02:	4429      	add	r1, r5
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
   dcd04:	3401      	adds	r4, #1
   dcd06:	e7f0      	b.n	dccea <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x42>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dcd08:	687a      	ldr	r2, [r7, #4]
   dcd0a:	6852      	ldr	r2, [r2, #4]
   dcd0c:	2438      	movs	r4, #56	; 0x38
   dcd0e:	fb04 3302 	mla	r3, r4, r2, r3
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dcd12:	6872      	ldr	r2, [r6, #4]

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dcd14:	b103      	cbz	r3, dcd18 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x70>
   dcd16:	685b      	ldr	r3, [r3, #4]
   dcd18:	3a01      	subs	r2, #1
   dcd1a:	3b01      	subs	r3, #1
  const int64_t num_elements = NumElements(input);
  const T* in_data = GetTensorData<T>(input);
  T* out_data = GetTensorData<T>(output);
  for (int64_t i = 0; i < num_elements; ++i) {
   dcd1c:	2400      	movs	r4, #0
   dcd1e:	2500      	movs	r5, #0
   dcd20:	4284      	cmp	r4, r0
   dcd22:	eb75 0601 	sbcs.w	r6, r5, r1
   dcd26:	da09      	bge.n	dcd3c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x94>
    out_data[i] = func(in_data[i]);
   dcd28:	f812 6f01 	ldrb.w	r6, [r2, #1]!
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
  const int64_t num_elements = NumElements(input);
  const T* in_data = GetTensorData<T>(input);
  T* out_data = GetTensorData<T>(output);
  for (int64_t i = 0; i < num_elements; ++i) {
   dcd2c:	3401      	adds	r4, #1
    out_data[i] = func(in_data[i]);
   dcd2e:	f086 0601 	eor.w	r6, r6, #1
   dcd32:	f803 6f01 	strb.w	r6, [r3, #1]!
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
  const int64_t num_elements = NumElements(input);
  const T* in_data = GetTensorData<T>(input);
  T* out_data = GetTensorData<T>(output);
  for (int64_t i = 0; i < num_elements; ++i) {
   dcd36:	f145 0500 	adc.w	r5, r5, #0
   dcd3a:	e7f1      	b.n	dcd20 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x78>
    out_data[i] = func(in_data[i]);
  }
  return kTfLiteOk;
   dcd3c:	2000      	movs	r0, #0
  return EvalNumeric(context, node, [](float f) { return f * f; });
}

TfLiteStatus LogicalNotEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalLogical(context, node, [](bool v) { return !v; });
}
   dcd3e:	b005      	add	sp, #20
   dcd40:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}
   dcd44:	000e9dcc 	.word	0x000e9dcc
   dcd48:	000e9933 	.word	0x000e9933
   dcd4c:	000e9cf5 	.word	0x000e9cf5
   dcd50:	000e98f8 	.word	0x000e98f8

000dcd54 <_ZN6tflite3ops5micro12Register_ABSEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::AbsEval};
  return &r;
}
   dcd54:	4800      	ldr	r0, [pc, #0]	; (dcd58 <_ZN6tflite3ops5micro12Register_ABSEv+0x4>)
   dcd56:	4770      	bx	lr
   dcd58:	2003bfa8 	.word	0x2003bfa8

000dcd5c <_ZN6tflite3ops5micro12Register_SINEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::SinEval};
  return &r;
}
   dcd5c:	4800      	ldr	r0, [pc, #0]	; (dcd60 <_ZN6tflite3ops5micro12Register_SINEv+0x4>)
   dcd5e:	4770      	bx	lr
   dcd60:	2003bf88 	.word	0x2003bf88

000dcd64 <_ZN6tflite3ops5micro12Register_COSEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::CosEval};
  return &r;
}
   dcd64:	4800      	ldr	r0, [pc, #0]	; (dcd68 <_ZN6tflite3ops5micro12Register_COSEv+0x4>)
   dcd66:	4770      	bx	lr
   dcd68:	2003bf08 	.word	0x2003bf08

000dcd6c <_ZN6tflite3ops5micro12Register_LOGEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::LogEval};
  return &r;
}
   dcd6c:	4800      	ldr	r0, [pc, #0]	; (dcd70 <_ZN6tflite3ops5micro12Register_LOGEv+0x4>)
   dcd6e:	4770      	bx	lr
   dcd70:	2003bec8 	.word	0x2003bec8

000dcd74 <_ZN6tflite3ops5micro13Register_SQRTEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::SqrtEval};
  return &r;
}
   dcd74:	4800      	ldr	r0, [pc, #0]	; (dcd78 <_ZN6tflite3ops5micro13Register_SQRTEv+0x4>)
   dcd76:	4770      	bx	lr
   dcd78:	2003bf68 	.word	0x2003bf68

000dcd7c <_ZN6tflite3ops5micro14Register_RSQRTEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::RsqrtEval};
  return &r;
}
   dcd7c:	4800      	ldr	r0, [pc, #0]	; (dcd80 <_ZN6tflite3ops5micro14Register_RSQRTEv+0x4>)
   dcd7e:	4770      	bx	lr
   dcd80:	2003bf48 	.word	0x2003bf48

000dcd84 <_ZN6tflite3ops5micro15Register_SQUAREEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::SquareEval};
  return &r;
}
   dcd84:	4800      	ldr	r0, [pc, #0]	; (dcd88 <_ZN6tflite3ops5micro15Register_SQUAREEv+0x4>)
   dcd86:	4770      	bx	lr
   dcd88:	2003bee8 	.word	0x2003bee8

000dcd8c <_ZN6tflite3ops5micro20Register_LOGICAL_NOTEv>:
  static TfLiteRegistration r = {
      /*init=*/nullptr, /*free=*/nullptr,
      elementwise::GenericPrepare<elementwise::IsLogicalSupportedType>,
      elementwise::LogicalNotEval};
  return &r;
}
   dcd8c:	4800      	ldr	r0, [pc, #0]	; (dcd90 <_ZN6tflite3ops5micro20Register_LOGICAL_NOTEv+0x4>)
   dcd8e:	4770      	bx	lr
   dcd90:	2003bf28 	.word	0x2003bf28

000dcd94 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode>:
namespace floor {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dcd94:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dcd98:	680b      	ldr	r3, [r1, #0]
   dcd9a:	6882      	ldr	r2, [r0, #8]
   dcd9c:	685b      	ldr	r3, [r3, #4]
   dcd9e:	2438      	movs	r4, #56	; 0x38
   dcda0:	4363      	muls	r3, r4
   dcda2:	18d5      	adds	r5, r2, r3
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
   dcda4:	5cd3      	ldrb	r3, [r2, r3]
   dcda6:	2b01      	cmp	r3, #1
namespace floor {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dcda8:	b08e      	sub	sp, #56	; 0x38
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
   dcdaa:	d00d      	beq.n	dcdc8 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x34>
   dcdac:	9302      	str	r3, [sp, #8]
   dcdae:	4b2b      	ldr	r3, [pc, #172]	; (dce5c <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xc8>)
   dcdb0:	9301      	str	r3, [sp, #4]
   dcdb2:	2401      	movs	r4, #1
   dcdb4:	4b2a      	ldr	r3, [pc, #168]	; (dce60 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xcc>)
   dcdb6:	9300      	str	r3, [sp, #0]
   dcdb8:	9403      	str	r4, [sp, #12]
   dcdba:	6945      	ldr	r5, [r0, #20]
   dcdbc:	4a29      	ldr	r2, [pc, #164]	; (dce64 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xd0>)
   dcdbe:	492a      	ldr	r1, [pc, #168]	; (dce68 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xd4>)
   dcdc0:	231f      	movs	r3, #31
   dcdc2:	47a8      	blx	r5
   dcdc4:	4620      	mov	r0, r4
   dcdc6:	e046      	b.n	dce56 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xc2>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dcdc8:	684b      	ldr	r3, [r1, #4]
   dcdca:	685b      	ldr	r3, [r3, #4]
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  reference_ops::Floor(GetTensorShape(input), GetTensorData<float>(input),
   dcdcc:	4629      	mov	r1, r5
   dcdce:	fb04 2403 	mla	r4, r4, r3, r2
   dcdd2:	a804      	add	r0, sp, #16
   dcdd4:	f7f9 fde7 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                       GetTensorShape(output), GetTensorData<float>(output));
   dcdd8:	4621      	mov	r1, r4
   dcdda:	a809      	add	r0, sp, #36	; 0x24
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dcddc:	686e      	ldr	r6, [r5, #4]
   dcdde:	f7f9 fde2 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dcde2:	b104      	cbz	r4, dcde6 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x52>
   dcde4:	6864      	ldr	r4, [r4, #4]
   dcde6:	9f04      	ldr	r7, [sp, #16]

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   dcde8:	9b09      	ldr	r3, [sp, #36]	; 0x24
   dcdea:	429f      	cmp	r7, r3
   dcdec:	d101      	bne.n	dcdf2 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x5e>
   dcdee:	2500      	movs	r5, #0
   dcdf0:	e00d      	b.n	dce0e <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x7a>
   dcdf2:	f007 fa9b 	bl	e432c <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   dcdf6:	4629      	mov	r1, r5
   dcdf8:	a804      	add	r0, sp, #16
   dcdfa:	f7f9 fb2f 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dcdfe:	4629      	mov	r1, r5
   dce00:	4680      	mov	r8, r0
   dce02:	a809      	add	r0, sp, #36	; 0x24
   dce04:	f7f9 fb2a 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dce08:	4580      	cmp	r8, r0
   dce0a:	d1f2      	bne.n	dcdf2 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x5e>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   dce0c:	3501      	adds	r5, #1
   dce0e:	42af      	cmp	r7, r5
   dce10:	dcf1      	bgt.n	dcdf6 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x62>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   dce12:	2f04      	cmp	r7, #4
   dce14:	bfcc      	ite	gt
   dce16:	9a05      	ldrgt	r2, [sp, #20]
   dce18:	aa05      	addle	r2, sp, #20
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dce1a:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
   dce1c:	f04f 0801 	mov.w	r8, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dce20:	429f      	cmp	r7, r3
   dce22:	dd05      	ble.n	dce30 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x9c>
      buffer_size *= dims_data[i];
   dce24:	f852 1023 	ldr.w	r1, [r2, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dce28:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
   dce2a:	fb01 f808 	mul.w	r8, r1, r8
   dce2e:	e7f7      	b.n	dce20 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x8c>
   dce30:	4635      	mov	r5, r6
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dce32:	2600      	movs	r6, #0

inline void Floor(const RuntimeShape& input_shape, const float* input_data,
                  const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; i++) {
   dce34:	4546      	cmp	r6, r8
   dce36:	da07      	bge.n	dce48 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xb4>
  using ::floor;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  floor(float __x)
  { return __builtin_floorf(__x); }
   dce38:	ecb5 0a01 	vldmia	r5!, {s0}
   dce3c:	f008 faf0 	bl	e5420 <floorf>
   dce40:	3601      	adds	r6, #1
    int offset = i;
    output_data[offset] = std::floor(input_data[offset]);
   dce42:	eca4 0a01 	vstmia	r4!, {s0}
   dce46:	e7f5      	b.n	dce34 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xa0>
   dce48:	a809      	add	r0, sp, #36	; 0x24
   dce4a:	f7f9 fafc 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  reference_ops::Floor(GetTensorShape(input), GetTensorData<float>(input),
   dce4e:	a804      	add	r0, sp, #16
   dce50:	f7f9 faf9 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                       GetTensorShape(output), GetTensorData<float>(output));
  return kTfLiteOk;
   dce54:	2000      	movs	r0, #0
}
   dce56:	b00e      	add	sp, #56	; 0x38
   dce58:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   dce5c:	000ea167 	.word	0x000ea167
   dce60:	000e9933 	.word	0x000e9933
   dce64:	000e9dda 	.word	0x000e9dda
   dce68:	000e98f8 	.word	0x000e98f8

000dce6c <_ZN6tflite3ops5micro14Register_FLOOREv>:
TfLiteRegistration* Register_FLOOR() {
  static TfLiteRegistration r = {/*init=*/nullptr,
                                 /*free=*/nullptr, /*prepare=*/nullptr,
                                 floor::Eval};
  return &r;
}
   dce6c:	4800      	ldr	r0, [pc, #0]	; (dce70 <_ZN6tflite3ops5micro14Register_FLOOREv+0x4>)
   dce6e:	4770      	bx	lr
   dce70:	2003bfc8 	.word	0x2003bfc8

000dce74 <_ZN6tflite3ops5micro15fully_connected4InitEP13TfLiteContextPKcj>:

}  // namespace

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
   dce74:	2000      	movs	r0, #0
   dce76:	4770      	bx	lr

000dce78 <_ZN6tflite3ops5micro15fully_connected4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
   dce78:	4770      	bx	lr

000dce7a <_ZN6tflite3ops5micro15fully_connected7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   dce7a:	2000      	movs	r0, #0
   dce7c:	4770      	bx	lr

000dce7e <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi>:
}

// Data is required to be contiguous, and so many operators can use either the
// full array flat size or the flat size with one dimension skipped (commonly
// the depth).
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
   dce7e:	b538      	push	{r3, r4, r5, lr}
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
   dce80:	2900      	cmp	r1, #0
   dce82:	6804      	ldr	r4, [r0, #0]
   dce84:	db01      	blt.n	dce8a <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi+0xc>
   dce86:	42a1      	cmp	r1, r4
   dce88:	db01      	blt.n	dce8e <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi+0x10>
   dce8a:	f007 fa4f 	bl	e432c <abort>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   dce8e:	2c04      	cmp	r4, #4
   dce90:	bfcc      	ite	gt
   dce92:	6843      	ldrgt	r3, [r0, #4]
   dce94:	1d03      	addle	r3, r0, #4
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
  const auto* dims_data = shape.DimsData();
  int flat_size = 1;
  for (int i = 0; i < dims_count; ++i) {
   dce96:	2200      	movs	r2, #0
// the depth).
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
  const auto* dims_data = shape.DimsData();
  int flat_size = 1;
   dce98:	2001      	movs	r0, #1
  for (int i = 0; i < dims_count; ++i) {
   dce9a:	42a2      	cmp	r2, r4
   dce9c:	da07      	bge.n	dceae <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi+0x30>
    flat_size *= (i == skip_dim) ? 1 : dims_data[i];
   dce9e:	428a      	cmp	r2, r1
   dcea0:	bf14      	ite	ne
   dcea2:	f853 5022 	ldrne.w	r5, [r3, r2, lsl #2]
   dcea6:	2501      	moveq	r5, #1
   dcea8:	4368      	muls	r0, r5
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
  const auto* dims_data = shape.DimsData();
  int flat_size = 1;
  for (int i = 0; i < dims_count; ++i) {
   dceaa:	3201      	adds	r2, #1
   dceac:	e7f5      	b.n	dce9a <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi+0x1c>
    flat_size *= (i == skip_dim) ? 1 : dims_data[i];
  }
  return flat_size;
}
   dceae:	bd38      	pop	{r3, r4, r5, pc}

000dceb0 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph>:
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    uint8* output_data) {
   dceb0:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dceb4:	b08b      	sub	sp, #44	; 0x2c
   dceb6:	461d      	mov	r5, r3
  const int32 input_offset = params.input_offset;
   dceb8:	6803      	ldr	r3, [r0, #0]
   dceba:	9302      	str	r3, [sp, #8]
  const int32 filter_offset = params.weights_offset;
   dcebc:	6843      	ldr	r3, [r0, #4]
   dcebe:	9303      	str	r3, [sp, #12]
  const int32 output_offset = params.output_offset;
   dcec0:	6883      	ldr	r3, [r0, #8]
   dcec2:	9304      	str	r3, [sp, #16]
   dcec4:	682e      	ldr	r6, [r5, #0]
  const int32 output_multiplier = params.output_multiplier;
   dcec6:	68c3      	ldr	r3, [r0, #12]
   dcec8:	9305      	str	r3, [sp, #20]
  const int output_shift = params.output_shift;
   dceca:	6903      	ldr	r3, [r0, #16]
   dcecc:	9306      	str	r3, [sp, #24]
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;
  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);
   dcece:	2e01      	cmp	r6, #1
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
   dced0:	6943      	ldr	r3, [r0, #20]
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    uint8* output_data) {
   dced2:	9f17      	ldr	r7, [sp, #92]	; 0x5c
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
   dced4:	9301      	str	r3, [sp, #4]
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    uint8* output_data) {
   dced6:	4614      	mov	r4, r2
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;
   dced8:	f8d0 b018 	ldr.w	fp, [r0, #24]
  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);
   dcedc:	dc01      	bgt.n	dcee2 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0x32>
   dcede:	f007 fa25 	bl	e432c <abort>
   dcee2:	683b      	ldr	r3, [r7, #0]
  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);
   dcee4:	2b00      	cmp	r3, #0
   dcee6:	ddfa      	ble.n	dcede <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0x2e>

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   dcee8:	9a01      	ldr	r2, [sp, #4]
   dceea:	455a      	cmp	r2, fp
   dceec:	dcf7      	bgt.n	dcede <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0x2e>
  // but the current --variable_batch hack consists in overwriting the 3rd
  // dimension with the runtime batch size, as we don't keep track for each
  // array of which dimension is the batch dimension in it.
  const int output_dim_count = output_shape.DimensionsCount();
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
   dceee:	f103 38ff 	add.w	r8, r3, #4294967295	; 0xffffffff
   dcef2:	4641      	mov	r1, r8
   dcef4:	4638      	mov	r0, r7
   dcef6:	f7ff ffc2 	bl	dce7e <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi>
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
   dcefa:	4643      	mov	r3, r8
   dcefc:	463a      	mov	r2, r7
   dcefe:	1eb1      	subs	r1, r6, #2
  // but the current --variable_batch hack consists in overwriting the 3rd
  // dimension with the runtime batch size, as we don't keep track for each
  // array of which dimension is the batch dimension in it.
  const int output_dim_count = output_shape.DimensionsCount();
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
   dcf00:	9007      	str	r0, [sp, #28]
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
   dcf02:	4628      	mov	r0, r5
   dcf04:	f7fe fe5b 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
   dcf08:	1e71      	subs	r1, r6, #1
  // array of which dimension is the batch dimension in it.
  const int output_dim_count = output_shape.DimensionsCount();
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
   dcf0a:	4682      	mov	sl, r0
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
   dcf0c:	4628      	mov	r0, r5
   dcf0e:	f7f9 faa5 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dcf12:	f8dd 8060 	ldr.w	r8, [sp, #96]	; 0x60
   dcf16:	4606      	mov	r6, r0
   dcf18:	4267      	negs	r7, r4
  for (int b = 0; b < batches; ++b) {
   dcf1a:	f04f 0900 	mov.w	r9, #0
   dcf1e:	9b07      	ldr	r3, [sp, #28]
   dcf20:	4599      	cmp	r9, r3
   dcf22:	da39      	bge.n	dcf98 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0xe8>
   dcf24:	9b14      	ldr	r3, [sp, #80]	; 0x50
   dcf26:	2500      	movs	r5, #0
    for (int out_c = 0; out_c < output_depth; ++out_c) {
   dcf28:	4555      	cmp	r5, sl
   dcf2a:	da2f      	bge.n	dcf8c <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0xdc>
   dcf2c:	469c      	mov	ip, r3
   dcf2e:	46a6      	mov	lr, r4
      int32 acc = 0;
   dcf30:	2000      	movs	r0, #0
      for (int d = 0; d < accum_depth; ++d) {
   dcf32:	eb0e 0207 	add.w	r2, lr, r7
   dcf36:	4296      	cmp	r6, r2
   dcf38:	dd0f      	ble.n	dcf5a <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0xaa>
        int32 input_val = input_data[b * accum_depth + d];
   dcf3a:	f81e 2b01 	ldrb.w	r2, [lr], #1
   dcf3e:	9208      	str	r2, [sp, #32]
        int32 filter_val = filter_data[out_c * accum_depth + d];
        acc += (filter_val + filter_offset) * (input_val + input_offset);
   dcf40:	9903      	ldr	r1, [sp, #12]
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
      int32 acc = 0;
      for (int d = 0; d < accum_depth; ++d) {
        int32 input_val = input_data[b * accum_depth + d];
        int32 filter_val = filter_data[out_c * accum_depth + d];
   dcf42:	f81c 2b01 	ldrb.w	r2, [ip], #1
        acc += (filter_val + filter_offset) * (input_val + input_offset);
   dcf46:	440a      	add	r2, r1
   dcf48:	9209      	str	r2, [sp, #36]	; 0x24
   dcf4a:	9902      	ldr	r1, [sp, #8]
   dcf4c:	9a08      	ldr	r2, [sp, #32]
   dcf4e:	440a      	add	r2, r1
   dcf50:	4611      	mov	r1, r2
   dcf52:	9a09      	ldr	r2, [sp, #36]	; 0x24
   dcf54:	fb01 0002 	mla	r0, r1, r2, r0
                                       output_shape, output_dim_count - 1);
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
      int32 acc = 0;
      for (int d = 0; d < accum_depth; ++d) {
   dcf58:	e7eb      	b.n	dcf32 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0x82>
        int32 input_val = input_data[b * accum_depth + d];
        int32 filter_val = filter_data[out_c * accum_depth + d];
        acc += (filter_val + filter_offset) * (input_val + input_offset);
      }
      if (bias_data) {
   dcf5a:	9a16      	ldr	r2, [sp, #88]	; 0x58
   dcf5c:	b112      	cbz	r2, dcf64 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0xb4>
        acc += bias_data[out_c];
   dcf5e:	f852 2025 	ldr.w	r2, [r2, r5, lsl #2]
   dcf62:	4410      	add	r0, r2
      }
      acc = MultiplyByQuantizedMultiplier(acc, output_multiplier, output_shift);
   dcf64:	9a06      	ldr	r2, [sp, #24]
   dcf66:	9905      	ldr	r1, [sp, #20]
   dcf68:	9308      	str	r3, [sp, #32]
   dcf6a:	f7fe fe37 	bl	dbbdc <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
      acc += output_offset;
   dcf6e:	9b04      	ldr	r3, [sp, #16]
   dcf70:	4418      	add	r0, r3
   dcf72:	9b01      	ldr	r3, [sp, #4]
   dcf74:	4298      	cmp	r0, r3
   dcf76:	bfb8      	it	lt
   dcf78:	4618      	movlt	r0, r3
      acc = std::max(acc, output_activation_min);
      acc = std::min(acc, output_activation_max);
      output_data[out_c + output_depth * b] = static_cast<uint8>(acc);
   dcf7a:	4558      	cmp	r0, fp
   dcf7c:	9b08      	ldr	r3, [sp, #32]
   dcf7e:	bfa8      	it	ge
   dcf80:	4658      	movge	r0, fp
   dcf82:	f808 0005 	strb.w	r0, [r8, r5]
   dcf86:	4433      	add	r3, r6
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
   dcf88:	3501      	adds	r5, #1
   dcf8a:	e7cd      	b.n	dcf28 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0x78>
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
   dcf8c:	f109 0901 	add.w	r9, r9, #1
   dcf90:	44d0      	add	r8, sl
   dcf92:	4434      	add	r4, r6
   dcf94:	1bbf      	subs	r7, r7, r6
   dcf96:	e7c2      	b.n	dcf1e <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0x6e>
      acc = std::max(acc, output_activation_min);
      acc = std::min(acc, output_activation_max);
      output_data[out_c + output_depth * b] = static_cast<uint8>(acc);
    }
  }
}
   dcf98:	b00b      	add	sp, #44	; 0x2c
   dcf9a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dcf9e <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps>:
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int16* output_data) {
   dcf9e:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dcfa2:	b089      	sub	sp, #36	; 0x24
   dcfa4:	461e      	mov	r6, r3
  const int32 input_offset = params.input_offset;
   dcfa6:	6803      	ldr	r3, [r0, #0]
   dcfa8:	9301      	str	r3, [sp, #4]
  const int32 filter_offset = params.weights_offset;
   dcfaa:	6843      	ldr	r3, [r0, #4]
   dcfac:	9302      	str	r3, [sp, #8]
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
   dcfae:	68c3      	ldr	r3, [r0, #12]
   dcfb0:	9303      	str	r3, [sp, #12]
  const int output_shift = params.output_shift;
   dcfb2:	6903      	ldr	r3, [r0, #16]
   dcfb4:	9304      	str	r3, [sp, #16]
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;
   dcfb6:	f8d0 a018 	ldr.w	sl, [r0, #24]
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
   dcfba:	6943      	ldr	r3, [r0, #20]
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int16* output_data) {
   dcfbc:	f8dd 8054 	ldr.w	r8, [sp, #84]	; 0x54
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
   dcfc0:	6885      	ldr	r5, [r0, #8]
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
   dcfc2:	9300      	str	r3, [sp, #0]
  const int32 output_activation_max = params.quantized_activation_max;

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   dcfc4:	4553      	cmp	r3, sl
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int16* output_data) {
   dcfc6:	4614      	mov	r4, r2
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   dcfc8:	dd01      	ble.n	dcfce <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0x30>
   dcfca:	f007 f9af 	bl	e432c <abort>
  TFLITE_DCHECK_EQ(output_offset, 0);
   dcfce:	2d00      	cmp	r5, #0
   dcfd0:	d1fb      	bne.n	dcfca <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0x2c>
  // but the current --variable_batch hack consists in overwriting the 3rd
  // dimension with the runtime batch size, as we don't keep track for each
  // array of which dimension is the batch dimension in it.
  const int output_dim_count = output_shape.DimensionsCount();
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
   dcfd2:	f8d8 3000 	ldr.w	r3, [r8]
   dcfd6:	6837      	ldr	r7, [r6, #0]
   dcfd8:	f103 39ff 	add.w	r9, r3, #4294967295	; 0xffffffff
   dcfdc:	4649      	mov	r1, r9
   dcfde:	4640      	mov	r0, r8
   dcfe0:	f7ff ff4d 	bl	dce7e <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi>
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
   dcfe4:	464b      	mov	r3, r9
   dcfe6:	4642      	mov	r2, r8
   dcfe8:	1eb9      	subs	r1, r7, #2
  // but the current --variable_batch hack consists in overwriting the 3rd
  // dimension with the runtime batch size, as we don't keep track for each
  // array of which dimension is the batch dimension in it.
  const int output_dim_count = output_shape.DimensionsCount();
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
   dcfea:	9005      	str	r0, [sp, #20]
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
   dcfec:	4630      	mov	r0, r6
   dcfee:	f7fe fde6 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
   dcff2:	1e79      	subs	r1, r7, #1
  // array of which dimension is the batch dimension in it.
  const int output_dim_count = output_shape.DimensionsCount();
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
   dcff4:	4683      	mov	fp, r0
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
   dcff6:	4630      	mov	r0, r6
   dcff8:	f7f9 fa30 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dcffc:	ea4f 034b 	mov.w	r3, fp, lsl #1
   dd000:	f8dd 9058 	ldr.w	r9, [sp, #88]	; 0x58
   dd004:	9306      	str	r3, [sp, #24]
   dd006:	4607      	mov	r7, r0
   dd008:	f1c4 0800 	rsb	r8, r4, #0
  for (int b = 0; b < batches; ++b) {
   dd00c:	9b05      	ldr	r3, [sp, #20]
   dd00e:	429d      	cmp	r5, r3
   dd010:	da33      	bge.n	dd07a <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0xdc>
   dd012:	9b12      	ldr	r3, [sp, #72]	; 0x48
   dd014:	2600      	movs	r6, #0
    for (int out_c = 0; out_c < output_depth; ++out_c) {
   dd016:	455e      	cmp	r6, fp
   dd018:	da28      	bge.n	dd06c <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0xce>
      // Internal accumulation.
      // Initialize accumulator with the bias-value.
      int32 accum = bias_data[out_c];
   dd01a:	9a14      	ldr	r2, [sp, #80]	; 0x50
   dd01c:	f852 0026 	ldr.w	r0, [r2, r6, lsl #2]
   dd020:	469c      	mov	ip, r3
   dd022:	46a6      	mov	lr, r4
      // Accumulation loop.
      for (int d = 0; d < accum_depth; ++d) {
   dd024:	eb08 020e 	add.w	r2, r8, lr
   dd028:	4297      	cmp	r7, r2
   dd02a:	dd0d      	ble.n	dd048 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0xaa>
        int16 input_val = input_data[b * accum_depth + d] + input_offset;
   dd02c:	9901      	ldr	r1, [sp, #4]
   dd02e:	f81e 2b01 	ldrb.w	r2, [lr], #1
   dd032:	440a      	add	r2, r1
   dd034:	9207      	str	r2, [sp, #28]
        int16 filter_val = filter_data[out_c * accum_depth + d] + filter_offset;
   dd036:	f81c 1b01 	ldrb.w	r1, [ip], #1
   dd03a:	9a02      	ldr	r2, [sp, #8]
   dd03c:	4411      	add	r1, r2
        accum += filter_val * input_val;
   dd03e:	f8bd 201c 	ldrh.w	r2, [sp, #28]
   dd042:	fb11 0002 	smlabb	r0, r1, r2, r0
    for (int out_c = 0; out_c < output_depth; ++out_c) {
      // Internal accumulation.
      // Initialize accumulator with the bias-value.
      int32 accum = bias_data[out_c];
      // Accumulation loop.
      for (int d = 0; d < accum_depth; ++d) {
   dd046:	e7ed      	b.n	dd024 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0x86>
      // Down-scale the final int32 accumulator to the scale used by our
      // (16-bit, typically 3 integer bits) fixed-point format. The quantized
      // multiplier and shift here have been pre-computed offline
      // (e.g. by toco).
      accum =
          MultiplyByQuantizedMultiplier(accum, output_multiplier, output_shift);
   dd048:	9a04      	ldr	r2, [sp, #16]
   dd04a:	9903      	ldr	r1, [sp, #12]
   dd04c:	9307      	str	r3, [sp, #28]
   dd04e:	f7fe fdc5 	bl	dbbdc <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
   dd052:	9b00      	ldr	r3, [sp, #0]
   dd054:	4298      	cmp	r0, r3
   dd056:	bfb8      	it	lt
   dd058:	4618      	movlt	r0, r3
      // Saturate, cast to int16, and store to output array.
      accum = std::max(accum, output_activation_min - output_offset);
      accum = std::min(accum, output_activation_max - output_offset);
      accum += output_offset;
      output_data[out_c + output_depth * b] = accum;
   dd05a:	4550      	cmp	r0, sl
   dd05c:	9b07      	ldr	r3, [sp, #28]
   dd05e:	bfa8      	it	ge
   dd060:	4650      	movge	r0, sl
   dd062:	f829 0016 	strh.w	r0, [r9, r6, lsl #1]
   dd066:	443b      	add	r3, r7
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
   dd068:	3601      	adds	r6, #1
   dd06a:	e7d4      	b.n	dd016 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0x78>
   dd06c:	9b06      	ldr	r3, [sp, #24]
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
   dd06e:	3501      	adds	r5, #1
   dd070:	4499      	add	r9, r3
   dd072:	443c      	add	r4, r7
   dd074:	ebc7 0808 	rsb	r8, r7, r8
   dd078:	e7c8      	b.n	dd00c <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0x6e>
      accum = std::min(accum, output_activation_max - output_offset);
      accum += output_offset;
      output_data[out_c + output_depth * b] = accum;
    }
  }
}
   dd07a:	b009      	add	sp, #36	; 0x24
   dd07c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dd080 <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa>:
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const int8_t* input_data, const RuntimeShape& filter_shape,
    const int8_t* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8_t* output_data) {
   dd080:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dd084:	b08b      	sub	sp, #44	; 0x2c
   dd086:	461d      	mov	r5, r3
  const int32 input_offset = params.input_offset;
   dd088:	6803      	ldr	r3, [r0, #0]
   dd08a:	9302      	str	r3, [sp, #8]
  const int32 filter_offset = params.weights_offset;
   dd08c:	6843      	ldr	r3, [r0, #4]
   dd08e:	9303      	str	r3, [sp, #12]
  const int32 output_offset = params.output_offset;
   dd090:	6883      	ldr	r3, [r0, #8]
   dd092:	9304      	str	r3, [sp, #16]
   dd094:	682e      	ldr	r6, [r5, #0]
  const int32 output_multiplier = params.output_multiplier;
   dd096:	68c3      	ldr	r3, [r0, #12]
   dd098:	9305      	str	r3, [sp, #20]
  const int output_shift = params.output_shift;
   dd09a:	6903      	ldr	r3, [r0, #16]
   dd09c:	9306      	str	r3, [sp, #24]
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;
  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);
   dd09e:	2e01      	cmp	r6, #1
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
   dd0a0:	6943      	ldr	r3, [r0, #20]
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const int8_t* input_data, const RuntimeShape& filter_shape,
    const int8_t* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8_t* output_data) {
   dd0a2:	9f17      	ldr	r7, [sp, #92]	; 0x5c
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
   dd0a4:	9301      	str	r3, [sp, #4]
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const int8_t* input_data, const RuntimeShape& filter_shape,
    const int8_t* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8_t* output_data) {
   dd0a6:	4614      	mov	r4, r2
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;
   dd0a8:	f8d0 b018 	ldr.w	fp, [r0, #24]
  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);
   dd0ac:	dc01      	bgt.n	dd0b2 <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x32>
   dd0ae:	f007 f93d 	bl	e432c <abort>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 2);
   dd0b2:	683b      	ldr	r3, [r7, #0]
   dd0b4:	2b02      	cmp	r3, #2
   dd0b6:	d1fa      	bne.n	dd0ae <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x2e>

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   dd0b8:	9b01      	ldr	r3, [sp, #4]
   dd0ba:	455b      	cmp	r3, fp
   dd0bc:	dcf7      	bgt.n	dd0ae <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x2e>
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = output_shape.Dims(0);
   dd0be:	2100      	movs	r1, #0
   dd0c0:	4638      	mov	r0, r7
   dd0c2:	f7f9 f9cb 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_depth = output_shape.Dims(1);
   dd0c6:	2101      	movs	r1, #1
  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 2);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = output_shape.Dims(0);
   dd0c8:	9007      	str	r0, [sp, #28]
  const int output_depth = output_shape.Dims(1);
   dd0ca:	4638      	mov	r0, r7
   dd0cc:	f7f9 f9c6 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  TFLITE_DCHECK_LE(output_depth, filter_shape.Dims(filter_dim_count - 2));
   dd0d0:	1eb1      	subs	r1, r6, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 2);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = output_shape.Dims(0);
  const int output_depth = output_shape.Dims(1);
   dd0d2:	4681      	mov	r9, r0
  TFLITE_DCHECK_LE(output_depth, filter_shape.Dims(filter_dim_count - 2));
   dd0d4:	4628      	mov	r0, r5
   dd0d6:	f7f9 f9c1 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dd0da:	4581      	cmp	r9, r0
   dd0dc:	dce7      	bgt.n	dd0ae <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x2e>
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
   dd0de:	1e71      	subs	r1, r6, #1
   dd0e0:	4628      	mov	r0, r5
   dd0e2:	f7f9 f9bb 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dd0e6:	f8dd 8060 	ldr.w	r8, [sp, #96]	; 0x60
   dd0ea:	4606      	mov	r6, r0
   dd0ec:	4267      	negs	r7, r4
  for (int b = 0; b < batches; ++b) {
   dd0ee:	f04f 0a00 	mov.w	sl, #0
   dd0f2:	9b07      	ldr	r3, [sp, #28]
   dd0f4:	459a      	cmp	sl, r3
   dd0f6:	da39      	bge.n	dd16c <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0xec>
   dd0f8:	9b14      	ldr	r3, [sp, #80]	; 0x50
   dd0fa:	2500      	movs	r5, #0
    for (int out_c = 0; out_c < output_depth; ++out_c) {
   dd0fc:	454d      	cmp	r5, r9
   dd0fe:	da2f      	bge.n	dd160 <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0xe0>
   dd100:	469c      	mov	ip, r3
   dd102:	46a6      	mov	lr, r4
      int32 acc = 0;
   dd104:	2000      	movs	r0, #0
      for (int d = 0; d < accum_depth; ++d) {
   dd106:	eb07 020e 	add.w	r2, r7, lr
   dd10a:	4296      	cmp	r6, r2
   dd10c:	dd0f      	ble.n	dd12e <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0xae>
        int32 input_val = input_data[b * accum_depth + d];
   dd10e:	f91e 2b01 	ldrsb.w	r2, [lr], #1
   dd112:	9208      	str	r2, [sp, #32]
        int32 filter_val = filter_data[out_c * accum_depth + d];
        acc += (filter_val + filter_offset) * (input_val + input_offset);
   dd114:	9903      	ldr	r1, [sp, #12]
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
      int32 acc = 0;
      for (int d = 0; d < accum_depth; ++d) {
        int32 input_val = input_data[b * accum_depth + d];
        int32 filter_val = filter_data[out_c * accum_depth + d];
   dd116:	f91c 2b01 	ldrsb.w	r2, [ip], #1
        acc += (filter_val + filter_offset) * (input_val + input_offset);
   dd11a:	440a      	add	r2, r1
   dd11c:	9209      	str	r2, [sp, #36]	; 0x24
   dd11e:	9902      	ldr	r1, [sp, #8]
   dd120:	9a08      	ldr	r2, [sp, #32]
   dd122:	440a      	add	r2, r1
   dd124:	4611      	mov	r1, r2
   dd126:	9a09      	ldr	r2, [sp, #36]	; 0x24
   dd128:	fb01 0002 	mla	r0, r1, r2, r0
  TFLITE_DCHECK_LE(output_depth, filter_shape.Dims(filter_dim_count - 2));
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
      int32 acc = 0;
      for (int d = 0; d < accum_depth; ++d) {
   dd12c:	e7eb      	b.n	dd106 <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x86>
        int32 input_val = input_data[b * accum_depth + d];
        int32 filter_val = filter_data[out_c * accum_depth + d];
        acc += (filter_val + filter_offset) * (input_val + input_offset);
      }
      if (bias_data) {
   dd12e:	9a16      	ldr	r2, [sp, #88]	; 0x58
   dd130:	b112      	cbz	r2, dd138 <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0xb8>
        acc += bias_data[out_c];
   dd132:	f852 2025 	ldr.w	r2, [r2, r5, lsl #2]
   dd136:	4410      	add	r0, r2
      }
      acc = MultiplyByQuantizedMultiplier(acc, output_multiplier, output_shift);
   dd138:	9a06      	ldr	r2, [sp, #24]
   dd13a:	9905      	ldr	r1, [sp, #20]
   dd13c:	9308      	str	r3, [sp, #32]
   dd13e:	f7fe fd4d 	bl	dbbdc <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
      acc += output_offset;
   dd142:	9b04      	ldr	r3, [sp, #16]
   dd144:	4418      	add	r0, r3
   dd146:	9b01      	ldr	r3, [sp, #4]
   dd148:	4298      	cmp	r0, r3
   dd14a:	bfb8      	it	lt
   dd14c:	4618      	movlt	r0, r3
      acc = std::max(acc, output_activation_min);
      acc = std::min(acc, output_activation_max);
      output_data[out_c + output_depth * b] = static_cast<int8_t>(acc);
   dd14e:	4558      	cmp	r0, fp
   dd150:	9b08      	ldr	r3, [sp, #32]
   dd152:	bfa8      	it	ge
   dd154:	4658      	movge	r0, fp
   dd156:	f808 0005 	strb.w	r0, [r8, r5]
   dd15a:	4433      	add	r3, r6
  const int batches = output_shape.Dims(0);
  const int output_depth = output_shape.Dims(1);
  TFLITE_DCHECK_LE(output_depth, filter_shape.Dims(filter_dim_count - 2));
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
   dd15c:	3501      	adds	r5, #1
   dd15e:	e7cd      	b.n	dd0fc <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x7c>
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = output_shape.Dims(0);
  const int output_depth = output_shape.Dims(1);
  TFLITE_DCHECK_LE(output_depth, filter_shape.Dims(filter_dim_count - 2));
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
   dd160:	f10a 0a01 	add.w	sl, sl, #1
   dd164:	44c8      	add	r8, r9
   dd166:	4434      	add	r4, r6
   dd168:	1bbf      	subs	r7, r7, r6
   dd16a:	e7c2      	b.n	dd0f2 <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x72>
      acc = std::max(acc, output_activation_min);
      acc = std::min(acc, output_activation_max);
      output_data[out_c + output_depth * b] = static_cast<int8_t>(acc);
    }
  }
}
   dd16c:	b00b      	add	sp, #44	; 0x2c
   dd16e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

000dd174 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode>:
      GetTensorShape(bias), GetTensorData<float>(bias), GetTensorShape(output),
      GetTensorData<float>(output));
  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dd174:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dd178:	ed2d 8b02 	vpush	{d8}
   dd17c:	680c      	ldr	r4, [r1, #0]
  auto* params =
      reinterpret_cast<TfLiteFullyConnectedParams*>(node->builtin_data);
   dd17e:	694b      	ldr	r3, [r1, #20]
   dd180:	f8d0 9008 	ldr.w	r9, [r0, #8]
      GetTensorShape(bias), GetTensorData<float>(bias), GetTensorShape(output),
      GetTensorData<float>(output));
  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dd184:	b0af      	sub	sp, #188	; 0xbc
   dd186:	4680      	mov	r8, r0
  auto* params =
      reinterpret_cast<TfLiteFullyConnectedParams*>(node->builtin_data);
   dd188:	9307      	str	r3, [sp, #28]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dd18a:	68a0      	ldr	r0, [r4, #8]
   dd18c:	6863      	ldr	r3, [r4, #4]
}

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
   dd18e:	68e4      	ldr	r4, [r4, #12]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dd190:	2238      	movs	r2, #56	; 0x38
   dd192:	fb02 fa00 	mul.w	sl, r2, r0

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
   dd196:	1c60      	adds	r0, r4, #1
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dd198:	fb02 f303 	mul.w	r3, r2, r3
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dd19c:	bf18      	it	ne
   dd19e:	fb02 9404 	mlane	r4, r2, r4, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dd1a2:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dd1a4:	eb09 0703 	add.w	r7, r9, r3
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dd1a8:	6852      	ldr	r2, [r2, #4]
                             TfLiteType data_type, const TfLiteTensor* input,
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output,
                             OpData* data) {
  TfLiteStatus status = kTfLiteOk;
  if (data_type != kTfLiteFloat32) {
   dd1aa:	f819 3003 	ldrb.w	r3, [r9, r3]
   dd1ae:	f04f 0b38 	mov.w	fp, #56	; 0x38
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
  }
  return nullptr;
   dd1b2:	bf08      	it	eq
   dd1b4:	2400      	moveq	r4, #0
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dd1b6:	fb0b fb02 	mul.w	fp, fp, r2
   dd1ba:	2b01      	cmp	r3, #1
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dd1bc:	eb09 060a 	add.w	r6, r9, sl
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dd1c0:	eb09 050b 	add.w	r5, r9, fp
   dd1c4:	d021      	beq.n	dd20a <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x96>
    double real_multiplier = 0.0;
   dd1c6:	ab2e      	add	r3, sp, #184	; 0xb8
   dd1c8:	2000      	movs	r0, #0
   dd1ca:	2100      	movs	r1, #0
   dd1cc:	e963 010a 	strd	r0, r1, [r3, #-40]!	; 0x28
    TF_LITE_ENSURE_STATUS(GetQuantizedConvolutionMultipler(
   dd1d0:	4632      	mov	r2, r6
   dd1d2:	9301      	str	r3, [sp, #4]
   dd1d4:	9500      	str	r5, [sp, #0]
   dd1d6:	4623      	mov	r3, r4
   dd1d8:	4639      	mov	r1, r7
   dd1da:	4640      	mov	r0, r8
   dd1dc:	f006 fc34 	bl	e3a48 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd>
   dd1e0:	2800      	cmp	r0, #0
   dd1e2:	d132      	bne.n	dd24a <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0xd6>
        context, input, filter, bias, output, &real_multiplier));
    int exponent;
    QuantizeMultiplier(real_multiplier, &data->output_multiplier, &exponent);
   dd1e4:	a91f      	add	r1, sp, #124	; 0x7c
   dd1e6:	a80b      	add	r0, sp, #44	; 0x2c
   dd1e8:	ed9d 0b24 	vldr	d0, [sp, #144]	; 0x90
   dd1ec:	f006 fdca 	bl	e3d84 <_ZN6tflite18QuantizeMultiplierEdPlPi>
    data->output_shift = -exponent;
   dd1f0:	9b1f      	ldr	r3, [sp, #124]	; 0x7c
   dd1f2:	425b      	negs	r3, r3
   dd1f4:	930c      	str	r3, [sp, #48]	; 0x30
    TF_LITE_ENSURE_STATUS(CalculateActivationRangeQuantized(
   dd1f6:	9b07      	ldr	r3, [sp, #28]
   dd1f8:	7819      	ldrb	r1, [r3, #0]
   dd1fa:	ab0e      	add	r3, sp, #56	; 0x38
   dd1fc:	9300      	str	r3, [sp, #0]
   dd1fe:	462a      	mov	r2, r5
   dd200:	ab0d      	add	r3, sp, #52	; 0x34
   dd202:	4640      	mov	r0, r8
   dd204:	f006 fc7e 	bl	e3b04 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_>
   dd208:	b9f8      	cbnz	r0, dd24a <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0xd6>
  OpData local_data_object;
  OpData* data = &local_data_object;
  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, data_type, input,
                                        filter, bias, output, data));

  switch (filter->type) {  // Already know in/out types are same.
   dd20a:	f819 200a 	ldrb.w	r2, [r9, sl]
   dd20e:	2a03      	cmp	r2, #3
   dd210:	d176      	bne.n	dd300 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x18c>
                           TfLiteFullyConnectedParams* params, OpData* data,
                           const TfLiteTensor* input,
                           const TfLiteTensor* filter, const TfLiteTensor* bias,
                           TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
   dd212:	6933      	ldr	r3, [r6, #16]
  const int32_t output_offset = output->params.zero_point;

  tflite::FullyConnectedParams op_params;
  op_params.input_offset = input_offset;
   dd214:	693a      	ldr	r2, [r7, #16]
                           const TfLiteTensor* input,
                           const TfLiteTensor* filter, const TfLiteTensor* bias,
                           TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;
   dd216:	6929      	ldr	r1, [r5, #16]

  tflite::FullyConnectedParams op_params;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
   dd218:	9126      	str	r1, [sp, #152]	; 0x98
                           TfLiteFullyConnectedParams* params, OpData* data,
                           const TfLiteTensor* input,
                           const TfLiteTensor* filter, const TfLiteTensor* bias,
                           TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
   dd21a:	425b      	negs	r3, r3
  const int32_t output_offset = output->params.zero_point;

  tflite::FullyConnectedParams op_params;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
   dd21c:	9325      	str	r3, [sp, #148]	; 0x94
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
   dd21e:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   dd220:	9327      	str	r3, [sp, #156]	; 0x9c
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;
   dd222:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   dd224:	425b      	negs	r3, r3
   dd226:	9328      	str	r3, [sp, #160]	; 0xa0
  op_params.quantized_activation_min = data->output_activation_min;
   dd228:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   dd22a:	9329      	str	r3, [sp, #164]	; 0xa4
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;

  tflite::FullyConnectedParams op_params;
  op_params.input_offset = input_offset;
   dd22c:	4252      	negs	r2, r2
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
   dd22e:	9b0e      	ldr	r3, [sp, #56]	; 0x38
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;

  tflite::FullyConnectedParams op_params;
  op_params.input_offset = input_offset;
   dd230:	9224      	str	r2, [sp, #144]	; 0x90
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
   dd232:	932a      	str	r3, [sp, #168]	; 0xa8
  reference_ops::FullyConnected(                                       \
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input), \
      GetTensorShape(filter), GetTensorData<uint8_t>(filter),          \
      GetTensorShape(bias), GetTensorData<int32_t>(bias),              \
      GetTensorShape(output), GetTensorData<output_data_type>(output))
  switch (output->type) {
   dd234:	f819 300b 	ldrb.w	r3, [r9, fp]
   dd238:	2b03      	cmp	r3, #3
   dd23a:	d008      	beq.n	dd24e <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0xda>
   dd23c:	2b07      	cmp	r3, #7
   dd23e:	d02c      	beq.n	dd29a <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x126>
      TF_LITE_FULLY_CONNECTED(int16_t);
      break;
    default:
      context->ReportError(
          context,
          "Quantized FullyConnected expects output data type uint8 or int16");
   dd240:	f8d8 3014 	ldr.w	r3, [r8, #20]
   dd244:	499d      	ldr	r1, [pc, #628]	; (dd4bc <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x348>)
   dd246:	4640      	mov	r0, r8
   dd248:	4798      	blx	r3
                           output);

    default:
      context->ReportError(context, "Type %d not currently supported.",
                           filter->type);
      return kTfLiteError;
   dd24a:	2001      	movs	r0, #1
   dd24c:	e131      	b.n	dd4b2 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x33e>
      GetTensorShape(filter), GetTensorData<uint8_t>(filter),          \
      GetTensorShape(bias), GetTensorData<int32_t>(bias),              \
      GetTensorShape(output), GetTensorData<output_data_type>(output))
  switch (output->type) {
    case kTfLiteUInt8:
      TF_LITE_FULLY_CONNECTED(uint8_t);
   dd24e:	4639      	mov	r1, r7
   dd250:	a810      	add	r0, sp, #64	; 0x40
   dd252:	f7f9 fba8 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd256:	4631      	mov	r1, r6
   dd258:	a815      	add	r0, sp, #84	; 0x54
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dd25a:	f8d7 8004 	ldr.w	r8, [r7, #4]
   dd25e:	f7f9 fba2 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd262:	f8d6 9004 	ldr.w	r9, [r6, #4]
   dd266:	ae1a      	add	r6, sp, #104	; 0x68
   dd268:	4621      	mov	r1, r4
   dd26a:	4630      	mov	r0, r6
   dd26c:	f7f9 fb9b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd270:	b104      	cbz	r4, dd274 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x100>
   dd272:	6864      	ldr	r4, [r4, #4]
   dd274:	af1f      	add	r7, sp, #124	; 0x7c
   dd276:	4629      	mov	r1, r5
   dd278:	4638      	mov	r0, r7
   dd27a:	f7f9 fb94 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd27e:	686b      	ldr	r3, [r5, #4]
   dd280:	9304      	str	r3, [sp, #16]
   dd282:	9703      	str	r7, [sp, #12]
   dd284:	9402      	str	r4, [sp, #8]
   dd286:	9601      	str	r6, [sp, #4]
   dd288:	f8cd 9000 	str.w	r9, [sp]
   dd28c:	ab15      	add	r3, sp, #84	; 0x54
   dd28e:	4642      	mov	r2, r8
   dd290:	a910      	add	r1, sp, #64	; 0x40
   dd292:	a824      	add	r0, sp, #144	; 0x90
   dd294:	f7ff fe0c 	bl	dceb0 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph>
   dd298:	e024      	b.n	dd2e4 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x170>
      break;
    case kTfLiteInt16:
      TF_LITE_FULLY_CONNECTED(int16_t);
   dd29a:	4639      	mov	r1, r7
   dd29c:	a810      	add	r0, sp, #64	; 0x40
   dd29e:	f7f9 fb82 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd2a2:	4631      	mov	r1, r6
   dd2a4:	a815      	add	r0, sp, #84	; 0x54
   dd2a6:	f8d7 8004 	ldr.w	r8, [r7, #4]
   dd2aa:	f7f9 fb7c 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd2ae:	f8d6 9004 	ldr.w	r9, [r6, #4]
   dd2b2:	ae1a      	add	r6, sp, #104	; 0x68
   dd2b4:	4621      	mov	r1, r4
   dd2b6:	4630      	mov	r0, r6
   dd2b8:	f7f9 fb75 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd2bc:	b104      	cbz	r4, dd2c0 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x14c>
   dd2be:	6864      	ldr	r4, [r4, #4]
   dd2c0:	af1f      	add	r7, sp, #124	; 0x7c
   dd2c2:	4629      	mov	r1, r5
   dd2c4:	4638      	mov	r0, r7
   dd2c6:	f7f9 fb6e 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd2ca:	686b      	ldr	r3, [r5, #4]
   dd2cc:	9304      	str	r3, [sp, #16]
   dd2ce:	9703      	str	r7, [sp, #12]
   dd2d0:	9402      	str	r4, [sp, #8]
   dd2d2:	9601      	str	r6, [sp, #4]
   dd2d4:	f8cd 9000 	str.w	r9, [sp]
   dd2d8:	ab15      	add	r3, sp, #84	; 0x54
   dd2da:	4642      	mov	r2, r8
   dd2dc:	a910      	add	r1, sp, #64	; 0x40
   dd2de:	a824      	add	r0, sp, #144	; 0x90
   dd2e0:	f7ff fe5d 	bl	dcf9e <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps>
   dd2e4:	4638      	mov	r0, r7
   dd2e6:	f7f9 f8ae 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   dd2ea:	4630      	mov	r0, r6
   dd2ec:	f7f9 f8ab 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   dd2f0:	a815      	add	r0, sp, #84	; 0x54
   dd2f2:	f7f9 f8a8 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   dd2f6:	a810      	add	r0, sp, #64	; 0x40
   dd2f8:	f7f9 f8a5 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
          context,
          "Quantized FullyConnected expects output data type uint8 or int16");
      return kTfLiteError;
  }

  return kTfLiteOk;
   dd2fc:	2000      	movs	r0, #0
   dd2fe:	e0d8      	b.n	dd4b2 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x33e>
  OpData local_data_object;
  OpData* data = &local_data_object;
  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, data_type, input,
                                        filter, bias, output, data));

  switch (filter->type) {  // Already know in/out types are same.
   dd300:	2a09      	cmp	r2, #9
   dd302:	d136      	bne.n	dd372 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x1fe>
                               TfLiteFullyConnectedParams* params, OpData* data,
                               const TfLiteTensor* input,
                               const TfLiteTensor* filter,
                               const TfLiteTensor* bias, TfLiteTensor* output) {
  FullyConnectedParams op_params;
  op_params.input_offset = -input->params.zero_point;
   dd304:	693b      	ldr	r3, [r7, #16]
   dd306:	425b      	negs	r3, r3
   dd308:	9324      	str	r3, [sp, #144]	; 0x90
  op_params.weights_offset = -filter->params.zero_point;
   dd30a:	6933      	ldr	r3, [r6, #16]
   dd30c:	425b      	negs	r3, r3
   dd30e:	9325      	str	r3, [sp, #148]	; 0x94
  op_params.output_offset = output->params.zero_point;
   dd310:	692b      	ldr	r3, [r5, #16]
   dd312:	9326      	str	r3, [sp, #152]	; 0x98
  op_params.output_multiplier = data->output_multiplier;
   dd314:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   dd316:	9327      	str	r3, [sp, #156]	; 0x9c
  // TODO(b/138810107): Figure out whether output shift should be inverted
  op_params.output_shift = -data->output_shift;
   dd318:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   dd31a:	425b      	negs	r3, r3
   dd31c:	9328      	str	r3, [sp, #160]	; 0xa0
  op_params.quantized_activation_min = data->output_activation_min;
   dd31e:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   dd320:	9329      	str	r3, [sp, #164]	; 0xa4
  op_params.quantized_activation_max = data->output_activation_max;

  reference_integer_ops::FullyConnected(
      op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
   dd322:	4639      	mov	r1, r7
  op_params.output_offset = output->params.zero_point;
  op_params.output_multiplier = data->output_multiplier;
  // TODO(b/138810107): Figure out whether output shift should be inverted
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
   dd324:	9b0e      	ldr	r3, [sp, #56]	; 0x38
   dd326:	932a      	str	r3, [sp, #168]	; 0xa8

  reference_integer_ops::FullyConnected(
      op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
   dd328:	a810      	add	r0, sp, #64	; 0x40
   dd32a:	f7f9 fb3c 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(filter), GetTensorData<int8_t>(filter),
   dd32e:	4631      	mov	r1, r6
   dd330:	a815      	add	r0, sp, #84	; 0x54
   dd332:	f8d7 8004 	ldr.w	r8, [r7, #4]
   dd336:	f7f9 fb36 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd33a:	f8d6 9004 	ldr.w	r9, [r6, #4]
      GetTensorShape(bias), GetTensorData<int32_t>(bias),
   dd33e:	ae1a      	add	r6, sp, #104	; 0x68
   dd340:	4621      	mov	r1, r4
   dd342:	4630      	mov	r0, r6
   dd344:	f7f9 fb2f 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd348:	b104      	cbz	r4, dd34c <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x1d8>
   dd34a:	6864      	ldr	r4, [r4, #4]
      GetTensorShape(output), GetTensorData<int8_t>(output));
   dd34c:	af1f      	add	r7, sp, #124	; 0x7c
   dd34e:	4629      	mov	r1, r5
   dd350:	4638      	mov	r0, r7
   dd352:	f7f9 fb28 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd356:	686b      	ldr	r3, [r5, #4]
   dd358:	9304      	str	r3, [sp, #16]
   dd35a:	9703      	str	r7, [sp, #12]
   dd35c:	9402      	str	r4, [sp, #8]
   dd35e:	9601      	str	r6, [sp, #4]
   dd360:	f8cd 9000 	str.w	r9, [sp]
   dd364:	ab15      	add	r3, sp, #84	; 0x54
   dd366:	4642      	mov	r2, r8
   dd368:	a910      	add	r1, sp, #64	; 0x40
   dd36a:	a824      	add	r0, sp, #144	; 0x90
   dd36c:	f7ff fe88 	bl	dd080 <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa>
   dd370:	e7b8      	b.n	dd2e4 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x170>
  OpData local_data_object;
  OpData* data = &local_data_object;
  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, data_type, input,
                                        filter, bias, output, data));

  switch (filter->type) {  // Already know in/out types are same.
   dd372:	2a01      	cmp	r2, #1
   dd374:	f040 8097 	bne.w	dd4a6 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x332>
   dd378:	9b07      	ldr	r3, [sp, #28]
   dd37a:	781b      	ldrb	r3, [r3, #0]
// Calculates the useful range of an activation layer given its activation
// tensor.a
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
   dd37c:	2b01      	cmp	r3, #1
   dd37e:	d011      	beq.n	dd3a4 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x230>
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
   dd380:	2b03      	cmp	r3, #3
   dd382:	d012      	beq.n	dd3aa <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x236>
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
    *activation_max = std::numeric_limits<T>::max();
   dd384:	ed9f 8a4e 	vldr	s16, [pc, #312]	; dd4c0 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x34c>
   dd388:	eddf 8a4e 	vldr	s17, [pc, #312]	; dd4c4 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x350>
   dd38c:	2b02      	cmp	r3, #2
   dd38e:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   dd392:	bf08      	it	eq
   dd394:	eeb0 8a67 	vmoveq.f32	s16, s15
   dd398:	eeff 7a00 	vmov.f32	s15, #240	; 0xbf800000 -1.0
   dd39c:	bf08      	it	eq
   dd39e:	eef0 8a67 	vmoveq.f32	s17, s15
   dd3a2:	e006      	b.n	dd3b2 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x23e>
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
   dd3a4:	ed9f 8a46 	vldr	s16, [pc, #280]	; dd4c0 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x34c>
   dd3a8:	e001      	b.n	dd3ae <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x23a>
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
    *activation_max = 6;
   dd3aa:	eeb1 8a08 	vmov.f32	s16, #24	; 0x40c00000  6.0
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
   dd3ae:	eddf 8a46 	vldr	s17, [pc, #280]	; dd4c8 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x354>
                           &output_activation_max);
  tflite::FullyConnectedParams op_params;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  tflite::reference_ops::FullyConnected(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
   dd3b2:	4639      	mov	r1, r7
   dd3b4:	a815      	add	r0, sp, #84	; 0x54
   dd3b6:	f7f9 faf6 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(filter), GetTensorData<float>(filter),
   dd3ba:	4631      	mov	r1, r6
   dd3bc:	a81a      	add	r0, sp, #104	; 0x68
   dd3be:	687f      	ldr	r7, [r7, #4]
   dd3c0:	f7f9 faf1 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd3c4:	6873      	ldr	r3, [r6, #4]
   dd3c6:	9308      	str	r3, [sp, #32]
      GetTensorShape(bias), GetTensorData<float>(bias), GetTensorShape(output),
   dd3c8:	4621      	mov	r1, r4
   dd3ca:	a81f      	add	r0, sp, #124	; 0x7c
   dd3cc:	f7f9 faeb 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd3d0:	b104      	cbz	r4, dd3d4 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x260>
   dd3d2:	6864      	ldr	r4, [r4, #4]
   dd3d4:	4629      	mov	r1, r5
   dd3d6:	a824      	add	r0, sp, #144	; 0x90
   dd3d8:	f7f9 fae5 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dd3dc:	b105      	cbz	r5, dd3e0 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x26c>
   dd3de:	686d      	ldr	r5, [r5, #4]
  // but the current --variable_batch hack consists in overwriting the 3rd
  // dimension with the runtime batch size, as we don't keep track for each
  // array of which dimension is the batch dimension in it.
  const int output_dims_count = output_shape.DimensionsCount();
  const int weights_dims_count = weights_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);
   dd3e0:	9e24      	ldr	r6, [sp, #144]	; 0x90
   dd3e2:	f8dd 9068 	ldr.w	r9, [sp, #104]	; 0x68
   dd3e6:	3e01      	subs	r6, #1
   dd3e8:	4631      	mov	r1, r6
   dd3ea:	a824      	add	r0, sp, #144	; 0x90
   dd3ec:	f7ff fd47 	bl	dce7e <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi>
  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,
                                       output_shape, output_dims_count - 1);
   dd3f0:	4633      	mov	r3, r6
   dd3f2:	aa24      	add	r2, sp, #144	; 0x90
  // but the current --variable_batch hack consists in overwriting the 3rd
  // dimension with the runtime batch size, as we don't keep track for each
  // array of which dimension is the batch dimension in it.
  const int output_dims_count = output_shape.DimensionsCount();
  const int weights_dims_count = weights_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);
   dd3f4:	9009      	str	r0, [sp, #36]	; 0x24
  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,
                                       output_shape, output_dims_count - 1);
   dd3f6:	f1a9 0102 	sub.w	r1, r9, #2
   dd3fa:	a81a      	add	r0, sp, #104	; 0x68
   dd3fc:	f7fe fbdf 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);
   dd400:	f109 31ff 	add.w	r1, r9, #4294967295	; 0xffffffff
  // array of which dimension is the batch dimension in it.
  const int output_dims_count = output_shape.DimensionsCount();
  const int weights_dims_count = weights_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);
  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,
                                       output_shape, output_dims_count - 1);
   dd404:	4680      	mov	r8, r0
  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);
   dd406:	a81a      	add	r0, sp, #104	; 0x68
   dd408:	f7f9 f828 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dd40c:	ea4f 0388 	mov.w	r3, r8, lsl #2
   dd410:	9307      	str	r3, [sp, #28]
   dd412:	ea4f 0c80 	mov.w	ip, r0, lsl #2
   dd416:	463b      	mov	r3, r7
  for (int b = 0; b < batches; ++b) {
   dd418:	2200      	movs	r2, #0
   dd41a:	9909      	ldr	r1, [sp, #36]	; 0x24
   dd41c:	4291      	cmp	r1, r2
   dd41e:	dd37      	ble.n	dd490 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x31c>
   dd420:	9908      	ldr	r1, [sp, #32]
   dd422:	4626      	mov	r6, r4
   dd424:	46a9      	mov	r9, r5
   dd426:	2700      	movs	r7, #0
    for (int out_c = 0; out_c < output_depth; ++out_c) {
   dd428:	45b8      	cmp	r8, r7
   dd42a:	dd2c      	ble.n	dd486 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x312>
   dd42c:	eddf 7a26 	vldr	s15, [pc, #152]	; dd4c8 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x354>
   dd430:	468a      	mov	sl, r1
   dd432:	469b      	mov	fp, r3
   dd434:	f04f 0e00 	mov.w	lr, #0
      float total = 0.f;
      for (int d = 0; d < accum_depth; ++d) {
   dd438:	4570      	cmp	r0, lr
   dd43a:	dd08      	ble.n	dd44e <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x2da>
        total += input_data[b * accum_depth + d] *
   dd43c:	ecfb 6a01 	vldmia	fp!, {s13}
   dd440:	ecba 7a01 	vldmia	sl!, {s14}
                                       output_shape, output_dims_count - 1);
  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
      float total = 0.f;
      for (int d = 0; d < accum_depth; ++d) {
   dd444:	f10e 0e01 	add.w	lr, lr, #1
        total += input_data[b * accum_depth + d] *
                 weights_data[out_c * accum_depth + d];
   dd448:	eee6 7a87 	vfma.f32	s15, s13, s14
   dd44c:	e7f4      	b.n	dd438 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x2c4>
      }
      float bias_value = 0.0f;
      if (bias_data) {
   dd44e:	b114      	cbz	r4, dd456 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x2e2>
        bias_value = bias_data[out_c];
   dd450:	ed96 7a00 	vldr	s14, [r6]
   dd454:	e001      	b.n	dd45a <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x2e6>
      float total = 0.f;
      for (int d = 0; d < accum_depth; ++d) {
        total += input_data[b * accum_depth + d] *
                 weights_data[out_c * accum_depth + d];
      }
      float bias_value = 0.0f;
   dd456:	ed9f 7a1c 	vldr	s14, [pc, #112]	; dd4c8 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x354>
      if (bias_data) {
        bias_value = bias_data[out_c];
      }
      output_data[out_c + output_depth * b] = ActivationFunctionWithMinMax(
   dd45a:	ee77 7a87 	vadd.f32	s15, s15, s14
  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);
  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,
                                       output_shape, output_dims_count - 1);
  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
   dd45e:	3701      	adds	r7, #1
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
	return __b;
      return __a;
   dd460:	eef4 7ae8 	vcmpe.f32	s15, s17
   dd464:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   dd468:	bf48      	it	mi
   dd46a:	eef0 7a68 	vmovmi.f32	s15, s17
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   dd46e:	eef4 7a48 	vcmp.f32	s15, s16
   dd472:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   dd476:	bfc8      	it	gt
   dd478:	eef0 7a48 	vmovgt.f32	s15, s16
      float bias_value = 0.0f;
      if (bias_data) {
        bias_value = bias_data[out_c];
      }
      output_data[out_c + output_depth * b] = ActivationFunctionWithMinMax(
          total + bias_value, output_activation_min, output_activation_max);
   dd47c:	ece9 7a01 	vstmia	r9!, {s15}
   dd480:	3604      	adds	r6, #4
   dd482:	4461      	add	r1, ip
   dd484:	e7d0      	b.n	dd428 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x2b4>
   dd486:	9907      	ldr	r1, [sp, #28]
  const int weights_dims_count = weights_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);
  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,
                                       output_shape, output_dims_count - 1);
  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);
  for (int b = 0; b < batches; ++b) {
   dd488:	3201      	adds	r2, #1
   dd48a:	440d      	add	r5, r1
   dd48c:	4463      	add	r3, ip
   dd48e:	e7c4      	b.n	dd41a <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x2a6>
   dd490:	a824      	add	r0, sp, #144	; 0x90
   dd492:	f7f8 ffd8 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   dd496:	a81f      	add	r0, sp, #124	; 0x7c
   dd498:	f7f8 ffd5 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  tflite::FullyConnectedParams op_params;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  tflite::reference_ops::FullyConnected(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
      GetTensorShape(filter), GetTensorData<float>(filter),
   dd49c:	a81a      	add	r0, sp, #104	; 0x68
   dd49e:	f7f8 ffd2 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                           &output_activation_max);
  tflite::FullyConnectedParams op_params;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  tflite::reference_ops::FullyConnected(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
   dd4a2:	a815      	add	r0, sp, #84	; 0x54
   dd4a4:	e728      	b.n	dd2f8 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x184>
      return EvalQuantized(context, node, params, data, input, filter, bias,
                           output);

    default:
      context->ReportError(context, "Type %d not currently supported.",
                           filter->type);
   dd4a6:	f8d8 3014 	ldr.w	r3, [r8, #20]
   dd4aa:	4908      	ldr	r1, [pc, #32]	; (dd4cc <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x358>)
   dd4ac:	4640      	mov	r0, r8
   dd4ae:	4798      	blx	r3
   dd4b0:	e6cb      	b.n	dd24a <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0xd6>
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   dd4b2:	b02f      	add	sp, #188	; 0xbc
   dd4b4:	ecbd 8b02 	vpop	{d8}
   dd4b8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dd4bc:	000e9e81 	.word	0x000e9e81
   dd4c0:	7f7fffff 	.word	0x7f7fffff
   dd4c4:	ff7fffff 	.word	0xff7fffff
   dd4c8:	00000000 	.word	0x00000000
   dd4cc:	000e9ec2 	.word	0x000e9ec2

000dd4d0 <_ZN6tflite3ops5micro24Register_FULLY_CONNECTEDEv>:
TfLiteRegistration* Register_FULLY_CONNECTED() {
  static TfLiteRegistration r = {fully_connected::Init, fully_connected::Free,
                                 fully_connected::Prepare,
                                 fully_connected::Eval};
  return &r;
}
   dd4d0:	4800      	ldr	r0, [pc, #0]	; (dd4d4 <_ZN6tflite3ops5micro24Register_FULLY_CONNECTEDEv+0x4>)
   dd4d2:	4770      	bx	lr
   dd4d4:	2003bfe8 	.word	0x2003bfe8

000dd4d8 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_19LogicalOrEbb>:
  }

  return kTfLiteOk;
}

bool LogicalOr(bool x, bool y) { return x || y; }
   dd4d8:	2800      	cmp	r0, #0
   dd4da:	bf0c      	ite	eq
   dd4dc:	4608      	moveq	r0, r1
   dd4de:	2001      	movne	r0, #1
   dd4e0:	4770      	bx	lr

000dd4e2 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_110LogicalAndEbb>:

TfLiteStatus LogicalOrEval(TfLiteContext* context, TfLiteNode* node) {
  return LogicalImpl(context, node, LogicalOr);
}

bool LogicalAnd(bool x, bool y) { return x && y; }
   dd4e2:	2800      	cmp	r0, #0
   dd4e4:	bf14      	ite	ne
   dd4e6:	4608      	movne	r0, r1
   dd4e8:	2000      	moveq	r0, #0
   dd4ea:	4770      	bx	lr

000dd4ec <_ZN6tflite3ops5micro19Register_LOGICAL_OREv>:
  // Init, Free, Prepare, Eval are satisfying the Interface required by
  // TfLiteRegistration.
  static TfLiteRegistration r = {/* init */ nullptr, /* free */ nullptr,
                                 /* prepare */ nullptr, logical::LogicalOrEval};
  return &r;
}
   dd4ec:	4800      	ldr	r0, [pc, #0]	; (dd4f0 <_ZN6tflite3ops5micro19Register_LOGICAL_OREv+0x4>)
   dd4ee:	4770      	bx	lr
   dd4f0:	2003c008 	.word	0x2003c008

000dd4f4 <_ZN6tflite3ops5micro20Register_LOGICAL_ANDEv>:
  // TfLiteRegistration.
  static TfLiteRegistration r = {/* init */ nullptr, /* free */ nullptr,
                                 /* prepare */ nullptr,
                                 logical::LogicalAndEval};
  return &r;
}
   dd4f4:	4800      	ldr	r0, [pc, #0]	; (dd4f8 <_ZN6tflite3ops5micro20Register_LOGICAL_ANDEv+0x4>)
   dd4f6:	4770      	bx	lr
   dd4f8:	2003c028 	.word	0x2003c028

000dd4fc <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E>:
}

// R: Result type. T1: Input 1 type. T2: Input 2 type.
// TODO(renjieliu): Refactor other binary functions to use this one.
template <typename R, typename T1, typename T2>
inline void BinaryFunction(const RuntimeShape& input1_shape,
   dd4fc:	e92d 4ff8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dd500:	4699      	mov	r9, r3
   dd502:	6807      	ldr	r7, [r0, #0]
}

inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0,
                            const RuntimeShape& check_shape_1) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   dd504:	6813      	ldr	r3, [r2, #0]
   dd506:	9d0a      	ldr	r5, [sp, #40]	; 0x28
   dd508:	429f      	cmp	r7, r3
   dd50a:	4604      	mov	r4, r0
   dd50c:	4688      	mov	r8, r1
   dd50e:	4616      	mov	r6, r2
   dd510:	d102      	bne.n	dd518 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1c>
   dd512:	f04f 0a00 	mov.w	sl, #0
   dd516:	e00e      	b.n	dd536 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x3a>
   dd518:	f006 ff08 	bl	e432c <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   dd51c:	4651      	mov	r1, sl
   dd51e:	4620      	mov	r0, r4
   dd520:	f7f8 ff9c 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dd524:	4651      	mov	r1, sl
   dd526:	4683      	mov	fp, r0
   dd528:	4630      	mov	r0, r6
   dd52a:	f7f8 ff97 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dd52e:	4583      	cmp	fp, r0
   dd530:	d1f2      	bne.n	dd518 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1c>
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0,
                            const RuntimeShape& check_shape_1) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   dd532:	f10a 0a01 	add.w	sl, sl, #1
   dd536:	4557      	cmp	r7, sl
   dd538:	dcf0      	bgt.n	dd51c <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x20>

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   dd53a:	682b      	ldr	r3, [r5, #0]
   dd53c:	429f      	cmp	r7, r3
   dd53e:	d1eb      	bne.n	dd518 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1c>
   dd540:	f04f 0a00 	mov.w	sl, #0
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   dd544:	4557      	cmp	r7, sl
   dd546:	dd0d      	ble.n	dd564 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x68>
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   dd548:	4651      	mov	r1, sl
   dd54a:	4620      	mov	r0, r4
   dd54c:	f7f8 ff86 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dd550:	4651      	mov	r1, sl
   dd552:	4606      	mov	r6, r0
   dd554:	4628      	mov	r0, r5
   dd556:	f7f8 ff81 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dd55a:	4286      	cmp	r6, r0
   dd55c:	d1dc      	bne.n	dd518 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1c>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   dd55e:	f10a 0a01 	add.w	sl, sl, #1
   dd562:	e7ef      	b.n	dd544 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x48>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   dd564:	2f04      	cmp	r7, #4
   dd566:	bfcc      	ite	gt
   dd568:	6864      	ldrgt	r4, [r4, #4]
   dd56a:	3404      	addle	r4, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dd56c:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
   dd56e:	f04f 0a01 	mov.w	sl, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dd572:	429f      	cmp	r7, r3
   dd574:	dc01      	bgt.n	dd57a <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x7e>
   dd576:	2400      	movs	r4, #0
   dd578:	e005      	b.n	dd586 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x8a>
      buffer_size *= dims_data[i];
   dd57a:	f854 2023 	ldr.w	r2, [r4, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dd57e:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
   dd580:	fb02 fa0a 	mul.w	sl, r2, sl
   dd584:	e7f5      	b.n	dd572 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x76>
                           const T2* input2_data,
                           const RuntimeShape& output_shape, R* output_data,
                           R (*func)(T1, T2)) {
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int i = 0; i < flat_size; ++i) {
   dd586:	4554      	cmp	r4, sl
   dd588:	da09      	bge.n	dd59e <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0xa2>
    output_data[i] = func(input1_data[i], input2_data[i]);
   dd58a:	f819 1004 	ldrb.w	r1, [r9, r4]
   dd58e:	f818 0004 	ldrb.w	r0, [r8, r4]
   dd592:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   dd594:	4798      	blx	r3
   dd596:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   dd598:	5518      	strb	r0, [r3, r4]
                           const T2* input2_data,
                           const RuntimeShape& output_shape, R* output_data,
                           R (*func)(T1, T2)) {
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int i = 0; i < flat_size; ++i) {
   dd59a:	3401      	adds	r4, #1
   dd59c:	e7f3      	b.n	dd586 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x8a>
   dd59e:	e8bd 8ff8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dd5a2 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E>:
//
// Also appears to duplicte MinimumMaximum.
//
// R: Result type. T1: Input 1 type. T2: Input 2 type.
template <typename R, typename T1, typename T2>
inline void BroadcastBinaryFunction4DSlow(
   dd5a2:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dd5a6:	469a      	mov	sl, r3
    const RuntimeShape& unextended_input1_shape, const T1* input1_data,
    const RuntimeShape& unextended_input2_shape, const T2* input2_data,
    const RuntimeShape& unextended_output_shape, R* output_data,
    R (*func)(T1, T2)) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   dd5a8:	6803      	ldr	r3, [r0, #0]
//
// Also appears to duplicte MinimumMaximum.
//
// R: Result type. T1: Input 1 type. T2: Input 2 type.
template <typename R, typename T1, typename T2>
inline void BroadcastBinaryFunction4DSlow(
   dd5aa:	b0a5      	sub	sp, #148	; 0x94
    const RuntimeShape& unextended_input1_shape, const T1* input1_data,
    const RuntimeShape& unextended_input2_shape, const T2* input2_data,
    const RuntimeShape& unextended_output_shape, R* output_data,
    R (*func)(T1, T2)) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   dd5ac:	2b04      	cmp	r3, #4
//
// Also appears to duplicte MinimumMaximum.
//
// R: Result type. T1: Input 1 type. T2: Input 2 type.
template <typename R, typename T1, typename T2>
inline void BroadcastBinaryFunction4DSlow(
   dd5ae:	4614      	mov	r4, r2
   dd5b0:	4605      	mov	r5, r0
   dd5b2:	9103      	str	r1, [sp, #12]
   dd5b4:	9a2e      	ldr	r2, [sp, #184]	; 0xb8
    const RuntimeShape& unextended_input1_shape, const T1* input1_data,
    const RuntimeShape& unextended_input2_shape, const T2* input2_data,
    const RuntimeShape& unextended_output_shape, R* output_data,
    R (*func)(T1, T2)) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   dd5b6:	dd01      	ble.n	dd5bc <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1a>
   dd5b8:	f006 feb8 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   dd5bc:	6823      	ldr	r3, [r4, #0]
   dd5be:	2b04      	cmp	r3, #4
   dd5c0:	dcfa      	bgt.n	dd5b8 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   dd5c2:	6813      	ldr	r3, [r2, #0]
   dd5c4:	2b04      	cmp	r3, #4
   dd5c6:	dcf7      	bgt.n	dd5b8 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
   dd5c8:	2301      	movs	r3, #1
   dd5ca:	2104      	movs	r1, #4
   dd5cc:	a805      	add	r0, sp, #20
   dd5ce:	f7f8 ff7e 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   dd5d2:	462a      	mov	r2, r5
   dd5d4:	2301      	movs	r3, #1
   dd5d6:	2104      	movs	r1, #4
   dd5d8:	a80a      	add	r0, sp, #40	; 0x28
   dd5da:	f7f8 ff78 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   dd5de:	4622      	mov	r2, r4
   dd5e0:	2301      	movs	r3, #1
   dd5e2:	2104      	movs	r1, #4
   dd5e4:	a80f      	add	r0, sp, #60	; 0x3c
   dd5e6:	f7f8 ff72 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   dd5ea:	f10d 0970 	add.w	r9, sp, #112	; 0x70
  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
   dd5ee:	f04f 0b01 	mov.w	fp, #1
   dd5f2:	f10d 0890 	add.w	r8, sp, #144	; 0x90

  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
   dd5f6:	465e      	mov	r6, fp
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
   dd5f8:	2403      	movs	r4, #3
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
   dd5fa:	ad14      	add	r5, sp, #80	; 0x50
   dd5fc:	464f      	mov	r7, r9
   dd5fe:	4621      	mov	r1, r4
   dd600:	a80a      	add	r0, sp, #40	; 0x28
   dd602:	f7f8 ff2b 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
   dd606:	4621      	mov	r1, r4

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
   dd608:	f845 0024 	str.w	r0, [r5, r4, lsl #2]
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
   dd60c:	a80a      	add	r0, sp, #40	; 0x28
  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
   dd60e:	f849 6d04 	str.w	r6, [r9, #-4]!
    desc0_stride *= extended_input0_shape.Dims(i);
   dd612:	f7f8 ff23 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
   dd616:	4621      	mov	r1, r4
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
   dd618:	4346      	muls	r6, r0
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
   dd61a:	a80f      	add	r0, sp, #60	; 0x3c
   dd61c:	f7f8 ff1e 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dd620:	ab1c      	add	r3, sp, #112	; 0x70
    desc1_out->strides[i] = desc1_stride;
    desc1_stride *= extended_input1_shape.Dims(i);
   dd622:	4621      	mov	r1, r4
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
   dd624:	f843 0024 	str.w	r0, [r3, r4, lsl #2]
    desc1_out->strides[i] = desc1_stride;
   dd628:	f848 bd04 	str.w	fp, [r8, #-4]!
    desc1_stride *= extended_input1_shape.Dims(i);
   dd62c:	a80f      	add	r0, sp, #60	; 0x3c
   dd62e:	f7f8 ff15 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
   dd632:	f114 34ff 	adds.w	r4, r4, #4294967295	; 0xffffffff
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
    desc1_out->strides[i] = desc1_stride;
    desc1_stride *= extended_input1_shape.Dims(i);
   dd636:	fb00 fb0b 	mul.w	fp, r0, fp
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
   dd63a:	d2e0      	bcs.n	dd5fe <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x5c>
   dd63c:	2400      	movs	r4, #0
      if (extent0 == 1) {
        desc0_out->strides[i] = 0;
        desc0_out->extents[i] = extent1;
      } else {
        TFLITE_DCHECK_EQ(extent1, 1);
        desc1_out->strides[i] = 0;
   dd63e:	46a0      	mov	r8, r4

  // Walk over each dimension. If the extents are equal do nothing.
  // Otherwise, set the desc with extent 1 to have extent equal to the other and
  // stride 0.
  for (int i = 0; i < N; ++i) {
    const int extent0 = extended_input0_shape.Dims(i);
   dd640:	4621      	mov	r1, r4
   dd642:	a80a      	add	r0, sp, #40	; 0x28
   dd644:	f7f8 ff0a 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
    const int extent1 = extended_input1_shape.Dims(i);
   dd648:	4621      	mov	r1, r4

  // Walk over each dimension. If the extents are equal do nothing.
  // Otherwise, set the desc with extent 1 to have extent equal to the other and
  // stride 0.
  for (int i = 0; i < N; ++i) {
    const int extent0 = extended_input0_shape.Dims(i);
   dd64a:	4606      	mov	r6, r0
    const int extent1 = extended_input1_shape.Dims(i);
   dd64c:	a80f      	add	r0, sp, #60	; 0x3c
   dd64e:	f7f8 ff05 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
    if (extent0 != extent1) {
   dd652:	4286      	cmp	r6, r0
   dd654:	d010      	beq.n	dd678 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0xd6>
      if (extent0 == 1) {
   dd656:	2e01      	cmp	r6, #1
   dd658:	ea4f 0384 	mov.w	r3, r4, lsl #2
   dd65c:	d105      	bne.n	dd66a <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0xc8>
        desc0_out->strides[i] = 0;
   dd65e:	442b      	add	r3, r5
   dd660:	f8c3 8010 	str.w	r8, [r3, #16]
        desc0_out->extents[i] = extent1;
   dd664:	f845 0024 	str.w	r0, [r5, r4, lsl #2]
   dd668:	e006      	b.n	dd678 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0xd6>
      } else {
        TFLITE_DCHECK_EQ(extent1, 1);
   dd66a:	2801      	cmp	r0, #1
   dd66c:	d1a4      	bne.n	dd5b8 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
        desc1_out->strides[i] = 0;
   dd66e:	443b      	add	r3, r7
   dd670:	f8c3 8010 	str.w	r8, [r3, #16]
        desc1_out->extents[i] = extent0;
   dd674:	f847 6024 	str.w	r6, [r7, r4, lsl #2]
  }

  // Walk over each dimension. If the extents are equal do nothing.
  // Otherwise, set the desc with extent 1 to have extent equal to the other and
  // stride 0.
  for (int i = 0; i < N; ++i) {
   dd678:	3401      	adds	r4, #1
   dd67a:	2c04      	cmp	r4, #4
   dd67c:	d1e0      	bne.n	dd640 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x9e>
    NdArrayDesc<N>* desc0_out, NdArrayDesc<N>* desc1_out) {
  TFLITE_DCHECK(desc0_out != nullptr);
  TFLITE_DCHECK(desc1_out != nullptr);

  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);
   dd67e:	a80f      	add	r0, sp, #60	; 0x3c
   dd680:	f7f8 fee1 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
    const RuntimeShape& input0_shape, const RuntimeShape& input1_shape,
    NdArrayDesc<N>* desc0_out, NdArrayDesc<N>* desc1_out) {
  TFLITE_DCHECK(desc0_out != nullptr);
  TFLITE_DCHECK(desc1_out != nullptr);

  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
   dd684:	a80a      	add	r0, sp, #40	; 0x28
   dd686:	f7f8 fede 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   dd68a:	2400      	movs	r4, #0
   dd68c:	2100      	movs	r1, #0
   dd68e:	a805      	add	r0, sp, #20
   dd690:	f7f8 fee4 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dd694:	4284      	cmp	r4, r0
   dd696:	da5d      	bge.n	dd754 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1b2>
   dd698:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   dd69a:	f10d 0914 	add.w	r9, sp, #20
   dd69e:	2101      	movs	r1, #1
   dd6a0:	4648      	mov	r0, r9
   dd6a2:	f7f8 fedb 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dd6a6:	4285      	cmp	r5, r0
   dd6a8:	da52      	bge.n	dd750 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1ae>
   dd6aa:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   dd6ac:	2102      	movs	r1, #2
   dd6ae:	4648      	mov	r0, r9
   dd6b0:	f7f8 fed4 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dd6b4:	4286      	cmp	r6, r0
   dd6b6:	da49      	bge.n	dd74c <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1aa>
   dd6b8:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dd6ba:	2103      	movs	r1, #3
   dd6bc:	4648      	mov	r0, r9
   dd6be:	f7f8 fecd 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dd6c2:	4287      	cmp	r7, r0
   dd6c4:	da40      	bge.n	dd748 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1a6>
  }
  return offset;
}

inline int Offset(const RuntimeShape& shape, int i0, int i1, int i2, int i3) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), 4);
   dd6c6:	9b05      	ldr	r3, [sp, #20]
   dd6c8:	2b04      	cmp	r3, #4
   dd6ca:	f47f af75 	bne.w	dd5b8 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  const int* dims_data = reinterpret_cast<const int*>(shape.DimsDataUpTo4D());
  TFLITE_DCHECK(i0 >= 0 && i0 < dims_data[0]);
   dd6ce:	2c00      	cmp	r4, #0
   dd6d0:	f6ff af72 	blt.w	dd5b8 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
   dd6d4:	9b06      	ldr	r3, [sp, #24]
   dd6d6:	429c      	cmp	r4, r3
   dd6d8:	f6bf af6e 	bge.w	dd5b8 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  TFLITE_DCHECK(i1 >= 0 && i1 < dims_data[1]);
   dd6dc:	2d00      	cmp	r5, #0
   dd6de:	f6ff af6b 	blt.w	dd5b8 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
   dd6e2:	9b07      	ldr	r3, [sp, #28]
   dd6e4:	429d      	cmp	r5, r3
   dd6e6:	f6bf af67 	bge.w	dd5b8 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  TFLITE_DCHECK(i2 >= 0 && i2 < dims_data[2]);
   dd6ea:	2e00      	cmp	r6, #0
   dd6ec:	f6ff af64 	blt.w	dd5b8 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
   dd6f0:	9908      	ldr	r1, [sp, #32]
   dd6f2:	428e      	cmp	r6, r1
   dd6f4:	f6bf af60 	bge.w	dd5b8 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  TFLITE_DCHECK(i3 >= 0 && i3 < dims_data[3]);
   dd6f8:	2f00      	cmp	r7, #0
   dd6fa:	f6ff af5d 	blt.w	dd5b8 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
   dd6fe:	9a09      	ldr	r2, [sp, #36]	; 0x24
   dd700:	4297      	cmp	r7, r2
   dd702:	f6bf af59 	bge.w	dd5b8 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  return ((i0 * dims_data[1] + i1) * dims_data[2] + i2) * dims_data[3] + i3;
   dd706:	fb03 5304 	mla	r3, r3, r4, r5
   dd70a:	fb01 6303 	mla	r3, r1, r3, r6
   dd70e:	fb02 7803 	mla	r8, r2, r3, r7
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   dd712:	9700      	str	r7, [sp, #0]
   dd714:	4633      	mov	r3, r6
   dd716:	462a      	mov	r2, r5
   dd718:	4621      	mov	r1, r4
   dd71a:	a814      	add	r0, sp, #80	; 0x50
   dd71c:	f7f8 ffb4 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   dd720:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   dd722:	4683      	mov	fp, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   dd724:	4633      	mov	r3, r6
   dd726:	462a      	mov	r2, r5
   dd728:	4621      	mov	r1, r4
   dd72a:	a81c      	add	r0, sp, #112	; 0x70
   dd72c:	f7f8 ffac 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = func(in1_val, in2_val);
   dd730:	9b03      	ldr	r3, [sp, #12]
   dd732:	f81a 1000 	ldrb.w	r1, [sl, r0]
   dd736:	f813 000b 	ldrb.w	r0, [r3, fp]
   dd73a:	9b30      	ldr	r3, [sp, #192]	; 0xc0
   dd73c:	4798      	blx	r3
   dd73e:	9b2f      	ldr	r3, [sp, #188]	; 0xbc
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dd740:	3701      	adds	r7, #1
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = func(in1_val, in2_val);
   dd742:	f803 0008 	strb.w	r0, [r3, r8]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dd746:	e7b8      	b.n	dd6ba <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x118>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   dd748:	3601      	adds	r6, #1
   dd74a:	e7af      	b.n	dd6ac <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x10a>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   dd74c:	3501      	adds	r5, #1
   dd74e:	e7a4      	b.n	dd69a <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0xf8>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   dd750:	3401      	adds	r4, #1
   dd752:	e79b      	b.n	dd68c <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0xea>
    R (*func)(T1, T2)) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   dd754:	a805      	add	r0, sp, #20
   dd756:	f7f8 fe76 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = func(in1_val, in2_val);
        }
      }
    }
  }
}
   dd75a:	b025      	add	sp, #148	; 0x94
   dd75c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dd760 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE>:
constexpr int kInputTensor1 = 0;
constexpr int kInputTensor2 = 1;
constexpr int kOutputTensor = 0;

TfLiteStatus LogicalImpl(TfLiteContext* context, TfLiteNode* node,
                         bool (*func)(bool, bool)) {
   dd760:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
   dd764:	680b      	ldr	r3, [r1, #0]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dd766:	685c      	ldr	r4, [r3, #4]
   dd768:	689d      	ldr	r5, [r3, #8]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dd76a:	684b      	ldr	r3, [r1, #4]
   dd76c:	4617      	mov	r7, r2
   dd76e:	6882      	ldr	r2, [r0, #8]
   dd770:	6859      	ldr	r1, [r3, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dd772:	2038      	movs	r0, #56	; 0x38
   dd774:	fb00 2404 	mla	r4, r0, r4, r2
   dd778:	fb00 2505 	mla	r5, r0, r5, r2
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dd77c:	fb00 2801 	mla	r8, r0, r1, r2
   dd780:	b094      	sub	sp, #80	; 0x50
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  if (HaveSameShapes(input1, input2)) {
   dd782:	4629      	mov	r1, r5
   dd784:	4620      	mov	r0, r4
   dd786:	f006 faf3 	bl	e3d70 <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
   dd78a:	ae0f      	add	r6, sp, #60	; 0x3c
    reference_ops::BinaryFunction<bool, bool, bool>(
        GetTensorShape(input1), GetTensorData<bool>(input1),
   dd78c:	4621      	mov	r1, r4
                         bool (*func)(bool, bool)) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  if (HaveSameShapes(input1, input2)) {
   dd78e:	b1f8      	cbz	r0, dd7d0 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x70>
    reference_ops::BinaryFunction<bool, bool, bool>(
        GetTensorShape(input1), GetTensorData<bool>(input1),
   dd790:	a805      	add	r0, sp, #20
   dd792:	f7f9 f908 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dd796:	b104      	cbz	r4, dd79a <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x3a>
   dd798:	6864      	ldr	r4, [r4, #4]
        GetTensorShape(input2), GetTensorData<bool>(input2),
   dd79a:	4629      	mov	r1, r5
   dd79c:	a80a      	add	r0, sp, #40	; 0x28
   dd79e:	f7f9 f902 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd7a2:	b105      	cbz	r5, dd7a6 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x46>
   dd7a4:	686d      	ldr	r5, [r5, #4]
        GetTensorShape(output), GetTensorData<bool>(output), func);
   dd7a6:	4641      	mov	r1, r8
   dd7a8:	4630      	mov	r0, r6
   dd7aa:	f7f9 f8fc 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dd7ae:	f1b8 0f00 	cmp.w	r8, #0
   dd7b2:	d002      	beq.n	dd7ba <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x5a>
   dd7b4:	f8d8 2004 	ldr.w	r2, [r8, #4]
   dd7b8:	e000      	b.n	dd7bc <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x5c>
   dd7ba:	4642      	mov	r2, r8
   dd7bc:	9201      	str	r2, [sp, #4]
   dd7be:	9702      	str	r7, [sp, #8]
   dd7c0:	9600      	str	r6, [sp, #0]
   dd7c2:	462b      	mov	r3, r5
   dd7c4:	aa0a      	add	r2, sp, #40	; 0x28
   dd7c6:	4621      	mov	r1, r4
   dd7c8:	a805      	add	r0, sp, #20
   dd7ca:	f7ff fe97 	bl	dd4fc <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E>
   dd7ce:	e01e      	b.n	dd80e <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0xae>
  } else {
    reference_ops::BroadcastBinaryFunction4DSlow<bool, bool, bool>(
        GetTensorShape(input1), GetTensorData<bool>(input1),
   dd7d0:	a805      	add	r0, sp, #20
   dd7d2:	f7f9 f8e8 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dd7d6:	b104      	cbz	r4, dd7da <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x7a>
   dd7d8:	6864      	ldr	r4, [r4, #4]
        GetTensorShape(input2), GetTensorData<bool>(input2),
   dd7da:	4629      	mov	r1, r5
   dd7dc:	a80a      	add	r0, sp, #40	; 0x28
   dd7de:	f7f9 f8e2 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd7e2:	b105      	cbz	r5, dd7e6 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x86>
   dd7e4:	686d      	ldr	r5, [r5, #4]
        GetTensorShape(output), GetTensorData<bool>(output), func);
   dd7e6:	4641      	mov	r1, r8
   dd7e8:	4630      	mov	r0, r6
   dd7ea:	f7f9 f8dc 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dd7ee:	f1b8 0f00 	cmp.w	r8, #0
   dd7f2:	d002      	beq.n	dd7fa <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x9a>
   dd7f4:	f8d8 1004 	ldr.w	r1, [r8, #4]
   dd7f8:	e000      	b.n	dd7fc <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x9c>
   dd7fa:	4641      	mov	r1, r8
   dd7fc:	9101      	str	r1, [sp, #4]
   dd7fe:	9702      	str	r7, [sp, #8]
   dd800:	9600      	str	r6, [sp, #0]
   dd802:	462b      	mov	r3, r5
   dd804:	aa0a      	add	r2, sp, #40	; 0x28
   dd806:	4621      	mov	r1, r4
   dd808:	a805      	add	r0, sp, #20
   dd80a:	f7ff feca 	bl	dd5a2 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E>
   dd80e:	4630      	mov	r0, r6
   dd810:	f7f8 fe19 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
        GetTensorShape(input2), GetTensorData<bool>(input2),
        GetTensorShape(output), GetTensorData<bool>(output), func);
  } else {
    reference_ops::BroadcastBinaryFunction4DSlow<bool, bool, bool>(
        GetTensorShape(input1), GetTensorData<bool>(input1),
        GetTensorShape(input2), GetTensorData<bool>(input2),
   dd814:	a80a      	add	r0, sp, #40	; 0x28
   dd816:	f7f8 fe16 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
        GetTensorShape(input1), GetTensorData<bool>(input1),
        GetTensorShape(input2), GetTensorData<bool>(input2),
        GetTensorShape(output), GetTensorData<bool>(output), func);
  } else {
    reference_ops::BroadcastBinaryFunction4DSlow<bool, bool, bool>(
        GetTensorShape(input1), GetTensorData<bool>(input1),
   dd81a:	a805      	add	r0, sp, #20
   dd81c:	f7f8 fe13 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
        GetTensorShape(input2), GetTensorData<bool>(input2),
        GetTensorShape(output), GetTensorData<bool>(output), func);
  }

  return kTfLiteOk;
}
   dd820:	2000      	movs	r0, #0
   dd822:	b014      	add	sp, #80	; 0x50
   dd824:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}

000dd828 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_113LogicalOrEvalEP13TfLiteContextP10TfLiteNode>:

bool LogicalOr(bool x, bool y) { return x || y; }

TfLiteStatus LogicalOrEval(TfLiteContext* context, TfLiteNode* node) {
  return LogicalImpl(context, node, LogicalOr);
   dd828:	4a01      	ldr	r2, [pc, #4]	; (dd830 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_113LogicalOrEvalEP13TfLiteContextP10TfLiteNode+0x8>)
   dd82a:	f7ff bf99 	b.w	dd760 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE>
   dd82e:	bf00      	nop
   dd830:	000dd4d9 	.word	0x000dd4d9

000dd834 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_114LogicalAndEvalEP13TfLiteContextP10TfLiteNode>:
}

bool LogicalAnd(bool x, bool y) { return x && y; }

TfLiteStatus LogicalAndEval(TfLiteContext* context, TfLiteNode* node) {
  return LogicalImpl(context, node, LogicalAnd);
   dd834:	4a01      	ldr	r2, [pc, #4]	; (dd83c <_ZN6tflite3ops5micro7logical12_GLOBAL__N_114LogicalAndEvalEP13TfLiteContextP10TfLiteNode+0x8>)
   dd836:	f7ff bf93 	b.w	dd760 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE>
   dd83a:	bf00      	nop
   dd83c:	000dd4e3 	.word	0x000dd4e3

000dd840 <_ZN6tflite3ops5micro11activations7PrepareEP13TfLiteContextP10TfLiteNode>:
constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   dd840:	2000      	movs	r0, #0
   dd842:	4770      	bx	lr

000dd844 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf>:

namespace tflite {
namespace reference_ops {

inline void Logistic(const RuntimeShape& input_shape, const float* input_data,
                     const RuntimeShape& output_shape, float* output_data) {
   dd844:	e92d 47f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, lr}
   dd848:	ed2d 8b02 	vpush	{d8}
   dd84c:	461e      	mov	r6, r3
   dd84e:	f8d0 8000 	ldr.w	r8, [r0]

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   dd852:	6813      	ldr	r3, [r2, #0]
   dd854:	4598      	cmp	r8, r3
   dd856:	4604      	mov	r4, r0
   dd858:	460f      	mov	r7, r1
   dd85a:	4691      	mov	r9, r2
   dd85c:	d101      	bne.n	dd862 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x1e>
   dd85e:	2500      	movs	r5, #0
   dd860:	e00d      	b.n	dd87e <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x3a>
   dd862:	f006 fd63 	bl	e432c <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   dd866:	4629      	mov	r1, r5
   dd868:	4620      	mov	r0, r4
   dd86a:	f7f8 fdf7 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dd86e:	4629      	mov	r1, r5
   dd870:	4682      	mov	sl, r0
   dd872:	4648      	mov	r0, r9
   dd874:	f7f8 fdf2 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dd878:	4582      	cmp	sl, r0
   dd87a:	d1f2      	bne.n	dd862 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x1e>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   dd87c:	3501      	adds	r5, #1
   dd87e:	45a8      	cmp	r8, r5
   dd880:	dcf1      	bgt.n	dd866 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x22>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   dd882:	f1b8 0f04 	cmp.w	r8, #4
   dd886:	bfcc      	ite	gt
   dd888:	6864      	ldrgt	r4, [r4, #4]
   dd88a:	3404      	addle	r4, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dd88c:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
   dd88e:	f04f 0901 	mov.w	r9, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dd892:	4598      	cmp	r8, r3
   dd894:	dd05      	ble.n	dd8a2 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x5e>
      buffer_size *= dims_data[i];
   dd896:	f854 2023 	ldr.w	r2, [r4, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dd89a:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
   dd89c:	fb02 f909 	mul.w	r9, r2, r9
   dd8a0:	e7f7      	b.n	dd892 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x4e>
   dd8a2:	4634      	mov	r4, r6
   dd8a4:	463d      	mov	r5, r7
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dd8a6:	2600      	movs	r6, #0
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; i++) {
    float val = input_data[i];
    float result = 1.f / (1.f + std::exp(-val));
    output_data[i] = result;
   dd8a8:	eeb7 8a00 	vmov.f32	s16, #112	; 0x3f800000  1.0

inline void Logistic(const RuntimeShape& input_shape, const float* input_data,
                     const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; i++) {
   dd8ac:	454e      	cmp	r6, r9
   dd8ae:	da0d      	bge.n	dd8cc <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x88>
    float val = input_data[i];
   dd8b0:	ecb5 0a01 	vldmia	r5!, {s0}
  using ::exp;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  exp(float __x)
  { return __builtin_expf(__x); }
   dd8b4:	eeb1 0a40 	vneg.f32	s0, s0
   dd8b8:	f007 ff52 	bl	e5760 <expf>
    float result = 1.f / (1.f + std::exp(-val));
    output_data[i] = result;
   dd8bc:	ee30 0a08 	vadd.f32	s0, s0, s16

inline void Logistic(const RuntimeShape& input_shape, const float* input_data,
                     const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; i++) {
   dd8c0:	3601      	adds	r6, #1
    float val = input_data[i];
    float result = 1.f / (1.f + std::exp(-val));
    output_data[i] = result;
   dd8c2:	eec8 7a00 	vdiv.f32	s15, s16, s0
   dd8c6:	ece4 7a01 	vstmia	r4!, {s15}

inline void Logistic(const RuntimeShape& input_shape, const float* input_data,
                     const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; i++) {
   dd8ca:	e7ef      	b.n	dd8ac <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x68>
    float val = input_data[i];
    float result = 1.f / (1.f + std::exp(-val));
    output_data[i] = result;
  }
}
   dd8cc:	ecbd 8b02 	vpop	{d8}
   dd8d0:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}

000dd8d4 <_ZN6tflite3ops5micro11activations4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dd8d4:	b570      	push	{r4, r5, r6, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dd8d6:	680a      	ldr	r2, [r1, #0]
   dd8d8:	6883      	ldr	r3, [r0, #8]
   dd8da:	6852      	ldr	r2, [r2, #4]
   dd8dc:	2438      	movs	r4, #56	; 0x38
   dd8de:	4362      	muls	r2, r4
   dd8e0:	4605      	mov	r5, r0
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (input->type) {
   dd8e2:	5c98      	ldrb	r0, [r3, r2]
   dd8e4:	2801      	cmp	r0, #1

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dd8e6:	b08a      	sub	sp, #40	; 0x28
   dd8e8:	eb03 0602 	add.w	r6, r3, r2
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (input->type) {
   dd8ec:	d11d      	bne.n	dd92a <_ZN6tflite3ops5micro11activations4EvalEP13TfLiteContextP10TfLiteNode+0x56>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dd8ee:	684a      	ldr	r2, [r1, #4]
   dd8f0:	6852      	ldr	r2, [r2, #4]
    case kTfLiteFloat32: {
      reference_ops::Logistic(
          GetTensorShape(input), GetTensorData<float>(input),
   dd8f2:	4631      	mov	r1, r6
   dd8f4:	fb04 3402 	mla	r4, r4, r2, r3
   dd8f8:	4668      	mov	r0, sp
   dd8fa:	f7f9 f854 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
          GetTensorShape(output), GetTensorData<float>(output));
   dd8fe:	4621      	mov	r1, r4
   dd900:	a805      	add	r0, sp, #20
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dd902:	6875      	ldr	r5, [r6, #4]
   dd904:	f7f9 f84f 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dd908:	b10c      	cbz	r4, dd90e <_ZN6tflite3ops5micro11activations4EvalEP13TfLiteContextP10TfLiteNode+0x3a>
   dd90a:	6863      	ldr	r3, [r4, #4]
   dd90c:	e000      	b.n	dd910 <_ZN6tflite3ops5micro11activations4EvalEP13TfLiteContextP10TfLiteNode+0x3c>
   dd90e:	4623      	mov	r3, r4
   dd910:	aa05      	add	r2, sp, #20
   dd912:	4629      	mov	r1, r5
   dd914:	4668      	mov	r0, sp
   dd916:	f7ff ff95 	bl	dd844 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf>
   dd91a:	a805      	add	r0, sp, #20
   dd91c:	f7f8 fd93 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (input->type) {
    case kTfLiteFloat32: {
      reference_ops::Logistic(
          GetTensorShape(input), GetTensorData<float>(input),
   dd920:	4668      	mov	r0, sp
   dd922:	f7f8 fd90 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
          GetTensorShape(output), GetTensorData<float>(output));
      return kTfLiteOk;
   dd926:	2000      	movs	r0, #0
   dd928:	e007      	b.n	dd93a <_ZN6tflite3ops5micro11activations4EvalEP13TfLiteContextP10TfLiteNode+0x66>
    }
    default: {
      // TODO(b/141211002): Also support other data types once we have supported
      // temporary tensors in TFLM.
      context->ReportError(context,
   dd92a:	696c      	ldr	r4, [r5, #20]
   dd92c:	f7f6 fbf4 	bl	d4118 <TfLiteTypeGetName>
                           "Only float32 is supported currently, got %s",
                           TfLiteTypeGetName(input->type));
   dd930:	4903      	ldr	r1, [pc, #12]	; (dd940 <_ZN6tflite3ops5micro11activations4EvalEP13TfLiteContextP10TfLiteNode+0x6c>)
   dd932:	4602      	mov	r2, r0
   dd934:	4628      	mov	r0, r5
   dd936:	47a0      	blx	r4
      return kTfLiteError;
   dd938:	2001      	movs	r0, #1
    }
  }
}
   dd93a:	b00a      	add	sp, #40	; 0x28
   dd93c:	bd70      	pop	{r4, r5, r6, pc}
   dd93e:	bf00      	nop
   dd940:	000e9ee3 	.word	0x000e9ee3

000dd944 <_ZN6tflite3ops5micro17Register_LOGISTICEv>:
TfLiteRegistration* Register_LOGISTIC() {
  static TfLiteRegistration r = {/*init=*/nullptr,
                                 /*free=*/nullptr, activations::Prepare,
                                 activations::Eval};
  return &r;
}
   dd944:	4800      	ldr	r0, [pc, #0]	; (dd948 <_ZN6tflite3ops5micro17Register_LOGISTICEv+0x4>)
   dd946:	4770      	bx	lr
   dd948:	2003c048 	.word	0x2003c048

000dd94c <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MaximumOp2opIfEET_S6_S6_>:
};

struct MaximumOp {
  template <typename data_type>
  static data_type op(data_type el1, data_type el2) {
    return el1 > el2 ? el1 : el2;
   dd94c:	eeb4 0ae0 	vcmpe.f32	s0, s1
   dd950:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
  }
   dd954:	bfd8      	it	le
   dd956:	eeb0 0a60 	vmovle.f32	s0, s1
   dd95a:	4770      	bx	lr

000dd95c <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MaximumOp2opIhEET_S6_S6_>:
   dd95c:	4288      	cmp	r0, r1
   dd95e:	bf38      	it	cc
   dd960:	4608      	movcc	r0, r1
   dd962:	4770      	bx	lr

000dd964 <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MaximumOp2opIaEET_S6_S6_>:
   dd964:	4288      	cmp	r0, r1
   dd966:	bfb8      	it	lt
   dd968:	4608      	movlt	r0, r1
   dd96a:	4770      	bx	lr

000dd96c <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MaximumOp2opIlEET_S6_S6_>:
   dd96c:	4288      	cmp	r0, r1
   dd96e:	bfb8      	it	lt
   dd970:	4608      	movlt	r0, r1
   dd972:	4770      	bx	lr

000dd974 <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MaximumOp2opIxEET_S6_S6_>:
  TfLiteTensor* output;
};

struct MaximumOp {
  template <typename data_type>
  static data_type op(data_type el1, data_type el2) {
   dd974:	b500      	push	{lr}
    return el1 > el2 ? el1 : el2;
   dd976:	4290      	cmp	r0, r2
   dd978:	eb71 0e03 	sbcs.w	lr, r1, r3
   dd97c:	bfbc      	itt	lt
   dd97e:	4610      	movlt	r0, r2
   dd980:	4619      	movlt	r1, r3
  }
   dd982:	f85d fb04 	ldr.w	pc, [sp], #4

000dd986 <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MinimumOp2opIfEET_S6_S6_>:
};

struct MinimumOp {
  template <typename data_type>
  static data_type op(data_type el1, data_type el2) {
    return el1 < el2 ? el1 : el2;
   dd986:	eeb4 0ae0 	vcmpe.f32	s0, s1
   dd98a:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
  }
   dd98e:	bf58      	it	pl
   dd990:	eeb0 0a60 	vmovpl.f32	s0, s1
   dd994:	4770      	bx	lr

000dd996 <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MinimumOp2opIhEET_S6_S6_>:
   dd996:	4288      	cmp	r0, r1
   dd998:	bf28      	it	cs
   dd99a:	4608      	movcs	r0, r1
   dd99c:	4770      	bx	lr

000dd99e <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MinimumOp2opIaEET_S6_S6_>:
   dd99e:	4288      	cmp	r0, r1
   dd9a0:	bfa8      	it	ge
   dd9a2:	4608      	movge	r0, r1
   dd9a4:	4770      	bx	lr

000dd9a6 <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MinimumOp2opIlEET_S6_S6_>:
   dd9a6:	4288      	cmp	r0, r1
   dd9a8:	bfa8      	it	ge
   dd9aa:	4608      	movge	r0, r1
   dd9ac:	4770      	bx	lr

000dd9ae <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MinimumOp2opIxEET_S6_S6_>:
  }
};

struct MinimumOp {
  template <typename data_type>
  static data_type op(data_type el1, data_type el2) {
   dd9ae:	b500      	push	{lr}
    return el1 < el2 ? el1 : el2;
   dd9b0:	4282      	cmp	r2, r0
   dd9b2:	eb73 0e01 	sbcs.w	lr, r3, r1
   dd9b6:	bfbc      	itt	lt
   dd9b8:	4610      	movlt	r0, r2
   dd9ba:	4619      	movlt	r1, r3
  }
   dd9bc:	f85d fb04 	ldr.w	pc, [sp], #4

000dd9c0 <_ZN6tflite3ops5micro16Register_MAXIMUMEv>:
      /* free */ nullptr,
      /* prepare */ nullptr,
      maximum_minimum::Eval<maximum_minimum::kReference,
                            maximum_minimum::MaximumOp>};
  return &r;
}
   dd9c0:	4800      	ldr	r0, [pc, #0]	; (dd9c4 <_ZN6tflite3ops5micro16Register_MAXIMUMEv+0x4>)
   dd9c2:	4770      	bx	lr
   dd9c4:	2003c088 	.word	0x2003c088

000dd9c8 <_ZN6tflite3ops5micro16Register_MINIMUMEv>:
      /* free */ nullptr,
      /* prepare */ nullptr,
      maximum_minimum::Eval<maximum_minimum::kReference,
                            maximum_minimum::MinimumOp>};
  return &r;
}
   dd9c8:	4800      	ldr	r0, [pc, #0]	; (dd9cc <_ZN6tflite3ops5micro16Register_MINIMUMEv+0x4>)
   dd9ca:	4770      	bx	lr
   dd9cc:	2003c068 	.word	0x2003c068

000dd9d0 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>:

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   dd9d0:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dd9d4:	469b      	mov	fp, r3
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   dd9d6:	6803      	ldr	r3, [r0, #0]

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   dd9d8:	b09b      	sub	sp, #108	; 0x6c
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   dd9da:	2b04      	cmp	r3, #4

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   dd9dc:	4616      	mov	r6, r2
   dd9de:	4604      	mov	r4, r0
   dd9e0:	9102      	str	r1, [sp, #8]
   dd9e2:	9a24      	ldr	r2, [sp, #144]	; 0x90
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   dd9e4:	dd01      	ble.n	dd9ea <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x1a>
   dd9e6:	f006 fca1 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   dd9ea:	6833      	ldr	r3, [r6, #0]
   dd9ec:	2b04      	cmp	r3, #4
   dd9ee:	dcfa      	bgt.n	dd9e6 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   dd9f0:	6813      	ldr	r3, [r2, #0]
   dd9f2:	2b04      	cmp	r3, #4
   dd9f4:	dcf7      	bgt.n	dd9e6 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
   dd9f6:	2301      	movs	r3, #1
   dd9f8:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   dd9fa:	ad0a      	add	r5, sp, #40	; 0x28
   dd9fc:	a805      	add	r0, sp, #20
   dd9fe:	f7f8 fd66 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   dda02:	4620      	mov	r0, r4
   dda04:	ab12      	add	r3, sp, #72	; 0x48
   dda06:	462a      	mov	r2, r5
   dda08:	4631      	mov	r1, r6
   dda0a:	f7f9 f875 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   dda0e:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   dda10:	9503      	str	r5, [sp, #12]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   dda12:	2100      	movs	r1, #0
   dda14:	a805      	add	r0, sp, #20
   dda16:	f7f8 fd21 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dda1a:	4284      	cmp	r4, r0
   dda1c:	da49      	bge.n	ddab2 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xe2>
   dda1e:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   dda20:	af05      	add	r7, sp, #20
   dda22:	2101      	movs	r1, #1
   dda24:	4638      	mov	r0, r7
   dda26:	f7f8 fd19 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dda2a:	4285      	cmp	r5, r0
   dda2c:	da3f      	bge.n	ddaae <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xde>
   dda2e:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   dda30:	2102      	movs	r1, #2
   dda32:	4638      	mov	r0, r7
   dda34:	f7f8 fd12 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dda38:	4286      	cmp	r6, r0
   dda3a:	da36      	bge.n	ddaaa <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xda>
   dda3c:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dda40:	2103      	movs	r1, #3
   dda42:	4638      	mov	r0, r7
   dda44:	f7f8 fd0a 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dda48:	4580      	cmp	r8, r0
   dda4a:	da2c      	bge.n	ddaa6 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd6>
          auto out_idx = Offset(output_shape, b, y, x, c);
   dda4c:	f8cd 8000 	str.w	r8, [sp]
   dda50:	4633      	mov	r3, r6
   dda52:	462a      	mov	r2, r5
   dda54:	4621      	mov	r1, r4
   dda56:	4638      	mov	r0, r7
   dda58:	f7f8 fd65 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   dda5c:	f8cd 8000 	str.w	r8, [sp]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
   dda60:	4681      	mov	r9, r0
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   dda62:	4633      	mov	r3, r6
   dda64:	462a      	mov	r2, r5
   dda66:	4621      	mov	r1, r4
   dda68:	9803      	ldr	r0, [sp, #12]
   dda6a:	f7f8 fe0d 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   dda6e:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   dda72:	4682      	mov	sl, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   dda74:	4633      	mov	r3, r6
   dda76:	462a      	mov	r2, r5
   dda78:	4621      	mov	r1, r4
   dda7a:	a812      	add	r0, sp, #72	; 0x48
   dda7c:	f7f8 fe04 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   dda80:	9b25      	ldr	r3, [sp, #148]	; 0x94
   dda82:	eb03 0989 	add.w	r9, r3, r9, lsl #2
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
   dda86:	9b02      	ldr	r3, [sp, #8]
          auto in2_val = input2_data[in2_idx];
   dda88:	eb0b 0080 	add.w	r0, fp, r0, lsl #2
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
   dda8c:	eb03 0a8a 	add.w	sl, r3, sl, lsl #2
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   dda90:	edd0 0a00 	vldr	s1, [r0]
   dda94:	ed9a 0a00 	vldr	s0, [sl]
   dda98:	9b26      	ldr	r3, [sp, #152]	; 0x98
   dda9a:	4798      	blx	r3
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dda9c:	f108 0801 	add.w	r8, r8, #1
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   ddaa0:	ed89 0a00 	vstr	s0, [r9]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddaa4:	e7cc      	b.n	dda40 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x70>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   ddaa6:	3601      	adds	r6, #1
   ddaa8:	e7c2      	b.n	dda30 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x60>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   ddaaa:	3501      	adds	r5, #1
   ddaac:	e7b8      	b.n	dda20 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x50>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddaae:	3401      	adds	r4, #1
   ddab0:	e7af      	b.n	dda12 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x42>
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   ddab2:	a805      	add	r0, sp, #20
   ddab4:	f7f8 fcc7 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = op(in1_val, in2_val);
        }
      }
    }
  }
}
   ddab8:	b01b      	add	sp, #108	; 0x6c
   ddaba:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000ddabe <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>:

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddabe:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   ddac2:	4699      	mov	r9, r3
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddac4:	6803      	ldr	r3, [r0, #0]

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddac6:	b09b      	sub	sp, #108	; 0x6c
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddac8:	2b04      	cmp	r3, #4

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddaca:	4616      	mov	r6, r2
   ddacc:	4604      	mov	r4, r0
   ddace:	9102      	str	r1, [sp, #8]
   ddad0:	9a24      	ldr	r2, [sp, #144]	; 0x90
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddad2:	dd01      	ble.n	ddad8 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x1a>
   ddad4:	f006 fc2a 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   ddad8:	6833      	ldr	r3, [r6, #0]
   ddada:	2b04      	cmp	r3, #4
   ddadc:	dcfa      	bgt.n	ddad4 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   ddade:	6813      	ldr	r3, [r2, #0]
   ddae0:	2b04      	cmp	r3, #4
   ddae2:	dcf7      	bgt.n	ddad4 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
   ddae4:	2301      	movs	r3, #1
   ddae6:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   ddae8:	ad0a      	add	r5, sp, #40	; 0x28
   ddaea:	a805      	add	r0, sp, #20
   ddaec:	f7f8 fcef 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   ddaf0:	4620      	mov	r0, r4
   ddaf2:	ab12      	add	r3, sp, #72	; 0x48
   ddaf4:	462a      	mov	r2, r5
   ddaf6:	4631      	mov	r1, r6
   ddaf8:	f7f8 fffe 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddafc:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddafe:	9503      	str	r5, [sp, #12]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddb00:	2100      	movs	r1, #0
   ddb02:	a805      	add	r0, sp, #20
   ddb04:	f7f8 fcaa 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   ddb08:	4284      	cmp	r4, r0
   ddb0a:	da43      	bge.n	ddb94 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd6>
   ddb0c:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   ddb0e:	af05      	add	r7, sp, #20
   ddb10:	2101      	movs	r1, #1
   ddb12:	4638      	mov	r0, r7
   ddb14:	f7f8 fca2 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   ddb18:	4285      	cmp	r5, r0
   ddb1a:	da39      	bge.n	ddb90 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd2>
   ddb1c:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   ddb1e:	2102      	movs	r1, #2
   ddb20:	4638      	mov	r0, r7
   ddb22:	f7f8 fc9b 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   ddb26:	4286      	cmp	r6, r0
   ddb28:	da30      	bge.n	ddb8c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xce>
   ddb2a:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddb2e:	2103      	movs	r1, #3
   ddb30:	4638      	mov	r0, r7
   ddb32:	f7f8 fc93 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   ddb36:	4580      	cmp	r8, r0
   ddb38:	da26      	bge.n	ddb88 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xca>
          auto out_idx = Offset(output_shape, b, y, x, c);
   ddb3a:	f8cd 8000 	str.w	r8, [sp]
   ddb3e:	4633      	mov	r3, r6
   ddb40:	462a      	mov	r2, r5
   ddb42:	4621      	mov	r1, r4
   ddb44:	4638      	mov	r0, r7
   ddb46:	f7f8 fcee 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddb4a:	f8cd 8000 	str.w	r8, [sp]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
   ddb4e:	4682      	mov	sl, r0
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddb50:	4633      	mov	r3, r6
   ddb52:	462a      	mov	r2, r5
   ddb54:	4621      	mov	r1, r4
   ddb56:	9803      	ldr	r0, [sp, #12]
   ddb58:	f7f8 fd96 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   ddb5c:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddb60:	4683      	mov	fp, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   ddb62:	4633      	mov	r3, r6
   ddb64:	462a      	mov	r2, r5
   ddb66:	4621      	mov	r1, r4
   ddb68:	a812      	add	r0, sp, #72	; 0x48
   ddb6a:	f7f8 fd8d 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   ddb6e:	9b02      	ldr	r3, [sp, #8]
   ddb70:	f819 1000 	ldrb.w	r1, [r9, r0]
   ddb74:	f813 000b 	ldrb.w	r0, [r3, fp]
   ddb78:	9b26      	ldr	r3, [sp, #152]	; 0x98
   ddb7a:	4798      	blx	r3
   ddb7c:	9b25      	ldr	r3, [sp, #148]	; 0x94
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddb7e:	f108 0801 	add.w	r8, r8, #1
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   ddb82:	f803 000a 	strb.w	r0, [r3, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddb86:	e7d2      	b.n	ddb2e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x70>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   ddb88:	3601      	adds	r6, #1
   ddb8a:	e7c8      	b.n	ddb1e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x60>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   ddb8c:	3501      	adds	r5, #1
   ddb8e:	e7be      	b.n	ddb0e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x50>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddb90:	3401      	adds	r4, #1
   ddb92:	e7b5      	b.n	ddb00 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x42>
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   ddb94:	a805      	add	r0, sp, #20
   ddb96:	f7f8 fc56 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = op(in1_val, in2_val);
        }
      }
    }
  }
}
   ddb9a:	b01b      	add	sp, #108	; 0x6c
   ddb9c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000ddba0 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>:

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddba0:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   ddba4:	4699      	mov	r9, r3
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddba6:	6803      	ldr	r3, [r0, #0]

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddba8:	b09b      	sub	sp, #108	; 0x6c
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddbaa:	2b04      	cmp	r3, #4

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddbac:	4616      	mov	r6, r2
   ddbae:	4604      	mov	r4, r0
   ddbb0:	9102      	str	r1, [sp, #8]
   ddbb2:	9a24      	ldr	r2, [sp, #144]	; 0x90
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddbb4:	dd01      	ble.n	ddbba <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x1a>
   ddbb6:	f006 fbb9 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   ddbba:	6833      	ldr	r3, [r6, #0]
   ddbbc:	2b04      	cmp	r3, #4
   ddbbe:	dcfa      	bgt.n	ddbb6 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   ddbc0:	6813      	ldr	r3, [r2, #0]
   ddbc2:	2b04      	cmp	r3, #4
   ddbc4:	dcf7      	bgt.n	ddbb6 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
   ddbc6:	2301      	movs	r3, #1
   ddbc8:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   ddbca:	ad0a      	add	r5, sp, #40	; 0x28
   ddbcc:	a805      	add	r0, sp, #20
   ddbce:	f7f8 fc7e 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   ddbd2:	4620      	mov	r0, r4
   ddbd4:	ab12      	add	r3, sp, #72	; 0x48
   ddbd6:	462a      	mov	r2, r5
   ddbd8:	4631      	mov	r1, r6
   ddbda:	f7f8 ff8d 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddbde:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddbe0:	9503      	str	r5, [sp, #12]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddbe2:	2100      	movs	r1, #0
   ddbe4:	a805      	add	r0, sp, #20
   ddbe6:	f7f8 fc39 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   ddbea:	4284      	cmp	r4, r0
   ddbec:	da43      	bge.n	ddc76 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd6>
   ddbee:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   ddbf0:	af05      	add	r7, sp, #20
   ddbf2:	2101      	movs	r1, #1
   ddbf4:	4638      	mov	r0, r7
   ddbf6:	f7f8 fc31 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   ddbfa:	4285      	cmp	r5, r0
   ddbfc:	da39      	bge.n	ddc72 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd2>
   ddbfe:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   ddc00:	2102      	movs	r1, #2
   ddc02:	4638      	mov	r0, r7
   ddc04:	f7f8 fc2a 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   ddc08:	4286      	cmp	r6, r0
   ddc0a:	da30      	bge.n	ddc6e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xce>
   ddc0c:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddc10:	2103      	movs	r1, #3
   ddc12:	4638      	mov	r0, r7
   ddc14:	f7f8 fc22 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   ddc18:	4580      	cmp	r8, r0
   ddc1a:	da26      	bge.n	ddc6a <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xca>
          auto out_idx = Offset(output_shape, b, y, x, c);
   ddc1c:	f8cd 8000 	str.w	r8, [sp]
   ddc20:	4633      	mov	r3, r6
   ddc22:	462a      	mov	r2, r5
   ddc24:	4621      	mov	r1, r4
   ddc26:	4638      	mov	r0, r7
   ddc28:	f7f8 fc7d 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddc2c:	f8cd 8000 	str.w	r8, [sp]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
   ddc30:	4682      	mov	sl, r0
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddc32:	4633      	mov	r3, r6
   ddc34:	462a      	mov	r2, r5
   ddc36:	4621      	mov	r1, r4
   ddc38:	9803      	ldr	r0, [sp, #12]
   ddc3a:	f7f8 fd25 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   ddc3e:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddc42:	4683      	mov	fp, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   ddc44:	4633      	mov	r3, r6
   ddc46:	462a      	mov	r2, r5
   ddc48:	4621      	mov	r1, r4
   ddc4a:	a812      	add	r0, sp, #72	; 0x48
   ddc4c:	f7f8 fd1c 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   ddc50:	9b02      	ldr	r3, [sp, #8]
   ddc52:	f919 1000 	ldrsb.w	r1, [r9, r0]
   ddc56:	f913 000b 	ldrsb.w	r0, [r3, fp]
   ddc5a:	9b26      	ldr	r3, [sp, #152]	; 0x98
   ddc5c:	4798      	blx	r3
   ddc5e:	9b25      	ldr	r3, [sp, #148]	; 0x94
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddc60:	f108 0801 	add.w	r8, r8, #1
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   ddc64:	f803 000a 	strb.w	r0, [r3, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddc68:	e7d2      	b.n	ddc10 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x70>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   ddc6a:	3601      	adds	r6, #1
   ddc6c:	e7c8      	b.n	ddc00 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x60>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   ddc6e:	3501      	adds	r5, #1
   ddc70:	e7be      	b.n	ddbf0 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x50>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddc72:	3401      	adds	r4, #1
   ddc74:	e7b5      	b.n	ddbe2 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x42>
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   ddc76:	a805      	add	r0, sp, #20
   ddc78:	f7f8 fbe5 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = op(in1_val, in2_val);
        }
      }
    }
  }
}
   ddc7c:	b01b      	add	sp, #108	; 0x6c
   ddc7e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000ddc82 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>:

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddc82:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   ddc86:	4699      	mov	r9, r3
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddc88:	6803      	ldr	r3, [r0, #0]

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddc8a:	b09b      	sub	sp, #108	; 0x6c
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddc8c:	2b04      	cmp	r3, #4

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddc8e:	4616      	mov	r6, r2
   ddc90:	4604      	mov	r4, r0
   ddc92:	9102      	str	r1, [sp, #8]
   ddc94:	9a24      	ldr	r2, [sp, #144]	; 0x90
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddc96:	dd01      	ble.n	ddc9c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x1a>
   ddc98:	f006 fb48 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   ddc9c:	6833      	ldr	r3, [r6, #0]
   ddc9e:	2b04      	cmp	r3, #4
   ddca0:	dcfa      	bgt.n	ddc98 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   ddca2:	6813      	ldr	r3, [r2, #0]
   ddca4:	2b04      	cmp	r3, #4
   ddca6:	dcf7      	bgt.n	ddc98 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
   ddca8:	2301      	movs	r3, #1
   ddcaa:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   ddcac:	ad0a      	add	r5, sp, #40	; 0x28
   ddcae:	a805      	add	r0, sp, #20
   ddcb0:	f7f8 fc0d 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   ddcb4:	4620      	mov	r0, r4
   ddcb6:	ab12      	add	r3, sp, #72	; 0x48
   ddcb8:	462a      	mov	r2, r5
   ddcba:	4631      	mov	r1, r6
   ddcbc:	f7f8 ff1c 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddcc0:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddcc2:	9503      	str	r5, [sp, #12]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddcc4:	2100      	movs	r1, #0
   ddcc6:	a805      	add	r0, sp, #20
   ddcc8:	f7f8 fbc8 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   ddccc:	4284      	cmp	r4, r0
   ddcce:	da43      	bge.n	ddd58 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd6>
   ddcd0:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   ddcd2:	af05      	add	r7, sp, #20
   ddcd4:	2101      	movs	r1, #1
   ddcd6:	4638      	mov	r0, r7
   ddcd8:	f7f8 fbc0 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   ddcdc:	4285      	cmp	r5, r0
   ddcde:	da39      	bge.n	ddd54 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd2>
   ddce0:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   ddce2:	2102      	movs	r1, #2
   ddce4:	4638      	mov	r0, r7
   ddce6:	f7f8 fbb9 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   ddcea:	4286      	cmp	r6, r0
   ddcec:	da30      	bge.n	ddd50 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xce>
   ddcee:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddcf2:	2103      	movs	r1, #3
   ddcf4:	4638      	mov	r0, r7
   ddcf6:	f7f8 fbb1 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   ddcfa:	4580      	cmp	r8, r0
   ddcfc:	da26      	bge.n	ddd4c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xca>
          auto out_idx = Offset(output_shape, b, y, x, c);
   ddcfe:	f8cd 8000 	str.w	r8, [sp]
   ddd02:	4633      	mov	r3, r6
   ddd04:	462a      	mov	r2, r5
   ddd06:	4621      	mov	r1, r4
   ddd08:	4638      	mov	r0, r7
   ddd0a:	f7f8 fc0c 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddd0e:	f8cd 8000 	str.w	r8, [sp]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
   ddd12:	4682      	mov	sl, r0
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddd14:	4633      	mov	r3, r6
   ddd16:	462a      	mov	r2, r5
   ddd18:	4621      	mov	r1, r4
   ddd1a:	9803      	ldr	r0, [sp, #12]
   ddd1c:	f7f8 fcb4 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   ddd20:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddd24:	4683      	mov	fp, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   ddd26:	4633      	mov	r3, r6
   ddd28:	462a      	mov	r2, r5
   ddd2a:	4621      	mov	r1, r4
   ddd2c:	a812      	add	r0, sp, #72	; 0x48
   ddd2e:	f7f8 fcab 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   ddd32:	9b02      	ldr	r3, [sp, #8]
   ddd34:	f859 1020 	ldr.w	r1, [r9, r0, lsl #2]
   ddd38:	f853 002b 	ldr.w	r0, [r3, fp, lsl #2]
   ddd3c:	9b26      	ldr	r3, [sp, #152]	; 0x98
   ddd3e:	4798      	blx	r3
   ddd40:	9b25      	ldr	r3, [sp, #148]	; 0x94
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddd42:	f108 0801 	add.w	r8, r8, #1
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   ddd46:	f843 002a 	str.w	r0, [r3, sl, lsl #2]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddd4a:	e7d2      	b.n	ddcf2 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x70>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   ddd4c:	3601      	adds	r6, #1
   ddd4e:	e7c8      	b.n	ddce2 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x60>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   ddd50:	3501      	adds	r5, #1
   ddd52:	e7be      	b.n	ddcd2 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x50>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddd54:	3401      	adds	r4, #1
   ddd56:	e7b5      	b.n	ddcc4 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x42>
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   ddd58:	a805      	add	r0, sp, #20
   ddd5a:	f7f8 fb74 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = op(in1_val, in2_val);
        }
      }
    }
  }
}
   ddd5e:	b01b      	add	sp, #108	; 0x6c
   ddd60:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000ddd64 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>:

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddd64:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   ddd68:	469b      	mov	fp, r3
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddd6a:	6803      	ldr	r3, [r0, #0]

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddd6c:	b09d      	sub	sp, #116	; 0x74
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddd6e:	2b04      	cmp	r3, #4

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddd70:	4616      	mov	r6, r2
   ddd72:	4604      	mov	r4, r0
   ddd74:	9104      	str	r1, [sp, #16]
   ddd76:	9a26      	ldr	r2, [sp, #152]	; 0x98
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddd78:	dd01      	ble.n	ddd7e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x1a>
   ddd7a:	f006 fad7 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   ddd7e:	6833      	ldr	r3, [r6, #0]
   ddd80:	2b04      	cmp	r3, #4
   ddd82:	dcfa      	bgt.n	ddd7a <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   ddd84:	6813      	ldr	r3, [r2, #0]
   ddd86:	2b04      	cmp	r3, #4
   ddd88:	dcf7      	bgt.n	ddd7a <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
   ddd8a:	2301      	movs	r3, #1
   ddd8c:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   ddd8e:	ad0c      	add	r5, sp, #48	; 0x30
   ddd90:	a807      	add	r0, sp, #28
   ddd92:	f7f8 fb9c 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   ddd96:	4620      	mov	r0, r4
   ddd98:	ab14      	add	r3, sp, #80	; 0x50
   ddd9a:	462a      	mov	r2, r5
   ddd9c:	4631      	mov	r1, r6
   ddd9e:	f7f8 feab 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddda2:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddda4:	9505      	str	r5, [sp, #20]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddda6:	2100      	movs	r1, #0
   ddda8:	a807      	add	r0, sp, #28
   dddaa:	f7f8 fb57 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dddae:	4284      	cmp	r4, r0
   dddb0:	da4a      	bge.n	dde48 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xe4>
   dddb2:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   dddb4:	af07      	add	r7, sp, #28
   dddb6:	2101      	movs	r1, #1
   dddb8:	4638      	mov	r0, r7
   dddba:	f7f8 fb4f 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dddbe:	4285      	cmp	r5, r0
   dddc0:	da40      	bge.n	dde44 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xe0>
   dddc2:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   dddc4:	9703      	str	r7, [sp, #12]
   dddc6:	2102      	movs	r1, #2
   dddc8:	9803      	ldr	r0, [sp, #12]
   dddca:	f7f8 fb47 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dddce:	4286      	cmp	r6, r0
   dddd0:	da36      	bge.n	dde40 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xdc>
   dddd2:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dddd6:	2103      	movs	r1, #3
   dddd8:	9803      	ldr	r0, [sp, #12]
   dddda:	f7f8 fb3f 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dddde:	4580      	cmp	r8, r0
   ddde0:	da2c      	bge.n	dde3c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd8>
          auto out_idx = Offset(output_shape, b, y, x, c);
   ddde2:	f8cd 8000 	str.w	r8, [sp]
   ddde6:	4633      	mov	r3, r6
   ddde8:	462a      	mov	r2, r5
   dddea:	4621      	mov	r1, r4
   dddec:	9803      	ldr	r0, [sp, #12]
   dddee:	f7f8 fb9a 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   dddf2:	f8cd 8000 	str.w	r8, [sp]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
   dddf6:	4681      	mov	r9, r0
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   dddf8:	4633      	mov	r3, r6
   dddfa:	462a      	mov	r2, r5
   dddfc:	4621      	mov	r1, r4
   dddfe:	9805      	ldr	r0, [sp, #20]
   dde00:	f7f8 fc42 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   dde04:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   dde08:	4682      	mov	sl, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   dde0a:	4633      	mov	r3, r6
   dde0c:	462a      	mov	r2, r5
   dde0e:	4621      	mov	r1, r4
   dde10:	a814      	add	r0, sp, #80	; 0x50
   dde12:	f7f8 fc39 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   dde16:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   dde18:	9f28      	ldr	r7, [sp, #160]	; 0xa0
   dde1a:	eb03 09c9 	add.w	r9, r3, r9, lsl #3
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
   dde1e:	9b04      	ldr	r3, [sp, #16]
          auto in2_val = input2_data[in2_idx];
   dde20:	eb0b 00c0 	add.w	r0, fp, r0, lsl #3
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
   dde24:	eb03 0aca 	add.w	sl, r3, sl, lsl #3
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   dde28:	e9d0 2300 	ldrd	r2, r3, [r0]
   dde2c:	e9da 0100 	ldrd	r0, r1, [sl]
   dde30:	47b8      	blx	r7
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dde32:	f108 0801 	add.w	r8, r8, #1
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   dde36:	e9c9 0100 	strd	r0, r1, [r9]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dde3a:	e7cc      	b.n	dddd6 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x72>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   dde3c:	3601      	adds	r6, #1
   dde3e:	e7c2      	b.n	dddc6 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x62>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   dde40:	3501      	adds	r5, #1
   dde42:	e7b7      	b.n	dddb4 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x50>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   dde44:	3401      	adds	r4, #1
   dde46:	e7ae      	b.n	ddda6 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x42>
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   dde48:	a807      	add	r0, sp, #28
   dde4a:	f7f8 fafc 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = op(in1_val, in2_val);
        }
      }
    }
  }
}
   dde4e:	b01d      	add	sp, #116	; 0x74
   dde50:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dde54 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode>:
      GetTensorData<data_type>(op_context.output),
      op_type::template op<data_type>);
}

template <KernelType kernel_type, typename OpType>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dde54:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
   dde58:	680b      	ldr	r3, [r1, #0]
   dde5a:	f8d0 8008 	ldr.w	r8, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dde5e:	685c      	ldr	r4, [r3, #4]
   dde60:	689d      	ldr	r5, [r3, #8]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dde62:	684b      	ldr	r3, [r1, #4]
   dde64:	685f      	ldr	r7, [r3, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dde66:	2238      	movs	r2, #56	; 0x38
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dde68:	4357      	muls	r7, r2
   dde6a:	4681      	mov	r9, r0
  OpContext op_context(context, node);

  if (kernel_type == kReference) {
    switch (op_context.output->type) {
   dde6c:	f818 0007 	ldrb.w	r0, [r8, r7]
   dde70:	1e43      	subs	r3, r0, #1
      GetTensorData<data_type>(op_context.output),
      op_type::template op<data_type>);
}

template <KernelType kernel_type, typename OpType>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dde72:	b095      	sub	sp, #84	; 0x54
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dde74:	fb02 8404 	mla	r4, r2, r4, r8
   dde78:	fb02 8505 	mla	r5, r2, r5, r8
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dde7c:	eb08 0607 	add.w	r6, r8, r7
  OpContext op_context(context, node);

  if (kernel_type == kReference) {
    switch (op_context.output->type) {
   dde80:	2b08      	cmp	r3, #8
   dde82:	f200 80a2 	bhi.w	ddfca <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x176>
   dde86:	e8df f003 	tbb	[pc, r3]
   dde8a:	5c05      	.short	0x5c05
   dde8c:	a0a07922 	.word	0xa0a07922
   dde90:	a0a0      	.short	0xa0a0
   dde92:	3f          	.byte	0x3f
   dde93:	00          	.byte	0x00
}  // namespace

template <typename data_type, typename op_type>
void TFLiteOperation(TfLiteContext* context, TfLiteNode* node,
                     const OpContext& op_context) {
  reference_ops::MaximumMinimumBroadcast4DSlow(
   dde94:	4621      	mov	r1, r4
   dde96:	a80f      	add	r0, sp, #60	; 0x3c
   dde98:	f7f8 fd85 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dde9c:	b104      	cbz	r4, ddea0 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x4c>
   dde9e:	6864      	ldr	r4, [r4, #4]
   ddea0:	4629      	mov	r1, r5
   ddea2:	a80a      	add	r0, sp, #40	; 0x28
   ddea4:	f7f8 fd7f 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddea8:	b105      	cbz	r5, ddeac <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x58>
   ddeaa:	686d      	ldr	r5, [r5, #4]
   ddeac:	af05      	add	r7, sp, #20
   ddeae:	4631      	mov	r1, r6
   ddeb0:	4638      	mov	r0, r7
   ddeb2:	f7f8 fd78 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddeb6:	4b4c      	ldr	r3, [pc, #304]	; (ddfe8 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x194>)
   ddeb8:	9302      	str	r3, [sp, #8]
   ddeba:	6873      	ldr	r3, [r6, #4]
   ddebc:	9301      	str	r3, [sp, #4]
   ddebe:	9700      	str	r7, [sp, #0]
   ddec0:	462b      	mov	r3, r5
   ddec2:	aa0a      	add	r2, sp, #40	; 0x28
   ddec4:	4621      	mov	r1, r4
   ddec6:	a80f      	add	r0, sp, #60	; 0x3c
   ddec8:	f7ff fd82 	bl	dd9d0 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   ddecc:	e072      	b.n	ddfb4 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
   ddece:	4621      	mov	r1, r4
   dded0:	a80f      	add	r0, sp, #60	; 0x3c
   dded2:	f7f8 fd68 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dded6:	b104      	cbz	r4, ddeda <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x86>
   dded8:	6864      	ldr	r4, [r4, #4]
   ddeda:	4629      	mov	r1, r5
   ddedc:	a80a      	add	r0, sp, #40	; 0x28
   ddede:	f7f8 fd62 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddee2:	b105      	cbz	r5, ddee6 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x92>
   ddee4:	686d      	ldr	r5, [r5, #4]
   ddee6:	af05      	add	r7, sp, #20
   ddee8:	4631      	mov	r1, r6
   ddeea:	4638      	mov	r0, r7
   ddeec:	f7f8 fd5b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddef0:	4b3e      	ldr	r3, [pc, #248]	; (ddfec <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x198>)
   ddef2:	9302      	str	r3, [sp, #8]
   ddef4:	6873      	ldr	r3, [r6, #4]
   ddef6:	9301      	str	r3, [sp, #4]
   ddef8:	9700      	str	r7, [sp, #0]
   ddefa:	462b      	mov	r3, r5
   ddefc:	aa0a      	add	r2, sp, #40	; 0x28
   ddefe:	4621      	mov	r1, r4
   ddf00:	a80f      	add	r0, sp, #60	; 0x3c
   ddf02:	f7ff fddc 	bl	ddabe <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   ddf06:	e055      	b.n	ddfb4 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
   ddf08:	4621      	mov	r1, r4
   ddf0a:	a80f      	add	r0, sp, #60	; 0x3c
   ddf0c:	f7f8 fd4b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddf10:	b104      	cbz	r4, ddf14 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xc0>
   ddf12:	6864      	ldr	r4, [r4, #4]
   ddf14:	4629      	mov	r1, r5
   ddf16:	a80a      	add	r0, sp, #40	; 0x28
   ddf18:	f7f8 fd45 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddf1c:	b105      	cbz	r5, ddf20 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xcc>
   ddf1e:	686d      	ldr	r5, [r5, #4]
   ddf20:	af05      	add	r7, sp, #20
   ddf22:	4631      	mov	r1, r6
   ddf24:	4638      	mov	r0, r7
   ddf26:	f7f8 fd3e 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddf2a:	4b31      	ldr	r3, [pc, #196]	; (ddff0 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x19c>)
   ddf2c:	9302      	str	r3, [sp, #8]
   ddf2e:	6873      	ldr	r3, [r6, #4]
   ddf30:	9301      	str	r3, [sp, #4]
   ddf32:	9700      	str	r7, [sp, #0]
   ddf34:	462b      	mov	r3, r5
   ddf36:	aa0a      	add	r2, sp, #40	; 0x28
   ddf38:	4621      	mov	r1, r4
   ddf3a:	a80f      	add	r0, sp, #60	; 0x3c
   ddf3c:	f7ff fe30 	bl	ddba0 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   ddf40:	e038      	b.n	ddfb4 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
   ddf42:	4621      	mov	r1, r4
   ddf44:	a80f      	add	r0, sp, #60	; 0x3c
   ddf46:	f7f8 fd2e 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddf4a:	b104      	cbz	r4, ddf4e <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xfa>
   ddf4c:	6864      	ldr	r4, [r4, #4]
   ddf4e:	4629      	mov	r1, r5
   ddf50:	a80a      	add	r0, sp, #40	; 0x28
   ddf52:	f7f8 fd28 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddf56:	b105      	cbz	r5, ddf5a <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x106>
   ddf58:	686d      	ldr	r5, [r5, #4]
   ddf5a:	af05      	add	r7, sp, #20
   ddf5c:	4631      	mov	r1, r6
   ddf5e:	4638      	mov	r0, r7
   ddf60:	f7f8 fd21 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddf64:	4b23      	ldr	r3, [pc, #140]	; (ddff4 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x1a0>)
   ddf66:	9302      	str	r3, [sp, #8]
   ddf68:	6873      	ldr	r3, [r6, #4]
   ddf6a:	9301      	str	r3, [sp, #4]
   ddf6c:	9700      	str	r7, [sp, #0]
   ddf6e:	462b      	mov	r3, r5
   ddf70:	aa0a      	add	r2, sp, #40	; 0x28
   ddf72:	4621      	mov	r1, r4
   ddf74:	a80f      	add	r0, sp, #60	; 0x3c
   ddf76:	f7ff fe84 	bl	ddc82 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   ddf7a:	e01b      	b.n	ddfb4 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
   ddf7c:	4621      	mov	r1, r4
   ddf7e:	a80f      	add	r0, sp, #60	; 0x3c
   ddf80:	f7f8 fd11 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddf84:	b104      	cbz	r4, ddf88 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x134>
   ddf86:	6864      	ldr	r4, [r4, #4]
   ddf88:	4629      	mov	r1, r5
   ddf8a:	a80a      	add	r0, sp, #40	; 0x28
   ddf8c:	f7f8 fd0b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddf90:	b105      	cbz	r5, ddf94 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x140>
   ddf92:	686d      	ldr	r5, [r5, #4]
   ddf94:	af05      	add	r7, sp, #20
   ddf96:	4631      	mov	r1, r6
   ddf98:	4638      	mov	r0, r7
   ddf9a:	f7f8 fd04 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddf9e:	4b16      	ldr	r3, [pc, #88]	; (ddff8 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x1a4>)
   ddfa0:	9302      	str	r3, [sp, #8]
   ddfa2:	6873      	ldr	r3, [r6, #4]
   ddfa4:	9301      	str	r3, [sp, #4]
   ddfa6:	9700      	str	r7, [sp, #0]
   ddfa8:	462b      	mov	r3, r5
   ddfaa:	aa0a      	add	r2, sp, #40	; 0x28
   ddfac:	4621      	mov	r1, r4
   ddfae:	a80f      	add	r0, sp, #60	; 0x3c
   ddfb0:	f7ff fed8 	bl	ddd64 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   ddfb4:	4638      	mov	r0, r7
   ddfb6:	f7f8 fa46 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   ddfba:	a80a      	add	r0, sp, #40	; 0x28
   ddfbc:	f7f8 fa43 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   ddfc0:	a80f      	add	r0, sp, #60	; 0x3c
   ddfc2:	f7f8 fa40 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  } else {
    context->ReportError(context,
                         "Kernel type not supported by Maximum/Minimum.");
    return kTfLiteError;
  }
  return kTfLiteOk;
   ddfc6:	2000      	movs	r0, #0
   ddfc8:	e00a      	b.n	ddfe0 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x18c>
        break;
      case kTfLiteInt64:
        TFLiteOperation<int64_t, OpType>(context, node, op_context);
        break;
      default:
        context->ReportError(
   ddfca:	f8d9 4014 	ldr.w	r4, [r9, #20]
   ddfce:	f7f6 f8a3 	bl	d4118 <TfLiteTypeGetName>
   ddfd2:	f818 3007 	ldrb.w	r3, [r8, r7]
   ddfd6:	4909      	ldr	r1, [pc, #36]	; (ddffc <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x1a8>)
   ddfd8:	4602      	mov	r2, r0
   ddfda:	4648      	mov	r0, r9
   ddfdc:	47a0      	blx	r4
            context, "Type %s (%d) is not supported by Maximum/Minimum.",
            TfLiteTypeGetName(op_context.output->type),
            op_context.output->type);
        return kTfLiteError;
   ddfde:	2001      	movs	r0, #1
    context->ReportError(context,
                         "Kernel type not supported by Maximum/Minimum.");
    return kTfLiteError;
  }
  return kTfLiteOk;
}
   ddfe0:	b015      	add	sp, #84	; 0x54
   ddfe2:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}
   ddfe6:	bf00      	nop
   ddfe8:	000dd987 	.word	0x000dd987
   ddfec:	000dd997 	.word	0x000dd997
   ddff0:	000dd99f 	.word	0x000dd99f
   ddff4:	000dd9a7 	.word	0x000dd9a7
   ddff8:	000dd9af 	.word	0x000dd9af
   ddffc:	000e9f0f 	.word	0x000e9f0f

000de000 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode>:
      GetTensorData<data_type>(op_context.output),
      op_type::template op<data_type>);
}

template <KernelType kernel_type, typename OpType>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   de000:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
   de004:	680b      	ldr	r3, [r1, #0]
   de006:	f8d0 8008 	ldr.w	r8, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   de00a:	685c      	ldr	r4, [r3, #4]
   de00c:	689d      	ldr	r5, [r3, #8]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   de00e:	684b      	ldr	r3, [r1, #4]
   de010:	685f      	ldr	r7, [r3, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   de012:	2238      	movs	r2, #56	; 0x38
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   de014:	4357      	muls	r7, r2
   de016:	4681      	mov	r9, r0
  OpContext op_context(context, node);

  if (kernel_type == kReference) {
    switch (op_context.output->type) {
   de018:	f818 0007 	ldrb.w	r0, [r8, r7]
   de01c:	1e43      	subs	r3, r0, #1
      GetTensorData<data_type>(op_context.output),
      op_type::template op<data_type>);
}

template <KernelType kernel_type, typename OpType>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   de01e:	b095      	sub	sp, #84	; 0x54
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   de020:	fb02 8404 	mla	r4, r2, r4, r8
   de024:	fb02 8505 	mla	r5, r2, r5, r8
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   de028:	eb08 0607 	add.w	r6, r8, r7
  OpContext op_context(context, node);

  if (kernel_type == kReference) {
    switch (op_context.output->type) {
   de02c:	2b08      	cmp	r3, #8
   de02e:	f200 80a2 	bhi.w	de176 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x176>
   de032:	e8df f003 	tbb	[pc, r3]
   de036:	5c05      	.short	0x5c05
   de038:	a0a07922 	.word	0xa0a07922
   de03c:	a0a0      	.short	0xa0a0
   de03e:	3f          	.byte	0x3f
   de03f:	00          	.byte	0x00
}  // namespace

template <typename data_type, typename op_type>
void TFLiteOperation(TfLiteContext* context, TfLiteNode* node,
                     const OpContext& op_context) {
  reference_ops::MaximumMinimumBroadcast4DSlow(
   de040:	4621      	mov	r1, r4
   de042:	a80f      	add	r0, sp, #60	; 0x3c
   de044:	f7f8 fcaf 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de048:	b104      	cbz	r4, de04c <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x4c>
   de04a:	6864      	ldr	r4, [r4, #4]
   de04c:	4629      	mov	r1, r5
   de04e:	a80a      	add	r0, sp, #40	; 0x28
   de050:	f7f8 fca9 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de054:	b105      	cbz	r5, de058 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x58>
   de056:	686d      	ldr	r5, [r5, #4]
   de058:	af05      	add	r7, sp, #20
   de05a:	4631      	mov	r1, r6
   de05c:	4638      	mov	r0, r7
   de05e:	f7f8 fca2 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de062:	4b4c      	ldr	r3, [pc, #304]	; (de194 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x194>)
   de064:	9302      	str	r3, [sp, #8]
   de066:	6873      	ldr	r3, [r6, #4]
   de068:	9301      	str	r3, [sp, #4]
   de06a:	9700      	str	r7, [sp, #0]
   de06c:	462b      	mov	r3, r5
   de06e:	aa0a      	add	r2, sp, #40	; 0x28
   de070:	4621      	mov	r1, r4
   de072:	a80f      	add	r0, sp, #60	; 0x3c
   de074:	f7ff fcac 	bl	dd9d0 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   de078:	e072      	b.n	de160 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
   de07a:	4621      	mov	r1, r4
   de07c:	a80f      	add	r0, sp, #60	; 0x3c
   de07e:	f7f8 fc92 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de082:	b104      	cbz	r4, de086 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x86>
   de084:	6864      	ldr	r4, [r4, #4]
   de086:	4629      	mov	r1, r5
   de088:	a80a      	add	r0, sp, #40	; 0x28
   de08a:	f7f8 fc8c 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de08e:	b105      	cbz	r5, de092 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x92>
   de090:	686d      	ldr	r5, [r5, #4]
   de092:	af05      	add	r7, sp, #20
   de094:	4631      	mov	r1, r6
   de096:	4638      	mov	r0, r7
   de098:	f7f8 fc85 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de09c:	4b3e      	ldr	r3, [pc, #248]	; (de198 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x198>)
   de09e:	9302      	str	r3, [sp, #8]
   de0a0:	6873      	ldr	r3, [r6, #4]
   de0a2:	9301      	str	r3, [sp, #4]
   de0a4:	9700      	str	r7, [sp, #0]
   de0a6:	462b      	mov	r3, r5
   de0a8:	aa0a      	add	r2, sp, #40	; 0x28
   de0aa:	4621      	mov	r1, r4
   de0ac:	a80f      	add	r0, sp, #60	; 0x3c
   de0ae:	f7ff fd06 	bl	ddabe <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   de0b2:	e055      	b.n	de160 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
   de0b4:	4621      	mov	r1, r4
   de0b6:	a80f      	add	r0, sp, #60	; 0x3c
   de0b8:	f7f8 fc75 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de0bc:	b104      	cbz	r4, de0c0 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xc0>
   de0be:	6864      	ldr	r4, [r4, #4]
   de0c0:	4629      	mov	r1, r5
   de0c2:	a80a      	add	r0, sp, #40	; 0x28
   de0c4:	f7f8 fc6f 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de0c8:	b105      	cbz	r5, de0cc <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xcc>
   de0ca:	686d      	ldr	r5, [r5, #4]
   de0cc:	af05      	add	r7, sp, #20
   de0ce:	4631      	mov	r1, r6
   de0d0:	4638      	mov	r0, r7
   de0d2:	f7f8 fc68 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de0d6:	4b31      	ldr	r3, [pc, #196]	; (de19c <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x19c>)
   de0d8:	9302      	str	r3, [sp, #8]
   de0da:	6873      	ldr	r3, [r6, #4]
   de0dc:	9301      	str	r3, [sp, #4]
   de0de:	9700      	str	r7, [sp, #0]
   de0e0:	462b      	mov	r3, r5
   de0e2:	aa0a      	add	r2, sp, #40	; 0x28
   de0e4:	4621      	mov	r1, r4
   de0e6:	a80f      	add	r0, sp, #60	; 0x3c
   de0e8:	f7ff fd5a 	bl	ddba0 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   de0ec:	e038      	b.n	de160 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
   de0ee:	4621      	mov	r1, r4
   de0f0:	a80f      	add	r0, sp, #60	; 0x3c
   de0f2:	f7f8 fc58 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de0f6:	b104      	cbz	r4, de0fa <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xfa>
   de0f8:	6864      	ldr	r4, [r4, #4]
   de0fa:	4629      	mov	r1, r5
   de0fc:	a80a      	add	r0, sp, #40	; 0x28
   de0fe:	f7f8 fc52 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de102:	b105      	cbz	r5, de106 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x106>
   de104:	686d      	ldr	r5, [r5, #4]
   de106:	af05      	add	r7, sp, #20
   de108:	4631      	mov	r1, r6
   de10a:	4638      	mov	r0, r7
   de10c:	f7f8 fc4b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de110:	4b23      	ldr	r3, [pc, #140]	; (de1a0 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x1a0>)
   de112:	9302      	str	r3, [sp, #8]
   de114:	6873      	ldr	r3, [r6, #4]
   de116:	9301      	str	r3, [sp, #4]
   de118:	9700      	str	r7, [sp, #0]
   de11a:	462b      	mov	r3, r5
   de11c:	aa0a      	add	r2, sp, #40	; 0x28
   de11e:	4621      	mov	r1, r4
   de120:	a80f      	add	r0, sp, #60	; 0x3c
   de122:	f7ff fdae 	bl	ddc82 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   de126:	e01b      	b.n	de160 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
   de128:	4621      	mov	r1, r4
   de12a:	a80f      	add	r0, sp, #60	; 0x3c
   de12c:	f7f8 fc3b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de130:	b104      	cbz	r4, de134 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x134>
   de132:	6864      	ldr	r4, [r4, #4]
   de134:	4629      	mov	r1, r5
   de136:	a80a      	add	r0, sp, #40	; 0x28
   de138:	f7f8 fc35 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de13c:	b105      	cbz	r5, de140 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x140>
   de13e:	686d      	ldr	r5, [r5, #4]
   de140:	af05      	add	r7, sp, #20
   de142:	4631      	mov	r1, r6
   de144:	4638      	mov	r0, r7
   de146:	f7f8 fc2e 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de14a:	4b16      	ldr	r3, [pc, #88]	; (de1a4 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x1a4>)
   de14c:	9302      	str	r3, [sp, #8]
   de14e:	6873      	ldr	r3, [r6, #4]
   de150:	9301      	str	r3, [sp, #4]
   de152:	9700      	str	r7, [sp, #0]
   de154:	462b      	mov	r3, r5
   de156:	aa0a      	add	r2, sp, #40	; 0x28
   de158:	4621      	mov	r1, r4
   de15a:	a80f      	add	r0, sp, #60	; 0x3c
   de15c:	f7ff fe02 	bl	ddd64 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   de160:	4638      	mov	r0, r7
   de162:	f7f8 f970 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   de166:	a80a      	add	r0, sp, #40	; 0x28
   de168:	f7f8 f96d 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   de16c:	a80f      	add	r0, sp, #60	; 0x3c
   de16e:	f7f8 f96a 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  } else {
    context->ReportError(context,
                         "Kernel type not supported by Maximum/Minimum.");
    return kTfLiteError;
  }
  return kTfLiteOk;
   de172:	2000      	movs	r0, #0
   de174:	e00a      	b.n	de18c <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x18c>
        break;
      case kTfLiteInt64:
        TFLiteOperation<int64_t, OpType>(context, node, op_context);
        break;
      default:
        context->ReportError(
   de176:	f8d9 4014 	ldr.w	r4, [r9, #20]
   de17a:	f7f5 ffcd 	bl	d4118 <TfLiteTypeGetName>
   de17e:	f818 3007 	ldrb.w	r3, [r8, r7]
   de182:	4909      	ldr	r1, [pc, #36]	; (de1a8 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x1a8>)
   de184:	4602      	mov	r2, r0
   de186:	4648      	mov	r0, r9
   de188:	47a0      	blx	r4
            context, "Type %s (%d) is not supported by Maximum/Minimum.",
            TfLiteTypeGetName(op_context.output->type),
            op_context.output->type);
        return kTfLiteError;
   de18a:	2001      	movs	r0, #1
    context->ReportError(context,
                         "Kernel type not supported by Maximum/Minimum.");
    return kTfLiteError;
  }
  return kTfLiteOk;
}
   de18c:	b015      	add	sp, #84	; 0x54
   de18e:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}
   de192:	bf00      	nop
   de194:	000dd94d 	.word	0x000dd94d
   de198:	000dd95d 	.word	0x000dd95d
   de19c:	000dd965 	.word	0x000dd965
   de1a0:	000dd96d 	.word	0x000dd96d
   de1a4:	000dd975 	.word	0x000dd975
   de1a8:	000e9f0f 	.word	0x000e9f0f

000de1ac <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode>:
namespace neg {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   de1ac:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   de1b0:	680b      	ldr	r3, [r1, #0]
   de1b2:	6884      	ldr	r4, [r0, #8]
   de1b4:	685b      	ldr	r3, [r3, #4]
   de1b6:	2738      	movs	r7, #56	; 0x38
   de1b8:	437b      	muls	r3, r7
   de1ba:	b08a      	sub	sp, #40	; 0x28
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  switch (input->type) {
   de1bc:	5ce2      	ldrb	r2, [r4, r3]
   de1be:	2a01      	cmp	r2, #1
   de1c0:	eb04 0603 	add.w	r6, r4, r3
   de1c4:	d145      	bne.n	de252 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0xa6>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   de1c6:	684b      	ldr	r3, [r1, #4]
   de1c8:	685b      	ldr	r3, [r3, #4]
    // TODO(wangtz): handle for kTfLiteInt8
    case kTfLiteFloat32:
      reference_ops::Negate(GetTensorShape(input), GetTensorData<float>(input),
   de1ca:	4631      	mov	r1, r6
   de1cc:	fb07 4403 	mla	r4, r7, r3, r4
   de1d0:	4668      	mov	r0, sp
   de1d2:	f7f8 fbe8 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                            GetTensorShape(output),
   de1d6:	4621      	mov	r1, r4
   de1d8:	a805      	add	r0, sp, #20
   de1da:	6876      	ldr	r6, [r6, #4]
   de1dc:	f7f8 fbe3 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   de1e0:	b104      	cbz	r4, de1e4 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x38>
   de1e2:	6864      	ldr	r4, [r4, #4]
   de1e4:	9f00      	ldr	r7, [sp, #0]

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   de1e6:	9b05      	ldr	r3, [sp, #20]
   de1e8:	429f      	cmp	r7, r3
   de1ea:	d101      	bne.n	de1f0 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x44>
   de1ec:	2500      	movs	r5, #0
   de1ee:	e00d      	b.n	de20c <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x60>
   de1f0:	f006 f89c 	bl	e432c <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   de1f4:	4629      	mov	r1, r5
   de1f6:	4668      	mov	r0, sp
   de1f8:	f7f8 f930 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   de1fc:	4629      	mov	r1, r5
   de1fe:	4680      	mov	r8, r0
   de200:	a805      	add	r0, sp, #20
   de202:	f7f8 f92b 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   de206:	4580      	cmp	r8, r0
   de208:	d1f2      	bne.n	de1f0 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x44>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   de20a:	3501      	adds	r5, #1
   de20c:	42af      	cmp	r7, r5
   de20e:	dcf1      	bgt.n	de1f4 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x48>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   de210:	2f04      	cmp	r7, #4
   de212:	bfcc      	ite	gt
   de214:	9a01      	ldrgt	r2, [sp, #4]
   de216:	aa01      	addle	r2, sp, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   de218:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
   de21a:	2101      	movs	r1, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   de21c:	429f      	cmp	r7, r3
   de21e:	dd04      	ble.n	de22a <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x7e>
      buffer_size *= dims_data[i];
   de220:	f852 0023 	ldr.w	r0, [r2, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   de224:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
   de226:	4341      	muls	r1, r0
   de228:	e7f8      	b.n	de21c <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x70>
   de22a:	4633      	mov	r3, r6
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   de22c:	2200      	movs	r2, #0
template <typename T>
inline void Negate(const RuntimeShape& input_shape, const T* input_data,
                   const RuntimeShape& output_shape, T* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; ++i) {
   de22e:	428a      	cmp	r2, r1
   de230:	da07      	bge.n	de242 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x96>
    output_data[i] = -input_data[i];
   de232:	ecf3 7a01 	vldmia	r3!, {s15}
   de236:	eef1 7a67 	vneg.f32	s15, s15
   de23a:	ece4 7a01 	vstmia	r4!, {s15}
template <typename T>
inline void Negate(const RuntimeShape& input_shape, const T* input_data,
                   const RuntimeShape& output_shape, T* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; ++i) {
   de23e:	3201      	adds	r2, #1
   de240:	e7f5      	b.n	de22e <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x82>
   de242:	a805      	add	r0, sp, #20
   de244:	f7f8 f8ff 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  switch (input->type) {
    // TODO(wangtz): handle for kTfLiteInt8
    case kTfLiteFloat32:
      reference_ops::Negate(GetTensorShape(input), GetTensorData<float>(input),
   de248:	4668      	mov	r0, sp
   de24a:	f7f8 f8fc 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
    default:
      context->ReportError(
          context, "Neg only currently supports float32, got %d.", input->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
   de24e:	2000      	movs	r0, #0
   de250:	e003      	b.n	de25a <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0xae>
                            GetTensorShape(output),
                            GetTensorData<float>(output));
      break;
    default:
      context->ReportError(
          context, "Neg only currently supports float32, got %d.", input->type);
   de252:	6943      	ldr	r3, [r0, #20]
   de254:	4902      	ldr	r1, [pc, #8]	; (de260 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0xb4>)
   de256:	4798      	blx	r3
      return kTfLiteError;
   de258:	2001      	movs	r0, #1
  }
  return kTfLiteOk;
}
   de25a:	b00a      	add	sp, #40	; 0x28
   de25c:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   de260:	000e9f41 	.word	0x000e9f41

000de264 <_ZN6tflite3ops5micro12Register_NEGEv>:

TfLiteRegistration* Register_NEG() {
  static TfLiteRegistration r = {/*init=*/nullptr, /*free=*/nullptr,
                                 /*prepare=*/nullptr, neg::Eval};
  return &r;
}
   de264:	4800      	ldr	r0, [pc, #0]	; (de268 <_ZN6tflite3ops5micro12Register_NEGEv+0x4>)
   de266:	4770      	bx	lr
   de268:	2003c0a8 	.word	0x2003c0a8

000de26c <_ZN6tflite3ops5micro4pack12_GLOBAL__N_17PrepareEP13TfLiteContextP10TfLiteNode>:

constexpr int kOutputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   de26c:	2000      	movs	r0, #0
   de26e:	4770      	bx	lr

000de270 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode>:
  }

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   de270:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   de274:	684b      	ldr	r3, [r1, #4]
   de276:	6885      	ldr	r5, [r0, #8]
  const TfLitePackParams* data =
      reinterpret_cast<TfLitePackParams*>(node->builtin_data);
   de278:	694a      	ldr	r2, [r1, #20]
  }

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   de27a:	b085      	sub	sp, #20
   de27c:	9000      	str	r0, [sp, #0]
   de27e:	6858      	ldr	r0, [r3, #4]
   de280:	2338      	movs	r3, #56	; 0x38
   de282:	4358      	muls	r0, r3
   de284:	182b      	adds	r3, r5, r0
  const TfLitePackParams* data =
      reinterpret_cast<TfLitePackParams*>(node->builtin_data);

  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (output->type) {
   de286:	5c28      	ldrb	r0, [r5, r0]
   de288:	1e46      	subs	r6, r0, #1
   de28a:	2e08      	cmp	r6, #8
   de28c:	f200 821a 	bhi.w	de6c4 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x454>
   de290:	e8df f016 	tbh	[pc, r6, lsl #1]
   de294:	013f0009 	.word	0x013f0009
   de298:	01aa0077 	.word	0x01aa0077
   de29c:	02180218 	.word	0x02180218
   de2a0:	02180218 	.word	0x02180218
   de2a4:	00da      	.short	0x00da

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
   de2a6:	6808      	ldr	r0, [r1, #0]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de2a8:	689f      	ldr	r7, [r3, #8]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de2aa:	6840      	ldr	r0, [r0, #4]

  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (output->type) {
    case kTfLiteFloat32: {
      return PackImpl<float>(context, node, output, data->values_count,
   de2ac:	f8d2 8000 	ldr.w	r8, [r2]
                             data->axis);
   de2b0:	6852      	ldr	r2, [r2, #4]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de2b2:	f8d7 e000 	ldr.w	lr, [r7]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de2b6:	2638      	movs	r6, #56	; 0x38
   de2b8:	fb06 5500 	mla	r5, r6, r0, r5
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
   de2bc:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de2be:	68ad      	ldr	r5, [r5, #8]
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
    axis += dimensions;
   de2c0:	bfb8      	it	lt
   de2c2:	4472      	addlt	r2, lr
   de2c4:	46bc      	mov	ip, r7
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de2c6:	2000      	movs	r0, #0

  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
   de2c8:	2601      	movs	r6, #1
  for (int i = 0; i < axis; ++i) {
   de2ca:	4282      	cmp	r2, r0
   de2cc:	dd05      	ble.n	de2da <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x6a>
    outer_size *= output_dims->data[i];
   de2ce:	f85c 9f04 	ldr.w	r9, [ip, #4]!
  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de2d2:	3001      	adds	r0, #1
    outer_size *= output_dims->data[i];
   de2d4:	fb09 f606 	mul.w	r6, r9, r6
   de2d8:	e7f7      	b.n	de2ca <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
   de2da:	1c50      	adds	r0, r2, #1
  }
  int copy_size = 1;
   de2dc:	2201      	movs	r2, #1
  for (int i = axis + 1; i < dimensions; ++i) {
   de2de:	4586      	cmp	lr, r0
   de2e0:	f100 0c01 	add.w	ip, r0, #1
   de2e4:	dd04      	ble.n	de2f0 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x80>
    copy_size *= output_dims->data[i];
   de2e6:	f857 002c 	ldr.w	r0, [r7, ip, lsl #2]
   de2ea:	4342      	muls	r2, r0
   de2ec:	4660      	mov	r0, ip
   de2ee:	e7f6      	b.n	de2de <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x6e>
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de2f0:	f8d5 c000 	ldr.w	ip, [r5]
   de2f4:	4628      	mov	r0, r5
   de2f6:	2700      	movs	r7, #0
   de2f8:	2501      	movs	r5, #1
   de2fa:	45bc      	cmp	ip, r7
   de2fc:	dd05      	ble.n	de30a <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x9a>
    input_size *= input_dims->data[i];
   de2fe:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= output_dims->data[i];
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de302:	3701      	adds	r7, #1
    input_size *= input_dims->data[i];
   de304:	fb0e f505 	mul.w	r5, lr, r5
   de308:	e7f7      	b.n	de2fa <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x8a>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);
   de30a:	fb02 f006 	mul.w	r0, r2, r6
   de30e:	4285      	cmp	r5, r0
   de310:	d001      	beq.n	de316 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xa6>
   de312:	f006 f80b 	bl	e432c <abort>
   de316:	685b      	ldr	r3, [r3, #4]
   de318:	9301      	str	r3, [sp, #4]
   de31a:	fb02 f308 	mul.w	r3, r2, r8
   de31e:	009b      	lsls	r3, r3, #2
   de320:	2000      	movs	r0, #0
   de322:	ea4f 0982 	mov.w	r9, r2, lsl #2
   de326:	9302      	str	r3, [sp, #8]

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de328:	4605      	mov	r5, r0
   de32a:	45a8      	cmp	r8, r5
   de32c:	dc01      	bgt.n	de332 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc2>
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (output->type) {
    case kTfLiteFloat32: {
      return PackImpl<float>(context, node, output, data->values_count,
                             data->axis);
   de32e:	2000      	movs	r0, #0
   de330:	e1d1      	b.n	de6d6 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x466>
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
   de332:	680b      	ldr	r3, [r1, #0]
   de334:	eb03 0385 	add.w	r3, r3, r5, lsl #2
   de338:	2438      	movs	r4, #56	; 0x38
   de33a:	685f      	ldr	r7, [r3, #4]
   de33c:	9b00      	ldr	r3, [sp, #0]
   de33e:	689b      	ldr	r3, [r3, #8]
   de340:	fb04 3307 	mla	r3, r4, r7, r3
   de344:	b103      	cbz	r3, de348 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xd8>
   de346:	685b      	ldr	r3, [r3, #4]
   de348:	2700      	movs	r7, #0
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de34a:	46bc      	mov	ip, r7
   de34c:	4566      	cmp	r6, ip
   de34e:	dd15      	ble.n	de37c <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x10c>
   de350:	9c01      	ldr	r4, [sp, #4]
   de352:	eb00 0e07 	add.w	lr, r0, r7
   de356:	44a6      	add	lr, r4
   de358:	469b      	mov	fp, r3
      const T* input_ptr = input_data + copy_size * k;
      int loc = k * values_count * copy_size + i * copy_size;
      T* output_ptr = output_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   de35a:	f04f 0a00 	mov.w	sl, #0
   de35e:	4552      	cmp	r2, sl
   de360:	dd06      	ble.n	de370 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x100>
   de362:	ecfb 7a01 	vldmia	fp!, {s15}
   de366:	f10a 0a01 	add.w	sl, sl, #1
   de36a:	ecee 7a01 	vstmia	lr!, {s15}
   de36e:	e7f6      	b.n	de35e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xee>
   de370:	9c02      	ldr	r4, [sp, #8]
  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de372:	f10c 0c01 	add.w	ip, ip, #1
   de376:	444b      	add	r3, r9
   de378:	4427      	add	r7, r4
   de37a:	e7e7      	b.n	de34c <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xdc>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de37c:	3501      	adds	r5, #1
   de37e:	4448      	add	r0, r9
   de380:	e7d3      	b.n	de32a <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xba>

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
   de382:	6808      	ldr	r0, [r1, #0]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de384:	689f      	ldr	r7, [r3, #8]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de386:	6840      	ldr	r0, [r0, #4]
    case kTfLiteFloat32: {
      return PackImpl<float>(context, node, output, data->values_count,
                             data->axis);
    }
    case kTfLiteUInt8: {
      return PackImpl<uint8_t>(context, node, output, data->values_count,
   de388:	f8d2 9000 	ldr.w	r9, [r2]
                               data->axis);
   de38c:	6852      	ldr	r2, [r2, #4]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de38e:	f8d7 e000 	ldr.w	lr, [r7]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de392:	2638      	movs	r6, #56	; 0x38
   de394:	fb06 5500 	mla	r5, r6, r0, r5
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
   de398:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de39a:	68ae      	ldr	r6, [r5, #8]
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
    axis += dimensions;
   de39c:	bfb8      	it	lt
   de39e:	4472      	addlt	r2, lr
   de3a0:	46bc      	mov	ip, r7
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de3a2:	2000      	movs	r0, #0

  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
   de3a4:	2501      	movs	r5, #1
  for (int i = 0; i < axis; ++i) {
   de3a6:	4282      	cmp	r2, r0
   de3a8:	dd05      	ble.n	de3b6 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x146>
    outer_size *= output_dims->data[i];
   de3aa:	f85c 8f04 	ldr.w	r8, [ip, #4]!
  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de3ae:	3001      	adds	r0, #1
    outer_size *= output_dims->data[i];
   de3b0:	fb08 f505 	mul.w	r5, r8, r5
   de3b4:	e7f7      	b.n	de3a6 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x136>
   de3b6:	1c50      	adds	r0, r2, #1
  }
  int copy_size = 1;
   de3b8:	2201      	movs	r2, #1
  for (int i = axis + 1; i < dimensions; ++i) {
   de3ba:	4586      	cmp	lr, r0
   de3bc:	f100 0c01 	add.w	ip, r0, #1
   de3c0:	dd04      	ble.n	de3cc <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x15c>
    copy_size *= output_dims->data[i];
   de3c2:	f857 002c 	ldr.w	r0, [r7, ip, lsl #2]
   de3c6:	4342      	muls	r2, r0
   de3c8:	4660      	mov	r0, ip
   de3ca:	e7f6      	b.n	de3ba <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x14a>
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de3cc:	f8d6 c000 	ldr.w	ip, [r6]
   de3d0:	4630      	mov	r0, r6
   de3d2:	2700      	movs	r7, #0
   de3d4:	2601      	movs	r6, #1
   de3d6:	45bc      	cmp	ip, r7
   de3d8:	dd05      	ble.n	de3e6 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x176>
    input_size *= input_dims->data[i];
   de3da:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= output_dims->data[i];
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de3de:	3701      	adds	r7, #1
    input_size *= input_dims->data[i];
   de3e0:	fb0e f606 	mul.w	r6, lr, r6
   de3e4:	e7f7      	b.n	de3d6 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x166>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);
   de3e6:	fb02 f005 	mul.w	r0, r2, r5
   de3ea:	4286      	cmp	r6, r0
   de3ec:	d191      	bne.n	de312 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xa2>
   de3ee:	685e      	ldr	r6, [r3, #4]
   de3f0:	fb02 f309 	mul.w	r3, r2, r9
   de3f4:	9301      	str	r3, [sp, #4]

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de3f6:	2000      	movs	r0, #0
   de3f8:	4581      	cmp	r9, r0
   de3fa:	dd98      	ble.n	de32e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xbe>
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
   de3fc:	680b      	ldr	r3, [r1, #0]
   de3fe:	eb03 0380 	add.w	r3, r3, r0, lsl #2
   de402:	2438      	movs	r4, #56	; 0x38
   de404:	685f      	ldr	r7, [r3, #4]
   de406:	9b00      	ldr	r3, [sp, #0]
   de408:	689b      	ldr	r3, [r3, #8]
   de40a:	fb04 3307 	mla	r3, r4, r7, r3
   de40e:	b103      	cbz	r3, de412 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1a2>
   de410:	685b      	ldr	r3, [r3, #4]
   de412:	425f      	negs	r7, r3
   de414:	46b6      	mov	lr, r6
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de416:	f04f 0c00 	mov.w	ip, #0
   de41a:	4565      	cmp	r5, ip
   de41c:	dd11      	ble.n	de442 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1d2>
   de41e:	46f2      	mov	sl, lr
   de420:	4698      	mov	r8, r3
      const T* input_ptr = input_data + copy_size * k;
      int loc = k * values_count * copy_size + i * copy_size;
      T* output_ptr = output_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   de422:	eb07 0b08 	add.w	fp, r7, r8
   de426:	455a      	cmp	r2, fp
   de428:	dd04      	ble.n	de434 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1c4>
   de42a:	f818 bb01 	ldrb.w	fp, [r8], #1
   de42e:	f80a bb01 	strb.w	fp, [sl], #1
   de432:	e7f6      	b.n	de422 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1b2>
   de434:	9c01      	ldr	r4, [sp, #4]
  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de436:	f10c 0c01 	add.w	ip, ip, #1
   de43a:	4413      	add	r3, r2
   de43c:	44a6      	add	lr, r4
   de43e:	1abf      	subs	r7, r7, r2
   de440:	e7eb      	b.n	de41a <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1aa>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de442:	3001      	adds	r0, #1
   de444:	4416      	add	r6, r2
   de446:	e7d7      	b.n	de3f8 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x188>

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
   de448:	6808      	ldr	r0, [r1, #0]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de44a:	689f      	ldr	r7, [r3, #8]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de44c:	6840      	ldr	r0, [r0, #4]
    case kTfLiteUInt8: {
      return PackImpl<uint8_t>(context, node, output, data->values_count,
                               data->axis);
    }
    case kTfLiteInt8: {
      return PackImpl<int8_t>(context, node, output, data->values_count,
   de44e:	f8d2 9000 	ldr.w	r9, [r2]
                              data->axis);
   de452:	6852      	ldr	r2, [r2, #4]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de454:	f8d7 e000 	ldr.w	lr, [r7]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de458:	2638      	movs	r6, #56	; 0x38
   de45a:	fb06 5500 	mla	r5, r6, r0, r5
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
   de45e:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de460:	68ae      	ldr	r6, [r5, #8]
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
    axis += dimensions;
   de462:	bfb8      	it	lt
   de464:	4472      	addlt	r2, lr
   de466:	46bc      	mov	ip, r7
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de468:	2000      	movs	r0, #0

  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
   de46a:	2501      	movs	r5, #1
  for (int i = 0; i < axis; ++i) {
   de46c:	4282      	cmp	r2, r0
   de46e:	dd05      	ble.n	de47c <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x20c>
    outer_size *= output_dims->data[i];
   de470:	f85c 8f04 	ldr.w	r8, [ip, #4]!
  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de474:	3001      	adds	r0, #1
    outer_size *= output_dims->data[i];
   de476:	fb08 f505 	mul.w	r5, r8, r5
   de47a:	e7f7      	b.n	de46c <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1fc>
   de47c:	1c50      	adds	r0, r2, #1
  }
  int copy_size = 1;
   de47e:	2201      	movs	r2, #1
  for (int i = axis + 1; i < dimensions; ++i) {
   de480:	4586      	cmp	lr, r0
   de482:	f100 0c01 	add.w	ip, r0, #1
   de486:	dd04      	ble.n	de492 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x222>
    copy_size *= output_dims->data[i];
   de488:	f857 002c 	ldr.w	r0, [r7, ip, lsl #2]
   de48c:	4342      	muls	r2, r0
   de48e:	4660      	mov	r0, ip
   de490:	e7f6      	b.n	de480 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x210>
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de492:	f8d6 c000 	ldr.w	ip, [r6]
   de496:	4630      	mov	r0, r6
   de498:	2700      	movs	r7, #0
   de49a:	2601      	movs	r6, #1
   de49c:	45bc      	cmp	ip, r7
   de49e:	dd05      	ble.n	de4ac <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x23c>
    input_size *= input_dims->data[i];
   de4a0:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= output_dims->data[i];
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de4a4:	3701      	adds	r7, #1
    input_size *= input_dims->data[i];
   de4a6:	fb0e f606 	mul.w	r6, lr, r6
   de4aa:	e7f7      	b.n	de49c <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x22c>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);
   de4ac:	fb02 f005 	mul.w	r0, r2, r5
   de4b0:	4286      	cmp	r6, r0
   de4b2:	f47f af2e 	bne.w	de312 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xa2>
   de4b6:	685e      	ldr	r6, [r3, #4]
   de4b8:	fb02 f309 	mul.w	r3, r2, r9
   de4bc:	9301      	str	r3, [sp, #4]

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de4be:	2000      	movs	r0, #0
   de4c0:	4581      	cmp	r9, r0
   de4c2:	f77f af34 	ble.w	de32e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xbe>
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
   de4c6:	680b      	ldr	r3, [r1, #0]
   de4c8:	eb03 0380 	add.w	r3, r3, r0, lsl #2
   de4cc:	2438      	movs	r4, #56	; 0x38
   de4ce:	685f      	ldr	r7, [r3, #4]
   de4d0:	9b00      	ldr	r3, [sp, #0]
   de4d2:	689b      	ldr	r3, [r3, #8]
   de4d4:	fb04 3307 	mla	r3, r4, r7, r3
   de4d8:	b103      	cbz	r3, de4dc <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x26c>
   de4da:	685b      	ldr	r3, [r3, #4]
   de4dc:	425f      	negs	r7, r3
   de4de:	46b6      	mov	lr, r6
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de4e0:	f04f 0c00 	mov.w	ip, #0
   de4e4:	4565      	cmp	r5, ip
   de4e6:	dd11      	ble.n	de50c <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x29c>
   de4e8:	46f2      	mov	sl, lr
   de4ea:	4698      	mov	r8, r3
      const T* input_ptr = input_data + copy_size * k;
      int loc = k * values_count * copy_size + i * copy_size;
      T* output_ptr = output_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   de4ec:	eb08 0b07 	add.w	fp, r8, r7
   de4f0:	455a      	cmp	r2, fp
   de4f2:	dd04      	ble.n	de4fe <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x28e>
   de4f4:	f918 bb01 	ldrsb.w	fp, [r8], #1
   de4f8:	f80a bb01 	strb.w	fp, [sl], #1
   de4fc:	e7f6      	b.n	de4ec <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x27c>
   de4fe:	9c01      	ldr	r4, [sp, #4]
  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de500:	f10c 0c01 	add.w	ip, ip, #1
   de504:	4413      	add	r3, r2
   de506:	44a6      	add	lr, r4
   de508:	1abf      	subs	r7, r7, r2
   de50a:	e7eb      	b.n	de4e4 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x274>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de50c:	3001      	adds	r0, #1
   de50e:	4416      	add	r6, r2
   de510:	e7d6      	b.n	de4c0 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x250>

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
   de512:	6808      	ldr	r0, [r1, #0]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de514:	689e      	ldr	r6, [r3, #8]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de516:	6840      	ldr	r0, [r0, #4]
    case kTfLiteInt8: {
      return PackImpl<int8_t>(context, node, output, data->values_count,
                              data->axis);
    }
    case kTfLiteInt32: {
      return PackImpl<int32_t>(context, node, output, data->values_count,
   de518:	f8d2 8000 	ldr.w	r8, [r2]
                               data->axis);
   de51c:	6852      	ldr	r2, [r2, #4]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de51e:	f8d6 e000 	ldr.w	lr, [r6]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de522:	2738      	movs	r7, #56	; 0x38
   de524:	fb07 5500 	mla	r5, r7, r0, r5
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
   de528:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de52a:	68ad      	ldr	r5, [r5, #8]
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
    axis += dimensions;
   de52c:	bfb8      	it	lt
   de52e:	4472      	addlt	r2, lr
   de530:	46b4      	mov	ip, r6
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de532:	2000      	movs	r0, #0

  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
   de534:	2701      	movs	r7, #1
  for (int i = 0; i < axis; ++i) {
   de536:	4282      	cmp	r2, r0
   de538:	dd05      	ble.n	de546 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2d6>
    outer_size *= output_dims->data[i];
   de53a:	f85c 9f04 	ldr.w	r9, [ip, #4]!
  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de53e:	3001      	adds	r0, #1
    outer_size *= output_dims->data[i];
   de540:	fb09 f707 	mul.w	r7, r9, r7
   de544:	e7f7      	b.n	de536 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2c6>
   de546:	1c50      	adds	r0, r2, #1
  }
  int copy_size = 1;
   de548:	2201      	movs	r2, #1
  for (int i = axis + 1; i < dimensions; ++i) {
   de54a:	4586      	cmp	lr, r0
   de54c:	f100 0c01 	add.w	ip, r0, #1
   de550:	dd04      	ble.n	de55c <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2ec>
    copy_size *= output_dims->data[i];
   de552:	f856 002c 	ldr.w	r0, [r6, ip, lsl #2]
   de556:	4342      	muls	r2, r0
   de558:	4660      	mov	r0, ip
   de55a:	e7f6      	b.n	de54a <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2da>
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de55c:	f8d5 c000 	ldr.w	ip, [r5]
   de560:	4628      	mov	r0, r5
   de562:	2600      	movs	r6, #0
   de564:	2501      	movs	r5, #1
   de566:	45b4      	cmp	ip, r6
   de568:	dd05      	ble.n	de576 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x306>
    input_size *= input_dims->data[i];
   de56a:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= output_dims->data[i];
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de56e:	3601      	adds	r6, #1
    input_size *= input_dims->data[i];
   de570:	fb0e f505 	mul.w	r5, lr, r5
   de574:	e7f7      	b.n	de566 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2f6>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);
   de576:	fb02 f007 	mul.w	r0, r2, r7
   de57a:	4285      	cmp	r5, r0
   de57c:	f47f aec9 	bne.w	de312 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xa2>
   de580:	685b      	ldr	r3, [r3, #4]
   de582:	9301      	str	r3, [sp, #4]
   de584:	fb02 f308 	mul.w	r3, r2, r8
   de588:	009b      	lsls	r3, r3, #2
   de58a:	2500      	movs	r5, #0
   de58c:	ea4f 0c82 	mov.w	ip, r2, lsl #2
   de590:	9302      	str	r3, [sp, #8]

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de592:	462e      	mov	r6, r5
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
   de594:	f04f 0938 	mov.w	r9, #56	; 0x38
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de598:	45b0      	cmp	r8, r6
   de59a:	f77f aec8 	ble.w	de32e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xbe>
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
   de59e:	680b      	ldr	r3, [r1, #0]
   de5a0:	eb03 0386 	add.w	r3, r3, r6, lsl #2
   de5a4:	6858      	ldr	r0, [r3, #4]
   de5a6:	9b00      	ldr	r3, [sp, #0]
   de5a8:	689b      	ldr	r3, [r3, #8]
   de5aa:	fb09 3300 	mla	r3, r9, r0, r3
   de5ae:	b103      	cbz	r3, de5b2 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x342>
   de5b0:	685b      	ldr	r3, [r3, #4]
   de5b2:	f04f 0e00 	mov.w	lr, #0
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de5b6:	46f2      	mov	sl, lr
   de5b8:	4557      	cmp	r7, sl
   de5ba:	dd12      	ble.n	de5e2 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x372>
      const T* input_ptr = input_data + copy_size * k;
      int loc = k * values_count * copy_size + i * copy_size;
      T* output_ptr = output_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   de5bc:	9c01      	ldr	r4, [sp, #4]
   de5be:	eb05 0b0e 	add.w	fp, r5, lr
  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de5c2:	2000      	movs	r0, #0
      const T* input_ptr = input_data + copy_size * k;
      int loc = k * values_count * copy_size + i * copy_size;
      T* output_ptr = output_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   de5c4:	44a3      	add	fp, r4
   de5c6:	4282      	cmp	r2, r0
   de5c8:	dd05      	ble.n	de5d6 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x366>
   de5ca:	f853 4020 	ldr.w	r4, [r3, r0, lsl #2]
   de5ce:	f84b 4020 	str.w	r4, [fp, r0, lsl #2]
   de5d2:	3001      	adds	r0, #1
   de5d4:	e7f7      	b.n	de5c6 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x356>
   de5d6:	9802      	ldr	r0, [sp, #8]
  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de5d8:	f10a 0a01 	add.w	sl, sl, #1
   de5dc:	4463      	add	r3, ip
   de5de:	4486      	add	lr, r0
   de5e0:	e7ea      	b.n	de5b8 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x348>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de5e2:	3601      	adds	r6, #1
   de5e4:	4465      	add	r5, ip
   de5e6:	e7d7      	b.n	de598 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x328>
    case kTfLiteInt32: {
      return PackImpl<int32_t>(context, node, output, data->values_count,
                               data->axis);
    }
    case kTfLiteInt64: {
      return PackImpl<int64_t>(context, node, output, data->values_count,
   de5e8:	6810      	ldr	r0, [r2, #0]
   de5ea:	9001      	str	r0, [sp, #4]

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
   de5ec:	6808      	ldr	r0, [r1, #0]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de5ee:	689e      	ldr	r6, [r3, #8]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de5f0:	6840      	ldr	r0, [r0, #4]
      return PackImpl<int32_t>(context, node, output, data->values_count,
                               data->axis);
    }
    case kTfLiteInt64: {
      return PackImpl<int64_t>(context, node, output, data->values_count,
                               data->axis);
   de5f2:	6852      	ldr	r2, [r2, #4]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de5f4:	6837      	ldr	r7, [r6, #0]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de5f6:	f04f 0e38 	mov.w	lr, #56	; 0x38
   de5fa:	fb0e 5500 	mla	r5, lr, r0, r5
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
   de5fe:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de600:	68ad      	ldr	r5, [r5, #8]
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
    axis += dimensions;
   de602:	bfb8      	it	lt
   de604:	19d2      	addlt	r2, r2, r7
   de606:	46b4      	mov	ip, r6
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de608:	2000      	movs	r0, #0

  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
   de60a:	f04f 0e01 	mov.w	lr, #1
  for (int i = 0; i < axis; ++i) {
   de60e:	4282      	cmp	r2, r0
   de610:	dd05      	ble.n	de61e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3ae>
    outer_size *= output_dims->data[i];
   de612:	f85c 8f04 	ldr.w	r8, [ip, #4]!
  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de616:	3001      	adds	r0, #1
    outer_size *= output_dims->data[i];
   de618:	fb08 fe0e 	mul.w	lr, r8, lr
   de61c:	e7f7      	b.n	de60e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x39e>
   de61e:	1c50      	adds	r0, r2, #1
  }
  int copy_size = 1;
   de620:	2201      	movs	r2, #1
  for (int i = axis + 1; i < dimensions; ++i) {
   de622:	4287      	cmp	r7, r0
   de624:	f100 0c01 	add.w	ip, r0, #1
   de628:	dd04      	ble.n	de634 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3c4>
    copy_size *= output_dims->data[i];
   de62a:	f856 002c 	ldr.w	r0, [r6, ip, lsl #2]
   de62e:	4342      	muls	r2, r0
   de630:	4660      	mov	r0, ip
   de632:	e7f6      	b.n	de622 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3b2>
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de634:	f8d5 c000 	ldr.w	ip, [r5]
   de638:	4628      	mov	r0, r5
   de63a:	2600      	movs	r6, #0
   de63c:	2501      	movs	r5, #1
   de63e:	45b4      	cmp	ip, r6
   de640:	dd04      	ble.n	de64c <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3dc>
    input_size *= input_dims->data[i];
   de642:	f850 7f04 	ldr.w	r7, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= output_dims->data[i];
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de646:	3601      	adds	r6, #1
    input_size *= input_dims->data[i];
   de648:	437d      	muls	r5, r7
   de64a:	e7f8      	b.n	de63e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3ce>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);
   de64c:	fb02 f00e 	mul.w	r0, r2, lr
   de650:	4285      	cmp	r5, r0
   de652:	f47f ae5e 	bne.w	de312 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xa2>
   de656:	685b      	ldr	r3, [r3, #4]
   de658:	9302      	str	r3, [sp, #8]
   de65a:	9b01      	ldr	r3, [sp, #4]
   de65c:	4353      	muls	r3, r2
   de65e:	00db      	lsls	r3, r3, #3
   de660:	2000      	movs	r0, #0
   de662:	00d7      	lsls	r7, r2, #3
   de664:	9303      	str	r3, [sp, #12]

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de666:	4605      	mov	r5, r0
   de668:	9b01      	ldr	r3, [sp, #4]
   de66a:	42ab      	cmp	r3, r5
   de66c:	f77f ae5f 	ble.w	de32e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xbe>
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
   de670:	680b      	ldr	r3, [r1, #0]
   de672:	eb03 0385 	add.w	r3, r3, r5, lsl #2
   de676:	2438      	movs	r4, #56	; 0x38
   de678:	685e      	ldr	r6, [r3, #4]
   de67a:	9b00      	ldr	r3, [sp, #0]
   de67c:	689b      	ldr	r3, [r3, #8]
   de67e:	fb04 3306 	mla	r3, r4, r6, r3
   de682:	b103      	cbz	r3, de686 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x416>
   de684:	685b      	ldr	r3, [r3, #4]
   de686:	f04f 0c00 	mov.w	ip, #0
   de68a:	461e      	mov	r6, r3
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de68c:	46e1      	mov	r9, ip
   de68e:	45ce      	cmp	lr, r9
   de690:	dd15      	ble.n	de6be <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x44e>
   de692:	9c02      	ldr	r4, [sp, #8]
   de694:	eb00 080c 	add.w	r8, r0, ip
   de698:	44a0      	add	r8, r4
   de69a:	46b3      	mov	fp, r6
      const T* input_ptr = input_data + copy_size * k;
      int loc = k * values_count * copy_size + i * copy_size;
      T* output_ptr = output_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   de69c:	f04f 0a00 	mov.w	sl, #0
   de6a0:	4552      	cmp	r2, sl
   de6a2:	dd06      	ble.n	de6b2 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x442>
   de6a4:	e8fb 3402 	ldrd	r3, r4, [fp], #8
   de6a8:	f10a 0a01 	add.w	sl, sl, #1
   de6ac:	e8e8 3402 	strd	r3, r4, [r8], #8
   de6b0:	e7f6      	b.n	de6a0 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x430>
   de6b2:	9c03      	ldr	r4, [sp, #12]
  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de6b4:	f109 0901 	add.w	r9, r9, #1
   de6b8:	443e      	add	r6, r7
   de6ba:	44a4      	add	ip, r4
   de6bc:	e7e7      	b.n	de68e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x41e>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de6be:	3501      	adds	r5, #1
   de6c0:	4438      	add	r0, r7
   de6c2:	e7d1      	b.n	de668 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3f8>
    case kTfLiteInt64: {
      return PackImpl<int64_t>(context, node, output, data->values_count,
                               data->axis);
    }
    default: {
      context->ReportError(context, "Type '%s' is not supported by pack.",
   de6c4:	9b00      	ldr	r3, [sp, #0]
   de6c6:	695d      	ldr	r5, [r3, #20]
   de6c8:	f7f5 fd26 	bl	d4118 <TfLiteTypeGetName>
                           TfLiteTypeGetName(output->type));
   de6cc:	4903      	ldr	r1, [pc, #12]	; (de6dc <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x46c>)
   de6ce:	4602      	mov	r2, r0
   de6d0:	9800      	ldr	r0, [sp, #0]
   de6d2:	47a8      	blx	r5
      return kTfLiteError;
   de6d4:	2001      	movs	r0, #1
    }
  }

  return kTfLiteOk;
}
   de6d6:	b005      	add	sp, #20
   de6d8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   de6dc:	000e9f6e 	.word	0x000e9f6e

000de6e0 <_ZN6tflite3ops5micro13Register_PACKEv>:
}  // namespace pack

TfLiteRegistration* Register_PACK() {
  static TfLiteRegistration r = {nullptr, nullptr, pack::Prepare, pack::Eval};
  return &r;
}
   de6e0:	4800      	ldr	r0, [pc, #0]	; (de6e4 <_ZN6tflite3ops5micro13Register_PACKEv+0x4>)
   de6e2:	4770      	bx	lr
   de6e4:	2003c0c8 	.word	0x2003c0c8

000de6e8 <_ZN6tflite3ops5micro7pooling4InitEP13TfLiteContextPKcj>:

}  // namespace

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
   de6e8:	2000      	movs	r0, #0
   de6ea:	4770      	bx	lr

000de6ec <_ZN6tflite3ops5micro7pooling4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
   de6ec:	4770      	bx	lr

000de6ee <_ZN6tflite3ops5micro7pooling7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   de6ee:	2000      	movs	r0, #0
   de6f0:	4770      	bx	lr
	...

000de6f4 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf>:
namespace reference_ops {

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape,
                        const float* input_data,
                        const RuntimeShape& output_shape, float* output_data) {
   de6f4:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   de6f8:	ed2d 8b04 	vpush	{d8-d9}
   de6fc:	461d      	mov	r5, r3
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   de6fe:	680b      	ldr	r3, [r1, #0]
namespace reference_ops {

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape,
                        const float* input_data,
                        const RuntimeShape& output_shape, float* output_data) {
   de700:	b095      	sub	sp, #84	; 0x54
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   de702:	2b04      	cmp	r3, #4
namespace reference_ops {

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape,
                        const float* input_data,
                        const RuntimeShape& output_shape, float* output_data) {
   de704:	4604      	mov	r4, r0
   de706:	468a      	mov	sl, r1
   de708:	9212      	str	r2, [sp, #72]	; 0x48
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   de70a:	d001      	beq.n	de710 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x1c>
   de70c:	f005 fe0e 	bl	e432c <abort>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   de710:	682b      	ldr	r3, [r5, #0]
   de712:	2b04      	cmp	r3, #4
   de714:	d1fa      	bne.n	de70c <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x18>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   de716:	2300      	movs	r3, #0
   de718:	4619      	mov	r1, r3
   de71a:	462a      	mov	r2, r5
   de71c:	4650      	mov	r0, sl
   de71e:	f7fd fa4e 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   de722:	2303      	movs	r3, #3
   de724:	4619      	mov	r1, r3
   de726:	462a      	mov	r2, r5
                        const RuntimeShape& input_shape,
                        const float* input_data,
                        const RuntimeShape& output_shape, float* output_data) {
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   de728:	9008      	str	r0, [sp, #32]
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   de72a:	4650      	mov	r0, sl
   de72c:	f7fd fa47 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
   de730:	2101      	movs	r1, #1
                        const float* input_data,
                        const RuntimeShape& output_shape, float* output_data) {
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   de732:	9009      	str	r0, [sp, #36]	; 0x24
  const int input_height = input_shape.Dims(1);
   de734:	4650      	mov	r0, sl
   de736:	f7f7 fe91 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   de73a:	2102      	movs	r1, #2
                        const RuntimeShape& output_shape, float* output_data) {
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
   de73c:	900a      	str	r0, [sp, #40]	; 0x28
  const int input_width = input_shape.Dims(2);
   de73e:	4650      	mov	r0, sl
   de740:	f7f7 fe8c 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   de744:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   de746:	900b      	str	r0, [sp, #44]	; 0x2c
  const int output_height = output_shape.Dims(1);
   de748:	4628      	mov	r0, r5
   de74a:	f7f7 fe87 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   de74e:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   de750:	900c      	str	r0, [sp, #48]	; 0x30
  const int output_width = output_shape.Dims(2);
   de752:	4628      	mov	r0, r5
   de754:	f7f7 fe82 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int stride_height = params.stride_height;
   de758:	68e3      	ldr	r3, [r4, #12]
   de75a:	930f      	str	r3, [sp, #60]	; 0x3c
  const int stride_width = params.stride_width;
   de75c:	6923      	ldr	r3, [r4, #16]
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   de75e:	900e      	str	r0, [sp, #56]	; 0x38
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
   de760:	9310      	str	r3, [sp, #64]	; 0x40
  for (int batch = 0; batch < batches; ++batch) {
   de762:	f04f 0b00 	mov.w	fp, #0
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              total +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
              filter_count++;
   de766:	eef7 9a00 	vmov.f32	s19, #112	; 0x3f800000  1.0
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
   de76a:	9b08      	ldr	r3, [sp, #32]
   de76c:	459b      	cmp	fp, r3
   de76e:	f280 8092 	bge.w	de896 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x1a2>
   de772:	2300      	movs	r3, #0
   de774:	9304      	str	r3, [sp, #16]
   de776:	9302      	str	r3, [sp, #8]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   de778:	9b02      	ldr	r3, [sp, #8]
   de77a:	9a0c      	ldr	r2, [sp, #48]	; 0x30
   de77c:	4293      	cmp	r3, r2
   de77e:	f280 8087 	bge.w	de890 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x19c>
   de782:	2300      	movs	r3, #0
   de784:	9305      	str	r3, [sp, #20]
   de786:	9303      	str	r3, [sp, #12]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   de788:	9b03      	ldr	r3, [sp, #12]
   de78a:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   de78c:	4293      	cmp	r3, r2
   de78e:	da77      	bge.n	de880 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x18c>
   de790:	f04f 0900 	mov.w	r9, #0
        for (int channel = 0; channel < depth; ++channel) {
   de794:	9b09      	ldr	r3, [sp, #36]	; 0x24
   de796:	4599      	cmp	r9, r3
   de798:	da6a      	bge.n	de870 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x17c>
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
   de79a:	f9b4 7002 	ldrsh.w	r7, [r4, #2]
   de79e:	9b05      	ldr	r3, [sp, #20]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
   de7a0:	f9b4 8004 	ldrsh.w	r8, [r4, #4]
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float total = 0.f;
          float filter_count = 0;
   de7a4:	ed9f 8a3e 	vldr	s16, [pc, #248]	; de8a0 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x1ac>
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
   de7a8:	1bdb      	subs	r3, r3, r7
   de7aa:	9306      	str	r3, [sp, #24]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
   de7ac:	9b04      	ldr	r3, [sp, #16]
   de7ae:	9a06      	ldr	r2, [sp, #24]
   de7b0:	ebc8 0803 	rsb	r8, r8, r3
   de7b4:	9b06      	ldr	r3, [sp, #24]
   de7b6:	425b      	negs	r3, r3
   de7b8:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
   de7bc:	9307      	str	r3, [sp, #28]
   de7be:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   de7c0:	1a9a      	subs	r2, r3, r2
   de7c2:	69a3      	ldr	r3, [r4, #24]
   de7c4:	429a      	cmp	r2, r3
   de7c6:	bfa8      	it	ge
   de7c8:	461a      	movge	r2, r3
   de7ca:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   de7cc:	920d      	str	r2, [sp, #52]	; 0x34
   de7ce:	6962      	ldr	r2, [r4, #20]
   de7d0:	ebc8 0303 	rsb	r3, r8, r3
   de7d4:	4293      	cmp	r3, r2
   de7d6:	f1c8 0600 	rsb	r6, r8, #0
   de7da:	bfa8      	it	ge
   de7dc:	4613      	movge	r3, r2
   de7de:	ea26 76e6 	bic.w	r6, r6, r6, asr #31
   de7e2:	9311      	str	r3, [sp, #68]	; 0x44
          const int filter_x_end =
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float total = 0.f;
   de7e4:	eef0 8a48 	vmov.f32	s17, s16
          float filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   de7e8:	9b11      	ldr	r3, [sp, #68]	; 0x44
   de7ea:	429e      	cmp	r6, r3
   de7ec:	da1c      	bge.n	de828 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x134>
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              total +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
   de7ee:	eb08 0306 	add.w	r3, r8, r6
   de7f2:	9f07      	ldr	r7, [sp, #28]
   de7f4:	9313      	str	r3, [sp, #76]	; 0x4c
              std::min(params.filter_height, input_height - in_y_origin);
          float total = 0.f;
          float filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   de7f6:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   de7f8:	429f      	cmp	r7, r3
   de7fa:	da13      	bge.n	de824 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x130>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              total +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
   de7fc:	9b06      	ldr	r3, [sp, #24]
   de7fe:	f8cd 9000 	str.w	r9, [sp]
   de802:	18fb      	adds	r3, r7, r3
   de804:	9a13      	ldr	r2, [sp, #76]	; 0x4c
   de806:	4659      	mov	r1, fp
   de808:	4650      	mov	r0, sl
   de80a:	f7f7 fe8c 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   de80e:	9b12      	ldr	r3, [sp, #72]	; 0x48
   de810:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   de814:	edd0 7a00 	vldr	s15, [r0]
              filter_count++;
   de818:	ee38 8a29 	vadd.f32	s16, s16, s19
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              total +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
   de81c:	ee78 8aa7 	vadd.f32	s17, s17, s15
              std::min(params.filter_height, input_height - in_y_origin);
          float total = 0.f;
          float filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   de820:	3701      	adds	r7, #1
   de822:	e7e8      	b.n	de7f6 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x102>
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float total = 0.f;
          float filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   de824:	3601      	adds	r6, #1
   de826:	e7df      	b.n	de7e8 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0xf4>
              total +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
              filter_count++;
            }
          }
          const float average = total / filter_count;
   de828:	ee88 9a88 	vdiv.f32	s18, s17, s16
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
   de82c:	f8cd 9000 	str.w	r9, [sp]
   de830:	9b03      	ldr	r3, [sp, #12]
   de832:	9a02      	ldr	r2, [sp, #8]
   de834:	4659      	mov	r1, fp
   de836:	4628      	mov	r0, r5
   de838:	f7f7 fe75 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   de83c:	9b22      	ldr	r3, [sp, #136]	; 0x88
   de83e:	eb03 0080 	add.w	r0, r3, r0, lsl #2
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
   de842:	f109 0901 	add.w	r9, r9, #1
              filter_count++;
            }
          }
          const float average = total / filter_count;
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
              ActivationFunctionWithMinMax(average, params.float_activation_min,
   de846:	edd4 7a09 	vldr	s15, [r4, #36]	; 0x24
                                           params.float_activation_max);
   de84a:	ed94 7a0a 	vldr	s14, [r4, #40]	; 0x28
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
	return __b;
      return __a;
   de84e:	eeb4 9ae7 	vcmpe.f32	s18, s15
   de852:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   de856:	bf58      	it	pl
   de858:	eef0 7a49 	vmovpl.f32	s15, s18
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   de85c:	eeb4 7a67 	vcmp.f32	s14, s15
   de860:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   de864:	bf48      	it	mi
   de866:	eef0 7a47 	vmovmi.f32	s15, s14
   de86a:	edc0 7a00 	vstr	s15, [r0]
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
   de86e:	e791      	b.n	de794 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0xa0>
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   de870:	9b03      	ldr	r3, [sp, #12]
   de872:	9a10      	ldr	r2, [sp, #64]	; 0x40
   de874:	3301      	adds	r3, #1
   de876:	9303      	str	r3, [sp, #12]
   de878:	9b05      	ldr	r3, [sp, #20]
   de87a:	4413      	add	r3, r2
   de87c:	9305      	str	r3, [sp, #20]
   de87e:	e783      	b.n	de788 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x94>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   de880:	9b02      	ldr	r3, [sp, #8]
   de882:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
   de884:	3301      	adds	r3, #1
   de886:	9302      	str	r3, [sp, #8]
   de888:	9b04      	ldr	r3, [sp, #16]
   de88a:	4413      	add	r3, r2
   de88c:	9304      	str	r3, [sp, #16]
   de88e:	e773      	b.n	de778 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x84>
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
   de890:	f10b 0b01 	add.w	fp, fp, #1
   de894:	e769      	b.n	de76a <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x76>
                                           params.float_activation_max);
        }
      }
    }
  }
}
   de896:	b015      	add	sp, #84	; 0x54
   de898:	ecbd 8b04 	vpop	{d8-d9}
   de89c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   de8a0:	00000000 	.word	0x00000000

000de8a4 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph>:

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape,
                        const uint8* input_data,
                        const RuntimeShape& output_shape, uint8* output_data) {
   de8a4:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   de8a8:	b097      	sub	sp, #92	; 0x5c
   de8aa:	461f      	mov	r7, r3
   de8ac:	9214      	str	r2, [sp, #80]	; 0x50
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
   de8ae:	6a03      	ldr	r3, [r0, #32]
   de8b0:	69c2      	ldr	r2, [r0, #28]
   de8b2:	429a      	cmp	r2, r3
}

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape,
                        const uint8* input_data,
                        const RuntimeShape& output_shape, uint8* output_data) {
   de8b4:	4604      	mov	r4, r0
   de8b6:	460e      	mov	r6, r1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
   de8b8:	dd01      	ble.n	de8be <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x1a>

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape,
                        const uint8* input_data,
                        const RuntimeShape& output_shape, uint8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
   de8ba:	f005 fd37 	bl	e432c <abort>
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   de8be:	680b      	ldr	r3, [r1, #0]
   de8c0:	2b04      	cmp	r3, #4
   de8c2:	d1fa      	bne.n	de8ba <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x16>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   de8c4:	683b      	ldr	r3, [r7, #0]
   de8c6:	2b04      	cmp	r3, #4
   de8c8:	d1f7      	bne.n	de8ba <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x16>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   de8ca:	2300      	movs	r3, #0
   de8cc:	4619      	mov	r1, r3
   de8ce:	463a      	mov	r2, r7
   de8d0:	4630      	mov	r0, r6
   de8d2:	f7fd f974 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   de8d6:	2303      	movs	r3, #3
   de8d8:	4619      	mov	r1, r3
   de8da:	463a      	mov	r2, r7
                        const RuntimeShape& output_shape, uint8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   de8dc:	900a      	str	r0, [sp, #40]	; 0x28
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   de8de:	4630      	mov	r0, r6
   de8e0:	f7fd f96d 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
   de8e4:	2101      	movs	r1, #1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   de8e6:	900b      	str	r0, [sp, #44]	; 0x2c
  const int input_height = input_shape.Dims(1);
   de8e8:	4630      	mov	r0, r6
   de8ea:	f7f7 fdb7 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   de8ee:	2102      	movs	r1, #2
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
   de8f0:	900c      	str	r0, [sp, #48]	; 0x30
  const int input_width = input_shape.Dims(2);
   de8f2:	4630      	mov	r0, r6
   de8f4:	f7f7 fdb2 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   de8f8:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   de8fa:	900d      	str	r0, [sp, #52]	; 0x34
  const int output_height = output_shape.Dims(1);
   de8fc:	4638      	mov	r0, r7
   de8fe:	f7f7 fdad 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   de902:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   de904:	900e      	str	r0, [sp, #56]	; 0x38
  const int output_width = output_shape.Dims(2);
   de906:	4638      	mov	r0, r7
   de908:	f7f7 fda8 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int stride_height = params.stride_height;
   de90c:	68e3      	ldr	r3, [r4, #12]
   de90e:	9310      	str	r3, [sp, #64]	; 0x40
  const int stride_width = params.stride_width;
   de910:	6923      	ldr	r3, [r4, #16]
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   de912:	900f      	str	r0, [sp, #60]	; 0x3c
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
   de914:	9311      	str	r3, [sp, #68]	; 0x44
  for (int batch = 0; batch < batches; ++batch) {
   de916:	f04f 0a00 	mov.w	sl, #0
   de91a:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   de91c:	459a      	cmp	sl, r3
   de91e:	f280 8088 	bge.w	dea32 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x18e>
   de922:	2300      	movs	r3, #0
   de924:	9305      	str	r3, [sp, #20]
   de926:	9303      	str	r3, [sp, #12]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   de928:	9b03      	ldr	r3, [sp, #12]
   de92a:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   de92c:	4293      	cmp	r3, r2
   de92e:	da7d      	bge.n	dea2c <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x188>
   de930:	2300      	movs	r3, #0
   de932:	9306      	str	r3, [sp, #24]
   de934:	9304      	str	r3, [sp, #16]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   de936:	9b04      	ldr	r3, [sp, #16]
   de938:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
   de93a:	4293      	cmp	r3, r2
   de93c:	da6e      	bge.n	dea1c <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x178>
   de93e:	2300      	movs	r3, #0
   de940:	9302      	str	r3, [sp, #8]
        for (int channel = 0; channel < depth; ++channel) {
   de942:	9b02      	ldr	r3, [sp, #8]
   de944:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   de946:	4293      	cmp	r3, r2
   de948:	da60      	bge.n	dea0c <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x168>
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
   de94a:	9a06      	ldr	r2, [sp, #24]
   de94c:	f9b4 3002 	ldrsh.w	r3, [r4, #2]
   de950:	1ad3      	subs	r3, r2, r3
   de952:	9308      	str	r3, [sp, #32]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
   de954:	9a05      	ldr	r2, [sp, #20]
   de956:	f9b4 3004 	ldrsh.w	r3, [r4, #4]
   de95a:	1ad3      	subs	r3, r2, r3
   de95c:	9309      	str	r3, [sp, #36]	; 0x24
   de95e:	9b08      	ldr	r3, [sp, #32]
   de960:	9a08      	ldr	r2, [sp, #32]
   de962:	425b      	negs	r3, r3
   de964:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
   de968:	9307      	str	r3, [sp, #28]
   de96a:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   de96c:	1a9a      	subs	r2, r3, r2
   de96e:	69a3      	ldr	r3, [r4, #24]
   de970:	429a      	cmp	r2, r3
   de972:	bfa8      	it	ge
   de974:	461a      	movge	r2, r3
   de976:	9b09      	ldr	r3, [sp, #36]	; 0x24
   de978:	9213      	str	r2, [sp, #76]	; 0x4c
   de97a:	f1c3 0800 	rsb	r8, r3, #0
   de97e:	9a09      	ldr	r2, [sp, #36]	; 0x24
   de980:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   de982:	1a9a      	subs	r2, r3, r2
   de984:	6963      	ldr	r3, [r4, #20]
   de986:	429a      	cmp	r2, r3
   de988:	bfa8      	it	ge
   de98a:	461a      	movge	r2, r3
          const int filter_x_end =
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
   de98c:	2500      	movs	r5, #0
   de98e:	ea28 78e8 	bic.w	r8, r8, r8, asr #31
   de992:	9212      	str	r2, [sp, #72]	; 0x48
          int filter_count = 0;
   de994:	46ab      	mov	fp, r5
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   de996:	9b12      	ldr	r3, [sp, #72]	; 0x48
   de998:	4598      	cmp	r8, r3
   de99a:	da1e      	bge.n	de9da <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x136>
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
   de99c:	9b09      	ldr	r3, [sp, #36]	; 0x24
   de99e:	f8dd 901c 	ldr.w	r9, [sp, #28]
   de9a2:	4443      	add	r3, r8
   de9a4:	ebc9 0b0b 	rsb	fp, r9, fp
   de9a8:	9315      	str	r3, [sp, #84]	; 0x54
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   de9aa:	9a13      	ldr	r2, [sp, #76]	; 0x4c
   de9ac:	4591      	cmp	r9, r2
   de9ae:	eb0b 0309 	add.w	r3, fp, r9
   de9b2:	da0e      	bge.n	de9d2 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x12e>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
   de9b4:	9b02      	ldr	r3, [sp, #8]
   de9b6:	9300      	str	r3, [sp, #0]
   de9b8:	9b08      	ldr	r3, [sp, #32]
   de9ba:	9a15      	ldr	r2, [sp, #84]	; 0x54
   de9bc:	444b      	add	r3, r9
   de9be:	4651      	mov	r1, sl
   de9c0:	4630      	mov	r0, r6
   de9c2:	f7f7 fdb0 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   de9c6:	9b14      	ldr	r3, [sp, #80]	; 0x50
   de9c8:	5c1b      	ldrb	r3, [r3, r0]
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   de9ca:	f109 0901 	add.w	r9, r9, #1
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
   de9ce:	441d      	add	r5, r3
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   de9d0:	e7eb      	b.n	de9aa <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x106>
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   de9d2:	f108 0801 	add.w	r8, r8, #1
   de9d6:	469b      	mov	fp, r3
   de9d8:	e7dd      	b.n	de996 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xf2>
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
              filter_count++;
            }
          }
          acc = (acc + filter_count / 2) / filter_count;
   de9da:	eb05 056b 	add.w	r5, r5, fp, asr #1
   de9de:	fb95 fbfb 	sdiv	fp, r5, fp
   de9e2:	69e5      	ldr	r5, [r4, #28]
   de9e4:	6a23      	ldr	r3, [r4, #32]
          acc = std::max(acc, params.quantized_activation_min);
          acc = std::min(acc, params.quantized_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
   de9e6:	9a03      	ldr	r2, [sp, #12]
   de9e8:	455d      	cmp	r5, fp
   de9ea:	bfb8      	it	lt
   de9ec:	465d      	movlt	r5, fp
   de9ee:	429d      	cmp	r5, r3
   de9f0:	bfa8      	it	ge
   de9f2:	461d      	movge	r5, r3
   de9f4:	9b02      	ldr	r3, [sp, #8]
   de9f6:	9300      	str	r3, [sp, #0]
   de9f8:	4651      	mov	r1, sl
   de9fa:	9b04      	ldr	r3, [sp, #16]
   de9fc:	4638      	mov	r0, r7
   de9fe:	f7f7 fd92 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<uint8>(acc);
   dea02:	9b20      	ldr	r3, [sp, #128]	; 0x80
   dea04:	541d      	strb	r5, [r3, r0]
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
   dea06:	9b02      	ldr	r3, [sp, #8]
   dea08:	3301      	adds	r3, #1
   dea0a:	e799      	b.n	de940 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x9c>
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   dea0c:	9b04      	ldr	r3, [sp, #16]
   dea0e:	9a11      	ldr	r2, [sp, #68]	; 0x44
   dea10:	3301      	adds	r3, #1
   dea12:	9304      	str	r3, [sp, #16]
   dea14:	9b06      	ldr	r3, [sp, #24]
   dea16:	4413      	add	r3, r2
   dea18:	9306      	str	r3, [sp, #24]
   dea1a:	e78c      	b.n	de936 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x92>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   dea1c:	9b03      	ldr	r3, [sp, #12]
   dea1e:	9a10      	ldr	r2, [sp, #64]	; 0x40
   dea20:	3301      	adds	r3, #1
   dea22:	9303      	str	r3, [sp, #12]
   dea24:	9b05      	ldr	r3, [sp, #20]
   dea26:	4413      	add	r3, r2
   dea28:	9305      	str	r3, [sp, #20]
   dea2a:	e77d      	b.n	de928 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x84>
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
   dea2c:	f10a 0a01 	add.w	sl, sl, #1
   dea30:	e773      	b.n	de91a <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x76>
              static_cast<uint8>(acc);
        }
      }
    }
  }
}
   dea32:	b017      	add	sp, #92	; 0x5c
   dea34:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dea38 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf>:
  }
}

inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const float* input_data, const RuntimeShape& output_shape,
                    float* output_data) {
   dea38:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dea3c:	ed2d 8b02 	vpush	{d8}
   dea40:	461e      	mov	r6, r3
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dea42:	680b      	ldr	r3, [r1, #0]
  }
}

inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const float* input_data, const RuntimeShape& output_shape,
                    float* output_data) {
   dea44:	b097      	sub	sp, #92	; 0x5c
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dea46:	2b04      	cmp	r3, #4
  }
}

inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const float* input_data, const RuntimeShape& output_shape,
                    float* output_data) {
   dea48:	4604      	mov	r4, r0
   dea4a:	460d      	mov	r5, r1
   dea4c:	9212      	str	r2, [sp, #72]	; 0x48
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dea4e:	d001      	beq.n	dea54 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x1c>
   dea50:	f005 fc6c 	bl	e432c <abort>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   dea54:	6833      	ldr	r3, [r6, #0]
   dea56:	2b04      	cmp	r3, #4
   dea58:	d1fa      	bne.n	dea50 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x18>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dea5a:	2300      	movs	r3, #0
   dea5c:	4619      	mov	r1, r3
   dea5e:	4632      	mov	r2, r6
   dea60:	4628      	mov	r0, r5
   dea62:	f7fd f8ac 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   dea66:	2303      	movs	r3, #3
   dea68:	4619      	mov	r1, r3
   dea6a:	4632      	mov	r2, r6
inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const float* input_data, const RuntimeShape& output_shape,
                    float* output_data) {
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dea6c:	9008      	str	r0, [sp, #32]
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   dea6e:	4628      	mov	r0, r5
   dea70:	f7fd f8a5 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
   dea74:	2101      	movs	r1, #1
                    const float* input_data, const RuntimeShape& output_shape,
                    float* output_data) {
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   dea76:	9009      	str	r0, [sp, #36]	; 0x24
  const int input_height = input_shape.Dims(1);
   dea78:	4628      	mov	r0, r5
   dea7a:	f7f7 fcef 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   dea7e:	2102      	movs	r1, #2
                    float* output_data) {
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
   dea80:	900a      	str	r0, [sp, #40]	; 0x28
  const int input_width = input_shape.Dims(2);
   dea82:	4628      	mov	r0, r5
   dea84:	f7f7 fcea 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   dea88:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   dea8a:	900b      	str	r0, [sp, #44]	; 0x2c
  const int output_height = output_shape.Dims(1);
   dea8c:	4630      	mov	r0, r6
   dea8e:	f7f7 fce5 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   dea92:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   dea94:	900c      	str	r0, [sp, #48]	; 0x30
  const int output_width = output_shape.Dims(2);
   dea96:	4630      	mov	r0, r6
   dea98:	f7f7 fce0 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int stride_height = params.stride_height;
   dea9c:	68e3      	ldr	r3, [r4, #12]
   dea9e:	930e      	str	r3, [sp, #56]	; 0x38
          const int filter_x_end =
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float max = std::numeric_limits<float>::lowest();
   deaa0:	ed9f 8a54 	vldr	s16, [pc, #336]	; debf4 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x1bc>
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
   deaa4:	6923      	ldr	r3, [r4, #16]
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   deaa6:	900d      	str	r0, [sp, #52]	; 0x34
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
   deaa8:	930f      	str	r3, [sp, #60]	; 0x3c
  for (int batch = 0; batch < batches; ++batch) {
   deaaa:	f04f 0b00 	mov.w	fp, #0
   deaae:	9b08      	ldr	r3, [sp, #32]
   deab0:	459b      	cmp	fp, r3
   deab2:	f280 8099 	bge.w	debe8 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x1b0>
   deab6:	2300      	movs	r3, #0
   deab8:	9304      	str	r3, [sp, #16]
   deaba:	9302      	str	r3, [sp, #8]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   deabc:	9b02      	ldr	r3, [sp, #8]
   deabe:	9a0c      	ldr	r2, [sp, #48]	; 0x30
   deac0:	4293      	cmp	r3, r2
   deac2:	f280 808e 	bge.w	debe2 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x1aa>
   deac6:	2300      	movs	r3, #0
   deac8:	9305      	str	r3, [sp, #20]
   deaca:	9303      	str	r3, [sp, #12]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   deacc:	9b03      	ldr	r3, [sp, #12]
   deace:	9a0d      	ldr	r2, [sp, #52]	; 0x34
   dead0:	4293      	cmp	r3, r2
   dead2:	da7e      	bge.n	debd2 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x19a>
   dead4:	f04f 0800 	mov.w	r8, #0
        for (int channel = 0; channel < depth; ++channel) {
   dead8:	9b09      	ldr	r3, [sp, #36]	; 0x24
   deada:	4598      	cmp	r8, r3
   deadc:	da71      	bge.n	debc2 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x18a>
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
   deade:	f9b4 9002 	ldrsh.w	r9, [r4, #2]
   deae2:	9b05      	ldr	r3, [sp, #20]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
   deae4:	f9b4 a004 	ldrsh.w	sl, [r4, #4]
          const int filter_x_end =
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float max = std::numeric_limits<float>::lowest();
   deae8:	ed8d 8a15 	vstr	s16, [sp, #84]	; 0x54
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
   deaec:	ebc9 0303 	rsb	r3, r9, r3
   deaf0:	9307      	str	r3, [sp, #28]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
   deaf2:	9b04      	ldr	r3, [sp, #16]
   deaf4:	9a07      	ldr	r2, [sp, #28]
   deaf6:	ebca 0a03 	rsb	sl, sl, r3
   deafa:	9b07      	ldr	r3, [sp, #28]
   deafc:	425b      	negs	r3, r3
   deafe:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
   deb02:	9306      	str	r3, [sp, #24]
   deb04:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   deb06:	1a9a      	subs	r2, r3, r2
   deb08:	69a3      	ldr	r3, [r4, #24]
   deb0a:	429a      	cmp	r2, r3
   deb0c:	bfa8      	it	ge
   deb0e:	461a      	movge	r2, r3
   deb10:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   deb12:	9210      	str	r2, [sp, #64]	; 0x40
   deb14:	ebca 0203 	rsb	r2, sl, r3
   deb18:	6963      	ldr	r3, [r4, #20]
   deb1a:	429a      	cmp	r2, r3
   deb1c:	f1ca 0700 	rsb	r7, sl, #0
   deb20:	bfa8      	it	ge
   deb22:	461a      	movge	r2, r3
   deb24:	ea27 77e7 	bic.w	r7, r7, r7, asr #31
   deb28:	9211      	str	r2, [sp, #68]	; 0x44
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float max = std::numeric_limits<float>::lowest();
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   deb2a:	9b11      	ldr	r3, [sp, #68]	; 0x44
   deb2c:	429f      	cmp	r7, r3
   deb2e:	da24      	bge.n	deb7a <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x142>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
   deb30:	eb07 030a 	add.w	r3, r7, sl
   deb34:	f8dd 9018 	ldr.w	r9, [sp, #24]
   deb38:	9313      	str	r3, [sp, #76]	; 0x4c
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float max = std::numeric_limits<float>::lowest();
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   deb3a:	9b10      	ldr	r3, [sp, #64]	; 0x40
   deb3c:	4599      	cmp	r9, r3
   deb3e:	da1a      	bge.n	deb76 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x13e>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
   deb40:	9b07      	ldr	r3, [sp, #28]
   deb42:	f8cd 8000 	str.w	r8, [sp]
   deb46:	444b      	add	r3, r9
   deb48:	9a13      	ldr	r2, [sp, #76]	; 0x4c
   deb4a:	4659      	mov	r1, fp
   deb4c:	4628      	mov	r0, r5
   deb4e:	f7f7 fcea 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   deb52:	9b12      	ldr	r3, [sp, #72]	; 0x48
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   deb54:	eddd 7a15 	vldr	s15, [sp, #84]	; 0x54
   deb58:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   deb5c:	ed90 7a00 	vldr	s14, [r0]
   deb60:	eeb4 7ae7 	vcmpe.f32	s14, s15
   deb64:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
	return __b;
      return __a;
   deb68:	bfd8      	it	le
   deb6a:	a815      	addle	r0, sp, #84	; 0x54
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float max = std::numeric_limits<float>::lowest();
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   deb6c:	f109 0901 	add.w	r9, r9, #1
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
   deb70:	6803      	ldr	r3, [r0, #0]
   deb72:	9315      	str	r3, [sp, #84]	; 0x54
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float max = std::numeric_limits<float>::lowest();
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   deb74:	e7e1      	b.n	deb3a <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x102>
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float max = std::numeric_limits<float>::lowest();
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   deb76:	3701      	adds	r7, #1
   deb78:	e7d7      	b.n	deb2a <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0xf2>
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
            }
          }
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
   deb7a:	f8cd 8000 	str.w	r8, [sp]
   deb7e:	9b03      	ldr	r3, [sp, #12]
   deb80:	9a02      	ldr	r2, [sp, #8]
   deb82:	4659      	mov	r1, fp
   deb84:	4630      	mov	r0, r6
   deb86:	f7f7 fcce 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              ActivationFunctionWithMinMax(max, params.float_activation_min,
   deb8a:	edd4 7a09 	vldr	s15, [r4, #36]	; 0x24
   deb8e:	eddd 6a15 	vldr	s13, [sp, #84]	; 0x54
                                           params.float_activation_max);
   deb92:	ed94 7a0a 	vldr	s14, [r4, #40]	; 0x28
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
            }
          }
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
   deb96:	9b22      	ldr	r3, [sp, #136]	; 0x88
   deb98:	eef4 6ae7 	vcmpe.f32	s13, s15
   deb9c:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   deba0:	bf58      	it	pl
   deba2:	eef0 7a66 	vmovpl.f32	s15, s13
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   deba6:	eeb4 7a67 	vcmp.f32	s14, s15
   debaa:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   debae:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   debb2:	bf48      	it	mi
   debb4:	eef0 7a47 	vmovmi.f32	s15, s14
              ActivationFunctionWithMinMax(max, params.float_activation_min,
                                           params.float_activation_max);
   debb8:	edc0 7a00 	vstr	s15, [r0]
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
   debbc:	f108 0801 	add.w	r8, r8, #1
   debc0:	e78a      	b.n	dead8 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0xa0>
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   debc2:	9b03      	ldr	r3, [sp, #12]
   debc4:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
   debc6:	3301      	adds	r3, #1
   debc8:	9303      	str	r3, [sp, #12]
   debca:	9b05      	ldr	r3, [sp, #20]
   debcc:	4413      	add	r3, r2
   debce:	9305      	str	r3, [sp, #20]
   debd0:	e77c      	b.n	deacc <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x94>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   debd2:	9b02      	ldr	r3, [sp, #8]
   debd4:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   debd6:	3301      	adds	r3, #1
   debd8:	9302      	str	r3, [sp, #8]
   debda:	9b04      	ldr	r3, [sp, #16]
   debdc:	4413      	add	r3, r2
   debde:	9304      	str	r3, [sp, #16]
   debe0:	e76c      	b.n	deabc <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x84>
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
   debe2:	f10b 0b01 	add.w	fp, fp, #1
   debe6:	e762      	b.n	deaae <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x76>
                                           params.float_activation_max);
        }
      }
    }
  }
}
   debe8:	b017      	add	sp, #92	; 0x5c
   debea:	ecbd 8b02 	vpop	{d8}
   debee:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   debf2:	bf00      	nop
   debf4:	ff7fffff 	.word	0xff7fffff

000debf8 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph>:

inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const uint8* input_data, const RuntimeShape& output_shape,
                    uint8* output_data) {
   debf8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   debfc:	b097      	sub	sp, #92	; 0x5c
   debfe:	461e      	mov	r6, r3
   dec00:	9208      	str	r2, [sp, #32]
  TFLITE_DCHECK_LE(params.quantized_activation_min,
   dec02:	6a03      	ldr	r3, [r0, #32]
   dec04:	69c2      	ldr	r2, [r0, #28]
                   params.quantized_activation_max);
   dec06:	429a      	cmp	r2, r3
  }
}

inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const uint8* input_data, const RuntimeShape& output_shape,
                    uint8* output_data) {
   dec08:	4604      	mov	r4, r0
   dec0a:	460d      	mov	r5, r1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
   dec0c:	dd01      	ble.n	dec12 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x1a>
}

inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const uint8* input_data, const RuntimeShape& output_shape,
                    uint8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
   dec0e:	f005 fb8d 	bl	e432c <abort>
                   params.quantized_activation_max);
  TFLITE_DCHECK_GE(params.quantized_activation_min, 0);
   dec12:	2a00      	cmp	r2, #0
   dec14:	dbfb      	blt.n	dec0e <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x16>
  TFLITE_DCHECK_LE(params.quantized_activation_max, 255);
   dec16:	2bff      	cmp	r3, #255	; 0xff
   dec18:	dcf9      	bgt.n	dec0e <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x16>
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dec1a:	680b      	ldr	r3, [r1, #0]
   dec1c:	2b04      	cmp	r3, #4
   dec1e:	d1f6      	bne.n	dec0e <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x16>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   dec20:	6833      	ldr	r3, [r6, #0]
   dec22:	2b04      	cmp	r3, #4
   dec24:	d1f3      	bne.n	dec0e <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x16>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dec26:	2300      	movs	r3, #0
   dec28:	4619      	mov	r1, r3
   dec2a:	4632      	mov	r2, r6
   dec2c:	4628      	mov	r0, r5
   dec2e:	f7fc ffc6 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   dec32:	2303      	movs	r3, #3
   dec34:	4619      	mov	r1, r3
   dec36:	4632      	mov	r2, r6
                   params.quantized_activation_max);
  TFLITE_DCHECK_GE(params.quantized_activation_min, 0);
  TFLITE_DCHECK_LE(params.quantized_activation_max, 255);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dec38:	9009      	str	r0, [sp, #36]	; 0x24
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   dec3a:	4628      	mov	r0, r5
   dec3c:	f7fc ffbf 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
   dec40:	2101      	movs	r1, #1
  TFLITE_DCHECK_GE(params.quantized_activation_min, 0);
  TFLITE_DCHECK_LE(params.quantized_activation_max, 255);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   dec42:	900b      	str	r0, [sp, #44]	; 0x2c
  const int input_height = input_shape.Dims(1);
   dec44:	4628      	mov	r0, r5
   dec46:	f7f7 fc09 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   dec4a:	2102      	movs	r1, #2
  TFLITE_DCHECK_LE(params.quantized_activation_max, 255);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
   dec4c:	900c      	str	r0, [sp, #48]	; 0x30
  const int input_width = input_shape.Dims(2);
   dec4e:	4628      	mov	r0, r5
   dec50:	f7f7 fc04 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   dec54:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   dec56:	900d      	str	r0, [sp, #52]	; 0x34
  const int output_height = output_shape.Dims(1);
   dec58:	4630      	mov	r0, r6
   dec5a:	f7f7 fbff 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   dec5e:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   dec60:	900e      	str	r0, [sp, #56]	; 0x38
  const int output_width = output_shape.Dims(2);
   dec62:	4630      	mov	r0, r6
   dec64:	f7f7 fbfa 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int stride_height = params.stride_height;
   dec68:	68e3      	ldr	r3, [r4, #12]
   dec6a:	9310      	str	r3, [sp, #64]	; 0x40
  const int stride_width = params.stride_width;
   dec6c:	6923      	ldr	r3, [r4, #16]
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   dec6e:	900f      	str	r0, [sp, #60]	; 0x3c
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
   dec70:	9311      	str	r3, [sp, #68]	; 0x44
  for (int batch = 0; batch < batches; ++batch) {
   dec72:	f04f 0b00 	mov.w	fp, #0
   dec76:	9b09      	ldr	r3, [sp, #36]	; 0x24
   dec78:	459b      	cmp	fp, r3
   dec7a:	f280 808d 	bge.w	ded98 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x1a0>
   dec7e:	2300      	movs	r3, #0
   dec80:	9305      	str	r3, [sp, #20]
   dec82:	9302      	str	r3, [sp, #8]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   dec84:	9b02      	ldr	r3, [sp, #8]
   dec86:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   dec88:	4293      	cmp	r3, r2
   dec8a:	f280 8082 	bge.w	ded92 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x19a>
   dec8e:	2300      	movs	r3, #0
   dec90:	9304      	str	r3, [sp, #16]
   dec92:	9303      	str	r3, [sp, #12]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   dec94:	9b03      	ldr	r3, [sp, #12]
   dec96:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
   dec98:	4293      	cmp	r3, r2
   dec9a:	da72      	bge.n	ded82 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x18a>
   dec9c:	f04f 0800 	mov.w	r8, #0
        for (int channel = 0; channel < depth; ++channel) {
   deca0:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   deca2:	4598      	cmp	r8, r3
   deca4:	da65      	bge.n	ded72 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x17a>
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
   deca6:	f9b4 9002 	ldrsh.w	r9, [r4, #2]
   decaa:	9b04      	ldr	r3, [sp, #16]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
   decac:	f9b4 a004 	ldrsh.w	sl, [r4, #4]
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
   decb0:	ebc9 0303 	rsb	r3, r9, r3
   decb4:	9307      	str	r3, [sp, #28]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
   decb6:	9b05      	ldr	r3, [sp, #20]
   decb8:	9a07      	ldr	r2, [sp, #28]
   decba:	ebca 0a03 	rsb	sl, sl, r3
   decbe:	9b07      	ldr	r3, [sp, #28]
   decc0:	425b      	negs	r3, r3
   decc2:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
   decc6:	9306      	str	r3, [sp, #24]
   decc8:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   decca:	1a9a      	subs	r2, r3, r2
   deccc:	69a3      	ldr	r3, [r4, #24]
   decce:	429a      	cmp	r2, r3
   decd0:	bfa8      	it	ge
   decd2:	461a      	movge	r2, r3
   decd4:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   decd6:	920a      	str	r2, [sp, #40]	; 0x28
   decd8:	ebca 0203 	rsb	r2, sl, r3
   decdc:	6963      	ldr	r3, [r4, #20]
   decde:	429a      	cmp	r2, r3
   dece0:	bfa8      	it	ge
   dece2:	461a      	movge	r2, r3
   dece4:	f1ca 0700 	rsb	r7, sl, #0
          const int filter_x_end =
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          uint8 max = 0;
   dece8:	f04f 0300 	mov.w	r3, #0
   decec:	ea27 77e7 	bic.w	r7, r7, r7, asr #31
   decf0:	9212      	str	r2, [sp, #72]	; 0x48
   decf2:	f88d 3057 	strb.w	r3, [sp, #87]	; 0x57
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   decf6:	9b12      	ldr	r3, [sp, #72]	; 0x48
   decf8:	429f      	cmp	r7, r3
   decfa:	da22      	bge.n	ded42 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x14a>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
   decfc:	eb0a 0307 	add.w	r3, sl, r7
   ded00:	f8dd 9018 	ldr.w	r9, [sp, #24]
   ded04:	9313      	str	r3, [sp, #76]	; 0x4c
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          uint8 max = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   ded06:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   ded08:	4599      	cmp	r9, r3
   ded0a:	da18      	bge.n	ded3e <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x146>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
   ded0c:	9b07      	ldr	r3, [sp, #28]
   ded0e:	9a13      	ldr	r2, [sp, #76]	; 0x4c
   ded10:	f8cd 8000 	str.w	r8, [sp]
   ded14:	444b      	add	r3, r9
   ded16:	4659      	mov	r1, fp
   ded18:	4628      	mov	r0, r5
   ded1a:	f7f7 fc04 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   ded1e:	9a08      	ldr	r2, [sp, #32]
   ded20:	9b08      	ldr	r3, [sp, #32]
   ded22:	5c11      	ldrb	r1, [r2, r0]
   ded24:	f89d 2057 	ldrb.w	r2, [sp, #87]	; 0x57
   ded28:	4291      	cmp	r1, r2
   ded2a:	4403      	add	r3, r0
	return __b;
      return __a;
   ded2c:	bf98      	it	ls
   ded2e:	f10d 0357 	addls.w	r3, sp, #87	; 0x57
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          uint8 max = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   ded32:	f109 0901 	add.w	r9, r9, #1
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
   ded36:	781b      	ldrb	r3, [r3, #0]
   ded38:	f88d 3057 	strb.w	r3, [sp, #87]	; 0x57
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          uint8 max = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   ded3c:	e7e3      	b.n	ded06 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x10e>
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          uint8 max = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   ded3e:	3701      	adds	r7, #1
   ded40:	e7d9      	b.n	decf6 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xfe>
   ded42:	7f27      	ldrb	r7, [r4, #28]
   ded44:	f89d 3057 	ldrb.w	r3, [sp, #87]	; 0x57
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
            }
          }
          max = std::max<uint8>(max, params.quantized_activation_min);
          max = std::min<uint8>(max, params.quantized_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
   ded48:	f8cd 8000 	str.w	r8, [sp]
   ded4c:	429f      	cmp	r7, r3
   ded4e:	bf38      	it	cc
   ded50:	461f      	movcc	r7, r3
   ded52:	f894 3020 	ldrb.w	r3, [r4, #32]
   ded56:	9a02      	ldr	r2, [sp, #8]
   ded58:	429f      	cmp	r7, r3
   ded5a:	bf28      	it	cs
   ded5c:	461f      	movcs	r7, r3
   ded5e:	4659      	mov	r1, fp
   ded60:	9b03      	ldr	r3, [sp, #12]
   ded62:	4630      	mov	r0, r6
   ded64:	f7f7 fbdf 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<uint8>(max);
   ded68:	9b20      	ldr	r3, [sp, #128]	; 0x80
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
   ded6a:	f108 0801 	add.w	r8, r8, #1
            }
          }
          max = std::max<uint8>(max, params.quantized_activation_min);
          max = std::min<uint8>(max, params.quantized_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
              static_cast<uint8>(max);
   ded6e:	541f      	strb	r7, [r3, r0]
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
   ded70:	e796      	b.n	deca0 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xa8>
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   ded72:	9b03      	ldr	r3, [sp, #12]
   ded74:	9a11      	ldr	r2, [sp, #68]	; 0x44
   ded76:	3301      	adds	r3, #1
   ded78:	9303      	str	r3, [sp, #12]
   ded7a:	9b04      	ldr	r3, [sp, #16]
   ded7c:	4413      	add	r3, r2
   ded7e:	9304      	str	r3, [sp, #16]
   ded80:	e788      	b.n	dec94 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x9c>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   ded82:	9b02      	ldr	r3, [sp, #8]
   ded84:	9a10      	ldr	r2, [sp, #64]	; 0x40
   ded86:	3301      	adds	r3, #1
   ded88:	9302      	str	r3, [sp, #8]
   ded8a:	9b05      	ldr	r3, [sp, #20]
   ded8c:	4413      	add	r3, r2
   ded8e:	9305      	str	r3, [sp, #20]
   ded90:	e778      	b.n	dec84 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x8c>
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
   ded92:	f10b 0b01 	add.w	fp, fp, #1
   ded96:	e76e      	b.n	dec76 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x7e>
              static_cast<uint8>(max);
        }
      }
    }
  }
}
   ded98:	b017      	add	sp, #92	; 0x5c
   ded9a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000ded9e <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa>:
namespace tflite {
namespace reference_integer_ops {

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape, const int8* input_data,
                        const RuntimeShape& output_shape, int8* output_data) {
   ded9e:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   deda2:	b097      	sub	sp, #92	; 0x5c
   deda4:	461f      	mov	r7, r3
   deda6:	9214      	str	r2, [sp, #80]	; 0x50
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
   deda8:	6a03      	ldr	r3, [r0, #32]
   dedaa:	69c2      	ldr	r2, [r0, #28]
   dedac:	429a      	cmp	r2, r3
namespace tflite {
namespace reference_integer_ops {

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape, const int8* input_data,
                        const RuntimeShape& output_shape, int8* output_data) {
   dedae:	4604      	mov	r4, r0
   dedb0:	4689      	mov	r9, r1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
   dedb2:	dd01      	ble.n	dedb8 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x1a>
namespace reference_integer_ops {

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape, const int8* input_data,
                        const RuntimeShape& output_shape, int8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
   dedb4:	f005 faba 	bl	e432c <abort>
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dedb8:	680b      	ldr	r3, [r1, #0]
   dedba:	2b04      	cmp	r3, #4
   dedbc:	d1fa      	bne.n	dedb4 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x16>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   dedbe:	683b      	ldr	r3, [r7, #0]
   dedc0:	2b04      	cmp	r3, #4
   dedc2:	d1f7      	bne.n	dedb4 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x16>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dedc4:	2300      	movs	r3, #0
   dedc6:	4619      	mov	r1, r3
   dedc8:	463a      	mov	r2, r7
   dedca:	4648      	mov	r0, r9
   dedcc:	f7fc fef7 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   dedd0:	2303      	movs	r3, #3
   dedd2:	4619      	mov	r1, r3
   dedd4:	463a      	mov	r2, r7
                        const RuntimeShape& output_shape, int8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dedd6:	900a      	str	r0, [sp, #40]	; 0x28
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   dedd8:	4648      	mov	r0, r9
   dedda:	f7fc fef0 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
   dedde:	2101      	movs	r1, #1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   dede0:	900b      	str	r0, [sp, #44]	; 0x2c
  const int input_height = input_shape.Dims(1);
   dede2:	4648      	mov	r0, r9
   dede4:	f7f7 fb3a 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   dede8:	2102      	movs	r1, #2
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
   dedea:	900c      	str	r0, [sp, #48]	; 0x30
  const int input_width = input_shape.Dims(2);
   dedec:	4648      	mov	r0, r9
   dedee:	f7f7 fb35 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   dedf2:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   dedf4:	900d      	str	r0, [sp, #52]	; 0x34
  const int output_height = output_shape.Dims(1);
   dedf6:	4638      	mov	r0, r7
   dedf8:	f7f7 fb30 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   dedfc:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   dedfe:	900e      	str	r0, [sp, #56]	; 0x38
  const int output_width = output_shape.Dims(2);
   dee00:	4638      	mov	r0, r7
   dee02:	f7f7 fb2b 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int stride_height = params.stride_height;
   dee06:	68e3      	ldr	r3, [r4, #12]
   dee08:	9310      	str	r3, [sp, #64]	; 0x40
  const int stride_width = params.stride_width;
   dee0a:	6923      	ldr	r3, [r4, #16]
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   dee0c:	900f      	str	r0, [sp, #60]	; 0x3c
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
   dee0e:	9311      	str	r3, [sp, #68]	; 0x44
  for (int batch = 0; batch < batches; ++batch) {
   dee10:	f04f 0a00 	mov.w	sl, #0
   dee14:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   dee16:	459a      	cmp	sl, r3
   dee18:	f280 808d 	bge.w	def36 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x198>
   dee1c:	2300      	movs	r3, #0
   dee1e:	9305      	str	r3, [sp, #20]
   dee20:	9303      	str	r3, [sp, #12]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   dee22:	9b03      	ldr	r3, [sp, #12]
   dee24:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   dee26:	4293      	cmp	r3, r2
   dee28:	f280 8082 	bge.w	def30 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x192>
   dee2c:	2300      	movs	r3, #0
   dee2e:	9306      	str	r3, [sp, #24]
   dee30:	9304      	str	r3, [sp, #16]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   dee32:	9b04      	ldr	r3, [sp, #16]
   dee34:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
   dee36:	4293      	cmp	r3, r2
   dee38:	da72      	bge.n	def20 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x182>
   dee3a:	2300      	movs	r3, #0
   dee3c:	9302      	str	r3, [sp, #8]
        for (int channel = 0; channel < depth; ++channel) {
   dee3e:	9b02      	ldr	r3, [sp, #8]
   dee40:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   dee42:	4293      	cmp	r3, r2
   dee44:	da64      	bge.n	def10 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x172>
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
   dee46:	9a06      	ldr	r2, [sp, #24]
   dee48:	f9b4 3002 	ldrsh.w	r3, [r4, #2]
   dee4c:	1ad3      	subs	r3, r2, r3
   dee4e:	9308      	str	r3, [sp, #32]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
   dee50:	9a05      	ldr	r2, [sp, #20]
   dee52:	f9b4 3004 	ldrsh.w	r3, [r4, #4]
   dee56:	1ad3      	subs	r3, r2, r3
   dee58:	9309      	str	r3, [sp, #36]	; 0x24
   dee5a:	9b08      	ldr	r3, [sp, #32]
   dee5c:	9a08      	ldr	r2, [sp, #32]
   dee5e:	425b      	negs	r3, r3
   dee60:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
   dee64:	9307      	str	r3, [sp, #28]
   dee66:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   dee68:	1a9a      	subs	r2, r3, r2
   dee6a:	69a3      	ldr	r3, [r4, #24]
   dee6c:	429a      	cmp	r2, r3
   dee6e:	bfa8      	it	ge
   dee70:	461a      	movge	r2, r3
   dee72:	9b09      	ldr	r3, [sp, #36]	; 0x24
   dee74:	9213      	str	r2, [sp, #76]	; 0x4c
   dee76:	425e      	negs	r6, r3
   dee78:	9a09      	ldr	r2, [sp, #36]	; 0x24
   dee7a:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   dee7c:	1a9a      	subs	r2, r3, r2
   dee7e:	6963      	ldr	r3, [r4, #20]
   dee80:	429a      	cmp	r2, r3
   dee82:	bfa8      	it	ge
   dee84:	461a      	movge	r2, r3
          const int filter_x_end =
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
   dee86:	2500      	movs	r5, #0
   dee88:	ea26 76e6 	bic.w	r6, r6, r6, asr #31
   dee8c:	9212      	str	r2, [sp, #72]	; 0x48
          int filter_count = 0;
   dee8e:	46ab      	mov	fp, r5
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   dee90:	9b12      	ldr	r3, [sp, #72]	; 0x48
   dee92:	429e      	cmp	r6, r3
   dee94:	da1d      	bge.n	deed2 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x134>
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
   dee96:	9b09      	ldr	r3, [sp, #36]	; 0x24
   dee98:	f8dd 801c 	ldr.w	r8, [sp, #28]
   dee9c:	4433      	add	r3, r6
   dee9e:	ebc8 0b0b 	rsb	fp, r8, fp
   deea2:	9315      	str	r3, [sp, #84]	; 0x54
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   deea4:	9a13      	ldr	r2, [sp, #76]	; 0x4c
   deea6:	4590      	cmp	r8, r2
   deea8:	eb0b 0308 	add.w	r3, fp, r8
   deeac:	da0e      	bge.n	deecc <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x12e>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
   deeae:	9b02      	ldr	r3, [sp, #8]
   deeb0:	9300      	str	r3, [sp, #0]
   deeb2:	9b08      	ldr	r3, [sp, #32]
   deeb4:	9a15      	ldr	r2, [sp, #84]	; 0x54
   deeb6:	4443      	add	r3, r8
   deeb8:	4651      	mov	r1, sl
   deeba:	4648      	mov	r0, r9
   deebc:	f7f7 fb33 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   deec0:	9b14      	ldr	r3, [sp, #80]	; 0x50
   deec2:	561b      	ldrsb	r3, [r3, r0]
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   deec4:	f108 0801 	add.w	r8, r8, #1
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
   deec8:	441d      	add	r5, r3
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   deeca:	e7eb      	b.n	deea4 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x106>
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   deecc:	3601      	adds	r6, #1
   deece:	469b      	mov	fp, r3
   deed0:	e7de      	b.n	dee90 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0xf2>
              filter_count++;
            }
          }
          // Round to the closest integer value.
          acc = acc > 0 ? (acc + filter_count / 2) / filter_count
                        : (acc - filter_count / 2) / filter_count;
   deed2:	2d00      	cmp	r5, #0
   deed4:	bfd7      	itett	le
   deed6:	2302      	movle	r3, #2
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
              filter_count++;
            }
          }
          // Round to the closest integer value.
          acc = acc > 0 ? (acc + filter_count / 2) / filter_count
   deed8:	eb05 056b 	addgt.w	r5, r5, fp, asr #1
                        : (acc - filter_count / 2) / filter_count;
   deedc:	fb9b f3f3 	sdivle	r3, fp, r3
   deee0:	1aed      	suble	r5, r5, r3
   deee2:	fb95 fbfb 	sdiv	fp, r5, fp
   deee6:	69e5      	ldr	r5, [r4, #28]
          acc = std::max(acc, params.quantized_activation_min);
          acc = std::min(acc, params.quantized_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
   deee8:	9b02      	ldr	r3, [sp, #8]
   deeea:	9300      	str	r3, [sp, #0]
   deeec:	45ab      	cmp	fp, r5
   deeee:	bfb8      	it	lt
   deef0:	46ab      	movlt	fp, r5
   deef2:	6a25      	ldr	r5, [r4, #32]
   deef4:	9b04      	ldr	r3, [sp, #16]
   deef6:	9a03      	ldr	r2, [sp, #12]
   deef8:	455d      	cmp	r5, fp
   deefa:	4651      	mov	r1, sl
   deefc:	4638      	mov	r0, r7
   deefe:	bfa8      	it	ge
   def00:	465d      	movge	r5, fp
   def02:	f7f7 fb10 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<int8>(acc);
   def06:	9b20      	ldr	r3, [sp, #128]	; 0x80
   def08:	541d      	strb	r5, [r3, r0]
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
   def0a:	9b02      	ldr	r3, [sp, #8]
   def0c:	3301      	adds	r3, #1
   def0e:	e795      	b.n	dee3c <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x9e>
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   def10:	9b04      	ldr	r3, [sp, #16]
   def12:	9a11      	ldr	r2, [sp, #68]	; 0x44
   def14:	3301      	adds	r3, #1
   def16:	9304      	str	r3, [sp, #16]
   def18:	9b06      	ldr	r3, [sp, #24]
   def1a:	4413      	add	r3, r2
   def1c:	9306      	str	r3, [sp, #24]
   def1e:	e788      	b.n	dee32 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x94>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   def20:	9b03      	ldr	r3, [sp, #12]
   def22:	9a10      	ldr	r2, [sp, #64]	; 0x40
   def24:	3301      	adds	r3, #1
   def26:	9303      	str	r3, [sp, #12]
   def28:	9b05      	ldr	r3, [sp, #20]
   def2a:	4413      	add	r3, r2
   def2c:	9305      	str	r3, [sp, #20]
   def2e:	e778      	b.n	dee22 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x84>
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
   def30:	f10a 0a01 	add.w	sl, sl, #1
   def34:	e76e      	b.n	dee14 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x76>
              static_cast<int8>(acc);
        }
      }
    }
  }
}
   def36:	b017      	add	sp, #92	; 0x5c
   def38:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000def3c <_ZN6tflite3ops5micro7pooling12_GLOBAL__N_115CalculateOpDataEPK13TfLiteContextPK16TfLitePoolParamsPK12TfLiteTensorSC_PNS3_6OpDataE.isra.2>:

struct OpData {
  TfLitePaddingValues padding;
};

TfLiteStatus CalculateOpData(const TfLiteContext* context,
   def3c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   def40:	688b      	ldr	r3, [r1, #8]
  int width = SizeOfDimension(input, 2);

  int out_height, out_width;

  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
   def42:	6846      	ldr	r6, [r0, #4]

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
inline int SizeOfDimension(const TfLiteTensor* t, int dim) {
  return t->dims->data[dim];
   def44:	689f      	ldr	r7, [r3, #8]
   def46:	f8d3 b00c 	ldr.w	fp, [r3, #12]
      /*dilation_rate_height=*/1,
      /*dilation_rate_width=*/1, height, width, params->filter_height,
      params->filter_width, params->padding, &out_height, &out_width);
   def4a:	f890 a000 	ldrb.w	sl, [r0]
   def4e:	68c3      	ldr	r3, [r0, #12]
  int width = SizeOfDimension(input, 2);

  int out_height, out_width;

  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
   def50:	f8d0 9008 	ldr.w	r9, [r0, #8]
      /*dilation_rate_height=*/1,
      /*dilation_rate_width=*/1, height, width, params->filter_height,
   def54:	f8d0 8010 	ldr.w	r8, [r0, #16]

struct OpData {
  TfLitePaddingValues padding;
};

TfLiteStatus CalculateOpData(const TfLiteContext* context,
   def58:	b085      	sub	sp, #20

inline TfLitePaddingValues ComputePaddingHeightWidth(
    int stride_height, int stride_width, int dilation_rate_height,
    int dilation_rate_width, int in_height, int in_width, int filter_height,
    int filter_width, TfLitePadding padding, int* out_height, int* out_width) {
  *out_width = ComputeOutSize(padding, in_width, filter_width, stride_width,
   def5a:	2401      	movs	r4, #1

  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      /*dilation_rate_height=*/1,
      /*dilation_rate_width=*/1, height, width, params->filter_height,
      params->filter_width, params->padding, &out_height, &out_width);
   def5c:	9302      	str	r3, [sp, #8]

struct OpData {
  TfLitePaddingValues padding;
};

TfLiteStatus CalculateOpData(const TfLiteContext* context,
   def5e:	4615      	mov	r5, r2
   def60:	9400      	str	r4, [sp, #0]
   def62:	4633      	mov	r3, r6
   def64:	9a02      	ldr	r2, [sp, #8]
   def66:	4659      	mov	r1, fp
   def68:	4650      	mov	r0, sl
   def6a:	f7fd f9a8 	bl	dc2be <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>
                              dilation_rate_width);
  *out_height = ComputeOutSize(padding, in_height, filter_height, stride_height,
   def6e:	9400      	str	r4, [sp, #0]

inline TfLitePaddingValues ComputePaddingHeightWidth(
    int stride_height, int stride_width, int dilation_rate_height,
    int dilation_rate_width, int in_height, int in_width, int filter_height,
    int filter_width, TfLitePadding padding, int* out_height, int* out_width) {
  *out_width = ComputeOutSize(padding, in_width, filter_width, stride_width,
   def70:	9003      	str	r0, [sp, #12]
                              dilation_rate_width);
  *out_height = ComputeOutSize(padding, in_height, filter_height, stride_height,
   def72:	464b      	mov	r3, r9
   def74:	4642      	mov	r2, r8
   def76:	4639      	mov	r1, r7
   def78:	4650      	mov	r0, sl
   def7a:	f7fd f9a0 	bl	dc2be <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>
inline int ComputePaddingWithOffset(int stride, int dilation_rate, int in_size,
                                    int filter_size, int out_size,
                                    int* offset) {
  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;
  int total_padding =
      ((out_size - 1) * stride + effective_filter_size - in_size);
   def7e:	9b03      	ldr	r3, [sp, #12]
   def80:	1e5c      	subs	r4, r3, #1
   def82:	9b02      	ldr	r3, [sp, #8]
   def84:	3801      	subs	r0, #1
   def86:	fb06 3604 	mla	r6, r6, r4, r3
   def8a:	fb09 8800 	mla	r8, r9, r0, r8
   def8e:	ebcb 0606 	rsb	r6, fp, r6
   def92:	ebc7 0708 	rsb	r7, r7, r8
  total_padding = total_padding > 0 ? total_padding : 0;
   def96:	ea26 76e6 	bic.w	r6, r6, r6, asr #31

  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      /*dilation_rate_height=*/1,
      /*dilation_rate_width=*/1, height, width, params->filter_height,
      params->filter_width, params->padding, &out_height, &out_width);
   def9a:	1073      	asrs	r3, r6, #1
   def9c:	ea27 77e7 	bic.w	r7, r7, r7, asr #31
   defa0:	602b      	str	r3, [r5, #0]
   defa2:	f006 0601 	and.w	r6, r6, #1
   defa6:	107b      	asrs	r3, r7, #1

  return kTfLiteOk;
}
   defa8:	2000      	movs	r0, #0

  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      /*dilation_rate_height=*/1,
      /*dilation_rate_width=*/1, height, width, params->filter_height,
      params->filter_width, params->padding, &out_height, &out_width);
   defaa:	f007 0701 	and.w	r7, r7, #1
   defae:	606b      	str	r3, [r5, #4]
   defb0:	60ae      	str	r6, [r5, #8]
   defb2:	60ef      	str	r7, [r5, #12]

  return kTfLiteOk;
}
   defb4:	b005      	add	sp, #20
   defb6:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

000defbc <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode>:
      return kTfLiteError;
  }
  return kTfLiteOk;
}

TfLiteStatus MaxEval(TfLiteContext* context, TfLiteNode* node) {
   defbc:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   defc0:	680b      	ldr	r3, [r1, #0]
   defc2:	f8d0 a008 	ldr.w	sl, [r0, #8]
   defc6:	685b      	ldr	r3, [r3, #4]
  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);
   defc8:	694d      	ldr	r5, [r1, #20]
   defca:	2438      	movs	r4, #56	; 0x38
   defcc:	fb04 f803 	mul.w	r8, r4, r3
      return kTfLiteError;
  }
  return kTfLiteOk;
}

TfLiteStatus MaxEval(TfLiteContext* context, TfLiteNode* node) {
   defd0:	b09f      	sub	sp, #124	; 0x7c
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   defd2:	684b      	ldr	r3, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   defd4:	eb0a 0708 	add.w	r7, sl, r8
   defd8:	4681      	mov	r9, r0
  OpData data;

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, input, output, &data));
   defda:	aa05      	add	r2, sp, #20
   defdc:	4639      	mov	r1, r7
   defde:	4628      	mov	r0, r5
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   defe0:	f8d3 b004 	ldr.w	fp, [r3, #4]
   defe4:	f7ff ffaa 	bl	def3c <_ZN6tflite3ops5micro7pooling12_GLOBAL__N_115CalculateOpDataEPK13TfLiteContextPK16TfLitePoolParamsPK12TfLiteTensorSC_PNS3_6OpDataE.isra.2>
   defe8:	4606      	mov	r6, r0
   defea:	2800      	cmp	r0, #0
   defec:	f040 8085 	bne.w	df0fa <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x13e>

  switch (input->type) {
   deff0:	f81a 0008 	ldrb.w	r0, [sl, r8]
   deff4:	2801      	cmp	r0, #1
   deff6:	fb04 a40b 	mla	r4, r4, fp, sl
   deffa:	d002      	beq.n	df002 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x46>
   deffc:	2803      	cmp	r0, #3
   deffe:	d041      	beq.n	df084 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0xc8>
   df000:	e073      	b.n	df0ea <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x12e>

void MaxEvalFloat(TfLiteContext* context, TfLiteNode* node,
                  TfLitePoolParams* params, OpData* data,
                  const TfLiteTensor* input, TfLiteTensor* output) {
  float activation_min, activation_max;
  CalculateActivationRange(params->activation, &activation_min,
   df002:	7d2b      	ldrb	r3, [r5, #20]
// Calculates the useful range of an activation layer given its activation
// tensor.a
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
   df004:	2b01      	cmp	r3, #1
   df006:	d011      	beq.n	df02c <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x70>
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
   df008:	2b03      	cmp	r3, #3
   df00a:	d012      	beq.n	df032 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x76>
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
    *activation_max = std::numeric_limits<T>::max();
   df00c:	ed9f 7a3d 	vldr	s14, [pc, #244]	; df104 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x148>
   df010:	eddf 6a3d 	vldr	s13, [pc, #244]	; df108 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x14c>
   df014:	2b02      	cmp	r3, #2
   df016:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   df01a:	bf18      	it	ne
   df01c:	eef0 7a47 	vmovne.f32	s15, s14
   df020:	eebf 7a00 	vmov.f32	s14, #240	; 0xbf800000 -1.0
   df024:	bf18      	it	ne
   df026:	eeb0 7a66 	vmovne.f32	s14, s13
   df02a:	e006      	b.n	df03a <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x7e>
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
   df02c:	eddf 7a35 	vldr	s15, [pc, #212]	; df104 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x148>
   df030:	e001      	b.n	df036 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x7a>
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
    *activation_max = 6;
   df032:	eef1 7a08 	vmov.f32	s15, #24	; 0x40c00000  6.0
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
   df036:	ed9f 7a35 	vldr	s14, [pc, #212]	; df10c <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x150>
                           &activation_max);

  tflite::PoolParams op_params;
  op_params.stride_height = params->stride_height;
   df03a:	68ab      	ldr	r3, [r5, #8]
   df03c:	9316      	str	r3, [sp, #88]	; 0x58
  op_params.stride_width = params->stride_width;
   df03e:	686b      	ldr	r3, [r5, #4]
   df040:	9317      	str	r3, [sp, #92]	; 0x5c
  op_params.filter_height = params->filter_height;
   df042:	692b      	ldr	r3, [r5, #16]
   df044:	9318      	str	r3, [sp, #96]	; 0x60
  op_params.filter_width = params->filter_width;
   df046:	68eb      	ldr	r3, [r5, #12]
   df048:	9319      	str	r3, [sp, #100]	; 0x64
  op_params.padding_values.height = data->padding.height;
   df04a:	9b06      	ldr	r3, [sp, #24]
   df04c:	f8ad 3050 	strh.w	r3, [sp, #80]	; 0x50
  op_params.padding_values.width = data->padding.width;
  op_params.float_activation_min = activation_min;
  op_params.float_activation_max = activation_max;
  reference_ops::MaxPool(op_params, GetTensorShape(input),
   df050:	4639      	mov	r1, r7
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.filter_height = params->filter_height;
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
   df052:	9b05      	ldr	r3, [sp, #20]
   df054:	f8ad 304e 	strh.w	r3, [sp, #78]	; 0x4e
  op_params.float_activation_min = activation_min;
  op_params.float_activation_max = activation_max;
  reference_ops::MaxPool(op_params, GetTensorShape(input),
   df058:	a809      	add	r0, sp, #36	; 0x24
  op_params.stride_width = params->stride_width;
  op_params.filter_height = params->filter_height;
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.float_activation_min = activation_min;
   df05a:	ed8d 7a1c 	vstr	s14, [sp, #112]	; 0x70
  op_params.float_activation_max = activation_max;
   df05e:	edcd 7a1d 	vstr	s15, [sp, #116]	; 0x74
  reference_ops::MaxPool(op_params, GetTensorShape(input),
   df062:	f7f7 fca0 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                         GetTensorData<float>(input), GetTensorShape(output),
   df066:	4621      	mov	r1, r4
   df068:	a80e      	add	r0, sp, #56	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   df06a:	687d      	ldr	r5, [r7, #4]
   df06c:	f7f7 fc9b 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   df070:	b104      	cbz	r4, df074 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0xb8>
   df072:	6864      	ldr	r4, [r4, #4]
                         GetTensorData<float>(output));
   df074:	9400      	str	r4, [sp, #0]
   df076:	ab0e      	add	r3, sp, #56	; 0x38
   df078:	462a      	mov	r2, r5
   df07a:	a909      	add	r1, sp, #36	; 0x24
   df07c:	a813      	add	r0, sp, #76	; 0x4c
   df07e:	f7ff fcdb 	bl	dea38 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf>
   df082:	e02b      	b.n	df0dc <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x120>
void MaxEvalQuantizedUInt8(TfLiteContext* context, TfLiteNode* node,
                           TfLitePoolParams* params, OpData* data,
                           const TfLiteTensor* input, TfLiteTensor* output) {
  int32_t activation_min, activation_max;
  CalculateActivationRangeUint8(params->activation, output, &activation_min,
                                &activation_max);
   df084:	7d28      	ldrb	r0, [r5, #20]
   df086:	ab04      	add	r3, sp, #16
   df088:	aa03      	add	r2, sp, #12
   df08a:	4621      	mov	r1, r4
   df08c:	f004 fd6a 	bl	e3b64 <_ZN6tflite29CalculateActivationRangeUint8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>

  tflite::PoolParams op_params;
  op_params.stride_height = params->stride_height;
   df090:	68ab      	ldr	r3, [r5, #8]
   df092:	9316      	str	r3, [sp, #88]	; 0x58
  op_params.stride_width = params->stride_width;
   df094:	686b      	ldr	r3, [r5, #4]
   df096:	9317      	str	r3, [sp, #92]	; 0x5c
  op_params.filter_height = params->filter_height;
   df098:	692b      	ldr	r3, [r5, #16]
   df09a:	9318      	str	r3, [sp, #96]	; 0x60
  op_params.filter_width = params->filter_width;
   df09c:	68eb      	ldr	r3, [r5, #12]
   df09e:	9319      	str	r3, [sp, #100]	; 0x64
  op_params.padding_values.height = data->padding.height;
   df0a0:	9b06      	ldr	r3, [sp, #24]
   df0a2:	f8ad 3050 	strh.w	r3, [sp, #80]	; 0x50
  op_params.padding_values.width = data->padding.width;
   df0a6:	9b05      	ldr	r3, [sp, #20]
   df0a8:	f8ad 304e 	strh.w	r3, [sp, #78]	; 0x4e
  op_params.quantized_activation_min = activation_min;
   df0ac:	9b03      	ldr	r3, [sp, #12]
   df0ae:	931a      	str	r3, [sp, #104]	; 0x68
  op_params.quantized_activation_max = activation_max;
  reference_ops::MaxPool(op_params, GetTensorShape(input),
   df0b0:	4639      	mov	r1, r7
  op_params.filter_height = params->filter_height;
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
   df0b2:	9b04      	ldr	r3, [sp, #16]
   df0b4:	931b      	str	r3, [sp, #108]	; 0x6c
  reference_ops::MaxPool(op_params, GetTensorShape(input),
   df0b6:	a809      	add	r0, sp, #36	; 0x24
   df0b8:	f7f7 fc75 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                         GetTensorData<uint8_t>(input), GetTensorShape(output),
   df0bc:	4621      	mov	r1, r4
   df0be:	a80e      	add	r0, sp, #56	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   df0c0:	687d      	ldr	r5, [r7, #4]
   df0c2:	f7f7 fc70 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   df0c6:	b10c      	cbz	r4, df0cc <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x110>
   df0c8:	6863      	ldr	r3, [r4, #4]
   df0ca:	e000      	b.n	df0ce <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x112>
   df0cc:	4633      	mov	r3, r6
                         GetTensorData<uint8_t>(output));
   df0ce:	9300      	str	r3, [sp, #0]
   df0d0:	462a      	mov	r2, r5
   df0d2:	ab0e      	add	r3, sp, #56	; 0x38
   df0d4:	a909      	add	r1, sp, #36	; 0x24
   df0d6:	a813      	add	r0, sp, #76	; 0x4c
   df0d8:	f7ff fd8e 	bl	debf8 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph>
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
  reference_ops::MaxPool(op_params, GetTensorShape(input),
                         GetTensorData<uint8_t>(input), GetTensorShape(output),
   df0dc:	a80e      	add	r0, sp, #56	; 0x38
   df0de:	f7f7 f9b2 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
  reference_ops::MaxPool(op_params, GetTensorShape(input),
   df0e2:	a809      	add	r0, sp, #36	; 0x24
   df0e4:	f7f7 f9af 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   df0e8:	e008      	b.n	df0fc <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x140>
      break;
    case kTfLiteUInt8:
      MaxEvalQuantizedUInt8(context, node, params, &data, input, output);
      break;
    default:
      context->ReportError(context, "Type %s not currently supported.",
   df0ea:	f8d9 4014 	ldr.w	r4, [r9, #20]
   df0ee:	f7f5 f813 	bl	d4118 <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type));
   df0f2:	4907      	ldr	r1, [pc, #28]	; (df110 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x154>)
   df0f4:	4602      	mov	r2, r0
   df0f6:	4648      	mov	r0, r9
   df0f8:	47a0      	blx	r4
  OpData data;

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, input, output, &data));
   df0fa:	2601      	movs	r6, #1
      context->ReportError(context, "Type %s not currently supported.",
                           TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   df0fc:	4630      	mov	r0, r6
   df0fe:	b01f      	add	sp, #124	; 0x7c
   df100:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   df104:	7f7fffff 	.word	0x7f7fffff
   df108:	ff7fffff 	.word	0xff7fffff
   df10c:	00000000 	.word	0x00000000
   df110:	000e9f92 	.word	0x000e9f92

000df114 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

TfLiteStatus AverageEval(TfLiteContext* context, TfLiteNode* node) {
   df114:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   df118:	680b      	ldr	r3, [r1, #0]
   df11a:	f8d0 a008 	ldr.w	sl, [r0, #8]
   df11e:	685b      	ldr	r3, [r3, #4]
  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);
   df120:	694d      	ldr	r5, [r1, #20]
   df122:	2438      	movs	r4, #56	; 0x38
   df124:	fb04 f803 	mul.w	r8, r4, r3

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

TfLiteStatus AverageEval(TfLiteContext* context, TfLiteNode* node) {
   df128:	b09f      	sub	sp, #124	; 0x7c
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   df12a:	684b      	ldr	r3, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   df12c:	eb0a 0608 	add.w	r6, sl, r8
   df130:	4681      	mov	r9, r0
  OpData data;

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, input, output, &data));
   df132:	aa05      	add	r2, sp, #20
   df134:	4631      	mov	r1, r6
   df136:	4628      	mov	r0, r5
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   df138:	f8d3 b004 	ldr.w	fp, [r3, #4]
   df13c:	f7ff fefe 	bl	def3c <_ZN6tflite3ops5micro7pooling12_GLOBAL__N_115CalculateOpDataEPK13TfLiteContextPK16TfLitePoolParamsPK12TfLiteTensorSC_PNS3_6OpDataE.isra.2>
   df140:	4607      	mov	r7, r0
   df142:	2800      	cmp	r0, #0
   df144:	f040 80b4 	bne.w	df2b0 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x19c>

  // Inputs and outputs share the same type, guarenteed by the converter.
  switch (input->type) {
   df148:	f81a 0008 	ldrb.w	r0, [sl, r8]
   df14c:	2803      	cmp	r0, #3
   df14e:	fb04 a40b 	mla	r4, r4, fp, sl
   df152:	d045      	beq.n	df1e0 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0xcc>
   df154:	2809      	cmp	r0, #9
   df156:	d070      	beq.n	df23a <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x126>
   df158:	2801      	cmp	r0, #1
   df15a:	f040 80a1 	bne.w	df2a0 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x18c>

void AverageEvalFloat(const TfLiteContext* context, const TfLiteNode* node,
                      const TfLitePoolParams* params, const OpData* data,
                      const TfLiteTensor* input, TfLiteTensor* output) {
  float activation_min, activation_max;
  CalculateActivationRange(params->activation, &activation_min,
   df15e:	7d2b      	ldrb	r3, [r5, #20]
// Calculates the useful range of an activation layer given its activation
// tensor.a
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
   df160:	2b01      	cmp	r3, #1
   df162:	d011      	beq.n	df188 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x74>
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
   df164:	2b03      	cmp	r3, #3
   df166:	d012      	beq.n	df18e <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x7a>
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
    *activation_max = std::numeric_limits<T>::max();
   df168:	ed9f 7a54 	vldr	s14, [pc, #336]	; df2bc <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x1a8>
   df16c:	eddf 6a54 	vldr	s13, [pc, #336]	; df2c0 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x1ac>
   df170:	2b02      	cmp	r3, #2
   df172:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   df176:	bf18      	it	ne
   df178:	eef0 7a47 	vmovne.f32	s15, s14
   df17c:	eebf 7a00 	vmov.f32	s14, #240	; 0xbf800000 -1.0
   df180:	bf18      	it	ne
   df182:	eeb0 7a66 	vmovne.f32	s14, s13
   df186:	e006      	b.n	df196 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x82>
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
   df188:	eddf 7a4c 	vldr	s15, [pc, #304]	; df2bc <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x1a8>
   df18c:	e001      	b.n	df192 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x7e>
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
    *activation_max = 6;
   df18e:	eef1 7a08 	vmov.f32	s15, #24	; 0x40c00000  6.0
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
   df192:	ed9f 7a4c 	vldr	s14, [pc, #304]	; df2c4 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x1b0>
                           &activation_max);

  PoolParams op_params;
  op_params.stride_height = params->stride_height;
   df196:	68ab      	ldr	r3, [r5, #8]
   df198:	9316      	str	r3, [sp, #88]	; 0x58
  op_params.stride_width = params->stride_width;
   df19a:	686b      	ldr	r3, [r5, #4]
   df19c:	9317      	str	r3, [sp, #92]	; 0x5c
  op_params.filter_height = params->filter_height;
   df19e:	692b      	ldr	r3, [r5, #16]
   df1a0:	9318      	str	r3, [sp, #96]	; 0x60
  op_params.filter_width = params->filter_width;
   df1a2:	68eb      	ldr	r3, [r5, #12]
   df1a4:	9319      	str	r3, [sp, #100]	; 0x64
  op_params.padding_values.height = data->padding.height;
   df1a6:	9b06      	ldr	r3, [sp, #24]
   df1a8:	f8ad 3050 	strh.w	r3, [sp, #80]	; 0x50
  op_params.padding_values.width = data->padding.width;
  op_params.float_activation_min = activation_min;
  op_params.float_activation_max = activation_max;
  reference_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
   df1ac:	4631      	mov	r1, r6
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.filter_height = params->filter_height;
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
   df1ae:	9b05      	ldr	r3, [sp, #20]
   df1b0:	f8ad 304e 	strh.w	r3, [sp, #78]	; 0x4e
  op_params.float_activation_min = activation_min;
  op_params.float_activation_max = activation_max;
  reference_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
   df1b4:	a809      	add	r0, sp, #36	; 0x24
  op_params.stride_width = params->stride_width;
  op_params.filter_height = params->filter_height;
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.float_activation_min = activation_min;
   df1b6:	ed8d 7a1c 	vstr	s14, [sp, #112]	; 0x70
  op_params.float_activation_max = activation_max;
   df1ba:	edcd 7a1d 	vstr	s15, [sp, #116]	; 0x74
  reference_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
   df1be:	f7f7 fbf2 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(output), GetTensorData<float>(output));
   df1c2:	4621      	mov	r1, r4
   df1c4:	a80e      	add	r0, sp, #56	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   df1c6:	6875      	ldr	r5, [r6, #4]
   df1c8:	f7f7 fbed 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   df1cc:	b104      	cbz	r4, df1d0 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0xbc>
   df1ce:	6864      	ldr	r4, [r4, #4]
   df1d0:	9400      	str	r4, [sp, #0]
   df1d2:	ab0e      	add	r3, sp, #56	; 0x38
   df1d4:	462a      	mov	r2, r5
   df1d6:	a909      	add	r1, sp, #36	; 0x24
   df1d8:	a813      	add	r0, sp, #76	; 0x4c
   df1da:	f7ff fa8b 	bl	de6f4 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf>
   df1de:	e058      	b.n	df292 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x17e>
void AverageEvalUint8(const TfLiteContext* context, const TfLiteNode* node,
                      const TfLitePoolParams* params, const OpData* data,
                      const TfLiteTensor* input, TfLiteTensor* output) {
  int32_t activation_min, activation_max;
  CalculateActivationRangeUint8(params->activation, output, &activation_min,
                                &activation_max);
   df1e0:	7d28      	ldrb	r0, [r5, #20]
   df1e2:	ab04      	add	r3, sp, #16
   df1e4:	aa03      	add	r2, sp, #12
   df1e6:	4621      	mov	r1, r4
   df1e8:	f004 fcbc 	bl	e3b64 <_ZN6tflite29CalculateActivationRangeUint8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>

  PoolParams op_params;
  op_params.stride_height = params->stride_height;
   df1ec:	68ab      	ldr	r3, [r5, #8]
   df1ee:	9316      	str	r3, [sp, #88]	; 0x58
  op_params.stride_width = params->stride_width;
   df1f0:	686b      	ldr	r3, [r5, #4]
   df1f2:	9317      	str	r3, [sp, #92]	; 0x5c
  op_params.filter_height = params->filter_height;
   df1f4:	692b      	ldr	r3, [r5, #16]
   df1f6:	9318      	str	r3, [sp, #96]	; 0x60
  op_params.filter_width = params->filter_width;
   df1f8:	68eb      	ldr	r3, [r5, #12]
   df1fa:	9319      	str	r3, [sp, #100]	; 0x64
  op_params.padding_values.height = data->padding.height;
   df1fc:	9b06      	ldr	r3, [sp, #24]
   df1fe:	f8ad 3050 	strh.w	r3, [sp, #80]	; 0x50
  op_params.padding_values.width = data->padding.width;
   df202:	9b05      	ldr	r3, [sp, #20]
   df204:	f8ad 304e 	strh.w	r3, [sp, #78]	; 0x4e
  op_params.quantized_activation_min = activation_min;
   df208:	9b03      	ldr	r3, [sp, #12]
   df20a:	931a      	str	r3, [sp, #104]	; 0x68
  op_params.quantized_activation_max = activation_max;
  reference_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   df20c:	4631      	mov	r1, r6
  op_params.filter_height = params->filter_height;
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
   df20e:	9b04      	ldr	r3, [sp, #16]
   df210:	931b      	str	r3, [sp, #108]	; 0x6c
  reference_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   df212:	a809      	add	r0, sp, #36	; 0x24
   df214:	f7f7 fbc7 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(output), GetTensorData<uint8_t>(output));
   df218:	4621      	mov	r1, r4
   df21a:	a80e      	add	r0, sp, #56	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   df21c:	6875      	ldr	r5, [r6, #4]
   df21e:	f7f7 fbc2 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   df222:	b10c      	cbz	r4, df228 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x114>
   df224:	6863      	ldr	r3, [r4, #4]
   df226:	e000      	b.n	df22a <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x116>
   df228:	463b      	mov	r3, r7
   df22a:	9300      	str	r3, [sp, #0]
   df22c:	462a      	mov	r2, r5
   df22e:	ab0e      	add	r3, sp, #56	; 0x38
   df230:	a909      	add	r1, sp, #36	; 0x24
   df232:	a813      	add	r0, sp, #76	; 0x4c
   df234:	f7ff fb36 	bl	de8a4 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph>
   df238:	e02b      	b.n	df292 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x17e>
void AverageEvalInt8(const TfLiteContext* context, const TfLiteNode* node,
                     const TfLitePoolParams* params, const OpData* data,
                     const TfLiteTensor* input, TfLiteTensor* output) {
  int32_t activation_min, activation_max;
  CalculateActivationRangeInt8(params->activation, output, &activation_min,
                               &activation_max);
   df23a:	7d28      	ldrb	r0, [r5, #20]
   df23c:	ab04      	add	r3, sp, #16
   df23e:	aa03      	add	r2, sp, #12
   df240:	4621      	mov	r1, r4
   df242:	f004 fd89 	bl	e3d58 <_ZN6tflite28CalculateActivationRangeInt8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>

  PoolParams op_params;
  op_params.stride_height = params->stride_height;
   df246:	68ab      	ldr	r3, [r5, #8]
   df248:	9316      	str	r3, [sp, #88]	; 0x58
  op_params.stride_width = params->stride_width;
   df24a:	686b      	ldr	r3, [r5, #4]
   df24c:	9317      	str	r3, [sp, #92]	; 0x5c
  op_params.filter_height = params->filter_height;
   df24e:	692b      	ldr	r3, [r5, #16]
   df250:	9318      	str	r3, [sp, #96]	; 0x60
  op_params.filter_width = params->filter_width;
   df252:	68eb      	ldr	r3, [r5, #12]
   df254:	9319      	str	r3, [sp, #100]	; 0x64
  op_params.padding_values.height = data->padding.height;
   df256:	9b06      	ldr	r3, [sp, #24]
   df258:	f8ad 3050 	strh.w	r3, [sp, #80]	; 0x50
  op_params.padding_values.width = data->padding.width;
   df25c:	9b05      	ldr	r3, [sp, #20]
   df25e:	f8ad 304e 	strh.w	r3, [sp, #78]	; 0x4e
  op_params.quantized_activation_min = activation_min;
   df262:	9b03      	ldr	r3, [sp, #12]
   df264:	931a      	str	r3, [sp, #104]	; 0x68
  op_params.quantized_activation_max = activation_max;
  reference_integer_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
   df266:	4631      	mov	r1, r6
  op_params.filter_height = params->filter_height;
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
   df268:	9b04      	ldr	r3, [sp, #16]
   df26a:	931b      	str	r3, [sp, #108]	; 0x6c
  reference_integer_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
   df26c:	a809      	add	r0, sp, #36	; 0x24
   df26e:	f7f7 fb9a 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(output), GetTensorData<int8_t>(output));
   df272:	4621      	mov	r1, r4
   df274:	a80e      	add	r0, sp, #56	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   df276:	6875      	ldr	r5, [r6, #4]
   df278:	f7f7 fb95 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   df27c:	b10c      	cbz	r4, df282 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x16e>
   df27e:	6863      	ldr	r3, [r4, #4]
   df280:	e000      	b.n	df284 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x170>
   df282:	463b      	mov	r3, r7
   df284:	9300      	str	r3, [sp, #0]
   df286:	462a      	mov	r2, r5
   df288:	ab0e      	add	r3, sp, #56	; 0x38
   df28a:	a909      	add	r1, sp, #36	; 0x24
   df28c:	a813      	add	r0, sp, #76	; 0x4c
   df28e:	f7ff fd86 	bl	ded9e <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa>
   df292:	a80e      	add	r0, sp, #56	; 0x38
   df294:	f7f7 f8d7 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
  reference_integer_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
   df298:	a809      	add	r0, sp, #36	; 0x24
   df29a:	f7f7 f8d4 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   df29e:	e008      	b.n	df2b2 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x19e>
      break;
    case kTfLiteInt8:
      AverageEvalInt8(context, node, params, &data, input, output);
      break;
    default:
      context->ReportError(context, "Input type %s is not currently supported",
   df2a0:	f8d9 4014 	ldr.w	r4, [r9, #20]
   df2a4:	f7f4 ff38 	bl	d4118 <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type));
   df2a8:	4907      	ldr	r1, [pc, #28]	; (df2c8 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x1b4>)
   df2aa:	4602      	mov	r2, r0
   df2ac:	4648      	mov	r0, r9
   df2ae:	47a0      	blx	r4
  OpData data;

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, input, output, &data));
   df2b0:	2701      	movs	r7, #1
      context->ReportError(context, "Input type %s is not currently supported",
                           TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   df2b2:	4638      	mov	r0, r7
   df2b4:	b01f      	add	sp, #124	; 0x7c
   df2b6:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   df2ba:	bf00      	nop
   df2bc:	7f7fffff 	.word	0x7f7fffff
   df2c0:	ff7fffff 	.word	0xff7fffff
   df2c4:	00000000 	.word	0x00000000
   df2c8:	000e9fb3 	.word	0x000e9fb3

000df2cc <_ZN6tflite3ops5micro24Register_AVERAGE_POOL_2DEv>:
      pooling::Free,
      pooling::Prepare,
      pooling::AverageEval,
  };
  return &r;
}
   df2cc:	4800      	ldr	r0, [pc, #0]	; (df2d0 <_ZN6tflite3ops5micro24Register_AVERAGE_POOL_2DEv+0x4>)
   df2ce:	4770      	bx	lr
   df2d0:	2003c0e8 	.word	0x2003c0e8

000df2d4 <_ZN6tflite3ops5micro20Register_MAX_POOL_2DEv>:

TfLiteRegistration* Register_MAX_POOL_2D() {
  static TfLiteRegistration r = {pooling::Init, pooling::Free, pooling::Prepare,
                                 pooling::MaxEval};
  return &r;
}
   df2d4:	4800      	ldr	r0, [pc, #0]	; (df2d8 <_ZN6tflite3ops5micro20Register_MAX_POOL_2DEv+0x4>)
   df2d6:	4770      	bx	lr
   df2d8:	2003c108 	.word	0x2003c108

000df2dc <_ZN6tflite3ops5micro11activations12PreluPrepareEP13TfLiteContextP10TfLiteNode>:
namespace micro {
namespace activations {

TfLiteStatus PreluPrepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   df2dc:	2000      	movs	r0, #0
   df2de:	4770      	bx	lr

000df2e0 <_ZN6tflite3ops5micro14Register_PRELUEv>:

TfLiteRegistration* Register_PRELU() {
  static TfLiteRegistration r = {nullptr, nullptr, activations::PreluPrepare,
                                 activations::PreluEval};
  return &r;
}
   df2e0:	4800      	ldr	r0, [pc, #0]	; (df2e4 <_ZN6tflite3ops5micro14Register_PRELUEv+0x4>)
   df2e2:	4770      	bx	lr
   df2e4:	2003c128 	.word	0x2003c128

000df2e8 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf>:
}

inline void BroadcastPrelu4DSlowFloat(
    const RuntimeShape& unextended_input1_shape, const float* input1_data,
    const RuntimeShape& unextended_input2_shape, const float* input2_data,
    const RuntimeShape& unextended_output_shape, float* output_data) {
   df2e8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   df2ec:	469b      	mov	fp, r3
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   df2ee:	6803      	ldr	r3, [r0, #0]
}

inline void BroadcastPrelu4DSlowFloat(
    const RuntimeShape& unextended_input1_shape, const float* input1_data,
    const RuntimeShape& unextended_input2_shape, const float* input2_data,
    const RuntimeShape& unextended_output_shape, float* output_data) {
   df2f0:	b09b      	sub	sp, #108	; 0x6c
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   df2f2:	2b04      	cmp	r3, #4
}

inline void BroadcastPrelu4DSlowFloat(
    const RuntimeShape& unextended_input1_shape, const float* input1_data,
    const RuntimeShape& unextended_input2_shape, const float* input2_data,
    const RuntimeShape& unextended_output_shape, float* output_data) {
   df2f4:	4616      	mov	r6, r2
   df2f6:	4604      	mov	r4, r0
   df2f8:	468a      	mov	sl, r1
   df2fa:	9a24      	ldr	r2, [sp, #144]	; 0x90
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   df2fc:	dd01      	ble.n	df302 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x1a>
   df2fe:	f005 f815 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   df302:	6833      	ldr	r3, [r6, #0]
   df304:	2b04      	cmp	r3, #4
   df306:	dcfa      	bgt.n	df2fe <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   df308:	6813      	ldr	r3, [r2, #0]
   df30a:	2b04      	cmp	r3, #4
   df30c:	dcf7      	bgt.n	df2fe <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x16>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
   df30e:	2301      	movs	r3, #1
   df310:	2104      	movs	r1, #4
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);
   df312:	ad0a      	add	r5, sp, #40	; 0x28
   df314:	a805      	add	r0, sp, #20
   df316:	f7f7 f8da 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   df31a:	4620      	mov	r0, r4
   df31c:	ab12      	add	r3, sp, #72	; 0x48
   df31e:	462a      	mov	r2, r5
   df320:	4631      	mov	r1, r6
   df322:	f7f7 fbe9 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   df326:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   df328:	9503      	str	r5, [sp, #12]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   df32a:	2100      	movs	r1, #0
   df32c:	a805      	add	r0, sp, #20
   df32e:	f7f7 f895 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   df332:	4284      	cmp	r4, r0
   df334:	da47      	bge.n	df3c6 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0xde>
   df336:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   df338:	2101      	movs	r1, #1
   df33a:	a805      	add	r0, sp, #20
   df33c:	f7f7 f88e 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   df340:	4285      	cmp	r5, r0
   df342:	da3e      	bge.n	df3c2 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0xda>
   df344:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   df346:	2102      	movs	r1, #2
   df348:	a805      	add	r0, sp, #20
   df34a:	f7f7 f887 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   df34e:	4286      	cmp	r6, r0
   df350:	da35      	bge.n	df3be <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0xd6>
   df352:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   df354:	2103      	movs	r1, #3
   df356:	a805      	add	r0, sp, #20
   df358:	f7f7 f880 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   df35c:	4287      	cmp	r7, r0
   df35e:	da2c      	bge.n	df3ba <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0xd2>
          auto out_idx = Offset(output_shape, b, y, x, c);
   df360:	9700      	str	r7, [sp, #0]
   df362:	4633      	mov	r3, r6
   df364:	462a      	mov	r2, r5
   df366:	4621      	mov	r1, r4
   df368:	a805      	add	r0, sp, #20
   df36a:	f7f7 f8dc 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   df36e:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
   df370:	4680      	mov	r8, r0
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   df372:	4633      	mov	r3, r6
   df374:	462a      	mov	r2, r5
   df376:	4621      	mov	r1, r4
   df378:	9803      	ldr	r0, [sp, #12]
   df37a:	f7f7 f985 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   df37e:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   df380:	4681      	mov	r9, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   df382:	4633      	mov	r3, r6
   df384:	462a      	mov	r2, r5
   df386:	4621      	mov	r1, r4
   df388:	a812      	add	r0, sp, #72	; 0x48
   df38a:	f7f7 f97d 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
   df38e:	eb0a 0989 	add.w	r9, sl, r9, lsl #2
   df392:	edd9 7a00 	vldr	s15, [r9]
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = in1_val >= 0.0 ? in1_val : in1_val * in2_val;
   df396:	9b25      	ldr	r3, [sp, #148]	; 0x94
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
   df398:	eb0b 0080 	add.w	r0, fp, r0, lsl #2
   df39c:	ed90 7a00 	vldr	s14, [r0]
          output_data[out_idx] = in1_val >= 0.0 ? in1_val : in1_val * in2_val;
   df3a0:	eef5 7ac0 	vcmpe.f32	s15, #0.0
   df3a4:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   df3a8:	bfb8      	it	lt
   df3aa:	ee67 7a87 	vmullt.f32	s15, s15, s14
   df3ae:	eb03 0888 	add.w	r8, r3, r8, lsl #2
   df3b2:	edc8 7a00 	vstr	s15, [r8]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   df3b6:	3701      	adds	r7, #1
   df3b8:	e7cc      	b.n	df354 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x6c>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   df3ba:	3601      	adds	r6, #1
   df3bc:	e7c3      	b.n	df346 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x5e>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   df3be:	3501      	adds	r5, #1
   df3c0:	e7ba      	b.n	df338 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x50>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   df3c2:	3401      	adds	r4, #1
   df3c4:	e7b1      	b.n	df32a <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x42>
    const RuntimeShape& unextended_output_shape, float* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   df3c6:	a805      	add	r0, sp, #20
   df3c8:	f7f7 f83d 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = in1_val >= 0.0 ? in1_val : in1_val * in2_val;
        }
      }
    }
  }
}
   df3cc:	b01b      	add	sp, #108	; 0x6c
   df3ce:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

000df3d4 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph>:
                                 const RuntimeShape& input_shape,
                                 const uint8* input_data,
                                 const RuntimeShape& alpha_shape,
                                 const uint8* alpha_data,
                                 const RuntimeShape& output_shape,
                                 uint8* output_data) {
   df3d4:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   df3d8:	461d      	mov	r5, r3
  TFLITE_DCHECK_LE(input_shape.DimensionsCount(), 4);
   df3da:	680b      	ldr	r3, [r1, #0]
                                 const RuntimeShape& input_shape,
                                 const uint8* input_data,
                                 const RuntimeShape& alpha_shape,
                                 const uint8* alpha_data,
                                 const RuntimeShape& output_shape,
                                 uint8* output_data) {
   df3dc:	b09d      	sub	sp, #116	; 0x74
  TFLITE_DCHECK_LE(input_shape.DimensionsCount(), 4);
   df3de:	2b04      	cmp	r3, #4
                                 const RuntimeShape& input_shape,
                                 const uint8* input_data,
                                 const RuntimeShape& alpha_shape,
                                 const uint8* alpha_data,
                                 const RuntimeShape& output_shape,
                                 uint8* output_data) {
   df3e0:	9204      	str	r2, [sp, #16]
   df3e2:	4681      	mov	r9, r0
   df3e4:	460c      	mov	r4, r1
   df3e6:	9a27      	ldr	r2, [sp, #156]	; 0x9c
  TFLITE_DCHECK_LE(input_shape.DimensionsCount(), 4);
   df3e8:	dd01      	ble.n	df3ee <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1a>
   df3ea:	f004 ff9f 	bl	e432c <abort>
  TFLITE_DCHECK_LE(alpha_shape.DimensionsCount(), 4);
   df3ee:	682b      	ldr	r3, [r5, #0]
   df3f0:	2b04      	cmp	r3, #4
   df3f2:	dcfa      	bgt.n	df3ea <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x16>
  TFLITE_DCHECK_LE(output_shape.DimensionsCount(), 4);
   df3f4:	6813      	ldr	r3, [r2, #0]
   df3f6:	2b04      	cmp	r3, #4
   df3f8:	dcf7      	bgt.n	df3ea <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x16>
   df3fa:	2301      	movs	r3, #1
   df3fc:	2104      	movs	r1, #4
   df3fe:	a807      	add	r0, sp, #28
   df400:	f7f7 f865 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  const RuntimeShape extended_output_shape =
      RuntimeShape::ExtendedShape(4, output_shape);
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input_shape, alpha_shape, &desc1, &desc2);
   df404:	4629      	mov	r1, r5
   df406:	ab14      	add	r3, sp, #80	; 0x50
   df408:	aa0c      	add	r2, sp, #48	; 0x30
   df40a:	4620      	mov	r0, r4
   df40c:	f7f7 fb74 	bl	d6af8 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>

  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   df410:	2500      	movs	r5, #0
   df412:	2100      	movs	r1, #0
   df414:	a807      	add	r0, sp, #28
   df416:	f7f7 f821 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   df41a:	4285      	cmp	r5, r0
   df41c:	f280 80ac 	bge.w	df578 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1a4>
   df420:	2600      	movs	r6, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
   df422:	2101      	movs	r1, #1
   df424:	a807      	add	r0, sp, #28
   df426:	f7f7 f819 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   df42a:	4286      	cmp	r6, r0
   df42c:	f280 80a2 	bge.w	df574 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1a0>
   df430:	2700      	movs	r7, #0
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
   df432:	f10d 0a1c 	add.w	sl, sp, #28
   df436:	2102      	movs	r1, #2
   df438:	4650      	mov	r0, sl
   df43a:	f7f7 f80f 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   df43e:	4287      	cmp	r7, r0
   df440:	f280 8096 	bge.w	df570 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x19c>
   df444:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   df448:	2103      	movs	r1, #3
   df44a:	4650      	mov	r0, sl
   df44c:	f7f7 f806 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   df450:	4580      	cmp	r8, r0
   df452:	f280 808b 	bge.w	df56c <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x198>
          int output_index = Offset(extended_output_shape, b, y, x, c);
   df456:	463b      	mov	r3, r7
   df458:	4632      	mov	r2, r6
   df45a:	4629      	mov	r1, r5
   df45c:	f8cd 8000 	str.w	r8, [sp]
   df460:	4650      	mov	r0, sl
   df462:	f7f7 f860 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          int input_index = SubscriptToIndex(desc1, b, y, x, c);
   df466:	463b      	mov	r3, r7

  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          int output_index = Offset(extended_output_shape, b, y, x, c);
   df468:	4683      	mov	fp, r0
          int input_index = SubscriptToIndex(desc1, b, y, x, c);
   df46a:	f8cd 8000 	str.w	r8, [sp]
   df46e:	4632      	mov	r2, r6
   df470:	4629      	mov	r1, r5
   df472:	a80c      	add	r0, sp, #48	; 0x30
   df474:	f7f7 f908 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 input_value =
              params.input_offset + input_data[input_index];
   df478:	9b04      	ldr	r3, [sp, #16]
   df47a:	f8d9 4000 	ldr.w	r4, [r9]
   df47e:	5c1b      	ldrb	r3, [r3, r0]
          if (input_value >= 0) {
   df480:	191c      	adds	r4, r3, r4
   df482:	d403      	bmi.n	df48c <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0xb8>
            output_data[output_index] = input_data[input_index];
   df484:	9a28      	ldr	r2, [sp, #160]	; 0xa0
   df486:	f802 300b 	strb.w	r3, [r2, fp]
   df48a:	e06c      	b.n	df566 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x192>
          } else {
            auto alpha_index = SubscriptToIndex(desc2, b, y, x, c);
   df48c:	463b      	mov	r3, r7
   df48e:	4632      	mov	r2, r6
   df490:	f8cd 8000 	str.w	r8, [sp]
   df494:	4629      	mov	r1, r5
   df496:	a814      	add	r0, sp, #80	; 0x50
   df498:	f7f7 f8f6 	bl	d6688 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
            const int32 alpha_value =
                params.alpha_offset + alpha_data[alpha_index];
            const int32 unclamped_output =
                params.output_offset +
   df49c:	f8d9 3008 	ldr.w	r3, [r9, #8]
   df4a0:	9303      	str	r3, [sp, #12]
                MultiplyByQuantizedMultiplierSmallerThanOneExp(
   df4a2:	9b26      	ldr	r3, [sp, #152]	; 0x98
                    input_value * alpha_value, params.output_multiplier,
                    params.output_shift);
   df4a4:	f8d9 e010 	ldr.w	lr, [r9, #16]
            auto alpha_index = SubscriptToIndex(desc2, b, y, x, c);
            const int32 alpha_value =
                params.alpha_offset + alpha_data[alpha_index];
            const int32 unclamped_output =
                params.output_offset +
                MultiplyByQuantizedMultiplierSmallerThanOneExp(
   df4a8:	5c1a      	ldrb	r2, [r3, r0]
   df4aa:	f8d9 3004 	ldr.w	r3, [r9, #4]
   df4ae:	441a      	add	r2, r3
                    input_value * alpha_value, params.output_multiplier,
   df4b0:	f8d9 300c 	ldr.w	r3, [r9, #12]
            auto alpha_index = SubscriptToIndex(desc2, b, y, x, c);
            const int32 alpha_value =
                params.alpha_offset + alpha_data[alpha_index];
            const int32 unclamped_output =
                params.output_offset +
                MultiplyByQuantizedMultiplierSmallerThanOneExp(
   df4b4:	4362      	muls	r2, r4
// This function implements the same computation as the ARMv7 NEON VQRDMULH
// instruction.
template <>
inline std::int32_t SaturatingRoundingDoublingHighMul(std::int32_t a,
                                                      std::int32_t b) {
  bool overflow = a == b && a == std::numeric_limits<std::int32_t>::min();
   df4b6:	429a      	cmp	r2, r3
   df4b8:	d104      	bne.n	df4c4 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0xf0>
   df4ba:	f102 4100 	add.w	r1, r2, #2147483648	; 0x80000000
   df4be:	424c      	negs	r4, r1
   df4c0:	414c      	adcs	r4, r1
   df4c2:	e000      	b.n	df4c6 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0xf2>
   df4c4:	2400      	movs	r4, #0
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
   df4c6:	fb82 2303 	smull	r2, r3, r2, r3
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
   df4ca:	2a00      	cmp	r2, #0
   df4cc:	f173 0100 	sbcs.w	r1, r3, #0
   df4d0:	482c      	ldr	r0, [pc, #176]	; (df584 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1b0>)
   df4d2:	bfa8      	it	ge
   df4d4:	f04f 4080 	movge.w	r0, #1073741824	; 0x40000000
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
   df4d8:	b994      	cbnz	r4, df500 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x12c>
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
   df4da:	1812      	adds	r2, r2, r0
   df4dc:	eb43 73e0 	adc.w	r3, r3, r0, asr #31
   df4e0:	2a00      	cmp	r2, #0
   df4e2:	f173 0100 	sbcs.w	r1, r3, #0
   df4e6:	da07      	bge.n	df4f8 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x124>
   df4e8:	f06f 4000 	mvn.w	r0, #2147483648	; 0x80000000
   df4ec:	1880      	adds	r0, r0, r2
   df4ee:	f04f 0100 	mov.w	r1, #0
   df4f2:	4159      	adcs	r1, r3
   df4f4:	4602      	mov	r2, r0
   df4f6:	460b      	mov	r3, r1
   df4f8:	0fd4      	lsrs	r4, r2, #31
   df4fa:	ea44 0443 	orr.w	r4, r4, r3, lsl #1
   df4fe:	e001      	b.n	df504 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x130>
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
   df500:	f06f 4400 	mvn.w	r4, #2147483648	; 0x80000000

inline int32 MultiplyByQuantizedMultiplierSmallerThanOneExp(
    int32 x, int32 quantized_multiplier, int left_shift) {
  using gemmlowp::RoundingDivideByPOT;
  using gemmlowp::SaturatingRoundingDoublingHighMul;
  return RoundingDivideByPOT(
   df504:	f1ce 0300 	rsb	r3, lr, #0

// Correctly-rounded-to-nearest division by a power-of-two.
// Also known as a rounding arithmetic right shift.
template <typename IntegerType>
inline IntegerType RoundingDivideByPOT(IntegerType x, int exponent) {
  assert(exponent >= 0);
   df508:	2b00      	cmp	r3, #0
   df50a:	da04      	bge.n	df516 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x142>
   df50c:	4b1e      	ldr	r3, [pc, #120]	; (df588 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1b4>)
   df50e:	4a1f      	ldr	r2, [pc, #124]	; (df58c <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1b8>)
   df510:	f44f 71b3 	mov.w	r1, #358	; 0x166
   df514:	e005      	b.n	df522 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x14e>
  assert(exponent <= 31);
   df516:	2b1f      	cmp	r3, #31
   df518:	dd06      	ble.n	df528 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x154>
   df51a:	4b1d      	ldr	r3, [pc, #116]	; (df590 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1bc>)
   df51c:	4a1b      	ldr	r2, [pc, #108]	; (df58c <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1b8>)
   df51e:	f240 1167 	movw	r1, #359	; 0x167
   df522:	481c      	ldr	r0, [pc, #112]	; (df594 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1c0>)
   df524:	f004 ff12 	bl	e434c <__assert_func>
  const IntegerType mask = Dup<IntegerType>((1ll << exponent) - 1);
   df528:	461a      	mov	r2, r3
   df52a:	2001      	movs	r0, #1
   df52c:	2100      	movs	r1, #0
   df52e:	9305      	str	r3, [sp, #20]
   df530:	f007 fbf4 	bl	e6d1c <__aeabi_llsl>
          } else {
            auto alpha_index = SubscriptToIndex(desc2, b, y, x, c);
            const int32 alpha_value =
                params.alpha_offset + alpha_data[alpha_index];
            const int32 unclamped_output =
                params.output_offset +
   df534:	9b05      	ldr	r3, [sp, #20]
   df536:	3801      	subs	r0, #1
   df538:	ea00 0204 	and.w	r2, r0, r4
   df53c:	1040      	asrs	r0, r0, #1
   df53e:	fa44 f303 	asr.w	r3, r4, r3
   df542:	eb00 70d4 	add.w	r0, r0, r4, lsr #31
   df546:	4282      	cmp	r2, r0
   df548:	bfd4      	ite	le
   df54a:	461c      	movle	r4, r3
   df54c:	1c5c      	addgt	r4, r3, #1
   df54e:	9b03      	ldr	r3, [sp, #12]
   df550:	441c      	add	r4, r3
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   df552:	2c00      	cmp	r4, #0
   df554:	dd03      	ble.n	df55e <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x18a>
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
   df556:	2cfe      	cmp	r4, #254	; 0xfe
   df558:	dd02      	ble.n	df560 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x18c>
	return __b;
      return __a;
   df55a:	24ff      	movs	r4, #255	; 0xff
   df55c:	e000      	b.n	df560 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x18c>
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
	return __b;
      return __a;
   df55e:	2400      	movs	r4, #0
                    params.output_shift);
            const int32 quantized_min = std::numeric_limits<uint8_t>::min();
            const int32 quantized_max = std::numeric_limits<uint8_t>::max();
            const int32 clamped_output = std::min(
                quantized_max, std::max(quantized_min, unclamped_output));
            output_data[output_index] = static_cast<uint8>(clamped_output);
   df560:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   df562:	f803 400b 	strb.w	r4, [r3, fp]
  NdArrayDescsForElementwiseBroadcast(input_shape, alpha_shape, &desc1, &desc2);

  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   df566:	f108 0801 	add.w	r8, r8, #1
   df56a:	e76d      	b.n	df448 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x74>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input_shape, alpha_shape, &desc1, &desc2);

  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
   df56c:	3701      	adds	r7, #1
   df56e:	e760      	b.n	df432 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x5e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input_shape, alpha_shape, &desc1, &desc2);

  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
   df570:	3601      	adds	r6, #1
   df572:	e756      	b.n	df422 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x4e>
      RuntimeShape::ExtendedShape(4, output_shape);
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input_shape, alpha_shape, &desc1, &desc2);

  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   df574:	3501      	adds	r5, #1
   df576:	e74c      	b.n	df412 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x3e>
                                 uint8* output_data) {
  TFLITE_DCHECK_LE(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(alpha_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(output_shape.DimensionsCount(), 4);
  const RuntimeShape extended_output_shape =
      RuntimeShape::ExtendedShape(4, output_shape);
   df578:	a807      	add	r0, sp, #28
   df57a:	f7f6 ff64 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
          }
        }
      }
    }
  }
}
   df57e:	b01d      	add	sp, #116	; 0x74
   df580:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   df584:	c0000001 	.word	0xc0000001
   df588:	000e9684 	.word	0x000e9684
   df58c:	000ea014 	.word	0x000ea014
   df590:	000e9731 	.word	0x000e9731
   df594:	000e9692 	.word	0x000e9692

000df598 <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus PreluEval(TfLiteContext* context, TfLiteNode* node) {
   df598:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
   df59c:	680b      	ldr	r3, [r1, #0]
   df59e:	6887      	ldr	r7, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   df5a0:	689c      	ldr	r4, [r3, #8]
   df5a2:	4681      	mov	r9, r0
   df5a4:	6858      	ldr	r0, [r3, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   df5a6:	684b      	ldr	r3, [r1, #4]
   df5a8:	685b      	ldr	r3, [r3, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   df5aa:	2238      	movs	r2, #56	; 0x38
   df5ac:	b09b      	sub	sp, #108	; 0x6c
   df5ae:	fb02 f800 	mul.w	r8, r2, r0
   df5b2:	fb02 7404 	mla	r4, r2, r4, r7
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   df5b6:	435a      	muls	r2, r3
  const TfLiteTensor* input = GetInput(context, node, 0);
  const TfLiteTensor* alpha = GetInput(context, node, 1);
  TfLiteTensor* output = GetOutput(context, node, 0);
  int32_t output_multiplier = 0;
   df5b8:	2300      	movs	r3, #0
   df5ba:	9304      	str	r3, [sp, #16]
  int output_shift = 0;
   df5bc:	9305      	str	r3, [sp, #20]
  if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt16) {
   df5be:	5cbb      	ldrb	r3, [r7, r2]
   df5c0:	f003 03fb 	and.w	r3, r3, #251	; 0xfb
   df5c4:	2b03      	cmp	r3, #3
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   df5c6:	eb07 0608 	add.w	r6, r7, r8
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   df5ca:	eb07 0502 	add.w	r5, r7, r2
   df5ce:	d113      	bne.n	df5f8 <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0x60>
    double real_multiplier =
        input->params.scale * alpha->params.scale / output->params.scale;
    QuantizeMultiplierSmallerThanOneExp(real_multiplier, &output_multiplier,
                                        &output_shift);
   df5d0:	ed94 7a03 	vldr	s14, [r4, #12]
   df5d4:	edd6 7a03 	vldr	s15, [r6, #12]
   df5d8:	ee67 7a87 	vmul.f32	s15, s15, s14
   df5dc:	ed95 7a03 	vldr	s14, [r5, #12]
   df5e0:	eec7 6a87 	vdiv.f32	s13, s15, s14
   df5e4:	ee16 0a90 	vmov	r0, s13
   df5e8:	f007 fd08 	bl	e6ffc <__aeabi_f2d>
   df5ec:	ec41 0b10 	vmov	d0, r0, r1
   df5f0:	a905      	add	r1, sp, #20
   df5f2:	a804      	add	r0, sp, #16
   df5f4:	f004 fc30 	bl	e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
  }
  switch (input->type) {
   df5f8:	f817 0008 	ldrb.w	r0, [r7, r8]
   df5fc:	2801      	cmp	r0, #1
   df5fe:	d028      	beq.n	df652 <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0xba>
   df600:	2803      	cmp	r0, #3
   df602:	d14a      	bne.n	df69a <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0x102>
          GetTensorShape(output), GetTensorData<float>(output));
      return kTfLiteOk;
    } break;
    case kTfLiteUInt8: {
      PreluParams op_params;
      op_params.input_offset = -input->params.zero_point;
   df604:	6933      	ldr	r3, [r6, #16]
   df606:	425b      	negs	r3, r3
   df608:	9306      	str	r3, [sp, #24]
      op_params.alpha_offset = -alpha->params.zero_point;
   df60a:	6923      	ldr	r3, [r4, #16]
   df60c:	425b      	negs	r3, r3
   df60e:	9307      	str	r3, [sp, #28]
      op_params.output_offset = output->params.zero_point;
   df610:	692b      	ldr	r3, [r5, #16]
   df612:	9308      	str	r3, [sp, #32]
      op_params.output_multiplier = output_multiplier;
   df614:	9b04      	ldr	r3, [sp, #16]
   df616:	9309      	str	r3, [sp, #36]	; 0x24
      op_params.output_shift = output_shift;
      reference_ops::BroadcastPrelu4DSlow(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   df618:	4631      	mov	r1, r6
      PreluParams op_params;
      op_params.input_offset = -input->params.zero_point;
      op_params.alpha_offset = -alpha->params.zero_point;
      op_params.output_offset = output->params.zero_point;
      op_params.output_multiplier = output_multiplier;
      op_params.output_shift = output_shift;
   df61a:	9b05      	ldr	r3, [sp, #20]
   df61c:	930a      	str	r3, [sp, #40]	; 0x28
      reference_ops::BroadcastPrelu4DSlow(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   df61e:	a80b      	add	r0, sp, #44	; 0x2c
   df620:	f7f7 f9c1 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
          GetTensorShape(alpha), GetTensorData<uint8_t>(alpha),
   df624:	4621      	mov	r1, r4
   df626:	a810      	add	r0, sp, #64	; 0x40
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   df628:	6876      	ldr	r6, [r6, #4]
   df62a:	f7f7 f9bc 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   df62e:	6867      	ldr	r7, [r4, #4]
          GetTensorShape(output), GetTensorData<uint8_t>(output));
   df630:	ac15      	add	r4, sp, #84	; 0x54
   df632:	4629      	mov	r1, r5
   df634:	4620      	mov	r0, r4
   df636:	f7f7 f9b6 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   df63a:	686b      	ldr	r3, [r5, #4]
   df63c:	9302      	str	r3, [sp, #8]
   df63e:	a806      	add	r0, sp, #24
   df640:	9401      	str	r4, [sp, #4]
   df642:	9700      	str	r7, [sp, #0]
   df644:	ab10      	add	r3, sp, #64	; 0x40
   df646:	4632      	mov	r2, r6
   df648:	a90b      	add	r1, sp, #44	; 0x2c
   df64a:	f7ff fec3 	bl	df3d4 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph>
   df64e:	4620      	mov	r0, r4
   df650:	e019      	b.n	df686 <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0xee>
                                        &output_shift);
  }
  switch (input->type) {
    case kTfLiteFloat32: {
      BroadcastPrelu4DSlowFloat(
          GetTensorShape(input), GetTensorData<float>(input),
   df652:	4631      	mov	r1, r6
   df654:	a80b      	add	r0, sp, #44	; 0x2c
   df656:	f7f7 f9a6 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
          GetTensorShape(alpha), GetTensorData<float>(alpha),
   df65a:	4621      	mov	r1, r4
   df65c:	a810      	add	r0, sp, #64	; 0x40
   df65e:	6877      	ldr	r7, [r6, #4]
   df660:	f7f7 f9a1 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   df664:	b104      	cbz	r4, df668 <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0xd0>
   df666:	6864      	ldr	r4, [r4, #4]
          GetTensorShape(output), GetTensorData<float>(output));
   df668:	ae15      	add	r6, sp, #84	; 0x54
   df66a:	4629      	mov	r1, r5
   df66c:	4630      	mov	r0, r6
   df66e:	f7f7 f99a 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   df672:	686b      	ldr	r3, [r5, #4]
   df674:	9301      	str	r3, [sp, #4]
   df676:	a80b      	add	r0, sp, #44	; 0x2c
   df678:	9600      	str	r6, [sp, #0]
   df67a:	4623      	mov	r3, r4
   df67c:	aa10      	add	r2, sp, #64	; 0x40
   df67e:	4639      	mov	r1, r7
   df680:	f7ff fe32 	bl	df2e8 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf>
   df684:	4630      	mov	r0, r6
      op_params.output_multiplier = output_multiplier;
      op_params.output_shift = output_shift;
      reference_ops::BroadcastPrelu4DSlow(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(alpha), GetTensorData<uint8_t>(alpha),
          GetTensorShape(output), GetTensorData<uint8_t>(output));
   df686:	f7f6 fede 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
      op_params.output_offset = output->params.zero_point;
      op_params.output_multiplier = output_multiplier;
      op_params.output_shift = output_shift;
      reference_ops::BroadcastPrelu4DSlow(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(alpha), GetTensorData<uint8_t>(alpha),
   df68a:	a810      	add	r0, sp, #64	; 0x40
   df68c:	f7f6 fedb 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
      op_params.alpha_offset = -alpha->params.zero_point;
      op_params.output_offset = output->params.zero_point;
      op_params.output_multiplier = output_multiplier;
      op_params.output_shift = output_shift;
      reference_ops::BroadcastPrelu4DSlow(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   df690:	a80b      	add	r0, sp, #44	; 0x2c
   df692:	f7f6 fed8 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
          GetTensorShape(alpha), GetTensorData<uint8_t>(alpha),
          GetTensorShape(output), GetTensorData<uint8_t>(output));
      return kTfLiteOk;
   df696:	2000      	movs	r0, #0
   df698:	e008      	b.n	df6ac <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0x114>
    } break;
    default:
      context->ReportError(
   df69a:	f8d9 4014 	ldr.w	r4, [r9, #20]
   df69e:	f7f4 fd3b 	bl	d4118 <TfLiteTypeGetName>
          context, "Only float32 and uint8 are supported currently, got %d.",
          TfLiteTypeGetName(input->type));
   df6a2:	4904      	ldr	r1, [pc, #16]	; (df6b4 <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0x11c>)
   df6a4:	4602      	mov	r2, r0
   df6a6:	4648      	mov	r0, r9
   df6a8:	47a0      	blx	r4
      return kTfLiteError;
   df6aa:	2001      	movs	r0, #1
  }
}
   df6ac:	b01b      	add	sp, #108	; 0x6c
   df6ae:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}
   df6b2:	bf00      	nop
   df6b4:	000e9fdc 	.word	0x000e9fdc

000df6b8 <_ZN6tflite3ops5micro8quantize4InitEP13TfLiteContextPKcj>:
namespace micro {
namespace quantize {

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
   df6b8:	2000      	movs	r0, #0
   df6ba:	4770      	bx	lr

000df6bc <_ZN6tflite3ops5micro8quantize4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
   df6bc:	4770      	bx	lr
	...

000df6c0 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   df6c0:	b5f0      	push	{r4, r5, r6, r7, lr}
   df6c2:	680f      	ldr	r7, [r1, #0]
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
   df6c4:	683c      	ldr	r4, [r7, #0]
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   df6c6:	2c01      	cmp	r4, #1
  return nullptr;
}

void Free(TfLiteContext* context, void* buffer) {}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   df6c8:	b085      	sub	sp, #20
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   df6ca:	d009      	beq.n	df6e0 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x20>
   df6cc:	4a32      	ldr	r2, [pc, #200]	; (df798 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xd8>)
   df6ce:	9201      	str	r2, [sp, #4]
   df6d0:	2501      	movs	r5, #1
   df6d2:	4a32      	ldr	r2, [pc, #200]	; (df79c <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xdc>)
   df6d4:	9503      	str	r5, [sp, #12]
   df6d6:	9402      	str	r4, [sp, #8]
   df6d8:	9200      	str	r2, [sp, #0]
   df6da:	6944      	ldr	r4, [r0, #20]
   df6dc:	2322      	movs	r3, #34	; 0x22
   df6de:	e021      	b.n	df724 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x64>
   df6e0:	684a      	ldr	r2, [r1, #4]
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
   df6e2:	6815      	ldr	r5, [r2, #0]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
   df6e4:	2d01      	cmp	r5, #1
   df6e6:	d00b      	beq.n	df700 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x40>
   df6e8:	4a2b      	ldr	r2, [pc, #172]	; (df798 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xd8>)
   df6ea:	9201      	str	r2, [sp, #4]
   df6ec:	4a2c      	ldr	r2, [pc, #176]	; (df7a0 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe0>)
   df6ee:	9200      	str	r2, [sp, #0]
   df6f0:	9403      	str	r4, [sp, #12]
   df6f2:	9502      	str	r5, [sp, #8]
   df6f4:	6945      	ldr	r5, [r0, #20]
   df6f6:	4a2b      	ldr	r2, [pc, #172]	; (df7a4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe4>)
   df6f8:	492b      	ldr	r1, [pc, #172]	; (df7a8 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe8>)
   df6fa:	2323      	movs	r3, #35	; 0x23
   df6fc:	47a8      	blx	r5
   df6fe:	e046      	b.n	df78e <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xce>

  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
   df700:	6852      	ldr	r2, [r2, #4]

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);

  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
   df702:	6886      	ldr	r6, [r0, #8]
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
   df704:	2138      	movs	r1, #56	; 0x38
   df706:	434a      	muls	r2, r1
   df708:	eb06 0e02 	add.w	lr, r6, r2

  // TODO(b/128934713): Add support for fixed-point per-channel quantization.
  // Currently this only support affine per-layer quantization.
  TF_LITE_ENSURE_EQ(context, output->quantization.type,
   df70c:	f89e 4030 	ldrb.w	r4, [lr, #48]	; 0x30
   df710:	2c01      	cmp	r4, #1
   df712:	d00c      	beq.n	df72e <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x6e>
   df714:	4a25      	ldr	r2, [pc, #148]	; (df7ac <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xec>)
   df716:	9201      	str	r2, [sp, #4]
   df718:	4a25      	ldr	r2, [pc, #148]	; (df7b0 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xf0>)
   df71a:	9503      	str	r5, [sp, #12]
   df71c:	9402      	str	r4, [sp, #8]
   df71e:	9200      	str	r2, [sp, #0]
   df720:	6944      	ldr	r4, [r0, #20]
   df722:	232b      	movs	r3, #43	; 0x2b
   df724:	4a1f      	ldr	r2, [pc, #124]	; (df7a4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe4>)
   df726:	4920      	ldr	r1, [pc, #128]	; (df7a8 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe8>)
   df728:	47a0      	blx	r4
   df72a:	4628      	mov	r0, r5
   df72c:	e032      	b.n	df794 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xd4>
                    kTfLiteAffineQuantization);
  const auto* affine_quantization =
      reinterpret_cast<TfLiteAffineQuantization*>(output->quantization.params);
   df72e:	f8de 5034 	ldr.w	r5, [lr, #52]	; 0x34
  TF_LITE_ENSURE(context, affine_quantization);
   df732:	b925      	cbnz	r5, df73e <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x7e>
   df734:	4a1f      	ldr	r2, [pc, #124]	; (df7b4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xf4>)
   df736:	9200      	str	r2, [sp, #0]
   df738:	6945      	ldr	r5, [r0, #20]
   df73a:	232e      	movs	r3, #46	; 0x2e
   df73c:	e024      	b.n	df788 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xc8>
  TF_LITE_ENSURE(context, affine_quantization->scale);
   df73e:	682d      	ldr	r5, [r5, #0]
   df740:	b925      	cbnz	r5, df74c <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x8c>
   df742:	4a1d      	ldr	r2, [pc, #116]	; (df7b8 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xf8>)
   df744:	9200      	str	r2, [sp, #0]
   df746:	6945      	ldr	r5, [r0, #20]
   df748:	232f      	movs	r3, #47	; 0x2f
   df74a:	e01d      	b.n	df788 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xc8>
  TF_LITE_ENSURE(context, affine_quantization->scale->size == 1);
   df74c:	682d      	ldr	r5, [r5, #0]
   df74e:	2d01      	cmp	r5, #1
   df750:	d004      	beq.n	df75c <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x9c>
   df752:	4a1a      	ldr	r2, [pc, #104]	; (df7bc <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xfc>)
   df754:	9200      	str	r2, [sp, #0]
   df756:	6945      	ldr	r5, [r0, #20]
   df758:	2330      	movs	r3, #48	; 0x30
   df75a:	e015      	b.n	df788 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xc8>

  TF_LITE_ENSURE(context, input->type == kTfLiteFloat32);
   df75c:	687c      	ldr	r4, [r7, #4]
   df75e:	4361      	muls	r1, r4
   df760:	5c74      	ldrb	r4, [r6, r1]
   df762:	2c01      	cmp	r4, #1
   df764:	d007      	beq.n	df776 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xb6>
   df766:	4a16      	ldr	r2, [pc, #88]	; (df7c0 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x100>)
   df768:	9200      	str	r2, [sp, #0]
   df76a:	6944      	ldr	r4, [r0, #20]
   df76c:	4a0d      	ldr	r2, [pc, #52]	; (df7a4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe4>)
   df76e:	4915      	ldr	r1, [pc, #84]	; (df7c4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x104>)
   df770:	2332      	movs	r3, #50	; 0x32
   df772:	47a0      	blx	r4
   df774:	e7d9      	b.n	df72a <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x6a>
  TF_LITE_ENSURE(context,
   df776:	5cb2      	ldrb	r2, [r6, r2]
   df778:	2a03      	cmp	r2, #3
   df77a:	d00a      	beq.n	df792 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xd2>
   df77c:	2a09      	cmp	r2, #9
   df77e:	d008      	beq.n	df792 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xd2>
   df780:	4a11      	ldr	r2, [pc, #68]	; (df7c8 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x108>)
   df782:	9200      	str	r2, [sp, #0]
   df784:	6945      	ldr	r5, [r0, #20]
   df786:	2334      	movs	r3, #52	; 0x34
   df788:	4a06      	ldr	r2, [pc, #24]	; (df7a4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe4>)
   df78a:	490e      	ldr	r1, [pc, #56]	; (df7c4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x104>)
   df78c:	47a8      	blx	r5
   df78e:	4620      	mov	r0, r4
   df790:	e000      	b.n	df794 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xd4>
                 output->type == kTfLiteUInt8 || output->type == kTfLiteInt8);

  return kTfLiteOk;
   df792:	2000      	movs	r0, #0
}
   df794:	b005      	add	sp, #20
   df796:	bdf0      	pop	{r4, r5, r6, r7, pc}
   df798:	000eb2c5 	.word	0x000eb2c5
   df79c:	000e9912 	.word	0x000e9912
   df7a0:	000e9922 	.word	0x000e9922
   df7a4:	000ea06e 	.word	0x000ea06e
   df7a8:	000e98f8 	.word	0x000e98f8
   df7ac:	000e9b17 	.word	0x000e9b17
   df7b0:	000ea118 	.word	0x000ea118
   df7b4:	000e9b4b 	.word	0x000e9b4b
   df7b8:	000e9b5f 	.word	0x000e9b5f
   df7bc:	000ea132 	.word	0x000ea132
   df7c0:	000ea158 	.word	0x000ea158
   df7c4:	000e9ac8 	.word	0x000e9ac8
   df7c8:	000ea176 	.word	0x000ea176

000df7cc <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   df7cc:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
   df7d0:	680b      	ldr	r3, [r1, #0]
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
   df7d2:	6849      	ldr	r1, [r1, #4]

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
   df7d4:	f8d0 8008 	ldr.w	r8, [r0, #8]
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
   df7d8:	684d      	ldr	r5, [r1, #4]

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
   df7da:	685b      	ldr	r3, [r3, #4]
   df7dc:	2238      	movs	r2, #56	; 0x38
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
   df7de:	4355      	muls	r5, r2
   df7e0:	eb08 0a05 	add.w	sl, r8, r5
                 output->type == kTfLiteUInt8 || output->type == kTfLiteInt8);

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   df7e4:	b08d      	sub	sp, #52	; 0x34
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
   df7e6:	4353      	muls	r3, r2
                 output->type == kTfLiteUInt8 || output->type == kTfLiteInt8);

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   df7e8:	4683      	mov	fp, r0
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  tflite::QuantizationParams op_params;
  op_params.zero_point = output->params.zero_point;
  op_params.scale = output->params.scale;
   df7ea:	f8da 000c 	ldr.w	r0, [sl, #12]

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
   df7ee:	9301      	str	r3, [sp, #4]
   df7f0:	eb08 0403 	add.w	r4, r8, r3
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  tflite::QuantizationParams op_params;
  op_params.zero_point = output->params.zero_point;
  op_params.scale = output->params.scale;
   df7f4:	f007 fc02 	bl	e6ffc <__aeabi_f2d>
  switch (output->type) {
   df7f8:	f818 2005 	ldrb.w	r2, [r8, r5]
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  tflite::QuantizationParams op_params;
  op_params.zero_point = output->params.zero_point;
   df7fc:	f8da 9010 	ldr.w	r9, [sl, #16]
  op_params.scale = output->params.scale;
  switch (output->type) {
   df800:	2a03      	cmp	r2, #3
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  tflite::QuantizationParams op_params;
  op_params.zero_point = output->params.zero_point;
  op_params.scale = output->params.scale;
   df802:	4606      	mov	r6, r0
   df804:	460f      	mov	r7, r1
  switch (output->type) {
   df806:	d035      	beq.n	df874 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0xa8>
   df808:	2a09      	cmp	r2, #9
   df80a:	9b01      	ldr	r3, [sp, #4]
   df80c:	d16b      	bne.n	df8e6 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x11a>
    case kTfLiteInt8:
      reference_ops::AffineQuantize(
          op_params, GetTensorShape(input), GetTensorData<float>(input),
   df80e:	4621      	mov	r1, r4
   df810:	a802      	add	r0, sp, #8
   df812:	f7f7 f8c8 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   df816:	b104      	cbz	r4, df81a <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x4e>
   df818:	6864      	ldr	r4, [r4, #4]
          GetTensorShape(output), GetTensorData<int8_t>(output));
   df81a:	4651      	mov	r1, sl
   df81c:	a807      	add	r0, sp, #28
   df81e:	f7f7 f8c2 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                           const RuntimeShape& input_shape,
                           const float* input_data,
                           const RuntimeShape& output_shape, T* output_data) {
  const int32 zero_point = op_params.zero_point;
  const double scale = static_cast<double>(op_params.scale);
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
   df822:	a907      	add	r1, sp, #28
   df824:	a802      	add	r0, sp, #8
   df826:	f7fd f89d 	bl	dc964 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_>
   df82a:	f8da a004 	ldr.w	sl, [sl, #4]
   df82e:	4680      	mov	r8, r0
   df830:	4655      	mov	r5, sl
  static constexpr int32 min_val = std::numeric_limits<T>::min();
  static constexpr int32 max_val = std::numeric_limits<T>::max();

  for (int i = 0; i < flat_size; i++) {
   df832:	ebca 0305 	rsb	r3, sl, r5
   df836:	4598      	cmp	r8, r3
   df838:	dd4d      	ble.n	df8d6 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x10a>
}
inline double TfLiteRound(const double x) { return ::round(x); }
#else
template <class T>
inline T TfLiteRound(const T x) {
  return std::round(x);
   df83a:	f854 0b04 	ldr.w	r0, [r4], #4
   df83e:	f007 fbdd 	bl	e6ffc <__aeabi_f2d>
   df842:	4632      	mov	r2, r6
   df844:	463b      	mov	r3, r7
   df846:	f007 fd57 	bl	e72f8 <__aeabi_ddiv>
   df84a:	ec41 0b10 	vmov	d0, r0, r1
   df84e:	f005 fd01 	bl	e5254 <round>
    const float val = input_data[i];
    int32 unclamped = static_cast<int32>(TfLiteRound(val / scale)) + zero_point;
   df852:	ec51 0b10 	vmov	r0, r1, d0
   df856:	f007 febf 	bl	e75d8 <__aeabi_d2iz>
   df85a:	4448      	add	r0, r9
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   df85c:	f110 0f80 	cmn.w	r0, #128	; 0x80
   df860:	db03      	blt.n	df86a <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x9e>
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   df862:	287f      	cmp	r0, #127	; 0x7f
   df864:	bfa8      	it	ge
   df866:	207f      	movge	r0, #127	; 0x7f
   df868:	e001      	b.n	df86e <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0xa2>
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
	return __b;
   df86a:	f06f 007f 	mvn.w	r0, #127	; 0x7f
    int32 clamped = std::min(std::max(unclamped, min_val), max_val);
    output_data[i] = clamped;
   df86e:	f805 0b01 	strb.w	r0, [r5], #1
   df872:	e7de      	b.n	df832 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x66>
      break;
    case kTfLiteUInt8:
      reference_ops::AffineQuantize(
          op_params, GetTensorShape(input), GetTensorData<float>(input),
   df874:	4621      	mov	r1, r4
   df876:	a802      	add	r0, sp, #8
   df878:	f7f7 f895 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   df87c:	b104      	cbz	r4, df880 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0xb4>
   df87e:	6864      	ldr	r4, [r4, #4]
          GetTensorShape(output), GetTensorData<uint8_t>(output));
   df880:	4651      	mov	r1, sl
   df882:	a807      	add	r0, sp, #28
   df884:	f7f7 f88f 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                           const RuntimeShape& input_shape,
                           const float* input_data,
                           const RuntimeShape& output_shape, T* output_data) {
  const int32 zero_point = op_params.zero_point;
  const double scale = static_cast<double>(op_params.scale);
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
   df888:	a907      	add	r1, sp, #28
   df88a:	a802      	add	r0, sp, #8
   df88c:	f7fd f86a 	bl	dc964 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_>
   df890:	f8da 8004 	ldr.w	r8, [sl, #4]
   df894:	4605      	mov	r5, r0
   df896:	46c2      	mov	sl, r8
  static constexpr int32 min_val = std::numeric_limits<T>::min();
  static constexpr int32 max_val = std::numeric_limits<T>::max();

  for (int i = 0; i < flat_size; i++) {
   df898:	ebc8 030a 	rsb	r3, r8, sl
   df89c:	429d      	cmp	r5, r3
   df89e:	dd1a      	ble.n	df8d6 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x10a>
   df8a0:	f854 0b04 	ldr.w	r0, [r4], #4
   df8a4:	f007 fbaa 	bl	e6ffc <__aeabi_f2d>
   df8a8:	4632      	mov	r2, r6
   df8aa:	463b      	mov	r3, r7
   df8ac:	f007 fd24 	bl	e72f8 <__aeabi_ddiv>
   df8b0:	ec41 0b10 	vmov	d0, r0, r1
   df8b4:	f005 fcce 	bl	e5254 <round>
    const float val = input_data[i];
    int32 unclamped = static_cast<int32>(TfLiteRound(val / scale)) + zero_point;
   df8b8:	ec51 0b10 	vmov	r0, r1, d0
   df8bc:	f007 fe8c 	bl	e75d8 <__aeabi_d2iz>
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   df8c0:	eb10 0009 	adds.w	r0, r0, r9
   df8c4:	d403      	bmi.n	df8ce <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x102>
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   df8c6:	28ff      	cmp	r0, #255	; 0xff
   df8c8:	bfa8      	it	ge
   df8ca:	20ff      	movge	r0, #255	; 0xff
   df8cc:	e000      	b.n	df8d0 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x104>
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
	return __b;
   df8ce:	2000      	movs	r0, #0
    int32 clamped = std::min(std::max(unclamped, min_val), max_val);
    output_data[i] = clamped;
   df8d0:	f80a 0b01 	strb.w	r0, [sl], #1
   df8d4:	e7e0      	b.n	df898 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0xcc>
   df8d6:	a807      	add	r0, sp, #28
   df8d8:	f7f6 fdb5 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
          op_params, GetTensorShape(input), GetTensorData<float>(input),
          GetTensorShape(output), GetTensorData<int8_t>(output));
      break;
    case kTfLiteUInt8:
      reference_ops::AffineQuantize(
          op_params, GetTensorShape(input), GetTensorData<float>(input),
   df8dc:	a802      	add	r0, sp, #8
   df8de:	f7f6 fdb2 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(context, "Output type %s (%d) not supported",
                           TfLiteTypeGetName(input->type), output->type);
      return kTfLiteError;
  }

  return kTfLiteOk;
   df8e2:	2000      	movs	r0, #0
      break;
    case kTfLiteUInt8:
      reference_ops::AffineQuantize(
          op_params, GetTensorShape(input), GetTensorData<float>(input),
          GetTensorShape(output), GetTensorData<uint8_t>(output));
      break;
   df8e4:	e00c      	b.n	df900 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x134>
    default:
      context->ReportError(context, "Output type %s (%d) not supported",
   df8e6:	f818 0003 	ldrb.w	r0, [r8, r3]
   df8ea:	f8db 4014 	ldr.w	r4, [fp, #20]
   df8ee:	f7f4 fc13 	bl	d4118 <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type), output->type);
   df8f2:	f818 3005 	ldrb.w	r3, [r8, r5]
   df8f6:	4904      	ldr	r1, [pc, #16]	; (df908 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x13c>)
   df8f8:	4602      	mov	r2, r0
   df8fa:	4658      	mov	r0, fp
   df8fc:	47a0      	blx	r4
      return kTfLiteError;
   df8fe:	2001      	movs	r0, #1
  }

  return kTfLiteOk;
}
   df900:	b00d      	add	sp, #52	; 0x34
   df902:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   df906:	bf00      	nop
   df908:	000ea1b2 	.word	0x000ea1b2

000df90c <_ZN6tflite3ops5micro17Register_QUANTIZEEv>:
// quantized output, in int8 or uint8 format.
TfLiteRegistration* Register_QUANTIZE() {
  static TfLiteRegistration r = {quantize::Init, quantize::Free,
                                 quantize::Prepare, quantize::Eval};
  return &r;
}
   df90c:	4800      	ldr	r0, [pc, #0]	; (df910 <_ZN6tflite3ops5micro17Register_QUANTIZEEv+0x4>)
   df90e:	4770      	bx	lr
   df910:	2003c148 	.word	0x2003c148

000df914 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode>:
  TF_LITE_ENSURE_EQ(context, input->type, output->type);
  TF_LITE_ENSURE_EQ(context, num_input_elements, num_output_elements);
  return kTfLiteOk;
}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   df914:	b530      	push	{r4, r5, lr}
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
   df916:	680b      	ldr	r3, [r1, #0]
  TF_LITE_ENSURE(context, NumInputs(node) == 1 || NumInputs(node) == 2);
   df918:	681b      	ldr	r3, [r3, #0]
   df91a:	3b01      	subs	r3, #1
   df91c:	2b01      	cmp	r3, #1
  TF_LITE_ENSURE_EQ(context, input->type, output->type);
  TF_LITE_ENSURE_EQ(context, num_input_elements, num_output_elements);
  return kTfLiteOk;
}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   df91e:	b085      	sub	sp, #20
   df920:	4602      	mov	r2, r0
  TF_LITE_ENSURE(context, NumInputs(node) == 1 || NumInputs(node) == 2);
   df922:	d813      	bhi.n	df94c <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x38>
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
   df924:	684b      	ldr	r3, [r1, #4]
   df926:	681b      	ldr	r3, [r3, #0]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
   df928:	2b01      	cmp	r3, #1
   df92a:	d00d      	beq.n	df948 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x34>
   df92c:	9302      	str	r3, [sp, #8]
   df92e:	4b0c      	ldr	r3, [pc, #48]	; (df960 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x4c>)
   df930:	9301      	str	r3, [sp, #4]
   df932:	2401      	movs	r4, #1
   df934:	4b0b      	ldr	r3, [pc, #44]	; (df964 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x50>)
   df936:	9300      	str	r3, [sp, #0]
   df938:	9403      	str	r4, [sp, #12]
   df93a:	6955      	ldr	r5, [r2, #20]
   df93c:	490a      	ldr	r1, [pc, #40]	; (df968 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x54>)
   df93e:	4a0b      	ldr	r2, [pc, #44]	; (df96c <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x58>)
   df940:	2348      	movs	r3, #72	; 0x48
   df942:	47a8      	blx	r5
   df944:	4620      	mov	r0, r4
   df946:	e009      	b.n	df95c <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x48>
  return kTfLiteOk;
   df948:	2000      	movs	r0, #0
   df94a:	e007      	b.n	df95c <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x48>
  TF_LITE_ENSURE_EQ(context, num_input_elements, num_output_elements);
  return kTfLiteOk;
}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  TF_LITE_ENSURE(context, NumInputs(node) == 1 || NumInputs(node) == 2);
   df94c:	4b08      	ldr	r3, [pc, #32]	; (df970 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x5c>)
   df94e:	9300      	str	r3, [sp, #0]
   df950:	6944      	ldr	r4, [r0, #20]
   df952:	4a06      	ldr	r2, [pc, #24]	; (df96c <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x58>)
   df954:	4907      	ldr	r1, [pc, #28]	; (df974 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x60>)
   df956:	2347      	movs	r3, #71	; 0x47
   df958:	47a0      	blx	r4
   df95a:	2001      	movs	r0, #1
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
  return kTfLiteOk;
}
   df95c:	b005      	add	sp, #20
   df95e:	bd30      	pop	{r4, r5, pc}
   df960:	000eb2c5 	.word	0x000eb2c5
   df964:	000e9922 	.word	0x000e9922
   df968:	000e98f8 	.word	0x000e98f8
   df96c:	000ea1d4 	.word	0x000ea1d4
   df970:	000ea27d 	.word	0x000ea27d
   df974:	000e9ac8 	.word	0x000e9ac8

000df978 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode>:

constexpr int kInputTensor = 0;
constexpr int kShapeTensor = 1;
constexpr int kOutputTensor = 0;

TfLiteStatus ReshapeOutput(TfLiteContext* context, TfLiteNode* node) {
   df978:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   df97c:	f8d1 c000 	ldr.w	ip, [r1]
   df980:	6886      	ldr	r6, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   df982:	f8dc 7004 	ldr.w	r7, [ip, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   df986:	6849      	ldr	r1, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   df988:	2338      	movs	r3, #56	; 0x38
   df98a:	435f      	muls	r7, r3
   df98c:	19f2      	adds	r2, r6, r7
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   df98e:	684d      	ldr	r5, [r1, #4]
   df990:	6891      	ldr	r1, [r2, #8]
   df992:	435d      	muls	r5, r3
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
   df994:	f8d1 b000 	ldr.w	fp, [r1]
   df998:	b085      	sub	sp, #20
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   df99a:	eb06 0a05 	add.w	sl, r6, r5
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
   df99e:	2400      	movs	r4, #0
inline int NumIntermediates(const TfLiteNode* node) {
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
   df9a0:	2201      	movs	r2, #1
   df9a2:	2300      	movs	r3, #0
  for (int i = 0; i < dims->size; ++i) {
   df9a4:	45a3      	cmp	fp, r4
   df9a6:	dd0c      	ble.n	df9c2 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x4a>
    count *= dims->data[i];
   df9a8:	f851 ef04 	ldr.w	lr, [r1, #4]!
   df9ac:	ea4f 79ee 	mov.w	r9, lr, asr #31
   df9b0:	fb02 f809 	mul.w	r8, r2, r9
   df9b4:	fb0e 8803 	mla	r8, lr, r3, r8
   df9b8:	fba2 230e 	umull	r2, r3, r2, lr
   df9bc:	4443      	add	r3, r8
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
   df9be:	3401      	adds	r4, #1
   df9c0:	e7f0      	b.n	df9a4 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x2c>
  // of output elements in the same as the number of input elements.
  int num_input_elements = NumElements(input);
  TfLiteIntArray* output_shape = output->dims;

  if (NumInputs(node) == 1 &&  // Legacy scalar supported with params.
      output_shape->size == 1 && output_shape->data[0] == 0) {
   df9c2:	f8dc 3000 	ldr.w	r3, [ip]
  // Tensorflow's Reshape allows one of the shape components to have the
  // special -1 value, meaning it will be calculated automatically based on the
  // input. Here we calculate what that dimension should be so that the number
  // of output elements in the same as the number of input elements.
  int num_input_elements = NumElements(input);
  TfLiteIntArray* output_shape = output->dims;
   df9c6:	f8da 4008 	ldr.w	r4, [sl, #8]

  if (NumInputs(node) == 1 &&  // Legacy scalar supported with params.
      output_shape->size == 1 && output_shape->data[0] == 0) {
   df9ca:	2b01      	cmp	r3, #1
   df9cc:	d105      	bne.n	df9da <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x62>
  // input. Here we calculate what that dimension should be so that the number
  // of output elements in the same as the number of input elements.
  int num_input_elements = NumElements(input);
  TfLiteIntArray* output_shape = output->dims;

  if (NumInputs(node) == 1 &&  // Legacy scalar supported with params.
   df9ce:	6823      	ldr	r3, [r4, #0]
   df9d0:	2b01      	cmp	r3, #1
   df9d2:	d102      	bne.n	df9da <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x62>
      output_shape->size == 1 && output_shape->data[0] == 0) {
   df9d4:	6863      	ldr	r3, [r4, #4]
   df9d6:	2b00      	cmp	r3, #0
   df9d8:	d04c      	beq.n	dfa74 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xfc>
    output_shape->size = 0;
  }

  int num_output_elements = 1;
  int stretch_dim = -1;
  for (int i = 0; i < output_shape->size; ++i) {
   df9da:	f8d4 9000 	ldr.w	r9, [r4]
   df9de:	46a0      	mov	r8, r4
   df9e0:	f04f 31ff 	mov.w	r1, #4294967295	; 0xffffffff
   df9e4:	2301      	movs	r3, #1
   df9e6:	f04f 0e00 	mov.w	lr, #0
   df9ea:	45ce      	cmp	lr, r9
   df9ec:	da18      	bge.n	dfa20 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xa8>
    int value = output_shape->data[i];
   df9ee:	f858 cf04 	ldr.w	ip, [r8, #4]!
    if (value == -1) {
   df9f2:	f1bc 3fff 	cmp.w	ip, #4294967295	; 0xffffffff
   df9f6:	d10c      	bne.n	dfa12 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x9a>
      TF_LITE_ENSURE_EQ(context, stretch_dim, -1);
   df9f8:	f1b1 3fff 	cmp.w	r1, #4294967295	; 0xffffffff
   df9fc:	d00c      	beq.n	dfa18 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xa0>
   df9fe:	4b20      	ldr	r3, [pc, #128]	; (dfa80 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x108>)
   dfa00:	9301      	str	r3, [sp, #4]
   dfa02:	4b20      	ldr	r3, [pc, #128]	; (dfa84 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x10c>)
   dfa04:	9300      	str	r3, [sp, #0]
   dfa06:	f8cd c00c 	str.w	ip, [sp, #12]
   dfa0a:	9102      	str	r1, [sp, #8]
   dfa0c:	6944      	ldr	r4, [r0, #20]
   dfa0e:	2336      	movs	r3, #54	; 0x36
   dfa10:	e029      	b.n	dfa66 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xee>
      stretch_dim = i;
    } else {
      num_output_elements *= value;
   dfa12:	fb0c f303 	mul.w	r3, ip, r3
   dfa16:	e000      	b.n	dfa1a <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xa2>
   dfa18:	4671      	mov	r1, lr
    output_shape->size = 0;
  }

  int num_output_elements = 1;
  int stretch_dim = -1;
  for (int i = 0; i < output_shape->size; ++i) {
   dfa1a:	f10e 0e01 	add.w	lr, lr, #1
   dfa1e:	e7e4      	b.n	df9ea <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x72>
      stretch_dim = i;
    } else {
      num_output_elements *= value;
    }
  }
  if (stretch_dim != -1) {
   dfa20:	f1b1 3fff 	cmp.w	r1, #4294967295	; 0xffffffff
    output_shape->data[stretch_dim] = num_input_elements / num_output_elements;
   dfa24:	bf1e      	ittt	ne
   dfa26:	eb04 0181 	addne.w	r1, r4, r1, lsl #2
   dfa2a:	fb92 fef3 	sdivne	lr, r2, r3
   dfa2e:	f8c1 e004 	strne.w	lr, [r1, #4]
    num_output_elements *= output_shape->data[stretch_dim];
  }

  TF_LITE_ENSURE_EQ(context, input->type, output->type);
   dfa32:	5df1      	ldrb	r1, [r6, r7]
   dfa34:	5d74      	ldrb	r4, [r6, r5]
      num_output_elements *= value;
    }
  }
  if (stretch_dim != -1) {
    output_shape->data[stretch_dim] = num_input_elements / num_output_elements;
    num_output_elements *= output_shape->data[stretch_dim];
   dfa36:	bf18      	it	ne
   dfa38:	fb0e f303 	mulne.w	r3, lr, r3
  }

  TF_LITE_ENSURE_EQ(context, input->type, output->type);
   dfa3c:	42a1      	cmp	r1, r4
   dfa3e:	d008      	beq.n	dfa52 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xda>
   dfa40:	4b11      	ldr	r3, [pc, #68]	; (dfa88 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x110>)
   dfa42:	9301      	str	r3, [sp, #4]
   dfa44:	4b11      	ldr	r3, [pc, #68]	; (dfa8c <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x114>)
   dfa46:	9300      	str	r3, [sp, #0]
   dfa48:	9403      	str	r4, [sp, #12]
   dfa4a:	9102      	str	r1, [sp, #8]
   dfa4c:	6944      	ldr	r4, [r0, #20]
   dfa4e:	2341      	movs	r3, #65	; 0x41
   dfa50:	e009      	b.n	dfa66 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xee>
  TF_LITE_ENSURE_EQ(context, num_input_elements, num_output_elements);
   dfa52:	429a      	cmp	r2, r3
   dfa54:	d00c      	beq.n	dfa70 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xf8>
   dfa56:	9303      	str	r3, [sp, #12]
   dfa58:	4b0d      	ldr	r3, [pc, #52]	; (dfa90 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x118>)
   dfa5a:	9301      	str	r3, [sp, #4]
   dfa5c:	4b0d      	ldr	r3, [pc, #52]	; (dfa94 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x11c>)
   dfa5e:	9300      	str	r3, [sp, #0]
   dfa60:	9202      	str	r2, [sp, #8]
   dfa62:	6944      	ldr	r4, [r0, #20]
   dfa64:	2342      	movs	r3, #66	; 0x42
   dfa66:	4a0c      	ldr	r2, [pc, #48]	; (dfa98 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x120>)
   dfa68:	490c      	ldr	r1, [pc, #48]	; (dfa9c <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x124>)
   dfa6a:	47a0      	blx	r4
   dfa6c:	2001      	movs	r0, #1
   dfa6e:	e003      	b.n	dfa78 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x100>
  return kTfLiteOk;
   dfa70:	2000      	movs	r0, #0
   dfa72:	e001      	b.n	dfa78 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x100>
  if (NumInputs(node) == 1 &&  // Legacy scalar supported with params.
      output_shape->size == 1 && output_shape->data[0] == 0) {
    // Legacy tflite models use a shape parameter of [0] to indicate scalars,
    // so adjust accordingly. TODO(b/111614235): Allow zero-sized buffers during
    // toco conversion.
    output_shape->size = 0;
   dfa74:	6023      	str	r3, [r4, #0]
   dfa76:	e7b0      	b.n	df9da <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x62>
  }

  TF_LITE_ENSURE_EQ(context, input->type, output->type);
  TF_LITE_ENSURE_EQ(context, num_input_elements, num_output_elements);
  return kTfLiteOk;
}
   dfa78:	b005      	add	sp, #20
   dfa7a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dfa7e:	bf00      	nop
   dfa80:	000ea2aa 	.word	0x000ea2aa
   dfa84:	000ea2ad 	.word	0x000ea2ad
   dfa88:	000e993f 	.word	0x000e993f
   dfa8c:	000e9933 	.word	0x000e9933
   dfa90:	000ea2b9 	.word	0x000ea2b9
   dfa94:	000ea2cd 	.word	0x000ea2cd
   dfa98:	000ea1d4 	.word	0x000ea1d4
   dfa9c:	000e98f8 	.word	0x000e98f8

000dfaa0 <_ZN6tflite3ops5micro7reshape4EvalEP13TfLiteContextP10TfLiteNode>:
  TF_LITE_ENSURE(context, NumInputs(node) == 1 || NumInputs(node) == 2);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dfaa0:	b570      	push	{r4, r5, r6, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dfaa2:	680a      	ldr	r2, [r1, #0]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dfaa4:	684b      	ldr	r3, [r1, #4]
   dfaa6:	6886      	ldr	r6, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dfaa8:	6854      	ldr	r4, [r2, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dfaaa:	685d      	ldr	r5, [r3, #4]
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  if (ReshapeOutput(context, node) != kTfLiteOk) {
   dfaac:	f7ff ff64 	bl	df978 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode>
   dfab0:	b970      	cbnz	r0, dfad0 <_ZN6tflite3ops5micro7reshape4EvalEP13TfLiteContextP10TfLiteNode+0x30>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dfab2:	2338      	movs	r3, #56	; 0x38
   dfab4:	fb03 6204 	mla	r2, r3, r4, r6
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dfab8:	fb03 6505 	mla	r5, r3, r5, r6
   dfabc:	4603      	mov	r3, r0
    return kTfLiteError;
  }

  for (int i = 0; i < input->bytes; ++i) {
   dfabe:	6991      	ldr	r1, [r2, #24]
   dfac0:	4299      	cmp	r1, r3
   dfac2:	d906      	bls.n	dfad2 <_ZN6tflite3ops5micro7reshape4EvalEP13TfLiteContextP10TfLiteNode+0x32>
    output->data.raw[i] = input->data.raw[i];
   dfac4:	6851      	ldr	r1, [r2, #4]
   dfac6:	5ccc      	ldrb	r4, [r1, r3]
   dfac8:	6869      	ldr	r1, [r5, #4]
   dfaca:	54cc      	strb	r4, [r1, r3]
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  if (ReshapeOutput(context, node) != kTfLiteOk) {
    return kTfLiteError;
  }

  for (int i = 0; i < input->bytes; ++i) {
   dfacc:	3301      	adds	r3, #1
   dface:	e7f6      	b.n	dfabe <_ZN6tflite3ops5micro7reshape4EvalEP13TfLiteContextP10TfLiteNode+0x1e>

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  if (ReshapeOutput(context, node) != kTfLiteOk) {
    return kTfLiteError;
   dfad0:	2001      	movs	r0, #1

  for (int i = 0; i < input->bytes; ++i) {
    output->data.raw[i] = input->data.raw[i];
  }
  return kTfLiteOk;
}
   dfad2:	bd70      	pop	{r4, r5, r6, pc}

000dfad4 <_ZN6tflite3ops5micro16Register_RESHAPEEv>:

TfLiteRegistration* Register_RESHAPE() {
  static TfLiteRegistration r = {nullptr, nullptr, reshape::Prepare,
                                 reshape::Eval};
  return &r;
}
   dfad4:	4800      	ldr	r0, [pc, #0]	; (dfad8 <_ZN6tflite3ops5micro16Register_RESHAPEEv+0x4>)
   dfad6:	4770      	bx	lr
   dfad8:	2003c168 	.word	0x2003c168

000dfadc <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode>:
namespace round {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   dfadc:	b5f0      	push	{r4, r5, r6, r7, lr}
   dfade:	680b      	ldr	r3, [r1, #0]
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
   dfae0:	681e      	ldr	r6, [r3, #0]
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   dfae2:	2e01      	cmp	r6, #1
namespace round {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   dfae4:	b085      	sub	sp, #20
   dfae6:	4605      	mov	r5, r0
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   dfae8:	d009      	beq.n	dfafe <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x22>
   dfaea:	4b3b      	ldr	r3, [pc, #236]	; (dfbd8 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xfc>)
   dfaec:	9301      	str	r3, [sp, #4]
   dfaee:	2401      	movs	r4, #1
   dfaf0:	4b3a      	ldr	r3, [pc, #232]	; (dfbdc <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x100>)
   dfaf2:	9300      	str	r3, [sp, #0]
   dfaf4:	9403      	str	r4, [sp, #12]
   dfaf6:	9602      	str	r6, [sp, #8]
   dfaf8:	6945      	ldr	r5, [r0, #20]
   dfafa:	2321      	movs	r3, #33	; 0x21
   dfafc:	e01e      	b.n	dfb3c <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
   dfafe:	f8d1 e004 	ldr.w	lr, [r1, #4]
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
   dfb02:	f8de 4000 	ldr.w	r4, [lr]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
   dfb06:	2c01      	cmp	r4, #1
   dfb08:	d008      	beq.n	dfb1c <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x40>
   dfb0a:	4b33      	ldr	r3, [pc, #204]	; (dfbd8 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xfc>)
   dfb0c:	9301      	str	r3, [sp, #4]
   dfb0e:	4b34      	ldr	r3, [pc, #208]	; (dfbe0 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x104>)
   dfb10:	9300      	str	r3, [sp, #0]
   dfb12:	9603      	str	r6, [sp, #12]
   dfb14:	9402      	str	r4, [sp, #8]
   dfb16:	6944      	ldr	r4, [r0, #20]
   dfb18:	2322      	movs	r3, #34	; 0x22
   dfb1a:	e022      	b.n	dfb62 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x86>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dfb1c:	6859      	ldr	r1, [r3, #4]
   dfb1e:	6882      	ldr	r2, [r0, #8]
   dfb20:	2338      	movs	r3, #56	; 0x38
   dfb22:	4359      	muls	r1, r3
   dfb24:	1857      	adds	r7, r2, r1
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
   dfb26:	5c56      	ldrb	r6, [r2, r1]
   dfb28:	2e01      	cmp	r6, #1
   dfb2a:	d00b      	beq.n	dfb44 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x68>
   dfb2c:	4b2d      	ldr	r3, [pc, #180]	; (dfbe4 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x108>)
   dfb2e:	9301      	str	r3, [sp, #4]
   dfb30:	4b2d      	ldr	r3, [pc, #180]	; (dfbe8 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x10c>)
   dfb32:	9300      	str	r3, [sp, #0]
   dfb34:	9403      	str	r4, [sp, #12]
   dfb36:	9602      	str	r6, [sp, #8]
   dfb38:	6945      	ldr	r5, [r0, #20]
   dfb3a:	2323      	movs	r3, #35	; 0x23
   dfb3c:	4a2b      	ldr	r2, [pc, #172]	; (dfbec <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x110>)
   dfb3e:	492c      	ldr	r1, [pc, #176]	; (dfbf0 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x114>)
   dfb40:	47a8      	blx	r5
   dfb42:	e042      	b.n	dfbca <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xee>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dfb44:	f8de 1004 	ldr.w	r1, [lr, #4]
   dfb48:	434b      	muls	r3, r1
   dfb4a:	18d1      	adds	r1, r2, r3
  TF_LITE_ENSURE_EQ(context, output->type, input->type);
   dfb4c:	5cd4      	ldrb	r4, [r2, r3]
   dfb4e:	2c01      	cmp	r4, #1
   dfb50:	d00a      	beq.n	dfb68 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x8c>
   dfb52:	4b25      	ldr	r3, [pc, #148]	; (dfbe8 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x10c>)
   dfb54:	9301      	str	r3, [sp, #4]
   dfb56:	4b27      	ldr	r3, [pc, #156]	; (dfbf4 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x118>)
   dfb58:	9300      	str	r3, [sp, #0]
   dfb5a:	9603      	str	r6, [sp, #12]
   dfb5c:	9402      	str	r4, [sp, #8]
   dfb5e:	6944      	ldr	r4, [r0, #20]
   dfb60:	2324      	movs	r3, #36	; 0x24
   dfb62:	4a22      	ldr	r2, [pc, #136]	; (dfbec <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x110>)
   dfb64:	4922      	ldr	r1, [pc, #136]	; (dfbf0 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x114>)
   dfb66:	e02f      	b.n	dfbc8 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xec>
  TF_LITE_ENSURE_EQ(context, output->bytes, input->bytes);
   dfb68:	698b      	ldr	r3, [r1, #24]
   dfb6a:	69ba      	ldr	r2, [r7, #24]
   dfb6c:	4293      	cmp	r3, r2
   dfb6e:	d008      	beq.n	dfb82 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xa6>
   dfb70:	9302      	str	r3, [sp, #8]
   dfb72:	4b21      	ldr	r3, [pc, #132]	; (dfbf8 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x11c>)
   dfb74:	9301      	str	r3, [sp, #4]
   dfb76:	4b21      	ldr	r3, [pc, #132]	; (dfbfc <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x120>)
   dfb78:	9300      	str	r3, [sp, #0]
   dfb7a:	9203      	str	r2, [sp, #12]
   dfb7c:	6945      	ldr	r5, [r0, #20]
   dfb7e:	2325      	movs	r3, #37	; 0x25
   dfb80:	e7dc      	b.n	dfb3c <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
  TF_LITE_ENSURE_EQ(context, output->dims->size, input->dims->size);
   dfb82:	688b      	ldr	r3, [r1, #8]
   dfb84:	68ba      	ldr	r2, [r7, #8]
   dfb86:	681e      	ldr	r6, [r3, #0]
   dfb88:	6811      	ldr	r1, [r2, #0]
   dfb8a:	428e      	cmp	r6, r1
   dfb8c:	d008      	beq.n	dfba0 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xc4>
   dfb8e:	4b1c      	ldr	r3, [pc, #112]	; (dfc00 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x124>)
   dfb90:	9301      	str	r3, [sp, #4]
   dfb92:	4b1c      	ldr	r3, [pc, #112]	; (dfc04 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x128>)
   dfb94:	9300      	str	r3, [sp, #0]
   dfb96:	9103      	str	r1, [sp, #12]
   dfb98:	9602      	str	r6, [sp, #8]
   dfb9a:	6945      	ldr	r5, [r0, #20]
   dfb9c:	2326      	movs	r3, #38	; 0x26
   dfb9e:	e7cd      	b.n	dfb3c <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
   dfba0:	2100      	movs	r1, #0
  for (int i = 0; i < output->dims->size; ++i) {
   dfba2:	42b1      	cmp	r1, r6
   dfba4:	da15      	bge.n	dfbd2 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xf6>
    TF_LITE_ENSURE_EQ(context, output->dims->data[i], input->dims->data[i]);
   dfba6:	f853 0f04 	ldr.w	r0, [r3, #4]!
   dfbaa:	f852 4f04 	ldr.w	r4, [r2, #4]!
   dfbae:	42a0      	cmp	r0, r4
   dfbb0:	d00d      	beq.n	dfbce <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xf2>
   dfbb2:	4b15      	ldr	r3, [pc, #84]	; (dfc08 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x12c>)
   dfbb4:	9301      	str	r3, [sp, #4]
   dfbb6:	4b15      	ldr	r3, [pc, #84]	; (dfc0c <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x130>)
   dfbb8:	9002      	str	r0, [sp, #8]
   dfbba:	9300      	str	r3, [sp, #0]
   dfbbc:	9403      	str	r4, [sp, #12]
   dfbbe:	696c      	ldr	r4, [r5, #20]
   dfbc0:	4a0a      	ldr	r2, [pc, #40]	; (dfbec <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x110>)
   dfbc2:	490b      	ldr	r1, [pc, #44]	; (dfbf0 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x114>)
   dfbc4:	2328      	movs	r3, #40	; 0x28
   dfbc6:	4628      	mov	r0, r5
   dfbc8:	47a0      	blx	r4
   dfbca:	2001      	movs	r0, #1
   dfbcc:	e002      	b.n	dfbd4 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xf8>
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
  TF_LITE_ENSURE_EQ(context, output->type, input->type);
  TF_LITE_ENSURE_EQ(context, output->bytes, input->bytes);
  TF_LITE_ENSURE_EQ(context, output->dims->size, input->dims->size);
  for (int i = 0; i < output->dims->size; ++i) {
   dfbce:	3101      	adds	r1, #1
   dfbd0:	e7e7      	b.n	dfba2 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xc6>
    TF_LITE_ENSURE_EQ(context, output->dims->data[i], input->dims->data[i]);
  }
  return kTfLiteOk;
   dfbd2:	2000      	movs	r0, #0
}
   dfbd4:	b005      	add	sp, #20
   dfbd6:	bdf0      	pop	{r4, r5, r6, r7, pc}
   dfbd8:	000eb2c5 	.word	0x000eb2c5
   dfbdc:	000e9912 	.word	0x000e9912
   dfbe0:	000e9922 	.word	0x000e9922
   dfbe4:	000ea167 	.word	0x000ea167
   dfbe8:	000e9933 	.word	0x000e9933
   dfbec:	000ea2e0 	.word	0x000ea2e0
   dfbf0:	000e98f8 	.word	0x000e98f8
   dfbf4:	000e993f 	.word	0x000e993f
   dfbf8:	000e994c 	.word	0x000e994c
   dfbfc:	000e9959 	.word	0x000e9959
   dfc00:	000e9967 	.word	0x000e9967
   dfc04:	000e9979 	.word	0x000e9979
   dfc08:	000e998c 	.word	0x000e998c
   dfc0c:	000e99a1 	.word	0x000e99a1

000dfc10 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf>:
    return floor_val = floor_val + 1.0f;
  }
}

inline void Round(const RuntimeShape& input_shape, const float* input_data,
                  const RuntimeShape& output_shape, float* output_data) {
   dfc10:	e92d 47f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, lr}
   dfc14:	ed2d 8b04 	vpush	{d8-d9}
   dfc18:	461e      	mov	r6, r3
   dfc1a:	f8d0 8000 	ldr.w	r8, [r0]

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   dfc1e:	6813      	ldr	r3, [r2, #0]
   dfc20:	4598      	cmp	r8, r3
   dfc22:	4604      	mov	r4, r0
   dfc24:	460f      	mov	r7, r1
   dfc26:	4691      	mov	r9, r2
   dfc28:	d101      	bne.n	dfc2e <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x1e>
   dfc2a:	2500      	movs	r5, #0
   dfc2c:	e00d      	b.n	dfc4a <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x3a>
   dfc2e:	f004 fb7d 	bl	e432c <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   dfc32:	4629      	mov	r1, r5
   dfc34:	4620      	mov	r0, r4
   dfc36:	f7f6 fc11 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dfc3a:	4629      	mov	r1, r5
   dfc3c:	4682      	mov	sl, r0
   dfc3e:	4648      	mov	r0, r9
   dfc40:	f7f6 fc0c 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dfc44:	4582      	cmp	sl, r0
   dfc46:	d1f2      	bne.n	dfc2e <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x1e>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   dfc48:	3501      	adds	r5, #1
   dfc4a:	45a8      	cmp	r8, r5
   dfc4c:	dcf1      	bgt.n	dfc32 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x22>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   dfc4e:	f1b8 0f04 	cmp.w	r8, #4
   dfc52:	bfcc      	ite	gt
   dfc54:	6864      	ldrgt	r4, [r4, #4]
   dfc56:	3404      	addle	r4, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dfc58:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
   dfc5a:	f04f 0901 	mov.w	r9, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dfc5e:	4598      	cmp	r8, r3
   dfc60:	dd05      	ble.n	dfc6e <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x5e>
      buffer_size *= dims_data[i];
   dfc62:	f854 2023 	ldr.w	r2, [r4, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dfc66:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
   dfc68:	fb02 f909 	mul.w	r9, r2, r9
   dfc6c:	e7f7      	b.n	dfc5e <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x4e>
   dfc6e:	4634      	mov	r4, r6
   dfc70:	463d      	mov	r5, r7
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dfc72:	2600      	movs	r6, #0
namespace reference_ops {

inline float RoundToNearest(float value) {
  auto floor_val = std::floor(value);
  auto diff = value - floor_val;
  if ((diff < 0.5f) ||
   dfc74:	eef6 8a00 	vmov.f32	s17, #96	; 0x3f000000  0.5
      ((diff == 0.5f) && (static_cast<int>(floor_val) % 2 == 0))) {
    return floor_val;
  } else {
    return floor_val = floor_val + 1.0f;
   dfc78:	eeb7 9a00 	vmov.f32	s18, #112	; 0x3f800000  1.0
}

inline void Round(const RuntimeShape& input_shape, const float* input_data,
                  const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
  for (int i = 0; i < flat_size; ++i) {
   dfc7c:	454e      	cmp	r6, r9
   dfc7e:	da1d      	bge.n	dfcbc <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0xac>
    // Note that this implementation matches that of tensorFlow tf.round
    // and corresponds to the bankers rounding method.
    // cfenv (for fesetround) is not yet supported universally on Android, so
    // using a work around.
    output_data[i] = RoundToNearest(input_data[i]);
   dfc80:	ecb5 8a01 	vldmia	r5!, {s16}
  using ::floor;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  floor(float __x)
  { return __builtin_floorf(__x); }
   dfc84:	eeb0 0a48 	vmov.f32	s0, s16
   dfc88:	f005 fbca 	bl	e5420 <floorf>

namespace reference_ops {

inline float RoundToNearest(float value) {
  auto floor_val = std::floor(value);
  auto diff = value - floor_val;
   dfc8c:	ee38 8a40 	vsub.f32	s16, s16, s0
  if ((diff < 0.5f) ||
   dfc90:	eeb4 8ae8 	vcmpe.f32	s16, s17
   dfc94:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   dfc98:	d40c      	bmi.n	dfcb4 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0xa4>
   dfc9a:	eeb4 8a68 	vcmp.f32	s16, s17
   dfc9e:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   dfca2:	d105      	bne.n	dfcb0 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0xa0>
      ((diff == 0.5f) && (static_cast<int>(floor_val) % 2 == 0))) {
   dfca4:	eefd 7ac0 	vcvt.s32.f32	s15, s0
   dfca8:	ee17 3a90 	vmov	r3, s15
   dfcac:	07db      	lsls	r3, r3, #31
   dfcae:	d501      	bpl.n	dfcb4 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0xa4>
    return floor_val;
  } else {
    return floor_val = floor_val + 1.0f;
   dfcb0:	ee30 0a09 	vadd.f32	s0, s0, s18
  for (int i = 0; i < flat_size; ++i) {
    // Note that this implementation matches that of tensorFlow tf.round
    // and corresponds to the bankers rounding method.
    // cfenv (for fesetround) is not yet supported universally on Android, so
    // using a work around.
    output_data[i] = RoundToNearest(input_data[i]);
   dfcb4:	eca4 0a01 	vstmia	r4!, {s0}
}

inline void Round(const RuntimeShape& input_shape, const float* input_data,
                  const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
  for (int i = 0; i < flat_size; ++i) {
   dfcb8:	3601      	adds	r6, #1
   dfcba:	e7df      	b.n	dfc7c <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x6c>
    // and corresponds to the bankers rounding method.
    // cfenv (for fesetround) is not yet supported universally on Android, so
    // using a work around.
    output_data[i] = RoundToNearest(input_data[i]);
  }
}
   dfcbc:	ecbd 8b04 	vpop	{d8-d9}
   dfcc0:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}

000dfcc4 <_ZN6tflite3ops5micro5round4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dfcc4:	b530      	push	{r4, r5, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dfcc6:	680a      	ldr	r2, [r1, #0]
   dfcc8:	6883      	ldr	r3, [r0, #8]
   dfcca:	6854      	ldr	r4, [r2, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dfccc:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dfcce:	2538      	movs	r5, #56	; 0x38
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dfcd0:	6852      	ldr	r2, [r2, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dfcd2:	fb05 3404 	mla	r4, r5, r4, r3
   dfcd6:	b08b      	sub	sp, #44	; 0x2c
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dfcd8:	fb05 3502 	mla	r5, r5, r2, r3
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
}

inline RuntimeShape GetTensorShape(const TfLiteTensor* tensor) {
  if (tensor == nullptr) {
   dfcdc:	b90c      	cbnz	r4, dfce2 <_ZN6tflite3ops5micro5round4EvalEP13TfLiteContextP10TfLiteNode+0x1e>
  // larger shapes are separately allocated.
  static constexpr int kMaxSmallSize = 4;

  RuntimeShape& operator=(RuntimeShape const&) = delete;

  RuntimeShape() : size_(0) {}
   dfcde:	9400      	str	r4, [sp, #0]
   dfce0:	e008      	b.n	dfcf4 <_ZN6tflite3ops5micro5round4EvalEP13TfLiteContextP10TfLiteNode+0x30>
    return RuntimeShape();
  }

  TfLiteIntArray* dims = tensor->dims;
  const int dims_size = dims->size;
   dfce2:	68a2      	ldr	r2, [r4, #8]
    for (int i = 0; i < shape_size; ++i) {
      SetDim(i, value);
    }
  }

  RuntimeShape(int dimensions_count, const int32* dims_data) : size_(0) {
   dfce4:	2300      	movs	r3, #0
   dfce6:	f852 1b04 	ldr.w	r1, [r2], #4
   dfcea:	9300      	str	r3, [sp, #0]
    ReplaceWith(dimensions_count, dims_data);
   dfcec:	4668      	mov	r0, sp
   dfcee:	f7f8 f849 	bl	d7d84 <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dfcf2:	6864      	ldr	r4, [r4, #4]
}

inline RuntimeShape GetTensorShape(const TfLiteTensor* tensor) {
  if (tensor == nullptr) {
   dfcf4:	b915      	cbnz	r5, dfcfc <_ZN6tflite3ops5micro5round4EvalEP13TfLiteContextP10TfLiteNode+0x38>
  // larger shapes are separately allocated.
  static constexpr int kMaxSmallSize = 4;

  RuntimeShape& operator=(RuntimeShape const&) = delete;

  RuntimeShape() : size_(0) {}
   dfcf6:	9505      	str	r5, [sp, #20]

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dfcf8:	462b      	mov	r3, r5
   dfcfa:	e008      	b.n	dfd0e <_ZN6tflite3ops5micro5round4EvalEP13TfLiteContextP10TfLiteNode+0x4a>
  if (tensor == nullptr) {
    return RuntimeShape();
  }

  TfLiteIntArray* dims = tensor->dims;
  const int dims_size = dims->size;
   dfcfc:	68aa      	ldr	r2, [r5, #8]
    for (int i = 0; i < shape_size; ++i) {
      SetDim(i, value);
    }
  }

  RuntimeShape(int dimensions_count, const int32* dims_data) : size_(0) {
   dfcfe:	2300      	movs	r3, #0
   dfd00:	f852 1b04 	ldr.w	r1, [r2], #4
   dfd04:	9305      	str	r3, [sp, #20]
    ReplaceWith(dimensions_count, dims_data);
   dfd06:	a805      	add	r0, sp, #20
   dfd08:	f7f8 f83c 	bl	d7d84 <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dfd0c:	686b      	ldr	r3, [r5, #4]
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  reference_ops::Round(GetTensorShape(input), GetTensorData<float>(input),
                       GetTensorShape(output), GetTensorData<float>(output));
   dfd0e:	aa05      	add	r2, sp, #20
   dfd10:	4621      	mov	r1, r4
   dfd12:	4668      	mov	r0, sp
   dfd14:	f7ff ff7c 	bl	dfc10 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf>
   dfd18:	a805      	add	r0, sp, #20
   dfd1a:	f7f6 fb94 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  reference_ops::Round(GetTensorShape(input), GetTensorData<float>(input),
   dfd1e:	4668      	mov	r0, sp
   dfd20:	f7f6 fb91 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                       GetTensorShape(output), GetTensorData<float>(output));

  return kTfLiteOk;
}
   dfd24:	2000      	movs	r0, #0
   dfd26:	b00b      	add	sp, #44	; 0x2c
   dfd28:	bd30      	pop	{r4, r5, pc}
	...

000dfd2c <_ZN6tflite3ops5micro14Register_ROUNDEv>:

TfLiteRegistration* Register_ROUND() {
  static TfLiteRegistration r = {/*init=*/nullptr,
                                 /*free=*/nullptr, round::Prepare, round::Eval};
  return &r;
}
   dfd2c:	4800      	ldr	r0, [pc, #0]	; (dfd30 <_ZN6tflite3ops5micro14Register_ROUNDEv+0x4>)
   dfd2e:	4770      	bx	lr
   dfd30:	2003c188 	.word	0x2003c188

000dfd34 <_ZN6tflite3ops5micro11activations4InitEP13TfLiteContextPKcj>:

}  // namespace

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
   dfd34:	2000      	movs	r0, #0
   dfd36:	4770      	bx	lr

000dfd38 <_ZN6tflite3ops5micro11activations4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
   dfd38:	4770      	bx	lr

000dfd3a <_ZN6tflite3ops5micro11activations14SoftmaxPrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus SoftmaxPrepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   dfd3a:	2000      	movs	r0, #0
   dfd3c:	4770      	bx	lr
	...

000dfd40 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>:
// This function implements the same computation as the ARMv7 NEON VQRDMULH
// instruction.
template <>
inline std::int32_t SaturatingRoundingDoublingHighMul(std::int32_t a,
                                                      std::int32_t b) {
  bool overflow = a == b && a == std::numeric_limits<std::int32_t>::min();
   dfd40:	4288      	cmp	r0, r1

// This function implements the same computation as the ARMv7 NEON VQRDMULH
// instruction.
template <>
inline std::int32_t SaturatingRoundingDoublingHighMul(std::int32_t a,
                                                      std::int32_t b) {
   dfd42:	b510      	push	{r4, lr}
  bool overflow = a == b && a == std::numeric_limits<std::int32_t>::min();
   dfd44:	d104      	bne.n	dfd50 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_+0x10>
   dfd46:	f100 4300 	add.w	r3, r0, #2147483648	; 0x80000000
   dfd4a:	425c      	negs	r4, r3
   dfd4c:	415c      	adcs	r4, r3
   dfd4e:	e000      	b.n	dfd52 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_+0x12>
   dfd50:	2400      	movs	r4, #0
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
   dfd52:	fb80 2301 	smull	r2, r3, r0, r1
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
   dfd56:	2a00      	cmp	r2, #0
   dfd58:	f173 0100 	sbcs.w	r1, r3, #0
   dfd5c:	490b      	ldr	r1, [pc, #44]	; (dfd8c <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_+0x4c>)
   dfd5e:	bfa8      	it	ge
   dfd60:	f04f 4180 	movge.w	r1, #1073741824	; 0x40000000
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
   dfd64:	b97c      	cbnz	r4, dfd86 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_+0x46>
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
   dfd66:	1852      	adds	r2, r2, r1
   dfd68:	eb43 73e1 	adc.w	r3, r3, r1, asr #31
   dfd6c:	2a00      	cmp	r2, #0
   dfd6e:	f173 0100 	sbcs.w	r1, r3, #0
   dfd72:	da04      	bge.n	dfd7e <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_+0x3e>
   dfd74:	f06f 4000 	mvn.w	r0, #2147483648	; 0x80000000
   dfd78:	2100      	movs	r1, #0
   dfd7a:	1812      	adds	r2, r2, r0
   dfd7c:	414b      	adcs	r3, r1
   dfd7e:	0fd0      	lsrs	r0, r2, #31
   dfd80:	ea40 0043 	orr.w	r0, r0, r3, lsl #1
   dfd84:	bd10      	pop	{r4, pc}
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
   dfd86:	f06f 4000 	mvn.w	r0, #2147483648	; 0x80000000
}
   dfd8a:	bd10      	pop	{r4, pc}
   dfd8c:	c0000001 	.word	0xc0000001

000dfd90 <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_>:
  return flat_size;
}

// A combination of MatchingFlatSize() and FlatSizeSkipDim().
inline int MatchingFlatSizeSkipDim(const RuntimeShape& shape, int skip_dim,
                                   const RuntimeShape& check_shape_0) {
   dfd90:	e92d 43f8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, lr}
   dfd94:	6806      	ldr	r6, [r0, #0]
   dfd96:	4604      	mov	r4, r0
   dfd98:	460f      	mov	r7, r1
   dfd9a:	4690      	mov	r8, r2
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   dfd9c:	2500      	movs	r5, #0
   dfd9e:	42b5      	cmp	r5, r6
   dfda0:	da10      	bge.n	dfdc4 <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x34>
    if (i != skip_dim) {
   dfda2:	42bd      	cmp	r5, r7
   dfda4:	d00c      	beq.n	dfdc0 <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x30>
      TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   dfda6:	4629      	mov	r1, r5
   dfda8:	4620      	mov	r0, r4
   dfdaa:	f7f6 fb57 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dfdae:	4629      	mov	r1, r5
   dfdb0:	4681      	mov	r9, r0
   dfdb2:	4640      	mov	r0, r8
   dfdb4:	f7f6 fb52 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dfdb8:	4581      	cmp	r9, r0
   dfdba:	d001      	beq.n	dfdc0 <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x30>
   dfdbc:	f004 fab6 	bl	e432c <abort>

// A combination of MatchingFlatSize() and FlatSizeSkipDim().
inline int MatchingFlatSizeSkipDim(const RuntimeShape& shape, int skip_dim,
                                   const RuntimeShape& check_shape_0) {
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   dfdc0:	3501      	adds	r5, #1
   dfdc2:	e7ec      	b.n	dfd9e <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0xe>
// Data is required to be contiguous, and so many operators can use either the
// full array flat size or the flat size with one dimension skipped (commonly
// the depth).
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
   dfdc4:	2f00      	cmp	r7, #0
   dfdc6:	dbf9      	blt.n	dfdbc <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x2c>
   dfdc8:	42b7      	cmp	r7, r6
   dfdca:	daf7      	bge.n	dfdbc <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x2c>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   dfdcc:	2e04      	cmp	r6, #4
   dfdce:	bfcc      	ite	gt
   dfdd0:	6864      	ldrgt	r4, [r4, #4]
   dfdd2:	3404      	addle	r4, #4
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
  const auto* dims_data = shape.DimsData();
  int flat_size = 1;
  for (int i = 0; i < dims_count; ++i) {
   dfdd4:	2300      	movs	r3, #0
// the depth).
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
  const auto* dims_data = shape.DimsData();
  int flat_size = 1;
   dfdd6:	2001      	movs	r0, #1
  for (int i = 0; i < dims_count; ++i) {
   dfdd8:	429e      	cmp	r6, r3
   dfdda:	dd07      	ble.n	dfdec <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x5c>
    flat_size *= (i == skip_dim) ? 1 : dims_data[i];
   dfddc:	429f      	cmp	r7, r3
   dfdde:	bf14      	ite	ne
   dfde0:	f854 2023 	ldrne.w	r2, [r4, r3, lsl #2]
   dfde4:	2201      	moveq	r2, #1
   dfde6:	4350      	muls	r0, r2
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
  const auto* dims_data = shape.DimsData();
  int flat_size = 1;
  for (int i = 0; i < dims_count; ++i) {
   dfde8:	3301      	adds	r3, #1
   dfdea:	e7f5      	b.n	dfdd8 <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x48>
    if (i != skip_dim) {
      TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
    }
  }
  return FlatSizeSkipDim(shape, skip_dim);
}
   dfdec:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}

000dfdf0 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf>:
namespace tflite {
namespace reference_ops {

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const float* input_data,
                    const RuntimeShape& output_shape, float* output_data) {
   dfdf0:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dfdf4:	ed2d 8b02 	vpush	{d8}
  const int trailing_dim = input_shape.DimensionsCount() - 1;
   dfdf8:	680e      	ldr	r6, [r1, #0]
namespace tflite {
namespace reference_ops {

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const float* input_data,
                    const RuntimeShape& output_shape, float* output_data) {
   dfdfa:	b089      	sub	sp, #36	; 0x24
   dfdfc:	460d      	mov	r5, r1
  const int trailing_dim = input_shape.DimensionsCount() - 1;
   dfdfe:	3e01      	subs	r6, #1
namespace tflite {
namespace reference_ops {

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const float* input_data,
                    const RuntimeShape& output_shape, float* output_data) {
   dfe00:	9002      	str	r0, [sp, #8]
  const int trailing_dim = input_shape.DimensionsCount() - 1;
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
   dfe02:	4631      	mov	r1, r6
   dfe04:	4628      	mov	r0, r5
namespace tflite {
namespace reference_ops {

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const float* input_data,
                    const RuntimeShape& output_shape, float* output_data) {
   dfe06:	4614      	mov	r4, r2
  const int trailing_dim = input_shape.DimensionsCount() - 1;
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
   dfe08:	461a      	mov	r2, r3
namespace tflite {
namespace reference_ops {

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const float* input_data,
                    const RuntimeShape& output_shape, float* output_data) {
   dfe0a:	461f      	mov	r7, r3
  const int trailing_dim = input_shape.DimensionsCount() - 1;
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
   dfe0c:	f7ff ffc0 	bl	dfd90 <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_>
}

// Get common shape dim, DCHECKing that they all agree.
inline int MatchingDim(const RuntimeShape& shape1, int index1,
                       const RuntimeShape& shape2, int index2) {
  TFLITE_DCHECK_EQ(shape1.Dims(index1), shape2.Dims(index2));
   dfe10:	4631      	mov	r1, r6
   dfe12:	9003      	str	r0, [sp, #12]
   dfe14:	4628      	mov	r0, r5
   dfe16:	f7f6 fb21 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dfe1a:	4631      	mov	r1, r6
   dfe1c:	4605      	mov	r5, r0
   dfe1e:	4638      	mov	r0, r7
   dfe20:	f7f6 fb1c 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   dfe24:	4285      	cmp	r5, r0
   dfe26:	d001      	beq.n	dfe2c <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x3c>
   dfe28:	f004 fa80 	bl	e432c <abort>
   dfe2c:	00ab      	lsls	r3, r5, #2
   dfe2e:	9e14      	ldr	r6, [sp, #80]	; 0x50

  for (int i = 0; i < outer_size; ++i) {
    // Find max element value which we'll use to ensure numerical stability
    // taking advantage of the following equality:
    // exp(x[i])/sum(exp(x[i])) == exp(x[i]+C)/sum(exp(x[i]+C))
    float max = std::numeric_limits<float>::lowest();
   dfe30:	ed9f 8a3f 	vldr	s16, [pc, #252]	; dff30 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x140>
   dfe34:	9301      	str	r3, [sp, #4]
   dfe36:	2700      	movs	r7, #0
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
  const int depth =
      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);

  for (int i = 0; i < outer_size; ++i) {
   dfe38:	9b03      	ldr	r3, [sp, #12]
   dfe3a:	429f      	cmp	r7, r3
   dfe3c:	da72      	bge.n	dff24 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x134>
    // Find max element value which we'll use to ensure numerical stability
    // taking advantage of the following equality:
    // exp(x[i])/sum(exp(x[i])) == exp(x[i]+C)/sum(exp(x[i]+C))
    float max = std::numeric_limits<float>::lowest();
   dfe3e:	ed8d 8a07 	vstr	s16, [sp, #28]
   dfe42:	4621      	mov	r1, r4
    for (int c = 0; c < depth; ++c) {
   dfe44:	2200      	movs	r2, #0
   dfe46:	42aa      	cmp	r2, r5
   dfe48:	da0f      	bge.n	dfe6a <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x7a>
      max = std::max(max, input_data[i * depth + c]);
   dfe4a:	460b      	mov	r3, r1
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   dfe4c:	ed93 7a00 	vldr	s14, [r3]
   dfe50:	eddd 7a07 	vldr	s15, [sp, #28]
   dfe54:	eeb4 7ae7 	vcmpe.f32	s14, s15
   dfe58:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
	return __b;
      return __a;
   dfe5c:	bfd8      	it	le
   dfe5e:	ab07      	addle	r3, sp, #28
   dfe60:	3104      	adds	r1, #4
   dfe62:	681b      	ldr	r3, [r3, #0]
   dfe64:	9307      	str	r3, [sp, #28]
  for (int i = 0; i < outer_size; ++i) {
    // Find max element value which we'll use to ensure numerical stability
    // taking advantage of the following equality:
    // exp(x[i])/sum(exp(x[i])) == exp(x[i]+C)/sum(exp(x[i]+C))
    float max = std::numeric_limits<float>::lowest();
    for (int c = 0; c < depth; ++c) {
   dfe66:	3201      	adds	r2, #1
   dfe68:	e7ed      	b.n	dfe46 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x56>
   dfe6a:	46a2      	mov	sl, r4
   dfe6c:	f04f 0800 	mov.w	r8, #0
   dfe70:	f04f 0900 	mov.w	r9, #0
      max = std::max(max, input_data[i * depth + c]);
    }

    // Compute sum.
    float sum = 0.f;
    for (int c = 0; c < depth; ++c) {
   dfe74:	45a8      	cmp	r8, r5
   dfe76:	da23      	bge.n	dfec0 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0xd0>
      sum += std::exp((input_data[i * depth + c] - max) * params.beta);
   dfe78:	ecba 7a01 	vldmia	sl!, {s14}
   dfe7c:	eddd 7a07 	vldr	s15, [sp, #28]
   dfe80:	ee77 7a67 	vsub.f32	s15, s14, s15
      max = std::max(max, input_data[i * depth + c]);
    }

    // Compute sum.
    float sum = 0.f;
    for (int c = 0; c < depth; ++c) {
   dfe84:	f108 0801 	add.w	r8, r8, #1
      sum += std::exp((input_data[i * depth + c] - max) * params.beta);
   dfe88:	ee17 0a90 	vmov	r0, s15
   dfe8c:	f007 f8b6 	bl	e6ffc <__aeabi_f2d>
   dfe90:	9b02      	ldr	r3, [sp, #8]
   dfe92:	e9d3 2300 	ldrd	r2, r3, [r3]
   dfe96:	f007 f905 	bl	e70a4 <__aeabi_dmul>
   dfe9a:	ec41 0b10 	vmov	d0, r0, r1
   dfe9e:	f005 fbdb 	bl	e5658 <exp>
   dfea2:	ec53 2b10 	vmov	r2, r3, d0
   dfea6:	4648      	mov	r0, r9
   dfea8:	e9cd 2304 	strd	r2, r3, [sp, #16]
   dfeac:	f007 f8a6 	bl	e6ffc <__aeabi_f2d>
   dfeb0:	e9dd 2304 	ldrd	r2, r3, [sp, #16]
   dfeb4:	f006 ff44 	bl	e6d40 <__adddf3>
   dfeb8:	f007 fbd6 	bl	e7668 <__aeabi_d2f>
   dfebc:	4681      	mov	r9, r0
      max = std::max(max, input_data[i * depth + c]);
    }

    // Compute sum.
    float sum = 0.f;
    for (int c = 0; c < depth; ++c) {
   dfebe:	e7d9      	b.n	dfe74 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x84>
   dfec0:	46b2      	mov	sl, r6
   dfec2:	46a3      	mov	fp, r4
   dfec4:	f04f 0800 	mov.w	r8, #0
      sum += std::exp((input_data[i * depth + c] - max) * params.beta);
    }

    // Compute result.
    for (int c = 0; c < depth; ++c) {
   dfec8:	45a8      	cmp	r8, r5
   dfeca:	da26      	bge.n	dff1a <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x12a>
      output_data[i * depth + c] =
          std::exp((input_data[i * depth + c] - max) * params.beta) / sum;
   dfecc:	ecbb 7a01 	vldmia	fp!, {s14}
   dfed0:	eddd 7a07 	vldr	s15, [sp, #28]
   dfed4:	ee77 7a67 	vsub.f32	s15, s14, s15
    for (int c = 0; c < depth; ++c) {
      sum += std::exp((input_data[i * depth + c] - max) * params.beta);
    }

    // Compute result.
    for (int c = 0; c < depth; ++c) {
   dfed8:	f108 0801 	add.w	r8, r8, #1
      output_data[i * depth + c] =
          std::exp((input_data[i * depth + c] - max) * params.beta) / sum;
   dfedc:	ee17 0a90 	vmov	r0, s15
   dfee0:	f007 f88c 	bl	e6ffc <__aeabi_f2d>
   dfee4:	9b02      	ldr	r3, [sp, #8]
   dfee6:	e9d3 2300 	ldrd	r2, r3, [r3]
   dfeea:	f007 f8db 	bl	e70a4 <__aeabi_dmul>
   dfeee:	ec41 0b10 	vmov	d0, r0, r1
   dfef2:	f005 fbb1 	bl	e5658 <exp>
   dfef6:	4648      	mov	r0, r9
   dfef8:	ed8d 0b04 	vstr	d0, [sp, #16]
   dfefc:	f007 f87e 	bl	e6ffc <__aeabi_f2d>
   dff00:	ed9d 0b04 	vldr	d0, [sp, #16]
   dff04:	4602      	mov	r2, r0
   dff06:	460b      	mov	r3, r1
   dff08:	ec51 0b10 	vmov	r0, r1, d0
   dff0c:	f007 f9f4 	bl	e72f8 <__aeabi_ddiv>
   dff10:	f007 fbaa 	bl	e7668 <__aeabi_d2f>
   dff14:	f84a 0b04 	str.w	r0, [sl], #4
    for (int c = 0; c < depth; ++c) {
      sum += std::exp((input_data[i * depth + c] - max) * params.beta);
    }

    // Compute result.
    for (int c = 0; c < depth; ++c) {
   dff18:	e7d6      	b.n	dfec8 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0xd8>
   dff1a:	9b01      	ldr	r3, [sp, #4]
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
  const int depth =
      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);

  for (int i = 0; i < outer_size; ++i) {
   dff1c:	3701      	adds	r7, #1
   dff1e:	441c      	add	r4, r3
   dff20:	441e      	add	r6, r3
   dff22:	e789      	b.n	dfe38 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x48>
    for (int c = 0; c < depth; ++c) {
      output_data[i * depth + c] =
          std::exp((input_data[i * depth + c] - max) * params.beta) / sum;
    }
  }
}
   dff24:	b009      	add	sp, #36	; 0x24
   dff26:	ecbd 8b02 	vpop	{d8}
   dff2a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dff2e:	bf00      	nop
   dff30:	ff7fffff 	.word	0xff7fffff

000dff34 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf>:
  }
}

// Performs softmax along the input of size (input_size * batch_size).
inline void Softmax(const float* in, const int input_size, const int batch_size,
                    const float beta, float* out) {
   dff34:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dff38:	ed2d 8b04 	vpush	{d8-d9}
   dff3c:	4605      	mov	r5, r0
   dff3e:	b085      	sub	sp, #20
   dff40:	460e      	mov	r6, r1
   dff42:	4693      	mov	fp, r2
   dff44:	eeb0 9a40 	vmov.f32	s18, s0
   dff48:	461c      	mov	r4, r3
    for (int i = 0; i < input_size; i++) {
      out[i] *= reciprocal_sum_exp;
    }

    // Advance in and out pointers for the next batch.
    in += input_size;
   dff4a:	ea4f 0a81 	mov.w	sl, r1, lsl #2
inline void Softmax(const float* in, const int input_size, const int batch_size,
                    const float beta, float* out) {
  //  TF_LITE_ASSERT(input_size > 0);

  // For each batch
  for (int b = 0; b < batch_size; b++) {
   dff4e:	2700      	movs	r7, #0
      out[i] = std::exp((in[i] - max_coeff) * beta);
      exp_sum += out[i];
    }

    // Divide by the sum of exps.
    float reciprocal_sum_exp = 1.f / exp_sum;
   dff50:	eef7 9a00 	vmov.f32	s19, #112	; 0x3f800000  1.0
inline void Softmax(const float* in, const int input_size, const int batch_size,
                    const float beta, float* out) {
  //  TF_LITE_ASSERT(input_size > 0);

  // For each batch
  for (int b = 0; b < batch_size; b++) {
   dff54:	455f      	cmp	r7, fp
   dff56:	da3e      	bge.n	dffd6 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0xa2>
    // Find the max coeff.
    float max_coeff = in[0];
   dff58:	462b      	mov	r3, r5
   dff5a:	ecb3 8a01 	vldmia	r3!, {s16}
    for (int i = 1; i < input_size; i++) {
   dff5e:	2101      	movs	r1, #1
   dff60:	42b1      	cmp	r1, r6
   dff62:	da0a      	bge.n	dff7a <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x46>
      if (in[i] > max_coeff) max_coeff = in[i];
   dff64:	ecf3 7a01 	vldmia	r3!, {s15}
   dff68:	eeb4 8a67 	vcmp.f32	s16, s15
   dff6c:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   dff70:	bf48      	it	mi
   dff72:	eeb0 8a67 	vmovmi.f32	s16, s15

  // For each batch
  for (int b = 0; b < batch_size; b++) {
    // Find the max coeff.
    float max_coeff = in[0];
    for (int i = 1; i < input_size; i++) {
   dff76:	3101      	adds	r1, #1
   dff78:	e7f2      	b.n	dff60 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x2c>
   dff7a:	eddf 8a19 	vldr	s17, [pc, #100]	; dffe0 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0xac>
   dff7e:	462a      	mov	r2, r5
   dff80:	46a0      	mov	r8, r4
   dff82:	4623      	mov	r3, r4
   dff84:	f04f 0900 	mov.w	r9, #0
      if (in[i] > max_coeff) max_coeff = in[i];
    }

    // Compute the normalized sum of exps.
    float exp_sum = 0.0;
    for (int i = 0; i < input_size; i++) {
   dff88:	45b1      	cmp	r9, r6
   dff8a:	9302      	str	r3, [sp, #8]
   dff8c:	da12      	bge.n	dffb4 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x80>
      out[i] = std::exp((in[i] - max_coeff) * beta);
   dff8e:	ecb2 0a01 	vldmia	r2!, {s0}
  using ::exp;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  exp(float __x)
  { return __builtin_expf(__x); }
   dff92:	ee30 0a48 	vsub.f32	s0, s0, s16
   dff96:	9201      	str	r2, [sp, #4]
   dff98:	ee20 0a09 	vmul.f32	s0, s0, s18
   dff9c:	9203      	str	r2, [sp, #12]
   dff9e:	f005 fbdf 	bl	e5760 <expf>
   dffa2:	9b02      	ldr	r3, [sp, #8]
      if (in[i] > max_coeff) max_coeff = in[i];
    }

    // Compute the normalized sum of exps.
    float exp_sum = 0.0;
    for (int i = 0; i < input_size; i++) {
   dffa4:	9a01      	ldr	r2, [sp, #4]
      out[i] = std::exp((in[i] - max_coeff) * beta);
   dffa6:	eca3 0a01 	vstmia	r3!, {s0}
      exp_sum += out[i];
   dffaa:	ee78 8a80 	vadd.f32	s17, s17, s0
      if (in[i] > max_coeff) max_coeff = in[i];
    }

    // Compute the normalized sum of exps.
    float exp_sum = 0.0;
    for (int i = 0; i < input_size; i++) {
   dffae:	f109 0901 	add.w	r9, r9, #1
   dffb2:	e7e9      	b.n	dff88 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x54>
      out[i] = std::exp((in[i] - max_coeff) * beta);
      exp_sum += out[i];
    }

    // Divide by the sum of exps.
    float reciprocal_sum_exp = 1.f / exp_sum;
   dffb4:	ee89 7aa8 	vdiv.f32	s14, s19, s17
    for (int i = 0; i < input_size; i++) {
   dffb8:	2300      	movs	r3, #0
   dffba:	42b3      	cmp	r3, r6
   dffbc:	da07      	bge.n	dffce <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x9a>
   dffbe:	3301      	adds	r3, #1
      out[i] *= reciprocal_sum_exp;
   dffc0:	edd8 7a00 	vldr	s15, [r8]
   dffc4:	ee67 7a87 	vmul.f32	s15, s15, s14
   dffc8:	ece8 7a01 	vstmia	r8!, {s15}
      exp_sum += out[i];
    }

    // Divide by the sum of exps.
    float reciprocal_sum_exp = 1.f / exp_sum;
    for (int i = 0; i < input_size; i++) {
   dffcc:	e7f5      	b.n	dffba <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x86>
      out[i] *= reciprocal_sum_exp;
    }

    // Advance in and out pointers for the next batch.
    in += input_size;
   dffce:	4455      	add	r5, sl
    out += input_size;
   dffd0:	4454      	add	r4, sl
inline void Softmax(const float* in, const int input_size, const int batch_size,
                    const float beta, float* out) {
  //  TF_LITE_ASSERT(input_size > 0);

  // For each batch
  for (int b = 0; b < batch_size; b++) {
   dffd2:	3701      	adds	r7, #1
   dffd4:	e7be      	b.n	dff54 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x20>

    // Advance in and out pointers for the next batch.
    in += input_size;
    out += input_size;
  }
}
   dffd6:	b005      	add	sp, #20
   dffd8:	ecbd 8b04 	vpop	{d8-d9}
   dffdc:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dffe0:	00000000 	.word	0x00000000

000dffe4 <_ZN6tflite3ops5micro11activations14Softmax1DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams>:

// Takes a 1D tensor and performs softmax along it.
void Softmax1DFloat(const TfLiteTensor* input, TfLiteTensor* output,
                    TfLiteSoftmaxParams* params) {
   dffe4:	b510      	push	{r4, lr}
  const int input_size = input->dims->data[0];
   dffe6:	6884      	ldr	r4, [r0, #8]
  tflite::reference_ops::Softmax(input->data.f, input_size, 1, params->beta,
                                 output->data.f);
   dffe8:	684b      	ldr	r3, [r1, #4]
   dffea:	ed92 0a00 	vldr	s0, [r2]
   dffee:	6861      	ldr	r1, [r4, #4]
   dfff0:	6840      	ldr	r0, [r0, #4]
   dfff2:	2201      	movs	r2, #1
   dfff4:	f7ff ff9e 	bl	dff34 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf>
   dfff8:	bd10      	pop	{r4, pc}

000dfffa <_ZN6tflite3ops5micro11activations14Softmax2DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams>:
}

// Takes a 2D tensor and perform softmax along the last dimension.
void Softmax2DFloat(const TfLiteTensor* input, TfLiteTensor* output,
                    TfLiteSoftmaxParams* params) {
   dfffa:	b510      	push	{r4, lr}
  const int batch_size = input->dims->data[0];
   dfffc:	6884      	ldr	r4, [r0, #8]
  const int input_size = input->dims->data[1];
  tflite::reference_ops::Softmax(input->data.f, input_size, batch_size,
                                 params->beta, output->data.f);
   dfffe:	684b      	ldr	r3, [r1, #4]
   e0000:	ed92 0a00 	vldr	s0, [r2]
   e0004:	68a1      	ldr	r1, [r4, #8]
   e0006:	6862      	ldr	r2, [r4, #4]
   e0008:	6840      	ldr	r0, [r0, #4]
   e000a:	f7ff ff93 	bl	dff34 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf>
   e000e:	bd10      	pop	{r4, pc}

000e0010 <_ZN6tflite3ops5micro11activations14Softmax4DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams>:
                                 GetTensorData<uint8_t>(output));
}

// Takes a 4D tensor and perform softmax along the forth dimension.
void Softmax4DFloat(const TfLiteTensor* input, TfLiteTensor* output,
                    TfLiteSoftmaxParams* params) {
   e0010:	b530      	push	{r4, r5, lr}
   e0012:	4604      	mov	r4, r0
   e0014:	b097      	sub	sp, #92	; 0x5c
  SoftmaxParams op_params;
  op_params.beta = params->beta;
   e0016:	6810      	ldr	r0, [r2, #0]
                                 GetTensorData<uint8_t>(output));
}

// Takes a 4D tensor and perform softmax along the forth dimension.
void Softmax4DFloat(const TfLiteTensor* input, TfLiteTensor* output,
                    TfLiteSoftmaxParams* params) {
   e0018:	460d      	mov	r5, r1
  SoftmaxParams op_params;
  op_params.beta = params->beta;
   e001a:	f006 ffef 	bl	e6ffc <__aeabi_f2d>
   e001e:	e9cd 010c 	strd	r0, r1, [sp, #48]	; 0x30
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
   e0022:	4621      	mov	r1, r4
   e0024:	a802      	add	r0, sp, #8
   e0026:	f7f6 fcbe 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e002a:	b104      	cbz	r4, e002e <_ZN6tflite3ops5micro11activations14Softmax4DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams+0x1e>
   e002c:	6864      	ldr	r4, [r4, #4]
      GetTensorShape(output), GetTensorData<float>(output));
   e002e:	4629      	mov	r1, r5
   e0030:	a807      	add	r0, sp, #28
   e0032:	f7f6 fcb8 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e0036:	b105      	cbz	r5, e003a <_ZN6tflite3ops5micro11activations14Softmax4DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams+0x2a>
   e0038:	686d      	ldr	r5, [r5, #4]
   e003a:	9500      	str	r5, [sp, #0]
   e003c:	ab07      	add	r3, sp, #28
   e003e:	4622      	mov	r2, r4
   e0040:	a902      	add	r1, sp, #8
   e0042:	a80c      	add	r0, sp, #48	; 0x30
   e0044:	f7ff fed4 	bl	dfdf0 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf>
   e0048:	a807      	add	r0, sp, #28
   e004a:	f7f6 f9fc 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
void Softmax4DFloat(const TfLiteTensor* input, TfLiteTensor* output,
                    TfLiteSoftmaxParams* params) {
  SoftmaxParams op_params;
  op_params.beta = params->beta;
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
   e004e:	a802      	add	r0, sp, #8
   e0050:	f7f6 f9f9 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
      GetTensorShape(output), GetTensorData<float>(output));
}
   e0054:	b017      	add	sp, #92	; 0x5c
   e0056:	bd30      	pop	{r4, r5, pc}

000e0058 <_ZN6tflite3ops5micro16Register_SOFTMAXEv>:
TfLiteRegistration* Register_SOFTMAX() {
  static TfLiteRegistration r = {activations::Init, activations::Free,
                                 activations::SoftmaxPrepare,
                                 activations::SoftmaxEval};
  return &r;
}
   e0058:	4800      	ldr	r0, [pc, #0]	; (e005c <_ZN6tflite3ops5micro16Register_SOFTMAXEv+0x4>)
   e005a:	4770      	bx	lr
   e005c:	2003c1a8 	.word	0x2003c1a8

000e0060 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i>:
}

// Correctly-rounded-to-nearest division by a power-of-two.
// Also known as a rounding arithmetic right shift.
template <typename IntegerType>
inline IntegerType RoundingDivideByPOT(IntegerType x, int exponent) {
   e0060:	b538      	push	{r3, r4, r5, lr}
  assert(exponent >= 0);
   e0062:	1e0d      	subs	r5, r1, #0
}

// Correctly-rounded-to-nearest division by a power-of-two.
// Also known as a rounding arithmetic right shift.
template <typename IntegerType>
inline IntegerType RoundingDivideByPOT(IntegerType x, int exponent) {
   e0064:	4604      	mov	r4, r0
  assert(exponent >= 0);
   e0066:	da04      	bge.n	e0072 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x12>
   e0068:	4b0f      	ldr	r3, [pc, #60]	; (e00a8 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x48>)
   e006a:	4a10      	ldr	r2, [pc, #64]	; (e00ac <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x4c>)
   e006c:	f44f 71b3 	mov.w	r1, #358	; 0x166
   e0070:	e005      	b.n	e007e <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x1e>
  assert(exponent <= 31);
   e0072:	2d1f      	cmp	r5, #31
   e0074:	dd06      	ble.n	e0084 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x24>
   e0076:	4b0e      	ldr	r3, [pc, #56]	; (e00b0 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x50>)
   e0078:	4a0c      	ldr	r2, [pc, #48]	; (e00ac <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x4c>)
   e007a:	f240 1167 	movw	r1, #359	; 0x167
   e007e:	480d      	ldr	r0, [pc, #52]	; (e00b4 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x54>)
   e0080:	f004 f964 	bl	e434c <__assert_func>
  const IntegerType mask = Dup<IntegerType>((1ll << exponent) - 1);
   e0084:	462a      	mov	r2, r5
   e0086:	2001      	movs	r0, #1
   e0088:	2100      	movs	r1, #0
   e008a:	f006 fe47 	bl	e6d1c <__aeabi_llsl>
   e008e:	3801      	subs	r0, #1
  const IntegerType one = Dup<IntegerType>(1);
  const IntegerType remainder = BitAnd(x, mask);
  const IntegerType threshold =
      Add(ShiftRight(mask, 1), BitAnd(MaskIfLessThan(x, zero), one));
  return Add(ShiftRight(x, exponent),
             BitAnd(MaskIfGreaterThan(remainder, threshold), one));
   e0090:	1043      	asrs	r3, r0, #1
   e0092:	ea00 0204 	and.w	r2, r0, r4
   e0096:	eb03 73d4 	add.w	r3, r3, r4, lsr #31
   e009a:	fa44 f005 	asr.w	r0, r4, r5
}
   e009e:	429a      	cmp	r2, r3
   e00a0:	bfc8      	it	gt
   e00a2:	3001      	addgt	r0, #1
   e00a4:	bd38      	pop	{r3, r4, r5, pc}
   e00a6:	bf00      	nop
   e00a8:	000e9684 	.word	0x000e9684
   e00ac:	000ea510 	.word	0x000ea510
   e00b0:	000e9731 	.word	0x000e9731
   e00b4:	000e9692 	.word	0x000e9692

000e00b8 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>:
// A FixedPoint multiplication is just a
// SaturatingRoundingDoublingHighMul operation on the underlying
// raw integer values. The IntegerBits simply add up, as is obvious
// from the fact that the range is [-2^IntegerBits, 2^IntegerBits).
template <typename tRawType, int tIntegerBits_a, int tIntegerBits_b>
FixedPoint<tRawType, tIntegerBits_a + tIntegerBits_b> operator*(
   e00b8:	b508      	push	{r3, lr}
    FixedPoint<tRawType, tIntegerBits_a> a,
    FixedPoint<tRawType, tIntegerBits_b> b) {
  FixedPoint<tRawType, tIntegerBits_a + tIntegerBits_b> c;
  c.raw() = SaturatingRoundingDoublingHighMul(a.raw(), b.raw());
   e00ba:	f7ff fe41 	bl	dfd40 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>
  return c;
}
   e00be:	bd08      	pop	{r3, pc}

000e00c0 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>:
// fixed-point value, regardless of the actual Scalar type. This allows
// writing generic code that applies just as well to the 32-bit and 16-bit
// cases. In the 16-bit case, the raw integer value is internally
// rounding-shifted by 16 bits to the right.
template <typename FixedPointType>
inline typename FixedPointType::ScalarRawType RescaleConstantInitializer(
   e00c0:	b508      	push	{r3, lr}
    std::int32_t int32_value) {
  typedef typename FixedPointType::ScalarRawType ScalarRawType;
  static constexpr int ScalarTypeBits = 8 * sizeof(ScalarRawType);
  return static_cast<ScalarRawType>(
      RoundingDivideByPOT<std::int32_t>(int32_value, 32 - ScalarTypeBits));
   e00c2:	2100      	movs	r1, #0
   e00c4:	f7ff ffcc 	bl	e0060 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i>
}
   e00c8:	bd08      	pop	{r3, pc}
	...

000e00cc <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_>:

// Implementation of logistic function.

// Returns 1 / (1 + x) for x in (0, 1).
template <typename tRawType>
FixedPoint<tRawType, 0> one_over_one_plus_x_for_x_in_0_1(
   e00cc:	e92d 4ff8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, lr}

template <>
inline std::int32_t RoundingHalfSum(std::int32_t a, std::int32_t b) {
  std::int64_t a64 = a;
  std::int64_t b64 = b;
  std::int64_t sum = a64 + b64;
   e00d0:	f06f 4600 	mvn.w	r6, #2147483648	; 0x80000000
   e00d4:	1833      	adds	r3, r6, r0
   e00d6:	f04f 0700 	mov.w	r7, #0
   e00da:	eb47 74e0 	adc.w	r4, r7, r0, asr #31
   e00de:	4618      	mov	r0, r3
  std::int64_t sign = sum >= 0 ? 1 : -1;
   e00e0:	1c63      	adds	r3, r4, #1
   e00e2:	bf05      	ittet	eq
   e00e4:	f1b0 3fff 	cmpeq.w	r0, #4294967295	; 0xffffffff
   e00e8:	4602      	moveq	r2, r0
   e00ea:	2201      	movne	r2, #1
   e00ec:	4623      	moveq	r3, r4
   e00ee:	bf18      	it	ne
   e00f0:	2300      	movne	r3, #0
  return static_cast<std::int32_t>((sum + sign) / 2);
   e00f2:	1886      	adds	r6, r0, r2
   e00f4:	eb44 0703 	adc.w	r7, r4, r3
   e00f8:	0ffb      	lsrs	r3, r7, #31
   e00fa:	18f6      	adds	r6, r6, r3
  F0 half_denominator = RoundingHalfSum(a, F0::One());
  // Newton-Raphson division
  // https://en.wikipedia.org/wiki/Division_algorithm#Newton.E2.80.93Raphson_division
  // Refer to that page for the logic behind the 48/17 and 32/17 constants.
  const F2 constant_48_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, 1515870810, 48.0 / 17.0);
   e00fc:	f04f 305a 	mov.w	r0, #1515870810	; 0x5a5a5a5a
inline std::int32_t RoundingHalfSum(std::int32_t a, std::int32_t b) {
  std::int64_t a64 = a;
  std::int64_t b64 = b;
  std::int64_t sum = a64 + b64;
  std::int64_t sign = sum >= 0 ? 1 : -1;
  return static_cast<std::int32_t>((sum + sign) / 2);
   e0100:	f147 0700 	adc.w	r7, r7, #0
  F0 half_denominator = RoundingHalfSum(a, F0::One());
  // Newton-Raphson division
  // https://en.wikipedia.org/wiki/Division_algorithm#Newton.E2.80.93Raphson_division
  // Refer to that page for the logic behind the 48/17 and 32/17 constants.
  const F2 constant_48_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, 1515870810, 48.0 / 17.0);
   e0104:	f7ff ffdc 	bl	e00c0 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
   e0108:	4604      	mov	r4, r0
  const F2 constant_neg_32_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, -1010580540, -32.0 / 17.0);
   e010a:	4840      	ldr	r0, [pc, #256]	; (e020c <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x140>)
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e010c:	f8df b100 	ldr.w	fp, [pc, #256]	; e0210 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x144>
  // https://en.wikipedia.org/wiki/Division_algorithm#Newton.E2.80.93Raphson_division
  // Refer to that page for the logic behind the 48/17 and 32/17 constants.
  const F2 constant_48_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, 1515870810, 48.0 / 17.0);
  const F2 constant_neg_32_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, -1010580540, -32.0 / 17.0);
   e0110:	f7ff ffd6 	bl	e00c0 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
inline std::int32_t RoundingHalfSum(std::int32_t a, std::int32_t b) {
  std::int64_t a64 = a;
  std::int64_t b64 = b;
  std::int64_t sum = a64 + b64;
  std::int64_t sign = sum >= 0 ? 1 : -1;
  return static_cast<std::int32_t>((sum + sign) / 2);
   e0114:	107f      	asrs	r7, r7, #1
   e0116:	ea4f 0636 	mov.w	r6, r6, rrx
template <typename tRawType, int tIntegerBits_a, int tIntegerBits_b>
FixedPoint<tRawType, tIntegerBits_a + tIntegerBits_b> operator*(
    FixedPoint<tRawType, tIntegerBits_a> a,
    FixedPoint<tRawType, tIntegerBits_b> b) {
  FixedPoint<tRawType, tIntegerBits_a + tIntegerBits_b> c;
  c.raw() = SaturatingRoundingDoublingHighMul(a.raw(), b.raw());
   e011a:	4601      	mov	r1, r0
   e011c:	4630      	mov	r0, r6
   e011e:	f7ff fe0f 	bl	dfd40 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>
inline std::int32_t RoundingHalfSum(std::int32_t a, std::int32_t b) {
  std::int64_t a64 = a;
  std::int64_t b64 = b;
  std::int64_t sum = a64 + b64;
  std::int64_t sign = sum >= 0 ? 1 : -1;
  return static_cast<std::int32_t>((sum + sign) / 2);
   e0122:	46b2      	mov	sl, r6
}

// Integer addition. Not saturating. Overflow is undefined behavior.
template <typename tIntegerType>
tIntegerType Add(tIntegerType a, tIntegerType b) {
  return a + b;
   e0124:	4404      	add	r4, r0
   e0126:	2503      	movs	r5, #3
  const auto min = std::numeric_limits<tIntegerType>::min();
  const auto max = std::numeric_limits<tIntegerType>::max();
  return wide_shifted < min
             ? min
             : wide_shifted > max ? max
                                  : static_cast<tIntegerType>(wide_shifted);
   e0128:	f06f 4600 	mvn.w	r6, #2147483648	; 0x80000000
   e012c:	2700      	movs	r7, #0
template <typename tRawType, int tIntegerBits_a, int tIntegerBits_b>
FixedPoint<tRawType, tIntegerBits_a + tIntegerBits_b> operator*(
    FixedPoint<tRawType, tIntegerBits_a> a,
    FixedPoint<tRawType, tIntegerBits_b> b) {
  FixedPoint<tRawType, tIntegerBits_a + tIntegerBits_b> c;
  c.raw() = SaturatingRoundingDoublingHighMul(a.raw(), b.raw());
   e012e:	4621      	mov	r1, r4
   e0130:	4650      	mov	r0, sl
   e0132:	f7ff fe05 	bl	dfd40 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>
   e0136:	f1c0 5100 	rsb	r1, r0, #536870912	; 0x20000000
   e013a:	4620      	mov	r0, r4
   e013c:	f7ff fe00 	bl	dfd40 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e0140:	f1b0 5f00 	cmp.w	r0, #536870912	; 0x20000000
   e0144:	da07      	bge.n	e0156 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x8a>
   e0146:	4558      	cmp	r0, fp
   e0148:	f04f 31ff 	mov.w	r1, #4294967295	; 0xffffffff
   e014c:	f04f 0e00 	mov.w	lr, #0
   e0150:	bfa8      	it	ge
   e0152:	2100      	movge	r1, #0
   e0154:	e002      	b.n	e015c <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x90>
   e0156:	f04f 3eff 	mov.w	lr, #4294967295	; 0xffffffff
   e015a:	2100      	movs	r1, #0
//
// tIntegerType may be int32 or any narrower signed type.
template <typename tIntegerType>
tIntegerType ShiftLeft(tIntegerType a, int offset) {
  const std::int64_t wide_a = static_cast<std::int64_t>(a);
  const std::int64_t wide_shifted = wide_a * (1 << offset);
   e015c:	17c3      	asrs	r3, r0, #31
   e015e:	ea4f 0983 	mov.w	r9, r3, lsl #2
   e0162:	ea4f 0880 	mov.w	r8, r0, lsl #2
   e0166:	ea49 7990 	orr.w	r9, r9, r0, lsr #30
  const auto min = std::numeric_limits<tIntegerType>::min();
  const auto max = std::numeric_limits<tIntegerType>::max();
  return wide_shifted < min
             ? min
             : wide_shifted > max ? max
                                  : static_cast<tIntegerType>(wide_shifted);
   e016a:	f1b8 4f00 	cmp.w	r8, #2147483648	; 0x80000000
   e016e:	f179 33ff 	sbcs.w	r3, r9, #4294967295	; 0xffffffff
   e0172:	db07      	blt.n	e0184 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0xb8>
   e0174:	4546      	cmp	r6, r8
   e0176:	eb77 0309 	sbcs.w	r3, r7, r9
   e017a:	bfac      	ite	ge
   e017c:	4643      	movge	r3, r8
   e017e:	f06f 4300 	mvnlt.w	r3, #2147483648	; 0x80000000
   e0182:	e001      	b.n	e0188 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0xbc>
   e0184:	f04f 4300 	mov.w	r3, #2147483648	; 0x80000000
}

// Integer addition. Not saturating. Overflow is undefined behavior.
template <typename tIntegerType>
tIntegerType Add(tIntegerType a, tIntegerType b) {
  return a + b;
   e0188:	ea6f 020e 	mvn.w	r2, lr
   e018c:	4013      	ands	r3, r2
   e018e:	f02e 4e00 	bic.w	lr, lr, #2147483648	; 0x80000000
   e0192:	ea83 0e0e 	eor.w	lr, r3, lr
   e0196:	43cb      	mvns	r3, r1
   e0198:	ea0e 0e03 	and.w	lr, lr, r3
   e019c:	f001 4100 	and.w	r1, r1, #2147483648	; 0x80000000
   e01a0:	ea8e 0101 	eor.w	r1, lr, r1
  const F2 constant_48_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, 1515870810, 48.0 / 17.0);
  const F2 constant_neg_32_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, -1010580540, -32.0 / 17.0);
  F2 x = constant_48_over_17 + half_denominator * constant_neg_32_over_17;
  for (int i = 0; i < 3; i++) {
   e01a4:	3d01      	subs	r5, #1
}

// Integer addition. Not saturating. Overflow is undefined behavior.
template <typename tIntegerType>
tIntegerType Add(tIntegerType a, tIntegerType b) {
  return a + b;
   e01a6:	440c      	add	r4, r1
  const F2 constant_48_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, 1515870810, 48.0 / 17.0);
  const F2 constant_neg_32_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, -1010580540, -32.0 / 17.0);
  F2 x = constant_48_over_17 + half_denominator * constant_neg_32_over_17;
  for (int i = 0; i < 3; i++) {
   e01a8:	d1c1      	bne.n	e012e <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x62>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e01aa:	f1b4 4f80 	cmp.w	r4, #1073741824	; 0x40000000
   e01ae:	da07      	bge.n	e01c0 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0xf4>
   e01b0:	f1b4 4f40 	cmp.w	r4, #3221225472	; 0xc0000000
   e01b4:	f04f 30ff 	mov.w	r0, #4294967295	; 0xffffffff
   e01b8:	4629      	mov	r1, r5
   e01ba:	bfc8      	it	gt
   e01bc:	2000      	movgt	r0, #0
   e01be:	e002      	b.n	e01c6 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0xfa>
   e01c0:	f04f 31ff 	mov.w	r1, #4294967295	; 0xffffffff
   e01c4:	4628      	mov	r0, r5
//
// tIntegerType may be int32 or any narrower signed type.
template <typename tIntegerType>
tIntegerType ShiftLeft(tIntegerType a, int offset) {
  const std::int64_t wide_a = static_cast<std::int64_t>(a);
  const std::int64_t wide_shifted = wide_a * (1 << offset);
   e01c6:	1922      	adds	r2, r4, r4
   e01c8:	ea4f 73e4 	mov.w	r3, r4, asr #31
   e01cc:	415b      	adcs	r3, r3
  const auto min = std::numeric_limits<tIntegerType>::min();
  const auto max = std::numeric_limits<tIntegerType>::max();
  return wide_shifted < min
             ? min
             : wide_shifted > max ? max
                                  : static_cast<tIntegerType>(wide_shifted);
   e01ce:	f1b2 4f00 	cmp.w	r2, #2147483648	; 0x80000000
   e01d2:	f173 34ff 	sbcs.w	r4, r3, #4294967295	; 0xffffffff
   e01d6:	db0a      	blt.n	e01ee <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x122>
   e01d8:	f06f 4400 	mvn.w	r4, #2147483648	; 0x80000000
   e01dc:	4294      	cmp	r4, r2
   e01de:	f04f 0500 	mov.w	r5, #0
   e01e2:	eb75 0403 	sbcs.w	r4, r5, r3
   e01e6:	bfb8      	it	lt
   e01e8:	f06f 4200 	mvnlt.w	r2, #2147483648	; 0x80000000
   e01ec:	e001      	b.n	e01f2 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x126>
   e01ee:	f04f 4200 	mov.w	r2, #2147483648	; 0x80000000
   e01f2:	ea22 0201 	bic.w	r2, r2, r1
   e01f6:	f021 4100 	bic.w	r1, r1, #2147483648	; 0x80000000
   e01fa:	404a      	eors	r2, r1
   e01fc:	ea22 0200 	bic.w	r2, r2, r0
   e0200:	f000 4000 	and.w	r0, r0, #2147483648	; 0x80000000
    F2 one_minus_half_denominator_times_x =
        F2::One() - half_denominator_times_x;
    x = x + Rescale<2>(x * one_minus_half_denominator_times_x);
  }
  return Rescale<0>(ExactMulByPot<-1>(x));
}
   e0204:	4050      	eors	r0, r2
   e0206:	e8bd 8ff8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e020a:	bf00      	nop
   e020c:	c3c3c3c4 	.word	0xc3c3c3c4
   e0210:	e0000001 	.word	0xe0000001

000e0214 <_ZN8gemmlowp55exp_on_interval_between_negative_one_quarter_and_0_exclIlEENS_10FixedPointIT_Li0EEES3_>:

// Implementation of exponential function.

// Returns exp(x) for x in [-1/4, 0).
template <typename tRawType>
FixedPoint<tRawType, 0> exp_on_interval_between_negative_one_quarter_and_0_excl(
   e0214:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
   e0218:	4604      	mov	r4, r0
    FixedPoint<tRawType, 0> a) {
  typedef FixedPoint<tRawType, 0> F;
  const F constant_term =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F, 1895147668, std::exp(-1.0 / 8.0));
   e021a:	4814      	ldr	r0, [pc, #80]	; (e026c <_ZN8gemmlowp55exp_on_interval_between_negative_one_quarter_and_0_exclIlEENS_10FixedPointIT_Li0EEES3_+0x58>)
   e021c:	f7ff ff50 	bl	e00c0 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
   e0220:	4606      	mov	r6, r0
  const F constant_1_over_3 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F, 715827883, 1.0 / 3.0);
   e0222:	4813      	ldr	r0, [pc, #76]	; (e0270 <_ZN8gemmlowp55exp_on_interval_between_negative_one_quarter_and_0_exclIlEENS_10FixedPointIT_Li0EEES3_+0x5c>)
   e0224:	f7ff ff4c 	bl	e00c0 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
}

// Integer addition. Not saturating. Overflow is undefined behavior.
template <typename tIntegerType>
tIntegerType Add(tIntegerType a, tIntegerType b) {
  return a + b;
   e0228:	f104 5480 	add.w	r4, r4, #268435456	; 0x10000000
    FixedPoint<tRawType, 0> a) {
  typedef FixedPoint<tRawType, 0> F;
  const F constant_term =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F, 1895147668, std::exp(-1.0 / 8.0));
  const F constant_1_over_3 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F, 715827883, 1.0 / 3.0);
   e022c:	4680      	mov	r8, r0
  // We're evaluating a Taylor expansion around -1/8, so we do the change of
  // variable: x = a + 1/8.
  // In fixed-point with 0 integer bits, 1/8 is represented by 1 << 28.
  F x = a + F::template ConstantPOT<-3>();
  F x2 = x * x;
   e022e:	4621      	mov	r1, r4
   e0230:	4620      	mov	r0, r4
   e0232:	f7ff ff41 	bl	e00b8 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
  F x3 = x2 * x;
   e0236:	4621      	mov	r1, r4
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F, 715827883, 1.0 / 3.0);
  // We're evaluating a Taylor expansion around -1/8, so we do the change of
  // variable: x = a + 1/8.
  // In fixed-point with 0 integer bits, 1/8 is represented by 1 << 28.
  F x = a + F::template ConstantPOT<-3>();
  F x2 = x * x;
   e0238:	4605      	mov	r5, r0
  F x3 = x2 * x;
   e023a:	f7ff ff3d 	bl	e00b8 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
  F x4 = x2 * x2;
   e023e:	4629      	mov	r1, r5
  // We're evaluating a Taylor expansion around -1/8, so we do the change of
  // variable: x = a + 1/8.
  // In fixed-point with 0 integer bits, 1/8 is represented by 1 << 28.
  F x = a + F::template ConstantPOT<-3>();
  F x2 = x * x;
  F x3 = x2 * x;
   e0240:	4607      	mov	r7, r0
  F x4 = x2 * x2;
   e0242:	4628      	mov	r0, r5
   e0244:	f7ff ff38 	bl	e00b8 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
};

template <int Exponent, typename IntegerType>
struct ImplSaturatingRoundingMultiplyByPOT<Exponent, IntegerType, -1> {
  static IntegerType eval(IntegerType x) {
    return RoundingDivideByPOT<IntegerType>(x, -Exponent);
   e0248:	2102      	movs	r1, #2
   e024a:	f7ff ff09 	bl	e0060 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i>
  F x2 = x * x;
  F x3 = x2 * x;
  F x4 = x2 * x2;
  F x4_over_4 = SaturatingRoundingMultiplyByPOT<-2>(x4);
  F x4_over_24_plus_x3_over_6_plus_x2_over_2 =
      SaturatingRoundingMultiplyByPOT<-1>(
   e024e:	4641      	mov	r1, r8
   e0250:	4438      	add	r0, r7
   e0252:	f7ff ff31 	bl	e00b8 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
};

template <int Exponent, typename IntegerType>
struct ImplSaturatingRoundingMultiplyByPOT<Exponent, IntegerType, -1> {
  static IntegerType eval(IntegerType x) {
    return RoundingDivideByPOT<IntegerType>(x, -Exponent);
   e0256:	2101      	movs	r1, #1
   e0258:	4428      	add	r0, r5
   e025a:	f7ff ff01 	bl	e0060 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i>
  F x4 = x2 * x2;
  F x4_over_4 = SaturatingRoundingMultiplyByPOT<-2>(x4);
  F x4_over_24_plus_x3_over_6_plus_x2_over_2 =
      SaturatingRoundingMultiplyByPOT<-1>(
          ((x4_over_4 + x3) * constant_1_over_3) + x2);
  return AddSaturatingIf16Bit(
   e025e:	1821      	adds	r1, r4, r0
   e0260:	4630      	mov	r0, r6
   e0262:	f7ff ff29 	bl	e00b8 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
      constant_term,
      constant_term * (x + x4_over_24_plus_x3_over_6_plus_x2_over_2));
}
   e0266:	4430      	add	r0, r6
   e0268:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   e026c:	70f5a894 	.word	0x70f5a894
   e0270:	2aaaaaab 	.word	0x2aaaaaab

000e0274 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE>:

// Returns exp(x) for x < 0.
template <typename tRawType, int tIntegerBits>
FixedPoint<tRawType, 0> exp_on_negative_values(
   e0274:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
  return a * b;
}

template <typename tIntegerType>
tIntegerType Sub(tIntegerType a, tIntegerType b) {
  return a - b;
   e0276:	f040 467f 	orr.w	r6, r0, #4278190080	; 0xff000000
      constant_term * (x + x4_over_24_plus_x3_over_6_plus_x2_over_2));
}

// Returns exp(x) for x < 0.
template <typename tRawType, int tIntegerBits>
FixedPoint<tRawType, 0> exp_on_negative_values(
   e027a:	4607      	mov	r7, r0
  static constexpr int kIntegerBits = InputF::kIntegerBits;
  const InputF kOneQuarter = InputF::template ConstantPOT<-2>();
  InputF mask = kOneQuarter - InputF::FromScalarRaw(1);
  InputF a_mod_quarter_minus_one_quarter = (a & mask) - kOneQuarter;
  ResultF result = exp_on_interval_between_negative_one_quarter_and_0_excl(
      Rescale<0>(a_mod_quarter_minus_one_quarter));
   e027c:	0170      	lsls	r0, r6, #5
   e027e:	f7ff ffc9 	bl	e0214 <_ZN8gemmlowp55exp_on_interval_between_negative_one_quarter_and_0_exclIlEENS_10FixedPointIT_Li0EEES3_>
   e0282:	4605      	mov	r5, r0
    result = SelectUsingMask(                                               \
        MaskIfNonZero(BitAnd(remainder, Dup<tRawType>(1 << kShiftAmount))), \
        result * kMultiplier, result);                                      \
  }

  GEMMLOWP_EXP_BARREL_SHIFTER(-2, 1672461947);
   e0284:	4833      	ldr	r0, [pc, #204]	; (e0354 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE+0xe0>)
   e0286:	f7ff ff1b 	bl	e00c0 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
   e028a:	4601      	mov	r1, r0
   e028c:	4628      	mov	r0, r5
   e028e:	f7ff ff13 	bl	e00b8 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
  return a * b;
}

template <typename tIntegerType>
tIntegerType Sub(tIntegerType a, tIntegerType b) {
  return a - b;
   e0292:	1bf6      	subs	r6, r6, r7
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e0294:	f346 6400 	sbfx	r4, r6, #24, #1
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e0298:	4020      	ands	r0, r4
   e029a:	ea25 0504 	bic.w	r5, r5, r4
   e029e:	ea80 0405 	eor.w	r4, r0, r5
        MaskIfNonZero(BitAnd(remainder, Dup<tRawType>(1 << kShiftAmount))), \
        result * kMultiplier, result);                                      \
  }

  GEMMLOWP_EXP_BARREL_SHIFTER(-2, 1672461947);
  GEMMLOWP_EXP_BARREL_SHIFTER(-1, 1302514674);
   e02a2:	482d      	ldr	r0, [pc, #180]	; (e0358 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE+0xe4>)
   e02a4:	f7ff ff0c 	bl	e00c0 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
   e02a8:	4601      	mov	r1, r0
   e02aa:	4620      	mov	r0, r4
   e02ac:	f7ff ff04 	bl	e00b8 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e02b0:	f346 6540 	sbfx	r5, r6, #25, #1
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e02b4:	4028      	ands	r0, r5
}

// Plain bit-wise AND
template <typename tIntegerType>
tIntegerType BitAnd(tIntegerType a, tIntegerType b) {
  return a & b;
   e02b6:	ea24 0405 	bic.w	r4, r4, r5
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e02ba:	4044      	eors	r4, r0
        result * kMultiplier, result);                                      \
  }

  GEMMLOWP_EXP_BARREL_SHIFTER(-2, 1672461947);
  GEMMLOWP_EXP_BARREL_SHIFTER(-1, 1302514674);
  GEMMLOWP_EXP_BARREL_SHIFTER(+0, 790015084);
   e02bc:	4827      	ldr	r0, [pc, #156]	; (e035c <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE+0xe8>)
   e02be:	f7ff feff 	bl	e00c0 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
   e02c2:	4601      	mov	r1, r0
   e02c4:	4620      	mov	r0, r4
   e02c6:	f7ff fef7 	bl	e00b8 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e02ca:	f346 6580 	sbfx	r5, r6, #26, #1
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e02ce:	4028      	ands	r0, r5
}

// Plain bit-wise AND
template <typename tIntegerType>
tIntegerType BitAnd(tIntegerType a, tIntegerType b) {
  return a & b;
   e02d0:	ea24 0405 	bic.w	r4, r4, r5
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e02d4:	4044      	eors	r4, r0
  }

  GEMMLOWP_EXP_BARREL_SHIFTER(-2, 1672461947);
  GEMMLOWP_EXP_BARREL_SHIFTER(-1, 1302514674);
  GEMMLOWP_EXP_BARREL_SHIFTER(+0, 790015084);
  GEMMLOWP_EXP_BARREL_SHIFTER(+1, 290630308);
   e02d6:	4822      	ldr	r0, [pc, #136]	; (e0360 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE+0xec>)
   e02d8:	f7ff fef2 	bl	e00c0 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
   e02dc:	4601      	mov	r1, r0
   e02de:	4620      	mov	r0, r4
   e02e0:	f7ff feea 	bl	e00b8 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e02e4:	f346 65c0 	sbfx	r5, r6, #27, #1
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e02e8:	4028      	ands	r0, r5
}

// Plain bit-wise AND
template <typename tIntegerType>
tIntegerType BitAnd(tIntegerType a, tIntegerType b) {
  return a & b;
   e02ea:	ea24 0405 	bic.w	r4, r4, r5
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e02ee:	4044      	eors	r4, r0

  GEMMLOWP_EXP_BARREL_SHIFTER(-2, 1672461947);
  GEMMLOWP_EXP_BARREL_SHIFTER(-1, 1302514674);
  GEMMLOWP_EXP_BARREL_SHIFTER(+0, 790015084);
  GEMMLOWP_EXP_BARREL_SHIFTER(+1, 290630308);
  GEMMLOWP_EXP_BARREL_SHIFTER(+2, 39332535);
   e02f0:	481c      	ldr	r0, [pc, #112]	; (e0364 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE+0xf0>)
   e02f2:	f7ff fee5 	bl	e00c0 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
   e02f6:	4601      	mov	r1, r0
   e02f8:	4620      	mov	r0, r4
   e02fa:	f7ff fedd 	bl	e00b8 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e02fe:	f346 7500 	sbfx	r5, r6, #28, #1
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e0302:	4028      	ands	r0, r5
}

// Plain bit-wise AND
template <typename tIntegerType>
tIntegerType BitAnd(tIntegerType a, tIntegerType b) {
  return a & b;
   e0304:	ea24 0405 	bic.w	r4, r4, r5
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e0308:	4044      	eors	r4, r0
  GEMMLOWP_EXP_BARREL_SHIFTER(-2, 1672461947);
  GEMMLOWP_EXP_BARREL_SHIFTER(-1, 1302514674);
  GEMMLOWP_EXP_BARREL_SHIFTER(+0, 790015084);
  GEMMLOWP_EXP_BARREL_SHIFTER(+1, 290630308);
  GEMMLOWP_EXP_BARREL_SHIFTER(+2, 39332535);
  GEMMLOWP_EXP_BARREL_SHIFTER(+3, 720401);
   e030a:	4817      	ldr	r0, [pc, #92]	; (e0368 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE+0xf4>)
   e030c:	f7ff fed8 	bl	e00c0 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
   e0310:	4601      	mov	r1, r0
   e0312:	4620      	mov	r0, r4
   e0314:	f7ff fed0 	bl	e00b8 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e0318:	f346 7540 	sbfx	r5, r6, #29, #1
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e031c:	4028      	ands	r0, r5
}

// Plain bit-wise AND
template <typename tIntegerType>
tIntegerType BitAnd(tIntegerType a, tIntegerType b) {
  return a & b;
   e031e:	ea24 0405 	bic.w	r4, r4, r5
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e0322:	4044      	eors	r4, r0
  GEMMLOWP_EXP_BARREL_SHIFTER(-1, 1302514674);
  GEMMLOWP_EXP_BARREL_SHIFTER(+0, 790015084);
  GEMMLOWP_EXP_BARREL_SHIFTER(+1, 290630308);
  GEMMLOWP_EXP_BARREL_SHIFTER(+2, 39332535);
  GEMMLOWP_EXP_BARREL_SHIFTER(+3, 720401);
  GEMMLOWP_EXP_BARREL_SHIFTER(+4, 242);
   e0324:	20f2      	movs	r0, #242	; 0xf2
   e0326:	f7ff fecb 	bl	e00c0 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
   e032a:	4601      	mov	r1, r0
   e032c:	4620      	mov	r0, r4
   e032e:	f7ff fec3 	bl	e00b8 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e0332:	fab7 f787 	clz	r7, r7
   e0336:	f346 7680 	sbfx	r6, r6, #30, #1
   e033a:	097f      	lsrs	r7, r7, #5
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e033c:	4030      	ands	r0, r6
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e033e:	427f      	negs	r7, r7
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e0340:	ea24 0406 	bic.w	r4, r4, r6
   e0344:	4044      	eors	r4, r0
        GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(InputF, -(1 << clampB), -32.0);
    result = SelectUsingMask(MaskIfLessThan(a, clamp), ResultF::Zero(), result);
  }

  result = SelectUsingMask(MaskIfZero(a), ResultF::One(), result);
  return result;
   e0346:	43f8      	mvns	r0, r7
   e0348:	4004      	ands	r4, r0
   e034a:	f027 4000 	bic.w	r0, r7, #2147483648	; 0x80000000
}
   e034e:	4060      	eors	r0, r4
   e0350:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
   e0352:	bf00      	nop
   e0354:	63afbe7b 	.word	0x63afbe7b
   e0358:	4da2cbf2 	.word	0x4da2cbf2
   e035c:	2f16ac6c 	.word	0x2f16ac6c
   e0360:	1152aaa4 	.word	0x1152aaa4
   e0364:	02582ab7 	.word	0x02582ab7
   e0368:	000afe11 	.word	0x000afe11

000e036c <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph>:
  }
}

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const uint8* input_data,
                    const RuntimeShape& output_shape, uint8* output_data) {
   e036c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e0370:	b08d      	sub	sp, #52	; 0x34
  using FixedPointScaledDiff =
      gemmlowp::FixedPoint<int32, kScaledDiffIntegerBits>;
  using FixedPointAccum = gemmlowp::FixedPoint<int32, kAccumulationIntegerBits>;
  using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;

  const int trailing_dim = input_shape.DimensionsCount() - 1;
   e0372:	680d      	ldr	r5, [r1, #0]
  }
}

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const uint8* input_data,
                    const RuntimeShape& output_shape, uint8* output_data) {
   e0374:	461e      	mov	r6, r3
  const int32 input_beta_multiplier = params.input_multiplier;
   e0376:	6883      	ldr	r3, [r0, #8]
   e0378:	9301      	str	r3, [sp, #4]
  }
}

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const uint8* input_data,
                    const RuntimeShape& output_shape, uint8* output_data) {
   e037a:	460c      	mov	r4, r1
  const int32 input_beta_multiplier = params.input_multiplier;
  const int32 input_beta_left_shift = params.input_left_shift;
   e037c:	68c3      	ldr	r3, [r0, #12]
   e037e:	9302      	str	r3, [sp, #8]
  using FixedPointScaledDiff =
      gemmlowp::FixedPoint<int32, kScaledDiffIntegerBits>;
  using FixedPointAccum = gemmlowp::FixedPoint<int32, kAccumulationIntegerBits>;
  using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;

  const int trailing_dim = input_shape.DimensionsCount() - 1;
   e0380:	3d01      	subs	r5, #1
inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const uint8* input_data,
                    const RuntimeShape& output_shape, uint8* output_data) {
  const int32 input_beta_multiplier = params.input_multiplier;
  const int32 input_beta_left_shift = params.input_left_shift;
  const int diff_min = params.diff_min;
   e0382:	6983      	ldr	r3, [r0, #24]
   e0384:	9303      	str	r3, [sp, #12]
  using FixedPointAccum = gemmlowp::FixedPoint<int32, kAccumulationIntegerBits>;
  using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;

  const int trailing_dim = input_shape.DimensionsCount() - 1;
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
   e0386:	4629      	mov	r1, r5
   e0388:	4620      	mov	r0, r4
  }
}

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const uint8* input_data,
                    const RuntimeShape& output_shape, uint8* output_data) {
   e038a:	4692      	mov	sl, r2
  using FixedPointAccum = gemmlowp::FixedPoint<int32, kAccumulationIntegerBits>;
  using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;

  const int trailing_dim = input_shape.DimensionsCount() - 1;
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
   e038c:	4632      	mov	r2, r6
   e038e:	f7ff fcff 	bl	dfd90 <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_>
   e0392:	4629      	mov	r1, r5
   e0394:	9004      	str	r0, [sp, #16]
   e0396:	4620      	mov	r0, r4
   e0398:	f7f6 f860 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   e039c:	4629      	mov	r1, r5
   e039e:	4604      	mov	r4, r0
   e03a0:	4630      	mov	r0, r6
   e03a2:	f7f6 f85b 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   e03a6:	4284      	cmp	r4, r0
   e03a8:	d001      	beq.n	e03ae <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x42>
   e03aa:	f003 ffbf 	bl	e432c <abort>
   e03ae:	f8dd 9058 	ldr.w	r9, [sp, #88]	; 0x58
   e03b2:	4656      	mov	r6, sl
   e03b4:	f1ca 0500 	rsb	r5, sl, #0
   e03b8:	2700      	movs	r7, #0
  const int depth =
      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);

  for (int i = 0; i < outer_size; ++i) {
   e03ba:	9b04      	ldr	r3, [sp, #16]
   e03bc:	429f      	cmp	r7, r3
   e03be:	da7d      	bge.n	e04bc <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x150>
    uint8 max_in_row = 0;
   e03c0:	f04f 0300 	mov.w	r3, #0
   e03c4:	f88d 3023 	strb.w	r3, [sp, #35]	; 0x23
   e03c8:	4632      	mov	r2, r6
    for (int c = 0; c < depth; ++c) {
   e03ca:	1953      	adds	r3, r2, r5
   e03cc:	42a3      	cmp	r3, r4
   e03ce:	da0c      	bge.n	e03ea <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x7e>
      max_in_row = std::max(max_in_row, input_data[i * depth + c]);
   e03d0:	4613      	mov	r3, r2
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   e03d2:	f89d 1023 	ldrb.w	r1, [sp, #35]	; 0x23
   e03d6:	7818      	ldrb	r0, [r3, #0]
   e03d8:	4288      	cmp	r0, r1
	return __b;
      return __a;
   e03da:	bf98      	it	ls
   e03dc:	f10d 0323 	addls.w	r3, sp, #35	; 0x23
   e03e0:	3201      	adds	r2, #1
   e03e2:	781b      	ldrb	r3, [r3, #0]
   e03e4:	f88d 3023 	strb.w	r3, [sp, #35]	; 0x23
  const int depth =
      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);

  for (int i = 0; i < outer_size; ++i) {
    uint8 max_in_row = 0;
    for (int c = 0; c < depth; ++c) {
   e03e8:	e7ef      	b.n	e03ca <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x5e>
   e03ea:	46b0      	mov	r8, r6
   e03ec:	f04f 0b00 	mov.w	fp, #0
      max_in_row = std::max(max_in_row, input_data[i * depth + c]);
    }

    FixedPointAccum sum_of_exps = FixedPointAccum::Zero();
    for (int c = 0; c < depth; ++c) {
   e03f0:	eb08 0305 	add.w	r3, r8, r5
   e03f4:	42a3      	cmp	r3, r4
   e03f6:	da13      	bge.n	e0420 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xb4>
      int32 input_diff =
          static_cast<int32>(input_data[i * depth + c]) - max_in_row;
   e03f8:	f818 3b01 	ldrb.w	r3, [r8], #1
   e03fc:	f89d 0023 	ldrb.w	r0, [sp, #35]	; 0x23
   e0400:	1a18      	subs	r0, r3, r0
      if (input_diff >= diff_min) {
   e0402:	9b03      	ldr	r3, [sp, #12]
   e0404:	4283      	cmp	r3, r0
   e0406:	dcf3      	bgt.n	e03f0 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x84>

inline int32 MultiplyByQuantizedMultiplierGreaterThanOne(
    int32 x, int32 quantized_multiplier, int left_shift) {
  using gemmlowp::SaturatingRoundingDoublingHighMul;
  return SaturatingRoundingDoublingHighMul(x * (1 << left_shift),
                                           quantized_multiplier);
   e0408:	9b02      	ldr	r3, [sp, #8]
   e040a:	9901      	ldr	r1, [sp, #4]
   e040c:	4098      	lsls	r0, r3
   e040e:	f7ff fc97 	bl	dfd40 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>
            MultiplyByQuantizedMultiplierGreaterThanOne(
                input_diff, input_beta_multiplier, input_beta_left_shift);
        const FixedPointScaledDiff scaled_diff_f8 =
            FixedPointScaledDiff::FromRaw(input_diff_rescaled);
        sum_of_exps = sum_of_exps + gemmlowp::Rescale<kAccumulationIntegerBits>(
                                        exp_on_negative_values(scaled_diff_f8));
   e0412:	f7ff ff2f 	bl	e0274 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE>
};

template <int Exponent, typename IntegerType>
struct ImplSaturatingRoundingMultiplyByPOT<Exponent, IntegerType, -1> {
  static IntegerType eval(IntegerType x) {
    return RoundingDivideByPOT<IntegerType>(x, -Exponent);
   e0416:	210c      	movs	r1, #12
   e0418:	f7ff fe22 	bl	e0060 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i>
}

// Integer addition. Not saturating. Overflow is undefined behavior.
template <typename tIntegerType>
tIntegerType Add(tIntegerType a, tIntegerType b) {
  return a + b;
   e041c:	4483      	add	fp, r0
    for (int c = 0; c < depth; ++c) {
      max_in_row = std::max(max_in_row, input_data[i * depth + c]);
    }

    FixedPointAccum sum_of_exps = FixedPointAccum::Zero();
    for (int c = 0; c < depth; ++c) {
   e041e:	e7e7      	b.n	e03f0 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x84>
int CountLeadingZeros(T integer_input) {
  static_assert(std::is_unsigned<T>::value,
                "Only unsigned integer types handled.");
#if defined(__GNUC__)
  return integer_input ? __builtin_clz(integer_input)
                       : std::numeric_limits<T>::digits;
   e0420:	f1bb 0f00 	cmp.w	fp, #0
   e0424:	d002      	beq.n	e042c <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xc0>
   e0426:	fabb f88b 	clz	r8, fp
   e042a:	e001      	b.n	e0430 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xc4>
   e042c:	f04f 0820 	mov.w	r8, #32
   e0430:	fa0b f008 	lsl.w	r0, fp, r8
      static_cast<int32>((static_cast<uint32>(x) << headroom_plus_one) -
                         (static_cast<uint32>(1) << 31));

  gemmlowp::FixedPoint<int32, 0> shifted_scale =
      gemmlowp::one_over_one_plus_x_for_x_in_0_1(
          gemmlowp::FixedPoint<int32, 0>::FromRaw(shifted_sum_minus_one));
   e0434:	f100 4000 	add.w	r0, r0, #2147483648	; 0x80000000
   e0438:	f7ff fe48 	bl	e00cc <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_>

    int num_bits_over_unit;
    FixedPoint0 shifted_scale = FixedPoint0::FromRaw(GetReciprocal(
        sum_of_exps.raw(), kAccumulationIntegerBits, &num_bits_over_unit));

    for (int c = 0; c < depth; ++c) {
   e043c:	9916      	ldr	r1, [sp, #88]	; 0x58
   e043e:	1a69      	subs	r1, r5, r1
   e0440:	4451      	add	r1, sl
   e0442:	4683      	mov	fp, r0
      }
    }

    int num_bits_over_unit;
    FixedPoint0 shifted_scale = FixedPoint0::FromRaw(GetReciprocal(
        sum_of_exps.raw(), kAccumulationIntegerBits, &num_bits_over_unit));
   e0444:	4632      	mov	r2, r6
   e0446:	464b      	mov	r3, r9

    for (int c = 0; c < depth; ++c) {
   e0448:	9105      	str	r1, [sp, #20]
                input_diff, input_beta_multiplier, input_beta_left_shift);
        const FixedPointScaledDiff scaled_diff_f8 =
            FixedPointScaledDiff::FromRaw(input_diff_rescaled);

        FixedPoint0 exp_in_0 = exp_on_negative_values(scaled_diff_f8);
        int32 unsat_output = gemmlowp::RoundingDivideByPOT(
   e044a:	f1c8 0823 	rsb	r8, r8, #35	; 0x23

    int num_bits_over_unit;
    FixedPoint0 shifted_scale = FixedPoint0::FromRaw(GetReciprocal(
        sum_of_exps.raw(), kAccumulationIntegerBits, &num_bits_over_unit));

    for (int c = 0; c < depth; ++c) {
   e044e:	9905      	ldr	r1, [sp, #20]
   e0450:	4419      	add	r1, r3
   e0452:	428c      	cmp	r4, r1
   e0454:	dd2d      	ble.n	e04b2 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x146>
      int32 input_diff =
          static_cast<int32>(input_data[i * depth + c]) - max_in_row;
   e0456:	f812 1b01 	ldrb.w	r1, [r2], #1
   e045a:	f89d 0023 	ldrb.w	r0, [sp, #35]	; 0x23
   e045e:	1a08      	subs	r0, r1, r0
      if (input_diff >= diff_min) {
   e0460:	9903      	ldr	r1, [sp, #12]
   e0462:	4281      	cmp	r1, r0
   e0464:	dc20      	bgt.n	e04a8 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x13c>
   e0466:	9306      	str	r3, [sp, #24]

inline int32 MultiplyByQuantizedMultiplierGreaterThanOne(
    int32 x, int32 quantized_multiplier, int left_shift) {
  using gemmlowp::SaturatingRoundingDoublingHighMul;
  return SaturatingRoundingDoublingHighMul(x * (1 << left_shift),
                                           quantized_multiplier);
   e0468:	9b02      	ldr	r3, [sp, #8]
   e046a:	9901      	ldr	r1, [sp, #4]
   e046c:	9207      	str	r2, [sp, #28]
   e046e:	4098      	lsls	r0, r3
   e0470:	f7ff fc66 	bl	dfd40 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>
            MultiplyByQuantizedMultiplierGreaterThanOne(
                input_diff, input_beta_multiplier, input_beta_left_shift);
        const FixedPointScaledDiff scaled_diff_f8 =
            FixedPointScaledDiff::FromRaw(input_diff_rescaled);

        FixedPoint0 exp_in_0 = exp_on_negative_values(scaled_diff_f8);
   e0474:	f7ff fefe 	bl	e0274 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE>
        int32 unsat_output = gemmlowp::RoundingDivideByPOT(
            (shifted_scale * exp_in_0).raw(), num_bits_over_unit + 31 - 8);
   e0478:	4601      	mov	r1, r0
   e047a:	4658      	mov	r0, fp
   e047c:	f7ff fe1c 	bl	e00b8 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
                input_diff, input_beta_multiplier, input_beta_left_shift);
        const FixedPointScaledDiff scaled_diff_f8 =
            FixedPointScaledDiff::FromRaw(input_diff_rescaled);

        FixedPoint0 exp_in_0 = exp_on_negative_values(scaled_diff_f8);
        int32 unsat_output = gemmlowp::RoundingDivideByPOT(
   e0480:	4641      	mov	r1, r8
   e0482:	f7ff fded 	bl	e0060 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i>
            (shifted_scale * exp_in_0).raw(), num_bits_over_unit + 31 - 8);

        output_data[i * depth + c] = static_cast<uint8>(
            std::max(std::min(unsat_output, static_cast<int32>(255)),
   e0486:	21ff      	movs	r1, #255	; 0xff
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
   e0488:	4288      	cmp	r0, r1
   e048a:	910a      	str	r1, [sp, #40]	; 0x28
	return __b;
      return __a;
   e048c:	bfd4      	ite	le
   e048e:	a909      	addle	r1, sp, #36	; 0x24
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   e0490:	a90a      	addgt	r1, sp, #40	; 0x28
        const FixedPointScaledDiff scaled_diff_f8 =
            FixedPointScaledDiff::FromRaw(input_diff_rescaled);

        FixedPoint0 exp_in_0 = exp_on_negative_values(scaled_diff_f8);
        int32 unsat_output = gemmlowp::RoundingDivideByPOT(
            (shifted_scale * exp_in_0).raw(), num_bits_over_unit + 31 - 8);
   e0492:	9009      	str	r0, [sp, #36]	; 0x24

        output_data[i * depth + c] = static_cast<uint8>(
            std::max(std::min(unsat_output, static_cast<int32>(255)),
                     static_cast<int32>(0)));
   e0494:	2000      	movs	r0, #0
   e0496:	900b      	str	r0, [sp, #44]	; 0x2c
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   e0498:	6808      	ldr	r0, [r1, #0]
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
   e049a:	9b06      	ldr	r3, [sp, #24]
   e049c:	9a07      	ldr	r2, [sp, #28]
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   e049e:	2800      	cmp	r0, #0
	return __b;
   e04a0:	bfb8      	it	lt
   e04a2:	a90b      	addlt	r1, sp, #44	; 0x2c
   e04a4:	6809      	ldr	r1, [r1, #0]
   e04a6:	e001      	b.n	e04ac <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x140>

      } else {
        output_data[i * depth + c] = 0;
   e04a8:	f04f 0100 	mov.w	r1, #0
   e04ac:	7019      	strb	r1, [r3, #0]
   e04ae:	3301      	adds	r3, #1

    int num_bits_over_unit;
    FixedPoint0 shifted_scale = FixedPoint0::FromRaw(GetReciprocal(
        sum_of_exps.raw(), kAccumulationIntegerBits, &num_bits_over_unit));

    for (int c = 0; c < depth; ++c) {
   e04b0:	e7cd      	b.n	e044e <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xe2>
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
  const int depth =
      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);

  for (int i = 0; i < outer_size; ++i) {
   e04b2:	3701      	adds	r7, #1
   e04b4:	44a1      	add	r9, r4
   e04b6:	4426      	add	r6, r4
   e04b8:	1b2d      	subs	r5, r5, r4
   e04ba:	e77e      	b.n	e03ba <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x4e>
      } else {
        output_data[i * depth + c] = 0;
      }
    }
  }
}
   e04bc:	b00d      	add	sp, #52	; 0x34
   e04be:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

000e04c4 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode>:
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
      GetTensorShape(output), GetTensorData<uint8_t>(output));
}

TfLiteStatus SoftmaxEval(TfLiteContext* context, TfLiteNode* node) {
   e04c4:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e04c8:	680b      	ldr	r3, [r1, #0]
   e04ca:	f8d0 8008 	ldr.w	r8, [r0, #8]
   e04ce:	685a      	ldr	r2, [r3, #4]
  auto* params = reinterpret_cast<TfLiteSoftmaxParams*>(node->builtin_data);
   e04d0:	694f      	ldr	r7, [r1, #20]
   e04d2:	2338      	movs	r3, #56	; 0x38
   e04d4:	fb03 f902 	mul.w	r9, r3, r2
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   e04d8:	684a      	ldr	r2, [r1, #4]
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
      GetTensorShape(output), GetTensorData<uint8_t>(output));
}

TfLiteStatus SoftmaxEval(TfLiteContext* context, TfLiteNode* node) {
   e04da:	b09f      	sub	sp, #124	; 0x7c
   e04dc:	6854      	ldr	r4, [r2, #4]
   e04de:	4605      	mov	r5, r0
  auto* params = reinterpret_cast<TfLiteSoftmaxParams*>(node->builtin_data);

  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);

  OpData local_data_object;
   e04e0:	2210      	movs	r2, #16
   e04e2:	2100      	movs	r1, #0
   e04e4:	a806      	add	r0, sp, #24
   e04e6:	fb03 8404 	mla	r4, r3, r4, r8
   e04ea:	f007 f985 	bl	e77f8 <memset>
TfLiteStatus CalculateSoftmaxOpData(TfLiteContext* context,
                                    const TfLiteTensor* input,
                                    TfLiteTensor* output,
                                    const TfLiteSoftmaxParams* params,
                                    OpData* data) {
  if (input->type == kTfLiteUInt8) {
   e04ee:	f818 3009 	ldrb.w	r3, [r8, r9]
   e04f2:	2b03      	cmp	r3, #3
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e04f4:	eb08 0609 	add.w	r6, r8, r9
   e04f8:	d13f      	bne.n	e057a <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0xb6>
    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);
   e04fa:	6923      	ldr	r3, [r4, #16]
   e04fc:	b16b      	cbz	r3, e051a <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x56>
   e04fe:	9302      	str	r3, [sp, #8]
   e0500:	4b67      	ldr	r3, [pc, #412]	; (e06a0 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1dc>)
   e0502:	9301      	str	r3, [sp, #4]
   e0504:	2200      	movs	r2, #0
   e0506:	4b67      	ldr	r3, [pc, #412]	; (e06a4 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1e0>)
   e0508:	9203      	str	r2, [sp, #12]
   e050a:	9300      	str	r3, [sp, #0]
   e050c:	696c      	ldr	r4, [r5, #20]
   e050e:	4a66      	ldr	r2, [pc, #408]	; (e06a8 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1e4>)
   e0510:	4966      	ldr	r1, [pc, #408]	; (e06ac <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1e8>)
   e0512:	232c      	movs	r3, #44	; 0x2c
   e0514:	4628      	mov	r0, r5
   e0516:	47a0      	blx	r4
   e0518:	e0bd      	b.n	e0696 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1d2>
    TF_LITE_ENSURE(context, output->params.scale == 1.f / 256);
   e051a:	ed94 7a03 	vldr	s14, [r4, #12]
   e051e:	eddf 7a64 	vldr	s15, [pc, #400]	; e06b0 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1ec>
   e0522:	eeb4 7a67 	vcmp.f32	s14, s15
   e0526:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e052a:	d008      	beq.n	e053e <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x7a>
   e052c:	4b61      	ldr	r3, [pc, #388]	; (e06b4 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1f0>)
   e052e:	9300      	str	r3, [sp, #0]
   e0530:	696c      	ldr	r4, [r5, #20]
   e0532:	4a5d      	ldr	r2, [pc, #372]	; (e06a8 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1e4>)
   e0534:	4960      	ldr	r1, [pc, #384]	; (e06b8 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1f4>)
   e0536:	232d      	movs	r3, #45	; 0x2d
   e0538:	4628      	mov	r0, r5
   e053a:	47a0      	blx	r4
   e053c:	e0ab      	b.n	e0696 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1d2>

    static const int kScaledDiffIntegerBits = 5;

    tflite::PreprocessSoftmaxScaling(
        params->beta, input->params.scale, kScaledDiffIntegerBits,
        &data->input_multiplier, &data->input_left_shift);
   e053e:	68f0      	ldr	r0, [r6, #12]
   e0540:	f006 fd5c 	bl	e6ffc <__aeabi_f2d>
   e0544:	e9cd 0104 	strd	r0, r1, [sp, #16]
   e0548:	6838      	ldr	r0, [r7, #0]
   e054a:	f006 fd57 	bl	e6ffc <__aeabi_f2d>
   e054e:	ed9d 1b04 	vldr	d1, [sp, #16]
   e0552:	ec41 0b10 	vmov	d0, r0, r1
   e0556:	aa07      	add	r2, sp, #28
   e0558:	a906      	add	r1, sp, #24
   e055a:	2005      	movs	r0, #5
   e055c:	f003 fca8 	bl	e3eb0 <_ZN6tflite24PreprocessSoftmaxScalingEddiPlPi>
    data->diff_min = -1.0 * tflite::CalculateInputRadius(
   e0560:	221f      	movs	r2, #31
   e0562:	9907      	ldr	r1, [sp, #28]
   e0564:	2005      	movs	r0, #5
   e0566:	f003 fcdb 	bl	e3f20 <_ZN6tflite20CalculateInputRadiusEiii>
                                kScaledDiffIntegerBits, data->input_left_shift);
   e056a:	f006 fd35 	bl	e6fd8 <__aeabi_i2d>
   e056e:	f101 4300 	add.w	r3, r1, #2147483648	; 0x80000000
   e0572:	4619      	mov	r1, r3
   e0574:	f007 f830 	bl	e75d8 <__aeabi_d2iz>
   e0578:	9009      	str	r0, [sp, #36]	; 0x24
  TF_LITE_ENSURE_STATUS(
      CalculateSoftmaxOpData(context, input, output, params, data));

  // TODO(ahentz): consider an implementation that works for many (all?)
  // dimensions.
  switch (input->type) {
   e057a:	f818 8009 	ldrb.w	r8, [r8, r9]
   e057e:	f1b8 0f01 	cmp.w	r8, #1
   e0582:	d11f      	bne.n	e05c4 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x100>
#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/c_api_internal.h"

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
   e0584:	68b3      	ldr	r3, [r6, #8]
   e0586:	681a      	ldr	r2, [r3, #0]
    case kTfLiteFloat32: {
      if (NumDimensions(input) == 1) {
   e0588:	2a01      	cmp	r2, #1
   e058a:	d105      	bne.n	e0598 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0xd4>
        Softmax1DFloat(input, output, params);
   e058c:	463a      	mov	r2, r7
   e058e:	4621      	mov	r1, r4
   e0590:	4630      	mov	r0, r6
   e0592:	f7ff fd27 	bl	dffe4 <_ZN6tflite3ops5micro11activations14Softmax1DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams>
   e0596:	e042      	b.n	e061e <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x15a>
        return kTfLiteOk;
      }
      if (NumDimensions(input) == 2) {
   e0598:	2a02      	cmp	r2, #2
   e059a:	d105      	bne.n	e05a8 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0xe4>
        Softmax2DFloat(input, output, params);
   e059c:	463a      	mov	r2, r7
   e059e:	4621      	mov	r1, r4
   e05a0:	4630      	mov	r0, r6
   e05a2:	f7ff fd2a 	bl	dfffa <_ZN6tflite3ops5micro11activations14Softmax2DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams>
   e05a6:	e03a      	b.n	e061e <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x15a>
        return kTfLiteOk;
      }
      if (NumDimensions(input) == 4) {
   e05a8:	2a04      	cmp	r2, #4
   e05aa:	d105      	bne.n	e05b8 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0xf4>
        Softmax4DFloat(input, output, params);
   e05ac:	463a      	mov	r2, r7
   e05ae:	4621      	mov	r1, r4
   e05b0:	4630      	mov	r0, r6
   e05b2:	f7ff fd2d 	bl	e0010 <_ZN6tflite3ops5micro11activations14Softmax4DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams>
   e05b6:	e032      	b.n	e061e <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x15a>
        return kTfLiteOk;
      }
      context->ReportError(
          context, "Only 1D, 2D and 4D tensors supported currently, got %dD.",
          NumDimensions(input));
   e05b8:	4628      	mov	r0, r5
   e05ba:	696b      	ldr	r3, [r5, #20]
   e05bc:	493f      	ldr	r1, [pc, #252]	; (e06bc <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1f8>)
   e05be:	4798      	blx	r3
      return kTfLiteError;
   e05c0:	4640      	mov	r0, r8
   e05c2:	e069      	b.n	e0698 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1d4>
  TF_LITE_ENSURE_STATUS(
      CalculateSoftmaxOpData(context, input, output, params, data));

  // TODO(ahentz): consider an implementation that works for many (all?)
  // dimensions.
  switch (input->type) {
   e05c4:	f1b8 0f03 	cmp.w	r8, #3
   e05c8:	d160      	bne.n	e068c <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1c8>
   e05ca:	68b3      	ldr	r3, [r6, #8]
   e05cc:	681f      	ldr	r7, [r3, #0]
          context, "Only 1D, 2D and 4D tensors supported currently, got %dD.",
          NumDimensions(input));
      return kTfLiteError;
    }
    case kTfLiteUInt8: {
      if (NumDimensions(input) == 1) {
   e05ce:	2f01      	cmp	r7, #1
   e05d0:	d127      	bne.n	e0622 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x15e>
  // TODO(ahentz): this is arguably a dirty trick. Since the implementation
  // always traverses the last dimension of a 4D tensor, we will pretend our 1D
  // tensor is 4D in a special way. We will convert a (Y) shape into a (1,
  // 1, 1, Y) shape.
  const int input_size = input->dims->data[0];
  const int32_t shape_data[4] = {1, 1, 1, input_size};
   e05d2:	ad0a      	add	r5, sp, #40	; 0x28
                        TfLiteSoftmaxParams* params, OpData* data) {
  // TODO(ahentz): this is arguably a dirty trick. Since the implementation
  // always traverses the last dimension of a 4D tensor, we will pretend our 1D
  // tensor is 4D in a special way. We will convert a (Y) shape into a (1,
  // 1, 1, Y) shape.
  const int input_size = input->dims->data[0];
   e05d4:	f8d3 8004 	ldr.w	r8, [r3, #4]
  const int32_t shape_data[4] = {1, 1, 1, input_size};
   e05d8:	2210      	movs	r2, #16
   e05da:	2100      	movs	r1, #0
   e05dc:	4628      	mov	r0, r5
   e05de:	f007 f90b 	bl	e77f8 <memset>
   e05e2:	970a      	str	r7, [sp, #40]	; 0x28
   e05e4:	970b      	str	r7, [sp, #44]	; 0x2c
   e05e6:	970c      	str	r7, [sp, #48]	; 0x30
   e05e8:	f8cd 8034 	str.w	r8, [sp, #52]	; 0x34
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
#endif  // TF_LITE_STATIC_MEMORY
    }
    size_ = dimensions_count;
   e05ec:	2304      	movs	r3, #4
   e05ee:	930f      	str	r3, [sp, #60]	; 0x3c
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
    Resize(dimensions_count);
    int32* dst_dims = DimsData();
    std::memcpy(dst_dims, dims_data, dimensions_count * sizeof(int32));
   e05f0:	e895 000f 	ldmia.w	r5, {r0, r1, r2, r3}
   e05f4:	af10      	add	r7, sp, #64	; 0x40
   e05f6:	e887 000f 	stmia.w	r7, {r0, r1, r2, r3}
  const int batch_size = input->dims->data[0];
  const int input_size = input->dims->data[1];
  const int32_t shape_data[4] = {batch_size, 1, 1, input_size};
  RuntimeShape shape(4, shape_data);
  SoftmaxParams op_params;
  op_params.input_multiplier = data->input_multiplier;
   e05fa:	9b06      	ldr	r3, [sp, #24]
   e05fc:	9316      	str	r3, [sp, #88]	; 0x58
  op_params.input_left_shift = data->input_left_shift;
   e05fe:	9b07      	ldr	r3, [sp, #28]
   e0600:	9317      	str	r3, [sp, #92]	; 0x5c
  op_params.diff_min = data->diff_min;
   e0602:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e0604:	931a      	str	r3, [sp, #104]	; 0x68
   e0606:	b104      	cbz	r4, e060a <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x146>
   e0608:	6864      	ldr	r4, [r4, #4]
  tflite::reference_ops::Softmax(op_params, shape,
                                 GetTensorData<uint8_t>(input), shape,
                                 GetTensorData<uint8_t>(output));
   e060a:	9400      	str	r4, [sp, #0]
   e060c:	ab0f      	add	r3, sp, #60	; 0x3c
   e060e:	a814      	add	r0, sp, #80	; 0x50
   e0610:	6872      	ldr	r2, [r6, #4]
   e0612:	4619      	mov	r1, r3
   e0614:	f7ff feaa 	bl	e036c <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph>
  // tensor is 4D in a special way. We will convert a (X, Y) shape into a (X,
  // 1, 1, Y) shape.
  const int batch_size = input->dims->data[0];
  const int input_size = input->dims->data[1];
  const int32_t shape_data[4] = {batch_size, 1, 1, input_size};
  RuntimeShape shape(4, shape_data);
   e0618:	a80f      	add	r0, sp, #60	; 0x3c
  SoftmaxParams op_params;
  op_params.input_multiplier = data->input_multiplier;
  op_params.input_left_shift = data->input_left_shift;
  op_params.diff_min = data->diff_min;
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   e061a:	f7f5 ff14 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
        Softmax2DQuantized(input, output, params, data);
        return kTfLiteOk;
      }
      if (NumDimensions(input) == 4) {
        Softmax4DQuantized(input, output, params, data);
        return kTfLiteOk;
   e061e:	2000      	movs	r0, #0
   e0620:	e03a      	b.n	e0698 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1d4>
    case kTfLiteUInt8: {
      if (NumDimensions(input) == 1) {
        Softmax1DQuantized(input, output, params, data);
        return kTfLiteOk;
      }
      if (NumDimensions(input) == 2) {
   e0622:	2f02      	cmp	r7, #2
   e0624:	d10f      	bne.n	e0646 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x182>
  // always traverses the last dimension of a 4D tensor, we will pretend our 2D
  // tensor is 4D in a special way. We will convert a (X, Y) shape into a (X,
  // 1, 1, Y) shape.
  const int batch_size = input->dims->data[0];
  const int input_size = input->dims->data[1];
  const int32_t shape_data[4] = {batch_size, 1, 1, input_size};
   e0626:	ad0a      	add	r5, sp, #40	; 0x28
   e0628:	2210      	movs	r2, #16
   e062a:	2100      	movs	r1, #0
   e062c:	4628      	mov	r0, r5
                        TfLiteSoftmaxParams* params, OpData* data) {
  // TODO(ahentz): this is arguably a dirty trick. Since the implementation
  // always traverses the last dimension of a 4D tensor, we will pretend our 2D
  // tensor is 4D in a special way. We will convert a (X, Y) shape into a (X,
  // 1, 1, Y) shape.
  const int batch_size = input->dims->data[0];
   e062e:	f8d3 8004 	ldr.w	r8, [r3, #4]
  const int input_size = input->dims->data[1];
   e0632:	689f      	ldr	r7, [r3, #8]
  const int32_t shape_data[4] = {batch_size, 1, 1, input_size};
   e0634:	f007 f8e0 	bl	e77f8 <memset>
   e0638:	2301      	movs	r3, #1
   e063a:	930b      	str	r3, [sp, #44]	; 0x2c
   e063c:	930c      	str	r3, [sp, #48]	; 0x30
   e063e:	f8cd 8028 	str.w	r8, [sp, #40]	; 0x28
   e0642:	970d      	str	r7, [sp, #52]	; 0x34
   e0644:	e7d2      	b.n	e05ec <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x128>
      }
      if (NumDimensions(input) == 2) {
        Softmax2DQuantized(input, output, params, data);
        return kTfLiteOk;
      }
      if (NumDimensions(input) == 4) {
   e0646:	2f04      	cmp	r7, #4
   e0648:	d11c      	bne.n	e0684 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1c0>
}

void Softmax4DQuantized(const TfLiteTensor* input, TfLiteTensor* output,
                        TfLiteSoftmaxParams* params, OpData* data) {
  SoftmaxParams op_params;
  op_params.input_multiplier = data->input_multiplier;
   e064a:	9b06      	ldr	r3, [sp, #24]
   e064c:	9316      	str	r3, [sp, #88]	; 0x58
  op_params.input_left_shift = data->input_left_shift;
   e064e:	9b07      	ldr	r3, [sp, #28]
   e0650:	9317      	str	r3, [sp, #92]	; 0x5c
  op_params.diff_min = data->diff_min;
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   e0652:	4631      	mov	r1, r6
void Softmax4DQuantized(const TfLiteTensor* input, TfLiteTensor* output,
                        TfLiteSoftmaxParams* params, OpData* data) {
  SoftmaxParams op_params;
  op_params.input_multiplier = data->input_multiplier;
  op_params.input_left_shift = data->input_left_shift;
  op_params.diff_min = data->diff_min;
   e0654:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e0656:	931a      	str	r3, [sp, #104]	; 0x68
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   e0658:	a80a      	add	r0, sp, #40	; 0x28
   e065a:	f7f6 f9a4 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(output), GetTensorData<uint8_t>(output));
   e065e:	4621      	mov	r1, r4
   e0660:	a80f      	add	r0, sp, #60	; 0x3c
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e0662:	6875      	ldr	r5, [r6, #4]
   e0664:	f7f6 f99f 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e0668:	b104      	cbz	r4, e066c <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1a8>
   e066a:	6864      	ldr	r4, [r4, #4]
   e066c:	9400      	str	r4, [sp, #0]
   e066e:	ab0f      	add	r3, sp, #60	; 0x3c
   e0670:	462a      	mov	r2, r5
   e0672:	a90a      	add	r1, sp, #40	; 0x28
   e0674:	a814      	add	r0, sp, #80	; 0x50
   e0676:	f7ff fe79 	bl	e036c <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph>
   e067a:	a80f      	add	r0, sp, #60	; 0x3c
   e067c:	f7f5 fee3 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  SoftmaxParams op_params;
  op_params.input_multiplier = data->input_multiplier;
  op_params.input_left_shift = data->input_left_shift;
  op_params.diff_min = data->diff_min;
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   e0680:	a80a      	add	r0, sp, #40	; 0x28
   e0682:	e7ca      	b.n	e061a <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x156>
        Softmax4DQuantized(input, output, params, data);
        return kTfLiteOk;
      }
      context->ReportError(
          context, "Only 2D and 4D tensors supported currently, got %dD.",
          NumDimensions(input));
   e0684:	696b      	ldr	r3, [r5, #20]
   e0686:	490e      	ldr	r1, [pc, #56]	; (e06c0 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1fc>)
   e0688:	463a      	mov	r2, r7
   e068a:	e002      	b.n	e0692 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1ce>
      return kTfLiteError;
    }
    default:
      context->ReportError(
          context, "Only float32 and uint8_t supported currently, got %d.",
          input->type);
   e068c:	696b      	ldr	r3, [r5, #20]
   e068e:	490d      	ldr	r1, [pc, #52]	; (e06c4 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x200>)
   e0690:	4642      	mov	r2, r8
   e0692:	4628      	mov	r0, r5
   e0694:	4798      	blx	r3
  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);

  OpData local_data_object;
  OpData* data = &local_data_object;
  TF_LITE_ENSURE_STATUS(
   e0696:	2001      	movs	r0, #1
      context->ReportError(
          context, "Only float32 and uint8_t supported currently, got %d.",
          input->type);
      return kTfLiteError;
  }
}
   e0698:	b01f      	add	sp, #124	; 0x7c
   e069a:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}
   e069e:	bf00      	nop
   e06a0:	000eb1bd 	.word	0x000eb1bd
   e06a4:	000ea430 	.word	0x000ea430
   e06a8:	000ea387 	.word	0x000ea387
   e06ac:	000e98f8 	.word	0x000e98f8
   e06b0:	3b800000 	.word	0x3b800000
   e06b4:	000ea44a 	.word	0x000ea44a
   e06b8:	000e9ac8 	.word	0x000e9ac8
   e06bc:	000ea46c 	.word	0x000ea46c
   e06c0:	000ea4a5 	.word	0x000ea4a5
   e06c4:	000ea4da 	.word	0x000ea4da

000e06c8 <_ZN6tflite3ops5micro5split7PrepareEP13TfLiteContextP10TfLiteNode>:
namespace micro {
namespace split {

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   e06c8:	2000      	movs	r0, #0
   e06ca:	4770      	bx	lr

000e06cc <_ZN6tflite3ops5micro14Register_SPLITEv>:
}  // namespace split

TfLiteRegistration* Register_SPLIT() {
  static TfLiteRegistration r = {nullptr, nullptr, split::Prepare, split::Eval};
  return &r;
}
   e06cc:	4800      	ldr	r0, [pc, #0]	; (e06d0 <_ZN6tflite3ops5micro14Register_SPLITEv+0x4>)
   e06ce:	4770      	bx	lr
   e06d0:	2003c1c8 	.word	0x2003c1c8

000e06d4 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>:
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e06d4:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e06d8:	684c      	ldr	r4, [r1, #4]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
   e06da:	6896      	ldr	r6, [r2, #8]
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e06dc:	6865      	ldr	r5, [r4, #4]

  const int split_dimensions = input_dims->size;
   e06de:	f8d6 c000 	ldr.w	ip, [r6]
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
   e06e2:	6827      	ldr	r7, [r4, #0]
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e06e4:	6884      	ldr	r4, [r0, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e06e6:	2b00      	cmp	r3, #0
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e06e8:	f04f 0e38 	mov.w	lr, #56	; 0x38
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e06ec:	b085      	sub	sp, #20
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e06ee:	fb0e 4405 	mla	r4, lr, r5, r4

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e06f2:	bfb8      	it	lt
   e06f4:	4463      	addlt	r3, ip

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e06f6:	4563      	cmp	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e06f8:	9102      	str	r1, [sp, #8]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e06fa:	68a4      	ldr	r4, [r4, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e06fc:	db01      	blt.n	e0702 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2e>
   e06fe:	f003 fe15 	bl	e432c <abort>
  TFLITE_DCHECK_EQ(output_dims->size, split_dimensions);
   e0702:	6825      	ldr	r5, [r4, #0]
   e0704:	45ac      	cmp	ip, r5
   e0706:	d1fa      	bne.n	e06fe <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e0708:	009d      	lsls	r5, r3, #2
   e070a:	eb04 0e05 	add.w	lr, r4, r5

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
   e070e:	4435      	add	r5, r6
   e0710:	f8de 4004 	ldr.w	r4, [lr, #4]
   e0714:	686d      	ldr	r5, [r5, #4]
   e0716:	437c      	muls	r4, r7
   e0718:	42ac      	cmp	r4, r5
   e071a:	d1f0      	bne.n	e06fe <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e071c:	2401      	movs	r4, #1
   e071e:	2500      	movs	r5, #0
   e0720:	e9cd 4500 	strd	r4, r5, [sp]
   e0724:	46b2      	mov	sl, r6
   e0726:	f04f 0800 	mov.w	r8, #0
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e072a:	4598      	cmp	r8, r3
   e072c:	da14      	bge.n	e0758 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x84>
    outer_size *= input_dims->data[i];
   e072e:	f85a 9f04 	ldr.w	r9, [sl, #4]!
   e0732:	9900      	ldr	r1, [sp, #0]
   e0734:	464c      	mov	r4, r9
   e0736:	17e5      	asrs	r5, r4, #31
   e0738:	fb01 f405 	mul.w	r4, r1, r5
   e073c:	9901      	ldr	r1, [sp, #4]
   e073e:	fb09 4b01 	mla	fp, r9, r1, r4
   e0742:	9900      	ldr	r1, [sp, #0]
   e0744:	fba1 4509 	umull	r4, r5, r1, r9
   e0748:	e9cd 4500 	strd	r4, r5, [sp]
   e074c:	9901      	ldr	r1, [sp, #4]
   e074e:	4459      	add	r1, fp
   e0750:	9101      	str	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e0752:	f108 0801 	add.w	r8, r8, #1
   e0756:	e7e8      	b.n	e072a <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x56>
   e0758:	3301      	adds	r3, #1
   e075a:	f10c 0c01 	add.w	ip, ip, #1
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
   e075e:	2401      	movs	r4, #1
   e0760:	2500      	movs	r5, #0
   e0762:	3301      	adds	r3, #1
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e0764:	4563      	cmp	r3, ip
   e0766:	d00b      	beq.n	e0780 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xac>
    base_inner_size *= input_dims->data[i];
   e0768:	f856 a023 	ldr.w	sl, [r6, r3, lsl #2]
   e076c:	ea4f 79ea 	mov.w	r9, sl, asr #31
   e0770:	fb04 f809 	mul.w	r8, r4, r9
   e0774:	fb0a 8805 	mla	r8, sl, r5, r8
   e0778:	fba4 450a 	umull	r4, r5, r4, sl
   e077c:	4445      	add	r5, r8
  for (int i = 0; i < axis; ++i) {
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e077e:	e7f0      	b.n	e0762 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x8e>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e0780:	f8d2 c004 	ldr.w	ip, [r2, #4]
   e0784:	f04f 0800 	mov.w	r8, #0
   e0788:	f04f 0900 	mov.w	r9, #0
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e078c:	e9dd 2300 	ldrd	r2, r3, [sp]
   e0790:	4590      	cmp	r8, r2
   e0792:	eb79 0303 	sbcs.w	r3, r9, r3
   e0796:	f8cd 800c 	str.w	r8, [sp, #12]
   e079a:	da2a      	bge.n	e07f2 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x11e>
   e079c:	2600      	movs	r6, #0
    for (int i = 0; i < output_count; ++i) {
   e079e:	42be      	cmp	r6, r7
   e07a0:	da22      	bge.n	e07e8 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x114>
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e07a2:	9b02      	ldr	r3, [sp, #8]
   e07a4:	685b      	ldr	r3, [r3, #4]
   e07a6:	eb03 0386 	add.w	r3, r3, r6, lsl #2
   e07aa:	2138      	movs	r1, #56	; 0x38
   e07ac:	685a      	ldr	r2, [r3, #4]
   e07ae:	6883      	ldr	r3, [r0, #8]
   e07b0:	fb01 3302 	mla	r3, r1, r2, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e07b4:	b103      	cbz	r3, e07b8 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xe4>
   e07b6:	685b      	ldr	r3, [r3, #4]
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
   e07b8:	f8de 2004 	ldr.w	r2, [lr, #4]
   e07bc:	9903      	ldr	r1, [sp, #12]
   e07be:	4362      	muls	r2, r4
   e07c0:	fb02 fb01 	mul.w	fp, r2, r1
   e07c4:	eb03 038b 	add.w	r3, r3, fp, lsl #2
   e07c8:	46e2      	mov	sl, ip
      T* output_ptr = output_data + k * copy_size;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e07ca:	f04f 0b00 	mov.w	fp, #0
   e07ce:	4593      	cmp	fp, r2
   e07d0:	da06      	bge.n	e07e0 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x10c>
   e07d2:	ecfa 7a01 	vldmia	sl!, {s15}
   e07d6:	f10b 0b01 	add.w	fp, fp, #1
   e07da:	ece3 7a01 	vstmia	r3!, {s15}
   e07de:	e7f6      	b.n	e07ce <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xfa>
      input_ptr += copy_size;
   e07e0:	eb0c 0c82 	add.w	ip, ip, r2, lsl #2
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
   e07e4:	3601      	adds	r6, #1
   e07e6:	e7da      	b.n	e079e <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xca>
   e07e8:	f118 0801 	adds.w	r8, r8, #1
   e07ec:	f149 0900 	adc.w	r9, r9, #0
  for (int i = axis + 1; i < split_dimensions; ++i) {
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e07f0:	e7cc      	b.n	e078c <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xb8>
      input_ptr += copy_size;
    }
  }

  return kTfLiteOk;
}
   e07f2:	2000      	movs	r0, #0
   e07f4:	b005      	add	sp, #20
   e07f6:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000e07fa <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>:
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e07fa:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e07fe:	684c      	ldr	r4, [r1, #4]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
   e0800:	6896      	ldr	r6, [r2, #8]
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0802:	6865      	ldr	r5, [r4, #4]

  const int split_dimensions = input_dims->size;
   e0804:	f8d6 c000 	ldr.w	ip, [r6]
   e0808:	6827      	ldr	r7, [r4, #0]
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e080a:	6884      	ldr	r4, [r0, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e080c:	2b00      	cmp	r3, #0
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e080e:	f04f 0e38 	mov.w	lr, #56	; 0x38
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e0812:	b085      	sub	sp, #20
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0814:	fb0e 4405 	mla	r4, lr, r5, r4

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e0818:	bfb8      	it	lt
   e081a:	4463      	addlt	r3, ip

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e081c:	4563      	cmp	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e081e:	9102      	str	r1, [sp, #8]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0820:	68a4      	ldr	r4, [r4, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e0822:	db01      	blt.n	e0828 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2e>
   e0824:	f003 fd82 	bl	e432c <abort>
  TFLITE_DCHECK_EQ(output_dims->size, split_dimensions);
   e0828:	6825      	ldr	r5, [r4, #0]
   e082a:	45ac      	cmp	ip, r5
   e082c:	d1fa      	bne.n	e0824 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e082e:	009d      	lsls	r5, r3, #2
   e0830:	eb04 0e05 	add.w	lr, r4, r5

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
   e0834:	4435      	add	r5, r6
   e0836:	f8de 4004 	ldr.w	r4, [lr, #4]
   e083a:	686d      	ldr	r5, [r5, #4]
   e083c:	437c      	muls	r4, r7
   e083e:	42ac      	cmp	r4, r5
   e0840:	d1f0      	bne.n	e0824 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e0842:	2401      	movs	r4, #1
   e0844:	2500      	movs	r5, #0
   e0846:	e9cd 4500 	strd	r4, r5, [sp]
   e084a:	46b2      	mov	sl, r6
   e084c:	f04f 0800 	mov.w	r8, #0
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e0850:	4598      	cmp	r8, r3
   e0852:	da14      	bge.n	e087e <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x84>
    outer_size *= input_dims->data[i];
   e0854:	f85a 9f04 	ldr.w	r9, [sl, #4]!
   e0858:	9900      	ldr	r1, [sp, #0]
   e085a:	464c      	mov	r4, r9
   e085c:	17e5      	asrs	r5, r4, #31
   e085e:	fb01 f405 	mul.w	r4, r1, r5
   e0862:	9901      	ldr	r1, [sp, #4]
   e0864:	fb09 4b01 	mla	fp, r9, r1, r4
   e0868:	9900      	ldr	r1, [sp, #0]
   e086a:	fba1 4509 	umull	r4, r5, r1, r9
   e086e:	e9cd 4500 	strd	r4, r5, [sp]
   e0872:	9901      	ldr	r1, [sp, #4]
   e0874:	4459      	add	r1, fp
   e0876:	9101      	str	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e0878:	f108 0801 	add.w	r8, r8, #1
   e087c:	e7e8      	b.n	e0850 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x56>
   e087e:	3301      	adds	r3, #1
   e0880:	f10c 0c01 	add.w	ip, ip, #1
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
   e0884:	2401      	movs	r4, #1
   e0886:	2500      	movs	r5, #0
   e0888:	3301      	adds	r3, #1
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e088a:	4563      	cmp	r3, ip
   e088c:	d00b      	beq.n	e08a6 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xac>
    base_inner_size *= input_dims->data[i];
   e088e:	f856 a023 	ldr.w	sl, [r6, r3, lsl #2]
   e0892:	ea4f 79ea 	mov.w	r9, sl, asr #31
   e0896:	fb04 f809 	mul.w	r8, r4, r9
   e089a:	fb0a 8805 	mla	r8, sl, r5, r8
   e089e:	fba4 450a 	umull	r4, r5, r4, sl
   e08a2:	4445      	add	r5, r8
  for (int i = 0; i < axis; ++i) {
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e08a4:	e7f0      	b.n	e0888 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x8e>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e08a6:	6856      	ldr	r6, [r2, #4]
   e08a8:	f04f 0800 	mov.w	r8, #0
   e08ac:	f04f 0900 	mov.w	r9, #0
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e08b0:	e9dd 2300 	ldrd	r2, r3, [sp]
   e08b4:	4590      	cmp	r8, r2
   e08b6:	eb79 0303 	sbcs.w	r3, r9, r3
   e08ba:	f8cd 800c 	str.w	r8, [sp, #12]
   e08be:	da27      	bge.n	e0910 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x116>
   e08c0:	f04f 0c00 	mov.w	ip, #0
    for (int i = 0; i < output_count; ++i) {
   e08c4:	45bc      	cmp	ip, r7
   e08c6:	da1e      	bge.n	e0906 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x10c>
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e08c8:	9b02      	ldr	r3, [sp, #8]
   e08ca:	685b      	ldr	r3, [r3, #4]
   e08cc:	eb03 038c 	add.w	r3, r3, ip, lsl #2
   e08d0:	2138      	movs	r1, #56	; 0x38
   e08d2:	685a      	ldr	r2, [r3, #4]
   e08d4:	6883      	ldr	r3, [r0, #8]
   e08d6:	fb01 3302 	mla	r3, r1, r2, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e08da:	b103      	cbz	r3, e08de <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xe4>
   e08dc:	685b      	ldr	r3, [r3, #4]
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
   e08de:	f8de 2004 	ldr.w	r2, [lr, #4]
   e08e2:	9903      	ldr	r1, [sp, #12]
   e08e4:	4362      	muls	r2, r4
   e08e6:	fb02 3301 	mla	r3, r2, r1, r3
      T* output_ptr = output_data + k * copy_size;
   e08ea:	46b2      	mov	sl, r6
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e08ec:	ebc6 0b0a 	rsb	fp, r6, sl
   e08f0:	4593      	cmp	fp, r2
   e08f2:	da04      	bge.n	e08fe <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x104>
   e08f4:	f81a bb01 	ldrb.w	fp, [sl], #1
   e08f8:	f803 bb01 	strb.w	fp, [r3], #1
   e08fc:	e7f6      	b.n	e08ec <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xf2>
      input_ptr += copy_size;
   e08fe:	4416      	add	r6, r2
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
   e0900:	f10c 0c01 	add.w	ip, ip, #1
   e0904:	e7de      	b.n	e08c4 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xca>
   e0906:	f118 0801 	adds.w	r8, r8, #1
   e090a:	f149 0900 	adc.w	r9, r9, #0
  for (int i = axis + 1; i < split_dimensions; ++i) {
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e090e:	e7cf      	b.n	e08b0 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xb6>
      input_ptr += copy_size;
    }
  }

  return kTfLiteOk;
}
   e0910:	2000      	movs	r0, #0
   e0912:	b005      	add	sp, #20
   e0914:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000e0918 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>:
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e0918:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e091c:	684c      	ldr	r4, [r1, #4]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
   e091e:	6896      	ldr	r6, [r2, #8]
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0920:	6865      	ldr	r5, [r4, #4]

  const int split_dimensions = input_dims->size;
   e0922:	f8d6 c000 	ldr.w	ip, [r6]
   e0926:	6827      	ldr	r7, [r4, #0]
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0928:	6884      	ldr	r4, [r0, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e092a:	2b00      	cmp	r3, #0
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e092c:	f04f 0e38 	mov.w	lr, #56	; 0x38
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e0930:	b085      	sub	sp, #20
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0932:	fb0e 4405 	mla	r4, lr, r5, r4

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e0936:	bfb8      	it	lt
   e0938:	4463      	addlt	r3, ip

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e093a:	4563      	cmp	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e093c:	9102      	str	r1, [sp, #8]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e093e:	68a4      	ldr	r4, [r4, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e0940:	db01      	blt.n	e0946 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2e>
   e0942:	f003 fcf3 	bl	e432c <abort>
  TFLITE_DCHECK_EQ(output_dims->size, split_dimensions);
   e0946:	6825      	ldr	r5, [r4, #0]
   e0948:	45ac      	cmp	ip, r5
   e094a:	d1fa      	bne.n	e0942 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e094c:	009d      	lsls	r5, r3, #2
   e094e:	eb04 0e05 	add.w	lr, r4, r5

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
   e0952:	4435      	add	r5, r6
   e0954:	f8de 4004 	ldr.w	r4, [lr, #4]
   e0958:	686d      	ldr	r5, [r5, #4]
   e095a:	437c      	muls	r4, r7
   e095c:	42ac      	cmp	r4, r5
   e095e:	d1f0      	bne.n	e0942 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e0960:	2401      	movs	r4, #1
   e0962:	2500      	movs	r5, #0
   e0964:	e9cd 4500 	strd	r4, r5, [sp]
   e0968:	46b2      	mov	sl, r6
   e096a:	f04f 0800 	mov.w	r8, #0
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e096e:	4598      	cmp	r8, r3
   e0970:	da14      	bge.n	e099c <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x84>
    outer_size *= input_dims->data[i];
   e0972:	f85a 9f04 	ldr.w	r9, [sl, #4]!
   e0976:	9900      	ldr	r1, [sp, #0]
   e0978:	464c      	mov	r4, r9
   e097a:	17e5      	asrs	r5, r4, #31
   e097c:	fb01 f405 	mul.w	r4, r1, r5
   e0980:	9901      	ldr	r1, [sp, #4]
   e0982:	fb09 4b01 	mla	fp, r9, r1, r4
   e0986:	9900      	ldr	r1, [sp, #0]
   e0988:	fba1 4509 	umull	r4, r5, r1, r9
   e098c:	e9cd 4500 	strd	r4, r5, [sp]
   e0990:	9901      	ldr	r1, [sp, #4]
   e0992:	4459      	add	r1, fp
   e0994:	9101      	str	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e0996:	f108 0801 	add.w	r8, r8, #1
   e099a:	e7e8      	b.n	e096e <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x56>
   e099c:	3301      	adds	r3, #1
   e099e:	f10c 0c01 	add.w	ip, ip, #1
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
   e09a2:	2401      	movs	r4, #1
   e09a4:	2500      	movs	r5, #0
   e09a6:	3301      	adds	r3, #1
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e09a8:	4563      	cmp	r3, ip
   e09aa:	d00b      	beq.n	e09c4 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xac>
    base_inner_size *= input_dims->data[i];
   e09ac:	f856 a023 	ldr.w	sl, [r6, r3, lsl #2]
   e09b0:	ea4f 79ea 	mov.w	r9, sl, asr #31
   e09b4:	fb04 f809 	mul.w	r8, r4, r9
   e09b8:	fb0a 8805 	mla	r8, sl, r5, r8
   e09bc:	fba4 450a 	umull	r4, r5, r4, sl
   e09c0:	4445      	add	r5, r8
  for (int i = 0; i < axis; ++i) {
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e09c2:	e7f0      	b.n	e09a6 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x8e>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e09c4:	6856      	ldr	r6, [r2, #4]
   e09c6:	f04f 0800 	mov.w	r8, #0
   e09ca:	f04f 0900 	mov.w	r9, #0
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e09ce:	e9dd 2300 	ldrd	r2, r3, [sp]
   e09d2:	4590      	cmp	r8, r2
   e09d4:	eb79 0303 	sbcs.w	r3, r9, r3
   e09d8:	f8cd 800c 	str.w	r8, [sp, #12]
   e09dc:	da27      	bge.n	e0a2e <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x116>
   e09de:	f04f 0c00 	mov.w	ip, #0
    for (int i = 0; i < output_count; ++i) {
   e09e2:	45bc      	cmp	ip, r7
   e09e4:	da1e      	bge.n	e0a24 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x10c>
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e09e6:	9b02      	ldr	r3, [sp, #8]
   e09e8:	685b      	ldr	r3, [r3, #4]
   e09ea:	eb03 038c 	add.w	r3, r3, ip, lsl #2
   e09ee:	2138      	movs	r1, #56	; 0x38
   e09f0:	685a      	ldr	r2, [r3, #4]
   e09f2:	6883      	ldr	r3, [r0, #8]
   e09f4:	fb01 3302 	mla	r3, r1, r2, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e09f8:	b103      	cbz	r3, e09fc <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xe4>
   e09fa:	685b      	ldr	r3, [r3, #4]
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
   e09fc:	f8de 2004 	ldr.w	r2, [lr, #4]
   e0a00:	9903      	ldr	r1, [sp, #12]
   e0a02:	4362      	muls	r2, r4
   e0a04:	fb02 3301 	mla	r3, r2, r1, r3
      T* output_ptr = output_data + k * copy_size;
   e0a08:	46b2      	mov	sl, r6
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e0a0a:	ebc6 0b0a 	rsb	fp, r6, sl
   e0a0e:	4593      	cmp	fp, r2
   e0a10:	da04      	bge.n	e0a1c <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x104>
   e0a12:	f91a bb01 	ldrsb.w	fp, [sl], #1
   e0a16:	f803 bb01 	strb.w	fp, [r3], #1
   e0a1a:	e7f6      	b.n	e0a0a <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xf2>
      input_ptr += copy_size;
   e0a1c:	4416      	add	r6, r2
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
   e0a1e:	f10c 0c01 	add.w	ip, ip, #1
   e0a22:	e7de      	b.n	e09e2 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xca>
   e0a24:	f118 0801 	adds.w	r8, r8, #1
   e0a28:	f149 0900 	adc.w	r9, r9, #0
  for (int i = axis + 1; i < split_dimensions; ++i) {
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e0a2c:	e7cf      	b.n	e09ce <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xb6>
      input_ptr += copy_size;
    }
  }

  return kTfLiteOk;
}
   e0a2e:	2000      	movs	r0, #0
   e0a30:	b005      	add	sp, #20
   e0a32:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000e0a36 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>:
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e0a36:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e0a3a:	684c      	ldr	r4, [r1, #4]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
   e0a3c:	6896      	ldr	r6, [r2, #8]
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0a3e:	6865      	ldr	r5, [r4, #4]

  const int split_dimensions = input_dims->size;
   e0a40:	f8d6 c000 	ldr.w	ip, [r6]
   e0a44:	6827      	ldr	r7, [r4, #0]
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0a46:	6884      	ldr	r4, [r0, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e0a48:	2b00      	cmp	r3, #0
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0a4a:	f04f 0e38 	mov.w	lr, #56	; 0x38
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e0a4e:	b085      	sub	sp, #20
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0a50:	fb0e 4405 	mla	r4, lr, r5, r4

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e0a54:	bfb8      	it	lt
   e0a56:	4463      	addlt	r3, ip

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e0a58:	4563      	cmp	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e0a5a:	9102      	str	r1, [sp, #8]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0a5c:	68a4      	ldr	r4, [r4, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e0a5e:	db01      	blt.n	e0a64 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2e>
   e0a60:	f003 fc64 	bl	e432c <abort>
  TFLITE_DCHECK_EQ(output_dims->size, split_dimensions);
   e0a64:	6825      	ldr	r5, [r4, #0]
   e0a66:	45ac      	cmp	ip, r5
   e0a68:	d1fa      	bne.n	e0a60 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e0a6a:	009d      	lsls	r5, r3, #2
   e0a6c:	eb04 0e05 	add.w	lr, r4, r5

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
   e0a70:	4435      	add	r5, r6
   e0a72:	f8de 4004 	ldr.w	r4, [lr, #4]
   e0a76:	686d      	ldr	r5, [r5, #4]
   e0a78:	437c      	muls	r4, r7
   e0a7a:	42ac      	cmp	r4, r5
   e0a7c:	d1f0      	bne.n	e0a60 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e0a7e:	2401      	movs	r4, #1
   e0a80:	2500      	movs	r5, #0
   e0a82:	e9cd 4500 	strd	r4, r5, [sp]
   e0a86:	46b2      	mov	sl, r6
   e0a88:	f04f 0800 	mov.w	r8, #0
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e0a8c:	4598      	cmp	r8, r3
   e0a8e:	da14      	bge.n	e0aba <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x84>
    outer_size *= input_dims->data[i];
   e0a90:	f85a 9f04 	ldr.w	r9, [sl, #4]!
   e0a94:	9900      	ldr	r1, [sp, #0]
   e0a96:	464c      	mov	r4, r9
   e0a98:	17e5      	asrs	r5, r4, #31
   e0a9a:	fb01 f405 	mul.w	r4, r1, r5
   e0a9e:	9901      	ldr	r1, [sp, #4]
   e0aa0:	fb09 4b01 	mla	fp, r9, r1, r4
   e0aa4:	9900      	ldr	r1, [sp, #0]
   e0aa6:	fba1 4509 	umull	r4, r5, r1, r9
   e0aaa:	e9cd 4500 	strd	r4, r5, [sp]
   e0aae:	9901      	ldr	r1, [sp, #4]
   e0ab0:	4459      	add	r1, fp
   e0ab2:	9101      	str	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e0ab4:	f108 0801 	add.w	r8, r8, #1
   e0ab8:	e7e8      	b.n	e0a8c <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x56>
   e0aba:	3301      	adds	r3, #1
   e0abc:	f10c 0c01 	add.w	ip, ip, #1
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
   e0ac0:	2401      	movs	r4, #1
   e0ac2:	2500      	movs	r5, #0
   e0ac4:	3301      	adds	r3, #1
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e0ac6:	4563      	cmp	r3, ip
   e0ac8:	d00b      	beq.n	e0ae2 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xac>
    base_inner_size *= input_dims->data[i];
   e0aca:	f856 a023 	ldr.w	sl, [r6, r3, lsl #2]
   e0ace:	ea4f 79ea 	mov.w	r9, sl, asr #31
   e0ad2:	fb04 f809 	mul.w	r8, r4, r9
   e0ad6:	fb0a 8805 	mla	r8, sl, r5, r8
   e0ada:	fba4 450a 	umull	r4, r5, r4, sl
   e0ade:	4445      	add	r5, r8
  for (int i = 0; i < axis; ++i) {
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e0ae0:	e7f0      	b.n	e0ac4 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x8e>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e0ae2:	f8d2 c004 	ldr.w	ip, [r2, #4]
   e0ae6:	f04f 0800 	mov.w	r8, #0
   e0aea:	f04f 0900 	mov.w	r9, #0
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e0aee:	e9dd 2300 	ldrd	r2, r3, [sp]
   e0af2:	4590      	cmp	r8, r2
   e0af4:	eb79 0303 	sbcs.w	r3, r9, r3
   e0af8:	f8cd 800c 	str.w	r8, [sp, #12]
   e0afc:	da29      	bge.n	e0b52 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x11c>
   e0afe:	2600      	movs	r6, #0
    for (int i = 0; i < output_count; ++i) {
   e0b00:	42be      	cmp	r6, r7
   e0b02:	da21      	bge.n	e0b48 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x112>
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e0b04:	9b02      	ldr	r3, [sp, #8]
   e0b06:	685b      	ldr	r3, [r3, #4]
   e0b08:	eb03 0386 	add.w	r3, r3, r6, lsl #2
   e0b0c:	2138      	movs	r1, #56	; 0x38
   e0b0e:	685a      	ldr	r2, [r3, #4]
   e0b10:	6883      	ldr	r3, [r0, #8]
   e0b12:	fb01 3302 	mla	r3, r1, r2, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e0b16:	b103      	cbz	r3, e0b1a <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xe4>
   e0b18:	685b      	ldr	r3, [r3, #4]
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
   e0b1a:	f8de 2004 	ldr.w	r2, [lr, #4]
      T* output_ptr = output_data + k * copy_size;
   e0b1e:	9903      	ldr	r1, [sp, #12]
  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
   e0b20:	4362      	muls	r2, r4
      T* output_ptr = output_data + k * copy_size;
   e0b22:	fb02 fb01 	mul.w	fp, r2, r1
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e0b26:	f04f 0a00 	mov.w	sl, #0
   e0b2a:	eb03 034b 	add.w	r3, r3, fp, lsl #1
   e0b2e:	4592      	cmp	sl, r2
   e0b30:	da06      	bge.n	e0b40 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x10a>
   e0b32:	f93c b01a 	ldrsh.w	fp, [ip, sl, lsl #1]
   e0b36:	f823 b01a 	strh.w	fp, [r3, sl, lsl #1]
   e0b3a:	f10a 0a01 	add.w	sl, sl, #1
   e0b3e:	e7f6      	b.n	e0b2e <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xf8>
      input_ptr += copy_size;
   e0b40:	eb0c 0c42 	add.w	ip, ip, r2, lsl #1
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
   e0b44:	3601      	adds	r6, #1
   e0b46:	e7db      	b.n	e0b00 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xca>
   e0b48:	f118 0801 	adds.w	r8, r8, #1
   e0b4c:	f149 0900 	adc.w	r9, r9, #0
  for (int i = axis + 1; i < split_dimensions; ++i) {
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e0b50:	e7cd      	b.n	e0aee <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xb8>
      input_ptr += copy_size;
    }
  }

  return kTfLiteOk;
}
   e0b52:	2000      	movs	r0, #0
   e0b54:	b005      	add	sp, #20
   e0b56:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000e0b5a <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>:
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e0b5a:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e0b5e:	684c      	ldr	r4, [r1, #4]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
   e0b60:	6896      	ldr	r6, [r2, #8]
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0b62:	6865      	ldr	r5, [r4, #4]

  const int split_dimensions = input_dims->size;
   e0b64:	f8d6 c000 	ldr.w	ip, [r6]
   e0b68:	6827      	ldr	r7, [r4, #0]
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0b6a:	6884      	ldr	r4, [r0, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e0b6c:	2b00      	cmp	r3, #0
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0b6e:	f04f 0e38 	mov.w	lr, #56	; 0x38
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e0b72:	b085      	sub	sp, #20
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0b74:	fb0e 4405 	mla	r4, lr, r5, r4

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e0b78:	bfb8      	it	lt
   e0b7a:	4463      	addlt	r3, ip

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e0b7c:	4563      	cmp	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e0b7e:	9102      	str	r1, [sp, #8]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0b80:	68a4      	ldr	r4, [r4, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e0b82:	db01      	blt.n	e0b88 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2e>
   e0b84:	f003 fbd2 	bl	e432c <abort>
  TFLITE_DCHECK_EQ(output_dims->size, split_dimensions);
   e0b88:	6825      	ldr	r5, [r4, #0]
   e0b8a:	45ac      	cmp	ip, r5
   e0b8c:	d1fa      	bne.n	e0b84 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e0b8e:	009d      	lsls	r5, r3, #2
   e0b90:	eb04 0e05 	add.w	lr, r4, r5

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
   e0b94:	4435      	add	r5, r6
   e0b96:	f8de 4004 	ldr.w	r4, [lr, #4]
   e0b9a:	686d      	ldr	r5, [r5, #4]
   e0b9c:	437c      	muls	r4, r7
   e0b9e:	42ac      	cmp	r4, r5
   e0ba0:	d1f0      	bne.n	e0b84 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e0ba2:	2401      	movs	r4, #1
   e0ba4:	2500      	movs	r5, #0
   e0ba6:	e9cd 4500 	strd	r4, r5, [sp]
   e0baa:	46b2      	mov	sl, r6
   e0bac:	f04f 0800 	mov.w	r8, #0
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e0bb0:	4598      	cmp	r8, r3
   e0bb2:	da14      	bge.n	e0bde <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x84>
    outer_size *= input_dims->data[i];
   e0bb4:	f85a 9f04 	ldr.w	r9, [sl, #4]!
   e0bb8:	9900      	ldr	r1, [sp, #0]
   e0bba:	464c      	mov	r4, r9
   e0bbc:	17e5      	asrs	r5, r4, #31
   e0bbe:	fb01 f405 	mul.w	r4, r1, r5
   e0bc2:	9901      	ldr	r1, [sp, #4]
   e0bc4:	fb09 4b01 	mla	fp, r9, r1, r4
   e0bc8:	9900      	ldr	r1, [sp, #0]
   e0bca:	fba1 4509 	umull	r4, r5, r1, r9
   e0bce:	e9cd 4500 	strd	r4, r5, [sp]
   e0bd2:	9901      	ldr	r1, [sp, #4]
   e0bd4:	4459      	add	r1, fp
   e0bd6:	9101      	str	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e0bd8:	f108 0801 	add.w	r8, r8, #1
   e0bdc:	e7e8      	b.n	e0bb0 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x56>
   e0bde:	3301      	adds	r3, #1
   e0be0:	f10c 0c01 	add.w	ip, ip, #1
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
   e0be4:	2401      	movs	r4, #1
   e0be6:	2500      	movs	r5, #0
   e0be8:	3301      	adds	r3, #1
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e0bea:	4563      	cmp	r3, ip
   e0bec:	d00b      	beq.n	e0c06 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xac>
    base_inner_size *= input_dims->data[i];
   e0bee:	f856 a023 	ldr.w	sl, [r6, r3, lsl #2]
   e0bf2:	ea4f 79ea 	mov.w	r9, sl, asr #31
   e0bf6:	fb04 f809 	mul.w	r8, r4, r9
   e0bfa:	fb0a 8805 	mla	r8, sl, r5, r8
   e0bfe:	fba4 450a 	umull	r4, r5, r4, sl
   e0c02:	4445      	add	r5, r8
  for (int i = 0; i < axis; ++i) {
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e0c04:	e7f0      	b.n	e0be8 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x8e>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e0c06:	f8d2 c004 	ldr.w	ip, [r2, #4]
   e0c0a:	f04f 0800 	mov.w	r8, #0
   e0c0e:	f04f 0900 	mov.w	r9, #0
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e0c12:	e9dd 2300 	ldrd	r2, r3, [sp]
   e0c16:	4590      	cmp	r8, r2
   e0c18:	eb79 0303 	sbcs.w	r3, r9, r3
   e0c1c:	f8cd 800c 	str.w	r8, [sp, #12]
   e0c20:	da29      	bge.n	e0c76 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x11c>
   e0c22:	2600      	movs	r6, #0
    for (int i = 0; i < output_count; ++i) {
   e0c24:	42be      	cmp	r6, r7
   e0c26:	da21      	bge.n	e0c6c <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x112>
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e0c28:	9b02      	ldr	r3, [sp, #8]
   e0c2a:	685b      	ldr	r3, [r3, #4]
   e0c2c:	eb03 0386 	add.w	r3, r3, r6, lsl #2
   e0c30:	2138      	movs	r1, #56	; 0x38
   e0c32:	685a      	ldr	r2, [r3, #4]
   e0c34:	6883      	ldr	r3, [r0, #8]
   e0c36:	fb01 3302 	mla	r3, r1, r2, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e0c3a:	b103      	cbz	r3, e0c3e <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xe4>
   e0c3c:	685b      	ldr	r3, [r3, #4]
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
   e0c3e:	f8de 2004 	ldr.w	r2, [lr, #4]
      T* output_ptr = output_data + k * copy_size;
   e0c42:	9903      	ldr	r1, [sp, #12]
  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
   e0c44:	4362      	muls	r2, r4
      T* output_ptr = output_data + k * copy_size;
   e0c46:	fb02 fb01 	mul.w	fp, r2, r1
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e0c4a:	f04f 0a00 	mov.w	sl, #0
   e0c4e:	eb03 038b 	add.w	r3, r3, fp, lsl #2
   e0c52:	4592      	cmp	sl, r2
   e0c54:	da06      	bge.n	e0c64 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x10a>
   e0c56:	f85c 102a 	ldr.w	r1, [ip, sl, lsl #2]
   e0c5a:	f843 102a 	str.w	r1, [r3, sl, lsl #2]
   e0c5e:	f10a 0a01 	add.w	sl, sl, #1
   e0c62:	e7f6      	b.n	e0c52 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xf8>
      input_ptr += copy_size;
   e0c64:	eb0c 0c82 	add.w	ip, ip, r2, lsl #2
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
   e0c68:	3601      	adds	r6, #1
   e0c6a:	e7db      	b.n	e0c24 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xca>
   e0c6c:	f118 0801 	adds.w	r8, r8, #1
   e0c70:	f149 0900 	adc.w	r9, r9, #0
  for (int i = axis + 1; i < split_dimensions; ++i) {
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e0c74:	e7cd      	b.n	e0c12 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xb8>
      input_ptr += copy_size;
    }
  }

  return kTfLiteOk;
}
   e0c76:	2000      	movs	r0, #0
   e0c78:	b005      	add	sp, #20
   e0c7a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

000e0c80 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e0c80:	e92d 41f3 	stmdb	sp!, {r0, r1, r4, r5, r6, r7, r8, lr}
   e0c84:	680a      	ldr	r2, [r1, #0]
   e0c86:	f8d0 e008 	ldr.w	lr, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e0c8a:	6857      	ldr	r7, [r2, #4]
   e0c8c:	2338      	movs	r3, #56	; 0x38
   e0c8e:	fb03 e707 	mla	r7, r3, r7, lr
   e0c92:	4605      	mov	r5, r0
  const TfLiteTensor* input = GetInput(context, node, 1);

  // Dynamic output tensors are needed if axis tensor is not constant.
  // But Micro doesn't support dynamic memeory allocation, so we only support
  // constant axis tensor for now.
  TF_LITE_ENSURE_MSG(context, IsConstantTensor(axis),
   e0c94:	f897 8014 	ldrb.w	r8, [r7, #20]
   e0c98:	f1b8 0f01 	cmp.w	r8, #1
   e0c9c:	d003      	beq.n	e0ca6 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0x26>
   e0c9e:	6943      	ldr	r3, [r0, #20]
   e0ca0:	4926      	ldr	r1, [pc, #152]	; (e0d3c <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xbc>)
   e0ca2:	4798      	blx	r3
   e0ca4:	e046      	b.n	e0d34 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb4>
   e0ca6:	6896      	ldr	r6, [r2, #8]
   e0ca8:	435e      	muls	r6, r3
                     "Non constant axis tensor not supported");

  int axis_value = GetTensorData<int32_t>(axis)[0];
   e0caa:	687b      	ldr	r3, [r7, #4]
   e0cac:	681b      	ldr	r3, [r3, #0]
   e0cae:	eb0e 0206 	add.w	r2, lr, r6
  if (axis_value < 0) {
   e0cb2:	2b00      	cmp	r3, #0
   e0cb4:	6897      	ldr	r7, [r2, #8]
   e0cb6:	da0a      	bge.n	e0cce <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0x4e>
    axis_value += NumDimensions(input);
  }

  TF_LITE_ENSURE(context, axis_value >= 0);
   e0cb8:	683c      	ldr	r4, [r7, #0]
   e0cba:	191b      	adds	r3, r3, r4
   e0cbc:	d507      	bpl.n	e0cce <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0x4e>
   e0cbe:	4b20      	ldr	r3, [pc, #128]	; (e0d40 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xc0>)
   e0cc0:	9300      	str	r3, [sp, #0]
   e0cc2:	6945      	ldr	r5, [r0, #20]
   e0cc4:	4a1f      	ldr	r2, [pc, #124]	; (e0d44 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xc4>)
   e0cc6:	4920      	ldr	r1, [pc, #128]	; (e0d48 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xc8>)
   e0cc8:	2357      	movs	r3, #87	; 0x57
   e0cca:	47a8      	blx	r5
   e0ccc:	e032      	b.n	e0d34 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb4>
  TF_LITE_ENSURE(context, axis_value < NumDimensions(input));
   e0cce:	6838      	ldr	r0, [r7, #0]
   e0cd0:	4283      	cmp	r3, r0
   e0cd2:	db08      	blt.n	e0ce6 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0x66>
   e0cd4:	4b1d      	ldr	r3, [pc, #116]	; (e0d4c <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xcc>)
   e0cd6:	9300      	str	r3, [sp, #0]
   e0cd8:	696c      	ldr	r4, [r5, #20]
   e0cda:	4a1a      	ldr	r2, [pc, #104]	; (e0d44 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xc4>)
   e0cdc:	491a      	ldr	r1, [pc, #104]	; (e0d48 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xc8>)
   e0cde:	2358      	movs	r3, #88	; 0x58
   e0ce0:	4628      	mov	r0, r5
   e0ce2:	47a0      	blx	r4
   e0ce4:	e026      	b.n	e0d34 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb4>

  switch (input->type) {
   e0ce6:	f81e 0006 	ldrb.w	r0, [lr, r6]
   e0cea:	1e44      	subs	r4, r0, #1
   e0cec:	2c08      	cmp	r4, #8
   e0cee:	d81a      	bhi.n	e0d26 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xa6>
   e0cf0:	e8df f004 	tbb	[pc, r4]
   e0cf4:	19091505 	.word	0x19091505
   e0cf8:	19111919 	.word	0x19111919
   e0cfc:	0d          	.byte	0x0d
   e0cfd:	00          	.byte	0x00
    case kTfLiteFloat32: {
      return SplitImpl<float>(context, node, input, axis_value);
   e0cfe:	4628      	mov	r0, r5
   e0d00:	f7ff fce8 	bl	e06d4 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>
   e0d04:	e017      	b.n	e0d36 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb6>
    }
    case kTfLiteUInt8: {
      return SplitImpl<uint8_t>(context, node, input, axis_value);
   e0d06:	4628      	mov	r0, r5
   e0d08:	f7ff fd77 	bl	e07fa <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>
   e0d0c:	e013      	b.n	e0d36 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb6>
    }
    case kTfLiteInt8: {
      return SplitImpl<int8_t>(context, node, input, axis_value);
   e0d0e:	4628      	mov	r0, r5
   e0d10:	f7ff fe02 	bl	e0918 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>
   e0d14:	e00f      	b.n	e0d36 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb6>
    }
    case kTfLiteInt16: {
      return SplitImpl<int16_t>(context, node, input, axis_value);
   e0d16:	4628      	mov	r0, r5
   e0d18:	f7ff fe8d 	bl	e0a36 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>
   e0d1c:	e00b      	b.n	e0d36 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb6>
    }
    case kTfLiteInt32: {
      return SplitImpl<int32_t>(context, node, input, axis_value);
   e0d1e:	4628      	mov	r0, r5
   e0d20:	f7ff ff1b 	bl	e0b5a <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>
   e0d24:	e007      	b.n	e0d36 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb6>
    }
    default:
      context->ReportError(context, "Type %s currently not supported.",
   e0d26:	696c      	ldr	r4, [r5, #20]
   e0d28:	f7f3 f9f6 	bl	d4118 <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type));
   e0d2c:	4908      	ldr	r1, [pc, #32]	; (e0d50 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xd0>)
   e0d2e:	4602      	mov	r2, r0
   e0d30:	4628      	mov	r0, r5
   e0d32:	47a0      	blx	r4
      return kTfLiteError;
   e0d34:	2001      	movs	r0, #1
  }
#undef TF_LITE_SPLIT

  return kTfLiteOk;
}
   e0d36:	b002      	add	sp, #8
   e0d38:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   e0d3c:	000ea56a 	.word	0x000ea56a
   e0d40:	000ea6df 	.word	0x000ea6df
   e0d44:	000ea638 	.word	0x000ea638
   e0d48:	000e9ac8 	.word	0x000e9ac8
   e0d4c:	000ea6ef 	.word	0x000ea6ef
   e0d50:	000ea711 	.word	0x000ea711

000e0d54 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi>:
}

inline void StridedSlicePadIndices(tflite::StridedSliceParams* p,
                                   int dim_count) {
  // Add indices and mask bits to fully include extra dimensions
  TFLITE_CHECK_LE(dim_count, 4);
   e0d54:	2904      	cmp	r1, #4
  if (v < lo) return lo;
  return v;
}

inline void StridedSlicePadIndices(tflite::StridedSliceParams* p,
                                   int dim_count) {
   e0d56:	b570      	push	{r4, r5, r6, lr}
  // Add indices and mask bits to fully include extra dimensions
  TFLITE_CHECK_LE(dim_count, 4);
   e0d58:	dd01      	ble.n	e0d5e <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0xa>
   e0d5a:	f003 fae7 	bl	e432c <abort>
  TFLITE_CHECK_GE(dim_count, p->start_indices_count);
   e0d5e:	f990 3000 	ldrsb.w	r3, [r0]
   e0d62:	428b      	cmp	r3, r1
   e0d64:	dcf9      	bgt.n	e0d5a <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x6>
  TFLITE_CHECK_EQ(p->start_indices_count, p->stop_indices_count);
   e0d66:	f990 200a 	ldrsb.w	r2, [r0, #10]
   e0d6a:	429a      	cmp	r2, r3
   e0d6c:	d1f5      	bne.n	e0d5a <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x6>
  TFLITE_CHECK_EQ(p->stop_indices_count, p->strides_count);
   e0d6e:	f990 3014 	ldrsb.w	r3, [r0, #20]
   e0d72:	4293      	cmp	r3, r2
   e0d74:	d1f1      	bne.n	e0d5a <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x6>

  const int pad_count = dim_count - p->start_indices_count;

  // Pad indices at start, so move arrays by pad_count.
  for (int i = p->start_indices_count - 1; i >= 0; --i) {
   e0d76:	1e5d      	subs	r5, r3, #1
   e0d78:	eb00 0443 	add.w	r4, r0, r3, lsl #1
   e0d7c:	eb00 0241 	add.w	r2, r0, r1, lsl #1
   e0d80:	1acb      	subs	r3, r1, r3
   e0d82:	2d00      	cmp	r5, #0
   e0d84:	db0c      	blt.n	e0da0 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x4c>
    p->strides[i + pad_count] = p->strides[i];
   e0d86:	f9b4 6014 	ldrsh.w	r6, [r4, #20]
   e0d8a:	8296      	strh	r6, [r2, #20]
    p->start_indices[i + pad_count] = p->start_indices[i];
   e0d8c:	f9b4 6000 	ldrsh.w	r6, [r4]
   e0d90:	8016      	strh	r6, [r2, #0]
    p->stop_indices[i + pad_count] = p->stop_indices[i];
   e0d92:	f9b4 600a 	ldrsh.w	r6, [r4, #10]
   e0d96:	8156      	strh	r6, [r2, #10]
  TFLITE_CHECK_EQ(p->stop_indices_count, p->strides_count);

  const int pad_count = dim_count - p->start_indices_count;

  // Pad indices at start, so move arrays by pad_count.
  for (int i = p->start_indices_count - 1; i >= 0; --i) {
   e0d98:	3d01      	subs	r5, #1
   e0d9a:	3c02      	subs	r4, #2
   e0d9c:	3a02      	subs	r2, #2
   e0d9e:	e7f0      	b.n	e0d82 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x2e>
   e0da0:	2400      	movs	r4, #0
   e0da2:	4602      	mov	r2, r0
    p->strides[i + pad_count] = p->strides[i];
    p->start_indices[i + pad_count] = p->start_indices[i];
    p->stop_indices[i + pad_count] = p->stop_indices[i];
  }
  for (int i = 0; i < pad_count; ++i) {
    p->start_indices[i] = 0;
   e0da4:	4626      	mov	r6, r4
    p->stop_indices[i] = 1;
   e0da6:	2501      	movs	r5, #1
  for (int i = p->start_indices_count - 1; i >= 0; --i) {
    p->strides[i + pad_count] = p->strides[i];
    p->start_indices[i + pad_count] = p->start_indices[i];
    p->stop_indices[i + pad_count] = p->stop_indices[i];
  }
  for (int i = 0; i < pad_count; ++i) {
   e0da8:	429c      	cmp	r4, r3
   e0daa:	f102 0202 	add.w	r2, r2, #2
   e0dae:	da04      	bge.n	e0dba <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x66>
    p->start_indices[i] = 0;
   e0db0:	8016      	strh	r6, [r2, #0]
    p->stop_indices[i] = 1;
   e0db2:	8155      	strh	r5, [r2, #10]
    p->strides[i] = 1;
   e0db4:	8295      	strh	r5, [r2, #20]
  for (int i = p->start_indices_count - 1; i >= 0; --i) {
    p->strides[i + pad_count] = p->strides[i];
    p->start_indices[i + pad_count] = p->start_indices[i];
    p->stop_indices[i + pad_count] = p->stop_indices[i];
  }
  for (int i = 0; i < pad_count; ++i) {
   e0db6:	3401      	adds	r4, #1
   e0db8:	e7f6      	b.n	e0da8 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x54>
    p->stop_indices[i] = 1;
    p->strides[i] = 1;
  }

  // Pad masks with 0s or 1s as required.
  p->shrink_axis_mask <<= pad_count;
   e0dba:	f9b0 2026 	ldrsh.w	r2, [r0, #38]	; 0x26
   e0dbe:	409a      	lsls	r2, r3
   e0dc0:	84c2      	strh	r2, [r0, #38]	; 0x26
  p->ellipsis_mask <<= pad_count;
   e0dc2:	f9b0 2020 	ldrsh.w	r2, [r0, #32]
   e0dc6:	409a      	lsls	r2, r3
   e0dc8:	8402      	strh	r2, [r0, #32]
  p->new_axis_mask <<= pad_count;
   e0dca:	f9b0 2024 	ldrsh.w	r2, [r0, #36]	; 0x24
  p->begin_mask <<= pad_count;
  p->end_mask <<= pad_count;
  p->begin_mask |= (1 << pad_count) - 1;
   e0dce:	2401      	movs	r4, #1
  }

  // Pad masks with 0s or 1s as required.
  p->shrink_axis_mask <<= pad_count;
  p->ellipsis_mask <<= pad_count;
  p->new_axis_mask <<= pad_count;
   e0dd0:	409a      	lsls	r2, r3
  p->begin_mask <<= pad_count;
  p->end_mask <<= pad_count;
  p->begin_mask |= (1 << pad_count) - 1;
   e0dd2:	409c      	lsls	r4, r3
  }

  // Pad masks with 0s or 1s as required.
  p->shrink_axis_mask <<= pad_count;
  p->ellipsis_mask <<= pad_count;
  p->new_axis_mask <<= pad_count;
   e0dd4:	8482      	strh	r2, [r0, #36]	; 0x24
  p->begin_mask <<= pad_count;
  p->end_mask <<= pad_count;
   e0dd6:	f9b0 2022 	ldrsh.w	r2, [r0, #34]	; 0x22
  p->begin_mask |= (1 << pad_count) - 1;
   e0dda:	3c01      	subs	r4, #1
  // Pad masks with 0s or 1s as required.
  p->shrink_axis_mask <<= pad_count;
  p->ellipsis_mask <<= pad_count;
  p->new_axis_mask <<= pad_count;
  p->begin_mask <<= pad_count;
  p->end_mask <<= pad_count;
   e0ddc:	fa02 f503 	lsl.w	r5, r2, r3
  p->begin_mask |= (1 << pad_count) - 1;
   e0de0:	b222      	sxth	r2, r4
   e0de2:	f9b0 401e 	ldrsh.w	r4, [r0, #30]
   e0de6:	fa04 f303 	lsl.w	r3, r4, r3
  p->end_mask |= (1 << pad_count) - 1;

  p->start_indices_count = dim_count;
   e0dea:	b249      	sxtb	r1, r1
  p->shrink_axis_mask <<= pad_count;
  p->ellipsis_mask <<= pad_count;
  p->new_axis_mask <<= pad_count;
  p->begin_mask <<= pad_count;
  p->end_mask <<= pad_count;
  p->begin_mask |= (1 << pad_count) - 1;
   e0dec:	4313      	orrs	r3, r2
  p->end_mask |= (1 << pad_count) - 1;
   e0dee:	432a      	orrs	r2, r5
  p->shrink_axis_mask <<= pad_count;
  p->ellipsis_mask <<= pad_count;
  p->new_axis_mask <<= pad_count;
  p->begin_mask <<= pad_count;
  p->end_mask <<= pad_count;
  p->begin_mask |= (1 << pad_count) - 1;
   e0df0:	83c3      	strh	r3, [r0, #30]
  p->end_mask |= (1 << pad_count) - 1;
   e0df2:	8442      	strh	r2, [r0, #34]	; 0x22

  p->start_indices_count = dim_count;
   e0df4:	7001      	strb	r1, [r0, #0]
  p->stop_indices_count = dim_count;
   e0df6:	7281      	strb	r1, [r0, #10]
  p->strides_count = dim_count;
   e0df8:	7501      	strb	r1, [r0, #20]
   e0dfa:	bd70      	pop	{r4, r5, r6, pc}

000e0dfc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>:

// Return the index for the first element along that axis. This index will be a
// positive integer between [0, axis_size - 1] that can be used to index
// directly into the data.
inline int StartForAxis(const tflite::StridedSliceParams& params,
                        const RuntimeShape& input_shape, int axis) {
   e0dfc:	b510      	push	{r4, lr}
   e0dfe:	4603      	mov	r3, r0
   e0e00:	4608      	mov	r0, r1
  const auto begin_mask = params.begin_mask;
  const auto* start_indices = params.start_indices;
  const auto* strides = params.strides;
  // Begin with the specified index.
  int start = start_indices[axis];
   e0e02:	eb03 0142 	add.w	r1, r3, r2, lsl #1

  // begin_mask override
  if (begin_mask & 1 << axis) {
   e0e06:	f9b3 301e 	ldrsh.w	r3, [r3, #30]
                        const RuntimeShape& input_shape, int axis) {
  const auto begin_mask = params.begin_mask;
  const auto* start_indices = params.start_indices;
  const auto* strides = params.strides;
  // Begin with the specified index.
  int start = start_indices[axis];
   e0e0a:	f9b1 4002 	ldrsh.w	r4, [r1, #2]

  // begin_mask override
  if (begin_mask & 1 << axis) {
   e0e0e:	4113      	asrs	r3, r2
   e0e10:	07db      	lsls	r3, r3, #31
   e0e12:	d507      	bpl.n	e0e24 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi+0x28>
    if (strides[axis] > 0) {
   e0e14:	f9b1 3016 	ldrsh.w	r3, [r1, #22]
   e0e18:	2b00      	cmp	r3, #0
      // clamped below (Note: We could have set them to 0 and axis_size-1, but
      // use lowest() and max() to maintain symmetry with StopForAxis())
      start = std::numeric_limits<int>::lowest();
    } else {
      // Backward iteration - use the last element.
      start = std::numeric_limits<int>::max();
   e0e1a:	bfcc      	ite	gt
   e0e1c:	f04f 4400 	movgt.w	r4, #2147483648	; 0x80000000
   e0e20:	f06f 4400 	mvnle.w	r4, #2147483648	; 0x80000000
    }
  }

  // Handle negative indices
  int axis_size = input_shape.Dims(axis);
   e0e24:	4611      	mov	r1, r2
   e0e26:	f7f5 fb19 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  if (start < 0) {
   e0e2a:	2c00      	cmp	r4, #0
    start += axis_size;
   e0e2c:	bfb8      	it	lt
   e0e2e:	1824      	addlt	r4, r4, r0
namespace tflite {
namespace strided_slice {

// Use until std::clamp() is available from C++17.
inline int Clamp(const int v, const int lo, const int hi) {
  TFLITE_DCHECK(!(hi < lo));
   e0e30:	3801      	subs	r0, #1
   e0e32:	d501      	bpl.n	e0e38 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi+0x3c>
   e0e34:	f003 fa7a 	bl	e432c <abort>
  if (hi < v) return hi;
   e0e38:	4284      	cmp	r4, r0
   e0e3a:	bfd8      	it	le
   e0e3c:	ea24 70e4 	bicle.w	r0, r4, r4, asr #31

  // Clamping
  start = Clamp(start, 0, axis_size - 1);

  return start;
}
   e0e40:	bd10      	pop	{r4, pc}

000e0e42 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>:
// element. ie. So if you were iterating through all elements of a 1D array of
// size 4, this function would return 4 as the stop, because it is one past the
// "real" indices of 0, 1, 2 & 3.
inline int StopForAxis(const tflite::StridedSliceParams& params,
                       const RuntimeShape& input_shape, int axis,
                       int start_for_axis) {
   e0e42:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
   e0e44:	4615      	mov	r5, r2
   e0e46:	460f      	mov	r7, r1
  const auto* stop_indices = params.stop_indices;
  const auto* strides = params.strides;

  // Begin with the specified index
  const bool shrink_axis = shrink_axis_mask & (1 << axis);
  int stop = stop_indices[axis];
   e0e48:	eb00 0145 	add.w	r1, r0, r5, lsl #1
// size 4, this function would return 4 as the stop, because it is one past the
// "real" indices of 0, 1, 2 & 3.
inline int StopForAxis(const tflite::StridedSliceParams& params,
                       const RuntimeShape& input_shape, int axis,
                       int start_for_axis) {
  const auto end_mask = params.end_mask;
   e0e4c:	f9b0 2022 	ldrsh.w	r2, [r0, #34]	; 0x22
  const auto* stop_indices = params.stop_indices;
  const auto* strides = params.strides;

  // Begin with the specified index
  const bool shrink_axis = shrink_axis_mask & (1 << axis);
  int stop = stop_indices[axis];
   e0e50:	f9b1 400c 	ldrsh.w	r4, [r1, #12]

  // When shrinking an axis, the end position does not matter (and can be
  // incorrect when negative indexing is used, see Issue #19260). Always use
  // start_for_axis + 1 to generate a length 1 slice, since start_for_axis has
  // already been adjusted for negative indices.
  if (shrink_axis) {
   e0e54:	f9b0 1026 	ldrsh.w	r1, [r0, #38]	; 0x26
   e0e58:	4129      	asrs	r1, r5
   e0e5a:	07c9      	lsls	r1, r1, #31
    stop = start_for_axis + 1;
   e0e5c:	bf48      	it	mi
   e0e5e:	1c5c      	addmi	r4, r3, #1
  }

  // end_mask override
  if (end_mask & (1 << axis)) {
   e0e60:	fa42 f305 	asr.w	r3, r2, r5
   e0e64:	07da      	lsls	r2, r3, #31
                       const RuntimeShape& input_shape, int axis,
                       int start_for_axis) {
  const auto end_mask = params.end_mask;
  const auto shrink_axis_mask = params.shrink_axis_mask;
  const auto* stop_indices = params.stop_indices;
  const auto* strides = params.strides;
   e0e66:	f100 0616 	add.w	r6, r0, #22
  if (shrink_axis) {
    stop = start_for_axis + 1;
  }

  // end_mask override
  if (end_mask & (1 << axis)) {
   e0e6a:	d507      	bpl.n	e0e7c <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii+0x3a>
    if (strides[axis] > 0) {
   e0e6c:	f936 3015 	ldrsh.w	r3, [r6, r5, lsl #1]
      // Forward iteration - use the last element. These values will get
      // clamped below
      stop = std::numeric_limits<int>::max();
    } else {
      // Backward iteration - use the first element.
      stop = std::numeric_limits<int>::lowest();
   e0e70:	2b00      	cmp	r3, #0
   e0e72:	bfcc      	ite	gt
   e0e74:	f06f 4400 	mvngt.w	r4, #2147483648	; 0x80000000
   e0e78:	f04f 4400 	movle.w	r4, #2147483648	; 0x80000000
    }
  }

  // Handle negative indices
  const int axis_size = input_shape.Dims(axis);
   e0e7c:	4629      	mov	r1, r5
   e0e7e:	4638      	mov	r0, r7
   e0e80:	f7f5 faec 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  }

  // Clamping
  // Because the end index points one past the last element, we need slightly
  // different clamping ranges depending on the direction.
  if (strides[axis] > 0) {
   e0e84:	f936 3015 	ldrsh.w	r3, [r6, r5, lsl #1]
    }
  }

  // Handle negative indices
  const int axis_size = input_shape.Dims(axis);
  if (stop < 0) {
   e0e88:	2c00      	cmp	r4, #0
    stop += axis_size;
   e0e8a:	bfb8      	it	lt
   e0e8c:	1824      	addlt	r4, r4, r0
  }

  // Clamping
  // Because the end index points one past the last element, we need slightly
  // different clamping ranges depending on the direction.
  if (strides[axis] > 0) {
   e0e8e:	2b00      	cmp	r3, #0
   e0e90:	dd08      	ble.n	e0ea4 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii+0x62>
namespace tflite {
namespace strided_slice {

// Use until std::clamp() is available from C++17.
inline int Clamp(const int v, const int lo, const int hi) {
  TFLITE_DCHECK(!(hi < lo));
   e0e92:	2800      	cmp	r0, #0
   e0e94:	da01      	bge.n	e0e9a <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii+0x58>
   e0e96:	f003 fa49 	bl	e432c <abort>
  if (hi < v) return hi;
   e0e9a:	4284      	cmp	r4, r0
   e0e9c:	dc09      	bgt.n	e0eb2 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii+0x70>
   e0e9e:	ea24 70e4 	bic.w	r0, r4, r4, asr #31
   e0ea2:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
  if (strides[axis] > 0) {
    // Forward iteration
    stop = Clamp(stop, 0, axis_size);
  } else {
    // Backward iteration
    stop = Clamp(stop, -1, axis_size - 1);
   e0ea4:	3801      	subs	r0, #1
namespace tflite {
namespace strided_slice {

// Use until std::clamp() is available from C++17.
inline int Clamp(const int v, const int lo, const int hi) {
  TFLITE_DCHECK(!(hi < lo));
   e0ea6:	1c43      	adds	r3, r0, #1
   e0ea8:	dbf5      	blt.n	e0e96 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii+0x54>
  if (hi < v) return hi;
   e0eaa:	4284      	cmp	r4, r0
   e0eac:	bfd8      	it	le
   e0eae:	ea44 70e4 	orrle.w	r0, r4, r4, asr #31
    // Backward iteration
    stop = Clamp(stop, -1, axis_size - 1);
  }

  return stop;
}
   e0eb2:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}

000e0eb4 <_ZN6tflite3ops5micro13strided_slice19StridedSliceContextC1EP13TfLiteContextP10TfLiteNode>:
constexpr int kEndTensor = 2;
constexpr int kStridesTensor = 3;
constexpr int kOutputTensor = 0;

struct StridedSliceContext {
  StridedSliceContext(TfLiteContext* context, TfLiteNode* node) {
   e0eb4:	b5f0      	push	{r4, r5, r6, r7, lr}
    params = reinterpret_cast<TfLiteStridedSliceParams*>(node->builtin_data);
   e0eb6:	6954      	ldr	r4, [r2, #20]
   e0eb8:	6004      	str	r4, [r0, #0]
   e0eba:	6814      	ldr	r4, [r2, #0]
   e0ebc:	688d      	ldr	r5, [r1, #8]
   e0ebe:	6866      	ldr	r6, [r4, #4]
   e0ec0:	2438      	movs	r4, #56	; 0x38
   e0ec2:	fb04 5506 	mla	r5, r4, r6, r5
    input = GetInput(context, node, kInputTensor);
   e0ec6:	6045      	str	r5, [r0, #4]
   e0ec8:	6816      	ldr	r6, [r2, #0]
    begin = GetInput(context, node, kBeginTensor);
   e0eca:	688f      	ldr	r7, [r1, #8]
   e0ecc:	68b6      	ldr	r6, [r6, #8]
   e0ece:	fb04 7606 	mla	r6, r4, r6, r7
   e0ed2:	6086      	str	r6, [r0, #8]
   e0ed4:	6816      	ldr	r6, [r2, #0]
    end = GetInput(context, node, kEndTensor);
   e0ed6:	688f      	ldr	r7, [r1, #8]
   e0ed8:	68f6      	ldr	r6, [r6, #12]
   e0eda:	fb04 7606 	mla	r6, r4, r6, r7
   e0ede:	60c6      	str	r6, [r0, #12]
   e0ee0:	6816      	ldr	r6, [r2, #0]
    strides = GetInput(context, node, kStridesTensor);
   e0ee2:	688f      	ldr	r7, [r1, #8]
   e0ee4:	6936      	ldr	r6, [r6, #16]
   e0ee6:	fb04 7606 	mla	r6, r4, r6, r7
   e0eea:	6106      	str	r6, [r0, #16]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   e0eec:	6852      	ldr	r2, [r2, #4]
    output = GetOutput(context, node, kOutputTensor);
   e0eee:	6889      	ldr	r1, [r1, #8]
   e0ef0:	6852      	ldr	r2, [r2, #4]
   e0ef2:	fb04 1402 	mla	r4, r4, r2, r1
   e0ef6:	6144      	str	r4, [r0, #20]
#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/c_api_internal.h"

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
   e0ef8:	68aa      	ldr	r2, [r5, #8]
   e0efa:	6812      	ldr	r2, [r2, #0]
    dims = NumDimensions(input);
   e0efc:	6182      	str	r2, [r0, #24]
  }
   e0efe:	bdf0      	pop	{r4, r5, r6, r7, pc}

000e0f00 <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE>:
// This Op only supports 1-4D cases and since we use the reference 4D
// implementation, the 1-3D tensors are mapped to 4D.
const int kMaxDim = 4;

tflite::StridedSliceParams BuildStridedSliceParams(
    StridedSliceContext* op_context) {
   e0f00:	b570      	push	{r4, r5, r6, lr}
  tflite::StridedSliceParams op_params;
  op_params.start_indices_count = op_context->dims;
   e0f02:	698e      	ldr	r6, [r1, #24]
  op_params.stop_indices_count = op_context->dims;
  op_params.strides_count = op_context->dims;
   e0f04:	4603      	mov	r3, r0
const int kMaxDim = 4;

tflite::StridedSliceParams BuildStridedSliceParams(
    StridedSliceContext* op_context) {
  tflite::StridedSliceParams op_params;
  op_params.start_indices_count = op_context->dims;
   e0f06:	b272      	sxtb	r2, r6
  op_params.stop_indices_count = op_context->dims;
  op_params.strides_count = op_context->dims;
   e0f08:	f803 2f14 	strb.w	r2, [r3, #20]!
const int kMaxDim = 4;

tflite::StridedSliceParams BuildStridedSliceParams(
    StridedSliceContext* op_context) {
  tflite::StridedSliceParams op_params;
  op_params.start_indices_count = op_context->dims;
   e0f0c:	7002      	strb	r2, [r0, #0]
  op_params.stop_indices_count = op_context->dims;
   e0f0e:	7282      	strb	r2, [r0, #10]
  op_params.strides_count = op_context->dims;

  for (int i = 0; i < op_context->dims; ++i) {
   e0f10:	2400      	movs	r4, #0
   e0f12:	42b4      	cmp	r4, r6
   e0f14:	da15      	bge.n	e0f42 <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE+0x42>
    op_params.start_indices[i] = GetTensorData<int32_t>(op_context->begin)[i];
   e0f16:	688a      	ldr	r2, [r1, #8]
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e0f18:	b102      	cbz	r2, e0f1c <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE+0x1c>
   e0f1a:	6852      	ldr	r2, [r2, #4]
   e0f1c:	f852 2024 	ldr.w	r2, [r2, r4, lsl #2]
   e0f20:	f823 2c12 	strh.w	r2, [r3, #-18]
    op_params.stop_indices[i] = GetTensorData<int32_t>(op_context->end)[i];
   e0f24:	68ca      	ldr	r2, [r1, #12]
   e0f26:	00a5      	lsls	r5, r4, #2
   e0f28:	b102      	cbz	r2, e0f2c <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE+0x2c>
   e0f2a:	6852      	ldr	r2, [r2, #4]
   e0f2c:	5952      	ldr	r2, [r2, r5]
   e0f2e:	f823 2c08 	strh.w	r2, [r3, #-8]
    op_params.strides[i] = GetTensorData<int32_t>(op_context->strides)[i];
   e0f32:	690a      	ldr	r2, [r1, #16]
   e0f34:	b102      	cbz	r2, e0f38 <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE+0x38>
   e0f36:	6852      	ldr	r2, [r2, #4]
   e0f38:	5952      	ldr	r2, [r2, r5]
   e0f3a:	f823 2f02 	strh.w	r2, [r3, #2]!
  tflite::StridedSliceParams op_params;
  op_params.start_indices_count = op_context->dims;
  op_params.stop_indices_count = op_context->dims;
  op_params.strides_count = op_context->dims;

  for (int i = 0; i < op_context->dims; ++i) {
   e0f3e:	3401      	adds	r4, #1
   e0f40:	e7e7      	b.n	e0f12 <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE+0x12>
    op_params.start_indices[i] = GetTensorData<int32_t>(op_context->begin)[i];
    op_params.stop_indices[i] = GetTensorData<int32_t>(op_context->end)[i];
    op_params.strides[i] = GetTensorData<int32_t>(op_context->strides)[i];
  }

  op_params.begin_mask = op_context->params->begin_mask;
   e0f42:	680b      	ldr	r3, [r1, #0]
   e0f44:	681a      	ldr	r2, [r3, #0]
  op_params.ellipsis_mask = 0;
  op_params.end_mask = op_context->params->end_mask;
   e0f46:	6859      	ldr	r1, [r3, #4]
    op_params.start_indices[i] = GetTensorData<int32_t>(op_context->begin)[i];
    op_params.stop_indices[i] = GetTensorData<int32_t>(op_context->end)[i];
    op_params.strides[i] = GetTensorData<int32_t>(op_context->strides)[i];
  }

  op_params.begin_mask = op_context->params->begin_mask;
   e0f48:	83c2      	strh	r2, [r0, #30]
  op_params.ellipsis_mask = 0;
  op_params.end_mask = op_context->params->end_mask;
  op_params.new_axis_mask = 0;
  op_params.shrink_axis_mask = op_context->params->shrink_axis_mask;
   e0f4a:	691b      	ldr	r3, [r3, #16]
    op_params.strides[i] = GetTensorData<int32_t>(op_context->strides)[i];
  }

  op_params.begin_mask = op_context->params->begin_mask;
  op_params.ellipsis_mask = 0;
  op_params.end_mask = op_context->params->end_mask;
   e0f4c:	8441      	strh	r1, [r0, #34]	; 0x22
    op_params.stop_indices[i] = GetTensorData<int32_t>(op_context->end)[i];
    op_params.strides[i] = GetTensorData<int32_t>(op_context->strides)[i];
  }

  op_params.begin_mask = op_context->params->begin_mask;
  op_params.ellipsis_mask = 0;
   e0f4e:	2200      	movs	r2, #0
   e0f50:	8402      	strh	r2, [r0, #32]
  op_params.end_mask = op_context->params->end_mask;
  op_params.new_axis_mask = 0;
   e0f52:	8482      	strh	r2, [r0, #36]	; 0x24
  op_params.shrink_axis_mask = op_context->params->shrink_axis_mask;
   e0f54:	84c3      	strh	r3, [r0, #38]	; 0x26
  return op_params;
}
   e0f56:	bd70      	pop	{r4, r5, r6, pc}

000e0f58 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE>:

// Processes the indexing tensors (begin, end and strides) to resize the
// output tensor. This function is callable from both Prepare() and Eval() as
// long as the caller ensures the indexing tensors are present.
TfLiteStatus CheckOutputSize(TfLiteContext* context,
                             StridedSliceContext* op_context) {
   e0f58:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e0f5c:	ed2d 8b02 	vpush	{d8}
   e0f60:	460f      	mov	r7, r1
   e0f62:	b095      	sub	sp, #84	; 0x54
  using ::tflite::strided_slice::StartForAxis;
  using ::tflite::strided_slice::StopForAxis;
  TfLiteIntArray* output_shape = op_context->output->dims;
   e0f64:	694b      	ldr	r3, [r1, #20]

// Processes the indexing tensors (begin, end and strides) to resize the
// output tensor. This function is callable from both Prepare() and Eval() as
// long as the caller ensures the indexing tensors are present.
TfLiteStatus CheckOutputSize(TfLiteContext* context,
                             StridedSliceContext* op_context) {
   e0f66:	4605      	mov	r5, r0
  using ::tflite::strided_slice::StartForAxis;
  using ::tflite::strided_slice::StopForAxis;
  TfLiteIntArray* output_shape = op_context->output->dims;
  int shape_size = 0;
  auto op_params = BuildStridedSliceParams(op_context);
   e0f68:	a80a      	add	r0, sp, #40	; 0x28
// long as the caller ensures the indexing tensors are present.
TfLiteStatus CheckOutputSize(TfLiteContext* context,
                             StridedSliceContext* op_context) {
  using ::tflite::strided_slice::StartForAxis;
  using ::tflite::strided_slice::StopForAxis;
  TfLiteIntArray* output_shape = op_context->output->dims;
   e0f6a:	f8d3 8008 	ldr.w	r8, [r3, #8]
  int shape_size = 0;
  auto op_params = BuildStridedSliceParams(op_context);
   e0f6e:	f7ff ffc7 	bl	e0f00 <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE>
  auto input_shape = GetTensorShape(op_context->input);
   e0f72:	6879      	ldr	r1, [r7, #4]
   e0f74:	a805      	add	r0, sp, #20
   e0f76:	f7f5 fd16 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
TfLiteStatus CheckOutputSize(TfLiteContext* context,
                             StridedSliceContext* op_context) {
  using ::tflite::strided_slice::StartForAxis;
  using ::tflite::strided_slice::StopForAxis;
  TfLiteIntArray* output_shape = op_context->output->dims;
  int shape_size = 0;
   e0f7a:	2600      	movs	r6, #0
  auto op_params = BuildStridedSliceParams(op_context);
  auto input_shape = GetTensorShape(op_context->input);
  for (int idx = 0; idx < op_context->dims; ++idx) {
   e0f7c:	f8d7 9018 	ldr.w	r9, [r7, #24]
   e0f80:	4634      	mov	r4, r6
   e0f82:	454c      	cmp	r4, r9
   e0f84:	da4b      	bge.n	e101e <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xc6>
    int32_t stride = GetTensorData<int32_t>(op_context->strides)[idx];
   e0f86:	693b      	ldr	r3, [r7, #16]
   e0f88:	b103      	cbz	r3, e0f8c <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0x34>
   e0f8a:	685b      	ldr	r3, [r3, #4]
   e0f8c:	f853 3024 	ldr.w	r3, [r3, r4, lsl #2]
   e0f90:	ee08 3a10 	vmov	s16, r3
    TF_LITE_ENSURE_MSG(context, stride != 0, "stride value has to be non-zero");
   e0f94:	b923      	cbnz	r3, e0fa0 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0x48>
   e0f96:	696b      	ldr	r3, [r5, #20]
   e0f98:	492c      	ldr	r1, [pc, #176]	; (e104c <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xf4>)
   e0f9a:	4628      	mov	r0, r5
   e0f9c:	4798      	blx	r3
   e0f9e:	e039      	b.n	e1014 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xbc>
    int32_t begin = StartForAxis(op_params, input_shape, idx);
   e0fa0:	4622      	mov	r2, r4
   e0fa2:	a905      	add	r1, sp, #20
   e0fa4:	a80a      	add	r0, sp, #40	; 0x28
   e0fa6:	f7ff ff29 	bl	e0dfc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
    int32_t end = StopForAxis(op_params, input_shape, idx, begin);
   e0faa:	4622      	mov	r2, r4
  auto op_params = BuildStridedSliceParams(op_context);
  auto input_shape = GetTensorShape(op_context->input);
  for (int idx = 0; idx < op_context->dims; ++idx) {
    int32_t stride = GetTensorData<int32_t>(op_context->strides)[idx];
    TF_LITE_ENSURE_MSG(context, stride != 0, "stride value has to be non-zero");
    int32_t begin = StartForAxis(op_params, input_shape, idx);
   e0fac:	4682      	mov	sl, r0
    int32_t end = StopForAxis(op_params, input_shape, idx, begin);
   e0fae:	4603      	mov	r3, r0
   e0fb0:	a905      	add	r1, sp, #20
   e0fb2:	a80a      	add	r0, sp, #40	; 0x28
   e0fb4:	f7ff ff45 	bl	e0e42 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>

    // When shrinking an axis, the end position does not matter (and can be
    // incorrect when negative indexing is used, see Issue #19260). Always use
    // begin + 1 to generate a length 1 slice, since begin has
    // already been adjusted for negative indices by StartForAxis.
    const bool shrink_axis = op_context->params->shrink_axis_mask & (1 << idx);
   e0fb8:	683b      	ldr	r3, [r7, #0]
   e0fba:	691a      	ldr	r2, [r3, #16]
   e0fbc:	4122      	asrs	r2, r4
    if (shrink_axis) {
   e0fbe:	f012 0b01 	ands.w	fp, r2, #1
      end = begin + 1;
   e0fc2:	bf18      	it	ne
   e0fc4:	f10a 0001 	addne.w	r0, sl, #1
  using ::ceil;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  ceil(float __x)
  { return __builtin_ceilf(__x); }
   e0fc8:	ebca 0000 	rsb	r0, sl, r0
   e0fcc:	ee07 0a90 	vmov	s15, r0
   e0fd0:	eeb8 0ac8 	vcvt.f32.s32	s0, s16
   e0fd4:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   e0fd8:	ee87 0a80 	vdiv.f32	s0, s15, s0
   e0fdc:	f004 f992 	bl	e5304 <ceilf>
    }

    // This is valid for both positive and negative strides
    int32_t dim_shape = std::ceil((end - begin) / static_cast<float>(stride));
    dim_shape = dim_shape < 0 ? 0 : dim_shape;
    if (!shrink_axis) {
   e0fe0:	f1bb 0f00 	cmp.w	fp, #0
   e0fe4:	d119      	bne.n	e101a <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xc2>
      TF_LITE_ENSURE_EQ(context, output_shape->data[shape_size], dim_shape);
   e0fe6:	eb08 0286 	add.w	r2, r8, r6, lsl #2
   e0fea:	6852      	ldr	r2, [r2, #4]
    if (shrink_axis) {
      end = begin + 1;
    }

    // This is valid for both positive and negative strides
    int32_t dim_shape = std::ceil((end - begin) / static_cast<float>(stride));
   e0fec:	eebd 0ac0 	vcvt.s32.f32	s0, s0
    dim_shape = dim_shape < 0 ? 0 : dim_shape;
   e0ff0:	ee10 3a10 	vmov	r3, s0
   e0ff4:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
    if (!shrink_axis) {
      TF_LITE_ENSURE_EQ(context, output_shape->data[shape_size], dim_shape);
   e0ff8:	4293      	cmp	r3, r2
   e0ffa:	d00d      	beq.n	e1018 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xc0>
   e0ffc:	9303      	str	r3, [sp, #12]
   e0ffe:	4b14      	ldr	r3, [pc, #80]	; (e1050 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xf8>)
   e1000:	9301      	str	r3, [sp, #4]
   e1002:	696c      	ldr	r4, [r5, #20]
   e1004:	4b13      	ldr	r3, [pc, #76]	; (e1054 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xfc>)
   e1006:	9300      	str	r3, [sp, #0]
   e1008:	9202      	str	r2, [sp, #8]
   e100a:	2373      	movs	r3, #115	; 0x73
   e100c:	4a12      	ldr	r2, [pc, #72]	; (e1058 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0x100>)
   e100e:	4913      	ldr	r1, [pc, #76]	; (e105c <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0x104>)
   e1010:	4628      	mov	r0, r5
   e1012:	47a0      	blx	r4
   e1014:	2401      	movs	r4, #1
   e1016:	e010      	b.n	e103a <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xe2>
      shape_size++;
   e1018:	3601      	adds	r6, #1
  using ::tflite::strided_slice::StopForAxis;
  TfLiteIntArray* output_shape = op_context->output->dims;
  int shape_size = 0;
  auto op_params = BuildStridedSliceParams(op_context);
  auto input_shape = GetTensorShape(op_context->input);
  for (int idx = 0; idx < op_context->dims; ++idx) {
   e101a:	3401      	adds	r4, #1
   e101c:	e7b1      	b.n	e0f82 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0x2a>
    if (!shrink_axis) {
      TF_LITE_ENSURE_EQ(context, output_shape->data[shape_size], dim_shape);
      shape_size++;
    }
  }
  TF_LITE_ENSURE_EQ(context, output_shape->size, shape_size);
   e101e:	f8d8 3000 	ldr.w	r3, [r8]
   e1022:	42b3      	cmp	r3, r6
   e1024:	d008      	beq.n	e1038 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xe0>
   e1026:	9302      	str	r3, [sp, #8]
   e1028:	4b0d      	ldr	r3, [pc, #52]	; (e1060 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0x108>)
   e102a:	9301      	str	r3, [sp, #4]
   e102c:	4b0d      	ldr	r3, [pc, #52]	; (e1064 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0x10c>)
   e102e:	9300      	str	r3, [sp, #0]
   e1030:	9603      	str	r6, [sp, #12]
   e1032:	696c      	ldr	r4, [r5, #20]
   e1034:	2377      	movs	r3, #119	; 0x77
   e1036:	e7e9      	b.n	e100c <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xb4>
  return kTfLiteOk;
   e1038:	2400      	movs	r4, #0
  using ::tflite::strided_slice::StartForAxis;
  using ::tflite::strided_slice::StopForAxis;
  TfLiteIntArray* output_shape = op_context->output->dims;
  int shape_size = 0;
  auto op_params = BuildStridedSliceParams(op_context);
  auto input_shape = GetTensorShape(op_context->input);
   e103a:	a805      	add	r0, sp, #20
   e103c:	f7f5 fa03 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
      shape_size++;
    }
  }
  TF_LITE_ENSURE_EQ(context, output_shape->size, shape_size);
  return kTfLiteOk;
}
   e1040:	4620      	mov	r0, r4
   e1042:	b015      	add	sp, #84	; 0x54
   e1044:	ecbd 8b02 	vpop	{d8}
   e1048:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e104c:	000ea732 	.word	0x000ea732
   e1050:	000ea8b0 	.word	0x000ea8b0
   e1054:	000ea8ba 	.word	0x000ea8ba
   e1058:	000ea801 	.word	0x000ea801
   e105c:	000e98f8 	.word	0x000e98f8
   e1060:	000ea8d9 	.word	0x000ea8d9
   e1064:	000ea8e4 	.word	0x000ea8e4

000e1068 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   e1068:	b570      	push	{r4, r5, r6, lr}
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
   e106a:	680b      	ldr	r3, [r1, #0]
   e106c:	681b      	ldr	r3, [r3, #0]
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 4);
   e106e:	2b04      	cmp	r3, #4
  }
  TF_LITE_ENSURE_EQ(context, output_shape->size, shape_size);
  return kTfLiteOk;
}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   e1070:	b08c      	sub	sp, #48	; 0x30
   e1072:	4605      	mov	r5, r0
   e1074:	460a      	mov	r2, r1
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 4);
   e1076:	d00d      	beq.n	e1094 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x2c>
   e1078:	9302      	str	r3, [sp, #8]
   e107a:	4b16      	ldr	r3, [pc, #88]	; (e10d4 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x6c>)
   e107c:	9301      	str	r3, [sp, #4]
   e107e:	2204      	movs	r2, #4
   e1080:	4b15      	ldr	r3, [pc, #84]	; (e10d8 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x70>)
   e1082:	9300      	str	r3, [sp, #0]
   e1084:	9203      	str	r2, [sp, #12]
   e1086:	6944      	ldr	r4, [r0, #20]
   e1088:	237c      	movs	r3, #124	; 0x7c
   e108a:	4a14      	ldr	r2, [pc, #80]	; (e10dc <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x74>)
   e108c:	4914      	ldr	r1, [pc, #80]	; (e10e0 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x78>)
   e108e:	47a0      	blx	r4
   e1090:	2001      	movs	r0, #1
   e1092:	e01d      	b.n	e10d0 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x68>
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
   e1094:	684b      	ldr	r3, [r1, #4]
   e1096:	681c      	ldr	r4, [r3, #0]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
   e1098:	2c01      	cmp	r4, #1
   e109a:	d009      	beq.n	e10b0 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x48>
   e109c:	4b11      	ldr	r3, [pc, #68]	; (e10e4 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x7c>)
   e109e:	9301      	str	r3, [sp, #4]
   e10a0:	2601      	movs	r6, #1
   e10a2:	4b11      	ldr	r3, [pc, #68]	; (e10e8 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x80>)
   e10a4:	9300      	str	r3, [sp, #0]
   e10a6:	9603      	str	r6, [sp, #12]
   e10a8:	9402      	str	r4, [sp, #8]
   e10aa:	6944      	ldr	r4, [r0, #20]
   e10ac:	237d      	movs	r3, #125	; 0x7d
   e10ae:	e7ec      	b.n	e108a <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x22>
  StridedSliceContext op_context(context, node);
   e10b0:	4601      	mov	r1, r0
   e10b2:	a805      	add	r0, sp, #20
   e10b4:	f7ff fefe 	bl	e0eb4 <_ZN6tflite3ops5micro13strided_slice19StridedSliceContextC1EP13TfLiteContextP10TfLiteNode>
  TF_LITE_ENSURE_MSG(context, op_context.dims <= kMaxDim,
   e10b8:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e10ba:	2b04      	cmp	r3, #4
   e10bc:	dd04      	ble.n	e10c8 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
   e10be:	696b      	ldr	r3, [r5, #20]
   e10c0:	490a      	ldr	r1, [pc, #40]	; (e10ec <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x84>)
   e10c2:	4628      	mov	r0, r5
   e10c4:	4798      	blx	r3
   e10c6:	e7e3      	b.n	e1090 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x28>
                     "input dim should not exceed 4");
  return CheckOutputSize(context, &op_context);
   e10c8:	a905      	add	r1, sp, #20
   e10ca:	4628      	mov	r0, r5
   e10cc:	f7ff ff44 	bl	e0f58 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE>
}
   e10d0:	b00c      	add	sp, #48	; 0x30
   e10d2:	bd70      	pop	{r4, r5, r6, pc}
   e10d4:	000ea9c2 	.word	0x000ea9c2
   e10d8:	000e9912 	.word	0x000e9912
   e10dc:	000ea801 	.word	0x000ea801
   e10e0:	000e98f8 	.word	0x000e98f8
   e10e4:	000eb2c5 	.word	0x000eb2c5
   e10e8:	000e9922 	.word	0x000e9922
   e10ec:	000ea8f7 	.word	0x000ea8f7

000e10f0 <_ZN6tflite3ops5micro22Register_STRIDED_SLICEEv>:
TfLiteRegistration* Register_STRIDED_SLICE() {
  static TfLiteRegistration r = {
      nullptr, nullptr, strided_slice::Prepare,
      strided_slice::Eval<strided_slice::kReference>};
  return &r;
}
   e10f0:	4800      	ldr	r0, [pc, #0]	; (e10f4 <_ZN6tflite3ops5micro22Register_STRIDED_SLICEEv+0x4>)
   e10f2:	4770      	bx	lr
   e10f4:	2003c1e8 	.word	0x2003c1e8

000e10f8 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_>:

namespace tflite {

namespace reference_ops {
template <typename T>
inline void StridedSlice(const tflite::StridedSliceParams& op_params,
   e10f8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e10fc:	b0a1      	sub	sp, #132	; 0x84
   e10fe:	461e      	mov	r6, r3
   e1100:	460f      	mov	r7, r1
   e1102:	920a      	str	r2, [sp, #40]	; 0x28
                         const RuntimeShape& unextended_input_shape,
                         const T* input_data,
                         const RuntimeShape& unextended_output_shape,
                         T* output_data) {
  // Note that the output_shape is not used herein.
  tflite::StridedSliceParams params_copy = op_params;
   e1104:	4603      	mov	r3, r0
   e1106:	ac16      	add	r4, sp, #88	; 0x58
   e1108:	f100 0528 	add.w	r5, r0, #40	; 0x28
   e110c:	6818      	ldr	r0, [r3, #0]
   e110e:	6859      	ldr	r1, [r3, #4]
   e1110:	4622      	mov	r2, r4
   e1112:	c203      	stmia	r2!, {r0, r1}
   e1114:	3308      	adds	r3, #8
   e1116:	42ab      	cmp	r3, r5
   e1118:	4614      	mov	r4, r2
   e111a:	d1f7      	bne.n	e110c <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x14>

  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
   e111c:	683b      	ldr	r3, [r7, #0]
   e111e:	2b04      	cmp	r3, #4
   e1120:	dd01      	ble.n	e1126 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x2e>
   e1122:	f003 f903 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   e1126:	6833      	ldr	r3, [r6, #0]
   e1128:	2b04      	cmp	r3, #4
   e112a:	dcfa      	bgt.n	e1122 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x2a>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
   e112c:	ad0c      	add	r5, sp, #48	; 0x30
   e112e:	2301      	movs	r3, #1
   e1130:	463a      	mov	r2, r7
   e1132:	2104      	movs	r1, #4
   e1134:	4628      	mov	r0, r5
   e1136:	f7f5 f9ca 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   e113a:	2301      	movs	r3, #1
   e113c:	4632      	mov	r2, r6
   e113e:	2104      	movs	r1, #4
   e1140:	a811      	add	r0, sp, #68	; 0x44
   e1142:	f7f5 f9c4 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  // Reverse and pad to 4 dimensions because that is what the runtime code
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);
   e1146:	2104      	movs	r1, #4
   e1148:	a816      	add	r0, sp, #88	; 0x58
   e114a:	f7ff fe03 	bl	e0d54 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi>

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
   e114e:	2200      	movs	r2, #0
   e1150:	4629      	mov	r1, r5
   e1152:	a816      	add	r0, sp, #88	; 0x58
   e1154:	f7ff fe52 	bl	e0dfc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
   e1158:	2200      	movs	r2, #0
   e115a:	4603      	mov	r3, r0
   e115c:	4629      	mov	r1, r5

  // Reverse and pad to 4 dimensions because that is what the runtime code
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
   e115e:	4604      	mov	r4, r0
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
   e1160:	a816      	add	r0, sp, #88	; 0x58
   e1162:	f7ff fe6e 	bl	e0e42 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
   e1166:	2201      	movs	r2, #1
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
   e1168:	9003      	str	r0, [sp, #12]
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
   e116a:	4629      	mov	r1, r5
   e116c:	a816      	add	r0, sp, #88	; 0x58
   e116e:	f7ff fe45 	bl	e0dfc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
   e1172:	2201      	movs	r2, #1
   e1174:	4603      	mov	r3, r0
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
   e1176:	9004      	str	r0, [sp, #16]
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
   e1178:	4629      	mov	r1, r5
   e117a:	a816      	add	r0, sp, #88	; 0x58
   e117c:	f7ff fe61 	bl	e0e42 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
   e1180:	2202      	movs	r2, #2
  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
   e1182:	9005      	str	r0, [sp, #20]
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
   e1184:	4629      	mov	r1, r5
   e1186:	a816      	add	r0, sp, #88	; 0x58
   e1188:	f7ff fe38 	bl	e0dfc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
   e118c:	2202      	movs	r2, #2
   e118e:	4603      	mov	r3, r0
   e1190:	4629      	mov	r1, r5
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
   e1192:	4680      	mov	r8, r0
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
   e1194:	a816      	add	r0, sp, #88	; 0x58
   e1196:	f7ff fe54 	bl	e0e42 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
   e119a:	2203      	movs	r2, #3
   e119c:	4629      	mov	r1, r5
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
   e119e:	4681      	mov	r9, r0
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
   e11a0:	a816      	add	r0, sp, #88	; 0x58
   e11a2:	f7ff fe2b 	bl	e0dfc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
   e11a6:	2203      	movs	r2, #3
   e11a8:	4603      	mov	r3, r0
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
   e11aa:	4682      	mov	sl, r0
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
   e11ac:	4629      	mov	r1, r5
   e11ae:	a816      	add	r0, sp, #88	; 0x58
   e11b0:	f7ff fe47 	bl	e0e42 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
   e11b4:	f9bd 306e 	ldrsh.w	r3, [sp, #110]	; 0x6e
   e11b8:	9306      	str	r3, [sp, #24]
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
   e11ba:	f9bd 3070 	ldrsh.w	r3, [sp, #112]	; 0x70
   e11be:	9307      	str	r3, [sp, #28]
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
   e11c0:	f9bd 3072 	ldrsh.w	r3, [sp, #114]	; 0x72
   e11c4:	9308      	str	r3, [sp, #32]
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e11c6:	f9bd 3074 	ldrsh.w	r3, [sp, #116]	; 0x74
   e11ca:	9309      	str	r3, [sp, #36]	; 0x24
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
   e11cc:	4683      	mov	fp, r0
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
   e11ce:	950b      	str	r5, [sp, #44]	; 0x2c

inline bool LoopCondition(int index, int stop, int stride) {
  // True when we have reached the end of an axis and should loop.
  return stride > 0 ? index >= stop : index <= stop;
   e11d0:	9b06      	ldr	r3, [sp, #24]
   e11d2:	2b00      	cmp	r3, #0
   e11d4:	9b03      	ldr	r3, [sp, #12]
   e11d6:	dd04      	ble.n	e11e2 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xea>
   e11d8:	429c      	cmp	r4, r3
   e11da:	bfb4      	ite	lt
   e11dc:	2300      	movlt	r3, #0
   e11de:	2301      	movge	r3, #1
   e11e0:	e003      	b.n	e11ea <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xf2>
   e11e2:	429c      	cmp	r4, r3
   e11e4:	bfcc      	ite	gt
   e11e6:	2300      	movgt	r3, #0
   e11e8:	2301      	movle	r3, #1
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
   e11ea:	2b00      	cmp	r3, #0
   e11ec:	d146      	bne.n	e127c <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x184>
   e11ee:	9d04      	ldr	r5, [sp, #16]
   e11f0:	9b07      	ldr	r3, [sp, #28]
   e11f2:	2b00      	cmp	r3, #0
   e11f4:	9b05      	ldr	r3, [sp, #20]
   e11f6:	dd04      	ble.n	e1202 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x10a>
   e11f8:	429d      	cmp	r5, r3
   e11fa:	bfb4      	ite	lt
   e11fc:	2300      	movlt	r3, #0
   e11fe:	2301      	movge	r3, #1
   e1200:	e003      	b.n	e120a <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x112>
   e1202:	429d      	cmp	r5, r3
   e1204:	bfcc      	ite	gt
   e1206:	2300      	movgt	r3, #0
   e1208:	2301      	movle	r3, #1
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
   e120a:	2b00      	cmp	r3, #0
   e120c:	d133      	bne.n	e1276 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x17e>
   e120e:	4646      	mov	r6, r8
   e1210:	9b08      	ldr	r3, [sp, #32]
   e1212:	2b00      	cmp	r3, #0
   e1214:	dd04      	ble.n	e1220 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x128>
   e1216:	454e      	cmp	r6, r9
   e1218:	bfb4      	ite	lt
   e121a:	2300      	movlt	r3, #0
   e121c:	2301      	movge	r3, #1
   e121e:	e003      	b.n	e1228 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x130>
   e1220:	454e      	cmp	r6, r9
   e1222:	bfcc      	ite	gt
   e1224:	2300      	movgt	r3, #0
   e1226:	2301      	movle	r3, #1
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
   e1228:	bb13      	cbnz	r3, e1270 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x178>
   e122a:	4657      	mov	r7, sl
   e122c:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e122e:	2b00      	cmp	r3, #0
   e1230:	dd04      	ble.n	e123c <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x144>
   e1232:	455f      	cmp	r7, fp
   e1234:	bfb4      	ite	lt
   e1236:	2300      	movlt	r3, #0
   e1238:	2301      	movge	r3, #1
   e123a:	e003      	b.n	e1244 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x14c>
   e123c:	455f      	cmp	r7, fp
   e123e:	bfcc      	ite	gt
   e1240:	2300      	movgt	r3, #0
   e1242:	2301      	movle	r3, #1
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e1244:	b98b      	cbnz	r3, e126a <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x172>
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
   e1246:	9700      	str	r7, [sp, #0]
   e1248:	4633      	mov	r3, r6
   e124a:	462a      	mov	r2, r5
   e124c:	4621      	mov	r1, r4
   e124e:	980b      	ldr	r0, [sp, #44]	; 0x2c
   e1250:	f7f5 f969 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   e1254:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e1256:	9a2a      	ldr	r2, [sp, #168]	; 0xa8
   e1258:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   e125c:	6803      	ldr	r3, [r0, #0]
   e125e:	f842 3b04 	str.w	r3, [r2], #4
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e1262:	9b09      	ldr	r3, [sp, #36]	; 0x24
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
   e1264:	922a      	str	r2, [sp, #168]	; 0xa8
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e1266:	441f      	add	r7, r3
   e1268:	e7e0      	b.n	e122c <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x134>
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
   e126a:	9b08      	ldr	r3, [sp, #32]
   e126c:	441e      	add	r6, r3
   e126e:	e7cf      	b.n	e1210 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x118>

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
   e1270:	9b07      	ldr	r3, [sp, #28]
   e1272:	441d      	add	r5, r3
   e1274:	e7bc      	b.n	e11f0 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xf8>
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
   e1276:	9b06      	ldr	r3, [sp, #24]
   e1278:	441c      	add	r4, r3
   e127a:	e7a9      	b.n	e11d0 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xd8>
  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape input_shape =
      RuntimeShape::ExtendedShape(4, unextended_input_shape);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   e127c:	a811      	add	r0, sp, #68	; 0x44
   e127e:	f7f5 f8e2 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  tflite::StridedSliceParams params_copy = op_params;

  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape input_shape =
      RuntimeShape::ExtendedShape(4, unextended_input_shape);
   e1282:	a80c      	add	r0, sp, #48	; 0x30
   e1284:	f7f5 f8df 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
        }
      }
    }
  }
}
   e1288:	b021      	add	sp, #132	; 0x84
   e128a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000e128e <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_>:

namespace tflite {

namespace reference_ops {
template <typename T>
inline void StridedSlice(const tflite::StridedSliceParams& op_params,
   e128e:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e1292:	b0a1      	sub	sp, #132	; 0x84
   e1294:	461e      	mov	r6, r3
   e1296:	460f      	mov	r7, r1
   e1298:	920a      	str	r2, [sp, #40]	; 0x28
                         const RuntimeShape& unextended_input_shape,
                         const T* input_data,
                         const RuntimeShape& unextended_output_shape,
                         T* output_data) {
  // Note that the output_shape is not used herein.
  tflite::StridedSliceParams params_copy = op_params;
   e129a:	4603      	mov	r3, r0
   e129c:	ac16      	add	r4, sp, #88	; 0x58
   e129e:	f100 0528 	add.w	r5, r0, #40	; 0x28
   e12a2:	6818      	ldr	r0, [r3, #0]
   e12a4:	6859      	ldr	r1, [r3, #4]
   e12a6:	4622      	mov	r2, r4
   e12a8:	c203      	stmia	r2!, {r0, r1}
   e12aa:	3308      	adds	r3, #8
   e12ac:	42ab      	cmp	r3, r5
   e12ae:	4614      	mov	r4, r2
   e12b0:	d1f7      	bne.n	e12a2 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x14>

  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
   e12b2:	683b      	ldr	r3, [r7, #0]
   e12b4:	2b04      	cmp	r3, #4
   e12b6:	dd01      	ble.n	e12bc <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x2e>
   e12b8:	f003 f838 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   e12bc:	6833      	ldr	r3, [r6, #0]
   e12be:	2b04      	cmp	r3, #4
   e12c0:	dcfa      	bgt.n	e12b8 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x2a>
   e12c2:	ad0c      	add	r5, sp, #48	; 0x30
   e12c4:	2301      	movs	r3, #1
   e12c6:	463a      	mov	r2, r7
   e12c8:	2104      	movs	r1, #4
   e12ca:	4628      	mov	r0, r5
   e12cc:	f7f5 f8ff 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   e12d0:	2301      	movs	r3, #1
   e12d2:	4632      	mov	r2, r6
   e12d4:	2104      	movs	r1, #4
   e12d6:	a811      	add	r0, sp, #68	; 0x44
   e12d8:	f7f5 f8f9 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  // Reverse and pad to 4 dimensions because that is what the runtime code
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);
   e12dc:	2104      	movs	r1, #4
   e12de:	a816      	add	r0, sp, #88	; 0x58
   e12e0:	f7ff fd38 	bl	e0d54 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi>

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
   e12e4:	2200      	movs	r2, #0
   e12e6:	4629      	mov	r1, r5
   e12e8:	a816      	add	r0, sp, #88	; 0x58
   e12ea:	f7ff fd87 	bl	e0dfc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
   e12ee:	2200      	movs	r2, #0
   e12f0:	4603      	mov	r3, r0
   e12f2:	4629      	mov	r1, r5

  // Reverse and pad to 4 dimensions because that is what the runtime code
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
   e12f4:	4604      	mov	r4, r0
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
   e12f6:	a816      	add	r0, sp, #88	; 0x58
   e12f8:	f7ff fda3 	bl	e0e42 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
   e12fc:	2201      	movs	r2, #1
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
   e12fe:	9003      	str	r0, [sp, #12]
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
   e1300:	4629      	mov	r1, r5
   e1302:	a816      	add	r0, sp, #88	; 0x58
   e1304:	f7ff fd7a 	bl	e0dfc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
   e1308:	2201      	movs	r2, #1
   e130a:	4603      	mov	r3, r0
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
   e130c:	9004      	str	r0, [sp, #16]
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
   e130e:	4629      	mov	r1, r5
   e1310:	a816      	add	r0, sp, #88	; 0x58
   e1312:	f7ff fd96 	bl	e0e42 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
   e1316:	2202      	movs	r2, #2
  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
   e1318:	9005      	str	r0, [sp, #20]
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
   e131a:	4629      	mov	r1, r5
   e131c:	a816      	add	r0, sp, #88	; 0x58
   e131e:	f7ff fd6d 	bl	e0dfc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
   e1322:	2202      	movs	r2, #2
   e1324:	4603      	mov	r3, r0
   e1326:	4629      	mov	r1, r5
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
   e1328:	4680      	mov	r8, r0
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
   e132a:	a816      	add	r0, sp, #88	; 0x58
   e132c:	f7ff fd89 	bl	e0e42 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
   e1330:	2203      	movs	r2, #3
   e1332:	4629      	mov	r1, r5
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
   e1334:	4681      	mov	r9, r0
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
   e1336:	a816      	add	r0, sp, #88	; 0x58
   e1338:	f7ff fd60 	bl	e0dfc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
   e133c:	2203      	movs	r2, #3
   e133e:	4603      	mov	r3, r0
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
   e1340:	4682      	mov	sl, r0
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
   e1342:	4629      	mov	r1, r5
   e1344:	a816      	add	r0, sp, #88	; 0x58
   e1346:	f7ff fd7c 	bl	e0e42 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
   e134a:	f9bd 306e 	ldrsh.w	r3, [sp, #110]	; 0x6e
   e134e:	9306      	str	r3, [sp, #24]
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
   e1350:	f9bd 3070 	ldrsh.w	r3, [sp, #112]	; 0x70
   e1354:	9307      	str	r3, [sp, #28]
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
   e1356:	f9bd 3072 	ldrsh.w	r3, [sp, #114]	; 0x72
   e135a:	9308      	str	r3, [sp, #32]
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e135c:	f9bd 3074 	ldrsh.w	r3, [sp, #116]	; 0x74
   e1360:	9309      	str	r3, [sp, #36]	; 0x24
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
   e1362:	4683      	mov	fp, r0
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
   e1364:	950b      	str	r5, [sp, #44]	; 0x2c
   e1366:	9b06      	ldr	r3, [sp, #24]
   e1368:	2b00      	cmp	r3, #0
   e136a:	9b03      	ldr	r3, [sp, #12]
   e136c:	dd04      	ble.n	e1378 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xea>
   e136e:	429c      	cmp	r4, r3
   e1370:	bfb4      	ite	lt
   e1372:	2300      	movlt	r3, #0
   e1374:	2301      	movge	r3, #1
   e1376:	e003      	b.n	e1380 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xf2>
   e1378:	429c      	cmp	r4, r3
   e137a:	bfcc      	ite	gt
   e137c:	2300      	movgt	r3, #0
   e137e:	2301      	movle	r3, #1
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
   e1380:	2b00      	cmp	r3, #0
   e1382:	d144      	bne.n	e140e <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x180>
   e1384:	9d04      	ldr	r5, [sp, #16]
   e1386:	9b07      	ldr	r3, [sp, #28]
   e1388:	2b00      	cmp	r3, #0
   e138a:	9b05      	ldr	r3, [sp, #20]
   e138c:	dd04      	ble.n	e1398 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x10a>
   e138e:	429d      	cmp	r5, r3
   e1390:	bfb4      	ite	lt
   e1392:	2300      	movlt	r3, #0
   e1394:	2301      	movge	r3, #1
   e1396:	e003      	b.n	e13a0 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x112>
   e1398:	429d      	cmp	r5, r3
   e139a:	bfcc      	ite	gt
   e139c:	2300      	movgt	r3, #0
   e139e:	2301      	movle	r3, #1
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
   e13a0:	2b00      	cmp	r3, #0
   e13a2:	d131      	bne.n	e1408 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x17a>
   e13a4:	4646      	mov	r6, r8
   e13a6:	9b08      	ldr	r3, [sp, #32]
   e13a8:	2b00      	cmp	r3, #0
   e13aa:	dd04      	ble.n	e13b6 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x128>
   e13ac:	454e      	cmp	r6, r9
   e13ae:	bfb4      	ite	lt
   e13b0:	2300      	movlt	r3, #0
   e13b2:	2301      	movge	r3, #1
   e13b4:	e003      	b.n	e13be <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x130>
   e13b6:	454e      	cmp	r6, r9
   e13b8:	bfcc      	ite	gt
   e13ba:	2300      	movgt	r3, #0
   e13bc:	2301      	movle	r3, #1
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
   e13be:	bb03      	cbnz	r3, e1402 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x174>
   e13c0:	4657      	mov	r7, sl
   e13c2:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e13c4:	2b00      	cmp	r3, #0
   e13c6:	dd04      	ble.n	e13d2 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x144>
   e13c8:	455f      	cmp	r7, fp
   e13ca:	bfb4      	ite	lt
   e13cc:	2300      	movlt	r3, #0
   e13ce:	2301      	movge	r3, #1
   e13d0:	e003      	b.n	e13da <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x14c>
   e13d2:	455f      	cmp	r7, fp
   e13d4:	bfcc      	ite	gt
   e13d6:	2300      	movgt	r3, #0
   e13d8:	2301      	movle	r3, #1
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e13da:	b97b      	cbnz	r3, e13fc <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x16e>
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
   e13dc:	9700      	str	r7, [sp, #0]
   e13de:	4633      	mov	r3, r6
   e13e0:	462a      	mov	r2, r5
   e13e2:	4621      	mov	r1, r4
   e13e4:	980b      	ldr	r0, [sp, #44]	; 0x2c
   e13e6:	f7f5 f89e 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   e13ea:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e13ec:	9a2a      	ldr	r2, [sp, #168]	; 0xa8
   e13ee:	5c1b      	ldrb	r3, [r3, r0]
   e13f0:	f802 3b01 	strb.w	r3, [r2], #1
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e13f4:	9b09      	ldr	r3, [sp, #36]	; 0x24
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
   e13f6:	922a      	str	r2, [sp, #168]	; 0xa8
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e13f8:	441f      	add	r7, r3
   e13fa:	e7e2      	b.n	e13c2 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x134>
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
   e13fc:	9b08      	ldr	r3, [sp, #32]
   e13fe:	441e      	add	r6, r3
   e1400:	e7d1      	b.n	e13a6 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x118>

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
   e1402:	9b07      	ldr	r3, [sp, #28]
   e1404:	441d      	add	r5, r3
   e1406:	e7be      	b.n	e1386 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xf8>
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
   e1408:	9b06      	ldr	r3, [sp, #24]
   e140a:	441c      	add	r4, r3
   e140c:	e7ab      	b.n	e1366 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xd8>
  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape input_shape =
      RuntimeShape::ExtendedShape(4, unextended_input_shape);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   e140e:	a811      	add	r0, sp, #68	; 0x44
   e1410:	f7f5 f819 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  tflite::StridedSliceParams params_copy = op_params;

  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape input_shape =
      RuntimeShape::ExtendedShape(4, unextended_input_shape);
   e1414:	a80c      	add	r0, sp, #48	; 0x30
   e1416:	f7f5 f816 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
        }
      }
    }
  }
}
   e141a:	b021      	add	sp, #132	; 0x84
   e141c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000e1420 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_>:

namespace tflite {

namespace reference_ops {
template <typename T>
inline void StridedSlice(const tflite::StridedSliceParams& op_params,
   e1420:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e1424:	b0a1      	sub	sp, #132	; 0x84
   e1426:	461e      	mov	r6, r3
   e1428:	460f      	mov	r7, r1
   e142a:	920a      	str	r2, [sp, #40]	; 0x28
                         const RuntimeShape& unextended_input_shape,
                         const T* input_data,
                         const RuntimeShape& unextended_output_shape,
                         T* output_data) {
  // Note that the output_shape is not used herein.
  tflite::StridedSliceParams params_copy = op_params;
   e142c:	4603      	mov	r3, r0
   e142e:	ac16      	add	r4, sp, #88	; 0x58
   e1430:	f100 0528 	add.w	r5, r0, #40	; 0x28
   e1434:	6818      	ldr	r0, [r3, #0]
   e1436:	6859      	ldr	r1, [r3, #4]
   e1438:	4622      	mov	r2, r4
   e143a:	c203      	stmia	r2!, {r0, r1}
   e143c:	3308      	adds	r3, #8
   e143e:	42ab      	cmp	r3, r5
   e1440:	4614      	mov	r4, r2
   e1442:	d1f7      	bne.n	e1434 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x14>

  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
   e1444:	683b      	ldr	r3, [r7, #0]
   e1446:	2b04      	cmp	r3, #4
   e1448:	dd01      	ble.n	e144e <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x2e>
   e144a:	f002 ff6f 	bl	e432c <abort>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   e144e:	6833      	ldr	r3, [r6, #0]
   e1450:	2b04      	cmp	r3, #4
   e1452:	dcfa      	bgt.n	e144a <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x2a>
   e1454:	ad0c      	add	r5, sp, #48	; 0x30
   e1456:	2301      	movs	r3, #1
   e1458:	463a      	mov	r2, r7
   e145a:	2104      	movs	r1, #4
   e145c:	4628      	mov	r0, r5
   e145e:	f7f5 f836 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   e1462:	2301      	movs	r3, #1
   e1464:	4632      	mov	r2, r6
   e1466:	2104      	movs	r1, #4
   e1468:	a811      	add	r0, sp, #68	; 0x44
   e146a:	f7f5 f830 	bl	d64ce <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  // Reverse and pad to 4 dimensions because that is what the runtime code
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);
   e146e:	2104      	movs	r1, #4
   e1470:	a816      	add	r0, sp, #88	; 0x58
   e1472:	f7ff fc6f 	bl	e0d54 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi>

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
   e1476:	2200      	movs	r2, #0
   e1478:	4629      	mov	r1, r5
   e147a:	a816      	add	r0, sp, #88	; 0x58
   e147c:	f7ff fcbe 	bl	e0dfc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
   e1480:	2200      	movs	r2, #0
   e1482:	4603      	mov	r3, r0
   e1484:	4629      	mov	r1, r5

  // Reverse and pad to 4 dimensions because that is what the runtime code
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
   e1486:	4604      	mov	r4, r0
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
   e1488:	a816      	add	r0, sp, #88	; 0x58
   e148a:	f7ff fcda 	bl	e0e42 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
   e148e:	2201      	movs	r2, #1
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
   e1490:	9003      	str	r0, [sp, #12]
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
   e1492:	4629      	mov	r1, r5
   e1494:	a816      	add	r0, sp, #88	; 0x58
   e1496:	f7ff fcb1 	bl	e0dfc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
   e149a:	2201      	movs	r2, #1
   e149c:	4603      	mov	r3, r0
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
   e149e:	9004      	str	r0, [sp, #16]
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
   e14a0:	4629      	mov	r1, r5
   e14a2:	a816      	add	r0, sp, #88	; 0x58
   e14a4:	f7ff fccd 	bl	e0e42 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
   e14a8:	2202      	movs	r2, #2
  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
   e14aa:	9005      	str	r0, [sp, #20]
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
   e14ac:	4629      	mov	r1, r5
   e14ae:	a816      	add	r0, sp, #88	; 0x58
   e14b0:	f7ff fca4 	bl	e0dfc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
   e14b4:	2202      	movs	r2, #2
   e14b6:	4603      	mov	r3, r0
   e14b8:	4629      	mov	r1, r5
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
   e14ba:	4680      	mov	r8, r0
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
   e14bc:	a816      	add	r0, sp, #88	; 0x58
   e14be:	f7ff fcc0 	bl	e0e42 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
   e14c2:	2203      	movs	r2, #3
   e14c4:	4629      	mov	r1, r5
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
   e14c6:	4681      	mov	r9, r0
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
   e14c8:	a816      	add	r0, sp, #88	; 0x58
   e14ca:	f7ff fc97 	bl	e0dfc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
   e14ce:	2203      	movs	r2, #3
   e14d0:	4603      	mov	r3, r0
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
   e14d2:	4682      	mov	sl, r0
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
   e14d4:	4629      	mov	r1, r5
   e14d6:	a816      	add	r0, sp, #88	; 0x58
   e14d8:	f7ff fcb3 	bl	e0e42 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
   e14dc:	f9bd 306e 	ldrsh.w	r3, [sp, #110]	; 0x6e
   e14e0:	9306      	str	r3, [sp, #24]
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
   e14e2:	f9bd 3070 	ldrsh.w	r3, [sp, #112]	; 0x70
   e14e6:	9307      	str	r3, [sp, #28]
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
   e14e8:	f9bd 3072 	ldrsh.w	r3, [sp, #114]	; 0x72
   e14ec:	9308      	str	r3, [sp, #32]
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e14ee:	f9bd 3074 	ldrsh.w	r3, [sp, #116]	; 0x74
   e14f2:	9309      	str	r3, [sp, #36]	; 0x24
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
   e14f4:	4683      	mov	fp, r0
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
   e14f6:	950b      	str	r5, [sp, #44]	; 0x2c
   e14f8:	9b06      	ldr	r3, [sp, #24]
   e14fa:	2b00      	cmp	r3, #0
   e14fc:	9b03      	ldr	r3, [sp, #12]
   e14fe:	dd04      	ble.n	e150a <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xea>
   e1500:	429c      	cmp	r4, r3
   e1502:	bfb4      	ite	lt
   e1504:	2300      	movlt	r3, #0
   e1506:	2301      	movge	r3, #1
   e1508:	e003      	b.n	e1512 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xf2>
   e150a:	429c      	cmp	r4, r3
   e150c:	bfcc      	ite	gt
   e150e:	2300      	movgt	r3, #0
   e1510:	2301      	movle	r3, #1
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
   e1512:	2b00      	cmp	r3, #0
   e1514:	d144      	bne.n	e15a0 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x180>
   e1516:	9d04      	ldr	r5, [sp, #16]
   e1518:	9b07      	ldr	r3, [sp, #28]
   e151a:	2b00      	cmp	r3, #0
   e151c:	9b05      	ldr	r3, [sp, #20]
   e151e:	dd04      	ble.n	e152a <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x10a>
   e1520:	429d      	cmp	r5, r3
   e1522:	bfb4      	ite	lt
   e1524:	2300      	movlt	r3, #0
   e1526:	2301      	movge	r3, #1
   e1528:	e003      	b.n	e1532 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x112>
   e152a:	429d      	cmp	r5, r3
   e152c:	bfcc      	ite	gt
   e152e:	2300      	movgt	r3, #0
   e1530:	2301      	movle	r3, #1
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
   e1532:	2b00      	cmp	r3, #0
   e1534:	d131      	bne.n	e159a <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x17a>
   e1536:	4646      	mov	r6, r8
   e1538:	9b08      	ldr	r3, [sp, #32]
   e153a:	2b00      	cmp	r3, #0
   e153c:	dd04      	ble.n	e1548 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x128>
   e153e:	454e      	cmp	r6, r9
   e1540:	bfb4      	ite	lt
   e1542:	2300      	movlt	r3, #0
   e1544:	2301      	movge	r3, #1
   e1546:	e003      	b.n	e1550 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x130>
   e1548:	454e      	cmp	r6, r9
   e154a:	bfcc      	ite	gt
   e154c:	2300      	movgt	r3, #0
   e154e:	2301      	movle	r3, #1
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
   e1550:	bb03      	cbnz	r3, e1594 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x174>
   e1552:	4657      	mov	r7, sl
   e1554:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e1556:	2b00      	cmp	r3, #0
   e1558:	dd04      	ble.n	e1564 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x144>
   e155a:	455f      	cmp	r7, fp
   e155c:	bfb4      	ite	lt
   e155e:	2300      	movlt	r3, #0
   e1560:	2301      	movge	r3, #1
   e1562:	e003      	b.n	e156c <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x14c>
   e1564:	455f      	cmp	r7, fp
   e1566:	bfcc      	ite	gt
   e1568:	2300      	movgt	r3, #0
   e156a:	2301      	movle	r3, #1
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e156c:	b97b      	cbnz	r3, e158e <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x16e>
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
   e156e:	9700      	str	r7, [sp, #0]
   e1570:	4633      	mov	r3, r6
   e1572:	462a      	mov	r2, r5
   e1574:	4621      	mov	r1, r4
   e1576:	980b      	ldr	r0, [sp, #44]	; 0x2c
   e1578:	f7f4 ffd5 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   e157c:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e157e:	9a2a      	ldr	r2, [sp, #168]	; 0xa8
   e1580:	561b      	ldrsb	r3, [r3, r0]
   e1582:	f802 3b01 	strb.w	r3, [r2], #1
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e1586:	9b09      	ldr	r3, [sp, #36]	; 0x24
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
   e1588:	922a      	str	r2, [sp, #168]	; 0xa8
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e158a:	441f      	add	r7, r3
   e158c:	e7e2      	b.n	e1554 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x134>
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
   e158e:	9b08      	ldr	r3, [sp, #32]
   e1590:	441e      	add	r6, r3
   e1592:	e7d1      	b.n	e1538 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x118>

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
   e1594:	9b07      	ldr	r3, [sp, #28]
   e1596:	441d      	add	r5, r3
   e1598:	e7be      	b.n	e1518 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xf8>
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
   e159a:	9b06      	ldr	r3, [sp, #24]
   e159c:	441c      	add	r4, r3
   e159e:	e7ab      	b.n	e14f8 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xd8>
  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape input_shape =
      RuntimeShape::ExtendedShape(4, unextended_input_shape);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   e15a0:	a811      	add	r0, sp, #68	; 0x44
   e15a2:	f7f4 ff50 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  tflite::StridedSliceParams params_copy = op_params;

  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape input_shape =
      RuntimeShape::ExtendedShape(4, unextended_input_shape);
   e15a6:	a80c      	add	r0, sp, #48	; 0x30
   e15a8:	f7f4 ff4d 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
        }
      }
    }
  }
}
   e15ac:	b021      	add	sp, #132	; 0x84
   e15ae:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

000e15b4 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode>:
                     "input dim should not exceed 4");
  return CheckOutputSize(context, &op_context);
}

template <KernelType kernel_type>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e15b4:	b510      	push	{r4, lr}
   e15b6:	b09e      	sub	sp, #120	; 0x78
  StridedSliceContext op_context(context, node);
   e15b8:	460a      	mov	r2, r1
                     "input dim should not exceed 4");
  return CheckOutputSize(context, &op_context);
}

template <KernelType kernel_type>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e15ba:	4604      	mov	r4, r0
  StridedSliceContext op_context(context, node);
   e15bc:	4601      	mov	r1, r0
   e15be:	a80d      	add	r0, sp, #52	; 0x34
   e15c0:	f7ff fc78 	bl	e0eb4 <_ZN6tflite3ops5micro13strided_slice19StridedSliceContextC1EP13TfLiteContextP10TfLiteNode>
  auto op_params = BuildStridedSliceParams(&op_context);
   e15c4:	a90d      	add	r1, sp, #52	; 0x34
   e15c6:	a814      	add	r0, sp, #80	; 0x50
   e15c8:	f7ff fc9a 	bl	e0f00 <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE>
  kernel_type::StridedSlice(op_params, GetTensorShape(op_context.input), \
                            GetTensorData<data_type>(op_context.input),  \
                            GetTensorShape(op_context.output),           \
                            GetTensorData<data_type>(op_context.output))

  switch (op_context.input->type) {
   e15cc:	990e      	ldr	r1, [sp, #56]	; 0x38
   e15ce:	780a      	ldrb	r2, [r1, #0]
   e15d0:	2a03      	cmp	r2, #3
   e15d2:	d01a      	beq.n	e160a <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x56>
   e15d4:	2a09      	cmp	r2, #9
   e15d6:	d036      	beq.n	e1646 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x92>
   e15d8:	2a01      	cmp	r2, #1
   e15da:	d14b      	bne.n	e1674 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xc0>
    case kTfLiteFloat32:
      if (kernel_type == kReference) {
        TF_LITE_STRIDED_SLICE(reference_ops, float);
   e15dc:	a803      	add	r0, sp, #12
   e15de:	f7f5 f9e2 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e15e2:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   e15e4:	b10a      	cbz	r2, e15ea <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x36>
   e15e6:	6854      	ldr	r4, [r2, #4]
   e15e8:	e000      	b.n	e15ec <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x38>
   e15ea:	4614      	mov	r4, r2
   e15ec:	9912      	ldr	r1, [sp, #72]	; 0x48
   e15ee:	a808      	add	r0, sp, #32
   e15f0:	f7f5 f9d9 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e15f4:	9b12      	ldr	r3, [sp, #72]	; 0x48

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e15f6:	b103      	cbz	r3, e15fa <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x46>
   e15f8:	685b      	ldr	r3, [r3, #4]
   e15fa:	9300      	str	r3, [sp, #0]
   e15fc:	4622      	mov	r2, r4
   e15fe:	ab08      	add	r3, sp, #32
   e1600:	a903      	add	r1, sp, #12
   e1602:	a814      	add	r0, sp, #80	; 0x50
   e1604:	f7ff fd78 	bl	e10f8 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_>
   e1608:	e015      	b.n	e1636 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x82>
      }
      break;
    case kTfLiteUInt8:
      if (kernel_type == kReference) {
        TF_LITE_STRIDED_SLICE(reference_ops, uint8_t);
   e160a:	a803      	add	r0, sp, #12
   e160c:	f7f5 f9cb 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e1610:	9a0e      	ldr	r2, [sp, #56]	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e1612:	b10a      	cbz	r2, e1618 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x64>
   e1614:	6854      	ldr	r4, [r2, #4]
   e1616:	e000      	b.n	e161a <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x66>
   e1618:	4614      	mov	r4, r2
   e161a:	9912      	ldr	r1, [sp, #72]	; 0x48
   e161c:	a808      	add	r0, sp, #32
   e161e:	f7f5 f9c2 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e1622:	9b12      	ldr	r3, [sp, #72]	; 0x48

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e1624:	b103      	cbz	r3, e1628 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x74>
   e1626:	685b      	ldr	r3, [r3, #4]
   e1628:	9300      	str	r3, [sp, #0]
   e162a:	4622      	mov	r2, r4
   e162c:	ab08      	add	r3, sp, #32
   e162e:	a903      	add	r1, sp, #12
   e1630:	a814      	add	r0, sp, #80	; 0x50
   e1632:	f7ff fe2c 	bl	e128e <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_>
   e1636:	a808      	add	r0, sp, #32
   e1638:	f7f4 ff05 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   e163c:	a803      	add	r0, sp, #12
   e163e:	f7f4 ff02 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
                           "by StridedSlice.",
                           op_context.input->type);
      return kTfLiteError;
  }
#undef TF_LITE_STRIDED_SLICE
  return kTfLiteOk;
   e1642:	2000      	movs	r0, #0
      break;
    case kTfLiteUInt8:
      if (kernel_type == kReference) {
        TF_LITE_STRIDED_SLICE(reference_ops, uint8_t);
      }
      break;
   e1644:	e01b      	b.n	e167e <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xca>
    case kTfLiteInt8:
      if (kernel_type == kReference) {
        TF_LITE_STRIDED_SLICE(reference_ops, int8_t);
   e1646:	a803      	add	r0, sp, #12
   e1648:	f7f5 f9ad 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e164c:	9a0e      	ldr	r2, [sp, #56]	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e164e:	b10a      	cbz	r2, e1654 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa0>
   e1650:	6854      	ldr	r4, [r2, #4]
   e1652:	e000      	b.n	e1656 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa2>
   e1654:	4614      	mov	r4, r2
   e1656:	9912      	ldr	r1, [sp, #72]	; 0x48
   e1658:	a808      	add	r0, sp, #32
   e165a:	f7f5 f9a4 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e165e:	9b12      	ldr	r3, [sp, #72]	; 0x48

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e1660:	b103      	cbz	r3, e1664 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xb0>
   e1662:	685b      	ldr	r3, [r3, #4]
   e1664:	9300      	str	r3, [sp, #0]
   e1666:	4622      	mov	r2, r4
   e1668:	ab08      	add	r3, sp, #32
   e166a:	a903      	add	r1, sp, #12
   e166c:	a814      	add	r0, sp, #80	; 0x50
   e166e:	f7ff fed7 	bl	e1420 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_>
   e1672:	e7e0      	b.n	e1636 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x82>
      }
      break;
    default:
      context->ReportError(context,
   e1674:	4620      	mov	r0, r4
   e1676:	6963      	ldr	r3, [r4, #20]
   e1678:	4902      	ldr	r1, [pc, #8]	; (e1684 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xd0>)
   e167a:	4798      	blx	r3
                           "Type %d is currently not supported "
                           "by StridedSlice.",
                           op_context.input->type);
      return kTfLiteError;
   e167c:	2001      	movs	r0, #1
  }
#undef TF_LITE_STRIDED_SLICE
  return kTfLiteOk;
}
   e167e:	b01e      	add	sp, #120	; 0x78
   e1680:	bd10      	pop	{r4, pc}
   e1682:	bf00      	nop
   e1684:	000ea9c4 	.word	0x000ea9c4

000e1688 <_ZN6tflite3ops5micro4svdf4InitEP13TfLiteContextPKcj>:
// Output tensor.
constexpr int kOutputTensor = 0;

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
   e1688:	2000      	movs	r0, #0
   e168a:	4770      	bx	lr

000e168c <_ZN6tflite3ops5micro4svdf4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
   e168c:	4770      	bx	lr
	...

000e1690 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_>:

static inline void ApplyTimeWeightsBiasAndActivation(
    int batch_size, int memory_size, int num_filters, int num_units, int rank,
    const TfLiteTensor* weights_time, const TfLiteTensor* bias,
    TfLiteFusedActivation activation, TfLiteTensor* activation_state,
    TfLiteTensor* scratch, TfLiteTensor* output) {
   e1690:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e1694:	b087      	sub	sp, #28
   e1696:	4693      	mov	fp, r2
   e1698:	9101      	str	r1, [sp, #4]
   e169a:	fb01 f10b 	mul.w	r1, r1, fp
   e169e:	0089      	lsls	r1, r1, #2
   e16a0:	9103      	str	r1, [sp, #12]
   e16a2:	ea4f 018b 	mov.w	r1, fp, lsl #2
   e16a6:	9104      	str	r1, [sp, #16]
   e16a8:	9901      	ldr	r1, [sp, #4]
   e16aa:	f89d 204c 	ldrb.w	r2, [sp, #76]	; 0x4c
   e16ae:	9205      	str	r2, [sp, #20]
   e16b0:	2500      	movs	r5, #0
   e16b2:	ea21 76e1 	bic.w	r6, r1, r1, asr #31
   e16b6:	9a16      	ldr	r2, [sp, #88]	; 0x58
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;
    const float* vector1_ptr = GetTensorData<float>(weights_time);
    const float* vector2_ptr =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int i = 0; i < num_filters; ++i) {
      *scratch_ptr_batch = 0.f;
   e16b8:	ed9f 7a77 	vldr	s14, [pc, #476]	; e1898 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x208>

static inline void ApplyTimeWeightsBiasAndActivation(
    int batch_size, int memory_size, int num_filters, int num_units, int rank,
    const TfLiteTensor* weights_time, const TfLiteTensor* bias,
    TfLiteFusedActivation activation, TfLiteTensor* activation_state,
    TfLiteTensor* scratch, TfLiteTensor* output) {
   e16bc:	9300      	str	r3, [sp, #0]
   e16be:	00b6      	lsls	r6, r6, #2
   e16c0:	46ae      	mov	lr, r5
  // Compute matmul(activation_state, weights_time).
  // The rightmost column is used to save temporary output (with the size of
  // num_filters). This is achieved by starting at
  // GetTensorData<float>(activation_state), and having the stride equal to
  // memory_size.
  for (int b = 0; b < batch_size; ++b) {
   e16c2:	46ac      	mov	ip, r5
   e16c4:	4584      	cmp	ip, r0
   e16c6:	da38      	bge.n	e173a <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xaa>
   e16c8:	9915      	ldr	r1, [sp, #84]	; 0x54
   e16ca:	b109      	cbz	r1, e16d0 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x40>
   e16cc:	6849      	ldr	r1, [r1, #4]
   e16ce:	e000      	b.n	e16d2 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x42>
   e16d0:	9915      	ldr	r1, [sp, #84]	; 0x54
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e16d2:	9c11      	ldr	r4, [sp, #68]	; 0x44
    // Perform batched vector dot product:
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;
   e16d4:	4429      	add	r1, r5
   e16d6:	b10c      	cbz	r4, e16dc <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x4c>
   e16d8:	6867      	ldr	r7, [r4, #4]
   e16da:	e000      	b.n	e16de <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x4e>
   e16dc:	9f11      	ldr	r7, [sp, #68]	; 0x44

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e16de:	9c14      	ldr	r4, [sp, #80]	; 0x50
   e16e0:	b10c      	cbz	r4, e16e6 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x56>
   e16e2:	6864      	ldr	r4, [r4, #4]
   e16e4:	e000      	b.n	e16e8 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x58>
   e16e6:	9c14      	ldr	r4, [sp, #80]	; 0x50
    const float* vector1_ptr = GetTensorData<float>(weights_time);
    const float* vector2_ptr =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
   e16e8:	4474      	add	r4, lr
    for (int i = 0; i < num_filters; ++i) {
   e16ea:	f04f 0800 	mov.w	r8, #0
   e16ee:	45d8      	cmp	r8, fp
   e16f0:	da1c      	bge.n	e172c <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x9c>
      *scratch_ptr_batch = 0.f;
   e16f2:	eca1 7a01 	vstmia	r1!, {s14}
   e16f6:	46a2      	mov	sl, r4
   e16f8:	9702      	str	r7, [sp, #8]
      for (int j = 0; j < memory_size; ++j) {
   e16fa:	f04f 0900 	mov.w	r9, #0
   e16fe:	9b01      	ldr	r3, [sp, #4]
   e1700:	4599      	cmp	r9, r3
   e1702:	da0e      	bge.n	e1722 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x92>
        *scratch_ptr_batch += *vector1_ptr++ * *vector2_ptr++;
   e1704:	9b02      	ldr	r3, [sp, #8]
   e1706:	ecfa 6a01 	vldmia	sl!, {s13}
   e170a:	ecb3 6a01 	vldmia	r3!, {s12}
   e170e:	ed51 7a01 	vldr	s15, [r1, #-4]
   e1712:	9302      	str	r3, [sp, #8]
   e1714:	eee6 7a26 	vfma.f32	s15, s12, s13
    const float* vector1_ptr = GetTensorData<float>(weights_time);
    const float* vector2_ptr =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int i = 0; i < num_filters; ++i) {
      *scratch_ptr_batch = 0.f;
      for (int j = 0; j < memory_size; ++j) {
   e1718:	f109 0901 	add.w	r9, r9, #1
        *scratch_ptr_batch += *vector1_ptr++ * *vector2_ptr++;
   e171c:	ed41 7a01 	vstr	s15, [r1, #-4]
    const float* vector1_ptr = GetTensorData<float>(weights_time);
    const float* vector2_ptr =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int i = 0; i < num_filters; ++i) {
      *scratch_ptr_batch = 0.f;
      for (int j = 0; j < memory_size; ++j) {
   e1720:	e7ed      	b.n	e16fe <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x6e>
   e1722:	4437      	add	r7, r6
   e1724:	4434      	add	r4, r6
    // Perform batched vector dot product:
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;
    const float* vector1_ptr = GetTensorData<float>(weights_time);
    const float* vector2_ptr =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int i = 0; i < num_filters; ++i) {
   e1726:	f108 0801 	add.w	r8, r8, #1
   e172a:	e7e0      	b.n	e16ee <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x5e>
   e172c:	9b03      	ldr	r3, [sp, #12]
   e172e:	449e      	add	lr, r3
   e1730:	9b04      	ldr	r3, [sp, #16]
  // Compute matmul(activation_state, weights_time).
  // The rightmost column is used to save temporary output (with the size of
  // num_filters). This is achieved by starting at
  // GetTensorData<float>(activation_state), and having the stride equal to
  // memory_size.
  for (int b = 0; b < batch_size; ++b) {
   e1732:	f10c 0c01 	add.w	ip, ip, #1
   e1736:	441d      	add	r5, r3
   e1738:	e7c4      	b.n	e16c4 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x34>
      scratch_ptr_batch++;
    }
  }

  // Initialize output with bias if provided.
  if (bias) {
   e173a:	9b12      	ldr	r3, [sp, #72]	; 0x48
   e173c:	b333      	cbz	r3, e178c <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xfc>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e173e:	f8d3 c004 	ldr.w	ip, [r3, #4]

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e1742:	b10a      	cbz	r2, e1748 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xb8>
   e1744:	6851      	ldr	r1, [r2, #4]
   e1746:	e000      	b.n	e174a <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xba>
   e1748:	4611      	mov	r1, r2
   e174a:	9b00      	ldr	r3, [sp, #0]
    // TODO(kreeger): doc me - VectorBatchVectorAssign
    const float* bias_data = GetTensorData<float>(bias);
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size; ++i) {
   e174c:	2400      	movs	r4, #0
   e174e:	ea4f 0e83 	mov.w	lr, r3, lsl #2
   e1752:	4284      	cmp	r4, r0
   e1754:	db0b      	blt.n	e176e <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xde>
   e1756:	9b00      	ldr	r3, [sp, #0]
   e1758:	ea4f 0983 	mov.w	r9, r3, lsl #2
   e175c:	9b10      	ldr	r3, [sp, #64]	; 0x40
        *output_ptr++ = *bias_ptr++;
      }
    }
  } else {
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size * num_units; ++i) {
   e175e:	2500      	movs	r5, #0
   e1760:	ea23 7ee3 	bic.w	lr, r3, r3, asr #31
   e1764:	ea4f 0e8e 	mov.w	lr, lr, lsl #2
   e1768:	462e      	mov	r6, r5
   e176a:	462f      	mov	r7, r5
   e176c:	e021      	b.n	e17b2 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x122>
   e176e:	460e      	mov	r6, r1
    // TODO(kreeger): doc me - VectorBatchVectorAssign
    const float* bias_data = GetTensorData<float>(bias);
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size; ++i) {
      float* output_ptr = output_data + i * num_units;
      const float* bias_ptr = bias_data;
   e1770:	4667      	mov	r7, ip
      for (int j = 0; j < num_units; ++j) {
   e1772:	2500      	movs	r5, #0
   e1774:	9b00      	ldr	r3, [sp, #0]
   e1776:	429d      	cmp	r5, r3
   e1778:	da05      	bge.n	e1786 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xf6>
        *output_ptr++ = *bias_ptr++;
   e177a:	f857 8b04 	ldr.w	r8, [r7], #4
   e177e:	f846 8b04 	str.w	r8, [r6], #4
    const float* bias_data = GetTensorData<float>(bias);
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size; ++i) {
      float* output_ptr = output_data + i * num_units;
      const float* bias_ptr = bias_data;
      for (int j = 0; j < num_units; ++j) {
   e1782:	3501      	adds	r5, #1
   e1784:	e7f6      	b.n	e1774 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xe4>
  // Initialize output with bias if provided.
  if (bias) {
    // TODO(kreeger): doc me - VectorBatchVectorAssign
    const float* bias_data = GetTensorData<float>(bias);
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size; ++i) {
   e1786:	3401      	adds	r4, #1
   e1788:	4471      	add	r1, lr
   e178a:	e7e2      	b.n	e1752 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xc2>
   e178c:	b10a      	cbz	r2, e1792 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x102>
   e178e:	6854      	ldr	r4, [r2, #4]
   e1790:	e000      	b.n	e1794 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x104>
   e1792:	4614      	mov	r4, r2
        *output_ptr++ = *bias_ptr++;
      }
    }
  } else {
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size * num_units; ++i) {
   e1794:	9b00      	ldr	r3, [sp, #0]
   e1796:	2100      	movs	r1, #0
   e1798:	fb03 f500 	mul.w	r5, r3, r0
      *output_data++ = 0.0f;
   e179c:	2600      	movs	r6, #0
        *output_ptr++ = *bias_ptr++;
      }
    }
  } else {
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size * num_units; ++i) {
   e179e:	42a9      	cmp	r1, r5
   e17a0:	dad9      	bge.n	e1756 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xc6>
      *output_data++ = 0.0f;
   e17a2:	f844 6b04 	str.w	r6, [r4], #4
        *output_ptr++ = *bias_ptr++;
      }
    }
  } else {
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size * num_units; ++i) {
   e17a6:	3101      	adds	r1, #1
   e17a8:	e7f9      	b.n	e179e <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x10e>
   e17aa:	9b04      	ldr	r3, [sp, #16]
      *output_data++ = 0.0f;
    }
  }

  // Reduction sum.
  for (int b = 0; b < batch_size; ++b) {
   e17ac:	3701      	adds	r7, #1
   e17ae:	441e      	add	r6, r3
   e17b0:	444d      	add	r5, r9
   e17b2:	4287      	cmp	r7, r0
   e17b4:	da25      	bge.n	e1802 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x172>
   e17b6:	b10a      	cbz	r2, e17bc <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x12c>
   e17b8:	6851      	ldr	r1, [r2, #4]
   e17ba:	e000      	b.n	e17be <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x12e>
   e17bc:	4611      	mov	r1, r2
   e17be:	9b15      	ldr	r3, [sp, #84]	; 0x54
   e17c0:	b10b      	cbz	r3, e17c6 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x136>
   e17c2:	685c      	ldr	r4, [r3, #4]
   e17c4:	e000      	b.n	e17c8 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x138>
   e17c6:	9c15      	ldr	r4, [sp, #84]	; 0x54
    float* output_ptr_batch = GetTensorData<float>(output) + b * num_units;
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;
   e17c8:	4434      	add	r4, r6
   e17ca:	4429      	add	r1, r5

    // Reduction sum vector
    const float* input_vector_ptr = scratch_ptr_batch;
    for (int i = 0; i < num_units; ++i) {
   e17cc:	f04f 0c00 	mov.w	ip, #0
   e17d0:	9b00      	ldr	r3, [sp, #0]
   e17d2:	459c      	cmp	ip, r3
   e17d4:	dae9      	bge.n	e17aa <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x11a>
   e17d6:	46a2      	mov	sl, r4
   e17d8:	f04f 0800 	mov.w	r8, #0
      for (int j = 0; j < rank; j++) {
   e17dc:	9b10      	ldr	r3, [sp, #64]	; 0x40
   e17de:	4598      	cmp	r8, r3
   e17e0:	da0a      	bge.n	e17f8 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x168>
        output_ptr_batch[i] += *input_vector_ptr++;
   e17e2:	edd1 7a00 	vldr	s15, [r1]
   e17e6:	ecba 7a01 	vldmia	sl!, {s14}
   e17ea:	ee77 7a87 	vadd.f32	s15, s15, s14
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;

    // Reduction sum vector
    const float* input_vector_ptr = scratch_ptr_batch;
    for (int i = 0; i < num_units; ++i) {
      for (int j = 0; j < rank; j++) {
   e17ee:	f108 0801 	add.w	r8, r8, #1
        output_ptr_batch[i] += *input_vector_ptr++;
   e17f2:	edc1 7a00 	vstr	s15, [r1]
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;

    // Reduction sum vector
    const float* input_vector_ptr = scratch_ptr_batch;
    for (int i = 0; i < num_units; ++i) {
      for (int j = 0; j < rank; j++) {
   e17f6:	e7f1      	b.n	e17dc <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x14c>
   e17f8:	4474      	add	r4, lr
    float* output_ptr_batch = GetTensorData<float>(output) + b * num_units;
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;

    // Reduction sum vector
    const float* input_vector_ptr = scratch_ptr_batch;
    for (int i = 0; i < num_units; ++i) {
   e17fa:	f10c 0c01 	add.w	ip, ip, #1
   e17fe:	3104      	adds	r1, #4
   e1800:	e7e6      	b.n	e17d0 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x140>
   e1802:	2400      	movs	r4, #0
inline float ActivationValFloat(TfLiteFusedActivation act, float a) {
  switch (act) {
    case kTfLiteActNone:
      return a;
    case kTfLiteActRelu:
      return a < 0.f ? 0.f : a;
   e1804:	ed9f 7a24 	vldr	s14, [pc, #144]	; e1898 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x208>
   e1808:	4625      	mov	r5, r4
      }
    }
  }

  // Apply activation.
  for (int b = 0; b < batch_size; ++b) {
   e180a:	4285      	cmp	r5, r0
   e180c:	da20      	bge.n	e1850 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1c0>
   e180e:	b10a      	cbz	r2, e1814 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x184>
   e1810:	6851      	ldr	r1, [r2, #4]
   e1812:	e000      	b.n	e1816 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x186>
   e1814:	4611      	mov	r1, r2
    float* output_ptr_batch = GetTensorData<float>(output) + b * num_units;
   e1816:	4421      	add	r1, r4
    for (int i = 0; i < num_units; ++i) {
   e1818:	2600      	movs	r6, #0
   e181a:	9b00      	ldr	r3, [sp, #0]
   e181c:	429e      	cmp	r6, r3
   e181e:	da14      	bge.n	e184a <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1ba>
namespace ops {
namespace micro {

// Returns the floating point value for a fused activation:
inline float ActivationValFloat(TfLiteFusedActivation act, float a) {
  switch (act) {
   e1820:	9b05      	ldr	r3, [sp, #20]
      *output_ptr_batch = ActivationValFloat(activation, *output_ptr_batch);
   e1822:	edd1 7a00 	vldr	s15, [r1]
   e1826:	b163      	cbz	r3, e1842 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1b2>
   e1828:	2b01      	cmp	r3, #1
   e182a:	d107      	bne.n	e183c <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1ac>
    case kTfLiteActNone:
      return a;
    case kTfLiteActRelu:
      return a < 0.f ? 0.f : a;
   e182c:	eef5 7a40 	vcmp.f32	s15, #0.0
   e1830:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e1834:	bf48      	it	mi
   e1836:	eef0 7a47 	vmovmi.f32	s15, s14
   e183a:	e002      	b.n	e1842 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1b2>
    default:
      // TODO(kreeger): Implement more activations.
      exit(1);
   e183c:	2001      	movs	r0, #1
   e183e:	f005 ffad 	bl	e779c <exit>
   e1842:	ece1 7a01 	vstmia	r1!, {s15}
  }

  // Apply activation.
  for (int b = 0; b < batch_size; ++b) {
    float* output_ptr_batch = GetTensorData<float>(output) + b * num_units;
    for (int i = 0; i < num_units; ++i) {
   e1846:	3601      	adds	r6, #1
   e1848:	e7e7      	b.n	e181a <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x18a>
      }
    }
  }

  // Apply activation.
  for (int b = 0; b < batch_size; ++b) {
   e184a:	3501      	adds	r5, #1
   e184c:	444c      	add	r4, r9
   e184e:	e7dc      	b.n	e180a <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x17a>
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int f = 0; f < num_filters; ++f) {
      // Shift the vector left:
      float* batch_ptr = state_ptr_batch;
      float* batch_start = state_ptr_batch + 1;
      float* batch_end = state_ptr_batch + memory_size;
   e1850:	9b01      	ldr	r3, [sp, #4]
      while (batch_start != batch_end) {
        *batch_ptr++ = *batch_start++;
      }
      state_ptr_batch[memory_size - 1] = 0.0f;
   e1852:	2200      	movs	r2, #0
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int f = 0; f < num_filters; ++f) {
      // Shift the vector left:
      float* batch_ptr = state_ptr_batch;
      float* batch_start = state_ptr_batch + 1;
      float* batch_end = state_ptr_batch + memory_size;
   e1854:	0099      	lsls	r1, r3, #2
      while (batch_start != batch_end) {
        *batch_ptr++ = *batch_start++;
      }
      state_ptr_batch[memory_size - 1] = 0.0f;
   e1856:	4615      	mov	r5, r2
   e1858:	2700      	movs	r7, #0
    }
  }

  // Left shift the activation_state to make room for next cycle's activation.
  // TODO(alanchiao): explore collapsing this into a single loop.
  for (int b = 0; b < batch_size; ++b) {
   e185a:	4285      	cmp	r5, r0
   e185c:	da19      	bge.n	e1892 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x202>
   e185e:	9b14      	ldr	r3, [sp, #80]	; 0x50
   e1860:	b10b      	cbz	r3, e1866 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1d6>
   e1862:	685b      	ldr	r3, [r3, #4]
   e1864:	e000      	b.n	e1868 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1d8>
   e1866:	9b14      	ldr	r3, [sp, #80]	; 0x50
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
   e1868:	4413      	add	r3, r2
    for (int f = 0; f < num_filters; ++f) {
   e186a:	2600      	movs	r6, #0
   e186c:	455e      	cmp	r6, fp
   e186e:	da0c      	bge.n	e188a <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1fa>
      // Shift the vector left:
      float* batch_ptr = state_ptr_batch;
      float* batch_start = state_ptr_batch + 1;
   e1870:	1d1c      	adds	r4, r3, #4
      float* batch_end = state_ptr_batch + memory_size;
   e1872:	440b      	add	r3, r1
      while (batch_start != batch_end) {
   e1874:	429c      	cmp	r4, r3
   e1876:	d004      	beq.n	e1882 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1f2>
        *batch_ptr++ = *batch_start++;
   e1878:	f854 eb04 	ldr.w	lr, [r4], #4
   e187c:	f844 ec08 	str.w	lr, [r4, #-8]
    for (int f = 0; f < num_filters; ++f) {
      // Shift the vector left:
      float* batch_ptr = state_ptr_batch;
      float* batch_start = state_ptr_batch + 1;
      float* batch_end = state_ptr_batch + memory_size;
      while (batch_start != batch_end) {
   e1880:	e7f8      	b.n	e1874 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1e4>
        *batch_ptr++ = *batch_start++;
      }
      state_ptr_batch[memory_size - 1] = 0.0f;
   e1882:	f843 7c04 	str.w	r7, [r3, #-4]
  // Left shift the activation_state to make room for next cycle's activation.
  // TODO(alanchiao): explore collapsing this into a single loop.
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int f = 0; f < num_filters; ++f) {
   e1886:	3601      	adds	r6, #1
   e1888:	e7f0      	b.n	e186c <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1dc>
   e188a:	9b03      	ldr	r3, [sp, #12]
    }
  }

  // Left shift the activation_state to make room for next cycle's activation.
  // TODO(alanchiao): explore collapsing this into a single loop.
  for (int b = 0; b < batch_size; ++b) {
   e188c:	3501      	adds	r5, #1
   e188e:	441a      	add	r2, r3
   e1890:	e7e3      	b.n	e185a <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1ca>
      }
      state_ptr_batch[memory_size - 1] = 0.0f;
      state_ptr_batch += memory_size;
    }
  }
}
   e1892:	b007      	add	sp, #28
   e1894:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e1898:	00000000 	.word	0x00000000

000e189c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode>:
  return nullptr;
}

void Free(TfLiteContext* context, void* buffer) {}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   e189c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  // [4] = Activation State (variable),
  //         {2, batch_size, memory_size * num_filters}
  // TODO(kreeger): Use input tensor as variable until scratch tensor allocation
  // has been implemented (cl/263032056)
  // TF_LITE_ENSURE_EQ(context, node->inputs->size, 5);
  TF_LITE_ENSURE_EQ(context, node->inputs->size, 6);
   e18a0:	680d      	ldr	r5, [r1, #0]
   e18a2:	682b      	ldr	r3, [r5, #0]
   e18a4:	2b06      	cmp	r3, #6
  return nullptr;
}

void Free(TfLiteContext* context, void* buffer) {}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   e18a6:	b087      	sub	sp, #28
   e18a8:	4607      	mov	r7, r0
   e18aa:	4689      	mov	r9, r1
  // [4] = Activation State (variable),
  //         {2, batch_size, memory_size * num_filters}
  // TODO(kreeger): Use input tensor as variable until scratch tensor allocation
  // has been implemented (cl/263032056)
  // TF_LITE_ENSURE_EQ(context, node->inputs->size, 5);
  TF_LITE_ENSURE_EQ(context, node->inputs->size, 6);
   e18ac:	d00e      	beq.n	e18cc <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x30>
   e18ae:	9302      	str	r3, [sp, #8]
   e18b0:	4b9f      	ldr	r3, [pc, #636]	; (e1b30 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x294>)
   e18b2:	9301      	str	r3, [sp, #4]
   e18b4:	2206      	movs	r2, #6
   e18b6:	4b9f      	ldr	r3, [pc, #636]	; (e1b34 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x298>)
   e18b8:	9203      	str	r2, [sp, #12]
   e18ba:	9300      	str	r3, [sp, #0]
   e18bc:	6944      	ldr	r4, [r0, #20]
   e18be:	4a9e      	ldr	r2, [pc, #632]	; (e1b38 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x29c>)
   e18c0:	499e      	ldr	r1, [pc, #632]	; (e1b3c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2a0>)
   e18c2:	f44f 739f 	mov.w	r3, #318	; 0x13e
   e18c6:	47a0      	blx	r4
   e18c8:	2001      	movs	r0, #1
   e18ca:	e297      	b.n	e1dfc <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x560>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e18cc:	68a9      	ldr	r1, [r5, #8]
   e18ce:	6883      	ldr	r3, [r0, #8]
   e18d0:	686c      	ldr	r4, [r5, #4]
   e18d2:	2238      	movs	r2, #56	; 0x38
   e18d4:	4351      	muls	r1, r2
   e18d6:	1858      	adds	r0, r3, r1
   e18d8:	9105      	str	r1, [sp, #20]
}

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
   e18da:	6929      	ldr	r1, [r5, #16]

  // Define input constants based on input tensor definition above:
  const int rank = params->rank;
  const int input_size = input->dims->data[1];
  const int batch_size = input->dims->data[0];
  const int num_filters = weights_feature->dims->data[0];
   e18dc:	6880      	ldr	r0, [r0, #8]
  if (use_tensor) {
   e18de:	f1b1 3fff 	cmp.w	r1, #4294967295	; 0xffffffff
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e18e2:	fb02 f404 	mul.w	r4, r2, r4
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e18e6:	bf18      	it	ne
   e18e8:	fb02 3e01 	mlane	lr, r2, r1, r3
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);
  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];

  // Define input constants based on input tensor definition above:
  const int rank = params->rank;
   e18ec:	f8d9 2014 	ldr.w	r2, [r9, #20]
  const int input_size = input->dims->data[1];
  const int batch_size = input->dims->data[0];
  const int num_filters = weights_feature->dims->data[0];
   e18f0:	6841      	ldr	r1, [r0, #4]
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);
  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];

  // Define input constants based on input tensor definition above:
  const int rank = params->rank;
   e18f2:	6812      	ldr	r2, [r2, #0]
  const int input_size = input->dims->data[1];
  const int batch_size = input->dims->data[0];
  const int num_filters = weights_feature->dims->data[0];
  TF_LITE_ENSURE_EQ(context, num_filters % rank, 0);
   e18f4:	fb91 fbf2 	sdiv	fp, r1, r2
   e18f8:	fb02 121b 	mls	r2, r2, fp, r1
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e18fc:	eb03 0604 	add.w	r6, r3, r4
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
  }
  return nullptr;
   e1900:	bf08      	it	eq
   e1902:	f04f 0e00 	moveq.w	lr, #0
   e1906:	b152      	cbz	r2, e191e <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x82>
   e1908:	2300      	movs	r3, #0
   e190a:	9303      	str	r3, [sp, #12]
   e190c:	4b8c      	ldr	r3, [pc, #560]	; (e1b40 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2a4>)
   e190e:	9301      	str	r3, [sp, #4]
   e1910:	4b8c      	ldr	r3, [pc, #560]	; (e1b44 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2a8>)
   e1912:	9300      	str	r3, [sp, #0]
   e1914:	9202      	str	r2, [sp, #8]
   e1916:	697c      	ldr	r4, [r7, #20]
   e1918:	f240 134d 	movw	r3, #333	; 0x14d
   e191c:	e21e      	b.n	e1d5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
  const int num_units = num_filters / rank;
  const int memory_size = weights_time->dims->data[1];

  // Validate Input Tensor:
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
   e191e:	5d1c      	ldrb	r4, [r3, r4]
   e1920:	2c01      	cmp	r4, #1
   e1922:	d00a      	beq.n	e193a <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x9e>
   e1924:	4b88      	ldr	r3, [pc, #544]	; (e1b48 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2ac>)
   e1926:	9301      	str	r3, [sp, #4]
   e1928:	2501      	movs	r5, #1
   e192a:	4b88      	ldr	r3, [pc, #544]	; (e1b4c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b0>)
   e192c:	9300      	str	r3, [sp, #0]
   e192e:	9503      	str	r5, [sp, #12]
   e1930:	9402      	str	r4, [sp, #8]
   e1932:	697c      	ldr	r4, [r7, #20]
   e1934:	f44f 73a9 	mov.w	r3, #338	; 0x152
   e1938:	e210      	b.n	e1d5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];

  // Define input constants based on input tensor definition above:
  const int rank = params->rank;
  const int input_size = input->dims->data[1];
   e193a:	68b6      	ldr	r6, [r6, #8]
#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/c_api_internal.h"

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
   e193c:	6832      	ldr	r2, [r6, #0]
  const int num_units = num_filters / rank;
  const int memory_size = weights_time->dims->data[1];

  // Validate Input Tensor:
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 2);
   e193e:	2a02      	cmp	r2, #2
   e1940:	d00a      	beq.n	e1958 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0xbc>
   e1942:	2302      	movs	r3, #2
   e1944:	9303      	str	r3, [sp, #12]
   e1946:	4b82      	ldr	r3, [pc, #520]	; (e1b50 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b4>)
   e1948:	9301      	str	r3, [sp, #4]
   e194a:	4b82      	ldr	r3, [pc, #520]	; (e1b54 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b8>)
   e194c:	9300      	str	r3, [sp, #0]
   e194e:	9202      	str	r2, [sp, #8]
   e1950:	697d      	ldr	r5, [r7, #20]
   e1952:	f240 1353 	movw	r3, #339	; 0x153
   e1956:	e22f      	b.n	e1db8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
   e1958:	f8d0 8000 	ldr.w	r8, [r0]

  // Validate Weights Feature Input Tensor:
  TF_LITE_ENSURE_EQ(context, NumDimensions(weights_feature), 2);
   e195c:	f1b8 0f02 	cmp.w	r8, #2
   e1960:	d00a      	beq.n	e1978 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0xdc>
   e1962:	4b7b      	ldr	r3, [pc, #492]	; (e1b50 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b4>)
   e1964:	9301      	str	r3, [sp, #4]
   e1966:	4b7c      	ldr	r3, [pc, #496]	; (e1b58 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2bc>)
   e1968:	9300      	str	r3, [sp, #0]
   e196a:	9203      	str	r2, [sp, #12]
   e196c:	f8cd 8008 	str.w	r8, [sp, #8]
   e1970:	697d      	ldr	r5, [r7, #20]
   e1972:	f44f 73ab 	mov.w	r3, #342	; 0x156
   e1976:	e21f      	b.n	e1db8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];

  // Define input constants based on input tensor definition above:
  const int rank = params->rank;
  const int input_size = input->dims->data[1];
   e1978:	f8d6 c008 	ldr.w	ip, [r6, #8]
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 2);

  // Validate Weights Feature Input Tensor:
  TF_LITE_ENSURE_EQ(context, NumDimensions(weights_feature), 2);
  TF_LITE_ENSURE_EQ(context, weights_feature->dims->data[1], input_size);
   e197c:	6882      	ldr	r2, [r0, #8]
   e197e:	4594      	cmp	ip, r2
   e1980:	d00a      	beq.n	e1998 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0xfc>
   e1982:	4b76      	ldr	r3, [pc, #472]	; (e1b5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2c0>)
   e1984:	9301      	str	r3, [sp, #4]
   e1986:	4b76      	ldr	r3, [pc, #472]	; (e1b60 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2c4>)
   e1988:	9300      	str	r3, [sp, #0]
   e198a:	f8cd c00c 	str.w	ip, [sp, #12]
   e198e:	9202      	str	r2, [sp, #8]
   e1990:	697d      	ldr	r5, [r7, #20]
   e1992:	f240 1357 	movw	r3, #343	; 0x157
   e1996:	e20f      	b.n	e1db8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e1998:	68ea      	ldr	r2, [r5, #12]
   e199a:	f04f 0c38 	mov.w	ip, #56	; 0x38
   e199e:	fb0c fc02 	mul.w	ip, ip, r2
   e19a2:	eb03 020c 	add.w	r2, r3, ip
   e19a6:	9204      	str	r2, [sp, #16]
  const int input_size = input->dims->data[1];
  const int batch_size = input->dims->data[0];
  const int num_filters = weights_feature->dims->data[0];
  TF_LITE_ENSURE_EQ(context, num_filters % rank, 0);
  const int num_units = num_filters / rank;
  const int memory_size = weights_time->dims->data[1];
   e19a8:	6890      	ldr	r0, [r2, #8]
#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/c_api_internal.h"

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
   e19aa:	6802      	ldr	r2, [r0, #0]
  // Validate Weights Feature Input Tensor:
  TF_LITE_ENSURE_EQ(context, NumDimensions(weights_feature), 2);
  TF_LITE_ENSURE_EQ(context, weights_feature->dims->data[1], input_size);

  // Validate Weights Time Input Tensor:
  TF_LITE_ENSURE_EQ(context, NumDimensions(weights_time), 2);
   e19ac:	2a02      	cmp	r2, #2
   e19ae:	d00a      	beq.n	e19c6 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x12a>
   e19b0:	4b67      	ldr	r3, [pc, #412]	; (e1b50 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b4>)
   e19b2:	9301      	str	r3, [sp, #4]
   e19b4:	4b6b      	ldr	r3, [pc, #428]	; (e1b64 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2c8>)
   e19b6:	9300      	str	r3, [sp, #0]
   e19b8:	f8cd 800c 	str.w	r8, [sp, #12]
   e19bc:	9202      	str	r2, [sp, #8]
   e19be:	697d      	ldr	r5, [r7, #20]
   e19c0:	f44f 73ad 	mov.w	r3, #346	; 0x15a
   e19c4:	e1f8      	b.n	e1db8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  TF_LITE_ENSURE_EQ(context, weights_time->dims->data[0], num_filters);
   e19c6:	6842      	ldr	r2, [r0, #4]
   e19c8:	4291      	cmp	r1, r2
   e19ca:	d009      	beq.n	e19e0 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x144>
   e19cc:	4b66      	ldr	r3, [pc, #408]	; (e1b68 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2cc>)
   e19ce:	9301      	str	r3, [sp, #4]
   e19d0:	4b66      	ldr	r3, [pc, #408]	; (e1b6c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2d0>)
   e19d2:	9300      	str	r3, [sp, #0]
   e19d4:	9103      	str	r1, [sp, #12]
   e19d6:	9202      	str	r2, [sp, #8]
   e19d8:	697d      	ldr	r5, [r7, #20]
   e19da:	f240 135b 	movw	r3, #347	; 0x15b
   e19de:	e1eb      	b.n	e1db8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  TF_LITE_ENSURE_EQ(context, weights_time->dims->data[1], memory_size);

  // Validate Optional Bias Input Tensor:
  if (bias) {
   e19e0:	f1be 0f00 	cmp.w	lr, #0
   e19e4:	d01e      	beq.n	e1a24 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x188>
    TF_LITE_ENSURE_EQ(context, bias->dims->data[0], num_units);
   e19e6:	f8de 2008 	ldr.w	r2, [lr, #8]
   e19ea:	6852      	ldr	r2, [r2, #4]
   e19ec:	4593      	cmp	fp, r2
   e19ee:	d00a      	beq.n	e1a06 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x16a>
   e19f0:	4b5f      	ldr	r3, [pc, #380]	; (e1b70 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2d4>)
   e19f2:	9301      	str	r3, [sp, #4]
   e19f4:	4b5f      	ldr	r3, [pc, #380]	; (e1b74 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2d8>)
   e19f6:	9300      	str	r3, [sp, #0]
   e19f8:	f8cd b00c 	str.w	fp, [sp, #12]
   e19fc:	9202      	str	r2, [sp, #8]
   e19fe:	697d      	ldr	r5, [r7, #20]
   e1a00:	f44f 73b0 	mov.w	r3, #352	; 0x160
   e1a04:	e1d8      	b.n	e1db8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
    TF_LITE_ENSURE_EQ(context, bias->type, kTfLiteFloat32);
   e1a06:	f89e 2000 	ldrb.w	r2, [lr]
   e1a0a:	2a01      	cmp	r2, #1
   e1a0c:	d00a      	beq.n	e1a24 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x188>
   e1a0e:	4b4e      	ldr	r3, [pc, #312]	; (e1b48 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2ac>)
   e1a10:	9301      	str	r3, [sp, #4]
   e1a12:	2401      	movs	r4, #1
   e1a14:	4b58      	ldr	r3, [pc, #352]	; (e1b78 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2dc>)
   e1a16:	9300      	str	r3, [sp, #0]
   e1a18:	9403      	str	r4, [sp, #12]
   e1a1a:	9202      	str	r2, [sp, #8]
   e1a1c:	697d      	ldr	r5, [r7, #20]
   e1a1e:	f240 1361 	movw	r3, #353	; 0x161
   e1a22:	e1c9      	b.n	e1db8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
      GetInput(context, node, kWeightsFeatureTensor);
  const TfLiteTensor* weights_time =
      GetInput(context, node, kWeightsTimeTensor);
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);
  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];
   e1a24:	696a      	ldr	r2, [r5, #20]
   e1a26:	f04f 0a38 	mov.w	sl, #56	; 0x38
   e1a2a:	fb0a f202 	mul.w	r2, sl, r2
   e1a2e:	eb03 0e02 	add.w	lr, r3, r2
    TF_LITE_ENSURE_EQ(context, bias->dims->data[0], num_units);
    TF_LITE_ENSURE_EQ(context, bias->type, kTfLiteFloat32);
  }

  // Validate Activation State Input Tensor:
  TF_LITE_ENSURE_EQ(context, activation_state->type, kTfLiteFloat32);
   e1a32:	5c9c      	ldrb	r4, [r3, r2]
   e1a34:	2c01      	cmp	r4, #1
   e1a36:	d00a      	beq.n	e1a4e <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x1b2>
   e1a38:	4b43      	ldr	r3, [pc, #268]	; (e1b48 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2ac>)
   e1a3a:	9301      	str	r3, [sp, #4]
   e1a3c:	2501      	movs	r5, #1
   e1a3e:	4b4f      	ldr	r3, [pc, #316]	; (e1b7c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2e0>)
   e1a40:	9300      	str	r3, [sp, #0]
   e1a42:	9503      	str	r5, [sp, #12]
   e1a44:	9402      	str	r4, [sp, #8]
   e1a46:	697c      	ldr	r4, [r7, #20]
   e1a48:	f240 1365 	movw	r3, #357	; 0x165
   e1a4c:	e186      	b.n	e1d5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
   e1a4e:	f8de e008 	ldr.w	lr, [lr, #8]
   e1a52:	f8de 2000 	ldr.w	r2, [lr]
  TF_LITE_ENSURE_EQ(context, NumDimensions(activation_state), 2);
   e1a56:	2a02      	cmp	r2, #2
   e1a58:	d00a      	beq.n	e1a70 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x1d4>
   e1a5a:	2302      	movs	r3, #2
   e1a5c:	9303      	str	r3, [sp, #12]
   e1a5e:	4b3c      	ldr	r3, [pc, #240]	; (e1b50 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b4>)
   e1a60:	9301      	str	r3, [sp, #4]
   e1a62:	4b47      	ldr	r3, [pc, #284]	; (e1b80 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2e4>)
   e1a64:	9300      	str	r3, [sp, #0]
   e1a66:	9202      	str	r2, [sp, #8]
   e1a68:	697d      	ldr	r5, [r7, #20]
   e1a6a:	f44f 73b3 	mov.w	r3, #358	; 0x166
   e1a6e:	e1a3      	b.n	e1db8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];

  // Define input constants based on input tensor definition above:
  const int rank = params->rank;
  const int input_size = input->dims->data[1];
  const int batch_size = input->dims->data[0];
   e1a70:	f8d6 8004 	ldr.w	r8, [r6, #4]
  }

  // Validate Activation State Input Tensor:
  TF_LITE_ENSURE_EQ(context, activation_state->type, kTfLiteFloat32);
  TF_LITE_ENSURE_EQ(context, NumDimensions(activation_state), 2);
  TF_LITE_ENSURE_EQ(context, activation_state->dims->data[0], batch_size);
   e1a74:	f8de 6004 	ldr.w	r6, [lr, #4]
   e1a78:	45b0      	cmp	r8, r6
   e1a7a:	d00a      	beq.n	e1a92 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x1f6>
   e1a7c:	4b41      	ldr	r3, [pc, #260]	; (e1b84 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2e8>)
   e1a7e:	9301      	str	r3, [sp, #4]
   e1a80:	4b41      	ldr	r3, [pc, #260]	; (e1b88 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2ec>)
   e1a82:	9300      	str	r3, [sp, #0]
   e1a84:	f8cd 800c 	str.w	r8, [sp, #12]
   e1a88:	9602      	str	r6, [sp, #8]
   e1a8a:	697d      	ldr	r5, [r7, #20]
   e1a8c:	f240 1367 	movw	r3, #359	; 0x167
   e1a90:	e192      	b.n	e1db8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  const int input_size = input->dims->data[1];
  const int batch_size = input->dims->data[0];
  const int num_filters = weights_feature->dims->data[0];
  TF_LITE_ENSURE_EQ(context, num_filters % rank, 0);
  const int num_units = num_filters / rank;
  const int memory_size = weights_time->dims->data[1];
   e1a92:	6880      	ldr	r0, [r0, #8]

  // Validate Activation State Input Tensor:
  TF_LITE_ENSURE_EQ(context, activation_state->type, kTfLiteFloat32);
  TF_LITE_ENSURE_EQ(context, NumDimensions(activation_state), 2);
  TF_LITE_ENSURE_EQ(context, activation_state->dims->data[0], batch_size);
  TF_LITE_ENSURE_EQ(context, activation_state->dims->data[1],
   e1a94:	f8de 6008 	ldr.w	r6, [lr, #8]
   e1a98:	fb00 fe01 	mul.w	lr, r0, r1
   e1a9c:	4576      	cmp	r6, lr
   e1a9e:	d00a      	beq.n	e1ab6 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x21a>
   e1aa0:	4b3a      	ldr	r3, [pc, #232]	; (e1b8c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2f0>)
   e1aa2:	9301      	str	r3, [sp, #4]
   e1aa4:	4b3a      	ldr	r3, [pc, #232]	; (e1b90 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2f4>)
   e1aa6:	9300      	str	r3, [sp, #0]
   e1aa8:	f8cd e00c 	str.w	lr, [sp, #12]
   e1aac:	9602      	str	r6, [sp, #8]
   e1aae:	697d      	ldr	r5, [r7, #20]
   e1ab0:	f240 1369 	movw	r3, #361	; 0x169
   e1ab4:	e180      	b.n	e1db8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  //       ApplyTimeWeightsBiasAndActivation():
  //         float, {2, batch_size, num_filters}
  // TODO(kreeger): Use input tensor as variable until scratch tensor allocation
  // has been implemented (cl/263032056)
  // TfLiteTensor* scratch_tensor = GetTemporary(context, node, 0);
  TfLiteTensor* scratch_tensor = &context->tensors[node->inputs->data[5]];
   e1ab6:	69ad      	ldr	r5, [r5, #24]
   e1ab8:	fb0a fa05 	mul.w	sl, sl, r5
   e1abc:	eb03 060a 	add.w	r6, r3, sl

  TF_LITE_ENSURE_EQ(context, scratch_tensor->type, kTfLiteFloat32);
   e1ac0:	f813 500a 	ldrb.w	r5, [r3, sl]
   e1ac4:	2d01      	cmp	r5, #1
   e1ac6:	d009      	beq.n	e1adc <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x240>
   e1ac8:	4b1f      	ldr	r3, [pc, #124]	; (e1b48 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2ac>)
   e1aca:	9301      	str	r3, [sp, #4]
   e1acc:	4b31      	ldr	r3, [pc, #196]	; (e1b94 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2f8>)
   e1ace:	9300      	str	r3, [sp, #0]
   e1ad0:	9403      	str	r4, [sp, #12]
   e1ad2:	9502      	str	r5, [sp, #8]
   e1ad4:	697d      	ldr	r5, [r7, #20]
   e1ad6:	f44f 73ba 	mov.w	r3, #372	; 0x174
   e1ada:	e16d      	b.n	e1db8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
   e1adc:	68b4      	ldr	r4, [r6, #8]
   e1ade:	6826      	ldr	r6, [r4, #0]
  TF_LITE_ENSURE_EQ(context, NumDimensions(scratch_tensor), 2);
   e1ae0:	2e02      	cmp	r6, #2
   e1ae2:	d009      	beq.n	e1af8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x25c>
   e1ae4:	4b1a      	ldr	r3, [pc, #104]	; (e1b50 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b4>)
   e1ae6:	9301      	str	r3, [sp, #4]
   e1ae8:	4b2b      	ldr	r3, [pc, #172]	; (e1b98 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2fc>)
   e1aea:	9300      	str	r3, [sp, #0]
   e1aec:	9203      	str	r2, [sp, #12]
   e1aee:	9602      	str	r6, [sp, #8]
   e1af0:	697c      	ldr	r4, [r7, #20]
   e1af2:	f240 1375 	movw	r3, #373	; 0x175
   e1af6:	e131      	b.n	e1d5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
  TF_LITE_ENSURE_EQ(context, scratch_tensor->dims->data[0], batch_size);
   e1af8:	6862      	ldr	r2, [r4, #4]
   e1afa:	4590      	cmp	r8, r2
   e1afc:	d00a      	beq.n	e1b14 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x278>
   e1afe:	4b21      	ldr	r3, [pc, #132]	; (e1b84 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2e8>)
   e1b00:	9301      	str	r3, [sp, #4]
   e1b02:	4b26      	ldr	r3, [pc, #152]	; (e1b9c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x300>)
   e1b04:	9300      	str	r3, [sp, #0]
   e1b06:	f8cd 800c 	str.w	r8, [sp, #12]
   e1b0a:	9202      	str	r2, [sp, #8]
   e1b0c:	697c      	ldr	r4, [r7, #20]
   e1b0e:	f44f 73bb 	mov.w	r3, #374	; 0x176
   e1b12:	e123      	b.n	e1d5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
  TF_LITE_ENSURE_EQ(context, scratch_tensor->dims->data[1], num_filters);
   e1b14:	68a2      	ldr	r2, [r4, #8]
   e1b16:	4291      	cmp	r1, r2
   e1b18:	d044      	beq.n	e1ba4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x308>
   e1b1a:	4b13      	ldr	r3, [pc, #76]	; (e1b68 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2cc>)
   e1b1c:	9301      	str	r3, [sp, #4]
   e1b1e:	4b20      	ldr	r3, [pc, #128]	; (e1ba0 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x304>)
   e1b20:	9300      	str	r3, [sp, #0]
   e1b22:	9103      	str	r1, [sp, #12]
   e1b24:	9202      	str	r2, [sp, #8]
   e1b26:	697c      	ldr	r4, [r7, #20]
   e1b28:	f240 1377 	movw	r3, #375	; 0x177
   e1b2c:	e116      	b.n	e1d5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
   e1b2e:	bf00      	nop
   e1b30:	000e78f0 	.word	0x000e78f0
   e1b34:	000eaa9e 	.word	0x000eaa9e
   e1b38:	000ea9f8 	.word	0x000ea9f8
   e1b3c:	000e98f8 	.word	0x000e98f8
   e1b40:	000eb1bd 	.word	0x000eb1bd
   e1b44:	000eaab1 	.word	0x000eaab1
   e1b48:	000ea167 	.word	0x000ea167
   e1b4c:	000e9933 	.word	0x000e9933
   e1b50:	000ea2a8 	.word	0x000ea2a8
   e1b54:	000ea6fc 	.word	0x000ea6fc
   e1b58:	000eaac4 	.word	0x000eaac4
   e1b5c:	000eaae3 	.word	0x000eaae3
   e1b60:	000eaaee 	.word	0x000eaaee
   e1b64:	000eab0d 	.word	0x000eab0d
   e1b68:	000eabc2 	.word	0x000eabc2
   e1b6c:	000eae07 	.word	0x000eae07
   e1b70:	000eab29 	.word	0x000eab29
   e1b74:	000eab33 	.word	0x000eab33
   e1b78:	000eab47 	.word	0x000eab47
   e1b7c:	000eab52 	.word	0x000eab52
   e1b80:	000eab69 	.word	0x000eab69
   e1b84:	000eab89 	.word	0x000eab89
   e1b88:	000eab94 	.word	0x000eab94
   e1b8c:	000eabb4 	.word	0x000eabb4
   e1b90:	000eabce 	.word	0x000eabce
   e1b94:	000eabee 	.word	0x000eabee
   e1b98:	000eac03 	.word	0x000eac03
   e1b9c:	000eac21 	.word	0x000eac21
   e1ba0:	000eac3f 	.word	0x000eac3f
   e1ba4:	9a05      	ldr	r2, [sp, #20]
   e1ba6:	5c9c      	ldrb	r4, [r3, r2]
}

// Determines whether it is a hybrid op - one that has float inputs and
// quantized weights.
inline bool IsHybridOp(const TfLiteTensor* input, const TfLiteTensor* weight) {
  return ((weight->type == kTfLiteUInt8 || weight->type == kTfLiteInt8) &&
   e1ba8:	2c03      	cmp	r4, #3
   e1baa:	d002      	beq.n	e1bb2 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x316>
   e1bac:	2c09      	cmp	r4, #9
   e1bae:	f040 810a 	bne.w	e1dc6 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x52a>
  // TODO(kreeger): Handle full quant svdf b/139435798
  if (is_hybrid_op) {
    // Validate Input Tensor dtypes:
    TF_LITE_ENSURE(context, weights_feature->type == kTfLiteUInt8 ||
                                weights_feature->type == kTfLiteInt8);
    TF_LITE_ENSURE(context, weights_time->type == kTfLiteUInt8 ||
   e1bb2:	f813 200c 	ldrb.w	r2, [r3, ip]
   e1bb6:	2a03      	cmp	r2, #3
   e1bb8:	d007      	beq.n	e1bca <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x32e>
   e1bba:	2a09      	cmp	r2, #9
   e1bbc:	d005      	beq.n	e1bca <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x32e>
   e1bbe:	4b91      	ldr	r3, [pc, #580]	; (e1e04 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x568>)
   e1bc0:	9300      	str	r3, [sp, #0]
   e1bc2:	697c      	ldr	r4, [r7, #20]
   e1bc4:	f240 1381 	movw	r3, #385	; 0x181
   e1bc8:	e026      	b.n	e1c18 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x37c>
    // Validate Scratch Tensors:
    // [0] = (shared - see above for usage)
    // [1] = Input Quantized, int8_t/uint8_t, {2, batch_size, input_size}
    // [2] = Scaling Factors, float, {1, batch_size}
    // [3] = Float Weights Time, float, {2, num_filters, memory_size}
    TF_LITE_ENSURE_EQ(context, node->temporaries->size, 4);
   e1bca:	f8d9 500c 	ldr.w	r5, [r9, #12]
   e1bce:	682a      	ldr	r2, [r5, #0]
   e1bd0:	2a04      	cmp	r2, #4
   e1bd2:	d00a      	beq.n	e1bea <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x34e>
   e1bd4:	2304      	movs	r3, #4
   e1bd6:	9303      	str	r3, [sp, #12]
   e1bd8:	4b8b      	ldr	r3, [pc, #556]	; (e1e08 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x56c>)
   e1bda:	9301      	str	r3, [sp, #4]
   e1bdc:	4b8b      	ldr	r3, [pc, #556]	; (e1e0c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x570>)
   e1bde:	9300      	str	r3, [sp, #0]
   e1be0:	9202      	str	r2, [sp, #8]
   e1be2:	697c      	ldr	r4, [r7, #20]
   e1be4:	f44f 73c4 	mov.w	r3, #392	; 0x188
   e1be8:	e0b8      	b.n	e1d5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
}
inline TfLiteTensor* GetTemporary(TfLiteContext* context, TfLiteNode* node,
                                  int index) {
  return &context->tensors[flatbuffers::EndianScalar(
      node->temporaries->data[index])];
   e1bea:	68ae      	ldr	r6, [r5, #8]
   e1bec:	68ec      	ldr	r4, [r5, #12]
   e1bee:	692d      	ldr	r5, [r5, #16]
   e1bf0:	2238      	movs	r2, #56	; 0x38
   e1bf2:	4356      	muls	r6, r2
   e1bf4:	4354      	muls	r4, r2
   e1bf6:	436a      	muls	r2, r5
    TfLiteTensor* scratch_input_quantized = GetTemporary(context, node, 1);
    TfLiteTensor* scratch_scaling_factors = GetTemporary(context, node, 2);
    TfLiteTensor* scratch_float_weights_time = GetTemporary(context, node, 3);

    // Validate Input Quantized Scratch Tensor:
    TF_LITE_ENSURE(context, scratch_input_quantized->type == kTfLiteUInt8 ||
   e1bf8:	5d9d      	ldrb	r5, [r3, r6]
   e1bfa:	2d03      	cmp	r5, #3
   e1bfc:	eb03 0a06 	add.w	sl, r3, r6
   e1c00:	eb03 0c04 	add.w	ip, r3, r4
   e1c04:	eb03 0e02 	add.w	lr, r3, r2
   e1c08:	d00b      	beq.n	e1c22 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x386>
   e1c0a:	2d09      	cmp	r5, #9
   e1c0c:	d009      	beq.n	e1c22 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x386>
   e1c0e:	4b80      	ldr	r3, [pc, #512]	; (e1e10 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x574>)
   e1c10:	9300      	str	r3, [sp, #0]
   e1c12:	697c      	ldr	r4, [r7, #20]
   e1c14:	f240 138f 	movw	r3, #399	; 0x18f
   e1c18:	4a7e      	ldr	r2, [pc, #504]	; (e1e14 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x578>)
   e1c1a:	497f      	ldr	r1, [pc, #508]	; (e1e18 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x57c>)
   e1c1c:	4638      	mov	r0, r7
   e1c1e:	47a0      	blx	r4
   e1c20:	e652      	b.n	e18c8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2c>
                                scratch_input_quantized->type == kTfLiteInt8);
    TF_LITE_ENSURE_EQ(context, scratch_input_quantized->dims->data[0],
   e1c22:	f8da 5008 	ldr.w	r5, [sl, #8]
   e1c26:	686d      	ldr	r5, [r5, #4]
   e1c28:	45a8      	cmp	r8, r5
   e1c2a:	d00a      	beq.n	e1c42 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x3a6>
   e1c2c:	4b7b      	ldr	r3, [pc, #492]	; (e1e1c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x580>)
   e1c2e:	9301      	str	r3, [sp, #4]
   e1c30:	4b7b      	ldr	r3, [pc, #492]	; (e1e20 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x584>)
   e1c32:	9300      	str	r3, [sp, #0]
   e1c34:	f8cd 800c 	str.w	r8, [sp, #12]
   e1c38:	9502      	str	r5, [sp, #8]
   e1c3a:	697c      	ldr	r4, [r7, #20]
   e1c3c:	f240 1391 	movw	r3, #401	; 0x191
   e1c40:	e08c      	b.n	e1d5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
                      batch_size);

    // Validate Scaling Factors Scratch Tensor:
    TF_LITE_ENSURE_EQ(context, scratch_scaling_factors->type, kTfLiteFloat32);
   e1c42:	5d1e      	ldrb	r6, [r3, r4]
   e1c44:	2e01      	cmp	r6, #1
   e1c46:	d00a      	beq.n	e1c5e <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x3c2>
   e1c48:	4b76      	ldr	r3, [pc, #472]	; (e1e24 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x588>)
   e1c4a:	9301      	str	r3, [sp, #4]
   e1c4c:	2401      	movs	r4, #1
   e1c4e:	4b76      	ldr	r3, [pc, #472]	; (e1e28 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x58c>)
   e1c50:	9300      	str	r3, [sp, #0]
   e1c52:	9403      	str	r4, [sp, #12]
   e1c54:	9602      	str	r6, [sp, #8]
   e1c56:	697d      	ldr	r5, [r7, #20]
   e1c58:	f44f 73ca 	mov.w	r3, #404	; 0x194
   e1c5c:	e0ac      	b.n	e1db8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
   e1c5e:	f8dc 4008 	ldr.w	r4, [ip, #8]
#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/c_api_internal.h"

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
   e1c62:	6825      	ldr	r5, [r4, #0]
    TF_LITE_ENSURE_EQ(context, NumDimensions(scratch_scaling_factors), 1);
   e1c64:	2d01      	cmp	r5, #1
   e1c66:	d009      	beq.n	e1c7c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x3e0>
   e1c68:	4b70      	ldr	r3, [pc, #448]	; (e1e2c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x590>)
   e1c6a:	9301      	str	r3, [sp, #4]
   e1c6c:	4b70      	ldr	r3, [pc, #448]	; (e1e30 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x594>)
   e1c6e:	9300      	str	r3, [sp, #0]
   e1c70:	9603      	str	r6, [sp, #12]
   e1c72:	9502      	str	r5, [sp, #8]
   e1c74:	697c      	ldr	r4, [r7, #20]
   e1c76:	f240 1395 	movw	r3, #405	; 0x195
   e1c7a:	e06f      	b.n	e1d5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
    TF_LITE_ENSURE_EQ(context, scratch_scaling_factors->dims->data[0],
   e1c7c:	6864      	ldr	r4, [r4, #4]
   e1c7e:	45a0      	cmp	r8, r4
   e1c80:	d00a      	beq.n	e1c98 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x3fc>
   e1c82:	4b66      	ldr	r3, [pc, #408]	; (e1e1c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x580>)
   e1c84:	9301      	str	r3, [sp, #4]
   e1c86:	4b6b      	ldr	r3, [pc, #428]	; (e1e34 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x598>)
   e1c88:	9300      	str	r3, [sp, #0]
   e1c8a:	f8cd 800c 	str.w	r8, [sp, #12]
   e1c8e:	9402      	str	r4, [sp, #8]
   e1c90:	697c      	ldr	r4, [r7, #20]
   e1c92:	f240 1397 	movw	r3, #407	; 0x197
   e1c96:	e061      	b.n	e1d5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
                      batch_size);

    // Validate Float Weights Time Scratch Tensor:
    TF_LITE_ENSURE_EQ(context, scratch_float_weights_time->type,
   e1c98:	5c9c      	ldrb	r4, [r3, r2]
   e1c9a:	2c01      	cmp	r4, #1
   e1c9c:	d009      	beq.n	e1cb2 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x416>
   e1c9e:	4b61      	ldr	r3, [pc, #388]	; (e1e24 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x588>)
   e1ca0:	9301      	str	r3, [sp, #4]
   e1ca2:	4b65      	ldr	r3, [pc, #404]	; (e1e38 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x59c>)
   e1ca4:	9300      	str	r3, [sp, #0]
   e1ca6:	9503      	str	r5, [sp, #12]
   e1ca8:	9402      	str	r4, [sp, #8]
   e1caa:	697c      	ldr	r4, [r7, #20]
   e1cac:	f240 139b 	movw	r3, #411	; 0x19b
   e1cb0:	e054      	b.n	e1d5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
   e1cb2:	f8de 3008 	ldr.w	r3, [lr, #8]
   e1cb6:	681a      	ldr	r2, [r3, #0]
                      kTfLiteFloat32);
    TF_LITE_ENSURE_EQ(context, NumDimensions(scratch_float_weights_time), 2);
   e1cb8:	2a02      	cmp	r2, #2
   e1cba:	d00a      	beq.n	e1cd2 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x436>
   e1cbc:	2302      	movs	r3, #2
   e1cbe:	9303      	str	r3, [sp, #12]
   e1cc0:	4b5e      	ldr	r3, [pc, #376]	; (e1e3c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5a0>)
   e1cc2:	9301      	str	r3, [sp, #4]
   e1cc4:	4b5e      	ldr	r3, [pc, #376]	; (e1e40 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5a4>)
   e1cc6:	9300      	str	r3, [sp, #0]
   e1cc8:	9202      	str	r2, [sp, #8]
   e1cca:	697d      	ldr	r5, [r7, #20]
   e1ccc:	f44f 73ce 	mov.w	r3, #412	; 0x19c
   e1cd0:	e072      	b.n	e1db8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
    TF_LITE_ENSURE_EQ(context, scratch_float_weights_time->dims->data[0],
   e1cd2:	685a      	ldr	r2, [r3, #4]
   e1cd4:	4291      	cmp	r1, r2
   e1cd6:	d009      	beq.n	e1cec <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x450>
   e1cd8:	4b5a      	ldr	r3, [pc, #360]	; (e1e44 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5a8>)
   e1cda:	9301      	str	r3, [sp, #4]
   e1cdc:	4b5a      	ldr	r3, [pc, #360]	; (e1e48 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5ac>)
   e1cde:	9300      	str	r3, [sp, #0]
   e1ce0:	9103      	str	r1, [sp, #12]
   e1ce2:	9202      	str	r2, [sp, #8]
   e1ce4:	697d      	ldr	r5, [r7, #20]
   e1ce6:	f44f 73cf 	mov.w	r3, #414	; 0x19e
   e1cea:	e065      	b.n	e1db8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
                      num_filters);
    TF_LITE_ENSURE_EQ(context, scratch_float_weights_time->dims->data[1],
   e1cec:	689b      	ldr	r3, [r3, #8]
   e1cee:	4298      	cmp	r0, r3
   e1cf0:	d009      	beq.n	e1d06 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x46a>
   e1cf2:	9302      	str	r3, [sp, #8]
   e1cf4:	4b55      	ldr	r3, [pc, #340]	; (e1e4c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5b0>)
   e1cf6:	9301      	str	r3, [sp, #4]
   e1cf8:	4b55      	ldr	r3, [pc, #340]	; (e1e50 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5b4>)
   e1cfa:	9300      	str	r3, [sp, #0]
   e1cfc:	9003      	str	r0, [sp, #12]
   e1cfe:	697d      	ldr	r5, [r7, #20]
   e1d00:	f44f 73d0 	mov.w	r3, #416	; 0x1a0
   e1d04:	e058      	b.n	e1db8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
      weights_time_ptr = GetTensorData<int8_t>(weights_time);
    }
    SymmetricDequantize(weights_time_ptr,
                        NumElements(scratch_float_weights_time),
                        weights_time->params.scale,
                        GetTensorData<float>(scratch_float_weights_time));
   e1d06:	9b04      	ldr	r3, [sp, #16]
   e1d08:	f8de 2004 	ldr.w	r2, [lr, #4]
   e1d0c:	ed93 0a03 	vldr	s0, [r3, #12]
   e1d10:	4341      	muls	r1, r0
   e1d12:	6858      	ldr	r0, [r3, #4]
   e1d14:	f7f4 fb54 	bl	d63c0 <_ZN6tflite19SymmetricDequantizeEPKaifPf>
    // TF_LITE_ENSURE_EQ(context, node->temporaries->size, 1);
  }

  // Validate Tensor Output:
  // [0] = float, {2, batch_size, num_units}
  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);
   e1d18:	f8d9 3004 	ldr.w	r3, [r9, #4]
   e1d1c:	681d      	ldr	r5, [r3, #0]
   e1d1e:	2d01      	cmp	r5, #1
   e1d20:	d00a      	beq.n	e1d38 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x49c>
   e1d22:	4b42      	ldr	r3, [pc, #264]	; (e1e2c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x590>)
   e1d24:	9301      	str	r3, [sp, #4]
   e1d26:	2401      	movs	r4, #1
   e1d28:	4b4a      	ldr	r3, [pc, #296]	; (e1e54 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5b8>)
   e1d2a:	9300      	str	r3, [sp, #0]
   e1d2c:	9403      	str	r4, [sp, #12]
   e1d2e:	9502      	str	r5, [sp, #8]
   e1d30:	697d      	ldr	r5, [r7, #20]
   e1d32:	f44f 73e0 	mov.w	r3, #448	; 0x1c0
   e1d36:	e03f      	b.n	e1db8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   e1d38:	685a      	ldr	r2, [r3, #4]
   e1d3a:	2338      	movs	r3, #56	; 0x38
   e1d3c:	4353      	muls	r3, r2
   e1d3e:	68ba      	ldr	r2, [r7, #8]
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  TF_LITE_ENSURE_EQ(context, output->type, kTfLiteFloat32);
   e1d40:	5cd4      	ldrb	r4, [r2, r3]
   e1d42:	2c01      	cmp	r4, #1
   e1d44:	eb02 0103 	add.w	r1, r2, r3
   e1d48:	d00c      	beq.n	e1d64 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c8>
   e1d4a:	4b36      	ldr	r3, [pc, #216]	; (e1e24 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x588>)
   e1d4c:	9301      	str	r3, [sp, #4]
   e1d4e:	4b42      	ldr	r3, [pc, #264]	; (e1e58 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5bc>)
   e1d50:	9300      	str	r3, [sp, #0]
   e1d52:	9503      	str	r5, [sp, #12]
   e1d54:	9402      	str	r4, [sp, #8]
   e1d56:	697c      	ldr	r4, [r7, #20]
   e1d58:	f44f 73e1 	mov.w	r3, #450	; 0x1c2
   e1d5c:	4a2d      	ldr	r2, [pc, #180]	; (e1e14 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x578>)
   e1d5e:	493f      	ldr	r1, [pc, #252]	; (e1e5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5c0>)
   e1d60:	4638      	mov	r0, r7
   e1d62:	e5b0      	b.n	e18c6 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2a>
   e1d64:	688b      	ldr	r3, [r1, #8]
#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/c_api_internal.h"

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
   e1d66:	681a      	ldr	r2, [r3, #0]
  TF_LITE_ENSURE_EQ(context, NumDimensions(output), 2);
   e1d68:	2a02      	cmp	r2, #2
   e1d6a:	d00a      	beq.n	e1d82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4e6>
   e1d6c:	2302      	movs	r3, #2
   e1d6e:	9303      	str	r3, [sp, #12]
   e1d70:	4b32      	ldr	r3, [pc, #200]	; (e1e3c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5a0>)
   e1d72:	9301      	str	r3, [sp, #4]
   e1d74:	4b3a      	ldr	r3, [pc, #232]	; (e1e60 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5c4>)
   e1d76:	9300      	str	r3, [sp, #0]
   e1d78:	9202      	str	r2, [sp, #8]
   e1d7a:	697d      	ldr	r5, [r7, #20]
   e1d7c:	f240 13c3 	movw	r3, #451	; 0x1c3
   e1d80:	e01a      	b.n	e1db8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  TF_LITE_ENSURE_EQ(context, output->dims->data[0], batch_size);
   e1d82:	685a      	ldr	r2, [r3, #4]
   e1d84:	4590      	cmp	r8, r2
   e1d86:	d00a      	beq.n	e1d9e <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x502>
   e1d88:	4b24      	ldr	r3, [pc, #144]	; (e1e1c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x580>)
   e1d8a:	9301      	str	r3, [sp, #4]
   e1d8c:	4b35      	ldr	r3, [pc, #212]	; (e1e64 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5c8>)
   e1d8e:	9300      	str	r3, [sp, #0]
   e1d90:	f8cd 800c 	str.w	r8, [sp, #12]
   e1d94:	9202      	str	r2, [sp, #8]
   e1d96:	697d      	ldr	r5, [r7, #20]
   e1d98:	f44f 73e2 	mov.w	r3, #452	; 0x1c4
   e1d9c:	e00c      	b.n	e1db8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  TF_LITE_ENSURE_EQ(context, output->dims->data[1], num_units);
   e1d9e:	689b      	ldr	r3, [r3, #8]
   e1da0:	459b      	cmp	fp, r3
   e1da2:	d00e      	beq.n	e1dc2 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x526>
   e1da4:	9302      	str	r3, [sp, #8]
   e1da6:	4b30      	ldr	r3, [pc, #192]	; (e1e68 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5cc>)
   e1da8:	9301      	str	r3, [sp, #4]
   e1daa:	4b30      	ldr	r3, [pc, #192]	; (e1e6c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5d0>)
   e1dac:	9300      	str	r3, [sp, #0]
   e1dae:	f8cd b00c 	str.w	fp, [sp, #12]
   e1db2:	697d      	ldr	r5, [r7, #20]
   e1db4:	f240 13c5 	movw	r3, #453	; 0x1c5
   e1db8:	4a16      	ldr	r2, [pc, #88]	; (e1e14 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x578>)
   e1dba:	4928      	ldr	r1, [pc, #160]	; (e1e5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5c0>)
   e1dbc:	4638      	mov	r0, r7
   e1dbe:	47a8      	blx	r5
   e1dc0:	e582      	b.n	e18c8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2c>

  return kTfLiteOk;
   e1dc2:	2000      	movs	r0, #0
   e1dc4:	e01a      	b.n	e1dfc <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x560>
                        NumElements(scratch_float_weights_time),
                        weights_time->params.scale,
                        GetTensorData<float>(scratch_float_weights_time));
  } else {
    // Validate Input Tensor dtypes:
    TF_LITE_ENSURE_EQ(context, weights_feature->type, kTfLiteFloat32);
   e1dc6:	2c01      	cmp	r4, #1
   e1dc8:	d00a      	beq.n	e1de0 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x544>
   e1dca:	4b16      	ldr	r3, [pc, #88]	; (e1e24 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x588>)
   e1dcc:	9301      	str	r3, [sp, #4]
   e1dce:	2501      	movs	r5, #1
   e1dd0:	4b27      	ldr	r3, [pc, #156]	; (e1e70 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5d4>)
   e1dd2:	9300      	str	r3, [sp, #0]
   e1dd4:	9503      	str	r5, [sp, #12]
   e1dd6:	9402      	str	r4, [sp, #8]
   e1dd8:	697c      	ldr	r4, [r7, #20]
   e1dda:	f44f 73da 	mov.w	r3, #436	; 0x1b4
   e1dde:	e7bd      	b.n	e1d5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
    TF_LITE_ENSURE_EQ(context, weights_time->type, kTfLiteFloat32);
   e1de0:	f813 300c 	ldrb.w	r3, [r3, ip]
   e1de4:	2b01      	cmp	r3, #1
   e1de6:	d097      	beq.n	e1d18 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x47c>
   e1de8:	9302      	str	r3, [sp, #8]
   e1dea:	4b0e      	ldr	r3, [pc, #56]	; (e1e24 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x588>)
   e1dec:	9301      	str	r3, [sp, #4]
   e1dee:	4b21      	ldr	r3, [pc, #132]	; (e1e74 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5d8>)
   e1df0:	9300      	str	r3, [sp, #0]
   e1df2:	9403      	str	r4, [sp, #12]
   e1df4:	697d      	ldr	r5, [r7, #20]
   e1df6:	f240 13b5 	movw	r3, #437	; 0x1b5
   e1dfa:	e7dd      	b.n	e1db8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  TF_LITE_ENSURE_EQ(context, NumDimensions(output), 2);
  TF_LITE_ENSURE_EQ(context, output->dims->data[0], batch_size);
  TF_LITE_ENSURE_EQ(context, output->dims->data[1], num_units);

  return kTfLiteOk;
}
   e1dfc:	b007      	add	sp, #28
   e1dfe:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e1e02:	bf00      	nop
   e1e04:	000eac5d 	.word	0x000eac5d
   e1e08:	000ea9c2 	.word	0x000ea9c2
   e1e0c:	000eaca5 	.word	0x000eaca5
   e1e10:	000eacbd 	.word	0x000eacbd
   e1e14:	000ea9f8 	.word	0x000ea9f8
   e1e18:	000e9ac8 	.word	0x000e9ac8
   e1e1c:	000eab89 	.word	0x000eab89
   e1e20:	000ead1b 	.word	0x000ead1b
   e1e24:	000ea167 	.word	0x000ea167
   e1e28:	000ead42 	.word	0x000ead42
   e1e2c:	000eb2c5 	.word	0x000eb2c5
   e1e30:	000ead60 	.word	0x000ead60
   e1e34:	000ead87 	.word	0x000ead87
   e1e38:	000eadae 	.word	0x000eadae
   e1e3c:	000ea2a8 	.word	0x000ea2a8
   e1e40:	000eadcf 	.word	0x000eadcf
   e1e44:	000eabc2 	.word	0x000eabc2
   e1e48:	000eadf9 	.word	0x000eadf9
   e1e4c:	000eae23 	.word	0x000eae23
   e1e50:	000eae2f 	.word	0x000eae2f
   e1e54:	000e9b03 	.word	0x000e9b03
   e1e58:	000e993f 	.word	0x000e993f
   e1e5c:	000e98f8 	.word	0x000e98f8
   e1e60:	000eae6f 	.word	0x000eae6f
   e1e64:	000eae85 	.word	0x000eae85
   e1e68:	000eab29 	.word	0x000eab29
   e1e6c:	000eae9b 	.word	0x000eae9b
   e1e70:	000eae59 	.word	0x000eae59
   e1e74:	000eadbc 	.word	0x000eadbc

000e1e78 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e1e78:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e1e7c:	ed2d 8b02 	vpush	{d8}
   e1e80:	b097      	sub	sp, #92	; 0x5c
   e1e82:	680a      	ldr	r2, [r1, #0]
  auto* params = reinterpret_cast<TfLiteSVDFParams*>(node->builtin_data);
   e1e84:	694b      	ldr	r3, [r1, #20]
   e1e86:	9309      	str	r3, [sp, #36]	; 0x24
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e1e88:	6893      	ldr	r3, [r2, #8]
   e1e8a:	6855      	ldr	r5, [r2, #4]
   e1e8c:	f04f 0c38 	mov.w	ip, #56	; 0x38
   e1e90:	fb0c fe03 	mul.w	lr, ip, r3
}

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
   e1e94:	6913      	ldr	r3, [r2, #16]
  TF_LITE_ENSURE_EQ(context, output->dims->data[1], num_units);

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e1e96:	4607      	mov	r7, r0
   e1e98:	6880      	ldr	r0, [r0, #8]
  if (use_tensor) {
   e1e9a:	1c5c      	adds	r4, r3, #1
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e1e9c:	bf14      	ite	ne
   e1e9e:	fb0c 0303 	mlane	r3, ip, r3, r0
  }
  return nullptr;
   e1ea2:	2300      	moveq	r3, #0
   e1ea4:	930b      	str	r3, [sp, #44]	; 0x2c
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);

  // TODO(kreeger): Use input tensor as variable until scratch tensor allocation
  // has been implemented (cl/263032056)
  // TfLiteTensor* scratch = GetTemporary(context, node, /*index=*/0);
  TfLiteTensor* scratch = &context->tensors[node->inputs->data[5]];
   e1ea6:	6993      	ldr	r3, [r2, #24]

  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];
   e1ea8:	6954      	ldr	r4, [r2, #20]
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);

  // TODO(kreeger): Use input tensor as variable until scratch tensor allocation
  // has been implemented (cl/263032056)
  // TfLiteTensor* scratch = GetTemporary(context, node, /*index=*/0);
  TfLiteTensor* scratch = &context->tensors[node->inputs->data[5]];
   e1eaa:	fb0c 0303 	mla	r3, ip, r3, r0
   e1eae:	930e      	str	r3, [sp, #56]	; 0x38

  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];
   e1eb0:	fb0c 0304 	mla	r3, ip, r4, r0
   e1eb4:	9308      	str	r3, [sp, #32]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   e1eb6:	684b      	ldr	r3, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e1eb8:	eb00 060e 	add.w	r6, r0, lr
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   e1ebc:	685b      	ldr	r3, [r3, #4]
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (weights_feature->type) {
   e1ebe:	f810 e00e 	ldrb.w	lr, [r0, lr]
   e1ec2:	fb0c 0303 	mla	r3, ip, r3, r0
   e1ec6:	f1be 0f03 	cmp.w	lr, #3
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e1eca:	fb0c 0505 	mla	r5, ip, r5, r0
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   e1ece:	930d      	str	r3, [sp, #52]	; 0x34
   e1ed0:	f000 809f 	beq.w	e2012 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x19a>
   e1ed4:	f1be 0f09 	cmp.w	lr, #9
   e1ed8:	f000 809b 	beq.w	e2012 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x19a>
   e1edc:	f1be 0f01 	cmp.w	lr, #1
   e1ee0:	f040 816d 	bne.w	e21be <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x346>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e1ee4:	68d3      	ldr	r3, [r2, #12]
   e1ee6:	2238      	movs	r2, #56	; 0x38
   e1ee8:	fb02 0303 	mla	r3, r2, r3, r0
   e1eec:	930c      	str	r3, [sp, #48]	; 0x30
                          const TfLiteTensor* weights_time,
                          const TfLiteTensor* bias,
                          const TfLiteSVDFParams* params, TfLiteTensor* scratch,
                          TfLiteTensor* activation_state,
                          TfLiteTensor* output) {
  const int rank = params->rank;
   e1eee:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e1ef0:	681b      	ldr	r3, [r3, #0]
   e1ef2:	930f      	str	r3, [sp, #60]	; 0x3c
  const int batch_size = input->dims->data[0];
   e1ef4:	68ab      	ldr	r3, [r5, #8]
   e1ef6:	f8d3 b004 	ldr.w	fp, [r3, #4]
  const int input_size = input->dims->data[1];
   e1efa:	689b      	ldr	r3, [r3, #8]
   e1efc:	930a      	str	r3, [sp, #40]	; 0x28
  const int num_filters = weights_feature->dims->data[0];
   e1efe:	68b3      	ldr	r3, [r6, #8]
   e1f00:	685a      	ldr	r2, [r3, #4]
  const int num_units = num_filters / rank;
   e1f02:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   e1f04:	fb92 f3f3 	sdiv	r3, r2, r3
   e1f08:	9310      	str	r3, [sp, #64]	; 0x40
  const int memory_size = weights_time->dims->data[1];
   e1f0a:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   e1f0c:	689b      	ldr	r3, [r3, #8]
   e1f0e:	f8d3 a008 	ldr.w	sl, [r3, #8]
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0f;
   e1f12:	f10a 4380 	add.w	r3, sl, #1073741824	; 0x40000000
   e1f16:	3b01      	subs	r3, #1
   e1f18:	009b      	lsls	r3, r3, #2
   e1f1a:	fb0a f702 	mul.w	r7, sl, r2
   e1f1e:	00bf      	lsls	r7, r7, #2
   e1f20:	f103 0804 	add.w	r8, r3, #4
   e1f24:	4618      	mov	r0, r3
  const int memory_size = weights_time->dims->data[1];

  // Clear the activation (activation_state's leftmost column).
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in leftmost column and make sure it passes.
  for (int b = 0; b < batch_size; ++b) {
   e1f26:	f04f 0e00 	mov.w	lr, #0
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0f;
   e1f2a:	f04f 0900 	mov.w	r9, #0
  const int memory_size = weights_time->dims->data[1];

  // Clear the activation (activation_state's leftmost column).
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in leftmost column and make sure it passes.
  for (int b = 0; b < batch_size; ++b) {
   e1f2e:	45f3      	cmp	fp, lr
   e1f30:	dd13      	ble.n	e1f5a <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xe2>
   e1f32:	9908      	ldr	r1, [sp, #32]
   e1f34:	b109      	cbz	r1, e1f3a <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xc2>
   e1f36:	6849      	ldr	r1, [r1, #4]
   e1f38:	e000      	b.n	e1f3c <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xc4>
   e1f3a:	9908      	ldr	r1, [sp, #32]
   e1f3c:	4401      	add	r1, r0
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
   e1f3e:	f04f 0c00 	mov.w	ip, #0
   e1f42:	4562      	cmp	r2, ip
   e1f44:	dd05      	ble.n	e1f52 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xda>
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0f;
   e1f46:	f8c1 9000 	str.w	r9, [r1]
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in leftmost column and make sure it passes.
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
   e1f4a:	f10c 0c01 	add.w	ip, ip, #1
   e1f4e:	4441      	add	r1, r8
   e1f50:	e7f7      	b.n	e1f42 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xca>
  const int memory_size = weights_time->dims->data[1];

  // Clear the activation (activation_state's leftmost column).
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in leftmost column and make sure it passes.
  for (int b = 0; b < batch_size; ++b) {
   e1f52:	f10e 0e01 	add.w	lr, lr, #1
   e1f56:	4438      	add	r0, r7
   e1f58:	e7e9      	b.n	e1f2e <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xb6>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e1f5a:	6871      	ldr	r1, [r6, #4]
   e1f5c:	9111      	str	r1, [sp, #68]	; 0x44
   e1f5e:	6869      	ldr	r1, [r5, #4]
   e1f60:	9112      	str	r1, [sp, #72]	; 0x48

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e1f62:	9908      	ldr	r1, [sp, #32]
   e1f64:	b109      	cbz	r1, e1f6a <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xf2>
   e1f66:	6849      	ldr	r1, [r1, #4]
   e1f68:	e000      	b.n	e1f6c <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xf4>
   e1f6a:	9908      	ldr	r1, [sp, #32]
   e1f6c:	980a      	ldr	r0, [sp, #40]	; 0x28
   e1f6e:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
   e1f72:	0080      	lsls	r0, r0, #2
  // stride equal to memory_size.

  // Perform batched matrix vector multiply accumulate operation:
  const float* matrix = GetTensorData<float>(weights_feature);
  const float* vector = GetTensorData<float>(input);
  float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
   e1f74:	4419      	add	r1, r3
   e1f76:	ea22 77e2 	bic.w	r7, r2, r2, asr #31
      const float* vector_in_batch = vector + i * input_size;
      for (int k = 0; k < input_size; ++k) {
        dot_prod += *matrix_ptr++ * *vector_in_batch++;
      }
      *result_in_batch += dot_prod;
      result_in_batch += memory_size;
   e1f7a:	3304      	adds	r3, #4
   e1f7c:	9015      	str	r0, [sp, #84]	; 0x54
   e1f7e:	fb03 f007 	mul.w	r0, r3, r7
   e1f82:	9014      	str	r0, [sp, #80]	; 0x50
   e1f84:	2000      	movs	r0, #0
  // Perform batched matrix vector multiply accumulate operation:
  const float* matrix = GetTensorData<float>(weights_feature);
  const float* vector = GetTensorData<float>(input);
  float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
  float* result_in_batch = result;
  for (int i = 0; i < batch_size; ++i) {
   e1f86:	4606      	mov	r6, r0
   e1f88:	45b3      	cmp	fp, r6
   e1f8a:	dd2f      	ble.n	e1fec <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x174>
    const float* matrix_ptr = matrix;
    for (int j = 0; j < num_filters; ++j) {
      float dot_prod = 0.0f;
      const float* vector_in_batch = vector + i * input_size;
   e1f8c:	9d12      	ldr	r5, [sp, #72]	; 0x48
   e1f8e:	f8dd c044 	ldr.w	ip, [sp, #68]	; 0x44
   e1f92:	eb05 0580 	add.w	r5, r5, r0, lsl #2
   e1f96:	9513      	str	r5, [sp, #76]	; 0x4c
   e1f98:	f04f 0e00 	mov.w	lr, #0
   e1f9c:	460d      	mov	r5, r1
  const float* vector = GetTensorData<float>(input);
  float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
  float* result_in_batch = result;
  for (int i = 0; i < batch_size; ++i) {
    const float* matrix_ptr = matrix;
    for (int j = 0; j < num_filters; ++j) {
   e1f9e:	4572      	cmp	r2, lr
   e1fa0:	dd1e      	ble.n	e1fe0 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x168>
   e1fa2:	f8dd 904c 	ldr.w	r9, [sp, #76]	; 0x4c
   e1fa6:	ed9f 7a8d 	vldr	s14, [pc, #564]	; e21dc <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x364>
   e1faa:	4667      	mov	r7, ip
   e1fac:	f04f 0800 	mov.w	r8, #0
      float dot_prod = 0.0f;
      const float* vector_in_batch = vector + i * input_size;
      for (int k = 0; k < input_size; ++k) {
   e1fb0:	9c0a      	ldr	r4, [sp, #40]	; 0x28
   e1fb2:	4544      	cmp	r4, r8
   e1fb4:	dd08      	ble.n	e1fc8 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x150>
        dot_prod += *matrix_ptr++ * *vector_in_batch++;
   e1fb6:	ecf7 6a01 	vldmia	r7!, {s13}
   e1fba:	ecf9 7a01 	vldmia	r9!, {s15}
  for (int i = 0; i < batch_size; ++i) {
    const float* matrix_ptr = matrix;
    for (int j = 0; j < num_filters; ++j) {
      float dot_prod = 0.0f;
      const float* vector_in_batch = vector + i * input_size;
      for (int k = 0; k < input_size; ++k) {
   e1fbe:	f108 0801 	add.w	r8, r8, #1
        dot_prod += *matrix_ptr++ * *vector_in_batch++;
   e1fc2:	eea6 7aa7 	vfma.f32	s14, s13, s15
   e1fc6:	e7f3      	b.n	e1fb0 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x138>
      }
      *result_in_batch += dot_prod;
   e1fc8:	edd5 7a00 	vldr	s15, [r5]
   e1fcc:	9c15      	ldr	r4, [sp, #84]	; 0x54
   e1fce:	ee77 7a87 	vadd.f32	s15, s15, s14
   e1fd2:	44a4      	add	ip, r4
   e1fd4:	edc5 7a00 	vstr	s15, [r5]
  const float* vector = GetTensorData<float>(input);
  float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
  float* result_in_batch = result;
  for (int i = 0; i < batch_size; ++i) {
    const float* matrix_ptr = matrix;
    for (int j = 0; j < num_filters; ++j) {
   e1fd8:	f10e 0e01 	add.w	lr, lr, #1
      const float* vector_in_batch = vector + i * input_size;
      for (int k = 0; k < input_size; ++k) {
        dot_prod += *matrix_ptr++ * *vector_in_batch++;
      }
      *result_in_batch += dot_prod;
      result_in_batch += memory_size;
   e1fdc:	441d      	add	r5, r3
   e1fde:	e7de      	b.n	e1f9e <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x126>
   e1fe0:	9c14      	ldr	r4, [sp, #80]	; 0x50
   e1fe2:	4421      	add	r1, r4
   e1fe4:	9c0a      	ldr	r4, [sp, #40]	; 0x28
  // Perform batched matrix vector multiply accumulate operation:
  const float* matrix = GetTensorData<float>(weights_feature);
  const float* vector = GetTensorData<float>(input);
  float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
  float* result_in_batch = result;
  for (int i = 0; i < batch_size; ++i) {
   e1fe6:	3601      	adds	r6, #1
   e1fe8:	4420      	add	r0, r4
   e1fea:	e7cd      	b.n	e1f88 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x110>
    }
  }

  ApplyTimeWeightsBiasAndActivation(
      batch_size, memory_size, num_filters, num_units, rank, weights_time, bias,
      params->activation, activation_state, scratch, output);
   e1fec:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   e1fee:	9306      	str	r3, [sp, #24]
   e1ff0:	9b0e      	ldr	r3, [sp, #56]	; 0x38
   e1ff2:	9305      	str	r3, [sp, #20]
   e1ff4:	9b08      	ldr	r3, [sp, #32]
   e1ff6:	9304      	str	r3, [sp, #16]
   e1ff8:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e1ffa:	791b      	ldrb	r3, [r3, #4]
   e1ffc:	9303      	str	r3, [sp, #12]
   e1ffe:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e2000:	9302      	str	r3, [sp, #8]
   e2002:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   e2004:	9301      	str	r3, [sp, #4]
   e2006:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   e2008:	9300      	str	r3, [sp, #0]
   e200a:	4651      	mov	r1, sl
   e200c:	9b10      	ldr	r3, [sp, #64]	; 0x40
   e200e:	4658      	mov	r0, fp
   e2010:	e0d1      	b.n	e21b6 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x33e>
   e2012:	68cf      	ldr	r7, [r1, #12]
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
}
inline TfLiteTensor* GetTemporary(TfLiteContext* context, TfLiteNode* node,
                                  int index) {
  return &context->tensors[flatbuffers::EndianScalar(
      node->temporaries->data[index])];
   e2014:	68ba      	ldr	r2, [r7, #8]
   e2016:	68fb      	ldr	r3, [r7, #12]
   e2018:	693f      	ldr	r7, [r7, #16]
   e201a:	2138      	movs	r1, #56	; 0x38
   e201c:	fb01 0202 	mla	r2, r1, r2, r0
   e2020:	fb01 0303 	mla	r3, r1, r3, r0
   e2024:	fb01 0107 	mla	r1, r1, r7, r0
   e2028:	9110      	str	r1, [sp, #64]	; 0x40
    const TfLiteTensor* weights_feature, const TfLiteTensor* weights_time,
    const TfLiteTensor* bias, const TfLiteSVDFParams* params,
    TfLiteTensor* scratch, TfLiteTensor* scaling_factors,
    TfLiteTensor* input_quantized, TfLiteTensor* activation_state,
    TfLiteTensor* output) {
  const int rank = params->rank;
   e202a:	9909      	ldr	r1, [sp, #36]	; 0x24
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e202c:	6868      	ldr	r0, [r5, #4]
   e202e:	6809      	ldr	r1, [r1, #0]
   e2030:	910f      	str	r1, [sp, #60]	; 0x3c
  const int batch_size = input->dims->data[0];
   e2032:	68a9      	ldr	r1, [r5, #8]
   e2034:	f8d1 a004 	ldr.w	sl, [r1, #4]
  const int input_size = input->dims->data[1];
   e2038:	6889      	ldr	r1, [r1, #8]
   e203a:	910a      	str	r1, [sp, #40]	; 0x28
  const int num_filters = weights_feature->dims->data[0];
   e203c:	68b1      	ldr	r1, [r6, #8]
   e203e:	f8d1 b004 	ldr.w	fp, [r1, #4]
  const int num_units = num_filters / rank;
   e2042:	990f      	ldr	r1, [sp, #60]	; 0x3c
   e2044:	fb9b f1f1 	sdiv	r1, fp, r1
   e2048:	9111      	str	r1, [sp, #68]	; 0x44
  const int memory_size = weights_time->dims->data[1];
   e204a:	9910      	ldr	r1, [sp, #64]	; 0x40
   e204c:	6889      	ldr	r1, [r1, #8]
   e204e:	6889      	ldr	r1, [r1, #8]
   e2050:	910c      	str	r1, [sp, #48]	; 0x30
   e2052:	6871      	ldr	r1, [r6, #4]
   e2054:	9114      	str	r1, [sp, #80]	; 0x50

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e2056:	b112      	cbz	r2, e205e <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x1e6>
   e2058:	f8d2 9004 	ldr.w	r9, [r2, #4]
   e205c:	e000      	b.n	e2060 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x1e8>
   e205e:	4691      	mov	r9, r2
   e2060:	b10b      	cbz	r3, e2066 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x1ee>
   e2062:	685f      	ldr	r7, [r3, #4]
   e2064:	e000      	b.n	e2068 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x1f0>
   e2066:	461f      	mov	r7, r3
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0;
   e2068:	9b0c      	ldr	r3, [sp, #48]	; 0x30

  // Initialize the pointer to storage for scaling factors.
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors);

  // Initialize the weights scale.
  const float weights_feature_scale = weights_feature->params.scale;
   e206a:	ed96 8a03 	vldr	s16, [r6, #12]
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0;
   e206e:	eddf 7a5b 	vldr	s15, [pc, #364]	; e21dc <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x364>
   e2072:	f103 4280 	add.w	r2, r3, #1073741824	; 0x40000000
   e2076:	3a01      	subs	r2, #1
   e2078:	0096      	lsls	r6, r2, #2
   e207a:	fb03 f10b 	mul.w	r1, r3, fp
   e207e:	0089      	lsls	r1, r1, #2
   e2080:	f106 0804 	add.w	r8, r6, #4
   e2084:	4632      	mov	r2, r6

  // Clear the activation (activation_state's leftmost column).
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in the leftmost column and make sure it passes.
  // TODO(kreeger): Use a port of tensor_utils when ready (b/140272187).
  for (int b = 0; b < batch_size; ++b) {
   e2086:	f04f 0e00 	mov.w	lr, #0
   e208a:	45f2      	cmp	sl, lr
   e208c:	dd13      	ble.n	e20b6 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x23e>
   e208e:	9b08      	ldr	r3, [sp, #32]
   e2090:	b10b      	cbz	r3, e2096 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x21e>
   e2092:	685b      	ldr	r3, [r3, #4]
   e2094:	e000      	b.n	e2098 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x220>
   e2096:	9b08      	ldr	r3, [sp, #32]
   e2098:	4413      	add	r3, r2
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
   e209a:	f04f 0c00 	mov.w	ip, #0
   e209e:	45e3      	cmp	fp, ip
   e20a0:	dd05      	ble.n	e20ae <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x236>
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0;
   e20a2:	edc3 7a00 	vstr	s15, [r3]
  // values in the leftmost column and make sure it passes.
  // TODO(kreeger): Use a port of tensor_utils when ready (b/140272187).
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
   e20a6:	f10c 0c01 	add.w	ip, ip, #1
   e20aa:	4443      	add	r3, r8
   e20ac:	e7f7      	b.n	e209e <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x226>

  // Clear the activation (activation_state's leftmost column).
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in the leftmost column and make sure it passes.
  // TODO(kreeger): Use a port of tensor_utils when ready (b/140272187).
  for (int b = 0; b < batch_size; ++b) {
   e20ae:	f10e 0e01 	add.w	lr, lr, #1
   e20b2:	440a      	add	r2, r1
   e20b4:	e7e9      	b.n	e208a <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x212>
    }
  }

  // Determine if input pointer batch is a zero based vector:
  bool is_zero_vector = true;
  for (int i = 0; i < batch_size * input_size && is_zero_vector; ++i) {
   e20b6:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e20b8:	4601      	mov	r1, r0
   e20ba:	fb03 fe0a 	mul.w	lr, r3, sl
   e20be:	2200      	movs	r2, #0
   e20c0:	2301      	movs	r3, #1
   e20c2:	4596      	cmp	lr, r2
   e20c4:	dd0b      	ble.n	e20de <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x266>
   e20c6:	b163      	cbz	r3, e20e2 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x26a>
    if (input_ptr_batch[i] != 0.0f) {
   e20c8:	ecf1 7a01 	vldmia	r1!, {s15}
   e20cc:	eef5 7a40 	vcmp.f32	s15, #0.0
   e20d0:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e20d4:	bf0c      	ite	eq
   e20d6:	2301      	moveq	r3, #1
   e20d8:	2300      	movne	r3, #0
    }
  }

  // Determine if input pointer batch is a zero based vector:
  bool is_zero_vector = true;
  for (int i = 0; i < batch_size * input_size && is_zero_vector; ++i) {
   e20da:	3201      	adds	r2, #1
   e20dc:	e7f1      	b.n	e20c2 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x24a>
    if (input_ptr_batch[i] != 0.0f) {
      is_zero_vector = false;
    }
  }

  if (!is_zero_vector) {
   e20de:	2b00      	cmp	r3, #0
   e20e0:	d156      	bne.n	e2190 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x318>
    SignedSymmetricPerChannelQuantize(input_ptr_batch, input->dims, 0,
                                      quantized_input_ptr_batch,
                                      scaling_factors_ptr);
   e20e2:	9700      	str	r7, [sp, #0]
   e20e4:	464b      	mov	r3, r9
   e20e6:	2200      	movs	r2, #0
   e20e8:	68a9      	ldr	r1, [r5, #8]
   e20ea:	f7f4 f8d1 	bl	d6290 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf>
   e20ee:	463b      	mov	r3, r7
   e20f0:	4639      	mov	r1, r7

    // Quantize input from float to int8.
    for (int b = 0; b < batch_size; ++b) {
   e20f2:	2200      	movs	r2, #0
   e20f4:	4592      	cmp	sl, r2
   e20f6:	dd07      	ble.n	e2108 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x290>
      scaling_factors_ptr[b] *= weights_feature_scale;
   e20f8:	edd1 7a00 	vldr	s15, [r1]
   e20fc:	ee67 7a88 	vmul.f32	s15, s15, s16
    SignedSymmetricPerChannelQuantize(input_ptr_batch, input->dims, 0,
                                      quantized_input_ptr_batch,
                                      scaling_factors_ptr);

    // Quantize input from float to int8.
    for (int b = 0; b < batch_size; ++b) {
   e2100:	3201      	adds	r2, #1
      scaling_factors_ptr[b] *= weights_feature_scale;
   e2102:	ece1 7a01 	vstmia	r1!, {s15}
   e2106:	e7f5      	b.n	e20f4 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x27c>
   e2108:	9a08      	ldr	r2, [sp, #32]
   e210a:	b10a      	cbz	r2, e2110 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x298>
   e210c:	6851      	ldr	r1, [r2, #4]
   e210e:	e000      	b.n	e2112 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x29a>
   e2110:	9908      	ldr	r1, [sp, #32]
         ++i, quantized_input_ptr_batch += input_size) {
      const float batch_scaling_factor = scaling_factors_ptr[i];

      // Get the address of the first row:
      const int8_t* row_ptr = weights_feature_ptr;
      for (int j = 0; j < num_filters; ++j, result += memory_size) {
   e2112:	1d32      	adds	r2, r6, #4
   e2114:	ea2b 7eeb 	bic.w	lr, fp, fp, asr #31
   e2118:	fb02 f00e 	mul.w	r0, r2, lr
   e211c:	9013      	str	r0, [sp, #76]	; 0x4c
   e211e:	980a      	ldr	r0, [sp, #40]	; 0x28
   e2120:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
    // Compute conv1d(inputs, weights_feature).
    // The rightmost column of activation_state is used to save the current
    // cycle activation. This is achieved by starting at
    // GetTensorData<float>(activation_state)[memory_size - 1] and having the
    // stride equal to memory_size. (Matrix batch vector multiply accumulate)
    float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
   e2124:	4431      	add	r1, r6
    for (int i = 0; i < batch_size;
   e2126:	2700      	movs	r7, #0
   e2128:	9015      	str	r0, [sp, #84]	; 0x54
   e212a:	45ba      	cmp	sl, r7
   e212c:	dd30      	ble.n	e2190 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x318>
         ++i, quantized_input_ptr_batch += input_size) {
      const float batch_scaling_factor = scaling_factors_ptr[i];
   e212e:	ecf3 6a01 	vldmia	r3!, {s13}

      // Get the address of the first row:
      const int8_t* row_ptr = weights_feature_ptr;
   e2132:	f8dd 8050 	ldr.w	r8, [sp, #80]	; 0x50
    // GetTensorData<float>(activation_state)[memory_size - 1] and having the
    // stride equal to memory_size. (Matrix batch vector multiply accumulate)
    float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
    for (int i = 0; i < batch_size;
         ++i, quantized_input_ptr_batch += input_size) {
      const float batch_scaling_factor = scaling_factors_ptr[i];
   e2136:	460e      	mov	r6, r1

      // Get the address of the first row:
      const int8_t* row_ptr = weights_feature_ptr;
      for (int j = 0; j < num_filters; ++j, result += memory_size) {
   e2138:	f04f 0c00 	mov.w	ip, #0
   e213c:	45e3      	cmp	fp, ip
   e213e:	dd21      	ble.n	e2184 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x30c>
   e2140:	2000      	movs	r0, #0
   e2142:	4605      	mov	r5, r0
        // Initialize the dot product sum for the row to 0.
        int32_t dotprod = 0;
        for (int k = 0; k < input_size; ++k, ++row_ptr) {
   e2144:	9c0a      	ldr	r4, [sp, #40]	; 0x28
   e2146:	4284      	cmp	r4, r0
   e2148:	dd0c      	ble.n	e2164 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x2ec>
          dotprod += (*row_ptr) * (quantized_input_ptr_batch[k]);
   e214a:	f918 4000 	ldrsb.w	r4, [r8, r0]
   e214e:	46a6      	mov	lr, r4
   e2150:	f919 4000 	ldrsb.w	r4, [r9, r0]
   e2154:	9412      	str	r4, [sp, #72]	; 0x48
   e2156:	4674      	mov	r4, lr
   e2158:	f8bd e048 	ldrh.w	lr, [sp, #72]	; 0x48
      // Get the address of the first row:
      const int8_t* row_ptr = weights_feature_ptr;
      for (int j = 0; j < num_filters; ++j, result += memory_size) {
        // Initialize the dot product sum for the row to 0.
        int32_t dotprod = 0;
        for (int k = 0; k < input_size; ++k, ++row_ptr) {
   e215c:	3001      	adds	r0, #1
          dotprod += (*row_ptr) * (quantized_input_ptr_batch[k]);
   e215e:	fb14 550e 	smlabb	r5, r4, lr, r5
   e2162:	e7ef      	b.n	e2144 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x2cc>
        }
        *result += dotprod * batch_scaling_factor;
   e2164:	ee07 5a90 	vmov	s15, r5
   e2168:	ed96 7a00 	vldr	s14, [r6]
   e216c:	9815      	ldr	r0, [sp, #84]	; 0x54
   e216e:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   e2172:	4480      	add	r8, r0
   e2174:	eea6 7aa7 	vfma.f32	s14, s13, s15
         ++i, quantized_input_ptr_batch += input_size) {
      const float batch_scaling_factor = scaling_factors_ptr[i];

      // Get the address of the first row:
      const int8_t* row_ptr = weights_feature_ptr;
      for (int j = 0; j < num_filters; ++j, result += memory_size) {
   e2178:	f10c 0c01 	add.w	ip, ip, #1
        // Initialize the dot product sum for the row to 0.
        int32_t dotprod = 0;
        for (int k = 0; k < input_size; ++k, ++row_ptr) {
          dotprod += (*row_ptr) * (quantized_input_ptr_batch[k]);
        }
        *result += dotprod * batch_scaling_factor;
   e217c:	ed86 7a00 	vstr	s14, [r6]
         ++i, quantized_input_ptr_batch += input_size) {
      const float batch_scaling_factor = scaling_factors_ptr[i];

      // Get the address of the first row:
      const int8_t* row_ptr = weights_feature_ptr;
      for (int j = 0; j < num_filters; ++j, result += memory_size) {
   e2180:	4416      	add	r6, r2
   e2182:	e7db      	b.n	e213c <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x2c4>
   e2184:	9813      	ldr	r0, [sp, #76]	; 0x4c
   e2186:	4401      	add	r1, r0
   e2188:	980a      	ldr	r0, [sp, #40]	; 0x28
    // The rightmost column of activation_state is used to save the current
    // cycle activation. This is achieved by starting at
    // GetTensorData<float>(activation_state)[memory_size - 1] and having the
    // stride equal to memory_size. (Matrix batch vector multiply accumulate)
    float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
    for (int i = 0; i < batch_size;
   e218a:	3701      	adds	r7, #1
   e218c:	4481      	add	r9, r0
   e218e:	e7cc      	b.n	e212a <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x2b2>

  // TODO(alanchiao): can optimize hybrid case ~5% by unrolling loop in applying
  // time weights so that the inner loop multiplies eight elements at a time.
  ApplyTimeWeightsBiasAndActivation(
      batch_size, memory_size, num_filters, num_units, rank, weights_time, bias,
      params->activation, activation_state, scratch, output);
   e2190:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   e2192:	9306      	str	r3, [sp, #24]
   e2194:	9b0e      	ldr	r3, [sp, #56]	; 0x38
   e2196:	9305      	str	r3, [sp, #20]
   e2198:	9b08      	ldr	r3, [sp, #32]
   e219a:	9304      	str	r3, [sp, #16]
   e219c:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e219e:	990c      	ldr	r1, [sp, #48]	; 0x30
   e21a0:	791b      	ldrb	r3, [r3, #4]
   e21a2:	9303      	str	r3, [sp, #12]
   e21a4:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e21a6:	9302      	str	r3, [sp, #8]
   e21a8:	9b10      	ldr	r3, [sp, #64]	; 0x40
   e21aa:	9301      	str	r3, [sp, #4]
   e21ac:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   e21ae:	9300      	str	r3, [sp, #0]
   e21b0:	9b11      	ldr	r3, [sp, #68]	; 0x44
   e21b2:	465a      	mov	r2, fp
   e21b4:	4650      	mov	r0, sl
   e21b6:	f7ff fa6b 	bl	e1690 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_>
      TfLiteTensor* scratch_float_weights_time = GetTemporary(context, node, 3);
      EvalHybridSVDF(context, node, input, weights_feature,
                     scratch_float_weights_time, bias, params, scratch,
                     scratch_scaling_factors, scratch_input_quantized,
                     activation_state, output);
      return kTfLiteOk;
   e21ba:	2000      	movs	r0, #0
   e21bc:	e008      	b.n	e21d0 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x358>
      break;
    }

    default:
      // TODO(kreeger): Handle this case for full quant svdf b/139435798
      context->ReportError(context, "Type %s not currently supported.",
   e21be:	4670      	mov	r0, lr
   e21c0:	697c      	ldr	r4, [r7, #20]
   e21c2:	f7f1 ffa9 	bl	d4118 <TfLiteTypeGetName>
                           TfLiteTypeGetName(weights_feature->type));
   e21c6:	4906      	ldr	r1, [pc, #24]	; (e21e0 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x368>)
   e21c8:	4602      	mov	r2, r0
   e21ca:	4638      	mov	r0, r7
   e21cc:	47a0      	blx	r4
      return kTfLiteError;
   e21ce:	2001      	movs	r0, #1
  }
  return kTfLiteOk;
}
   e21d0:	b017      	add	sp, #92	; 0x5c
   e21d2:	ecbd 8b02 	vpop	{d8}
   e21d6:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e21da:	bf00      	nop
   e21dc:	00000000 	.word	0x00000000
   e21e0:	000e9f92 	.word	0x000e9f92

000e21e4 <_ZN6tflite3ops5micro13Register_SVDFEv>:

TfLiteRegistration* Register_SVDF() {
  static TfLiteRegistration r = {svdf::Init, svdf::Free, svdf::Prepare,
                                 svdf::Eval};
  return &r;
}
   e21e4:	4800      	ldr	r0, [pc, #0]	; (e21e8 <_ZN6tflite3ops5micro13Register_SVDFEv+0x4>)
   e21e6:	4770      	bx	lr
   e21e8:	2003c208 	.word	0x2003c208

000e21ec <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_17PrepareEP13TfLiteContextP10TfLiteNode>:

constexpr int kInputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   e21ec:	2000      	movs	r0, #0
   e21ee:	4770      	bx	lr

000e21f0 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode>:
  }

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e21f0:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e21f4:	680b      	ldr	r3, [r1, #0]
   e21f6:	6885      	ldr	r5, [r0, #8]
  TfLiteUnpackParams* data =
      reinterpret_cast<TfLiteUnpackParams*>(node->builtin_data);
   e21f8:	694a      	ldr	r2, [r1, #20]
  }

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e21fa:	b085      	sub	sp, #20
   e21fc:	9001      	str	r0, [sp, #4]
   e21fe:	6858      	ldr	r0, [r3, #4]
   e2200:	2338      	movs	r3, #56	; 0x38
   e2202:	4358      	muls	r0, r3
   e2204:	182b      	adds	r3, r5, r0
  TfLiteUnpackParams* data =
      reinterpret_cast<TfLiteUnpackParams*>(node->builtin_data);

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);

  switch (input->type) {
   e2206:	5c28      	ldrb	r0, [r5, r0]
   e2208:	1e46      	subs	r6, r0, #1
   e220a:	2e08      	cmp	r6, #8
   e220c:	f200 81c3 	bhi.w	e2596 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3a6>
   e2210:	e8df f016 	tbh	[pc, r6, lsl #1]
   e2214:	007a0009 	.word	0x007a0009
   e2218:	01c100e7 	.word	0x01c100e7
   e221c:	01c101c1 	.word	0x01c101c1
   e2220:	01c101c1 	.word	0x01c101c1
   e2224:	0154      	.short	0x0154
}

template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
   e2226:	6848      	ldr	r0, [r1, #4]
  const TfLiteIntArray* input_dims = input->dims;
   e2228:	f8d3 c008 	ldr.w	ip, [r3, #8]
  const TfLiteIntArray* output_dims = output0->dims;
   e222c:	6840      	ldr	r0, [r0, #4]

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);

  switch (input->type) {
    case kTfLiteFloat32: {
      return UnpackImpl<float>(context, node, input, data->num, data->axis);
   e222e:	f8d2 8000 	ldr.w	r8, [r2]
   e2232:	6852      	ldr	r2, [r2, #4]
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
   e2234:	2638      	movs	r6, #56	; 0x38
   e2236:	fb06 5500 	mla	r5, r6, r0, r5
  const int dimensions = input_dims->size;

  if (axis < 0) {
   e223a:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
   e223c:	68af      	ldr	r7, [r5, #8]
  const int dimensions = input_dims->size;
   e223e:	f8dc 5000 	ldr.w	r5, [ip]

  if (axis < 0) {
    axis += NumDimensions(input);
   e2242:	bfb8      	it	lt
   e2244:	1952      	addlt	r2, r2, r5
  }

  TFLITE_DCHECK_LT(axis, dimensions);
   e2246:	4295      	cmp	r5, r2
   e2248:	dc01      	bgt.n	e224e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5e>
   e224a:	f002 f86f 	bl	e432c <abort>
   e224e:	46e6      	mov	lr, ip
   e2250:	2000      	movs	r0, #0
   e2252:	2601      	movs	r6, #1

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e2254:	4282      	cmp	r2, r0
   e2256:	dd05      	ble.n	e2264 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x74>
    outer_size *= input_dims->data[i];
   e2258:	f85e 9f04 	ldr.w	r9, [lr, #4]!
  }

  TFLITE_DCHECK_LT(axis, dimensions);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e225c:	3001      	adds	r0, #1
    outer_size *= input_dims->data[i];
   e225e:	fb09 f606 	mul.w	r6, r9, r6
   e2262:	e7f7      	b.n	e2254 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x64>
   e2264:	1c50      	adds	r0, r2, #1
   e2266:	f105 0e01 	add.w	lr, r5, #1
  }
  int copy_size = 1;
   e226a:	2201      	movs	r2, #1
   e226c:	3001      	adds	r0, #1
  for (int i = axis + 1; i < dimensions; ++i) {
   e226e:	4570      	cmp	r0, lr
   e2270:	d003      	beq.n	e227a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x8a>
    copy_size *= input_dims->data[i];
   e2272:	f85c 5020 	ldr.w	r5, [ip, r0, lsl #2]
   e2276:	436a      	muls	r2, r5
   e2278:	e7f8      	b.n	e226c <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x7c>
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
   e227a:	f8d7 c000 	ldr.w	ip, [r7]
   e227e:	4638      	mov	r0, r7
   e2280:	2501      	movs	r5, #1
   e2282:	2700      	movs	r7, #0
   e2284:	45bc      	cmp	ip, r7
   e2286:	dd05      	ble.n	e2294 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xa4>
    output_size *= output_dims->data[i];
   e2288:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= input_dims->data[i];
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
   e228c:	3701      	adds	r7, #1
    output_size *= output_dims->data[i];
   e228e:	fb0e f505 	mul.w	r5, lr, r5
   e2292:	e7f7      	b.n	e2284 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x94>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);
   e2294:	fb02 f006 	mul.w	r0, r2, r6
   e2298:	4285      	cmp	r5, r0
   e229a:	d1d6      	bne.n	e224a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e229c:	685b      	ldr	r3, [r3, #4]
   e229e:	9302      	str	r3, [sp, #8]
   e22a0:	fb02 f308 	mul.w	r3, r2, r8
   e22a4:	009b      	lsls	r3, r3, #2
   e22a6:	2000      	movs	r0, #0
   e22a8:	ea4f 0982 	mov.w	r9, r2, lsl #2
   e22ac:	9303      	str	r3, [sp, #12]

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
   e22ae:	4605      	mov	r5, r0
   e22b0:	45a8      	cmp	r8, r5
   e22b2:	dc01      	bgt.n	e22b8 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc8>

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);

  switch (input->type) {
    case kTfLiteFloat32: {
      return UnpackImpl<float>(context, node, input, data->num, data->axis);
   e22b4:	2000      	movs	r0, #0
   e22b6:	e177      	b.n	e25a8 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3b8>
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e22b8:	684b      	ldr	r3, [r1, #4]
   e22ba:	eb03 0385 	add.w	r3, r3, r5, lsl #2
   e22be:	2438      	movs	r4, #56	; 0x38
   e22c0:	685f      	ldr	r7, [r3, #4]
   e22c2:	9b01      	ldr	r3, [sp, #4]
   e22c4:	689b      	ldr	r3, [r3, #8]
   e22c6:	fb04 3307 	mla	r3, r4, r7, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e22ca:	b103      	cbz	r3, e22ce <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xde>
   e22cc:	685b      	ldr	r3, [r3, #4]
   e22ce:	2700      	movs	r7, #0
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   e22d0:	46bc      	mov	ip, r7
   e22d2:	4566      	cmp	r6, ip
   e22d4:	dd15      	ble.n	e2302 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x112>
   e22d6:	9c02      	ldr	r4, [sp, #8]
   e22d8:	eb00 0e07 	add.w	lr, r0, r7
   e22dc:	44a6      	add	lr, r4
   e22de:	469b      	mov	fp, r3
      T* output_ptr = output_data + copy_size * k;
      int loc = k * output_count * copy_size + i * copy_size;
      const T* input_ptr = input_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e22e0:	f04f 0a00 	mov.w	sl, #0
   e22e4:	4552      	cmp	r2, sl
   e22e6:	dd06      	ble.n	e22f6 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x106>
   e22e8:	ecfe 7a01 	vldmia	lr!, {s15}
   e22ec:	f10a 0a01 	add.w	sl, sl, #1
   e22f0:	eceb 7a01 	vstmia	fp!, {s15}
   e22f4:	e7f6      	b.n	e22e4 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xf4>
   e22f6:	9c03      	ldr	r4, [sp, #12]
  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   e22f8:	f10c 0c01 	add.w	ip, ip, #1
   e22fc:	4427      	add	r7, r4
   e22fe:	444b      	add	r3, r9
   e2300:	e7e7      	b.n	e22d2 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xe2>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
   e2302:	3501      	adds	r5, #1
   e2304:	4448      	add	r0, r9
   e2306:	e7d3      	b.n	e22b0 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc0>
}

template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
   e2308:	6848      	ldr	r0, [r1, #4]
  const TfLiteIntArray* input_dims = input->dims;
   e230a:	f8d3 c008 	ldr.w	ip, [r3, #8]
  const TfLiteIntArray* output_dims = output0->dims;
   e230e:	6840      	ldr	r0, [r0, #4]
  switch (input->type) {
    case kTfLiteFloat32: {
      return UnpackImpl<float>(context, node, input, data->num, data->axis);
    }
    case kTfLiteInt32: {
      return UnpackImpl<int32_t>(context, node, input, data->num, data->axis);
   e2310:	f8d2 8000 	ldr.w	r8, [r2]
   e2314:	6852      	ldr	r2, [r2, #4]
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
   e2316:	2638      	movs	r6, #56	; 0x38
   e2318:	fb06 5500 	mla	r5, r6, r0, r5
  const int dimensions = input_dims->size;

  if (axis < 0) {
   e231c:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
   e231e:	68ae      	ldr	r6, [r5, #8]
  const int dimensions = input_dims->size;
   e2320:	f8dc 5000 	ldr.w	r5, [ip]

  if (axis < 0) {
    axis += NumDimensions(input);
   e2324:	bfb8      	it	lt
   e2326:	1952      	addlt	r2, r2, r5
  }

  TFLITE_DCHECK_LT(axis, dimensions);
   e2328:	4295      	cmp	r5, r2
   e232a:	dd8e      	ble.n	e224a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
   e232c:	46e6      	mov	lr, ip
   e232e:	2000      	movs	r0, #0
   e2330:	2701      	movs	r7, #1

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e2332:	4282      	cmp	r2, r0
   e2334:	dd05      	ble.n	e2342 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x152>
    outer_size *= input_dims->data[i];
   e2336:	f85e 9f04 	ldr.w	r9, [lr, #4]!
  }

  TFLITE_DCHECK_LT(axis, dimensions);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e233a:	3001      	adds	r0, #1
    outer_size *= input_dims->data[i];
   e233c:	fb09 f707 	mul.w	r7, r9, r7
   e2340:	e7f7      	b.n	e2332 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x142>
   e2342:	1c50      	adds	r0, r2, #1
   e2344:	f105 0e01 	add.w	lr, r5, #1
  }
  int copy_size = 1;
   e2348:	2201      	movs	r2, #1
   e234a:	3001      	adds	r0, #1
  for (int i = axis + 1; i < dimensions; ++i) {
   e234c:	4570      	cmp	r0, lr
   e234e:	d003      	beq.n	e2358 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x168>
    copy_size *= input_dims->data[i];
   e2350:	f85c 5020 	ldr.w	r5, [ip, r0, lsl #2]
   e2354:	436a      	muls	r2, r5
   e2356:	e7f8      	b.n	e234a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x15a>
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
   e2358:	f8d6 c000 	ldr.w	ip, [r6]
   e235c:	4630      	mov	r0, r6
   e235e:	2501      	movs	r5, #1
   e2360:	2600      	movs	r6, #0
   e2362:	45b4      	cmp	ip, r6
   e2364:	dd05      	ble.n	e2372 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x182>
    output_size *= output_dims->data[i];
   e2366:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= input_dims->data[i];
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
   e236a:	3601      	adds	r6, #1
    output_size *= output_dims->data[i];
   e236c:	fb0e f505 	mul.w	r5, lr, r5
   e2370:	e7f7      	b.n	e2362 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x172>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);
   e2372:	fb02 f007 	mul.w	r0, r2, r7
   e2376:	4285      	cmp	r5, r0
   e2378:	f47f af67 	bne.w	e224a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e237c:	685b      	ldr	r3, [r3, #4]
   e237e:	9302      	str	r3, [sp, #8]
   e2380:	fb02 f308 	mul.w	r3, r2, r8
   e2384:	009b      	lsls	r3, r3, #2
   e2386:	2500      	movs	r5, #0
   e2388:	ea4f 0c82 	mov.w	ip, r2, lsl #2
   e238c:	9303      	str	r3, [sp, #12]

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
   e238e:	462e      	mov	r6, r5
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e2390:	f04f 0938 	mov.w	r9, #56	; 0x38
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
   e2394:	45b0      	cmp	r8, r6
   e2396:	dd8d      	ble.n	e22b4 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc4>
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e2398:	684b      	ldr	r3, [r1, #4]
   e239a:	eb03 0386 	add.w	r3, r3, r6, lsl #2
   e239e:	6858      	ldr	r0, [r3, #4]
   e23a0:	9b01      	ldr	r3, [sp, #4]
   e23a2:	689b      	ldr	r3, [r3, #8]
   e23a4:	fb09 3300 	mla	r3, r9, r0, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e23a8:	b103      	cbz	r3, e23ac <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1bc>
   e23aa:	685b      	ldr	r3, [r3, #4]
   e23ac:	f04f 0e00 	mov.w	lr, #0
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   e23b0:	46f2      	mov	sl, lr
   e23b2:	4557      	cmp	r7, sl
   e23b4:	dd12      	ble.n	e23dc <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1ec>
      T* output_ptr = output_data + copy_size * k;
      int loc = k * output_count * copy_size + i * copy_size;
      const T* input_ptr = input_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e23b6:	9c02      	ldr	r4, [sp, #8]
   e23b8:	eb05 0b0e 	add.w	fp, r5, lr
  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   e23bc:	2000      	movs	r0, #0
      T* output_ptr = output_data + copy_size * k;
      int loc = k * output_count * copy_size + i * copy_size;
      const T* input_ptr = input_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e23be:	44a3      	add	fp, r4
   e23c0:	4282      	cmp	r2, r0
   e23c2:	dd05      	ble.n	e23d0 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1e0>
   e23c4:	f85b 4020 	ldr.w	r4, [fp, r0, lsl #2]
   e23c8:	f843 4020 	str.w	r4, [r3, r0, lsl #2]
   e23cc:	3001      	adds	r0, #1
   e23ce:	e7f7      	b.n	e23c0 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1d0>
   e23d0:	9803      	ldr	r0, [sp, #12]
  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   e23d2:	f10a 0a01 	add.w	sl, sl, #1
   e23d6:	4486      	add	lr, r0
   e23d8:	4463      	add	r3, ip
   e23da:	e7ea      	b.n	e23b2 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1c2>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
   e23dc:	3601      	adds	r6, #1
   e23de:	4465      	add	r5, ip
   e23e0:	e7d8      	b.n	e2394 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1a4>
    }
    case kTfLiteInt32: {
      return UnpackImpl<int32_t>(context, node, input, data->num, data->axis);
    }
    case kTfLiteUInt8: {
      return UnpackImpl<uint8_t>(context, node, input, data->num, data->axis);
   e23e2:	6810      	ldr	r0, [r2, #0]
   e23e4:	9002      	str	r0, [sp, #8]
}

template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
   e23e6:	6848      	ldr	r0, [r1, #4]
  const TfLiteIntArray* input_dims = input->dims;
   e23e8:	f8d3 c008 	ldr.w	ip, [r3, #8]
  const TfLiteIntArray* output_dims = output0->dims;
   e23ec:	6840      	ldr	r0, [r0, #4]
    }
    case kTfLiteInt32: {
      return UnpackImpl<int32_t>(context, node, input, data->num, data->axis);
    }
    case kTfLiteUInt8: {
      return UnpackImpl<uint8_t>(context, node, input, data->num, data->axis);
   e23ee:	6852      	ldr	r2, [r2, #4]
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
   e23f0:	2638      	movs	r6, #56	; 0x38
   e23f2:	fb06 5500 	mla	r5, r6, r0, r5
  const int dimensions = input_dims->size;

  if (axis < 0) {
   e23f6:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
   e23f8:	68af      	ldr	r7, [r5, #8]
  const int dimensions = input_dims->size;
   e23fa:	f8dc 5000 	ldr.w	r5, [ip]

  if (axis < 0) {
    axis += NumDimensions(input);
   e23fe:	bfb8      	it	lt
   e2400:	1952      	addlt	r2, r2, r5
  }

  TFLITE_DCHECK_LT(axis, dimensions);
   e2402:	4295      	cmp	r5, r2
   e2404:	f77f af21 	ble.w	e224a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
   e2408:	46e6      	mov	lr, ip
   e240a:	2000      	movs	r0, #0
   e240c:	2601      	movs	r6, #1

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e240e:	4282      	cmp	r2, r0
   e2410:	dd05      	ble.n	e241e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x22e>
    outer_size *= input_dims->data[i];
   e2412:	f85e 8f04 	ldr.w	r8, [lr, #4]!
  }

  TFLITE_DCHECK_LT(axis, dimensions);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e2416:	3001      	adds	r0, #1
    outer_size *= input_dims->data[i];
   e2418:	fb08 f606 	mul.w	r6, r8, r6
   e241c:	e7f7      	b.n	e240e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x21e>
   e241e:	1c50      	adds	r0, r2, #1
   e2420:	f105 0e01 	add.w	lr, r5, #1
  }
  int copy_size = 1;
   e2424:	2201      	movs	r2, #1
   e2426:	3001      	adds	r0, #1
  for (int i = axis + 1; i < dimensions; ++i) {
   e2428:	4570      	cmp	r0, lr
   e242a:	d003      	beq.n	e2434 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x244>
    copy_size *= input_dims->data[i];
   e242c:	f85c 5020 	ldr.w	r5, [ip, r0, lsl #2]
   e2430:	436a      	muls	r2, r5
   e2432:	e7f8      	b.n	e2426 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x236>
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
   e2434:	f8d7 c000 	ldr.w	ip, [r7]
   e2438:	4638      	mov	r0, r7
   e243a:	2501      	movs	r5, #1
   e243c:	2700      	movs	r7, #0
   e243e:	45bc      	cmp	ip, r7
   e2440:	dd05      	ble.n	e244e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x25e>
    output_size *= output_dims->data[i];
   e2442:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= input_dims->data[i];
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
   e2446:	3701      	adds	r7, #1
    output_size *= output_dims->data[i];
   e2448:	fb0e f505 	mul.w	r5, lr, r5
   e244c:	e7f7      	b.n	e243e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x24e>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);
   e244e:	fb02 f006 	mul.w	r0, r2, r6
   e2452:	4285      	cmp	r5, r0
   e2454:	f47f aef9 	bne.w	e224a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
   e2458:	6858      	ldr	r0, [r3, #4]
   e245a:	9b02      	ldr	r3, [sp, #8]
   e245c:	4247      	negs	r7, r0
   e245e:	fb02 f903 	mul.w	r9, r2, r3

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
   e2462:	2500      	movs	r5, #0
   e2464:	9b02      	ldr	r3, [sp, #8]
   e2466:	42ab      	cmp	r3, r5
   e2468:	f77f af24 	ble.w	e22b4 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc4>
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e246c:	684b      	ldr	r3, [r1, #4]
   e246e:	9c01      	ldr	r4, [sp, #4]
   e2470:	eb03 0385 	add.w	r3, r3, r5, lsl #2
   e2474:	68a4      	ldr	r4, [r4, #8]
   e2476:	685b      	ldr	r3, [r3, #4]
   e2478:	f04f 0e38 	mov.w	lr, #56	; 0x38
   e247c:	fb0e 4303 	mla	r3, lr, r3, r4
   e2480:	b103      	cbz	r3, e2484 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x294>
   e2482:	685b      	ldr	r3, [r3, #4]
   e2484:	46be      	mov	lr, r7
   e2486:	4684      	mov	ip, r0
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   e2488:	f04f 0800 	mov.w	r8, #0
   e248c:	4546      	cmp	r6, r8
   e248e:	dd11      	ble.n	e24b4 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2c4>
   e2490:	461c      	mov	r4, r3
   e2492:	46e2      	mov	sl, ip
      T* output_ptr = output_data + copy_size * k;
      int loc = k * output_count * copy_size + i * copy_size;
      const T* input_ptr = input_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e2494:	eb0a 0b0e 	add.w	fp, sl, lr
   e2498:	455a      	cmp	r2, fp
   e249a:	dd04      	ble.n	e24a6 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2b6>
   e249c:	f81a bb01 	ldrb.w	fp, [sl], #1
   e24a0:	f804 bb01 	strb.w	fp, [r4], #1
   e24a4:	e7f6      	b.n	e2494 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2a4>
  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   e24a6:	f108 0801 	add.w	r8, r8, #1
   e24aa:	44cc      	add	ip, r9
   e24ac:	4413      	add	r3, r2
   e24ae:	ebc9 0e0e 	rsb	lr, r9, lr
   e24b2:	e7eb      	b.n	e248c <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x29c>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
   e24b4:	3501      	adds	r5, #1
   e24b6:	4410      	add	r0, r2
   e24b8:	1abf      	subs	r7, r7, r2
   e24ba:	e7d3      	b.n	e2464 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x274>
    }
    case kTfLiteUInt8: {
      return UnpackImpl<uint8_t>(context, node, input, data->num, data->axis);
    }
    case kTfLiteInt8: {
      return UnpackImpl<int8_t>(context, node, input, data->num, data->axis);
   e24bc:	6810      	ldr	r0, [r2, #0]
   e24be:	9002      	str	r0, [sp, #8]
}

template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
   e24c0:	6848      	ldr	r0, [r1, #4]
  const TfLiteIntArray* input_dims = input->dims;
   e24c2:	f8d3 c008 	ldr.w	ip, [r3, #8]
  const TfLiteIntArray* output_dims = output0->dims;
   e24c6:	6840      	ldr	r0, [r0, #4]
    }
    case kTfLiteUInt8: {
      return UnpackImpl<uint8_t>(context, node, input, data->num, data->axis);
    }
    case kTfLiteInt8: {
      return UnpackImpl<int8_t>(context, node, input, data->num, data->axis);
   e24c8:	6852      	ldr	r2, [r2, #4]
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
   e24ca:	2638      	movs	r6, #56	; 0x38
   e24cc:	fb06 5500 	mla	r5, r6, r0, r5
  const int dimensions = input_dims->size;

  if (axis < 0) {
   e24d0:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
   e24d2:	68af      	ldr	r7, [r5, #8]
  const int dimensions = input_dims->size;
   e24d4:	f8dc 5000 	ldr.w	r5, [ip]

  if (axis < 0) {
    axis += NumDimensions(input);
   e24d8:	bfb8      	it	lt
   e24da:	1952      	addlt	r2, r2, r5
  }

  TFLITE_DCHECK_LT(axis, dimensions);
   e24dc:	4295      	cmp	r5, r2
   e24de:	f77f aeb4 	ble.w	e224a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
   e24e2:	46e6      	mov	lr, ip
   e24e4:	2000      	movs	r0, #0
   e24e6:	2601      	movs	r6, #1

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e24e8:	4282      	cmp	r2, r0
   e24ea:	dd05      	ble.n	e24f8 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x308>
    outer_size *= input_dims->data[i];
   e24ec:	f85e 8f04 	ldr.w	r8, [lr, #4]!
  }

  TFLITE_DCHECK_LT(axis, dimensions);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e24f0:	3001      	adds	r0, #1
    outer_size *= input_dims->data[i];
   e24f2:	fb08 f606 	mul.w	r6, r8, r6
   e24f6:	e7f7      	b.n	e24e8 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2f8>
   e24f8:	1c50      	adds	r0, r2, #1
   e24fa:	f105 0e01 	add.w	lr, r5, #1
  }
  int copy_size = 1;
   e24fe:	2201      	movs	r2, #1
   e2500:	3001      	adds	r0, #1
  for (int i = axis + 1; i < dimensions; ++i) {
   e2502:	4570      	cmp	r0, lr
   e2504:	d003      	beq.n	e250e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x31e>
    copy_size *= input_dims->data[i];
   e2506:	f85c 5020 	ldr.w	r5, [ip, r0, lsl #2]
   e250a:	436a      	muls	r2, r5
   e250c:	e7f8      	b.n	e2500 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x310>
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
   e250e:	f8d7 c000 	ldr.w	ip, [r7]
   e2512:	4638      	mov	r0, r7
   e2514:	2501      	movs	r5, #1
   e2516:	2700      	movs	r7, #0
   e2518:	45bc      	cmp	ip, r7
   e251a:	dd05      	ble.n	e2528 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x338>
    output_size *= output_dims->data[i];
   e251c:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= input_dims->data[i];
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
   e2520:	3701      	adds	r7, #1
    output_size *= output_dims->data[i];
   e2522:	fb0e f505 	mul.w	r5, lr, r5
   e2526:	e7f7      	b.n	e2518 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x328>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);
   e2528:	fb02 f006 	mul.w	r0, r2, r6
   e252c:	4285      	cmp	r5, r0
   e252e:	f47f ae8c 	bne.w	e224a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
   e2532:	6858      	ldr	r0, [r3, #4]
   e2534:	9b02      	ldr	r3, [sp, #8]
   e2536:	4247      	negs	r7, r0
   e2538:	fb02 f903 	mul.w	r9, r2, r3

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
   e253c:	2500      	movs	r5, #0
   e253e:	9b02      	ldr	r3, [sp, #8]
   e2540:	42ab      	cmp	r3, r5
   e2542:	f77f aeb7 	ble.w	e22b4 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc4>
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e2546:	684b      	ldr	r3, [r1, #4]
   e2548:	9c01      	ldr	r4, [sp, #4]
   e254a:	eb03 0385 	add.w	r3, r3, r5, lsl #2
   e254e:	68a4      	ldr	r4, [r4, #8]
   e2550:	685b      	ldr	r3, [r3, #4]
   e2552:	f04f 0e38 	mov.w	lr, #56	; 0x38
   e2556:	fb0e 4303 	mla	r3, lr, r3, r4
   e255a:	b103      	cbz	r3, e255e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x36e>
   e255c:	685b      	ldr	r3, [r3, #4]
   e255e:	46be      	mov	lr, r7
   e2560:	4684      	mov	ip, r0
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   e2562:	f04f 0800 	mov.w	r8, #0
   e2566:	4546      	cmp	r6, r8
   e2568:	dd11      	ble.n	e258e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x39e>
   e256a:	461c      	mov	r4, r3
   e256c:	46e2      	mov	sl, ip
      T* output_ptr = output_data + copy_size * k;
      int loc = k * output_count * copy_size + i * copy_size;
      const T* input_ptr = input_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e256e:	eb0a 0b0e 	add.w	fp, sl, lr
   e2572:	455a      	cmp	r2, fp
   e2574:	dd04      	ble.n	e2580 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x390>
   e2576:	f91a bb01 	ldrsb.w	fp, [sl], #1
   e257a:	f804 bb01 	strb.w	fp, [r4], #1
   e257e:	e7f6      	b.n	e256e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x37e>
  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   e2580:	f108 0801 	add.w	r8, r8, #1
   e2584:	44cc      	add	ip, r9
   e2586:	4413      	add	r3, r2
   e2588:	ebc9 0e0e 	rsb	lr, r9, lr
   e258c:	e7eb      	b.n	e2566 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x376>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
   e258e:	3501      	adds	r5, #1
   e2590:	4410      	add	r0, r2
   e2592:	1abf      	subs	r7, r7, r2
   e2594:	e7d3      	b.n	e253e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x34e>
    }
    case kTfLiteInt8: {
      return UnpackImpl<int8_t>(context, node, input, data->num, data->axis);
    }
    default: {
      context->ReportError(context, "Type '%s' is not supported by unpack.",
   e2596:	9b01      	ldr	r3, [sp, #4]
   e2598:	695d      	ldr	r5, [r3, #20]
   e259a:	f7f1 fdbd 	bl	d4118 <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type));
   e259e:	4904      	ldr	r1, [pc, #16]	; (e25b0 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3c0>)
   e25a0:	4602      	mov	r2, r0
   e25a2:	9801      	ldr	r0, [sp, #4]
   e25a4:	47a8      	blx	r5
      return kTfLiteError;
   e25a6:	2001      	movs	r0, #1
    }
  }

  return kTfLiteOk;
}
   e25a8:	b005      	add	sp, #20
   e25aa:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e25ae:	bf00      	nop
   e25b0:	000eaeb1 	.word	0x000eaeb1

000e25b4 <_ZN6tflite3ops5micro15Register_UNPACKEv>:

TfLiteRegistration* Register_UNPACK() {
  static TfLiteRegistration r = {nullptr, nullptr, unpack::Prepare,
                                 unpack::Eval};
  return &r;
}
   e25b4:	4800      	ldr	r0, [pc, #0]	; (e25b8 <_ZN6tflite3ops5micro15Register_UNPACKEv+0x4>)
   e25b6:	4770      	bx	lr
   e25b8:	2003c228 	.word	0x2003c228

000e25bc <_ZN6tflite3ops5micro14depthwise_conv4InitEP13TfLiteContextPKcj>:

}  // namespace

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
   e25bc:	2000      	movs	r0, #0
   e25be:	4770      	bx	lr

000e25c0 <_ZN6tflite3ops5micro14depthwise_conv4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
   e25c0:	4770      	bx	lr

000e25c2 <_ZN6tflite3ops5micro14depthwise_conv7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   e25c2:	2000      	movs	r0, #0
   e25c4:	4770      	bx	lr

000e25c6 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa>:
    const DepthwiseParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
   e25c6:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e25ca:	b0a5      	sub	sp, #148	; 0x94
   e25cc:	469b      	mov	fp, r3
  // Get parameters.
  // TODO(b/141565753): Re-introduce ScopedProfilingLabel on Micro.
  const int stride_width = params.stride_width;
   e25ce:	f9b0 300a 	ldrsh.w	r3, [r0, #10]
   e25d2:	930f      	str	r3, [sp, #60]	; 0x3c
  const int stride_height = params.stride_height;
   e25d4:	f9b0 300c 	ldrsh.w	r3, [r0, #12]
   e25d8:	9310      	str	r3, [sp, #64]	; 0x40
  const int dilation_width_factor = params.dilation_width_factor;
   e25da:	f9b0 300e 	ldrsh.w	r3, [r0, #14]
   e25de:	9311      	str	r3, [sp, #68]	; 0x44
  const int dilation_height_factor = params.dilation_height_factor;
   e25e0:	f9b0 3010 	ldrsh.w	r3, [r0, #16]
   e25e4:	9312      	str	r3, [sp, #72]	; 0x48
  const int pad_width = params.padding_values.width;
   e25e6:	f9b0 3002 	ldrsh.w	r3, [r0, #2]
   e25ea:	9313      	str	r3, [sp, #76]	; 0x4c
  const int pad_height = params.padding_values.height;
   e25ec:	f9b0 3004 	ldrsh.w	r3, [r0, #4]
   e25f0:	9314      	str	r3, [sp, #80]	; 0x50
  const int depth_multiplier = params.depth_multiplier;
   e25f2:	f9b0 3012 	ldrsh.w	r3, [r0, #18]
   e25f6:	9308      	str	r3, [sp, #32]
  const int32 input_offset = params.input_offset;
   e25f8:	6943      	ldr	r3, [r0, #20]
   e25fa:	9315      	str	r3, [sp, #84]	; 0x54
  const int32 output_offset = params.output_offset;
   e25fc:	69c3      	ldr	r3, [r0, #28]
   e25fe:	9316      	str	r3, [sp, #88]	; 0x58
  const int32 output_activation_min = params.quantized_activation_min;
   e2600:	6a83      	ldr	r3, [r0, #40]	; 0x28
   e2602:	930b      	str	r3, [sp, #44]	; 0x2c
  const int32 output_activation_max = params.quantized_activation_max;
   e2604:	6ac3      	ldr	r3, [r0, #44]	; 0x2c
   e2606:	930c      	str	r3, [sp, #48]	; 0x30

  // Check dimensions of the tensors.
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e2608:	f8db 3000 	ldr.w	r3, [fp]
    const DepthwiseParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
   e260c:	9122      	str	r1, [sp, #136]	; 0x88
  const int32 output_offset = params.output_offset;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;

  // Check dimensions of the tensors.
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e260e:	2b04      	cmp	r3, #4
    const DepthwiseParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
   e2610:	9223      	str	r2, [sp, #140]	; 0x8c
   e2612:	f8dd a0cc 	ldr.w	sl, [sp, #204]	; 0xcc
  const int32 output_offset = params.output_offset;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;

  // Check dimensions of the tensors.
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e2616:	d001      	beq.n	e261c <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x56>
   e2618:	f001 fe88 	bl	e432c <abort>
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
   e261c:	9b2f      	ldr	r3, [sp, #188]	; 0xbc
   e261e:	681b      	ldr	r3, [r3, #0]
   e2620:	2b04      	cmp	r3, #4
   e2622:	d1f9      	bne.n	e2618 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x52>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   e2624:	f8da 3000 	ldr.w	r3, [sl]
   e2628:	2b04      	cmp	r3, #4
   e262a:	d1f5      	bne.n	e2618 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x52>

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   e262c:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e262e:	9a0c      	ldr	r2, [sp, #48]	; 0x30
   e2630:	4293      	cmp	r3, r2
   e2632:	dcf1      	bgt.n	e2618 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x52>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   e2634:	2300      	movs	r3, #0
   e2636:	4619      	mov	r1, r3
   e2638:	4652      	mov	r2, sl
   e263a:	4658      	mov	r0, fp
   e263c:	f7f9 fabf 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e2640:	2303      	movs	r3, #3
   e2642:	4619      	mov	r1, r3
   e2644:	4652      	mov	r2, sl
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   e2646:	9017      	str	r0, [sp, #92]	; 0x5c
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e2648:	982f      	ldr	r0, [sp, #188]	; 0xbc
   e264a:	f7f9 fab8 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
   e264e:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e2650:	4604      	mov	r4, r0
  const int input_height = input_shape.Dims(1);
   e2652:	4658      	mov	r0, fp
   e2654:	f7f3 ff02 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   e2658:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
   e265a:	9018      	str	r0, [sp, #96]	; 0x60
  const int input_width = input_shape.Dims(2);
   e265c:	4658      	mov	r0, fp
   e265e:	f7f3 fefd 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_depth = input_shape.Dims(3);
   e2662:	2103      	movs	r1, #3

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   e2664:	9019      	str	r0, [sp, #100]	; 0x64
  const int input_depth = input_shape.Dims(3);
   e2666:	4658      	mov	r0, fp
   e2668:	f7f3 fef8 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_height = filter_shape.Dims(1);
   e266c:	2101      	movs	r1, #1
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
   e266e:	900d      	str	r0, [sp, #52]	; 0x34
  const int filter_height = filter_shape.Dims(1);
   e2670:	982f      	ldr	r0, [sp, #188]	; 0xbc
   e2672:	f7f3 fef3 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_width = filter_shape.Dims(2);
   e2676:	2102      	movs	r1, #2
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
   e2678:	901a      	str	r0, [sp, #104]	; 0x68
  const int filter_width = filter_shape.Dims(2);
   e267a:	982f      	ldr	r0, [sp, #188]	; 0xbc
   e267c:	f7f3 feee 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   e2680:	2101      	movs	r1, #1
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
   e2682:	901b      	str	r0, [sp, #108]	; 0x6c
  const int output_height = output_shape.Dims(1);
   e2684:	4650      	mov	r0, sl
   e2686:	f7f3 fee9 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   e268a:	2102      	movs	r1, #2
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   e268c:	901c      	str	r0, [sp, #112]	; 0x70
  const int output_width = output_shape.Dims(2);
   e268e:	4650      	mov	r0, sl
   e2690:	f7f3 fee4 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
   e2694:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   e2696:	9a08      	ldr	r2, [sp, #32]
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   e2698:	901d      	str	r0, [sp, #116]	; 0x74
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
   e269a:	4353      	muls	r3, r2
   e269c:	429c      	cmp	r4, r3
   e269e:	d1bb      	bne.n	e2618 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x52>
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
   e26a0:	9831      	ldr	r0, [sp, #196]	; 0xc4
   e26a2:	f7f9 fa7c 	bl	dbb9e <_ZNK6tflite12RuntimeShape8FlatSizeEv>
   e26a6:	4284      	cmp	r4, r0
   e26a8:	d1b6      	bne.n	e2618 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x52>
   e26aa:	f04f 0900 	mov.w	r9, #0

  for (int batch = 0; batch < batches; ++batch) {
   e26ae:	9b17      	ldr	r3, [sp, #92]	; 0x5c
   e26b0:	4599      	cmp	r9, r3
   e26b2:	f280 80ab 	bge.w	e280c <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x246>
   e26b6:	9b14      	ldr	r3, [sp, #80]	; 0x50
   e26b8:	425b      	negs	r3, r3
   e26ba:	9309      	str	r3, [sp, #36]	; 0x24
   e26bc:	2300      	movs	r3, #0
   e26be:	9303      	str	r3, [sp, #12]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   e26c0:	9b03      	ldr	r3, [sp, #12]
   e26c2:	9a1c      	ldr	r2, [sp, #112]	; 0x70
   e26c4:	4293      	cmp	r3, r2
   e26c6:	f280 8083 	bge.w	e27d0 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x20a>
   e26ca:	9b13      	ldr	r3, [sp, #76]	; 0x4c
   e26cc:	425b      	negs	r3, r3
   e26ce:	930a      	str	r3, [sp, #40]	; 0x28
   e26d0:	2300      	movs	r3, #0
   e26d2:	9304      	str	r3, [sp, #16]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   e26d4:	9b04      	ldr	r3, [sp, #16]
   e26d6:	9a1d      	ldr	r2, [sp, #116]	; 0x74
   e26d8:	4293      	cmp	r3, r2
   e26da:	da71      	bge.n	e27c0 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1fa>
   e26dc:	f04f 0800 	mov.w	r8, #0
   e26e0:	f8cd 8014 	str.w	r8, [sp, #20]
        for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
   e26e4:	9b05      	ldr	r3, [sp, #20]
   e26e6:	9a0d      	ldr	r2, [sp, #52]	; 0x34
   e26e8:	4293      	cmp	r3, r2
   e26ea:	da61      	bge.n	e27b0 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1ea>
   e26ec:	9a32      	ldr	r2, [sp, #200]	; 0xc8
   e26ee:	ea4f 0388 	mov.w	r3, r8, lsl #2
   e26f2:	441a      	add	r2, r3
   e26f4:	9221      	str	r2, [sp, #132]	; 0x84
   e26f6:	9a22      	ldr	r2, [sp, #136]	; 0x88
   e26f8:	441a      	add	r2, r3
   e26fa:	9220      	str	r2, [sp, #128]	; 0x80
   e26fc:	9a23      	ldr	r2, [sp, #140]	; 0x8c
   e26fe:	18d3      	adds	r3, r2, r3
   e2700:	931f      	str	r3, [sp, #124]	; 0x7c
   e2702:	2400      	movs	r4, #0
          for (int m = 0; m < depth_multiplier; ++m) {
   e2704:	9b08      	ldr	r3, [sp, #32]
   e2706:	429c      	cmp	r4, r3
   e2708:	da4c      	bge.n	e27a4 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1de>
   e270a:	eb08 0304 	add.w	r3, r8, r4
            const int output_channel = m + in_channel * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            int32 acc = 0;
   e270e:	2500      	movs	r5, #0

  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
          for (int m = 0; m < depth_multiplier; ++m) {
   e2710:	9e09      	ldr	r6, [sp, #36]	; 0x24
   e2712:	930e      	str	r3, [sp, #56]	; 0x38
            const int output_channel = m + in_channel * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            int32 acc = 0;
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   e2714:	9506      	str	r5, [sp, #24]
   e2716:	9b06      	ldr	r3, [sp, #24]
   e2718:	9a1a      	ldr	r2, [sp, #104]	; 0x68
   e271a:	4293      	cmp	r3, r2
   e271c:	da1c      	bge.n	e2758 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x192>
   e271e:	2300      	movs	r3, #0
   e2720:	9f0a      	ldr	r7, [sp, #40]	; 0x28
   e2722:	9307      	str	r3, [sp, #28]
              for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   e2724:	9b07      	ldr	r3, [sp, #28]
   e2726:	9a1b      	ldr	r2, [sp, #108]	; 0x6c
   e2728:	4293      	cmp	r3, r2
   e272a:	da0f      	bge.n	e274c <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x186>
                const int in_x = in_x_origin + dilation_width_factor * filter_x;
                const int in_y =
                    in_y_origin + dilation_height_factor * filter_y;
                // Zero padding by omitting the areas outside the image.
                const bool is_point_inside_image =
                    (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
   e272c:	2f00      	cmp	r7, #0
   e272e:	db07      	blt.n	e2740 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x17a>
   e2730:	9b19      	ldr	r3, [sp, #100]	; 0x64
   e2732:	42bb      	cmp	r3, r7
   e2734:	dd04      	ble.n	e2740 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x17a>
   e2736:	2e00      	cmp	r6, #0
   e2738:	db02      	blt.n	e2740 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x17a>
   e273a:	9b18      	ldr	r3, [sp, #96]	; 0x60
   e273c:	42b3      	cmp	r3, r6
   e273e:	dc4a      	bgt.n	e27d6 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x210>
            const int output_channel = m + in_channel * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            int32 acc = 0;
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
              for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   e2740:	9b07      	ldr	r3, [sp, #28]
   e2742:	3301      	adds	r3, #1
   e2744:	9307      	str	r3, [sp, #28]
   e2746:	9b11      	ldr	r3, [sp, #68]	; 0x44
   e2748:	441f      	add	r7, r3
   e274a:	e7eb      	b.n	e2724 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x15e>
          for (int m = 0; m < depth_multiplier; ++m) {
            const int output_channel = m + in_channel * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            int32 acc = 0;
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   e274c:	9b06      	ldr	r3, [sp, #24]
   e274e:	3301      	adds	r3, #1
   e2750:	9306      	str	r3, [sp, #24]
   e2752:	9b12      	ldr	r3, [sp, #72]	; 0x48
   e2754:	441e      	add	r6, r3
   e2756:	e7de      	b.n	e2716 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x150>
                  // accumulator depth is smaller than 2^16.
                  acc += filter_val * (input_val + input_offset);
                }
              }
            }
            if (bias_data) {
   e2758:	9b32      	ldr	r3, [sp, #200]	; 0xc8
   e275a:	b11b      	cbz	r3, e2764 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x19e>
              acc += bias_data[output_channel];
   e275c:	9b21      	ldr	r3, [sp, #132]	; 0x84
   e275e:	f853 3024 	ldr.w	r3, [r3, r4, lsl #2]
   e2762:	441d      	add	r5, r3
            }
            acc = MultiplyByQuantizedMultiplier(
   e2764:	9b1f      	ldr	r3, [sp, #124]	; 0x7c
   e2766:	f853 2024 	ldr.w	r2, [r3, r4, lsl #2]
   e276a:	9b20      	ldr	r3, [sp, #128]	; 0x80
   e276c:	4628      	mov	r0, r5
   e276e:	f853 1024 	ldr.w	r1, [r3, r4, lsl #2]
   e2772:	f7f9 fa33 	bl	dbbdc <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
                acc, output_multiplier[output_channel],
                output_shift[output_channel]);
            acc += output_offset;
   e2776:	9b16      	ldr	r3, [sp, #88]	; 0x58
            acc = std::max(acc, output_activation_min);
            acc = std::min(acc, output_activation_max);
            output_data[Offset(output_shape, batch, out_y, out_x,
   e2778:	9a03      	ldr	r2, [sp, #12]
              acc += bias_data[output_channel];
            }
            acc = MultiplyByQuantizedMultiplier(
                acc, output_multiplier[output_channel],
                output_shift[output_channel]);
            acc += output_offset;
   e277a:	4418      	add	r0, r3
   e277c:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e277e:	4283      	cmp	r3, r0
   e2780:	bfb8      	it	lt
   e2782:	4603      	movlt	r3, r0
   e2784:	461d      	mov	r5, r3
   e2786:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   e2788:	429d      	cmp	r5, r3
   e278a:	bfa8      	it	ge
   e278c:	461d      	movge	r5, r3
            acc = std::max(acc, output_activation_min);
            acc = std::min(acc, output_activation_max);
            output_data[Offset(output_shape, batch, out_y, out_x,
   e278e:	9b0e      	ldr	r3, [sp, #56]	; 0x38
   e2790:	9300      	str	r3, [sp, #0]
   e2792:	4649      	mov	r1, r9
   e2794:	9b04      	ldr	r3, [sp, #16]
   e2796:	4650      	mov	r0, sl
   e2798:	f7f3 fec5 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                               output_channel)] = static_cast<int8_t>(acc);
   e279c:	9b34      	ldr	r3, [sp, #208]	; 0xd0

  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
          for (int m = 0; m < depth_multiplier; ++m) {
   e279e:	3401      	adds	r4, #1
                output_shift[output_channel]);
            acc += output_offset;
            acc = std::max(acc, output_activation_min);
            acc = std::min(acc, output_activation_max);
            output_data[Offset(output_shape, batch, out_y, out_x,
                               output_channel)] = static_cast<int8_t>(acc);
   e27a0:	541d      	strb	r5, [r3, r0]

  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
          for (int m = 0; m < depth_multiplier; ++m) {
   e27a2:	e7af      	b.n	e2704 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x13e>
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
   e27a4:	9b05      	ldr	r3, [sp, #20]
   e27a6:	3301      	adds	r3, #1
   e27a8:	9305      	str	r3, [sp, #20]
   e27aa:	9b08      	ldr	r3, [sp, #32]
   e27ac:	4498      	add	r8, r3
   e27ae:	e799      	b.n	e26e4 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x11e>
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   e27b0:	9b04      	ldr	r3, [sp, #16]
   e27b2:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
   e27b4:	3301      	adds	r3, #1
   e27b6:	9304      	str	r3, [sp, #16]
   e27b8:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e27ba:	4413      	add	r3, r2
   e27bc:	930a      	str	r3, [sp, #40]	; 0x28
   e27be:	e789      	b.n	e26d4 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x10e>
  const int output_width = output_shape.Dims(2);
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   e27c0:	9b03      	ldr	r3, [sp, #12]
   e27c2:	9a10      	ldr	r2, [sp, #64]	; 0x40
   e27c4:	3301      	adds	r3, #1
   e27c6:	9303      	str	r3, [sp, #12]
   e27c8:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e27ca:	4413      	add	r3, r2
   e27cc:	9309      	str	r3, [sp, #36]	; 0x24
   e27ce:	e777      	b.n	e26c0 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0xfa>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int batch = 0; batch < batches; ++batch) {
   e27d0:	f109 0901 	add.w	r9, r9, #1
   e27d4:	e76b      	b.n	e26ae <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0xe8>
                // Zero padding by omitting the areas outside the image.
                const bool is_point_inside_image =
                    (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height);
                if (is_point_inside_image) {
                  int32 input_val = input_data[Offset(input_shape, batch, in_y,
   e27d6:	9b05      	ldr	r3, [sp, #20]
   e27d8:	9300      	str	r3, [sp, #0]
   e27da:	4632      	mov	r2, r6
   e27dc:	463b      	mov	r3, r7
   e27de:	4649      	mov	r1, r9
   e27e0:	4658      	mov	r0, fp
   e27e2:	f7f3 fea0 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                                                      in_x, in_channel)];
                  int32 filter_val = filter_data[Offset(
   e27e6:	9b0e      	ldr	r3, [sp, #56]	; 0x38
                // Zero padding by omitting the areas outside the image.
                const bool is_point_inside_image =
                    (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height);
                if (is_point_inside_image) {
                  int32 input_val = input_data[Offset(input_shape, batch, in_y,
   e27e8:	901e      	str	r0, [sp, #120]	; 0x78
                                                      in_x, in_channel)];
                  int32 filter_val = filter_data[Offset(
   e27ea:	9300      	str	r3, [sp, #0]
   e27ec:	9a06      	ldr	r2, [sp, #24]
   e27ee:	9b07      	ldr	r3, [sp, #28]
   e27f0:	982f      	ldr	r0, [sp, #188]	; 0xbc
   e27f2:	2100      	movs	r1, #0
   e27f4:	f7f3 fe97 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                  // long as the filter size (filter_y * filter_x * in_channel)
                  // does not exceed 2^16, which is the case in all the models
                  // we have seen so far.
                  // TODO(jianlijianli): Add a check to make sure the
                  // accumulator depth is smaller than 2^16.
                  acc += filter_val * (input_val + input_offset);
   e27f8:	9a1e      	ldr	r2, [sp, #120]	; 0x78
   e27fa:	9b2e      	ldr	r3, [sp, #184]	; 0xb8
   e27fc:	569b      	ldrsb	r3, [r3, r2]
   e27fe:	9a15      	ldr	r2, [sp, #84]	; 0x54
   e2800:	4413      	add	r3, r2
   e2802:	9a30      	ldr	r2, [sp, #192]	; 0xc0
   e2804:	5612      	ldrsb	r2, [r2, r0]
   e2806:	fb02 5503 	mla	r5, r2, r3, r5
   e280a:	e799      	b.n	e2740 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x17a>
          }
        }
      }
    }
  }
}
   e280c:	b025      	add	sp, #148	; 0x94
   e280e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

000e2814 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf>:
inline void DepthwiseConv(
    const DepthwiseParams& params, const RuntimeShape& input_shape,
    const float* input_data, const RuntimeShape& filter_shape,
    const float* filter_data, const RuntimeShape& bias_shape,
    const float* bias_data, const RuntimeShape& output_shape,
    float* output_data) {
   e2814:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e2818:	ed2d 8b04 	vpush	{d8-d9}
   e281c:	b09d      	sub	sp, #116	; 0x74
   e281e:	4698      	mov	r8, r3
  const int stride_width = params.stride_width;
   e2820:	f9b0 300a 	ldrsh.w	r3, [r0, #10]
   e2824:	930d      	str	r3, [sp, #52]	; 0x34
  const int stride_height = params.stride_height;
   e2826:	f9b0 300c 	ldrsh.w	r3, [r0, #12]
   e282a:	930e      	str	r3, [sp, #56]	; 0x38
  const int dilation_width_factor = params.dilation_width_factor;
   e282c:	f9b0 300e 	ldrsh.w	r3, [r0, #14]
   e2830:	930f      	str	r3, [sp, #60]	; 0x3c
  const int dilation_height_factor = params.dilation_height_factor;
   e2832:	f9b0 3010 	ldrsh.w	r3, [r0, #16]
   e2836:	9310      	str	r3, [sp, #64]	; 0x40
  const int pad_width = params.padding_values.width;
   e2838:	f9b0 3002 	ldrsh.w	r3, [r0, #2]
   e283c:	9311      	str	r3, [sp, #68]	; 0x44
  const int pad_height = params.padding_values.height;
   e283e:	f9b0 3004 	ldrsh.w	r3, [r0, #4]
   e2842:	9312      	str	r3, [sp, #72]	; 0x48
  const int depth_multiplier = params.depth_multiplier;
   e2844:	f9b0 3012 	ldrsh.w	r3, [r0, #18]
   e2848:	9307      	str	r3, [sp, #28]
  const float output_activation_min = params.float_activation_min;
  const float output_activation_max = params.float_activation_max;
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e284a:	680b      	ldr	r3, [r1, #0]
inline void DepthwiseConv(
    const DepthwiseParams& params, const RuntimeShape& input_shape,
    const float* input_data, const RuntimeShape& filter_shape,
    const float* filter_data, const RuntimeShape& bias_shape,
    const float* bias_data, const RuntimeShape& output_shape,
    float* output_data) {
   e284c:	921b      	str	r2, [sp, #108]	; 0x6c
  const int pad_width = params.padding_values.width;
  const int pad_height = params.padding_values.height;
  const int depth_multiplier = params.depth_multiplier;
  const float output_activation_min = params.float_activation_min;
  const float output_activation_max = params.float_activation_max;
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e284e:	2b04      	cmp	r3, #4
inline void DepthwiseConv(
    const DepthwiseParams& params, const RuntimeShape& input_shape,
    const float* input_data, const RuntimeShape& filter_shape,
    const float* filter_data, const RuntimeShape& bias_shape,
    const float* bias_data, const RuntimeShape& output_shape,
    float* output_data) {
   e2850:	460f      	mov	r7, r1
   e2852:	f8dd 90b4 	ldr.w	r9, [sp, #180]	; 0xb4
  const int dilation_width_factor = params.dilation_width_factor;
  const int dilation_height_factor = params.dilation_height_factor;
  const int pad_width = params.padding_values.width;
  const int pad_height = params.padding_values.height;
  const int depth_multiplier = params.depth_multiplier;
  const float output_activation_min = params.float_activation_min;
   e2856:	edd0 8a0c 	vldr	s17, [r0, #48]	; 0x30
  const float output_activation_max = params.float_activation_max;
   e285a:	ed90 9a0d 	vldr	s18, [r0, #52]	; 0x34
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e285e:	d001      	beq.n	e2864 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x50>
   e2860:	f001 fd64 	bl	e432c <abort>
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
   e2864:	f8d8 3000 	ldr.w	r3, [r8]
   e2868:	2b04      	cmp	r3, #4
   e286a:	d1f9      	bne.n	e2860 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x4c>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   e286c:	f8d9 3000 	ldr.w	r3, [r9]
   e2870:	2b04      	cmp	r3, #4
   e2872:	d1f5      	bne.n	e2860 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x4c>

  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   e2874:	2300      	movs	r3, #0
   e2876:	4619      	mov	r1, r3
   e2878:	464a      	mov	r2, r9
   e287a:	4638      	mov	r0, r7
   e287c:	f7f9 f99f 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e2880:	2303      	movs	r3, #3
   e2882:	4619      	mov	r1, r3
   e2884:	464a      	mov	r2, r9
  const float output_activation_max = params.float_activation_max;
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   e2886:	9013      	str	r0, [sp, #76]	; 0x4c
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e2888:	4640      	mov	r0, r8
   e288a:	f7f9 f998 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
   e288e:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e2890:	4604      	mov	r4, r0
  const int input_height = input_shape.Dims(1);
   e2892:	4638      	mov	r0, r7
   e2894:	f7f3 fde2 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   e2898:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
   e289a:	9014      	str	r0, [sp, #80]	; 0x50
  const int input_width = input_shape.Dims(2);
   e289c:	4638      	mov	r0, r7
   e289e:	f7f3 fddd 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_depth = input_shape.Dims(3);
   e28a2:	2103      	movs	r1, #3
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   e28a4:	9015      	str	r0, [sp, #84]	; 0x54
  const int input_depth = input_shape.Dims(3);
   e28a6:	4638      	mov	r0, r7
   e28a8:	f7f3 fdd8 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_height = filter_shape.Dims(1);
   e28ac:	2101      	movs	r1, #1

  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
   e28ae:	900b      	str	r0, [sp, #44]	; 0x2c
  const int filter_height = filter_shape.Dims(1);
   e28b0:	4640      	mov	r0, r8
   e28b2:	f7f3 fdd3 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_width = filter_shape.Dims(2);
   e28b6:	2102      	movs	r1, #2
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
   e28b8:	9016      	str	r0, [sp, #88]	; 0x58
  const int filter_width = filter_shape.Dims(2);
   e28ba:	4640      	mov	r0, r8
   e28bc:	f7f3 fdce 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   e28c0:	2101      	movs	r1, #1
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
   e28c2:	9017      	str	r0, [sp, #92]	; 0x5c
  const int output_height = output_shape.Dims(1);
   e28c4:	4648      	mov	r0, r9
   e28c6:	f7f3 fdc9 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   e28ca:	2102      	movs	r1, #2
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   e28cc:	9018      	str	r0, [sp, #96]	; 0x60
  const int output_width = output_shape.Dims(2);
   e28ce:	4648      	mov	r0, r9
   e28d0:	f7f3 fdc4 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
   e28d4:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e28d6:	9a07      	ldr	r2, [sp, #28]
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   e28d8:	9019      	str	r0, [sp, #100]	; 0x64
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
   e28da:	4353      	muls	r3, r2
   e28dc:	429c      	cmp	r4, r3
   e28de:	d1bf      	bne.n	e2860 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x4c>
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
   e28e0:	982b      	ldr	r0, [sp, #172]	; 0xac
   e28e2:	f7f9 f95c 	bl	dbb9e <_ZNK6tflite12RuntimeShape8FlatSizeEv>
   e28e6:	4284      	cmp	r4, r0
   e28e8:	d1ba      	bne.n	e2860 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x4c>
   e28ea:	f04f 0b00 	mov.w	fp, #0

  for (int b = 0; b < batches; ++b) {
   e28ee:	9b13      	ldr	r3, [sp, #76]	; 0x4c
   e28f0:	459b      	cmp	fp, r3
   e28f2:	f280 80ad 	bge.w	e2a50 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x23c>
   e28f6:	9b12      	ldr	r3, [sp, #72]	; 0x48
   e28f8:	425b      	negs	r3, r3
   e28fa:	9309      	str	r3, [sp, #36]	; 0x24
   e28fc:	2300      	movs	r3, #0
   e28fe:	9302      	str	r3, [sp, #8]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   e2900:	9b02      	ldr	r3, [sp, #8]
   e2902:	9a18      	ldr	r2, [sp, #96]	; 0x60
   e2904:	4293      	cmp	r3, r2
   e2906:	f280 80a0 	bge.w	e2a4a <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x236>
   e290a:	9b11      	ldr	r3, [sp, #68]	; 0x44
   e290c:	425b      	negs	r3, r3
   e290e:	9308      	str	r3, [sp, #32]
   e2910:	2300      	movs	r3, #0
   e2912:	9303      	str	r3, [sp, #12]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   e2914:	9b03      	ldr	r3, [sp, #12]
   e2916:	9a19      	ldr	r2, [sp, #100]	; 0x64
   e2918:	4293      	cmp	r3, r2
   e291a:	f280 808e 	bge.w	e2a3a <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x226>
   e291e:	2400      	movs	r4, #0
   e2920:	9404      	str	r4, [sp, #16]
        for (int ic = 0; ic < input_depth; ++ic) {
   e2922:	9b04      	ldr	r3, [sp, #16]
   e2924:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   e2926:	4293      	cmp	r3, r2
   e2928:	da7f      	bge.n	e2a2a <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x216>
   e292a:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   e292c:	eb03 0384 	add.w	r3, r3, r4, lsl #2
   e2930:	930a      	str	r3, [sp, #40]	; 0x28
   e2932:	2300      	movs	r3, #0
   e2934:	9305      	str	r3, [sp, #20]
          for (int m = 0; m < depth_multiplier; m++) {
   e2936:	9b05      	ldr	r3, [sp, #20]
   e2938:	9a07      	ldr	r2, [sp, #28]
   e293a:	4293      	cmp	r3, r2
   e293c:	da6f      	bge.n	e2a1e <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x20a>
   e293e:	4423      	add	r3, r4
   e2940:	930c      	str	r3, [sp, #48]	; 0x30
   e2942:	9d09      	ldr	r5, [sp, #36]	; 0x24
            const int oc = m + ic * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            float total = 0.f;
   e2944:	ed9f 8a45 	vldr	s16, [pc, #276]	; e2a5c <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x248>
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   e2948:	2300      	movs	r3, #0
   e294a:	9306      	str	r3, [sp, #24]
   e294c:	9b06      	ldr	r3, [sp, #24]
   e294e:	9a16      	ldr	r2, [sp, #88]	; 0x58
   e2950:	4293      	cmp	r3, r2
   e2952:	da38      	bge.n	e29c6 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x1b2>
   e2954:	9e08      	ldr	r6, [sp, #32]
   e2956:	f04f 0a00 	mov.w	sl, #0
              for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   e295a:	9b17      	ldr	r3, [sp, #92]	; 0x5c
   e295c:	459a      	cmp	sl, r3
   e295e:	da2c      	bge.n	e29ba <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x1a6>
                const int in_x = in_x_origin + dilation_width_factor * filter_x;
                const int in_y =
                    in_y_origin + dilation_height_factor * filter_y;
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
   e2960:	2e00      	cmp	r6, #0
   e2962:	db25      	blt.n	e29b0 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x19c>
   e2964:	9b15      	ldr	r3, [sp, #84]	; 0x54
   e2966:	42b3      	cmp	r3, r6
   e2968:	dd22      	ble.n	e29b0 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x19c>
   e296a:	2d00      	cmp	r5, #0
   e296c:	db20      	blt.n	e29b0 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x19c>
   e296e:	9b14      	ldr	r3, [sp, #80]	; 0x50
   e2970:	42ab      	cmp	r3, r5
   e2972:	dd1d      	ble.n	e29b0 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x19c>
                    (in_y < input_height)) {
                  float input_value =
                      input_data[Offset(input_shape, b, in_y, in_x, ic)];
   e2974:	9b04      	ldr	r3, [sp, #16]
   e2976:	9300      	str	r3, [sp, #0]
   e2978:	462a      	mov	r2, r5
   e297a:	4633      	mov	r3, r6
   e297c:	4659      	mov	r1, fp
   e297e:	4638      	mov	r0, r7
   e2980:	f7f3 fdd1 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                  float filter_value = filter_data[Offset(
   e2984:	9b0c      	ldr	r3, [sp, #48]	; 0x30
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height)) {
                  float input_value =
                      input_data[Offset(input_shape, b, in_y, in_x, ic)];
   e2986:	901a      	str	r0, [sp, #104]	; 0x68
                  float filter_value = filter_data[Offset(
   e2988:	9300      	str	r3, [sp, #0]
   e298a:	9a06      	ldr	r2, [sp, #24]
   e298c:	4653      	mov	r3, sl
   e298e:	2100      	movs	r1, #0
   e2990:	4640      	mov	r0, r8
   e2992:	f7f3 fdc8 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height)) {
                  float input_value =
                      input_data[Offset(input_shape, b, in_y, in_x, ic)];
   e2996:	9a1a      	ldr	r2, [sp, #104]	; 0x68
   e2998:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
   e299a:	eb03 0382 	add.w	r3, r3, r2, lsl #2
                  float filter_value = filter_data[Offset(
                      filter_shape, 0, filter_y, filter_x, oc)];
   e299e:	9a2a      	ldr	r2, [sp, #168]	; 0xa8
                  total += (input_value * filter_value);
   e29a0:	ed93 7a00 	vldr	s14, [r3]
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height)) {
                  float input_value =
                      input_data[Offset(input_shape, b, in_y, in_x, ic)];
                  float filter_value = filter_data[Offset(
                      filter_shape, 0, filter_y, filter_x, oc)];
   e29a4:	eb02 0080 	add.w	r0, r2, r0, lsl #2
                  total += (input_value * filter_value);
   e29a8:	edd0 7a00 	vldr	s15, [r0]
   e29ac:	eea7 8a27 	vfma.f32	s16, s14, s15
   e29b0:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
            const int oc = m + ic * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            float total = 0.f;
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
              for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   e29b2:	f10a 0a01 	add.w	sl, sl, #1
   e29b6:	441e      	add	r6, r3
   e29b8:	e7cf      	b.n	e295a <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x146>
          for (int m = 0; m < depth_multiplier; m++) {
            const int oc = m + ic * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            float total = 0.f;
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   e29ba:	9b06      	ldr	r3, [sp, #24]
   e29bc:	3301      	adds	r3, #1
   e29be:	9306      	str	r3, [sp, #24]
   e29c0:	9b10      	ldr	r3, [sp, #64]	; 0x40
   e29c2:	441d      	add	r5, r3
   e29c4:	e7c2      	b.n	e294c <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x138>
                  total += (input_value * filter_value);
                }
              }
            }
            float bias_value = 0.0f;
            if (bias_data) {
   e29c6:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   e29c8:	b11b      	cbz	r3, e29d2 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x1be>
              bias_value = bias_data[oc];
   e29ca:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e29cc:	edd3 9a00 	vldr	s19, [r3]
   e29d0:	e001      	b.n	e29d6 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x1c2>
                      filter_shape, 0, filter_y, filter_x, oc)];
                  total += (input_value * filter_value);
                }
              }
            }
            float bias_value = 0.0f;
   e29d2:	eddf 9a22 	vldr	s19, [pc, #136]	; e2a5c <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x248>
            if (bias_data) {
              bias_value = bias_data[oc];
            }
            output_data[Offset(output_shape, b, out_y, out_x, oc)] =
   e29d6:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   e29d8:	9300      	str	r3, [sp, #0]
   e29da:	9a02      	ldr	r2, [sp, #8]
   e29dc:	9b03      	ldr	r3, [sp, #12]
   e29de:	4659      	mov	r1, fp
   e29e0:	4648      	mov	r0, r9
   e29e2:	f7f3 fda0 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   e29e6:	9b2e      	ldr	r3, [sp, #184]	; 0xb8
                ActivationFunctionWithMinMax(total + bias_value,
   e29e8:	ee78 7a29 	vadd.f32	s15, s16, s19
            }
            float bias_value = 0.0f;
            if (bias_data) {
              bias_value = bias_data[oc];
            }
            output_data[Offset(output_shape, b, out_y, out_x, oc)] =
   e29ec:	eb03 0080 	add.w	r0, r3, r0, lsl #2
      return __a;
   e29f0:	eef4 8ae7 	vcmpe.f32	s17, s15

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int ic = 0; ic < input_depth; ++ic) {
          for (int m = 0; m < depth_multiplier; m++) {
   e29f4:	9b05      	ldr	r3, [sp, #20]
   e29f6:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e29fa:	bfc8      	it	gt
   e29fc:	eef0 7a68 	vmovgt.f32	s15, s17
   e2a00:	3301      	adds	r3, #1
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   e2a02:	eeb4 9a67 	vcmp.f32	s18, s15
   e2a06:	9305      	str	r3, [sp, #20]
   e2a08:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e2a0a:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e2a0e:	bf48      	it	mi
   e2a10:	eef0 7a49 	vmovmi.f32	s15, s18
   e2a14:	3304      	adds	r3, #4
              bias_value = bias_data[oc];
            }
            output_data[Offset(output_shape, b, out_y, out_x, oc)] =
                ActivationFunctionWithMinMax(total + bias_value,
                                             output_activation_min,
                                             output_activation_max);
   e2a16:	edc0 7a00 	vstr	s15, [r0]
   e2a1a:	930a      	str	r3, [sp, #40]	; 0x28

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int ic = 0; ic < input_depth; ++ic) {
          for (int m = 0; m < depth_multiplier; m++) {
   e2a1c:	e78b      	b.n	e2936 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x122>
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int ic = 0; ic < input_depth; ++ic) {
   e2a1e:	9b04      	ldr	r3, [sp, #16]
   e2a20:	3301      	adds	r3, #1
   e2a22:	9304      	str	r3, [sp, #16]
   e2a24:	9b07      	ldr	r3, [sp, #28]
   e2a26:	441c      	add	r4, r3
   e2a28:	e77b      	b.n	e2922 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x10e>
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   e2a2a:	9b03      	ldr	r3, [sp, #12]
   e2a2c:	9a0d      	ldr	r2, [sp, #52]	; 0x34
   e2a2e:	3301      	adds	r3, #1
   e2a30:	9303      	str	r3, [sp, #12]
   e2a32:	9b08      	ldr	r3, [sp, #32]
   e2a34:	4413      	add	r3, r2
   e2a36:	9308      	str	r3, [sp, #32]
   e2a38:	e76c      	b.n	e2914 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x100>
  const int output_width = output_shape.Dims(2);
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   e2a3a:	9b02      	ldr	r3, [sp, #8]
   e2a3c:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   e2a3e:	3301      	adds	r3, #1
   e2a40:	9302      	str	r3, [sp, #8]
   e2a42:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e2a44:	4413      	add	r3, r2
   e2a46:	9309      	str	r3, [sp, #36]	; 0x24
   e2a48:	e75a      	b.n	e2900 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0xec>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int b = 0; b < batches; ++b) {
   e2a4a:	f10b 0b01 	add.w	fp, fp, #1
   e2a4e:	e74e      	b.n	e28ee <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0xda>
          }
        }
      }
    }
  }
}
   e2a50:	b01d      	add	sp, #116	; 0x74
   e2a52:	ecbd 8b04 	vpop	{d8-d9}
   e2a56:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e2a5a:	bf00      	nop
   e2a5c:	00000000 	.word	0x00000000

000e2a60 <_ZN6tflite3ops5micro26Register_DEPTHWISE_CONV_2DEv>:

TfLiteRegistration* Register_DEPTHWISE_CONV_2D() {
  static TfLiteRegistration r = {depthwise_conv::Init, depthwise_conv::Free,
                                 depthwise_conv::Prepare, depthwise_conv::Eval};
  return &r;
}
   e2a60:	4800      	ldr	r0, [pc, #0]	; (e2a64 <_ZN6tflite3ops5micro26Register_DEPTHWISE_CONV_2DEv+0x4>)
   e2a62:	4770      	bx	lr
   e2a64:	2003c248 	.word	0x2003c248

000e2a68 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph>:
         right_shift;
}

template <DepthwiseConvOutputRounding output_rounding>
struct DepthwiseConvBasicKernel {
  static inline void Run(const DepthwiseParams& params,
   e2a68:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e2a6c:	b0a5      	sub	sp, #148	; 0x94
   e2a6e:	469a      	mov	sl, r3
                         const uint8* input_data,
                         const RuntimeShape& filter_shape,
                         const uint8* filter_data,
                         const RuntimeShape& bias_shape, const int32* bias_data,
                         const RuntimeShape& output_shape, uint8* output_data) {
    const int stride_width = params.stride_width;
   e2a70:	f9b0 300a 	ldrsh.w	r3, [r0, #10]
   e2a74:	930f      	str	r3, [sp, #60]	; 0x3c
    const int stride_height = params.stride_height;
   e2a76:	f9b0 300c 	ldrsh.w	r3, [r0, #12]
   e2a7a:	9310      	str	r3, [sp, #64]	; 0x40
    const int dilation_width_factor = params.dilation_width_factor;
   e2a7c:	f9b0 300e 	ldrsh.w	r3, [r0, #14]
   e2a80:	9311      	str	r3, [sp, #68]	; 0x44
    const int dilation_height_factor = params.dilation_height_factor;
   e2a82:	f9b0 3010 	ldrsh.w	r3, [r0, #16]
   e2a86:	9312      	str	r3, [sp, #72]	; 0x48
    const int pad_width = params.padding_values.width;
   e2a88:	f9b0 3002 	ldrsh.w	r3, [r0, #2]
   e2a8c:	9313      	str	r3, [sp, #76]	; 0x4c
    const int pad_height = params.padding_values.height;
   e2a8e:	f9b0 3004 	ldrsh.w	r3, [r0, #4]
   e2a92:	9314      	str	r3, [sp, #80]	; 0x50
    const int depth_multiplier = params.depth_multiplier;
   e2a94:	f9b0 3012 	ldrsh.w	r3, [r0, #18]
   e2a98:	9308      	str	r3, [sp, #32]
    const int32 output_activation_min = params.quantized_activation_min;
   e2a9a:	6a83      	ldr	r3, [r0, #40]	; 0x28
   e2a9c:	930b      	str	r3, [sp, #44]	; 0x2c
    const int32 output_activation_max = params.quantized_activation_max;
   e2a9e:	6ac3      	ldr	r3, [r0, #44]	; 0x2c
   e2aa0:	930c      	str	r3, [sp, #48]	; 0x30
    const int32 input_offset = params.input_offset;
   e2aa2:	6943      	ldr	r3, [r0, #20]
   e2aa4:	9315      	str	r3, [sp, #84]	; 0x54
    const int32 filter_offset = params.weights_offset;
   e2aa6:	6983      	ldr	r3, [r0, #24]
   e2aa8:	9316      	str	r3, [sp, #88]	; 0x58
    const int32 output_offset = params.output_offset;
   e2aaa:	69c3      	ldr	r3, [r0, #28]
   e2aac:	9317      	str	r3, [sp, #92]	; 0x5c
    const int32 output_multiplier = params.output_multiplier;
   e2aae:	6a03      	ldr	r3, [r0, #32]
   e2ab0:	9318      	str	r3, [sp, #96]	; 0x60
    const int output_shift = params.output_shift;
   e2ab2:	6a43      	ldr	r3, [r0, #36]	; 0x24
   e2ab4:	9319      	str	r3, [sp, #100]	; 0x64
    TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e2ab6:	680b      	ldr	r3, [r1, #0]
         right_shift;
}

template <DepthwiseConvOutputRounding output_rounding>
struct DepthwiseConvBasicKernel {
  static inline void Run(const DepthwiseParams& params,
   e2ab8:	9223      	str	r2, [sp, #140]	; 0x8c
    const int32 input_offset = params.input_offset;
    const int32 filter_offset = params.weights_offset;
    const int32 output_offset = params.output_offset;
    const int32 output_multiplier = params.output_multiplier;
    const int output_shift = params.output_shift;
    TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e2aba:	2b04      	cmp	r3, #4
         right_shift;
}

template <DepthwiseConvOutputRounding output_rounding>
struct DepthwiseConvBasicKernel {
  static inline void Run(const DepthwiseParams& params,
   e2abc:	4689      	mov	r9, r1
    const int32 input_offset = params.input_offset;
    const int32 filter_offset = params.weights_offset;
    const int32 output_offset = params.output_offset;
    const int32 output_multiplier = params.output_multiplier;
    const int output_shift = params.output_shift;
    TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e2abe:	d001      	beq.n	e2ac4 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x5c>
   e2ac0:	f001 fc34 	bl	e432c <abort>
    TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
   e2ac4:	f8da 3000 	ldr.w	r3, [sl]
   e2ac8:	2b04      	cmp	r3, #4
   e2aca:	d1f9      	bne.n	e2ac0 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x58>
    TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   e2acc:	9b31      	ldr	r3, [sp, #196]	; 0xc4
   e2ace:	681b      	ldr	r3, [r3, #0]
   e2ad0:	2b04      	cmp	r3, #4
   e2ad2:	d1f5      	bne.n	e2ac0 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x58>

    TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   e2ad4:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e2ad6:	9a0c      	ldr	r2, [sp, #48]	; 0x30
   e2ad8:	4293      	cmp	r3, r2
   e2ada:	dcf1      	bgt.n	e2ac0 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x58>
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   e2adc:	2300      	movs	r3, #0
   e2ade:	4619      	mov	r1, r3
   e2ae0:	9a31      	ldr	r2, [sp, #196]	; 0xc4
   e2ae2:	4648      	mov	r0, r9
   e2ae4:	f7f9 f86b 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e2ae8:	2303      	movs	r3, #3
   e2aea:	4619      	mov	r1, r3
   e2aec:	9a31      	ldr	r2, [sp, #196]	; 0xc4
    TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
    TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
    TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

    TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   e2aee:	901a      	str	r0, [sp, #104]	; 0x68
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e2af0:	4650      	mov	r0, sl
   e2af2:	f7f9 f864 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
    const int input_height = input_shape.Dims(1);
   e2af6:	2101      	movs	r1, #1
    TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
    TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

    TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e2af8:	4604      	mov	r4, r0
    const int input_height = input_shape.Dims(1);
   e2afa:	4648      	mov	r0, r9
   e2afc:	f7f3 fcae 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
    const int input_width = input_shape.Dims(2);
   e2b00:	2102      	movs	r1, #2
    TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

    TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
    const int input_height = input_shape.Dims(1);
   e2b02:	901b      	str	r0, [sp, #108]	; 0x6c
    const int input_width = input_shape.Dims(2);
   e2b04:	4648      	mov	r0, r9
   e2b06:	f7f3 fca9 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
    const int input_depth = input_shape.Dims(3);
   e2b0a:	2103      	movs	r1, #3

    TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
    const int input_height = input_shape.Dims(1);
    const int input_width = input_shape.Dims(2);
   e2b0c:	901c      	str	r0, [sp, #112]	; 0x70
    const int input_depth = input_shape.Dims(3);
   e2b0e:	4648      	mov	r0, r9
   e2b10:	f7f3 fca4 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
    const int filter_height = filter_shape.Dims(1);
   e2b14:	2101      	movs	r1, #1
    TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
    const int input_height = input_shape.Dims(1);
    const int input_width = input_shape.Dims(2);
    const int input_depth = input_shape.Dims(3);
   e2b16:	900d      	str	r0, [sp, #52]	; 0x34
    const int filter_height = filter_shape.Dims(1);
   e2b18:	4650      	mov	r0, sl
   e2b1a:	f7f3 fc9f 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
    const int filter_width = filter_shape.Dims(2);
   e2b1e:	2102      	movs	r1, #2
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
    const int input_height = input_shape.Dims(1);
    const int input_width = input_shape.Dims(2);
    const int input_depth = input_shape.Dims(3);
    const int filter_height = filter_shape.Dims(1);
   e2b20:	901d      	str	r0, [sp, #116]	; 0x74
    const int filter_width = filter_shape.Dims(2);
   e2b22:	4650      	mov	r0, sl
   e2b24:	f7f3 fc9a 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
    const int output_height = output_shape.Dims(1);
   e2b28:	2101      	movs	r1, #1
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
    const int input_height = input_shape.Dims(1);
    const int input_width = input_shape.Dims(2);
    const int input_depth = input_shape.Dims(3);
    const int filter_height = filter_shape.Dims(1);
    const int filter_width = filter_shape.Dims(2);
   e2b2a:	901e      	str	r0, [sp, #120]	; 0x78
    const int output_height = output_shape.Dims(1);
   e2b2c:	9831      	ldr	r0, [sp, #196]	; 0xc4
   e2b2e:	f7f3 fc95 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
    const int output_width = output_shape.Dims(2);
   e2b32:	2102      	movs	r1, #2
    const int input_height = input_shape.Dims(1);
    const int input_width = input_shape.Dims(2);
    const int input_depth = input_shape.Dims(3);
    const int filter_height = filter_shape.Dims(1);
    const int filter_width = filter_shape.Dims(2);
    const int output_height = output_shape.Dims(1);
   e2b34:	901f      	str	r0, [sp, #124]	; 0x7c
    const int output_width = output_shape.Dims(2);
   e2b36:	9831      	ldr	r0, [sp, #196]	; 0xc4
   e2b38:	f7f3 fc90 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
    TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
   e2b3c:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   e2b3e:	9a08      	ldr	r2, [sp, #32]
    const int input_width = input_shape.Dims(2);
    const int input_depth = input_shape.Dims(3);
    const int filter_height = filter_shape.Dims(1);
    const int filter_width = filter_shape.Dims(2);
    const int output_height = output_shape.Dims(1);
    const int output_width = output_shape.Dims(2);
   e2b40:	9020      	str	r0, [sp, #128]	; 0x80
    TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
   e2b42:	4353      	muls	r3, r2
   e2b44:	429c      	cmp	r4, r3
   e2b46:	d1bb      	bne.n	e2ac0 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x58>
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
   e2b48:	982f      	ldr	r0, [sp, #188]	; 0xbc
   e2b4a:	f7f9 f828 	bl	dbb9e <_ZNK6tflite12RuntimeShape8FlatSizeEv>
   e2b4e:	4284      	cmp	r4, r0
   e2b50:	d1b6      	bne.n	e2ac0 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x58>
   e2b52:	f04f 0b00 	mov.w	fp, #0

    for (int b = 0; b < batches; ++b) {
   e2b56:	9b1a      	ldr	r3, [sp, #104]	; 0x68
   e2b58:	459b      	cmp	fp, r3
   e2b5a:	f280 80a1 	bge.w	e2ca0 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x238>
   e2b5e:	9b14      	ldr	r3, [sp, #80]	; 0x50
   e2b60:	425b      	negs	r3, r3
   e2b62:	9309      	str	r3, [sp, #36]	; 0x24
   e2b64:	2300      	movs	r3, #0
   e2b66:	9303      	str	r3, [sp, #12]
      for (int out_y = 0; out_y < output_height; ++out_y) {
   e2b68:	9b03      	ldr	r3, [sp, #12]
   e2b6a:	9a1f      	ldr	r2, [sp, #124]	; 0x7c
   e2b6c:	4293      	cmp	r3, r2
   e2b6e:	f280 8094 	bge.w	e2c9a <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x232>
   e2b72:	9b13      	ldr	r3, [sp, #76]	; 0x4c
   e2b74:	425b      	negs	r3, r3
   e2b76:	930a      	str	r3, [sp, #40]	; 0x28
   e2b78:	2300      	movs	r3, #0
   e2b7a:	9304      	str	r3, [sp, #16]
        for (int out_x = 0; out_x < output_width; ++out_x) {
   e2b7c:	9b04      	ldr	r3, [sp, #16]
   e2b7e:	9a20      	ldr	r2, [sp, #128]	; 0x80
   e2b80:	4293      	cmp	r3, r2
   e2b82:	f280 8082 	bge.w	e2c8a <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x222>
   e2b86:	2500      	movs	r5, #0
   e2b88:	9505      	str	r5, [sp, #20]
          for (int ic = 0; ic < input_depth; ++ic) {
   e2b8a:	9b05      	ldr	r3, [sp, #20]
   e2b8c:	9a0d      	ldr	r2, [sp, #52]	; 0x34
   e2b8e:	4293      	cmp	r3, r2
   e2b90:	da73      	bge.n	e2c7a <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x212>
   e2b92:	9b30      	ldr	r3, [sp, #192]	; 0xc0
   e2b94:	eb03 0385 	add.w	r3, r3, r5, lsl #2
   e2b98:	9322      	str	r3, [sp, #136]	; 0x88
   e2b9a:	2600      	movs	r6, #0
            for (int m = 0; m < depth_multiplier; m++) {
   e2b9c:	9b08      	ldr	r3, [sp, #32]
   e2b9e:	429e      	cmp	r6, r3
   e2ba0:	da65      	bge.n	e2c6e <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x206>
   e2ba2:	1973      	adds	r3, r6, r5
              const int oc = m + ic * depth_multiplier;
              const int in_x_origin = (out_x * stride_width) - pad_width;
              const int in_y_origin = (out_y * stride_height) - pad_height;
              int32 acc = 0;
   e2ba4:	2400      	movs	r4, #0

    for (int b = 0; b < batches; ++b) {
      for (int out_y = 0; out_y < output_height; ++out_y) {
        for (int out_x = 0; out_x < output_width; ++out_x) {
          for (int ic = 0; ic < input_depth; ++ic) {
            for (int m = 0; m < depth_multiplier; m++) {
   e2ba6:	9f09      	ldr	r7, [sp, #36]	; 0x24
   e2ba8:	930e      	str	r3, [sp, #56]	; 0x38
              const int oc = m + ic * depth_multiplier;
              const int in_x_origin = (out_x * stride_width) - pad_width;
              const int in_y_origin = (out_y * stride_height) - pad_height;
              int32 acc = 0;
              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   e2baa:	9406      	str	r4, [sp, #24]
   e2bac:	9b06      	ldr	r3, [sp, #24]
   e2bae:	9a1d      	ldr	r2, [sp, #116]	; 0x74
   e2bb0:	4293      	cmp	r3, r2
   e2bb2:	da3a      	bge.n	e2c2a <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1c2>
   e2bb4:	2300      	movs	r3, #0
   e2bb6:	f8dd 8028 	ldr.w	r8, [sp, #40]	; 0x28
   e2bba:	9307      	str	r3, [sp, #28]
                for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   e2bbc:	9b07      	ldr	r3, [sp, #28]
   e2bbe:	9a1e      	ldr	r2, [sp, #120]	; 0x78
   e2bc0:	4293      	cmp	r3, r2
   e2bc2:	da2c      	bge.n	e2c1e <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1b6>
                      in_x_origin + dilation_width_factor * filter_x;
                  const int in_y =
                      in_y_origin + dilation_height_factor * filter_y;
                  // If the location is outside the bounds of the input image,
                  // use zero as a default value.
                  if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
   e2bc4:	f1b8 0f00 	cmp.w	r8, #0
   e2bc8:	db23      	blt.n	e2c12 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1aa>
   e2bca:	9b1c      	ldr	r3, [sp, #112]	; 0x70
   e2bcc:	4543      	cmp	r3, r8
   e2bce:	dd20      	ble.n	e2c12 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1aa>
   e2bd0:	2f00      	cmp	r7, #0
   e2bd2:	db1e      	blt.n	e2c12 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1aa>
   e2bd4:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
   e2bd6:	42bb      	cmp	r3, r7
   e2bd8:	dd1b      	ble.n	e2c12 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1aa>
                      (in_y < input_height)) {
                    int32 input_val =
                        input_data[Offset(input_shape, b, in_y, in_x, ic)];
   e2bda:	9b05      	ldr	r3, [sp, #20]
   e2bdc:	9300      	str	r3, [sp, #0]
   e2bde:	463a      	mov	r2, r7
   e2be0:	4643      	mov	r3, r8
   e2be2:	4659      	mov	r1, fp
   e2be4:	4648      	mov	r0, r9
   e2be6:	f7f3 fc9e 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                    int32 filter_val = filter_data[Offset(
   e2bea:	9b0e      	ldr	r3, [sp, #56]	; 0x38
                  // If the location is outside the bounds of the input image,
                  // use zero as a default value.
                  if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                      (in_y < input_height)) {
                    int32 input_val =
                        input_data[Offset(input_shape, b, in_y, in_x, ic)];
   e2bec:	9021      	str	r0, [sp, #132]	; 0x84
                    int32 filter_val = filter_data[Offset(
   e2bee:	9300      	str	r3, [sp, #0]
   e2bf0:	9a06      	ldr	r2, [sp, #24]
   e2bf2:	9b07      	ldr	r3, [sp, #28]
   e2bf4:	2100      	movs	r1, #0
   e2bf6:	4650      	mov	r0, sl
   e2bf8:	f7f3 fc95 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                        filter_shape, 0, filter_y, filter_x, oc)];
                    acc += (filter_val + filter_offset) *
   e2bfc:	9b2e      	ldr	r3, [sp, #184]	; 0xb8
   e2bfe:	9a16      	ldr	r2, [sp, #88]	; 0x58
   e2c00:	5c1b      	ldrb	r3, [r3, r0]
   e2c02:	9921      	ldr	r1, [sp, #132]	; 0x84
   e2c04:	4413      	add	r3, r2
   e2c06:	9a23      	ldr	r2, [sp, #140]	; 0x8c
   e2c08:	5c52      	ldrb	r2, [r2, r1]
   e2c0a:	9915      	ldr	r1, [sp, #84]	; 0x54
   e2c0c:	440a      	add	r2, r1
   e2c0e:	fb02 4403 	mla	r4, r2, r3, r4
              const int oc = m + ic * depth_multiplier;
              const int in_x_origin = (out_x * stride_width) - pad_width;
              const int in_y_origin = (out_y * stride_height) - pad_height;
              int32 acc = 0;
              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
                for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   e2c12:	9b07      	ldr	r3, [sp, #28]
   e2c14:	3301      	adds	r3, #1
   e2c16:	9307      	str	r3, [sp, #28]
   e2c18:	9b11      	ldr	r3, [sp, #68]	; 0x44
   e2c1a:	4498      	add	r8, r3
   e2c1c:	e7ce      	b.n	e2bbc <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x154>
            for (int m = 0; m < depth_multiplier; m++) {
              const int oc = m + ic * depth_multiplier;
              const int in_x_origin = (out_x * stride_width) - pad_width;
              const int in_y_origin = (out_y * stride_height) - pad_height;
              int32 acc = 0;
              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   e2c1e:	9b06      	ldr	r3, [sp, #24]
   e2c20:	3301      	adds	r3, #1
   e2c22:	9306      	str	r3, [sp, #24]
   e2c24:	9b12      	ldr	r3, [sp, #72]	; 0x48
   e2c26:	441f      	add	r7, r3
   e2c28:	e7c0      	b.n	e2bac <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x144>
                    acc += (filter_val + filter_offset) *
                           (input_val + input_offset);
                  }
                }
              }
              if (bias_data) {
   e2c2a:	9b30      	ldr	r3, [sp, #192]	; 0xc0
   e2c2c:	b11b      	cbz	r3, e2c36 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1ce>
                acc += bias_data[oc];
   e2c2e:	9b22      	ldr	r3, [sp, #136]	; 0x88
   e2c30:	f853 3026 	ldr.w	r3, [r3, r6, lsl #2]
   e2c34:	441c      	add	r4, r3
}

template <>
inline int32 DepthwiseConvRound<DepthwiseConvOutputRounding::kAwayFromZero>(
    int32 x, int32 quantized_multiplier, int shift) {
  return MultiplyByQuantizedMultiplier(x, quantized_multiplier, shift);
   e2c36:	9a19      	ldr	r2, [sp, #100]	; 0x64
   e2c38:	9918      	ldr	r1, [sp, #96]	; 0x60
   e2c3a:	4620      	mov	r0, r4
   e2c3c:	f7f8 ffce 	bl	dbbdc <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
              if (bias_data) {
                acc += bias_data[oc];
              }
              acc = DepthwiseConvRound<output_rounding>(acc, output_multiplier,
                                                        output_shift);
              acc += output_offset;
   e2c40:	9b17      	ldr	r3, [sp, #92]	; 0x5c
              acc = std::max(acc, output_activation_min);
              acc = std::min(acc, output_activation_max);
              output_data[Offset(output_shape, b, out_y, out_x, oc)] =
   e2c42:	9a03      	ldr	r2, [sp, #12]
              if (bias_data) {
                acc += bias_data[oc];
              }
              acc = DepthwiseConvRound<output_rounding>(acc, output_multiplier,
                                                        output_shift);
              acc += output_offset;
   e2c44:	4418      	add	r0, r3
   e2c46:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e2c48:	4283      	cmp	r3, r0
   e2c4a:	bfb8      	it	lt
   e2c4c:	4603      	movlt	r3, r0
   e2c4e:	461c      	mov	r4, r3
   e2c50:	9b0c      	ldr	r3, [sp, #48]	; 0x30
              acc = std::max(acc, output_activation_min);
              acc = std::min(acc, output_activation_max);
              output_data[Offset(output_shape, b, out_y, out_x, oc)] =
   e2c52:	9831      	ldr	r0, [sp, #196]	; 0xc4
   e2c54:	429c      	cmp	r4, r3
   e2c56:	bfa8      	it	ge
   e2c58:	461c      	movge	r4, r3
   e2c5a:	9b0e      	ldr	r3, [sp, #56]	; 0x38
   e2c5c:	9300      	str	r3, [sp, #0]
   e2c5e:	4659      	mov	r1, fp
   e2c60:	9b04      	ldr	r3, [sp, #16]
   e2c62:	f7f3 fc60 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   e2c66:	9b32      	ldr	r3, [sp, #200]	; 0xc8

    for (int b = 0; b < batches; ++b) {
      for (int out_y = 0; out_y < output_height; ++out_y) {
        for (int out_x = 0; out_x < output_width; ++out_x) {
          for (int ic = 0; ic < input_depth; ++ic) {
            for (int m = 0; m < depth_multiplier; m++) {
   e2c68:	3601      	adds	r6, #1
              acc = DepthwiseConvRound<output_rounding>(acc, output_multiplier,
                                                        output_shift);
              acc += output_offset;
              acc = std::max(acc, output_activation_min);
              acc = std::min(acc, output_activation_max);
              output_data[Offset(output_shape, b, out_y, out_x, oc)] =
   e2c6a:	541c      	strb	r4, [r3, r0]

    for (int b = 0; b < batches; ++b) {
      for (int out_y = 0; out_y < output_height; ++out_y) {
        for (int out_x = 0; out_x < output_width; ++out_x) {
          for (int ic = 0; ic < input_depth; ++ic) {
            for (int m = 0; m < depth_multiplier; m++) {
   e2c6c:	e796      	b.n	e2b9c <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x134>
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

    for (int b = 0; b < batches; ++b) {
      for (int out_y = 0; out_y < output_height; ++out_y) {
        for (int out_x = 0; out_x < output_width; ++out_x) {
          for (int ic = 0; ic < input_depth; ++ic) {
   e2c6e:	9b05      	ldr	r3, [sp, #20]
   e2c70:	3301      	adds	r3, #1
   e2c72:	9305      	str	r3, [sp, #20]
   e2c74:	9b08      	ldr	r3, [sp, #32]
   e2c76:	441d      	add	r5, r3
   e2c78:	e787      	b.n	e2b8a <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x122>
    TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

    for (int b = 0; b < batches; ++b) {
      for (int out_y = 0; out_y < output_height; ++out_y) {
        for (int out_x = 0; out_x < output_width; ++out_x) {
   e2c7a:	9b04      	ldr	r3, [sp, #16]
   e2c7c:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
   e2c7e:	3301      	adds	r3, #1
   e2c80:	9304      	str	r3, [sp, #16]
   e2c82:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e2c84:	4413      	add	r3, r2
   e2c86:	930a      	str	r3, [sp, #40]	; 0x28
   e2c88:	e778      	b.n	e2b7c <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x114>
    const int output_width = output_shape.Dims(2);
    TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

    for (int b = 0; b < batches; ++b) {
      for (int out_y = 0; out_y < output_height; ++out_y) {
   e2c8a:	9b03      	ldr	r3, [sp, #12]
   e2c8c:	9a10      	ldr	r2, [sp, #64]	; 0x40
   e2c8e:	3301      	adds	r3, #1
   e2c90:	9303      	str	r3, [sp, #12]
   e2c92:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e2c94:	4413      	add	r3, r2
   e2c96:	9309      	str	r3, [sp, #36]	; 0x24
   e2c98:	e766      	b.n	e2b68 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x100>
    const int output_height = output_shape.Dims(1);
    const int output_width = output_shape.Dims(2);
    TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

    for (int b = 0; b < batches; ++b) {
   e2c9a:	f10b 0b01 	add.w	fp, fp, #1
   e2c9e:	e75a      	b.n	e2b56 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0xee>
            }
          }
        }
      }
    }
  }
   e2ca0:	b025      	add	sp, #148	; 0x94
   e2ca2:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

000e2ca8 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode>:
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
        GetTensorShape(output), GetTensorData<uint8_t>(output));
  }
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e2ca8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e2cac:	f5ad 7d69 	sub.w	sp, sp, #932	; 0x3a4
   e2cb0:	684b      	ldr	r3, [r1, #4]
   e2cb2:	f8d0 a008 	ldr.w	sl, [r0, #8]
   e2cb6:	930c      	str	r3, [sp, #48]	; 0x30
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   e2cb8:	685b      	ldr	r3, [r3, #4]
   e2cba:	f8d1 b000 	ldr.w	fp, [r1]
  auto* params =
      reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);
   e2cbe:	694d      	ldr	r5, [r1, #20]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e2cc0:	f8db 7008 	ldr.w	r7, [fp, #8]
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
        GetTensorShape(output), GetTensorData<uint8_t>(output));
  }
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e2cc4:	910a      	str	r1, [sp, #40]	; 0x28
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   e2cc6:	2238      	movs	r2, #56	; 0x38
   e2cc8:	fb02 a303 	mla	r3, r2, r3, sl
   e2ccc:	9309      	str	r3, [sp, #36]	; 0x24
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e2cce:	f8db 3004 	ldr.w	r3, [fp, #4]
   e2cd2:	4353      	muls	r3, r2
   e2cd4:	930b      	str	r3, [sp, #44]	; 0x2c
   e2cd6:	eb0a 0903 	add.w	r9, sl, r3
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
   e2cda:	f8db 3000 	ldr.w	r3, [fp]

  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
  const TfLiteTensor* bias =
      (NumInputs(node) == 3) ? GetInput(context, node, kBiasTensor) : nullptr;
   e2cde:	2b03      	cmp	r3, #3
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e2ce0:	bf08      	it	eq
   e2ce2:	f8db 400c 	ldreq.w	r4, [fp, #12]
   e2ce6:	fb02 a707 	mla	r7, r2, r7, sl
   e2cea:	bf08      	it	eq
   e2cec:	fb02 a404 	mlaeq	r4, r2, r4, sl

  const TfLiteType data_type = input->type;
   e2cf0:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   e2cf2:	f81a 2002 	ldrb.w	r2, [sl, r2]
   e2cf6:	920d      	str	r2, [sp, #52]	; 0x34
   e2cf8:	f8d9 2008 	ldr.w	r2, [r9, #8]

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
inline int SizeOfDimension(const TfLiteTensor* t, int dim) {
  return t->dims->data[dim];
   e2cfc:	68d1      	ldr	r1, [r2, #12]
   e2cfe:	6892      	ldr	r2, [r2, #8]
   e2d00:	9213      	str	r2, [sp, #76]	; 0x4c
   e2d02:	68ba      	ldr	r2, [r7, #8]
   e2d04:	9114      	str	r1, [sp, #80]	; 0x50
   e2d06:	68d1      	ldr	r1, [r2, #12]
   e2d08:	6892      	ldr	r2, [r2, #8]
   e2d0a:	9211      	str	r2, [sp, #68]	; 0x44
  int width = SizeOfDimension(input, 2);
  int height = SizeOfDimension(input, 1);
  int filter_width = SizeOfDimension(filter, 2);
  int filter_height = SizeOfDimension(filter, 1);
  int out_width = ComputeOutSize(params->padding, width, filter_width,
   e2d0c:	782a      	ldrb	r2, [r5, #0]
   e2d0e:	920e      	str	r2, [sp, #56]	; 0x38
   e2d10:	686a      	ldr	r2, [r5, #4]
   e2d12:	920f      	str	r2, [sp, #60]	; 0x3c
                                 params->stride_width);
  int out_height = ComputeOutSize(params->padding, height, filter_height,
   e2d14:	68aa      	ldr	r2, [r5, #8]
   e2d16:	9112      	str	r1, [sp, #72]	; 0x48
   e2d18:	9210      	str	r2, [sp, #64]	; 0x40
                                  params->stride_height);
  OpData data;
  if (input->type != kTfLiteFloat32) {
   e2d1a:	9a0d      	ldr	r2, [sp, #52]	; 0x34

  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
  const TfLiteTensor* bias =
      (NumInputs(node) == 3) ? GetInput(context, node, kBiasTensor) : nullptr;
   e2d1c:	bf18      	it	ne
   e2d1e:	2400      	movne	r4, #0
  int out_width = ComputeOutSize(params->padding, width, filter_width,
                                 params->stride_width);
  int out_height = ComputeOutSize(params->padding, height, filter_height,
                                  params->stride_height);
  OpData data;
  if (input->type != kTfLiteFloat32) {
   e2d20:	2a01      	cmp	r2, #1
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
        GetTensorShape(output), GetTensorData<uint8_t>(output));
  }
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e2d22:	4606      	mov	r6, r0
  int out_width = ComputeOutSize(params->padding, width, filter_width,
                                 params->stride_width);
  int out_height = ComputeOutSize(params->padding, height, filter_height,
                                  params->stride_height);
  OpData data;
  if (input->type != kTfLiteFloat32) {
   e2d24:	d02b      	beq.n	e2d7e <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0xd6>
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
   e2d26:	f897 8030 	ldrb.w	r8, [r7, #48]	; 0x30
   e2d2a:	f1b8 0f01 	cmp.w	r8, #1
   e2d2e:	d010      	beq.n	e2d52 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0xaa>
   e2d30:	4baa      	ldr	r3, [pc, #680]	; (e2fdc <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x334>)
   e2d32:	9301      	str	r3, [sp, #4]
   e2d34:	2401      	movs	r4, #1
   e2d36:	4baa      	ldr	r3, [pc, #680]	; (e2fe0 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x338>)
   e2d38:	9300      	str	r3, [sp, #0]
   e2d3a:	9403      	str	r4, [sp, #12]
   e2d3c:	f8cd 8008 	str.w	r8, [sp, #8]
   e2d40:	6945      	ldr	r5, [r0, #20]
   e2d42:	4aa8      	ldr	r2, [pc, #672]	; (e2fe4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x33c>)
   e2d44:	49a8      	ldr	r1, [pc, #672]	; (e2fe8 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x340>)
   e2d46:	f240 13cd 	movw	r3, #461	; 0x1cd
   e2d4a:	47a8      	blx	r5
   e2d4c:	4620      	mov	r0, r4
   e2d4e:	f000 bc8e 	b.w	e366e <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x9c6>
                      kTfLiteAffineQuantization);

    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
   e2d52:	6b7a      	ldr	r2, [r7, #52]	; 0x34
    TF_LITE_ENSURE(context, affine_quantization);
   e2d54:	b92a      	cbnz	r2, e2d62 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0xba>
   e2d56:	4ba5      	ldr	r3, [pc, #660]	; (e2fec <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x344>)
   e2d58:	9300      	str	r3, [sp, #0]
   e2d5a:	6944      	ldr	r4, [r0, #20]
   e2d5c:	f44f 73e9 	mov.w	r3, #466	; 0x1d2
   e2d60:	e006      	b.n	e2d70 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0xc8>
    TF_LITE_ENSURE(context, affine_quantization->scale);
   e2d62:	6812      	ldr	r2, [r2, #0]
   e2d64:	b95a      	cbnz	r2, e2d7e <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0xd6>
   e2d66:	4ba2      	ldr	r3, [pc, #648]	; (e2ff0 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x348>)
   e2d68:	9300      	str	r3, [sp, #0]
   e2d6a:	6944      	ldr	r4, [r0, #20]
   e2d6c:	f240 13d3 	movw	r3, #467	; 0x1d3
   e2d70:	4630      	mov	r0, r6
   e2d72:	4a9c      	ldr	r2, [pc, #624]	; (e2fe4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x33c>)
   e2d74:	499f      	ldr	r1, [pc, #636]	; (e2ff4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x34c>)
   e2d76:	47a0      	blx	r4
   e2d78:	4640      	mov	r0, r8
   e2d7a:	f000 bc78 	b.w	e366e <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x9c6>
                             int height, int filter_width, int filter_height,
                             int out_width, int out_height,
                             const TfLiteType data_type, OpData* data) {
  bool has_bias = node->inputs->size == 3;
  // Check number of inputs/outputs
  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
   e2d7e:	3b02      	subs	r3, #2
   e2d80:	2b01      	cmp	r3, #1
   e2d82:	d908      	bls.n	e2d96 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0xee>
   e2d84:	4b9c      	ldr	r3, [pc, #624]	; (e2ff8 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x350>)
   e2d86:	9300      	str	r3, [sp, #0]
   e2d88:	6974      	ldr	r4, [r6, #20]
   e2d8a:	4a96      	ldr	r2, [pc, #600]	; (e2fe4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x33c>)
   e2d8c:	4999      	ldr	r1, [pc, #612]	; (e2ff4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x34c>)
   e2d8e:	2343      	movs	r3, #67	; 0x43
   e2d90:	4630      	mov	r0, r6
   e2d92:	47a0      	blx	r4
   e2d94:	e2b8      	b.n	e3308 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x660>
  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);
   e2d96:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   e2d98:	681b      	ldr	r3, [r3, #0]
   e2d9a:	2b01      	cmp	r3, #1
   e2d9c:	d00d      	beq.n	e2dba <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x112>
   e2d9e:	9302      	str	r3, [sp, #8]
   e2da0:	4b96      	ldr	r3, [pc, #600]	; (e2ffc <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x354>)
   e2da2:	9301      	str	r3, [sp, #4]
   e2da4:	2201      	movs	r2, #1
   e2da6:	4b96      	ldr	r3, [pc, #600]	; (e3000 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x358>)
   e2da8:	9203      	str	r2, [sp, #12]
   e2daa:	9300      	str	r3, [sp, #0]
   e2dac:	6974      	ldr	r4, [r6, #20]
   e2dae:	4a8d      	ldr	r2, [pc, #564]	; (e2fe4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x33c>)
   e2db0:	498d      	ldr	r1, [pc, #564]	; (e2fe8 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x340>)
   e2db2:	2344      	movs	r3, #68	; 0x44
   e2db4:	4630      	mov	r0, r6
   e2db6:	47a0      	blx	r4
   e2db8:	e2a6      	b.n	e3308 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x660>
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
   e2dba:	696b      	ldr	r3, [r5, #20]
   e2dbc:	f8d5 8018 	ldr.w	r8, [r5, #24]
   e2dc0:	9315      	str	r3, [sp, #84]	; 0x54

inline TfLitePaddingValues ComputePaddingHeightWidth(
    int stride_height, int stride_width, int dilation_rate_height,
    int dilation_rate_width, int in_height, int in_width, int filter_height,
    int filter_width, TfLitePadding padding, int* out_height, int* out_width) {
  *out_width = ComputeOutSize(padding, in_width, filter_width, stride_width,
   e2dc2:	9300      	str	r3, [sp, #0]
   e2dc4:	9a12      	ldr	r2, [sp, #72]	; 0x48
   e2dc6:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   e2dc8:	9914      	ldr	r1, [sp, #80]	; 0x50
   e2dca:	980e      	ldr	r0, [sp, #56]	; 0x38
   e2dcc:	f7f9 fa77 	bl	dc2be <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>
                              dilation_rate_width);
  *out_height = ComputeOutSize(padding, in_height, filter_height, stride_height,
   e2dd0:	f8cd 8000 	str.w	r8, [sp]

inline TfLitePaddingValues ComputePaddingHeightWidth(
    int stride_height, int stride_width, int dilation_rate_height,
    int dilation_rate_width, int in_height, int in_width, int filter_height,
    int filter_width, TfLitePadding padding, int* out_height, int* out_width) {
  *out_width = ComputeOutSize(padding, in_width, filter_width, stride_width,
   e2dd4:	9016      	str	r0, [sp, #88]	; 0x58
                              dilation_rate_width);
  *out_height = ComputeOutSize(padding, in_height, filter_height, stride_height,
   e2dd6:	9b10      	ldr	r3, [sp, #64]	; 0x40
   e2dd8:	9a11      	ldr	r2, [sp, #68]	; 0x44
   e2dda:	9913      	ldr	r1, [sp, #76]	; 0x4c
   e2ddc:	980e      	ldr	r0, [sp, #56]	; 0x38
   e2dde:	f7f9 fa6e 	bl	dc2be <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>
inline int ComputePaddingWithOffset(int stride, int dilation_rate, int in_size,
                                    int filter_size, int out_size,
                                    int* offset) {
  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;
  int total_padding =
      ((out_size - 1) * stride + effective_filter_size - in_size);
   e2de2:	9b11      	ldr	r3, [sp, #68]	; 0x44
   e2de4:	9a15      	ldr	r2, [sp, #84]	; 0x54
   e2de6:	990f      	ldr	r1, [sp, #60]	; 0x3c
   e2de8:	3b01      	subs	r3, #1
   e2dea:	fb08 f803 	mul.w	r8, r8, r3
   e2dee:	9b10      	ldr	r3, [sp, #64]	; 0x40
   e2df0:	f108 0801 	add.w	r8, r8, #1
   e2df4:	3801      	subs	r0, #1
   e2df6:	fb03 8000 	mla	r0, r3, r0, r8
   e2dfa:	9b13      	ldr	r3, [sp, #76]	; 0x4c
   e2dfc:	1ac0      	subs	r0, r0, r3
   e2dfe:	9b12      	ldr	r3, [sp, #72]	; 0x48
   e2e00:	3b01      	subs	r3, #1
   e2e02:	4353      	muls	r3, r2
   e2e04:	9a16      	ldr	r2, [sp, #88]	; 0x58
   e2e06:	3301      	adds	r3, #1
   e2e08:	3a01      	subs	r2, #1
   e2e0a:	fb01 3302 	mla	r3, r1, r2, r3
   e2e0e:	9a14      	ldr	r2, [sp, #80]	; 0x50
   e2e10:	1a9b      	subs	r3, r3, r2
  total_padding = total_padding > 0 ? total_padding : 0;
   e2e12:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
   e2e16:	105a      	asrs	r2, r3, #1
   e2e18:	f003 0301 	and.w	r3, r3, #1
   e2e1c:	9362      	str	r3, [sp, #392]	; 0x188

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training.
  if (data_type != kTfLiteFloat32) {
   e2e1e:	9b0d      	ldr	r3, [sp, #52]	; 0x34
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
   e2e20:	9260      	str	r2, [sp, #384]	; 0x180
   e2e22:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
   e2e26:	1042      	asrs	r2, r0, #1

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training.
  if (data_type != kTfLiteFloat32) {
   e2e28:	2b01      	cmp	r3, #1
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
   e2e2a:	f000 0001 	and.w	r0, r0, #1
   e2e2e:	9261      	str	r2, [sp, #388]	; 0x184
   e2e30:	9063      	str	r0, [sp, #396]	; 0x18c

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training.
  if (data_type != kTfLiteFloat32) {
   e2e32:	d02d      	beq.n	e2e90 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x1e8>
}

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
   e2e34:	f8db 000c 	ldr.w	r0, [fp, #12]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e2e38:	f8db 1004 	ldr.w	r1, [fp, #4]
   e2e3c:	f8db 2008 	ldr.w	r2, [fp, #8]
   e2e40:	2338      	movs	r3, #56	; 0x38

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
   e2e42:	f1b0 3fff 	cmp.w	r0, #4294967295	; 0xffffffff
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e2e46:	fb03 a101 	mla	r1, r3, r1, sl
   e2e4a:	fb03 a202 	mla	r2, r3, r2, sl
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e2e4e:	bf18      	it	ne
   e2e50:	fb03 a300 	mlane	r3, r3, r0, sl
    const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
    const TfLiteTensor* bias =
        GetOptionalInputTensor(context, node, kBiasTensor);
    TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
   e2e54:	a8a6      	add	r0, sp, #664	; 0x298
   e2e56:	9007      	str	r0, [sp, #28]
   e2e58:	a866      	add	r0, sp, #408	; 0x198
   e2e5a:	9006      	str	r0, [sp, #24]
   e2e5c:	a8e7      	add	r0, sp, #924	; 0x39c
   e2e5e:	9005      	str	r0, [sp, #20]
   e2e60:	a8e6      	add	r0, sp, #920	; 0x398
   e2e62:	9004      	str	r0, [sp, #16]
   e2e64:	a865      	add	r0, sp, #404	; 0x194
   e2e66:	9003      	str	r0, [sp, #12]
   e2e68:	a864      	add	r0, sp, #400	; 0x190
   e2e6a:	9002      	str	r0, [sp, #8]
   e2e6c:	f105 0010 	add.w	r0, r5, #16
   e2e70:	9001      	str	r0, [sp, #4]
   e2e72:	980c      	ldr	r0, [sp, #48]	; 0x30
   e2e74:	6840      	ldr	r0, [r0, #4]
   e2e76:	f04f 0e38 	mov.w	lr, #56	; 0x38
   e2e7a:	fb0e a000 	mla	r0, lr, r0, sl
  }
  return nullptr;
   e2e7e:	bf08      	it	eq
   e2e80:	2300      	moveq	r3, #0
   e2e82:	9000      	str	r0, [sp, #0]
   e2e84:	4630      	mov	r0, r6
   e2e86:	f000 fe79 	bl	e3b7c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_>
   e2e8a:	2800      	cmp	r0, #0
   e2e8c:	f040 823c 	bne.w	e3308 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x660>
                                        filter_width, filter_height, out_width,
                                        out_height, data_type, &data));

  // TODO(aselle): Consider whether float conv and quantized conv should be
  // separate ops to avoid dispatch overhead here.
  switch (input->type) {  // Already know in/out types are same.
   e2e90:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e2e92:	f81a 8003 	ldrb.w	r8, [sl, r3]
   e2e96:	f1b8 0f03 	cmp.w	r8, #3
   e2e9a:	f040 80bb 	bne.w	e3014 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x36c>

void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteDepthwiseConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
   e2e9e:	f8d9 3010 	ldr.w	r3, [r9, #16]
  const int32_t output_offset = output->params.zero_point;

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
   e2ea2:	9960      	ldr	r1, [sp, #384]	; 0x180
void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteDepthwiseConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
   e2ea4:	693a      	ldr	r2, [r7, #16]

void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteDepthwiseConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
   e2ea6:	f1c3 0a00 	rsb	sl, r3, #0
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;
   e2eaa:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e2eac:	6918      	ldr	r0, [r3, #16]

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
   e2eae:	f8ad 114a 	strh.w	r1, [sp, #330]	; 0x14a
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
   e2eb2:	2301      	movs	r3, #1
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
   e2eb4:	9961      	ldr	r1, [sp, #388]	; 0x184
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
   e2eb6:	f88d 3148 	strb.w	r3, [sp, #328]	; 0x148
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
   e2eba:	f8ad 114c 	strh.w	r1, [sp, #332]	; 0x14c
  op_params.stride_width = params->stride_width;
   e2ebe:	6869      	ldr	r1, [r5, #4]
   e2ec0:	f8ad 1152 	strh.w	r1, [sp, #338]	; 0x152
  op_params.stride_height = params->stride_height;
   e2ec4:	68a9      	ldr	r1, [r5, #8]
   e2ec6:	f8ad 1154 	strh.w	r1, [sp, #340]	; 0x154
  op_params.dilation_width_factor = 1;
   e2eca:	f8ad 3156 	strh.w	r3, [sp, #342]	; 0x156
  op_params.dilation_height_factor = 1;
   e2ece:	f8ad 3158 	strh.w	r3, [sp, #344]	; 0x158
void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteDepthwiseConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
   e2ed2:	4252      	negs	r2, r2
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
  op_params.depth_multiplier = params->depth_multiplier;
   e2ed4:	68e9      	ldr	r1, [r5, #12]
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
   e2ed6:	9258      	str	r2, [sp, #352]	; 0x160
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
   e2ed8:	9a64      	ldr	r2, [sp, #400]	; 0x190
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
  op_params.depth_multiplier = params->depth_multiplier;
   e2eda:	f8ad 115a 	strh.w	r1, [sp, #346]	; 0x15a
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
   e2ede:	925a      	str	r2, [sp, #360]	; 0x168
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.quantized_activation_min = data->output_activation_min;
   e2ee0:	99e6      	ldr	r1, [sp, #920]	; 0x398
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;
   e2ee2:	9a65      	ldr	r2, [sp, #404]	; 0x194
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.quantized_activation_min = data->output_activation_min;
   e2ee4:	915c      	str	r1, [sp, #368]	; 0x170
  op_params.quantized_activation_max = data->output_activation_max;
   e2ee6:	99e7      	ldr	r1, [sp, #924]	; 0x39c
   e2ee8:	915d      	str	r1, [sp, #372]	; 0x174
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;
   e2eea:	4252      	negs	r2, r2
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
   e2eec:	9059      	str	r0, [sp, #356]	; 0x164
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;

  // Figure out if we can use the optimized path for this set of parameters.
  const int filter_width = GetTensorShape(filter).Dims(2);
   e2eee:	4639      	mov	r1, r7
   e2ef0:	a84d      	add	r0, sp, #308	; 0x134
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;
   e2ef2:	925b      	str	r2, [sp, #364]	; 0x16c
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
   e2ef4:	930b      	str	r3, [sp, #44]	; 0x2c
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  op_params.input_offset = input_offset;
   e2ef6:	f8cd a15c 	str.w	sl, [sp, #348]	; 0x15c
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;

  // Figure out if we can use the optimized path for this set of parameters.
  const int filter_width = GetTensorShape(filter).Dims(2);
   e2efa:	f7f3 fd54 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e2efe:	2102      	movs	r1, #2
   e2f00:	a84d      	add	r0, sp, #308	; 0x134
   e2f02:	f7f3 faab 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   e2f06:	4605      	mov	r5, r0
   e2f08:	a84d      	add	r0, sp, #308	; 0x134
   e2f0a:	f7f3 fa9c 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  const int input_depth = GetTensorShape(input).Dims(3);
   e2f0e:	4649      	mov	r1, r9
   e2f10:	a84d      	add	r0, sp, #308	; 0x134
   e2f12:	f7f3 fd48 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e2f16:	4641      	mov	r1, r8
   e2f18:	a84d      	add	r0, sp, #308	; 0x134
   e2f1a:	f7f3 fa9f 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   e2f1e:	4683      	mov	fp, r0
   e2f20:	a84d      	add	r0, sp, #308	; 0x134
   e2f22:	f7f3 fa90 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  const int output_depth = GetTensorShape(filter).Dims(3);
   e2f26:	4639      	mov	r1, r7
   e2f28:	a84d      	add	r0, sp, #308	; 0x134
   e2f2a:	f7f3 fd3c 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e2f2e:	4641      	mov	r1, r8
   e2f30:	a84d      	add	r0, sp, #308	; 0x134
   e2f32:	f7f3 fa93 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   e2f36:	4680      	mov	r8, r0
   e2f38:	a84d      	add	r0, sp, #308	; 0x134
   e2f3a:	f7f3 fa84 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  const int filter_height = GetTensorShape(filter).Dims(1);
   e2f3e:	4639      	mov	r1, r7
   e2f40:	a84d      	add	r0, sp, #308	; 0x134
   e2f42:	f7f3 fd30 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e2f46:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e2f48:	a84d      	add	r0, sp, #308	; 0x134
   e2f4a:	4619      	mov	r1, r3
   e2f4c:	f7f3 fa86 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
   e2f50:	900b      	str	r0, [sp, #44]	; 0x2c
   e2f52:	a84d      	add	r0, sp, #308	; 0x134
   e2f54:	f7f3 fa77 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  const int needed_size =
      output_depth * filter_width * filter_height * input_depth;
   e2f58:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e2f5a:	fb08 f805 	mul.w	r8, r8, r5
   e2f5e:	fb03 f308 	mul.w	r3, r3, r8
  bool use_optimized_path = false;
  if ((filter_width == 8) && (input_offset == 0) && (input_depth == 1) &&
   e2f62:	2d08      	cmp	r5, #8
  const int filter_width = GetTensorShape(filter).Dims(2);
  const int input_depth = GetTensorShape(input).Dims(3);
  const int output_depth = GetTensorShape(filter).Dims(3);
  const int filter_height = GetTensorShape(filter).Dims(1);
  const int needed_size =
      output_depth * filter_width * filter_height * input_depth;
   e2f64:	fb0b f303 	mul.w	r3, fp, r3
   e2f68:	ad3e      	add	r5, sp, #248	; 0xf8
   e2f6a:	f50d 7886 	add.w	r8, sp, #268	; 0x10c
  bool use_optimized_path = false;
  if ((filter_width == 8) && (input_offset == 0) && (input_depth == 1) &&
   e2f6e:	f040 8351 	bne.w	e3614 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x96c>
   e2f72:	f1ba 0f00 	cmp.w	sl, #0
   e2f76:	f040 834d 	bne.w	e3614 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x96c>
   e2f7a:	f1bb 0f01 	cmp.w	fp, #1
   e2f7e:	f040 8349 	bne.w	e3614 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x96c>
   e2f82:	f5b3 6f80 	cmp.w	r3, #1024	; 0x400
   e2f86:	f300 8345 	bgt.w	e3614 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x96c>
    // with an allocation mechanism available through the context API.
    // Use the address of the node as a proxy for its identity, since we need
    // to ensure the weight values are consistent between calls, and there's
    // no easy way to do that quickly other than relying on the identity of
    // the owning node.
    static TfLiteNode* initialized_node_address = node;
   e2f8a:	f8df b07c 	ldr.w	fp, [pc, #124]	; e3008 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x360>
   e2f8e:	f8df a07c 	ldr.w	sl, [pc, #124]	; e300c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x364>
   e2f92:	f8db 3000 	ldr.w	r3, [fp]
   e2f96:	f013 0f01 	tst.w	r3, #1
   e2f9a:	d109      	bne.n	e2fb0 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x308>
   e2f9c:	4658      	mov	r0, fp
   e2f9e:	f7f1 f88d 	bl	d40bc <__cxa_guard_acquire>
   e2fa2:	b128      	cbz	r0, e2fb0 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x308>
   e2fa4:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e2fa6:	f8ca 3000 	str.w	r3, [sl]
   e2faa:	4658      	mov	r0, fp
   e2fac:	f7f1 f88b 	bl	d40c6 <__cxa_guard_release>
    if (initialized_node_address == node) {
   e2fb0:	f8da 3000 	ldr.w	r3, [sl]
   e2fb4:	9a0a      	ldr	r2, [sp, #40]	; 0x28
   e2fb6:	429a      	cmp	r2, r3
   e2fb8:	f000 808b 	beq.w	e30d2 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x42a>
      use_optimized_path = true;
    } else {
      static bool has_warned = false;
      if (!has_warned) {
   e2fbc:	f8df a050 	ldr.w	sl, [pc, #80]	; e3010 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x368>
   e2fc0:	f89a 3000 	ldrb.w	r3, [sl]
   e2fc4:	2b00      	cmp	r3, #0
   e2fc6:	f040 8325 	bne.w	e3614 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x96c>
        context->ReportError(
            context,
            "Multiple depthwise conv ops match optimization parameters, but "
            "only the first will use the fast path, because there's only one "
            "RAM cache available");
   e2fca:	6973      	ldr	r3, [r6, #20]
   e2fcc:	490d      	ldr	r1, [pc, #52]	; (e3004 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x35c>)
   e2fce:	4630      	mov	r0, r6
   e2fd0:	4798      	blx	r3
        has_warned = true;
   e2fd2:	2301      	movs	r3, #1
   e2fd4:	f88a 3000 	strb.w	r3, [sl]
   e2fd8:	e31c      	b.n	e3614 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x96c>
   e2fda:	bf00      	nop
   e2fdc:	000e9b17 	.word	0x000e9b17
   e2fe0:	000e9b31 	.word	0x000e9b31
   e2fe4:	000eaed7 	.word	0x000eaed7
   e2fe8:	000e98f8 	.word	0x000e98f8
   e2fec:	000e9b4b 	.word	0x000e9b4b
   e2ff0:	000e9b5f 	.word	0x000e9b5f
   e2ff4:	000e9ac8 	.word	0x000e9ac8
   e2ff8:	000e9adf 	.word	0x000e9adf
   e2ffc:	000eb2c5 	.word	0x000eb2c5
   e3000:	000e9b03 	.word	0x000e9b03
   e3004:	000eaf9a 	.word	0x000eaf9a
   e3008:	2003e39c 	.word	0x2003e39c
   e300c:	2003e394 	.word	0x2003e394
   e3010:	2003e398 	.word	0x2003e398
                                        filter_width, filter_height, out_width,
                                        out_height, data_type, &data));

  // TODO(aselle): Consider whether float conv and quantized conv should be
  // separate ops to avoid dispatch overhead here.
  switch (input->type) {  // Already know in/out types are same.
   e3014:	f1b8 0f09 	cmp.w	r8, #9
   e3018:	f040 8101 	bne.w	e321e <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x576>
                             TfLiteDepthwiseConvParams* params, OpData* data,
                             const TfLiteTensor* input,
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output) {
  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
   e301c:	2301      	movs	r3, #1
   e301e:	f88d 3148 	strb.w	r3, [sp, #328]	; 0x148
  op_params.padding_values.width = data->padding.width;
   e3022:	9b60      	ldr	r3, [sp, #384]	; 0x180
   e3024:	f8ad 314a 	strh.w	r3, [sp, #330]	; 0x14a
  op_params.padding_values.height = data->padding.height;
   e3028:	9b61      	ldr	r3, [sp, #388]	; 0x184
   e302a:	f8ad 314c 	strh.w	r3, [sp, #332]	; 0x14c
  op_params.stride_width = params->stride_width;
   e302e:	686b      	ldr	r3, [r5, #4]
   e3030:	f8ad 3152 	strh.w	r3, [sp, #338]	; 0x152
  op_params.stride_height = params->stride_height;
   e3034:	68ab      	ldr	r3, [r5, #8]
   e3036:	f8ad 3154 	strh.w	r3, [sp, #340]	; 0x154
  op_params.dilation_width_factor = params->dilation_width_factor;
   e303a:	696b      	ldr	r3, [r5, #20]
   e303c:	f8ad 3156 	strh.w	r3, [sp, #342]	; 0x156
  op_params.dilation_height_factor = params->dilation_height_factor;
   e3040:	69ab      	ldr	r3, [r5, #24]
   e3042:	f8ad 3158 	strh.w	r3, [sp, #344]	; 0x158
  op_params.depth_multiplier = params->depth_multiplier;
   e3046:	68eb      	ldr	r3, [r5, #12]
   e3048:	f8ad 315a 	strh.w	r3, [sp, #346]	; 0x15a
  op_params.input_offset = -input->params.zero_point;
   e304c:	f8d9 3010 	ldr.w	r3, [r9, #16]
   e3050:	425b      	negs	r3, r3
   e3052:	9357      	str	r3, [sp, #348]	; 0x15c
  op_params.weights_offset = 0;
   e3054:	2300      	movs	r3, #0
   e3056:	9358      	str	r3, [sp, #352]	; 0x160
  op_params.output_offset = output->params.zero_point;
   e3058:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e305a:	691b      	ldr	r3, [r3, #16]
   e305c:	9359      	str	r3, [sp, #356]	; 0x164
  // TODO(b/130439627): Use calculated value for clamping.
  op_params.quantized_activation_min = std::numeric_limits<int8_t>::min();
   e305e:	f06f 037f 	mvn.w	r3, #127	; 0x7f
  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
   e3062:	4649      	mov	r1, r9
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.input_offset = -input->params.zero_point;
  op_params.weights_offset = 0;
  op_params.output_offset = output->params.zero_point;
  // TODO(b/130439627): Use calculated value for clamping.
  op_params.quantized_activation_min = std::numeric_limits<int8_t>::min();
   e3064:	935c      	str	r3, [sp, #368]	; 0x170
  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
   e3066:	a83e      	add	r0, sp, #248	; 0xf8
  op_params.input_offset = -input->params.zero_point;
  op_params.weights_offset = 0;
  op_params.output_offset = output->params.zero_point;
  // TODO(b/130439627): Use calculated value for clamping.
  op_params.quantized_activation_min = std::numeric_limits<int8_t>::min();
  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();
   e3068:	237f      	movs	r3, #127	; 0x7f

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
   e306a:	ae43      	add	r6, sp, #268	; 0x10c
  op_params.input_offset = -input->params.zero_point;
  op_params.weights_offset = 0;
  op_params.output_offset = output->params.zero_point;
  // TODO(b/130439627): Use calculated value for clamping.
  op_params.quantized_activation_min = std::numeric_limits<int8_t>::min();
  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();
   e306c:	935d      	str	r3, [sp, #372]	; 0x174

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
   e306e:	f7f3 fc9a 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorData<int8>(input), GetTensorShape(filter),
   e3072:	4639      	mov	r1, r7
   e3074:	4630      	mov	r0, r6
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e3076:	f8d9 9004 	ldr.w	r9, [r9, #4]
   e307a:	f7f3 fc94 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e307e:	f8d7 a004 	ldr.w	sl, [r7, #4]
      GetTensorData<int8>(filter), GetTensorShape(bias),
   e3082:	af48      	add	r7, sp, #288	; 0x120
   e3084:	4621      	mov	r1, r4
   e3086:	4638      	mov	r0, r7
   e3088:	f7f3 fc8d 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e308c:	b104      	cbz	r4, e3090 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x3e8>
   e308e:	6864      	ldr	r4, [r4, #4]
      GetTensorData<int32>(bias), GetTensorShape(output),
   e3090:	9909      	ldr	r1, [sp, #36]	; 0x24
   e3092:	ad4d      	add	r5, sp, #308	; 0x134
   e3094:	4628      	mov	r0, r5
   e3096:	f7f3 fc86 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorData<int8>(output));
   e309a:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e309c:	685b      	ldr	r3, [r3, #4]
   e309e:	9306      	str	r3, [sp, #24]
   e30a0:	aaa6      	add	r2, sp, #664	; 0x298
   e30a2:	ab3e      	add	r3, sp, #248	; 0xf8
   e30a4:	a966      	add	r1, sp, #408	; 0x198
   e30a6:	a852      	add	r0, sp, #328	; 0x148
   e30a8:	9505      	str	r5, [sp, #20]
   e30aa:	9404      	str	r4, [sp, #16]
   e30ac:	9703      	str	r7, [sp, #12]
   e30ae:	f8cd a008 	str.w	sl, [sp, #8]
   e30b2:	9601      	str	r6, [sp, #4]
   e30b4:	f8cd 9000 	str.w	r9, [sp]
   e30b8:	f7ff fa85 	bl	e25c6 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa>
  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
      GetTensorData<int8>(filter), GetTensorShape(bias),
      GetTensorData<int32>(bias), GetTensorShape(output),
   e30bc:	4628      	mov	r0, r5
   e30be:	f7f3 f9c2 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
      GetTensorData<int8>(filter), GetTensorShape(bias),
   e30c2:	4638      	mov	r0, r7
   e30c4:	f7f3 f9bf 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
   e30c8:	4630      	mov	r0, r6
   e30ca:	f7f3 f9bc 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.quantized_activation_min = std::numeric_limits<int8_t>::min();
  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
   e30ce:	a83e      	add	r0, sp, #248	; 0xf8
   e30d0:	e0a1      	b.n	e3216 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x56e>
      }
    }
  }
  if (use_optimized_path) {
    DepthwiseConvOptimizedForFilterWidthEight(
        context, op_params, GetTensorShape(input),
   e30d2:	4649      	mov	r1, r9
   e30d4:	a848      	add	r0, sp, #288	; 0x120
   e30d6:	f7f3 fc66 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e30da:	f8d9 3004 	ldr.w	r3, [r9, #4]
   e30de:	931a      	str	r3, [sp, #104]	; 0x68
        GetTensorData<uint8_t>(input), GetTensorShape(filter),
   e30e0:	4639      	mov	r1, r7
   e30e2:	4640      	mov	r0, r8
   e30e4:	f7f3 fc5f 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e30e8:	687b      	ldr	r3, [r7, #4]
   e30ea:	931b      	str	r3, [sp, #108]	; 0x6c
        GetTensorData<uint8_t>(filter), GetTensorShape(bias),
   e30ec:	4621      	mov	r1, r4
   e30ee:	4628      	mov	r0, r5
   e30f0:	f7f3 fc59 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e30f4:	2c00      	cmp	r4, #0
   e30f6:	f000 8287 	beq.w	e3608 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x960>
   e30fa:	6863      	ldr	r3, [r4, #4]
   e30fc:	9316      	str	r3, [sp, #88]	; 0x58
        GetTensorData<int32_t>(bias), GetTensorShape(output),
   e30fe:	9909      	ldr	r1, [sp, #36]	; 0x24
   e3100:	a839      	add	r0, sp, #228	; 0xe4
   e3102:	f7f3 fc50 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e3106:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e3108:	685b      	ldr	r3, [r3, #4]
   e310a:	9326      	str	r3, [sp, #152]	; 0x98
    TfLiteContext* context, const DepthwiseParams& params,
    const RuntimeShape& input_shape, const uint8* input_data,
    const RuntimeShape& filter_shape, const uint8* filter_data,
    const RuntimeShape& bias_shape, const int32* bias_data,
    const RuntimeShape& output_shape, uint8* output_data) {
  const int stride_width = params.stride_width;
   e310c:	f9bd 3152 	ldrsh.w	r3, [sp, #338]	; 0x152
   e3110:	9319      	str	r3, [sp, #100]	; 0x64
  const int stride_height = params.stride_height;
   e3112:	f9bd 3154 	ldrsh.w	r3, [sp, #340]	; 0x154
   e3116:	931c      	str	r3, [sp, #112]	; 0x70
  const int pad_width = params.padding_values.width;
   e3118:	f9bd 314a 	ldrsh.w	r3, [sp, #330]	; 0x14a
   e311c:	931d      	str	r3, [sp, #116]	; 0x74
  const int pad_height = params.padding_values.height;
   e311e:	f9bd 314c 	ldrsh.w	r3, [sp, #332]	; 0x14c
   e3122:	931e      	str	r3, [sp, #120]	; 0x78
  const int depth_multiplier = params.depth_multiplier;
   e3124:	f9bd 315a 	ldrsh.w	r3, [sp, #346]	; 0x15a
   e3128:	9317      	str	r3, [sp, #92]	; 0x5c
  const int32 output_activation_min = params.quantized_activation_min;
   e312a:	9b5c      	ldr	r3, [sp, #368]	; 0x170
   e312c:	931f      	str	r3, [sp, #124]	; 0x7c
  const int32 output_activation_max = params.quantized_activation_max;
   e312e:	9b5d      	ldr	r3, [sp, #372]	; 0x174
   e3130:	9320      	str	r3, [sp, #128]	; 0x80
  const int32 input_offset = params.input_offset;
   e3132:	9b57      	ldr	r3, [sp, #348]	; 0x15c
   e3134:	9327      	str	r3, [sp, #156]	; 0x9c
  const int32 filter_offset = params.weights_offset;
   e3136:	9b58      	ldr	r3, [sp, #352]	; 0x160
   e3138:	9321      	str	r3, [sp, #132]	; 0x84
  const int32 output_offset = params.output_offset;
   e313a:	9b59      	ldr	r3, [sp, #356]	; 0x164
   e313c:	9328      	str	r3, [sp, #160]	; 0xa0
  const int32 output_multiplier = params.output_multiplier;
   e313e:	9b5a      	ldr	r3, [sp, #360]	; 0x168
   e3140:	9329      	str	r3, [sp, #164]	; 0xa4
  const int output_shift = params.output_shift;
   e3142:	9b5b      	ldr	r3, [sp, #364]	; 0x16c
   e3144:	932a      	str	r3, [sp, #168]	; 0xa8
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e3146:	9b48      	ldr	r3, [sp, #288]	; 0x120
   e3148:	2b04      	cmp	r3, #4
   e314a:	f040 80df 	bne.w	e330c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x664>
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
   e314e:	9b43      	ldr	r3, [sp, #268]	; 0x10c
   e3150:	2b04      	cmp	r3, #4
   e3152:	f040 80db 	bne.w	e330c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x664>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   e3156:	9c39      	ldr	r4, [sp, #228]	; 0xe4
   e3158:	2c04      	cmp	r4, #4
   e315a:	f040 80d7 	bne.w	e330c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x664>

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   e315e:	9b1f      	ldr	r3, [sp, #124]	; 0x7c
   e3160:	9a20      	ldr	r2, [sp, #128]	; 0x80
   e3162:	4293      	cmp	r3, r2
   e3164:	f300 80d2 	bgt.w	e330c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x664>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   e3168:	2300      	movs	r3, #0
   e316a:	4619      	mov	r1, r3
   e316c:	aa39      	add	r2, sp, #228	; 0xe4
   e316e:	a848      	add	r0, sp, #288	; 0x120
   e3170:	f7f8 fd25 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e3174:	2303      	movs	r3, #3
   e3176:	4619      	mov	r1, r3
   e3178:	aa39      	add	r2, sp, #228	; 0xe4
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   e317a:	902b      	str	r0, [sp, #172]	; 0xac
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e317c:	a843      	add	r0, sp, #268	; 0x10c
   e317e:	f7f8 fd1e 	bl	dbbbe <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
   e3182:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e3184:	4682      	mov	sl, r0
  const int input_height = input_shape.Dims(1);
   e3186:	a848      	add	r0, sp, #288	; 0x120
   e3188:	f7f3 f968 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   e318c:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
   e318e:	9018      	str	r0, [sp, #96]	; 0x60
  const int input_width = input_shape.Dims(2);
   e3190:	a848      	add	r0, sp, #288	; 0x120
   e3192:	f7f3 f963 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_depth = input_shape.Dims(3);
   e3196:	2103      	movs	r1, #3

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   e3198:	900e      	str	r0, [sp, #56]	; 0x38
  const int input_depth = input_shape.Dims(3);
   e319a:	a848      	add	r0, sp, #288	; 0x120
   e319c:	f7f3 f95e 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_height = filter_shape.Dims(1);
   e31a0:	2101      	movs	r1, #1
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
   e31a2:	900a      	str	r0, [sp, #40]	; 0x28
  const int filter_height = filter_shape.Dims(1);
   e31a4:	a843      	add	r0, sp, #268	; 0x10c
   e31a6:	f7f3 f959 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_width = filter_shape.Dims(2);
   e31aa:	2102      	movs	r1, #2
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
   e31ac:	9009      	str	r0, [sp, #36]	; 0x24
  const int filter_width = filter_shape.Dims(2);
   e31ae:	a843      	add	r0, sp, #268	; 0x10c
   e31b0:	f7f3 f954 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   e31b4:	2101      	movs	r1, #1
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
   e31b6:	4683      	mov	fp, r0
  const int output_height = output_shape.Dims(1);
   e31b8:	a839      	add	r0, sp, #228	; 0xe4
   e31ba:	f7f3 f94f 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   e31be:	2102      	movs	r1, #2
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   e31c0:	902c      	str	r0, [sp, #176]	; 0xb0
  const int output_width = output_shape.Dims(2);
   e31c2:	a839      	add	r0, sp, #228	; 0xe4
   e31c4:	f7f3 f94a 	bl	d645c <_ZNK6tflite12RuntimeShape4DimsEi>
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
   e31c8:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e31ca:	9a17      	ldr	r2, [sp, #92]	; 0x5c
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   e31cc:	902d      	str	r0, [sp, #180]	; 0xb4
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
   e31ce:	4353      	muls	r3, r2
   e31d0:	459a      	cmp	sl, r3
   e31d2:	f040 809b 	bne.w	e330c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x664>
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
   e31d6:	a83e      	add	r0, sp, #248	; 0xf8
   e31d8:	f7f8 fce1 	bl	dbb9e <_ZNK6tflite12RuntimeShape8FlatSizeEv>
   e31dc:	4582      	cmp	sl, r0
   e31de:	f040 8095 	bne.w	e330c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x664>

  static int16_t reshaped_filter_data[kReshapedFilterDataSize];
  const int needed_size =
      output_depth * filter_width * filter_height * input_depth;
   e31e2:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e31e4:	fb0b f20a 	mul.w	r2, fp, sl
   e31e8:	435a      	muls	r2, r3
   e31ea:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e31ec:	435a      	muls	r2, r3
  if (needed_size > kReshapedFilterDataSize) {
   e31ee:	f5b2 6f80 	cmp.w	r2, #1024	; 0x400
   e31f2:	f340 808d 	ble.w	e3310 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x668>
    context->ReportError(
        context,
        "Size too large for reshaped weight buffer (%d needed, %d available)",
        needed_size, kReshapedFilterDataSize);
   e31f6:	6974      	ldr	r4, [r6, #20]
   e31f8:	4962      	ldr	r1, [pc, #392]	; (e3384 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6dc>)
   e31fa:	f44f 6380 	mov.w	r3, #1024	; 0x400
   e31fe:	4630      	mov	r0, r6
   e3200:	47a0      	blx	r4
  if (use_optimized_path) {
    DepthwiseConvOptimizedForFilterWidthEight(
        context, op_params, GetTensorShape(input),
        GetTensorData<uint8_t>(input), GetTensorShape(filter),
        GetTensorData<uint8_t>(filter), GetTensorShape(bias),
        GetTensorData<int32_t>(bias), GetTensorShape(output),
   e3202:	a839      	add	r0, sp, #228	; 0xe4
   e3204:	f7f3 f91f 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  }
  if (use_optimized_path) {
    DepthwiseConvOptimizedForFilterWidthEight(
        context, op_params, GetTensorShape(input),
        GetTensorData<uint8_t>(input), GetTensorShape(filter),
        GetTensorData<uint8_t>(filter), GetTensorShape(bias),
   e3208:	a83e      	add	r0, sp, #248	; 0xf8
   e320a:	f7f3 f91c 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
    }
  }
  if (use_optimized_path) {
    DepthwiseConvOptimizedForFilterWidthEight(
        context, op_params, GetTensorShape(input),
        GetTensorData<uint8_t>(input), GetTensorShape(filter),
   e320e:	a843      	add	r0, sp, #268	; 0x10c
   e3210:	f7f3 f919 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
      }
    }
  }
  if (use_optimized_path) {
    DepthwiseConvOptimizedForFilterWidthEight(
        context, op_params, GetTensorShape(input),
   e3214:	a848      	add	r0, sp, #288	; 0x120
        GetTensorData<uint8_t>(filter), GetTensorShape(bias),
        GetTensorData<int32_t>(bias), GetTensorShape(output),
        GetTensorData<uint8_t>(output));
  } else {
    tflite::reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   e3216:	f7f3 f916 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
    default:
      context->ReportError(context, "Type %s (%d) not supported.",
                           TfLiteTypeGetName(input->type), input->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
   e321a:	2000      	movs	r0, #0
   e321c:	e227      	b.n	e366e <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x9c6>
                                        filter_width, filter_height, out_width,
                                        out_height, data_type, &data));

  // TODO(aselle): Consider whether float conv and quantized conv should be
  // separate ops to avoid dispatch overhead here.
  switch (input->type) {  // Already know in/out types are same.
   e321e:	f1b8 0f01 	cmp.w	r8, #1
   e3222:	d166      	bne.n	e32f2 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x64a>
void EvalFloat(TfLiteContext* context, TfLiteNode* node,
               TfLiteDepthwiseConvParams* params, OpData* data,
               const TfLiteTensor* input, const TfLiteTensor* filter,
               const TfLiteTensor* bias, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
   e3224:	7c2b      	ldrb	r3, [r5, #16]
// Calculates the useful range of an activation layer given its activation
// tensor.a
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
   e3226:	2b01      	cmp	r3, #1
   e3228:	d011      	beq.n	e324e <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x5a6>
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
   e322a:	2b03      	cmp	r3, #3
   e322c:	d012      	beq.n	e3254 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x5ac>
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
    *activation_max = std::numeric_limits<T>::max();
   e322e:	ed9f 7a56 	vldr	s14, [pc, #344]	; e3388 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6e0>
   e3232:	eddf 6a56 	vldr	s13, [pc, #344]	; e338c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6e4>
   e3236:	2b02      	cmp	r3, #2
   e3238:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   e323c:	bf18      	it	ne
   e323e:	eef0 7a47 	vmovne.f32	s15, s14
   e3242:	eebf 7a00 	vmov.f32	s14, #240	; 0xbf800000 -1.0
   e3246:	bf18      	it	ne
   e3248:	eeb0 7a66 	vmovne.f32	s14, s13
   e324c:	e006      	b.n	e325c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x5b4>
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
   e324e:	eddf 7a4e 	vldr	s15, [pc, #312]	; e3388 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6e0>
   e3252:	e001      	b.n	e3258 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x5b0>
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
    *activation_max = 6;
   e3254:	eef1 7a08 	vmov.f32	s15, #24	; 0x40c00000  6.0
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
   e3258:	ed9f 7a4d 	vldr	s14, [pc, #308]	; e3390 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6e8>
                           &output_activation_max);

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
   e325c:	9a60      	ldr	r2, [sp, #384]	; 0x180
   e325e:	f8ad 214a 	strh.w	r2, [sp, #330]	; 0x14a
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
   e3262:	2301      	movs	r3, #1
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
   e3264:	9a61      	ldr	r2, [sp, #388]	; 0x184
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
   e3266:	f88d 3148 	strb.w	r3, [sp, #328]	; 0x148
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
   e326a:	f8ad 214c 	strh.w	r2, [sp, #332]	; 0x14c
  op_params.stride_width = params->stride_width;
   e326e:	686a      	ldr	r2, [r5, #4]
   e3270:	f8ad 2152 	strh.w	r2, [sp, #338]	; 0x152
  op_params.stride_height = params->stride_height;
   e3274:	68aa      	ldr	r2, [r5, #8]
   e3276:	f8ad 2154 	strh.w	r2, [sp, #340]	; 0x154
  op_params.dilation_width_factor = 1;
   e327a:	f8ad 3156 	strh.w	r3, [sp, #342]	; 0x156
  op_params.dilation_height_factor = 1;
   e327e:	f8ad 3158 	strh.w	r3, [sp, #344]	; 0x158
  op_params.depth_multiplier = params->depth_multiplier;
   e3282:	68eb      	ldr	r3, [r5, #12]
   e3284:	f8ad 315a 	strh.w	r3, [sp, #346]	; 0x15a
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  tflite::reference_ops::DepthwiseConv(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
   e3288:	4649      	mov	r1, r9
   e328a:	a83e      	add	r0, sp, #248	; 0xf8
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.float_activation_min = output_activation_min;
   e328c:	ed8d 7a5e 	vstr	s14, [sp, #376]	; 0x178
  op_params.float_activation_max = output_activation_max;
   e3290:	edcd 7a5f 	vstr	s15, [sp, #380]	; 0x17c

  tflite::reference_ops::DepthwiseConv(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
      GetTensorShape(filter), GetTensorData<float>(filter),
      GetTensorShape(bias), GetTensorData<float>(bias), GetTensorShape(output),
   e3294:	ad48      	add	r5, sp, #288	; 0x120
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  tflite::reference_ops::DepthwiseConv(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
   e3296:	f7f3 fb86 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(filter), GetTensorData<float>(filter),
   e329a:	4639      	mov	r1, r7
   e329c:	a843      	add	r0, sp, #268	; 0x10c
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e329e:	f8d9 8004 	ldr.w	r8, [r9, #4]
   e32a2:	f7f3 fb80 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(bias), GetTensorData<float>(bias), GetTensorShape(output),
   e32a6:	4621      	mov	r1, r4
   e32a8:	4628      	mov	r0, r5
   e32aa:	f8d7 9004 	ldr.w	r9, [r7, #4]
   e32ae:	f7f3 fb7a 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e32b2:	b104      	cbz	r4, e32b6 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x60e>
   e32b4:	6864      	ldr	r4, [r4, #4]
   e32b6:	9909      	ldr	r1, [sp, #36]	; 0x24
   e32b8:	af4d      	add	r7, sp, #308	; 0x134
   e32ba:	4638      	mov	r0, r7
   e32bc:	f7f3 fb73 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e32c0:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e32c2:	b10b      	cbz	r3, e32c8 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x620>
   e32c4:	685e      	ldr	r6, [r3, #4]
   e32c6:	e000      	b.n	e32ca <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x622>
   e32c8:	9e09      	ldr	r6, [sp, #36]	; 0x24
      GetTensorData<float>(output));
   e32ca:	9604      	str	r6, [sp, #16]
   e32cc:	ab43      	add	r3, sp, #268	; 0x10c
   e32ce:	4642      	mov	r2, r8
   e32d0:	a93e      	add	r1, sp, #248	; 0xf8
   e32d2:	a852      	add	r0, sp, #328	; 0x148
   e32d4:	9703      	str	r7, [sp, #12]
   e32d6:	9402      	str	r4, [sp, #8]
   e32d8:	9501      	str	r5, [sp, #4]
   e32da:	f8cd 9000 	str.w	r9, [sp]
   e32de:	f7ff fa99 	bl	e2814 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf>
  op_params.float_activation_max = output_activation_max;

  tflite::reference_ops::DepthwiseConv(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
      GetTensorShape(filter), GetTensorData<float>(filter),
      GetTensorShape(bias), GetTensorData<float>(bias), GetTensorShape(output),
   e32e2:	4638      	mov	r0, r7
   e32e4:	f7f3 f8af 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   e32e8:	4628      	mov	r0, r5
   e32ea:	f7f3 f8ac 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  tflite::reference_ops::DepthwiseConv(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
      GetTensorShape(filter), GetTensorData<float>(filter),
   e32ee:	a843      	add	r0, sp, #268	; 0x10c
   e32f0:	e6eb      	b.n	e30ca <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x422>
      break;
    case kTfLiteUInt8:
      EvalQuantized(context, node, params, &data, input, filter, bias, output);
      break;
    default:
      context->ReportError(context, "Type %s (%d) not supported.",
   e32f2:	4640      	mov	r0, r8
   e32f4:	6974      	ldr	r4, [r6, #20]
   e32f6:	f7f0 ff0f 	bl	d4118 <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type), input->type);
   e32fa:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e32fc:	4925      	ldr	r1, [pc, #148]	; (e3394 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6ec>)
   e32fe:	f81a 3003 	ldrb.w	r3, [sl, r3]
   e3302:	4602      	mov	r2, r0
   e3304:	4630      	mov	r0, r6
   e3306:	47a0      	blx	r4
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
  }

  TF_LITE_ENSURE_STATUS(CalculateOpData(context, node, params, width, height,
   e3308:	2001      	movs	r0, #1
   e330a:	e1b0      	b.n	e366e <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x9c6>
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e330c:	f001 f80e 	bl	e432c <abort>
    return;
  }

  RuntimeShape reshaped_filter_shape;
  reshaped_filter_shape.BuildFrom(
      {1, output_depth, filter_height, filter_width});
   e3310:	2301      	movs	r3, #1
   e3312:	9335      	str	r3, [sp, #212]	; 0xd4
    const int dimensions_count =
        std::distance(src_iterable.begin(), src_iterable.end());
    Resize(dimensions_count);
    int32* data = DimsData();
    for (auto it : src_iterable) {
      *data = it;
   e3314:	934e      	str	r3, [sp, #312]	; 0x138

  // If this is the first time through, repack the weights into a cached buffer
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
   e3316:	4b20      	ldr	r3, [pc, #128]	; (e3398 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6f0>)
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
#endif  // TF_LITE_STATIC_MEMORY
    }
    size_ = dimensions_count;
   e3318:	944d      	str	r4, [sp, #308]	; 0x134
   e331a:	781c      	ldrb	r4, [r3, #0]
    return;
  }

  RuntimeShape reshaped_filter_shape;
  reshaped_filter_shape.BuildFrom(
      {1, output_depth, filter_height, filter_width});
   e331c:	9a09      	ldr	r2, [sp, #36]	; 0x24
   e331e:	f8cd a0d8 	str.w	sl, [sp, #216]	; 0xd8
   e3322:	9237      	str	r2, [sp, #220]	; 0xdc
   e3324:	f8cd b0e0 	str.w	fp, [sp, #224]	; 0xe0
    const int dimensions_count =
        std::distance(src_iterable.begin(), src_iterable.end());
    Resize(dimensions_count);
    int32* data = DimsData();
    for (auto it : src_iterable) {
      *data = it;
   e3328:	f8cd a13c 	str.w	sl, [sp, #316]	; 0x13c
   e332c:	9250      	str	r2, [sp, #320]	; 0x140
   e332e:	f8cd b144 	str.w	fp, [sp, #324]	; 0x144

  // If this is the first time through, repack the weights into a cached buffer
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
   e3332:	2c00      	cmp	r4, #0
   e3334:	d137      	bne.n	e33a6 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6fe>
              filter_data + Offset(filter_shape, 0, filter_y, filter_x, oc);
          int16_t* reshaped_filter =
              reshaped_filter_data +
              Offset(reshaped_filter_shape, 0, oc, filter_y, filter_x);
          *reshaped_filter =
              static_cast<int16_t>(*current_filter) + filter_offset;
   e3336:	4f19      	ldr	r7, [pc, #100]	; (e339c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6f4>)

  // If this is the first time through, repack the weights into a cached buffer
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
    for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   e3338:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e333a:	42a3      	cmp	r3, r4
   e333c:	dd30      	ble.n	e33a0 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6f8>
   e333e:	2500      	movs	r5, #0
      for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   e3340:	45ab      	cmp	fp, r5
   e3342:	dd1c      	ble.n	e337e <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6d6>
   e3344:	2600      	movs	r6, #0
        for (int oc = 0; oc < output_depth; ++oc) {
   e3346:	45b2      	cmp	sl, r6
   e3348:	dd17      	ble.n	e337a <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6d2>
          const uint8* current_filter =
              filter_data + Offset(filter_shape, 0, filter_y, filter_x, oc);
   e334a:	9600      	str	r6, [sp, #0]
   e334c:	462b      	mov	r3, r5
   e334e:	4622      	mov	r2, r4
   e3350:	2100      	movs	r1, #0
   e3352:	a843      	add	r0, sp, #268	; 0x10c
   e3354:	f7f3 f8e7 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          int16_t* reshaped_filter =
              reshaped_filter_data +
              Offset(reshaped_filter_shape, 0, oc, filter_y, filter_x);
   e3358:	4632      	mov	r2, r6
  if (!is_reshaped_filter_initialized) {
    for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
      for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
        for (int oc = 0; oc < output_depth; ++oc) {
          const uint8* current_filter =
              filter_data + Offset(filter_shape, 0, filter_y, filter_x, oc);
   e335a:	4680      	mov	r8, r0
          int16_t* reshaped_filter =
              reshaped_filter_data +
              Offset(reshaped_filter_shape, 0, oc, filter_y, filter_x);
   e335c:	4623      	mov	r3, r4
   e335e:	9500      	str	r5, [sp, #0]
   e3360:	2100      	movs	r1, #0
   e3362:	a84d      	add	r0, sp, #308	; 0x134
   e3364:	f7f3 f8df 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          *reshaped_filter =
              static_cast<int16_t>(*current_filter) + filter_offset;
   e3368:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
   e336a:	9a21      	ldr	r2, [sp, #132]	; 0x84
   e336c:	f813 3008 	ldrb.w	r3, [r3, r8]
   e3370:	4413      	add	r3, r2
   e3372:	f827 3010 	strh.w	r3, [r7, r0, lsl #1]
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
    for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
      for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
        for (int oc = 0; oc < output_depth; ++oc) {
   e3376:	3601      	adds	r6, #1
   e3378:	e7e5      	b.n	e3346 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x69e>
  // If this is the first time through, repack the weights into a cached buffer
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
    for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
      for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   e337a:	3501      	adds	r5, #1
   e337c:	e7e0      	b.n	e3340 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x698>

  // If this is the first time through, repack the weights into a cached buffer
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
    for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   e337e:	3401      	adds	r4, #1
   e3380:	e7da      	b.n	e3338 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x690>
   e3382:	bf00      	nop
   e3384:	000eb02d 	.word	0x000eb02d
   e3388:	7f7fffff 	.word	0x7f7fffff
   e338c:	ff7fffff 	.word	0xff7fffff
   e3390:	00000000 	.word	0x00000000
   e3394:	000e9b7a 	.word	0x000e9b7a
   e3398:	2003db91 	.word	0x2003db91
   e339c:	2003db92 	.word	0x2003db92
          *reshaped_filter =
              static_cast<int16_t>(*current_filter) + filter_offset;
        }
      }
    }
    is_reshaped_filter_initialized = true;
   e33a0:	4b9a      	ldr	r3, [pc, #616]	; (e360c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x964>)
   e33a2:	2201      	movs	r2, #1
   e33a4:	701a      	strb	r2, [r3, #0]
  // If this is the first time through, repack the weights into a cached buffer
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
    for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
      for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   e33a6:	2300      	movs	r3, #0
   e33a8:	930b      	str	r3, [sp, #44]	; 0x2c
                    Offset(filter_shape, 0, filter_y, filter_x_start, oc);
                for (int filter_x = filter_x_start; filter_x < filter_x_end;
                     ++filter_x) {
                  int32 input_val = *current_input;
                  current_input += input_depth;
                  int32 filter_val = *current_filter;
   e33aa:	f1ca 0300 	rsb	r3, sl, #0
   e33ae:	9333      	str	r3, [sp, #204]	; 0xcc
      }
    }
    is_reshaped_filter_initialized = true;
  }

  for (int b = 0; b < batches; ++b) {
   e33b0:	9b2b      	ldr	r3, [sp, #172]	; 0xac
   e33b2:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   e33b4:	4293      	cmp	r3, r2
   e33b6:	f340 8123 	ble.w	e3600 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x958>
   e33ba:	9b1e      	ldr	r3, [sp, #120]	; 0x78
   e33bc:	9a18      	ldr	r2, [sp, #96]	; 0x60
   e33be:	4413      	add	r3, r2
   e33c0:	9313      	str	r3, [sp, #76]	; 0x4c
   e33c2:	9b1e      	ldr	r3, [sp, #120]	; 0x78
   e33c4:	425b      	negs	r3, r3
   e33c6:	930c      	str	r3, [sp, #48]	; 0x30
   e33c8:	2300      	movs	r3, #0
   e33ca:	930f      	str	r3, [sp, #60]	; 0x3c
    for (int out_y = 0; out_y < output_height; ++out_y) {
   e33cc:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   e33ce:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
   e33d0:	4293      	cmp	r3, r2
   e33d2:	f340 8111 	ble.w	e35f8 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x950>
   e33d6:	9a0c      	ldr	r2, [sp, #48]	; 0x30
   e33d8:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e33da:	9818      	ldr	r0, [sp, #96]	; 0x60
   e33dc:	9909      	ldr	r1, [sp, #36]	; 0x24
   e33de:	4413      	add	r3, r2
   e33e0:	9a13      	ldr	r2, [sp, #76]	; 0x4c
   e33e2:	4298      	cmp	r0, r3
   e33e4:	bfc8      	it	gt
   e33e6:	460a      	movgt	r2, r1
   e33e8:	9b1d      	ldr	r3, [sp, #116]	; 0x74
   e33ea:	922e      	str	r2, [sp, #184]	; 0xb8
   e33ec:	ebc3 030b 	rsb	r3, r3, fp
   e33f0:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   e33f2:	930d      	str	r3, [sp, #52]	; 0x34
   e33f4:	9b1d      	ldr	r3, [sp, #116]	; 0x74
   e33f6:	4413      	add	r3, r2
   e33f8:	9314      	str	r3, [sp, #80]	; 0x50
   e33fa:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   e33fc:	9a0c      	ldr	r2, [sp, #48]	; 0x30
   e33fe:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
   e3402:	9331      	str	r3, [sp, #196]	; 0xc4
   e3404:	9b13      	ldr	r3, [sp, #76]	; 0x4c
   e3406:	2a00      	cmp	r2, #0
   e3408:	eba3 0300 	sub.w	r3, r3, r0
   e340c:	bfa8      	it	ge
   e340e:	2300      	movge	r3, #0
   e3410:	9324      	str	r3, [sp, #144]	; 0x90
   e3412:	2300      	movs	r3, #0
   e3414:	9310      	str	r3, [sp, #64]	; 0x40
      for (int out_x = 0; out_x < output_width; ++out_x) {
   e3416:	9b2d      	ldr	r3, [sp, #180]	; 0xb4
   e3418:	9a10      	ldr	r2, [sp, #64]	; 0x40
   e341a:	4293      	cmp	r3, r2
   e341c:	f340 80e1 	ble.w	e35e2 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x93a>
   e3420:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   e3422:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   e3424:	990d      	ldr	r1, [sp, #52]	; 0x34
   e3426:	ebcb 0303 	rsb	r3, fp, r3
   e342a:	9325      	str	r3, [sp, #148]	; 0x94
   e342c:	9b14      	ldr	r3, [sp, #80]	; 0x50
   e342e:	1a9b      	subs	r3, r3, r2
   e3430:	9330      	str	r3, [sp, #192]	; 0xc0
   e3432:	9b14      	ldr	r3, [sp, #80]	; 0x50
   e3434:	f04f 0800 	mov.w	r8, #0
   e3438:	428a      	cmp	r2, r1
   e343a:	bfc8      	it	gt
   e343c:	465b      	movgt	r3, fp
   e343e:	932f      	str	r3, [sp, #188]	; 0xbc
   e3440:	f8cd 8044 	str.w	r8, [sp, #68]	; 0x44
        for (int ic = 0; ic < input_depth; ++ic) {
   e3444:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e3446:	9a11      	ldr	r2, [sp, #68]	; 0x44
   e3448:	4293      	cmp	r3, r2
   e344a:	f340 80bf 	ble.w	e35cc <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x924>
   e344e:	9b16      	ldr	r3, [sp, #88]	; 0x58
   e3450:	eb03 0388 	add.w	r3, r3, r8, lsl #2
   e3454:	9332      	str	r3, [sp, #200]	; 0xc8
   e3456:	f04f 0900 	mov.w	r9, #0
          for (int m = 0; m < depth_multiplier; m++) {
   e345a:	9b17      	ldr	r3, [sp, #92]	; 0x5c
   e345c:	454b      	cmp	r3, r9
   e345e:	f340 80af 	ble.w	e35c0 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x918>
   e3462:	eb09 0308 	add.w	r3, r9, r8
   e3466:	9315      	str	r3, [sp, #84]	; 0x54
            }
            int in_y = in_y_start;
            int in_x_start = in_x_origin;
            int filter_x_start = 0;
            bool is_out_of_x_bounds = false;
            if (in_x_origin < 0) {
   e3468:	9b25      	ldr	r3, [sp, #148]	; 0x94
              is_out_of_x_bounds = true;
            }
            int filter_x_end = filter_width;
            if ((in_x_origin + filter_width) >= input_width) {
              filter_x_end -= (in_x_origin + filter_width) - input_width;
              is_out_of_x_bounds = true;
   e346a:	9a0d      	ldr	r2, [sp, #52]	; 0x34
            }
            for (int filter_y = filter_y_start; filter_y < filter_y_end;
   e346c:	9d24      	ldr	r5, [sp, #144]	; 0x90
            }
            int in_y = in_y_start;
            int in_x_start = in_x_origin;
            int filter_x_start = 0;
            bool is_out_of_x_bounds = false;
            if (in_x_origin < 0) {
   e346e:	2b00      	cmp	r3, #0
              in_x_start = 0;
              filter_x_start = 0 - in_x_origin;
   e3470:	bfbd      	ittte	lt
   e3472:	9b30      	ldrlt	r3, [sp, #192]	; 0xc0
   e3474:	9312      	strlt	r3, [sp, #72]	; 0x48
            int in_y = in_y_start;
            int in_x_start = in_x_origin;
            int filter_x_start = 0;
            bool is_out_of_x_bounds = false;
            if (in_x_origin < 0) {
              in_x_start = 0;
   e3476:	2300      	movlt	r3, #0
            }
            int in_y = in_y_start;
            int in_x_start = in_x_origin;
            int filter_x_start = 0;
            bool is_out_of_x_bounds = false;
            if (in_x_origin < 0) {
   e3478:	9322      	strge	r3, [sp, #136]	; 0x88
              in_x_start = 0;
   e347a:	bfb8      	it	lt
   e347c:	9322      	strlt	r3, [sp, #136]	; 0x88
              is_out_of_x_bounds = true;
            }
            int filter_x_end = filter_width;
            if ((in_x_origin + filter_width) >= input_width) {
              filter_x_end -= (in_x_origin + filter_width) - input_width;
              is_out_of_x_bounds = true;
   e347e:	9b0e      	ldr	r3, [sp, #56]	; 0x38
              filter_y_end -= (in_y_origin + filter_height) - input_height;
            }
            int in_y = in_y_start;
            int in_x_start = in_x_origin;
            int filter_x_start = 0;
            bool is_out_of_x_bounds = false;
   e3480:	bfaa      	itet	ge
   e3482:	2600      	movge	r6, #0
            if (in_x_origin < 0) {
              in_x_start = 0;
              filter_x_start = 0 - in_x_origin;
              is_out_of_x_bounds = true;
   e3484:	2601      	movlt	r6, #1
            if ((in_y_origin + filter_height) >= input_height) {
              filter_y_end -= (in_y_origin + filter_height) - input_height;
            }
            int in_y = in_y_start;
            int in_x_start = in_x_origin;
            int filter_x_start = 0;
   e3486:	9612      	strge	r6, [sp, #72]	; 0x48
        for (int ic = 0; ic < input_depth; ++ic) {
          for (int m = 0; m < depth_multiplier; m++) {
            const int oc = m + ic * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            int32 acc = 0;
   e3488:	2400      	movs	r4, #0
              is_out_of_x_bounds = true;
            }
            int filter_x_end = filter_width;
            if ((in_x_origin + filter_width) >= input_width) {
              filter_x_end -= (in_x_origin + filter_width) - input_width;
              is_out_of_x_bounds = true;
   e348a:	4293      	cmp	r3, r2
   e348c:	bfd8      	it	le
   e348e:	2601      	movle	r6, #1
   e3490:	9b31      	ldr	r3, [sp, #196]	; 0xc4
   e3492:	9a24      	ldr	r2, [sp, #144]	; 0x90
   e3494:	1a9a      	subs	r2, r3, r2
            }
            for (int filter_y = filter_y_start; filter_y < filter_y_end;
   e3496:	9b2e      	ldr	r3, [sp, #184]	; 0xb8
   e3498:	42ab      	cmp	r3, r5
   e349a:	442a      	add	r2, r5
   e349c:	dd6d      	ble.n	e357a <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x8d2>
                 ++filter_y, ++in_y) {
              const uint8* current_input =
                  input_data + Offset(input_shape, b, in_y, in_x_start, ic);
   e349e:	9b11      	ldr	r3, [sp, #68]	; 0x44
   e34a0:	9300      	str	r3, [sp, #0]
   e34a2:	990b      	ldr	r1, [sp, #44]	; 0x2c
   e34a4:	9b22      	ldr	r3, [sp, #136]	; 0x88
   e34a6:	a848      	add	r0, sp, #288	; 0x120
   e34a8:	f7f3 f83d 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   e34ac:	9b1a      	ldr	r3, [sp, #104]	; 0x68
   e34ae:	9023      	str	r0, [sp, #140]	; 0x8c
              if ((filter_width == 8) && !is_out_of_x_bounds) {
   e34b0:	f1bb 0f08 	cmp.w	fp, #8
              is_out_of_x_bounds = true;
            }
            for (int filter_y = filter_y_start; filter_y < filter_y_end;
                 ++filter_y, ++in_y) {
              const uint8* current_input =
                  input_data + Offset(input_shape, b, in_y, in_x_start, ic);
   e34b4:	eb03 0700 	add.w	r7, r3, r0
              if ((filter_width == 8) && !is_out_of_x_bounds) {
   e34b8:	d13c      	bne.n	e3534 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x88c>
   e34ba:	2e00      	cmp	r6, #0
   e34bc:	d13a      	bne.n	e3534 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x88c>
                int16* current_filter =
                    reshaped_filter_data + Offset(reshaped_filter_shape, 0, oc,
   e34be:	9b12      	ldr	r3, [sp, #72]	; 0x48
   e34c0:	9300      	str	r3, [sp, #0]
   e34c2:	9a15      	ldr	r2, [sp, #84]	; 0x54
   e34c4:	462b      	mov	r3, r5
   e34c6:	4631      	mov	r1, r6
   e34c8:	a84d      	add	r0, sp, #308	; 0x134
   e34ca:	f7f3 f82c 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                                                  filter_y, filter_x_start);
   e34ce:	4b50      	ldr	r3, [pc, #320]	; (e3610 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x968>)
                const uint32_t input_vals0 =
                    *reinterpret_cast<const uint32_t*>(current_input);
   e34d0:	9a23      	ldr	r2, [sp, #140]	; 0x8c
              const uint8* current_input =
                  input_data + Offset(input_shape, b, in_y, in_x_start, ic);
              if ((filter_width == 8) && !is_out_of_x_bounds) {
                int16* current_filter =
                    reshaped_filter_data + Offset(reshaped_filter_shape, 0, oc,
                                                  filter_y, filter_x_start);
   e34d2:	eb03 0040 	add.w	r0, r3, r0, lsl #1
                const uint32_t input_vals0 =
                    *reinterpret_cast<const uint32_t*>(current_input);
   e34d6:	9b1a      	ldr	r3, [sp, #104]	; 0x68
                current_input += 4;
                const int32_t filter_vals0 =
                    *reinterpret_cast<const int32_t*>(current_filter);
   e34d8:	f8d0 c000 	ldr.w	ip, [r0]
              if ((filter_width == 8) && !is_out_of_x_bounds) {
                int16* current_filter =
                    reshaped_filter_data + Offset(reshaped_filter_shape, 0, oc,
                                                  filter_y, filter_x_start);
                const uint32_t input_vals0 =
                    *reinterpret_cast<const uint32_t*>(current_input);
   e34dc:	5899      	ldr	r1, [r3, r2]
                const int32_t filter_vals0 =
                    *reinterpret_cast<const int32_t*>(current_filter);
                current_filter += 2;
                const uint8 input_val0 = input_vals0 & 0xff;
                const int16 filter_val0 = filter_vals0 & 0xffff;
                acc += filter_val0 * input_val0;
   e34de:	fa0f f28c 	sxth.w	r2, ip
   e34e2:	b2cb      	uxtb	r3, r1
   e34e4:	fb02 4203 	mla	r2, r2, r3, r4
                const uint8 input_val1 = (input_vals0 >> 8) & 0xff;
                const int16 filter_val1 = (filter_vals0 >> 16) & 0xffff;
                acc += filter_val1 * input_val1;
   e34e8:	ea4f 4e2c 	mov.w	lr, ip, asr #16
   e34ec:	f3c1 2307 	ubfx	r3, r1, #8, #8
   e34f0:	fb0e 2e03 	mla	lr, lr, r3, r2

                const int32_t filter_vals1 =
                    *reinterpret_cast<const int32_t*>(current_filter);
   e34f4:	6843      	ldr	r3, [r0, #4]
                current_filter += 2;
                const uint8 input_val2 = (input_vals0 >> 16) & 0xff;
                const int16 filter_val2 = filter_vals1 & 0xffff;
                acc += filter_val2 * input_val2;
   e34f6:	f3c1 4c07 	ubfx	ip, r1, #16, #8
   e34fa:	b21a      	sxth	r2, r3
                const uint8 input_val3 = (input_vals0 >> 24) & 0xff;
                const int16 filter_val3 = (filter_vals1 >> 16) & 0xffff;
                acc += filter_val3 * input_val3;
   e34fc:	0e09      	lsrs	r1, r1, #24
   e34fe:	141b      	asrs	r3, r3, #16
                const int32_t filter_vals1 =
                    *reinterpret_cast<const int32_t*>(current_filter);
                current_filter += 2;
                const uint8 input_val2 = (input_vals0 >> 16) & 0xff;
                const int16 filter_val2 = filter_vals1 & 0xffff;
                acc += filter_val2 * input_val2;
   e3500:	fb02 e20c 	mla	r2, r2, ip, lr
                const uint8 input_val3 = (input_vals0 >> 24) & 0xff;
                const int16 filter_val3 = (filter_vals1 >> 16) & 0xffff;
                acc += filter_val3 * input_val3;
   e3504:	fb01 2203 	mla	r2, r1, r3, r2

                const uint32_t input_vals1 =
                    *reinterpret_cast<const uint32_t*>(current_input);
   e3508:	687b      	ldr	r3, [r7, #4]
                const int32_t filter_vals2 =
                    *reinterpret_cast<const int32_t*>(current_filter);
   e350a:	6881      	ldr	r1, [r0, #8]
                current_filter += 2;
                const uint8 input_val4 = input_vals1 & 0xff;
                const int16 filter_val4 = filter_vals2 & 0xffff;
                acc += filter_val4 * input_val4;
   e350c:	b2dc      	uxtb	r4, r3
   e350e:	b20f      	sxth	r7, r1
   e3510:	fb07 2204 	mla	r2, r7, r4, r2
                const uint8 input_val5 = (input_vals1 >> 8) & 0xff;
                const int16 filter_val5 = (filter_vals2 >> 16) & 0xffff;
                acc += filter_val5 * input_val5;
   e3514:	1409      	asrs	r1, r1, #16
   e3516:	f3c3 2707 	ubfx	r7, r3, #8, #8
   e351a:	fb01 2207 	mla	r2, r1, r7, r2

                const int32_t filter_vals3 =
                    *reinterpret_cast<const int32_t*>(current_filter);
   e351e:	68c1      	ldr	r1, [r0, #12]
                const uint8 input_val6 = (input_vals1 >> 16) & 0xff;
                const int16 filter_val6 = filter_vals3 & 0xffff;
                acc += filter_val6 * input_val6;
   e3520:	f3c3 4007 	ubfx	r0, r3, #16, #8
   e3524:	b20c      	sxth	r4, r1
   e3526:	fb04 2400 	mla	r4, r4, r0, r2
                const uint8 input_val7 = (input_vals1 >> 24) & 0xff;
                const int16 filter_val7 = (filter_vals3 >> 16) & 0xffff;
                acc += filter_val7 * input_val7;
   e352a:	1409      	asrs	r1, r1, #16
   e352c:	0e1b      	lsrs	r3, r3, #24
   e352e:	fb03 4401 	mla	r4, r3, r1, r4
   e3532:	e020      	b.n	e3576 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x8ce>
              } else {
                const uint8* current_filter =
                    filter_data +
                    Offset(filter_shape, 0, filter_y, filter_x_start, oc);
   e3534:	9b15      	ldr	r3, [sp, #84]	; 0x54
   e3536:	9300      	str	r3, [sp, #0]
   e3538:	462a      	mov	r2, r5
   e353a:	9b12      	ldr	r3, [sp, #72]	; 0x48
   e353c:	2100      	movs	r1, #0
   e353e:	a843      	add	r0, sp, #268	; 0x10c
   e3540:	f7f2 fff1 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   e3544:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
                for (int filter_x = filter_x_start; filter_x < filter_x_end;
                     ++filter_x) {
                  int32 input_val = *current_input;
   e3546:	9a0a      	ldr	r2, [sp, #40]	; 0x28
   e3548:	4418      	add	r0, r3
                acc += filter_val7 * input_val7;
              } else {
                const uint8* current_filter =
                    filter_data +
                    Offset(filter_shape, 0, filter_y, filter_x_start, oc);
                for (int filter_x = filter_x_start; filter_x < filter_x_end;
   e354a:	9b12      	ldr	r3, [sp, #72]	; 0x48
                     ++filter_x) {
                  int32 input_val = *current_input;
   e354c:	f1c2 0e00 	rsb	lr, r2, #0
   e3550:	9a0a      	ldr	r2, [sp, #40]	; 0x28
   e3552:	4417      	add	r7, r2
                acc += filter_val7 * input_val7;
              } else {
                const uint8* current_filter =
                    filter_data +
                    Offset(filter_shape, 0, filter_y, filter_x_start, oc);
                for (int filter_x = filter_x_start; filter_x < filter_x_end;
   e3554:	9a2f      	ldr	r2, [sp, #188]	; 0xbc
   e3556:	429a      	cmp	r2, r3
   e3558:	4450      	add	r0, sl
   e355a:	dd0c      	ble.n	e3576 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x8ce>
                  int32 input_val = *current_input;
                  current_input += input_depth;
                  int32 filter_val = *current_filter;
                  current_filter += output_depth;
                  acc +=
                      (filter_val + filter_offset) * (input_val + input_offset);
   e355c:	9a33      	ldr	r2, [sp, #204]	; 0xcc
   e355e:	9921      	ldr	r1, [sp, #132]	; 0x84
   e3560:	5c82      	ldrb	r2, [r0, r2]
   e3562:	eb02 0c01 	add.w	ip, r2, r1
   e3566:	f817 100e 	ldrb.w	r1, [r7, lr]
   e356a:	9a27      	ldr	r2, [sp, #156]	; 0x9c
   e356c:	4411      	add	r1, r2
   e356e:	fb01 440c 	mla	r4, r1, ip, r4
                acc += filter_val7 * input_val7;
              } else {
                const uint8* current_filter =
                    filter_data +
                    Offset(filter_shape, 0, filter_y, filter_x_start, oc);
                for (int filter_x = filter_x_start; filter_x < filter_x_end;
   e3572:	3301      	adds	r3, #1
   e3574:	e7ec      	b.n	e3550 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x8a8>
            int filter_x_end = filter_width;
            if ((in_x_origin + filter_width) >= input_width) {
              filter_x_end -= (in_x_origin + filter_width) - input_width;
              is_out_of_x_bounds = true;
            }
            for (int filter_y = filter_y_start; filter_y < filter_y_end;
   e3576:	3501      	adds	r5, #1
   e3578:	e78a      	b.n	e3490 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x7e8>
                  acc +=
                      (filter_val + filter_offset) * (input_val + input_offset);
                }
              }
            }
            if (bias_data) {
   e357a:	9b16      	ldr	r3, [sp, #88]	; 0x58
   e357c:	b11b      	cbz	r3, e3586 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x8de>
              acc += bias_data[oc];
   e357e:	9b32      	ldr	r3, [sp, #200]	; 0xc8
   e3580:	f853 3029 	ldr.w	r3, [r3, r9, lsl #2]
   e3584:	441c      	add	r4, r3
}

template <>
inline int32 DepthwiseConvRound<DepthwiseConvOutputRounding::kAwayFromZero>(
    int32 x, int32 quantized_multiplier, int shift) {
  return MultiplyByQuantizedMultiplier(x, quantized_multiplier, shift);
   e3586:	9a2a      	ldr	r2, [sp, #168]	; 0xa8
   e3588:	9929      	ldr	r1, [sp, #164]	; 0xa4
   e358a:	4620      	mov	r0, r4
   e358c:	f7f8 fb26 	bl	dbbdc <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
            }
            acc = reference_ops::depthwise_conv::DepthwiseConvRound<
                DepthwiseConvOutputRounding::kAwayFromZero>(
                acc, output_multiplier, output_shift);
            acc += output_offset;
   e3590:	9b28      	ldr	r3, [sp, #160]	; 0xa0
            acc = std::max(acc, output_activation_min);
            acc = std::min(acc, output_activation_max);
            output_data[Offset(output_shape, b, out_y, out_x, oc)] =
   e3592:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
   e3594:	990b      	ldr	r1, [sp, #44]	; 0x2c
              acc += bias_data[oc];
            }
            acc = reference_ops::depthwise_conv::DepthwiseConvRound<
                DepthwiseConvOutputRounding::kAwayFromZero>(
                acc, output_multiplier, output_shift);
            acc += output_offset;
   e3596:	4418      	add	r0, r3
   e3598:	9b1f      	ldr	r3, [sp, #124]	; 0x7c
   e359a:	4283      	cmp	r3, r0
   e359c:	bfb8      	it	lt
   e359e:	4603      	movlt	r3, r0
   e35a0:	461c      	mov	r4, r3
   e35a2:	9b20      	ldr	r3, [sp, #128]	; 0x80
   e35a4:	429c      	cmp	r4, r3
   e35a6:	bfa8      	it	ge
   e35a8:	461c      	movge	r4, r3
            acc = std::max(acc, output_activation_min);
            acc = std::min(acc, output_activation_max);
            output_data[Offset(output_shape, b, out_y, out_x, oc)] =
   e35aa:	9b15      	ldr	r3, [sp, #84]	; 0x54
   e35ac:	9300      	str	r3, [sp, #0]
   e35ae:	a839      	add	r0, sp, #228	; 0xe4
   e35b0:	9b10      	ldr	r3, [sp, #64]	; 0x40
   e35b2:	f7f2 ffb8 	bl	d6526 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                static_cast<uint8>(acc);
   e35b6:	9b26      	ldr	r3, [sp, #152]	; 0x98

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int ic = 0; ic < input_depth; ++ic) {
          for (int m = 0; m < depth_multiplier; m++) {
   e35b8:	f109 0901 	add.w	r9, r9, #1
                acc, output_multiplier, output_shift);
            acc += output_offset;
            acc = std::max(acc, output_activation_min);
            acc = std::min(acc, output_activation_max);
            output_data[Offset(output_shape, b, out_y, out_x, oc)] =
                static_cast<uint8>(acc);
   e35bc:	541c      	strb	r4, [r3, r0]
   e35be:	e74c      	b.n	e345a <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x7b2>
  }

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int ic = 0; ic < input_depth; ++ic) {
   e35c0:	9b11      	ldr	r3, [sp, #68]	; 0x44
   e35c2:	3301      	adds	r3, #1
   e35c4:	9311      	str	r3, [sp, #68]	; 0x44
   e35c6:	9b17      	ldr	r3, [sp, #92]	; 0x5c
   e35c8:	4498      	add	r8, r3
   e35ca:	e73b      	b.n	e3444 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x79c>
    is_reshaped_filter_initialized = true;
  }

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   e35cc:	9b10      	ldr	r3, [sp, #64]	; 0x40
   e35ce:	9a19      	ldr	r2, [sp, #100]	; 0x64
   e35d0:	3301      	adds	r3, #1
   e35d2:	9310      	str	r3, [sp, #64]	; 0x40
   e35d4:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   e35d6:	4413      	add	r3, r2
   e35d8:	930d      	str	r3, [sp, #52]	; 0x34
   e35da:	9b14      	ldr	r3, [sp, #80]	; 0x50
   e35dc:	1a9b      	subs	r3, r3, r2
   e35de:	9314      	str	r3, [sp, #80]	; 0x50
   e35e0:	e719      	b.n	e3416 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x76e>
    }
    is_reshaped_filter_initialized = true;
  }

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   e35e2:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   e35e4:	9a1c      	ldr	r2, [sp, #112]	; 0x70
   e35e6:	3301      	adds	r3, #1
   e35e8:	930f      	str	r3, [sp, #60]	; 0x3c
   e35ea:	9b13      	ldr	r3, [sp, #76]	; 0x4c
   e35ec:	1a9b      	subs	r3, r3, r2
   e35ee:	9313      	str	r3, [sp, #76]	; 0x4c
   e35f0:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   e35f2:	4413      	add	r3, r2
   e35f4:	930c      	str	r3, [sp, #48]	; 0x30
   e35f6:	e6e9      	b.n	e33cc <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x724>
      }
    }
    is_reshaped_filter_initialized = true;
  }

  for (int b = 0; b < batches; ++b) {
   e35f8:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e35fa:	3301      	adds	r3, #1
   e35fc:	930b      	str	r3, [sp, #44]	; 0x2c
   e35fe:	e6d7      	b.n	e33b0 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x708>
        "Size too large for reshaped weight buffer (%d needed, %d available)",
        needed_size, kReshapedFilterDataSize);
    return;
  }

  RuntimeShape reshaped_filter_shape;
   e3600:	a84d      	add	r0, sp, #308	; 0x134
   e3602:	f7f2 ff20 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
   e3606:	e5fc      	b.n	e3202 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x55a>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e3608:	9416      	str	r4, [sp, #88]	; 0x58
   e360a:	e578      	b.n	e30fe <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x456>
   e360c:	2003db91 	.word	0x2003db91
   e3610:	2003db92 	.word	0x2003db92
        GetTensorData<uint8_t>(filter), GetTensorShape(bias),
        GetTensorData<int32_t>(bias), GetTensorShape(output),
        GetTensorData<uint8_t>(output));
  } else {
    tflite::reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   e3614:	4649      	mov	r1, r9
   e3616:	a84d      	add	r0, sp, #308	; 0x134
   e3618:	f7f3 f9c5 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
        GetTensorShape(filter), GetTensorData<uint8_t>(filter),
   e361c:	4639      	mov	r1, r7
   e361e:	a848      	add	r0, sp, #288	; 0x120
   e3620:	f8d9 9004 	ldr.w	r9, [r9, #4]
   e3624:	f7f3 f9bf 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
   e3628:	4621      	mov	r1, r4
   e362a:	4640      	mov	r0, r8
   e362c:	687f      	ldr	r7, [r7, #4]
   e362e:	f7f3 f9ba 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e3632:	b104      	cbz	r4, e3636 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x98e>
   e3634:	6864      	ldr	r4, [r4, #4]
        GetTensorShape(output), GetTensorData<uint8_t>(output));
   e3636:	9909      	ldr	r1, [sp, #36]	; 0x24
   e3638:	4628      	mov	r0, r5
   e363a:	f7f3 f9b4 	bl	d69a6 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
  return depthwise_conv::DepthwiseConvBasicKernel<
      DepthwiseConvOutputRounding::kAwayFromZero>::Run(params, input_shape,
                                                       input_data, filter_shape,
                                                       filter_data, bias_shape,
                                                       bias_data, output_shape,
                                                       output_data);
   e363e:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e3640:	685b      	ldr	r3, [r3, #4]
   e3642:	9304      	str	r3, [sp, #16]
   e3644:	464a      	mov	r2, r9
   e3646:	ab48      	add	r3, sp, #288	; 0x120
   e3648:	a94d      	add	r1, sp, #308	; 0x134
   e364a:	a852      	add	r0, sp, #328	; 0x148
   e364c:	9503      	str	r5, [sp, #12]
   e364e:	9402      	str	r4, [sp, #8]
   e3650:	e88d 0180 	stmia.w	sp, {r7, r8}
   e3654:	f7ff fa08 	bl	e2a68 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph>
   e3658:	4628      	mov	r0, r5
   e365a:	f7f2 fef4 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
        GetTensorData<uint8_t>(output));
  } else {
    tflite::reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
        GetTensorShape(filter), GetTensorData<uint8_t>(filter),
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
   e365e:	4640      	mov	r0, r8
   e3660:	f7f2 fef1 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
        GetTensorData<int32_t>(bias), GetTensorShape(output),
        GetTensorData<uint8_t>(output));
  } else {
    tflite::reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
        GetTensorShape(filter), GetTensorData<uint8_t>(filter),
   e3664:	a848      	add	r0, sp, #288	; 0x120
   e3666:	f7f2 feee 	bl	d6446 <_ZN6tflite12RuntimeShapeD1Ev>
        GetTensorData<uint8_t>(filter), GetTensorShape(bias),
        GetTensorData<int32_t>(bias), GetTensorShape(output),
        GetTensorData<uint8_t>(output));
  } else {
    tflite::reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   e366a:	a84d      	add	r0, sp, #308	; 0x134
   e366c:	e5d3      	b.n	e3216 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x56e>
      context->ReportError(context, "Type %s (%d) not supported.",
                           TfLiteTypeGetName(input->type), input->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   e366e:	f50d 7d69 	add.w	sp, sp, #932	; 0x3a4
   e3672:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e3676:	bf00      	nop

000e3678 <_ZN6tflite19GreedyMemoryPlannerD1Ev>:
  buffer_offsets_ = reinterpret_cast<int*>(next_free);
}

GreedyMemoryPlanner::~GreedyMemoryPlanner() {
  // We don't own the scratch buffer, so don't deallocate anything.
}
   e3678:	4770      	bx	lr

000e367a <_ZN6tflite19GreedyMemoryPlanner14GetBufferCountEv>:
    line[kLineWidth] = 0;
    error_reporter->Report("%s", line);
  }
}

int GreedyMemoryPlanner::GetBufferCount() { return buffer_count_; }
   e367a:	6880      	ldr	r0, [r0, #8]
   e367c:	4770      	bx	lr

000e367e <_ZN6tflite19GreedyMemoryPlannerD0Ev>:
  next_free += sizeof(ListEntry) * max_buffer_count_;

  buffer_offsets_ = reinterpret_cast<int*>(next_free);
}

GreedyMemoryPlanner::~GreedyMemoryPlanner() {
   e367e:	b510      	push	{r4, lr}
  // We don't own the scratch buffer, so don't deallocate anything.
}
   e3680:	2128      	movs	r1, #40	; 0x28
  next_free += sizeof(ListEntry) * max_buffer_count_;

  buffer_offsets_ = reinterpret_cast<int*>(next_free);
}

GreedyMemoryPlanner::~GreedyMemoryPlanner() {
   e3682:	4604      	mov	r4, r0
  // We don't own the scratch buffer, so don't deallocate anything.
}
   e3684:	f001 fbd5 	bl	e4e32 <_ZdlPvj>
   e3688:	4620      	mov	r0, r4
   e368a:	bd10      	pop	{r4, pc}

000e368c <_ZN6tflite19GreedyMemoryPlanner9AddBufferEPNS_13ErrorReporterEiii>:

TfLiteStatus GreedyMemoryPlanner::AddBuffer(
    tflite::ErrorReporter* error_reporter, int size, int first_time_used,
    int last_time_used) {
   e368c:	b570      	push	{r4, r5, r6, lr}
   e368e:	4616      	mov	r6, r2
  if (buffer_count_ >= max_buffer_count_) {
   e3690:	6884      	ldr	r4, [r0, #8]
   e3692:	6842      	ldr	r2, [r0, #4]
   e3694:	4294      	cmp	r4, r2
  // We don't own the scratch buffer, so don't deallocate anything.
}

TfLiteStatus GreedyMemoryPlanner::AddBuffer(
    tflite::ErrorReporter* error_reporter, int size, int first_time_used,
    int last_time_used) {
   e3696:	460d      	mov	r5, r1
  if (buffer_count_ >= max_buffer_count_) {
   e3698:	db05      	blt.n	e36a6 <_ZN6tflite19GreedyMemoryPlanner9AddBufferEPNS_13ErrorReporterEiii+0x1a>
    error_reporter->Report("Too many buffers (max is %d)", max_buffer_count_);
   e369a:	490b      	ldr	r1, [pc, #44]	; (e36c8 <_ZN6tflite19GreedyMemoryPlanner9AddBufferEPNS_13ErrorReporterEiii+0x3c>)
   e369c:	4628      	mov	r0, r5
   e369e:	f7f0 fe93 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return kTfLiteError;
   e36a2:	2001      	movs	r0, #1
   e36a4:	bd70      	pop	{r4, r5, r6, pc}
  }
  BufferRequirements* current = &requirements_[buffer_count_];
   e36a6:	68c5      	ldr	r5, [r0, #12]
   e36a8:	210c      	movs	r1, #12
   e36aa:	4361      	muls	r1, r4
   e36ac:	186c      	adds	r4, r5, r1
  current->size = size;
   e36ae:	506e      	str	r6, [r5, r1]
  current->first_time_used = first_time_used;
   e36b0:	6063      	str	r3, [r4, #4]
  current->last_time_used = last_time_used;
   e36b2:	9b04      	ldr	r3, [sp, #16]
   e36b4:	60a3      	str	r3, [r4, #8]
  ++buffer_count_;
   e36b6:	6883      	ldr	r3, [r0, #8]
   e36b8:	3301      	adds	r3, #1
   e36ba:	6083      	str	r3, [r0, #8]
  need_to_calculate_offsets_ = true;
   e36bc:	2301      	movs	r3, #1
   e36be:	f880 3024 	strb.w	r3, [r0, #36]	; 0x24
  return kTfLiteOk;
   e36c2:	2000      	movs	r0, #0
}
   e36c4:	bd70      	pop	{r4, r5, r6, pc}
   e36c6:	bf00      	nop
   e36c8:	000eb071 	.word	0x000eb071

000e36cc <_ZN6tflite18ReverseSortInPlaceEPiS0_i>:
namespace tflite {

// Simple stable in-place sort function. Not time-efficient for large arrays.
// Would normally be in an anonymous namespace to keep it private, but we want
// to be able to test it externally.
void ReverseSortInPlace(int* values, int* ids, int size) {
   e36cc:	b5f0      	push	{r4, r5, r6, r7, lr}
   e36ce:	4696      	mov	lr, r2
   e36d0:	4604      	mov	r4, r0
   e36d2:	460b      	mov	r3, r1
  bool any_swapped;
  do {
    any_swapped = false;
    for (int i = 1; i < size; ++i) {
   e36d4:	2501      	movs	r5, #1
// Would normally be in an anonymous namespace to keep it private, but we want
// to be able to test it externally.
void ReverseSortInPlace(int* values, int* ids, int size) {
  bool any_swapped;
  do {
    any_swapped = false;
   e36d6:	2600      	movs	r6, #0
    for (int i = 1; i < size; ++i) {
   e36d8:	4575      	cmp	r5, lr
   e36da:	da0f      	bge.n	e36fc <_ZN6tflite18ReverseSortInPlaceEPiS0_i+0x30>
      if (values[i - 1] < values[i]) {
   e36dc:	6827      	ldr	r7, [r4, #0]
   e36de:	f854 2f04 	ldr.w	r2, [r4, #4]!
   e36e2:	4297      	cmp	r7, r2
   e36e4:	da07      	bge.n	e36f6 <_ZN6tflite18ReverseSortInPlaceEPiS0_i+0x2a>
        const int value_temp = values[i - 1];
        values[i - 1] = values[i];
   e36e6:	f844 2c04 	str.w	r2, [r4, #-4]
        values[i] = value_temp;
   e36ea:	6027      	str	r7, [r4, #0]
        const int id_temp = ids[i - 1];
        ids[i - 1] = ids[i];
   e36ec:	e893 0044 	ldmia.w	r3, {r2, r6}
   e36f0:	601e      	str	r6, [r3, #0]
        ids[i] = id_temp;
   e36f2:	605a      	str	r2, [r3, #4]
        any_swapped = true;
   e36f4:	2601      	movs	r6, #1
// to be able to test it externally.
void ReverseSortInPlace(int* values, int* ids, int size) {
  bool any_swapped;
  do {
    any_swapped = false;
    for (int i = 1; i < size; ++i) {
   e36f6:	3501      	adds	r5, #1
   e36f8:	3304      	adds	r3, #4
   e36fa:	e7ed      	b.n	e36d8 <_ZN6tflite18ReverseSortInPlaceEPiS0_i+0xc>
// Simple stable in-place sort function. Not time-efficient for large arrays.
// Would normally be in an anonymous namespace to keep it private, but we want
// to be able to test it externally.
void ReverseSortInPlace(int* values, int* ids, int size) {
  bool any_swapped;
  do {
   e36fc:	2e00      	cmp	r6, #0
   e36fe:	d1e7      	bne.n	e36d0 <_ZN6tflite18ReverseSortInPlaceEPiS0_i+0x4>
        ids[i] = id_temp;
        any_swapped = true;
      }
    }
  } while (any_swapped);
}
   e3700:	bdf0      	pop	{r4, r5, r6, r7, pc}
	...

000e3704 <_ZN6tflite19GreedyMemoryPlannerC1EPhi>:

GreedyMemoryPlanner::GreedyMemoryPlanner(unsigned char* scratch_buffer,
                                         int scratch_buffer_size)
    : buffer_count_(0), need_to_calculate_offsets_(true) {
   e3704:	4b0c      	ldr	r3, [pc, #48]	; (e3738 <_ZN6tflite19GreedyMemoryPlannerC1EPhi+0x34>)
   e3706:	6003      	str	r3, [r0, #0]
   e3708:	2300      	movs	r3, #0
   e370a:	6083      	str	r3, [r0, #8]
   e370c:	2301      	movs	r3, #1
   e370e:	f880 3024 	strb.w	r3, [r0, #36]	; 0x24
                              sizeof(int) +  // buffer_sizes_sorted_by_size_
                              sizeof(int) +  // buffer_ids_sorted_by_size_
                              sizeof(ListEntry) +  // buffers_sorted_by_offset_
                              sizeof(int);         // buffer_offsets_;
  // Allocate the arrays we need within the scratch buffer arena.
  max_buffer_count_ = scratch_buffer_size / per_buffer_size;
   e3712:	2324      	movs	r3, #36	; 0x24
   e3714:	fb92 f2f3 	sdiv	r2, r2, r3

  unsigned char* next_free = scratch_buffer;
  requirements_ = reinterpret_cast<BufferRequirements*>(next_free);
  next_free += sizeof(BufferRequirements) * max_buffer_count_;
   e3718:	230c      	movs	r3, #12
   e371a:	4353      	muls	r3, r2
      }
    }
  } while (any_swapped);
}

GreedyMemoryPlanner::GreedyMemoryPlanner(unsigned char* scratch_buffer,
   e371c:	b510      	push	{r4, lr}
                              sizeof(int) +  // buffer_sizes_sorted_by_size_
                              sizeof(int) +  // buffer_ids_sorted_by_size_
                              sizeof(ListEntry) +  // buffers_sorted_by_offset_
                              sizeof(int);         // buffer_offsets_;
  // Allocate the arrays we need within the scratch buffer arena.
  max_buffer_count_ = scratch_buffer_size / per_buffer_size;
   e371e:	6042      	str	r2, [r0, #4]

  unsigned char* next_free = scratch_buffer;
  requirements_ = reinterpret_cast<BufferRequirements*>(next_free);
   e3720:	60c1      	str	r1, [r0, #12]
  next_free += sizeof(BufferRequirements) * max_buffer_count_;

  buffer_sizes_sorted_by_size_ = reinterpret_cast<int*>(next_free);
  next_free += sizeof(int) * max_buffer_count_;
   e3722:	0092      	lsls	r2, r2, #2
  // Allocate the arrays we need within the scratch buffer arena.
  max_buffer_count_ = scratch_buffer_size / per_buffer_size;

  unsigned char* next_free = scratch_buffer;
  requirements_ = reinterpret_cast<BufferRequirements*>(next_free);
  next_free += sizeof(BufferRequirements) * max_buffer_count_;
   e3724:	4419      	add	r1, r3

  buffer_sizes_sorted_by_size_ = reinterpret_cast<int*>(next_free);
   e3726:	6101      	str	r1, [r0, #16]
  next_free += sizeof(int) * max_buffer_count_;
   e3728:	4411      	add	r1, r2

  buffer_ids_sorted_by_size_ = reinterpret_cast<int*>(next_free);
  next_free += sizeof(int) * max_buffer_count_;
   e372a:	440a      	add	r2, r1

  buffers_sorted_by_offset_ = reinterpret_cast<ListEntry*>(next_free);
   e372c:	6182      	str	r2, [r0, #24]
  next_free += sizeof(ListEntry) * max_buffer_count_;

  buffer_offsets_ = reinterpret_cast<int*>(next_free);
   e372e:	441a      	add	r2, r3
  next_free += sizeof(BufferRequirements) * max_buffer_count_;

  buffer_sizes_sorted_by_size_ = reinterpret_cast<int*>(next_free);
  next_free += sizeof(int) * max_buffer_count_;

  buffer_ids_sorted_by_size_ = reinterpret_cast<int*>(next_free);
   e3730:	6141      	str	r1, [r0, #20]
  next_free += sizeof(int) * max_buffer_count_;

  buffers_sorted_by_offset_ = reinterpret_cast<ListEntry*>(next_free);
  next_free += sizeof(ListEntry) * max_buffer_count_;

  buffer_offsets_ = reinterpret_cast<int*>(next_free);
   e3732:	6202      	str	r2, [r0, #32]
}
   e3734:	bd10      	pop	{r4, pc}
   e3736:	bf00      	nop
   e3738:	000eb0f4 	.word	0x000eb0f4

000e373c <_ZNK6tflite19GreedyMemoryPlanner22DoesEntryOverlapInTimeEPKNS0_9ListEntryEii>:
  return kTfLiteOk;
}

bool GreedyMemoryPlanner::DoesEntryOverlapInTime(
    const GreedyMemoryPlanner::ListEntry* entry, const int first_time_used,
    const int last_time_used) const {
   e373c:	b510      	push	{r4, lr}
  const BufferRequirements* entry_requirements =
      &requirements_[entry->requirements_index];
   e373e:	684c      	ldr	r4, [r1, #4]
   e3740:	68c1      	ldr	r1, [r0, #12]
   e3742:	200c      	movs	r0, #12
   e3744:	fb00 1104 	mla	r1, r0, r4, r1
  if (entry_requirements->first_time_used > last_time_used) {
   e3748:	6848      	ldr	r0, [r1, #4]
   e374a:	4298      	cmp	r0, r3
   e374c:	dc05      	bgt.n	e375a <_ZNK6tflite19GreedyMemoryPlanner22DoesEntryOverlapInTimeEPKNS0_9ListEntryEii+0x1e>
    return false;
  }
  if (first_time_used > entry_requirements->last_time_used) {
   e374e:	6888      	ldr	r0, [r1, #8]
   e3750:	4290      	cmp	r0, r2
   e3752:	bfb4      	ite	lt
   e3754:	2000      	movlt	r0, #0
   e3756:	2001      	movge	r0, #1
   e3758:	bd10      	pop	{r4, pc}
    const GreedyMemoryPlanner::ListEntry* entry, const int first_time_used,
    const int last_time_used) const {
  const BufferRequirements* entry_requirements =
      &requirements_[entry->requirements_index];
  if (entry_requirements->first_time_used > last_time_used) {
    return false;
   e375a:	2000      	movs	r0, #0
  }
  if (first_time_used > entry_requirements->last_time_used) {
    return false;
  }
  return true;
}
   e375c:	bd10      	pop	{r4, pc}

000e375e <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii>:

GreedyMemoryPlanner::ListEntry*
GreedyMemoryPlanner::NextSimultaneouslyActiveBuffer(
    const GreedyMemoryPlanner::ListEntry* start, const int first_time_used,
    const int last_time_used) {
   e375e:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
   e3762:	4605      	mov	r5, r0
   e3764:	4616      	mov	r6, r2
   e3766:	461f      	mov	r7, r3
  ListEntry* result = nullptr;
  ListEntry* candidate_next_entry;
  if (start == nullptr) {
   e3768:	b919      	cbnz	r1, e3772 <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0x14>
    candidate_next_entry = &buffers_sorted_by_offset_[0];
   e376a:	6984      	ldr	r4, [r0, #24]
    }
    if (candidate_next_entry->next_entry_index == -1) {
      break;
    }
    candidate_next_entry =
        &buffers_sorted_by_offset_[candidate_next_entry->next_entry_index];
   e376c:	f04f 080c 	mov.w	r8, #12
   e3770:	e00d      	b.n	e378e <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0x30>
  ListEntry* result = nullptr;
  ListEntry* candidate_next_entry;
  if (start == nullptr) {
    candidate_next_entry = &buffers_sorted_by_offset_[0];
  } else {
    if (start->next_entry_index == -1) {
   e3772:	688b      	ldr	r3, [r1, #8]
   e3774:	1c59      	adds	r1, r3, #1
   e3776:	d013      	beq.n	e37a0 <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0x42>
      return nullptr;
    }
    candidate_next_entry = &buffers_sorted_by_offset_[start->next_entry_index];
   e3778:	6982      	ldr	r2, [r0, #24]
   e377a:	240c      	movs	r4, #12
   e377c:	fb04 2403 	mla	r4, r4, r3, r2
   e3780:	e7f4      	b.n	e376c <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0xe>
    if (DoesEntryOverlapInTime(candidate_next_entry, first_time_used,
                               last_time_used)) {
      result = candidate_next_entry;
      break;
    }
    if (candidate_next_entry->next_entry_index == -1) {
   e3782:	68a3      	ldr	r3, [r4, #8]
   e3784:	1c5a      	adds	r2, r3, #1
   e3786:	d00f      	beq.n	e37a8 <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0x4a>
      break;
    }
    candidate_next_entry =
        &buffers_sorted_by_offset_[candidate_next_entry->next_entry_index];
   e3788:	69ac      	ldr	r4, [r5, #24]
   e378a:	fb08 4403 	mla	r4, r8, r3, r4
      return nullptr;
    }
    candidate_next_entry = &buffers_sorted_by_offset_[start->next_entry_index];
  }
  do {
    if (DoesEntryOverlapInTime(candidate_next_entry, first_time_used,
   e378e:	463b      	mov	r3, r7
   e3790:	4632      	mov	r2, r6
   e3792:	4621      	mov	r1, r4
   e3794:	4628      	mov	r0, r5
   e3796:	f7ff ffd1 	bl	e373c <_ZNK6tflite19GreedyMemoryPlanner22DoesEntryOverlapInTimeEPKNS0_9ListEntryEii>
   e379a:	2800      	cmp	r0, #0
   e379c:	d0f1      	beq.n	e3782 <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0x24>
   e379e:	e002      	b.n	e37a6 <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0x48>
  ListEntry* candidate_next_entry;
  if (start == nullptr) {
    candidate_next_entry = &buffers_sorted_by_offset_[0];
  } else {
    if (start->next_entry_index == -1) {
      return nullptr;
   e37a0:	2000      	movs	r0, #0
   e37a2:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   e37a6:	4620      	mov	r0, r4
    }
    candidate_next_entry =
        &buffers_sorted_by_offset_[candidate_next_entry->next_entry_index];
  } while (true);
  return result;
}
   e37a8:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}

000e37ac <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv>:

void GreedyMemoryPlanner::CalculateOffsetsIfNeeded() {
   e37ac:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  if (!need_to_calculate_offsets_ || (buffer_count_ == 0)) {
   e37b0:	f890 3024 	ldrb.w	r3, [r0, #36]	; 0x24
        &buffers_sorted_by_offset_[candidate_next_entry->next_entry_index];
  } while (true);
  return result;
}

void GreedyMemoryPlanner::CalculateOffsetsIfNeeded() {
   e37b4:	b085      	sub	sp, #20
   e37b6:	4604      	mov	r4, r0
  if (!need_to_calculate_offsets_ || (buffer_count_ == 0)) {
   e37b8:	2b00      	cmp	r3, #0
   e37ba:	f000 8089 	beq.w	e38d0 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x124>
   e37be:	6883      	ldr	r3, [r0, #8]
   e37c0:	2b00      	cmp	r3, #0
   e37c2:	f000 8085 	beq.w	e38d0 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x124>
    return;
  }
  need_to_calculate_offsets_ = false;
   e37c6:	2300      	movs	r3, #0
   e37c8:	f880 3024 	strb.w	r3, [r0, #36]	; 0x24
  // This helps find a more compact layout. Intuitively, you can think
  // about putting the large buffers in place first, and then the
  // smaller buffers can fit in the gaps, rather than fragmenting the
  // gaps with small buffers at the beginning.
  for (int i = 0; i < buffer_count_; ++i) {
    buffer_sizes_sorted_by_size_[i] = requirements_[i].size;
   e37cc:	250c      	movs	r5, #12
    buffer_ids_sorted_by_size_[i] = i;
    buffer_offsets_[i] = -1;
   e37ce:	f04f 31ff 	mov.w	r1, #4294967295	; 0xffffffff
  // Start off by ordering the buffers in descending order of size.
  // This helps find a more compact layout. Intuitively, you can think
  // about putting the large buffers in place first, and then the
  // smaller buffers can fit in the gaps, rather than fragmenting the
  // gaps with small buffers at the beginning.
  for (int i = 0; i < buffer_count_; ++i) {
   e37d2:	68a2      	ldr	r2, [r4, #8]
   e37d4:	429a      	cmp	r2, r3
   e37d6:	dd0e      	ble.n	e37f6 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x4a>
    buffer_sizes_sorted_by_size_[i] = requirements_[i].size;
   e37d8:	68e0      	ldr	r0, [r4, #12]
   e37da:	fb05 f203 	mul.w	r2, r5, r3
   e37de:	5880      	ldr	r0, [r0, r2]
   e37e0:	6922      	ldr	r2, [r4, #16]
   e37e2:	f842 0023 	str.w	r0, [r2, r3, lsl #2]
    buffer_ids_sorted_by_size_[i] = i;
   e37e6:	6962      	ldr	r2, [r4, #20]
   e37e8:	f842 3023 	str.w	r3, [r2, r3, lsl #2]
    buffer_offsets_[i] = -1;
   e37ec:	6a22      	ldr	r2, [r4, #32]
   e37ee:	f842 1023 	str.w	r1, [r2, r3, lsl #2]
  // Start off by ordering the buffers in descending order of size.
  // This helps find a more compact layout. Intuitively, you can think
  // about putting the large buffers in place first, and then the
  // smaller buffers can fit in the gaps, rather than fragmenting the
  // gaps with small buffers at the beginning.
  for (int i = 0; i < buffer_count_; ++i) {
   e37f2:	3301      	adds	r3, #1
   e37f4:	e7ed      	b.n	e37d2 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x26>
    buffer_offsets_[i] = -1;
  }
  // This sorting algorithm is naive, and may end up taking a very long time
  // with hundreds of buffers.
  ReverseSortInPlace(buffer_sizes_sorted_by_size_, buffer_ids_sorted_by_size_,
                     buffer_count_);
   e37f6:	6961      	ldr	r1, [r4, #20]
   e37f8:	6920      	ldr	r0, [r4, #16]
   e37fa:	f7ff ff67 	bl	e36cc <_ZN6tflite18ReverseSortInPlaceEPiS0_i>

  // Put the largest buffer at offset zero to start the process.
  ListEntry* first_entry = &buffers_sorted_by_offset_[0];
   e37fe:	f8d4 8018 	ldr.w	r8, [r4, #24]
  first_entry->offset = 0;
   e3802:	2300      	movs	r3, #0
   e3804:	f8c8 3000 	str.w	r3, [r8]
  first_entry->requirements_index = buffer_ids_sorted_by_size_[0];
   e3808:	6962      	ldr	r2, [r4, #20]
   e380a:	6812      	ldr	r2, [r2, #0]
   e380c:	f8c8 2004 	str.w	r2, [r8, #4]
  first_entry->next_entry_index = -1;
   e3810:	f04f 32ff 	mov.w	r2, #4294967295	; 0xffffffff
   e3814:	f8c8 2008 	str.w	r2, [r8, #8]
  next_free_entry_ = 1;
  buffer_offsets_[buffer_ids_sorted_by_size_[0]] = 0;
   e3818:	6962      	ldr	r2, [r4, #20]
  // Put the largest buffer at offset zero to start the process.
  ListEntry* first_entry = &buffers_sorted_by_offset_[0];
  first_entry->offset = 0;
  first_entry->requirements_index = buffer_ids_sorted_by_size_[0];
  first_entry->next_entry_index = -1;
  next_free_entry_ = 1;
   e381a:	2501      	movs	r5, #1
   e381c:	61e5      	str	r5, [r4, #28]
  buffer_offsets_[buffer_ids_sorted_by_size_[0]] = 0;
   e381e:	6811      	ldr	r1, [r2, #0]
   e3820:	6a22      	ldr	r2, [r4, #32]
   e3822:	f842 3021 	str.w	r3, [r2, r1, lsl #2]
  // Work through the rest of the buffers to find a good gap to place each one.
  for (int i = 1; i < buffer_count_; ++i) {
    // The id is the order the buffer was originally added by the client.
    const int buffer_id = buffer_ids_sorted_by_size_[i];
    // Look at what size and time range the buffer needs to be active.
    BufferRequirements* wanted_requirements = &requirements_[buffer_id];
   e3826:	f04f 090c 	mov.w	r9, #12
  first_entry->next_entry_index = -1;
  next_free_entry_ = 1;
  buffer_offsets_[buffer_ids_sorted_by_size_[0]] = 0;

  // Work through the rest of the buffers to find a good gap to place each one.
  for (int i = 1; i < buffer_count_; ++i) {
   e382a:	68a3      	ldr	r3, [r4, #8]
   e382c:	42ab      	cmp	r3, r5
   e382e:	dd4f      	ble.n	e38d0 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x124>
    // The id is the order the buffer was originally added by the client.
    const int buffer_id = buffer_ids_sorted_by_size_[i];
   e3830:	6963      	ldr	r3, [r4, #20]
    // Look at what size and time range the buffer needs to be active.
    BufferRequirements* wanted_requirements = &requirements_[buffer_id];
   e3832:	f8d4 b00c 	ldr.w	fp, [r4, #12]
  buffer_offsets_[buffer_ids_sorted_by_size_[0]] = 0;

  // Work through the rest of the buffers to find a good gap to place each one.
  for (int i = 1; i < buffer_count_; ++i) {
    // The id is the order the buffer was originally added by the client.
    const int buffer_id = buffer_ids_sorted_by_size_[i];
   e3836:	f853 a025 	ldr.w	sl, [r3, r5, lsl #2]
    // Look at what size and time range the buffer needs to be active.
    BufferRequirements* wanted_requirements = &requirements_[buffer_id];
   e383a:	fb09 f20a 	mul.w	r2, r9, sl
   e383e:	eb0b 0302 	add.w	r3, fp, r2
    const int wanted_size = wanted_requirements->size;
   e3842:	f85b 2002 	ldr.w	r2, [fp, r2]
   e3846:	9201      	str	r2, [sp, #4]
    // buffers are stored in the order of their starting position in the arena
    // so that it's easy to find the next buffer in memory, and so the gap.
    // The candidate_entry variable holds the buffer that we're considering
    // placing the current buffer after.
    ListEntry* prior_entry = nullptr;
    int candidate_offset = 0;
   e3848:	2600      	movs	r6, #0
    // The id is the order the buffer was originally added by the client.
    const int buffer_id = buffer_ids_sorted_by_size_[i];
    // Look at what size and time range the buffer needs to be active.
    BufferRequirements* wanted_requirements = &requirements_[buffer_id];
    const int wanted_size = wanted_requirements->size;
    const int wanted_first_time_used = wanted_requirements->first_time_used;
   e384a:	685a      	ldr	r2, [r3, #4]
    const int wanted_last_time_used = wanted_requirements->last_time_used;
   e384c:	689b      	ldr	r3, [r3, #8]
    // The id is the order the buffer was originally added by the client.
    const int buffer_id = buffer_ids_sorted_by_size_[i];
    // Look at what size and time range the buffer needs to be active.
    BufferRequirements* wanted_requirements = &requirements_[buffer_id];
    const int wanted_size = wanted_requirements->size;
    const int wanted_first_time_used = wanted_requirements->first_time_used;
   e384e:	9202      	str	r2, [sp, #8]
    const int wanted_last_time_used = wanted_requirements->last_time_used;
   e3850:	9303      	str	r3, [sp, #12]
    // Find the first buffer that's active in our time range. All placed
    // buffers are stored in the order of their starting position in the arena
    // so that it's easy to find the next buffer in memory, and so the gap.
    // The candidate_entry variable holds the buffer that we're considering
    // placing the current buffer after.
    ListEntry* prior_entry = nullptr;
   e3852:	4637      	mov	r7, r6
    int candidate_offset = 0;
    // Loop through the offset-ordered list of buffers, looking for gaps.
    while (true) {
      // Find out what the next active buffer is.
      ListEntry* next_entry = NextSimultaneouslyActiveBuffer(
          prior_entry, wanted_first_time_used, wanted_last_time_used);
   e3854:	9b03      	ldr	r3, [sp, #12]
   e3856:	9a02      	ldr	r2, [sp, #8]
   e3858:	4639      	mov	r1, r7
   e385a:	4620      	mov	r0, r4
   e385c:	f7ff ff7f 	bl	e375e <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii>

      if (prior_entry) {
   e3860:	b14f      	cbz	r7, e3876 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0xca>
        BufferRequirements* candidate_requirements =
            &requirements_[prior_entry->requirements_index];
        const int prior_entry_offset =
            prior_entry->offset + candidate_requirements->size;
   e3862:	687b      	ldr	r3, [r7, #4]
   e3864:	fb09 f303 	mul.w	r3, r9, r3
   e3868:	f85b 2003 	ldr.w	r2, [fp, r3]
   e386c:	683b      	ldr	r3, [r7, #0]
   e386e:	4413      	add	r3, r2
   e3870:	429e      	cmp	r6, r3
   e3872:	bfb8      	it	lt
   e3874:	461e      	movlt	r6, r3
        if (prior_entry_offset > candidate_offset) {
          candidate_offset = prior_entry_offset;
        }
      }
      if (next_entry == nullptr) {
   e3876:	b978      	cbnz	r0, e3898 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0xec>
    }
    // At this point, we've either found a gap (possibly at the end of the
    // list) and want to place the buffer there, or there are no other active
    // buffers in this time range and so we can put it at offset zero.
    // Record the buffer's offset in our plan.
    buffer_offsets_[buffer_id] = candidate_offset;
   e3878:	6a23      	ldr	r3, [r4, #32]
   e387a:	f843 602a 	str.w	r6, [r3, sl, lsl #2]
    // Add the newly-placed buffer to our offset-ordered list, so that
    // subsequent passes can fit in their buffers around it.
    ListEntry* new_entry = &buffers_sorted_by_offset_[next_free_entry_];
   e387e:	69e3      	ldr	r3, [r4, #28]
   e3880:	69a2      	ldr	r2, [r4, #24]
   e3882:	fb09 f303 	mul.w	r3, r9, r3
   e3886:	18d7      	adds	r7, r2, r3
    new_entry->offset = candidate_offset;
   e3888:	50d6      	str	r6, [r2, r3]
    new_entry->requirements_index = buffer_id;
   e388a:	f8c7 a004 	str.w	sl, [r7, #4]
    const int new_entry_index = next_free_entry_;
   e388e:	69e0      	ldr	r0, [r4, #28]
    ++next_free_entry_;
   e3890:	1c43      	adds	r3, r0, #1
   e3892:	61e3      	str	r3, [r4, #28]
   e3894:	4643      	mov	r3, r8
   e3896:	e011      	b.n	e38bc <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x110>
        // here.
        break;
      }
      // Find out how much space there is between us and the next buffer.
      const int gap = next_entry->offset - candidate_offset;
      if (gap >= wanted_size) {
   e3898:	6803      	ldr	r3, [r0, #0]
   e389a:	9a01      	ldr	r2, [sp, #4]
   e389c:	1b9b      	subs	r3, r3, r6
   e389e:	429a      	cmp	r2, r3
   e38a0:	4607      	mov	r7, r0
   e38a2:	dcd7      	bgt.n	e3854 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0xa8>
   e38a4:	e7e8      	b.n	e3878 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0xcc>
        // We're at the end of the list, so just add the new entry here.
        current_entry->next_entry_index = new_entry_index;
        new_entry->next_entry_index = -1;
        break;
      }
      ListEntry* next_entry = &buffers_sorted_by_offset_[next_entry_index];
   e38a6:	fb09 f102 	mul.w	r1, r9, r2
   e38aa:	f8d4 e018 	ldr.w	lr, [r4, #24]
   e38ae:	eb0e 0c01 	add.w	ip, lr, r1
      if (next_entry->offset > candidate_offset) {
   e38b2:	f85e 1001 	ldr.w	r1, [lr, r1]
   e38b6:	428e      	cmp	r6, r1
   e38b8:	db06      	blt.n	e38c8 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x11c>
   e38ba:	4663      	mov	r3, ip
    ++next_free_entry_;
    ListEntry* current_entry = first_entry;
    // Make sure that we insert the buffer at the correct place in the ordered
    // list.
    while (true) {
      const int next_entry_index = current_entry->next_entry_index;
   e38bc:	689a      	ldr	r2, [r3, #8]
      if (next_entry_index == -1) {
   e38be:	1c51      	adds	r1, r2, #1
   e38c0:	d1f1      	bne.n	e38a6 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0xfa>
        // We're at the end of the list, so just add the new entry here.
        current_entry->next_entry_index = new_entry_index;
   e38c2:	6098      	str	r0, [r3, #8]
        new_entry->next_entry_index = -1;
   e38c4:	60ba      	str	r2, [r7, #8]
   e38c6:	e001      	b.n	e38cc <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x120>
      }
      ListEntry* next_entry = &buffers_sorted_by_offset_[next_entry_index];
      if (next_entry->offset > candidate_offset) {
        // We're at the right spot to do an insertion and retain the sorting
        // order, so place the new entry here.
        new_entry->next_entry_index = current_entry->next_entry_index;
   e38c8:	60ba      	str	r2, [r7, #8]
        current_entry->next_entry_index = new_entry_index;
   e38ca:	6098      	str	r0, [r3, #8]
  first_entry->next_entry_index = -1;
  next_free_entry_ = 1;
  buffer_offsets_[buffer_ids_sorted_by_size_[0]] = 0;

  // Work through the rest of the buffers to find a good gap to place each one.
  for (int i = 1; i < buffer_count_; ++i) {
   e38cc:	3501      	adds	r5, #1
   e38ce:	e7ac      	b.n	e382a <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x7e>
        break;
      }
      current_entry = next_entry;
    }
  }
}
   e38d0:	b005      	add	sp, #20
   e38d2:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000e38d6 <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv>:

int GreedyMemoryPlanner::GetMaximumMemorySize() {
   e38d6:	b570      	push	{r4, r5, r6, lr}
   e38d8:	4604      	mov	r4, r0
  CalculateOffsetsIfNeeded();
   e38da:	f7ff ff67 	bl	e37ac <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv>
  if (buffer_count_ == 0) {
   e38de:	68a0      	ldr	r0, [r4, #8]
   e38e0:	b198      	cbz	r0, e390a <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv+0x34>
    return 0;
  }
  ListEntry* entry = &buffers_sorted_by_offset_[0];
   e38e2:	69a1      	ldr	r1, [r4, #24]
  int max_size = 0;
   e38e4:	2000      	movs	r0, #0
int GreedyMemoryPlanner::GetMaximumMemorySize() {
  CalculateOffsetsIfNeeded();
  if (buffer_count_ == 0) {
    return 0;
  }
  ListEntry* entry = &buffers_sorted_by_offset_[0];
   e38e6:	460b      	mov	r3, r1
  int max_size = 0;
  while (entry) {
    BufferRequirements* requirements =
        &requirements_[entry->requirements_index];
    const int current_size = entry->offset + requirements->size;
   e38e8:	250c      	movs	r5, #12
  if (buffer_count_ == 0) {
    return 0;
  }
  ListEntry* entry = &buffers_sorted_by_offset_[0];
  int max_size = 0;
  while (entry) {
   e38ea:	b173      	cbz	r3, e390a <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv+0x34>
    BufferRequirements* requirements =
        &requirements_[entry->requirements_index];
    const int current_size = entry->offset + requirements->size;
   e38ec:	685a      	ldr	r2, [r3, #4]
   e38ee:	68e6      	ldr	r6, [r4, #12]
   e38f0:	436a      	muls	r2, r5
   e38f2:	58b6      	ldr	r6, [r6, r2]
   e38f4:	681a      	ldr	r2, [r3, #0]
    if (current_size > max_size) {
      max_size = current_size;
    }
    if (entry->next_entry_index == -1) {
   e38f6:	689b      	ldr	r3, [r3, #8]
  ListEntry* entry = &buffers_sorted_by_offset_[0];
  int max_size = 0;
  while (entry) {
    BufferRequirements* requirements =
        &requirements_[entry->requirements_index];
    const int current_size = entry->offset + requirements->size;
   e38f8:	4432      	add	r2, r6
   e38fa:	4290      	cmp	r0, r2
   e38fc:	bfb8      	it	lt
   e38fe:	4610      	movlt	r0, r2
    if (current_size > max_size) {
      max_size = current_size;
    }
    if (entry->next_entry_index == -1) {
   e3900:	1c5a      	adds	r2, r3, #1
   e3902:	d002      	beq.n	e390a <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv+0x34>
      break;
    }
    entry = &buffers_sorted_by_offset_[entry->next_entry_index];
   e3904:	fb05 1303 	mla	r3, r5, r3, r1
   e3908:	e7ef      	b.n	e38ea <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv+0x14>
  }
  return max_size;
}
   e390a:	bd70      	pop	{r4, r5, r6, pc}

000e390c <_ZN6tflite19GreedyMemoryPlanner18GetOffsetForBufferEPNS_13ErrorReporterEiPi>:
}

int GreedyMemoryPlanner::GetBufferCount() { return buffer_count_; }

TfLiteStatus GreedyMemoryPlanner::GetOffsetForBuffer(
    tflite::ErrorReporter* error_reporter, int buffer_index, int* offset) {
   e390c:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
   e390e:	4614      	mov	r4, r2
   e3910:	4605      	mov	r5, r0
   e3912:	460f      	mov	r7, r1
   e3914:	461e      	mov	r6, r3
  CalculateOffsetsIfNeeded();
   e3916:	f7ff ff49 	bl	e37ac <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv>
  if ((buffer_index < 0) || (buffer_index >= buffer_count_)) {
   e391a:	2c00      	cmp	r4, #0
   e391c:	db02      	blt.n	e3924 <_ZN6tflite19GreedyMemoryPlanner18GetOffsetForBufferEPNS_13ErrorReporterEiPi+0x18>
   e391e:	68ab      	ldr	r3, [r5, #8]
   e3920:	429c      	cmp	r4, r3
   e3922:	db07      	blt.n	e3934 <_ZN6tflite19GreedyMemoryPlanner18GetOffsetForBufferEPNS_13ErrorReporterEiPi+0x28>
    error_reporter->Report("buffer index %d is outside range 0 to %d",
                           buffer_index, buffer_count_);
   e3924:	68ab      	ldr	r3, [r5, #8]
   e3926:	4906      	ldr	r1, [pc, #24]	; (e3940 <_ZN6tflite19GreedyMemoryPlanner18GetOffsetForBufferEPNS_13ErrorReporterEiPi+0x34>)
   e3928:	4622      	mov	r2, r4
   e392a:	4638      	mov	r0, r7
   e392c:	f7f0 fd4c 	bl	d43c8 <_ZN6tflite13ErrorReporter6ReportEPKcz>
   e3930:	2001      	movs	r0, #1
   e3932:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
    return kTfLiteError;
  }
  *offset = buffer_offsets_[buffer_index];
   e3934:	6a2b      	ldr	r3, [r5, #32]
   e3936:	f853 3024 	ldr.w	r3, [r3, r4, lsl #2]
   e393a:	6033      	str	r3, [r6, #0]
  return kTfLiteOk;
   e393c:	2000      	movs	r0, #0
}
   e393e:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
   e3940:	000eb08e 	.word	0x000eb08e

000e3944 <_ZZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_ENKUlfE_clEf>:
                                           TfLiteTensor* output,
                                           int32_t* act_min, int32_t* act_max) {
  const auto scale = output->params.scale;
  const auto zero_point = output->params.zero_point;

  auto quantize = [scale, zero_point](float f) {
   e3944:	b510      	push	{r4, lr}
    { return __builtin_rint(__x); }

#ifndef __CORRECT_ISO_CPP11_MATH_H_PROTO
  constexpr float
  round(float __x)
  { return __builtin_roundf(__x); }
   e3946:	edd0 7a00 	vldr	s15, [r0]
   e394a:	ee80 0a27 	vdiv.f32	s0, s0, s15
   e394e:	4604      	mov	r4, r0
   e3950:	f001 fe06 	bl	e5560 <roundf>
    return zero_point + static_cast<int32_t>(TfLiteRound(f / scale));
  };
   e3954:	6863      	ldr	r3, [r4, #4]
   e3956:	eefd 7ac0 	vcvt.s32.f32	s15, s0
   e395a:	ee17 0a90 	vmov	r0, s15
   e395e:	4418      	add	r0, r3
   e3960:	bd10      	pop	{r4, pc}
	...

000e3964 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_>:

namespace {
void CalculateActivationRangeQuantizedImpl(TfLiteFusedActivation activation,
                                           int32_t qmin, int32_t qmax,
                                           TfLiteTensor* output,
                                           int32_t* act_min, int32_t* act_max) {
   e3964:	b5f7      	push	{r0, r1, r2, r4, r5, r6, r7, lr}

  auto quantize = [scale, zero_point](float f) {
    return zero_point + static_cast<int32_t>(TfLiteRound(f / scale));
  };

  if (activation == kTfLiteActRelu) {
   e3966:	2801      	cmp	r0, #1

namespace {
void CalculateActivationRangeQuantizedImpl(TfLiteFusedActivation activation,
                                           int32_t qmin, int32_t qmax,
                                           TfLiteTensor* output,
                                           int32_t* act_min, int32_t* act_max) {
   e3968:	4615      	mov	r5, r2
  const auto scale = output->params.scale;
  const auto zero_point = output->params.zero_point;
   e396a:	691a      	ldr	r2, [r3, #16]

  auto quantize = [scale, zero_point](float f) {
    return zero_point + static_cast<int32_t>(TfLiteRound(f / scale));
  };
   e396c:	68db      	ldr	r3, [r3, #12]

namespace {
void CalculateActivationRangeQuantizedImpl(TfLiteFusedActivation activation,
                                           int32_t qmin, int32_t qmax,
                                           TfLiteTensor* output,
                                           int32_t* act_min, int32_t* act_max) {
   e396e:	9e08      	ldr	r6, [sp, #32]
   e3970:	9c09      	ldr	r4, [sp, #36]	; 0x24
  const auto scale = output->params.scale;
  const auto zero_point = output->params.zero_point;

  auto quantize = [scale, zero_point](float f) {
    return zero_point + static_cast<int32_t>(TfLiteRound(f / scale));
  };
   e3972:	9300      	str	r3, [sp, #0]

namespace {
void CalculateActivationRangeQuantizedImpl(TfLiteFusedActivation activation,
                                           int32_t qmin, int32_t qmax,
                                           TfLiteTensor* output,
                                           int32_t* act_min, int32_t* act_max) {
   e3974:	460f      	mov	r7, r1
  const auto scale = output->params.scale;
  const auto zero_point = output->params.zero_point;

  auto quantize = [scale, zero_point](float f) {
    return zero_point + static_cast<int32_t>(TfLiteRound(f / scale));
  };
   e3976:	9201      	str	r2, [sp, #4]

  if (activation == kTfLiteActRelu) {
   e3978:	d109      	bne.n	e398e <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x2a>
    *act_min = std::max(qmin, quantize(0.0));
   e397a:	ed9f 0a18 	vldr	s0, [pc, #96]	; e39dc <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x78>
   e397e:	4668      	mov	r0, sp
   e3980:	f7ff ffe0 	bl	e3944 <_ZZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_ENKUlfE_clEf>
   e3984:	42b8      	cmp	r0, r7
   e3986:	bfac      	ite	ge
   e3988:	6030      	strge	r0, [r6, #0]
   e398a:	6037      	strlt	r7, [r6, #0]
   e398c:	e023      	b.n	e39d6 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x72>
    *act_max = qmax;
  } else if (activation == kTfLiteActRelu6) {
   e398e:	2803      	cmp	r0, #3
   e3990:	d10b      	bne.n	e39aa <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x46>
    *act_min = std::max(qmin, quantize(0.0));
   e3992:	ed9f 0a12 	vldr	s0, [pc, #72]	; e39dc <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x78>
   e3996:	4668      	mov	r0, sp
   e3998:	f7ff ffd4 	bl	e3944 <_ZZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_ENKUlfE_clEf>
    *act_max = std::min(qmax, quantize(6.0));
   e399c:	eeb1 0a08 	vmov.f32	s0, #24	; 0x40c00000  6.0

  if (activation == kTfLiteActRelu) {
    *act_min = std::max(qmin, quantize(0.0));
    *act_max = qmax;
  } else if (activation == kTfLiteActRelu6) {
    *act_min = std::max(qmin, quantize(0.0));
   e39a0:	42b8      	cmp	r0, r7
   e39a2:	bfac      	ite	ge
   e39a4:	6030      	strge	r0, [r6, #0]
   e39a6:	6037      	strlt	r7, [r6, #0]
   e39a8:	e00c      	b.n	e39c4 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x60>
    *act_max = std::min(qmax, quantize(6.0));
  } else if (activation == kTfLiteActRelu1) {
   e39aa:	2802      	cmp	r0, #2
   e39ac:	d112      	bne.n	e39d4 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x70>
    *act_min = std::max(qmin, quantize(-1.0));
   e39ae:	eebf 0a00 	vmov.f32	s0, #240	; 0xbf800000 -1.0
   e39b2:	4668      	mov	r0, sp
   e39b4:	f7ff ffc6 	bl	e3944 <_ZZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_ENKUlfE_clEf>
    *act_max = std::min(qmax, quantize(1.0));
   e39b8:	eeb7 0a00 	vmov.f32	s0, #112	; 0x3f800000  1.0
    *act_max = qmax;
  } else if (activation == kTfLiteActRelu6) {
    *act_min = std::max(qmin, quantize(0.0));
    *act_max = std::min(qmax, quantize(6.0));
  } else if (activation == kTfLiteActRelu1) {
    *act_min = std::max(qmin, quantize(-1.0));
   e39bc:	42b8      	cmp	r0, r7
   e39be:	bfac      	ite	ge
   e39c0:	6030      	strge	r0, [r6, #0]
   e39c2:	6037      	strlt	r7, [r6, #0]
    *act_max = std::min(qmax, quantize(1.0));
   e39c4:	4668      	mov	r0, sp
   e39c6:	f7ff ffbd 	bl	e3944 <_ZZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_ENKUlfE_clEf>
   e39ca:	4285      	cmp	r5, r0
   e39cc:	bfd4      	ite	le
   e39ce:	6025      	strle	r5, [r4, #0]
   e39d0:	6020      	strgt	r0, [r4, #0]
   e39d2:	e001      	b.n	e39d8 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x74>
  } else {
    *act_min = qmin;
   e39d4:	6031      	str	r1, [r6, #0]
    *act_max = qmax;
   e39d6:	6025      	str	r5, [r4, #0]
  }
}
   e39d8:	b003      	add	sp, #12
   e39da:	bdf0      	pop	{r4, r5, r6, r7, pc}
   e39dc:	00000000 	.word	0x00000000

000e39e0 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd>:

TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              TfLiteTensor* output,
                                              double* multiplier) {
   e39e0:	b5f7      	push	{r0, r1, r2, r4, r5, r6, r7, lr}
  const double input_product_scale = input->params.scale * filter->params.scale;
   e39e2:	edd2 7a03 	vldr	s15, [r2, #12]
   e39e6:	ed91 7a03 	vldr	s14, [r1, #12]
   e39ea:	ee67 7a27 	vmul.f32	s15, s14, s15

TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              TfLiteTensor* output,
                                              double* multiplier) {
   e39ee:	4604      	mov	r4, r0
  const double input_product_scale = input->params.scale * filter->params.scale;
   e39f0:	ee17 0a90 	vmov	r0, s15

TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              TfLiteTensor* output,
                                              double* multiplier) {
   e39f4:	461d      	mov	r5, r3
  const double input_product_scale = input->params.scale * filter->params.scale;
   e39f6:	f003 fb01 	bl	e6ffc <__aeabi_f2d>
  TF_LITE_ENSURE(context, input_product_scale >= 0);
   e39fa:	2200      	movs	r2, #0
   e39fc:	2300      	movs	r3, #0
TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              TfLiteTensor* output,
                                              double* multiplier) {
  const double input_product_scale = input->params.scale * filter->params.scale;
   e39fe:	4606      	mov	r6, r0
   e3a00:	460f      	mov	r7, r1
  TF_LITE_ENSURE(context, input_product_scale >= 0);
   e3a02:	f003 fdd5 	bl	e75b0 <__aeabi_dcmpge>
   e3a06:	b948      	cbnz	r0, e3a1c <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd+0x3c>
   e3a08:	4b0c      	ldr	r3, [pc, #48]	; (e3a3c <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd+0x5c>)
   e3a0a:	9300      	str	r3, [sp, #0]
   e3a0c:	4620      	mov	r0, r4
   e3a0e:	6965      	ldr	r5, [r4, #20]
   e3a10:	4a0b      	ldr	r2, [pc, #44]	; (e3a40 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd+0x60>)
   e3a12:	490c      	ldr	r1, [pc, #48]	; (e3a44 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd+0x64>)
   e3a14:	2376      	movs	r3, #118	; 0x76
   e3a16:	47a8      	blx	r5
   e3a18:	2001      	movs	r0, #1
   e3a1a:	e00c      	b.n	e3a36 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd+0x56>
  *multiplier = input_product_scale / output->params.scale;
   e3a1c:	68e8      	ldr	r0, [r5, #12]
   e3a1e:	f003 faed 	bl	e6ffc <__aeabi_f2d>
   e3a22:	460b      	mov	r3, r1
   e3a24:	4602      	mov	r2, r0
   e3a26:	4639      	mov	r1, r7
   e3a28:	4630      	mov	r0, r6
   e3a2a:	f003 fc65 	bl	e72f8 <__aeabi_ddiv>
   e3a2e:	9b08      	ldr	r3, [sp, #32]
   e3a30:	e9c3 0100 	strd	r0, r1, [r3]

  return kTfLiteOk;
   e3a34:	2000      	movs	r0, #0
}
   e3a36:	b003      	add	sp, #12
   e3a38:	bdf0      	pop	{r4, r5, r6, r7, pc}
   e3a3a:	bf00      	nop
   e3a3c:	000eb1a6 	.word	0x000eb1a6
   e3a40:	000eb10c 	.word	0x000eb10c
   e3a44:	000e9ac8 	.word	0x000e9ac8

000e3a48 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd>:
TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              const TfLiteTensor* bias,
                                              TfLiteTensor* output,
                                              double* multiplier) {
   e3a48:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  const double input_product_scale = input->params.scale * filter->params.scale;
   e3a4c:	ed91 7a03 	vldr	s14, [r1, #12]
   e3a50:	edd2 7a03 	vldr	s15, [r2, #12]
TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              const TfLiteTensor* bias,
                                              TfLiteTensor* output,
                                              double* multiplier) {
   e3a54:	b087      	sub	sp, #28
  const double input_product_scale = input->params.scale * filter->params.scale;
   e3a56:	ee67 7a27 	vmul.f32	s15, s14, s15
TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              const TfLiteTensor* bias,
                                              TfLiteTensor* output,
                                              double* multiplier) {
   e3a5a:	4604      	mov	r4, r0
   e3a5c:	461e      	mov	r6, r3
  const double input_product_scale = input->params.scale * filter->params.scale;
   e3a5e:	ee17 0a90 	vmov	r0, s15
TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              const TfLiteTensor* bias,
                                              TfLiteTensor* output,
                                              double* multiplier) {
   e3a62:	9b11      	ldr	r3, [sp, #68]	; 0x44
   e3a64:	9305      	str	r3, [sp, #20]
   e3a66:	460d      	mov	r5, r1
   e3a68:	4690      	mov	r8, r2
  const double input_product_scale = input->params.scale * filter->params.scale;
   e3a6a:	f003 fac7 	bl	e6ffc <__aeabi_f2d>
TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              const TfLiteTensor* bias,
                                              TfLiteTensor* output,
                                              double* multiplier) {
   e3a6e:	f8dd 9040 	ldr.w	r9, [sp, #64]	; 0x40
  const double input_product_scale = input->params.scale * filter->params.scale;
   e3a72:	4682      	mov	sl, r0
   e3a74:	468b      	mov	fp, r1
  // TODO(ahentz): The following conditions must be guaranteed by the training
  // pipeline.
  if (bias) {
   e3a76:	b316      	cbz	r6, e3abe <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0x76>
    const double bias_scale = bias->params.scale;
   e3a78:	68f0      	ldr	r0, [r6, #12]
   e3a7a:	f003 fabf 	bl	e6ffc <__aeabi_f2d>
   e3a7e:	e9cd 0102 	strd	r0, r1, [sp, #8]
_GLIBCXX_BEGIN_NAMESPACE_VERSION

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR double
  abs(double __x)
  { return __builtin_fabs(__x); }
   e3a82:	4602      	mov	r2, r0
   e3a84:	460b      	mov	r3, r1
   e3a86:	4650      	mov	r0, sl
   e3a88:	4659      	mov	r1, fp
   e3a8a:	f003 f957 	bl	e6d3c <__aeabi_dsub>
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
   e3a8e:	e9dd 2302 	ldrd	r2, r3, [sp, #8]
   e3a92:	4606      	mov	r6, r0
   e3a94:	f021 4700 	bic.w	r7, r1, #2147483648	; 0x80000000
   e3a98:	4650      	mov	r0, sl
   e3a9a:	4659      	mov	r1, fp
   e3a9c:	f003 fd92 	bl	e75c4 <__aeabi_dcmpgt>
   e3aa0:	b108      	cbz	r0, e3aa6 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0x5e>
	return __b;
   e3aa2:	e9dd ab02 	ldrd	sl, fp, [sp, #8]
    TF_LITE_ENSURE(context,
   e3aa6:	a315      	add	r3, pc, #84	; (adr r3, e3afc <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0xb4>)
   e3aa8:	e9d3 2300 	ldrd	r2, r3, [r3]
   e3aac:	4650      	mov	r0, sl
   e3aae:	4659      	mov	r1, fp
   e3ab0:	f003 faf8 	bl	e70a4 <__aeabi_dmul>
   e3ab4:	4632      	mov	r2, r6
   e3ab6:	463b      	mov	r3, r7
   e3ab8:	f003 fd7a 	bl	e75b0 <__aeabi_dcmpge>
   e3abc:	b150      	cbz	r0, e3ad4 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0x8c>
                   std::abs(input_product_scale - bias_scale) <=
                       1e-6 * std::min(input_product_scale, bias_scale));
  }
  return GetQuantizedConvolutionMultipler(context, input, filter, output,
                                          multiplier);
   e3abe:	9b05      	ldr	r3, [sp, #20]
   e3ac0:	9310      	str	r3, [sp, #64]	; 0x40
   e3ac2:	4642      	mov	r2, r8
   e3ac4:	464b      	mov	r3, r9
   e3ac6:	4629      	mov	r1, r5
   e3ac8:	4620      	mov	r0, r4
}
   e3aca:	b007      	add	sp, #28
   e3acc:	e8bd 4ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
    TF_LITE_ENSURE(context,
                   std::abs(input_product_scale - bias_scale) <=
                       1e-6 * std::min(input_product_scale, bias_scale));
  }
  return GetQuantizedConvolutionMultipler(context, input, filter, output,
                                          multiplier);
   e3ad0:	f7ff bf86 	b.w	e39e0 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd>
  const double input_product_scale = input->params.scale * filter->params.scale;
  // TODO(ahentz): The following conditions must be guaranteed by the training
  // pipeline.
  if (bias) {
    const double bias_scale = bias->params.scale;
    TF_LITE_ENSURE(context,
   e3ad4:	4b06      	ldr	r3, [pc, #24]	; (e3af0 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0xa8>)
   e3ad6:	9300      	str	r3, [sp, #0]
   e3ad8:	4620      	mov	r0, r4
   e3ada:	6965      	ldr	r5, [r4, #20]
   e3adc:	4a05      	ldr	r2, [pc, #20]	; (e3af4 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0xac>)
   e3ade:	4906      	ldr	r1, [pc, #24]	; (e3af8 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0xb0>)
   e3ae0:	236a      	movs	r3, #106	; 0x6a
   e3ae2:	47a8      	blx	r5
                   std::abs(input_product_scale - bias_scale) <=
                       1e-6 * std::min(input_product_scale, bias_scale));
  }
  return GetQuantizedConvolutionMultipler(context, input, filter, output,
                                          multiplier);
}
   e3ae4:	2001      	movs	r0, #1
   e3ae6:	b007      	add	sp, #28
   e3ae8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e3aec:	f3af 8000 	nop.w
   e3af0:	000eb1bf 	.word	0x000eb1bf
   e3af4:	000eb10c 	.word	0x000eb10c
   e3af8:	000e9ac8 	.word	0x000e9ac8
   e3afc:	a0b5ed8d 	.word	0xa0b5ed8d
   e3b00:	3eb0c6f7 	.word	0x3eb0c6f7

000e3b04 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_>:

TfLiteStatus CalculateActivationRangeQuantized(TfLiteContext* context,
                                               TfLiteFusedActivation activation,
                                               TfLiteTensor* output,
                                               int32_t* act_min,
                                               int32_t* act_max) {
   e3b04:	b573      	push	{r0, r1, r4, r5, r6, lr}
   e3b06:	460d      	mov	r5, r1
  int32_t qmin = 0;
  int32_t qmax = 0;
  if (output->type == kTfLiteUInt8) {
   e3b08:	7811      	ldrb	r1, [r2, #0]
   e3b0a:	2903      	cmp	r1, #3

TfLiteStatus CalculateActivationRangeQuantized(TfLiteContext* context,
                                               TfLiteFusedActivation activation,
                                               TfLiteTensor* output,
                                               int32_t* act_min,
                                               int32_t* act_max) {
   e3b0c:	4614      	mov	r4, r2
  int32_t qmin = 0;
  int32_t qmax = 0;
  if (output->type == kTfLiteUInt8) {
   e3b0e:	d00c      	beq.n	e3b2a <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x26>
    qmin = std::numeric_limits<uint8_t>::min();
    qmax = std::numeric_limits<uint8_t>::max();
  } else if (output->type == kTfLiteInt8) {
   e3b10:	2909      	cmp	r1, #9
   e3b12:	d00d      	beq.n	e3b30 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x2c>
    qmin = std::numeric_limits<int8_t>::min();
    qmax = std::numeric_limits<int8_t>::max();
  } else if (output->type == kTfLiteInt16) {
   e3b14:	2907      	cmp	r1, #7
   e3b16:	d00f      	beq.n	e3b38 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x34>
    qmin = std::numeric_limits<int16_t>::min();
    qmax = std::numeric_limits<int16_t>::max();
  } else {
    TF_LITE_ENSURE(context, false);
   e3b18:	4b0e      	ldr	r3, [pc, #56]	; (e3b54 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x50>)
   e3b1a:	9300      	str	r3, [sp, #0]
   e3b1c:	6944      	ldr	r4, [r0, #20]
   e3b1e:	4a0e      	ldr	r2, [pc, #56]	; (e3b58 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x54>)
   e3b20:	490e      	ldr	r1, [pc, #56]	; (e3b5c <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x58>)
   e3b22:	23a9      	movs	r3, #169	; 0xa9
   e3b24:	47a0      	blx	r4
   e3b26:	2001      	movs	r0, #1
   e3b28:	e011      	b.n	e3b4e <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x4a>
                                               int32_t* act_max) {
  int32_t qmin = 0;
  int32_t qmax = 0;
  if (output->type == kTfLiteUInt8) {
    qmin = std::numeric_limits<uint8_t>::min();
    qmax = std::numeric_limits<uint8_t>::max();
   e3b2a:	22ff      	movs	r2, #255	; 0xff
                                               int32_t* act_min,
                                               int32_t* act_max) {
  int32_t qmin = 0;
  int32_t qmax = 0;
  if (output->type == kTfLiteUInt8) {
    qmin = std::numeric_limits<uint8_t>::min();
   e3b2c:	2100      	movs	r1, #0
   e3b2e:	e006      	b.n	e3b3e <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x3a>
    qmax = std::numeric_limits<uint8_t>::max();
  } else if (output->type == kTfLiteInt8) {
    qmin = std::numeric_limits<int8_t>::min();
    qmax = std::numeric_limits<int8_t>::max();
   e3b30:	227f      	movs	r2, #127	; 0x7f
  int32_t qmax = 0;
  if (output->type == kTfLiteUInt8) {
    qmin = std::numeric_limits<uint8_t>::min();
    qmax = std::numeric_limits<uint8_t>::max();
  } else if (output->type == kTfLiteInt8) {
    qmin = std::numeric_limits<int8_t>::min();
   e3b32:	f06f 017f 	mvn.w	r1, #127	; 0x7f
   e3b36:	e002      	b.n	e3b3e <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x3a>
    qmax = std::numeric_limits<int8_t>::max();
  } else if (output->type == kTfLiteInt16) {
    qmin = std::numeric_limits<int16_t>::min();
   e3b38:	4909      	ldr	r1, [pc, #36]	; (e3b60 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x5c>)
    qmax = std::numeric_limits<int16_t>::max();
   e3b3a:	f647 72ff 	movw	r2, #32767	; 0x7fff
  } else {
    TF_LITE_ENSURE(context, false);
  }

  CalculateActivationRangeQuantizedImpl(activation, qmin, qmax, output, act_min,
                                        act_max);
   e3b3e:	9806      	ldr	r0, [sp, #24]
   e3b40:	9001      	str	r0, [sp, #4]
   e3b42:	9300      	str	r3, [sp, #0]
   e3b44:	4628      	mov	r0, r5
   e3b46:	4623      	mov	r3, r4
   e3b48:	f7ff ff0c 	bl	e3964 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_>
  return kTfLiteOk;
   e3b4c:	2000      	movs	r0, #0
}
   e3b4e:	b002      	add	sp, #8
   e3b50:	bd70      	pop	{r4, r5, r6, pc}
   e3b52:	bf00      	nop
   e3b54:	000eb21e 	.word	0x000eb21e
   e3b58:	000eb10c 	.word	0x000eb10c
   e3b5c:	000e9ac8 	.word	0x000e9ac8
   e3b60:	ffff8000 	.word	0xffff8000

000e3b64 <_ZN6tflite29CalculateActivationRangeUint8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>:

void CalculateActivationRangeUint8(TfLiteFusedActivation activation,
                                   TfLiteTensor* output, int32_t* act_min,
                                   int32_t* act_max) {
   e3b64:	b507      	push	{r0, r1, r2, lr}
  const int32_t qmin = std::numeric_limits<uint8_t>::min();
  const int32_t qmax = std::numeric_limits<uint8_t>::max();

  CalculateActivationRangeQuantizedImpl(activation, qmin, qmax, output, act_min,
                                        act_max);
   e3b66:	e88d 000c 	stmia.w	sp, {r2, r3}
   e3b6a:	460b      	mov	r3, r1
   e3b6c:	22ff      	movs	r2, #255	; 0xff
   e3b6e:	2100      	movs	r1, #0
   e3b70:	f7ff fef8 	bl	e3964 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_>
}
   e3b74:	b003      	add	sp, #12
   e3b76:	f85d fb04 	ldr.w	pc, [sp], #4
	...

000e3b7c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_>:
TfLiteStatus PopulateConvolutionQuantizationParams(
    TfLiteContext* context, const TfLiteTensor* input,
    const TfLiteTensor* filter, const TfLiteTensor* bias, TfLiteTensor* output,
    const TfLiteFusedActivation& activation, int32_t* multiplier, int* shift,
    int32_t* output_activation_min, int32_t* output_activation_max,
    int32_t* per_channel_multiplier, int* per_channel_shift) {
   e3b7c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  TF_LITE_ENSURE_EQ(context, input->quantization.type,
   e3b80:	f891 8030 	ldrb.w	r8, [r1, #48]	; 0x30
TfLiteStatus PopulateConvolutionQuantizationParams(
    TfLiteContext* context, const TfLiteTensor* input,
    const TfLiteTensor* filter, const TfLiteTensor* bias, TfLiteTensor* output,
    const TfLiteFusedActivation& activation, int32_t* multiplier, int* shift,
    int32_t* output_activation_min, int32_t* output_activation_max,
    int32_t* per_channel_multiplier, int* per_channel_shift) {
   e3b84:	b08d      	sub	sp, #52	; 0x34
  TF_LITE_ENSURE_EQ(context, input->quantization.type,
   e3b86:	f1b8 0f01 	cmp.w	r8, #1
TfLiteStatus PopulateConvolutionQuantizationParams(
    TfLiteContext* context, const TfLiteTensor* input,
    const TfLiteTensor* filter, const TfLiteTensor* bias, TfLiteTensor* output,
    const TfLiteFusedActivation& activation, int32_t* multiplier, int* shift,
    int32_t* output_activation_min, int32_t* output_activation_max,
    int32_t* per_channel_multiplier, int* per_channel_shift) {
   e3b8a:	4604      	mov	r4, r0
   e3b8c:	460e      	mov	r6, r1
   e3b8e:	4617      	mov	r7, r2
   e3b90:	9307      	str	r3, [sp, #28]
  TF_LITE_ENSURE_EQ(context, input->quantization.type,
   e3b92:	d00a      	beq.n	e3baa <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x2e>
   e3b94:	4b63      	ldr	r3, [pc, #396]	; (e3d24 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1a8>)
   e3b96:	9301      	str	r3, [sp, #4]
   e3b98:	2501      	movs	r5, #1
   e3b9a:	4b63      	ldr	r3, [pc, #396]	; (e3d28 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1ac>)
   e3b9c:	9300      	str	r3, [sp, #0]
   e3b9e:	9503      	str	r5, [sp, #12]
   e3ba0:	f8cd 8008 	str.w	r8, [sp, #8]
   e3ba4:	6944      	ldr	r4, [r0, #20]
   e3ba6:	2321      	movs	r3, #33	; 0x21
   e3ba8:	e033      	b.n	e3c12 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x96>
                    kTfLiteAffineQuantization);
  TF_LITE_ENSURE_EQ(context, filter->quantization.type,
   e3baa:	f892 5030 	ldrb.w	r5, [r2, #48]	; 0x30
   e3bae:	2d01      	cmp	r5, #1
   e3bb0:	d00d      	beq.n	e3bce <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x52>
   e3bb2:	4b5c      	ldr	r3, [pc, #368]	; (e3d24 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1a8>)
   e3bb4:	9301      	str	r3, [sp, #4]
   e3bb6:	4b5d      	ldr	r3, [pc, #372]	; (e3d2c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1b0>)
   e3bb8:	9502      	str	r5, [sp, #8]
   e3bba:	9300      	str	r3, [sp, #0]
   e3bbc:	f8cd 800c 	str.w	r8, [sp, #12]
   e3bc0:	6944      	ldr	r4, [r0, #20]
   e3bc2:	4a5b      	ldr	r2, [pc, #364]	; (e3d30 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1b4>)
   e3bc4:	495b      	ldr	r1, [pc, #364]	; (e3d34 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1b8>)
   e3bc6:	2323      	movs	r3, #35	; 0x23
   e3bc8:	47a0      	blx	r4
   e3bca:	4645      	mov	r5, r8
   e3bcc:	e0a6      	b.n	e3d1c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1a0>
  // TF_LITE_ENSURE_EQ(context, bias->quantization.type,
  // kTfLiteAffineQuantization);

  // Check data type.
  const auto* affine_quantization =
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);
   e3bce:	6b51      	ldr	r1, [r2, #52]	; 0x34
  TF_LITE_ENSURE(context, affine_quantization);
   e3bd0:	b921      	cbnz	r1, e3bdc <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x60>
   e3bd2:	4b59      	ldr	r3, [pc, #356]	; (e3d38 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1bc>)
   e3bd4:	9300      	str	r3, [sp, #0]
   e3bd6:	6944      	ldr	r4, [r0, #20]
   e3bd8:	232d      	movs	r3, #45	; 0x2d
   e3bda:	e005      	b.n	e3be8 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x6c>
  TF_LITE_ENSURE(context, affine_quantization->scale);
   e3bdc:	680b      	ldr	r3, [r1, #0]
   e3bde:	b93b      	cbnz	r3, e3bf0 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x74>
   e3be0:	4b56      	ldr	r3, [pc, #344]	; (e3d3c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1c0>)
   e3be2:	9300      	str	r3, [sp, #0]
   e3be4:	6944      	ldr	r4, [r0, #20]
   e3be6:	232e      	movs	r3, #46	; 0x2e
   e3be8:	4a51      	ldr	r2, [pc, #324]	; (e3d30 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1b4>)
   e3bea:	4955      	ldr	r1, [pc, #340]	; (e3d40 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1c4>)
   e3bec:	47a0      	blx	r4
   e3bee:	e095      	b.n	e3d1c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1a0>
  const bool is_per_channel = affine_quantization->scale->size > 1;
   e3bf0:	f8d3 b000 	ldr.w	fp, [r3]
  if (is_per_channel) {
   e3bf4:	f1bb 0f01 	cmp.w	fp, #1
   e3bf8:	dd2f      	ble.n	e3c5a <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0xde>
    //  Currently only Int8 is supported for per channel quantization.
    TF_LITE_ENSURE_EQ(context, input->type, kTfLiteInt8);
   e3bfa:	7832      	ldrb	r2, [r6, #0]
   e3bfc:	2a09      	cmp	r2, #9
   e3bfe:	d00c      	beq.n	e3c1a <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x9e>
   e3c00:	2309      	movs	r3, #9
   e3c02:	9303      	str	r3, [sp, #12]
   e3c04:	4b4f      	ldr	r3, [pc, #316]	; (e3d44 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1c8>)
   e3c06:	9301      	str	r3, [sp, #4]
   e3c08:	4b4f      	ldr	r3, [pc, #316]	; (e3d48 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1cc>)
   e3c0a:	9300      	str	r3, [sp, #0]
   e3c0c:	9202      	str	r2, [sp, #8]
   e3c0e:	6944      	ldr	r4, [r0, #20]
   e3c10:	2332      	movs	r3, #50	; 0x32
   e3c12:	4a47      	ldr	r2, [pc, #284]	; (e3d30 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1b4>)
   e3c14:	4947      	ldr	r1, [pc, #284]	; (e3d34 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1b8>)
   e3c16:	47a0      	blx	r4
   e3c18:	e080      	b.n	e3d1c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1a0>
    TF_LITE_ENSURE_EQ(context, filter->type, kTfLiteInt8);
   e3c1a:	f897 e000 	ldrb.w	lr, [r7]
   e3c1e:	f1be 0f09 	cmp.w	lr, #9
   e3c22:	d009      	beq.n	e3c38 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0xbc>
   e3c24:	4b47      	ldr	r3, [pc, #284]	; (e3d44 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1c8>)
   e3c26:	9301      	str	r3, [sp, #4]
   e3c28:	4b48      	ldr	r3, [pc, #288]	; (e3d4c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1d0>)
   e3c2a:	9300      	str	r3, [sp, #0]
   e3c2c:	9203      	str	r2, [sp, #12]
   e3c2e:	f8cd e008 	str.w	lr, [sp, #8]
   e3c32:	6944      	ldr	r4, [r0, #20]
   e3c34:	2333      	movs	r3, #51	; 0x33
   e3c36:	e7ec      	b.n	e3c12 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x96>
    TF_LITE_ENSURE_EQ(
   e3c38:	68ba      	ldr	r2, [r7, #8]
   e3c3a:	6889      	ldr	r1, [r1, #8]
   e3c3c:	eb02 0281 	add.w	r2, r2, r1, lsl #2
   e3c40:	6852      	ldr	r2, [r2, #4]
   e3c42:	4593      	cmp	fp, r2
   e3c44:	d009      	beq.n	e3c5a <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0xde>
   e3c46:	4b42      	ldr	r3, [pc, #264]	; (e3d50 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1d4>)
   e3c48:	9301      	str	r3, [sp, #4]
   e3c4a:	4b42      	ldr	r3, [pc, #264]	; (e3d54 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1d8>)
   e3c4c:	9300      	str	r3, [sp, #0]
   e3c4e:	9203      	str	r2, [sp, #12]
   e3c50:	f8cd b008 	str.w	fp, [sp, #8]
   e3c54:	6944      	ldr	r4, [r0, #20]
   e3c56:	2336      	movs	r3, #54	; 0x36
   e3c58:	e7db      	b.n	e3c12 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x96>
        filter->dims->data[affine_quantization->quantized_dimension]);
  }

  // Populate multiplier and shift using affine quantization.
  const int num_channels = affine_quantization->scale->size;
  const float input_scale = input->params.scale;
   e3c5a:	edd6 7a03 	vldr	s15, [r6, #12]
  const float output_scale = output->params.scale;
   e3c5e:	9a16      	ldr	r2, [sp, #88]	; 0x58
        filter->dims->data[affine_quantization->quantized_dimension]);
  }

  // Populate multiplier and shift using affine quantization.
  const int num_channels = affine_quantization->scale->size;
  const float input_scale = input->params.scale;
   e3c60:	edcd 7a05 	vstr	s15, [sp, #20]
  const float output_scale = output->params.scale;
   e3c64:	edd2 7a03 	vldr	s15, [r2, #12]
  const float* filter_scales = affine_quantization->scale->data;
   e3c68:	1d1d      	adds	r5, r3, #4
  }

  // Populate multiplier and shift using affine quantization.
  const int num_channels = affine_quantization->scale->size;
  const float input_scale = input->params.scale;
  const float output_scale = output->params.scale;
   e3c6a:	edcd 7a06 	vstr	s15, [sp, #24]
  const float* filter_scales = affine_quantization->scale->data;
  for (int i = 0; i < num_channels; ++i) {
   e3c6e:	f04f 0a00 	mov.w	sl, #0
   e3c72:	45da      	cmp	sl, fp
   e3c74:	da2a      	bge.n	e3ccc <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x150>
    const double effective_output_scale = static_cast<double>(input_scale) *
                                          filter_scale /
                                          static_cast<double>(output_scale);
    int32_t significand;
    int shift;
    QuantizeMultiplier(effective_output_scale, &significand, &shift);
   e3c76:	f855 0b04 	ldr.w	r0, [r5], #4
   e3c7a:	f003 f9bf 	bl	e6ffc <__aeabi_f2d>
   e3c7e:	4680      	mov	r8, r0
   e3c80:	9805      	ldr	r0, [sp, #20]
   e3c82:	4689      	mov	r9, r1
   e3c84:	f003 f9ba 	bl	e6ffc <__aeabi_f2d>
   e3c88:	4602      	mov	r2, r0
   e3c8a:	460b      	mov	r3, r1
   e3c8c:	4640      	mov	r0, r8
   e3c8e:	4649      	mov	r1, r9
   e3c90:	f003 fa08 	bl	e70a4 <__aeabi_dmul>
   e3c94:	4680      	mov	r8, r0
   e3c96:	9806      	ldr	r0, [sp, #24]
   e3c98:	4689      	mov	r9, r1
   e3c9a:	f003 f9af 	bl	e6ffc <__aeabi_f2d>
   e3c9e:	4602      	mov	r2, r0
   e3ca0:	460b      	mov	r3, r1
   e3ca2:	4640      	mov	r0, r8
   e3ca4:	4649      	mov	r1, r9
   e3ca6:	f003 fb27 	bl	e72f8 <__aeabi_ddiv>
   e3caa:	ec41 0b10 	vmov	d0, r0, r1
   e3cae:	a90a      	add	r1, sp, #40	; 0x28
   e3cb0:	a809      	add	r0, sp, #36	; 0x24
   e3cb2:	f000 f867 	bl	e3d84 <_ZN6tflite18QuantizeMultiplierEdPlPi>
    per_channel_multiplier[i] = significand;
   e3cb6:	9a1c      	ldr	r2, [sp, #112]	; 0x70
   e3cb8:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e3cba:	f842 302a 	str.w	r3, [r2, sl, lsl #2]
    per_channel_shift[i] = shift;
   e3cbe:	9a1d      	ldr	r2, [sp, #116]	; 0x74
   e3cc0:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e3cc2:	f842 302a 	str.w	r3, [r2, sl, lsl #2]
  // Populate multiplier and shift using affine quantization.
  const int num_channels = affine_quantization->scale->size;
  const float input_scale = input->params.scale;
  const float output_scale = output->params.scale;
  const float* filter_scales = affine_quantization->scale->data;
  for (int i = 0; i < num_channels; ++i) {
   e3cc6:	f10a 0a01 	add.w	sl, sl, #1
   e3cca:	e7d2      	b.n	e3c72 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0xf6>
  }

  // Populate scalar quantization parameters.
  // This check on legacy quantization parameters is kept only for backward
  // compatibility.
  if (input->type == kTfLiteUInt8) {
   e3ccc:	7833      	ldrb	r3, [r6, #0]
   e3cce:	2b03      	cmp	r3, #3
   e3cd0:	d123      	bne.n	e3d1a <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x19e>
    // Check bias scale == input scale * filter scale.
    double real_multiplier = 0.0;
   e3cd2:	ab0c      	add	r3, sp, #48	; 0x30
   e3cd4:	2000      	movs	r0, #0
   e3cd6:	2100      	movs	r1, #0
   e3cd8:	e963 0102 	strd	r0, r1, [r3, #-8]!
    TF_LITE_ENSURE_STATUS(GetQuantizedConvolutionMultipler(
   e3cdc:	9301      	str	r3, [sp, #4]
   e3cde:	9b16      	ldr	r3, [sp, #88]	; 0x58
   e3ce0:	9300      	str	r3, [sp, #0]
   e3ce2:	463a      	mov	r2, r7
   e3ce4:	9b07      	ldr	r3, [sp, #28]
   e3ce6:	4631      	mov	r1, r6
   e3ce8:	4620      	mov	r0, r4
   e3cea:	f7ff fead 	bl	e3a48 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd>
   e3cee:	4605      	mov	r5, r0
   e3cf0:	b108      	cbz	r0, e3cf6 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x17a>
   e3cf2:	2501      	movs	r5, #1
   e3cf4:	e012      	b.n	e3d1c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1a0>
        context, input, filter, bias, output, &real_multiplier));
    int exponent;

    // Populate quantization parameteters with multiplier and shift.
    QuantizeMultiplier(real_multiplier, multiplier, &exponent);
   e3cf6:	a909      	add	r1, sp, #36	; 0x24
   e3cf8:	9818      	ldr	r0, [sp, #96]	; 0x60
   e3cfa:	ed9d 0b0a 	vldr	d0, [sp, #40]	; 0x28
   e3cfe:	f000 f841 	bl	e3d84 <_ZN6tflite18QuantizeMultiplierEdPlPi>
    *shift = -exponent;
   e3d02:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e3d04:	9a19      	ldr	r2, [sp, #100]	; 0x64
    CalculateActivationRangeUint8(activation, output, output_activation_min,
                                  output_activation_max);
   e3d06:	9817      	ldr	r0, [sp, #92]	; 0x5c
   e3d08:	9916      	ldr	r1, [sp, #88]	; 0x58
        context, input, filter, bias, output, &real_multiplier));
    int exponent;

    // Populate quantization parameteters with multiplier and shift.
    QuantizeMultiplier(real_multiplier, multiplier, &exponent);
    *shift = -exponent;
   e3d0a:	425b      	negs	r3, r3
   e3d0c:	6013      	str	r3, [r2, #0]
    CalculateActivationRangeUint8(activation, output, output_activation_min,
                                  output_activation_max);
   e3d0e:	7800      	ldrb	r0, [r0, #0]
   e3d10:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
   e3d12:	9a1a      	ldr	r2, [sp, #104]	; 0x68
   e3d14:	f7ff ff26 	bl	e3b64 <_ZN6tflite29CalculateActivationRangeUint8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>
   e3d18:	e000      	b.n	e3d1c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1a0>
  }
  return kTfLiteOk;
   e3d1a:	2500      	movs	r5, #0
}
   e3d1c:	4628      	mov	r0, r5
   e3d1e:	b00d      	add	sp, #52	; 0x34
   e3d20:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e3d24:	000e9b17 	.word	0x000e9b17
   e3d28:	000eb224 	.word	0x000eb224
   e3d2c:	000e9b31 	.word	0x000e9b31
   e3d30:	000eb10c 	.word	0x000eb10c
   e3d34:	000e98f8 	.word	0x000e98f8
   e3d38:	000e9b4b 	.word	0x000e9b4b
   e3d3c:	000e9b5f 	.word	0x000e9b5f
   e3d40:	000e9ac8 	.word	0x000e9ac8
   e3d44:	000ead0f 	.word	0x000ead0f
   e3d48:	000e9933 	.word	0x000e9933
   e3d4c:	000eb23d 	.word	0x000eb23d
   e3d50:	000eb24a 	.word	0x000eb24a
   e3d54:	000eb287 	.word	0x000eb287

000e3d58 <_ZN6tflite28CalculateActivationRangeInt8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>:
                                        act_max);
}

void CalculateActivationRangeInt8(TfLiteFusedActivation activation,
                                  TfLiteTensor* output, int32_t* act_min,
                                  int32_t* act_max) {
   e3d58:	b507      	push	{r0, r1, r2, lr}
  const int32_t qmin = std::numeric_limits<int8_t>::min();
  const int32_t qmax = std::numeric_limits<int8_t>::max();

  CalculateActivationRangeQuantizedImpl(activation, qmin, qmax, output, act_min,
                                        act_max);
   e3d5a:	e88d 000c 	stmia.w	sp, {r2, r3}
   e3d5e:	460b      	mov	r3, r1
   e3d60:	227f      	movs	r2, #127	; 0x7f
   e3d62:	f06f 017f 	mvn.w	r1, #127	; 0x7f
   e3d66:	f7ff fdfd 	bl	e3964 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_>
}
   e3d6a:	b003      	add	sp, #12
   e3d6c:	f85d fb04 	ldr.w	pc, [sp], #4

000e3d70 <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>:

bool HaveSameShapes(const TfLiteTensor* input1, const TfLiteTensor* input2) {
   e3d70:	b508      	push	{r3, lr}
  return TfLiteIntArrayEqual(input1->dims, input2->dims);
   e3d72:	6889      	ldr	r1, [r1, #8]
   e3d74:	6880      	ldr	r0, [r0, #8]
   e3d76:	f7f0 f9c1 	bl	d40fc <TfLiteIntArrayEqual>
}
   e3d7a:	3000      	adds	r0, #0
   e3d7c:	bf18      	it	ne
   e3d7e:	2001      	movne	r0, #1
   e3d80:	bd08      	pop	{r3, pc}
	...

000e3d84 <_ZN6tflite18QuantizeMultiplierEdPlPi>:
constexpr uint32_t kFractionRoundingMask = 0x003fffff;
constexpr uint32_t kFractionRoundingThreshold = 0x00200000;
}  // namespace

void QuantizeMultiplier(double double_multiplier, int32_t* quantized_multiplier,
                        int* shift) {
   e3d84:	b538      	push	{r3, r4, r5, lr}
  if (double_multiplier == 0.) {
   e3d86:	2200      	movs	r2, #0
constexpr uint32_t kFractionRoundingMask = 0x003fffff;
constexpr uint32_t kFractionRoundingThreshold = 0x00200000;
}  // namespace

void QuantizeMultiplier(double double_multiplier, int32_t* quantized_multiplier,
                        int* shift) {
   e3d88:	ed2d 8b02 	vpush	{d8}
   e3d8c:	eeb0 8a40 	vmov.f32	s16, s0
   e3d90:	eef0 8a60 	vmov.f32	s17, s1
   e3d94:	4605      	mov	r5, r0
   e3d96:	460c      	mov	r4, r1
  if (double_multiplier == 0.) {
   e3d98:	2300      	movs	r3, #0
   e3d9a:	ec51 0b10 	vmov	r0, r1, d0
   e3d9e:	f003 fbe9 	bl	e7574 <__aeabi_dcmpeq>
   e3da2:	b118      	cbz	r0, e3dac <_ZN6tflite18QuantizeMultiplierEdPlPi+0x28>
    *quantized_multiplier = 0;
   e3da4:	2300      	movs	r3, #0
   e3da6:	602b      	str	r3, [r5, #0]
    *shift = 0;
   e3da8:	6023      	str	r3, [r4, #0]
   e3daa:	e02d      	b.n	e3e08 <_ZN6tflite18QuantizeMultiplierEdPlPi+0x84>
  // example on microcontrollers) then use an alternative implementation
  // that only requires integer and bitwise operations. To enable this, you
  // need to set the define during the build process for your platform.
  int64_t q_fixed = IntegerFrExp(double_multiplier, shift);
#else   // TFLITE_EMULATE_FLOAT
  const double q = std::frexp(double_multiplier, shift);
   e3dac:	4620      	mov	r0, r4
   e3dae:	eeb0 0a48 	vmov.f32	s0, s16
   e3db2:	eef0 0a68 	vmov.f32	s1, s17
   e3db6:	f001 fa17 	bl	e51e8 <frexp>
   e3dba:	2200      	movs	r2, #0
   e3dbc:	4b14      	ldr	r3, [pc, #80]	; (e3e10 <_ZN6tflite18QuantizeMultiplierEdPlPi+0x8c>)
   e3dbe:	ec51 0b10 	vmov	r0, r1, d0
   e3dc2:	f003 f96f 	bl	e70a4 <__aeabi_dmul>
   e3dc6:	ec41 0b10 	vmov	d0, r0, r1
   e3dca:	f001 fa43 	bl	e5254 <round>
  auto q_fixed = static_cast<int64_t>(TfLiteRound(q * (1ll << 31)));
   e3dce:	ec51 0b10 	vmov	r0, r1, d0
   e3dd2:	f003 fc99 	bl	e7708 <__aeabi_d2lz>
#endif  // TFLITE_EMULATE_FLOAT
  TFLITE_CHECK(q_fixed <= (1ll << 31));
   e3dd6:	f04f 4200 	mov.w	r2, #2147483648	; 0x80000000
   e3dda:	2300      	movs	r3, #0
   e3ddc:	4282      	cmp	r2, r0
   e3dde:	418b      	sbcs	r3, r1
   e3de0:	da01      	bge.n	e3de6 <_ZN6tflite18QuantizeMultiplierEdPlPi+0x62>
   e3de2:	f000 faa3 	bl	e432c <abort>
  if (q_fixed == (1ll << 31)) {
   e3de6:	2900      	cmp	r1, #0
   e3de8:	bf01      	itttt	eq
   e3dea:	f1b0 4f00 	cmpeq.w	r0, #2147483648	; 0x80000000
    q_fixed /= 2;
    ++*shift;
   e3dee:	6823      	ldreq	r3, [r4, #0]
   e3df0:	3301      	addeq	r3, #1
   e3df2:	6023      	streq	r3, [r4, #0]
  // that we're effectively flushing tiny double_multiplier's to zero.
  // We could conceivably handle values in the range (roughly) [32, 63]
  // as 'denormals' i.e. (shift==0, q_fixed < 2^30). In that point of view
  // the present handling is just doing 'flush denormals to zero'. We could
  // reconsider and actually generate nonzero denormals if a need arises.
  if (*shift < -31) {
   e3df4:	6823      	ldr	r3, [r4, #0]
  const double q = std::frexp(double_multiplier, shift);
  auto q_fixed = static_cast<int64_t>(TfLiteRound(q * (1ll << 31)));
#endif  // TFLITE_EMULATE_FLOAT
  TFLITE_CHECK(q_fixed <= (1ll << 31));
  if (q_fixed == (1ll << 31)) {
    q_fixed /= 2;
   e3df6:	bf08      	it	eq
   e3df8:	f04f 4080 	moveq.w	r0, #1073741824	; 0x40000000
  // that we're effectively flushing tiny double_multiplier's to zero.
  // We could conceivably handle values in the range (roughly) [32, 63]
  // as 'denormals' i.e. (shift==0, q_fixed < 2^30). In that point of view
  // the present handling is just doing 'flush denormals to zero'. We could
  // reconsider and actually generate nonzero denormals if a need arises.
  if (*shift < -31) {
   e3dfc:	331f      	adds	r3, #31
    *shift = 0;
   e3dfe:	bfbe      	ittt	lt
   e3e00:	2300      	movlt	r3, #0
    q_fixed = 0;
   e3e02:	2000      	movlt	r0, #0
  // We could conceivably handle values in the range (roughly) [32, 63]
  // as 'denormals' i.e. (shift==0, q_fixed < 2^30). In that point of view
  // the present handling is just doing 'flush denormals to zero'. We could
  // reconsider and actually generate nonzero denormals if a need arises.
  if (*shift < -31) {
    *shift = 0;
   e3e04:	6023      	strlt	r3, [r4, #0]
    q_fixed = 0;
  }
  *quantized_multiplier = static_cast<int32_t>(q_fixed);
   e3e06:	6028      	str	r0, [r5, #0]
}
   e3e08:	ecbd 8b02 	vpop	{d8}
   e3e0c:	bd38      	pop	{r3, r4, r5, pc}
   e3e0e:	bf00      	nop
   e3e10:	41e00000 	.word	0x41e00000

000e3e14 <_ZN6tflite32QuantizeMultiplierGreaterThanOneEdPlPi>:

void QuantizeMultiplierGreaterThanOne(double double_multiplier,
                                      int32_t* quantized_multiplier,
                                      int* left_shift) {
   e3e14:	b538      	push	{r3, r4, r5, lr}
  TFLITE_CHECK_GT(double_multiplier, 1.);
   e3e16:	2200      	movs	r2, #0
  *quantized_multiplier = static_cast<int32_t>(q_fixed);
}

void QuantizeMultiplierGreaterThanOne(double double_multiplier,
                                      int32_t* quantized_multiplier,
                                      int* left_shift) {
   e3e18:	ed2d 8b02 	vpush	{d8}
   e3e1c:	eeb0 8a40 	vmov.f32	s16, s0
   e3e20:	eef0 8a60 	vmov.f32	s17, s1
   e3e24:	4605      	mov	r5, r0
   e3e26:	460c      	mov	r4, r1
  TFLITE_CHECK_GT(double_multiplier, 1.);
   e3e28:	4b0a      	ldr	r3, [pc, #40]	; (e3e54 <_ZN6tflite32QuantizeMultiplierGreaterThanOneEdPlPi+0x40>)
   e3e2a:	ec51 0b10 	vmov	r0, r1, d0
   e3e2e:	f003 fbc9 	bl	e75c4 <__aeabi_dcmpgt>
   e3e32:	b908      	cbnz	r0, e3e38 <_ZN6tflite32QuantizeMultiplierGreaterThanOneEdPlPi+0x24>
   e3e34:	f000 fa7a 	bl	e432c <abort>
  QuantizeMultiplier(double_multiplier, quantized_multiplier, left_shift);
   e3e38:	4621      	mov	r1, r4
   e3e3a:	4628      	mov	r0, r5
   e3e3c:	eeb0 0a48 	vmov.f32	s0, s16
   e3e40:	eef0 0a68 	vmov.f32	s1, s17
   e3e44:	f7ff ff9e 	bl	e3d84 <_ZN6tflite18QuantizeMultiplierEdPlPi>
  TFLITE_CHECK_GE(*left_shift, 0);
   e3e48:	6823      	ldr	r3, [r4, #0]
   e3e4a:	2b00      	cmp	r3, #0
   e3e4c:	dbf2      	blt.n	e3e34 <_ZN6tflite32QuantizeMultiplierGreaterThanOneEdPlPi+0x20>
}
   e3e4e:	ecbd 8b02 	vpop	{d8}
   e3e52:	bd38      	pop	{r3, r4, r5, pc}
   e3e54:	3ff00000 	.word	0x3ff00000

000e3e58 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>:

void QuantizeMultiplierSmallerThanOneExp(double double_multiplier,
                                         int32_t* quantized_multiplier,
                                         int* left_shift) {
   e3e58:	b530      	push	{r4, r5, lr}
  TFLITE_CHECK_LT(double_multiplier, 1.);
   e3e5a:	2200      	movs	r2, #0
  TFLITE_CHECK_GE(*left_shift, 0);
}

void QuantizeMultiplierSmallerThanOneExp(double double_multiplier,
                                         int32_t* quantized_multiplier,
                                         int* left_shift) {
   e3e5c:	b085      	sub	sp, #20
   e3e5e:	4605      	mov	r5, r0
   e3e60:	460c      	mov	r4, r1
  TFLITE_CHECK_LT(double_multiplier, 1.);
   e3e62:	4b11      	ldr	r3, [pc, #68]	; (e3ea8 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi+0x50>)
   e3e64:	ec51 0b10 	vmov	r0, r1, d0
   e3e68:	ed8d 0b00 	vstr	d0, [sp]
   e3e6c:	f003 fb8c 	bl	e7588 <__aeabi_dcmplt>
   e3e70:	ed9d 0b00 	vldr	d0, [sp]
   e3e74:	b908      	cbnz	r0, e3e7a <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi+0x22>
   e3e76:	f000 fa59 	bl	e432c <abort>
  TFLITE_CHECK_GT(double_multiplier, 0.);
   e3e7a:	2200      	movs	r2, #0
   e3e7c:	2300      	movs	r3, #0
   e3e7e:	ec51 0b10 	vmov	r0, r1, d0
   e3e82:	ed8d 0b00 	vstr	d0, [sp]
   e3e86:	f003 fb9d 	bl	e75c4 <__aeabi_dcmpgt>
   e3e8a:	2800      	cmp	r0, #0
   e3e8c:	d0f3      	beq.n	e3e76 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi+0x1e>
  int shift;
  QuantizeMultiplier(double_multiplier, quantized_multiplier, &shift);
   e3e8e:	a903      	add	r1, sp, #12
   e3e90:	4628      	mov	r0, r5
   e3e92:	ed9d 0b00 	vldr	d0, [sp]
   e3e96:	f7ff ff75 	bl	e3d84 <_ZN6tflite18QuantizeMultiplierEdPlPi>
  TFLITE_CHECK_LE(shift, 0);
   e3e9a:	9b03      	ldr	r3, [sp, #12]
   e3e9c:	2b00      	cmp	r3, #0
   e3e9e:	dcea      	bgt.n	e3e76 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi+0x1e>
  *left_shift = shift;
   e3ea0:	6023      	str	r3, [r4, #0]
}
   e3ea2:	b005      	add	sp, #20
   e3ea4:	bd30      	pop	{r4, r5, pc}
   e3ea6:	bf00      	nop
   e3ea8:	3ff00000 	.word	0x3ff00000
   e3eac:	00000000 	.word	0x00000000

000e3eb0 <_ZN6tflite24PreprocessSoftmaxScalingEddiPlPi>:
  }
}

void PreprocessSoftmaxScaling(double beta, double input_scale,
                              int input_integer_bits,
                              int32_t* quantized_multiplier, int* left_shift) {
   e3eb0:	b5f0      	push	{r4, r5, r6, r7, lr}
  if (IntegerDoubleCompare(input_beta_real_multiplier, (1ll << 31) - 1.0) > 0) {
    input_beta_real_multiplier = (1ll << 31) - 1.0;
  }
#else   // TFLITE_EMULATE_FLOAT
  const double input_beta_real_multiplier = std::min(
      beta * input_scale * (1 << (31 - input_integer_bits)), (1ll << 31) - 1.0);
   e3eb2:	2301      	movs	r3, #1
   e3eb4:	f1c0 001f 	rsb	r0, r0, #31
  }
}

void PreprocessSoftmaxScaling(double beta, double input_scale,
                              int input_integer_bits,
                              int32_t* quantized_multiplier, int* left_shift) {
   e3eb8:	b085      	sub	sp, #20
  if (IntegerDoubleCompare(input_beta_real_multiplier, (1ll << 31) - 1.0) > 0) {
    input_beta_real_multiplier = (1ll << 31) - 1.0;
  }
#else   // TFLITE_EMULATE_FLOAT
  const double input_beta_real_multiplier = std::min(
      beta * input_scale * (1 << (31 - input_integer_bits)), (1ll << 31) - 1.0);
   e3eba:	fa03 f000 	lsl.w	r0, r3, r0
  }
}

void PreprocessSoftmaxScaling(double beta, double input_scale,
                              int input_integer_bits,
                              int32_t* quantized_multiplier, int* left_shift) {
   e3ebe:	ed8d 0b02 	vstr	d0, [sp, #8]
   e3ec2:	ed8d 1b00 	vstr	d1, [sp]
   e3ec6:	4615      	mov	r5, r2
   e3ec8:	460c      	mov	r4, r1
  if (IntegerDoubleCompare(input_beta_real_multiplier, (1ll << 31) - 1.0) > 0) {
    input_beta_real_multiplier = (1ll << 31) - 1.0;
  }
#else   // TFLITE_EMULATE_FLOAT
  const double input_beta_real_multiplier = std::min(
      beta * input_scale * (1 << (31 - input_integer_bits)), (1ll << 31) - 1.0);
   e3eca:	f003 f885 	bl	e6fd8 <__aeabi_i2d>
   e3ece:	ed9d 1b00 	vldr	d1, [sp]
   e3ed2:	ed9d 0b02 	vldr	d0, [sp, #8]
   e3ed6:	ec53 2b11 	vmov	r2, r3, d1
   e3eda:	4606      	mov	r6, r0
   e3edc:	460f      	mov	r7, r1
   e3ede:	ec51 0b10 	vmov	r0, r1, d0
   e3ee2:	f003 f8df 	bl	e70a4 <__aeabi_dmul>
   e3ee6:	4602      	mov	r2, r0
   e3ee8:	460b      	mov	r3, r1
   e3eea:	4630      	mov	r0, r6
   e3eec:	4639      	mov	r1, r7
   e3eee:	f003 f8d9 	bl	e70a4 <__aeabi_dmul>
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
   e3ef2:	a309      	add	r3, pc, #36	; (adr r3, e3f18 <_ZN6tflite24PreprocessSoftmaxScalingEddiPlPi+0x68>)
   e3ef4:	e9d3 2300 	ldrd	r2, r3, [r3]
   e3ef8:	e9cd 0100 	strd	r0, r1, [sp]
   e3efc:	f003 fb62 	bl	e75c4 <__aeabi_dcmpgt>
   e3f00:	ed9d 0b00 	vldr	d0, [sp]
   e3f04:	b108      	cbz	r0, e3f0a <_ZN6tflite24PreprocessSoftmaxScalingEddiPlPi+0x5a>
	return __b;
   e3f06:	ed9f 0b04 	vldr	d0, [pc, #16]	; e3f18 <_ZN6tflite24PreprocessSoftmaxScalingEddiPlPi+0x68>
#endif  // TFLITE_EMULATE_FLOAT

  QuantizeMultiplierGreaterThanOne(input_beta_real_multiplier,
                                   quantized_multiplier, left_shift);
   e3f0a:	4629      	mov	r1, r5
   e3f0c:	4620      	mov	r0, r4
}
   e3f0e:	b005      	add	sp, #20
   e3f10:	e8bd 40f0 	ldmia.w	sp!, {r4, r5, r6, r7, lr}
  const double input_beta_real_multiplier = std::min(
      beta * input_scale * (1 << (31 - input_integer_bits)), (1ll << 31) - 1.0);
#endif  // TFLITE_EMULATE_FLOAT

  QuantizeMultiplierGreaterThanOne(input_beta_real_multiplier,
                                   quantized_multiplier, left_shift);
   e3f14:	f7ff bf7e 	b.w	e3e14 <_ZN6tflite32QuantizeMultiplierGreaterThanOneEdPlPi>
   e3f18:	ffc00000 	.word	0xffc00000
   e3f1c:	41dfffff 	.word	0x41dfffff

000e3f20 <_ZN6tflite20CalculateInputRadiusEiii>:
                                              reverse_scaling_divisor,
                                              reverse_scaling_left_shift);
}

int CalculateInputRadius(int input_integer_bits, int input_left_shift,
                         int total_signed_bits) {
   e3f20:	e92d 4370 	stmdb	sp!, {r4, r5, r6, r8, r9, lr}
   e3f24:	4604      	mov	r4, r0
      (1ll << (total_signed_bits - input_integer_bits)) /
      (1ll << input_left_shift);
  // Tighten bound using floor.  Suppose that we could use the exact value.
  // After scaling the difference, the result would be at the maximum.  Thus we
  // must ensure that our value has lower magnitude.
  return static_cast<int>(std::floor(max_input_rescaled));
   e3f26:	2001      	movs	r0, #1
   e3f28:	40a0      	lsls	r0, r4
   e3f2a:	3801      	subs	r0, #1
                                              reverse_scaling_divisor,
                                              reverse_scaling_left_shift);
}

int CalculateInputRadius(int input_integer_bits, int input_left_shift,
                         int total_signed_bits) {
   e3f2c:	460e      	mov	r6, r1
   e3f2e:	4615      	mov	r5, r2
      (1ll << (total_signed_bits - input_integer_bits)) /
      (1ll << input_left_shift);
  // Tighten bound using floor.  Suppose that we could use the exact value.
  // After scaling the difference, the result would be at the maximum.  Thus we
  // must ensure that our value has lower magnitude.
  return static_cast<int>(std::floor(max_input_rescaled));
   e3f30:	f003 f852 	bl	e6fd8 <__aeabi_i2d>
   e3f34:	1b2a      	subs	r2, r5, r4
   e3f36:	4680      	mov	r8, r0
   e3f38:	4689      	mov	r9, r1
   e3f3a:	2001      	movs	r0, #1
   e3f3c:	2100      	movs	r1, #0
   e3f3e:	f002 feed 	bl	e6d1c <__aeabi_llsl>
   e3f42:	f003 f881 	bl	e7048 <__aeabi_l2d>
   e3f46:	460b      	mov	r3, r1
   e3f48:	4602      	mov	r2, r0
   e3f4a:	4649      	mov	r1, r9
   e3f4c:	4640      	mov	r0, r8
   e3f4e:	f003 f8a9 	bl	e70a4 <__aeabi_dmul>
   e3f52:	4632      	mov	r2, r6
   e3f54:	4604      	mov	r4, r0
   e3f56:	460d      	mov	r5, r1
   e3f58:	2001      	movs	r0, #1
   e3f5a:	2100      	movs	r1, #0
   e3f5c:	f002 fede 	bl	e6d1c <__aeabi_llsl>
   e3f60:	f003 f872 	bl	e7048 <__aeabi_l2d>
   e3f64:	4602      	mov	r2, r0
   e3f66:	460b      	mov	r3, r1
   e3f68:	4620      	mov	r0, r4
   e3f6a:	4629      	mov	r1, r5
   e3f6c:	f003 f9c4 	bl	e72f8 <__aeabi_ddiv>
   e3f70:	ec41 0b10 	vmov	d0, r0, r1
   e3f74:	f001 f8a8 	bl	e50c8 <floor>
   e3f78:	ec51 0b10 	vmov	r0, r1, d0
   e3f7c:	f003 fb2c 	bl	e75d8 <__aeabi_d2iz>
#endif  // TFLITE_EMULATE_FLOAT
}
   e3f80:	e8bd 8370 	ldmia.w	sp!, {r4, r5, r6, r8, r9, pc}

000e3f84 <os_thread_is_current>:
DYNALIB_BEGIN(hal_concurrent)

#if PLATFORM_THREADING
DYNALIB_FN(0, hal_concurrent, __gthread_equal, bool(__gthread_t, __gthread_t))
DYNALIB_FN(1, hal_concurrent, os_thread_create, os_result_t(os_thread_t*, const char*, os_thread_prio_t, os_thread_fn_t, void*, size_t))
DYNALIB_FN(2, hal_concurrent, os_thread_is_current, bool(os_thread_t))
   e3f84:	b508      	push	{r3, lr}
   e3f86:	4b02      	ldr	r3, [pc, #8]	; (e3f90 <os_thread_is_current+0xc>)
   e3f88:	681b      	ldr	r3, [r3, #0]
   e3f8a:	689b      	ldr	r3, [r3, #8]
   e3f8c:	9301      	str	r3, [sp, #4]
   e3f8e:	bd08      	pop	{r3, pc}
   e3f90:	00030248 	.word	0x00030248

000e3f94 <os_thread_join>:
DYNALIB_FN(3, hal_concurrent, os_thread_yield, os_result_t(void))
DYNALIB_FN(4, hal_concurrent, os_thread_join, os_result_t(os_thread_t))
   e3f94:	b508      	push	{r3, lr}
   e3f96:	4b02      	ldr	r3, [pc, #8]	; (e3fa0 <os_thread_join+0xc>)
   e3f98:	681b      	ldr	r3, [r3, #0]
   e3f9a:	691b      	ldr	r3, [r3, #16]
   e3f9c:	9301      	str	r3, [sp, #4]
   e3f9e:	bd08      	pop	{r3, pc}
   e3fa0:	00030248 	.word	0x00030248

000e3fa4 <os_thread_cleanup>:
DYNALIB_FN(5, hal_concurrent, os_thread_cleanup, os_result_t(os_thread_t))
   e3fa4:	b508      	push	{r3, lr}
   e3fa6:	4b02      	ldr	r3, [pc, #8]	; (e3fb0 <os_thread_cleanup+0xc>)
   e3fa8:	681b      	ldr	r3, [r3, #0]
   e3faa:	695b      	ldr	r3, [r3, #20]
   e3fac:	9301      	str	r3, [sp, #4]
   e3fae:	bd08      	pop	{r3, pc}
   e3fb0:	00030248 	.word	0x00030248

000e3fb4 <os_mutex_create>:
DYNALIB_FN(8, hal_concurrent, os_timer_create, int(os_timer_t*, unsigned, void(*)(os_timer_t), void*, bool, void*))
DYNALIB_FN(9, hal_concurrent, os_timer_destroy, int(os_timer_t, void*))
DYNALIB_FN(10, hal_concurrent, os_timer_get_id, int(os_timer_t, void**))
DYNALIB_FN(11, hal_concurrent, os_timer_change, int(os_timer_t, os_timer_change_t, bool, unsigned, unsigned, void*))

DYNALIB_FN(12, hal_concurrent, os_mutex_create, int(os_mutex_t*))
   e3fb4:	b508      	push	{r3, lr}
   e3fb6:	4b02      	ldr	r3, [pc, #8]	; (e3fc0 <os_mutex_create+0xc>)
   e3fb8:	681b      	ldr	r3, [r3, #0]
   e3fba:	6b1b      	ldr	r3, [r3, #48]	; 0x30
   e3fbc:	9301      	str	r3, [sp, #4]
   e3fbe:	bd08      	pop	{r3, pc}
   e3fc0:	00030248 	.word	0x00030248

000e3fc4 <os_mutex_recursive_create>:
DYNALIB_FN(13, hal_concurrent, os_mutex_destroy, int(os_mutex_t))
DYNALIB_FN(14, hal_concurrent, os_mutex_lock, int(os_mutex_t))
DYNALIB_FN(15, hal_concurrent, os_mutex_trylock, int(os_mutex_t))
DYNALIB_FN(16, hal_concurrent, os_mutex_unlock, int(os_mutex_t))

DYNALIB_FN(17, hal_concurrent, os_mutex_recursive_create, int(os_mutex_recursive_t*))
   e3fc4:	b508      	push	{r3, lr}
   e3fc6:	4b02      	ldr	r3, [pc, #8]	; (e3fd0 <os_mutex_recursive_create+0xc>)
   e3fc8:	681b      	ldr	r3, [r3, #0]
   e3fca:	6c5b      	ldr	r3, [r3, #68]	; 0x44
   e3fcc:	9301      	str	r3, [sp, #4]
   e3fce:	bd08      	pop	{r3, pc}
   e3fd0:	00030248 	.word	0x00030248

000e3fd4 <HAL_RNG_GetRandomNumber>:

DYNALIB_BEGIN(hal)

#if PLATFORM_ID > 3
DYNALIB_FN(0, hal, HAL_RNG_Configuration, void(void))
DYNALIB_FN(1, hal, HAL_RNG_GetRandomNumber, uint32_t(void))
   e3fd4:	b508      	push	{r3, lr}
   e3fd6:	4b02      	ldr	r3, [pc, #8]	; (e3fe0 <HAL_RNG_GetRandomNumber+0xc>)
   e3fd8:	681b      	ldr	r3, [r3, #0]
   e3fda:	685b      	ldr	r3, [r3, #4]
   e3fdc:	9301      	str	r3, [sp, #4]
   e3fde:	bd08      	pop	{r3, pc}
   e3fe0:	00030218 	.word	0x00030218

000e3fe4 <HAL_Delay_Microseconds>:
#else
#define BASE_IDX 0
#endif

DYNALIB_FN(BASE_IDX + 0, hal, HAL_Delay_Milliseconds, void(uint32_t))
DYNALIB_FN(BASE_IDX + 1, hal, HAL_Delay_Microseconds, void(uint32_t))
   e3fe4:	b508      	push	{r3, lr}
   e3fe6:	4b02      	ldr	r3, [pc, #8]	; (e3ff0 <HAL_Delay_Microseconds+0xc>)
   e3fe8:	681b      	ldr	r3, [r3, #0]
   e3fea:	68db      	ldr	r3, [r3, #12]
   e3fec:	9301      	str	r3, [sp, #4]
   e3fee:	bd08      	pop	{r3, pc}
   e3ff0:	00030218 	.word	0x00030218

000e3ff4 <HAL_Timer_Get_Milli_Seconds>:
DYNALIB_FN(BASE_IDX + 2, hal, HAL_Timer_Get_Micro_Seconds, system_tick_t(void))
DYNALIB_FN(BASE_IDX + 3, hal, HAL_Timer_Get_Milli_Seconds, system_tick_t(void))
   e3ff4:	b508      	push	{r3, lr}
   e3ff6:	4b02      	ldr	r3, [pc, #8]	; (e4000 <HAL_Timer_Get_Milli_Seconds+0xc>)
   e3ff8:	681b      	ldr	r3, [r3, #0]
   e3ffa:	695b      	ldr	r3, [r3, #20]
   e3ffc:	9301      	str	r3, [sp, #4]
   e3ffe:	bd08      	pop	{r3, pc}
   e4000:	00030218 	.word	0x00030218

000e4004 <HAL_Pin_Map>:
// New HAL functions must be added to the end of this list.
// GNINRAW

DYNALIB_BEGIN(hal_gpio)

DYNALIB_FN(0, hal_gpio, HAL_Pin_Map, Hal_Pin_Info*(void))
   e4004:	b508      	push	{r3, lr}
   e4006:	4b02      	ldr	r3, [pc, #8]	; (e4010 <HAL_Pin_Map+0xc>)
   e4008:	681b      	ldr	r3, [r3, #0]
   e400a:	681b      	ldr	r3, [r3, #0]
   e400c:	9301      	str	r3, [sp, #4]
   e400e:	bd08      	pop	{r3, pc}
   e4010:	0003022c 	.word	0x0003022c

000e4014 <HAL_Validate_Pin_Function>:
DYNALIB_FN(1, hal_gpio, HAL_Validate_Pin_Function, PinFunction(pin_t, PinFunction))
   e4014:	b508      	push	{r3, lr}
   e4016:	4b02      	ldr	r3, [pc, #8]	; (e4020 <HAL_Validate_Pin_Function+0xc>)
   e4018:	681b      	ldr	r3, [r3, #0]
   e401a:	685b      	ldr	r3, [r3, #4]
   e401c:	9301      	str	r3, [sp, #4]
   e401e:	bd08      	pop	{r3, pc}
   e4020:	0003022c 	.word	0x0003022c

000e4024 <HAL_Pin_Mode>:
DYNALIB_FN(2, hal_gpio, HAL_Pin_Mode, void(pin_t, PinMode))
   e4024:	b508      	push	{r3, lr}
   e4026:	4b02      	ldr	r3, [pc, #8]	; (e4030 <HAL_Pin_Mode+0xc>)
   e4028:	681b      	ldr	r3, [r3, #0]
   e402a:	689b      	ldr	r3, [r3, #8]
   e402c:	9301      	str	r3, [sp, #4]
   e402e:	bd08      	pop	{r3, pc}
   e4030:	0003022c 	.word	0x0003022c

000e4034 <HAL_Get_Pin_Mode>:
DYNALIB_FN(3, hal_gpio, HAL_Get_Pin_Mode, PinMode(pin_t))
   e4034:	b508      	push	{r3, lr}
   e4036:	4b02      	ldr	r3, [pc, #8]	; (e4040 <HAL_Get_Pin_Mode+0xc>)
   e4038:	681b      	ldr	r3, [r3, #0]
   e403a:	68db      	ldr	r3, [r3, #12]
   e403c:	9301      	str	r3, [sp, #4]
   e403e:	bd08      	pop	{r3, pc}
   e4040:	0003022c 	.word	0x0003022c

000e4044 <HAL_DAC_Write>:
DYNALIB_FN(6, hal_gpio, HAL_Interrupts_Attach, int(uint16_t, HAL_InterruptHandler, void*, InterruptMode, HAL_InterruptExtraConfiguration*))
DYNALIB_FN(7, hal_gpio, HAL_Interrupts_Detach, int(uint16_t))
DYNALIB_FN(8, hal_gpio, HAL_Interrupts_Enable_All, void(void))
DYNALIB_FN(9, hal_gpio, HAL_Interrupts_Disable_All, void(void))

DYNALIB_FN(10, hal_gpio, HAL_DAC_Write, void(pin_t, uint16_t))
   e4044:	b508      	push	{r3, lr}
   e4046:	4b02      	ldr	r3, [pc, #8]	; (e4050 <HAL_DAC_Write+0xc>)
   e4048:	681b      	ldr	r3, [r3, #0]
   e404a:	6a9b      	ldr	r3, [r3, #40]	; 0x28
   e404c:	9301      	str	r3, [sp, #4]
   e404e:	bd08      	pop	{r3, pc}
   e4050:	0003022c 	.word	0x0003022c

000e4054 <HAL_PWM_Write_Ext>:
DYNALIB_FN(25, hal_gpio, HAL_DAC_Get_Resolution, uint8_t(pin_t))
DYNALIB_FN(26, hal_gpio, HAL_DAC_Set_Resolution, void(pin_t, uint8_t))
DYNALIB_FN(27, hal_gpio, HAL_DAC_Enable_Buffer, void(pin_t pin, uint8_t state))
DYNALIB_FN(28, hal_gpio, HAL_PWM_Get_Resolution, uint8_t(uint16_t))
DYNALIB_FN(29, hal_gpio, HAL_PWM_Set_Resolution, void(uint16_t, uint8_t))
DYNALIB_FN(30, hal_gpio, HAL_PWM_Write_Ext, void(uint16_t, uint32_t))
   e4054:	b508      	push	{r3, lr}
   e4056:	4b02      	ldr	r3, [pc, #8]	; (e4060 <HAL_PWM_Write_Ext+0xc>)
   e4058:	681b      	ldr	r3, [r3, #0]
   e405a:	6f9b      	ldr	r3, [r3, #120]	; 0x78
   e405c:	9301      	str	r3, [sp, #4]
   e405e:	bd08      	pop	{r3, pc}
   e4060:	0003022c 	.word	0x0003022c

000e4064 <HAL_I2C_Write_Data>:
DYNALIB_FN(BASE_IDX + 3, hal_i2c, HAL_I2C_Begin, void(HAL_I2C_Interface, I2C_Mode, uint8_t, void*))
DYNALIB_FN(BASE_IDX + 4, hal_i2c, HAL_I2C_End, void(HAL_I2C_Interface, void*))
DYNALIB_FN(BASE_IDX + 5, hal_i2c, HAL_I2C_Request_Data, uint32_t(HAL_I2C_Interface, uint8_t, uint8_t, uint8_t, void*))
DYNALIB_FN(BASE_IDX + 6, hal_i2c, HAL_I2C_Begin_Transmission, void(HAL_I2C_Interface, uint8_t, void*))
DYNALIB_FN(BASE_IDX + 7, hal_i2c, HAL_I2C_End_Transmission, uint8_t(HAL_I2C_Interface, uint8_t, void*))
DYNALIB_FN(BASE_IDX + 8, hal_i2c, HAL_I2C_Write_Data, uint32_t(HAL_I2C_Interface, uint8_t, void*))
   e4064:	b508      	push	{r3, lr}
   e4066:	4b02      	ldr	r3, [pc, #8]	; (e4070 <HAL_I2C_Write_Data+0xc>)
   e4068:	681b      	ldr	r3, [r3, #0]
   e406a:	6a1b      	ldr	r3, [r3, #32]
   e406c:	9301      	str	r3, [sp, #4]
   e406e:	bd08      	pop	{r3, pc}
   e4070:	00030228 	.word	0x00030228

000e4074 <HAL_I2C_Available_Data>:
DYNALIB_FN(BASE_IDX + 9, hal_i2c, HAL_I2C_Available_Data, int32_t(HAL_I2C_Interface, void*))
   e4074:	b508      	push	{r3, lr}
   e4076:	4b02      	ldr	r3, [pc, #8]	; (e4080 <HAL_I2C_Available_Data+0xc>)
   e4078:	681b      	ldr	r3, [r3, #0]
   e407a:	6a5b      	ldr	r3, [r3, #36]	; 0x24
   e407c:	9301      	str	r3, [sp, #4]
   e407e:	bd08      	pop	{r3, pc}
   e4080:	00030228 	.word	0x00030228

000e4084 <HAL_I2C_Read_Data>:
DYNALIB_FN(BASE_IDX + 10, hal_i2c, HAL_I2C_Read_Data, int32_t(HAL_I2C_Interface, void*))
   e4084:	b508      	push	{r3, lr}
   e4086:	4b02      	ldr	r3, [pc, #8]	; (e4090 <HAL_I2C_Read_Data+0xc>)
   e4088:	681b      	ldr	r3, [r3, #0]
   e408a:	6a9b      	ldr	r3, [r3, #40]	; 0x28
   e408c:	9301      	str	r3, [sp, #4]
   e408e:	bd08      	pop	{r3, pc}
   e4090:	00030228 	.word	0x00030228

000e4094 <HAL_I2C_Peek_Data>:
DYNALIB_FN(BASE_IDX + 11, hal_i2c, HAL_I2C_Peek_Data, int32_t(HAL_I2C_Interface, void*))
   e4094:	b508      	push	{r3, lr}
   e4096:	4b02      	ldr	r3, [pc, #8]	; (e40a0 <HAL_I2C_Peek_Data+0xc>)
   e4098:	681b      	ldr	r3, [r3, #0]
   e409a:	6adb      	ldr	r3, [r3, #44]	; 0x2c
   e409c:	9301      	str	r3, [sp, #4]
   e409e:	bd08      	pop	{r3, pc}
   e40a0:	00030228 	.word	0x00030228

000e40a4 <HAL_I2C_Flush_Data>:
DYNALIB_FN(BASE_IDX + 12, hal_i2c, HAL_I2C_Flush_Data, void(HAL_I2C_Interface, void*))
   e40a4:	b508      	push	{r3, lr}
   e40a6:	4b02      	ldr	r3, [pc, #8]	; (e40b0 <HAL_I2C_Flush_Data+0xc>)
   e40a8:	681b      	ldr	r3, [r3, #0]
   e40aa:	6b1b      	ldr	r3, [r3, #48]	; 0x30
   e40ac:	9301      	str	r3, [sp, #4]
   e40ae:	bd08      	pop	{r3, pc}
   e40b0:	00030228 	.word	0x00030228

000e40b4 <HAL_I2C_Is_Enabled>:
DYNALIB_FN(BASE_IDX + 13, hal_i2c, HAL_I2C_Is_Enabled, bool(HAL_I2C_Interface, void*))
   e40b4:	b508      	push	{r3, lr}
   e40b6:	4b02      	ldr	r3, [pc, #8]	; (e40c0 <HAL_I2C_Is_Enabled+0xc>)
   e40b8:	681b      	ldr	r3, [r3, #0]
   e40ba:	6b5b      	ldr	r3, [r3, #52]	; 0x34
   e40bc:	9301      	str	r3, [sp, #4]
   e40be:	bd08      	pop	{r3, pc}
   e40c0:	00030228 	.word	0x00030228

000e40c4 <HAL_I2C_Init>:
DYNALIB_FN(BASE_IDX + 14, hal_i2c, HAL_I2C_Set_Callback_On_Receive, void(HAL_I2C_Interface, void(*)(int), void*))
DYNALIB_FN(BASE_IDX + 15, hal_i2c, HAL_I2C_Set_Callback_On_Request, void(HAL_I2C_Interface, void(*)(void), void*))
DYNALIB_FN(BASE_IDX + 16, hal_i2c, HAL_I2C_Init, void(HAL_I2C_Interface, void*))
   e40c4:	b508      	push	{r3, lr}
   e40c6:	4b02      	ldr	r3, [pc, #8]	; (e40d0 <HAL_I2C_Init+0xc>)
   e40c8:	681b      	ldr	r3, [r3, #0]
   e40ca:	6c1b      	ldr	r3, [r3, #64]	; 0x40
   e40cc:	9301      	str	r3, [sp, #4]
   e40ce:	bd08      	pop	{r3, pc}
   e40d0:	00030228 	.word	0x00030228

000e40d4 <inet_inet_ntop>:
DYNALIB_FN(0, hal_inet, inet_inet_addr, in_addr_t(const char*))
DYNALIB_FN(1, hal_inet, inet_inet_aton, int(const char*, struct in_addr*))
DYNALIB_FN(2, hal_inet, inet_inet_network, in_addr_t(const char*))
DYNALIB_FN(3, hal_inet, inet_inet_ntoa, char*(struct in_addr))
DYNALIB_FN(4, hal_inet, inet_inet_ntoa_r, char*(struct in_addr, char*, socklen_t))
DYNALIB_FN(5, hal_inet, inet_inet_ntop, const char*(int, const void*, char*, socklen_t))
   e40d4:	b508      	push	{r3, lr}
   e40d6:	4b02      	ldr	r3, [pc, #8]	; (e40e0 <inet_inet_ntop+0xc>)
   e40d8:	681b      	ldr	r3, [r3, #0]
   e40da:	695b      	ldr	r3, [r3, #20]
   e40dc:	9301      	str	r3, [sp, #4]
   e40de:	bd08      	pop	{r3, pc}
   e40e0:	00030264 	.word	0x00030264

000e40e4 <netdb_freeaddrinfo>:

DYNALIB_BEGIN(hal_netdb)

DYNALIB_FN(0, hal_netdb, netdb_gethostbyname, struct hostent*(const char*))
DYNALIB_FN(1, hal_netdb, netdb_gethostbyname_r, int(const char*, struct hostent*, char*, size_t, struct hostent**, int*))
DYNALIB_FN(2, hal_netdb, netdb_freeaddrinfo, void(struct addrinfo*))
   e40e4:	b508      	push	{r3, lr}
   e40e6:	4b02      	ldr	r3, [pc, #8]	; (e40f0 <netdb_freeaddrinfo+0xc>)
   e40e8:	681b      	ldr	r3, [r3, #0]
   e40ea:	689b      	ldr	r3, [r3, #8]
   e40ec:	9301      	str	r3, [sp, #4]
   e40ee:	bd08      	pop	{r3, pc}
   e40f0:	00030268 	.word	0x00030268

000e40f4 <netdb_getaddrinfo>:
DYNALIB_FN(3, hal_netdb, netdb_getaddrinfo, int(const char*, const char*, const struct addrinfo*, struct addrinfo**))
   e40f4:	b508      	push	{r3, lr}
   e40f6:	4b02      	ldr	r3, [pc, #8]	; (e4100 <netdb_getaddrinfo+0xc>)
   e40f8:	681b      	ldr	r3, [r3, #0]
   e40fa:	68db      	ldr	r3, [r3, #12]
   e40fc:	9301      	str	r3, [sp, #4]
   e40fe:	bd08      	pop	{r3, pc}
   e4100:	00030268 	.word	0x00030268

000e4104 <HAL_SPI_Init>:
DYNALIB_FN(2, hal_spi, HAL_SPI_Set_Bit_Order, void(HAL_SPI_Interface, uint8_t))
DYNALIB_FN(3, hal_spi, HAL_SPI_Set_Data_Mode, void(HAL_SPI_Interface, uint8_t))
DYNALIB_FN(4, hal_spi, HAL_SPI_Set_Clock_Divider, void(HAL_SPI_Interface, uint8_t))
DYNALIB_FN(5, hal_spi, HAL_SPI_Send_Receive_Data, uint16_t(HAL_SPI_Interface, uint16_t))
DYNALIB_FN(6, hal_spi, HAL_SPI_Is_Enabled_Old, bool(void))
DYNALIB_FN(7, hal_spi, HAL_SPI_Init, void(HAL_SPI_Interface))
   e4104:	b508      	push	{r3, lr}
   e4106:	4b02      	ldr	r3, [pc, #8]	; (e4110 <HAL_SPI_Init+0xc>)
   e4108:	681b      	ldr	r3, [r3, #0]
   e410a:	69db      	ldr	r3, [r3, #28]
   e410c:	9301      	str	r3, [sp, #4]
   e410e:	bd08      	pop	{r3, pc}
   e4110:	00030230 	.word	0x00030230

000e4114 <HAL_SPI_Is_Enabled>:
DYNALIB_FN(8, hal_spi, HAL_SPI_Is_Enabled, bool(HAL_SPI_Interface))
   e4114:	b508      	push	{r3, lr}
   e4116:	4b02      	ldr	r3, [pc, #8]	; (e4120 <HAL_SPI_Is_Enabled+0xc>)
   e4118:	681b      	ldr	r3, [r3, #0]
   e411a:	6a1b      	ldr	r3, [r3, #32]
   e411c:	9301      	str	r3, [sp, #4]
   e411e:	bd08      	pop	{r3, pc}
   e4120:	00030230 	.word	0x00030230

000e4124 <HAL_USART_Init>:
#define BASE_IDX 6 // Base index for all subsequent functions
#else
#define BASE_IDX 0
#endif

DYNALIB_FN(BASE_IDX + 0, hal_usart, HAL_USART_Init, void(HAL_USART_Serial, Ring_Buffer*, Ring_Buffer*))
   e4124:	b508      	push	{r3, lr}
   e4126:	4b02      	ldr	r3, [pc, #8]	; (e4130 <HAL_USART_Init+0xc>)
   e4128:	681b      	ldr	r3, [r3, #0]
   e412a:	681b      	ldr	r3, [r3, #0]
   e412c:	9301      	str	r3, [sp, #4]
   e412e:	bd08      	pop	{r3, pc}
   e4130:	0003023c 	.word	0x0003023c

000e4134 <HAL_USART_Write_Data>:
DYNALIB_FN(BASE_IDX + 1, hal_usart, HAL_USART_Begin, void(HAL_USART_Serial, uint32_t))
DYNALIB_FN(BASE_IDX + 2, hal_usart, HAL_USART_End, void(HAL_USART_Serial))
DYNALIB_FN(BASE_IDX + 3, hal_usart, HAL_USART_Write_Data, uint32_t(HAL_USART_Serial, uint8_t))
   e4134:	b508      	push	{r3, lr}
   e4136:	4b02      	ldr	r3, [pc, #8]	; (e4140 <HAL_USART_Write_Data+0xc>)
   e4138:	681b      	ldr	r3, [r3, #0]
   e413a:	68db      	ldr	r3, [r3, #12]
   e413c:	9301      	str	r3, [sp, #4]
   e413e:	bd08      	pop	{r3, pc}
   e4140:	0003023c 	.word	0x0003023c

000e4144 <HAL_USART_Available_Data>:
DYNALIB_FN(BASE_IDX + 4, hal_usart, HAL_USART_Available_Data, int32_t(HAL_USART_Serial))
   e4144:	b508      	push	{r3, lr}
   e4146:	4b02      	ldr	r3, [pc, #8]	; (e4150 <HAL_USART_Available_Data+0xc>)
   e4148:	681b      	ldr	r3, [r3, #0]
   e414a:	691b      	ldr	r3, [r3, #16]
   e414c:	9301      	str	r3, [sp, #4]
   e414e:	bd08      	pop	{r3, pc}
   e4150:	0003023c 	.word	0x0003023c

000e4154 <HAL_USART_Read_Data>:
DYNALIB_FN(BASE_IDX + 5, hal_usart, HAL_USART_Read_Data, int32_t(HAL_USART_Serial))
   e4154:	b508      	push	{r3, lr}
   e4156:	4b02      	ldr	r3, [pc, #8]	; (e4160 <HAL_USART_Read_Data+0xc>)
   e4158:	681b      	ldr	r3, [r3, #0]
   e415a:	695b      	ldr	r3, [r3, #20]
   e415c:	9301      	str	r3, [sp, #4]
   e415e:	bd08      	pop	{r3, pc}
   e4160:	0003023c 	.word	0x0003023c

000e4164 <HAL_USART_Peek_Data>:
DYNALIB_FN(BASE_IDX + 6, hal_usart, HAL_USART_Peek_Data, int32_t(HAL_USART_Serial))
   e4164:	b508      	push	{r3, lr}
   e4166:	4b02      	ldr	r3, [pc, #8]	; (e4170 <HAL_USART_Peek_Data+0xc>)
   e4168:	681b      	ldr	r3, [r3, #0]
   e416a:	699b      	ldr	r3, [r3, #24]
   e416c:	9301      	str	r3, [sp, #4]
   e416e:	bd08      	pop	{r3, pc}
   e4170:	0003023c 	.word	0x0003023c

000e4174 <HAL_USART_Flush_Data>:
DYNALIB_FN(BASE_IDX + 7, hal_usart, HAL_USART_Flush_Data, void(HAL_USART_Serial))
   e4174:	b508      	push	{r3, lr}
   e4176:	4b02      	ldr	r3, [pc, #8]	; (e4180 <HAL_USART_Flush_Data+0xc>)
   e4178:	681b      	ldr	r3, [r3, #0]
   e417a:	69db      	ldr	r3, [r3, #28]
   e417c:	9301      	str	r3, [sp, #4]
   e417e:	bd08      	pop	{r3, pc}
   e4180:	0003023c 	.word	0x0003023c

000e4184 <HAL_USART_Is_Enabled>:
DYNALIB_FN(BASE_IDX + 8, hal_usart, HAL_USART_Is_Enabled, bool(HAL_USART_Serial))
   e4184:	b508      	push	{r3, lr}
   e4186:	4b02      	ldr	r3, [pc, #8]	; (e4190 <HAL_USART_Is_Enabled+0xc>)
   e4188:	681b      	ldr	r3, [r3, #0]
   e418a:	6a1b      	ldr	r3, [r3, #32]
   e418c:	9301      	str	r3, [sp, #4]
   e418e:	bd08      	pop	{r3, pc}
   e4190:	0003023c 	.word	0x0003023c

000e4194 <HAL_USART_Available_Data_For_Write>:
DYNALIB_FN(BASE_IDX + 9, hal_usart, HAL_USART_Half_Duplex, void(HAL_USART_Serial, bool))
DYNALIB_FN(BASE_IDX + 10, hal_usart, HAL_USART_Available_Data_For_Write, int32_t(HAL_USART_Serial))
   e4194:	b508      	push	{r3, lr}
   e4196:	4b02      	ldr	r3, [pc, #8]	; (e41a0 <HAL_USART_Available_Data_For_Write+0xc>)
   e4198:	681b      	ldr	r3, [r3, #0]
   e419a:	6a9b      	ldr	r3, [r3, #40]	; 0x28
   e419c:	9301      	str	r3, [sp, #4]
   e419e:	bd08      	pop	{r3, pc}
   e41a0:	0003023c 	.word	0x0003023c

000e41a4 <HAL_USB_USART_Init>:
#endif

DYNALIB_BEGIN(hal_usb)

#ifdef USB_CDC_ENABLE
DYNALIB_FN(0, hal_usb, HAL_USB_USART_Init, void(HAL_USB_USART_Serial, const HAL_USB_USART_Config*))
   e41a4:	b508      	push	{r3, lr}
   e41a6:	4b02      	ldr	r3, [pc, #8]	; (e41b0 <HAL_USB_USART_Init+0xc>)
   e41a8:	681b      	ldr	r3, [r3, #0]
   e41aa:	681b      	ldr	r3, [r3, #0]
   e41ac:	9301      	str	r3, [sp, #4]
   e41ae:	bd08      	pop	{r3, pc}
   e41b0:	0003024c 	.word	0x0003024c

000e41b4 <HAL_USB_USART_Begin>:
DYNALIB_FN(1, hal_usb, HAL_USB_USART_Begin, void(HAL_USB_USART_Serial, uint32_t, void *))
   e41b4:	b508      	push	{r3, lr}
   e41b6:	4b02      	ldr	r3, [pc, #8]	; (e41c0 <HAL_USB_USART_Begin+0xc>)
   e41b8:	681b      	ldr	r3, [r3, #0]
   e41ba:	685b      	ldr	r3, [r3, #4]
   e41bc:	9301      	str	r3, [sp, #4]
   e41be:	bd08      	pop	{r3, pc}
   e41c0:	0003024c 	.word	0x0003024c

000e41c4 <HAL_USB_USART_Available_Data>:
DYNALIB_FN(2, hal_usb, HAL_USB_USART_End, void(HAL_USB_USART_Serial))
DYNALIB_FN(3, hal_usb, HAL_USB_USART_Baud_Rate, unsigned int(HAL_USB_USART_Serial))
DYNALIB_FN(4, hal_usb, HAL_USB_USART_Available_Data, int32_t(HAL_USB_USART_Serial))
   e41c4:	b508      	push	{r3, lr}
   e41c6:	4b02      	ldr	r3, [pc, #8]	; (e41d0 <HAL_USB_USART_Available_Data+0xc>)
   e41c8:	681b      	ldr	r3, [r3, #0]
   e41ca:	691b      	ldr	r3, [r3, #16]
   e41cc:	9301      	str	r3, [sp, #4]
   e41ce:	bd08      	pop	{r3, pc}
   e41d0:	0003024c 	.word	0x0003024c

000e41d4 <HAL_USB_USART_Available_Data_For_Write>:
DYNALIB_FN(5, hal_usb, HAL_USB_USART_Available_Data_For_Write, int32_t(HAL_USB_USART_Serial))
   e41d4:	b508      	push	{r3, lr}
   e41d6:	4b02      	ldr	r3, [pc, #8]	; (e41e0 <HAL_USB_USART_Available_Data_For_Write+0xc>)
   e41d8:	681b      	ldr	r3, [r3, #0]
   e41da:	695b      	ldr	r3, [r3, #20]
   e41dc:	9301      	str	r3, [sp, #4]
   e41de:	bd08      	pop	{r3, pc}
   e41e0:	0003024c 	.word	0x0003024c

000e41e4 <HAL_USB_USART_Receive_Data>:
DYNALIB_FN(6, hal_usb, HAL_USB_USART_Receive_Data, int32_t(HAL_USB_USART_Serial, uint8_t))
   e41e4:	b508      	push	{r3, lr}
   e41e6:	4b02      	ldr	r3, [pc, #8]	; (e41f0 <HAL_USB_USART_Receive_Data+0xc>)
   e41e8:	681b      	ldr	r3, [r3, #0]
   e41ea:	699b      	ldr	r3, [r3, #24]
   e41ec:	9301      	str	r3, [sp, #4]
   e41ee:	bd08      	pop	{r3, pc}
   e41f0:	0003024c 	.word	0x0003024c

000e41f4 <HAL_USB_USART_Send_Data>:
DYNALIB_FN(7, hal_usb, HAL_USB_USART_Send_Data, int32_t(HAL_USB_USART_Serial, uint8_t))
   e41f4:	b508      	push	{r3, lr}
   e41f6:	4b02      	ldr	r3, [pc, #8]	; (e4200 <HAL_USB_USART_Send_Data+0xc>)
   e41f8:	681b      	ldr	r3, [r3, #0]
   e41fa:	69db      	ldr	r3, [r3, #28]
   e41fc:	9301      	str	r3, [sp, #4]
   e41fe:	bd08      	pop	{r3, pc}
   e4200:	0003024c 	.word	0x0003024c

000e4204 <HAL_USB_USART_Flush_Data>:
DYNALIB_FN(8, hal_usb, HAL_USB_USART_Flush_Data, void(HAL_USB_USART_Serial))
   e4204:	b508      	push	{r3, lr}
   e4206:	4b02      	ldr	r3, [pc, #8]	; (e4210 <HAL_USB_USART_Flush_Data+0xc>)
   e4208:	681b      	ldr	r3, [r3, #0]
   e420a:	6a1b      	ldr	r3, [r3, #32]
   e420c:	9301      	str	r3, [sp, #4]
   e420e:	bd08      	pop	{r3, pc}
   e4210:	0003024c 	.word	0x0003024c

000e4214 <panic_>:
DYNALIB_FN(9, services, LED_Toggle, void(Led_TypeDef))
DYNALIB_FN(10, services, LED_Fade, void(Led_TypeDef))
DYNALIB_FN(11, services, Get_LED_Brightness, uint8_t(void))

DYNALIB_FN(12, services, set_logger_output, void(debug_output_fn, LoggerOutputLevel)) // Deprecated
DYNALIB_FN(13, services, panic_, void(ePanicCode, void*, void(*)(uint32_t)))
   e4214:	b508      	push	{r3, lr}
   e4216:	4b02      	ldr	r3, [pc, #8]	; (e4220 <panic_+0xc>)
   e4218:	681b      	ldr	r3, [r3, #0]
   e421a:	6b5b      	ldr	r3, [r3, #52]	; 0x34
   e421c:	9301      	str	r3, [sp, #4]
   e421e:	bd08      	pop	{r3, pc}
   e4220:	00030260 	.word	0x00030260

000e4224 <set_system_mode>:
#endif

DYNALIB_BEGIN(system)

DYNALIB_FN(0, system, system_mode, System_Mode_TypeDef(void))
DYNALIB_FN(1, system, set_system_mode, void(System_Mode_TypeDef))
   e4224:	b508      	push	{r3, lr}
   e4226:	4b02      	ldr	r3, [pc, #8]	; (e4230 <set_system_mode+0xc>)
   e4228:	681b      	ldr	r3, [r3, #0]
   e422a:	685b      	ldr	r3, [r3, #4]
   e422c:	9301      	str	r3, [sp, #4]
   e422e:	bd08      	pop	{r3, pc}
   e4230:	00030220 	.word	0x00030220

000e4234 <system_ctrl_set_app_request_handler>:
DYNALIB_FN(BASE_IDX + 6, system, led_pattern_period, uint16_t(int, int, void*))
DYNALIB_FN(BASE_IDX + 7, system, system_set_tester_handlers, int(system_tester_handlers_t*, void*))
DYNALIB_FN(BASE_IDX + 8, system, system_format_diag_data, int(const uint16_t*, size_t, unsigned, appender_fn, void*, void*))

// Control requests
DYNALIB_FN(BASE_IDX + 9, system, system_ctrl_set_app_request_handler, int(ctrl_request_handler_fn, void*))
   e4234:	b508      	push	{r3, lr}
   e4236:	4b03      	ldr	r3, [pc, #12]	; (e4244 <system_ctrl_set_app_request_handler+0x10>)
   e4238:	681b      	ldr	r3, [r3, #0]
   e423a:	f8d3 3088 	ldr.w	r3, [r3, #136]	; 0x88
   e423e:	9301      	str	r3, [sp, #4]
   e4240:	bd08      	pop	{r3, pc}
   e4242:	0000      	.short	0x0000
   e4244:	00030220 	.word	0x00030220

000e4248 <system_ctrl_set_result>:
DYNALIB_FN(BASE_IDX + 10, system, system_ctrl_alloc_reply_data, int(ctrl_request*, size_t, void*))
DYNALIB_FN(BASE_IDX + 11, system, system_ctrl_free_request_data, void(ctrl_request*, void*))
DYNALIB_FN(BASE_IDX + 12, system, system_ctrl_set_result, void(ctrl_request*, int, ctrl_completion_handler_fn, void*, void*))
   e4248:	b508      	push	{r3, lr}
   e424a:	4b03      	ldr	r3, [pc, #12]	; (e4258 <system_ctrl_set_result+0x10>)
   e424c:	681b      	ldr	r3, [r3, #0]
   e424e:	f8d3 3094 	ldr.w	r3, [r3, #148]	; 0x94
   e4252:	9301      	str	r3, [sp, #4]
   e4254:	bd08      	pop	{r3, pc}
   e4256:	0000      	.short	0x0000
   e4258:	00030220 	.word	0x00030220

000e425c <spark_set_random_seed_from_cloud_handler>:
DYNALIB_FN(10, system_cloud, spark_unsubscribe, void(void*))
DYNALIB_FN(11, system_cloud, spark_sync_time, bool(void*))
DYNALIB_FN(12, system_cloud, spark_sync_time_pending, bool(void*))
DYNALIB_FN(13, system_cloud, spark_sync_time_last, system_tick_t(time_t*, void*))
DYNALIB_FN(14, system_cloud, spark_set_connection_property, int(unsigned, unsigned, particle::protocol::connection_properties_t*, void*))
DYNALIB_FN(15, system_cloud, spark_set_random_seed_from_cloud_handler, int(void (*handler)(unsigned int), void*))
   e425c:	b508      	push	{r3, lr}
   e425e:	4b02      	ldr	r3, [pc, #8]	; (e4268 <spark_set_random_seed_from_cloud_handler+0xc>)
   e4260:	681b      	ldr	r3, [r3, #0]
   e4262:	6bdb      	ldr	r3, [r3, #60]	; 0x3c
   e4264:	9301      	str	r3, [sp, #4]
   e4266:	bd08      	pop	{r3, pc}
   e4268:	00030244 	.word	0x00030244

000e426c <network_connect>:
#endif

DYNALIB_BEGIN(system_net)

DYNALIB_FN(0, system_net, network_config, const void*(network_handle_t, uint32_t, void*))
DYNALIB_FN(1, system_net, network_connect, void(network_handle_t, uint32_t, uint32_t, void*))
   e426c:	b508      	push	{r3, lr}
   e426e:	4b02      	ldr	r3, [pc, #8]	; (e4278 <network_connect+0xc>)
   e4270:	681b      	ldr	r3, [r3, #0]
   e4272:	685b      	ldr	r3, [r3, #4]
   e4274:	9301      	str	r3, [sp, #4]
   e4276:	bd08      	pop	{r3, pc}
   e4278:	00030240 	.word	0x00030240

000e427c <network_connecting>:
DYNALIB_FN(2, system_net, network_connecting, bool(network_handle_t, uint32_t, void*))
   e427c:	b508      	push	{r3, lr}
   e427e:	4b02      	ldr	r3, [pc, #8]	; (e4288 <network_connecting+0xc>)
   e4280:	681b      	ldr	r3, [r3, #0]
   e4282:	689b      	ldr	r3, [r3, #8]
   e4284:	9301      	str	r3, [sp, #4]
   e4286:	bd08      	pop	{r3, pc}
   e4288:	00030240 	.word	0x00030240

000e428c <network_disconnect>:
DYNALIB_FN(3, system_net, network_disconnect, void(network_handle_t, uint32_t, void*))
   e428c:	b508      	push	{r3, lr}
   e428e:	4b02      	ldr	r3, [pc, #8]	; (e4298 <network_disconnect+0xc>)
   e4290:	681b      	ldr	r3, [r3, #0]
   e4292:	68db      	ldr	r3, [r3, #12]
   e4294:	9301      	str	r3, [sp, #4]
   e4296:	bd08      	pop	{r3, pc}
   e4298:	00030240 	.word	0x00030240

000e429c <network_ready>:
DYNALIB_FN(4, system_net, network_ready, bool(network_handle_t, uint32_t, void*))
   e429c:	b508      	push	{r3, lr}
   e429e:	4b02      	ldr	r3, [pc, #8]	; (e42a8 <network_ready+0xc>)
   e42a0:	681b      	ldr	r3, [r3, #0]
   e42a2:	691b      	ldr	r3, [r3, #16]
   e42a4:	9301      	str	r3, [sp, #4]
   e42a6:	bd08      	pop	{r3, pc}
   e42a8:	00030240 	.word	0x00030240

000e42ac <network_on>:
DYNALIB_FN(5, system_net, network_on, void(network_handle_t, uint32_t, uint32_t, void*))
   e42ac:	b508      	push	{r3, lr}
   e42ae:	4b02      	ldr	r3, [pc, #8]	; (e42b8 <network_on+0xc>)
   e42b0:	681b      	ldr	r3, [r3, #0]
   e42b2:	695b      	ldr	r3, [r3, #20]
   e42b4:	9301      	str	r3, [sp, #4]
   e42b6:	bd08      	pop	{r3, pc}
   e42b8:	00030240 	.word	0x00030240

000e42bc <network_off>:
DYNALIB_FN(6, system_net, network_off, void(network_handle_t, uint32_t, uint32_t, void*))
   e42bc:	b508      	push	{r3, lr}
   e42be:	4b02      	ldr	r3, [pc, #8]	; (e42c8 <network_off+0xc>)
   e42c0:	681b      	ldr	r3, [r3, #0]
   e42c2:	699b      	ldr	r3, [r3, #24]
   e42c4:	9301      	str	r3, [sp, #4]
   e42c6:	bd08      	pop	{r3, pc}
   e42c8:	00030240 	.word	0x00030240

000e42cc <network_listen>:
DYNALIB_FN(7, system_net, network_listen, void(network_handle_t, uint32_t, void*))
   e42cc:	b508      	push	{r3, lr}
   e42ce:	4b02      	ldr	r3, [pc, #8]	; (e42d8 <network_listen+0xc>)
   e42d0:	681b      	ldr	r3, [r3, #0]
   e42d2:	69db      	ldr	r3, [r3, #28]
   e42d4:	9301      	str	r3, [sp, #4]
   e42d6:	bd08      	pop	{r3, pc}
   e42d8:	00030240 	.word	0x00030240

000e42dc <network_listening>:
DYNALIB_FN(8, system_net, network_listening, bool(network_handle_t, uint32_t, void*))
   e42dc:	b508      	push	{r3, lr}
   e42de:	4b02      	ldr	r3, [pc, #8]	; (e42e8 <network_listening+0xc>)
   e42e0:	681b      	ldr	r3, [r3, #0]
   e42e2:	6a1b      	ldr	r3, [r3, #32]
   e42e4:	9301      	str	r3, [sp, #4]
   e42e6:	bd08      	pop	{r3, pc}
   e42e8:	00030240 	.word	0x00030240

000e42ec <network_set_listen_timeout>:
DYNALIB_FN(9, system_net, network_has_credentials, bool(network_handle_t, uint32_t, void*))
DYNALIB_FN(10, system_net, network_set_credentials, int(network_handle_t, uint32_t, NetworkCredentials*, void*))
DYNALIB_FN(11, system_net, network_clear_credentials, bool(network_handle_t, uint32_t, NetworkCredentials*, void*))
DYNALIB_FN(12, system_net, network_set_listen_timeout, void(network_handle_t, uint16_t, void*))
   e42ec:	b508      	push	{r3, lr}
   e42ee:	4b02      	ldr	r3, [pc, #8]	; (e42f8 <network_set_listen_timeout+0xc>)
   e42f0:	681b      	ldr	r3, [r3, #0]
   e42f2:	6b1b      	ldr	r3, [r3, #48]	; 0x30
   e42f4:	9301      	str	r3, [sp, #4]
   e42f6:	bd08      	pop	{r3, pc}
   e42f8:	00030240 	.word	0x00030240

000e42fc <network_get_listen_timeout>:
DYNALIB_FN(13, system_net, network_get_listen_timeout, uint16_t(network_handle_t, uint32_t, void*))
   e42fc:	b508      	push	{r3, lr}
   e42fe:	4b02      	ldr	r3, [pc, #8]	; (e4308 <network_get_listen_timeout+0xc>)
   e4300:	681b      	ldr	r3, [r3, #0]
   e4302:	6b5b      	ldr	r3, [r3, #52]	; 0x34
   e4304:	9301      	str	r3, [sp, #4]
   e4306:	bd08      	pop	{r3, pc}
   e4308:	00030240 	.word	0x00030240

000e430c <malloc>:
#include <assert.h>
#endif

DYNALIB_BEGIN(rt)

DYNALIB_FN(0, rt, malloc, void*(size_t))
   e430c:	b508      	push	{r3, lr}
   e430e:	4b02      	ldr	r3, [pc, #8]	; (e4318 <malloc+0xc>)
   e4310:	681b      	ldr	r3, [r3, #0]
   e4312:	681b      	ldr	r3, [r3, #0]
   e4314:	9301      	str	r3, [sp, #4]
   e4316:	bd08      	pop	{r3, pc}
   e4318:	0003021c 	.word	0x0003021c

000e431c <free>:
DYNALIB_FN(1, rt, free, void(void*))
   e431c:	b508      	push	{r3, lr}
   e431e:	4b02      	ldr	r3, [pc, #8]	; (e4328 <free+0xc>)
   e4320:	681b      	ldr	r3, [r3, #0]
   e4322:	685b      	ldr	r3, [r3, #4]
   e4324:	9301      	str	r3, [sp, #4]
   e4326:	bd08      	pop	{r3, pc}
   e4328:	0003021c 	.word	0x0003021c

000e432c <abort>:
DYNALIB_FN(6, rt, siscanf, int(const char*, const char*, ...))
DYNALIB_FN(7, rt, snprintf, int(char*, size_t, const char*, ...))
DYNALIB_FN(8, rt, sniprintf, int(char*, size_t, const char*, ...))
DYNALIB_FN(9, rt, vsnprintf, int(char*, size_t, const char*, va_list))
DYNALIB_FN(10, rt, vsniprintf, int(char*, size_t, const char*, va_list))
DYNALIB_FN(11, rt, abort, void(void))
   e432c:	b508      	push	{r3, lr}
   e432e:	4b02      	ldr	r3, [pc, #8]	; (e4338 <abort+0xc>)
   e4330:	681b      	ldr	r3, [r3, #0]
   e4332:	6adb      	ldr	r3, [r3, #44]	; 0x2c
   e4334:	9301      	str	r3, [sp, #4]
   e4336:	bd08      	pop	{r3, pc}
   e4338:	0003021c 	.word	0x0003021c

000e433c <__errno>:
DYNALIB_FN(12, rt, _malloc_r, void*(struct _reent*, size_t))
DYNALIB_FN(13, rt, _free_r, void(struct _reent*, void*))
DYNALIB_FN(14, rt, _realloc_r, void*(struct _reent*, void*, size_t))
DYNALIB_FN(15, rt, __errno, int*())
   e433c:	b508      	push	{r3, lr}
   e433e:	4b02      	ldr	r3, [pc, #8]	; (e4348 <__errno+0xc>)
   e4340:	681b      	ldr	r3, [r3, #0]
   e4342:	6bdb      	ldr	r3, [r3, #60]	; 0x3c
   e4344:	9301      	str	r3, [sp, #4]
   e4346:	bd08      	pop	{r3, pc}
   e4348:	0003021c 	.word	0x0003021c

000e434c <__assert_func>:
// on Gen 2 platforms without breaking inter-module dependencies.
// RT is currently being imported into system-part1 from system-part2,
// which is the reverse direction.

#if defined(DYNALIB_IMPORT) && !defined(RT_DYNALIB_NO_DEPENDENCY_BREAKING_IMPORTS)
DYNALIB_FN(16, rt, __assert_func, void(const char*, int, const char*, const char*))
   e434c:	b508      	push	{r3, lr}
   e434e:	4b02      	ldr	r3, [pc, #8]	; (e4358 <__assert_func+0xc>)
   e4350:	681b      	ldr	r3, [r3, #0]
   e4352:	6c1b      	ldr	r3, [r3, #64]	; 0x40
   e4354:	9301      	str	r3, [sp, #4]
   e4356:	bd08      	pop	{r3, pc}
   e4358:	0003021c 	.word	0x0003021c

000e435c <_GLOBAL__sub_I__ZN8particle3ble13WiringBleLock6mutex_E>:
    /**
     * Creates a shared mutex.
     */
    RecursiveMutex(os_mutex_recursive_t handle) : handle_(handle) {}

    RecursiveMutex() : handle_(nullptr)
   e435c:	4802      	ldr	r0, [pc, #8]	; (e4368 <_GLOBAL__sub_I__ZN8particle3ble13WiringBleLock6mutex_E+0xc>)
   e435e:	2300      	movs	r3, #0
   e4360:	6003      	str	r3, [r0, #0]
    {
        os_mutex_recursive_create(&handle_);
   e4362:	f7ff be2f 	b.w	e3fc4 <os_mutex_recursive_create>
   e4366:	bf00      	nop
   e4368:	2003e3a0 	.word	0x2003e3a0

000e436c <_ZN5spark13CellularClass5readyEv>:
        return network_listening(*this, 0, NULL);
    }

    bool ready()
    {
        return network_ready(*this, 0,  NULL);
   e436c:	2200      	movs	r2, #0
   e436e:	4611      	mov	r1, r2
   e4370:	6840      	ldr	r0, [r0, #4]
   e4372:	f7ff bf93 	b.w	e429c <network_ready>

000e4376 <_ZN5spark13CellularClass9listeningEv>:
    uint16_t getListenTimeout(void) {
        return network_get_listen_timeout(*this, 0, NULL);
    }

    bool listening(void) {
        return network_listening(*this, 0, NULL);
   e4376:	2200      	movs	r2, #0
   e4378:	4611      	mov	r1, r2
   e437a:	6840      	ldr	r0, [r0, #4]
   e437c:	f7ff bfae 	b.w	e42dc <network_listening>

000e4380 <_ZN5spark13CellularClass16getListenTimeoutEv>:
    void setListenTimeout(uint16_t timeout) {
        network_set_listen_timeout(*this, timeout, NULL);
    }

    uint16_t getListenTimeout(void) {
        return network_get_listen_timeout(*this, 0, NULL);
   e4380:	2200      	movs	r2, #0
   e4382:	4611      	mov	r1, r2
   e4384:	6840      	ldr	r0, [r0, #4]
   e4386:	f7ff bfb9 	b.w	e42fc <network_get_listen_timeout>

000e438a <_ZN5spark13CellularClass16setListenTimeoutEt>:
    void listen(bool begin=true) {
        network_listen(*this, begin ? 0 : 1, NULL);
    }

    void setListenTimeout(uint16_t timeout) {
        network_set_listen_timeout(*this, timeout, NULL);
   e438a:	2200      	movs	r2, #0
   e438c:	6840      	ldr	r0, [r0, #4]
   e438e:	f7ff bfad 	b.w	e42ec <network_set_listen_timeout>

000e4392 <_ZN5spark13CellularClass6listenEb>:
        cellular_credentials_clear(nullptr);
    }
#endif // HAL_PLATFORM_MESH

    void listen(bool begin=true) {
        network_listen(*this, begin ? 0 : 1, NULL);
   e4392:	2200      	movs	r2, #0
   e4394:	f081 0101 	eor.w	r1, r1, #1
   e4398:	6840      	ldr	r0, [r0, #4]
   e439a:	f7ff bf97 	b.w	e42cc <network_listen>

000e439e <_ZN5spark13CellularClass3offEv>:
    }
    void on() {
        network_on(*this, 0, 0, NULL);
    }
    void off() {
        network_off(*this, 0, 0, NULL);
   e439e:	2300      	movs	r3, #0
   e43a0:	461a      	mov	r2, r3
   e43a2:	4619      	mov	r1, r3
   e43a4:	6840      	ldr	r0, [r0, #4]
   e43a6:	f7ff bf89 	b.w	e42bc <network_off>

000e43aa <_ZN5spark13CellularClass2onEv>:

    IPAddress localIP() {
        return IPAddress(((CellularConfig*)network_config(*this, 0, NULL))->nw.aucIP);
    }
    void on() {
        network_on(*this, 0, 0, NULL);
   e43aa:	2300      	movs	r3, #0
   e43ac:	461a      	mov	r2, r3
   e43ae:	4619      	mov	r1, r3
   e43b0:	6840      	ldr	r0, [r0, #4]
   e43b2:	f7ff bf7b 	b.w	e42ac <network_on>

000e43b6 <_ZN5spark13CellularClass10connectingEv>:
    }
    void connect(unsigned flags=0) {
        network_connect(*this, flags, 0, NULL);
    }
    bool connecting(void) {
        return network_connecting(*this, 0, NULL);
   e43b6:	2200      	movs	r2, #0
   e43b8:	4611      	mov	r1, r2
   e43ba:	6840      	ldr	r0, [r0, #4]
   e43bc:	f7ff bf5e 	b.w	e427c <network_connecting>

000e43c0 <_ZN5spark13CellularClass10disconnectEv>:
    }

    void disconnect() {
        network_disconnect(*this, NETWORK_DISCONNECT_REASON_USER, NULL);
   e43c0:	2200      	movs	r2, #0
   e43c2:	2102      	movs	r1, #2
   e43c4:	6840      	ldr	r0, [r0, #4]
   e43c6:	f7ff bf61 	b.w	e428c <network_disconnect>

000e43ca <_ZN5spark13CellularClass7connectEj>:
    }
    void off() {
        network_off(*this, 0, 0, NULL);
    }
    void connect(unsigned flags=0) {
        network_connect(*this, flags, 0, NULL);
   e43ca:	2300      	movs	r3, #0
   e43cc:	461a      	mov	r2, r3
   e43ce:	6840      	ldr	r0, [r0, #4]
   e43d0:	f7ff bf4c 	b.w	e426c <network_connect>

000e43d4 <_GLOBAL__sub_I__ZN5spark13CellularClass4RSSIEv>:
        return (band_get.ok = true);
    }

    CellularClass Cellular;
    // NetworkClass& Network = Cellular;
}
   e43d4:	b538      	push	{r3, r4, r5, lr}
    int tx_total;
    int rx_total;

    CellularDataHal()
    {
        memset(this, 0, sizeof(*this));
   e43d6:	4c08      	ldr	r4, [pc, #32]	; (e43f8 <_GLOBAL__sub_I__ZN5spark13CellularClass4RSSIEv+0x24>)
   e43d8:	2528      	movs	r5, #40	; 0x28
   e43da:	462a      	mov	r2, r5
   e43dc:	2100      	movs	r1, #0
   e43de:	4620      	mov	r0, r4
   e43e0:	f003 fa0a 	bl	e77f8 <memset>
        cid = -1;
   e43e4:	f04f 33ff 	mov.w	r3, #4294967295	; 0xffffffff
   e43e8:	6063      	str	r3, [r4, #4]
    static NetworkClass& from(network_interface_t nif);

    virtual IPAddress resolve(const char* name);

    explicit NetworkClass(network_interface_t iface)
            : iface_(iface) {
   e43ea:	4b04      	ldr	r3, [pc, #16]	; (e43fc <_GLOBAL__sub_I__ZN5spark13CellularClass4RSSIEv+0x28>)
        size = sizeof(*this);
   e43ec:	8025      	strh	r5, [r4, #0]
   e43ee:	2204      	movs	r2, #4
   e43f0:	605a      	str	r2, [r3, #4]

class CellularClass : public NetworkClass
{
public:
    CellularClass() :
            NetworkClass(NETWORK_INTERFACE_CELLULAR) {
   e43f2:	4a03      	ldr	r2, [pc, #12]	; (e4400 <_GLOBAL__sub_I__ZN5spark13CellularClass4RSSIEv+0x2c>)
   e43f4:	601a      	str	r2, [r3, #0]
   e43f6:	bd38      	pop	{r3, r4, r5, pc}
   e43f8:	2003e3ac 	.word	0x2003e3ac
   e43fc:	2003e3a4 	.word	0x2003e3a4
   e4400:	000eb2d0 	.word	0x000eb2d0

000e4404 <_ZNSt14_Function_baseD1Ev>:
	}
      };

    _Function_base() : _M_manager(nullptr) { }

    ~_Function_base()
   e4404:	b510      	push	{r4, lr}
    {
      if (_M_manager)
   e4406:	6883      	ldr	r3, [r0, #8]
	}
      };

    _Function_base() : _M_manager(nullptr) { }

    ~_Function_base()
   e4408:	4604      	mov	r4, r0
    {
      if (_M_manager)
   e440a:	b113      	cbz	r3, e4412 <_ZNSt14_Function_baseD1Ev+0xe>
	_M_manager(_M_functor, _M_functor, __destroy_functor);
   e440c:	2203      	movs	r2, #3
   e440e:	4601      	mov	r1, r0
   e4410:	4798      	blx	r3
    }
   e4412:	4620      	mov	r0, r4
   e4414:	bd10      	pop	{r4, pc}

000e4416 <_ZN5spark13EthernetClass9listeningEv>:
    uint16_t getListenTimeout(void) {
        return network_get_listen_timeout(*this, 0, NULL);
    }

    bool listening(void) {
        return network_listening(*this, 0, NULL);
   e4416:	2200      	movs	r2, #0
   e4418:	4611      	mov	r1, r2
   e441a:	6840      	ldr	r0, [r0, #4]
   e441c:	f7ff bf5e 	b.w	e42dc <network_listening>

000e4420 <_ZN5spark13EthernetClass16getListenTimeoutEv>:
    void setListenTimeout(uint16_t timeout) {
        network_set_listen_timeout(*this, timeout, NULL);
    }

    uint16_t getListenTimeout(void) {
        return network_get_listen_timeout(*this, 0, NULL);
   e4420:	2200      	movs	r2, #0
   e4422:	4611      	mov	r1, r2
   e4424:	6840      	ldr	r0, [r0, #4]
   e4426:	f7ff bf69 	b.w	e42fc <network_get_listen_timeout>

000e442a <_ZN5spark13EthernetClass16setListenTimeoutEt>:
    void listen(bool begin=true) {
        network_listen(*this, begin ? 0 : 1, NULL);
    }

    void setListenTimeout(uint16_t timeout) {
        network_set_listen_timeout(*this, timeout, NULL);
   e442a:	2200      	movs	r2, #0
   e442c:	6840      	ldr	r0, [r0, #4]
   e442e:	f7ff bf5d 	b.w	e42ec <network_set_listen_timeout>

000e4432 <_ZN5spark13EthernetClass6listenEb>:
    void disconnect() {
        network_disconnect(*this, NETWORK_DISCONNECT_REASON_USER, NULL);
    }

    void listen(bool begin=true) {
        network_listen(*this, begin ? 0 : 1, NULL);
   e4432:	2200      	movs	r2, #0
   e4434:	f081 0101 	eor.w	r1, r1, #1
   e4438:	6840      	ldr	r0, [r0, #4]
   e443a:	f7ff bf47 	b.w	e42cc <network_listen>

000e443e <_ZN5spark13EthernetClass3offEv>:
    void on() {
        network_on(*this, 0, 0, NULL);
    }

    void off() {
        network_off(*this, 0, 0, NULL);
   e443e:	2300      	movs	r3, #0
   e4440:	461a      	mov	r2, r3
   e4442:	4619      	mov	r1, r3
   e4444:	6840      	ldr	r0, [r0, #4]
   e4446:	f7ff bf39 	b.w	e42bc <network_off>

000e444a <_ZN5spark13EthernetClass2onEv>:
    EthernetClass() :
            NetworkClass(NETWORK_INTERFACE_ETHERNET) {
    }

    void on() {
        network_on(*this, 0, 0, NULL);
   e444a:	2300      	movs	r3, #0
   e444c:	461a      	mov	r2, r3
   e444e:	4619      	mov	r1, r3
   e4450:	6840      	ldr	r0, [r0, #4]
   e4452:	f7ff bf2b 	b.w	e42ac <network_on>

000e4456 <_ZN5spark13EthernetClass5readyEv>:
    bool listening(void) {
        return network_listening(*this, 0, NULL);
    }

    bool ready() {
        return network_ready(*this, 0,  NULL);
   e4456:	2200      	movs	r2, #0
   e4458:	4611      	mov	r1, r2
   e445a:	6840      	ldr	r0, [r0, #4]
   e445c:	f7ff bf1e 	b.w	e429c <network_ready>

000e4460 <_ZN5spark13EthernetClass10connectingEv>:
    void connect(unsigned flags=0) {
        network_connect(*this, flags, 0, NULL);
    }

    bool connecting(void) {
        return network_connecting(*this, 0, NULL);
   e4460:	2200      	movs	r2, #0
   e4462:	4611      	mov	r1, r2
   e4464:	6840      	ldr	r0, [r0, #4]
   e4466:	f7ff bf09 	b.w	e427c <network_connecting>

000e446a <_ZN5spark13EthernetClass10disconnectEv>:
    }

    void disconnect() {
        network_disconnect(*this, NETWORK_DISCONNECT_REASON_USER, NULL);
   e446a:	2200      	movs	r2, #0
   e446c:	2102      	movs	r1, #2
   e446e:	6840      	ldr	r0, [r0, #4]
   e4470:	f7ff bf0c 	b.w	e428c <network_disconnect>

000e4474 <_ZN5spark13EthernetClass7connectEj>:
    void off() {
        network_off(*this, 0, 0, NULL);
    }

    void connect(unsigned flags=0) {
        network_connect(*this, flags, 0, NULL);
   e4474:	2300      	movs	r3, #0
   e4476:	461a      	mov	r2, r3
   e4478:	6840      	ldr	r0, [r0, #4]
   e447a:	f7ff bef7 	b.w	e426c <network_connect>
	...

000e4480 <_GLOBAL__sub_I__ZN5spark8EthernetE>:
   e4480:	4b02      	ldr	r3, [pc, #8]	; (e448c <_GLOBAL__sub_I__ZN5spark8EthernetE+0xc>)
   e4482:	2203      	movs	r2, #3
   e4484:	605a      	str	r2, [r3, #4]
    }

class EthernetClass : public NetworkClass {
public:
    EthernetClass() :
            NetworkClass(NETWORK_INTERFACE_ETHERNET) {
   e4486:	4a02      	ldr	r2, [pc, #8]	; (e4490 <_GLOBAL__sub_I__ZN5spark8EthernetE+0x10>)
   e4488:	601a      	str	r2, [r3, #0]
   e448a:	4770      	bx	lr
   e448c:	2003e3d4 	.word	0x2003e3d4
   e4490:	000eb304 	.word	0x000eb304

000e4494 <_ZN7TwoWireD1Ev>:
private:
  HAL_I2C_Interface _i2c;

public:
  TwoWire(HAL_I2C_Interface i2c);
  virtual ~TwoWire() {};
   e4494:	4770      	bx	lr

000e4496 <_ZN7TwoWire5writeEPKhj>:

// must be called in:
// slave tx event callback
// or after beginTransmission(address)
size_t TwoWire::write(const uint8_t *data, size_t quantity)
{
   e4496:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
   e4498:	4606      	mov	r6, r0
   e449a:	4615      	mov	r5, r2
   e449c:	460c      	mov	r4, r1
   e449e:	188f      	adds	r7, r1, r2
  // in master/slave transmitter mode
  for(size_t i = 0; i < quantity; ++i)
   e44a0:	42bc      	cmp	r4, r7
   e44a2:	d006      	beq.n	e44b2 <_ZN7TwoWire5writeEPKhj+0x1c>
  {
    write(data[i]);
   e44a4:	6833      	ldr	r3, [r6, #0]
   e44a6:	f814 1b01 	ldrb.w	r1, [r4], #1
   e44aa:	689b      	ldr	r3, [r3, #8]
   e44ac:	4630      	mov	r0, r6
   e44ae:	4798      	blx	r3
// slave tx event callback
// or after beginTransmission(address)
size_t TwoWire::write(const uint8_t *data, size_t quantity)
{
  // in master/slave transmitter mode
  for(size_t i = 0; i < quantity; ++i)
   e44b0:	e7f6      	b.n	e44a0 <_ZN7TwoWire5writeEPKhj+0xa>
  {
    write(data[i]);
  }

  return quantity;
}
   e44b2:	4628      	mov	r0, r5
   e44b4:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}

000e44b6 <_ZN7TwoWire5writeEh>:
// must be called in:
// slave tx event callback
// or after beginTransmission(address)
size_t TwoWire::write(uint8_t data)
{
  return HAL_I2C_Write_Data(_i2c, data, NULL);
   e44b6:	2200      	movs	r2, #0
   e44b8:	7c00      	ldrb	r0, [r0, #16]
   e44ba:	f7ff bdd3 	b.w	e4064 <HAL_I2C_Write_Data>

000e44be <_ZN7TwoWire9availableEv>:
// must be called in:
// slave rx event callback
// or after requestFrom(address, numBytes)
int TwoWire::available(void)
{
  return HAL_I2C_Available_Data(_i2c, NULL);
   e44be:	2100      	movs	r1, #0
   e44c0:	7c00      	ldrb	r0, [r0, #16]
   e44c2:	f7ff bdd7 	b.w	e4074 <HAL_I2C_Available_Data>

000e44c6 <_ZN7TwoWire4readEv>:
// must be called in:
// slave rx event callback
// or after requestFrom(address, numBytes)
int TwoWire::read(void)
{
  return HAL_I2C_Read_Data(_i2c, NULL);
   e44c6:	2100      	movs	r1, #0
   e44c8:	7c00      	ldrb	r0, [r0, #16]
   e44ca:	f7ff bddb 	b.w	e4084 <HAL_I2C_Read_Data>

000e44ce <_ZN7TwoWire4peekEv>:
// must be called in:
// slave rx event callback
// or after requestFrom(address, numBytes)
int TwoWire::peek(void)
{
  return HAL_I2C_Peek_Data(_i2c, NULL);
   e44ce:	2100      	movs	r1, #0
   e44d0:	7c00      	ldrb	r0, [r0, #16]
   e44d2:	f7ff bddf 	b.w	e4094 <HAL_I2C_Peek_Data>

000e44d6 <_ZN7TwoWire5flushEv>:
}

void TwoWire::flush(void)
{
  HAL_I2C_Flush_Data(_i2c, NULL);
   e44d6:	2100      	movs	r1, #0
   e44d8:	7c00      	ldrb	r0, [r0, #16]
   e44da:	f7ff bde3 	b.w	e40a4 <HAL_I2C_Flush_Data>

000e44de <_ZN7TwoWireD0Ev>:
   e44de:	b510      	push	{r4, lr}
   e44e0:	2114      	movs	r1, #20
   e44e2:	4604      	mov	r4, r0
   e44e4:	f000 fca5 	bl	e4e32 <_ZdlPvj>
   e44e8:	4620      	mov	r0, r4
   e44ea:	bd10      	pop	{r4, pc}

000e44ec <_ZN7TwoWireC1E17HAL_I2C_Interface>:
#include "i2c_hal.h"
#include "spark_wiring_thread.h"

// Constructors ////////////////////////////////////////////////////////////////

TwoWire::TwoWire(HAL_I2C_Interface i2c)
   e44ec:	b510      	push	{r4, lr}
   e44ee:	4604      	mov	r4, r0
    virtual int available() = 0;
    virtual int read() = 0;
    virtual int peek() = 0;
    virtual void flush() = 0;

    Stream() {_timeout=1000;}
   e44f0:	f44f 737a 	mov.w	r3, #1000	; 0x3e8
   e44f4:	4608      	mov	r0, r1
   e44f6:	60a3      	str	r3, [r4, #8]
  protected:
    void setWriteError(int err = 1) { write_error = err; }
    size_t printf_impl(bool newline, const char* format, ...);

  public:
    Print() : write_error(0) {}
   e44f8:	2100      	movs	r1, #0
   e44fa:	4b04      	ldr	r3, [pc, #16]	; (e450c <_ZN7TwoWireC1E17HAL_I2C_Interface+0x20>)
{
  _i2c = i2c;
   e44fc:	7420      	strb	r0, [r4, #16]
   e44fe:	6061      	str	r1, [r4, #4]
#include "i2c_hal.h"
#include "spark_wiring_thread.h"

// Constructors ////////////////////////////////////////////////////////////////

TwoWire::TwoWire(HAL_I2C_Interface i2c)
   e4500:	6023      	str	r3, [r4, #0]
{
  _i2c = i2c;
  HAL_I2C_Init(_i2c, NULL);
   e4502:	f7ff fddf 	bl	e40c4 <HAL_I2C_Init>

}
   e4506:	4620      	mov	r0, r4
   e4508:	bd10      	pop	{r4, pc}
   e450a:	bf00      	nop
   e450c:	000eb338 	.word	0x000eb338

000e4510 <_ZN7TwoWire9isEnabledEv>:
  HAL_I2C_Set_Callback_On_Request(_i2c, function, NULL);
}

bool TwoWire::isEnabled()
{
  return HAL_I2C_Is_Enabled(_i2c, NULL);
   e4510:	2100      	movs	r1, #0
   e4512:	7c00      	ldrb	r0, [r0, #16]
   e4514:	f7ff bdce 	b.w	e40b4 <HAL_I2C_Is_Enabled>

000e4518 <_ZN9IPAddressD1Ev>:
    IPAddress(uint8_t first_octet, uint8_t second_octet, uint8_t third_octet, uint8_t fourth_octet);
    IPAddress(uint32_t address);
    IPAddress(const uint8_t* address);
    IPAddress(const HAL_IPAddress& address);

    virtual ~IPAddress() {}
   e4518:	4770      	bx	lr

000e451a <_ZN9IPAddressD0Ev>:
   e451a:	b510      	push	{r4, lr}
   e451c:	2118      	movs	r1, #24
   e451e:	4604      	mov	r4, r0
   e4520:	f000 fc87 	bl	e4e32 <_ZdlPvj>
   e4524:	4620      	mov	r0, r4
   e4526:	bd10      	pop	{r4, pc}

000e4528 <_ZNK9IPAddress7printToER5Print>:
#endif // Wiring_IPv6
	return address.ipv4==that.address.ipv4;
}

size_t IPAddress::printTo(Print& p) const
{
   e4528:	b5f0      	push	{r4, r5, r6, r7, lr}
#if Wiring_IPv6
#if HAL_USE_INET_HAL_POSIX
	if (address.v==6) {
   e452a:	7d03      	ldrb	r3, [r0, #20]
   e452c:	2b06      	cmp	r3, #6
#endif // Wiring_IPv6
	return address.ipv4==that.address.ipv4;
}

size_t IPAddress::printTo(Print& p) const
{
   e452e:	b08d      	sub	sp, #52	; 0x34
   e4530:	460e      	mov	r6, r1
   e4532:	f100 0704 	add.w	r7, r0, #4
   e4536:	f04f 0400 	mov.w	r4, #0
#if Wiring_IPv6
#if HAL_USE_INET_HAL_POSIX
	if (address.v==6) {
   e453a:	d002      	beq.n	e4542 <_ZNK9IPAddress7printToER5Print+0x1a>
   e453c:	f100 0508 	add.w	r5, r0, #8
   e4540:	e018      	b.n	e4574 <_ZNK9IPAddress7printToER5Print+0x4c>
		char buf[INET6_ADDRSTRLEN+1];
		buf[0] = 0;
   e4542:	ad0c      	add	r5, sp, #48	; 0x30
		inet_inet_ntop(AF_INET6, address.ipv6, buf, sizeof(buf));
   e4544:	4639      	mov	r1, r7
{
#if Wiring_IPv6
#if HAL_USE_INET_HAL_POSIX
	if (address.v==6) {
		char buf[INET6_ADDRSTRLEN+1];
		buf[0] = 0;
   e4546:	f805 4d30 	strb.w	r4, [r5, #-48]!
		inet_inet_ntop(AF_INET6, address.ipv6, buf, sizeof(buf));
   e454a:	232f      	movs	r3, #47	; 0x2f
   e454c:	462a      	mov	r2, r5
   e454e:	200a      	movs	r0, #10
   e4550:	f7ff fdc0 	bl	e40d4 <inet_inet_ntop>
    void clearWriteError() { setWriteError(0); }

    virtual size_t write(uint8_t) = 0;
    size_t write(const char *str) {
      if (str == NULL) return 0;
      return write((const uint8_t *)str, strlen(str));
   e4554:	4628      	mov	r0, r5
   e4556:	f003 f989 	bl	e786c <strlen>
   e455a:	6833      	ldr	r3, [r6, #0]
   e455c:	4602      	mov	r2, r0
   e455e:	68db      	ldr	r3, [r3, #12]
   e4560:	4629      	mov	r1, r5
   e4562:	4630      	mov	r0, r6
   e4564:	4798      	blx	r3
   e4566:	e00f      	b.n	e4588 <_ZNK9IPAddress7printToER5Print+0x60>
#endif // HAL_USE_INET_HAL_POSIX
#endif // Wiring_IPv6
    size_t n = 0;
    for (int i = 0; i < 4; i++)
    {
        if (n)
   e4568:	b124      	cbz	r4, e4574 <_ZNK9IPAddress7printToER5Print+0x4c>
            n += p.print('.');
   e456a:	212e      	movs	r1, #46	; 0x2e
   e456c:	4630      	mov	r0, r6
   e456e:	f000 f9cd 	bl	e490c <_ZN5Print5printEc>
   e4572:	4404      	add	r4, r0
        n += p.print((*this)[i], DEC);
   e4574:	f815 1d01 	ldrb.w	r1, [r5, #-1]!
   e4578:	220a      	movs	r2, #10
   e457a:	4630      	mov	r0, r6
   e457c:	f000 f9f6 	bl	e496c <_ZN5Print5printEhi>
#else
#pragma message "HAL_USE_INET_HAL_POSIX is required for IPv6 support in IPAddress::printTo()"
#endif // HAL_USE_INET_HAL_POSIX
#endif // Wiring_IPv6
    size_t n = 0;
    for (int i = 0; i < 4; i++)
   e4580:	42bd      	cmp	r5, r7
    {
        if (n)
            n += p.print('.');
        n += p.print((*this)[i], DEC);
   e4582:	4404      	add	r4, r0
#else
#pragma message "HAL_USE_INET_HAL_POSIX is required for IPv6 support in IPAddress::printTo()"
#endif // HAL_USE_INET_HAL_POSIX
#endif // Wiring_IPv6
    size_t n = 0;
    for (int i = 0; i < 4; i++)
   e4584:	d1f0      	bne.n	e4568 <_ZNK9IPAddress7printToER5Print+0x40>
    {
        if (n)
            n += p.print('.');
        n += p.print((*this)[i], DEC);
   e4586:	4620      	mov	r0, r4
    }
    return n;
}
   e4588:	b00d      	add	sp, #52	; 0x34
   e458a:	bdf0      	pop	{r4, r5, r6, r7, pc}

000e458c <_ZN9IPAddressC1Ev>:

#if HAL_USE_INET_HAL_POSIX
#include <arpa/inet.h>
#endif // HAL_USE_INET_HAL_POSIX

IPAddress::IPAddress()
   e458c:	b510      	push	{r4, lr}
   e458e:	4b05      	ldr	r3, [pc, #20]	; (e45a4 <_ZN9IPAddressC1Ev+0x18>)
   e4590:	4604      	mov	r4, r0
        return address;
    }

    virtual size_t printTo(Print& p) const;

    void clear() { memset(&address, 0, sizeof (address)); }
   e4592:	2211      	movs	r2, #17
   e4594:	f840 3b04 	str.w	r3, [r0], #4
   e4598:	2100      	movs	r1, #0
   e459a:	f003 f92d 	bl	e77f8 <memset>
{
    clear();
}
   e459e:	4620      	mov	r0, r4
   e45a0:	bd10      	pop	{r4, pc}
   e45a2:	bf00      	nop
   e45a4:	000eb360 	.word	0x000eb360

000e45a8 <_ZN9IPAddressC1ERK16_HAL_IPAddress_t>:

IPAddress::IPAddress(const HAL_IPAddress& address)
   e45a8:	4603      	mov	r3, r0
   e45aa:	4a07      	ldr	r2, [pc, #28]	; (e45c8 <_ZN9IPAddressC1ERK16_HAL_IPAddress_t+0x20>)
   e45ac:	b510      	push	{r4, lr}
   e45ae:	f843 2b04 	str.w	r2, [r3], #4
{
    memcpy(&this->address, &address, sizeof(address));
   e45b2:	f101 0210 	add.w	r2, r1, #16
   e45b6:	f851 4b04 	ldr.w	r4, [r1], #4
   e45ba:	f843 4b04 	str.w	r4, [r3], #4
   e45be:	4291      	cmp	r1, r2
   e45c0:	d1f9      	bne.n	e45b6 <_ZN9IPAddressC1ERK16_HAL_IPAddress_t+0xe>
   e45c2:	780a      	ldrb	r2, [r1, #0]
   e45c4:	701a      	strb	r2, [r3, #0]
}
   e45c6:	bd10      	pop	{r4, pc}
   e45c8:	000eb360 	.word	0x000eb360

000e45cc <_ZN9IPAddress8set_ipv4Ehhhh>:
    return address.ipv4!=0;
#endif
}

void IPAddress::set_ipv4(uint8_t b0, uint8_t b1, uint8_t b2, uint8_t b3)
{
   e45cc:	b510      	push	{r4, lr}
    address.ipv4 = b0<<24 | b1 << 16 | b2 << 8 | b3;
   e45ce:	f89d 4008 	ldrb.w	r4, [sp, #8]
   e45d2:	ea44 2303 	orr.w	r3, r4, r3, lsl #8
   e45d6:	ea43 4202 	orr.w	r2, r3, r2, lsl #16
   e45da:	ea42 6101 	orr.w	r1, r2, r1, lsl #24
        return &address;
    }

    inline void setVersion(uint8_t version) {
#if HAL_IPv6
        address.v = version;
   e45de:	2304      	movs	r3, #4
   e45e0:	6041      	str	r1, [r0, #4]
   e45e2:	7503      	strb	r3, [r0, #20]
   e45e4:	bd10      	pop	{r4, pc}

000e45e6 <_ZN9IPAddressaSEPKh>:
    setVersion(4);
}

IPAddress& IPAddress::operator=(const uint8_t* address)
{
   e45e6:	b537      	push	{r0, r1, r2, r4, r5, lr}
    set_ipv4(address[0], address[1], address[2], address[3]);
   e45e8:	780d      	ldrb	r5, [r1, #0]
   e45ea:	788b      	ldrb	r3, [r1, #2]
   e45ec:	784a      	ldrb	r2, [r1, #1]
   e45ee:	78c9      	ldrb	r1, [r1, #3]
   e45f0:	9100      	str	r1, [sp, #0]
   e45f2:	4629      	mov	r1, r5
   e45f4:	f7ff ffea 	bl	e45cc <_ZN9IPAddress8set_ipv4Ehhhh>
    return *this;
}
   e45f8:	b003      	add	sp, #12
   e45fa:	bd30      	pop	{r4, r5, pc}

000e45fc <_GLOBAL__sub_I__ZN5spark3LogE>:
    // This handler doesn't support direct logging
}

// spark::Logger
inline spark::Logger::Logger(const char *name) :
        name_(name) {
   e45fc:	4b01      	ldr	r3, [pc, #4]	; (e4604 <_GLOBAL__sub_I__ZN5spark3LogE+0x8>)
   e45fe:	4a02      	ldr	r2, [pc, #8]	; (e4608 <_GLOBAL__sub_I__ZN5spark3LogE+0xc>)
   e4600:	601a      	str	r2, [r3, #0]
   e4602:	4770      	bx	lr
   e4604:	2003e3dc 	.word	0x2003e3dc
   e4608:	000eb3da 	.word	0x000eb3da

000e460c <_ZN5spark9MeshClass9listeningEv>:
    uint16_t getListenTimeout(void) {
        return network_get_listen_timeout(*this, 0, NULL);
    }

    bool listening(void) {
        return network_listening(*this, 0, NULL);
   e460c:	2200      	movs	r2, #0
   e460e:	4611      	mov	r1, r2
   e4610:	6840      	ldr	r0, [r0, #4]
   e4612:	f7ff be63 	b.w	e42dc <network_listening>

000e4616 <_ZN5spark9MeshClass16getListenTimeoutEv>:
    void setListenTimeout(uint16_t timeout) {
        network_set_listen_timeout(*this, timeout, NULL);
    }

    uint16_t getListenTimeout(void) {
        return network_get_listen_timeout(*this, 0, NULL);
   e4616:	2200      	movs	r2, #0
   e4618:	4611      	mov	r1, r2
   e461a:	6840      	ldr	r0, [r0, #4]
   e461c:	f7ff be6e 	b.w	e42fc <network_get_listen_timeout>

000e4620 <_ZN5spark9MeshClass16setListenTimeoutEt>:
    void listen(bool begin=true) {
        network_listen(*this, begin ? 0 : 1, NULL);
    }

    void setListenTimeout(uint16_t timeout) {
        network_set_listen_timeout(*this, timeout, NULL);
   e4620:	2200      	movs	r2, #0
   e4622:	6840      	ldr	r0, [r0, #4]
   e4624:	f7ff be62 	b.w	e42ec <network_set_listen_timeout>

000e4628 <_ZN5spark9MeshClass6listenEb>:
    void disconnect() {
        network_disconnect(*this, NETWORK_DISCONNECT_REASON_USER, NULL);
    }

    void listen(bool begin=true) {
        network_listen(*this, begin ? 0 : 1, NULL);
   e4628:	2200      	movs	r2, #0
   e462a:	f081 0101 	eor.w	r1, r1, #1
   e462e:	6840      	ldr	r0, [r0, #4]
   e4630:	f7ff be4c 	b.w	e42cc <network_listen>

000e4634 <_ZN5spark9MeshClass3offEv>:
    void on() {
        network_on(*this, 0, 0, NULL);
    }

    void off() {
        network_off(*this, 1, 0, NULL);
   e4634:	2300      	movs	r3, #0
   e4636:	461a      	mov	r2, r3
   e4638:	2101      	movs	r1, #1
   e463a:	6840      	ldr	r0, [r0, #4]
   e463c:	f7ff be3e 	b.w	e42bc <network_off>

000e4640 <_ZN5spark9MeshClass2onEv>:
    MeshClass() :
            NetworkClass(NETWORK_INTERFACE_MESH) {
    }

    void on() {
        network_on(*this, 0, 0, NULL);
   e4640:	2300      	movs	r3, #0
   e4642:	461a      	mov	r2, r3
   e4644:	4619      	mov	r1, r3
   e4646:	6840      	ldr	r0, [r0, #4]
   e4648:	f7ff be30 	b.w	e42ac <network_on>

000e464c <_ZN5spark9MeshClass5readyEv>:
    bool listening(void) {
        return network_listening(*this, 0, NULL);
    }

    bool ready() {
        return network_ready(*this, 0,  NULL);
   e464c:	2200      	movs	r2, #0
   e464e:	4611      	mov	r1, r2
   e4650:	6840      	ldr	r0, [r0, #4]
   e4652:	f7ff be23 	b.w	e429c <network_ready>

000e4656 <_ZN5spark9MeshClass10connectingEv>:
    void connect(unsigned flags=0) {
        network_connect(*this, flags, 0, NULL);
    }

    bool connecting(void) {
        return network_connecting(*this, 0, NULL);
   e4656:	2200      	movs	r2, #0
   e4658:	4611      	mov	r1, r2
   e465a:	6840      	ldr	r0, [r0, #4]
   e465c:	f7ff be0e 	b.w	e427c <network_connecting>

000e4660 <_ZN5spark9MeshClass10disconnectEv>:
    }

    void disconnect() {
        network_disconnect(*this, NETWORK_DISCONNECT_REASON_USER, NULL);
   e4660:	2200      	movs	r2, #0
   e4662:	2102      	movs	r1, #2
   e4664:	6840      	ldr	r0, [r0, #4]
   e4666:	f7ff be11 	b.w	e428c <network_disconnect>

000e466a <_ZN5spark9MeshClass7connectEj>:
    void off() {
        network_off(*this, 1, 0, NULL);
    }

    void connect(unsigned flags=0) {
        network_connect(*this, flags, 0, NULL);
   e466a:	2300      	movs	r3, #0
   e466c:	461a      	mov	r2, r3
   e466e:	6840      	ldr	r0, [r0, #4]
   e4670:	f7ff bdfc 	b.w	e426c <network_connect>

000e4674 <_ZNKSt14default_deleteIN6Thread4DataEEclEPS1_.isra.6>:
	       enable_if<is_convertible<_Up*, _Tp*>::value>::type>
        default_delete(const default_delete<_Up>&) noexcept { }

      /// Calls @c delete @p __ptr
      void
      operator()(_Tp* __ptr) const
   e4674:	b538      	push	{r3, r4, r5, lr}
      {
	static_assert(!is_void<_Tp>::value,
		      "can't delete pointer to incomplete type");
	static_assert(sizeof(_Tp)>0,
		      "can't delete pointer to incomplete type");
	delete __ptr;
   e4676:	4605      	mov	r5, r0
   e4678:	b188      	cbz	r0, e469e <_ZNKSt14default_deleteIN6Thread4DataEEclEPS1_.isra.6+0x2a>

      /// Destructor, invokes the deleter if the stored pointer is not null.
      ~unique_ptr() noexcept
      {
	auto& __ptr = std::get<0>(_M_t);
	if (__ptr != nullptr)
   e467a:	6804      	ldr	r4, [r0, #0]
   e467c:	b14c      	cbz	r4, e4692 <_ZNKSt14default_deleteIN6Thread4DataEEclEPS1_.isra.6+0x1e>

    _Function_base() : _M_manager(nullptr) { }

    ~_Function_base()
    {
      if (_M_manager)
   e467e:	68a3      	ldr	r3, [r4, #8]
   e4680:	b11b      	cbz	r3, e468a <_ZNKSt14default_deleteIN6Thread4DataEEclEPS1_.isra.6+0x16>
	_M_manager(_M_functor, _M_functor, __destroy_functor);
   e4682:	2203      	movs	r2, #3
   e4684:	4621      	mov	r1, r4
   e4686:	4620      	mov	r0, r4
   e4688:	4798      	blx	r3
      {
	static_assert(!is_void<_Tp>::value,
		      "can't delete pointer to incomplete type");
	static_assert(sizeof(_Tp)>0,
		      "can't delete pointer to incomplete type");
	delete __ptr;
   e468a:	2110      	movs	r1, #16
   e468c:	4620      	mov	r0, r4
   e468e:	f000 fbd0 	bl	e4e32 <_ZdlPvj>
   e4692:	4628      	mov	r0, r5
   e4694:	2114      	movs	r1, #20
      }
   e4696:	e8bd 4038 	ldmia.w	sp!, {r3, r4, r5, lr}
      {
	static_assert(!is_void<_Tp>::value,
		      "can't delete pointer to incomplete type");
	static_assert(sizeof(_Tp)>0,
		      "can't delete pointer to incomplete type");
	delete __ptr;
   e469a:	f000 bbca 	b.w	e4e32 <_ZdlPvj>
   e469e:	bd38      	pop	{r3, r4, r5, pc}

000e46a0 <_ZNKSt14default_deleteI3UDPEclEPS0_.isra.8.constprop.13>:
   e46a0:	b110      	cbz	r0, e46a8 <_ZNKSt14default_deleteI3UDPEclEPS0_.isra.8.constprop.13+0x8>
   e46a2:	6803      	ldr	r3, [r0, #0]
   e46a4:	685b      	ldr	r3, [r3, #4]
   e46a6:	4718      	bx	r3
   e46a8:	4770      	bx	lr

000e46aa <_ZN6ThreadD1Ev>:
    Thread(Thread&& thread)
        : d_(std::move(thread.d_))
    {
    }

    ~Thread()
   e46aa:	b510      	push	{r4, lr}
      }

      /// Return the stored pointer.
      pointer
      get() const noexcept
      { return std::get<0>(_M_t); }
   e46ac:	6803      	ldr	r3, [r0, #0]
   e46ae:	4604      	mov	r4, r0
        dispose();
    }

    void dispose()
    {
        if (!isValid())
   e46b0:	b1bb      	cbz	r3, e46e2 <_ZN6ThreadD1Ev+0x38>
        return isCurrent();
    }

    bool isCurrent() const
    {
        return isValid() && os_thread_is_current(d_->handle);
   e46b2:	6858      	ldr	r0, [r3, #4]
   e46b4:	f7ff fc66 	bl	e3f84 <os_thread_is_current>
   e46b8:	b978      	cbnz	r0, e46da <_ZN6ThreadD1Ev+0x30>
   e46ba:	6823      	ldr	r3, [r4, #0]

        // We shouldn't dispose of current thread
        if (isCurrent())
            return;

        if (!d_->exited) {
   e46bc:	7c5a      	ldrb	r2, [r3, #17]
   e46be:	b912      	cbnz	r2, e46c6 <_ZN6ThreadD1Ev+0x1c>
        d_.reset();
    }

    bool join()
    {
        return isValid() && os_thread_join(d_->handle)==0;
   e46c0:	6858      	ldr	r0, [r3, #4]
   e46c2:	f7ff fc67 	bl	e3f94 <os_thread_join>

        if (!d_->exited) {
            join();
        }

        os_thread_cleanup(d_->handle);
   e46c6:	6823      	ldr	r3, [r4, #0]
   e46c8:	6858      	ldr	r0, [r3, #4]
   e46ca:	f7ff fc6b 	bl	e3fa4 <os_thread_cleanup>
#endif
    {
      // concept requirements
      __glibcxx_function_requires(_SGIAssignableConcept<_Tp>)

      _Tp __tmp = _GLIBCXX_MOVE(__a);
   e46ce:	6820      	ldr	r0, [r4, #0]
      __a = _GLIBCXX_MOVE(__b);
   e46d0:	2300      	movs	r3, #0
   e46d2:	6023      	str	r3, [r4, #0]
      void
      reset(pointer __p = pointer()) noexcept
      {
	using std::swap;
	swap(std::get<0>(_M_t), __p);
	if (__p != pointer())
   e46d4:	b128      	cbz	r0, e46e2 <_ZN6ThreadD1Ev+0x38>
	  get_deleter()(__p);
   e46d6:	f7ff ffcd 	bl	e4674 <_ZNKSt14default_deleteIN6Thread4DataEEclEPS1_.isra.6>

      /// Destructor, invokes the deleter if the stored pointer is not null.
      ~unique_ptr() noexcept
      {
	auto& __ptr = std::get<0>(_M_t);
	if (__ptr != nullptr)
   e46da:	6820      	ldr	r0, [r4, #0]
   e46dc:	b108      	cbz	r0, e46e2 <_ZN6ThreadD1Ev+0x38>
	  get_deleter()(__ptr);
   e46de:	f7ff ffc9 	bl	e4674 <_ZNKSt14default_deleteIN6Thread4DataEEclEPS1_.isra.6>
    }

    ~Thread()
    {
        dispose();
    }
   e46e2:	4620      	mov	r0, r4
   e46e4:	bd10      	pop	{r4, pc}
	...

000e46e8 <_ZN5spark9MeshClassD1Ev>:
    RecursiveMutex mutex_;
    std::unique_ptr<uint8_t[]> buffer_;
    std::atomic_bool exit_;
};

class MeshClass : public NetworkClass, public MeshPublish {
   e46e8:	b538      	push	{r3, r4, r5, lr}
   e46ea:	4b0c      	ldr	r3, [pc, #48]	; (e471c <_ZN5spark9MeshClassD1Ev+0x34>)
   e46ec:	6003      	str	r3, [r0, #0]
   e46ee:	4604      	mov	r4, r0

      /// Destructor, invokes the deleter if the stored pointer is not null.
      ~unique_ptr()
      {
	auto& __ptr = std::get<0>(_M_t);
	if (__ptr != nullptr)
   e46f0:	f8d0 01cc 	ldr.w	r0, [r0, #460]	; 0x1cc
   e46f4:	b108      	cbz	r0, e46fa <_ZN5spark9MeshClassD1Ev+0x12>
      void
      operator()(_Tp* __ptr) const
      {
	static_assert(sizeof(_Tp)>0,
		      "can't delete pointer to incomplete type");
	delete [] __ptr;
   e46f6:	f7ef fcd4 	bl	d40a2 <_ZdaPv>

      /// Destructor, invokes the deleter if the stored pointer is not null.
      ~unique_ptr() noexcept
      {
	auto& __ptr = std::get<0>(_M_t);
	if (__ptr != nullptr)
   e46fa:	f8d4 51c4 	ldr.w	r5, [r4, #452]	; 0x1c4
   e46fe:	b135      	cbz	r5, e470e <_ZN5spark9MeshClassD1Ev+0x26>
      {
	static_assert(!is_void<_Tp>::value,
		      "can't delete pointer to incomplete type");
	static_assert(sizeof(_Tp)>0,
		      "can't delete pointer to incomplete type");
	delete __ptr;
   e4700:	4628      	mov	r0, r5
   e4702:	f7ff ffd2 	bl	e46aa <_ZN6ThreadD1Ev>
   e4706:	2104      	movs	r1, #4
   e4708:	4628      	mov	r0, r5
   e470a:	f000 fb92 	bl	e4e32 <_ZdlPvj>

      /// Destructor, invokes the deleter if the stored pointer is not null.
      ~unique_ptr() noexcept
      {
	auto& __ptr = std::get<0>(_M_t);
	if (__ptr != nullptr)
   e470e:	68a0      	ldr	r0, [r4, #8]
   e4710:	b108      	cbz	r0, e4716 <_ZN5spark9MeshClassD1Ev+0x2e>
	  get_deleter()(__ptr);
   e4712:	f7ff ffc5 	bl	e46a0 <_ZNKSt14default_deleteI3UDPEclEPS0_.isra.8.constprop.13>
   e4716:	4620      	mov	r0, r4
   e4718:	bd38      	pop	{r3, r4, r5, pc}
   e471a:	bf00      	nop
   e471c:	000eb448 	.word	0x000eb448

000e4720 <_GLOBAL__sub_I__ZN5spark11MeshPublish13Subscriptions20event_handler_existsEPKcPFvS3_S3_EPvN17SubscriptionScope4EnumES3_>:

    return addr;
}

MeshClass Mesh;
} // namespace spark
   e4720:	b538      	push	{r3, r4, r5, lr}
   e4722:	4c0e      	ldr	r4, [pc, #56]	; (e475c <_GLOBAL__sub_I__ZN5spark11MeshPublish13Subscriptions20event_handler_existsEPKcPFvS3_S3_EPvN17SubscriptionScope4EnumES3_+0x3c>)
   e4724:	4b0e      	ldr	r3, [pc, #56]	; (e4760 <_GLOBAL__sub_I__ZN5spark11MeshPublish13Subscriptions20event_handler_existsEPKcPFvS3_S3_EPvN17SubscriptionScope4EnumES3_+0x40>)
   e4726:	6023      	str	r3, [r4, #0]

  template<std::size_t _Idx, typename _Head>
    struct _Head_base<_Idx, _Head, false>
    {
      constexpr _Head_base()
      : _M_head_impl() { }
   e4728:	2500      	movs	r5, #0
   e472a:	2302      	movs	r3, #2
     */
    RecursiveMutex(os_mutex_recursive_t handle) : handle_(handle) {}

    RecursiveMutex() : handle_(nullptr)
    {
        os_mutex_recursive_create(&handle_);
   e472c:	f504 70e4 	add.w	r0, r4, #456	; 0x1c8
   e4730:	6063      	str	r3, [r4, #4]
   e4732:	60a5      	str	r5, [r4, #8]
   e4734:	f8c4 51c4 	str.w	r5, [r4, #452]	; 0x1c4
    /**
     * Creates a shared mutex.
     */
    RecursiveMutex(os_mutex_recursive_t handle) : handle_(handle) {}

    RecursiveMutex() : handle_(nullptr)
   e4738:	f8c4 51c8 	str.w	r5, [r4, #456]	; 0x1c8
    {
        os_mutex_recursive_create(&handle_);
   e473c:	f7ff fc42 	bl	e3fc4 <os_mutex_recursive_create>
public:
    MeshClass() :
            NetworkClass(NETWORK_INTERFACE_MESH) {
   e4740:	4b08      	ldr	r3, [pc, #32]	; (e4764 <_GLOBAL__sub_I__ZN5spark11MeshPublish13Subscriptions20event_handler_existsEPKcPFvS3_S3_EPvN17SubscriptionScope4EnumES3_+0x44>)
   e4742:	f8c4 51cc 	str.w	r5, [r4, #460]	; 0x1cc
      __atomic_base(const __atomic_base&) = delete;
      __atomic_base& operator=(const __atomic_base&) = delete;
      __atomic_base& operator=(const __atomic_base&) volatile = delete;

      // Requires __int_type convertible to _M_i.
      constexpr __atomic_base(__int_type __i) noexcept : _M_i (__i) { }
   e4746:	f884 51d0 	strb.w	r5, [r4, #464]	; 0x1d0
   e474a:	6023      	str	r3, [r4, #0]
    }

    return addr;
}

MeshClass Mesh;
   e474c:	4620      	mov	r0, r4
   e474e:	4a06      	ldr	r2, [pc, #24]	; (e4768 <_GLOBAL__sub_I__ZN5spark11MeshPublish13Subscriptions20event_handler_existsEPKcPFvS3_S3_EPvN17SubscriptionScope4EnumES3_+0x48>)
   e4750:	4906      	ldr	r1, [pc, #24]	; (e476c <_GLOBAL__sub_I__ZN5spark11MeshPublish13Subscriptions20event_handler_existsEPKcPFvS3_S3_EPvN17SubscriptionScope4EnumES3_+0x4c>)
} // namespace spark
   e4752:	e8bd 4038 	ldmia.w	sp!, {r3, r4, r5, lr}
    }

    return addr;
}

MeshClass Mesh;
   e4756:	f000 bb67 	b.w	e4e28 <__aeabi_atexit>
   e475a:	bf00      	nop
   e475c:	2003e3e0 	.word	0x2003e3e0
   e4760:	000eb47c 	.word	0x000eb47c
   e4764:	000eb448 	.word	0x000eb448
   e4768:	2003c2d4 	.word	0x2003c2d4
   e476c:	000e46e9 	.word	0x000e46e9

000e4770 <_ZN5spark12NetworkClass7connectEj>:
        return Network;
    }
}

void NetworkClass::connect(unsigned flags) {
    network_connect(*this, flags, 0, nullptr);
   e4770:	2300      	movs	r3, #0
   e4772:	461a      	mov	r2, r3
   e4774:	6840      	ldr	r0, [r0, #4]
   e4776:	f7ff bd79 	b.w	e426c <network_connect>

000e477a <_ZN5spark12NetworkClass10disconnectEv>:
}

void NetworkClass::disconnect() {
    network_disconnect(*this, NETWORK_DISCONNECT_REASON_USER, nullptr);
   e477a:	2200      	movs	r2, #0
   e477c:	2102      	movs	r1, #2
   e477e:	6840      	ldr	r0, [r0, #4]
   e4780:	f7ff bd84 	b.w	e428c <network_disconnect>

000e4784 <_ZN5spark12NetworkClass10connectingEv>:
}

bool NetworkClass::connecting() {
    return network_connecting(*this, 0, nullptr);
   e4784:	2200      	movs	r2, #0
   e4786:	4611      	mov	r1, r2
   e4788:	6840      	ldr	r0, [r0, #4]
   e478a:	f7ff bd77 	b.w	e427c <network_connecting>

000e478e <_ZN5spark12NetworkClass5readyEv>:
}

bool NetworkClass::ready() {
    return network_ready(*this, 0, nullptr);
   e478e:	2200      	movs	r2, #0
   e4790:	4611      	mov	r1, r2
   e4792:	6840      	ldr	r0, [r0, #4]
   e4794:	f7ff bd82 	b.w	e429c <network_ready>

000e4798 <_ZN5spark12NetworkClass2onEv>:
}

void NetworkClass::on() {
    network_on(*this, 0, 0, nullptr);
   e4798:	2300      	movs	r3, #0
   e479a:	461a      	mov	r2, r3
   e479c:	4619      	mov	r1, r3
   e479e:	6840      	ldr	r0, [r0, #4]
   e47a0:	f7ff bd84 	b.w	e42ac <network_on>

000e47a4 <_ZN5spark12NetworkClass3offEv>:
}

void NetworkClass::off() {
    network_off(*this, 0, 0, nullptr);
   e47a4:	2300      	movs	r3, #0
   e47a6:	461a      	mov	r2, r3
   e47a8:	4619      	mov	r1, r3
   e47aa:	6840      	ldr	r0, [r0, #4]
   e47ac:	f7ff bd86 	b.w	e42bc <network_off>

000e47b0 <_ZN5spark12NetworkClass6listenEb>:
}

void NetworkClass::listen(bool begin) {
    network_listen(*this, begin ? 0 : 1, nullptr);
   e47b0:	2200      	movs	r2, #0
   e47b2:	f081 0101 	eor.w	r1, r1, #1
   e47b6:	6840      	ldr	r0, [r0, #4]
   e47b8:	f7ff bd88 	b.w	e42cc <network_listen>

000e47bc <_ZN5spark12NetworkClass16setListenTimeoutEt>:
}

void NetworkClass::setListenTimeout(uint16_t timeout) {
    network_set_listen_timeout(*this, timeout, nullptr);
   e47bc:	2200      	movs	r2, #0
   e47be:	6840      	ldr	r0, [r0, #4]
   e47c0:	f7ff bd94 	b.w	e42ec <network_set_listen_timeout>

000e47c4 <_ZN5spark12NetworkClass16getListenTimeoutEv>:
}

uint16_t NetworkClass::getListenTimeout() {
    return network_get_listen_timeout(*this, 0, nullptr);
   e47c4:	2200      	movs	r2, #0
   e47c6:	4611      	mov	r1, r2
   e47c8:	6840      	ldr	r0, [r0, #4]
   e47ca:	f7ff bd97 	b.w	e42fc <network_get_listen_timeout>

000e47ce <_ZN5spark12NetworkClass9listeningEv>:
}

bool NetworkClass::listening() {
    return network_listening(*this, 0, nullptr);
   e47ce:	2200      	movs	r2, #0
   e47d0:	4611      	mov	r1, r2
   e47d2:	6840      	ldr	r0, [r0, #4]
   e47d4:	f7ff bd82 	b.w	e42dc <network_listening>

000e47d8 <_ZN5spark12NetworkClass7resolveEPKc>:
}

IPAddress NetworkClass::resolve(const char* name) {
   e47d8:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
    IPAddress addr;
#if HAL_USE_INET_HAL_POSIX
    struct addrinfo *ai = nullptr;
   e47dc:	2400      	movs	r4, #0

bool NetworkClass::listening() {
    return network_listening(*this, 0, nullptr);
}

IPAddress NetworkClass::resolve(const char* name) {
   e47de:	b095      	sub	sp, #84	; 0x54
   e47e0:	4616      	mov	r6, r2
   e47e2:	460d      	mov	r5, r1
   e47e4:	4607      	mov	r7, r0
    IPAddress addr;
   e47e6:	f7ff fed1 	bl	e458c <_ZN9IPAddressC1Ev>
#if HAL_USE_INET_HAL_POSIX
    struct addrinfo *ai = nullptr;
    struct addrinfo hints = {};
   e47ea:	4621      	mov	r1, r4
   e47ec:	2220      	movs	r2, #32
   e47ee:	a80c      	add	r0, sp, #48	; 0x30
}

IPAddress NetworkClass::resolve(const char* name) {
    IPAddress addr;
#if HAL_USE_INET_HAL_POSIX
    struct addrinfo *ai = nullptr;
   e47f0:	9400      	str	r4, [sp, #0]
    struct addrinfo hints = {};
   e47f2:	f003 f801 	bl	e77f8 <memset>
    hints.ai_flags = AI_ADDRCONFIG;
   e47f6:	2340      	movs	r3, #64	; 0x40
   e47f8:	930c      	str	r3, [sp, #48]	; 0x30
    hints.ai_family = AF_UNSPEC;
    const int r = getaddrinfo(name, nullptr, &hints, &ai);
   e47fa:	4621      	mov	r1, r4
   e47fc:	466b      	mov	r3, sp
   e47fe:	aa0c      	add	r2, sp, #48	; 0x30
   e4800:	4630      	mov	r0, r6
   e4802:	f7ff fc77 	bl	e40f4 <netdb_getaddrinfo>
    if (!r) {
   e4806:	4604      	mov	r4, r0
   e4808:	2800      	cmp	r0, #0
   e480a:	d144      	bne.n	e4896 <_ZN5spark12NetworkClass7resolveEPKc+0xbe>
        bool ok = false;
        // This is not really needed if AI_ADDRCONFIG is properly supported
        bool ipv4 = network_ready(*this, NETWORK_READY_TYPE_IPV4, nullptr);
   e480c:	4602      	mov	r2, r0
   e480e:	2101      	movs	r1, #1
   e4810:	6868      	ldr	r0, [r5, #4]
   e4812:	f7ff fd43 	bl	e429c <network_ready>
        bool ipv6 = network_ready(*this, NETWORK_READY_TYPE_IPV6, nullptr);
   e4816:	2102      	movs	r1, #2
    hints.ai_family = AF_UNSPEC;
    const int r = getaddrinfo(name, nullptr, &hints, &ai);
    if (!r) {
        bool ok = false;
        // This is not really needed if AI_ADDRCONFIG is properly supported
        bool ipv4 = network_ready(*this, NETWORK_READY_TYPE_IPV4, nullptr);
   e4818:	4680      	mov	r8, r0
        bool ipv6 = network_ready(*this, NETWORK_READY_TYPE_IPV6, nullptr);
   e481a:	4622      	mov	r2, r4
   e481c:	6868      	ldr	r0, [r5, #4]
   e481e:	f7ff fd3d 	bl	e429c <network_ready>
        for (auto cur = ai; cur != nullptr && !ok; cur = cur->ai_next) {
   e4822:	9e00      	ldr	r6, [sp, #0]
    const int r = getaddrinfo(name, nullptr, &hints, &ai);
    if (!r) {
        bool ok = false;
        // This is not really needed if AI_ADDRCONFIG is properly supported
        bool ipv4 = network_ready(*this, NETWORK_READY_TYPE_IPV4, nullptr);
        bool ipv6 = network_ready(*this, NETWORK_READY_TYPE_IPV6, nullptr);
   e4824:	4681      	mov	r9, r0
    struct addrinfo hints = {};
    hints.ai_flags = AI_ADDRCONFIG;
    hints.ai_family = AF_UNSPEC;
    const int r = getaddrinfo(name, nullptr, &hints, &ai);
    if (!r) {
        bool ok = false;
   e4826:	4621      	mov	r1, r4
        // This is not really needed if AI_ADDRCONFIG is properly supported
        bool ipv4 = network_ready(*this, NETWORK_READY_TYPE_IPV4, nullptr);
        bool ipv6 = network_ready(*this, NETWORK_READY_TYPE_IPV6, nullptr);
        for (auto cur = ai; cur != nullptr && !ok; cur = cur->ai_next) {
   e4828:	2e00      	cmp	r6, #0
   e482a:	d034      	beq.n	e4896 <_ZN5spark12NetworkClass7resolveEPKc+0xbe>
   e482c:	2900      	cmp	r1, #0
   e482e:	d132      	bne.n	e4896 <_ZN5spark12NetworkClass7resolveEPKc+0xbe>
            // NOTE: using only the first entry that matches the current state of IPv4/IPv6 connectivity
            switch (cur->ai_family) {
   e4830:	6873      	ldr	r3, [r6, #4]
   e4832:	2b02      	cmp	r3, #2
   e4834:	d002      	beq.n	e483c <_ZN5spark12NetworkClass7resolveEPKc+0x64>
   e4836:	2b0a      	cmp	r3, #10
   e4838:	d009      	beq.n	e484e <_ZN5spark12NetworkClass7resolveEPKc+0x76>
   e483a:	e02a      	b.n	e4892 <_ZN5spark12NetworkClass7resolveEPKc+0xba>
                case AF_INET: {
                    if (!ipv4) {
   e483c:	f1b8 0f00 	cmp.w	r8, #0
   e4840:	d027      	beq.n	e4892 <_ZN5spark12NetworkClass7resolveEPKc+0xba>
                        continue;
                    }
                    // NOTE: HAL_IPAddress is little-endian
                    auto in = (struct sockaddr_in*)cur->ai_addr;
                    addr = (const uint8_t*)(&in->sin_addr.s_addr);
   e4842:	6971      	ldr	r1, [r6, #20]
   e4844:	4638      	mov	r0, r7
   e4846:	3104      	adds	r1, #4
   e4848:	f7ff fecd 	bl	e45e6 <_ZN9IPAddressaSEPKh>
   e484c:	e020      	b.n	e4890 <_ZN5spark12NetworkClass7resolveEPKc+0xb8>
                    ok = true;
                    break;
                }
                case AF_INET6: {
                    if (!ipv6) {
   e484e:	f1b9 0f00 	cmp.w	r9, #0
   e4852:	d01e      	beq.n	e4892 <_ZN5spark12NetworkClass7resolveEPKc+0xba>
                        continue;
                    }
                    auto in6 = (struct sockaddr_in6*)cur->ai_addr;
   e4854:	6974      	ldr	r4, [r6, #20]
                    HAL_IPAddress a = {};
   e4856:	2211      	movs	r2, #17
   e4858:	a801      	add	r0, sp, #4
   e485a:	f002 ffcd 	bl	e77f8 <memset>
                    a.v = 6;
   e485e:	2306      	movs	r3, #6
   e4860:	f88d 3014 	strb.w	r3, [sp, #20]
                    memcpy(a.ipv6, in6->sin6_addr.s6_addr, sizeof(a.ipv6));
   e4864:	ad01      	add	r5, sp, #4
   e4866:	f104 0308 	add.w	r3, r4, #8
   e486a:	3418      	adds	r4, #24
   e486c:	6818      	ldr	r0, [r3, #0]
   e486e:	6859      	ldr	r1, [r3, #4]
   e4870:	462a      	mov	r2, r5
   e4872:	c203      	stmia	r2!, {r0, r1}
   e4874:	3308      	adds	r3, #8
   e4876:	42a3      	cmp	r3, r4
   e4878:	4615      	mov	r5, r2
   e487a:	d1f7      	bne.n	e486c <_ZN5spark12NetworkClass7resolveEPKc+0x94>
                    addr = IPAddress(a);
   e487c:	a901      	add	r1, sp, #4
   e487e:	a806      	add	r0, sp, #24

/**
 * The IP address stored in host order.
 *
 */
class IPAddress : public Printable {
   e4880:	ad07      	add	r5, sp, #28
   e4882:	f7ff fe91 	bl	e45a8 <_ZN9IPAddressC1ERK16_HAL_IPAddress_t>
   e4886:	cd0f      	ldmia	r5!, {r0, r1, r2, r3}
   e4888:	1d3c      	adds	r4, r7, #4
   e488a:	c40f      	stmia	r4!, {r0, r1, r2, r3}
   e488c:	682b      	ldr	r3, [r5, #0]
   e488e:	7023      	strb	r3, [r4, #0]
                    ok = true;
   e4890:	2101      	movs	r1, #1
    if (!r) {
        bool ok = false;
        // This is not really needed if AI_ADDRCONFIG is properly supported
        bool ipv4 = network_ready(*this, NETWORK_READY_TYPE_IPV4, nullptr);
        bool ipv6 = network_ready(*this, NETWORK_READY_TYPE_IPV6, nullptr);
        for (auto cur = ai; cur != nullptr && !ok; cur = cur->ai_next) {
   e4892:	69f6      	ldr	r6, [r6, #28]
   e4894:	e7c8      	b.n	e4828 <_ZN5spark12NetworkClass7resolveEPKc+0x50>
                    break;
                }
            }
        }
    }
    freeaddrinfo(ai);
   e4896:	9800      	ldr	r0, [sp, #0]
   e4898:	f7ff fc24 	bl	e40e4 <netdb_freeaddrinfo>
    return Cellular.resolve(name);
#endif // Wiring_Cellular

#endif // HAL_USE_INET_HAL_POSIX
    return addr;
}
   e489c:	4638      	mov	r0, r7
   e489e:	b015      	add	sp, #84	; 0x54
   e48a0:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}

000e48a4 <_GLOBAL__sub_I__ZN5spark7NetworkE>:
   e48a4:	4b02      	ldr	r3, [pc, #8]	; (e48b0 <_GLOBAL__sub_I__ZN5spark7NetworkE+0xc>)
   e48a6:	4a03      	ldr	r2, [pc, #12]	; (e48b4 <_GLOBAL__sub_I__ZN5spark7NetworkE+0x10>)
   e48a8:	601a      	str	r2, [r3, #0]
   e48aa:	2200      	movs	r2, #0
   e48ac:	605a      	str	r2, [r3, #4]
   e48ae:	4770      	bx	lr
   e48b0:	2003e5b4 	.word	0x2003e5b4
   e48b4:	000eb47c 	.word	0x000eb47c

000e48b8 <_ZN5Print5writeEPKhj>:

// Public Methods //////////////////////////////////////////////////////////////

/* default implementation: may be overridden */
size_t Print::write(const uint8_t *buffer, size_t size)
{
   e48b8:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
   e48ba:	4606      	mov	r6, r0
   e48bc:	460d      	mov	r5, r1
   e48be:	188f      	adds	r7, r1, r2
  size_t n = 0;
   e48c0:	2400      	movs	r4, #0
  while (size--) {
   e48c2:	42bd      	cmp	r5, r7
   e48c4:	d00c      	beq.n	e48e0 <_ZN5Print5writeEPKhj+0x28>
     int chunk = write(*buffer++);
   e48c6:	6833      	ldr	r3, [r6, #0]
   e48c8:	f815 1b01 	ldrb.w	r1, [r5], #1
   e48cc:	689b      	ldr	r3, [r3, #8]
   e48ce:	4630      	mov	r0, r6
   e48d0:	4798      	blx	r3
     if (chunk>=0)
   e48d2:	2800      	cmp	r0, #0
   e48d4:	db01      	blt.n	e48da <_ZN5Print5writeEPKhj+0x22>
         n += chunk;
   e48d6:	4404      	add	r4, r0

/* default implementation: may be overridden */
size_t Print::write(const uint8_t *buffer, size_t size)
{
  size_t n = 0;
  while (size--) {
   e48d8:	e7f3      	b.n	e48c2 <_ZN5Print5writeEPKhj+0xa>
     int chunk = write(*buffer++);
   e48da:	2c00      	cmp	r4, #0
   e48dc:	bf08      	it	eq
   e48de:	4604      	moveq	r4, r0
             n = chunk;
         break;
     }
  }
  return n;
}
   e48e0:	4620      	mov	r0, r4
   e48e2:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}

000e48e4 <_ZN5Print5writeEPKc>:

    int getWriteError() { return write_error; }
    void clearWriteError() { setWriteError(0); }

    virtual size_t write(uint8_t) = 0;
    size_t write(const char *str) {
   e48e4:	b570      	push	{r4, r5, r6, lr}
   e48e6:	4605      	mov	r5, r0
      if (str == NULL) return 0;
   e48e8:	460c      	mov	r4, r1
      return write((const uint8_t *)str, strlen(str));
    }
   e48ea:	4608      	mov	r0, r1
    int getWriteError() { return write_error; }
    void clearWriteError() { setWriteError(0); }

    virtual size_t write(uint8_t) = 0;
    size_t write(const char *str) {
      if (str == NULL) return 0;
   e48ec:	b149      	cbz	r1, e4902 <_ZN5Print5writeEPKc+0x1e>
      return write((const uint8_t *)str, strlen(str));
   e48ee:	f002 ffbd 	bl	e786c <strlen>
   e48f2:	682b      	ldr	r3, [r5, #0]
   e48f4:	4602      	mov	r2, r0
   e48f6:	4621      	mov	r1, r4
   e48f8:	4628      	mov	r0, r5
   e48fa:	68db      	ldr	r3, [r3, #12]
    }
   e48fc:	e8bd 4070 	ldmia.w	sp!, {r4, r5, r6, lr}
    void clearWriteError() { setWriteError(0); }

    virtual size_t write(uint8_t) = 0;
    size_t write(const char *str) {
      if (str == NULL) return 0;
      return write((const uint8_t *)str, strlen(str));
   e4900:	4718      	bx	r3
    }
   e4902:	bd70      	pop	{r4, r5, r6, pc}

000e4904 <_ZN5Print5printEPKc>:
   e4904:	b508      	push	{r3, lr}
   e4906:	f7ff ffed 	bl	e48e4 <_ZN5Print5writeEPKc>
   e490a:	bd08      	pop	{r3, pc}

000e490c <_ZN5Print5printEc>:
  return write(str);
}

size_t Print::print(char c)
{
  return write(c);
   e490c:	6803      	ldr	r3, [r0, #0]
   e490e:	689b      	ldr	r3, [r3, #8]
   e4910:	4718      	bx	r3

000e4912 <_ZN5Print11printNumberEmh>:
  return println(reinterpret_cast<const char*>(str));
}

// Private Methods /////////////////////////////////////////////////////////////

size_t Print::printNumber(unsigned long n, uint8_t base) {
   e4912:	b530      	push	{r4, r5, lr}
   e4914:	b08b      	sub	sp, #44	; 0x2c
   e4916:	460b      	mov	r3, r1
  char buf[8 * sizeof(long) + 1]; // Assumes 8-bit chars plus zero byte.
  char *str = &buf[sizeof(buf) - 1];

  *str = '\0';
   e4918:	2100      	movs	r1, #0
   e491a:	f88d 1024 	strb.w	r1, [sp, #36]	; 0x24

  // prevent crash if called with base == 1
  if (base < 2) base = 10;
   e491e:	2a01      	cmp	r2, #1
   e4920:	bf98      	it	ls
   e4922:	220a      	movls	r2, #10
   e4924:	f10d 0423 	add.w	r4, sp, #35	; 0x23

  do {
    unsigned long m = n;
    n /= base;
   e4928:	fbb3 f5f2 	udiv	r5, r3, r2
    char c = m - base * n;
   e492c:	fb05 3312 	mls	r3, r5, r2, r3
   e4930:	f003 03ff 	and.w	r3, r3, #255	; 0xff
    *--str = c < 10 ? c + '0' : c + 'A' - 10;
   e4934:	2b09      	cmp	r3, #9
   e4936:	bf94      	ite	ls
   e4938:	3330      	addls	r3, #48	; 0x30
   e493a:	3337      	addhi	r3, #55	; 0x37
   e493c:	b2db      	uxtb	r3, r3
   e493e:	4621      	mov	r1, r4
   e4940:	f804 3901 	strb.w	r3, [r4], #-1
   e4944:	462b      	mov	r3, r5
  *str = '\0';

  // prevent crash if called with base == 1
  if (base < 2) base = 10;

  do {
   e4946:	2d00      	cmp	r5, #0
   e4948:	d1ee      	bne.n	e4928 <_ZN5Print11printNumberEmh+0x16>
    n /= base;
    char c = m - base * n;
    *--str = c < 10 ? c + '0' : c + 'A' - 10;
  } while(n);

  return write(str);
   e494a:	f7ff ffcb 	bl	e48e4 <_ZN5Print5writeEPKc>
}
   e494e:	b00b      	add	sp, #44	; 0x2c
   e4950:	bd30      	pop	{r4, r5, pc}

000e4952 <_ZN5Print5printEmi>:
    return printNumber(n, base);
  }
}

size_t Print::print(unsigned long n, int base)
{
   e4952:	b410      	push	{r4}
  if (base == 0) return write(n);
   e4954:	b92a      	cbnz	r2, e4962 <_ZN5Print5printEmi+0x10>
   e4956:	6803      	ldr	r3, [r0, #0]
  else return printNumber(n, base);
}
   e4958:	f85d 4b04 	ldr.w	r4, [sp], #4
  }
}

size_t Print::print(unsigned long n, int base)
{
  if (base == 0) return write(n);
   e495c:	689b      	ldr	r3, [r3, #8]
   e495e:	b2c9      	uxtb	r1, r1
   e4960:	4718      	bx	r3
  else return printNumber(n, base);
   e4962:	b2d2      	uxtb	r2, r2
}
   e4964:	f85d 4b04 	ldr.w	r4, [sp], #4
}

size_t Print::print(unsigned long n, int base)
{
  if (base == 0) return write(n);
  else return printNumber(n, base);
   e4968:	f7ff bfd3 	b.w	e4912 <_ZN5Print11printNumberEmh>

000e496c <_ZN5Print5printEhi>:
  return write(c);
}

size_t Print::print(unsigned char b, int base)
{
  return print((unsigned long) b, base);
   e496c:	f7ff bff1 	b.w	e4952 <_ZN5Print5printEmi>

000e4970 <_ZN8RGBClassD1Ev>:
#include "rgbled.h"

typedef void (raw_rgb_change_handler_t)(uint8_t, uint8_t, uint8_t);
typedef std::function<raw_rgb_change_handler_t> wiring_rgb_change_handler_t;

class RGBClass {
   e4970:	b510      	push	{r4, lr}
   e4972:	4604      	mov	r4, r0
   *  @ingroup functors
   *
   *  Polymorphic function wrapper.
   */
  template<typename _Res, typename... _ArgTypes>
    class function<_Res(_ArgTypes...)>
   e4974:	f7ff fd46 	bl	e4404 <_ZNSt14_Function_baseD1Ev>
   e4978:	4620      	mov	r0, r4
   e497a:	bd10      	pop	{r4, pc}

000e497c <_GLOBAL__sub_I_RGB>:
	{
	  _Base::_M_init_functor(__functor, std::__addressof(__f.get()));
	}
      };

    _Function_base() : _M_manager(nullptr) { }
   e497c:	4803      	ldr	r0, [pc, #12]	; (e498c <_GLOBAL__sub_I_RGB+0x10>)
#include "spark_wiring_rgb.h"
#include "spark_wiring_interrupts.h"

#include "core_hal.h"

RGBClass RGB;
   e497e:	4a04      	ldr	r2, [pc, #16]	; (e4990 <_GLOBAL__sub_I_RGB+0x14>)
   e4980:	4904      	ldr	r1, [pc, #16]	; (e4994 <_GLOBAL__sub_I_RGB+0x18>)
   e4982:	2300      	movs	r3, #0
   e4984:	6083      	str	r3, [r0, #8]
   e4986:	f000 ba4f 	b.w	e4e28 <__aeabi_atexit>
   e498a:	bf00      	nop
   e498c:	2003e5bc 	.word	0x2003e5bc
   e4990:	2003c2d4 	.word	0x2003c2d4
   e4994:	000e4971 	.word	0x000e4971

000e4998 <_ZN8SPIClassD1Ev>:
  Mutex mutex_;
#endif

public:
  SPIClass(HAL_SPI_Interface spi);
  virtual ~SPIClass() {};
   e4998:	4770      	bx	lr

000e499a <_ZN8SPIClassD0Ev>:
   e499a:	b510      	push	{r4, lr}
   e499c:	2110      	movs	r1, #16
   e499e:	4604      	mov	r4, r0
   e49a0:	f000 fa47 	bl	e4e32 <_ZdlPvj>
   e49a4:	4620      	mov	r0, r4
   e49a6:	bd10      	pop	{r4, pc}

000e49a8 <_ZN8SPIClassC1E17HAL_SPI_Interface>:
  if (!info->enabled || info->default_settings)
    return particle::__SPISettings();
  return particle::__SPISettings(info->clock, info->bit_order, info->data_mode);
}

SPIClass::SPIClass(HAL_SPI_Interface spi)
   e49a8:	b570      	push	{r4, r5, r6, lr}
   e49aa:	4b08      	ldr	r3, [pc, #32]	; (e49cc <_ZN8SPIClassC1E17HAL_SPI_Interface+0x24>)
   e49ac:	6003      	str	r3, [r0, #0]
   e49ae:	4604      	mov	r4, r0
    Mutex(os_mutex_t handle) : handle_(handle) {}

    /**
     * Creates a new mutex.
     */
    Mutex() : handle_(nullptr)
   e49b0:	2500      	movs	r5, #0
   e49b2:	460e      	mov	r6, r1
   e49b4:	f840 5f0c 	str.w	r5, [r0, #12]!
    {
        os_mutex_create(&handle_);
   e49b8:	f7ff fafc 	bl	e3fb4 <os_mutex_create>
{
  _spi = spi;
  HAL_SPI_Init(_spi);
   e49bc:	4630      	mov	r0, r6
  return particle::__SPISettings(info->clock, info->bit_order, info->data_mode);
}

SPIClass::SPIClass(HAL_SPI_Interface spi)
{
  _spi = spi;
   e49be:	7126      	strb	r6, [r4, #4]
  HAL_SPI_Init(_spi);
   e49c0:	f7ff fba0 	bl	e4104 <HAL_SPI_Init>
  dividerReference = SPI_CLK_SYSTEM;     // 0 indicates the system clock
   e49c4:	60a5      	str	r5, [r4, #8]
}
   e49c6:	4620      	mov	r0, r4
   e49c8:	bd70      	pop	{r4, r5, r6, pc}
   e49ca:	bf00      	nop
   e49cc:	000eb4b0 	.word	0x000eb4b0

000e49d0 <_ZN8SPIClass9isEnabledEv>:
  //To Do
}

bool SPIClass::isEnabled()
{
  return HAL_SPI_Is_Enabled(_spi);
   e49d0:	7900      	ldrb	r0, [r0, #4]
   e49d2:	f7ff bb9f 	b.w	e4114 <HAL_SPI_Is_Enabled>
	...

000e49d8 <_GLOBAL__sub_I_System>:
    WAKEUP_REASON_RTC = 2,
    WAKEUP_REASON_PIN_OR_RTC = 3
};

struct SleepResult {
    SleepResult() {}
   e49d8:	4b04      	ldr	r3, [pc, #16]	; (e49ec <_GLOBAL__sub_I_System+0x14>)
   e49da:	2000      	movs	r0, #0
   e49dc:	f64f 72ff 	movw	r2, #65535	; 0xffff
   e49e0:	7018      	strb	r0, [r3, #0]
   e49e2:	8058      	strh	r0, [r3, #2]
   e49e4:	809a      	strh	r2, [r3, #4]

class SystemClass {
public:

    SystemClass(System_Mode_TypeDef mode = DEFAULT) {
        set_system_mode(mode);
   e49e6:	f7ff bc1d 	b.w	e4224 <set_system_mode>
   e49ea:	bf00      	nop
   e49ec:	2003e5cc 	.word	0x2003e5cc

000e49f0 <_GLOBAL__sub_I_TIME_FORMAT_DEFAULT>:
            calendar_time_cache = Convert_UnixTime_To_CalendarTime(unix_time);
            unix_time_cache = unix_time;
    }
}

const char* TimeClass::format_spec = TIME_FORMAT_DEFAULT;
   e49f0:	4b02      	ldr	r3, [pc, #8]	; (e49fc <_GLOBAL__sub_I_TIME_FORMAT_DEFAULT+0xc>)
   e49f2:	681a      	ldr	r2, [r3, #0]
   e49f4:	4b02      	ldr	r3, [pc, #8]	; (e4a00 <_GLOBAL__sub_I_TIME_FORMAT_DEFAULT+0x10>)
   e49f6:	601a      	str	r2, [r3, #0]
   e49f8:	4770      	bx	lr
   e49fa:	bf00      	nop
   e49fc:	2003c268 	.word	0x2003c268
   e4a00:	2003e5d4 	.word	0x2003e5d4

000e4a04 <_ZN11USARTSerialD1Ev>:
private:
  HAL_USART_Serial _serial;
  bool _blocking;
public:
  USARTSerial(HAL_USART_Serial serial, Ring_Buffer *rx_buffer, Ring_Buffer *tx_buffer);
  virtual ~USARTSerial() {};
   e4a04:	4770      	bx	lr

000e4a06 <_ZN11USARTSerial14blockOnOverrunEb>:
    HAL_USART_Half_Duplex(_serial, Enable);
}

void USARTSerial::blockOnOverrun(bool block)
{
  _blocking = block;
   e4a06:	7441      	strb	r1, [r0, #17]
   e4a08:	4770      	bx	lr

000e4a0a <_ZN11USARTSerial17availableForWriteEv>:
}


int USARTSerial::availableForWrite(void)
{
   e4a0a:	b508      	push	{r3, lr}
  return std::max(0, (int)HAL_USART_Available_Data_For_Write(_serial));
   e4a0c:	7c00      	ldrb	r0, [r0, #16]
   e4a0e:	f7ff fbc1 	bl	e4194 <HAL_USART_Available_Data_For_Write>
}
   e4a12:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
   e4a16:	bd08      	pop	{r3, pc}

000e4a18 <_ZN11USARTSerial9availableEv>:

int USARTSerial::available(void)
{
   e4a18:	b508      	push	{r3, lr}
  return std::max(0, (int)HAL_USART_Available_Data(_serial));
   e4a1a:	7c00      	ldrb	r0, [r0, #16]
   e4a1c:	f7ff fb92 	bl	e4144 <HAL_USART_Available_Data>
}
   e4a20:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
   e4a24:	bd08      	pop	{r3, pc}

000e4a26 <_ZN11USARTSerial4peekEv>:

int USARTSerial::peek(void)
{
   e4a26:	b508      	push	{r3, lr}
  return std::max(-1, (int)HAL_USART_Peek_Data(_serial));
   e4a28:	7c00      	ldrb	r0, [r0, #16]
   e4a2a:	f7ff fb9b 	bl	e4164 <HAL_USART_Peek_Data>
}
   e4a2e:	ea30 0020 	bics.w	r0, r0, r0, asr #32
   e4a32:	bf28      	it	cs
   e4a34:	f04f 30ff 	movcs.w	r0, #4294967295	; 0xffffffff
   e4a38:	bd08      	pop	{r3, pc}

000e4a3a <_ZN11USARTSerial4readEv>:

int USARTSerial::read(void)
{
   e4a3a:	b508      	push	{r3, lr}
  return std::max(-1, (int)HAL_USART_Read_Data(_serial));
   e4a3c:	7c00      	ldrb	r0, [r0, #16]
   e4a3e:	f7ff fb89 	bl	e4154 <HAL_USART_Read_Data>
}
   e4a42:	ea30 0020 	bics.w	r0, r0, r0, asr #32
   e4a46:	bf28      	it	cs
   e4a48:	f04f 30ff 	movcs.w	r0, #4294967295	; 0xffffffff
   e4a4c:	bd08      	pop	{r3, pc}

000e4a4e <_ZN11USARTSerial5flushEv>:

void USARTSerial::flush()
{
  HAL_USART_Flush_Data(_serial);
   e4a4e:	7c00      	ldrb	r0, [r0, #16]
   e4a50:	f7ff bb90 	b.w	e4174 <HAL_USART_Flush_Data>

000e4a54 <_ZN11USARTSerialD0Ev>:
   e4a54:	b510      	push	{r4, lr}
   e4a56:	2114      	movs	r1, #20
   e4a58:	4604      	mov	r4, r0
   e4a5a:	f000 f9ea 	bl	e4e32 <_ZdlPvj>
   e4a5e:	4620      	mov	r0, r4
   e4a60:	bd10      	pop	{r4, pc}

000e4a62 <_ZN11USARTSerial5writeEh>:
}

size_t USARTSerial::write(uint8_t c)
{
   e4a62:	b570      	push	{r4, r5, r6, lr}
  // attempt a write if blocking, or for non-blocking if there is room.
  if (_blocking || HAL_USART_Available_Data_For_Write(_serial) > 0) {
   e4a64:	7c45      	ldrb	r5, [r0, #17]
{
  HAL_USART_Flush_Data(_serial);
}

size_t USARTSerial::write(uint8_t c)
{
   e4a66:	4604      	mov	r4, r0
   e4a68:	460e      	mov	r6, r1
  // attempt a write if blocking, or for non-blocking if there is room.
  if (_blocking || HAL_USART_Available_Data_For_Write(_serial) > 0) {
   e4a6a:	b925      	cbnz	r5, e4a76 <_ZN11USARTSerial5writeEh+0x14>
   e4a6c:	7c00      	ldrb	r0, [r0, #16]
   e4a6e:	f7ff fb91 	bl	e4194 <HAL_USART_Available_Data_For_Write>
   e4a72:	2800      	cmp	r0, #0
   e4a74:	dd05      	ble.n	e4a82 <_ZN11USARTSerial5writeEh+0x20>
    // the HAL always blocks.
	  return HAL_USART_Write_Data(_serial, c);
   e4a76:	4631      	mov	r1, r6
   e4a78:	7c20      	ldrb	r0, [r4, #16]
  }
  return 0;
}
   e4a7a:	e8bd 4070 	ldmia.w	sp!, {r4, r5, r6, lr}
size_t USARTSerial::write(uint8_t c)
{
  // attempt a write if blocking, or for non-blocking if there is room.
  if (_blocking || HAL_USART_Available_Data_For_Write(_serial) > 0) {
    // the HAL always blocks.
	  return HAL_USART_Write_Data(_serial, c);
   e4a7e:	f7ff bb59 	b.w	e4134 <HAL_USART_Write_Data>
  }
  return 0;
}
   e4a82:	4628      	mov	r0, r5
   e4a84:	bd70      	pop	{r4, r5, r6, pc}
	...

000e4a88 <_ZN11USARTSerialC1E16HAL_USART_SerialP11Ring_BufferS2_>:
#include "spark_wiring_constants.h"
#include "module_info.h"

// Constructors ////////////////////////////////////////////////////////////////

USARTSerial::USARTSerial(HAL_USART_Serial serial, Ring_Buffer *rx_buffer, Ring_Buffer *tx_buffer)
   e4a88:	b510      	push	{r4, lr}
   e4a8a:	4604      	mov	r4, r0
   e4a8c:	4608      	mov	r0, r1
   e4a8e:	4611      	mov	r1, r2
  protected:
    void setWriteError(int err = 1) { write_error = err; }
    size_t printf_impl(bool newline, const char* format, ...);

  public:
    Print() : write_error(0) {}
   e4a90:	2200      	movs	r2, #0
   e4a92:	6062      	str	r2, [r4, #4]
   e4a94:	f44f 727a 	mov.w	r2, #1000	; 0x3e8
   e4a98:	60a2      	str	r2, [r4, #8]
   e4a9a:	4a05      	ldr	r2, [pc, #20]	; (e4ab0 <_ZN11USARTSerialC1E16HAL_USART_SerialP11Ring_BufferS2_+0x28>)
   e4a9c:	6022      	str	r2, [r4, #0]
{
  _serial = serial;
  // Default is blocking mode
  _blocking = true;
   e4a9e:	2201      	movs	r2, #1

// Constructors ////////////////////////////////////////////////////////////////

USARTSerial::USARTSerial(HAL_USART_Serial serial, Ring_Buffer *rx_buffer, Ring_Buffer *tx_buffer)
{
  _serial = serial;
   e4aa0:	7420      	strb	r0, [r4, #16]
  // Default is blocking mode
  _blocking = true;
   e4aa2:	7462      	strb	r2, [r4, #17]
  HAL_USART_Init(serial, rx_buffer, tx_buffer);
   e4aa4:	461a      	mov	r2, r3
   e4aa6:	f7ff fb3d 	bl	e4124 <HAL_USART_Init>
}
   e4aaa:	4620      	mov	r0, r4
   e4aac:	bd10      	pop	{r4, pc}
   e4aae:	bf00      	nop
   e4ab0:	000eb4ec 	.word	0x000eb4ec

000e4ab4 <_ZN11USARTSerial9isEnabledEv>:
USARTSerial::operator bool() {
  return true;
}

bool USARTSerial::isEnabled() {
  return HAL_USART_Is_Enabled(_serial);
   e4ab4:	7c00      	ldrb	r0, [r0, #16]
   e4ab6:	f7ff bb65 	b.w	e4184 <HAL_USART_Is_Enabled>
	...

000e4abc <_Z22__fetch_global_Serial1v>:
static Ring_Buffer* serial1_rx_buffer = NULL;
static Ring_Buffer* serial1_tx_buffer = NULL;
#endif

USARTSerial& __fetch_global_Serial1()
{
   e4abc:	b538      	push	{r3, r4, r5, lr}
#if ((MODULE_FUNCTION == MOD_FUNC_USER_PART) || (MODULE_FUNCTION == MOD_FUNC_MONO_FIRMWARE))
	static USARTSerial serial1(HAL_USART_SERIAL1, &serial1_rx_buffer, &serial1_tx_buffer);
   e4abe:	4d0c      	ldr	r5, [pc, #48]	; (e4af0 <_Z22__fetch_global_Serial1v+0x34>)
   e4ac0:	6829      	ldr	r1, [r5, #0]
   e4ac2:	f011 0401 	ands.w	r4, r1, #1
   e4ac6:	d111      	bne.n	e4aec <_Z22__fetch_global_Serial1v+0x30>
   e4ac8:	4628      	mov	r0, r5
   e4aca:	f7ef faf7 	bl	d40bc <__cxa_guard_acquire>
   e4ace:	b168      	cbz	r0, e4aec <_Z22__fetch_global_Serial1v+0x30>
   e4ad0:	4a08      	ldr	r2, [pc, #32]	; (e4af4 <_Z22__fetch_global_Serial1v+0x38>)
   e4ad2:	4b09      	ldr	r3, [pc, #36]	; (e4af8 <_Z22__fetch_global_Serial1v+0x3c>)
   e4ad4:	4809      	ldr	r0, [pc, #36]	; (e4afc <_Z22__fetch_global_Serial1v+0x40>)
   e4ad6:	4621      	mov	r1, r4
   e4ad8:	f7ff ffd6 	bl	e4a88 <_ZN11USARTSerialC1E16HAL_USART_SerialP11Ring_BufferS2_>
   e4adc:	4628      	mov	r0, r5
   e4ade:	f7ef faf2 	bl	d40c6 <__cxa_guard_release>
   e4ae2:	4a07      	ldr	r2, [pc, #28]	; (e4b00 <_Z22__fetch_global_Serial1v+0x44>)
   e4ae4:	4907      	ldr	r1, [pc, #28]	; (e4b04 <_Z22__fetch_global_Serial1v+0x48>)
   e4ae6:	4805      	ldr	r0, [pc, #20]	; (e4afc <_Z22__fetch_global_Serial1v+0x40>)
   e4ae8:	f000 f99e 	bl	e4e28 <__aeabi_atexit>
    serial1_tx_buffer = new Ring_Buffer();
  }
  static USARTSerial serial1(HAL_USART_SERIAL1, serial1_rx_buffer, serial1_tx_buffer);
#endif
	return serial1;
}
   e4aec:	4803      	ldr	r0, [pc, #12]	; (e4afc <_Z22__fetch_global_Serial1v+0x40>)
   e4aee:	bd38      	pop	{r3, r4, r5, pc}
   e4af0:	2003e670 	.word	0x2003e670
   e4af4:	2003e674 	.word	0x2003e674
   e4af8:	2003e5ec 	.word	0x2003e5ec
   e4afc:	2003e5d8 	.word	0x2003e5d8
   e4b00:	2003c2d4 	.word	0x2003c2d4
   e4b04:	000e4a05 	.word	0x000e4a05

000e4b08 <_ZN9USBSerial14blockOnOverrunEb>:
  HAL_USB_USART_Flush_Data(_serial);
}

void USBSerial::blockOnOverrun(bool block)
{
  _blocking = block;
   e4b08:	7441      	strb	r1, [r0, #17]
   e4b0a:	4770      	bx	lr

000e4b0c <_ZN9USBSerialD1Ev>:
#include "usb_hal.h"
#include "system_task.h"
#include "spark_wiring_startup.h"
#include "concurrent_hal.h"

class USBSerial : public Stream
   e4b0c:	4770      	bx	lr

000e4b0e <_ZN9USBSerial4readEv>:
}


// Read data from buffer
int USBSerial::read()
{
   e4b0e:	b508      	push	{r3, lr}
	return std::max(-1, (int)HAL_USB_USART_Receive_Data(_serial, false));
   e4b10:	2100      	movs	r1, #0
   e4b12:	7c00      	ldrb	r0, [r0, #16]
   e4b14:	f7ff fb66 	bl	e41e4 <HAL_USB_USART_Receive_Data>
}
   e4b18:	ea30 0020 	bics.w	r0, r0, r0, asr #32
   e4b1c:	bf28      	it	cs
   e4b1e:	f04f 30ff 	movcs.w	r0, #4294967295	; 0xffffffff
   e4b22:	bd08      	pop	{r3, pc}

000e4b24 <_ZN9USBSerial4peekEv>:
{
  _blocking = block;
}

int USBSerial::peek()
{
   e4b24:	b508      	push	{r3, lr}
	return std::max(-1, (int)HAL_USB_USART_Receive_Data(_serial, true));
   e4b26:	2101      	movs	r1, #1
   e4b28:	7c00      	ldrb	r0, [r0, #16]
   e4b2a:	f7ff fb5b 	bl	e41e4 <HAL_USB_USART_Receive_Data>
}
   e4b2e:	ea30 0020 	bics.w	r0, r0, r0, asr #32
   e4b32:	bf28      	it	cs
   e4b34:	f04f 30ff 	movcs.w	r0, #4294967295	; 0xffffffff
   e4b38:	bd08      	pop	{r3, pc}

000e4b3a <_ZN9USBSerial17availableForWriteEv>:
{
	return std::max(-1, (int)HAL_USB_USART_Receive_Data(_serial, false));
}

int USBSerial::availableForWrite()
{
   e4b3a:	b508      	push	{r3, lr}
  return std::max(0, (int)HAL_USB_USART_Available_Data_For_Write(_serial));
   e4b3c:	7c00      	ldrb	r0, [r0, #16]
   e4b3e:	f7ff fb49 	bl	e41d4 <HAL_USB_USART_Available_Data_For_Write>
}
   e4b42:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
   e4b46:	bd08      	pop	{r3, pc}

000e4b48 <_ZN9USBSerial9availableEv>:

int USBSerial::available()
{
   e4b48:	b508      	push	{r3, lr}
	return std::max(0, (int)HAL_USB_USART_Available_Data(_serial));
   e4b4a:	7c00      	ldrb	r0, [r0, #16]
   e4b4c:	f7ff fb3a 	bl	e41c4 <HAL_USB_USART_Available_Data>
}
   e4b50:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
   e4b54:	bd08      	pop	{r3, pc}

000e4b56 <_ZN9USBSerial5flushEv>:
  return 0;
}

void USBSerial::flush()
{
  HAL_USB_USART_Flush_Data(_serial);
   e4b56:	7c00      	ldrb	r0, [r0, #16]
   e4b58:	f7ff bb54 	b.w	e4204 <HAL_USB_USART_Flush_Data>

000e4b5c <_ZN9USBSerialD0Ev>:
   e4b5c:	b510      	push	{r4, lr}
   e4b5e:	2114      	movs	r1, #20
   e4b60:	4604      	mov	r4, r0
   e4b62:	f000 f966 	bl	e4e32 <_ZdlPvj>
   e4b66:	4620      	mov	r0, r4
   e4b68:	bd10      	pop	{r4, pc}

000e4b6a <_ZN9USBSerial5writeEh>:
{
	return std::max(0, (int)HAL_USB_USART_Available_Data(_serial));
}

size_t USBSerial::write(uint8_t byte)
{
   e4b6a:	b538      	push	{r3, r4, r5, lr}
   e4b6c:	4604      	mov	r4, r0
  if (HAL_USB_USART_Available_Data_For_Write(_serial) > 0 || _blocking) {
   e4b6e:	7c00      	ldrb	r0, [r0, #16]
{
	return std::max(0, (int)HAL_USB_USART_Available_Data(_serial));
}

size_t USBSerial::write(uint8_t byte)
{
   e4b70:	460d      	mov	r5, r1
  if (HAL_USB_USART_Available_Data_For_Write(_serial) > 0 || _blocking) {
   e4b72:	f7ff fb2f 	bl	e41d4 <HAL_USB_USART_Available_Data_For_Write>
   e4b76:	2800      	cmp	r0, #0
   e4b78:	dc01      	bgt.n	e4b7e <_ZN9USBSerial5writeEh+0x14>
   e4b7a:	7c60      	ldrb	r0, [r4, #17]
   e4b7c:	b128      	cbz	r0, e4b8a <_ZN9USBSerial5writeEh+0x20>
    return std::max(0, (int)HAL_USB_USART_Send_Data(_serial, byte));
   e4b7e:	4629      	mov	r1, r5
   e4b80:	7c20      	ldrb	r0, [r4, #16]
   e4b82:	f7ff fb37 	bl	e41f4 <HAL_USB_USART_Send_Data>
   e4b86:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
  }
  return 0;
}
   e4b8a:	bd38      	pop	{r3, r4, r5, pc}

000e4b8c <_ZN9USBSerialC1E20HAL_USB_USART_SerialRK20HAL_USB_USART_Config>:

  HAL_USB_USART_Config conf = acquireSerialBuffer();
  HAL_USB_USART_Init(_serial, &conf);
}

USBSerial::USBSerial(HAL_USB_USART_Serial serial, const HAL_USB_USART_Config& conf)
   e4b8c:	b510      	push	{r4, lr}
   e4b8e:	4604      	mov	r4, r0
   e4b90:	2300      	movs	r3, #0
   e4b92:	6063      	str	r3, [r4, #4]
   e4b94:	f44f 737a 	mov.w	r3, #1000	; 0x3e8
   e4b98:	60a3      	str	r3, [r4, #8]
   e4b9a:	4b05      	ldr	r3, [pc, #20]	; (e4bb0 <_ZN9USBSerialC1E20HAL_USB_USART_SerialRK20HAL_USB_USART_Config+0x24>)
   e4b9c:	6023      	str	r3, [r4, #0]
{
  _serial = serial;
  _blocking = true;
   e4b9e:	2301      	movs	r3, #1

  HAL_USB_USART_Config conf = acquireSerialBuffer();
  HAL_USB_USART_Init(_serial, &conf);
}

USBSerial::USBSerial(HAL_USB_USART_Serial serial, const HAL_USB_USART_Config& conf)
   e4ba0:	4608      	mov	r0, r1
{
  _serial = serial;
   e4ba2:	7421      	strb	r1, [r4, #16]
  _blocking = true;
   e4ba4:	7463      	strb	r3, [r4, #17]

  HAL_USB_USART_Init(_serial, &conf);
   e4ba6:	4611      	mov	r1, r2
   e4ba8:	f7ff fafc 	bl	e41a4 <HAL_USB_USART_Init>
}
   e4bac:	4620      	mov	r0, r4
   e4bae:	bd10      	pop	{r4, pc}
   e4bb0:	000eb51c 	.word	0x000eb51c

000e4bb4 <_ZN9USBSerial5beginEl>:
// Public methods
//

void USBSerial::begin(long speed)
{
    HAL_USB_USART_Begin(_serial, speed, NULL);
   e4bb4:	2200      	movs	r2, #0
   e4bb6:	7c00      	ldrb	r0, [r0, #16]
   e4bb8:	f7ff bafc 	b.w	e41b4 <HAL_USB_USART_Begin>

000e4bbc <_Z19acquireSerialBufferv>:

// Preinstantiate Objects //////////////////////////////////////////////////////
#ifdef SPARK_USB_SERIAL

HAL_USB_USART_Config __attribute__((weak)) acquireSerialBuffer()
{
   e4bbc:	b510      	push	{r4, lr}
  HAL_USB_USART_Config conf = {0};
   e4bbe:	2214      	movs	r2, #20

// Preinstantiate Objects //////////////////////////////////////////////////////
#ifdef SPARK_USB_SERIAL

HAL_USB_USART_Config __attribute__((weak)) acquireSerialBuffer()
{
   e4bc0:	4604      	mov	r4, r0
  HAL_USB_USART_Config conf = {0};
   e4bc2:	2100      	movs	r1, #0
   e4bc4:	f002 fe18 	bl	e77f8 <memset>
  conf.rx_buffer_size = USB_RX_BUFFER_SIZE;
  conf.tx_buffer_size = USB_TX_BUFFER_SIZE;
#endif

  return conf;
}
   e4bc8:	4620      	mov	r0, r4
   e4bca:	bd10      	pop	{r4, pc}

000e4bcc <_Z16_fetch_usbserialv>:

USBSerial& _fetch_usbserial()
{
   e4bcc:	b530      	push	{r4, r5, lr}
  HAL_USB_USART_Config conf = acquireSerialBuffer();
	static USBSerial _usbserial(HAL_USB_USART_SERIAL, conf);
   e4bce:	4d0e      	ldr	r5, [pc, #56]	; (e4c08 <_Z16_fetch_usbserialv+0x3c>)

  return conf;
}

USBSerial& _fetch_usbserial()
{
   e4bd0:	b087      	sub	sp, #28
  HAL_USB_USART_Config conf = acquireSerialBuffer();
   e4bd2:	a801      	add	r0, sp, #4
   e4bd4:	f7ff fff2 	bl	e4bbc <_Z19acquireSerialBufferv>
	static USBSerial _usbserial(HAL_USB_USART_SERIAL, conf);
   e4bd8:	6829      	ldr	r1, [r5, #0]
   e4bda:	f011 0401 	ands.w	r4, r1, #1
   e4bde:	d110      	bne.n	e4c02 <_Z16_fetch_usbserialv+0x36>
   e4be0:	4628      	mov	r0, r5
   e4be2:	f7ef fa6b 	bl	d40bc <__cxa_guard_acquire>
   e4be6:	b160      	cbz	r0, e4c02 <_Z16_fetch_usbserialv+0x36>
   e4be8:	aa01      	add	r2, sp, #4
   e4bea:	4621      	mov	r1, r4
   e4bec:	4807      	ldr	r0, [pc, #28]	; (e4c0c <_Z16_fetch_usbserialv+0x40>)
   e4bee:	f7ff ffcd 	bl	e4b8c <_ZN9USBSerialC1E20HAL_USB_USART_SerialRK20HAL_USB_USART_Config>
   e4bf2:	4628      	mov	r0, r5
   e4bf4:	f7ef fa67 	bl	d40c6 <__cxa_guard_release>
   e4bf8:	4a05      	ldr	r2, [pc, #20]	; (e4c10 <_Z16_fetch_usbserialv+0x44>)
   e4bfa:	4906      	ldr	r1, [pc, #24]	; (e4c14 <_Z16_fetch_usbserialv+0x48>)
   e4bfc:	4803      	ldr	r0, [pc, #12]	; (e4c0c <_Z16_fetch_usbserialv+0x40>)
   e4bfe:	f000 f913 	bl	e4e28 <__aeabi_atexit>
	return _usbserial;
}
   e4c02:	4802      	ldr	r0, [pc, #8]	; (e4c0c <_Z16_fetch_usbserialv+0x40>)
   e4c04:	b007      	add	sp, #28
   e4c06:	bd30      	pop	{r4, r5, pc}
   e4c08:	2003e6f8 	.word	0x2003e6f8
   e4c0c:	2003e6fc 	.word	0x2003e6fc
   e4c10:	2003c2d4 	.word	0x2003c2d4
   e4c14:	000e4b0d 	.word	0x000e4b0d

000e4c18 <serialEventRun>:

/**
 * Provides background processing of serial data.
 */
void serialEventRun()
{
   e4c18:	b508      	push	{r3, lr}
    if (serialEvent && Serial.available()>0)
   e4c1a:	4b0c      	ldr	r3, [pc, #48]	; (e4c4c <serialEventRun+0x34>)
   e4c1c:	b133      	cbz	r3, e4c2c <serialEventRun+0x14>
   e4c1e:	f7ff ffd5 	bl	e4bcc <_Z16_fetch_usbserialv>
   e4c22:	6803      	ldr	r3, [r0, #0]
   e4c24:	691b      	ldr	r3, [r3, #16]
   e4c26:	4798      	blx	r3
   e4c28:	2800      	cmp	r0, #0
   e4c2a:	dc09      	bgt.n	e4c40 <serialEventRun+0x28>
        serialEvent();

    if (serialEvent1 && Serial1.available()>0)
   e4c2c:	4b08      	ldr	r3, [pc, #32]	; (e4c50 <serialEventRun+0x38>)
   e4c2e:	b163      	cbz	r3, e4c4a <serialEventRun+0x32>
   e4c30:	f7ff ff44 	bl	e4abc <_Z22__fetch_global_Serial1v>
   e4c34:	6803      	ldr	r3, [r0, #0]
   e4c36:	691b      	ldr	r3, [r3, #16]
   e4c38:	4798      	blx	r3
   e4c3a:	2800      	cmp	r0, #0
   e4c3c:	dc03      	bgt.n	e4c46 <serialEventRun+0x2e>
   e4c3e:	bd08      	pop	{r3, pc}
 * Provides background processing of serial data.
 */
void serialEventRun()
{
    if (serialEvent && Serial.available()>0)
        serialEvent();
   e4c40:	f3af 8000 	nop.w
   e4c44:	e7f2      	b.n	e4c2c <serialEventRun+0x14>

    if (serialEvent1 && Serial1.available()>0)
        serialEvent1();
   e4c46:	f3af 8000 	nop.w
   e4c4a:	bd08      	pop	{r3, pc}
	...

000e4c54 <_post_loop>:
#if Wiring_Serial5
void serialEvent5() __attribute__((weak));
#endif

void _post_loop()
{
   e4c54:	b508      	push	{r3, lr}
	serialEventRun();
   e4c56:	f7ff ffdf 	bl	e4c18 <serialEventRun>
		return !timeout_fn;
	}

	static inline system_tick_t current_time()
	{
		return HAL_Timer_Get_Milli_Seconds();
   e4c5a:	f7ff f9cb 	bl	e3ff4 <HAL_Timer_Get_Milli_Seconds>
	/**
	 * Lifesign that the application is still working normally.
	 */
	static void checkin()
	{
		last_checkin = current_time();
   e4c5e:	4b01      	ldr	r3, [pc, #4]	; (e4c64 <_post_loop+0x10>)
   e4c60:	6018      	str	r0, [r3, #0]
   e4c62:	bd08      	pop	{r3, pc}
   e4c64:	2003e714 	.word	0x2003e714

000e4c68 <_Z27ctrl_request_custom_handlerP12ctrl_request>:
bool __backup_ram_was_valid() { return false; }

#endif

// Default handler for CTRL_REQUEST_APP_CUSTOM requests
void __attribute((weak)) ctrl_request_custom_handler(ctrl_request* req) {
   e4c68:	b507      	push	{r0, r1, r2, lr}
    system_ctrl_set_result(req, SYSTEM_ERROR_NOT_SUPPORTED, nullptr, nullptr, nullptr);
   e4c6a:	2300      	movs	r3, #0
   e4c6c:	9300      	str	r3, [sp, #0]
   e4c6e:	461a      	mov	r2, r3
   e4c70:	f06f 0177 	mvn.w	r1, #119	; 0x77
   e4c74:	f7ff fae8 	bl	e4248 <system_ctrl_set_result>
}
   e4c78:	b003      	add	sp, #12
   e4c7a:	f85d fb04 	ldr.w	pc, [sp], #4
	...

000e4c80 <_ZL20ctrl_request_handlerP12ctrl_request>:
// Callback invoked to process a logging configuration request
void(*log_process_ctrl_request_callback)(ctrl_request* req) = nullptr;
#endif

// Application handler for control requests
static void ctrl_request_handler(ctrl_request* req) {
   e4c80:	b507      	push	{r0, r1, r2, lr}
    switch (req->type) {
   e4c82:	8843      	ldrh	r3, [r0, #2]
   e4c84:	2b0a      	cmp	r3, #10
   e4c86:	d008      	beq.n	e4c9a <_ZL20ctrl_request_handlerP12ctrl_request+0x1a>
   e4c88:	2b50      	cmp	r3, #80	; 0x50
   e4c8a:	d109      	bne.n	e4ca0 <_ZL20ctrl_request_handlerP12ctrl_request+0x20>
#if Wiring_LogConfig
    case CTRL_REQUEST_LOG_CONFIG: {
        if (log_process_ctrl_request_callback) {
   e4c8c:	4b09      	ldr	r3, [pc, #36]	; (e4cb4 <_ZL20ctrl_request_handlerP12ctrl_request+0x34>)
   e4c8e:	681b      	ldr	r3, [r3, #0]
   e4c90:	b13b      	cbz	r3, e4ca2 <_ZL20ctrl_request_handlerP12ctrl_request+0x22>
    }
    default:
        system_ctrl_set_result(req, SYSTEM_ERROR_NOT_SUPPORTED, nullptr, nullptr, nullptr);
        break;
    }
}
   e4c92:	b003      	add	sp, #12
   e4c94:	f85d eb04 	ldr.w	lr, [sp], #4
static void ctrl_request_handler(ctrl_request* req) {
    switch (req->type) {
#if Wiring_LogConfig
    case CTRL_REQUEST_LOG_CONFIG: {
        if (log_process_ctrl_request_callback) {
            log_process_ctrl_request_callback(req);
   e4c98:	4718      	bx	r3
        }
        break;
    }
#endif
    case CTRL_REQUEST_APP_CUSTOM: {
        ctrl_request_custom_handler(req);
   e4c9a:	f7ff ffe5 	bl	e4c68 <_Z27ctrl_request_custom_handlerP12ctrl_request>
        break;
   e4c9e:	e006      	b.n	e4cae <_ZL20ctrl_request_handlerP12ctrl_request+0x2e>
    }
    default:
        system_ctrl_set_result(req, SYSTEM_ERROR_NOT_SUPPORTED, nullptr, nullptr, nullptr);
   e4ca0:	2300      	movs	r3, #0
   e4ca2:	9300      	str	r3, [sp, #0]
   e4ca4:	461a      	mov	r2, r3
   e4ca6:	f06f 0177 	mvn.w	r1, #119	; 0x77
   e4caa:	f7ff facd 	bl	e4248 <system_ctrl_set_result>
        break;
    }
}
   e4cae:	b003      	add	sp, #12
   e4cb0:	f85d fb04 	ldr.w	pc, [sp], #4
   e4cb4:	2003e710 	.word	0x2003e710

000e4cb8 <module_user_init_hook>:

void module_user_init_hook()
{
   e4cb8:	b510      	push	{r4, lr}
    }
#endif

#if HAL_PLATFORM_RNG
    // Initialize the default stdlib PRNG using hardware RNG as a seed
    const uint32_t seed = HAL_RNG_GetRandomNumber();
   e4cba:	f7ff f98b 	bl	e3fd4 <HAL_RNG_GetRandomNumber>
   e4cbe:	4604      	mov	r4, r0
    srand(seed);
   e4cc0:	f002 fda2 	bl	e7808 <srand>

    // If the user defines random_seed_from_cloud, call it with a seed value
    // generated by a hardware RNG as well.
    if (random_seed_from_cloud) {
   e4cc4:	4b07      	ldr	r3, [pc, #28]	; (e4ce4 <module_user_init_hook+0x2c>)
   e4cc6:	b113      	cbz	r3, e4cce <module_user_init_hook+0x16>
        random_seed_from_cloud(seed);
   e4cc8:	4620      	mov	r0, r4
   e4cca:	f3af 8000 	nop.w
    }
#endif
    // Register the random_seed_from_cloud handler
    spark_set_random_seed_from_cloud_handler(&random_seed_from_cloud, nullptr);
   e4cce:	2100      	movs	r1, #0
   e4cd0:	4804      	ldr	r0, [pc, #16]	; (e4ce4 <module_user_init_hook+0x2c>)
   e4cd2:	f7ff fac3 	bl	e425c <spark_set_random_seed_from_cloud_handler>

    // Register application handler for the control requests
    system_ctrl_set_app_request_handler(ctrl_request_handler, nullptr);
   e4cd6:	2100      	movs	r1, #0
   e4cd8:	4803      	ldr	r0, [pc, #12]	; (e4ce8 <module_user_init_hook+0x30>)
}
   e4cda:	e8bd 4010 	ldmia.w	sp!, {r4, lr}
#endif
    // Register the random_seed_from_cloud handler
    spark_set_random_seed_from_cloud_handler(&random_seed_from_cloud, nullptr);

    // Register application handler for the control requests
    system_ctrl_set_app_request_handler(ctrl_request_handler, nullptr);
   e4cde:	f7ff baa9 	b.w	e4234 <system_ctrl_set_app_request_handler>
   e4ce2:	bf00      	nop
   e4ce4:	00000000 	.word	0x00000000
   e4ce8:	000e4c81 	.word	0x000e4c81

000e4cec <pinAvailable>:

/*
 * @brief Perform safety check on desired pin to see if it's already
 * being used.  Return 0 if used, otherwise return 1 if available.
 */
bool pinAvailable(uint16_t pin) {
   e4cec:	b510      	push	{r4, lr}
   e4cee:	4604      	mov	r4, r0

  // SPI safety check
#ifndef SPARK_WIRING_NO_SPI
  if(SPI.isEnabled() == true && (pin == SCK || pin == MOSI || pin == MISO))
   e4cf0:	480f      	ldr	r0, [pc, #60]	; (e4d30 <pinAvailable+0x44>)
   e4cf2:	f7ff fe6d 	bl	e49d0 <_ZN8SPIClass9isEnabledEv>
   e4cf6:	b128      	cbz	r0, e4d04 <pinAvailable+0x18>
   e4cf8:	f1a4 030b 	sub.w	r3, r4, #11
   e4cfc:	2b02      	cmp	r3, #2
   e4cfe:	d801      	bhi.n	e4d04 <pinAvailable+0x18>
  {
    return 0; // 'pin' is used
   e4d00:	2000      	movs	r0, #0
   e4d02:	bd10      	pop	{r4, pc}
  }
#endif
  // I2C safety check
#ifndef SPARK_WIRING_NO_I2C
  if(Wire.isEnabled() == true && (pin == SCL || pin == SDA))
   e4d04:	f000 f84e 	bl	e4da4 <_Z19__fetch_global_Wirev>
   e4d08:	f7ff fc02 	bl	e4510 <_ZN7TwoWire9isEnabledEv>
   e4d0c:	b108      	cbz	r0, e4d12 <pinAvailable+0x26>
   e4d0e:	2c01      	cmp	r4, #1
   e4d10:	d9f6      	bls.n	e4d00 <pinAvailable+0x14>
    return 0; // 'pin' is used
  }
#endif
#ifndef SPARK_WIRING_NO_USART_SERIAL
  // Serial1 safety check
  if(Serial1.isEnabled() == true && (pin == RX || pin == TX))
   e4d12:	f7ff fed3 	bl	e4abc <_Z22__fetch_global_Serial1v>
   e4d16:	f7ff fecd 	bl	e4ab4 <_ZN11USARTSerial9isEnabledEv>
   e4d1a:	b118      	cbz	r0, e4d24 <pinAvailable+0x38>
   e4d1c:	f1a4 0309 	sub.w	r3, r4, #9
   e4d20:	2b01      	cmp	r3, #1
   e4d22:	d9ed      	bls.n	e4d00 <pinAvailable+0x14>
  {
    return 0; // 'pin' is used
  }
#endif

  if (pin >= TOTAL_PINS)
   e4d24:	2c23      	cmp	r4, #35	; 0x23
   e4d26:	bf8c      	ite	hi
   e4d28:	2000      	movhi	r0, #0
   e4d2a:	2001      	movls	r0, #1
    return 0;
  else
    return 1; // 'pin' is available
}
   e4d2c:	bd10      	pop	{r4, pc}
   e4d2e:	bf00      	nop
   e4d30:	2003e730 	.word	0x2003e730

000e4d34 <pinMode>:
 * or INPUT_PULLDOWN
 */
void pinMode(uint16_t pin, PinMode setMode)
{

  if(pin >= TOTAL_PINS || setMode == PIN_MODE_NONE )
   e4d34:	2823      	cmp	r0, #35	; 0x23
/*
 * @brief Set the mode of the pin to OUTPUT, INPUT, INPUT_PULLUP,
 * or INPUT_PULLDOWN
 */
void pinMode(uint16_t pin, PinMode setMode)
{
   e4d36:	b538      	push	{r3, r4, r5, lr}
   e4d38:	4604      	mov	r4, r0
   e4d3a:	460d      	mov	r5, r1

  if(pin >= TOTAL_PINS || setMode == PIN_MODE_NONE )
   e4d3c:	d80a      	bhi.n	e4d54 <pinMode+0x20>
   e4d3e:	29ff      	cmp	r1, #255	; 0xff
   e4d40:	d008      	beq.n	e4d54 <pinMode+0x20>
  {
    return;
  }

  // Safety check
  if( !pinAvailable(pin) ) {
   e4d42:	f7ff ffd3 	bl	e4cec <pinAvailable>
   e4d46:	b128      	cbz	r0, e4d54 <pinMode+0x20>
    return;
  }

  HAL_Pin_Mode(pin, setMode);
   e4d48:	4629      	mov	r1, r5
   e4d4a:	4620      	mov	r0, r4
}
   e4d4c:	e8bd 4038 	ldmia.w	sp!, {r3, r4, r5, lr}
  // Safety check
  if( !pinAvailable(pin) ) {
    return;
  }

  HAL_Pin_Mode(pin, setMode);
   e4d50:	f7ff b968 	b.w	e4024 <HAL_Pin_Mode>
   e4d54:	bd38      	pop	{r3, r4, r5, pc}

000e4d56 <_Z11analogWritetm>:
/*
 * @brief Should take an integer 0-255 and create a 500Hz PWM signal with a duty cycle from 0-100%.
 * On Photon, DAC1 and DAC2 act as true analog outputs(values: 0 to 4095) using onchip DAC peripheral
 */
void analogWrite(pin_t pin, uint32_t value)
{
   e4d56:	b538      	push	{r3, r4, r5, lr}
   e4d58:	4604      	mov	r4, r0
   e4d5a:	460d      	mov	r5, r1
    // Safety check
    if (!pinAvailable(pin))
   e4d5c:	f7ff ffc6 	bl	e4cec <pinAvailable>
   e4d60:	b1f0      	cbz	r0, e4da0 <_Z11analogWritetm+0x4a>
    {
        return;
    }

    if (HAL_Validate_Pin_Function(pin, PF_DAC) == PF_DAC)
   e4d62:	2104      	movs	r1, #4
   e4d64:	4620      	mov	r0, r4
   e4d66:	f7ff f955 	bl	e4014 <HAL_Validate_Pin_Function>
   e4d6a:	2804      	cmp	r0, #4
   e4d6c:	d105      	bne.n	e4d7a <_Z11analogWritetm+0x24>
    {
        HAL_DAC_Write(pin, value);
   e4d6e:	b2a9      	uxth	r1, r5
   e4d70:	4620      	mov	r0, r4
            return;
        }

        HAL_PWM_Write_Ext(pin, value);
    }
}
   e4d72:	e8bd 4038 	ldmia.w	sp!, {r3, r4, r5, lr}
        return;
    }

    if (HAL_Validate_Pin_Function(pin, PF_DAC) == PF_DAC)
    {
        HAL_DAC_Write(pin, value);
   e4d76:	f7ff b965 	b.w	e4044 <HAL_DAC_Write>
    }
    else if (HAL_Validate_Pin_Function(pin, PF_TIMER) == PF_TIMER)
   e4d7a:	2102      	movs	r1, #2
   e4d7c:	4620      	mov	r0, r4
   e4d7e:	f7ff f949 	bl	e4014 <HAL_Validate_Pin_Function>
   e4d82:	2802      	cmp	r0, #2
   e4d84:	d10c      	bne.n	e4da0 <_Z11analogWritetm+0x4a>
    {
        PinMode mode = HAL_Get_Pin_Mode(pin);
   e4d86:	4620      	mov	r0, r4
   e4d88:	f7ff f954 	bl	e4034 <HAL_Get_Pin_Mode>

        if (mode != OUTPUT && mode != AF_OUTPUT_PUSHPULL)
   e4d8c:	2801      	cmp	r0, #1
   e4d8e:	d001      	beq.n	e4d94 <_Z11analogWritetm+0x3e>
   e4d90:	2804      	cmp	r0, #4
   e4d92:	d105      	bne.n	e4da0 <_Z11analogWritetm+0x4a>
        {
            return;
        }

        HAL_PWM_Write_Ext(pin, value);
   e4d94:	4629      	mov	r1, r5
   e4d96:	4620      	mov	r0, r4
    }
}
   e4d98:	e8bd 4038 	ldmia.w	sp!, {r3, r4, r5, lr}
        if (mode != OUTPUT && mode != AF_OUTPUT_PUSHPULL)
        {
            return;
        }

        HAL_PWM_Write_Ext(pin, value);
   e4d9c:	f7ff b95a 	b.w	e4054 <HAL_PWM_Write_Ext>
   e4da0:	bd38      	pop	{r3, r4, r5, pc}
	...

000e4da4 <_Z19__fetch_global_Wirev>:
#include "i2c_hal.h"

#ifndef SPARK_WIRING_NO_I2C

TwoWire& __fetch_global_Wire()
{
   e4da4:	b538      	push	{r3, r4, r5, lr}
	static TwoWire wire(HAL_I2C_INTERFACE1);
   e4da6:	4d0b      	ldr	r5, [pc, #44]	; (e4dd4 <_Z19__fetch_global_Wirev+0x30>)
   e4da8:	6829      	ldr	r1, [r5, #0]
   e4daa:	f011 0401 	ands.w	r4, r1, #1
   e4dae:	d10f      	bne.n	e4dd0 <_Z19__fetch_global_Wirev+0x2c>
   e4db0:	4628      	mov	r0, r5
   e4db2:	f7ef f983 	bl	d40bc <__cxa_guard_acquire>
   e4db6:	b158      	cbz	r0, e4dd0 <_Z19__fetch_global_Wirev+0x2c>
   e4db8:	4621      	mov	r1, r4
   e4dba:	4807      	ldr	r0, [pc, #28]	; (e4dd8 <_Z19__fetch_global_Wirev+0x34>)
   e4dbc:	f7ff fb96 	bl	e44ec <_ZN7TwoWireC1E17HAL_I2C_Interface>
   e4dc0:	4628      	mov	r0, r5
   e4dc2:	f7ef f980 	bl	d40c6 <__cxa_guard_release>
   e4dc6:	4a05      	ldr	r2, [pc, #20]	; (e4ddc <_Z19__fetch_global_Wirev+0x38>)
   e4dc8:	4905      	ldr	r1, [pc, #20]	; (e4de0 <_Z19__fetch_global_Wirev+0x3c>)
   e4dca:	4803      	ldr	r0, [pc, #12]	; (e4dd8 <_Z19__fetch_global_Wirev+0x34>)
   e4dcc:	f000 f82c 	bl	e4e28 <__aeabi_atexit>
	return wire;
}
   e4dd0:	4801      	ldr	r0, [pc, #4]	; (e4dd8 <_Z19__fetch_global_Wirev+0x34>)
   e4dd2:	bd38      	pop	{r3, r4, r5, pc}
   e4dd4:	2003e718 	.word	0x2003e718
   e4dd8:	2003e71c 	.word	0x2003e71c
   e4ddc:	2003c2d4 	.word	0x2003c2d4
   e4de0:	000e4495 	.word	0x000e4495

000e4de4 <_GLOBAL__sub_I_SPI>:
#ifndef SPARK_WIRING_NO_SPI

SPIClass SPI(HAL_SPI_INTERFACE1);

#if Wiring_SPI1
SPIClass SPI1(HAL_SPI_INTERFACE2);
   e4de4:	b570      	push	{r4, r5, r6, lr}
#include "core_hal.h"
#include "spark_macros.h"

#ifndef SPARK_WIRING_NO_SPI

SPIClass SPI(HAL_SPI_INTERFACE1);
   e4de6:	4c0c      	ldr	r4, [pc, #48]	; (e4e18 <_GLOBAL__sub_I_SPI+0x34>)
   e4de8:	4e0c      	ldr	r6, [pc, #48]	; (e4e1c <_GLOBAL__sub_I_SPI+0x38>)
   e4dea:	4d0d      	ldr	r5, [pc, #52]	; (e4e20 <_GLOBAL__sub_I_SPI+0x3c>)
   e4dec:	2100      	movs	r1, #0
   e4dee:	4620      	mov	r0, r4
   e4df0:	f7ff fdda 	bl	e49a8 <_ZN8SPIClassC1E17HAL_SPI_Interface>
   e4df4:	4620      	mov	r0, r4

#if Wiring_SPI1
SPIClass SPI1(HAL_SPI_INTERFACE2);
   e4df6:	4c0b      	ldr	r4, [pc, #44]	; (e4e24 <_GLOBAL__sub_I_SPI+0x40>)
#include "core_hal.h"
#include "spark_macros.h"

#ifndef SPARK_WIRING_NO_SPI

SPIClass SPI(HAL_SPI_INTERFACE1);
   e4df8:	4632      	mov	r2, r6
   e4dfa:	4629      	mov	r1, r5
   e4dfc:	f000 f814 	bl	e4e28 <__aeabi_atexit>

#if Wiring_SPI1
SPIClass SPI1(HAL_SPI_INTERFACE2);
   e4e00:	2101      	movs	r1, #1
   e4e02:	4620      	mov	r0, r4
   e4e04:	f7ff fdd0 	bl	e49a8 <_ZN8SPIClassC1E17HAL_SPI_Interface>
   e4e08:	4632      	mov	r2, r6
   e4e0a:	4629      	mov	r1, r5
   e4e0c:	4620      	mov	r0, r4
   e4e0e:	e8bd 4070 	ldmia.w	sp!, {r4, r5, r6, lr}
   e4e12:	f000 b809 	b.w	e4e28 <__aeabi_atexit>
   e4e16:	bf00      	nop
   e4e18:	2003e730 	.word	0x2003e730
   e4e1c:	2003c2d4 	.word	0x2003c2d4
   e4e20:	000e4999 	.word	0x000e4999
   e4e24:	2003e740 	.word	0x2003e740

000e4e28 <__aeabi_atexit>:
   e4e28:	460b      	mov	r3, r1
   e4e2a:	4601      	mov	r1, r0
   e4e2c:	4618      	mov	r0, r3
   e4e2e:	f002 bca7 	b.w	e7780 <__cxa_atexit>

000e4e32 <_ZdlPvj>:
   e4e32:	f7ef b934 	b.w	d409e <_ZdlPv>
	...

000e4e38 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj>:
   e4e38:	4b24      	ldr	r3, [pc, #144]	; (e4ecc <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0x94>)
   e4e3a:	681a      	ldr	r2, [r3, #0]
   e4e3c:	07d0      	lsls	r0, r2, #31
   e4e3e:	bf5c      	itt	pl
   e4e40:	2201      	movpl	r2, #1
   e4e42:	601a      	strpl	r2, [r3, #0]
   e4e44:	4b22      	ldr	r3, [pc, #136]	; (e4ed0 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0x98>)
   e4e46:	681a      	ldr	r2, [r3, #0]
   e4e48:	07d1      	lsls	r1, r2, #31
   e4e4a:	bf5c      	itt	pl
   e4e4c:	2201      	movpl	r2, #1
   e4e4e:	601a      	strpl	r2, [r3, #0]
   e4e50:	4b20      	ldr	r3, [pc, #128]	; (e4ed4 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0x9c>)
   e4e52:	681a      	ldr	r2, [r3, #0]
   e4e54:	07d2      	lsls	r2, r2, #31
   e4e56:	bf5c      	itt	pl
   e4e58:	2201      	movpl	r2, #1
   e4e5a:	601a      	strpl	r2, [r3, #0]
   e4e5c:	4b1e      	ldr	r3, [pc, #120]	; (e4ed8 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xa0>)
   e4e5e:	681a      	ldr	r2, [r3, #0]
   e4e60:	07d0      	lsls	r0, r2, #31
   e4e62:	bf5c      	itt	pl
   e4e64:	2201      	movpl	r2, #1
   e4e66:	601a      	strpl	r2, [r3, #0]
   e4e68:	4b1c      	ldr	r3, [pc, #112]	; (e4edc <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xa4>)
   e4e6a:	681a      	ldr	r2, [r3, #0]
   e4e6c:	07d1      	lsls	r1, r2, #31
   e4e6e:	bf5c      	itt	pl
   e4e70:	2201      	movpl	r2, #1
   e4e72:	601a      	strpl	r2, [r3, #0]
   e4e74:	4b1a      	ldr	r3, [pc, #104]	; (e4ee0 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xa8>)
   e4e76:	681a      	ldr	r2, [r3, #0]
   e4e78:	07d2      	lsls	r2, r2, #31
   e4e7a:	bf5c      	itt	pl
   e4e7c:	2201      	movpl	r2, #1
   e4e7e:	601a      	strpl	r2, [r3, #0]
   e4e80:	4b18      	ldr	r3, [pc, #96]	; (e4ee4 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xac>)
   e4e82:	681a      	ldr	r2, [r3, #0]
   e4e84:	07d0      	lsls	r0, r2, #31
   e4e86:	bf5c      	itt	pl
   e4e88:	2201      	movpl	r2, #1
   e4e8a:	601a      	strpl	r2, [r3, #0]
   e4e8c:	4b16      	ldr	r3, [pc, #88]	; (e4ee8 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xb0>)
   e4e8e:	681a      	ldr	r2, [r3, #0]
   e4e90:	07d1      	lsls	r1, r2, #31
   e4e92:	bf5c      	itt	pl
   e4e94:	2201      	movpl	r2, #1
   e4e96:	601a      	strpl	r2, [r3, #0]
   e4e98:	4b14      	ldr	r3, [pc, #80]	; (e4eec <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xb4>)
   e4e9a:	681a      	ldr	r2, [r3, #0]
   e4e9c:	07d2      	lsls	r2, r2, #31
   e4e9e:	bf5c      	itt	pl
   e4ea0:	2201      	movpl	r2, #1
   e4ea2:	601a      	strpl	r2, [r3, #0]
   e4ea4:	4b12      	ldr	r3, [pc, #72]	; (e4ef0 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xb8>)
   e4ea6:	681a      	ldr	r2, [r3, #0]
   e4ea8:	07d0      	lsls	r0, r2, #31
   e4eaa:	bf5c      	itt	pl
   e4eac:	2201      	movpl	r2, #1
   e4eae:	601a      	strpl	r2, [r3, #0]
   e4eb0:	4b10      	ldr	r3, [pc, #64]	; (e4ef4 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xbc>)
   e4eb2:	681a      	ldr	r2, [r3, #0]
   e4eb4:	07d1      	lsls	r1, r2, #31
   e4eb6:	bf5c      	itt	pl
   e4eb8:	2201      	movpl	r2, #1
   e4eba:	601a      	strpl	r2, [r3, #0]
   e4ebc:	4b0e      	ldr	r3, [pc, #56]	; (e4ef8 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xc0>)
   e4ebe:	681a      	ldr	r2, [r3, #0]
   e4ec0:	07d2      	lsls	r2, r2, #31
   e4ec2:	bf5c      	itt	pl
   e4ec4:	2201      	movpl	r2, #1
   e4ec6:	601a      	strpl	r2, [r3, #0]
   e4ec8:	4770      	bx	lr
   e4eca:	bf00      	nop
   e4ecc:	2003e77c 	.word	0x2003e77c
   e4ed0:	2003e778 	.word	0x2003e778
   e4ed4:	2003e774 	.word	0x2003e774
   e4ed8:	2003e770 	.word	0x2003e770
   e4edc:	2003e76c 	.word	0x2003e76c
   e4ee0:	2003e768 	.word	0x2003e768
   e4ee4:	2003e764 	.word	0x2003e764
   e4ee8:	2003e760 	.word	0x2003e760
   e4eec:	2003e75c 	.word	0x2003e75c
   e4ef0:	2003e758 	.word	0x2003e758
   e4ef4:	2003e754 	.word	0x2003e754
   e4ef8:	2003e750 	.word	0x2003e750

000e4efc <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj>:
   e4efc:	4b18      	ldr	r3, [pc, #96]	; (e4f60 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x64>)
   e4efe:	681a      	ldr	r2, [r3, #0]
   e4f00:	07d1      	lsls	r1, r2, #31
   e4f02:	bf5c      	itt	pl
   e4f04:	2201      	movpl	r2, #1
   e4f06:	601a      	strpl	r2, [r3, #0]
   e4f08:	4b16      	ldr	r3, [pc, #88]	; (e4f64 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x68>)
   e4f0a:	681a      	ldr	r2, [r3, #0]
   e4f0c:	07d2      	lsls	r2, r2, #31
   e4f0e:	bf5c      	itt	pl
   e4f10:	2201      	movpl	r2, #1
   e4f12:	601a      	strpl	r2, [r3, #0]
   e4f14:	4b14      	ldr	r3, [pc, #80]	; (e4f68 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x6c>)
   e4f16:	681a      	ldr	r2, [r3, #0]
   e4f18:	07d0      	lsls	r0, r2, #31
   e4f1a:	bf5c      	itt	pl
   e4f1c:	2201      	movpl	r2, #1
   e4f1e:	601a      	strpl	r2, [r3, #0]
   e4f20:	4b12      	ldr	r3, [pc, #72]	; (e4f6c <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x70>)
   e4f22:	681a      	ldr	r2, [r3, #0]
   e4f24:	07d1      	lsls	r1, r2, #31
   e4f26:	bf5c      	itt	pl
   e4f28:	2201      	movpl	r2, #1
   e4f2a:	601a      	strpl	r2, [r3, #0]
   e4f2c:	4b10      	ldr	r3, [pc, #64]	; (e4f70 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x74>)
   e4f2e:	681a      	ldr	r2, [r3, #0]
   e4f30:	07d2      	lsls	r2, r2, #31
   e4f32:	bf5c      	itt	pl
   e4f34:	2201      	movpl	r2, #1
   e4f36:	601a      	strpl	r2, [r3, #0]
   e4f38:	4b0e      	ldr	r3, [pc, #56]	; (e4f74 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x78>)
   e4f3a:	681a      	ldr	r2, [r3, #0]
   e4f3c:	07d0      	lsls	r0, r2, #31
   e4f3e:	bf5c      	itt	pl
   e4f40:	2201      	movpl	r2, #1
   e4f42:	601a      	strpl	r2, [r3, #0]
   e4f44:	4b0c      	ldr	r3, [pc, #48]	; (e4f78 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x7c>)
   e4f46:	681a      	ldr	r2, [r3, #0]
   e4f48:	07d1      	lsls	r1, r2, #31
   e4f4a:	bf5c      	itt	pl
   e4f4c:	2201      	movpl	r2, #1
   e4f4e:	601a      	strpl	r2, [r3, #0]
   e4f50:	4b0a      	ldr	r3, [pc, #40]	; (e4f7c <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x80>)
   e4f52:	681a      	ldr	r2, [r3, #0]
   e4f54:	07d2      	lsls	r2, r2, #31
   e4f56:	bf5c      	itt	pl
   e4f58:	2201      	movpl	r2, #1
   e4f5a:	601a      	strpl	r2, [r3, #0]
   e4f5c:	4770      	bx	lr
   e4f5e:	bf00      	nop
   e4f60:	2003e79c 	.word	0x2003e79c
   e4f64:	2003e798 	.word	0x2003e798
   e4f68:	2003e794 	.word	0x2003e794
   e4f6c:	2003e790 	.word	0x2003e790
   e4f70:	2003e78c 	.word	0x2003e78c
   e4f74:	2003e788 	.word	0x2003e788
   e4f78:	2003e784 	.word	0x2003e784
   e4f7c:	2003e780 	.word	0x2003e780

000e4f80 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj>:
   e4f80:	4b18      	ldr	r3, [pc, #96]	; (e4fe4 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x64>)
   e4f82:	681a      	ldr	r2, [r3, #0]
   e4f84:	07d1      	lsls	r1, r2, #31
   e4f86:	bf5c      	itt	pl
   e4f88:	2201      	movpl	r2, #1
   e4f8a:	601a      	strpl	r2, [r3, #0]
   e4f8c:	4b16      	ldr	r3, [pc, #88]	; (e4fe8 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x68>)
   e4f8e:	681a      	ldr	r2, [r3, #0]
   e4f90:	07d2      	lsls	r2, r2, #31
   e4f92:	bf5c      	itt	pl
   e4f94:	2201      	movpl	r2, #1
   e4f96:	601a      	strpl	r2, [r3, #0]
   e4f98:	4b14      	ldr	r3, [pc, #80]	; (e4fec <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x6c>)
   e4f9a:	681a      	ldr	r2, [r3, #0]
   e4f9c:	07d0      	lsls	r0, r2, #31
   e4f9e:	bf5c      	itt	pl
   e4fa0:	2201      	movpl	r2, #1
   e4fa2:	601a      	strpl	r2, [r3, #0]
   e4fa4:	4b12      	ldr	r3, [pc, #72]	; (e4ff0 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x70>)
   e4fa6:	681a      	ldr	r2, [r3, #0]
   e4fa8:	07d1      	lsls	r1, r2, #31
   e4faa:	bf5c      	itt	pl
   e4fac:	2201      	movpl	r2, #1
   e4fae:	601a      	strpl	r2, [r3, #0]
   e4fb0:	4b10      	ldr	r3, [pc, #64]	; (e4ff4 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x74>)
   e4fb2:	681a      	ldr	r2, [r3, #0]
   e4fb4:	07d2      	lsls	r2, r2, #31
   e4fb6:	bf5c      	itt	pl
   e4fb8:	2201      	movpl	r2, #1
   e4fba:	601a      	strpl	r2, [r3, #0]
   e4fbc:	4b0e      	ldr	r3, [pc, #56]	; (e4ff8 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x78>)
   e4fbe:	681a      	ldr	r2, [r3, #0]
   e4fc0:	07d0      	lsls	r0, r2, #31
   e4fc2:	bf5c      	itt	pl
   e4fc4:	2201      	movpl	r2, #1
   e4fc6:	601a      	strpl	r2, [r3, #0]
   e4fc8:	4b0c      	ldr	r3, [pc, #48]	; (e4ffc <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x7c>)
   e4fca:	681a      	ldr	r2, [r3, #0]
   e4fcc:	07d1      	lsls	r1, r2, #31
   e4fce:	bf5c      	itt	pl
   e4fd0:	2201      	movpl	r2, #1
   e4fd2:	601a      	strpl	r2, [r3, #0]
   e4fd4:	4b0a      	ldr	r3, [pc, #40]	; (e5000 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x80>)
   e4fd6:	681a      	ldr	r2, [r3, #0]
   e4fd8:	07d2      	lsls	r2, r2, #31
   e4fda:	bf5c      	itt	pl
   e4fdc:	2201      	movpl	r2, #1
   e4fde:	601a      	strpl	r2, [r3, #0]
   e4fe0:	4770      	bx	lr
   e4fe2:	bf00      	nop
   e4fe4:	2003e7bc 	.word	0x2003e7bc
   e4fe8:	2003e7b8 	.word	0x2003e7b8
   e4fec:	2003e7b4 	.word	0x2003e7b4
   e4ff0:	2003e7b0 	.word	0x2003e7b0
   e4ff4:	2003e7ac 	.word	0x2003e7ac
   e4ff8:	2003e7a8 	.word	0x2003e7a8
   e4ffc:	2003e7a4 	.word	0x2003e7a4
   e5000:	2003e7a0 	.word	0x2003e7a0

000e5004 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj>:
   e5004:	4b24      	ldr	r3, [pc, #144]	; (e5098 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0x94>)
   e5006:	681a      	ldr	r2, [r3, #0]
   e5008:	07d0      	lsls	r0, r2, #31
   e500a:	bf5c      	itt	pl
   e500c:	2201      	movpl	r2, #1
   e500e:	601a      	strpl	r2, [r3, #0]
   e5010:	4b22      	ldr	r3, [pc, #136]	; (e509c <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0x98>)
   e5012:	681a      	ldr	r2, [r3, #0]
   e5014:	07d1      	lsls	r1, r2, #31
   e5016:	bf5c      	itt	pl
   e5018:	2201      	movpl	r2, #1
   e501a:	601a      	strpl	r2, [r3, #0]
   e501c:	4b20      	ldr	r3, [pc, #128]	; (e50a0 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0x9c>)
   e501e:	681a      	ldr	r2, [r3, #0]
   e5020:	07d2      	lsls	r2, r2, #31
   e5022:	bf5c      	itt	pl
   e5024:	2201      	movpl	r2, #1
   e5026:	601a      	strpl	r2, [r3, #0]
   e5028:	4b1e      	ldr	r3, [pc, #120]	; (e50a4 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xa0>)
   e502a:	681a      	ldr	r2, [r3, #0]
   e502c:	07d0      	lsls	r0, r2, #31
   e502e:	bf5c      	itt	pl
   e5030:	2201      	movpl	r2, #1
   e5032:	601a      	strpl	r2, [r3, #0]
   e5034:	4b1c      	ldr	r3, [pc, #112]	; (e50a8 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xa4>)
   e5036:	681a      	ldr	r2, [r3, #0]
   e5038:	07d1      	lsls	r1, r2, #31
   e503a:	bf5c      	itt	pl
   e503c:	2201      	movpl	r2, #1
   e503e:	601a      	strpl	r2, [r3, #0]
   e5040:	4b1a      	ldr	r3, [pc, #104]	; (e50ac <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xa8>)
   e5042:	681a      	ldr	r2, [r3, #0]
   e5044:	07d2      	lsls	r2, r2, #31
   e5046:	bf5c      	itt	pl
   e5048:	2201      	movpl	r2, #1
   e504a:	601a      	strpl	r2, [r3, #0]
   e504c:	4b18      	ldr	r3, [pc, #96]	; (e50b0 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xac>)
   e504e:	681a      	ldr	r2, [r3, #0]
   e5050:	07d0      	lsls	r0, r2, #31
   e5052:	bf5c      	itt	pl
   e5054:	2201      	movpl	r2, #1
   e5056:	601a      	strpl	r2, [r3, #0]
   e5058:	4b16      	ldr	r3, [pc, #88]	; (e50b4 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xb0>)
   e505a:	681a      	ldr	r2, [r3, #0]
   e505c:	07d1      	lsls	r1, r2, #31
   e505e:	bf5c      	itt	pl
   e5060:	2201      	movpl	r2, #1
   e5062:	601a      	strpl	r2, [r3, #0]
   e5064:	4b14      	ldr	r3, [pc, #80]	; (e50b8 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xb4>)
   e5066:	681a      	ldr	r2, [r3, #0]
   e5068:	07d2      	lsls	r2, r2, #31
   e506a:	bf5c      	itt	pl
   e506c:	2201      	movpl	r2, #1
   e506e:	601a      	strpl	r2, [r3, #0]
   e5070:	4b12      	ldr	r3, [pc, #72]	; (e50bc <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xb8>)
   e5072:	681a      	ldr	r2, [r3, #0]
   e5074:	07d0      	lsls	r0, r2, #31
   e5076:	bf5c      	itt	pl
   e5078:	2201      	movpl	r2, #1
   e507a:	601a      	strpl	r2, [r3, #0]
   e507c:	4b10      	ldr	r3, [pc, #64]	; (e50c0 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xbc>)
   e507e:	681a      	ldr	r2, [r3, #0]
   e5080:	07d1      	lsls	r1, r2, #31
   e5082:	bf5c      	itt	pl
   e5084:	2201      	movpl	r2, #1
   e5086:	601a      	strpl	r2, [r3, #0]
   e5088:	4b0e      	ldr	r3, [pc, #56]	; (e50c4 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xc0>)
   e508a:	681a      	ldr	r2, [r3, #0]
   e508c:	07d2      	lsls	r2, r2, #31
   e508e:	bf5c      	itt	pl
   e5090:	2201      	movpl	r2, #1
   e5092:	601a      	strpl	r2, [r3, #0]
   e5094:	4770      	bx	lr
   e5096:	bf00      	nop
   e5098:	2003e7ec 	.word	0x2003e7ec
   e509c:	2003e7e8 	.word	0x2003e7e8
   e50a0:	2003e7e4 	.word	0x2003e7e4
   e50a4:	2003e7e0 	.word	0x2003e7e0
   e50a8:	2003e7dc 	.word	0x2003e7dc
   e50ac:	2003e7d8 	.word	0x2003e7d8
   e50b0:	2003e7d4 	.word	0x2003e7d4
   e50b4:	2003e7d0 	.word	0x2003e7d0
   e50b8:	2003e7cc 	.word	0x2003e7cc
   e50bc:	2003e7c8 	.word	0x2003e7c8
   e50c0:	2003e7c4 	.word	0x2003e7c4
   e50c4:	2003e7c0 	.word	0x2003e7c0

000e50c8 <floor>:
   e50c8:	ec51 0b10 	vmov	r0, r1, d0
   e50cc:	f3c1 530a 	ubfx	r3, r1, #20, #11
   e50d0:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
   e50d4:	f2a3 35ff 	subw	r5, r3, #1023	; 0x3ff
   e50d8:	2d13      	cmp	r5, #19
   e50da:	460c      	mov	r4, r1
   e50dc:	460f      	mov	r7, r1
   e50de:	ee10 6a10 	vmov	r6, s0
   e50e2:	dc1d      	bgt.n	e5120 <floor+0x58>
   e50e4:	2d00      	cmp	r5, #0
   e50e6:	db43      	blt.n	e5170 <floor+0xa8>
   e50e8:	4b3d      	ldr	r3, [pc, #244]	; (e51e0 <floor+0x118>)
   e50ea:	fa43 f805 	asr.w	r8, r3, r5
   e50ee:	ea01 0308 	and.w	r3, r1, r8
   e50f2:	4303      	orrs	r3, r0
   e50f4:	d019      	beq.n	e512a <floor+0x62>
   e50f6:	a338      	add	r3, pc, #224	; (adr r3, e51d8 <floor+0x110>)
   e50f8:	e9d3 2300 	ldrd	r2, r3, [r3]
   e50fc:	f001 fe20 	bl	e6d40 <__adddf3>
   e5100:	2200      	movs	r2, #0
   e5102:	2300      	movs	r3, #0
   e5104:	f002 fa5e 	bl	e75c4 <__aeabi_dcmpgt>
   e5108:	b120      	cbz	r0, e5114 <floor+0x4c>
   e510a:	2c00      	cmp	r4, #0
   e510c:	db49      	blt.n	e51a2 <floor+0xda>
   e510e:	ea27 0408 	bic.w	r4, r7, r8
   e5112:	2600      	movs	r6, #0
   e5114:	4623      	mov	r3, r4
   e5116:	4632      	mov	r2, r6
   e5118:	ec43 2b10 	vmov	d0, r2, r3
   e511c:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   e5120:	2d33      	cmp	r5, #51	; 0x33
   e5122:	dd06      	ble.n	e5132 <floor+0x6a>
   e5124:	f5b5 6f80 	cmp.w	r5, #1024	; 0x400
   e5128:	d032      	beq.n	e5190 <floor+0xc8>
   e512a:	ec41 0b10 	vmov	d0, r0, r1
   e512e:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   e5132:	f2a3 4313 	subw	r3, r3, #1043	; 0x413
   e5136:	f04f 38ff 	mov.w	r8, #4294967295	; 0xffffffff
   e513a:	fa28 f803 	lsr.w	r8, r8, r3
   e513e:	ea10 0f08 	tst.w	r0, r8
   e5142:	d0f2      	beq.n	e512a <floor+0x62>
   e5144:	a324      	add	r3, pc, #144	; (adr r3, e51d8 <floor+0x110>)
   e5146:	e9d3 2300 	ldrd	r2, r3, [r3]
   e514a:	f001 fdf9 	bl	e6d40 <__adddf3>
   e514e:	2200      	movs	r2, #0
   e5150:	2300      	movs	r3, #0
   e5152:	f002 fa37 	bl	e75c4 <__aeabi_dcmpgt>
   e5156:	2800      	cmp	r0, #0
   e5158:	d0dc      	beq.n	e5114 <floor+0x4c>
   e515a:	2c00      	cmp	r4, #0
   e515c:	db27      	blt.n	e51ae <floor+0xe6>
   e515e:	463c      	mov	r4, r7
   e5160:	ea26 0608 	bic.w	r6, r6, r8
   e5164:	4623      	mov	r3, r4
   e5166:	4632      	mov	r2, r6
   e5168:	ec43 2b10 	vmov	d0, r2, r3
   e516c:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   e5170:	a319      	add	r3, pc, #100	; (adr r3, e51d8 <floor+0x110>)
   e5172:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5176:	f001 fde3 	bl	e6d40 <__adddf3>
   e517a:	2200      	movs	r2, #0
   e517c:	2300      	movs	r3, #0
   e517e:	f002 fa21 	bl	e75c4 <__aeabi_dcmpgt>
   e5182:	2800      	cmp	r0, #0
   e5184:	d0c6      	beq.n	e5114 <floor+0x4c>
   e5186:	2c00      	cmp	r4, #0
   e5188:	db1c      	blt.n	e51c4 <floor+0xfc>
   e518a:	2600      	movs	r6, #0
   e518c:	4634      	mov	r4, r6
   e518e:	e7c1      	b.n	e5114 <floor+0x4c>
   e5190:	ee10 2a10 	vmov	r2, s0
   e5194:	460b      	mov	r3, r1
   e5196:	f001 fdd3 	bl	e6d40 <__adddf3>
   e519a:	ec41 0b10 	vmov	d0, r0, r1
   e519e:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   e51a2:	f44f 1380 	mov.w	r3, #1048576	; 0x100000
   e51a6:	fa43 f505 	asr.w	r5, r3, r5
   e51aa:	442f      	add	r7, r5
   e51ac:	e7af      	b.n	e510e <floor+0x46>
   e51ae:	2d14      	cmp	r5, #20
   e51b0:	d010      	beq.n	e51d4 <floor+0x10c>
   e51b2:	2301      	movs	r3, #1
   e51b4:	f1c5 0534 	rsb	r5, r5, #52	; 0x34
   e51b8:	fa03 f505 	lsl.w	r5, r3, r5
   e51bc:	19ae      	adds	r6, r5, r6
   e51be:	bf28      	it	cs
   e51c0:	18ff      	addcs	r7, r7, r3
   e51c2:	e7cc      	b.n	e515e <floor+0x96>
   e51c4:	f024 4200 	bic.w	r2, r4, #2147483648	; 0x80000000
   e51c8:	4b06      	ldr	r3, [pc, #24]	; (e51e4 <floor+0x11c>)
   e51ca:	4332      	orrs	r2, r6
   e51cc:	bf18      	it	ne
   e51ce:	461c      	movne	r4, r3
   e51d0:	2600      	movs	r6, #0
   e51d2:	e79f      	b.n	e5114 <floor+0x4c>
   e51d4:	3701      	adds	r7, #1
   e51d6:	e7c2      	b.n	e515e <floor+0x96>
   e51d8:	8800759c 	.word	0x8800759c
   e51dc:	7e37e43c 	.word	0x7e37e43c
   e51e0:	000fffff 	.word	0x000fffff
   e51e4:	bff00000 	.word	0xbff00000

000e51e8 <frexp>:
   e51e8:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
   e51ea:	ec53 2b10 	vmov	r2, r3, d0
   e51ee:	4e17      	ldr	r6, [pc, #92]	; (e524c <frexp+0x64>)
   e51f0:	f023 4100 	bic.w	r1, r3, #2147483648	; 0x80000000
   e51f4:	2500      	movs	r5, #0
   e51f6:	42b1      	cmp	r1, r6
   e51f8:	4604      	mov	r4, r0
   e51fa:	6005      	str	r5, [r0, #0]
   e51fc:	dc23      	bgt.n	e5246 <frexp+0x5e>
   e51fe:	ea52 0601 	orrs.w	r6, r2, r1
   e5202:	d020      	beq.n	e5246 <frexp+0x5e>
   e5204:	f5b1 1f80 	cmp.w	r1, #1048576	; 0x100000
   e5208:	4618      	mov	r0, r3
   e520a:	da0c      	bge.n	e5226 <frexp+0x3e>
   e520c:	4619      	mov	r1, r3
   e520e:	2200      	movs	r2, #0
   e5210:	ee10 0a10 	vmov	r0, s0
   e5214:	4b0e      	ldr	r3, [pc, #56]	; (e5250 <frexp+0x68>)
   e5216:	f001 ff45 	bl	e70a4 <__aeabi_dmul>
   e521a:	f06f 0535 	mvn.w	r5, #53	; 0x35
   e521e:	4602      	mov	r2, r0
   e5220:	4608      	mov	r0, r1
   e5222:	f021 4100 	bic.w	r1, r1, #2147483648	; 0x80000000
   e5226:	f020 40ff 	bic.w	r0, r0, #2139095040	; 0x7f800000
   e522a:	f420 00e0 	bic.w	r0, r0, #7340032	; 0x700000
   e522e:	1509      	asrs	r1, r1, #20
   e5230:	f040 537f 	orr.w	r3, r0, #1069547520	; 0x3fc00000
   e5234:	f2a1 31fe 	subw	r1, r1, #1022	; 0x3fe
   e5238:	4429      	add	r1, r5
   e523a:	f443 1300 	orr.w	r3, r3, #2097152	; 0x200000
   e523e:	6021      	str	r1, [r4, #0]
   e5240:	ec43 2b10 	vmov	d0, r2, r3
   e5244:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
   e5246:	ec43 2b10 	vmov	d0, r2, r3
   e524a:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
   e524c:	7fefffff 	.word	0x7fefffff
   e5250:	43500000 	.word	0x43500000

000e5254 <round>:
   e5254:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
   e5256:	ec53 2b10 	vmov	r2, r3, d0
   e525a:	f3c3 540a 	ubfx	r4, r3, #20, #11
   e525e:	f2a4 30ff 	subw	r0, r4, #1023	; 0x3ff
   e5262:	2813      	cmp	r0, #19
   e5264:	4619      	mov	r1, r3
   e5266:	ee10 7a10 	vmov	r7, s0
   e526a:	dc12      	bgt.n	e5292 <round+0x3e>
   e526c:	2800      	cmp	r0, #0
   e526e:	db32      	blt.n	e52d6 <round+0x82>
   e5270:	4e23      	ldr	r6, [pc, #140]	; (e5300 <round+0xac>)
   e5272:	4106      	asrs	r6, r0
   e5274:	4233      	tst	r3, r6
   e5276:	461d      	mov	r5, r3
   e5278:	d02a      	beq.n	e52d0 <round+0x7c>
   e527a:	f44f 2100 	mov.w	r1, #524288	; 0x80000
   e527e:	4101      	asrs	r1, r0
   e5280:	4429      	add	r1, r5
   e5282:	ea21 0106 	bic.w	r1, r1, r6
   e5286:	2400      	movs	r4, #0
   e5288:	460b      	mov	r3, r1
   e528a:	4622      	mov	r2, r4
   e528c:	ec43 2b10 	vmov	d0, r2, r3
   e5290:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
   e5292:	2833      	cmp	r0, #51	; 0x33
   e5294:	dd05      	ble.n	e52a2 <round+0x4e>
   e5296:	f5b0 6f80 	cmp.w	r0, #1024	; 0x400
   e529a:	d022      	beq.n	e52e2 <round+0x8e>
   e529c:	ec43 2b10 	vmov	d0, r2, r3
   e52a0:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
   e52a2:	f2a4 4413 	subw	r4, r4, #1043	; 0x413
   e52a6:	f04f 35ff 	mov.w	r5, #4294967295	; 0xffffffff
   e52aa:	fa25 f404 	lsr.w	r4, r5, r4
   e52ae:	4222      	tst	r2, r4
   e52b0:	d0f4      	beq.n	e529c <round+0x48>
   e52b2:	2301      	movs	r3, #1
   e52b4:	f1c0 0033 	rsb	r0, r0, #51	; 0x33
   e52b8:	fa03 f000 	lsl.w	r0, r3, r0
   e52bc:	19c0      	adds	r0, r0, r7
   e52be:	bf28      	it	cs
   e52c0:	18c9      	addcs	r1, r1, r3
   e52c2:	ea20 0404 	bic.w	r4, r0, r4
   e52c6:	460b      	mov	r3, r1
   e52c8:	4622      	mov	r2, r4
   e52ca:	ec43 2b10 	vmov	d0, r2, r3
   e52ce:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
   e52d0:	2a00      	cmp	r2, #0
   e52d2:	d1d2      	bne.n	e527a <round+0x26>
   e52d4:	e7e2      	b.n	e529c <round+0x48>
   e52d6:	3001      	adds	r0, #1
   e52d8:	f003 4100 	and.w	r1, r3, #2147483648	; 0x80000000
   e52dc:	d009      	beq.n	e52f2 <round+0x9e>
   e52de:	2400      	movs	r4, #0
   e52e0:	e7d2      	b.n	e5288 <round+0x34>
   e52e2:	ee10 0a10 	vmov	r0, s0
   e52e6:	4619      	mov	r1, r3
   e52e8:	f001 fd2a 	bl	e6d40 <__adddf3>
   e52ec:	ec41 0b10 	vmov	d0, r0, r1
   e52f0:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
   e52f2:	f041 517f 	orr.w	r1, r1, #1069547520	; 0x3fc00000
   e52f6:	f441 1140 	orr.w	r1, r1, #3145728	; 0x300000
   e52fa:	2400      	movs	r4, #0
   e52fc:	e7c4      	b.n	e5288 <round+0x34>
   e52fe:	bf00      	nop
   e5300:	000fffff 	.word	0x000fffff

000e5304 <ceilf>:
   e5304:	ee10 2a10 	vmov	r2, s0
   e5308:	f022 4100 	bic.w	r1, r2, #2147483648	; 0x80000000
   e530c:	0dcb      	lsrs	r3, r1, #23
   e530e:	3b7f      	subs	r3, #127	; 0x7f
   e5310:	2b16      	cmp	r3, #22
   e5312:	dc1c      	bgt.n	e534e <ceilf+0x4a>
   e5314:	2b00      	cmp	r3, #0
   e5316:	ee10 0a10 	vmov	r0, s0
   e531a:	db21      	blt.n	e5360 <ceilf+0x5c>
   e531c:	4919      	ldr	r1, [pc, #100]	; (e5384 <ceilf+0x80>)
   e531e:	4119      	asrs	r1, r3
   e5320:	420a      	tst	r2, r1
   e5322:	d01c      	beq.n	e535e <ceilf+0x5a>
   e5324:	eddf 7a18 	vldr	s15, [pc, #96]	; e5388 <ceilf+0x84>
   e5328:	ee70 7a27 	vadd.f32	s15, s0, s15
   e532c:	eef5 7ac0 	vcmpe.f32	s15, #0.0
   e5330:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5334:	dd13      	ble.n	e535e <ceilf+0x5a>
   e5336:	2a00      	cmp	r2, #0
   e5338:	dd04      	ble.n	e5344 <ceilf+0x40>
   e533a:	f44f 0200 	mov.w	r2, #8388608	; 0x800000
   e533e:	fa42 f303 	asr.w	r3, r2, r3
   e5342:	4418      	add	r0, r3
   e5344:	ea20 0301 	bic.w	r3, r0, r1
   e5348:	ee00 3a10 	vmov	s0, r3
   e534c:	4770      	bx	lr
   e534e:	f1b1 4fff 	cmp.w	r1, #2139095040	; 0x7f800000
   e5352:	d304      	bcc.n	e535e <ceilf+0x5a>
   e5354:	ee30 0a00 	vadd.f32	s0, s0, s0
   e5358:	4770      	bx	lr
   e535a:	ed9f 0a0c 	vldr	s0, [pc, #48]	; e538c <ceilf+0x88>
   e535e:	4770      	bx	lr
   e5360:	eddf 7a09 	vldr	s15, [pc, #36]	; e5388 <ceilf+0x84>
   e5364:	ee70 7a27 	vadd.f32	s15, s0, s15
   e5368:	eef5 7ac0 	vcmpe.f32	s15, #0.0
   e536c:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5370:	ddf5      	ble.n	e535e <ceilf+0x5a>
   e5372:	2a00      	cmp	r2, #0
   e5374:	dbf1      	blt.n	e535a <ceilf+0x56>
   e5376:	2900      	cmp	r1, #0
   e5378:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   e537c:	bf18      	it	ne
   e537e:	eeb0 0a67 	vmovne.f32	s0, s15
   e5382:	4770      	bx	lr
   e5384:	007fffff 	.word	0x007fffff
   e5388:	7149f2ca 	.word	0x7149f2ca
   e538c:	80000000 	.word	0x80000000

000e5390 <cosf>:
   e5390:	b500      	push	{lr}
   e5392:	ee10 3a10 	vmov	r3, s0
   e5396:	4a20      	ldr	r2, [pc, #128]	; (e5418 <cosf+0x88>)
   e5398:	f023 4300 	bic.w	r3, r3, #2147483648	; 0x80000000
   e539c:	4293      	cmp	r3, r2
   e539e:	b083      	sub	sp, #12
   e53a0:	dd19      	ble.n	e53d6 <cosf+0x46>
   e53a2:	f1b3 4fff 	cmp.w	r3, #2139095040	; 0x7f800000
   e53a6:	db04      	blt.n	e53b2 <cosf+0x22>
   e53a8:	ee30 0a40 	vsub.f32	s0, s0, s0
   e53ac:	b003      	add	sp, #12
   e53ae:	f85d fb04 	ldr.w	pc, [sp], #4
   e53b2:	4668      	mov	r0, sp
   e53b4:	f000 fe9e 	bl	e60f4 <__ieee754_rem_pio2f>
   e53b8:	f000 0003 	and.w	r0, r0, #3
   e53bc:	2801      	cmp	r0, #1
   e53be:	d01a      	beq.n	e53f6 <cosf+0x66>
   e53c0:	2802      	cmp	r0, #2
   e53c2:	d00f      	beq.n	e53e4 <cosf+0x54>
   e53c4:	b300      	cbz	r0, e5408 <cosf+0x78>
   e53c6:	2001      	movs	r0, #1
   e53c8:	eddd 0a01 	vldr	s1, [sp, #4]
   e53cc:	ed9d 0a00 	vldr	s0, [sp]
   e53d0:	f001 fbc4 	bl	e6b5c <__kernel_sinf>
   e53d4:	e7ea      	b.n	e53ac <cosf+0x1c>
   e53d6:	eddf 0a11 	vldr	s1, [pc, #68]	; e541c <cosf+0x8c>
   e53da:	f001 f829 	bl	e6430 <__kernel_cosf>
   e53de:	b003      	add	sp, #12
   e53e0:	f85d fb04 	ldr.w	pc, [sp], #4
   e53e4:	eddd 0a01 	vldr	s1, [sp, #4]
   e53e8:	ed9d 0a00 	vldr	s0, [sp]
   e53ec:	f001 f820 	bl	e6430 <__kernel_cosf>
   e53f0:	eeb1 0a40 	vneg.f32	s0, s0
   e53f4:	e7da      	b.n	e53ac <cosf+0x1c>
   e53f6:	eddd 0a01 	vldr	s1, [sp, #4]
   e53fa:	ed9d 0a00 	vldr	s0, [sp]
   e53fe:	f001 fbad 	bl	e6b5c <__kernel_sinf>
   e5402:	eeb1 0a40 	vneg.f32	s0, s0
   e5406:	e7d1      	b.n	e53ac <cosf+0x1c>
   e5408:	eddd 0a01 	vldr	s1, [sp, #4]
   e540c:	ed9d 0a00 	vldr	s0, [sp]
   e5410:	f001 f80e 	bl	e6430 <__kernel_cosf>
   e5414:	e7ca      	b.n	e53ac <cosf+0x1c>
   e5416:	bf00      	nop
   e5418:	3f490fd8 	.word	0x3f490fd8
   e541c:	00000000 	.word	0x00000000

000e5420 <floorf>:
   e5420:	ee10 2a10 	vmov	r2, s0
   e5424:	f022 4100 	bic.w	r1, r2, #2147483648	; 0x80000000
   e5428:	0dcb      	lsrs	r3, r1, #23
   e542a:	3b7f      	subs	r3, #127	; 0x7f
   e542c:	2b16      	cmp	r3, #22
   e542e:	dc17      	bgt.n	e5460 <floorf+0x40>
   e5430:	2b00      	cmp	r3, #0
   e5432:	ee10 0a10 	vmov	r0, s0
   e5436:	db19      	blt.n	e546c <floorf+0x4c>
   e5438:	491a      	ldr	r1, [pc, #104]	; (e54a4 <floorf+0x84>)
   e543a:	4119      	asrs	r1, r3
   e543c:	420a      	tst	r2, r1
   e543e:	d022      	beq.n	e5486 <floorf+0x66>
   e5440:	eddf 7a19 	vldr	s15, [pc, #100]	; e54a8 <floorf+0x88>
   e5444:	ee70 7a27 	vadd.f32	s15, s0, s15
   e5448:	eef5 7ac0 	vcmpe.f32	s15, #0.0
   e544c:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5450:	dd19      	ble.n	e5486 <floorf+0x66>
   e5452:	2a00      	cmp	r2, #0
   e5454:	db18      	blt.n	e5488 <floorf+0x68>
   e5456:	ea20 0301 	bic.w	r3, r0, r1
   e545a:	ee00 3a10 	vmov	s0, r3
   e545e:	4770      	bx	lr
   e5460:	f1b1 4fff 	cmp.w	r1, #2139095040	; 0x7f800000
   e5464:	d30f      	bcc.n	e5486 <floorf+0x66>
   e5466:	ee30 0a00 	vadd.f32	s0, s0, s0
   e546a:	4770      	bx	lr
   e546c:	eddf 7a0e 	vldr	s15, [pc, #56]	; e54a8 <floorf+0x88>
   e5470:	ee70 7a27 	vadd.f32	s15, s0, s15
   e5474:	eef5 7ac0 	vcmpe.f32	s15, #0.0
   e5478:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e547c:	dd03      	ble.n	e5486 <floorf+0x66>
   e547e:	2a00      	cmp	r2, #0
   e5480:	db08      	blt.n	e5494 <floorf+0x74>
   e5482:	ed9f 0a0a 	vldr	s0, [pc, #40]	; e54ac <floorf+0x8c>
   e5486:	4770      	bx	lr
   e5488:	f44f 0200 	mov.w	r2, #8388608	; 0x800000
   e548c:	fa42 f303 	asr.w	r3, r2, r3
   e5490:	4418      	add	r0, r3
   e5492:	e7e0      	b.n	e5456 <floorf+0x36>
   e5494:	2900      	cmp	r1, #0
   e5496:	eeff 7a00 	vmov.f32	s15, #240	; 0xbf800000 -1.0
   e549a:	bf18      	it	ne
   e549c:	eeb0 0a67 	vmovne.f32	s0, s15
   e54a0:	4770      	bx	lr
   e54a2:	bf00      	nop
   e54a4:	007fffff 	.word	0x007fffff
   e54a8:	7149f2ca 	.word	0x7149f2ca
   e54ac:	00000000 	.word	0x00000000

000e54b0 <fmaxf>:
   e54b0:	b508      	push	{r3, lr}
   e54b2:	ed2d 8b02 	vpush	{d8}
   e54b6:	eeb0 8a60 	vmov.f32	s16, s1
   e54ba:	eef0 8a40 	vmov.f32	s17, s0
   e54be:	f000 f833 	bl	e5528 <__fpclassifyf>
   e54c2:	b920      	cbnz	r0, e54ce <fmaxf+0x1e>
   e54c4:	eeb0 0a48 	vmov.f32	s0, s16
   e54c8:	ecbd 8b02 	vpop	{d8}
   e54cc:	bd08      	pop	{r3, pc}
   e54ce:	eeb0 0a48 	vmov.f32	s0, s16
   e54d2:	f000 f829 	bl	e5528 <__fpclassifyf>
   e54d6:	b120      	cbz	r0, e54e2 <fmaxf+0x32>
   e54d8:	eef4 8ac8 	vcmpe.f32	s17, s16
   e54dc:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e54e0:	ddf0      	ble.n	e54c4 <fmaxf+0x14>
   e54e2:	eeb0 0a68 	vmov.f32	s0, s17
   e54e6:	ecbd 8b02 	vpop	{d8}
   e54ea:	bd08      	pop	{r3, pc}

000e54ec <fminf>:
   e54ec:	b508      	push	{r3, lr}
   e54ee:	ed2d 8b02 	vpush	{d8}
   e54f2:	eeb0 8a60 	vmov.f32	s16, s1
   e54f6:	eef0 8a40 	vmov.f32	s17, s0
   e54fa:	f000 f815 	bl	e5528 <__fpclassifyf>
   e54fe:	b920      	cbnz	r0, e550a <fminf+0x1e>
   e5500:	eeb0 0a48 	vmov.f32	s0, s16
   e5504:	ecbd 8b02 	vpop	{d8}
   e5508:	bd08      	pop	{r3, pc}
   e550a:	eeb0 0a48 	vmov.f32	s0, s16
   e550e:	f000 f80b 	bl	e5528 <__fpclassifyf>
   e5512:	b120      	cbz	r0, e551e <fminf+0x32>
   e5514:	eef4 8ac8 	vcmpe.f32	s17, s16
   e5518:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e551c:	d5f0      	bpl.n	e5500 <fminf+0x14>
   e551e:	eeb0 0a68 	vmov.f32	s0, s17
   e5522:	ecbd 8b02 	vpop	{d8}
   e5526:	bd08      	pop	{r3, pc}

000e5528 <__fpclassifyf>:
   e5528:	ee10 3a10 	vmov	r3, s0
   e552c:	f033 4000 	bics.w	r0, r3, #2147483648	; 0x80000000
   e5530:	d101      	bne.n	e5536 <__fpclassifyf+0xe>
   e5532:	2002      	movs	r0, #2
   e5534:	4770      	bx	lr
   e5536:	f5a0 0300 	sub.w	r3, r0, #8388608	; 0x800000
   e553a:	f1b3 4ffe 	cmp.w	r3, #2130706432	; 0x7f000000
   e553e:	d201      	bcs.n	e5544 <__fpclassifyf+0x1c>
   e5540:	2004      	movs	r0, #4
   e5542:	4770      	bx	lr
   e5544:	4b05      	ldr	r3, [pc, #20]	; (e555c <__fpclassifyf+0x34>)
   e5546:	1e42      	subs	r2, r0, #1
   e5548:	429a      	cmp	r2, r3
   e554a:	d801      	bhi.n	e5550 <__fpclassifyf+0x28>
   e554c:	2003      	movs	r0, #3
   e554e:	4770      	bx	lr
   e5550:	f1a0 40ff 	sub.w	r0, r0, #2139095040	; 0x7f800000
   e5554:	fab0 f080 	clz	r0, r0
   e5558:	0940      	lsrs	r0, r0, #5
   e555a:	4770      	bx	lr
   e555c:	007ffffe 	.word	0x007ffffe

000e5560 <roundf>:
   e5560:	b082      	sub	sp, #8
   e5562:	ed8d 0a01 	vstr	s0, [sp, #4]
   e5566:	9901      	ldr	r1, [sp, #4]
   e5568:	f3c1 53c7 	ubfx	r3, r1, #23, #8
   e556c:	3b7f      	subs	r3, #127	; 0x7f
   e556e:	2b16      	cmp	r3, #22
   e5570:	dc10      	bgt.n	e5594 <roundf+0x34>
   e5572:	2b00      	cmp	r3, #0
   e5574:	db1a      	blt.n	e55ac <roundf+0x4c>
   e5576:	4a11      	ldr	r2, [pc, #68]	; (e55bc <roundf+0x5c>)
   e5578:	fa42 f003 	asr.w	r0, r2, r3
   e557c:	4201      	tst	r1, r0
   e557e:	d00b      	beq.n	e5598 <roundf+0x38>
   e5580:	f44f 0280 	mov.w	r2, #4194304	; 0x400000
   e5584:	411a      	asrs	r2, r3
   e5586:	440a      	add	r2, r1
   e5588:	ea22 0200 	bic.w	r2, r2, r0
   e558c:	ee00 2a10 	vmov	s0, r2
   e5590:	b002      	add	sp, #8
   e5592:	4770      	bx	lr
   e5594:	2b80      	cmp	r3, #128	; 0x80
   e5596:	d003      	beq.n	e55a0 <roundf+0x40>
   e5598:	ed9d 0a01 	vldr	s0, [sp, #4]
   e559c:	b002      	add	sp, #8
   e559e:	4770      	bx	lr
   e55a0:	eddd 7a01 	vldr	s15, [sp, #4]
   e55a4:	ee37 0aa7 	vadd.f32	s0, s15, s15
   e55a8:	b002      	add	sp, #8
   e55aa:	4770      	bx	lr
   e55ac:	3301      	adds	r3, #1
   e55ae:	f001 4200 	and.w	r2, r1, #2147483648	; 0x80000000
   e55b2:	d1eb      	bne.n	e558c <roundf+0x2c>
   e55b4:	f042 527e 	orr.w	r2, r2, #1065353216	; 0x3f800000
   e55b8:	e7e8      	b.n	e558c <roundf+0x2c>
   e55ba:	bf00      	nop
   e55bc:	007fffff 	.word	0x007fffff

000e55c0 <sinf>:
   e55c0:	b500      	push	{lr}
   e55c2:	ee10 3a10 	vmov	r3, s0
   e55c6:	4a21      	ldr	r2, [pc, #132]	; (e564c <sinf+0x8c>)
   e55c8:	f023 4300 	bic.w	r3, r3, #2147483648	; 0x80000000
   e55cc:	4293      	cmp	r3, r2
   e55ce:	b083      	sub	sp, #12
   e55d0:	dd1a      	ble.n	e5608 <sinf+0x48>
   e55d2:	f1b3 4fff 	cmp.w	r3, #2139095040	; 0x7f800000
   e55d6:	db04      	blt.n	e55e2 <sinf+0x22>
   e55d8:	ee30 0a40 	vsub.f32	s0, s0, s0
   e55dc:	b003      	add	sp, #12
   e55de:	f85d fb04 	ldr.w	pc, [sp], #4
   e55e2:	4668      	mov	r0, sp
   e55e4:	f000 fd86 	bl	e60f4 <__ieee754_rem_pio2f>
   e55e8:	f000 0003 	and.w	r0, r0, #3
   e55ec:	2801      	cmp	r0, #1
   e55ee:	d01d      	beq.n	e562c <sinf+0x6c>
   e55f0:	2802      	cmp	r0, #2
   e55f2:	d011      	beq.n	e5618 <sinf+0x58>
   e55f4:	b308      	cbz	r0, e563a <sinf+0x7a>
   e55f6:	eddd 0a01 	vldr	s1, [sp, #4]
   e55fa:	ed9d 0a00 	vldr	s0, [sp]
   e55fe:	f000 ff17 	bl	e6430 <__kernel_cosf>
   e5602:	eeb1 0a40 	vneg.f32	s0, s0
   e5606:	e7e9      	b.n	e55dc <sinf+0x1c>
   e5608:	2000      	movs	r0, #0
   e560a:	eddf 0a11 	vldr	s1, [pc, #68]	; e5650 <sinf+0x90>
   e560e:	f001 faa5 	bl	e6b5c <__kernel_sinf>
   e5612:	b003      	add	sp, #12
   e5614:	f85d fb04 	ldr.w	pc, [sp], #4
   e5618:	2001      	movs	r0, #1
   e561a:	eddd 0a01 	vldr	s1, [sp, #4]
   e561e:	ed9d 0a00 	vldr	s0, [sp]
   e5622:	f001 fa9b 	bl	e6b5c <__kernel_sinf>
   e5626:	eeb1 0a40 	vneg.f32	s0, s0
   e562a:	e7d7      	b.n	e55dc <sinf+0x1c>
   e562c:	eddd 0a01 	vldr	s1, [sp, #4]
   e5630:	ed9d 0a00 	vldr	s0, [sp]
   e5634:	f000 fefc 	bl	e6430 <__kernel_cosf>
   e5638:	e7d0      	b.n	e55dc <sinf+0x1c>
   e563a:	2001      	movs	r0, #1
   e563c:	eddd 0a01 	vldr	s1, [sp, #4]
   e5640:	ed9d 0a00 	vldr	s0, [sp]
   e5644:	f001 fa8a 	bl	e6b5c <__kernel_sinf>
   e5648:	e7c8      	b.n	e55dc <sinf+0x1c>
   e564a:	bf00      	nop
   e564c:	3f490fd8 	.word	0x3f490fd8
	...

000e5658 <exp>:
   e5658:	b5f0      	push	{r4, r5, r6, r7, lr}
   e565a:	ed2d 8b04 	vpush	{d8-d9}
   e565e:	eeb0 9a40 	vmov.f32	s18, s0
   e5662:	eef0 9a60 	vmov.f32	s19, s1
   e5666:	4c3a      	ldr	r4, [pc, #232]	; (e5750 <exp+0xf8>)
   e5668:	b08b      	sub	sp, #44	; 0x2c
   e566a:	f000 f9cd 	bl	e5a08 <__ieee754_exp>
   e566e:	f994 3000 	ldrsb.w	r3, [r4]
   e5672:	eeb0 8a40 	vmov.f32	s16, s0
   e5676:	eef0 8a60 	vmov.f32	s17, s1
   e567a:	3301      	adds	r3, #1
   e567c:	d038      	beq.n	e56f0 <exp+0x98>
   e567e:	eeb0 0a49 	vmov.f32	s0, s18
   e5682:	eef0 0a69 	vmov.f32	s1, s19
   e5686:	f001 fab1 	bl	e6bec <finite>
   e568a:	b388      	cbz	r0, e56f0 <exp+0x98>
   e568c:	a32c      	add	r3, pc, #176	; (adr r3, e5740 <exp+0xe8>)
   e568e:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5692:	ec51 0b19 	vmov	r0, r1, d9
   e5696:	f001 ff95 	bl	e75c4 <__aeabi_dcmpgt>
   e569a:	4605      	mov	r5, r0
   e569c:	bb80      	cbnz	r0, e5700 <exp+0xa8>
   e569e:	a32a      	add	r3, pc, #168	; (adr r3, e5748 <exp+0xf0>)
   e56a0:	e9d3 2300 	ldrd	r2, r3, [r3]
   e56a4:	ec51 0b19 	vmov	r0, r1, d9
   e56a8:	f001 ff6e 	bl	e7588 <__aeabi_dcmplt>
   e56ac:	b300      	cbz	r0, e56f0 <exp+0x98>
   e56ae:	f994 3000 	ldrsb.w	r3, [r4]
   e56b2:	4a28      	ldr	r2, [pc, #160]	; (e5754 <exp+0xfc>)
   e56b4:	9508      	str	r5, [sp, #32]
   e56b6:	2600      	movs	r6, #0
   e56b8:	2700      	movs	r7, #0
   e56ba:	2104      	movs	r1, #4
   e56bc:	2b02      	cmp	r3, #2
   e56be:	ed8d 9b04 	vstr	d9, [sp, #16]
   e56c2:	ed8d 9b02 	vstr	d9, [sp, #8]
   e56c6:	e9cd 6706 	strd	r6, r7, [sp, #24]
   e56ca:	e88d 0006 	stmia.w	sp, {r1, r2}
   e56ce:	d030      	beq.n	e5732 <exp+0xda>
   e56d0:	4668      	mov	r0, sp
   e56d2:	f001 fa93 	bl	e6bfc <matherr>
   e56d6:	b360      	cbz	r0, e5732 <exp+0xda>
   e56d8:	9b08      	ldr	r3, [sp, #32]
   e56da:	b11b      	cbz	r3, e56e4 <exp+0x8c>
   e56dc:	f7fe fe2e 	bl	e433c <__errno>
   e56e0:	9b08      	ldr	r3, [sp, #32]
   e56e2:	6003      	str	r3, [r0, #0]
   e56e4:	ed9d 0b06 	vldr	d0, [sp, #24]
   e56e8:	b00b      	add	sp, #44	; 0x2c
   e56ea:	ecbd 8b04 	vpop	{d8-d9}
   e56ee:	bdf0      	pop	{r4, r5, r6, r7, pc}
   e56f0:	eeb0 0a48 	vmov.f32	s0, s16
   e56f4:	eef0 0a68 	vmov.f32	s1, s17
   e56f8:	b00b      	add	sp, #44	; 0x2c
   e56fa:	ecbd 8b04 	vpop	{d8-d9}
   e56fe:	bdf0      	pop	{r4, r5, r6, r7, pc}
   e5700:	4a14      	ldr	r2, [pc, #80]	; (e5754 <exp+0xfc>)
   e5702:	f994 3000 	ldrsb.w	r3, [r4]
   e5706:	9201      	str	r2, [sp, #4]
   e5708:	2103      	movs	r1, #3
   e570a:	2200      	movs	r2, #0
   e570c:	ed8d 9b04 	vstr	d9, [sp, #16]
   e5710:	ed8d 9b02 	vstr	d9, [sp, #8]
   e5714:	9100      	str	r1, [sp, #0]
   e5716:	9208      	str	r2, [sp, #32]
   e5718:	b92b      	cbnz	r3, e5726 <exp+0xce>
   e571a:	4b0f      	ldr	r3, [pc, #60]	; (e5758 <exp+0x100>)
   e571c:	f04f 4260 	mov.w	r2, #3758096384	; 0xe0000000
   e5720:	e9cd 2306 	strd	r2, r3, [sp, #24]
   e5724:	e7d4      	b.n	e56d0 <exp+0x78>
   e5726:	490d      	ldr	r1, [pc, #52]	; (e575c <exp+0x104>)
   e5728:	2000      	movs	r0, #0
   e572a:	2b02      	cmp	r3, #2
   e572c:	e9cd 0106 	strd	r0, r1, [sp, #24]
   e5730:	d1ce      	bne.n	e56d0 <exp+0x78>
   e5732:	f7fe fe03 	bl	e433c <__errno>
   e5736:	2322      	movs	r3, #34	; 0x22
   e5738:	6003      	str	r3, [r0, #0]
   e573a:	e7cd      	b.n	e56d8 <exp+0x80>
   e573c:	f3af 8000 	nop.w
   e5740:	fefa39ef 	.word	0xfefa39ef
   e5744:	40862e42 	.word	0x40862e42
   e5748:	d52d3051 	.word	0xd52d3051
   e574c:	c0874910 	.word	0xc0874910
   e5750:	2003c26c 	.word	0x2003c26c
   e5754:	000eb544 	.word	0x000eb544
   e5758:	47efffff 	.word	0x47efffff
   e575c:	7ff00000 	.word	0x7ff00000

000e5760 <expf>:
   e5760:	b5d0      	push	{r4, r6, r7, lr}
   e5762:	ed2d 8b02 	vpush	{d8}
   e5766:	4c39      	ldr	r4, [pc, #228]	; (e584c <expf+0xec>)
   e5768:	b08a      	sub	sp, #40	; 0x28
   e576a:	eef0 8a40 	vmov.f32	s17, s0
   e576e:	f000 fadd 	bl	e5d2c <__ieee754_expf>
   e5772:	f994 3000 	ldrsb.w	r3, [r4]
   e5776:	3301      	adds	r3, #1
   e5778:	eeb0 8a40 	vmov.f32	s16, s0
   e577c:	d03e      	beq.n	e57fc <expf+0x9c>
   e577e:	eeb0 0a68 	vmov.f32	s0, s17
   e5782:	f001 fa4d 	bl	e6c20 <finitef>
   e5786:	2800      	cmp	r0, #0
   e5788:	d038      	beq.n	e57fc <expf+0x9c>
   e578a:	eddf 7a31 	vldr	s15, [pc, #196]	; e5850 <expf+0xf0>
   e578e:	eef4 8ae7 	vcmpe.f32	s17, s15
   e5792:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5796:	dc37      	bgt.n	e5808 <expf+0xa8>
   e5798:	eddf 7a2e 	vldr	s15, [pc, #184]	; e5854 <expf+0xf4>
   e579c:	eef4 8ae7 	vcmpe.f32	s17, s15
   e57a0:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e57a4:	d52a      	bpl.n	e57fc <expf+0x9c>
   e57a6:	2304      	movs	r3, #4
   e57a8:	4a2b      	ldr	r2, [pc, #172]	; (e5858 <expf+0xf8>)
   e57aa:	9300      	str	r3, [sp, #0]
   e57ac:	ee18 0a90 	vmov	r0, s17
   e57b0:	2300      	movs	r3, #0
   e57b2:	9308      	str	r3, [sp, #32]
   e57b4:	9201      	str	r2, [sp, #4]
   e57b6:	f001 fc21 	bl	e6ffc <__aeabi_f2d>
   e57ba:	f994 3000 	ldrsb.w	r3, [r4]
   e57be:	2600      	movs	r6, #0
   e57c0:	2700      	movs	r7, #0
   e57c2:	2b02      	cmp	r3, #2
   e57c4:	e9cd 0104 	strd	r0, r1, [sp, #16]
   e57c8:	e9cd 0102 	strd	r0, r1, [sp, #8]
   e57cc:	e9cd 6706 	strd	r6, r7, [sp, #24]
   e57d0:	d037      	beq.n	e5842 <expf+0xe2>
   e57d2:	4668      	mov	r0, sp
   e57d4:	f001 fa12 	bl	e6bfc <matherr>
   e57d8:	2800      	cmp	r0, #0
   e57da:	d032      	beq.n	e5842 <expf+0xe2>
   e57dc:	9b08      	ldr	r3, [sp, #32]
   e57de:	b11b      	cbz	r3, e57e8 <expf+0x88>
   e57e0:	f7fe fdac 	bl	e433c <__errno>
   e57e4:	9b08      	ldr	r3, [sp, #32]
   e57e6:	6003      	str	r3, [r0, #0]
   e57e8:	e9dd 0106 	ldrd	r0, r1, [sp, #24]
   e57ec:	f001 ff3c 	bl	e7668 <__aeabi_d2f>
   e57f0:	ee00 0a10 	vmov	s0, r0
   e57f4:	b00a      	add	sp, #40	; 0x28
   e57f6:	ecbd 8b02 	vpop	{d8}
   e57fa:	bdd0      	pop	{r4, r6, r7, pc}
   e57fc:	eeb0 0a48 	vmov.f32	s0, s16
   e5800:	b00a      	add	sp, #40	; 0x28
   e5802:	ecbd 8b02 	vpop	{d8}
   e5806:	bdd0      	pop	{r4, r6, r7, pc}
   e5808:	2303      	movs	r3, #3
   e580a:	4a13      	ldr	r2, [pc, #76]	; (e5858 <expf+0xf8>)
   e580c:	9300      	str	r3, [sp, #0]
   e580e:	ee18 0a90 	vmov	r0, s17
   e5812:	2300      	movs	r3, #0
   e5814:	9308      	str	r3, [sp, #32]
   e5816:	9201      	str	r2, [sp, #4]
   e5818:	f001 fbf0 	bl	e6ffc <__aeabi_f2d>
   e581c:	f994 3000 	ldrsb.w	r3, [r4]
   e5820:	e9cd 0104 	strd	r0, r1, [sp, #16]
   e5824:	e9cd 0102 	strd	r0, r1, [sp, #8]
   e5828:	b92b      	cbnz	r3, e5836 <expf+0xd6>
   e582a:	4b0c      	ldr	r3, [pc, #48]	; (e585c <expf+0xfc>)
   e582c:	f04f 4260 	mov.w	r2, #3758096384	; 0xe0000000
   e5830:	e9cd 2306 	strd	r2, r3, [sp, #24]
   e5834:	e7cd      	b.n	e57d2 <expf+0x72>
   e5836:	490a      	ldr	r1, [pc, #40]	; (e5860 <expf+0x100>)
   e5838:	2000      	movs	r0, #0
   e583a:	2b02      	cmp	r3, #2
   e583c:	e9cd 0106 	strd	r0, r1, [sp, #24]
   e5840:	d1c7      	bne.n	e57d2 <expf+0x72>
   e5842:	f7fe fd7b 	bl	e433c <__errno>
   e5846:	2322      	movs	r3, #34	; 0x22
   e5848:	6003      	str	r3, [r0, #0]
   e584a:	e7c7      	b.n	e57dc <expf+0x7c>
   e584c:	2003c26c 	.word	0x2003c26c
   e5850:	42b17180 	.word	0x42b17180
   e5854:	c2cff1b5 	.word	0xc2cff1b5
   e5858:	000eb548 	.word	0x000eb548
   e585c:	47efffff 	.word	0x47efffff
   e5860:	7ff00000 	.word	0x7ff00000

000e5864 <logf>:
   e5864:	b510      	push	{r4, lr}
   e5866:	ed2d 8b02 	vpush	{d8}
   e586a:	b08a      	sub	sp, #40	; 0x28
   e586c:	eeb0 8a40 	vmov.f32	s16, s0
   e5870:	f000 fb34 	bl	e5edc <__ieee754_logf>
   e5874:	4b34      	ldr	r3, [pc, #208]	; (e5948 <logf+0xe4>)
   e5876:	f993 4000 	ldrsb.w	r4, [r3]
   e587a:	1c63      	adds	r3, r4, #1
   e587c:	d009      	beq.n	e5892 <logf+0x2e>
   e587e:	eeb4 8a48 	vcmp.f32	s16, s16
   e5882:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5886:	d604      	bvs.n	e5892 <logf+0x2e>
   e5888:	eeb5 8ac0 	vcmpe.f32	s16, #0.0
   e588c:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5890:	dd03      	ble.n	e589a <logf+0x36>
   e5892:	b00a      	add	sp, #40	; 0x28
   e5894:	ecbd 8b02 	vpop	{d8}
   e5898:	bd10      	pop	{r4, pc}
   e589a:	4b2c      	ldr	r3, [pc, #176]	; (e594c <logf+0xe8>)
   e589c:	9301      	str	r3, [sp, #4]
   e589e:	ee18 0a10 	vmov	r0, s16
   e58a2:	2300      	movs	r3, #0
   e58a4:	9308      	str	r3, [sp, #32]
   e58a6:	f001 fba9 	bl	e6ffc <__aeabi_f2d>
   e58aa:	e9cd 0104 	strd	r0, r1, [sp, #16]
   e58ae:	e9cd 0102 	strd	r0, r1, [sp, #8]
   e58b2:	b9dc      	cbnz	r4, e58ec <logf+0x88>
   e58b4:	4b26      	ldr	r3, [pc, #152]	; (e5950 <logf+0xec>)
   e58b6:	eeb5 8a40 	vcmp.f32	s16, #0.0
   e58ba:	f04f 4260 	mov.w	r2, #3758096384	; 0xe0000000
   e58be:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e58c2:	e9cd 2306 	strd	r2, r3, [sp, #24]
   e58c6:	d136      	bne.n	e5936 <logf+0xd2>
   e58c8:	2302      	movs	r3, #2
   e58ca:	9300      	str	r3, [sp, #0]
   e58cc:	4668      	mov	r0, sp
   e58ce:	f001 f995 	bl	e6bfc <matherr>
   e58d2:	b1c0      	cbz	r0, e5906 <logf+0xa2>
   e58d4:	9b08      	ldr	r3, [sp, #32]
   e58d6:	b9db      	cbnz	r3, e5910 <logf+0xac>
   e58d8:	e9dd 0106 	ldrd	r0, r1, [sp, #24]
   e58dc:	f001 fec4 	bl	e7668 <__aeabi_d2f>
   e58e0:	ee00 0a10 	vmov	s0, r0
   e58e4:	b00a      	add	sp, #40	; 0x28
   e58e6:	ecbd 8b02 	vpop	{d8}
   e58ea:	bd10      	pop	{r4, pc}
   e58ec:	4b19      	ldr	r3, [pc, #100]	; (e5954 <logf+0xf0>)
   e58ee:	eeb5 8a40 	vcmp.f32	s16, #0.0
   e58f2:	2200      	movs	r2, #0
   e58f4:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e58f8:	e9cd 2306 	strd	r2, r3, [sp, #24]
   e58fc:	d10d      	bne.n	e591a <logf+0xb6>
   e58fe:	2302      	movs	r3, #2
   e5900:	429c      	cmp	r4, r3
   e5902:	9300      	str	r3, [sp, #0]
   e5904:	d1e2      	bne.n	e58cc <logf+0x68>
   e5906:	f7fe fd19 	bl	e433c <__errno>
   e590a:	2322      	movs	r3, #34	; 0x22
   e590c:	6003      	str	r3, [r0, #0]
   e590e:	e7e1      	b.n	e58d4 <logf+0x70>
   e5910:	f7fe fd14 	bl	e433c <__errno>
   e5914:	9b08      	ldr	r3, [sp, #32]
   e5916:	6003      	str	r3, [r0, #0]
   e5918:	e7de      	b.n	e58d8 <logf+0x74>
   e591a:	2301      	movs	r3, #1
   e591c:	2c02      	cmp	r4, #2
   e591e:	9300      	str	r3, [sp, #0]
   e5920:	d10b      	bne.n	e593a <logf+0xd6>
   e5922:	f7fe fd0b 	bl	e433c <__errno>
   e5926:	2321      	movs	r3, #33	; 0x21
   e5928:	6003      	str	r3, [r0, #0]
   e592a:	480b      	ldr	r0, [pc, #44]	; (e5958 <logf+0xf4>)
   e592c:	f001 f968 	bl	e6c00 <nan>
   e5930:	ed8d 0b06 	vstr	d0, [sp, #24]
   e5934:	e7ce      	b.n	e58d4 <logf+0x70>
   e5936:	2301      	movs	r3, #1
   e5938:	9300      	str	r3, [sp, #0]
   e593a:	4668      	mov	r0, sp
   e593c:	f001 f95e 	bl	e6bfc <matherr>
   e5940:	2800      	cmp	r0, #0
   e5942:	d1f2      	bne.n	e592a <logf+0xc6>
   e5944:	e7ed      	b.n	e5922 <logf+0xbe>
   e5946:	bf00      	nop
   e5948:	2003c26c 	.word	0x2003c26c
   e594c:	000eb550 	.word	0x000eb550
   e5950:	c7efffff 	.word	0xc7efffff
   e5954:	fff00000 	.word	0xfff00000
   e5958:	000eb554 	.word	0x000eb554

000e595c <sqrtf>:
   e595c:	b510      	push	{r4, lr}
   e595e:	ed2d 8b02 	vpush	{d8}
   e5962:	b08a      	sub	sp, #40	; 0x28
   e5964:	eeb0 8a40 	vmov.f32	s16, s0
   e5968:	f000 fd10 	bl	e638c <__ieee754_sqrtf>
   e596c:	4b24      	ldr	r3, [pc, #144]	; (e5a00 <sqrtf+0xa4>)
   e596e:	f993 4000 	ldrsb.w	r4, [r3]
   e5972:	1c63      	adds	r3, r4, #1
   e5974:	d009      	beq.n	e598a <sqrtf+0x2e>
   e5976:	eeb4 8a48 	vcmp.f32	s16, s16
   e597a:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e597e:	d604      	bvs.n	e598a <sqrtf+0x2e>
   e5980:	eeb5 8ac0 	vcmpe.f32	s16, #0.0
   e5984:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5988:	d403      	bmi.n	e5992 <sqrtf+0x36>
   e598a:	b00a      	add	sp, #40	; 0x28
   e598c:	ecbd 8b02 	vpop	{d8}
   e5990:	bd10      	pop	{r4, pc}
   e5992:	2301      	movs	r3, #1
   e5994:	4a1b      	ldr	r2, [pc, #108]	; (e5a04 <sqrtf+0xa8>)
   e5996:	9300      	str	r3, [sp, #0]
   e5998:	ee18 0a10 	vmov	r0, s16
   e599c:	2300      	movs	r3, #0
   e599e:	9201      	str	r2, [sp, #4]
   e59a0:	9308      	str	r3, [sp, #32]
   e59a2:	f001 fb2b 	bl	e6ffc <__aeabi_f2d>
   e59a6:	2200      	movs	r2, #0
   e59a8:	e9cd 0104 	strd	r0, r1, [sp, #16]
   e59ac:	e9cd 0102 	strd	r0, r1, [sp, #8]
   e59b0:	2300      	movs	r3, #0
   e59b2:	b1bc      	cbz	r4, e59e4 <sqrtf+0x88>
   e59b4:	4610      	mov	r0, r2
   e59b6:	4619      	mov	r1, r3
   e59b8:	f001 fc9e 	bl	e72f8 <__aeabi_ddiv>
   e59bc:	2c02      	cmp	r4, #2
   e59be:	e9cd 0106 	strd	r0, r1, [sp, #24]
   e59c2:	d111      	bne.n	e59e8 <sqrtf+0x8c>
   e59c4:	f7fe fcba 	bl	e433c <__errno>
   e59c8:	2321      	movs	r3, #33	; 0x21
   e59ca:	6003      	str	r3, [r0, #0]
   e59cc:	9b08      	ldr	r3, [sp, #32]
   e59ce:	b98b      	cbnz	r3, e59f4 <sqrtf+0x98>
   e59d0:	e9dd 0106 	ldrd	r0, r1, [sp, #24]
   e59d4:	f001 fe48 	bl	e7668 <__aeabi_d2f>
   e59d8:	ee00 0a10 	vmov	s0, r0
   e59dc:	b00a      	add	sp, #40	; 0x28
   e59de:	ecbd 8b02 	vpop	{d8}
   e59e2:	bd10      	pop	{r4, pc}
   e59e4:	e9cd 2306 	strd	r2, r3, [sp, #24]
   e59e8:	4668      	mov	r0, sp
   e59ea:	f001 f907 	bl	e6bfc <matherr>
   e59ee:	2800      	cmp	r0, #0
   e59f0:	d1ec      	bne.n	e59cc <sqrtf+0x70>
   e59f2:	e7e7      	b.n	e59c4 <sqrtf+0x68>
   e59f4:	f7fe fca2 	bl	e433c <__errno>
   e59f8:	9b08      	ldr	r3, [sp, #32]
   e59fa:	6003      	str	r3, [r0, #0]
   e59fc:	e7e8      	b.n	e59d0 <sqrtf+0x74>
   e59fe:	bf00      	nop
   e5a00:	2003c26c 	.word	0x2003c26c
   e5a04:	000eb558 	.word	0x000eb558

000e5a08 <__ieee754_exp>:
   e5a08:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e5a0c:	ec55 4b10 	vmov	r4, r5, d0
   e5a10:	49bd      	ldr	r1, [pc, #756]	; (e5d08 <__ieee754_exp+0x300>)
   e5a12:	f025 4200 	bic.w	r2, r5, #2147483648	; 0x80000000
   e5a16:	428a      	cmp	r2, r1
   e5a18:	b083      	sub	sp, #12
   e5a1a:	ea4f 77d5 	mov.w	r7, r5, lsr #31
   e5a1e:	d90d      	bls.n	e5a3c <__ieee754_exp+0x34>
   e5a20:	49ba      	ldr	r1, [pc, #744]	; (e5d0c <__ieee754_exp+0x304>)
   e5a22:	428a      	cmp	r2, r1
   e5a24:	d92a      	bls.n	e5a7c <__ieee754_exp+0x74>
   e5a26:	f3c5 0313 	ubfx	r3, r5, #0, #20
   e5a2a:	4323      	orrs	r3, r4
   e5a2c:	f040 80fa 	bne.w	e5c24 <__ieee754_exp+0x21c>
   e5a30:	b10f      	cbz	r7, e5a36 <__ieee754_exp+0x2e>
   e5a32:	ed9f 0b9d 	vldr	d0, [pc, #628]	; e5ca8 <__ieee754_exp+0x2a0>
   e5a36:	b003      	add	sp, #12
   e5a38:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e5a3c:	4bb4      	ldr	r3, [pc, #720]	; (e5d10 <__ieee754_exp+0x308>)
   e5a3e:	429a      	cmp	r2, r3
   e5a40:	f200 80d5 	bhi.w	e5bee <__ieee754_exp+0x1e6>
   e5a44:	4bb3      	ldr	r3, [pc, #716]	; (e5d14 <__ieee754_exp+0x30c>)
   e5a46:	429a      	cmp	r2, r3
   e5a48:	f200 80ea 	bhi.w	e5c20 <__ieee754_exp+0x218>
   e5a4c:	a398      	add	r3, pc, #608	; (adr r3, e5cb0 <__ieee754_exp+0x2a8>)
   e5a4e:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5a52:	ee10 0a10 	vmov	r0, s0
   e5a56:	4629      	mov	r1, r5
   e5a58:	f001 f972 	bl	e6d40 <__adddf3>
   e5a5c:	2200      	movs	r2, #0
   e5a5e:	4bae      	ldr	r3, [pc, #696]	; (e5d18 <__ieee754_exp+0x310>)
   e5a60:	f001 fdb0 	bl	e75c4 <__aeabi_dcmpgt>
   e5a64:	2800      	cmp	r0, #0
   e5a66:	f000 811c 	beq.w	e5ca2 <__ieee754_exp+0x29a>
   e5a6a:	4620      	mov	r0, r4
   e5a6c:	4629      	mov	r1, r5
   e5a6e:	2200      	movs	r2, #0
   e5a70:	4ba9      	ldr	r3, [pc, #676]	; (e5d18 <__ieee754_exp+0x310>)
   e5a72:	f001 f965 	bl	e6d40 <__adddf3>
   e5a76:	ec41 0b10 	vmov	d0, r0, r1
   e5a7a:	e7dc      	b.n	e5a36 <__ieee754_exp+0x2e>
   e5a7c:	a38e      	add	r3, pc, #568	; (adr r3, e5cb8 <__ieee754_exp+0x2b0>)
   e5a7e:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5a82:	ee10 0a10 	vmov	r0, s0
   e5a86:	4629      	mov	r1, r5
   e5a88:	f001 fd9c 	bl	e75c4 <__aeabi_dcmpgt>
   e5a8c:	2800      	cmp	r0, #0
   e5a8e:	f040 80d3 	bne.w	e5c38 <__ieee754_exp+0x230>
   e5a92:	a38b      	add	r3, pc, #556	; (adr r3, e5cc0 <__ieee754_exp+0x2b8>)
   e5a94:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5a98:	4620      	mov	r0, r4
   e5a9a:	4629      	mov	r1, r5
   e5a9c:	f001 fd74 	bl	e7588 <__aeabi_dcmplt>
   e5aa0:	2800      	cmp	r0, #0
   e5aa2:	d1c6      	bne.n	e5a32 <__ieee754_exp+0x2a>
   e5aa4:	4e9d      	ldr	r6, [pc, #628]	; (e5d1c <__ieee754_exp+0x314>)
   e5aa6:	a388      	add	r3, pc, #544	; (adr r3, e5cc8 <__ieee754_exp+0x2c0>)
   e5aa8:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5aac:	eb06 06c7 	add.w	r6, r6, r7, lsl #3
   e5ab0:	4620      	mov	r0, r4
   e5ab2:	4629      	mov	r1, r5
   e5ab4:	f001 faf6 	bl	e70a4 <__aeabi_dmul>
   e5ab8:	e9d6 2300 	ldrd	r2, r3, [r6]
   e5abc:	f001 f940 	bl	e6d40 <__adddf3>
   e5ac0:	f001 fd8a 	bl	e75d8 <__aeabi_d2iz>
   e5ac4:	4606      	mov	r6, r0
   e5ac6:	f001 fa87 	bl	e6fd8 <__aeabi_i2d>
   e5aca:	a381      	add	r3, pc, #516	; (adr r3, e5cd0 <__ieee754_exp+0x2c8>)
   e5acc:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5ad0:	4680      	mov	r8, r0
   e5ad2:	4689      	mov	r9, r1
   e5ad4:	f001 fae6 	bl	e70a4 <__aeabi_dmul>
   e5ad8:	4602      	mov	r2, r0
   e5ada:	460b      	mov	r3, r1
   e5adc:	4620      	mov	r0, r4
   e5ade:	4629      	mov	r1, r5
   e5ae0:	f001 f92c 	bl	e6d3c <__aeabi_dsub>
   e5ae4:	a37c      	add	r3, pc, #496	; (adr r3, e5cd8 <__ieee754_exp+0x2d0>)
   e5ae6:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5aea:	e9cd 0100 	strd	r0, r1, [sp]
   e5aee:	4640      	mov	r0, r8
   e5af0:	4649      	mov	r1, r9
   e5af2:	f001 fad7 	bl	e70a4 <__aeabi_dmul>
   e5af6:	4682      	mov	sl, r0
   e5af8:	468b      	mov	fp, r1
   e5afa:	4652      	mov	r2, sl
   e5afc:	465b      	mov	r3, fp
   e5afe:	e9dd 0100 	ldrd	r0, r1, [sp]
   e5b02:	f001 f91b 	bl	e6d3c <__aeabi_dsub>
   e5b06:	4604      	mov	r4, r0
   e5b08:	460d      	mov	r5, r1
   e5b0a:	4622      	mov	r2, r4
   e5b0c:	462b      	mov	r3, r5
   e5b0e:	4620      	mov	r0, r4
   e5b10:	4629      	mov	r1, r5
   e5b12:	f001 fac7 	bl	e70a4 <__aeabi_dmul>
   e5b16:	a372      	add	r3, pc, #456	; (adr r3, e5ce0 <__ieee754_exp+0x2d8>)
   e5b18:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5b1c:	4680      	mov	r8, r0
   e5b1e:	4689      	mov	r9, r1
   e5b20:	f001 fac0 	bl	e70a4 <__aeabi_dmul>
   e5b24:	a370      	add	r3, pc, #448	; (adr r3, e5ce8 <__ieee754_exp+0x2e0>)
   e5b26:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5b2a:	f001 f907 	bl	e6d3c <__aeabi_dsub>
   e5b2e:	4642      	mov	r2, r8
   e5b30:	464b      	mov	r3, r9
   e5b32:	f001 fab7 	bl	e70a4 <__aeabi_dmul>
   e5b36:	a36e      	add	r3, pc, #440	; (adr r3, e5cf0 <__ieee754_exp+0x2e8>)
   e5b38:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5b3c:	f001 f900 	bl	e6d40 <__adddf3>
   e5b40:	4642      	mov	r2, r8
   e5b42:	464b      	mov	r3, r9
   e5b44:	f001 faae 	bl	e70a4 <__aeabi_dmul>
   e5b48:	a36b      	add	r3, pc, #428	; (adr r3, e5cf8 <__ieee754_exp+0x2f0>)
   e5b4a:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5b4e:	f001 f8f5 	bl	e6d3c <__aeabi_dsub>
   e5b52:	4642      	mov	r2, r8
   e5b54:	464b      	mov	r3, r9
   e5b56:	f001 faa5 	bl	e70a4 <__aeabi_dmul>
   e5b5a:	a369      	add	r3, pc, #420	; (adr r3, e5d00 <__ieee754_exp+0x2f8>)
   e5b5c:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5b60:	f001 f8ee 	bl	e6d40 <__adddf3>
   e5b64:	4642      	mov	r2, r8
   e5b66:	464b      	mov	r3, r9
   e5b68:	f001 fa9c 	bl	e70a4 <__aeabi_dmul>
   e5b6c:	4602      	mov	r2, r0
   e5b6e:	460b      	mov	r3, r1
   e5b70:	4620      	mov	r0, r4
   e5b72:	4629      	mov	r1, r5
   e5b74:	f001 f8e2 	bl	e6d3c <__aeabi_dsub>
   e5b78:	4680      	mov	r8, r0
   e5b7a:	4689      	mov	r9, r1
   e5b7c:	2e00      	cmp	r6, #0
   e5b7e:	d065      	beq.n	e5c4c <__ieee754_exp+0x244>
   e5b80:	4620      	mov	r0, r4
   e5b82:	4629      	mov	r1, r5
   e5b84:	4642      	mov	r2, r8
   e5b86:	464b      	mov	r3, r9
   e5b88:	f001 fa8c 	bl	e70a4 <__aeabi_dmul>
   e5b8c:	4642      	mov	r2, r8
   e5b8e:	4604      	mov	r4, r0
   e5b90:	460d      	mov	r5, r1
   e5b92:	464b      	mov	r3, r9
   e5b94:	2000      	movs	r0, #0
   e5b96:	f04f 4180 	mov.w	r1, #1073741824	; 0x40000000
   e5b9a:	f001 f8cf 	bl	e6d3c <__aeabi_dsub>
   e5b9e:	4602      	mov	r2, r0
   e5ba0:	460b      	mov	r3, r1
   e5ba2:	4620      	mov	r0, r4
   e5ba4:	4629      	mov	r1, r5
   e5ba6:	f001 fba7 	bl	e72f8 <__aeabi_ddiv>
   e5baa:	4602      	mov	r2, r0
   e5bac:	460b      	mov	r3, r1
   e5bae:	4650      	mov	r0, sl
   e5bb0:	4659      	mov	r1, fp
   e5bb2:	f001 f8c3 	bl	e6d3c <__aeabi_dsub>
   e5bb6:	e9dd 2300 	ldrd	r2, r3, [sp]
   e5bba:	f001 f8bf 	bl	e6d3c <__aeabi_dsub>
   e5bbe:	460b      	mov	r3, r1
   e5bc0:	4602      	mov	r2, r0
   e5bc2:	4955      	ldr	r1, [pc, #340]	; (e5d18 <__ieee754_exp+0x310>)
   e5bc4:	2000      	movs	r0, #0
   e5bc6:	f001 f8b9 	bl	e6d3c <__aeabi_dsub>
   e5bca:	f46f 737f 	mvn.w	r3, #1020	; 0x3fc
   e5bce:	429e      	cmp	r6, r3
   e5bd0:	da60      	bge.n	e5c94 <__ieee754_exp+0x28c>
   e5bd2:	f506 767a 	add.w	r6, r6, #1000	; 0x3e8
   e5bd6:	eb01 5106 	add.w	r1, r1, r6, lsl #20
   e5bda:	2200      	movs	r2, #0
   e5bdc:	f04f 73b8 	mov.w	r3, #24117248	; 0x1700000
   e5be0:	f001 fa60 	bl	e70a4 <__aeabi_dmul>
   e5be4:	ec41 0b10 	vmov	d0, r0, r1
   e5be8:	b003      	add	sp, #12
   e5bea:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e5bee:	4b4c      	ldr	r3, [pc, #304]	; (e5d20 <__ieee754_exp+0x318>)
   e5bf0:	429a      	cmp	r2, r3
   e5bf2:	f63f af57 	bhi.w	e5aa4 <__ieee754_exp+0x9c>
   e5bf6:	4b4b      	ldr	r3, [pc, #300]	; (e5d24 <__ieee754_exp+0x31c>)
   e5bf8:	ea4f 08c7 	mov.w	r8, r7, lsl #3
   e5bfc:	4443      	add	r3, r8
   e5bfe:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5c02:	ee10 0a10 	vmov	r0, s0
   e5c06:	4629      	mov	r1, r5
   e5c08:	f001 f898 	bl	e6d3c <__aeabi_dsub>
   e5c0c:	4b46      	ldr	r3, [pc, #280]	; (e5d28 <__ieee754_exp+0x320>)
   e5c0e:	f1c7 0601 	rsb	r6, r7, #1
   e5c12:	4498      	add	r8, r3
   e5c14:	e9cd 0100 	strd	r0, r1, [sp]
   e5c18:	e9d8 ab00 	ldrd	sl, fp, [r8]
   e5c1c:	1bf6      	subs	r6, r6, r7
   e5c1e:	e76c      	b.n	e5afa <__ieee754_exp+0xf2>
   e5c20:	2600      	movs	r6, #0
   e5c22:	e772      	b.n	e5b0a <__ieee754_exp+0x102>
   e5c24:	ee10 2a10 	vmov	r2, s0
   e5c28:	462b      	mov	r3, r5
   e5c2a:	4620      	mov	r0, r4
   e5c2c:	4629      	mov	r1, r5
   e5c2e:	f001 f887 	bl	e6d40 <__adddf3>
   e5c32:	ec41 0b10 	vmov	d0, r0, r1
   e5c36:	e6fe      	b.n	e5a36 <__ieee754_exp+0x2e>
   e5c38:	a31d      	add	r3, pc, #116	; (adr r3, e5cb0 <__ieee754_exp+0x2a8>)
   e5c3a:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5c3e:	4610      	mov	r0, r2
   e5c40:	4619      	mov	r1, r3
   e5c42:	f001 fa2f 	bl	e70a4 <__aeabi_dmul>
   e5c46:	ec41 0b10 	vmov	d0, r0, r1
   e5c4a:	e6f4      	b.n	e5a36 <__ieee754_exp+0x2e>
   e5c4c:	4602      	mov	r2, r0
   e5c4e:	460b      	mov	r3, r1
   e5c50:	4620      	mov	r0, r4
   e5c52:	4629      	mov	r1, r5
   e5c54:	f001 fa26 	bl	e70a4 <__aeabi_dmul>
   e5c58:	2200      	movs	r2, #0
   e5c5a:	4606      	mov	r6, r0
   e5c5c:	460f      	mov	r7, r1
   e5c5e:	f04f 4380 	mov.w	r3, #1073741824	; 0x40000000
   e5c62:	4640      	mov	r0, r8
   e5c64:	4649      	mov	r1, r9
   e5c66:	f001 f869 	bl	e6d3c <__aeabi_dsub>
   e5c6a:	4602      	mov	r2, r0
   e5c6c:	460b      	mov	r3, r1
   e5c6e:	4630      	mov	r0, r6
   e5c70:	4639      	mov	r1, r7
   e5c72:	f001 fb41 	bl	e72f8 <__aeabi_ddiv>
   e5c76:	4622      	mov	r2, r4
   e5c78:	462b      	mov	r3, r5
   e5c7a:	f001 f85f 	bl	e6d3c <__aeabi_dsub>
   e5c7e:	4602      	mov	r2, r0
   e5c80:	460b      	mov	r3, r1
   e5c82:	2000      	movs	r0, #0
   e5c84:	4924      	ldr	r1, [pc, #144]	; (e5d18 <__ieee754_exp+0x310>)
   e5c86:	f001 f859 	bl	e6d3c <__aeabi_dsub>
   e5c8a:	ec41 0b10 	vmov	d0, r0, r1
   e5c8e:	b003      	add	sp, #12
   e5c90:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e5c94:	eb01 5106 	add.w	r1, r1, r6, lsl #20
   e5c98:	ec41 0b10 	vmov	d0, r0, r1
   e5c9c:	b003      	add	sp, #12
   e5c9e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e5ca2:	4606      	mov	r6, r0
   e5ca4:	e731      	b.n	e5b0a <__ieee754_exp+0x102>
   e5ca6:	bf00      	nop
	...
   e5cb0:	8800759c 	.word	0x8800759c
   e5cb4:	7e37e43c 	.word	0x7e37e43c
   e5cb8:	fefa39ef 	.word	0xfefa39ef
   e5cbc:	40862e42 	.word	0x40862e42
   e5cc0:	d52d3051 	.word	0xd52d3051
   e5cc4:	c0874910 	.word	0xc0874910
   e5cc8:	652b82fe 	.word	0x652b82fe
   e5ccc:	3ff71547 	.word	0x3ff71547
   e5cd0:	fee00000 	.word	0xfee00000
   e5cd4:	3fe62e42 	.word	0x3fe62e42
   e5cd8:	35793c76 	.word	0x35793c76
   e5cdc:	3dea39ef 	.word	0x3dea39ef
   e5ce0:	72bea4d0 	.word	0x72bea4d0
   e5ce4:	3e663769 	.word	0x3e663769
   e5ce8:	c5d26bf1 	.word	0xc5d26bf1
   e5cec:	3ebbbd41 	.word	0x3ebbbd41
   e5cf0:	af25de2c 	.word	0xaf25de2c
   e5cf4:	3f11566a 	.word	0x3f11566a
   e5cf8:	16bebd93 	.word	0x16bebd93
   e5cfc:	3f66c16c 	.word	0x3f66c16c
   e5d00:	5555553e 	.word	0x5555553e
   e5d04:	3fc55555 	.word	0x3fc55555
   e5d08:	40862e41 	.word	0x40862e41
   e5d0c:	7fefffff 	.word	0x7fefffff
   e5d10:	3fd62e42 	.word	0x3fd62e42
   e5d14:	3e2fffff 	.word	0x3e2fffff
   e5d18:	3ff00000 	.word	0x3ff00000
   e5d1c:	000eb560 	.word	0x000eb560
   e5d20:	3ff0a2b1 	.word	0x3ff0a2b1
   e5d24:	000eb580 	.word	0x000eb580
   e5d28:	000eb570 	.word	0x000eb570

000e5d2c <__ieee754_expf>:
   e5d2c:	ee10 3a10 	vmov	r3, s0
   e5d30:	f023 4200 	bic.w	r2, r3, #2147483648	; 0x80000000
   e5d34:	f1b2 4fff 	cmp.w	r2, #2139095040	; 0x7f800000
   e5d38:	d856      	bhi.n	e5de8 <__ieee754_expf+0xbc>
   e5d3a:	ea4f 71d3 	mov.w	r1, r3, lsr #31
   e5d3e:	d056      	beq.n	e5dee <__ieee754_expf+0xc2>
   e5d40:	4854      	ldr	r0, [pc, #336]	; (e5e94 <__ieee754_expf+0x168>)
   e5d42:	4283      	cmp	r3, r0
   e5d44:	dc73      	bgt.n	e5e2e <__ieee754_expf+0x102>
   e5d46:	2b00      	cmp	r3, #0
   e5d48:	db69      	blt.n	e5e1e <__ieee754_expf+0xf2>
   e5d4a:	4b53      	ldr	r3, [pc, #332]	; (e5e98 <__ieee754_expf+0x16c>)
   e5d4c:	429a      	cmp	r2, r3
   e5d4e:	d955      	bls.n	e5dfc <__ieee754_expf+0xd0>
   e5d50:	4b52      	ldr	r3, [pc, #328]	; (e5e9c <__ieee754_expf+0x170>)
   e5d52:	429a      	cmp	r2, r3
   e5d54:	d87d      	bhi.n	e5e52 <__ieee754_expf+0x126>
   e5d56:	4852      	ldr	r0, [pc, #328]	; (e5ea0 <__ieee754_expf+0x174>)
   e5d58:	4a52      	ldr	r2, [pc, #328]	; (e5ea4 <__ieee754_expf+0x178>)
   e5d5a:	008b      	lsls	r3, r1, #2
   e5d5c:	4418      	add	r0, r3
   e5d5e:	ed90 7a00 	vldr	s14, [r0]
   e5d62:	441a      	add	r2, r3
   e5d64:	ee70 4a47 	vsub.f32	s9, s0, s14
   e5d68:	f1c1 0301 	rsb	r3, r1, #1
   e5d6c:	ed92 7a00 	vldr	s14, [r2]
   e5d70:	1a5b      	subs	r3, r3, r1
   e5d72:	ee34 0ac7 	vsub.f32	s0, s9, s14
   e5d76:	ee60 7a00 	vmul.f32	s15, s0, s0
   e5d7a:	ed9f 4a4b 	vldr	s8, [pc, #300]	; e5ea8 <__ieee754_expf+0x17c>
   e5d7e:	ed9f 5a4b 	vldr	s10, [pc, #300]	; e5eac <__ieee754_expf+0x180>
   e5d82:	eddf 5a4b 	vldr	s11, [pc, #300]	; e5eb0 <__ieee754_expf+0x184>
   e5d86:	ed9f 6a4b 	vldr	s12, [pc, #300]	; e5eb4 <__ieee754_expf+0x188>
   e5d8a:	eddf 6a4b 	vldr	s13, [pc, #300]	; e5eb8 <__ieee754_expf+0x18c>
   e5d8e:	eea7 5a84 	vfma.f32	s10, s15, s8
   e5d92:	eee7 5a85 	vfma.f32	s11, s15, s10
   e5d96:	eea7 6aa5 	vfma.f32	s12, s15, s11
   e5d9a:	eee7 6a86 	vfma.f32	s13, s15, s12
   e5d9e:	eeb0 6a40 	vmov.f32	s12, s0
   e5da2:	eea7 6ae6 	vfms.f32	s12, s15, s13
   e5da6:	eef0 6a00 	vmov.f32	s13, #0	; 0x40000000  2.0
   e5daa:	2b00      	cmp	r3, #0
   e5dac:	d044      	beq.n	e5e38 <__ieee754_expf+0x10c>
   e5dae:	ee20 0a06 	vmul.f32	s0, s0, s12
   e5db2:	ee76 7ac6 	vsub.f32	s15, s13, s12
   e5db6:	f113 0f7d 	cmn.w	r3, #125	; 0x7d
   e5dba:	ee80 6a27 	vdiv.f32	s12, s0, s15
   e5dbe:	eef7 6a00 	vmov.f32	s13, #112	; 0x3f800000  1.0
   e5dc2:	ee37 7a46 	vsub.f32	s14, s14, s12
   e5dc6:	ee37 7a64 	vsub.f32	s14, s14, s9
   e5dca:	ee36 0ac7 	vsub.f32	s0, s13, s14
   e5dce:	da5a      	bge.n	e5e86 <__ieee754_expf+0x15a>
   e5dd0:	ee10 2a10 	vmov	r2, s0
   e5dd4:	3364      	adds	r3, #100	; 0x64
   e5dd6:	eb02 53c3 	add.w	r3, r2, r3, lsl #23
   e5dda:	eddf 7a38 	vldr	s15, [pc, #224]	; e5ebc <__ieee754_expf+0x190>
   e5dde:	ee00 3a10 	vmov	s0, r3
   e5de2:	ee20 0a27 	vmul.f32	s0, s0, s15
   e5de6:	4770      	bx	lr
   e5de8:	ee30 0a00 	vadd.f32	s0, s0, s0
   e5dec:	4770      	bx	lr
   e5dee:	eddf 7a34 	vldr	s15, [pc, #208]	; e5ec0 <__ieee754_expf+0x194>
   e5df2:	2900      	cmp	r1, #0
   e5df4:	bf18      	it	ne
   e5df6:	eeb0 0a67 	vmovne.f32	s0, s15
   e5dfa:	4770      	bx	lr
   e5dfc:	f1b2 5f46 	cmp.w	r2, #830472192	; 0x31800000
   e5e00:	d213      	bcs.n	e5e2a <__ieee754_expf+0xfe>
   e5e02:	eddf 7a30 	vldr	s15, [pc, #192]	; e5ec4 <__ieee754_expf+0x198>
   e5e06:	ee70 7a27 	vadd.f32	s15, s0, s15
   e5e0a:	eef7 6a00 	vmov.f32	s13, #112	; 0x3f800000  1.0
   e5e0e:	eef4 7ae6 	vcmpe.f32	s15, s13
   e5e12:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5e16:	dd08      	ble.n	e5e2a <__ieee754_expf+0xfe>
   e5e18:	ee30 0a26 	vadd.f32	s0, s0, s13
   e5e1c:	4770      	bx	lr
   e5e1e:	4b2a      	ldr	r3, [pc, #168]	; (e5ec8 <__ieee754_expf+0x19c>)
   e5e20:	429a      	cmp	r2, r3
   e5e22:	d992      	bls.n	e5d4a <__ieee754_expf+0x1e>
   e5e24:	ed9f 0a26 	vldr	s0, [pc, #152]	; e5ec0 <__ieee754_expf+0x194>
   e5e28:	4770      	bx	lr
   e5e2a:	2300      	movs	r3, #0
   e5e2c:	e7a3      	b.n	e5d76 <__ieee754_expf+0x4a>
   e5e2e:	ed9f 0a25 	vldr	s0, [pc, #148]	; e5ec4 <__ieee754_expf+0x198>
   e5e32:	ee20 0a00 	vmul.f32	s0, s0, s0
   e5e36:	4770      	bx	lr
   e5e38:	ee60 7a06 	vmul.f32	s15, s0, s12
   e5e3c:	ee76 6a66 	vsub.f32	s13, s12, s13
   e5e40:	eeb7 6a00 	vmov.f32	s12, #112	; 0x3f800000  1.0
   e5e44:	ee87 7aa6 	vdiv.f32	s14, s15, s13
   e5e48:	ee37 0a40 	vsub.f32	s0, s14, s0
   e5e4c:	ee36 0a40 	vsub.f32	s0, s12, s0
   e5e50:	4770      	bx	lr
   e5e52:	4b1e      	ldr	r3, [pc, #120]	; (e5ecc <__ieee754_expf+0x1a0>)
   e5e54:	ed9f 6a1e 	vldr	s12, [pc, #120]	; e5ed0 <__ieee754_expf+0x1a4>
   e5e58:	eddf 6a1e 	vldr	s13, [pc, #120]	; e5ed4 <__ieee754_expf+0x1a8>
   e5e5c:	ed9f 7a1e 	vldr	s14, [pc, #120]	; e5ed8 <__ieee754_expf+0x1ac>
   e5e60:	eb03 0381 	add.w	r3, r3, r1, lsl #2
   e5e64:	edd3 7a00 	vldr	s15, [r3]
   e5e68:	eee0 7a06 	vfma.f32	s15, s0, s12
   e5e6c:	eef0 4a40 	vmov.f32	s9, s0
   e5e70:	eefd 7ae7 	vcvt.s32.f32	s15, s15
   e5e74:	ee17 3a90 	vmov	r3, s15
   e5e78:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   e5e7c:	eee7 4ae6 	vfms.f32	s9, s15, s13
   e5e80:	ee27 7a87 	vmul.f32	s14, s15, s14
   e5e84:	e775      	b.n	e5d72 <__ieee754_expf+0x46>
   e5e86:	ee10 2a10 	vmov	r2, s0
   e5e8a:	eb02 53c3 	add.w	r3, r2, r3, lsl #23
   e5e8e:	ee00 3a10 	vmov	s0, r3
   e5e92:	4770      	bx	lr
   e5e94:	42b17217 	.word	0x42b17217
   e5e98:	3eb17218 	.word	0x3eb17218
   e5e9c:	3f851591 	.word	0x3f851591
   e5ea0:	000eb5a0 	.word	0x000eb5a0
   e5ea4:	000eb598 	.word	0x000eb598
   e5ea8:	3331bb4c 	.word	0x3331bb4c
   e5eac:	b5ddea0e 	.word	0xb5ddea0e
   e5eb0:	388ab355 	.word	0x388ab355
   e5eb4:	bb360b61 	.word	0xbb360b61
   e5eb8:	3e2aaaab 	.word	0x3e2aaaab
   e5ebc:	0d800000 	.word	0x0d800000
   e5ec0:	00000000 	.word	0x00000000
   e5ec4:	7149f2ca 	.word	0x7149f2ca
   e5ec8:	42cff1b5 	.word	0x42cff1b5
   e5ecc:	000eb590 	.word	0x000eb590
   e5ed0:	3fb8aa3b 	.word	0x3fb8aa3b
   e5ed4:	3f317180 	.word	0x3f317180
   e5ed8:	3717f7d1 	.word	0x3717f7d1

000e5edc <__ieee754_logf>:
   e5edc:	b430      	push	{r4, r5}
   e5ede:	b082      	sub	sp, #8
   e5ee0:	ed8d 0a01 	vstr	s0, [sp, #4]
   e5ee4:	9b01      	ldr	r3, [sp, #4]
   e5ee6:	f023 4200 	bic.w	r2, r3, #2147483648	; 0x80000000
   e5eea:	b372      	cbz	r2, e5f4a <__ieee754_logf+0x6e>
   e5eec:	2b00      	cmp	r3, #0
   e5eee:	db40      	blt.n	e5f72 <__ieee754_logf+0x96>
   e5ef0:	f1b3 4fff 	cmp.w	r3, #2139095040	; 0x7f800000
   e5ef4:	da48      	bge.n	e5f88 <__ieee754_logf+0xac>
   e5ef6:	f5b3 0f00 	cmp.w	r3, #8388608	; 0x800000
   e5efa:	db2f      	blt.n	e5f5c <__ieee754_logf+0x80>
   e5efc:	2200      	movs	r2, #0
   e5efe:	496e      	ldr	r1, [pc, #440]	; (e60b8 <__ieee754_logf+0x1dc>)
   e5f00:	f3c3 0516 	ubfx	r5, r3, #0, #23
   e5f04:	4429      	add	r1, r5
   e5f06:	f401 0100 	and.w	r1, r1, #8388608	; 0x800000
   e5f0a:	15db      	asrs	r3, r3, #23
   e5f0c:	3b7f      	subs	r3, #127	; 0x7f
   e5f0e:	f081 547e 	eor.w	r4, r1, #1065353216	; 0x3f800000
   e5f12:	4413      	add	r3, r2
   e5f14:	f105 000f 	add.w	r0, r5, #15
   e5f18:	ea44 0205 	orr.w	r2, r4, r5
   e5f1c:	ee00 2a10 	vmov	s0, r2
   e5f20:	f3c0 0216 	ubfx	r2, r0, #0, #23
   e5f24:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   e5f28:	2a0f      	cmp	r2, #15
   e5f2a:	eb03 53d1 	add.w	r3, r3, r1, lsr #23
   e5f2e:	ee70 7a67 	vsub.f32	s15, s0, s15
   e5f32:	dc30      	bgt.n	e5f96 <__ieee754_logf+0xba>
   e5f34:	eef5 7a40 	vcmp.f32	s15, #0.0
   e5f38:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5f3c:	d16c      	bne.n	e6018 <__ieee754_logf+0x13c>
   e5f3e:	2b00      	cmp	r3, #0
   e5f40:	f040 8096 	bne.w	e6070 <__ieee754_logf+0x194>
   e5f44:	ed9f 0a5d 	vldr	s0, [pc, #372]	; e60bc <__ieee754_logf+0x1e0>
   e5f48:	e005      	b.n	e5f56 <__ieee754_logf+0x7a>
   e5f4a:	ed9f 7a5d 	vldr	s14, [pc, #372]	; e60c0 <__ieee754_logf+0x1e4>
   e5f4e:	eddf 7a5b 	vldr	s15, [pc, #364]	; e60bc <__ieee754_logf+0x1e0>
   e5f52:	ee87 0a27 	vdiv.f32	s0, s14, s15
   e5f56:	b002      	add	sp, #8
   e5f58:	bc30      	pop	{r4, r5}
   e5f5a:	4770      	bx	lr
   e5f5c:	eddf 7a59 	vldr	s15, [pc, #356]	; e60c4 <__ieee754_logf+0x1e8>
   e5f60:	ed9d 7a01 	vldr	s14, [sp, #4]
   e5f64:	ee67 7a27 	vmul.f32	s15, s14, s15
   e5f68:	f06f 0218 	mvn.w	r2, #24
   e5f6c:	ee17 3a90 	vmov	r3, s15
   e5f70:	e7c5      	b.n	e5efe <__ieee754_logf+0x22>
   e5f72:	eddd 7a01 	vldr	s15, [sp, #4]
   e5f76:	ee37 7ae7 	vsub.f32	s14, s15, s15
   e5f7a:	eddf 7a50 	vldr	s15, [pc, #320]	; e60bc <__ieee754_logf+0x1e0>
   e5f7e:	ee87 0a27 	vdiv.f32	s0, s14, s15
   e5f82:	b002      	add	sp, #8
   e5f84:	bc30      	pop	{r4, r5}
   e5f86:	4770      	bx	lr
   e5f88:	eddd 7a01 	vldr	s15, [sp, #4]
   e5f8c:	ee37 0aa7 	vadd.f32	s0, s15, s15
   e5f90:	b002      	add	sp, #8
   e5f92:	bc30      	pop	{r4, r5}
   e5f94:	4770      	bx	lr
   e5f96:	eef0 6a00 	vmov.f32	s13, #0	; 0x40000000  2.0
   e5f9a:	ee77 6aa6 	vadd.f32	s13, s15, s13
   e5f9e:	ed9f 2a4a 	vldr	s4, [pc, #296]	; e60c8 <__ieee754_logf+0x1ec>
   e5fa2:	ed9f 4a4a 	vldr	s8, [pc, #296]	; e60cc <__ieee754_logf+0x1f0>
   e5fa6:	ed9f 5a4a 	vldr	s10, [pc, #296]	; e60d0 <__ieee754_logf+0x1f4>
   e5faa:	eddf 2a4a 	vldr	s5, [pc, #296]	; e60d4 <__ieee754_logf+0x1f8>
   e5fae:	eddf 4a4a 	vldr	s9, [pc, #296]	; e60d8 <__ieee754_logf+0x1fc>
   e5fb2:	ed9f 7a4a 	vldr	s14, [pc, #296]	; e60dc <__ieee754_logf+0x200>
   e5fb6:	ed9f 6a4a 	vldr	s12, [pc, #296]	; e60e0 <__ieee754_logf+0x204>
   e5fba:	4a4a      	ldr	r2, [pc, #296]	; (e60e4 <__ieee754_logf+0x208>)
   e5fbc:	eec7 3aa6 	vdiv.f32	s7, s15, s13
   e5fc0:	f5c5 1157 	rsb	r1, r5, #3522560	; 0x35c000
   e5fc4:	442a      	add	r2, r5
   e5fc6:	f501 7122 	add.w	r1, r1, #648	; 0x288
   e5fca:	430a      	orrs	r2, r1
   e5fcc:	2a00      	cmp	r2, #0
   e5fce:	ee06 3a90 	vmov	s13, r3
   e5fd2:	ee63 5aa3 	vmul.f32	s11, s7, s7
   e5fd6:	eeb8 3ae6 	vcvt.f32.s32	s6, s13
   e5fda:	ee65 6aa5 	vmul.f32	s13, s11, s11
   e5fde:	eea6 4a82 	vfma.f32	s8, s13, s4
   e5fe2:	eee6 4aa2 	vfma.f32	s9, s13, s5
   e5fe6:	eea6 5a84 	vfma.f32	s10, s13, s8
   e5fea:	eea6 6aa4 	vfma.f32	s12, s13, s9
   e5fee:	eea6 7a85 	vfma.f32	s14, s13, s10
   e5ff2:	ee27 7a25 	vmul.f32	s14, s14, s11
   e5ff6:	eea6 7a86 	vfma.f32	s14, s13, s12
   e5ffa:	dd46      	ble.n	e608a <__ieee754_logf+0x1ae>
   e5ffc:	eeb6 0a00 	vmov.f32	s0, #96	; 0x3f000000  0.5
   e6000:	ee27 0a80 	vmul.f32	s0, s15, s0
   e6004:	ee20 0a27 	vmul.f32	s0, s0, s15
   e6008:	bb0b      	cbnz	r3, e604e <__ieee754_logf+0x172>
   e600a:	ee37 7a00 	vadd.f32	s14, s14, s0
   e600e:	eea3 0ac7 	vfms.f32	s0, s7, s14
   e6012:	ee37 0ac0 	vsub.f32	s0, s15, s0
   e6016:	e79e      	b.n	e5f56 <__ieee754_logf+0x7a>
   e6018:	ed9f 7a33 	vldr	s14, [pc, #204]	; e60e8 <__ieee754_logf+0x20c>
   e601c:	eeb6 0a00 	vmov.f32	s0, #96	; 0x3f000000  0.5
   e6020:	eea7 0ac7 	vfms.f32	s0, s15, s14
   e6024:	ee27 7aa7 	vmul.f32	s14, s15, s15
   e6028:	ee20 0a07 	vmul.f32	s0, s0, s14
   e602c:	2b00      	cmp	r3, #0
   e602e:	d0f0      	beq.n	e6012 <__ieee754_logf+0x136>
   e6030:	ee07 3a10 	vmov	s14, r3
   e6034:	ed9f 6a2d 	vldr	s12, [pc, #180]	; e60ec <__ieee754_logf+0x210>
   e6038:	eddf 6a2d 	vldr	s13, [pc, #180]	; e60f0 <__ieee754_logf+0x214>
   e603c:	eeb8 7ac7 	vcvt.f32.s32	s14, s14
   e6040:	eea7 0a46 	vfms.f32	s0, s14, s12
   e6044:	ee30 0a67 	vsub.f32	s0, s0, s15
   e6048:	ee97 0a26 	vfnms.f32	s0, s14, s13
   e604c:	e783      	b.n	e5f56 <__ieee754_logf+0x7a>
   e604e:	eddf 6a27 	vldr	s13, [pc, #156]	; e60ec <__ieee754_logf+0x210>
   e6052:	ed9f 6a27 	vldr	s12, [pc, #156]	; e60f0 <__ieee754_logf+0x214>
   e6056:	ee37 7a00 	vadd.f32	s14, s14, s0
   e605a:	ee63 6a26 	vmul.f32	s13, s6, s13
   e605e:	eee3 6a87 	vfma.f32	s13, s7, s14
   e6062:	ee30 0a66 	vsub.f32	s0, s0, s13
   e6066:	ee30 0a67 	vsub.f32	s0, s0, s15
   e606a:	ee93 0a06 	vfnms.f32	s0, s6, s12
   e606e:	e772      	b.n	e5f56 <__ieee754_logf+0x7a>
   e6070:	ee07 3a90 	vmov	s15, r3
   e6074:	ed9f 0a1d 	vldr	s0, [pc, #116]	; e60ec <__ieee754_logf+0x210>
   e6078:	ed9f 7a1d 	vldr	s14, [pc, #116]	; e60f0 <__ieee754_logf+0x214>
   e607c:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   e6080:	ee27 0a80 	vmul.f32	s0, s15, s0
   e6084:	eea7 0a87 	vfma.f32	s0, s15, s14
   e6088:	e765      	b.n	e5f56 <__ieee754_logf+0x7a>
   e608a:	b173      	cbz	r3, e60aa <__ieee754_logf+0x1ce>
   e608c:	ed9f 0a17 	vldr	s0, [pc, #92]	; e60ec <__ieee754_logf+0x210>
   e6090:	eddf 6a17 	vldr	s13, [pc, #92]	; e60f0 <__ieee754_logf+0x214>
   e6094:	ee37 7ac7 	vsub.f32	s14, s15, s14
   e6098:	ee20 0a43 	vnmul.f32	s0, s0, s6
   e609c:	eea3 0a87 	vfma.f32	s0, s7, s14
   e60a0:	ee30 0a67 	vsub.f32	s0, s0, s15
   e60a4:	ee93 0a26 	vfnms.f32	s0, s6, s13
   e60a8:	e755      	b.n	e5f56 <__ieee754_logf+0x7a>
   e60aa:	ee37 7ac7 	vsub.f32	s14, s15, s14
   e60ae:	eee3 7ac7 	vfms.f32	s15, s7, s14
   e60b2:	eeb0 0a67 	vmov.f32	s0, s15
   e60b6:	e74e      	b.n	e5f56 <__ieee754_logf+0x7a>
   e60b8:	004afb20 	.word	0x004afb20
   e60bc:	00000000 	.word	0x00000000
   e60c0:	cc000000 	.word	0xcc000000
   e60c4:	4c000000 	.word	0x4c000000
   e60c8:	3e178897 	.word	0x3e178897
   e60cc:	3e3a3325 	.word	0x3e3a3325
   e60d0:	3e924925 	.word	0x3e924925
   e60d4:	3e1cd04f 	.word	0x3e1cd04f
   e60d8:	3e638e29 	.word	0x3e638e29
   e60dc:	3f2aaaab 	.word	0x3f2aaaab
   e60e0:	3ecccccd 	.word	0x3ecccccd
   e60e4:	ffcf5c30 	.word	0xffcf5c30
   e60e8:	3eaaaaab 	.word	0x3eaaaaab
   e60ec:	3717f7d1 	.word	0x3717f7d1
   e60f0:	3f317180 	.word	0x3f317180

000e60f4 <__ieee754_rem_pio2f>:
   e60f4:	b570      	push	{r4, r5, r6, lr}
   e60f6:	ee10 3a10 	vmov	r3, s0
   e60fa:	4a96      	ldr	r2, [pc, #600]	; (e6354 <__ieee754_rem_pio2f+0x260>)
   e60fc:	f023 4400 	bic.w	r4, r3, #2147483648	; 0x80000000
   e6100:	4294      	cmp	r4, r2
   e6102:	b086      	sub	sp, #24
   e6104:	dd5f      	ble.n	e61c6 <__ieee754_rem_pio2f+0xd2>
   e6106:	4a94      	ldr	r2, [pc, #592]	; (e6358 <__ieee754_rem_pio2f+0x264>)
   e6108:	4294      	cmp	r4, r2
   e610a:	ee10 6a10 	vmov	r6, s0
   e610e:	dc1b      	bgt.n	e6148 <__ieee754_rem_pio2f+0x54>
   e6110:	2b00      	cmp	r3, #0
   e6112:	eddf 7a92 	vldr	s15, [pc, #584]	; e635c <__ieee754_rem_pio2f+0x268>
   e6116:	4a92      	ldr	r2, [pc, #584]	; (e6360 <__ieee754_rem_pio2f+0x26c>)
   e6118:	f024 040f 	bic.w	r4, r4, #15
   e611c:	f340 80d5 	ble.w	e62ca <__ieee754_rem_pio2f+0x1d6>
   e6120:	4294      	cmp	r4, r2
   e6122:	ee70 7a67 	vsub.f32	s15, s0, s15
   e6126:	d05e      	beq.n	e61e6 <__ieee754_rem_pio2f+0xf2>
   e6128:	ed9f 7a8e 	vldr	s14, [pc, #568]	; e6364 <__ieee754_rem_pio2f+0x270>
   e612c:	ee77 6ac7 	vsub.f32	s13, s15, s14
   e6130:	2301      	movs	r3, #1
   e6132:	ee77 7ae6 	vsub.f32	s15, s15, s13
   e6136:	edc0 6a00 	vstr	s13, [r0]
   e613a:	ee77 7ac7 	vsub.f32	s15, s15, s14
   e613e:	edc0 7a01 	vstr	s15, [r0, #4]
   e6142:	4618      	mov	r0, r3
   e6144:	b006      	add	sp, #24
   e6146:	bd70      	pop	{r4, r5, r6, pc}
   e6148:	4a87      	ldr	r2, [pc, #540]	; (e6368 <__ieee754_rem_pio2f+0x274>)
   e614a:	4294      	cmp	r4, r2
   e614c:	4605      	mov	r5, r0
   e614e:	dd5c      	ble.n	e620a <__ieee754_rem_pio2f+0x116>
   e6150:	f1b4 4fff 	cmp.w	r4, #2139095040	; 0x7f800000
   e6154:	da3f      	bge.n	e61d6 <__ieee754_rem_pio2f+0xe2>
   e6156:	15e2      	asrs	r2, r4, #23
   e6158:	3a86      	subs	r2, #134	; 0x86
   e615a:	eba4 53c2 	sub.w	r3, r4, r2, lsl #23
   e615e:	ee07 3a10 	vmov	s14, r3
   e6162:	eefd 6ac7 	vcvt.s32.f32	s13, s14
   e6166:	eddf 7a81 	vldr	s15, [pc, #516]	; e636c <__ieee754_rem_pio2f+0x278>
   e616a:	eef8 6ae6 	vcvt.f32.s32	s13, s13
   e616e:	ee37 7a66 	vsub.f32	s14, s14, s13
   e6172:	edcd 6a03 	vstr	s13, [sp, #12]
   e6176:	ee27 7a27 	vmul.f32	s14, s14, s15
   e617a:	eefd 6ac7 	vcvt.s32.f32	s13, s14
   e617e:	eef8 6ae6 	vcvt.f32.s32	s13, s13
   e6182:	ee37 7a66 	vsub.f32	s14, s14, s13
   e6186:	edcd 6a04 	vstr	s13, [sp, #16]
   e618a:	ee67 7a27 	vmul.f32	s15, s14, s15
   e618e:	eef5 7a40 	vcmp.f32	s15, #0.0
   e6192:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e6196:	edcd 7a05 	vstr	s15, [sp, #20]
   e619a:	f040 80b7 	bne.w	e630c <__ieee754_rem_pio2f+0x218>
   e619e:	eef5 6a40 	vcmp.f32	s13, #0.0
   e61a2:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e61a6:	bf0c      	ite	eq
   e61a8:	2301      	moveq	r3, #1
   e61aa:	2302      	movne	r3, #2
   e61ac:	4970      	ldr	r1, [pc, #448]	; (e6370 <__ieee754_rem_pio2f+0x27c>)
   e61ae:	9101      	str	r1, [sp, #4]
   e61b0:	2102      	movs	r1, #2
   e61b2:	9100      	str	r1, [sp, #0]
   e61b4:	a803      	add	r0, sp, #12
   e61b6:	4629      	mov	r1, r5
   e61b8:	f000 f9bc 	bl	e6534 <__kernel_rem_pio2f>
   e61bc:	2e00      	cmp	r6, #0
   e61be:	f2c0 8097 	blt.w	e62f0 <__ieee754_rem_pio2f+0x1fc>
   e61c2:	4603      	mov	r3, r0
   e61c4:	e004      	b.n	e61d0 <__ieee754_rem_pio2f+0xdc>
   e61c6:	2200      	movs	r2, #0
   e61c8:	ed80 0a00 	vstr	s0, [r0]
   e61cc:	6042      	str	r2, [r0, #4]
   e61ce:	2300      	movs	r3, #0
   e61d0:	4618      	mov	r0, r3
   e61d2:	b006      	add	sp, #24
   e61d4:	bd70      	pop	{r4, r5, r6, pc}
   e61d6:	ee70 7a40 	vsub.f32	s15, s0, s0
   e61da:	2300      	movs	r3, #0
   e61dc:	edc0 7a01 	vstr	s15, [r0, #4]
   e61e0:	edc0 7a00 	vstr	s15, [r0]
   e61e4:	e7f4      	b.n	e61d0 <__ieee754_rem_pio2f+0xdc>
   e61e6:	eddf 6a63 	vldr	s13, [pc, #396]	; e6374 <__ieee754_rem_pio2f+0x280>
   e61ea:	ed9f 7a63 	vldr	s14, [pc, #396]	; e6378 <__ieee754_rem_pio2f+0x284>
   e61ee:	ee77 7ae6 	vsub.f32	s15, s15, s13
   e61f2:	2301      	movs	r3, #1
   e61f4:	ee77 6ac7 	vsub.f32	s13, s15, s14
   e61f8:	ee77 7ae6 	vsub.f32	s15, s15, s13
   e61fc:	edc0 6a00 	vstr	s13, [r0]
   e6200:	ee77 7ac7 	vsub.f32	s15, s15, s14
   e6204:	edc0 7a01 	vstr	s15, [r0, #4]
   e6208:	e7e2      	b.n	e61d0 <__ieee754_rem_pio2f+0xdc>
   e620a:	f000 fd01 	bl	e6c10 <fabsf>
   e620e:	eddf 6a5b 	vldr	s13, [pc, #364]	; e637c <__ieee754_rem_pio2f+0x288>
   e6212:	eddf 5a52 	vldr	s11, [pc, #328]	; e635c <__ieee754_rem_pio2f+0x268>
   e6216:	ed9f 7a53 	vldr	s14, [pc, #332]	; e6364 <__ieee754_rem_pio2f+0x270>
   e621a:	eef6 7a00 	vmov.f32	s15, #96	; 0x3f000000  0.5
   e621e:	eee0 7a26 	vfma.f32	s15, s0, s13
   e6222:	eefd 7ae7 	vcvt.s32.f32	s15, s15
   e6226:	ee17 3a90 	vmov	r3, s15
   e622a:	eef8 6ae7 	vcvt.f32.s32	s13, s15
   e622e:	2b1f      	cmp	r3, #31
   e6230:	eeb1 6a66 	vneg.f32	s12, s13
   e6234:	eea6 0a25 	vfma.f32	s0, s12, s11
   e6238:	ee66 7a87 	vmul.f32	s15, s13, s14
   e623c:	dc1d      	bgt.n	e627a <__ieee754_rem_pio2f+0x186>
   e623e:	4950      	ldr	r1, [pc, #320]	; (e6380 <__ieee754_rem_pio2f+0x28c>)
   e6240:	1e58      	subs	r0, r3, #1
   e6242:	f024 02ff 	bic.w	r2, r4, #255	; 0xff
   e6246:	f851 1020 	ldr.w	r1, [r1, r0, lsl #2]
   e624a:	428a      	cmp	r2, r1
   e624c:	d015      	beq.n	e627a <__ieee754_rem_pio2f+0x186>
   e624e:	ee30 7a67 	vsub.f32	s14, s0, s15
   e6252:	ed85 7a00 	vstr	s14, [r5]
   e6256:	ee30 0a47 	vsub.f32	s0, s0, s14
   e625a:	2e00      	cmp	r6, #0
   e625c:	ee30 0a67 	vsub.f32	s0, s0, s15
   e6260:	ed85 0a01 	vstr	s0, [r5, #4]
   e6264:	dab4      	bge.n	e61d0 <__ieee754_rem_pio2f+0xdc>
   e6266:	eeb1 7a47 	vneg.f32	s14, s14
   e626a:	eeb1 0a40 	vneg.f32	s0, s0
   e626e:	ed85 7a00 	vstr	s14, [r5]
   e6272:	ed85 0a01 	vstr	s0, [r5, #4]
   e6276:	425b      	negs	r3, r3
   e6278:	e7aa      	b.n	e61d0 <__ieee754_rem_pio2f+0xdc>
   e627a:	ee30 7a67 	vsub.f32	s14, s0, s15
   e627e:	15e4      	asrs	r4, r4, #23
   e6280:	ee17 2a10 	vmov	r2, s14
   e6284:	f3c2 52c7 	ubfx	r2, r2, #23, #8
   e6288:	1aa2      	subs	r2, r4, r2
   e628a:	2a08      	cmp	r2, #8
   e628c:	dde1      	ble.n	e6252 <__ieee754_rem_pio2f+0x15e>
   e628e:	eddf 7a39 	vldr	s15, [pc, #228]	; e6374 <__ieee754_rem_pio2f+0x280>
   e6292:	ed9f 7a39 	vldr	s14, [pc, #228]	; e6378 <__ieee754_rem_pio2f+0x284>
   e6296:	eef0 5a40 	vmov.f32	s11, s0
   e629a:	eee6 5a27 	vfma.f32	s11, s12, s15
   e629e:	ee30 0a65 	vsub.f32	s0, s0, s11
   e62a2:	eea6 0a27 	vfma.f32	s0, s12, s15
   e62a6:	eef0 7a40 	vmov.f32	s15, s0
   e62aa:	eed6 7a87 	vfnms.f32	s15, s13, s14
   e62ae:	ee35 7ae7 	vsub.f32	s14, s11, s15
   e62b2:	ee17 2a10 	vmov	r2, s14
   e62b6:	f3c2 52c7 	ubfx	r2, r2, #23, #8
   e62ba:	1aa4      	subs	r4, r4, r2
   e62bc:	2c19      	cmp	r4, #25
   e62be:	dc3a      	bgt.n	e6336 <__ieee754_rem_pio2f+0x242>
   e62c0:	ed85 7a00 	vstr	s14, [r5]
   e62c4:	eeb0 0a65 	vmov.f32	s0, s11
   e62c8:	e7c5      	b.n	e6256 <__ieee754_rem_pio2f+0x162>
   e62ca:	4294      	cmp	r4, r2
   e62cc:	ee70 7a27 	vadd.f32	s15, s0, s15
   e62d0:	d01e      	beq.n	e6310 <__ieee754_rem_pio2f+0x21c>
   e62d2:	ed9f 7a24 	vldr	s14, [pc, #144]	; e6364 <__ieee754_rem_pio2f+0x270>
   e62d6:	ee77 6a87 	vadd.f32	s13, s15, s14
   e62da:	f04f 33ff 	mov.w	r3, #4294967295	; 0xffffffff
   e62de:	ee77 7ae6 	vsub.f32	s15, s15, s13
   e62e2:	edc0 6a00 	vstr	s13, [r0]
   e62e6:	ee77 7a87 	vadd.f32	s15, s15, s14
   e62ea:	edc0 7a01 	vstr	s15, [r0, #4]
   e62ee:	e76f      	b.n	e61d0 <__ieee754_rem_pio2f+0xdc>
   e62f0:	ed95 7a00 	vldr	s14, [r5]
   e62f4:	edd5 7a01 	vldr	s15, [r5, #4]
   e62f8:	eeb1 7a47 	vneg.f32	s14, s14
   e62fc:	eef1 7a67 	vneg.f32	s15, s15
   e6300:	4243      	negs	r3, r0
   e6302:	ed85 7a00 	vstr	s14, [r5]
   e6306:	edc5 7a01 	vstr	s15, [r5, #4]
   e630a:	e761      	b.n	e61d0 <__ieee754_rem_pio2f+0xdc>
   e630c:	2303      	movs	r3, #3
   e630e:	e74d      	b.n	e61ac <__ieee754_rem_pio2f+0xb8>
   e6310:	eddf 6a18 	vldr	s13, [pc, #96]	; e6374 <__ieee754_rem_pio2f+0x280>
   e6314:	ed9f 7a18 	vldr	s14, [pc, #96]	; e6378 <__ieee754_rem_pio2f+0x284>
   e6318:	ee77 7aa6 	vadd.f32	s15, s15, s13
   e631c:	f04f 33ff 	mov.w	r3, #4294967295	; 0xffffffff
   e6320:	ee77 6a87 	vadd.f32	s13, s15, s14
   e6324:	ee77 7ae6 	vsub.f32	s15, s15, s13
   e6328:	edc0 6a00 	vstr	s13, [r0]
   e632c:	ee77 7a87 	vadd.f32	s15, s15, s14
   e6330:	edc0 7a01 	vstr	s15, [r0, #4]
   e6334:	e74c      	b.n	e61d0 <__ieee754_rem_pio2f+0xdc>
   e6336:	ed9f 7a13 	vldr	s14, [pc, #76]	; e6384 <__ieee754_rem_pio2f+0x290>
   e633a:	ed9f 5a13 	vldr	s10, [pc, #76]	; e6388 <__ieee754_rem_pio2f+0x294>
   e633e:	eeb0 0a65 	vmov.f32	s0, s11
   e6342:	eea6 0a07 	vfma.f32	s0, s12, s14
   e6346:	ee75 7ac0 	vsub.f32	s15, s11, s0
   e634a:	eee6 7a07 	vfma.f32	s15, s12, s14
   e634e:	eed6 7a85 	vfnms.f32	s15, s13, s10
   e6352:	e77c      	b.n	e624e <__ieee754_rem_pio2f+0x15a>
   e6354:	3f490fd8 	.word	0x3f490fd8
   e6358:	4016cbe3 	.word	0x4016cbe3
   e635c:	3fc90f80 	.word	0x3fc90f80
   e6360:	3fc90fd0 	.word	0x3fc90fd0
   e6364:	37354443 	.word	0x37354443
   e6368:	43490f80 	.word	0x43490f80
   e636c:	43800000 	.word	0x43800000
   e6370:	000eb628 	.word	0x000eb628
   e6374:	37354400 	.word	0x37354400
   e6378:	2e85a308 	.word	0x2e85a308
   e637c:	3f22f984 	.word	0x3f22f984
   e6380:	000eb5a8 	.word	0x000eb5a8
   e6384:	2e85a300 	.word	0x2e85a300
   e6388:	248d3132 	.word	0x248d3132

000e638c <__ieee754_sqrtf>:
   e638c:	ee10 3a10 	vmov	r3, s0
   e6390:	f023 4200 	bic.w	r2, r3, #2147483648	; 0x80000000
   e6394:	f1b2 4fff 	cmp.w	r2, #2139095040	; 0x7f800000
   e6398:	b470      	push	{r4, r5, r6}
   e639a:	d230      	bcs.n	e63fe <__ieee754_sqrtf+0x72>
   e639c:	b36a      	cbz	r2, e63fa <__ieee754_sqrtf+0x6e>
   e639e:	2b00      	cmp	r3, #0
   e63a0:	db3d      	blt.n	e641e <__ieee754_sqrtf+0x92>
   e63a2:	f5b2 0f00 	cmp.w	r2, #8388608	; 0x800000
   e63a6:	ea4f 50e3 	mov.w	r0, r3, asr #23
   e63aa:	d32c      	bcc.n	e6406 <__ieee754_sqrtf+0x7a>
   e63ac:	f1a0 027f 	sub.w	r2, r0, #127	; 0x7f
   e63b0:	f3c3 0316 	ubfx	r3, r3, #0, #23
   e63b4:	07d1      	lsls	r1, r2, #31
   e63b6:	f443 0300 	orr.w	r3, r3, #8388608	; 0x800000
   e63ba:	bf48      	it	mi
   e63bc:	005b      	lslmi	r3, r3, #1
   e63be:	2400      	movs	r4, #0
   e63c0:	1056      	asrs	r6, r2, #1
   e63c2:	005b      	lsls	r3, r3, #1
   e63c4:	4625      	mov	r5, r4
   e63c6:	2119      	movs	r1, #25
   e63c8:	f04f 7280 	mov.w	r2, #16777216	; 0x1000000
   e63cc:	18a8      	adds	r0, r5, r2
   e63ce:	4298      	cmp	r0, r3
   e63d0:	dc02      	bgt.n	e63d8 <__ieee754_sqrtf+0x4c>
   e63d2:	1a1b      	subs	r3, r3, r0
   e63d4:	1885      	adds	r5, r0, r2
   e63d6:	4414      	add	r4, r2
   e63d8:	3901      	subs	r1, #1
   e63da:	ea4f 0343 	mov.w	r3, r3, lsl #1
   e63de:	ea4f 0252 	mov.w	r2, r2, lsr #1
   e63e2:	d1f3      	bne.n	e63cc <__ieee754_sqrtf+0x40>
   e63e4:	b113      	cbz	r3, e63ec <__ieee754_sqrtf+0x60>
   e63e6:	f004 0301 	and.w	r3, r4, #1
   e63ea:	441c      	add	r4, r3
   e63ec:	1064      	asrs	r4, r4, #1
   e63ee:	f104 547c 	add.w	r4, r4, #1056964608	; 0x3f000000
   e63f2:	eb04 53c6 	add.w	r3, r4, r6, lsl #23
   e63f6:	ee00 3a10 	vmov	s0, r3
   e63fa:	bc70      	pop	{r4, r5, r6}
   e63fc:	4770      	bx	lr
   e63fe:	eea0 0a00 	vfma.f32	s0, s0, s0
   e6402:	bc70      	pop	{r4, r5, r6}
   e6404:	4770      	bx	lr
   e6406:	f413 0200 	ands.w	r2, r3, #8388608	; 0x800000
   e640a:	d001      	beq.n	e6410 <__ieee754_sqrtf+0x84>
   e640c:	e00c      	b.n	e6428 <__ieee754_sqrtf+0x9c>
   e640e:	460a      	mov	r2, r1
   e6410:	005b      	lsls	r3, r3, #1
   e6412:	021c      	lsls	r4, r3, #8
   e6414:	f102 0101 	add.w	r1, r2, #1
   e6418:	d5f9      	bpl.n	e640e <__ieee754_sqrtf+0x82>
   e641a:	1a80      	subs	r0, r0, r2
   e641c:	e7c6      	b.n	e63ac <__ieee754_sqrtf+0x20>
   e641e:	ee70 7a40 	vsub.f32	s15, s0, s0
   e6422:	ee87 0aa7 	vdiv.f32	s0, s15, s15
   e6426:	e7e8      	b.n	e63fa <__ieee754_sqrtf+0x6e>
   e6428:	f04f 32ff 	mov.w	r2, #4294967295	; 0xffffffff
   e642c:	e7f5      	b.n	e641a <__ieee754_sqrtf+0x8e>
   e642e:	bf00      	nop

000e6430 <__kernel_cosf>:
   e6430:	ee10 3a10 	vmov	r3, s0
   e6434:	f023 4300 	bic.w	r3, r3, #2147483648	; 0x80000000
   e6438:	f1b3 5f48 	cmp.w	r3, #838860800	; 0x32000000
   e643c:	da2c      	bge.n	e6498 <__kernel_cosf+0x68>
   e643e:	eefd 7ac0 	vcvt.s32.f32	s15, s0
   e6442:	ee17 3a90 	vmov	r3, s15
   e6446:	2b00      	cmp	r3, #0
   e6448:	d060      	beq.n	e650c <__kernel_cosf+0xdc>
   e644a:	ee20 7a00 	vmul.f32	s14, s0, s0
   e644e:	eddf 4a31 	vldr	s9, [pc, #196]	; e6514 <__kernel_cosf+0xe4>
   e6452:	ed9f 5a31 	vldr	s10, [pc, #196]	; e6518 <__kernel_cosf+0xe8>
   e6456:	eddf 5a31 	vldr	s11, [pc, #196]	; e651c <__kernel_cosf+0xec>
   e645a:	ed9f 6a31 	vldr	s12, [pc, #196]	; e6520 <__kernel_cosf+0xf0>
   e645e:	eddf 7a31 	vldr	s15, [pc, #196]	; e6524 <__kernel_cosf+0xf4>
   e6462:	eddf 6a31 	vldr	s13, [pc, #196]	; e6528 <__kernel_cosf+0xf8>
   e6466:	eea7 5a24 	vfma.f32	s10, s14, s9
   e646a:	eee7 5a05 	vfma.f32	s11, s14, s10
   e646e:	eea7 6a25 	vfma.f32	s12, s14, s11
   e6472:	eee7 7a06 	vfma.f32	s15, s14, s12
   e6476:	eee7 6a27 	vfma.f32	s13, s14, s15
   e647a:	ee66 6a87 	vmul.f32	s13, s13, s14
   e647e:	ee60 0ac0 	vnmul.f32	s1, s1, s0
   e6482:	eeb6 6a00 	vmov.f32	s12, #96	; 0x3f000000  0.5
   e6486:	eee7 0a26 	vfma.f32	s1, s14, s13
   e648a:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   e648e:	eed7 0a06 	vfnms.f32	s1, s14, s12
   e6492:	ee37 0ae0 	vsub.f32	s0, s15, s1
   e6496:	4770      	bx	lr
   e6498:	ee20 7a00 	vmul.f32	s14, s0, s0
   e649c:	eddf 4a1d 	vldr	s9, [pc, #116]	; e6514 <__kernel_cosf+0xe4>
   e64a0:	ed9f 5a1d 	vldr	s10, [pc, #116]	; e6518 <__kernel_cosf+0xe8>
   e64a4:	eddf 5a1d 	vldr	s11, [pc, #116]	; e651c <__kernel_cosf+0xec>
   e64a8:	ed9f 6a1d 	vldr	s12, [pc, #116]	; e6520 <__kernel_cosf+0xf0>
   e64ac:	eddf 7a1d 	vldr	s15, [pc, #116]	; e6524 <__kernel_cosf+0xf4>
   e64b0:	eddf 6a1d 	vldr	s13, [pc, #116]	; e6528 <__kernel_cosf+0xf8>
   e64b4:	4a1d      	ldr	r2, [pc, #116]	; (e652c <__kernel_cosf+0xfc>)
   e64b6:	eea7 5a24 	vfma.f32	s10, s14, s9
   e64ba:	4293      	cmp	r3, r2
   e64bc:	eee7 5a05 	vfma.f32	s11, s14, s10
   e64c0:	eea7 6a25 	vfma.f32	s12, s14, s11
   e64c4:	eee7 7a06 	vfma.f32	s15, s14, s12
   e64c8:	eee7 6a27 	vfma.f32	s13, s14, s15
   e64cc:	ee66 6a87 	vmul.f32	s13, s13, s14
   e64d0:	ddd5      	ble.n	e647e <__kernel_cosf+0x4e>
   e64d2:	4a17      	ldr	r2, [pc, #92]	; (e6530 <__kernel_cosf+0x100>)
   e64d4:	4293      	cmp	r3, r2
   e64d6:	dc14      	bgt.n	e6502 <__kernel_cosf+0xd2>
   e64d8:	f103 437f 	add.w	r3, r3, #4278190080	; 0xff000000
   e64dc:	ee07 3a90 	vmov	s15, r3
   e64e0:	eeb7 6a00 	vmov.f32	s12, #112	; 0x3f800000  1.0
   e64e4:	ee36 6a67 	vsub.f32	s12, s12, s15
   e64e8:	ee60 0ac0 	vnmul.f32	s1, s1, s0
   e64ec:	eef6 5a00 	vmov.f32	s11, #96	; 0x3f000000  0.5
   e64f0:	eee7 0a26 	vfma.f32	s1, s14, s13
   e64f4:	eed7 7a25 	vfnms.f32	s15, s14, s11
   e64f8:	ee77 7ae0 	vsub.f32	s15, s15, s1
   e64fc:	ee36 0a67 	vsub.f32	s0, s12, s15
   e6500:	4770      	bx	lr
   e6502:	eeb6 6a07 	vmov.f32	s12, #103	; 0x3f380000  0.7187500
   e6506:	eef5 7a02 	vmov.f32	s15, #82	; 0x3e900000  0.2812500
   e650a:	e7ed      	b.n	e64e8 <__kernel_cosf+0xb8>
   e650c:	eeb7 0a00 	vmov.f32	s0, #112	; 0x3f800000  1.0
   e6510:	4770      	bx	lr
   e6512:	bf00      	nop
   e6514:	ad47d74e 	.word	0xad47d74e
   e6518:	310f74f6 	.word	0x310f74f6
   e651c:	b493f27c 	.word	0xb493f27c
   e6520:	37d00d01 	.word	0x37d00d01
   e6524:	bab60b61 	.word	0xbab60b61
   e6528:	3d2aaaab 	.word	0x3d2aaaab
   e652c:	3e999999 	.word	0x3e999999
   e6530:	3f480000 	.word	0x3f480000

000e6534 <__kernel_rem_pio2f>:
   e6534:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e6538:	ed2d 8b04 	vpush	{d8-d9}
   e653c:	b0d7      	sub	sp, #348	; 0x15c
   e653e:	1e5f      	subs	r7, r3, #1
   e6540:	4cda      	ldr	r4, [pc, #872]	; (e68ac <__kernel_rem_pio2f+0x378>)
   e6542:	9d64      	ldr	r5, [sp, #400]	; 0x190
   e6544:	9301      	str	r3, [sp, #4]
   e6546:	1ed3      	subs	r3, r2, #3
   e6548:	bf48      	it	mi
   e654a:	1d13      	addmi	r3, r2, #4
   e654c:	f854 6025 	ldr.w	r6, [r4, r5, lsl #2]
   e6550:	10db      	asrs	r3, r3, #3
   e6552:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
   e6556:	f103 0a01 	add.w	sl, r3, #1
   e655a:	468b      	mov	fp, r1
   e655c:	19f1      	adds	r1, r6, r7
   e655e:	9302      	str	r3, [sp, #8]
   e6560:	4681      	mov	r9, r0
   e6562:	eba2 0aca 	sub.w	sl, r2, sl, lsl #3
   e6566:	eba3 0307 	sub.w	r3, r3, r7
   e656a:	d414      	bmi.n	e6596 <__kernel_rem_pio2f+0x62>
   e656c:	4419      	add	r1, r3
   e656e:	9865      	ldr	r0, [sp, #404]	; 0x194
   e6570:	3101      	adds	r1, #1
   e6572:	aa1a      	add	r2, sp, #104	; 0x68
   e6574:	2b00      	cmp	r3, #0
   e6576:	bfaa      	itet	ge
   e6578:	f850 4023 	ldrge.w	r4, [r0, r3, lsl #2]
   e657c:	eddf 7ad0 	vldrlt	s15, [pc, #832]	; e68c0 <__kernel_rem_pio2f+0x38c>
   e6580:	ee07 4a90 	vmovge	s15, r4
   e6584:	f103 0301 	add.w	r3, r3, #1
   e6588:	bfa8      	it	ge
   e658a:	eef8 7ae7 	vcvtge.f32.s32	s15, s15
   e658e:	428b      	cmp	r3, r1
   e6590:	ece2 7a01 	vstmia	r2!, {s15}
   e6594:	d1ee      	bne.n	e6574 <__kernel_rem_pio2f+0x40>
   e6596:	2e00      	cmp	r6, #0
   e6598:	f2c0 82d6 	blt.w	e6b48 <__kernel_rem_pio2f+0x614>
   e659c:	9b01      	ldr	r3, [sp, #4]
   e659e:	ad42      	add	r5, sp, #264	; 0x108
   e65a0:	009c      	lsls	r4, r3, #2
   e65a2:	f106 0e01 	add.w	lr, r6, #1
   e65a6:	ab1a      	add	r3, sp, #104	; 0x68
   e65a8:	eb05 0e8e 	add.w	lr, r5, lr, lsl #2
   e65ac:	1918      	adds	r0, r3, r4
   e65ae:	eb09 0104 	add.w	r1, r9, r4
   e65b2:	2f00      	cmp	r7, #0
   e65b4:	f2c0 81c0 	blt.w	e6938 <__kernel_rem_pio2f+0x404>
   e65b8:	eddf 7ac1 	vldr	s15, [pc, #772]	; e68c0 <__kernel_rem_pio2f+0x38c>
   e65bc:	464b      	mov	r3, r9
   e65be:	4602      	mov	r2, r0
   e65c0:	ecf3 6a01 	vldmia	r3!, {s13}
   e65c4:	ed32 7a01 	vldmdb	r2!, {s14}
   e65c8:	428b      	cmp	r3, r1
   e65ca:	eee6 7a87 	vfma.f32	s15, s13, s14
   e65ce:	d1f7      	bne.n	e65c0 <__kernel_rem_pio2f+0x8c>
   e65d0:	ece5 7a01 	vstmia	r5!, {s15}
   e65d4:	4575      	cmp	r5, lr
   e65d6:	f100 0004 	add.w	r0, r0, #4
   e65da:	d1ea      	bne.n	e65b2 <__kernel_rem_pio2f+0x7e>
   e65dc:	f106 4380 	add.w	r3, r6, #1073741824	; 0x40000000
   e65e0:	3b02      	subs	r3, #2
   e65e2:	009b      	lsls	r3, r3, #2
   e65e4:	aa06      	add	r2, sp, #24
   e65e6:	f103 0804 	add.w	r8, r3, #4
   e65ea:	eddf 8ab1 	vldr	s17, [pc, #708]	; e68b0 <__kernel_rem_pio2f+0x37c>
   e65ee:	ed9f 8ab1 	vldr	s16, [pc, #708]	; e68b4 <__kernel_rem_pio2f+0x380>
   e65f2:	f8cd b010 	str.w	fp, [sp, #16]
   e65f6:	4413      	add	r3, r2
   e65f8:	444c      	add	r4, r9
   e65fa:	4490      	add	r8, r2
   e65fc:	9303      	str	r3, [sp, #12]
   e65fe:	4635      	mov	r5, r6
   e6600:	ab56      	add	r3, sp, #344	; 0x158
   e6602:	eb03 0385 	add.w	r3, r3, r5, lsl #2
   e6606:	2d00      	cmp	r5, #0
   e6608:	ed13 0a14 	vldr	s0, [r3, #-80]	; 0xffffffb0
   e660c:	dd19      	ble.n	e6642 <__kernel_rem_pio2f+0x10e>
   e660e:	a942      	add	r1, sp, #264	; 0x108
   e6610:	eb01 0385 	add.w	r3, r1, r5, lsl #2
   e6614:	aa05      	add	r2, sp, #20
   e6616:	ee60 7a28 	vmul.f32	s15, s0, s17
   e661a:	eeb0 7a40 	vmov.f32	s14, s0
   e661e:	eefd 7ae7 	vcvt.s32.f32	s15, s15
   e6622:	ed73 6a01 	vldmdb	r3!, {s13}
   e6626:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   e662a:	428b      	cmp	r3, r1
   e662c:	eea7 7ac8 	vfms.f32	s14, s15, s16
   e6630:	ee37 0aa6 	vadd.f32	s0, s15, s13
   e6634:	eebd 7ac7 	vcvt.s32.f32	s14, s14
   e6638:	ee17 0a10 	vmov	r0, s14
   e663c:	f842 0f04 	str.w	r0, [r2, #4]!
   e6640:	d1e9      	bne.n	e6616 <__kernel_rem_pio2f+0xe2>
   e6642:	4650      	mov	r0, sl
   e6644:	f000 faf6 	bl	e6c34 <scalbnf>
   e6648:	eeb0 9a40 	vmov.f32	s18, s0
   e664c:	eeb4 0a00 	vmov.f32	s0, #64	; 0x3e000000  0.125
   e6650:	ee29 0a00 	vmul.f32	s0, s18, s0
   e6654:	f7fe fee4 	bl	e5420 <floorf>
   e6658:	eef2 7a00 	vmov.f32	s15, #32	; 0x41000000  8.0
   e665c:	eea0 9a67 	vfms.f32	s18, s0, s15
   e6660:	f1ba 0f00 	cmp.w	sl, #0
   e6664:	eefd 7ac9 	vcvt.s32.f32	s15, s18
   e6668:	ee17 ba90 	vmov	fp, s15
   e666c:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   e6670:	ee39 9a67 	vsub.f32	s18, s18, s15
   e6674:	f340 8145 	ble.w	e6902 <__kernel_rem_pio2f+0x3ce>
   e6678:	f105 3eff 	add.w	lr, r5, #4294967295	; 0xffffffff
   e667c:	ab06      	add	r3, sp, #24
   e667e:	f1ca 0208 	rsb	r2, sl, #8
   e6682:	f853 302e 	ldr.w	r3, [r3, lr, lsl #2]
   e6686:	fa43 f002 	asr.w	r0, r3, r2
   e668a:	fa00 f202 	lsl.w	r2, r0, r2
   e668e:	a906      	add	r1, sp, #24
   e6690:	1a9b      	subs	r3, r3, r2
   e6692:	f1ca 0207 	rsb	r2, sl, #7
   e6696:	f841 302e 	str.w	r3, [r1, lr, lsl #2]
   e669a:	4483      	add	fp, r0
   e669c:	fa43 f102 	asr.w	r1, r3, r2
   e66a0:	2900      	cmp	r1, #0
   e66a2:	dd37      	ble.n	e6714 <__kernel_rem_pio2f+0x1e0>
   e66a4:	2d00      	cmp	r5, #0
   e66a6:	f10b 0b01 	add.w	fp, fp, #1
   e66aa:	f340 8228 	ble.w	e6afe <__kernel_rem_pio2f+0x5ca>
   e66ae:	2200      	movs	r2, #0
   e66b0:	4610      	mov	r0, r2
   e66b2:	f10d 0e14 	add.w	lr, sp, #20
   e66b6:	468c      	mov	ip, r1
   e66b8:	e008      	b.n	e66cc <__kernel_rem_pio2f+0x198>
   e66ba:	f5c3 7180 	rsb	r1, r3, #256	; 0x100
   e66be:	b113      	cbz	r3, e66c6 <__kernel_rem_pio2f+0x192>
   e66c0:	f8ce 1000 	str.w	r1, [lr]
   e66c4:	2001      	movs	r0, #1
   e66c6:	3201      	adds	r2, #1
   e66c8:	4295      	cmp	r5, r2
   e66ca:	dd0c      	ble.n	e66e6 <__kernel_rem_pio2f+0x1b2>
   e66cc:	f85e 3f04 	ldr.w	r3, [lr, #4]!
   e66d0:	2800      	cmp	r0, #0
   e66d2:	d0f2      	beq.n	e66ba <__kernel_rem_pio2f+0x186>
   e66d4:	3201      	adds	r2, #1
   e66d6:	f1c3 03ff 	rsb	r3, r3, #255	; 0xff
   e66da:	4295      	cmp	r5, r2
   e66dc:	f8ce 3000 	str.w	r3, [lr]
   e66e0:	f04f 0001 	mov.w	r0, #1
   e66e4:	dcf2      	bgt.n	e66cc <__kernel_rem_pio2f+0x198>
   e66e6:	4661      	mov	r1, ip
   e66e8:	f1ba 0f00 	cmp.w	sl, #0
   e66ec:	dd10      	ble.n	e6710 <__kernel_rem_pio2f+0x1dc>
   e66ee:	f1ba 0f01 	cmp.w	sl, #1
   e66f2:	f000 810d 	beq.w	e6910 <__kernel_rem_pio2f+0x3dc>
   e66f6:	f1ba 0f02 	cmp.w	sl, #2
   e66fa:	d109      	bne.n	e6710 <__kernel_rem_pio2f+0x1dc>
   e66fc:	1e6a      	subs	r2, r5, #1
   e66fe:	ab06      	add	r3, sp, #24
   e6700:	f10d 0e18 	add.w	lr, sp, #24
   e6704:	f853 3022 	ldr.w	r3, [r3, r2, lsl #2]
   e6708:	f003 033f 	and.w	r3, r3, #63	; 0x3f
   e670c:	f84e 3022 	str.w	r3, [lr, r2, lsl #2]
   e6710:	2902      	cmp	r1, #2
   e6712:	d05c      	beq.n	e67ce <__kernel_rem_pio2f+0x29a>
   e6714:	eeb5 9a40 	vcmp.f32	s18, #0.0
   e6718:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e671c:	d169      	bne.n	e67f2 <__kernel_rem_pio2f+0x2be>
   e671e:	f105 3eff 	add.w	lr, r5, #4294967295	; 0xffffffff
   e6722:	4576      	cmp	r6, lr
   e6724:	dc0f      	bgt.n	e6746 <__kernel_rem_pio2f+0x212>
   e6726:	f105 4280 	add.w	r2, r5, #1073741824	; 0x40000000
   e672a:	3a01      	subs	r2, #1
   e672c:	ab06      	add	r3, sp, #24
   e672e:	eb03 0282 	add.w	r2, r3, r2, lsl #2
   e6732:	2000      	movs	r0, #0
   e6734:	f852 3904 	ldr.w	r3, [r2], #-4
   e6738:	4542      	cmp	r2, r8
   e673a:	ea40 0003 	orr.w	r0, r0, r3
   e673e:	d1f9      	bne.n	e6734 <__kernel_rem_pio2f+0x200>
   e6740:	2800      	cmp	r0, #0
   e6742:	f040 8110 	bne.w	e6966 <__kernel_rem_pio2f+0x432>
   e6746:	1e73      	subs	r3, r6, #1
   e6748:	aa06      	add	r2, sp, #24
   e674a:	f852 3023 	ldr.w	r3, [r2, r3, lsl #2]
   e674e:	2b00      	cmp	r3, #0
   e6750:	f040 81d2 	bne.w	e6af8 <__kernel_rem_pio2f+0x5c4>
   e6754:	9b03      	ldr	r3, [sp, #12]
   e6756:	f04f 0e01 	mov.w	lr, #1
   e675a:	f853 2904 	ldr.w	r2, [r3], #-4
   e675e:	f10e 0e01 	add.w	lr, lr, #1
   e6762:	2a00      	cmp	r2, #0
   e6764:	d0f9      	beq.n	e675a <__kernel_rem_pio2f+0x226>
   e6766:	44ae      	add	lr, r5
   e6768:	1c6b      	adds	r3, r5, #1
   e676a:	4573      	cmp	r3, lr
   e676c:	dc2d      	bgt.n	e67ca <__kernel_rem_pio2f+0x296>
   e676e:	9a02      	ldr	r2, [sp, #8]
   e6770:	1898      	adds	r0, r3, r2
   e6772:	9a01      	ldr	r2, [sp, #4]
   e6774:	f100 4080 	add.w	r0, r0, #1073741824	; 0x40000000
   e6778:	1951      	adds	r1, r2, r5
   e677a:	eb0e 0c02 	add.w	ip, lr, r2
   e677e:	9a65      	ldr	r2, [sp, #404]	; 0x194
   e6780:	3801      	subs	r0, #1
   e6782:	eb02 0080 	add.w	r0, r2, r0, lsl #2
   e6786:	aa1a      	add	r2, sp, #104	; 0x68
   e6788:	eb02 0181 	add.w	r1, r2, r1, lsl #2
   e678c:	eb02 0c8c 	add.w	ip, r2, ip, lsl #2
   e6790:	aa42      	add	r2, sp, #264	; 0x108
   e6792:	eb02 0583 	add.w	r5, r2, r3, lsl #2
   e6796:	f850 3f04 	ldr.w	r3, [r0, #4]!
   e679a:	ee07 3a90 	vmov	s15, r3
   e679e:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   e67a2:	2f00      	cmp	r7, #0
   e67a4:	ece1 7a01 	vstmia	r1!, {s15}
   e67a8:	eddf 7a45 	vldr	s15, [pc, #276]	; e68c0 <__kernel_rem_pio2f+0x38c>
   e67ac:	db09      	blt.n	e67c2 <__kernel_rem_pio2f+0x28e>
   e67ae:	464b      	mov	r3, r9
   e67b0:	460a      	mov	r2, r1
   e67b2:	ecf3 6a01 	vldmia	r3!, {s13}
   e67b6:	ed32 7a01 	vldmdb	r2!, {s14}
   e67ba:	42a3      	cmp	r3, r4
   e67bc:	eee6 7a87 	vfma.f32	s15, s13, s14
   e67c0:	d1f7      	bne.n	e67b2 <__kernel_rem_pio2f+0x27e>
   e67c2:	4561      	cmp	r1, ip
   e67c4:	ece5 7a01 	vstmia	r5!, {s15}
   e67c8:	d1e5      	bne.n	e6796 <__kernel_rem_pio2f+0x262>
   e67ca:	4675      	mov	r5, lr
   e67cc:	e718      	b.n	e6600 <__kernel_rem_pio2f+0xcc>
   e67ce:	eeb7 0a00 	vmov.f32	s0, #112	; 0x3f800000  1.0
   e67d2:	ee30 9a49 	vsub.f32	s18, s0, s18
   e67d6:	2800      	cmp	r0, #0
   e67d8:	d09c      	beq.n	e6714 <__kernel_rem_pio2f+0x1e0>
   e67da:	4650      	mov	r0, sl
   e67dc:	9105      	str	r1, [sp, #20]
   e67de:	f000 fa29 	bl	e6c34 <scalbnf>
   e67e2:	ee39 9a40 	vsub.f32	s18, s18, s0
   e67e6:	9905      	ldr	r1, [sp, #20]
   e67e8:	eeb5 9a40 	vcmp.f32	s18, #0.0
   e67ec:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e67f0:	d095      	beq.n	e671e <__kernel_rem_pio2f+0x1ea>
   e67f2:	eeb0 0a49 	vmov.f32	s0, s18
   e67f6:	f1ca 0000 	rsb	r0, sl, #0
   e67fa:	ee09 ba90 	vmov	s19, fp
   e67fe:	4688      	mov	r8, r1
   e6800:	f8dd b010 	ldr.w	fp, [sp, #16]
   e6804:	f000 fa16 	bl	e6c34 <scalbnf>
   e6808:	ed9f 7a2a 	vldr	s14, [pc, #168]	; e68b4 <__kernel_rem_pio2f+0x380>
   e680c:	eeb4 0ac7 	vcmpe.f32	s0, s14
   e6810:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e6814:	f2c0 817e 	blt.w	e6b14 <__kernel_rem_pio2f+0x5e0>
   e6818:	eddf 7a25 	vldr	s15, [pc, #148]	; e68b0 <__kernel_rem_pio2f+0x37c>
   e681c:	ee60 7a27 	vmul.f32	s15, s0, s15
   e6820:	a906      	add	r1, sp, #24
   e6822:	eefd 7ae7 	vcvt.s32.f32	s15, s15
   e6826:	1c6b      	adds	r3, r5, #1
   e6828:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   e682c:	f10a 0a08 	add.w	sl, sl, #8
   e6830:	eea7 0ac7 	vfms.f32	s0, s15, s14
   e6834:	eefd 7ae7 	vcvt.s32.f32	s15, s15
   e6838:	eebd 0ac0 	vcvt.s32.f32	s0, s0
   e683c:	ee10 2a10 	vmov	r2, s0
   e6840:	f841 2025 	str.w	r2, [r1, r5, lsl #2]
   e6844:	ee17 2a90 	vmov	r2, s15
   e6848:	f841 2023 	str.w	r2, [r1, r3, lsl #2]
   e684c:	4650      	mov	r0, sl
   e684e:	eeb7 0a00 	vmov.f32	s0, #112	; 0x3f800000  1.0
   e6852:	9301      	str	r3, [sp, #4]
   e6854:	f000 f9ee 	bl	e6c34 <scalbnf>
   e6858:	9b01      	ldr	r3, [sp, #4]
   e685a:	2b00      	cmp	r3, #0
   e685c:	f2c0 8166 	blt.w	e6b2c <__kernel_rem_pio2f+0x5f8>
   e6860:	009f      	lsls	r7, r3, #2
   e6862:	ac42      	add	r4, sp, #264	; 0x108
   e6864:	aa06      	add	r2, sp, #24
   e6866:	1d38      	adds	r0, r7, #4
   e6868:	eb04 0e07 	add.w	lr, r4, r7
   e686c:	ed9f 7a10 	vldr	s14, [pc, #64]	; e68b0 <__kernel_rem_pio2f+0x37c>
   e6870:	4410      	add	r0, r2
   e6872:	f10e 0204 	add.w	r2, lr, #4
   e6876:	ed70 7a01 	vldmdb	r0!, {s15}
   e687a:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   e687e:	ee67 7a80 	vmul.f32	s15, s15, s0
   e6882:	ee20 0a07 	vmul.f32	s0, s0, s14
   e6886:	ed62 7a01 	vstmdb	r2!, {s15}
   e688a:	42a2      	cmp	r2, r4
   e688c:	d1f3      	bne.n	e6876 <__kernel_rem_pio2f+0x342>
   e688e:	f50d 7c82 	add.w	ip, sp, #260	; 0x104
   e6892:	2500      	movs	r5, #0
   e6894:	2e00      	cmp	r6, #0
   e6896:	f2c0 8121 	blt.w	e6adc <__kernel_rem_pio2f+0x5a8>
   e689a:	4807      	ldr	r0, [pc, #28]	; (e68b8 <__kernel_rem_pio2f+0x384>)
   e689c:	ed9f 7a07 	vldr	s14, [pc, #28]	; e68bc <__kernel_rem_pio2f+0x388>
   e68a0:	eddf 7a07 	vldr	s15, [pc, #28]	; e68c0 <__kernel_rem_pio2f+0x38c>
   e68a4:	4671      	mov	r1, lr
   e68a6:	2200      	movs	r2, #0
   e68a8:	e011      	b.n	e68ce <__kernel_rem_pio2f+0x39a>
   e68aa:	bf00      	nop
   e68ac:	000eb940 	.word	0x000eb940
   e68b0:	3b800000 	.word	0x3b800000
   e68b4:	43800000 	.word	0x43800000
   e68b8:	000eb94c 	.word	0x000eb94c
   e68bc:	3fc90000 	.word	0x3fc90000
   e68c0:	00000000 	.word	0x00000000
   e68c4:	4295      	cmp	r5, r2
   e68c6:	db09      	blt.n	e68dc <__kernel_rem_pio2f+0x3a8>
   e68c8:	3004      	adds	r0, #4
   e68ca:	ed90 7a00 	vldr	s14, [r0]
   e68ce:	ecf1 6a01 	vldmia	r1!, {s13}
   e68d2:	3201      	adds	r2, #1
   e68d4:	4296      	cmp	r6, r2
   e68d6:	eee6 7a87 	vfma.f32	s15, s13, s14
   e68da:	daf3      	bge.n	e68c4 <__kernel_rem_pio2f+0x390>
   e68dc:	f1ae 0e04 	sub.w	lr, lr, #4
   e68e0:	aa56      	add	r2, sp, #344	; 0x158
   e68e2:	eb02 0285 	add.w	r2, r2, r5, lsl #2
   e68e6:	45f4      	cmp	ip, lr
   e68e8:	ed42 7a28 	vstr	s15, [r2, #-160]	; 0xffffff60
   e68ec:	f105 0501 	add.w	r5, r5, #1
   e68f0:	d1d0      	bne.n	e6894 <__kernel_rem_pio2f+0x360>
   e68f2:	9a64      	ldr	r2, [sp, #400]	; 0x190
   e68f4:	2a03      	cmp	r2, #3
   e68f6:	f200 80ae 	bhi.w	e6a56 <__kernel_rem_pio2f+0x522>
   e68fa:	e8df f002 	tbb	[pc, r2]
   e68fe:	b5dc      	.short	0xb5dc
   e6900:	50b5      	.short	0x50b5
   e6902:	d110      	bne.n	e6926 <__kernel_rem_pio2f+0x3f2>
   e6904:	1e6b      	subs	r3, r5, #1
   e6906:	aa06      	add	r2, sp, #24
   e6908:	f852 1023 	ldr.w	r1, [r2, r3, lsl #2]
   e690c:	1209      	asrs	r1, r1, #8
   e690e:	e6c7      	b.n	e66a0 <__kernel_rem_pio2f+0x16c>
   e6910:	1e6a      	subs	r2, r5, #1
   e6912:	ab06      	add	r3, sp, #24
   e6914:	f10d 0e18 	add.w	lr, sp, #24
   e6918:	f853 3022 	ldr.w	r3, [r3, r2, lsl #2]
   e691c:	f003 037f 	and.w	r3, r3, #127	; 0x7f
   e6920:	f84e 3022 	str.w	r3, [lr, r2, lsl #2]
   e6924:	e6f4      	b.n	e6710 <__kernel_rem_pio2f+0x1dc>
   e6926:	eef6 7a00 	vmov.f32	s15, #96	; 0x3f000000  0.5
   e692a:	eeb4 9ae7 	vcmpe.f32	s18, s15
   e692e:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e6932:	da0b      	bge.n	e694c <__kernel_rem_pio2f+0x418>
   e6934:	2100      	movs	r1, #0
   e6936:	e6ed      	b.n	e6714 <__kernel_rem_pio2f+0x1e0>
   e6938:	ed5f 7a1f 	vldr	s15, [pc, #-124]	; e68c0 <__kernel_rem_pio2f+0x38c>
   e693c:	ece5 7a01 	vstmia	r5!, {s15}
   e6940:	4575      	cmp	r5, lr
   e6942:	f100 0004 	add.w	r0, r0, #4
   e6946:	f47f ae34 	bne.w	e65b2 <__kernel_rem_pio2f+0x7e>
   e694a:	e647      	b.n	e65dc <__kernel_rem_pio2f+0xa8>
   e694c:	2d00      	cmp	r5, #0
   e694e:	f10b 0b01 	add.w	fp, fp, #1
   e6952:	bfc8      	it	gt
   e6954:	2102      	movgt	r1, #2
   e6956:	f73f aeaa 	bgt.w	e66ae <__kernel_rem_pio2f+0x17a>
   e695a:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   e695e:	ee37 9ac9 	vsub.f32	s18, s15, s18
   e6962:	2102      	movs	r1, #2
   e6964:	e6d6      	b.n	e6714 <__kernel_rem_pio2f+0x1e0>
   e6966:	aa06      	add	r2, sp, #24
   e6968:	ee09 ba90 	vmov	s19, fp
   e696c:	f852 202e 	ldr.w	r2, [r2, lr, lsl #2]
   e6970:	f8dd b010 	ldr.w	fp, [sp, #16]
   e6974:	4673      	mov	r3, lr
   e6976:	4688      	mov	r8, r1
   e6978:	f1aa 0a08 	sub.w	sl, sl, #8
   e697c:	2a00      	cmp	r2, #0
   e697e:	f47f af65 	bne.w	e684c <__kernel_rem_pio2f+0x318>
   e6982:	f10e 4280 	add.w	r2, lr, #1073741824	; 0x40000000
   e6986:	3a01      	subs	r2, #1
   e6988:	a906      	add	r1, sp, #24
   e698a:	eb01 0282 	add.w	r2, r1, r2, lsl #2
   e698e:	f852 1904 	ldr.w	r1, [r2], #-4
   e6992:	3b01      	subs	r3, #1
   e6994:	f1aa 0a08 	sub.w	sl, sl, #8
   e6998:	2900      	cmp	r1, #0
   e699a:	d0f8      	beq.n	e698e <__kernel_rem_pio2f+0x45a>
   e699c:	e756      	b.n	e684c <__kernel_rem_pio2f+0x318>
   e699e:	2b00      	cmp	r3, #0
   e69a0:	f340 80c1 	ble.w	e6b26 <__kernel_rem_pio2f+0x5f2>
   e69a4:	f103 4280 	add.w	r2, r3, #1073741824	; 0x40000000
   e69a8:	3a01      	subs	r2, #1
   e69aa:	0090      	lsls	r0, r2, #2
   e69ac:	a956      	add	r1, sp, #344	; 0x158
   e69ae:	19cd      	adds	r5, r1, r7
   e69b0:	1d04      	adds	r4, r0, #4
   e69b2:	a92e      	add	r1, sp, #184	; 0xb8
   e69b4:	3008      	adds	r0, #8
   e69b6:	ed15 7a28 	vldr	s14, [r5, #-160]	; 0xffffff60
   e69ba:	440c      	add	r4, r1
   e69bc:	4408      	add	r0, r1
   e69be:	ad2f      	add	r5, sp, #188	; 0xbc
   e69c0:	ed74 7a01 	vldmdb	r4!, {s15}
   e69c4:	ee77 6a87 	vadd.f32	s13, s15, s14
   e69c8:	ee77 7ae6 	vsub.f32	s15, s15, s13
   e69cc:	ee77 7a87 	vadd.f32	s15, s15, s14
   e69d0:	eeb0 7a66 	vmov.f32	s14, s13
   e69d4:	ed60 7a01 	vstmdb	r0!, {s15}
   e69d8:	42a8      	cmp	r0, r5
   e69da:	edc4 6a00 	vstr	s13, [r4]
   e69de:	d1ef      	bne.n	e69c0 <__kernel_rem_pio2f+0x48c>
   e69e0:	2b01      	cmp	r3, #1
   e69e2:	f340 80a0 	ble.w	e6b26 <__kernel_rem_pio2f+0x5f2>
   e69e6:	0092      	lsls	r2, r2, #2
   e69e8:	ab56      	add	r3, sp, #344	; 0x158
   e69ea:	441f      	add	r7, r3
   e69ec:	f102 0008 	add.w	r0, r2, #8
   e69f0:	ab2e      	add	r3, sp, #184	; 0xb8
   e69f2:	4418      	add	r0, r3
   e69f4:	3204      	adds	r2, #4
   e69f6:	ed17 7a28 	vldr	s14, [r7, #-160]	; 0xffffff60
   e69fa:	4413      	add	r3, r2
   e69fc:	ac30      	add	r4, sp, #192	; 0xc0
   e69fe:	4602      	mov	r2, r0
   e6a00:	ed73 7a01 	vldmdb	r3!, {s15}
   e6a04:	ee77 6a27 	vadd.f32	s13, s14, s15
   e6a08:	ee77 7ae6 	vsub.f32	s15, s15, s13
   e6a0c:	ee77 7a87 	vadd.f32	s15, s15, s14
   e6a10:	eeb0 7a66 	vmov.f32	s14, s13
   e6a14:	ed62 7a01 	vstmdb	r2!, {s15}
   e6a18:	4294      	cmp	r4, r2
   e6a1a:	edc3 6a00 	vstr	s13, [r3]
   e6a1e:	d1ef      	bne.n	e6a00 <__kernel_rem_pio2f+0x4cc>
   e6a20:	ed5f 7a59 	vldr	s15, [pc, #-356]	; e68c0 <__kernel_rem_pio2f+0x38c>
   e6a24:	ed30 7a01 	vldmdb	r0!, {s14}
   e6a28:	4284      	cmp	r4, r0
   e6a2a:	ee77 7a87 	vadd.f32	s15, s15, s14
   e6a2e:	d1f9      	bne.n	e6a24 <__kernel_rem_pio2f+0x4f0>
   e6a30:	4643      	mov	r3, r8
   e6a32:	2b00      	cmp	r3, #0
   e6a34:	d065      	beq.n	e6b02 <__kernel_rem_pio2f+0x5ce>
   e6a36:	eddd 6a2e 	vldr	s13, [sp, #184]	; 0xb8
   e6a3a:	ed9d 7a2f 	vldr	s14, [sp, #188]	; 0xbc
   e6a3e:	eef1 7a67 	vneg.f32	s15, s15
   e6a42:	eef1 6a66 	vneg.f32	s13, s13
   e6a46:	eeb1 7a47 	vneg.f32	s14, s14
   e6a4a:	edcb 7a02 	vstr	s15, [fp, #8]
   e6a4e:	edcb 6a00 	vstr	s13, [fp]
   e6a52:	ed8b 7a01 	vstr	s14, [fp, #4]
   e6a56:	ee19 3a90 	vmov	r3, s19
   e6a5a:	f003 0007 	and.w	r0, r3, #7
   e6a5e:	b057      	add	sp, #348	; 0x15c
   e6a60:	ecbd 8b04 	vpop	{d8-d9}
   e6a64:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e6a68:	1d3a      	adds	r2, r7, #4
   e6a6a:	a82e      	add	r0, sp, #184	; 0xb8
   e6a6c:	ed5f 7a6c 	vldr	s15, [pc, #-432]	; e68c0 <__kernel_rem_pio2f+0x38c>
   e6a70:	4402      	add	r2, r0
   e6a72:	ed32 7a01 	vldmdb	r2!, {s14}
   e6a76:	4282      	cmp	r2, r0
   e6a78:	ee77 7a87 	vadd.f32	s15, s15, s14
   e6a7c:	d1f9      	bne.n	e6a72 <__kernel_rem_pio2f+0x53e>
   e6a7e:	4642      	mov	r2, r8
   e6a80:	b37a      	cbz	r2, e6ae2 <__kernel_rem_pio2f+0x5ae>
   e6a82:	eddd 6a2e 	vldr	s13, [sp, #184]	; 0xb8
   e6a86:	eeb1 7a67 	vneg.f32	s14, s15
   e6a8a:	2b00      	cmp	r3, #0
   e6a8c:	ee76 7ae7 	vsub.f32	s15, s13, s15
   e6a90:	ed8b 7a00 	vstr	s14, [fp]
   e6a94:	dd0a      	ble.n	e6aac <__kernel_rem_pio2f+0x578>
   e6a96:	a82f      	add	r0, sp, #188	; 0xbc
   e6a98:	2201      	movs	r2, #1
   e6a9a:	ecb0 7a01 	vldmia	r0!, {s14}
   e6a9e:	3201      	adds	r2, #1
   e6aa0:	4293      	cmp	r3, r2
   e6aa2:	ee77 7a87 	vadd.f32	s15, s15, s14
   e6aa6:	daf8      	bge.n	e6a9a <__kernel_rem_pio2f+0x566>
   e6aa8:	4643      	mov	r3, r8
   e6aaa:	b10b      	cbz	r3, e6ab0 <__kernel_rem_pio2f+0x57c>
   e6aac:	eef1 7a67 	vneg.f32	s15, s15
   e6ab0:	edcb 7a01 	vstr	s15, [fp, #4]
   e6ab4:	e7cf      	b.n	e6a56 <__kernel_rem_pio2f+0x522>
   e6ab6:	aa56      	add	r2, sp, #344	; 0x158
   e6ab8:	443a      	add	r2, r7
   e6aba:	ed5f 7a7f 	vldr	s15, [pc, #-508]	; e68c0 <__kernel_rem_pio2f+0x38c>
   e6abe:	3a9c      	subs	r2, #156	; 0x9c
   e6ac0:	ed32 7a01 	vldmdb	r2!, {s14}
   e6ac4:	3b01      	subs	r3, #1
   e6ac6:	1c59      	adds	r1, r3, #1
   e6ac8:	ee77 7a87 	vadd.f32	s15, s15, s14
   e6acc:	d1f8      	bne.n	e6ac0 <__kernel_rem_pio2f+0x58c>
   e6ace:	4643      	mov	r3, r8
   e6ad0:	b10b      	cbz	r3, e6ad6 <__kernel_rem_pio2f+0x5a2>
   e6ad2:	eef1 7a67 	vneg.f32	s15, s15
   e6ad6:	edcb 7a00 	vstr	s15, [fp]
   e6ada:	e7bc      	b.n	e6a56 <__kernel_rem_pio2f+0x522>
   e6adc:	ed5f 7a88 	vldr	s15, [pc, #-544]	; e68c0 <__kernel_rem_pio2f+0x38c>
   e6ae0:	e6fc      	b.n	e68dc <__kernel_rem_pio2f+0x3a8>
   e6ae2:	ed9d 7a2e 	vldr	s14, [sp, #184]	; 0xb8
   e6ae6:	edcb 7a00 	vstr	s15, [fp]
   e6aea:	2b00      	cmp	r3, #0
   e6aec:	ee77 7a67 	vsub.f32	s15, s14, s15
   e6af0:	dcd1      	bgt.n	e6a96 <__kernel_rem_pio2f+0x562>
   e6af2:	edcb 7a01 	vstr	s15, [fp, #4]
   e6af6:	e7ae      	b.n	e6a56 <__kernel_rem_pio2f+0x522>
   e6af8:	f04f 0e01 	mov.w	lr, #1
   e6afc:	e633      	b.n	e6766 <__kernel_rem_pio2f+0x232>
   e6afe:	2000      	movs	r0, #0
   e6b00:	e5f2      	b.n	e66e8 <__kernel_rem_pio2f+0x1b4>
   e6b02:	9a2e      	ldr	r2, [sp, #184]	; 0xb8
   e6b04:	9b2f      	ldr	r3, [sp, #188]	; 0xbc
   e6b06:	edcb 7a02 	vstr	s15, [fp, #8]
   e6b0a:	f8cb 2000 	str.w	r2, [fp]
   e6b0e:	f8cb 3004 	str.w	r3, [fp, #4]
   e6b12:	e7a0      	b.n	e6a56 <__kernel_rem_pio2f+0x522>
   e6b14:	eebd 0ac0 	vcvt.s32.f32	s0, s0
   e6b18:	a906      	add	r1, sp, #24
   e6b1a:	ee10 2a10 	vmov	r2, s0
   e6b1e:	462b      	mov	r3, r5
   e6b20:	f841 2025 	str.w	r2, [r1, r5, lsl #2]
   e6b24:	e692      	b.n	e684c <__kernel_rem_pio2f+0x318>
   e6b26:	ed5f 7a9a 	vldr	s15, [pc, #-616]	; e68c0 <__kernel_rem_pio2f+0x38c>
   e6b2a:	e781      	b.n	e6a30 <__kernel_rem_pio2f+0x4fc>
   e6b2c:	9a64      	ldr	r2, [sp, #400]	; 0x190
   e6b2e:	2a03      	cmp	r2, #3
   e6b30:	d891      	bhi.n	e6a56 <__kernel_rem_pio2f+0x522>
   e6b32:	a101      	add	r1, pc, #4	; (adr r1, e6b38 <__kernel_rem_pio2f+0x604>)
   e6b34:	f851 f022 	ldr.w	pc, [r1, r2, lsl #2]
   e6b38:	000e6b55 	.word	0x000e6b55
   e6b3c:	000e6b4f 	.word	0x000e6b4f
   e6b40:	000e6b4f 	.word	0x000e6b4f
   e6b44:	000e6b27 	.word	0x000e6b27
   e6b48:	9b01      	ldr	r3, [sp, #4]
   e6b4a:	009c      	lsls	r4, r3, #2
   e6b4c:	e546      	b.n	e65dc <__kernel_rem_pio2f+0xa8>
   e6b4e:	ed5f 7aa4 	vldr	s15, [pc, #-656]	; e68c0 <__kernel_rem_pio2f+0x38c>
   e6b52:	e794      	b.n	e6a7e <__kernel_rem_pio2f+0x54a>
   e6b54:	ed5f 7aa6 	vldr	s15, [pc, #-664]	; e68c0 <__kernel_rem_pio2f+0x38c>
   e6b58:	e7b9      	b.n	e6ace <__kernel_rem_pio2f+0x59a>
   e6b5a:	bf00      	nop

000e6b5c <__kernel_sinf>:
   e6b5c:	ee10 3a10 	vmov	r3, s0
   e6b60:	f023 4300 	bic.w	r3, r3, #2147483648	; 0x80000000
   e6b64:	f1b3 5f48 	cmp.w	r3, #838860800	; 0x32000000
   e6b68:	da04      	bge.n	e6b74 <__kernel_sinf+0x18>
   e6b6a:	eefd 7ac0 	vcvt.s32.f32	s15, s0
   e6b6e:	ee17 3a90 	vmov	r3, s15
   e6b72:	b323      	cbz	r3, e6bbe <__kernel_sinf+0x62>
   e6b74:	ee60 7a00 	vmul.f32	s15, s0, s0
   e6b78:	ed9f 5a15 	vldr	s10, [pc, #84]	; e6bd0 <__kernel_sinf+0x74>
   e6b7c:	eddf 5a15 	vldr	s11, [pc, #84]	; e6bd4 <__kernel_sinf+0x78>
   e6b80:	ed9f 6a15 	vldr	s12, [pc, #84]	; e6bd8 <__kernel_sinf+0x7c>
   e6b84:	eddf 6a15 	vldr	s13, [pc, #84]	; e6bdc <__kernel_sinf+0x80>
   e6b88:	ed9f 7a15 	vldr	s14, [pc, #84]	; e6be0 <__kernel_sinf+0x84>
   e6b8c:	eee7 5a85 	vfma.f32	s11, s15, s10
   e6b90:	ee20 5a27 	vmul.f32	s10, s0, s15
   e6b94:	eea7 6aa5 	vfma.f32	s12, s15, s11
   e6b98:	eee7 6a86 	vfma.f32	s13, s15, s12
   e6b9c:	eea7 7aa6 	vfma.f32	s14, s15, s13
   e6ba0:	b170      	cbz	r0, e6bc0 <__kernel_sinf+0x64>
   e6ba2:	ee27 7a45 	vnmul.f32	s14, s14, s10
   e6ba6:	eef6 6a00 	vmov.f32	s13, #96	; 0x3f000000  0.5
   e6baa:	eea0 7aa6 	vfma.f32	s14, s1, s13
   e6bae:	eddf 6a0d 	vldr	s13, [pc, #52]	; e6be4 <__kernel_sinf+0x88>
   e6bb2:	eed7 0a87 	vfnms.f32	s1, s15, s14
   e6bb6:	eee5 0a26 	vfma.f32	s1, s10, s13
   e6bba:	ee30 0a60 	vsub.f32	s0, s0, s1
   e6bbe:	4770      	bx	lr
   e6bc0:	eddf 6a09 	vldr	s13, [pc, #36]	; e6be8 <__kernel_sinf+0x8c>
   e6bc4:	eee7 6a87 	vfma.f32	s13, s15, s14
   e6bc8:	eea5 0a26 	vfma.f32	s0, s10, s13
   e6bcc:	4770      	bx	lr
   e6bce:	bf00      	nop
   e6bd0:	2f2ec9d3 	.word	0x2f2ec9d3
   e6bd4:	b2d72f34 	.word	0xb2d72f34
   e6bd8:	3638ef1b 	.word	0x3638ef1b
   e6bdc:	b9500d01 	.word	0xb9500d01
   e6be0:	3c088889 	.word	0x3c088889
   e6be4:	3e2aaaab 	.word	0x3e2aaaab
   e6be8:	be2aaaab 	.word	0xbe2aaaab

000e6bec <finite>:
   e6bec:	ee10 3a90 	vmov	r3, s1
   e6bf0:	f043 4000 	orr.w	r0, r3, #2147483648	; 0x80000000
   e6bf4:	f500 1080 	add.w	r0, r0, #1048576	; 0x100000
   e6bf8:	0fc0      	lsrs	r0, r0, #31
   e6bfa:	4770      	bx	lr

000e6bfc <matherr>:
   e6bfc:	2000      	movs	r0, #0
   e6bfe:	4770      	bx	lr

000e6c00 <nan>:
   e6c00:	ed9f 0b01 	vldr	d0, [pc, #4]	; e6c08 <nan+0x8>
   e6c04:	4770      	bx	lr
   e6c06:	bf00      	nop
   e6c08:	00000000 	.word	0x00000000
   e6c0c:	7ff80000 	.word	0x7ff80000

000e6c10 <fabsf>:
   e6c10:	ee10 3a10 	vmov	r3, s0
   e6c14:	f023 4300 	bic.w	r3, r3, #2147483648	; 0x80000000
   e6c18:	ee00 3a10 	vmov	s0, r3
   e6c1c:	4770      	bx	lr
   e6c1e:	bf00      	nop

000e6c20 <finitef>:
   e6c20:	ee10 3a10 	vmov	r3, s0
   e6c24:	f023 4000 	bic.w	r0, r3, #2147483648	; 0x80000000
   e6c28:	f1b0 4fff 	cmp.w	r0, #2139095040	; 0x7f800000
   e6c2c:	bfac      	ite	ge
   e6c2e:	2000      	movge	r0, #0
   e6c30:	2001      	movlt	r0, #1
   e6c32:	4770      	bx	lr

000e6c34 <scalbnf>:
   e6c34:	b508      	push	{r3, lr}
   e6c36:	ee10 3a10 	vmov	r3, s0
   e6c3a:	f033 4200 	bics.w	r2, r3, #2147483648	; 0x80000000
   e6c3e:	ed2d 8b02 	vpush	{d8}
   e6c42:	d011      	beq.n	e6c68 <scalbnf+0x34>
   e6c44:	f1b2 4fff 	cmp.w	r2, #2139095040	; 0x7f800000
   e6c48:	d211      	bcs.n	e6c6e <scalbnf+0x3a>
   e6c4a:	f5b2 0f00 	cmp.w	r2, #8388608	; 0x800000
   e6c4e:	d313      	bcc.n	e6c78 <scalbnf+0x44>
   e6c50:	0dd2      	lsrs	r2, r2, #23
   e6c52:	4402      	add	r2, r0
   e6c54:	2afe      	cmp	r2, #254	; 0xfe
   e6c56:	dc2e      	bgt.n	e6cb6 <scalbnf+0x82>
   e6c58:	2a00      	cmp	r2, #0
   e6c5a:	dd1a      	ble.n	e6c92 <scalbnf+0x5e>
   e6c5c:	f023 43ff 	bic.w	r3, r3, #2139095040	; 0x7f800000
   e6c60:	ea43 53c2 	orr.w	r3, r3, r2, lsl #23
   e6c64:	ee00 3a10 	vmov	s0, r3
   e6c68:	ecbd 8b02 	vpop	{d8}
   e6c6c:	bd08      	pop	{r3, pc}
   e6c6e:	ecbd 8b02 	vpop	{d8}
   e6c72:	ee30 0a00 	vadd.f32	s0, s0, s0
   e6c76:	bd08      	pop	{r3, pc}
   e6c78:	4b1d      	ldr	r3, [pc, #116]	; (e6cf0 <scalbnf+0xbc>)
   e6c7a:	eddf 7a1e 	vldr	s15, [pc, #120]	; e6cf4 <scalbnf+0xc0>
   e6c7e:	4298      	cmp	r0, r3
   e6c80:	ee20 0a27 	vmul.f32	s0, s0, s15
   e6c84:	db22      	blt.n	e6ccc <scalbnf+0x98>
   e6c86:	ee10 3a10 	vmov	r3, s0
   e6c8a:	f3c3 52c7 	ubfx	r2, r3, #23, #8
   e6c8e:	3a19      	subs	r2, #25
   e6c90:	e7df      	b.n	e6c52 <scalbnf+0x1e>
   e6c92:	f112 0f16 	cmn.w	r2, #22
   e6c96:	da1e      	bge.n	e6cd6 <scalbnf+0xa2>
   e6c98:	f24c 3350 	movw	r3, #50000	; 0xc350
   e6c9c:	4298      	cmp	r0, r3
   e6c9e:	dc0a      	bgt.n	e6cb6 <scalbnf+0x82>
   e6ca0:	ed9f 8a15 	vldr	s16, [pc, #84]	; e6cf8 <scalbnf+0xc4>
   e6ca4:	eef0 0a40 	vmov.f32	s1, s0
   e6ca8:	eeb0 0a48 	vmov.f32	s0, s16
   e6cac:	f000 f82a 	bl	e6d04 <copysignf>
   e6cb0:	ee20 0a08 	vmul.f32	s0, s0, s16
   e6cb4:	e7d8      	b.n	e6c68 <scalbnf+0x34>
   e6cb6:	ed9f 8a11 	vldr	s16, [pc, #68]	; e6cfc <scalbnf+0xc8>
   e6cba:	eef0 0a40 	vmov.f32	s1, s0
   e6cbe:	eeb0 0a48 	vmov.f32	s0, s16
   e6cc2:	f000 f81f 	bl	e6d04 <copysignf>
   e6cc6:	ee20 0a08 	vmul.f32	s0, s0, s16
   e6cca:	e7cd      	b.n	e6c68 <scalbnf+0x34>
   e6ccc:	eddf 0a0a 	vldr	s1, [pc, #40]	; e6cf8 <scalbnf+0xc4>
   e6cd0:	ee20 0a20 	vmul.f32	s0, s0, s1
   e6cd4:	e7c8      	b.n	e6c68 <scalbnf+0x34>
   e6cd6:	3219      	adds	r2, #25
   e6cd8:	f023 43ff 	bic.w	r3, r3, #2139095040	; 0x7f800000
   e6cdc:	ea43 53c2 	orr.w	r3, r3, r2, lsl #23
   e6ce0:	eddf 7a07 	vldr	s15, [pc, #28]	; e6d00 <scalbnf+0xcc>
   e6ce4:	ee00 3a10 	vmov	s0, r3
   e6ce8:	ee20 0a27 	vmul.f32	s0, s0, s15
   e6cec:	e7bc      	b.n	e6c68 <scalbnf+0x34>
   e6cee:	bf00      	nop
   e6cf0:	ffff3cb0 	.word	0xffff3cb0
   e6cf4:	4c000000 	.word	0x4c000000
   e6cf8:	0da24260 	.word	0x0da24260
   e6cfc:	7149f2ca 	.word	0x7149f2ca
   e6d00:	33000000 	.word	0x33000000

000e6d04 <copysignf>:
   e6d04:	ee10 3a10 	vmov	r3, s0
   e6d08:	f023 4200 	bic.w	r2, r3, #2147483648	; 0x80000000
   e6d0c:	ee10 3a90 	vmov	r3, s1
   e6d10:	f003 4300 	and.w	r3, r3, #2147483648	; 0x80000000
   e6d14:	4313      	orrs	r3, r2
   e6d16:	ee00 3a10 	vmov	s0, r3
   e6d1a:	4770      	bx	lr

000e6d1c <__aeabi_llsl>:
   e6d1c:	4091      	lsls	r1, r2
   e6d1e:	1c03      	adds	r3, r0, #0
   e6d20:	4090      	lsls	r0, r2
   e6d22:	469c      	mov	ip, r3
   e6d24:	3a20      	subs	r2, #32
   e6d26:	4093      	lsls	r3, r2
   e6d28:	4319      	orrs	r1, r3
   e6d2a:	4252      	negs	r2, r2
   e6d2c:	4663      	mov	r3, ip
   e6d2e:	40d3      	lsrs	r3, r2
   e6d30:	4319      	orrs	r1, r3
   e6d32:	4770      	bx	lr

000e6d34 <__aeabi_drsub>:
   e6d34:	f081 4100 	eor.w	r1, r1, #2147483648	; 0x80000000
   e6d38:	e002      	b.n	e6d40 <__adddf3>
   e6d3a:	bf00      	nop

000e6d3c <__aeabi_dsub>:
   e6d3c:	f083 4300 	eor.w	r3, r3, #2147483648	; 0x80000000

000e6d40 <__adddf3>:
   e6d40:	b530      	push	{r4, r5, lr}
   e6d42:	ea4f 0441 	mov.w	r4, r1, lsl #1
   e6d46:	ea4f 0543 	mov.w	r5, r3, lsl #1
   e6d4a:	ea94 0f05 	teq	r4, r5
   e6d4e:	bf08      	it	eq
   e6d50:	ea90 0f02 	teqeq	r0, r2
   e6d54:	bf1f      	itttt	ne
   e6d56:	ea54 0c00 	orrsne.w	ip, r4, r0
   e6d5a:	ea55 0c02 	orrsne.w	ip, r5, r2
   e6d5e:	ea7f 5c64 	mvnsne.w	ip, r4, asr #21
   e6d62:	ea7f 5c65 	mvnsne.w	ip, r5, asr #21
   e6d66:	f000 80e2 	beq.w	e6f2e <__adddf3+0x1ee>
   e6d6a:	ea4f 5454 	mov.w	r4, r4, lsr #21
   e6d6e:	ebd4 5555 	rsbs	r5, r4, r5, lsr #21
   e6d72:	bfb8      	it	lt
   e6d74:	426d      	neglt	r5, r5
   e6d76:	dd0c      	ble.n	e6d92 <__adddf3+0x52>
   e6d78:	442c      	add	r4, r5
   e6d7a:	ea80 0202 	eor.w	r2, r0, r2
   e6d7e:	ea81 0303 	eor.w	r3, r1, r3
   e6d82:	ea82 0000 	eor.w	r0, r2, r0
   e6d86:	ea83 0101 	eor.w	r1, r3, r1
   e6d8a:	ea80 0202 	eor.w	r2, r0, r2
   e6d8e:	ea81 0303 	eor.w	r3, r1, r3
   e6d92:	2d36      	cmp	r5, #54	; 0x36
   e6d94:	bf88      	it	hi
   e6d96:	bd30      	pophi	{r4, r5, pc}
   e6d98:	f011 4f00 	tst.w	r1, #2147483648	; 0x80000000
   e6d9c:	ea4f 3101 	mov.w	r1, r1, lsl #12
   e6da0:	f44f 1c80 	mov.w	ip, #1048576	; 0x100000
   e6da4:	ea4c 3111 	orr.w	r1, ip, r1, lsr #12
   e6da8:	d002      	beq.n	e6db0 <__adddf3+0x70>
   e6daa:	4240      	negs	r0, r0
   e6dac:	eb61 0141 	sbc.w	r1, r1, r1, lsl #1
   e6db0:	f013 4f00 	tst.w	r3, #2147483648	; 0x80000000
   e6db4:	ea4f 3303 	mov.w	r3, r3, lsl #12
   e6db8:	ea4c 3313 	orr.w	r3, ip, r3, lsr #12
   e6dbc:	d002      	beq.n	e6dc4 <__adddf3+0x84>
   e6dbe:	4252      	negs	r2, r2
   e6dc0:	eb63 0343 	sbc.w	r3, r3, r3, lsl #1
   e6dc4:	ea94 0f05 	teq	r4, r5
   e6dc8:	f000 80a7 	beq.w	e6f1a <__adddf3+0x1da>
   e6dcc:	f1a4 0401 	sub.w	r4, r4, #1
   e6dd0:	f1d5 0e20 	rsbs	lr, r5, #32
   e6dd4:	db0d      	blt.n	e6df2 <__adddf3+0xb2>
   e6dd6:	fa02 fc0e 	lsl.w	ip, r2, lr
   e6dda:	fa22 f205 	lsr.w	r2, r2, r5
   e6dde:	1880      	adds	r0, r0, r2
   e6de0:	f141 0100 	adc.w	r1, r1, #0
   e6de4:	fa03 f20e 	lsl.w	r2, r3, lr
   e6de8:	1880      	adds	r0, r0, r2
   e6dea:	fa43 f305 	asr.w	r3, r3, r5
   e6dee:	4159      	adcs	r1, r3
   e6df0:	e00e      	b.n	e6e10 <__adddf3+0xd0>
   e6df2:	f1a5 0520 	sub.w	r5, r5, #32
   e6df6:	f10e 0e20 	add.w	lr, lr, #32
   e6dfa:	2a01      	cmp	r2, #1
   e6dfc:	fa03 fc0e 	lsl.w	ip, r3, lr
   e6e00:	bf28      	it	cs
   e6e02:	f04c 0c02 	orrcs.w	ip, ip, #2
   e6e06:	fa43 f305 	asr.w	r3, r3, r5
   e6e0a:	18c0      	adds	r0, r0, r3
   e6e0c:	eb51 71e3 	adcs.w	r1, r1, r3, asr #31
   e6e10:	f001 4500 	and.w	r5, r1, #2147483648	; 0x80000000
   e6e14:	d507      	bpl.n	e6e26 <__adddf3+0xe6>
   e6e16:	f04f 0e00 	mov.w	lr, #0
   e6e1a:	f1dc 0c00 	rsbs	ip, ip, #0
   e6e1e:	eb7e 0000 	sbcs.w	r0, lr, r0
   e6e22:	eb6e 0101 	sbc.w	r1, lr, r1
   e6e26:	f5b1 1f80 	cmp.w	r1, #1048576	; 0x100000
   e6e2a:	d31b      	bcc.n	e6e64 <__adddf3+0x124>
   e6e2c:	f5b1 1f00 	cmp.w	r1, #2097152	; 0x200000
   e6e30:	d30c      	bcc.n	e6e4c <__adddf3+0x10c>
   e6e32:	0849      	lsrs	r1, r1, #1
   e6e34:	ea5f 0030 	movs.w	r0, r0, rrx
   e6e38:	ea4f 0c3c 	mov.w	ip, ip, rrx
   e6e3c:	f104 0401 	add.w	r4, r4, #1
   e6e40:	ea4f 5244 	mov.w	r2, r4, lsl #21
   e6e44:	f512 0f80 	cmn.w	r2, #4194304	; 0x400000
   e6e48:	f080 809a 	bcs.w	e6f80 <__adddf3+0x240>
   e6e4c:	f1bc 4f00 	cmp.w	ip, #2147483648	; 0x80000000
   e6e50:	bf08      	it	eq
   e6e52:	ea5f 0c50 	movseq.w	ip, r0, lsr #1
   e6e56:	f150 0000 	adcs.w	r0, r0, #0
   e6e5a:	eb41 5104 	adc.w	r1, r1, r4, lsl #20
   e6e5e:	ea41 0105 	orr.w	r1, r1, r5
   e6e62:	bd30      	pop	{r4, r5, pc}
   e6e64:	ea5f 0c4c 	movs.w	ip, ip, lsl #1
   e6e68:	4140      	adcs	r0, r0
   e6e6a:	eb41 0101 	adc.w	r1, r1, r1
   e6e6e:	f411 1f80 	tst.w	r1, #1048576	; 0x100000
   e6e72:	f1a4 0401 	sub.w	r4, r4, #1
   e6e76:	d1e9      	bne.n	e6e4c <__adddf3+0x10c>
   e6e78:	f091 0f00 	teq	r1, #0
   e6e7c:	bf04      	itt	eq
   e6e7e:	4601      	moveq	r1, r0
   e6e80:	2000      	moveq	r0, #0
   e6e82:	fab1 f381 	clz	r3, r1
   e6e86:	bf08      	it	eq
   e6e88:	3320      	addeq	r3, #32
   e6e8a:	f1a3 030b 	sub.w	r3, r3, #11
   e6e8e:	f1b3 0220 	subs.w	r2, r3, #32
   e6e92:	da0c      	bge.n	e6eae <__adddf3+0x16e>
   e6e94:	320c      	adds	r2, #12
   e6e96:	dd08      	ble.n	e6eaa <__adddf3+0x16a>
   e6e98:	f102 0c14 	add.w	ip, r2, #20
   e6e9c:	f1c2 020c 	rsb	r2, r2, #12
   e6ea0:	fa01 f00c 	lsl.w	r0, r1, ip
   e6ea4:	fa21 f102 	lsr.w	r1, r1, r2
   e6ea8:	e00c      	b.n	e6ec4 <__adddf3+0x184>
   e6eaa:	f102 0214 	add.w	r2, r2, #20
   e6eae:	bfd8      	it	le
   e6eb0:	f1c2 0c20 	rsble	ip, r2, #32
   e6eb4:	fa01 f102 	lsl.w	r1, r1, r2
   e6eb8:	fa20 fc0c 	lsr.w	ip, r0, ip
   e6ebc:	bfdc      	itt	le
   e6ebe:	ea41 010c 	orrle.w	r1, r1, ip
   e6ec2:	4090      	lslle	r0, r2
   e6ec4:	1ae4      	subs	r4, r4, r3
   e6ec6:	bfa2      	ittt	ge
   e6ec8:	eb01 5104 	addge.w	r1, r1, r4, lsl #20
   e6ecc:	4329      	orrge	r1, r5
   e6ece:	bd30      	popge	{r4, r5, pc}
   e6ed0:	ea6f 0404 	mvn.w	r4, r4
   e6ed4:	3c1f      	subs	r4, #31
   e6ed6:	da1c      	bge.n	e6f12 <__adddf3+0x1d2>
   e6ed8:	340c      	adds	r4, #12
   e6eda:	dc0e      	bgt.n	e6efa <__adddf3+0x1ba>
   e6edc:	f104 0414 	add.w	r4, r4, #20
   e6ee0:	f1c4 0220 	rsb	r2, r4, #32
   e6ee4:	fa20 f004 	lsr.w	r0, r0, r4
   e6ee8:	fa01 f302 	lsl.w	r3, r1, r2
   e6eec:	ea40 0003 	orr.w	r0, r0, r3
   e6ef0:	fa21 f304 	lsr.w	r3, r1, r4
   e6ef4:	ea45 0103 	orr.w	r1, r5, r3
   e6ef8:	bd30      	pop	{r4, r5, pc}
   e6efa:	f1c4 040c 	rsb	r4, r4, #12
   e6efe:	f1c4 0220 	rsb	r2, r4, #32
   e6f02:	fa20 f002 	lsr.w	r0, r0, r2
   e6f06:	fa01 f304 	lsl.w	r3, r1, r4
   e6f0a:	ea40 0003 	orr.w	r0, r0, r3
   e6f0e:	4629      	mov	r1, r5
   e6f10:	bd30      	pop	{r4, r5, pc}
   e6f12:	fa21 f004 	lsr.w	r0, r1, r4
   e6f16:	4629      	mov	r1, r5
   e6f18:	bd30      	pop	{r4, r5, pc}
   e6f1a:	f094 0f00 	teq	r4, #0
   e6f1e:	f483 1380 	eor.w	r3, r3, #1048576	; 0x100000
   e6f22:	bf06      	itte	eq
   e6f24:	f481 1180 	eoreq.w	r1, r1, #1048576	; 0x100000
   e6f28:	3401      	addeq	r4, #1
   e6f2a:	3d01      	subne	r5, #1
   e6f2c:	e74e      	b.n	e6dcc <__adddf3+0x8c>
   e6f2e:	ea7f 5c64 	mvns.w	ip, r4, asr #21
   e6f32:	bf18      	it	ne
   e6f34:	ea7f 5c65 	mvnsne.w	ip, r5, asr #21
   e6f38:	d029      	beq.n	e6f8e <__adddf3+0x24e>
   e6f3a:	ea94 0f05 	teq	r4, r5
   e6f3e:	bf08      	it	eq
   e6f40:	ea90 0f02 	teqeq	r0, r2
   e6f44:	d005      	beq.n	e6f52 <__adddf3+0x212>
   e6f46:	ea54 0c00 	orrs.w	ip, r4, r0
   e6f4a:	bf04      	itt	eq
   e6f4c:	4619      	moveq	r1, r3
   e6f4e:	4610      	moveq	r0, r2
   e6f50:	bd30      	pop	{r4, r5, pc}
   e6f52:	ea91 0f03 	teq	r1, r3
   e6f56:	bf1e      	ittt	ne
   e6f58:	2100      	movne	r1, #0
   e6f5a:	2000      	movne	r0, #0
   e6f5c:	bd30      	popne	{r4, r5, pc}
   e6f5e:	ea5f 5c54 	movs.w	ip, r4, lsr #21
   e6f62:	d105      	bne.n	e6f70 <__adddf3+0x230>
   e6f64:	0040      	lsls	r0, r0, #1
   e6f66:	4149      	adcs	r1, r1
   e6f68:	bf28      	it	cs
   e6f6a:	f041 4100 	orrcs.w	r1, r1, #2147483648	; 0x80000000
   e6f6e:	bd30      	pop	{r4, r5, pc}
   e6f70:	f514 0480 	adds.w	r4, r4, #4194304	; 0x400000
   e6f74:	bf3c      	itt	cc
   e6f76:	f501 1180 	addcc.w	r1, r1, #1048576	; 0x100000
   e6f7a:	bd30      	popcc	{r4, r5, pc}
   e6f7c:	f001 4500 	and.w	r5, r1, #2147483648	; 0x80000000
   e6f80:	f045 41fe 	orr.w	r1, r5, #2130706432	; 0x7f000000
   e6f84:	f441 0170 	orr.w	r1, r1, #15728640	; 0xf00000
   e6f88:	f04f 0000 	mov.w	r0, #0
   e6f8c:	bd30      	pop	{r4, r5, pc}
   e6f8e:	ea7f 5c64 	mvns.w	ip, r4, asr #21
   e6f92:	bf1a      	itte	ne
   e6f94:	4619      	movne	r1, r3
   e6f96:	4610      	movne	r0, r2
   e6f98:	ea7f 5c65 	mvnseq.w	ip, r5, asr #21
   e6f9c:	bf1c      	itt	ne
   e6f9e:	460b      	movne	r3, r1
   e6fa0:	4602      	movne	r2, r0
   e6fa2:	ea50 3401 	orrs.w	r4, r0, r1, lsl #12
   e6fa6:	bf06      	itte	eq
   e6fa8:	ea52 3503 	orrseq.w	r5, r2, r3, lsl #12
   e6fac:	ea91 0f03 	teqeq	r1, r3
   e6fb0:	f441 2100 	orrne.w	r1, r1, #524288	; 0x80000
   e6fb4:	bd30      	pop	{r4, r5, pc}
   e6fb6:	bf00      	nop

000e6fb8 <__aeabi_ui2d>:
   e6fb8:	f090 0f00 	teq	r0, #0
   e6fbc:	bf04      	itt	eq
   e6fbe:	2100      	moveq	r1, #0
   e6fc0:	4770      	bxeq	lr
   e6fc2:	b530      	push	{r4, r5, lr}
   e6fc4:	f44f 6480 	mov.w	r4, #1024	; 0x400
   e6fc8:	f104 0432 	add.w	r4, r4, #50	; 0x32
   e6fcc:	f04f 0500 	mov.w	r5, #0
   e6fd0:	f04f 0100 	mov.w	r1, #0
   e6fd4:	e750      	b.n	e6e78 <__adddf3+0x138>
   e6fd6:	bf00      	nop

000e6fd8 <__aeabi_i2d>:
   e6fd8:	f090 0f00 	teq	r0, #0
   e6fdc:	bf04      	itt	eq
   e6fde:	2100      	moveq	r1, #0
   e6fe0:	4770      	bxeq	lr
   e6fe2:	b530      	push	{r4, r5, lr}
   e6fe4:	f44f 6480 	mov.w	r4, #1024	; 0x400
   e6fe8:	f104 0432 	add.w	r4, r4, #50	; 0x32
   e6fec:	f010 4500 	ands.w	r5, r0, #2147483648	; 0x80000000
   e6ff0:	bf48      	it	mi
   e6ff2:	4240      	negmi	r0, r0
   e6ff4:	f04f 0100 	mov.w	r1, #0
   e6ff8:	e73e      	b.n	e6e78 <__adddf3+0x138>
   e6ffa:	bf00      	nop

000e6ffc <__aeabi_f2d>:
   e6ffc:	0042      	lsls	r2, r0, #1
   e6ffe:	ea4f 01e2 	mov.w	r1, r2, asr #3
   e7002:	ea4f 0131 	mov.w	r1, r1, rrx
   e7006:	ea4f 7002 	mov.w	r0, r2, lsl #28
   e700a:	bf1f      	itttt	ne
   e700c:	f012 437f 	andsne.w	r3, r2, #4278190080	; 0xff000000
   e7010:	f093 4f7f 	teqne	r3, #4278190080	; 0xff000000
   e7014:	f081 5160 	eorne.w	r1, r1, #939524096	; 0x38000000
   e7018:	4770      	bxne	lr
   e701a:	f092 0f00 	teq	r2, #0
   e701e:	bf14      	ite	ne
   e7020:	f093 4f7f 	teqne	r3, #4278190080	; 0xff000000
   e7024:	4770      	bxeq	lr
   e7026:	b530      	push	{r4, r5, lr}
   e7028:	f44f 7460 	mov.w	r4, #896	; 0x380
   e702c:	f001 4500 	and.w	r5, r1, #2147483648	; 0x80000000
   e7030:	f021 4100 	bic.w	r1, r1, #2147483648	; 0x80000000
   e7034:	e720      	b.n	e6e78 <__adddf3+0x138>
   e7036:	bf00      	nop

000e7038 <__aeabi_ul2d>:
   e7038:	ea50 0201 	orrs.w	r2, r0, r1
   e703c:	bf08      	it	eq
   e703e:	4770      	bxeq	lr
   e7040:	b530      	push	{r4, r5, lr}
   e7042:	f04f 0500 	mov.w	r5, #0
   e7046:	e00a      	b.n	e705e <__aeabi_l2d+0x16>

000e7048 <__aeabi_l2d>:
   e7048:	ea50 0201 	orrs.w	r2, r0, r1
   e704c:	bf08      	it	eq
   e704e:	4770      	bxeq	lr
   e7050:	b530      	push	{r4, r5, lr}
   e7052:	f011 4500 	ands.w	r5, r1, #2147483648	; 0x80000000
   e7056:	d502      	bpl.n	e705e <__aeabi_l2d+0x16>
   e7058:	4240      	negs	r0, r0
   e705a:	eb61 0141 	sbc.w	r1, r1, r1, lsl #1
   e705e:	f44f 6480 	mov.w	r4, #1024	; 0x400
   e7062:	f104 0432 	add.w	r4, r4, #50	; 0x32
   e7066:	ea5f 5c91 	movs.w	ip, r1, lsr #22
   e706a:	f43f aedc 	beq.w	e6e26 <__adddf3+0xe6>
   e706e:	f04f 0203 	mov.w	r2, #3
   e7072:	ea5f 0cdc 	movs.w	ip, ip, lsr #3
   e7076:	bf18      	it	ne
   e7078:	3203      	addne	r2, #3
   e707a:	ea5f 0cdc 	movs.w	ip, ip, lsr #3
   e707e:	bf18      	it	ne
   e7080:	3203      	addne	r2, #3
   e7082:	eb02 02dc 	add.w	r2, r2, ip, lsr #3
   e7086:	f1c2 0320 	rsb	r3, r2, #32
   e708a:	fa00 fc03 	lsl.w	ip, r0, r3
   e708e:	fa20 f002 	lsr.w	r0, r0, r2
   e7092:	fa01 fe03 	lsl.w	lr, r1, r3
   e7096:	ea40 000e 	orr.w	r0, r0, lr
   e709a:	fa21 f102 	lsr.w	r1, r1, r2
   e709e:	4414      	add	r4, r2
   e70a0:	e6c1      	b.n	e6e26 <__adddf3+0xe6>
   e70a2:	bf00      	nop

000e70a4 <__aeabi_dmul>:
   e70a4:	b570      	push	{r4, r5, r6, lr}
   e70a6:	f04f 0cff 	mov.w	ip, #255	; 0xff
   e70aa:	f44c 6ce0 	orr.w	ip, ip, #1792	; 0x700
   e70ae:	ea1c 5411 	ands.w	r4, ip, r1, lsr #20
   e70b2:	bf1d      	ittte	ne
   e70b4:	ea1c 5513 	andsne.w	r5, ip, r3, lsr #20
   e70b8:	ea94 0f0c 	teqne	r4, ip
   e70bc:	ea95 0f0c 	teqne	r5, ip
   e70c0:	f000 f8de 	bleq	e7280 <__aeabi_dmul+0x1dc>
   e70c4:	442c      	add	r4, r5
   e70c6:	ea81 0603 	eor.w	r6, r1, r3
   e70ca:	ea21 514c 	bic.w	r1, r1, ip, lsl #21
   e70ce:	ea23 534c 	bic.w	r3, r3, ip, lsl #21
   e70d2:	ea50 3501 	orrs.w	r5, r0, r1, lsl #12
   e70d6:	bf18      	it	ne
   e70d8:	ea52 3503 	orrsne.w	r5, r2, r3, lsl #12
   e70dc:	f441 1180 	orr.w	r1, r1, #1048576	; 0x100000
   e70e0:	f443 1380 	orr.w	r3, r3, #1048576	; 0x100000
   e70e4:	d038      	beq.n	e7158 <__aeabi_dmul+0xb4>
   e70e6:	fba0 ce02 	umull	ip, lr, r0, r2
   e70ea:	f04f 0500 	mov.w	r5, #0
   e70ee:	fbe1 e502 	umlal	lr, r5, r1, r2
   e70f2:	f006 4200 	and.w	r2, r6, #2147483648	; 0x80000000
   e70f6:	fbe0 e503 	umlal	lr, r5, r0, r3
   e70fa:	f04f 0600 	mov.w	r6, #0
   e70fe:	fbe1 5603 	umlal	r5, r6, r1, r3
   e7102:	f09c 0f00 	teq	ip, #0
   e7106:	bf18      	it	ne
   e7108:	f04e 0e01 	orrne.w	lr, lr, #1
   e710c:	f1a4 04ff 	sub.w	r4, r4, #255	; 0xff
   e7110:	f5b6 7f00 	cmp.w	r6, #512	; 0x200
   e7114:	f564 7440 	sbc.w	r4, r4, #768	; 0x300
   e7118:	d204      	bcs.n	e7124 <__aeabi_dmul+0x80>
   e711a:	ea5f 0e4e 	movs.w	lr, lr, lsl #1
   e711e:	416d      	adcs	r5, r5
   e7120:	eb46 0606 	adc.w	r6, r6, r6
   e7124:	ea42 21c6 	orr.w	r1, r2, r6, lsl #11
   e7128:	ea41 5155 	orr.w	r1, r1, r5, lsr #21
   e712c:	ea4f 20c5 	mov.w	r0, r5, lsl #11
   e7130:	ea40 505e 	orr.w	r0, r0, lr, lsr #21
   e7134:	ea4f 2ece 	mov.w	lr, lr, lsl #11
   e7138:	f1b4 0cfd 	subs.w	ip, r4, #253	; 0xfd
   e713c:	bf88      	it	hi
   e713e:	f5bc 6fe0 	cmphi.w	ip, #1792	; 0x700
   e7142:	d81e      	bhi.n	e7182 <__aeabi_dmul+0xde>
   e7144:	f1be 4f00 	cmp.w	lr, #2147483648	; 0x80000000
   e7148:	bf08      	it	eq
   e714a:	ea5f 0e50 	movseq.w	lr, r0, lsr #1
   e714e:	f150 0000 	adcs.w	r0, r0, #0
   e7152:	eb41 5104 	adc.w	r1, r1, r4, lsl #20
   e7156:	bd70      	pop	{r4, r5, r6, pc}
   e7158:	f006 4600 	and.w	r6, r6, #2147483648	; 0x80000000
   e715c:	ea46 0101 	orr.w	r1, r6, r1
   e7160:	ea40 0002 	orr.w	r0, r0, r2
   e7164:	ea81 0103 	eor.w	r1, r1, r3
   e7168:	ebb4 045c 	subs.w	r4, r4, ip, lsr #1
   e716c:	bfc2      	ittt	gt
   e716e:	ebd4 050c 	rsbsgt	r5, r4, ip
   e7172:	ea41 5104 	orrgt.w	r1, r1, r4, lsl #20
   e7176:	bd70      	popgt	{r4, r5, r6, pc}
   e7178:	f441 1180 	orr.w	r1, r1, #1048576	; 0x100000
   e717c:	f04f 0e00 	mov.w	lr, #0
   e7180:	3c01      	subs	r4, #1
   e7182:	f300 80ab 	bgt.w	e72dc <__aeabi_dmul+0x238>
   e7186:	f114 0f36 	cmn.w	r4, #54	; 0x36
   e718a:	bfde      	ittt	le
   e718c:	2000      	movle	r0, #0
   e718e:	f001 4100 	andle.w	r1, r1, #2147483648	; 0x80000000
   e7192:	bd70      	pople	{r4, r5, r6, pc}
   e7194:	f1c4 0400 	rsb	r4, r4, #0
   e7198:	3c20      	subs	r4, #32
   e719a:	da35      	bge.n	e7208 <__aeabi_dmul+0x164>
   e719c:	340c      	adds	r4, #12
   e719e:	dc1b      	bgt.n	e71d8 <__aeabi_dmul+0x134>
   e71a0:	f104 0414 	add.w	r4, r4, #20
   e71a4:	f1c4 0520 	rsb	r5, r4, #32
   e71a8:	fa00 f305 	lsl.w	r3, r0, r5
   e71ac:	fa20 f004 	lsr.w	r0, r0, r4
   e71b0:	fa01 f205 	lsl.w	r2, r1, r5
   e71b4:	ea40 0002 	orr.w	r0, r0, r2
   e71b8:	f001 4200 	and.w	r2, r1, #2147483648	; 0x80000000
   e71bc:	f021 4100 	bic.w	r1, r1, #2147483648	; 0x80000000
   e71c0:	eb10 70d3 	adds.w	r0, r0, r3, lsr #31
   e71c4:	fa21 f604 	lsr.w	r6, r1, r4
   e71c8:	eb42 0106 	adc.w	r1, r2, r6
   e71cc:	ea5e 0e43 	orrs.w	lr, lr, r3, lsl #1
   e71d0:	bf08      	it	eq
   e71d2:	ea20 70d3 	biceq.w	r0, r0, r3, lsr #31
   e71d6:	bd70      	pop	{r4, r5, r6, pc}
   e71d8:	f1c4 040c 	rsb	r4, r4, #12
   e71dc:	f1c4 0520 	rsb	r5, r4, #32
   e71e0:	fa00 f304 	lsl.w	r3, r0, r4
   e71e4:	fa20 f005 	lsr.w	r0, r0, r5
   e71e8:	fa01 f204 	lsl.w	r2, r1, r4
   e71ec:	ea40 0002 	orr.w	r0, r0, r2
   e71f0:	f001 4100 	and.w	r1, r1, #2147483648	; 0x80000000
   e71f4:	eb10 70d3 	adds.w	r0, r0, r3, lsr #31
   e71f8:	f141 0100 	adc.w	r1, r1, #0
   e71fc:	ea5e 0e43 	orrs.w	lr, lr, r3, lsl #1
   e7200:	bf08      	it	eq
   e7202:	ea20 70d3 	biceq.w	r0, r0, r3, lsr #31
   e7206:	bd70      	pop	{r4, r5, r6, pc}
   e7208:	f1c4 0520 	rsb	r5, r4, #32
   e720c:	fa00 f205 	lsl.w	r2, r0, r5
   e7210:	ea4e 0e02 	orr.w	lr, lr, r2
   e7214:	fa20 f304 	lsr.w	r3, r0, r4
   e7218:	fa01 f205 	lsl.w	r2, r1, r5
   e721c:	ea43 0302 	orr.w	r3, r3, r2
   e7220:	fa21 f004 	lsr.w	r0, r1, r4
   e7224:	f001 4100 	and.w	r1, r1, #2147483648	; 0x80000000
   e7228:	fa21 f204 	lsr.w	r2, r1, r4
   e722c:	ea20 0002 	bic.w	r0, r0, r2
   e7230:	eb00 70d3 	add.w	r0, r0, r3, lsr #31
   e7234:	ea5e 0e43 	orrs.w	lr, lr, r3, lsl #1
   e7238:	bf08      	it	eq
   e723a:	ea20 70d3 	biceq.w	r0, r0, r3, lsr #31
   e723e:	bd70      	pop	{r4, r5, r6, pc}
   e7240:	f094 0f00 	teq	r4, #0
   e7244:	d10f      	bne.n	e7266 <__aeabi_dmul+0x1c2>
   e7246:	f001 4600 	and.w	r6, r1, #2147483648	; 0x80000000
   e724a:	0040      	lsls	r0, r0, #1
   e724c:	eb41 0101 	adc.w	r1, r1, r1
   e7250:	f411 1f80 	tst.w	r1, #1048576	; 0x100000
   e7254:	bf08      	it	eq
   e7256:	3c01      	subeq	r4, #1
   e7258:	d0f7      	beq.n	e724a <__aeabi_dmul+0x1a6>
   e725a:	ea41 0106 	orr.w	r1, r1, r6
   e725e:	f095 0f00 	teq	r5, #0
   e7262:	bf18      	it	ne
   e7264:	4770      	bxne	lr
   e7266:	f003 4600 	and.w	r6, r3, #2147483648	; 0x80000000
   e726a:	0052      	lsls	r2, r2, #1
   e726c:	eb43 0303 	adc.w	r3, r3, r3
   e7270:	f413 1f80 	tst.w	r3, #1048576	; 0x100000
   e7274:	bf08      	it	eq
   e7276:	3d01      	subeq	r5, #1
   e7278:	d0f7      	beq.n	e726a <__aeabi_dmul+0x1c6>
   e727a:	ea43 0306 	orr.w	r3, r3, r6
   e727e:	4770      	bx	lr
   e7280:	ea94 0f0c 	teq	r4, ip
   e7284:	ea0c 5513 	and.w	r5, ip, r3, lsr #20
   e7288:	bf18      	it	ne
   e728a:	ea95 0f0c 	teqne	r5, ip
   e728e:	d00c      	beq.n	e72aa <__aeabi_dmul+0x206>
   e7290:	ea50 0641 	orrs.w	r6, r0, r1, lsl #1
   e7294:	bf18      	it	ne
   e7296:	ea52 0643 	orrsne.w	r6, r2, r3, lsl #1
   e729a:	d1d1      	bne.n	e7240 <__aeabi_dmul+0x19c>
   e729c:	ea81 0103 	eor.w	r1, r1, r3
   e72a0:	f001 4100 	and.w	r1, r1, #2147483648	; 0x80000000
   e72a4:	f04f 0000 	mov.w	r0, #0
   e72a8:	bd70      	pop	{r4, r5, r6, pc}
   e72aa:	ea50 0641 	orrs.w	r6, r0, r1, lsl #1
   e72ae:	bf06      	itte	eq
   e72b0:	4610      	moveq	r0, r2
   e72b2:	4619      	moveq	r1, r3
   e72b4:	ea52 0643 	orrsne.w	r6, r2, r3, lsl #1
   e72b8:	d019      	beq.n	e72ee <__aeabi_dmul+0x24a>
   e72ba:	ea94 0f0c 	teq	r4, ip
   e72be:	d102      	bne.n	e72c6 <__aeabi_dmul+0x222>
   e72c0:	ea50 3601 	orrs.w	r6, r0, r1, lsl #12
   e72c4:	d113      	bne.n	e72ee <__aeabi_dmul+0x24a>
   e72c6:	ea95 0f0c 	teq	r5, ip
   e72ca:	d105      	bne.n	e72d8 <__aeabi_dmul+0x234>
   e72cc:	ea52 3603 	orrs.w	r6, r2, r3, lsl #12
   e72d0:	bf1c      	itt	ne
   e72d2:	4610      	movne	r0, r2
   e72d4:	4619      	movne	r1, r3
   e72d6:	d10a      	bne.n	e72ee <__aeabi_dmul+0x24a>
   e72d8:	ea81 0103 	eor.w	r1, r1, r3
   e72dc:	f001 4100 	and.w	r1, r1, #2147483648	; 0x80000000
   e72e0:	f041 41fe 	orr.w	r1, r1, #2130706432	; 0x7f000000
   e72e4:	f441 0170 	orr.w	r1, r1, #15728640	; 0xf00000
   e72e8:	f04f 0000 	mov.w	r0, #0
   e72ec:	bd70      	pop	{r4, r5, r6, pc}
   e72ee:	f041 41fe 	orr.w	r1, r1, #2130706432	; 0x7f000000
   e72f2:	f441 0178 	orr.w	r1, r1, #16252928	; 0xf80000
   e72f6:	bd70      	pop	{r4, r5, r6, pc}

000e72f8 <__aeabi_ddiv>:
   e72f8:	b570      	push	{r4, r5, r6, lr}
   e72fa:	f04f 0cff 	mov.w	ip, #255	; 0xff
   e72fe:	f44c 6ce0 	orr.w	ip, ip, #1792	; 0x700
   e7302:	ea1c 5411 	ands.w	r4, ip, r1, lsr #20
   e7306:	bf1d      	ittte	ne
   e7308:	ea1c 5513 	andsne.w	r5, ip, r3, lsr #20
   e730c:	ea94 0f0c 	teqne	r4, ip
   e7310:	ea95 0f0c 	teqne	r5, ip
   e7314:	f000 f8a7 	bleq	e7466 <__aeabi_ddiv+0x16e>
   e7318:	eba4 0405 	sub.w	r4, r4, r5
   e731c:	ea81 0e03 	eor.w	lr, r1, r3
   e7320:	ea52 3503 	orrs.w	r5, r2, r3, lsl #12
   e7324:	ea4f 3101 	mov.w	r1, r1, lsl #12
   e7328:	f000 8088 	beq.w	e743c <__aeabi_ddiv+0x144>
   e732c:	ea4f 3303 	mov.w	r3, r3, lsl #12
   e7330:	f04f 5580 	mov.w	r5, #268435456	; 0x10000000
   e7334:	ea45 1313 	orr.w	r3, r5, r3, lsr #4
   e7338:	ea43 6312 	orr.w	r3, r3, r2, lsr #24
   e733c:	ea4f 2202 	mov.w	r2, r2, lsl #8
   e7340:	ea45 1511 	orr.w	r5, r5, r1, lsr #4
   e7344:	ea45 6510 	orr.w	r5, r5, r0, lsr #24
   e7348:	ea4f 2600 	mov.w	r6, r0, lsl #8
   e734c:	f00e 4100 	and.w	r1, lr, #2147483648	; 0x80000000
   e7350:	429d      	cmp	r5, r3
   e7352:	bf08      	it	eq
   e7354:	4296      	cmpeq	r6, r2
   e7356:	f144 04fd 	adc.w	r4, r4, #253	; 0xfd
   e735a:	f504 7440 	add.w	r4, r4, #768	; 0x300
   e735e:	d202      	bcs.n	e7366 <__aeabi_ddiv+0x6e>
   e7360:	085b      	lsrs	r3, r3, #1
   e7362:	ea4f 0232 	mov.w	r2, r2, rrx
   e7366:	1ab6      	subs	r6, r6, r2
   e7368:	eb65 0503 	sbc.w	r5, r5, r3
   e736c:	085b      	lsrs	r3, r3, #1
   e736e:	ea4f 0232 	mov.w	r2, r2, rrx
   e7372:	f44f 1080 	mov.w	r0, #1048576	; 0x100000
   e7376:	f44f 2c00 	mov.w	ip, #524288	; 0x80000
   e737a:	ebb6 0e02 	subs.w	lr, r6, r2
   e737e:	eb75 0e03 	sbcs.w	lr, r5, r3
   e7382:	bf22      	ittt	cs
   e7384:	1ab6      	subcs	r6, r6, r2
   e7386:	4675      	movcs	r5, lr
   e7388:	ea40 000c 	orrcs.w	r0, r0, ip
   e738c:	085b      	lsrs	r3, r3, #1
   e738e:	ea4f 0232 	mov.w	r2, r2, rrx
   e7392:	ebb6 0e02 	subs.w	lr, r6, r2
   e7396:	eb75 0e03 	sbcs.w	lr, r5, r3
   e739a:	bf22      	ittt	cs
   e739c:	1ab6      	subcs	r6, r6, r2
   e739e:	4675      	movcs	r5, lr
   e73a0:	ea40 005c 	orrcs.w	r0, r0, ip, lsr #1
   e73a4:	085b      	lsrs	r3, r3, #1
   e73a6:	ea4f 0232 	mov.w	r2, r2, rrx
   e73aa:	ebb6 0e02 	subs.w	lr, r6, r2
   e73ae:	eb75 0e03 	sbcs.w	lr, r5, r3
   e73b2:	bf22      	ittt	cs
   e73b4:	1ab6      	subcs	r6, r6, r2
   e73b6:	4675      	movcs	r5, lr
   e73b8:	ea40 009c 	orrcs.w	r0, r0, ip, lsr #2
   e73bc:	085b      	lsrs	r3, r3, #1
   e73be:	ea4f 0232 	mov.w	r2, r2, rrx
   e73c2:	ebb6 0e02 	subs.w	lr, r6, r2
   e73c6:	eb75 0e03 	sbcs.w	lr, r5, r3
   e73ca:	bf22      	ittt	cs
   e73cc:	1ab6      	subcs	r6, r6, r2
   e73ce:	4675      	movcs	r5, lr
   e73d0:	ea40 00dc 	orrcs.w	r0, r0, ip, lsr #3
   e73d4:	ea55 0e06 	orrs.w	lr, r5, r6
   e73d8:	d018      	beq.n	e740c <__aeabi_ddiv+0x114>
   e73da:	ea4f 1505 	mov.w	r5, r5, lsl #4
   e73de:	ea45 7516 	orr.w	r5, r5, r6, lsr #28
   e73e2:	ea4f 1606 	mov.w	r6, r6, lsl #4
   e73e6:	ea4f 03c3 	mov.w	r3, r3, lsl #3
   e73ea:	ea43 7352 	orr.w	r3, r3, r2, lsr #29
   e73ee:	ea4f 02c2 	mov.w	r2, r2, lsl #3
   e73f2:	ea5f 1c1c 	movs.w	ip, ip, lsr #4
   e73f6:	d1c0      	bne.n	e737a <__aeabi_ddiv+0x82>
   e73f8:	f411 1f80 	tst.w	r1, #1048576	; 0x100000
   e73fc:	d10b      	bne.n	e7416 <__aeabi_ddiv+0x11e>
   e73fe:	ea41 0100 	orr.w	r1, r1, r0
   e7402:	f04f 0000 	mov.w	r0, #0
   e7406:	f04f 4c00 	mov.w	ip, #2147483648	; 0x80000000
   e740a:	e7b6      	b.n	e737a <__aeabi_ddiv+0x82>
   e740c:	f411 1f80 	tst.w	r1, #1048576	; 0x100000
   e7410:	bf04      	itt	eq
   e7412:	4301      	orreq	r1, r0
   e7414:	2000      	moveq	r0, #0
   e7416:	f1b4 0cfd 	subs.w	ip, r4, #253	; 0xfd
   e741a:	bf88      	it	hi
   e741c:	f5bc 6fe0 	cmphi.w	ip, #1792	; 0x700
   e7420:	f63f aeaf 	bhi.w	e7182 <__aeabi_dmul+0xde>
   e7424:	ebb5 0c03 	subs.w	ip, r5, r3
   e7428:	bf04      	itt	eq
   e742a:	ebb6 0c02 	subseq.w	ip, r6, r2
   e742e:	ea5f 0c50 	movseq.w	ip, r0, lsr #1
   e7432:	f150 0000 	adcs.w	r0, r0, #0
   e7436:	eb41 5104 	adc.w	r1, r1, r4, lsl #20
   e743a:	bd70      	pop	{r4, r5, r6, pc}
   e743c:	f00e 4e00 	and.w	lr, lr, #2147483648	; 0x80000000
   e7440:	ea4e 3111 	orr.w	r1, lr, r1, lsr #12
   e7444:	eb14 045c 	adds.w	r4, r4, ip, lsr #1
   e7448:	bfc2      	ittt	gt
   e744a:	ebd4 050c 	rsbsgt	r5, r4, ip
   e744e:	ea41 5104 	orrgt.w	r1, r1, r4, lsl #20
   e7452:	bd70      	popgt	{r4, r5, r6, pc}
   e7454:	f441 1180 	orr.w	r1, r1, #1048576	; 0x100000
   e7458:	f04f 0e00 	mov.w	lr, #0
   e745c:	3c01      	subs	r4, #1
   e745e:	e690      	b.n	e7182 <__aeabi_dmul+0xde>
   e7460:	ea45 0e06 	orr.w	lr, r5, r6
   e7464:	e68d      	b.n	e7182 <__aeabi_dmul+0xde>
   e7466:	ea0c 5513 	and.w	r5, ip, r3, lsr #20
   e746a:	ea94 0f0c 	teq	r4, ip
   e746e:	bf08      	it	eq
   e7470:	ea95 0f0c 	teqeq	r5, ip
   e7474:	f43f af3b 	beq.w	e72ee <__aeabi_dmul+0x24a>
   e7478:	ea94 0f0c 	teq	r4, ip
   e747c:	d10a      	bne.n	e7494 <__aeabi_ddiv+0x19c>
   e747e:	ea50 3401 	orrs.w	r4, r0, r1, lsl #12
   e7482:	f47f af34 	bne.w	e72ee <__aeabi_dmul+0x24a>
   e7486:	ea95 0f0c 	teq	r5, ip
   e748a:	f47f af25 	bne.w	e72d8 <__aeabi_dmul+0x234>
   e748e:	4610      	mov	r0, r2
   e7490:	4619      	mov	r1, r3
   e7492:	e72c      	b.n	e72ee <__aeabi_dmul+0x24a>
   e7494:	ea95 0f0c 	teq	r5, ip
   e7498:	d106      	bne.n	e74a8 <__aeabi_ddiv+0x1b0>
   e749a:	ea52 3503 	orrs.w	r5, r2, r3, lsl #12
   e749e:	f43f aefd 	beq.w	e729c <__aeabi_dmul+0x1f8>
   e74a2:	4610      	mov	r0, r2
   e74a4:	4619      	mov	r1, r3
   e74a6:	e722      	b.n	e72ee <__aeabi_dmul+0x24a>
   e74a8:	ea50 0641 	orrs.w	r6, r0, r1, lsl #1
   e74ac:	bf18      	it	ne
   e74ae:	ea52 0643 	orrsne.w	r6, r2, r3, lsl #1
   e74b2:	f47f aec5 	bne.w	e7240 <__aeabi_dmul+0x19c>
   e74b6:	ea50 0441 	orrs.w	r4, r0, r1, lsl #1
   e74ba:	f47f af0d 	bne.w	e72d8 <__aeabi_dmul+0x234>
   e74be:	ea52 0543 	orrs.w	r5, r2, r3, lsl #1
   e74c2:	f47f aeeb 	bne.w	e729c <__aeabi_dmul+0x1f8>
   e74c6:	e712      	b.n	e72ee <__aeabi_dmul+0x24a>

000e74c8 <__gedf2>:
   e74c8:	f04f 3cff 	mov.w	ip, #4294967295	; 0xffffffff
   e74cc:	e006      	b.n	e74dc <__cmpdf2+0x4>
   e74ce:	bf00      	nop

000e74d0 <__ledf2>:
   e74d0:	f04f 0c01 	mov.w	ip, #1
   e74d4:	e002      	b.n	e74dc <__cmpdf2+0x4>
   e74d6:	bf00      	nop

000e74d8 <__cmpdf2>:
   e74d8:	f04f 0c01 	mov.w	ip, #1
   e74dc:	f84d cd04 	str.w	ip, [sp, #-4]!
   e74e0:	ea4f 0c41 	mov.w	ip, r1, lsl #1
   e74e4:	ea7f 5c6c 	mvns.w	ip, ip, asr #21
   e74e8:	ea4f 0c43 	mov.w	ip, r3, lsl #1
   e74ec:	bf18      	it	ne
   e74ee:	ea7f 5c6c 	mvnsne.w	ip, ip, asr #21
   e74f2:	d01b      	beq.n	e752c <__cmpdf2+0x54>
   e74f4:	b001      	add	sp, #4
   e74f6:	ea50 0c41 	orrs.w	ip, r0, r1, lsl #1
   e74fa:	bf0c      	ite	eq
   e74fc:	ea52 0c43 	orrseq.w	ip, r2, r3, lsl #1
   e7500:	ea91 0f03 	teqne	r1, r3
   e7504:	bf02      	ittt	eq
   e7506:	ea90 0f02 	teqeq	r0, r2
   e750a:	2000      	moveq	r0, #0
   e750c:	4770      	bxeq	lr
   e750e:	f110 0f00 	cmn.w	r0, #0
   e7512:	ea91 0f03 	teq	r1, r3
   e7516:	bf58      	it	pl
   e7518:	4299      	cmppl	r1, r3
   e751a:	bf08      	it	eq
   e751c:	4290      	cmpeq	r0, r2
   e751e:	bf2c      	ite	cs
   e7520:	17d8      	asrcs	r0, r3, #31
   e7522:	ea6f 70e3 	mvncc.w	r0, r3, asr #31
   e7526:	f040 0001 	orr.w	r0, r0, #1
   e752a:	4770      	bx	lr
   e752c:	ea4f 0c41 	mov.w	ip, r1, lsl #1
   e7530:	ea7f 5c6c 	mvns.w	ip, ip, asr #21
   e7534:	d102      	bne.n	e753c <__cmpdf2+0x64>
   e7536:	ea50 3c01 	orrs.w	ip, r0, r1, lsl #12
   e753a:	d107      	bne.n	e754c <__cmpdf2+0x74>
   e753c:	ea4f 0c43 	mov.w	ip, r3, lsl #1
   e7540:	ea7f 5c6c 	mvns.w	ip, ip, asr #21
   e7544:	d1d6      	bne.n	e74f4 <__cmpdf2+0x1c>
   e7546:	ea52 3c03 	orrs.w	ip, r2, r3, lsl #12
   e754a:	d0d3      	beq.n	e74f4 <__cmpdf2+0x1c>
   e754c:	f85d 0b04 	ldr.w	r0, [sp], #4
   e7550:	4770      	bx	lr
   e7552:	bf00      	nop

000e7554 <__aeabi_cdrcmple>:
   e7554:	4684      	mov	ip, r0
   e7556:	4610      	mov	r0, r2
   e7558:	4662      	mov	r2, ip
   e755a:	468c      	mov	ip, r1
   e755c:	4619      	mov	r1, r3
   e755e:	4663      	mov	r3, ip
   e7560:	e000      	b.n	e7564 <__aeabi_cdcmpeq>
   e7562:	bf00      	nop

000e7564 <__aeabi_cdcmpeq>:
   e7564:	b501      	push	{r0, lr}
   e7566:	f7ff ffb7 	bl	e74d8 <__cmpdf2>
   e756a:	2800      	cmp	r0, #0
   e756c:	bf48      	it	mi
   e756e:	f110 0f00 	cmnmi.w	r0, #0
   e7572:	bd01      	pop	{r0, pc}

000e7574 <__aeabi_dcmpeq>:
   e7574:	f84d ed08 	str.w	lr, [sp, #-8]!
   e7578:	f7ff fff4 	bl	e7564 <__aeabi_cdcmpeq>
   e757c:	bf0c      	ite	eq
   e757e:	2001      	moveq	r0, #1
   e7580:	2000      	movne	r0, #0
   e7582:	f85d fb08 	ldr.w	pc, [sp], #8
   e7586:	bf00      	nop

000e7588 <__aeabi_dcmplt>:
   e7588:	f84d ed08 	str.w	lr, [sp, #-8]!
   e758c:	f7ff ffea 	bl	e7564 <__aeabi_cdcmpeq>
   e7590:	bf34      	ite	cc
   e7592:	2001      	movcc	r0, #1
   e7594:	2000      	movcs	r0, #0
   e7596:	f85d fb08 	ldr.w	pc, [sp], #8
   e759a:	bf00      	nop

000e759c <__aeabi_dcmple>:
   e759c:	f84d ed08 	str.w	lr, [sp, #-8]!
   e75a0:	f7ff ffe0 	bl	e7564 <__aeabi_cdcmpeq>
   e75a4:	bf94      	ite	ls
   e75a6:	2001      	movls	r0, #1
   e75a8:	2000      	movhi	r0, #0
   e75aa:	f85d fb08 	ldr.w	pc, [sp], #8
   e75ae:	bf00      	nop

000e75b0 <__aeabi_dcmpge>:
   e75b0:	f84d ed08 	str.w	lr, [sp, #-8]!
   e75b4:	f7ff ffce 	bl	e7554 <__aeabi_cdrcmple>
   e75b8:	bf94      	ite	ls
   e75ba:	2001      	movls	r0, #1
   e75bc:	2000      	movhi	r0, #0
   e75be:	f85d fb08 	ldr.w	pc, [sp], #8
   e75c2:	bf00      	nop

000e75c4 <__aeabi_dcmpgt>:
   e75c4:	f84d ed08 	str.w	lr, [sp, #-8]!
   e75c8:	f7ff ffc4 	bl	e7554 <__aeabi_cdrcmple>
   e75cc:	bf34      	ite	cc
   e75ce:	2001      	movcc	r0, #1
   e75d0:	2000      	movcs	r0, #0
   e75d2:	f85d fb08 	ldr.w	pc, [sp], #8
   e75d6:	bf00      	nop

000e75d8 <__aeabi_d2iz>:
   e75d8:	ea4f 0241 	mov.w	r2, r1, lsl #1
   e75dc:	f512 1200 	adds.w	r2, r2, #2097152	; 0x200000
   e75e0:	d215      	bcs.n	e760e <__aeabi_d2iz+0x36>
   e75e2:	d511      	bpl.n	e7608 <__aeabi_d2iz+0x30>
   e75e4:	f46f 7378 	mvn.w	r3, #992	; 0x3e0
   e75e8:	ebb3 5262 	subs.w	r2, r3, r2, asr #21
   e75ec:	d912      	bls.n	e7614 <__aeabi_d2iz+0x3c>
   e75ee:	ea4f 23c1 	mov.w	r3, r1, lsl #11
   e75f2:	f043 4300 	orr.w	r3, r3, #2147483648	; 0x80000000
   e75f6:	ea43 5350 	orr.w	r3, r3, r0, lsr #21
   e75fa:	f011 4f00 	tst.w	r1, #2147483648	; 0x80000000
   e75fe:	fa23 f002 	lsr.w	r0, r3, r2
   e7602:	bf18      	it	ne
   e7604:	4240      	negne	r0, r0
   e7606:	4770      	bx	lr
   e7608:	f04f 0000 	mov.w	r0, #0
   e760c:	4770      	bx	lr
   e760e:	ea50 3001 	orrs.w	r0, r0, r1, lsl #12
   e7612:	d105      	bne.n	e7620 <__aeabi_d2iz+0x48>
   e7614:	f011 4000 	ands.w	r0, r1, #2147483648	; 0x80000000
   e7618:	bf08      	it	eq
   e761a:	f06f 4000 	mvneq.w	r0, #2147483648	; 0x80000000
   e761e:	4770      	bx	lr
   e7620:	f04f 0000 	mov.w	r0, #0
   e7624:	4770      	bx	lr
   e7626:	bf00      	nop

000e7628 <__aeabi_d2uiz>:
   e7628:	004a      	lsls	r2, r1, #1
   e762a:	d211      	bcs.n	e7650 <__aeabi_d2uiz+0x28>
   e762c:	f512 1200 	adds.w	r2, r2, #2097152	; 0x200000
   e7630:	d211      	bcs.n	e7656 <__aeabi_d2uiz+0x2e>
   e7632:	d50d      	bpl.n	e7650 <__aeabi_d2uiz+0x28>
   e7634:	f46f 7378 	mvn.w	r3, #992	; 0x3e0
   e7638:	ebb3 5262 	subs.w	r2, r3, r2, asr #21
   e763c:	d40e      	bmi.n	e765c <__aeabi_d2uiz+0x34>
   e763e:	ea4f 23c1 	mov.w	r3, r1, lsl #11
   e7642:	f043 4300 	orr.w	r3, r3, #2147483648	; 0x80000000
   e7646:	ea43 5350 	orr.w	r3, r3, r0, lsr #21
   e764a:	fa23 f002 	lsr.w	r0, r3, r2
   e764e:	4770      	bx	lr
   e7650:	f04f 0000 	mov.w	r0, #0
   e7654:	4770      	bx	lr
   e7656:	ea50 3001 	orrs.w	r0, r0, r1, lsl #12
   e765a:	d102      	bne.n	e7662 <__aeabi_d2uiz+0x3a>
   e765c:	f04f 30ff 	mov.w	r0, #4294967295	; 0xffffffff
   e7660:	4770      	bx	lr
   e7662:	f04f 0000 	mov.w	r0, #0
   e7666:	4770      	bx	lr

000e7668 <__aeabi_d2f>:
   e7668:	ea4f 0241 	mov.w	r2, r1, lsl #1
   e766c:	f1b2 43e0 	subs.w	r3, r2, #1879048192	; 0x70000000
   e7670:	bf24      	itt	cs
   e7672:	f5b3 1c00 	subscs.w	ip, r3, #2097152	; 0x200000
   e7676:	f1dc 5cfe 	rsbscs	ip, ip, #532676608	; 0x1fc00000
   e767a:	d90d      	bls.n	e7698 <__aeabi_d2f+0x30>
   e767c:	f001 4c00 	and.w	ip, r1, #2147483648	; 0x80000000
   e7680:	ea4f 02c0 	mov.w	r2, r0, lsl #3
   e7684:	ea4c 7050 	orr.w	r0, ip, r0, lsr #29
   e7688:	f1b2 4f00 	cmp.w	r2, #2147483648	; 0x80000000
   e768c:	eb40 0083 	adc.w	r0, r0, r3, lsl #2
   e7690:	bf08      	it	eq
   e7692:	f020 0001 	biceq.w	r0, r0, #1
   e7696:	4770      	bx	lr
   e7698:	f011 4f80 	tst.w	r1, #1073741824	; 0x40000000
   e769c:	d121      	bne.n	e76e2 <__aeabi_d2f+0x7a>
   e769e:	f113 7238 	adds.w	r2, r3, #48234496	; 0x2e00000
   e76a2:	bfbc      	itt	lt
   e76a4:	f001 4000 	andlt.w	r0, r1, #2147483648	; 0x80000000
   e76a8:	4770      	bxlt	lr
   e76aa:	f441 1180 	orr.w	r1, r1, #1048576	; 0x100000
   e76ae:	ea4f 5252 	mov.w	r2, r2, lsr #21
   e76b2:	f1c2 0218 	rsb	r2, r2, #24
   e76b6:	f1c2 0c20 	rsb	ip, r2, #32
   e76ba:	fa10 f30c 	lsls.w	r3, r0, ip
   e76be:	fa20 f002 	lsr.w	r0, r0, r2
   e76c2:	bf18      	it	ne
   e76c4:	f040 0001 	orrne.w	r0, r0, #1
   e76c8:	ea4f 23c1 	mov.w	r3, r1, lsl #11
   e76cc:	ea4f 23d3 	mov.w	r3, r3, lsr #11
   e76d0:	fa03 fc0c 	lsl.w	ip, r3, ip
   e76d4:	ea40 000c 	orr.w	r0, r0, ip
   e76d8:	fa23 f302 	lsr.w	r3, r3, r2
   e76dc:	ea4f 0343 	mov.w	r3, r3, lsl #1
   e76e0:	e7cc      	b.n	e767c <__aeabi_d2f+0x14>
   e76e2:	ea7f 5362 	mvns.w	r3, r2, asr #21
   e76e6:	d107      	bne.n	e76f8 <__aeabi_d2f+0x90>
   e76e8:	ea50 3301 	orrs.w	r3, r0, r1, lsl #12
   e76ec:	bf1e      	ittt	ne
   e76ee:	f04f 40fe 	movne.w	r0, #2130706432	; 0x7f000000
   e76f2:	f440 0040 	orrne.w	r0, r0, #12582912	; 0xc00000
   e76f6:	4770      	bxne	lr
   e76f8:	f001 4000 	and.w	r0, r1, #2147483648	; 0x80000000
   e76fc:	f040 40fe 	orr.w	r0, r0, #2130706432	; 0x7f000000
   e7700:	f440 0000 	orr.w	r0, r0, #8388608	; 0x800000
   e7704:	4770      	bx	lr
   e7706:	bf00      	nop

000e7708 <__aeabi_d2lz>:
   e7708:	b538      	push	{r3, r4, r5, lr}
   e770a:	2200      	movs	r2, #0
   e770c:	2300      	movs	r3, #0
   e770e:	4604      	mov	r4, r0
   e7710:	460d      	mov	r5, r1
   e7712:	f7ff ff39 	bl	e7588 <__aeabi_dcmplt>
   e7716:	b928      	cbnz	r0, e7724 <__aeabi_d2lz+0x1c>
   e7718:	4620      	mov	r0, r4
   e771a:	4629      	mov	r1, r5
   e771c:	e8bd 4038 	ldmia.w	sp!, {r3, r4, r5, lr}
   e7720:	f000 b80a 	b.w	e7738 <__aeabi_d2ulz>
   e7724:	4620      	mov	r0, r4
   e7726:	f105 4100 	add.w	r1, r5, #2147483648	; 0x80000000
   e772a:	f000 f805 	bl	e7738 <__aeabi_d2ulz>
   e772e:	4240      	negs	r0, r0
   e7730:	eb61 0141 	sbc.w	r1, r1, r1, lsl #1
   e7734:	bd38      	pop	{r3, r4, r5, pc}
   e7736:	bf00      	nop

000e7738 <__aeabi_d2ulz>:
   e7738:	b5d0      	push	{r4, r6, r7, lr}
   e773a:	2200      	movs	r2, #0
   e773c:	4b0e      	ldr	r3, [pc, #56]	; (e7778 <__aeabi_d2ulz+0x40>)
   e773e:	4606      	mov	r6, r0
   e7740:	460f      	mov	r7, r1
   e7742:	f7ff fcaf 	bl	e70a4 <__aeabi_dmul>
   e7746:	f7ff ff6f 	bl	e7628 <__aeabi_d2uiz>
   e774a:	4604      	mov	r4, r0
   e774c:	f7ff fc34 	bl	e6fb8 <__aeabi_ui2d>
   e7750:	2200      	movs	r2, #0
   e7752:	4b0a      	ldr	r3, [pc, #40]	; (e777c <__aeabi_d2ulz+0x44>)
   e7754:	f7ff fca6 	bl	e70a4 <__aeabi_dmul>
   e7758:	4602      	mov	r2, r0
   e775a:	460b      	mov	r3, r1
   e775c:	4630      	mov	r0, r6
   e775e:	4639      	mov	r1, r7
   e7760:	f7ff faec 	bl	e6d3c <__aeabi_dsub>
   e7764:	f7ff ff60 	bl	e7628 <__aeabi_d2uiz>
   e7768:	4623      	mov	r3, r4
   e776a:	2200      	movs	r2, #0
   e776c:	ea42 0200 	orr.w	r2, r2, r0
   e7770:	4610      	mov	r0, r2
   e7772:	4619      	mov	r1, r3
   e7774:	bdd0      	pop	{r4, r6, r7, pc}
   e7776:	bf00      	nop
   e7778:	3df00000 	.word	0x3df00000
   e777c:	41f00000 	.word	0x41f00000

000e7780 <__cxa_atexit>:
   e7780:	b510      	push	{r4, lr}
   e7782:	4c05      	ldr	r4, [pc, #20]	; (e7798 <__cxa_atexit+0x18>)
   e7784:	4613      	mov	r3, r2
   e7786:	b12c      	cbz	r4, e7794 <__cxa_atexit+0x14>
   e7788:	460a      	mov	r2, r1
   e778a:	4601      	mov	r1, r0
   e778c:	2002      	movs	r0, #2
   e778e:	f3af 8000 	nop.w
   e7792:	bd10      	pop	{r4, pc}
   e7794:	4620      	mov	r0, r4
   e7796:	bd10      	pop	{r4, pc}
   e7798:	00000000 	.word	0x00000000

000e779c <exit>:
   e779c:	b508      	push	{r3, lr}
   e779e:	4b07      	ldr	r3, [pc, #28]	; (e77bc <exit+0x20>)
   e77a0:	4604      	mov	r4, r0
   e77a2:	b113      	cbz	r3, e77aa <exit+0xe>
   e77a4:	2100      	movs	r1, #0
   e77a6:	f3af 8000 	nop.w
   e77aa:	4b05      	ldr	r3, [pc, #20]	; (e77c0 <exit+0x24>)
   e77ac:	6818      	ldr	r0, [r3, #0]
   e77ae:	6a83      	ldr	r3, [r0, #40]	; 0x28
   e77b0:	b103      	cbz	r3, e77b4 <exit+0x18>
   e77b2:	4798      	blx	r3
   e77b4:	4620      	mov	r0, r4
   e77b6:	f7ec fc77 	bl	d40a8 <_exit>
   e77ba:	bf00      	nop
   e77bc:	00000000 	.word	0x00000000
   e77c0:	000eb9d8 	.word	0x000eb9d8

000e77c4 <memcmp>:
   e77c4:	b510      	push	{r4, lr}
   e77c6:	3901      	subs	r1, #1
   e77c8:	4402      	add	r2, r0
   e77ca:	4290      	cmp	r0, r2
   e77cc:	d007      	beq.n	e77de <memcmp+0x1a>
   e77ce:	f810 3b01 	ldrb.w	r3, [r0], #1
   e77d2:	f811 4f01 	ldrb.w	r4, [r1, #1]!
   e77d6:	42a3      	cmp	r3, r4
   e77d8:	d0f7      	beq.n	e77ca <memcmp+0x6>
   e77da:	1b18      	subs	r0, r3, r4
   e77dc:	bd10      	pop	{r4, pc}
   e77de:	2000      	movs	r0, #0
   e77e0:	bd10      	pop	{r4, pc}

000e77e2 <memcpy>:
   e77e2:	b510      	push	{r4, lr}
   e77e4:	1e43      	subs	r3, r0, #1
   e77e6:	440a      	add	r2, r1
   e77e8:	4291      	cmp	r1, r2
   e77ea:	d004      	beq.n	e77f6 <memcpy+0x14>
   e77ec:	f811 4b01 	ldrb.w	r4, [r1], #1
   e77f0:	f803 4f01 	strb.w	r4, [r3, #1]!
   e77f4:	e7f8      	b.n	e77e8 <memcpy+0x6>
   e77f6:	bd10      	pop	{r4, pc}

000e77f8 <memset>:
   e77f8:	4402      	add	r2, r0
   e77fa:	4603      	mov	r3, r0
   e77fc:	4293      	cmp	r3, r2
   e77fe:	d002      	beq.n	e7806 <memset+0xe>
   e7800:	f803 1b01 	strb.w	r1, [r3], #1
   e7804:	e7fa      	b.n	e77fc <memset+0x4>
   e7806:	4770      	bx	lr

000e7808 <srand>:
   e7808:	b538      	push	{r3, r4, r5, lr}
   e780a:	4b12      	ldr	r3, [pc, #72]	; (e7854 <srand+0x4c>)
   e780c:	681c      	ldr	r4, [r3, #0]
   e780e:	6ba3      	ldr	r3, [r4, #56]	; 0x38
   e7810:	4605      	mov	r5, r0
   e7812:	b9d3      	cbnz	r3, e784a <srand+0x42>
   e7814:	2018      	movs	r0, #24
   e7816:	f7fc fd79 	bl	e430c <malloc>
   e781a:	f243 330e 	movw	r3, #13070	; 0x330e
   e781e:	63a0      	str	r0, [r4, #56]	; 0x38
   e7820:	8003      	strh	r3, [r0, #0]
   e7822:	f64a 33cd 	movw	r3, #43981	; 0xabcd
   e7826:	8043      	strh	r3, [r0, #2]
   e7828:	f241 2334 	movw	r3, #4660	; 0x1234
   e782c:	8083      	strh	r3, [r0, #4]
   e782e:	f24e 636d 	movw	r3, #58989	; 0xe66d
   e7832:	80c3      	strh	r3, [r0, #6]
   e7834:	f64d 63ec 	movw	r3, #57068	; 0xdeec
   e7838:	8103      	strh	r3, [r0, #8]
   e783a:	2305      	movs	r3, #5
   e783c:	8143      	strh	r3, [r0, #10]
   e783e:	230b      	movs	r3, #11
   e7840:	8183      	strh	r3, [r0, #12]
   e7842:	2201      	movs	r2, #1
   e7844:	2300      	movs	r3, #0
   e7846:	e9c0 2304 	strd	r2, r3, [r0, #16]
   e784a:	6ba3      	ldr	r3, [r4, #56]	; 0x38
   e784c:	2200      	movs	r2, #0
   e784e:	611d      	str	r5, [r3, #16]
   e7850:	615a      	str	r2, [r3, #20]
   e7852:	bd38      	pop	{r3, r4, r5, pc}
   e7854:	2003c2d0 	.word	0x2003c2d0

000e7858 <strcmp>:
   e7858:	f810 2b01 	ldrb.w	r2, [r0], #1
   e785c:	f811 3b01 	ldrb.w	r3, [r1], #1
   e7860:	2a01      	cmp	r2, #1
   e7862:	bf28      	it	cs
   e7864:	429a      	cmpcs	r2, r3
   e7866:	d0f7      	beq.n	e7858 <strcmp>
   e7868:	1ad0      	subs	r0, r2, r3
   e786a:	4770      	bx	lr

000e786c <strlen>:
   e786c:	4603      	mov	r3, r0
   e786e:	f813 2b01 	ldrb.w	r2, [r3], #1
   e7872:	2a00      	cmp	r2, #0
   e7874:	d1fb      	bne.n	e786e <strlen+0x2>
   e7876:	1a18      	subs	r0, r3, r0
   e7878:	3801      	subs	r0, #1
   e787a:	4770      	bx	lr

000e787c <dynalib_user>:
   e787c:	4021 000d 405d 000d 4089 000d 408d 000d     !@..]@...@...@..
   e788c:	0000 0000 7325 203a 656c 676e 6874 253d     ....%s: length=%
   e789c:	2064 005b 6e55 6e6b 776f 206e 7974 6570     d [.Unknown type
   e78ac:	4e00 544f 5059 0045 4c46 414f 3354 0032     .NOTYPE.FLOAT32.
   e78bc:	4e49 3354 0032 4955 544e 0038 4e49 3654     INT32.UINT8.INT6
   e78cc:	0034 5453 4952 474e 4200 4f4f 004c 4e49     4.STRING.BOOL.IN
   e78dc:	3154 0036 4f43 504d 454c 3658 0034 4c46     T16.COMPLEX64.FL
   e78ec:	414f 3154 0036 0000                         OAT16...

000e78f4 <CSWTCH.19>:
   e78f4:	78ad 000e 78b4 000e 78bc 000e 78c2 000e     .x...x...x...x..
   e7904:	78c8 000e 78ce 000e 78d5 000e 78da 000e     .x...x...x...x..
   e7914:	78e0 000e 78c3 000e 78ea 000e               .x...x...x..

000e7920 <kInferencesPerCycle>:
   e7920:	03e8 0000 6f4d 6564 206c 7270 766f 6469     ....Model provid
   e7930:	6465 6920 2073 6373 6568 616d 7620 7265     ed is schema ver
   e7940:	6973 6e6f 2520 2064 6f6e 2074 7165 6175     sion %d not equa
   e7950:	206c 6f74 7320 7075 6f70 7472 6465 7620     l to supported v
   e7960:	7265 6973 6e6f 2520 2e64 4100 6c6c 636f     ersion %d..Alloc
   e7970:	7461 5465 6e65 6f73 7372 2928 6620 6961     ateTensors() fai
   e7980:	656c 0064 6e49 6f76 656b 6620 6961 656c     led.Invoke faile
   e7990:	2064 6e6f 7820 765f 6c61 203a 6625 000a     d on x_val: %f..
   e79a0:	6c46 7461 7542 6666 7265 2073 2e31 3131     FlatBuffers 1.11
   e79b0:	302e 0000                                   .0..

000e79b4 <g_sine_model_data>:
   e79b4:	0018 0000 4654 334c 0000 000e 0018 0004     ....TFL3........
   e79c4:	0008 000c 0010 0014 000e 0000 0003 0000     ................
   e79d4:	0a10 0000 05b8 0000 05a0 0000 0004 0000     ................
   e79e4:	000b 0000 0590 0000 057c 0000 0524 0000     ........|...$...
   e79f4:	04d4 0000 00c4 0000 0074 0000 0024 0000     ........t...$...
   e7a04:	001c 0000 0014 0000 000c 0000 0004 0000     ................
   e7a14:	f654 ffff f658 ffff f65c ffff f660 ffff     T...X...\...`...
   e7a24:	fac2 ffff 0004 0000 0040 0000 197c 3ea7     ........@...|..>
   e7a34:	8199 3eb9 8b56 3e9f d888 bf12 1074 3e56     ...>V..>....t.V>
   e7a44:	c6fe bedf 10f2 be5a e2f0 be0a 5a10 be98     ......Z......Z..
   e7a54:	36b9 3dce 7f8f 3e87 b12c bdfd a6e6 be8a     .6.=...>,.......
   e7a64:	3ea5 3eda 3450 bded 9190 be69 fb0e ffff     .>.>P4....i.....
   e7a74:	0004 0000 0040 0000 4167 bf48 cd24 bea0     ....@...gAH.$...
   e7a84:	92b7 bf0c 0000 0000 fe98 3f3c 0000 0000     ..........<?....
	...
   e7aa0:	174a be9a cb41 beb6 0000 0000 0000 0000     J...A...........
   e7ab0:	d613 3e1e 0000 0000 0000 0000 fb5a ffff     ...>........Z...
   e7ac0:	0004 0000 0400 0000 984b bddd 6b40 becb     ........K...@k..
   e7ad0:	0c36 3cd4 44bd 3eb5 7095 3ee3 ace7 3e86     6..<.D.>.p.>...>
   e7ae0:	c400 3d4e a67e 3e1d 87bd 3ebb b8b4 bf09     ..N=~..>...>....
   e7af0:	1fa1 bef8 908d 3edd fade be6f 75b2 3de4     .......>..o..u.=
   e7b00:	fe6e 3e36 1820 bec2 c739 befb a4fe be30     n.6> ...9.....0.
   e7b10:	91f7 bede abde 3e24 bbfb 3ece 23eb be80     ......$>...>.#..
   e7b20:	587b be73 2e9a 3e03 4210 bca9 1210 bd64     {Xs....>.B....d.
   e7b30:	8de3 3d0c 489e be97 5134 bed4 3b02 3e0d     ...=.H..4Q...;.>
   e7b40:	6762 be89 df74 3da2 25f3 beb3 34ef 3d7b     bg..t..=.%...4{=
   e7b50:	7061 3de3 76ba bec0 e97d 3ea7 abc3 bed0     ap.=.v..}..>....
   e7b60:	7ccf bedb 2770 be9a f598 bd3c 4bff 3e4b     .|..p'....<..KK>
   e7b70:	a07e bdf8 6ed4 3d86 4a00 3a07 244c be61     ~....n.=.J.:L$a.
   e7b80:	6854 bdf7 3f02 be77 7923 3eb3 831c bdad     Th...?w.#y.>....
   e7b90:	92c8 3e8d f3a8 bd15 4de6 3d6c e7ac be98     ...>.....Ml=....
   e7ba0:	ec81 3ebd 55e2 3e73 77c1 3ec7 1b6e 3d5e     ...>.Us>.w.>n.^=
   e7bb0:	7827 3f02 21d4 3d90 dc52 3e1f dabf 3e88     'x.?.!.=R..>...>
   e7bc0:	7980 bde3 6f40 be10 4320 bd2e 76f0 bdc5     .y..@o.. C...v..
   e7bd0:	a0cc be04 69f0 bed7 feb1 be64 4120 be84     .....i....d. A..
   e7be0:	c3b2 be26 f4d8 be09 4464 3dd1 e1d5 bec8     ..&.....dD.=....
   e7bf0:	bc35 be3f 94c0 3d82 2bdc bdb1 db02 bebf     5.?....=.+......
   e7c00:	7fa5 3e8a b421 3ea2 86cd bf56 3b9c bc76     ...>!..>..V..;v.
   e7c10:	6d85 bf60 0086 be3c 23c1 3e7e cd96 3e3f     .m`...<..#~>..?>
   e7c20:	9186 3e2d ef55 3e87 977e be03 cd2a 3e01     ..->U..>~...*..>
   e7c30:	c932 be8e 7772 be3b a1e0 bebc b78d 3ea7     2...rw;........>
   e7c40:	051c be95 1ff7 3ebb 3ec9 3ed6 4280 bde9     .......>.>.>.B..
   e7c50:	0c27 bed2 325c be34 cb14 bdca 3add be67     '...\24......:g.
   e7c60:	bb1c be8d ac91 be5c 4052 be6f 71d7 3e94     ......\.R@o..q.>
   e7c70:	7118 be09 299b bed9 667d bed2 d698 beb2     .q...)..}f......
   e7c80:	c900 3a84 dabc bdc2 c21d bf1b ddd4 3e92     ...:...........>
   e7c90:	8707 be6c c240 be3b e2bd 3e9c b50a bea0     ..l.@.;....>....
   e7ca0:	d5e2 be9c bb3e 3e7c b417 3ecf 8ed5 bec8     ....>.|>...>....
   e7cb0:	f97c 3e5c fc80 3d0d d5c5 3e8b 17f5 3ea2     |.\>...=...>...>
   e7cc0:	60c7 be89 95ec 3d87 c27a bf5d 9477 3e98     .`.....=z.].w..>
   e7cd0:	3977 bc07 2942 3e00 d0af 3ea9 2331 bec4     w9..B).>...>1#..
   e7ce0:	3695 be5b dcc7 be83 6b1e 3e47 245b 3e99     .6[......kG>[$.>
   e7cf0:	2799 3e54 20c8 bddd 865a 3e2f f080 be69     .'T>. ..Z./>..i.
   e7d00:	fc44 bd84 a082 be2a e687 3e2a 34d8 3dae     D.....*...*>.4.=
   e7d10:	bd50 3eb5 8cc4 be88 bce3 3ea5 daa9 3e9e     P..>.......>...>
   e7d20:	b83e be23 9080 3d15 3f97 3ec3 5cca 3e9d     >.#....=.?.>.\.>
   e7d30:	e821 3ee1 49c0 bc01 0b00 bd88 f73f 3cca     !..>.I......?..<
   e7d40:	5afb 3eb1 d260 3c0d 23ce bf78 4f8f beb9     .Z.>`..<.#x..O..
   e7d50:	6a69 bf34 5e4b 3ea9 8c64 3ed9 7752 3e36     ij4.K^.>d..>Rw6>
   e7d60:	afeb 3ebe be40 3c36 6508 bd3b e055 bd66     ...>@.6<.e;.U.f.
   e7d70:	e8d2 be9b e386 be09 3d93 3edd 660f 3f18     .........=.>.f.?
   e7d80:	0518 bd33 15de bed7 cfaa be49 a5a2 3e64     ..3.......I...d>
   e7d90:	9ce6 be42 4254 3dcc bda0 be9d 69c2 3e48     ..B.TB.=.....iH>
   e7da0:	8b5b bea2 13c0 3d87 fd36 3e69 8605 be40     [......=6.i>..@.
   e7db0:	7a1e bece 1346 bea7 5268 be86 9e04 bd86     .z..F...hR......
   e7dc0:	548c 3dc1 3be0 3cad 6742 bd85 97ea 3e42     .T.=.;.<Bg....B>
   e7dd0:	136e bf3b 5b56 3e16 abaa 3edf 41c8 3d36     n.;.V[.>...>.A6=
   e7de0:	2d24 be47 a577 3eae c2c0 3c5b acac 3e4e     $-G.w..>..[<..N>
   e7df0:	ec99 be13 abf2 3e73 a1aa be48 d3e8 be01     ......s>..H.....
   e7e00:	b760 bdc7 7264 3dd3 d383 3e99 760c be34     `...dr.=...>.v4.
   e7e10:	da42 3e0d 47fb 3e9a dc8b be92 7f56 3e6b     B..>.G.>....V.k>
   e7e20:	d404 bd88 9e11 3e80 893c 3dff 3eb3 3e88     .......><..=.>.>
   e7e30:	f0f7 3e88 fb28 bec9 3e53 3ecf 75ac bedc     ...>(...S>.>.u..
   e7e40:	cadd 3ed7 5801 3ea7 b829 bf13 8176 bc12     ...>.X.>)...v...
   e7e50:	8b28 bf16 ec0e 3e0e 0a40 bddb ec98 bdbf     (......>@.......
   e7e60:	5532 be0c f9fb 3ec9 4a83 be6d 5976 bee2     2U.....>.Jm.vY..
   e7e70:	7d54 bb9f e89d 3e95 d35c 3dd0 8a19 3eb0     T}.....>\..=...>
   e7e80:	6fde be2e 16d0 3d83 7d9c bf11 cc2b 3c25     .o.....=.}..+.%<
   e7e90:	a52a be27 1422 bec7 7a5e 3eac 414e be94     *.'."...^z.>NA..
   e7ea0:	685a 3e7b fd86 3e4e 56a2 be6a feca be81     Zh{>..N>.Vj.....
   e7eb0:	c343 bdb1 b8c5 3ea7 2355 3ecd 2eaf 3e76     C......>U#.>..v>
   e7ec0:	a869 be90 ba0d 3eb9 ff66 ffff 0004 0000     i......>f.......
   e7ed0:	0040 0000 d653 3de2 b666 3ecc e703 3ef6     @...S..=f..>...>
   e7ee0:	28e0 bf10 0000 0000 3d3e 3eb0 0000 0000     .(......>=.>....
   e7ef0:	f062 3e77 9da6 3ea4 4b3a bef3 9e71 3ea7     b.w>...>:K..q..>
   e7f00:	0000 0000 3934 3ea2 0000 0000 9ccc 3e4a     ....49.>......J>
   e7f10:	40ab 3ea3 ffb2 ffff 0004 0000 0040 0000     .@.>........@...
   e7f20:	71b3 3f67 7a9a bf95 48e1 bee8 728a 3e96     .qg?.z...H...r.>
   e7f30:	d200 bbd3 c51a 3fd7 7eac bec8 a790 be95     .......?.~......
   e7f40:	d73b bedc a841 3f16 5b50 3fcb b952 beed     ;...A..?P[.?R...
   e7f50:	a72e bec6 0faf bf14 dab3 3f59 ec02 bed7     ..........Y?....
   e7f60:	0000 0006 0008 0004 0006 0000 0004 0000     ................
   e7f70:	0004 0000 1166 bf1f fbb8 ffff 000f 0000     ....f...........
   e7f80:	4f54 4f43 4320 6e6f 6576 7472 6465 002e     TOCO Converted..
   e7f90:	0001 0000 0010 0000 000c 0014 0004 0008     ................
   e7fa0:	000c 0010 000c 0000 00f0 0000 00e4 0000     ................
   e7fb0:	00d8 0000 0004 0000 0003 0000 0090 0000     ................
   e7fc0:	0048 0000 0004 0000 ffce ffff 0000 0800     H...............
   e7fd0:	0018 0000 000c 0000 0004 0000 fc1c ffff     ................
   e7fe0:	0001 0000 0000 0000 0003 0000 0007 0000     ................
   e7ff0:	0008 0000 0009 0000 0000 000e 0014 0000     ................
   e8000:	0008 000c 0007 0010 000e 0000 0000 0800     ................
   e8010:	001c 0000 0010 0000 0004 0000 ffba ffff     ................
   e8020:	0000 0100 0001 0000 0007 0000 0003 0000     ................
   e8030:	0004 0000 0005 0000 0006 0000 0000 000e     ................
   e8040:	0016 0000 0008 000c 0007 0010 000e 0000     ................
   e8050:	0000 0800 0024 0000 0018 0000 000c 0000     ....$...........
   e8060:	0000 0006 0008 0007 0006 0000 0000 0100     ................
   e8070:	0001 0000 0004 0000 0003 0000 0001 0000     ................
   e8080:	0002 0000 0003 0000 0001 0000 0000 0000     ................
   e8090:	0001 0000 0001 0000 000a 0000 0310 0000     ................
   e80a0:	02a4 0000 0240 0000 01f4 0000 01ac 0000     ....@...........
   e80b0:	0148 0000 00fc 0000 00b4 0000 0050 0000     H...........P...
   e80c0:	0004 0000 fd26 ffff 003c 0000 0001 0000     ....&...<.......
   e80d0:	000c 0000 0004 0000 fd18 ffff 0020 0000     ............ ...
   e80e0:	6573 7571 6e65 6974 6c61 315f 642f 6e65     sequential_1/den
   e80f0:	6573 345f 4d2f 7461 754d 5f6c 6962 7361     se_4/MatMul_bias
   e8100:	0000 0000 0001 0000 0001 0000 fd6e ffff     ............n...
   e8110:	0050 0000 0002 0000 000c 0000 0004 0000     P...............
   e8120:	fd60 ffff 0034 0000 6573 7571 6e65 6974     `...4...sequenti
   e8130:	6c61 315f 642f 6e65 6573 345f 4d2f 7461     al_1/dense_4/Mat
   e8140:	754d 2f6c 6552 6461 6156 6972 6261 656c     Mul/ReadVariable
   e8150:	704f 742f 6172 736e 6f70 6573 0000 0000     Op/transpose....
   e8160:	0002 0000 0001 0000 0010 0000 fdce ffff     ................
   e8170:	0034 0000 0008 0000 000c 0000 0004 0000     4...............
   e8180:	fdc0 ffff 0019 0000 6573 7571 6e65 6974     ........sequenti
   e8190:	6c61 315f 642f 6e65 6573 335f 522f 6c65     al_1/dense_3/Rel
   e81a0:	0075 0000 0002 0000 0001 0000 0010 0000     u...............
   e81b0:	fe12 ffff 003c 0000 0003 0000 000c 0000     ....<...........
   e81c0:	0004 0000 fe04 ffff 0020 0000 6573 7571     ........ ...sequ
   e81d0:	6e65 6974 6c61 315f 642f 6e65 6573 335f     ential_1/dense_3
   e81e0:	4d2f 7461 754d 5f6c 6962 7361 0000 0000     /MatMul_bias....
   e81f0:	0001 0000 0010 0000 fe5a ffff 0050 0000     ........Z...P...
   e8200:	0004 0000 000c 0000 0004 0000 fe4c ffff     ............L...
   e8210:	0034 0000 6573 7571 6e65 6974 6c61 315f     4...sequential_1
   e8220:	642f 6e65 6573 335f 4d2f 7461 754d 2f6c     /dense_3/MatMul/
   e8230:	6552 6461 6156 6972 6261 656c 704f 742f     ReadVariableOp/t
   e8240:	6172 736e 6f70 6573 0000 0000 0002 0000     ranspose........
   e8250:	0010 0000 0010 0000 feba ffff 0034 0000     ............4...
   e8260:	000a 0000 000c 0000 0004 0000 feac ffff     ................
   e8270:	0019 0000 6573 7571 6e65 6974 6c61 315f     ....sequential_1
   e8280:	642f 6e65 6573 325f 522f 6c65 0075 0000     /dense_2/Relu...
   e8290:	0002 0000 0001 0000 0010 0000 fefe ffff     ................
   e82a0:	003c 0000 0005 0000 000c 0000 0004 0000     <...............
   e82b0:	fef0 ffff 0020 0000 6573 7571 6e65 6974     .... ...sequenti
   e82c0:	6c61 315f 642f 6e65 6573 325f 4d2f 7461     al_1/dense_2/Mat
   e82d0:	754d 5f6c 6962 7361 0000 0000 0001 0000     Mul_bias........
   e82e0:	0010 0000 ff46 ffff 0050 0000 0006 0000     ....F...P.......
   e82f0:	000c 0000 0004 0000 ff38 ffff 0034 0000     ........8...4...
   e8300:	6573 7571 6e65 6974 6c61 315f 642f 6e65     sequential_1/den
   e8310:	6573 325f 4d2f 7461 754d 2f6c 6552 6461     se_2/MatMul/Read
   e8320:	6156 6972 6261 656c 704f 742f 6172 736e     VariableOp/trans
   e8330:	6f70 6573 0000 0000 0002 0000 0010 0000     pose............
   e8340:	0001 0000 ffa6 ffff 0048 0000 0009 0000     ........H.......
   e8350:	002c 0000 000c 0000 0008 000c 0004 0008     ,...............
   e8360:	0008 0000 0010 0000 0004 0000 0001 0000     ................
   e8370:	0000 437f 0001 0000 0000 0000 000d 0000     ...C............
   e8380:	6564 736e 5f65 5f32 6e69 7570 0074 0000     dense_2_input...
   e8390:	0002 0000 0001 0000 0001 0000 0000 000e     ................
   e83a0:	0014 0004 0000 0008 000c 0010 000e 0000     ................
   e83b0:	0028 0000 0007 0000 0010 0000 0008 0000     (...............
   e83c0:	0004 0004 0004 0000 0008 0000 6449 6e65     ............Iden
   e83d0:	6974 7974 0000 0000 0002 0000 0001 0000     tity............
   e83e0:	0001 0000 0001 0000 0010 0000 0000 000a     ................
   e83f0:	000c 0007 0000 0008 000a 0000 0000 0900     ................
   e8400:	0003 0000                                   ....

000e8404 <_ZZNK11flatbuffers6VectorIlE3GetEmE19__PRETTY_FUNCTION__>:
   e8404:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
   e8414:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
   e8424:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
   e8434:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
   e8444:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
   e8454:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
   e8464:	7469 2068 2054 203d 6f6c 676e 6920 746e     ith T = long int
   e8474:	203b 6c66 7461 7562 6666 7265 3a73 563a     ; flatbuffers::V
   e8484:	6365 6f74 3c72 3e54 3a3a 6572 7574 6e72     ector<T>::return
   e8494:	745f 7079 2065 203d 6f6c 676e 6920 746e     _type = long int
   e84a4:	203b 6c66 7461 7562 6666 7265 3a73 753a     ; flatbuffers::u
   e84b4:	666f 7366 7465 745f 3d20 6c20 6e6f 2067     offset_t = long 
   e84c4:	6e75 6973 6e67 6465 6920 746e 005d 6e49     unsigned int].In
   e84d4:	7570 2074 7261 6172 2079 6f6e 2074 7270     put array not pr
   e84e4:	766f 6469 6465 6620 726f 6f20 6570 6172     ovided for opera
   e84f4:	6974 6e6f 2720 7325 2e27 000a 6f46 6e75     tion '%s'...Foun
   e8504:	2064 6f74 206f 616d 796e 6420 6d69 6e65     d too many dimen
   e8514:	6973 6e6f 2073 6e69 7420 6568 6920 706e     sions in the inp
   e8524:	7475 6120 7272 7961 6f20 2066 706f 7265     ut array of oper
   e8534:	7461 6f69 206e 2527 2773 0a2e 6900 3c20     ation '%s'...i <
   e8544:	7320 7a69 2865 0029 552f 6573 7372 622f      size()./Users/b
   e8554:	6173 7274 6d6f 442f 7665 6c65 706f 656d     satrom/Developme
   e8564:	746e 702f 7261 6974 6c63 2f65 696c 7262     nt/particle/libr
   e8574:	7261 6569 2f73 6150 7472 6369 656c 545f     aries/Particle_T
   e8584:	6e65 6f73 4672 6f6c 4c77 7469 5f65 7845     ensorFlowLite_Ex
   e8594:	6d61 6c70 7365 682f 6c65 6f6c 775f 726f     amples/hello_wor
   e85a4:	646c 6c2f 6269 542f 6e65 6f73 4672 6f6c     ld/lib/TensorFlo
   e85b4:	4c77 7469 2f65 7273 2f63 6874 7269 5f64     wLite/src/third_
   e85c4:	6170 7472 2f79 6c66 7461 7562 6666 7265     party/flatbuffer
   e85d4:	2f73 6e69 6c63 6475 2f65 6c66 7461 7562     s/include/flatbu
   e85e4:	6666 7265 2f73 6c66 7461 7562 6666 7265     ffers/flatbuffer
   e85f4:	2e73 0068 6e55 7573 7070 726f 6574 2064     s.h.Unsupported 
   e8604:	6164 6174 7420 7079 2065 6425 6920 206e     data type %d in 
   e8614:	6574 736e 726f 000a 6e55 6168 646e 656c     tensor..Unhandle
   e8624:	2064 7566 6c6c 2d79 6f63 6e6e 6365 6574     d fully-connecte
   e8634:	2064 6577 6769 7468 2073 6f66 6d72 7461     d weights format
   e8644:	002e 6e55 6168 646e 656c 2064 534c 4d54     ..Unhandled LSTM
   e8654:	6b20 7265 656e 206c 7974 6570 203a 6425      kernel type: %d
   e8664:	4e00 206f 6176 696c 2064 534c 4d54 6220     .No valid LSTM b
   e8674:	6975 746c 6e69 6f20 7470 6f69 736e 6520     uiltin options e
   e8684:	6978 7473 7200 7365 6168 6570 7300 7571     xist.reshape.squ
   e8694:	6565 657a 4400 4c45 4745 5441 2045 706f     eeze.DELEGATE op
   e86a4:	7320 6f68 6c75 6e64 7427 6520 6978 7473      shouldn't exist
   e86b4:	6920 206e 6f6d 6564 2e6c 0100                     in model..

000e86bf <CSWTCH.73>:
   e86bf:	0201 0403 7005                                   .....

000e86c4 <_ZZN6tflite24EnumNamesBuiltinOperatorEvE5names>:
   e86c4:	8970 000e 8974 000e 8984 000e 899c 000e     p...t...........
   e86d4:	8992 000e 89a4 000e 89b3 000e 89be 000e     ................
   e86e4:	89cf 000e 89d5 000e 89e5 000e 89f6 000e     ................
   e86f4:	8a07 000e 8a12 000e 8a2f 000e 8a38 000e     ......../...8...
   e8704:	8bc1 000e 8a47 000e 8a53 000e 8bcc 000e     ....G...S.......
   e8714:	8a57 000e 8a64 000e 8a6a 000e 8a72 000e     W...d...j...r...
   e8724:	8b7f 000e 8b99 000e 8a82 000e 8a91 000e     ................
   e8734:	8a96 000e 8a9b 000e 8aad 000e 8ab7 000e     ................
   e8744:	8abc 000e 8ac3 000e 8d50 000e 8adb 000e     ........P.......
   e8754:	8af7 000e 8afe 000e 8b10 000e 8b22 000e     ............"...
   e8764:	8b2c 000e 8b31 000e 8cdd 000e 8b35 000e     ,...1.......5...
   e8774:	8b3d 000e 8b5a 000e 8b68 000e 8b83 000e     =...Z...h.......
   e8784:	8b87 000e 8b8f 000e 8b95 000e 8ba1 000e     ................
   e8794:	8baa 000e 8bc6 000e 8bcb 000e 8bd1 000e     ................
   e87a4:	8bd9 000e 8be1 000e 8be9 000e 8bee 000e     ................
   e87b4:	8bf2 000e 8bf8 000e 8c00 000e 8c0e 000e     ................
   e87c4:	8c19 000e 8b62 000e 8c20 000e 8c24 000e     ....b... ...$...
   e87d4:	8c33 000e 8c43 000e 8c48 000e 8c08 000e     3...C...H.......
   e87e4:	8c54 000e 8c5e 000e 8c62 000e 8c67 000e     T...^...b...g...
   e87f4:	8c66 000e 8a6c 000e 8c6c 000e 8c70 000e     f...l...l...p...
   e8804:	8c78 000e 8c83 000e 8c8f 000e 8cc7 000e     x...............
   e8814:	8c9a 000e 8ca5 000e 8cad 000e 8cb9 000e     ................
   e8824:	8cc5 000e 8ccc 000e 8cd7 000e 8ce1 000e     ................
   e8834:	8cec 000e 8cf3 000e 8cfe 000e 8d03 000e     ................
   e8844:	8d0d 000e 8d13 000e 8d2b 000e 8d36 000e     ........+...6...
   e8854:	8d49 000e 8d54 000e 8d58 000e 8d60 000e     I...T...X...`...
   e8864:	8d67 000e 8d6c 000e 8d77 000e 8d7d 000e     g...l...w...}...
   e8874:	8d87 000e 8d8b 000e 8d91 000e 8bcd 000e     ................
   e8884:	8d96 000e 8da7 000e 89b5 000e 8db3 000e     ................
   e8894:	8dc3 000e 8dc9 000e 8dd4 000e 8dd7 000e     ................
   e88a4:	8ddd 000e 8df4 000e 0000 0000 704f 6220     ............Op b
   e88b4:	6975 746c 6e69 635f 646f 2065 756f 2074     uiltin_code out 
   e88c4:	666f 7220 6e61 6567 203a 6425 202e 7241     of range: %d. Ar
   e88d4:	2065 6f79 2075 7375 6e69 2067 6c6f 2064     e you using old 
   e88e4:	4654 694c 6574 6220 6e69 7261 2079 6977     TFLite binary wi
   e88f4:	6874 6e20 7765 7265 6d20 646f 6c65 003f     th newer model?.
   e8904:	6944 6e64 7427 6620 6e69 2064 706f 6620     Didn't find op f
   e8914:	726f 6220 6975 746c 6e69 6f20 6370 646f     or builtin opcod
   e8924:	2065 2527 2773 7620 7265 6973 6e6f 2720     e '%s' version '
   e8934:	6425 0a27 4f00 6570 6172 6f74 2072 6977     %d'..Operator wi
   e8944:	6874 4320 5355 4f54 204d 7562 6c69 6974     th CUSTOM builti
   e8954:	5f6e 6f63 6564 6820 7361 6e20 206f 7563     n_code has no cu
   e8964:	7473 6d6f 635f 646f 2e65 000a 4441 0044     stom_code...ADD.
   e8974:	5641 5245 4741 5f45 4f50 4c4f 325f 0044     AVERAGE_POOL_2D.
   e8984:	4f43 434e 5441 4e45 5441 4f49 004e 4544     CONCATENATION.DE
   e8994:	5450 5748 5349 5f45 4f43 564e 325f 0044     PTHWISE_CONV_2D.
   e89a4:	4544 5450 5f48 4f54 535f 4150 4543 4400     DEPTH_TO_SPACE.D
   e89b4:	5145 4155 544e 5a49 0045 4d45 4542 4444     EQUANTIZE.EMBEDD
   e89c4:	4e49 5f47 4f4c 4b4f 5055 4600 4f4c 524f     ING_LOOKUP.FLOOR
   e89d4:	4600 4c55 594c 435f 4e4f 454e 5443 4445     .FULLY_CONNECTED
   e89e4:	4800 5341 5448 4241 454c 4c5f 4f4f 554b     .HASHTABLE_LOOKU
   e89f4:	0050 324c 4e5f 524f 414d 494c 415a 4954     P.L2_NORMALIZATI
   e8a04:	4e4f 4c00 5f32 4f50 4c4f 325f 0044 4f4c     ON.L2_POOL_2D.LO
   e8a14:	4143 5f4c 4552 5053 4e4f 4553 4e5f 524f     CAL_RESPONSE_NOR
   e8a24:	414d 494c 415a 4954 4e4f 4c00 474f 5349     MALIZATION.LOGIS
   e8a34:	4954 0043 534c 5f48 5250 4a4f 4345 4954     TIC.LSH_PROJECTI
   e8a44:	4e4f 4d00 5841 505f 4f4f 5f4c 4432 4d00     ON.MAX_POOL_2D.M
   e8a54:	4c55 5200 4c45 5f55 314e 545f 5f4f 0031     UL.RELU_N1_TO_1.
   e8a64:	4552 554c 0036 4552 4853 5041 0045 4552     RELU6.RESHAPE.RE
   e8a74:	4953 455a 425f 4c49 4e49 4145 0052 5053     SIZE_BILINEAR.SP
   e8a84:	4341 5f45 4f54 445f 5045 4854 5300 4456     ACE_TO_DEPTH.SVD
   e8a94:	0046 4154 484e 4300 4e4f 4143 5f54 4d45     F.TANH.CONCAT_EM
   e8aa4:	4542 4444 4e49 5347 5300 494b 5f50 5247     BEDDINGS.SKIP_GR
   e8ab4:	4d41 4300 4c41 004c 5543 5453 4d4f 4500     AM.CALL.CUSTOM.E
   e8ac4:	424d 4445 4944 474e 4c5f 4f4f 554b 5f50     MBEDDING_LOOKUP_
   e8ad4:	5053 5241 4553 5500 494e 4944 4552 5443     SPARSE.UNIDIRECT
   e8ae4:	4f49 414e 5f4c 4553 5551 4e45 4543 525f     IONAL_SEQUENCE_R
   e8af4:	4e4e 4700 5441 4548 0052 4142 4354 5f48     NN.GATHER.BATCH_
   e8b04:	4f54 535f 4150 4543 4e5f 0044 5053 4341     TO_SPACE_ND.SPAC
   e8b14:	5f45 4f54 425f 5441 4843 4e5f 0044 5254     E_TO_BATCH_ND.TR
   e8b24:	4e41 5053 534f 0045 454d 4e41 5300 4255     ANSPOSE.MEAN.SUB
   e8b34:	5300 5551 4545 455a 5500 494e 4944 4552     .SQUEEZE.UNIDIRE
   e8b44:	5443 4f49 414e 5f4c 4553 5551 4e45 4543     CTIONAL_SEQUENCE
   e8b54:	4c5f 5453 004d 5453 4952 4544 5f44 4c53     _LSTM.STRIDED_SL
   e8b64:	4349 0045 4942 4944 4552 5443 4f49 414e     ICE.BIDIRECTIONA
   e8b74:	5f4c 4553 5551 4e45 4543 525f 4e4e 4500     L_SEQUENCE_RNN.E
   e8b84:	5058 5400 504f 5f4b 3256 5300 4c50 5449     XP.TOPK_V2.SPLIT
   e8b94:	4c00 474f 535f 464f 4d54 5841 4400 4c45     .LOG_SOFTMAX.DEL
   e8ba4:	4745 5441 0045 4942 4944 4552 5443 4f49     EGATE.BIDIRECTIO
   e8bb4:	414e 5f4c 4553 5551 4e45 4543 4c5f 5453     NAL_SEQUENCE_LST
   e8bc4:	004d 4143 5453 5000 4552 554c 4d00 5841     M.CAST.PRELU.MAX
   e8bd4:	4d49 4d55 4100 4752 4d5f 5841 4d00 4e49     IMUM.ARG_MAX.MIN
   e8be4:	4d49 4d55 4c00 5345 0053 454e 0047 4150     IMUM.LESS.NEG.PA
   e8bf4:	5644 0032 5247 4145 4554 0052 5247 4145     DV2.GREATER.GREA
   e8c04:	4554 5f52 5145 4155 004c 454c 5353 455f     TER_EQUAL.LESS_E
   e8c14:	5551 4c41 5300 4c45 4345 0054 4953 004e     QUAL.SELECT.SIN.
   e8c24:	5254 4e41 5053 534f 5f45 4f43 564e 5300     TRANSPOSE_CONV.S
   e8c34:	4150 5352 5f45 4f54 445f 4e45 4553 5400     PARSE_TO_DENSE.T
   e8c44:	4c49 0045 5845 4150 444e 445f 4d49 0053     ILE.EXPAND_DIMS.
   e8c54:	4f4e 5f54 5145 4155 004c 4f4c 0047 5553     NOT_EQUAL.LOG.SU
   e8c64:	004d 5352 5251 0054 4f50 0057 5241 5f47     M.RSQRT.POW.ARG_
   e8c74:	494d 004e 4146 454b 515f 4155 544e 5200     MIN.FAKE_QUANT.R
   e8c84:	4445 4355 5f45 5250 444f 5200 4445 4355     EDUCE_PROD.REDUC
   e8c94:	5f45 414d 0058 4f4c 4947 4143 5f4c 524f     E_MAX.LOGICAL_OR
   e8ca4:	4f00 454e 485f 544f 4c00 474f 4349 4c41     .ONE_HOT.LOGICAL
   e8cb4:	415f 444e 4c00 474f 4349 4c41 4e5f 544f     _AND.LOGICAL_NOT
   e8cc4:	5500 504e 4341 004b 4552 5544 4543 4d5f     .UNPACK.REDUCE_M
   e8cd4:	4e49 4600 4f4c 524f 445f 5649 5200 4445     IN.FLOOR_DIV.RED
   e8ce4:	4355 5f45 4e41 0059 5153 4155 4552 5a00     UCE_ANY.SQUARE.Z
   e8cf4:	5245 534f 4c5f 4b49 0045 4946 4c4c 4600     EROS_LIKE.FILL.F
   e8d04:	4f4c 524f 4d5f 444f 5200 4e41 4547 5200     LOOR_MOD.RANGE.R
   e8d14:	5345 5a49 5f45 454e 5241 5345 5f54 454e     ESIZE_NEAREST_NE
   e8d24:	4749 4248 524f 4c00 4145 594b 525f 4c45     IGHBOR.LEAKY_REL
   e8d34:	0055 5153 4155 4552 5f44 4944 4646 5245     U.SQUARED_DIFFER
   e8d44:	4e45 4543 4d00 5249 4f52 5f52 4150 0044     ENCE.MIRROR_PAD.
   e8d54:	4241 0053 5053 494c 5f54 0056 4e55 5149     ABS.SPLIT_V.UNIQ
   e8d64:	4555 4300 4945 004c 4552 4556 5352 5f45     UE.CEIL.REVERSE_
   e8d74:	3256 4100 4444 4e5f 4700 5441 4548 5f52     V2.ADD_N.GATHER_
   e8d84:	444e 4300 534f 5700 4548 4552 5200 4e41     ND.COS.WHERE.RAN
   e8d94:	004b 4552 4556 5352 5f45 4553 5551 4e45     K.REVERSE_SEQUEN
   e8da4:	4543 4d00 5441 4952 5f58 4944 4741 4d00     CE.MATRIX_DIAG.M
   e8db4:	5441 4952 5f58 4553 5f54 4944 4741 5200     ATRIX_SET_DIAG.R
   e8dc4:	554f 444e 4800 5241 5f44 5753 5349 0048     OUND.HARD_SWISH.
   e8dd4:	4649 5700 4948 454c 4e00 4e4f 4d5f 5841     IF.WHILE.NON_MAX
   e8de4:	535f 5055 5250 5345 4953 4e4f 565f 0034     _SUPPRESSION_V4.
   e8df4:	4f4e 5f4e 414d 5f58 5553 5050 4552 5353     NON_MAX_SUPPRESS
   e8e04:	4f49 5f4e 3556 0300 0804 0d0b 110e 1312     ION_V5..........
   e8e14:	1514 1716 6e49 0066 614e 004e 322a 005e     ....Inf.NaN.*2^.
   e8e24:	7954 6570 2520 2073 2528 2964 6e20 746f     Type %s (%d) not
   e8e34:	6920 2073 6f6e 2074 7573 7070 726f 6574      is not supporte
   e8e44:	0064                                        d.

000e8e46 <_ZZNK11flatbuffers6VectorIlE3GetEmE19__PRETTY_FUNCTION__>:
   e8e46:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
   e8e56:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
   e8e66:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
   e8e76:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
   e8e86:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
   e8e96:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
   e8ea6:	7469 2068 2054 203d 6f6c 676e 6920 746e     ith T = long int
   e8eb6:	203b 6c66 7461 7562 6666 7265 3a73 563a     ; flatbuffers::V
   e8ec6:	6365 6f74 3c72 3e54 3a3a 6572 7574 6e72     ector<T>::return
   e8ed6:	745f 7079 2065 203d 6f6c 676e 6920 746e     _type = long int
   e8ee6:	203b 6c66 7461 7562 6666 7265 3a73 753a     ; flatbuffers::u
   e8ef6:	666f 7366 7465 745f 3d20 6c20 6e6f 2067     offset_t = long 
   e8f06:	6e75 6973 6e67 6465 6920 746e 005d          unsigned int].

000e8f14 <_ZZNK11flatbuffers6VectorINS_6OffsetIN6tflite8OperatorEEEE3GetEmE19__PRETTY_FUNCTION__>:
   e8f14:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
   e8f24:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
   e8f34:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
   e8f44:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
   e8f54:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
   e8f64:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
   e8f74:	7469 2068 2054 203d 6c66 7461 7562 6666     ith T = flatbuff
   e8f84:	7265 3a73 4f3a 6666 6573 3c74 6674 696c     ers::Offset<tfli
   e8f94:	6574 3a3a 704f 7265 7461 726f 3b3e 6620     te::Operator>; f
   e8fa4:	616c 6274 6675 6566 7372 3a3a 6556 7463     latbuffers::Vect
   e8fb4:	726f 543c 3a3e 723a 7465 7275 5f6e 7974     or<T>::return_ty
   e8fc4:	6570 3d20 6320 6e6f 7473 7420 6c66 7469     pe = const tflit
   e8fd4:	3a65 4f3a 6570 6172 6f74 2a72 203b 6c66     e::Operator*; fl
   e8fe4:	7461 7562 6666 7265 3a73 753a 666f 7366     atbuffers::uoffs
   e8ff4:	7465 745f 3d20 6c20 6e6f 2067 6e75 6973     et_t = long unsi
   e9004:	6e67 6465 6920 746e 005d                    gned int].

000e900e <_ZZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEmE19__PRETTY_FUNCTION__>:
   e900e:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
   e901e:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
   e902e:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
   e903e:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
   e904e:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
   e905e:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
   e906e:	7469 2068 2054 203d 6c66 7461 7562 6666     ith T = flatbuff
   e907e:	7265 3a73 4f3a 6666 6573 3c74 6674 696c     ers::Offset<tfli
   e908e:	6574 3a3a 6554 736e 726f 3b3e 6620 616c     te::Tensor>; fla
   e909e:	6274 6675 6566 7372 3a3a 6556 7463 726f     tbuffers::Vector
   e90ae:	543c 3a3e 723a 7465 7275 5f6e 7974 6570     <T>::return_type
   e90be:	3d20 6320 6e6f 7473 7420 6c66 7469 3a65      = const tflite:
   e90ce:	543a 6e65 6f73 2a72 203b 6c66 7461 7562     :Tensor*; flatbu
   e90de:	6666 7265 3a73 753a 666f 7366 7465 745f     ffers::uoffset_t
   e90ee:	3d20 6c20 6e6f 2067 6e75 6973 6e67 6465      = long unsigned
   e90fe:	6920 746e 005d 6e4f 796c 3120 7320 6275      int].Only 1 sub
   e910e:	7267 7061 2068 7369 6320 7275 6572 746e     graph is current
   e911e:	796c 7320 7075 6f70 7472 6465 0a2e 3c00     ly supported...<
   e912e:	6f4e 6e20 6d61 3e65 4900 766e 6c61 6469     No name>.Invalid
   e913e:	7020 6572 612d 6c6c 636f 7461 6465 6920      pre-allocated i
   e914e:	706e 7475 2520 2064 7270 766f 6469 6465     nput %d provided
   e915e:	002e 7241 6e65 2061 6973 657a 6920 2073     ..Arena size is 
   e916e:	6f74 206f 6d73 6c61 206c 6f66 2072 6361     too small for ac
   e917e:	6974 6176 6974 6e6f 6220 6675 6566 7372     tivation buffers
   e918e:	202e 654e 6465 6465 2520 2064 7562 2074     . Needed %d but 
   e919e:	6e6f 796c 2520 2064 6177 2073 7661 6961     only %d was avai
   e91ae:	616c 6c62 2e65 5600 7261 6169 6c62 2065     lable..Variable 
   e91be:	7369 6e20 746f 6120 6c6c 636f 7461 6465     is not allocated
   e91ce:	4c00 676f 6369 6520 7272 726f 6920 206e     .Logic error in 
   e91de:	656d 6f6d 7972 7020 616c 6e6e 7265 202c     memory planner, 
   e91ee:	6574 736e 726f 2520 2064 6168 2073 6e61     tensor %d has an
   e91fe:	6920 766e 6c61 6469 6c20 6669 7465 6d69      invalid lifetim
   e920e:	0065                                        e.

000e9210 <_ZZNK11flatbuffers6VectorIfE3GetEmE19__PRETTY_FUNCTION__>:
   e9210:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
   e9220:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
   e9230:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
   e9240:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
   e9250:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
   e9260:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
   e9270:	7469 2068 2054 203d 6c66 616f 3b74 6620     ith T = float; f
   e9280:	616c 6274 6675 6566 7372 3a3a 6556 7463     latbuffers::Vect
   e9290:	726f 543c 3a3e 723a 7465 7275 5f6e 7974     or<T>::return_ty
   e92a0:	6570 3d20 6620 6f6c 7461 203b 6c66 7461     pe = float; flat
   e92b0:	7562 6666 7265 3a73 753a 666f 7366 7465     buffers::uoffset
   e92c0:	745f 3d20 6c20 6e6f 2067 6e75 6973 6e67     _t = long unsign
   e92d0:	6465 6920 746e 005d                         ed int].

000e92d8 <_ZZNK11flatbuffers6VectorINS_6OffsetIN6tflite6BufferEEEE3GetEmE19__PRETTY_FUNCTION__>:
   e92d8:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
   e92e8:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
   e92f8:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
   e9308:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
   e9318:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
   e9328:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
   e9338:	7469 2068 2054 203d 6c66 7461 7562 6666     ith T = flatbuff
   e9348:	7265 3a73 4f3a 6666 6573 3c74 6674 696c     ers::Offset<tfli
   e9358:	6574 3a3a 7542 6666 7265 3b3e 6620 616c     te::Buffer>; fla
   e9368:	6274 6675 6566 7372 3a3a 6556 7463 726f     tbuffers::Vector
   e9378:	543c 3a3e 723a 7465 7275 5f6e 7974 6570     <T>::return_type
   e9388:	3d20 6320 6e6f 7473 7420 6c66 7469 3a65      = const tflite:
   e9398:	423a 6675 6566 2a72 203b 6c66 7461 7562     :Buffer*; flatbu
   e93a8:	6666 7265 3a73 753a 666f 7366 7465 745f     ffers::uoffset_t
   e93b8:	3d20 6c20 6e6f 2067 6e75 6973 6e67 6465      = long unsigned
   e93c8:	6920 746e 005d                               int].

000e93ce <_ZZNK11flatbuffers6VectorIxE3GetEmE19__PRETTY_FUNCTION__>:
   e93ce:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
   e93de:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
   e93ee:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
   e93fe:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
   e940e:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
   e941e:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
   e942e:	7469 2068 2054 203d 6f6c 676e 6c20 6e6f     ith T = long lon
   e943e:	2067 6e69 3b74 6620 616c 6274 6675 6566     g int; flatbuffe
   e944e:	7372 3a3a 6556 7463 726f 543c 3a3e 723a     rs::Vector<T>::r
   e945e:	7465 7275 5f6e 7974 6570 3d20 6c20 6e6f     eturn_type = lon
   e946e:	2067 6f6c 676e 6920 746e 203b 6c66 7461     g long int; flat
   e947e:	7562 6666 7265 3a73 753a 666f 7366 7465     buffers::uoffset
   e948e:	745f 3d20 6c20 6e6f 2067 6e75 6973 6e67     _t = long unsign
   e949e:	6465 6920 746e 005d 0000                    ed int]...

000e94a8 <_ZTVN6tflite18MicroErrorReporterE>:
	...
   e94b0:	419d 000d 41b1 000d 5d85 000d 0a0d 5400     .A...A...].....T
   e94c0:	6e65 6f73 2072 6e69 6564 2078 6425 6f20     ensor index %d o
   e94d0:	7475 6f20 2066 6172 676e 2065 6c28 6e65     ut of range (len
   e94e0:	7467 2068 7369 2520 2964 4f00 7475 7570     gth is %d).Outpu
   e94f0:	2074 6e69 6564 2078 6425 6f20 7475 6f20     t index %d out o
   e9500:	2066 6172 676e 2065 6c28 6e65 7467 2068     f range (length 
   e9510:	7369 2520 2964 4900 706e 7475 6920 646e     is %d).Input ind
   e9520:	7865 2520 2064 756f 2074 666f 7220 6e61     ex %d out of ran
   e9530:	6567 2820 656c 676e 6874 6920 2073 6425     ge (length is %d
   e9540:	0029 6e49 6f76 656b 2928 6320 6c61 656c     ).Invoke() calle
   e9550:	2064 6661 6574 2072 6e69 7469 6169 696c     d after initiali
   e9560:	617a 6974 6e6f 6620 6961 656c 0a64 4d00     zation failed..M
   e9570:	7369 6973 676e 7220 6765 7369 7274 7461     issing registrat
   e9580:	6f69 206e 6f66 2072 706f 6f63 6564 695f     ion for opcode_i
   e9590:	646e 7865 2520 0a64 5300 696b 7070 6e69     ndex %d..Skippin
   e95a0:	2067 706f 6620 726f 6f20 6370 646f 5f65     g op for opcode_
   e95b0:	6e69 6564 2078 6425 000a 6e55 7573 7070     index %d..Unsupp
   e95c0:	726f 6574 2064 6562 6168 6976 726f 203a     orted behavior: 
   e95d0:	6f66 6e75 2064 7562 6c69 6974 206e 706f     found builtin op
   e95e0:	7265 7461 726f 2520 2073 6977 6874 6320     erator %s with c
   e95f0:	7375 6f74 206d 706f 6974 6e6f 2e73 000a     ustom options...
   e9600:	6f4e 6564 2520 2073 6e28 6d75 6562 2072     Node %s (number 
   e9610:	6425 2029 6166 6c69 6465 7420 206f 7270     %d) failed to pr
   e9620:	7065 7261 2065 6977 6874 7320 6174 7574     epare with statu
   e9630:	2073 6425 4e00 646f 2065 7325 2820 756e     s %d.Node %s (nu
   e9640:	626d 7265 2520 2964 6620 6961 656c 2064     mber %d) failed 
   e9650:	6f74 6920 766e 6b6f 2065 6977 6874 7320     to invoke with s
   e9660:	6174 7574 2073 6425 0000 0000               tatus %d....

000e966c <_ZTVN6tflite12_GLOBAL__N_118StackDataAllocatorE>:
	...
   e9674:	5e3d 000d 5e47 000d 5e49 000d 5e6b 000d     =^..G^..I^..k^..
   e9684:	7865 6f70 656e 746e 3e20 203d 0030 552f     exponent >= 0./U
   e9694:	6573 7372 622f 6173 7274 6d6f 442f 7665     sers/bsatrom/Dev
   e96a4:	6c65 706f 656d 746e 702f 7261 6974 6c63     elopment/particl
   e96b4:	2f65 696c 7262 7261 6569 2f73 6150 7472     e/libraries/Part
   e96c4:	6369 656c 545f 6e65 6f73 4672 6f6c 4c77     icle_TensorFlowL
   e96d4:	7469 5f65 7845 6d61 6c70 7365 682f 6c65     ite_Examples/hel
   e96e4:	6f6c 775f 726f 646c 6c2f 6269 542f 6e65     lo_world/lib/Ten
   e96f4:	6f73 4672 6f6c 4c77 7469 2f65 7273 2f63     sorFlowLite/src/
   e9704:	6874 7269 5f64 6170 7472 2f79 6567 6d6d     third_party/gemm
   e9714:	6f6c 7077 662f 7869 6465 6f70 6e69 2f74     lowp/fixedpoint/
   e9724:	6966 6578 7064 696f 746e 682e 6500 7078     fixedpoint.h.exp
   e9734:	6e6f 6e65 2074 3d3c 3320 0031 6e49 7570     onent <= 31.Inpu
   e9744:	7374 6120 646e 6f20 7475 7570 7374 6e20     ts and outputs n
   e9754:	746f 6120 6c6c 6620 6f6c 7461 757c 6e69     ot all float|uin
   e9764:	3874 697c 746e 2038 7974 6570 2e73 4900          t8|int8 types..

000e9773 <_ZZN8gemmlowp19RoundingDivideByPOTIlEET_S1_iE19__PRETTY_FUNCTION__>:
   e9773:	6e49 6574 6567 5472 7079 2065 6567 6d6d     IntegerType gemm
   e9783:	6f6c 7077 3a3a 6f52 6e75 6964 676e 6944     lowp::RoundingDi
   e9793:	6976 6564 7942 4f50 2854 6e49 6574 6567     videByPOT(Intege
   e97a3:	5472 7079 2c65 6920 746e 2029 775b 7469     rType, int) [wit
   e97b3:	2068 6e49 6574 6567 5472 7079 2065 203d     h IntegerType = 
   e97c3:	6f6c 676e 6920 746e 005d 0000 0000               long int]....

000e97d0 <_ZTVN6tflite3ops5micro14AllOpsResolverE>:
	...
   e97d8:	61b9 000d 61e7 000d 419f 000d 41a1 000d     .a...a...A...A..
   e97e8:	6e4f 796c 6620 6f6c 7461 3233 202c 6975     Only float32, ui
   e97f8:	746e 2038 6e61 2064 6e69 3874 6120 6572     nt8 and int8 are
   e9808:	7320 7075 6f70 7472 6465 6320 7275 6572      supported curre
   e9818:	746e 796c 202c 6f67 2074 7325 002e 6e4f     ntly, got %s..On
   e9828:	796c 6920 746e 3233 6120 6572 7320 7075     ly int32 are sup
   e9838:	6f70 7472 6465 6320 7275 6572 746e 796c     ported currently
   e9848:	202c 6f67 2074 7325 002e 552f 6573 7372     , got %s../Users
   e9858:	622f 6173 7274 6d6f 442f 7665 6c65 706f     /bsatrom/Develop
   e9868:	656d 746e 702f 7261 6974 6c63 2f65 696c     ment/particle/li
   e9878:	7262 7261 6569 2f73 6150 7472 6369 656c     braries/Particle
   e9888:	545f 6e65 6f73 4672 6f6c 4c77 7469 5f65     _TensorFlowLite_
   e9898:	7845 6d61 6c70 7365 682f 6c65 6f6c 775f     Examples/hello_w
   e98a8:	726f 646c 6c2f 6269 542f 6e65 6f73 4672     orld/lib/TensorF
   e98b8:	6f6c 4c77 7469 2f65 7273 2f63 6574 736e     lowLite/src/tens
   e98c8:	726f 6c66 776f 6c2f 7469 2f65 7865 6570     orflow/lite/expe
   e98d8:	6972 656d 746e 6c61 6d2f 6369 6f72 6b2f     rimental/micro/k
   e98e8:	7265 656e 736c 632f 6965 2e6c 7063 0070     ernels/ceil.cpp.
   e98f8:	7325 253a 2064 7325 2120 203d 7325 2820     %s:%d %s != %s (
   e9908:	6425 2120 203d 6425 0029 754e 496d 706e     %d != %d).NumInp
   e9918:	7475 2873 6f6e 6564 0029 754e 4f6d 7475     uts(node).NumOut
   e9928:	7570 7374 6e28 646f 2965 6900 706e 7475     puts(node).input
   e9938:	3e2d 7974 6570 6f00 7475 7570 2d74 743e     ->type.output->t
   e9948:	7079 0065 6e69 7570 2d74 623e 7479 7365     ype.input->bytes
   e9958:	6f00 7475 7570 2d74 623e 7479 7365 6900     .output->bytes.i
   e9968:	706e 7475 3e2d 6964 736d 3e2d 6973 657a     nput->dims->size
   e9978:	6f00 7475 7570 2d74 643e 6d69 2d73 733e     .output->dims->s
   e9988:	7a69 0065 6e69 7570 2d74 643e 6d69 2d73     ize.input->dims-
   e9998:	643e 7461 5b61 5d69 6f00 7475 7570 2d74     >data[i].output-
   e99a8:	643e 6d69 2d73 643e 7461 5b61 5d69 4400     >dims->data[i].D
   e99b8:	656f 2073 6f6e 2074 7573 7070 726f 2074     oes not support 
   e99c8:	7974 6570 2520 2c64 7220 7165 6975 6572     type %d, require
   e99d8:	2073 6f62 6c6f 667c 6f6c 7461 697c 746e     s bool|float|int
   e99e8:	757c 6e69 3874 4400 656f 2073 6f6e 2074     |uint8.Does not 
   e99f8:	7573 7070 726f 2074 7974 6570 2520 2c64     support type %d,
   e9a08:	7220 7165 6975 6572 2073 6c66 616f 7c74      requires float|
   e9a18:	6e69 7c74 6975 746e 0038 552f 6573 7372     int|uint8./Users
   e9a28:	622f 6173 7274 6d6f 442f 7665 6c65 706f     /bsatrom/Develop
   e9a38:	656d 746e 702f 7261 6974 6c63 2f65 696c     ment/particle/li
   e9a48:	7262 7261 6569 2f73 6150 7472 6369 656c     braries/Particle
   e9a58:	545f 6e65 6f73 4672 6f6c 4c77 7469 5f65     _TensorFlowLite_
   e9a68:	7845 6d61 6c70 7365 682f 6c65 6f6c 775f     Examples/hello_w
   e9a78:	726f 646c 6c2f 6269 542f 6e65 6f73 4672     orld/lib/TensorF
   e9a88:	6f6c 4c77 7469 2f65 7273 2f63 6574 736e     lowLite/src/tens
   e9a98:	726f 6c66 776f 6c2f 7469 2f65 7865 6570     orflow/lite/expe
   e9aa8:	6972 656d 746e 6c61 6d2f 6369 6f72 6b2f     rimental/micro/k
   e9ab8:	7265 656e 736c 632f 6e6f 2e76 7063 0070     ernels/conv.cpp.
   e9ac8:	7325 253a 2064 7325 7720 7361 6e20 746f     %s:%d %s was not
   e9ad8:	7420 7572 2e65 6800 7361 625f 6169 2073      true..has_bias 
   e9ae8:	7c7c 6e20 646f 2d65 693e 706e 7475 2d73     || node->inputs-
   e9af8:	733e 7a69 2065 3d3d 3220 6e00 646f 2d65     >size == 2.node-
   e9b08:	6f3e 7475 7570 7374 3e2d 6973 657a 6b00     >outputs->size.k
   e9b18:	6654 694c 6574 6641 6966 656e 7551 6e61     TfLiteAffineQuan
   e9b28:	6974 617a 6974 6e6f 6600 6c69 6574 2d72     tization.filter-
   e9b38:	713e 6175 746e 7a69 7461 6f69 2e6e 7974     >quantization.ty
   e9b48:	6570 6100 6666 6e69 5f65 7571 6e61 6974     pe.affine_quanti
   e9b58:	617a 6974 6e6f 6100 6666 6e69 5f65 7571     zation.affine_qu
   e9b68:	6e61 6974 617a 6974 6e6f 3e2d 6373 6c61     antization->scal
   e9b78:	0065 7954 6570 2520 2073 2528 2964 6e20     e.Type %s (%d) n
   e9b88:	746f 7320 7075 6f70 7472 6465 002e          ot supported..

000e9b96 <_ZZN8gemmlowp19RoundingDivideByPOTIlEET_S1_iE19__PRETTY_FUNCTION__>:
   e9b96:	6e49 6574 6567 5472 7079 2065 6567 6d6d     IntegerType gemm
   e9ba6:	6f6c 7077 3a3a 6f52 6e75 6964 676e 6944     lowp::RoundingDi
   e9bb6:	6976 6564 7942 4f50 2854 6e49 6574 6567     videByPOT(Intege
   e9bc6:	5472 7079 2c65 6920 746e 2029 775b 7469     rType, int) [wit
   e9bd6:	2068 6e49 6574 6567 5472 7079 2065 203d     h IntegerType = 
   e9be6:	6f6c 676e 6920 746e 005d 552f 6573 7372     long int]./Users
   e9bf6:	622f 6173 7274 6d6f 442f 7665 6c65 706f     /bsatrom/Develop
   e9c06:	656d 746e 702f 7261 6974 6c63 2f65 696c     ment/particle/li
   e9c16:	7262 7261 6569 2f73 6150 7472 6369 656c     braries/Particle
   e9c26:	545f 6e65 6f73 4672 6f6c 4c77 7469 5f65     _TensorFlowLite_
   e9c36:	7845 6d61 6c70 7365 682f 6c65 6f6c 775f     Examples/hello_w
   e9c46:	726f 646c 6c2f 6269 542f 6e65 6f73 4672     orld/lib/TensorF
   e9c56:	6f6c 4c77 7469 2f65 7273 2f63 6574 736e     lowLite/src/tens
   e9c66:	726f 6c66 776f 6c2f 7469 2f65 7865 6570     orflow/lite/expe
   e9c76:	6972 656d 746e 6c61 6d2f 6369 6f72 6b2f     rimental/micro/k
   e9c86:	7265 656e 736c 642f 7165 6175 746e 7a69     ernels/dequantiz
   e9c96:	2e65 7063 0070 6e69 7570 2d74 743e 7079     e.cpp.input->typ
   e9ca6:	2065 3d3d 6b20 6654 694c 6574 4955 746e     e == kTfLiteUInt
   e9cb6:	2038 7c7c 6920 706e 7475 3e2d 7974 6570     8 || input->type
   e9cc6:	3d20 203d 546b 4c66 7469 4965 746e 0038      == kTfLiteInt8.
   e9cd6:	756f 7074 7475 3e2d 7974 6570 3d20 203d     output->type == 
   e9ce6:	546b 4c66 7469 4665 6f6c 7461 3233 2f00     kTfLiteFloat32./
   e9cf6:	7355 7265 2f73 7362 7461 6f72 2f6d 6544     Users/bsatrom/De
   e9d06:	6576 6f6c 6d70 6e65 2f74 6170 7472 6369     velopment/partic
   e9d16:	656c 6c2f 6269 6172 6972 7365 502f 7261     le/libraries/Par
   e9d26:	6974 6c63 5f65 6554 736e 726f 6c46 776f     ticle_TensorFlow
   e9d36:	694c 6574 455f 6178 706d 656c 2f73 6568     Lite_Examples/he
   e9d46:	6c6c 5f6f 6f77 6c72 2f64 696c 2f62 6554     llo_world/lib/Te
   e9d56:	736e 726f 6c46 776f 694c 6574 732f 6372     nsorFlowLite/src
   e9d66:	742f 6e65 6f73 6672 6f6c 2f77 696c 6574     /tensorflow/lite
   e9d76:	652f 7078 7265 6d69 6e65 6174 2f6c 696d     /experimental/mi
   e9d86:	7263 2f6f 656b 6e72 6c65 2f73 6c65 6d65     cro/kernels/elem
   e9d96:	6e65 7774 7369 2e65 7063 0070 6e49 7570     entwise.cpp.Inpu
   e9da6:	2074 6164 6174 7420 7079 2065 7325 2820     t data type %s (
   e9db6:	6425 2029 7369 6e20 746f 7320 7075 6f70     %d) is not suppo
   e9dc6:	7472 6465 002e 7865 6570 7463 6465 745f     rted..expected_t
   e9dd6:	7079 0065 552f 6573 7372 622f 6173 7274     ype./Users/bsatr
   e9de6:	6d6f 442f 7665 6c65 706f 656d 746e 702f     om/Development/p
   e9df6:	7261 6974 6c63 2f65 696c 7262 7261 6569     article/librarie
   e9e06:	2f73 6150 7472 6369 656c 545f 6e65 6f73     s/Particle_Tenso
   e9e16:	4672 6f6c 4c77 7469 5f65 7845 6d61 6c70     rFlowLite_Exampl
   e9e26:	7365 682f 6c65 6f6c 775f 726f 646c 6c2f     es/hello_world/l
   e9e36:	6269 542f 6e65 6f73 4672 6f6c 4c77 7469     ib/TensorFlowLit
   e9e46:	2f65 7273 2f63 6574 736e 726f 6c66 776f     e/src/tensorflow
   e9e56:	6c2f 7469 2f65 7865 6570 6972 656d 746e     /lite/experiment
   e9e66:	6c61 6d2f 6369 6f72 6b2f 7265 656e 736c     al/micro/kernels
   e9e76:	662f 6f6c 726f 632e 7070 5100 6175 746e     /floor.cpp.Quant
   e9e86:	7a69 6465 4620 6c75 796c 6f43 6e6e 6365     ized FullyConnec
   e9e96:	6574 2064 7865 6570 7463 2073 756f 7074     ted expects outp
   e9ea6:	7475 6420 7461 2061 7974 6570 7520 6e69     ut data type uin
   e9eb6:	3874 6f20 2072 6e69 3174 0036 7954 6570     t8 or int16.Type
   e9ec6:	2520 2064 6f6e 2074 7563 7272 6e65 6c74      %d not currentl
   e9ed6:	2079 7573 7070 726f 6574 2e64 4f00 6c6e     y supported..Onl
   e9ee6:	2079 6c66 616f 3374 2032 7369 7320 7075     y float32 is sup
   e9ef6:	6f70 7472 6465 6320 7275 6572 746e 796c     ported currently
   e9f06:	202c 6f67 2074 7325 5400 7079 2065 7325     , got %s.Type %s
   e9f16:	2820 6425 2029 7369 6e20 746f 7320 7075      (%d) is not sup
   e9f26:	6f70 7472 6465 6220 2079 614d 6978 756d     ported by Maximu
   e9f36:	2f6d 694d 696e 756d 2e6d 4e00 6765 6f20     m/Minimum..Neg o
   e9f46:	6c6e 2079 7563 7272 6e65 6c74 2079 7573     nly currently su
   e9f56:	7070 726f 7374 6620 6f6c 7461 3233 202c     pports float32, 
   e9f66:	6f67 2074 6425 002e 7954 6570 2720 7325     got %d..Type '%s
   e9f76:	2027 7369 6e20 746f 7320 7075 6f70 7472     ' is not support
   e9f86:	6465 6220 2079 6170 6b63 002e 7954 6570     ed by pack..Type
   e9f96:	2520 2073 6f6e 2074 7563 7272 6e65 6c74      %s not currentl
   e9fa6:	2079 7573 7070 726f 6574 2e64 4900 706e     y supported..Inp
   e9fb6:	7475 7420 7079 2065 7325 6920 2073 6f6e     ut type %s is no
   e9fc6:	2074 7563 7272 6e65 6c74 2079 7573 7070     t currently supp
   e9fd6:	726f 6574 0064 6e4f 796c 6620 6f6c 7461     orted.Only float
   e9fe6:	3233 6120 646e 7520 6e69 3874 6120 6572     32 and uint8 are
   e9ff6:	7320 7075 6f70 7472 6465 6320 7275 6572      supported curre
   ea006:	746e 796c 202c 6f67 2074 6425 002e          ntly, got %d..

000ea014 <_ZZN8gemmlowp19RoundingDivideByPOTIlEET_S1_iE19__PRETTY_FUNCTION__>:
   ea014:	6e49 6574 6567 5472 7079 2065 6567 6d6d     IntegerType gemm
   ea024:	6f6c 7077 3a3a 6f52 6e75 6964 676e 6944     lowp::RoundingDi
   ea034:	6976 6564 7942 4f50 2854 6e49 6574 6567     videByPOT(Intege
   ea044:	5472 7079 2c65 6920 746e 2029 775b 7469     rType, int) [wit
   ea054:	2068 6e49 6574 6567 5472 7079 2065 203d     h IntegerType = 
   ea064:	6f6c 676e 6920 746e 005d 552f 6573 7372     long int]./Users
   ea074:	622f 6173 7274 6d6f 442f 7665 6c65 706f     /bsatrom/Develop
   ea084:	656d 746e 702f 7261 6974 6c63 2f65 696c     ment/particle/li
   ea094:	7262 7261 6569 2f73 6150 7472 6369 656c     braries/Particle
   ea0a4:	545f 6e65 6f73 4672 6f6c 4c77 7469 5f65     _TensorFlowLite_
   ea0b4:	7845 6d61 6c70 7365 682f 6c65 6f6c 775f     Examples/hello_w
   ea0c4:	726f 646c 6c2f 6269 542f 6e65 6f73 4672     orld/lib/TensorF
   ea0d4:	6f6c 4c77 7469 2f65 7273 2f63 6574 736e     lowLite/src/tens
   ea0e4:	726f 6c66 776f 6c2f 7469 2f65 7865 6570     orflow/lite/expe
   ea0f4:	6972 656d 746e 6c61 6d2f 6369 6f72 6b2f     rimental/micro/k
   ea104:	7265 656e 736c 712f 6175 746e 7a69 2e65     ernels/quantize.
   ea114:	7063 0070 756f 7074 7475 3e2d 7571 6e61     cpp.output->quan
   ea124:	6974 617a 6974 6e6f 742e 7079 0065 6661     tization.type.af
   ea134:	6966 656e 715f 6175 746e 7a69 7461 6f69     fine_quantizatio
   ea144:	2d6e 733e 6163 656c 3e2d 6973 657a 3d20     n->scale->size =
   ea154:	203d 0031 6e69 7570 2d74 743e 7079 2065     = 1.input->type 
   ea164:	3d3d 6b20 6654 694c 6574 6c46 616f 3374     == kTfLiteFloat3
   ea174:	0032 756f 7074 7475 3e2d 7974 6570 3d20     2.output->type =
   ea184:	203d 546b 4c66 7469 5565 6e49 3874 7c20     = kTfLiteUInt8 |
   ea194:	207c 756f 7074 7475 3e2d 7974 6570 3d20     | output->type =
   ea1a4:	203d 546b 4c66 7469 4965 746e 0038 754f     = kTfLiteInt8.Ou
   ea1b4:	7074 7475 7420 7079 2065 7325 2820 6425     tput type %s (%d
   ea1c4:	2029 6f6e 2074 7573 7070 726f 6574 0064     ) not supported.
   ea1d4:	552f 6573 7372 622f 6173 7274 6d6f 442f     /Users/bsatrom/D
   ea1e4:	7665 6c65 706f 656d 746e 702f 7261 6974     evelopment/parti
   ea1f4:	6c63 2f65 696c 7262 7261 6569 2f73 6150     cle/libraries/Pa
   ea204:	7472 6369 656c 545f 6e65 6f73 4672 6f6c     rticle_TensorFlo
   ea214:	4c77 7469 5f65 7845 6d61 6c70 7365 682f     wLite_Examples/h
   ea224:	6c65 6f6c 775f 726f 646c 6c2f 6269 542f     ello_world/lib/T
   ea234:	6e65 6f73 4672 6f6c 4c77 7469 2f65 7273     ensorFlowLite/sr
   ea244:	2f63 6574 736e 726f 6c66 776f 6c2f 7469     c/tensorflow/lit
   ea254:	2f65 7865 6570 6972 656d 746e 6c61 6d2f     e/experimental/m
   ea264:	6369 6f72 6b2f 7265 656e 736c 722f 7365     icro/kernels/res
   ea274:	6168 6570 632e 7070 4e00 6d75 6e49 7570     hape.cpp.NumInpu
   ea284:	7374 6e28 646f 2965 3d20 203d 2031 7c7c     ts(node) == 1 ||
   ea294:	4e20 6d75 6e49 7570 7374 6e28 646f 2965      NumInputs(node)
   ea2a4:	3d20 203d 0032 312d 7300 7274 7465 6863      == 2.-1.stretch
   ea2b4:	645f 6d69 6e00 6d75 6f5f 7475 7570 5f74     _dim.num_output_
   ea2c4:	6c65 6d65 6e65 7374 6e00 6d75 695f 706e     elements.num_inp
   ea2d4:	7475 655f 656c 656d 746e 0073 552f 6573     ut_elements./Use
   ea2e4:	7372 622f 6173 7274 6d6f 442f 7665 6c65     rs/bsatrom/Devel
   ea2f4:	706f 656d 746e 702f 7261 6974 6c63 2f65     opment/particle/
   ea304:	696c 7262 7261 6569 2f73 6150 7472 6369     libraries/Partic
   ea314:	656c 545f 6e65 6f73 4672 6f6c 4c77 7469     le_TensorFlowLit
   ea324:	5f65 7845 6d61 6c70 7365 682f 6c65 6f6c     e_Examples/hello
   ea334:	775f 726f 646c 6c2f 6269 542f 6e65 6f73     _world/lib/Tenso
   ea344:	4672 6f6c 4c77 7469 2f65 7273 2f63 6574     rFlowLite/src/te
   ea354:	736e 726f 6c66 776f 6c2f 7469 2f65 7865     nsorflow/lite/ex
   ea364:	6570 6972 656d 746e 6c61 6d2f 6369 6f72     perimental/micro
   ea374:	6b2f 7265 656e 736c 722f 756f 646e 632e     /kernels/round.c
   ea384:	7070 2f00 7355 7265 2f73 7362 7461 6f72     pp./Users/bsatro
   ea394:	2f6d 6544 6576 6f6c 6d70 6e65 2f74 6170     m/Development/pa
   ea3a4:	7472 6369 656c 6c2f 6269 6172 6972 7365     rticle/libraries
   ea3b4:	502f 7261 6974 6c63 5f65 6554 736e 726f     /Particle_Tensor
   ea3c4:	6c46 776f 694c 6574 455f 6178 706d 656c     FlowLite_Example
   ea3d4:	2f73 6568 6c6c 5f6f 6f77 6c72 2f64 696c     s/hello_world/li
   ea3e4:	2f62 6554 736e 726f 6c46 776f 694c 6574     b/TensorFlowLite
   ea3f4:	732f 6372 742f 6e65 6f73 6672 6f6c 2f77     /src/tensorflow/
   ea404:	696c 6574 652f 7078 7265 6d69 6e65 6174     lite/experimenta
   ea414:	2f6c 696d 7263 2f6f 656b 6e72 6c65 2f73     l/micro/kernels/
   ea424:	6f73 7466 616d 2e78 7063 0070 756f 7074     softmax.cpp.outp
   ea434:	7475 3e2d 6170 6172 736d 7a2e 7265 5f6f     ut->params.zero_
   ea444:	6f70 6e69 0074 756f 7074 7475 3e2d 6170     point.output->pa
   ea454:	6172 736d 732e 6163 656c 3d20 203d 2e31     rams.scale == 1.
   ea464:	2066 202f 3532 0036 6e4f 796c 3120 2c44     f / 256.Only 1D,
   ea474:	3220 2044 6e61 2064 4434 7420 6e65 6f73      2D and 4D tenso
   ea484:	7372 7320 7075 6f70 7472 6465 6320 7275     rs supported cur
   ea494:	6572 746e 796c 202c 6f67 2074 6425 2e44     rently, got %dD.
   ea4a4:	4f00 6c6e 2079 4432 6120 646e 3420 2044     .Only 2D and 4D 
   ea4b4:	6574 736e 726f 2073 7573 7070 726f 6574     tensors supporte
   ea4c4:	2064 7563 7272 6e65 6c74 2c79 6720 746f     d currently, got
   ea4d4:	2520 4464 002e 6e4f 796c 6620 6f6c 7461      %dD..Only float
   ea4e4:	3233 6120 646e 7520 6e69 3874 745f 7320     32 and uint8_t s
   ea4f4:	7075 6f70 7472 6465 6320 7275 6572 746e     upported current
   ea504:	796c 202c 6f67 2074 6425 002e               ly, got %d..

000ea510 <_ZZN8gemmlowp19RoundingDivideByPOTIlEET_S1_iE19__PRETTY_FUNCTION__>:
   ea510:	6e49 6574 6567 5472 7079 2065 6567 6d6d     IntegerType gemm
   ea520:	6f6c 7077 3a3a 6f52 6e75 6964 676e 6944     lowp::RoundingDi
   ea530:	6976 6564 7942 4f50 2854 6e49 6574 6567     videByPOT(Intege
   ea540:	5472 7079 2c65 6920 746e 2029 775b 7469     rType, int) [wit
   ea550:	2068 6e49 6574 6567 5472 7079 2065 203d     h IntegerType = 
   ea560:	6f6c 676e 6920 746e 005d 552f 6573 7372     long int]./Users
   ea570:	622f 6173 7274 6d6f 442f 7665 6c65 706f     /bsatrom/Develop
   ea580:	656d 746e 702f 7261 6974 6c63 2f65 696c     ment/particle/li
   ea590:	7262 7261 6569 2f73 6150 7472 6369 656c     braries/Particle
   ea5a0:	545f 6e65 6f73 4672 6f6c 4c77 7469 5f65     _TensorFlowLite_
   ea5b0:	7845 6d61 6c70 7365 682f 6c65 6f6c 775f     Examples/hello_w
   ea5c0:	726f 646c 6c2f 6269 542f 6e65 6f73 4672     orld/lib/TensorF
   ea5d0:	6f6c 4c77 7469 2f65 7273 2f63 6574 736e     lowLite/src/tens
   ea5e0:	726f 6c66 776f 6c2f 7469 2f65 7865 6570     orflow/lite/expe
   ea5f0:	6972 656d 746e 6c61 6d2f 6369 6f72 6b2f     rimental/micro/k
   ea600:	7265 656e 736c 732f 6c70 7469 632e 7070     ernels/split.cpp
   ea610:	4e20 6e6f 6320 6e6f 7473 6e61 2074 7861      Non constant ax
   ea620:	7369 7420 6e65 6f73 2072 6f6e 2074 7573     is tensor not su
   ea630:	7070 726f 6574 0064 552f 6573 7372 622f     pported./Users/b
   ea640:	6173 7274 6d6f 442f 7665 6c65 706f 656d     satrom/Developme
   ea650:	746e 702f 7261 6974 6c63 2f65 696c 7262     nt/particle/libr
   ea660:	7261 6569 2f73 6150 7472 6369 656c 545f     aries/Particle_T
   ea670:	6e65 6f73 4672 6f6c 4c77 7469 5f65 7845     ensorFlowLite_Ex
   ea680:	6d61 6c70 7365 682f 6c65 6f6c 775f 726f     amples/hello_wor
   ea690:	646c 6c2f 6269 542f 6e65 6f73 4672 6f6c     ld/lib/TensorFlo
   ea6a0:	4c77 7469 2f65 7273 2f63 6574 736e 726f     wLite/src/tensor
   ea6b0:	6c66 776f 6c2f 7469 2f65 7865 6570 6972     flow/lite/experi
   ea6c0:	656d 746e 6c61 6d2f 6369 6f72 6b2f 7265     mental/micro/ker
   ea6d0:	656e 736c 732f 6c70 7469 632e 7070 6100     nels/split.cpp.a
   ea6e0:	6978 5f73 6176 756c 2065 3d3e 3020 6100     xis_value >= 0.a
   ea6f0:	6978 5f73 6176 756c 2065 203c 754e 446d     xis_value < NumD
   ea700:	6d69 6e65 6973 6e6f 2873 6e69 7570 2974     imensions(input)
   ea710:	5400 7079 2065 7325 6320 7275 6572 746e     .Type %s current
   ea720:	796c 6e20 746f 7320 7075 6f70 7472 6465     ly not supported
   ea730:	002e 552f 6573 7372 622f 6173 7274 6d6f     ../Users/bsatrom
   ea740:	442f 7665 6c65 706f 656d 746e 702f 7261     /Development/par
   ea750:	6974 6c63 2f65 696c 7262 7261 6569 2f73     ticle/libraries/
   ea760:	6150 7472 6369 656c 545f 6e65 6f73 4672     Particle_TensorF
   ea770:	6f6c 4c77 7469 5f65 7845 6d61 6c70 7365     lowLite_Examples
   ea780:	682f 6c65 6f6c 775f 726f 646c 6c2f 6269     /hello_world/lib
   ea790:	542f 6e65 6f73 4672 6f6c 4c77 7469 2f65     /TensorFlowLite/
   ea7a0:	7273 2f63 6574 736e 726f 6c66 776f 6c2f     src/tensorflow/l
   ea7b0:	7469 2f65 7865 6570 6972 656d 746e 6c61     ite/experimental
   ea7c0:	6d2f 6369 6f72 6b2f 7265 656e 736c 732f     /micro/kernels/s
   ea7d0:	7274 6469 6465 735f 696c 6563 632e 7070     trided_slice.cpp
   ea7e0:	7320 7274 6469 2065 6176 756c 2065 6168      stride value ha
   ea7f0:	2073 6f74 6220 2065 6f6e 2d6e 657a 6f72     s to be non-zero
   ea800:	2f00 7355 7265 2f73 7362 7461 6f72 2f6d     ./Users/bsatrom/
   ea810:	6544 6576 6f6c 6d70 6e65 2f74 6170 7472     Development/part
   ea820:	6369 656c 6c2f 6269 6172 6972 7365 502f     icle/libraries/P
   ea830:	7261 6974 6c63 5f65 6554 736e 726f 6c46     article_TensorFl
   ea840:	776f 694c 6574 455f 6178 706d 656c 2f73     owLite_Examples/
   ea850:	6568 6c6c 5f6f 6f77 6c72 2f64 696c 2f62     hello_world/lib/
   ea860:	6554 736e 726f 6c46 776f 694c 6574 732f     TensorFlowLite/s
   ea870:	6372 742f 6e65 6f73 6672 6f6c 2f77 696c     rc/tensorflow/li
   ea880:	6574 652f 7078 7265 6d69 6e65 6174 2f6c     te/experimental/
   ea890:	696d 7263 2f6f 656b 6e72 6c65 2f73 7473     micro/kernels/st
   ea8a0:	6972 6564 5f64 6c73 6369 2e65 7063 0070     rided_slice.cpp.
   ea8b0:	6964 5f6d 6873 7061 0065 756f 7074 7475     dim_shape.output
   ea8c0:	735f 6168 6570 3e2d 6164 6174 735b 6168     _shape->data[sha
   ea8d0:	6570 735f 7a69 5d65 7300 6168 6570 735f     pe_size].shape_s
   ea8e0:	7a69 0065 756f 7074 7475 735f 6168 6570     ize.output_shape
   ea8f0:	3e2d 6973 657a 2f00 7355 7265 2f73 7362     ->size./Users/bs
   ea900:	7461 6f72 2f6d 6544 6576 6f6c 6d70 6e65     atrom/Developmen
   ea910:	2f74 6170 7472 6369 656c 6c2f 6269 6172     t/particle/libra
   ea920:	6972 7365 502f 7261 6974 6c63 5f65 6554     ries/Particle_Te
   ea930:	736e 726f 6c46 776f 694c 6574 455f 6178     nsorFlowLite_Exa
   ea940:	706d 656c 2f73 6568 6c6c 5f6f 6f77 6c72     mples/hello_worl
   ea950:	2f64 696c 2f62 6554 736e 726f 6c46 776f     d/lib/TensorFlow
   ea960:	694c 6574 732f 6372 742f 6e65 6f73 6672     Lite/src/tensorf
   ea970:	6f6c 2f77 696c 6574 652f 7078 7265 6d69     low/lite/experim
   ea980:	6e65 6174 2f6c 696d 7263 2f6f 656b 6e72     ental/micro/kern
   ea990:	6c65 2f73 7473 6972 6564 5f64 6c73 6369     els/strided_slic
   ea9a0:	2e65 7063 2070 6e69 7570 2074 6964 206d     e.cpp input dim 
   ea9b0:	6873 756f 646c 6e20 746f 6520 6378 6565     should not excee
   ea9c0:	2064 0034 7954 6570 2520 2064 7369 6320     d 4.Type %d is c
   ea9d0:	7275 6572 746e 796c 6e20 746f 7320 7075     urrently not sup
   ea9e0:	6f70 7472 6465 6220 2079 7453 6972 6564     ported by Stride
   ea9f0:	5364 696c 6563 002e 552f 6573 7372 622f     dSlice../Users/b
   eaa00:	6173 7274 6d6f 442f 7665 6c65 706f 656d     satrom/Developme
   eaa10:	746e 702f 7261 6974 6c63 2f65 696c 7262     nt/particle/libr
   eaa20:	7261 6569 2f73 6150 7472 6369 656c 545f     aries/Particle_T
   eaa30:	6e65 6f73 4672 6f6c 4c77 7469 5f65 7845     ensorFlowLite_Ex
   eaa40:	6d61 6c70 7365 682f 6c65 6f6c 775f 726f     amples/hello_wor
   eaa50:	646c 6c2f 6269 542f 6e65 6f73 4672 6f6c     ld/lib/TensorFlo
   eaa60:	4c77 7469 2f65 7273 2f63 6574 736e 726f     wLite/src/tensor
   eaa70:	6c66 776f 6c2f 7469 2f65 7865 6570 6972     flow/lite/experi
   eaa80:	656d 746e 6c61 6d2f 6369 6f72 6b2f 7265     mental/micro/ker
   eaa90:	656e 736c 732f 6476 2e66 7063 0070 6f6e     nels/svdf.cpp.no
   eaaa0:	6564 3e2d 6e69 7570 7374 3e2d 6973 657a     de->inputs->size
   eaab0:	6e00 6d75 665f 6c69 6574 7372 2520 7220     .num_filters % r
   eaac0:	6e61 006b 754e 446d 6d69 6e65 6973 6e6f     ank.NumDimension
   eaad0:	2873 6577 6769 7468 5f73 6566 7461 7275     s(weights_featur
   eaae0:	2965 6900 706e 7475 735f 7a69 0065 6577     e).input_size.we
   eaaf0:	6769 7468 5f73 6566 7461 7275 2d65 643e     ights_feature->d
   eab00:	6d69 2d73 643e 7461 5b61 5d31 4e00 6d75     ims->data[1].Num
   eab10:	6944 656d 736e 6f69 736e 7728 6965 6867     Dimensions(weigh
   eab20:	7374 745f 6d69 2965 6e00 6d75 755f 696e     ts_time).num_uni
   eab30:	7374 6200 6169 2d73 643e 6d69 2d73 643e     ts.bias->dims->d
   eab40:	7461 5b61 5d30 6200 6169 2d73 743e 7079     ata[0].bias->typ
   eab50:	0065 6361 6974 6176 6974 6e6f 735f 6174     e.activation_sta
   eab60:	6574 3e2d 7974 6570 4e00 6d75 6944 656d     te->type.NumDime
   eab70:	736e 6f69 736e 6128 7463 7669 7461 6f69     nsions(activatio
   eab80:	5f6e 7473 7461 2965 6200 7461 6863 735f     n_state).batch_s
   eab90:	7a69 0065 6361 6974 6176 6974 6e6f 735f     ize.activation_s
   eaba0:	6174 6574 3e2d 6964 736d 3e2d 6164 6174     tate->dims->data
   eabb0:	305b 005d 656d 6f6d 7972 735f 7a69 2065     [0].memory_size 
   eabc0:	202a 756e 5f6d 6966 746c 7265 0073 6361     * num_filters.ac
   eabd0:	6974 6176 6974 6e6f 735f 6174 6574 3e2d     tivation_state->
   eabe0:	6964 736d 3e2d 6164 6174 315b 005d 6373     dims->data[1].sc
   eabf0:	6172 6374 5f68 6574 736e 726f 3e2d 7974     ratch_tensor->ty
   eac00:	6570 4e00 6d75 6944 656d 736e 6f69 736e     pe.NumDimensions
   eac10:	7328 7263 7461 6863 745f 6e65 6f73 2972     (scratch_tensor)
   eac20:	7300 7263 7461 6863 745f 6e65 6f73 2d72     .scratch_tensor-
   eac30:	643e 6d69 2d73 643e 7461 5b61 5d30 7300     >dims->data[0].s
   eac40:	7263 7461 6863 745f 6e65 6f73 2d72 643e     cratch_tensor->d
   eac50:	6d69 2d73 643e 7461 5b61 5d31 7700 6965     ims->data[1].wei
   eac60:	6867 7374 745f 6d69 2d65 743e 7079 2065     ghts_time->type 
   eac70:	3d3d 6b20 6654 694c 6574 4955 746e 2038     == kTfLiteUInt8 
   eac80:	7c7c 7720 6965 6867 7374 745f 6d69 2d65     || weights_time-
   eac90:	743e 7079 2065 3d3d 6b20 6654 694c 6574     >type == kTfLite
   eaca0:	6e49 3874 6e00 646f 2d65 743e 6d65 6f70     Int8.node->tempo
   eacb0:	6172 6972 7365 3e2d 6973 657a 7300 7263     raries->size.scr
   eacc0:	7461 6863 695f 706e 7475 715f 6175 746e     atch_input_quant
   eacd0:	7a69 6465 3e2d 7974 6570 3d20 203d 546b     ized->type == kT
   eace0:	4c66 7469 5565 6e49 3874 7c20 207c 6373     fLiteUInt8 || sc
   eacf0:	6172 6374 5f68 6e69 7570 5f74 7571 6e61     ratch_input_quan
   ead00:	6974 657a 2d64 743e 7079 2065 3d3d 6b20     tized->type == k
   ead10:	6654 694c 6574 6e49 3874 7300 7263 7461     TfLiteInt8.scrat
   ead20:	6863 695f 706e 7475 715f 6175 746e 7a69     ch_input_quantiz
   ead30:	6465 3e2d 6964 736d 3e2d 6164 6174 305b     ed->dims->data[0
   ead40:	005d 6373 6172 6374 5f68 6373 6c61 6e69     ].scratch_scalin
   ead50:	5f67 6166 7463 726f 2d73 743e 7079 0065     g_factors->type.
   ead60:	754e 446d 6d69 6e65 6973 6e6f 2873 6373     NumDimensions(sc
   ead70:	6172 6374 5f68 6373 6c61 6e69 5f67 6166     ratch_scaling_fa
   ead80:	7463 726f 2973 7300 7263 7461 6863 735f     ctors).scratch_s
   ead90:	6163 696c 676e 665f 6361 6f74 7372 3e2d     caling_factors->
   eada0:	6964 736d 3e2d 6164 6174 305b 005d 6373     dims->data[0].sc
   eadb0:	6172 6374 5f68 6c66 616f 5f74 6577 6769     ratch_float_weig
   eadc0:	7468 5f73 6974 656d 3e2d 7974 6570 4e00     hts_time->type.N
   eadd0:	6d75 6944 656d 736e 6f69 736e 7328 7263     umDimensions(scr
   eade0:	7461 6863 665f 6f6c 7461 775f 6965 6867     atch_float_weigh
   eadf0:	7374 745f 6d69 2965 7300 7263 7461 6863     ts_time).scratch
   eae00:	665f 6f6c 7461 775f 6965 6867 7374 745f     _float_weights_t
   eae10:	6d69 2d65 643e 6d69 2d73 643e 7461 5b61     ime->dims->data[
   eae20:	5d30 6d00 6d65 726f 5f79 6973 657a 7300     0].memory_size.s
   eae30:	7263 7461 6863 665f 6f6c 7461 775f 6965     cratch_float_wei
   eae40:	6867 7374 745f 6d69 2d65 643e 6d69 2d73     ghts_time->dims-
   eae50:	643e 7461 5b61 5d31 7700 6965 6867 7374     >data[1].weights
   eae60:	665f 6165 7574 6572 3e2d 7974 6570 4e00     _feature->type.N
   eae70:	6d75 6944 656d 736e 6f69 736e 6f28 7475     umDimensions(out
   eae80:	7570 2974 6f00 7475 7570 2d74 643e 6d69     put).output->dim
   eae90:	2d73 643e 7461 5b61 5d30 6f00 7475 7570     s->data[0].outpu
   eaea0:	2d74 643e 6d69 2d73 643e 7461 5b61 5d31     t->dims->data[1]
   eaeb0:	5400 7079 2065 2527 2773 6920 2073 6f6e     .Type '%s' is no
   eaec0:	2074 7573 7070 726f 6574 2064 7962 7520     t supported by u
   eaed0:	706e 6361 2e6b 2f00 7355 7265 2f73 7362     npack../Users/bs
   eaee0:	7461 6f72 2f6d 6544 6576 6f6c 6d70 6e65     atrom/Developmen
   eaef0:	2f74 6170 7472 6369 656c 6c2f 6269 6172     t/particle/libra
   eaf00:	6972 7365 502f 7261 6974 6c63 5f65 6554     ries/Particle_Te
   eaf10:	736e 726f 6c46 776f 694c 6574 455f 6178     nsorFlowLite_Exa
   eaf20:	706d 656c 2f73 6568 6c6c 5f6f 6f77 6c72     mples/hello_worl
   eaf30:	2f64 696c 2f62 6554 736e 726f 6c46 776f     d/lib/TensorFlow
   eaf40:	694c 6574 732f 6372 742f 6e65 6f73 6672     Lite/src/tensorf
   eaf50:	6f6c 2f77 696c 6574 652f 7078 7265 6d69     low/lite/experim
   eaf60:	6e65 6174 2f6c 696d 7263 2f6f 656b 6e72     ental/micro/kern
   eaf70:	6c65 2f73 6f70 7472 6261 656c 6f5f 7470     els/portable_opt
   eaf80:	6d69 7a69 6465 642f 7065 6874 6977 6573     imized/depthwise
   eaf90:	635f 6e6f 2e76 7063 0070 754d 746c 7069     _conv.cpp.Multip
   eafa0:	656c 6420 7065 6874 6977 6573 6320 6e6f     le depthwise con
   eafb0:	2076 706f 2073 616d 6374 2068 706f 6974     v ops match opti
   eafc0:	696d 617a 6974 6e6f 7020 7261 6d61 7465     mization paramet
   eafd0:	7265 2c73 6220 7475 6f20 6c6e 2079 6874     ers, but only th
   eafe0:	2065 6966 7372 2074 6977 6c6c 7520 6573     e first will use
   eaff0:	7420 6568 6620 7361 2074 6170 6874 202c      the fast path, 
   eb000:	6562 6163 7375 2065 6874 7265 2765 2073     because there's 
   eb010:	6e6f 796c 6f20 656e 5220 4d41 6320 6361     only one RAM cac
   eb020:	6568 6120 6176 6c69 6261 656c 5300 7a69     he available.Siz
   eb030:	2065 6f74 206f 616c 6772 2065 6f66 2072     e too large for 
   eb040:	6572 6873 7061 6465 7720 6965 6867 2074     reshaped weight 
   eb050:	7562 6666 7265 2820 6425 6e20 6565 6564     buffer (%d neede
   eb060:	2c64 2520 2064 7661 6961 616c 6c62 2965     d, %d available)
   eb070:	5400 6f6f 6d20 6e61 2079 7562 6666 7265     .Too many buffer
   eb080:	2073 6d28 7861 6920 2073 6425 0029 7562     s (max is %d).bu
   eb090:	6666 7265 6920 646e 7865 2520 2064 7369     ffer index %d is
   eb0a0:	6f20 7475 6973 6564 7220 6e61 6567 3020      outside range 0
   eb0b0:	7420 206f 6425 4f00 6576 6c72 7061 203a      to %d.Overlap: 
   eb0c0:	6425 2820 6425 3e3d 6425 202c 6425 3e2d     %d (%d=>%d, %d->
   eb0d0:	6425 2029 7376 2520 2064 2528 3d64 253e     %d) vs %d (%d=>%
   eb0e0:	2c64 2520 2d64 253e 2964 0000               d, %d->%d)..

000eb0ec <_ZTVN6tflite19GreedyMemoryPlannerE>:
	...
   eb0f4:	3679 000e 367f 000e 368d 000e 38d7 000e     y6...6...6...8..
   eb104:	367b 000e 390d 000e 552f 6573 7372 622f     {6...9../Users/b
   eb114:	6173 7274 6d6f 442f 7665 6c65 706f 656d     satrom/Developme
   eb124:	746e 702f 7261 6974 6c63 2f65 696c 7262     nt/particle/libr
   eb134:	7261 6569 2f73 6150 7472 6369 656c 545f     aries/Particle_T
   eb144:	6e65 6f73 4672 6f6c 4c77 7469 5f65 7845     ensorFlowLite_Ex
   eb154:	6d61 6c70 7365 682f 6c65 6f6c 775f 726f     amples/hello_wor
   eb164:	646c 6c2f 6269 542f 6e65 6f73 4672 6f6c     ld/lib/TensorFlo
   eb174:	4c77 7469 2f65 7273 2f63 6574 736e 726f     wLite/src/tensor
   eb184:	6c66 776f 6c2f 7469 2f65 656b 6e72 6c65     flow/lite/kernel
   eb194:	2f73 656b 6e72 6c65 755f 6974 2e6c 7063     s/kernel_util.cp
   eb1a4:	0070 6e69 7570 5f74 7270 646f 6375 5f74     p.input_product_
   eb1b4:	6373 6c61 2065 3d3e 3020 7300 6474 3a3a     scale >= 0.std::
   eb1c4:	6261 2873 6e69 7570 5f74 7270 646f 6375     abs(input_produc
   eb1d4:	5f74 6373 6c61 2065 202d 6962 7361 735f     t_scale - bias_s
   eb1e4:	6163 656c 2029 3d3c 3120 2d65 2036 202a     cale) <= 1e-6 * 
   eb1f4:	7473 3a64 6d3a 6e69 6928 706e 7475 705f     std::min(input_p
   eb204:	6f72 7564 7463 735f 6163 656c 202c 6962     roduct_scale, bi
   eb214:	7361 735f 6163 656c 0029 6166 736c 0065     as_scale).false.
   eb224:	6e69 7570 2d74 713e 6175 746e 7a69 7461     input->quantizat
   eb234:	6f69 2e6e 7974 6570 6600 6c69 6574 2d72     ion.type.filter-
   eb244:	743e 7079 0065 6966 746c 7265 3e2d 6964     >type.filter->di
   eb254:	736d 3e2d 6164 6174 615b 6666 6e69 5f65     ms->data[affine_
   eb264:	7571 6e61 6974 617a 6974 6e6f 3e2d 7571     quantization->qu
   eb274:	6e61 6974 657a 5f64 6964 656d 736e 6f69     antized_dimensio
   eb284:	5d6e 6100 6666 6e69 5f65 7571 6e61 6974     n].affine_quanti
   eb294:	617a 6974 6e6f 3e2d 6373 6c61 2d65 733e     zation->scale->s
   eb2a4:	7a69 0065 3164 3d20 203d 3264 7c20 207c     ize.d1 == d2 || 
   eb2b4:	3164 3d20 203d 2031 7c7c 6420 2032 3d3d     d1 == 1 || d2 ==
   eb2c4:	3120 0000                                    1..

000eb2c8 <_ZTVN5spark13CellularClassE>:
	...
   eb2d0:	43cb 000e 43c1 000e 43b7 000e 436d 000e     .C...C...C..mC..
   eb2e0:	43ab 000e 439f 000e 4393 000e 438b 000e     .C...C...C...C..
   eb2f0:	4381 000e 4377 000e 47d9 000e               .C..wC...G..

000eb2fc <_ZTVN5spark13EthernetClassE>:
	...
   eb304:	4475 000e 446b 000e 4461 000e 4457 000e     uD..kD..aD..WD..
   eb314:	444b 000e 443f 000e 4433 000e 442b 000e     KD..?D..3D..+D..
   eb324:	4421 000e 4417 000e 47d9 000e               !D...D...G..

000eb330 <_ZTV7TwoWire>:
	...
   eb338:	4495 000e 44df 000e 44b7 000e 4497 000e     .D...D...D...D..
   eb348:	44bf 000e 44c7 000e 44cf 000e 44d7 000e     .D...D...D...D..

000eb358 <_ZTV9IPAddress>:
	...
   eb360:	4529 000e 4519 000e 451b 000e 6162 6475     )E...E...E..baud
   eb370:	5300 7265 6169 006c 6553 6972 6c61 0031     .Serial.Serial1.
   eb380:	6170 6172 006d 6d63 0064 6469 6800 646e     param.cmd.id.hnd
   eb390:	7300 7274 006d 6966 746c 6c00 6c76 6100     .strm.filt.lvl.a
   eb3a0:	6464 6148 646e 656c 0072 6572 6f6d 6576     ddHandler.remove
   eb3b0:	6148 646e 656c 0072 6e65 6d75 6148 646e     Handler.enumHand
   eb3c0:	656c 7372 4a00 4f53 534e 7274 6165 4c6d     lers.JSONStreamL
   eb3d0:	676f 6148 646e 656c 0072 7061 0070 3025     ogHandler.app.%0
   eb3e0:	3031 2075 5d00 0020 202c 2800 3a29 0020     10u .] ., .(): .
   eb3f0:	6f63 6564 3d20 0020 6925 6400 7465 6961     code = .%i.detai
   eb400:	736c 3d20 0020 6e6c 6600 006e 6f63 6564     ls = .ln.fn.code
   eb410:	6400 7465 6961 006c 6f6e 656e 7400 6172     .detail.none.tra
   eb420:	6563 6900 666e 006f 6177 6e72 6500 7272     ce.info.warn.err
   eb430:	726f 7000 6e61 6369 6100 6c6c 0000 0000     or.panic.all....

000eb440 <_ZTVN5spark9MeshClassE>:
	...
   eb448:	466b 000e 4661 000e 4657 000e 464d 000e     kF..aF..WF..MF..
   eb458:	4641 000e 4635 000e 4629 000e 4621 000e     AF..5F..)F..!F..
   eb468:	4617 000e 460d 000e 47d9 000e               .F...F...G..

000eb474 <_ZTVN5spark12NetworkClassE>:
	...
   eb47c:	4771 000e 477b 000e 4785 000e 478f 000e     qG..{G...G...G..
   eb48c:	4799 000e 47a5 000e 47b1 000e 47bd 000e     .G...G...G...G..
   eb49c:	47c5 000e 47cf 000e 47d9 000e               .G...G...G..

000eb4a8 <_ZTV8SPIClass>:
	...
   eb4b0:	4999 000e 499b 000e 005a 2b25 3330 3a64     .I...I..Z.%+03d:
   eb4c0:	3025 7532 2500 2d59 6d25 252d 5464 4825     %02u.%Y-%m-%dT%H
   eb4d0:	253a 3a4d 5325 7a25 6100 6373 6974 656d     :%M:%S%z.asctime
   eb4e0:	0000 0000                                   ....

000eb4e4 <_ZTV11USARTSerial>:
	...
   eb4ec:	4a05 000e 4a55 000e 4a63 000e 48b9 000e     .J..UJ..cJ...H..
   eb4fc:	4a19 000e 4a3b 000e 4a27 000e 4a4f 000e     .J..;J..'J..OJ..
   eb50c:	4a07 000e 4a0b 000e                         .J...J..

000eb514 <_ZTV9USBSerial>:
	...
   eb51c:	4b0d 000e 4b5d 000e 4b6b 000e 48b9 000e     .K..]K..kK...H..
   eb52c:	4b49 000e 4b0f 000e 4b25 000e 4b57 000e     IK...K..%K..WK..
   eb53c:	4b3b 000e 4b09 000e 7865 0070 7865 6670     ;K...K..exp.expf
   eb54c:	0000 0000 6f6c 6667 0000 0000 7173 7472     ....logf....sqrt
   eb55c:	0066 0000                                   f...

000eb560 <halF>:
   eb560:	0000 0000 0000 3fe0 0000 0000 0000 bfe0     .......?........

000eb570 <ln2LO>:
   eb570:	3c76 3579 39ef 3dea 3c76 3579 39ef bdea     v<y5.9.=v<y5.9..

000eb580 <ln2HI>:
   eb580:	0000 fee0 2e42 3fe6 0000 fee0 2e42 bfe6     ....B..?....B...

000eb590 <halF>:
   eb590:	0000 3f00 0000 bf00                         ...?....

000eb598 <ln2LO>:
   eb598:	f7d1 3717 f7d1 b717                         ...7....

000eb5a0 <ln2HI>:
   eb5a0:	7180 3f31 7180 bf31                         .q1?.q1.

000eb5a8 <npio2_hw>:
   eb5a8:	0f00 3fc9 0f00 4049 cb00 4096 0f00 40c9     ...?..I@...@...@
   eb5b8:	5300 40fb cb00 4116 ed00 412f 0f00 4149     .S.@...A../A..IA
   eb5c8:	3100 4162 5300 417b 3a00 418a cb00 4196     .1bA.S{A.:.A...A
   eb5d8:	5c00 41a3 ed00 41af 7e00 41bc 0f00 41c9     .\.A...A.~.A...A
   eb5e8:	a000 41d5 3100 41e2 c200 41ee 5300 41fb     ...A.1.A...A.S.A
   eb5f8:	f200 4203 3a00 420a 8300 4210 cb00 4216     ...B.:.B...B...B
   eb608:	1400 421d 5c00 4223 a500 4229 ed00 422f     ...B.\#B..)B../B
   eb618:	3600 4236 7e00 423c c700 4242 0f00 4249     .66B.~<B..BB..IB

000eb628 <two_over_pi>:
   eb628:	00a2 0000 00f9 0000 0083 0000 006e 0000     ............n...
   eb638:	004e 0000 0044 0000 0015 0000 0029 0000     N...D.......)...
   eb648:	00fc 0000 0027 0000 0057 0000 00d1 0000     ....'...W.......
   eb658:	00f5 0000 0034 0000 00dd 0000 00c0 0000     ....4...........
   eb668:	00db 0000 0062 0000 0095 0000 0099 0000     ....b...........
   eb678:	003c 0000 0043 0000 0090 0000 0041 0000     <...C.......A...
   eb688:	00fe 0000 0051 0000 0063 0000 00ab 0000     ....Q...c.......
   eb698:	00de 0000 00bb 0000 00c5 0000 0061 0000     ............a...
   eb6a8:	00b7 0000 0024 0000 006e 0000 003a 0000     ....$...n...:...
   eb6b8:	0042 0000 004d 0000 00d2 0000 00e0 0000     B...M...........
   eb6c8:	0006 0000 0049 0000 002e 0000 00ea 0000     ....I...........
   eb6d8:	0009 0000 00d1 0000 0092 0000 001c 0000     ................
   eb6e8:	00fe 0000 001d 0000 00eb 0000 001c 0000     ................
   eb6f8:	00b1 0000 0029 0000 00a7 0000 003e 0000     ....).......>...
   eb708:	00e8 0000 0082 0000 0035 0000 00f5 0000     ........5.......
   eb718:	002e 0000 00bb 0000 0044 0000 0084 0000     ........D.......
   eb728:	00e9 0000 009c 0000 0070 0000 0026 0000     ........p...&...
   eb738:	00b4 0000 005f 0000 007e 0000 0041 0000     ...._...~...A...
   eb748:	0039 0000 0091 0000 00d6 0000 0039 0000     9...........9...
   eb758:	0083 0000 0053 0000 0039 0000 00f4 0000     ....S...9.......
   eb768:	009c 0000 0084 0000 005f 0000 008b 0000     ........_.......
   eb778:	00bd 0000 00f9 0000 0028 0000 003b 0000     ........(...;...
   eb788:	001f 0000 00f8 0000 0097 0000 00ff 0000     ................
   eb798:	00de 0000 0005 0000 0098 0000 000f 0000     ................
   eb7a8:	00ef 0000 002f 0000 0011 0000 008b 0000     ..../...........
   eb7b8:	005a 0000 000a 0000 006d 0000 001f 0000     Z.......m.......
   eb7c8:	006d 0000 0036 0000 007e 0000 00cf 0000     m...6...~.......
   eb7d8:	0027 0000 00cb 0000 0009 0000 00b7 0000     '...............
   eb7e8:	004f 0000 0046 0000 003f 0000 0066 0000     O...F...?...f...
   eb7f8:	009e 0000 005f 0000 00ea 0000 002d 0000     ...._.......-...
   eb808:	0075 0000 0027 0000 00ba 0000 00c7 0000     u...'...........
   eb818:	00eb 0000 00e5 0000 00f1 0000 007b 0000     ............{...
   eb828:	003d 0000 0007 0000 0039 0000 00f7 0000     =.......9.......
   eb838:	008a 0000 0052 0000 0092 0000 00ea 0000     ....R...........
   eb848:	006b 0000 00fb 0000 005f 0000 00b1 0000     k......._.......
   eb858:	001f 0000 008d 0000 005d 0000 0008 0000     ........].......
   eb868:	0056 0000 0003 0000 0030 0000 0046 0000     V.......0...F...
   eb878:	00fc 0000 007b 0000 006b 0000 00ab 0000     ....{...k.......
   eb888:	00f0 0000 00cf 0000 00bc 0000 0020 0000     ............ ...
   eb898:	009a 0000 00f4 0000 0036 0000 001d 0000     ........6.......
   eb8a8:	00a9 0000 00e3 0000 0091 0000 0061 0000     ............a...
   eb8b8:	005e 0000 00e6 0000 001b 0000 0008 0000     ^...............
   eb8c8:	0065 0000 0099 0000 0085 0000 005f 0000     e..........._...
   eb8d8:	0014 0000 00a0 0000 0068 0000 0040 0000     ........h...@...
   eb8e8:	008d 0000 00ff 0000 00d8 0000 0080 0000     ................
   eb8f8:	004d 0000 0073 0000 0027 0000 0031 0000     M...s...'...1...
   eb908:	0006 0000 0006 0000 0015 0000 0056 0000     ............V...
   eb918:	00ca 0000 0073 0000 00a8 0000 00c9 0000     ....s...........
   eb928:	0060 0000 00e2 0000 007b 0000 00c0 0000     `.......{.......
   eb938:	008c 0000 006b 0000                         ....k...

000eb940 <init_jk>:
   eb940:	0004 0000 0007 0000 0009 0000               ............

000eb94c <PIo2>:
   eb94c:	0000 3fc9 0000 39f0 0000 37da 0000 33a2     ...?...9...7...3
   eb95c:	0000 2e84 0000 2b50 0000 27c2 0000 22d0     ......P+...'..."
   eb96c:	0000 1fc4 0000 1bc6 0000 1744               ..........D.

000eb978 <__sf_fake_stdin>:
	...

000eb998 <__sf_fake_stdout>:
	...

000eb9b8 <__sf_fake_stderr>:
	...

000eb9d8 <_global_impure_ptr>:
   eb9d8:	c270 2003                                   p.. 

000eb9dc <link_const_variable_data_end>:
   eb9dc:	000d4199 	.word	0x000d4199
   eb9e0:	000d43a5 	.word	0x000d43a5
   eb9e4:	000d6439 	.word	0x000d6439
   eb9e8:	000e435d 	.word	0x000e435d
   eb9ec:	000e43d5 	.word	0x000e43d5
   eb9f0:	000e4481 	.word	0x000e4481
   eb9f4:	000e45fd 	.word	0x000e45fd
   eb9f8:	000e4721 	.word	0x000e4721
   eb9fc:	000e48a5 	.word	0x000e48a5
   eba00:	000e497d 	.word	0x000e497d
   eba04:	000e49d9 	.word	0x000e49d9
   eba08:	000e49f1 	.word	0x000e49f1
   eba0c:	000e4de5 	.word	0x000e4de5
   eba10:	000e4e39 	.word	0x000e4e39
   eba14:	000e4efd 	.word	0x000e4efd
   eba18:	000e4f81 	.word	0x000e4f81
   eba1c:	000e5005 	.word	0x000e5005
