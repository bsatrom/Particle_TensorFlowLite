
/Users/bsatrom/Development/particle/libraries/Particle_TensorFlowLite_Examples/hello_world/target/1.4.2/photon/hello_world.elf:     file format elf32-littlearm

Sections:
Idx Name          Size      VMA       LMA       File off  Algn
  0 .module_info  00000018  080a0000  080a0000  00010000  2**2
                  CONTENTS, ALLOC, LOAD, READONLY, DATA
  1 .dynalib      00000004  080a0018  080a0018  00010018  2**2
                  CONTENTS, ALLOC, LOAD, READONLY, DATA
  2 .text         00017c80  080a0020  080a0020  00010020  2**3
                  CONTENTS, ALLOC, LOAD, READONLY, CODE
  3 .backup       00000004  40024000  080b7ca0  00034000  2**2
                  CONTENTS, ALLOC, LOAD, DATA
  4 .data         00000594  20000000  080b7ca4  00040000  2**2
                  CONTENTS, ALLOC, LOAD, DATA
  5 .bss          00002650  20000594  20000594  00050594  2**2
                  ALLOC
  6 .module_info_suffix 00000028  080b8238  080b8238  00048238  2**0
                  CONTENTS, ALLOC, LOAD, READONLY, DATA
  7 .module_info_crc 00000004  080b8260  080b8260  00048260  2**0
                  CONTENTS, ALLOC, LOAD, READONLY, DATA
  8 .debug_info   00264637  00000000  00000000  00048264  2**0
                  CONTENTS, READONLY, DEBUGGING
  9 .debug_abbrev 00024bbe  00000000  00000000  002ac89b  2**0
                  CONTENTS, READONLY, DEBUGGING
 10 .debug_loc    0004853e  00000000  00000000  002d1459  2**0
                  CONTENTS, READONLY, DEBUGGING
 11 .debug_aranges 000030a0  00000000  00000000  00319997  2**0
                  CONTENTS, READONLY, DEBUGGING
 12 .debug_ranges 000089c8  00000000  00000000  0031ca37  2**0
                  CONTENTS, READONLY, DEBUGGING
 13 .debug_macro  00039d42  00000000  00000000  003253ff  2**0
                  CONTENTS, READONLY, DEBUGGING
 14 .debug_line   00056977  00000000  00000000  0035f141  2**0
                  CONTENTS, READONLY, DEBUGGING
 15 .debug_str    00132c5c  00000000  00000000  003b5ab8  2**0
                  CONTENTS, READONLY, DEBUGGING
 16 .debug_frame  0000f1d8  00000000  00000000  004e8714  2**2
                  CONTENTS, READONLY, DEBUGGING

Disassembly of section .text:

080a0020 <module_user_pre_init>:
/**
 * Initializes this user module. Returns the start of the heap.
 */
void* module_user_pre_init() {

    if ( (&link_global_data_start!=&link_global_data_initial_values) && (link_global_data_size != 0))
 80a0020:	4809      	ldr	r0, [pc, #36]	; (80a0048 <module_user_pre_init+0x28>)
 80a0022:	490a      	ldr	r1, [pc, #40]	; (80a004c <module_user_pre_init+0x2c>)
extern constructor_ptr_t link_constructors_end;

/**
 * Initializes this user module. Returns the start of the heap.
 */
void* module_user_pre_init() {
 80a0024:	b508      	push	{r3, lr}

    if ( (&link_global_data_start!=&link_global_data_initial_values) && (link_global_data_size != 0))
 80a0026:	4288      	cmp	r0, r1
 80a0028:	d005      	beq.n	80a0036 <module_user_pre_init+0x16>
 80a002a:	4a09      	ldr	r2, [pc, #36]	; (80a0050 <module_user_pre_init+0x30>)
 80a002c:	4282      	cmp	r2, r0
 80a002e:	d002      	beq.n	80a0036 <module_user_pre_init+0x16>
    {
        memcpy(&link_global_data_start, &link_global_data_initial_values, link_global_data_size);
 80a0030:	1a12      	subs	r2, r2, r0
 80a0032:	f013 fd4a 	bl	80b3aca <memcpy>
    }

    memset(&link_bss_location, 0, link_bss_size );
 80a0036:	4807      	ldr	r0, [pc, #28]	; (80a0054 <module_user_pre_init+0x34>)
 80a0038:	4a07      	ldr	r2, [pc, #28]	; (80a0058 <module_user_pre_init+0x38>)
 80a003a:	2100      	movs	r1, #0
 80a003c:	1a12      	subs	r2, r2, r0
 80a003e:	f013 fd4f 	bl	80b3ae0 <memset>
    return &link_heap_start;
}
 80a0042:	4806      	ldr	r0, [pc, #24]	; (80a005c <module_user_pre_init+0x3c>)
 80a0044:	bd08      	pop	{r3, pc}
 80a0046:	bf00      	nop
 80a0048:	20000000 	.word	0x20000000
 80a004c:	080b7ca4 	.word	0x080b7ca4
 80a0050:	20000594 	.word	0x20000594
 80a0054:	20000594 	.word	0x20000594
 80a0058:	20002be4 	.word	0x20002be4
 80a005c:	20002be4 	.word	0x20002be4

080a0060 <module_user_init>:
extern constructor_ptr_t link_constructors_location[];
extern constructor_ptr_t link_constructors_end;
#define link_constructors_size   ((unsigned long)&link_constructors_end  -  (unsigned long)&link_constructors_location )

void module_user_init()
{
 80a0060:	b570      	push	{r4, r5, r6, lr}
    module_user_init_hook();
 80a0062:	f010 fbe3 	bl	80b082c <module_user_init_hook>
 80a0066:	4b07      	ldr	r3, [pc, #28]	; (80a0084 <module_user_init+0x24>)
 80a0068:	4c07      	ldr	r4, [pc, #28]	; (80a0088 <module_user_init+0x28>)
 80a006a:	461e      	mov	r6, r3
 80a006c:	1ae4      	subs	r4, r4, r3
 80a006e:	08a4      	lsrs	r4, r4, #2

    // invoke constructors
    int ctor_num;
    for (ctor_num=0; ctor_num < link_constructors_size/sizeof(constructor_ptr_t); ctor_num++ )
 80a0070:	2500      	movs	r5, #0
 80a0072:	42a5      	cmp	r5, r4
 80a0074:	d004      	beq.n	80a0080 <module_user_init+0x20>
    {
        link_constructors_location[ctor_num]();
 80a0076:	f856 3025 	ldr.w	r3, [r6, r5, lsl #2]
 80a007a:	4798      	blx	r3
{
    module_user_init_hook();

    // invoke constructors
    int ctor_num;
    for (ctor_num=0; ctor_num < link_constructors_size/sizeof(constructor_ptr_t); ctor_num++ )
 80a007c:	3501      	adds	r5, #1
 80a007e:	e7f8      	b.n	80a0072 <module_user_init+0x12>
    {
        link_constructors_location[ctor_num]();
    }
}
 80a0080:	bd70      	pop	{r4, r5, r6, pc}
 80a0082:	bf00      	nop
 80a0084:	080b7c5c 	.word	0x080b7c5c
 80a0088:	080b7c98 	.word	0x080b7c98

080a008c <module_user_setup>:

/**
 * Export these functions with a fuller name so they don't clash with the setup/loop wrappers in the system module.
 */
void module_user_setup() {
    setup();
 80a008c:	f000 b864 	b.w	80a0158 <setup>

080a0090 <module_user_loop>:
}

void module_user_loop() {
 80a0090:	b508      	push	{r3, lr}
    loop();
 80a0092:	f000 f8fd 	bl	80a0290 <loop>
    _post_loop();
}
 80a0096:	e8bd 4008 	ldmia.w	sp!, {r3, lr}
    setup();
}

void module_user_loop() {
    loop();
    _post_loop();
 80a009a:	f010 bb89 	b.w	80b07b0 <_post_loop>

080a009e <_Znaj>:
	return malloc(size);
}

void *operator new[](size_t size)
{
	return malloc(size);
 80a009e:	f00f bfb7 	b.w	80b0010 <malloc>

080a00a2 <_ZdlPv>:
 80a00a2:	f00f bfbd 	b.w	80b0020 <free>

080a00a6 <_ZdaPv>:
	free(p);
}

void operator delete[](void *p)
{
	free(p);
 80a00a6:	f00f bfbb 	b.w	80b0020 <free>
	...

080a00ac <_exit>:
int _getpid(void)
{
	return 1;
}

void _exit(int status) {
 80a00ac:	b508      	push	{r3, lr}
    PANIC(Exit,"Exit Called");
 80a00ae:	4a03      	ldr	r2, [pc, #12]	; (80a00bc <_exit+0x10>)
 80a00b0:	2100      	movs	r1, #0
 80a00b2:	2007      	movs	r0, #7
 80a00b4:	f00f ff28 	bl	80aff08 <panic_>
 80a00b8:	e7fe      	b.n	80a00b8 <_exit+0xc>
 80a00ba:	bf00      	nop
 80a00bc:	080afcf5 	.word	0x080afcf5

080a00c0 <__cxa_guard_acquire>:

/* Provide default implemenation for __cxa_guard_acquire() and
 * __cxa_guard_release(). Note: these must be revisited if a multitasking
 * OS is ported to this platform. */
__extension__ typedef int __guard __attribute__((mode (__DI__)));
int __cxa_guard_acquire(__guard *g) {return !*(char *)(g);};
 80a00c0:	7800      	ldrb	r0, [r0, #0]
 80a00c2:	fab0 f080 	clz	r0, r0
 80a00c6:	0940      	lsrs	r0, r0, #5
 80a00c8:	4770      	bx	lr

080a00ca <__cxa_guard_release>:
void __cxa_guard_release (__guard *g) {*(char *)g = 1;};
 80a00ca:	2301      	movs	r3, #1
 80a00cc:	7003      	strb	r3, [r0, #0]
 80a00ce:	4770      	bx	lr

080a00d0 <TfLiteIntArrayEqualsArray>:
  if (a == NULL || b == NULL) return 0;
  return TfLiteIntArrayEqualsArray(a, b->size, b->data);
}

int TfLiteIntArrayEqualsArray(const TfLiteIntArray* a, int b_size,
                              const int b_data[]) {
 80a00d0:	b530      	push	{r4, r5, lr}
  if (a == NULL) return (b_size == 0);
 80a00d2:	b918      	cbnz	r0, 80a00dc <TfLiteIntArrayEqualsArray+0xc>
 80a00d4:	fab1 f081 	clz	r0, r1
 80a00d8:	0940      	lsrs	r0, r0, #5
 80a00da:	bd30      	pop	{r4, r5, pc}
  if (a->size != b_size) return 0;
 80a00dc:	6803      	ldr	r3, [r0, #0]
 80a00de:	4299      	cmp	r1, r3
 80a00e0:	d10c      	bne.n	80a00fc <TfLiteIntArrayEqualsArray+0x2c>
 80a00e2:	2300      	movs	r3, #0
  int i = 0;
  for (; i < a->size; i++)
 80a00e4:	428b      	cmp	r3, r1
 80a00e6:	da07      	bge.n	80a00f8 <TfLiteIntArrayEqualsArray+0x28>
    if (a->data[i] != b_data[i]) return 0;
 80a00e8:	f850 5f04 	ldr.w	r5, [r0, #4]!
 80a00ec:	f852 4023 	ldr.w	r4, [r2, r3, lsl #2]
 80a00f0:	42a5      	cmp	r5, r4
 80a00f2:	d103      	bne.n	80a00fc <TfLiteIntArrayEqualsArray+0x2c>
int TfLiteIntArrayEqualsArray(const TfLiteIntArray* a, int b_size,
                              const int b_data[]) {
  if (a == NULL) return (b_size == 0);
  if (a->size != b_size) return 0;
  int i = 0;
  for (; i < a->size; i++)
 80a00f4:	3301      	adds	r3, #1
 80a00f6:	e7f5      	b.n	80a00e4 <TfLiteIntArrayEqualsArray+0x14>
    if (a->data[i] != b_data[i]) return 0;
  return 1;
 80a00f8:	2001      	movs	r0, #1
 80a00fa:	bd30      	pop	{r4, r5, pc}
}

int TfLiteIntArrayEqualsArray(const TfLiteIntArray* a, int b_size,
                              const int b_data[]) {
  if (a == NULL) return (b_size == 0);
  if (a->size != b_size) return 0;
 80a00fc:	2000      	movs	r0, #0
  int i = 0;
  for (; i < a->size; i++)
    if (a->data[i] != b_data[i]) return 0;
  return 1;
}
 80a00fe:	bd30      	pop	{r4, r5, pc}

080a0100 <TfLiteIntArrayEqual>:
  static TfLiteIntArray dummy;
  return sizeof(dummy) + sizeof(dummy.data[0]) * size;
}

int TfLiteIntArrayEqual(const TfLiteIntArray* a, const TfLiteIntArray* b) {
  if (a == b) return 1;
 80a0100:	4288      	cmp	r0, r1
 80a0102:	d005      	beq.n	80a0110 <TfLiteIntArrayEqual+0x10>
  if (a == NULL || b == NULL) return 0;
 80a0104:	b130      	cbz	r0, 80a0114 <TfLiteIntArrayEqual+0x14>
 80a0106:	b131      	cbz	r1, 80a0116 <TfLiteIntArrayEqual+0x16>
  return TfLiteIntArrayEqualsArray(a, b->size, b->data);
 80a0108:	1d0a      	adds	r2, r1, #4
 80a010a:	6809      	ldr	r1, [r1, #0]
 80a010c:	f7ff bfe0 	b.w	80a00d0 <TfLiteIntArrayEqualsArray>
  static TfLiteIntArray dummy;
  return sizeof(dummy) + sizeof(dummy.data[0]) * size;
}

int TfLiteIntArrayEqual(const TfLiteIntArray* a, const TfLiteIntArray* b) {
  if (a == b) return 1;
 80a0110:	2001      	movs	r0, #1
 80a0112:	4770      	bx	lr
 80a0114:	4770      	bx	lr
  if (a == NULL || b == NULL) return 0;
 80a0116:	4608      	mov	r0, r1
  return TfLiteIntArrayEqualsArray(a, b->size, b->data);
}
 80a0118:	4770      	bx	lr
	...

080a011c <TfLiteTypeGetName>:
  }
  tensor->bytes = num_bytes;
}
#endif  // TF_LITE_STATIC_MEMORY

const char* TfLiteTypeGetName(TfLiteType type) {
 80a011c:	280a      	cmp	r0, #10
 80a011e:	bf9a      	itte	ls
 80a0120:	4b02      	ldrls	r3, [pc, #8]	; (80a012c <TfLiteTypeGetName+0x10>)
 80a0122:	f853 0020 	ldrls.w	r0, [r3, r0, lsl #2]
 80a0126:	4802      	ldrhi	r0, [pc, #8]	; (80a0130 <TfLiteTypeGetName+0x14>)
      return "STRING";
    case kTfLiteFloat16:
      return "FLOAT16";
  }
  return "Unknown type";
}
 80a0128:	4770      	bx	lr
 80a012a:	bf00      	nop
 80a012c:	080b3bdc 	.word	0x080b3bdc
 80a0130:	080b3b88 	.word	0x080b3b88

080a0134 <_ZN6tflite18MicroErrorReporterD1Ev>:

namespace tflite {

class MicroErrorReporter : public ErrorReporter {
 public:
  ~MicroErrorReporter() {}
 80a0134:	4770      	bx	lr

080a0136 <_ZN6tflite3ops5micro14AllOpsResolverD1Ev>:

namespace tflite {
namespace ops {
namespace micro {

class AllOpsResolver : public MicroMutableOpResolver {
 80a0136:	4770      	bx	lr

080a0138 <_ZN6tflite3ops5micro14AllOpsResolverD0Ev>:
 80a0138:	b510      	push	{r4, lr}
 80a013a:	4604      	mov	r4, r0
 80a013c:	f241 0108 	movw	r1, #4104	; 0x1008
 80a0140:	f010 fc5b 	bl	80b09fa <_ZdlPvj>
 80a0144:	4620      	mov	r0, r4
 80a0146:	bd10      	pop	{r4, pc}

080a0148 <_ZN6tflite18MicroErrorReporterD0Ev>:
 80a0148:	b510      	push	{r4, lr}
 80a014a:	4604      	mov	r4, r0
 80a014c:	2104      	movs	r1, #4
 80a014e:	f010 fc54 	bl	80b09fa <_ZdlPvj>
 80a0152:	4620      	mov	r0, r4
 80a0154:	bd10      	pop	{r4, pc}
	...

080a0158 <setup>:
uint8_t tensor_arena[kTensorArenaSize];
} // namespace

// The name of this function is important for Arduino compatibility.
void setup()
{
 80a0158:	b573      	push	{r0, r1, r4, r5, r6, lr}
  // Set up logging. Google style is to avoid globals or statics because of
  // lifetime uncertainty, but since this has a trivial destructor it's okay.
  // NOLINTNEXTLINE(runtime-global-variables)
  static tflite::MicroErrorReporter micro_error_reporter;
 80a015a:	4c3a      	ldr	r4, [pc, #232]	; (80a0244 <setup+0xec>)
 80a015c:	6823      	ldr	r3, [r4, #0]
 80a015e:	07d9      	lsls	r1, r3, #31
 80a0160:	d40b      	bmi.n	80a017a <setup+0x22>
 80a0162:	4620      	mov	r0, r4
 80a0164:	f7ff ffac 	bl	80a00c0 <__cxa_guard_acquire>
 80a0168:	b138      	cbz	r0, 80a017a <setup+0x22>
 80a016a:	4620      	mov	r0, r4
 80a016c:	f7ff ffad 	bl	80a00ca <__cxa_guard_release>
 80a0170:	4a35      	ldr	r2, [pc, #212]	; (80a0248 <setup+0xf0>)
 80a0172:	4936      	ldr	r1, [pc, #216]	; (80a024c <setup+0xf4>)
 80a0174:	4836      	ldr	r0, [pc, #216]	; (80a0250 <setup+0xf8>)
 80a0176:	f010 fc3b 	bl	80b09f0 <__aeabi_atexit>
  error_reporter = &micro_error_reporter;
 80a017a:	4b35      	ldr	r3, [pc, #212]	; (80a0250 <setup+0xf8>)
 80a017c:	4c35      	ldr	r4, [pc, #212]	; (80a0254 <setup+0xfc>)

  // Map the model into a usable data structure. This doesn't involve any
  // copying or parsing, it's a very lightweight operation.
  model = tflite::GetModel(g_sine_model_data);
 80a017e:	4a36      	ldr	r2, [pc, #216]	; (80a0258 <setup+0x100>)
{
  // Set up logging. Google style is to avoid globals or statics because of
  // lifetime uncertainty, but since this has a trivial destructor it's okay.
  // NOLINTNEXTLINE(runtime-global-variables)
  static tflite::MicroErrorReporter micro_error_reporter;
  error_reporter = &micro_error_reporter;
 80a0180:	6023      	str	r3, [r4, #0]
// Helpers to get a typed pointer to the root object contained in the buffer.
template<typename T> T *GetMutableRoot(void *buf) {
  EndianCheck();
  return reinterpret_cast<T *>(
      reinterpret_cast<uint8_t *>(buf) +
      EndianScalar(*reinterpret_cast<uoffset_t *>(buf)));
 80a0182:	4b36      	ldr	r3, [pc, #216]	; (80a025c <setup+0x104>)
 80a0184:	4616      	mov	r6, r2
 80a0186:	6818      	ldr	r0, [r3, #0]
 80a0188:	18c1      	adds	r1, r0, r3
// "tables" use an offset table (possibly shared) that allows fields to be
// omitted and added at will, but uses an extra indirection to read.
class Table {
 public:
  const uint8_t *GetVTable() const {
    return data_ - ReadScalar<soffset_t>(data_);
 80a018a:	58c3      	ldr	r3, [r0, r3]

  // Map the model into a usable data structure. This doesn't involve any
  // copying or parsing, it's a very lightweight operation.
  model = tflite::GetModel(g_sine_model_data);
 80a018c:	6011      	str	r1, [r2, #0]
 80a018e:	1acb      	subs	r3, r1, r3
    auto vtable = GetVTable();
    // The first element is the size of the vtable (fields + type id + itself).
    auto vtsize = ReadScalar<voffset_t>(vtable);
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
 80a0190:	8818      	ldrh	r0, [r3, #0]
 80a0192:	2804      	cmp	r0, #4
 80a0194:	d905      	bls.n	80a01a2 <setup+0x4a>

template<typename T>
// UBSAN: C++ aliasing type rules, see std::bit_cast<> for details.
__supress_ubsan__("alignment")
T ReadScalar(const void *p) {
  return EndianScalar(*reinterpret_cast<const T *>(p));
 80a0196:	889a      	ldrh	r2, [r3, #4]
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
 80a0198:	b122      	cbz	r2, 80a01a4 <setup+0x4c>
 80a019a:	588a      	ldr	r2, [r1, r2]
  if (model->version() != TFLITE_SCHEMA_VERSION)
 80a019c:	2a03      	cmp	r2, #3
 80a019e:	d009      	beq.n	80a01b4 <setup+0x5c>
 80a01a0:	e000      	b.n	80a01a4 <setup+0x4c>
 80a01a2:	2200      	movs	r2, #0
  {
    error_reporter->Report(
        "Model provided is schema version %d not equal "
        "to supported version %d.",
        model->version(), TFLITE_SCHEMA_VERSION);
 80a01a4:	492e      	ldr	r1, [pc, #184]	; (80a0260 <setup+0x108>)
 80a01a6:	482a      	ldr	r0, [pc, #168]	; (80a0250 <setup+0xf8>)
 80a01a8:	2303      	movs	r3, #3
  input = interpreter->input(0);
  output = interpreter->output(0);

  // Keep track of how many inferences we have performed.
  inference_count = 0;
}
 80a01aa:	b002      	add	sp, #8
 80a01ac:	e8bd 4070 	ldmia.w	sp!, {r4, r5, r6, lr}
  if (model->version() != TFLITE_SCHEMA_VERSION)
  {
    error_reporter->Report(
        "Model provided is schema version %d not equal "
        "to supported version %d.",
        model->version(), TFLITE_SCHEMA_VERSION);
 80a01b0:	f000 b8fc 	b.w	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return;
  }

  // This pulls in all the operation implementations we need.
  // NOLINTNEXTLINE(runtime-global-variables)
  static tflite::ops::micro::AllOpsResolver resolver;
 80a01b4:	4d2b      	ldr	r5, [pc, #172]	; (80a0264 <setup+0x10c>)
 80a01b6:	682b      	ldr	r3, [r5, #0]
 80a01b8:	07da      	lsls	r2, r3, #31
 80a01ba:	d40e      	bmi.n	80a01da <setup+0x82>
 80a01bc:	4628      	mov	r0, r5
 80a01be:	f7ff ff7f 	bl	80a00c0 <__cxa_guard_acquire>
 80a01c2:	b150      	cbz	r0, 80a01da <setup+0x82>
 80a01c4:	4828      	ldr	r0, [pc, #160]	; (80a0268 <setup+0x110>)
 80a01c6:	f002 ffbd 	bl	80a3144 <_ZN6tflite3ops5micro14AllOpsResolverC1Ev>
 80a01ca:	4628      	mov	r0, r5
 80a01cc:	f7ff ff7d 	bl	80a00ca <__cxa_guard_release>
 80a01d0:	4a1d      	ldr	r2, [pc, #116]	; (80a0248 <setup+0xf0>)
 80a01d2:	4926      	ldr	r1, [pc, #152]	; (80a026c <setup+0x114>)
 80a01d4:	4824      	ldr	r0, [pc, #144]	; (80a0268 <setup+0x110>)
 80a01d6:	f010 fc0b 	bl	80b09f0 <__aeabi_atexit>

  // Build an interpreter to run the model with.
  static tflite::MicroInterpreter static_interpreter(
      model, resolver, tensor_arena, kTensorArenaSize, error_reporter);
 80a01da:	4d25      	ldr	r5, [pc, #148]	; (80a0270 <setup+0x118>)
 80a01dc:	682b      	ldr	r3, [r5, #0]
 80a01de:	07db      	lsls	r3, r3, #31
 80a01e0:	d411      	bmi.n	80a0206 <setup+0xae>
 80a01e2:	4628      	mov	r0, r5
 80a01e4:	f7ff ff6c 	bl	80a00c0 <__cxa_guard_acquire>
 80a01e8:	b168      	cbz	r0, 80a0206 <setup+0xae>
 80a01ea:	6823      	ldr	r3, [r4, #0]
 80a01ec:	4a1e      	ldr	r2, [pc, #120]	; (80a0268 <setup+0x110>)
 80a01ee:	9301      	str	r3, [sp, #4]
 80a01f0:	f44f 6300 	mov.w	r3, #2048	; 0x800
 80a01f4:	9300      	str	r3, [sp, #0]
 80a01f6:	6831      	ldr	r1, [r6, #0]
 80a01f8:	4b1e      	ldr	r3, [pc, #120]	; (80a0274 <setup+0x11c>)
 80a01fa:	481f      	ldr	r0, [pc, #124]	; (80a0278 <setup+0x120>)
 80a01fc:	f001 fe1c 	bl	80a1e38 <_ZN6tflite16MicroInterpreterC1EPKNS_5ModelERKNS_10OpResolverEPhjPNS_13ErrorReporterE>
 80a0200:	4628      	mov	r0, r5
 80a0202:	f7ff ff62 	bl	80a00ca <__cxa_guard_release>
  interpreter = &static_interpreter;
 80a0206:	481c      	ldr	r0, [pc, #112]	; (80a0278 <setup+0x120>)
 80a0208:	4e1c      	ldr	r6, [pc, #112]	; (80a027c <setup+0x124>)
 80a020a:	6030      	str	r0, [r6, #0]

  // Allocate memory from the tensor_arena for the model's tensors.
  TfLiteStatus allocate_status = interpreter->AllocateTensors();
 80a020c:	f001 fdfa 	bl	80a1e04 <_ZN6tflite16MicroInterpreter15AllocateTensorsEv>
  if (allocate_status != kTfLiteOk)
 80a0210:	4605      	mov	r5, r0
 80a0212:	b130      	cbz	r0, 80a0222 <setup+0xca>
  {
    error_reporter->Report("AllocateTensors() failed");
 80a0214:	491a      	ldr	r1, [pc, #104]	; (80a0280 <setup+0x128>)
 80a0216:	6820      	ldr	r0, [r4, #0]
  input = interpreter->input(0);
  output = interpreter->output(0);

  // Keep track of how many inferences we have performed.
  inference_count = 0;
}
 80a0218:	b002      	add	sp, #8
 80a021a:	e8bd 4070 	ldmia.w	sp!, {r4, r5, r6, lr}

  // Allocate memory from the tensor_arena for the model's tensors.
  TfLiteStatus allocate_status = interpreter->AllocateTensors();
  if (allocate_status != kTfLiteOk)
  {
    error_reporter->Report("AllocateTensors() failed");
 80a021e:	f000 b8c5 	b.w	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return;
  }

  // Obtain pointers to the model's input and output tensors.
  input = interpreter->input(0);
 80a0222:	4601      	mov	r1, r0
 80a0224:	6830      	ldr	r0, [r6, #0]
 80a0226:	f001 fe6b 	bl	80a1f00 <_ZN6tflite16MicroInterpreter5inputEj>
 80a022a:	4b16      	ldr	r3, [pc, #88]	; (80a0284 <setup+0x12c>)
  output = interpreter->output(0);
 80a022c:	4629      	mov	r1, r5
    error_reporter->Report("AllocateTensors() failed");
    return;
  }

  // Obtain pointers to the model's input and output tensors.
  input = interpreter->input(0);
 80a022e:	6018      	str	r0, [r3, #0]
  output = interpreter->output(0);
 80a0230:	6830      	ldr	r0, [r6, #0]
 80a0232:	f001 fe49 	bl	80a1ec8 <_ZN6tflite16MicroInterpreter6outputEj>
 80a0236:	4b14      	ldr	r3, [pc, #80]	; (80a0288 <setup+0x130>)
 80a0238:	6018      	str	r0, [r3, #0]

  // Keep track of how many inferences we have performed.
  inference_count = 0;
 80a023a:	4b14      	ldr	r3, [pc, #80]	; (80a028c <setup+0x134>)
 80a023c:	601d      	str	r5, [r3, #0]
}
 80a023e:	b002      	add	sp, #8
 80a0240:	bd70      	pop	{r4, r5, r6, pc}
 80a0242:	bf00      	nop
 80a0244:	20001e3c 	.word	0x20001e3c
 80a0248:	20000594 	.word	0x20000594
 80a024c:	080a0135 	.word	0x080a0135
 80a0250:	20000000 	.word	0x20000000
 80a0254:	20000dac 	.word	0x20000dac
 80a0258:	20000da8 	.word	0x20000da8
 80a025c:	080b3c9c 	.word	0x080b3c9c
 80a0260:	080b3c08 	.word	0x080b3c08
 80a0264:	2000059c 	.word	0x2000059c
 80a0268:	20000db4 	.word	0x20000db4
 80a026c:	080a0137 	.word	0x080a0137
 80a0270:	20001e44 	.word	0x20001e44
 80a0274:	200005a0 	.word	0x200005a0
 80a0278:	20001dbc 	.word	0x20001dbc
 80a027c:	20000db0 	.word	0x20000db0
 80a0280:	080b3c4f 	.word	0x080b3c4f
 80a0284:	20000598 	.word	0x20000598
 80a0288:	20001e40 	.word	0x20001e40
 80a028c:	20001e48 	.word	0x20001e48

080a0290 <loop>:

// The name of this function is important for Arduino compatibility.
void loop()
{
 80a0290:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
  // Calculate an x value to feed into the model. We compare the current
  // inference_count to the number of inferences per cycle to determine
  // our position within the range of possible x values the model was
  // trained on, and use this to calculate a value.
  float position = static_cast<float>(inference_count) /
 80a0294:	4c1b      	ldr	r4, [pc, #108]	; (80a0304 <loop+0x74>)
 80a0296:	4b1c      	ldr	r3, [pc, #112]	; (80a0308 <loop+0x78>)
                   static_cast<float>(kInferencesPerCycle);
  float x_val = position * kXrange;
 80a0298:	6820      	ldr	r0, [r4, #0]
{
  // Calculate an x value to feed into the model. We compare the current
  // inference_count to the number of inferences per cycle to determine
  // our position within the range of possible x values the model was
  // trained on, and use this to calculate a value.
  float position = static_cast<float>(inference_count) /
 80a029a:	681e      	ldr	r6, [r3, #0]
                   static_cast<float>(kInferencesPerCycle);
  float x_val = position * kXrange;
 80a029c:	f013 f952 	bl	80b3544 <__aeabi_i2f>
 80a02a0:	4605      	mov	r5, r0
 80a02a2:	4630      	mov	r0, r6
 80a02a4:	f013 f94e 	bl	80b3544 <__aeabi_i2f>
 80a02a8:	4601      	mov	r1, r0
 80a02aa:	4628      	mov	r0, r5
 80a02ac:	f013 fa52 	bl	80b3754 <__aeabi_fdiv>
 80a02b0:	4916      	ldr	r1, [pc, #88]	; (80a030c <loop+0x7c>)
 80a02b2:	f013 f99b 	bl	80b35ec <__aeabi_fmul>

  // Place our calculated x value in the model's input tensor
  input->data.f[0] = x_val;
 80a02b6:	4b16      	ldr	r3, [pc, #88]	; (80a0310 <loop+0x80>)
  // inference_count to the number of inferences per cycle to determine
  // our position within the range of possible x values the model was
  // trained on, and use this to calculate a value.
  float position = static_cast<float>(inference_count) /
                   static_cast<float>(kInferencesPerCycle);
  float x_val = position * kXrange;
 80a02b8:	4605      	mov	r5, r0

  // Place our calculated x value in the model's input tensor
  input->data.f[0] = x_val;
 80a02ba:	681b      	ldr	r3, [r3, #0]
 80a02bc:	4f15      	ldr	r7, [pc, #84]	; (80a0314 <loop+0x84>)
 80a02be:	685b      	ldr	r3, [r3, #4]
 80a02c0:	6018      	str	r0, [r3, #0]

  // Run inference, and report any error
  TfLiteStatus invoke_status = interpreter->Invoke();
 80a02c2:	4b15      	ldr	r3, [pc, #84]	; (80a0318 <loop+0x88>)
 80a02c4:	6818      	ldr	r0, [r3, #0]
 80a02c6:	f001 fe37 	bl	80a1f38 <_ZN6tflite16MicroInterpreter6InvokeEv>
  if (invoke_status != kTfLiteOk)
 80a02ca:	b150      	cbz	r0, 80a02e2 <loop+0x52>
  {
    error_reporter->Report("Invoke failed on x_val: %f\n",
                           static_cast<double>(x_val));
 80a02cc:	4628      	mov	r0, r5
 80a02ce:	f012 fcf9 	bl	80b2cc4 <__aeabi_f2d>
 80a02d2:	4602      	mov	r2, r0
 80a02d4:	460b      	mov	r3, r1
 80a02d6:	6838      	ldr	r0, [r7, #0]
 80a02d8:	4910      	ldr	r1, [pc, #64]	; (80a031c <loop+0x8c>)
  // Increment the inference_counter, and reset it if we have reached
  // the total number per cycle
  inference_count += 1;
  if (inference_count >= kInferencesPerCycle)
    inference_count = 0;
}
 80a02da:	e8bd 41f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, lr}
  // Run inference, and report any error
  TfLiteStatus invoke_status = interpreter->Invoke();
  if (invoke_status != kTfLiteOk)
  {
    error_reporter->Report("Invoke failed on x_val: %f\n",
                           static_cast<double>(x_val));
 80a02de:	f000 b865 	b.w	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return;
  }

  // Read the predicted y value from the model's output tensor
  float y_val = output->data.f[0];
 80a02e2:	4b0f      	ldr	r3, [pc, #60]	; (80a0320 <loop+0x90>)

  // Output the results. A custom HandleOutput function can be implemented
  // for each supported hardware target.
  HandleOutput(error_reporter, x_val, y_val);
 80a02e4:	6838      	ldr	r0, [r7, #0]
                           static_cast<double>(x_val));
    return;
  }

  // Read the predicted y value from the model's output tensor
  float y_val = output->data.f[0];
 80a02e6:	681b      	ldr	r3, [r3, #0]

  // Output the results. A custom HandleOutput function can be implemented
  // for each supported hardware target.
  HandleOutput(error_reporter, x_val, y_val);
 80a02e8:	4629      	mov	r1, r5
                           static_cast<double>(x_val));
    return;
  }

  // Read the predicted y value from the model's output tensor
  float y_val = output->data.f[0];
 80a02ea:	685b      	ldr	r3, [r3, #4]

  // Output the results. A custom HandleOutput function can be implemented
  // for each supported hardware target.
  HandleOutput(error_reporter, x_val, y_val);
 80a02ec:	681a      	ldr	r2, [r3, #0]
 80a02ee:	f000 f82f 	bl	80a0350 <_Z12HandleOutputPN6tflite13ErrorReporterEff>

  // Increment the inference_counter, and reset it if we have reached
  // the total number per cycle
  inference_count += 1;
 80a02f2:	6820      	ldr	r0, [r4, #0]
 80a02f4:	3001      	adds	r0, #1
 80a02f6:	4286      	cmp	r6, r0
 80a02f8:	bfd8      	it	le
 80a02fa:	2000      	movle	r0, #0
 80a02fc:	6020      	str	r0, [r4, #0]
 80a02fe:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80a0302:	bf00      	nop
 80a0304:	20001e48 	.word	0x20001e48
 80a0308:	080b3c98 	.word	0x080b3c98
 80a030c:	40c90fdb 	.word	0x40c90fdb
 80a0310:	20000598 	.word	0x20000598
 80a0314:	20000dac 	.word	0x20000dac
 80a0318:	20000db0 	.word	0x20000db0
 80a031c:	080b3c68 	.word	0x080b3c68
 80a0320:	20001e40 	.word	0x20001e40

080a0324 <_GLOBAL__sub_I_SystemMode>:
  if (inference_count >= kInferencesPerCycle)
    inference_count = 0;
}
 80a0324:	b510      	push	{r4, lr}
inline int32_t pinReadFast(pin_t _pin)
{
    return ((PIN_MAP[_pin].gpio_peripheral->IDR & PIN_MAP[_pin].gpio_pin) == 0 ? LOW : HIGH);
}
#elif defined(STM32F2XX)
static Hal_Pin_Info* PIN_MAP = HAL_Pin_Map();
 80a0326:	f00f fcf5 	bl	80afd14 <HAL_Pin_Map>
    WAKEUP_REASON_RTC = 2,
    WAKEUP_REASON_PIN_OR_RTC = 3
};

struct SleepResult {
    SleepResult() {}
 80a032a:	f64f 72ff 	movw	r2, #65535	; 0xffff
 80a032e:	4b07      	ldr	r3, [pc, #28]	; (80a034c <_GLOBAL__sub_I_SystemMode+0x28>)
 80a0330:	2400      	movs	r4, #0
 80a0332:	701c      	strb	r4, [r3, #0]
 80a0334:	805c      	strh	r4, [r3, #2]

class SystemClass {
public:

    SystemClass(System_Mode_TypeDef mode = DEFAULT) {
        set_system_mode(mode);
 80a0336:	2003      	movs	r0, #3
    WAKEUP_REASON_RTC = 2,
    WAKEUP_REASON_PIN_OR_RTC = 3
};

struct SleepResult {
    SleepResult() {}
 80a0338:	809a      	strh	r2, [r3, #4]

class SystemClass {
public:

    SystemClass(System_Mode_TypeDef mode = DEFAULT) {
        set_system_mode(mode);
 80a033a:	f00f fded 	bl	80aff18 <set_system_mode>

#include <TensorFlowLite.h>
#include <Particle.h>

SYSTEM_MODE(MANUAL);
SYSTEM_THREAD(ENABLED);
 80a033e:	4621      	mov	r1, r4
  // Increment the inference_counter, and reset it if we have reached
  // the total number per cycle
  inference_count += 1;
  if (inference_count >= kInferencesPerCycle)
    inference_count = 0;
}
 80a0340:	e8bd 4010 	ldmia.w	sp!, {r4, lr}

#include <TensorFlowLite.h>
#include <Particle.h>

SYSTEM_MODE(MANUAL);
SYSTEM_THREAD(ENABLED);
 80a0344:	2001      	movs	r0, #1
 80a0346:	f00f bdef 	b.w	80aff28 <system_thread_set_state>
 80a034a:	bf00      	nop
 80a034c:	20000da0 	.word	0x20000da0

080a0350 <_Z12HandleOutputPN6tflite13ErrorReporterEff>:
bool initialized = false;

// Animates a dot across the screen to represent the current x and y values
void HandleOutput(tflite::ErrorReporter *error_reporter, float x_value,
                  float y_value)
{
 80a0350:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
  // Do this only once
  if (!initialized)
 80a0352:	4c11      	ldr	r4, [pc, #68]	; (80a0398 <_Z12HandleOutputPN6tflite13ErrorReporterEff+0x48>)
bool initialized = false;

// Animates a dot across the screen to represent the current x and y values
void HandleOutput(tflite::ErrorReporter *error_reporter, float x_value,
                  float y_value)
{
 80a0354:	4606      	mov	r6, r0
  // Do this only once
  if (!initialized)
 80a0356:	7823      	ldrb	r3, [r4, #0]
bool initialized = false;

// Animates a dot across the screen to represent the current x and y values
void HandleOutput(tflite::ErrorReporter *error_reporter, float x_value,
                  float y_value)
{
 80a0358:	4617      	mov	r7, r2
 80a035a:	4d10      	ldr	r5, [pc, #64]	; (80a039c <_Z12HandleOutputPN6tflite13ErrorReporterEff+0x4c>)
  // Do this only once
  if (!initialized)
 80a035c:	b92b      	cbnz	r3, 80a036a <_Z12HandleOutputPN6tflite13ErrorReporterEff+0x1a>
  {
    // Set the LED pin to output
    pinMode(led, OUTPUT);
 80a035e:	2101      	movs	r1, #1
 80a0360:	8828      	ldrh	r0, [r5, #0]
 80a0362:	f010 fab3 	bl	80b08cc <pinMode>
    initialized = true;
 80a0366:	2301      	movs	r3, #1
 80a0368:	7023      	strb	r3, [r4, #0]
  }

  // Calculate the brightness of the LED such that y=-1 is fully off
  // and y=1 is fully on. The LED's brightness can range from 0-255.
  int brightness = (int)(127.5f * (y_value + 1));
 80a036a:	f04f 517e 	mov.w	r1, #1065353216	; 0x3f800000
 80a036e:	4638      	mov	r0, r7
 80a0370:	f013 f834 	bl	80b33dc <__addsf3>
 80a0374:	490a      	ldr	r1, [pc, #40]	; (80a03a0 <_Z12HandleOutputPN6tflite13ErrorReporterEff+0x50>)
 80a0376:	f013 f939 	bl	80b35ec <__aeabi_fmul>
 80a037a:	f013 fb13 	bl	80b39a4 <__aeabi_f2iz>
 80a037e:	4604      	mov	r4, r0

  // Set the brightness of the LED. If the specified pin does not support PWM,
  // this will result in the LED being on when y > 127, off otherwise.
  analogWrite(led, brightness);
 80a0380:	4601      	mov	r1, r0
 80a0382:	8828      	ldrh	r0, [r5, #0]
 80a0384:	f010 fab3 	bl	80b08ee <_Z11analogWritetm>

  // Log the current brightness value
  error_reporter->Report("%d\n", brightness);
 80a0388:	4622      	mov	r2, r4
 80a038a:	4630      	mov	r0, r6
}
 80a038c:	e8bd 40f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, lr}
  // Set the brightness of the LED. If the specified pin does not support PWM,
  // this will result in the LED being on when y > 127, off otherwise.
  analogWrite(led, brightness);

  // Log the current brightness value
  error_reporter->Report("%d\n", brightness);
 80a0390:	4904      	ldr	r1, [pc, #16]	; (80a03a4 <_Z12HandleOutputPN6tflite13ErrorReporterEff+0x54>)
 80a0392:	f000 b80b 	b.w	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
 80a0396:	bf00      	nop
 80a0398:	20001e4c 	.word	0x20001e4c
 80a039c:	20000004 	.word	0x20000004
 80a03a0:	42ff0000 	.word	0x42ff0000
 80a03a4:	080b587d 	.word	0x080b587d

080a03a8 <_GLOBAL__sub_I_led>:
 80a03a8:	f00f bcb4 	b.w	80afd14 <HAL_Pin_Map>

080a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>:
#include "tensorflow/lite/core/api/error_reporter.h"
#include <cstdarg>

namespace tflite {

int ErrorReporter::Report(const char* format, ...) {
 80a03ac:	b40e      	push	{r1, r2, r3}
 80a03ae:	b503      	push	{r0, r1, lr}
 80a03b0:	aa03      	add	r2, sp, #12
 80a03b2:	f852 1b04 	ldr.w	r1, [r2], #4
  va_list args;
  va_start(args, format);
  int code = Report(format, args);
 80a03b6:	6803      	ldr	r3, [r0, #0]

namespace tflite {

int ErrorReporter::Report(const char* format, ...) {
  va_list args;
  va_start(args, format);
 80a03b8:	9201      	str	r2, [sp, #4]
  int code = Report(format, args);
 80a03ba:	689b      	ldr	r3, [r3, #8]
 80a03bc:	4798      	blx	r3
  va_end(args);
  return code;
}
 80a03be:	b002      	add	sp, #8
 80a03c0:	f85d eb04 	ldr.w	lr, [sp], #4
 80a03c4:	b003      	add	sp, #12
 80a03c6:	4770      	bx	lr

080a03c8 <_ZN6tflite12_GLOBAL__N_124SafeBuiltinDataAllocator18BuiltinDataDeleterclEPv>:
  class BuiltinDataDeleter {
   public:
    explicit BuiltinDataDeleter(BuiltinDataAllocator* allocator)
        : allocator_(allocator) {}

    void operator()(void* data) { allocator_->Deallocate(data); }
 80a03c8:	6800      	ldr	r0, [r0, #0]
 80a03ca:	6803      	ldr	r3, [r0, #0]
 80a03cc:	685b      	ldr	r3, [r3, #4]
 80a03ce:	4718      	bx	r3

080a03d0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_7PaddingEE_clESA_.isra.0>:
// If it returns kTfLiteError, `builtin_data` will be `nullptr`.
TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,
                         ErrorReporter* error_reporter,
                         BuiltinDataAllocator* allocator, void** builtin_data) {
  auto parse_padding = [](Padding padding) {
    switch (padding) {
 80a03d0:	b120      	cbz	r0, 80a03dc <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_7PaddingEE_clESA_.isra.0+0xc>
 80a03d2:	2801      	cmp	r0, #1
      case Padding_SAME:
        return kTfLitePaddingSame;
      case Padding_VALID:
        return kTfLitePaddingValid;
    }
    return kTfLitePaddingUnknown;
 80a03d4:	bf0c      	ite	eq
 80a03d6:	2002      	moveq	r0, #2
 80a03d8:	2000      	movne	r0, #0
 80a03da:	4770      	bx	lr
                         ErrorReporter* error_reporter,
                         BuiltinDataAllocator* allocator, void** builtin_data) {
  auto parse_padding = [](Padding padding) {
    switch (padding) {
      case Padding_SAME:
        return kTfLitePaddingSame;
 80a03dc:	2001      	movs	r0, #1
      case Padding_VALID:
        return kTfLitePaddingValid;
    }
    return kTfLitePaddingUnknown;
  };
 80a03de:	4770      	bx	lr

080a03e0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>:
  auto parse_activation = [](ActivationFunctionType activation) {
 80a03e0:	3801      	subs	r0, #1
 80a03e2:	b2c0      	uxtb	r0, r0
 80a03e4:	2804      	cmp	r0, #4
 80a03e6:	bf9a      	itte	ls
 80a03e8:	4b01      	ldrls	r3, [pc, #4]	; (80a03f0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1+0x10>)
 80a03ea:	5c18      	ldrbls	r0, [r3, r0]
 80a03ec:	2000      	movhi	r0, #0
        return kTfLiteActTanh;
      case ActivationFunctionType_SIGN_BIT:
        return kTfLiteActSignBit;
    }
    return kTfLiteActNone;
  };
 80a03ee:	4770      	bx	lr
 80a03f0:	080b49a7 	.word	0x080b49a7

080a03f4 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4>:
};

// Copies the contents from the flatbuffer int vector `flatbuffer` into the
// int array `buffer`. `flat_vector` and `buffer` represent the same
// configuration operation for a given operation.
TfLiteStatus FlatBufferIntVectorToArray(
 80a03f4:	b538      	push	{r3, r4, r5, lr}
 80a03f6:	4615      	mov	r5, r2
 80a03f8:	461a      	mov	r2, r3
    int max_size_of_buffer, const flatbuffers::Vector<int32_t>* flat_vector,
    int* buffer, ErrorReporter* error_reporter, const char* op_name) {
  if (!flat_vector) {
 80a03fa:	b908      	cbnz	r0, 80a0400 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0xc>
    error_reporter->Report("Input array not provided for operation '%s'.\n",
                           op_name);
 80a03fc:	490f      	ldr	r1, [pc, #60]	; (80a043c <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x48>)
 80a03fe:	e003      	b.n	80a0408 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x14>
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
 80a0400:	6804      	ldr	r4, [r0, #0]
    return kTfLiteError;
  } else {
    int num_dimensions = flat_vector->size();
    if (num_dimensions > max_size_of_buffer / sizeof(int)) {
 80a0402:	2c08      	cmp	r4, #8
 80a0404:	d905      	bls.n	80a0412 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x1e>
      error_reporter->Report(
          "Found too many dimensions in the input array of operation '%s'.\n",
          op_name);
 80a0406:	490e      	ldr	r1, [pc, #56]	; (80a0440 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x4c>)
 80a0408:	4628      	mov	r0, r5
 80a040a:	f7ff ffcf 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
      return kTfLiteError;
 80a040e:	2001      	movs	r0, #1
 80a0410:	bd38      	pop	{r3, r4, r5, pc}
 80a0412:	4602      	mov	r2, r0
    error_reporter->Report("Input array not provided for operation '%s'.\n",
                           op_name);
    return kTfLiteError;
  } else {
    int num_dimensions = flat_vector->size();
    if (num_dimensions > max_size_of_buffer / sizeof(int)) {
 80a0414:	2300      	movs	r3, #0
      error_reporter->Report(
          "Found too many dimensions in the input array of operation '%s'.\n",
          op_name);
      return kTfLiteError;
    } else {
      for (int i = 0; i < num_dimensions; ++i) {
 80a0416:	429c      	cmp	r4, r3
 80a0418:	d00e      	beq.n	80a0438 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x44>

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
 80a041a:	6805      	ldr	r5, [r0, #0]
 80a041c:	42ab      	cmp	r3, r5
 80a041e:	d305      	bcc.n	80a042c <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x38>
 80a0420:	4b08      	ldr	r3, [pc, #32]	; (80a0444 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x50>)
 80a0422:	4a09      	ldr	r2, [pc, #36]	; (80a0448 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x54>)
 80a0424:	21ed      	movs	r1, #237	; 0xed
 80a0426:	4809      	ldr	r0, [pc, #36]	; (80a044c <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x58>)
 80a0428:	f00f fe12 	bl	80b0050 <__assert_func>
template<typename T> struct IndirectHelper {
  typedef T return_type;
  typedef T mutable_return_type;
  static const size_t element_stride = sizeof(T);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    return EndianScalar((reinterpret_cast<const T *>(p))[i]);
 80a042c:	f852 5f04 	ldr.w	r5, [r2, #4]!
        buffer[i] = flat_vector->Get(i);
 80a0430:	f841 5023 	str.w	r5, [r1, r3, lsl #2]
      error_reporter->Report(
          "Found too many dimensions in the input array of operation '%s'.\n",
          op_name);
      return kTfLiteError;
    } else {
      for (int i = 0; i < num_dimensions; ++i) {
 80a0434:	3301      	adds	r3, #1
 80a0436:	e7ee      	b.n	80a0416 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x22>
        buffer[i] = flat_vector->Get(i);
      }
    }
  }
  return kTfLiteOk;
 80a0438:	2000      	movs	r0, #0
}
 80a043a:	bd38      	pop	{r3, r4, r5, pc}
 80a043c:	080b47ba 	.word	0x080b47ba
 80a0440:	080b47e8 	.word	0x080b47e8
 80a0444:	080b4829 	.word	0x080b4829
 80a0448:	080b46ec 	.word	0x080b46ec
 80a044c:	080b4834 	.word	0x080b4834

080a0450 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>:
// "tables" use an offset table (possibly shared) that allows fields to be
// omitted and added at will, but uses an extra indirection to read.
class Table {
 public:
  const uint8_t *GetVTable() const {
    return data_ - ReadScalar<soffset_t>(data_);
 80a0450:	6803      	ldr	r3, [r0, #0]
 80a0452:	1ac0      	subs	r0, r0, r3
    auto vtable = GetVTable();
    // The first element is the size of the vtable (fields + type id + itself).
    auto vtsize = ReadScalar<voffset_t>(vtable);
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
 80a0454:	8803      	ldrh	r3, [r0, #0]
 80a0456:	428b      	cmp	r3, r1
 80a0458:	bf8c      	ite	hi
 80a045a:	5a40      	ldrhhi	r0, [r0, r1]
 80a045c:	2000      	movls	r0, #0
  }
 80a045e:	4770      	bx	lr

080a0460 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>:

}  // namespace

TfLiteStatus ConvertTensorType(TensorType tensor_type, TfLiteType* type,
                               ErrorReporter* error_reporter) {
 80a0460:	b508      	push	{r3, lr}
 80a0462:	4603      	mov	r3, r0
 80a0464:	4610      	mov	r0, r2
  *type = kTfLiteNoType;
  switch (tensor_type) {
 80a0466:	2b09      	cmp	r3, #9
 80a0468:	d806      	bhi.n	80a0478 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x18>
 80a046a:	e8df f003 	tbb	[pc, r3]
 80a046e:	0907      	.short	0x0907
 80a0470:	15130f0d 	.word	0x15130f0d
 80a0474:	11190b17 	.word	0x11190b17

}  // namespace

TfLiteStatus ConvertTensorType(TensorType tensor_type, TfLiteType* type,
                               ErrorReporter* error_reporter) {
  *type = kTfLiteNoType;
 80a0478:	2200      	movs	r2, #0
 80a047a:	e012      	b.n	80a04a2 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
  switch (tensor_type) {
    case TensorType_FLOAT32:
      *type = kTfLiteFloat32;
 80a047c:	2201      	movs	r2, #1
 80a047e:	e010      	b.n	80a04a2 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_FLOAT16:
      *type = kTfLiteFloat16;
 80a0480:	220a      	movs	r2, #10
 80a0482:	e00e      	b.n	80a04a2 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_INT16:
      *type = kTfLiteInt16;
 80a0484:	2207      	movs	r2, #7
 80a0486:	e00c      	b.n	80a04a2 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_INT32:
      *type = kTfLiteInt32;
 80a0488:	2202      	movs	r2, #2
 80a048a:	e00a      	b.n	80a04a2 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_UINT8:
      *type = kTfLiteUInt8;
 80a048c:	2203      	movs	r2, #3
 80a048e:	e008      	b.n	80a04a2 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_INT8:
      *type = kTfLiteInt8;
 80a0490:	2209      	movs	r2, #9
 80a0492:	e006      	b.n	80a04a2 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_INT64:
      *type = kTfLiteInt64;
 80a0494:	2204      	movs	r2, #4
 80a0496:	e004      	b.n	80a04a2 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_STRING:
      *type = kTfLiteString;
 80a0498:	2205      	movs	r2, #5
 80a049a:	e002      	b.n	80a04a2 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_BOOL:
      *type = kTfLiteBool;
 80a049c:	2206      	movs	r2, #6
 80a049e:	e000      	b.n	80a04a2 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_COMPLEX64:
      *type = kTfLiteComplex64;
 80a04a0:	2208      	movs	r2, #8
 80a04a2:	700a      	strb	r2, [r1, #0]
      break;
  }
  if (*type == kTfLiteNoType) {
 80a04a4:	780a      	ldrb	r2, [r1, #0]
 80a04a6:	b92a      	cbnz	r2, 80a04b4 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x54>
    error_reporter->Report("Unsupported data type %d in tensor\n", tensor_type);
 80a04a8:	461a      	mov	r2, r3
 80a04aa:	4903      	ldr	r1, [pc, #12]	; (80a04b8 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x58>)
 80a04ac:	f7ff ff7e 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return kTfLiteError;
 80a04b0:	2001      	movs	r0, #1
 80a04b2:	bd08      	pop	{r3, pc}
  }
  return kTfLiteOk;
 80a04b4:	2000      	movs	r0, #0
}
 80a04b6:	bd08      	pop	{r3, pc}
 80a04b8:	080b48e0 	.word	0x080b48e0

080a04bc <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>:

  template<typename T> T GetField(voffset_t field, T defaultval) const {
 80a04bc:	b538      	push	{r3, r4, r5, lr}
 80a04be:	4605      	mov	r5, r0
 80a04c0:	4614      	mov	r4, r2
    auto field_offset = GetOptionalFieldOffset(field);
 80a04c2:	f7ff ffc5 	bl	80a0450 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
 80a04c6:	b108      	cbz	r0, 80a04cc <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_+0x10>
 80a04c8:	5c28      	ldrb	r0, [r5, r0]
 80a04ca:	bd38      	pop	{r3, r4, r5, pc}
 80a04cc:	4620      	mov	r0, r4
  }
 80a04ce:	bd38      	pop	{r3, r4, r5, pc}

080a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>:
    return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_INPUTS);
  }
  const flatbuffers::Vector<int32_t> *outputs() const {
    return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_OUTPUTS);
  }
  BuiltinOptions builtin_options_type() const {
 80a04d0:	b508      	push	{r3, lr}
    return static_cast<BuiltinOptions>(GetField<uint8_t>(VT_BUILTIN_OPTIONS_TYPE, 0));
 80a04d2:	2200      	movs	r2, #0
 80a04d4:	210a      	movs	r1, #10
 80a04d6:	f7ff fff1 	bl	80a04bc <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
  }
 80a04da:	bd08      	pop	{r3, pc}

080a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>:
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
 80a04dc:	b538      	push	{r3, r4, r5, lr}
 80a04de:	4605      	mov	r5, r0
 80a04e0:	4614      	mov	r4, r2
    auto field_offset = GetOptionalFieldOffset(field);
 80a04e2:	f7ff ffb5 	bl	80a0450 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
 80a04e6:	b108      	cbz	r0, 80a04ec <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_+0x10>
 80a04e8:	5828      	ldr	r0, [r5, r0]
 80a04ea:	bd38      	pop	{r3, r4, r5, pc}
 80a04ec:	4620      	mov	r0, r4
  }
 80a04ee:	bd38      	pop	{r3, r4, r5, pc}

080a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>:
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
 80a04f0:	b538      	push	{r3, r4, r5, lr}
 80a04f2:	4605      	mov	r5, r0
 80a04f4:	4614      	mov	r4, r2
    auto field_offset = GetOptionalFieldOffset(field);
 80a04f6:	f7ff ffab 	bl	80a0450 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
 80a04fa:	b108      	cbz	r0, 80a0500 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_+0x10>
 80a04fc:	5628      	ldrsb	r0, [r5, r0]
 80a04fe:	bd38      	pop	{r3, r4, r5, pc}
 80a0500:	4620      	mov	r0, r4
  }
 80a0502:	bd38      	pop	{r3, r4, r5, pc}

080a0504 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>:
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
 80a0504:	b538      	push	{r3, r4, r5, lr}
 80a0506:	4605      	mov	r5, r0
 80a0508:	4614      	mov	r4, r2
    auto field_offset = GetOptionalFieldOffset(field);
 80a050a:	f7ff ffa1 	bl	80a0450 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
 80a050e:	b108      	cbz	r0, 80a0514 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_+0x10>
 80a0510:	5828      	ldr	r0, [r5, r0]
 80a0512:	bd38      	pop	{r3, r4, r5, pc}
 80a0514:	4620      	mov	r0, r4
  }
 80a0516:	bd38      	pop	{r3, r4, r5, pc}

080a0518 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorIlEEEET_t>:

  template<typename P> P GetPointer(voffset_t field) {
 80a0518:	b510      	push	{r4, lr}
 80a051a:	4604      	mov	r4, r0
    auto field_offset = GetOptionalFieldOffset(field);
 80a051c:	f7ff ff98 	bl	80a0450 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    auto p = data_ + field_offset;
 80a0520:	1822      	adds	r2, r4, r0
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
 80a0522:	b108      	cbz	r0, 80a0528 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorIlEEEET_t+0x10>
 80a0524:	5823      	ldr	r3, [r4, r0]
 80a0526:	18d0      	adds	r0, r2, r3
  }
 80a0528:	bd10      	pop	{r4, pc}

080a052a <_ZNK6tflite8Operator15builtin_optionsEv>:
  const void *builtin_options() const {
 80a052a:	b508      	push	{r3, lr}
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
 80a052c:	210c      	movs	r1, #12
 80a052e:	f7ff fff3 	bl	80a0518 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorIlEEEET_t>
    return GetPointer<const void *>(VT_BUILTIN_OPTIONS);
  }
 80a0532:	bd08      	pop	{r3, pc}

080a0534 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI23TfLiteSequenceRNNParamsEEPT_v>:
  // extension part will take ownership so destructors  will not be run during
  // deallocation.
  template <typename T>
  T* AllocatePOD() {
    static_assert(std::is_pod<T>::value, "Builtin data structure must be POD.");
    return static_cast<T*>(this->Allocate(sizeof(T)));
 80a0534:	6803      	ldr	r3, [r0, #0]
 80a0536:	2102      	movs	r1, #2
 80a0538:	681b      	ldr	r3, [r3, #0]
 80a053a:	4718      	bx	r3

080a053c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI26TfLiteFullyConnectedParamsEEPT_v>:
 80a053c:	6803      	ldr	r3, [r0, #0]
 80a053e:	2103      	movs	r1, #3
 80a0540:	681b      	ldr	r3, [r3, #0]
 80a0542:	4718      	bx	r3

080a0544 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI19TfLiteSqueezeParamsEEPT_v>:
 80a0544:	6803      	ldr	r3, [r0, #0]
 80a0546:	2124      	movs	r1, #36	; 0x24
 80a0548:	681b      	ldr	r3, [r3, #0]
 80a054a:	4718      	bx	r3

080a054c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI25TfLiteTransposeConvParamsEEPT_v>:
 80a054c:	6803      	ldr	r3, [r0, #0]
 80a054e:	210c      	movs	r1, #12
 80a0550:	681b      	ldr	r3, [r3, #0]
 80a0552:	4718      	bx	r3

080a0554 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteFakeQuantParamsEEPT_v>:
 80a0554:	6803      	ldr	r3, [r0, #0]
 80a0556:	2110      	movs	r1, #16
 80a0558:	681b      	ldr	r3, [r3, #0]
 80a055a:	4718      	bx	r3

080a055c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>:
 80a055c:	6803      	ldr	r3, [r0, #0]
 80a055e:	2104      	movs	r1, #4
 80a0560:	681b      	ldr	r3, [r3, #0]
 80a0562:	4718      	bx	r3

080a0564 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>:
 80a0564:	6803      	ldr	r3, [r0, #0]
 80a0566:	2101      	movs	r1, #1
 80a0568:	681b      	ldr	r3, [r3, #0]
 80a056a:	4718      	bx	r3

080a056c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>:
 80a056c:	6803      	ldr	r3, [r0, #0]
 80a056e:	2108      	movs	r1, #8
 80a0570:	681b      	ldr	r3, [r3, #0]
 80a0572:	4718      	bx	r3

080a0574 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv>:
// If it returns kTfLiteOk, it passes the data out with `builtin_data`, which
// need to be released by calling `free`.`
// If it returns kTfLiteError, `builtin_data` will be `nullptr`.
TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,
                         ErrorReporter* error_reporter,
                         BuiltinDataAllocator* allocator, void** builtin_data) {
 80a0574:	e92d 41f3 	stmdb	sp!, {r0, r1, r4, r5, r6, r7, r8, lr}
 80a0578:	9e08      	ldr	r6, [sp, #32]
 80a057a:	461d      	mov	r5, r3
        return kTfLiteCombinerTypeSum;
    }
  };

  SafeBuiltinDataAllocator safe_allocator(allocator);
  *builtin_data = nullptr;
 80a057c:	2300      	movs	r3, #0
// If it returns kTfLiteOk, it passes the data out with `builtin_data`, which
// need to be released by calling `free`.`
// If it returns kTfLiteError, `builtin_data` will be `nullptr`.
TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,
                         ErrorReporter* error_reporter,
                         BuiltinDataAllocator* allocator, void** builtin_data) {
 80a057e:	4604      	mov	r4, r0
 80a0580:	4617      	mov	r7, r2
        return kTfLiteCombinerTypeSum;
    }
  };

  SafeBuiltinDataAllocator safe_allocator(allocator);
  *builtin_data = nullptr;
 80a0582:	6033      	str	r3, [r6, #0]
 80a0584:	4698      	mov	r8, r3
  switch (op_type) {
 80a0586:	2977      	cmp	r1, #119	; 0x77
 80a0588:	f200 86ed 	bhi.w	80a1366 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf2>
 80a058c:	e8df f011 	tbh	[pc, r1, lsl #1]
 80a0590:	010402bc 	.word	0x010402bc
 80a0594:	00780281 	.word	0x00780281
 80a0598:	04770142 	.word	0x04770142
 80a059c:	06eb06eb 	.word	0x06eb06eb
 80a05a0:	023006eb 	.word	0x023006eb
 80a05a4:	030806eb 	.word	0x030806eb
 80a05a8:	03210104 	.word	0x03210104
 80a05ac:	00e606eb 	.word	0x00e606eb
 80a05b0:	0104034a 	.word	0x0104034a
 80a05b4:	06eb02a2 	.word	0x06eb02a2
 80a05b8:	06eb06eb 	.word	0x06eb06eb
 80a05bc:	03eb0423 	.word	0x03eb0423
 80a05c0:	026a01f9 	.word	0x026a01f9
 80a05c4:	01860461 	.word	0x01860461
 80a05c8:	06eb06eb 	.word	0x06eb06eb
 80a05cc:	06eb043b 	.word	0x06eb043b
 80a05d0:	021306eb 	.word	0x021306eb
 80a05d4:	01a806eb 	.word	0x01a806eb
 80a05d8:	06eb048d 	.word	0x06eb048d
 80a05dc:	06eb06eb 	.word	0x06eb06eb
 80a05e0:	02ef04a5 	.word	0x02ef04a5
 80a05e4:	04ea02d6 	.word	0x04ea02d6
 80a05e8:	05100384 	.word	0x05100384
 80a05ec:	06eb01cc 	.word	0x06eb01cc
 80a05f0:	04be06eb 	.word	0x04be06eb
 80a05f4:	05ef06eb 	.word	0x05ef06eb
 80a05f8:	00b703b3 	.word	0x00b703b3
 80a05fc:	06eb06eb 	.word	0x06eb06eb
 80a0600:	06eb0541 	.word	0x06eb0541
 80a0604:	06eb06eb 	.word	0x06eb06eb
 80a0608:	06eb06eb 	.word	0x06eb06eb
 80a060c:	06eb06eb 	.word	0x06eb06eb
 80a0610:	06eb06eb 	.word	0x06eb06eb
 80a0614:	057506eb 	.word	0x057506eb
 80a0618:	06eb059b 	.word	0x06eb059b
 80a061c:	06eb06eb 	.word	0x06eb06eb
 80a0620:	06eb06eb 	.word	0x06eb06eb
 80a0624:	06eb04a5 	.word	0x06eb04a5
 80a0628:	05b406eb 	.word	0x05b406eb
 80a062c:	055b06eb 	.word	0x055b06eb
 80a0630:	04a505f5 	.word	0x04a505f5
 80a0634:	05d204a5 	.word	0x05d204a5
 80a0638:	062106eb 	.word	0x062106eb
 80a063c:	06eb06eb 	.word	0x06eb06eb
 80a0640:	04a50637 	.word	0x04a50637
 80a0644:	04a506eb 	.word	0x04a506eb
 80a0648:	06eb06eb 	.word	0x06eb06eb
 80a064c:	06eb06eb 	.word	0x06eb06eb
 80a0650:	040a06eb 	.word	0x040a06eb
 80a0654:	06eb0654 	.word	0x06eb0654
 80a0658:	06eb066a 	.word	0x06eb066a
 80a065c:	068304d4 	.word	0x068304d4
 80a0660:	06eb06eb 	.word	0x06eb06eb
 80a0664:	06eb06eb 	.word	0x06eb06eb
 80a0668:	06eb06eb 	.word	0x06eb06eb
 80a066c:	06eb06eb 	.word	0x06eb06eb
 80a0670:	06eb069c 	.word	0x06eb069c
 80a0674:	06eb06eb 	.word	0x06eb06eb
 80a0678:	06eb06eb 	.word	0x06eb06eb
 80a067c:	06d106b7 	.word	0x06d106b7
 80a0680:	682b      	ldr	r3, [r5, #0]
 80a0682:	2118      	movs	r1, #24
 80a0684:	681b      	ldr	r3, [r3, #0]
 80a0686:	4628      	mov	r0, r5
 80a0688:	4798      	blx	r3
 80a068a:	4605      	mov	r5, r0
  template<typename T> const T *builtin_options_as() const;
  const Conv2DOptions *builtin_options_as_Conv2DOptions() const {
    return builtin_options_type() == BuiltinOptions_Conv2DOptions ? static_cast<const Conv2DOptions *>(builtin_options()) : nullptr;
 80a068c:	4620      	mov	r0, r4
 80a068e:	f7ff ff1f 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0692:	2801      	cmp	r0, #1
 80a0694:	4607      	mov	r7, r0
 80a0696:	f040 8665 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a069a:	4620      	mov	r0, r4
 80a069c:	f7ff ff45 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
    case BuiltinOperator_CONV_2D: {
      auto params = safe_allocator.Allocate<TfLiteConvParams>();
      if (auto* conv_params = op->builtin_options_as_Conv2DOptions()) {
 80a06a0:	4604      	mov	r4, r0
 80a06a2:	2800      	cmp	r0, #0
 80a06a4:	f000 865e 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
    VT_FUSED_ACTIVATION_FUNCTION = 10,
    VT_DILATION_W_FACTOR = 12,
    VT_DILATION_H_FACTOR = 14
  };
  Padding padding() const {
    return static_cast<Padding>(GetField<int8_t>(VT_PADDING, 0));
 80a06a8:	2200      	movs	r2, #0
 80a06aa:	2104      	movs	r1, #4
 80a06ac:	f7ff ff20 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->padding = parse_padding(conv_params->padding());
 80a06b0:	b2c0      	uxtb	r0, r0
 80a06b2:	f7ff fe8d 	bl	80a03d0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_7PaddingEE_clESA_.isra.0>
  }
  int32_t stride_w() const {
    return GetField<int32_t>(VT_STRIDE_W, 0);
 80a06b6:	2200      	movs	r2, #0
 80a06b8:	7028      	strb	r0, [r5, #0]
 80a06ba:	2106      	movs	r1, #6
 80a06bc:	4620      	mov	r0, r4
 80a06be:	f7ff ff0d 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t stride_h() const {
    return GetField<int32_t>(VT_STRIDE_H, 0);
 80a06c2:	2200      	movs	r2, #0
        params->stride_width = conv_params->stride_w();
 80a06c4:	6068      	str	r0, [r5, #4]
 80a06c6:	2108      	movs	r1, #8
 80a06c8:	4620      	mov	r0, r4
 80a06ca:	f7ff ff07 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
 80a06ce:	2200      	movs	r2, #0
 80a06d0:	210a      	movs	r1, #10
        params->stride_height = conv_params->stride_h();
 80a06d2:	60a8      	str	r0, [r5, #8]
 80a06d4:	4620      	mov	r0, r4
 80a06d6:	f7ff ff0b 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(conv_params->fused_activation_function());
 80a06da:	b2c0      	uxtb	r0, r0
 80a06dc:	f7ff fe80 	bl	80a03e0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  }
  int32_t dilation_w_factor() const {
    return GetField<int32_t>(VT_DILATION_W_FACTOR, 1);
 80a06e0:	463a      	mov	r2, r7
 80a06e2:	7528      	strb	r0, [r5, #20]
 80a06e4:	210c      	movs	r1, #12
 80a06e6:	4620      	mov	r0, r4
 80a06e8:	f7ff fef8 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t dilation_h_factor() const {
    return GetField<int32_t>(VT_DILATION_H_FACTOR, 1);
 80a06ec:	463a      	mov	r2, r7

        params->dilation_width_factor = conv_params->dilation_w_factor();
 80a06ee:	60e8      	str	r0, [r5, #12]
 80a06f0:	210e      	movs	r1, #14
 80a06f2:	4620      	mov	r0, r4
 80a06f4:	f7ff fef2 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->dilation_height_factor = conv_params->dilation_h_factor();
 80a06f8:	6128      	str	r0, [r5, #16]
 80a06fa:	f000 be33 	b.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a06fe:	4628      	mov	r0, r5
 80a0700:	f7ff ff18 	bl	80a0534 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI23TfLiteSequenceRNNParamsEEPT_v>
 80a0704:	4680      	mov	r8, r0
  }
  const LogSoftmaxOptions *builtin_options_as_LogSoftmaxOptions() const {
    return builtin_options_type() == BuiltinOptions_LogSoftmaxOptions ? static_cast<const LogSoftmaxOptions *>(builtin_options()) : nullptr;
  }
  const CastOptions *builtin_options_as_CastOptions() const {
    return builtin_options_type() == BuiltinOptions_CastOptions ? static_cast<const CastOptions *>(builtin_options()) : nullptr;
 80a0706:	4620      	mov	r0, r4
      constexpr _Head_base(const _Head_base&) = default;
      constexpr _Head_base(_Head_base&&) = default;

      template<typename _UHead>
        constexpr _Head_base(_UHead&& __h)
	: _M_head_impl(std::forward<_UHead>(__h)) { }
 80a0708:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a070c:	f7ff fee0 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0710:	2825      	cmp	r0, #37	; 0x25
 80a0712:	f040 844a 	bne.w	80a0faa <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa36>
 80a0716:	4620      	mov	r0, r4
 80a0718:	f7ff ff07 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_CAST: {
      auto params = safe_allocator.Allocate<TfLiteCastParams>();
      if (const auto* schema_params = op->builtin_options_as_CastOptions()) {
 80a071c:	4605      	mov	r5, r0
 80a071e:	2800      	cmp	r0, #0
 80a0720:	f000 8443 	beq.w	80a0faa <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa36>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_IN_DATA_TYPE = 4,
    VT_OUT_DATA_TYPE = 6
  };
  TensorType in_data_type() const {
    return static_cast<TensorType>(GetField<int8_t>(VT_IN_DATA_TYPE, 0));
 80a0724:	2200      	movs	r2, #0
 80a0726:	2104      	movs	r1, #4
 80a0728:	f7ff fee2 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        auto in_status =
            ConvertTensorType(schema_params->in_data_type(),
                              &params->in_data_type, error_reporter);
 80a072c:	463a      	mov	r2, r7
 80a072e:	4641      	mov	r1, r8
 80a0730:	b2c0      	uxtb	r0, r0
 80a0732:	f7ff fe95 	bl	80a0460 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
  }
  TensorType out_data_type() const {
    return static_cast<TensorType>(GetField<int8_t>(VT_OUT_DATA_TYPE, 0));
 80a0736:	2200      	movs	r2, #0
 80a0738:	4604      	mov	r4, r0
 80a073a:	2106      	movs	r1, #6
 80a073c:	4628      	mov	r0, r5
 80a073e:	f7ff fed7 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        auto out_status =
            ConvertTensorType(schema_params->out_data_type(),
                              &params->out_data_type, error_reporter);
 80a0742:	9901      	ldr	r1, [sp, #4]
 80a0744:	463a      	mov	r2, r7
 80a0746:	3101      	adds	r1, #1
 80a0748:	b2c0      	uxtb	r0, r0
 80a074a:	f7ff fe89 	bl	80a0460 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
        if (in_status != kTfLiteOk || out_status != kTfLiteOk) {
 80a074e:	2c00      	cmp	r4, #0
 80a0750:	f040 8186 	bne.w	80a0a60 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x4ec>
 80a0754:	2800      	cmp	r0, #0
 80a0756:	f000 8428 	beq.w	80a0faa <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa36>
 80a075a:	e181      	b.n	80a0a60 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x4ec>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a075c:	4628      	mov	r0, r5
 80a075e:	f7ff ff01 	bl	80a0564 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
 80a0762:	4605      	mov	r5, r0
  }
  const ConcatEmbeddingsOptions *builtin_options_as_ConcatEmbeddingsOptions() const {
    return builtin_options_type() == BuiltinOptions_ConcatEmbeddingsOptions ? static_cast<const ConcatEmbeddingsOptions *>(builtin_options()) : nullptr;
  }
  const LSHProjectionOptions *builtin_options_as_LSHProjectionOptions() const {
    return builtin_options_type() == BuiltinOptions_LSHProjectionOptions ? static_cast<const LSHProjectionOptions *>(builtin_options()) : nullptr;
 80a0764:	4620      	mov	r0, r4
 80a0766:	f7ff feb3 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a076a:	2804      	cmp	r0, #4
 80a076c:	4607      	mov	r7, r0
 80a076e:	f040 85f9 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0772:	4620      	mov	r0, r4
 80a0774:	f7ff fed9 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_LSH_PROJECTION: {
      auto params = safe_allocator.Allocate<TfLiteLSHProjectionParams>();
      if (const auto* lshParams =
 80a0778:	2800      	cmp	r0, #0
 80a077a:	f000 85f3 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef LSHProjectionOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TYPE = 4
  };
  LSHProjectionType type() const {
    return static_cast<LSHProjectionType>(GetField<int8_t>(VT_TYPE, 0));
 80a077e:	2200      	movs	r2, #0
 80a0780:	4639      	mov	r1, r7
 80a0782:	f7ff feb5 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
      default:
        return kTfLiteLshProjectionUnknown;
    }
  };
  auto parseCombinerType = [](CombinerType type) {
    switch (type) {
 80a0786:	2801      	cmp	r0, #1
 80a0788:	d003      	beq.n	80a0792 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x21e>
        return kTfLiteCombinerTypeMean;
      case CombinerType_SQRTN:
        return kTfLiteCombinerTypeSqrtn;
      case CombinerType_SUM:
      default:
        return kTfLiteCombinerTypeSum;
 80a078a:	2802      	cmp	r0, #2
 80a078c:	bf0c      	ite	eq
 80a078e:	2002      	moveq	r0, #2
 80a0790:	2000      	movne	r0, #0
    }
    case BuiltinOperator_LSH_PROJECTION: {
      auto params = safe_allocator.Allocate<TfLiteLSHProjectionParams>();
      if (const auto* lshParams =
              op->builtin_options_as_LSHProjectionOptions()) {
        params->type = parseLSHProjectionType(lshParams->type());
 80a0792:	7028      	strb	r0, [r5, #0]
 80a0794:	f000 bde6 	b.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0798:	682b      	ldr	r3, [r5, #0]
 80a079a:	2128      	movs	r1, #40	; 0x28
 80a079c:	681b      	ldr	r3, [r3, #0]
 80a079e:	4628      	mov	r0, r5
 80a07a0:	4798      	blx	r3
 80a07a2:	4605      	mov	r5, r0
  }
  const LSHProjectionOptions *builtin_options_as_LSHProjectionOptions() const {
    return builtin_options_type() == BuiltinOptions_LSHProjectionOptions ? static_cast<const LSHProjectionOptions *>(builtin_options()) : nullptr;
  }
  const Pool2DOptions *builtin_options_as_Pool2DOptions() const {
    return builtin_options_type() == BuiltinOptions_Pool2DOptions ? static_cast<const Pool2DOptions *>(builtin_options()) : nullptr;
 80a07a4:	4620      	mov	r0, r4
 80a07a6:	f7ff fe93 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a07aa:	2805      	cmp	r0, #5
 80a07ac:	f040 85da 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a07b0:	4620      	mov	r0, r4
 80a07b2:	f7ff feba 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
    }
    case BuiltinOperator_AVERAGE_POOL_2D:
    case BuiltinOperator_MAX_POOL_2D:
    case BuiltinOperator_L2_POOL_2D: {
      auto params = safe_allocator.Allocate<TfLitePoolParams>();
      if (const auto* pool_params = op->builtin_options_as_Pool2DOptions()) {
 80a07b6:	4604      	mov	r4, r0
 80a07b8:	2800      	cmp	r0, #0
 80a07ba:	f000 85d3 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
    VT_FILTER_WIDTH = 10,
    VT_FILTER_HEIGHT = 12,
    VT_FUSED_ACTIVATION_FUNCTION = 14
  };
  Padding padding() const {
    return static_cast<Padding>(GetField<int8_t>(VT_PADDING, 0));
 80a07be:	2200      	movs	r2, #0
 80a07c0:	2104      	movs	r1, #4
 80a07c2:	f7ff fe95 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->padding = parse_padding(pool_params->padding());
 80a07c6:	b2c0      	uxtb	r0, r0
 80a07c8:	f7ff fe02 	bl	80a03d0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_7PaddingEE_clESA_.isra.0>
  }
  int32_t stride_w() const {
    return GetField<int32_t>(VT_STRIDE_W, 0);
 80a07cc:	2200      	movs	r2, #0
 80a07ce:	7028      	strb	r0, [r5, #0]
 80a07d0:	2106      	movs	r1, #6
 80a07d2:	4620      	mov	r0, r4
 80a07d4:	f7ff fe82 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t stride_h() const {
    return GetField<int32_t>(VT_STRIDE_H, 0);
 80a07d8:	2200      	movs	r2, #0
        params->stride_width = pool_params->stride_w();
 80a07da:	6068      	str	r0, [r5, #4]
 80a07dc:	2108      	movs	r1, #8
 80a07de:	4620      	mov	r0, r4
 80a07e0:	f7ff fe7c 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t filter_width() const {
    return GetField<int32_t>(VT_FILTER_WIDTH, 0);
 80a07e4:	2200      	movs	r2, #0
        params->stride_height = pool_params->stride_h();
 80a07e6:	60a8      	str	r0, [r5, #8]
 80a07e8:	210a      	movs	r1, #10
 80a07ea:	4620      	mov	r0, r4
 80a07ec:	f7ff fe76 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t filter_height() const {
    return GetField<int32_t>(VT_FILTER_HEIGHT, 0);
 80a07f0:	2200      	movs	r2, #0
        params->filter_width = pool_params->filter_width();
 80a07f2:	60e8      	str	r0, [r5, #12]
 80a07f4:	210c      	movs	r1, #12
 80a07f6:	4620      	mov	r0, r4
 80a07f8:	f7ff fe70 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
 80a07fc:	2200      	movs	r2, #0
        params->filter_height = pool_params->filter_height();
 80a07fe:	6128      	str	r0, [r5, #16]
 80a0800:	210e      	movs	r1, #14
 80a0802:	4620      	mov	r0, r4
 80a0804:	f7ff fe74 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(pool_params->fused_activation_function());
 80a0808:	b2c0      	uxtb	r0, r0
 80a080a:	f7ff fde9 	bl	80a03e0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
 80a080e:	7528      	strb	r0, [r5, #20]
 80a0810:	f000 bda8 	b.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0814:	682b      	ldr	r3, [r5, #0]
 80a0816:	211c      	movs	r1, #28
 80a0818:	681b      	ldr	r3, [r3, #0]
 80a081a:	4628      	mov	r0, r5
 80a081c:	4798      	blx	r3
 80a081e:	4605      	mov	r5, r0
  template<typename T> const T *builtin_options_as() const;
  const Conv2DOptions *builtin_options_as_Conv2DOptions() const {
    return builtin_options_type() == BuiltinOptions_Conv2DOptions ? static_cast<const Conv2DOptions *>(builtin_options()) : nullptr;
  }
  const DepthwiseConv2DOptions *builtin_options_as_DepthwiseConv2DOptions() const {
    return builtin_options_type() == BuiltinOptions_DepthwiseConv2DOptions ? static_cast<const DepthwiseConv2DOptions *>(builtin_options()) : nullptr;
 80a0820:	4620      	mov	r0, r4
 80a0822:	f7ff fe55 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0826:	2802      	cmp	r0, #2
 80a0828:	f040 859c 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a082c:	4620      	mov	r0, r4
 80a082e:	f7ff fe7c 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_DEPTHWISE_CONV_2D: {
      auto params = safe_allocator.Allocate<TfLiteDepthwiseConvParams>();
      if (const auto* conv_params =
 80a0832:	4604      	mov	r4, r0
 80a0834:	2800      	cmp	r0, #0
 80a0836:	f000 8595 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
    VT_FUSED_ACTIVATION_FUNCTION = 12,
    VT_DILATION_W_FACTOR = 14,
    VT_DILATION_H_FACTOR = 16
  };
  Padding padding() const {
    return static_cast<Padding>(GetField<int8_t>(VT_PADDING, 0));
 80a083a:	2200      	movs	r2, #0
 80a083c:	2104      	movs	r1, #4
 80a083e:	f7ff fe57 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_DepthwiseConv2DOptions()) {
        params->padding = parse_padding(conv_params->padding());
 80a0842:	b2c0      	uxtb	r0, r0
 80a0844:	f7ff fdc4 	bl	80a03d0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_7PaddingEE_clESA_.isra.0>
  }
  int32_t stride_w() const {
    return GetField<int32_t>(VT_STRIDE_W, 0);
 80a0848:	2200      	movs	r2, #0
 80a084a:	7028      	strb	r0, [r5, #0]
 80a084c:	2106      	movs	r1, #6
 80a084e:	4620      	mov	r0, r4
 80a0850:	f7ff fe44 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t stride_h() const {
    return GetField<int32_t>(VT_STRIDE_H, 0);
 80a0854:	2200      	movs	r2, #0
        params->stride_width = conv_params->stride_w();
 80a0856:	6068      	str	r0, [r5, #4]
 80a0858:	2108      	movs	r1, #8
 80a085a:	4620      	mov	r0, r4
 80a085c:	f7ff fe3e 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t depth_multiplier() const {
    return GetField<int32_t>(VT_DEPTH_MULTIPLIER, 0);
 80a0860:	2200      	movs	r2, #0
        params->stride_height = conv_params->stride_h();
 80a0862:	60a8      	str	r0, [r5, #8]
 80a0864:	210a      	movs	r1, #10
 80a0866:	4620      	mov	r0, r4
 80a0868:	f7ff fe38 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
 80a086c:	2200      	movs	r2, #0
 80a086e:	210c      	movs	r1, #12
        params->depth_multiplier = conv_params->depth_multiplier();
 80a0870:	60e8      	str	r0, [r5, #12]
 80a0872:	4620      	mov	r0, r4
 80a0874:	f7ff fe3c 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(conv_params->fused_activation_function());
 80a0878:	b2c0      	uxtb	r0, r0
 80a087a:	f7ff fdb1 	bl	80a03e0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  }
  int32_t dilation_w_factor() const {
    return GetField<int32_t>(VT_DILATION_W_FACTOR, 1);
 80a087e:	2201      	movs	r2, #1
 80a0880:	7428      	strb	r0, [r5, #16]
 80a0882:	210e      	movs	r1, #14
 80a0884:	4620      	mov	r0, r4
 80a0886:	f7ff fe29 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t dilation_h_factor() const {
    return GetField<int32_t>(VT_DILATION_H_FACTOR, 1);
 80a088a:	2201      	movs	r2, #1

        params->dilation_width_factor = conv_params->dilation_w_factor();
 80a088c:	6168      	str	r0, [r5, #20]
 80a088e:	2110      	movs	r1, #16
 80a0890:	4620      	mov	r0, r4
 80a0892:	f7ff fe23 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->dilation_height_factor = conv_params->dilation_h_factor();
 80a0896:	61a8      	str	r0, [r5, #24]
 80a0898:	f000 bd64 	b.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a089c:	4628      	mov	r0, r5
 80a089e:	f7ff fe65 	bl	80a056c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
 80a08a2:	4605      	mov	r5, r0
  }
  const Pool2DOptions *builtin_options_as_Pool2DOptions() const {
    return builtin_options_type() == BuiltinOptions_Pool2DOptions ? static_cast<const Pool2DOptions *>(builtin_options()) : nullptr;
  }
  const SVDFOptions *builtin_options_as_SVDFOptions() const {
    return builtin_options_type() == BuiltinOptions_SVDFOptions ? static_cast<const SVDFOptions *>(builtin_options()) : nullptr;
 80a08a4:	4620      	mov	r0, r4
 80a08a6:	f7ff fe13 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a08aa:	2806      	cmp	r0, #6
 80a08ac:	4607      	mov	r7, r0
 80a08ae:	f040 8559 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a08b2:	4620      	mov	r0, r4
 80a08b4:	f7ff fe39 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SVDF: {
      auto params = safe_allocator.Allocate<TfLiteSVDFParams>();
      if (const auto* svdf_params = op->builtin_options_as_SVDFOptions()) {
 80a08b8:	4604      	mov	r4, r0
 80a08ba:	2800      	cmp	r0, #0
 80a08bc:	f000 8552 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_RANK = 4,
    VT_FUSED_ACTIVATION_FUNCTION = 6
  };
  int32_t rank() const {
    return GetField<int32_t>(VT_RANK, 0);
 80a08c0:	2200      	movs	r2, #0
 80a08c2:	2104      	movs	r1, #4
 80a08c4:	f7ff fe0a 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
 80a08c8:	2200      	movs	r2, #0
        params->rank = svdf_params->rank();
 80a08ca:	6028      	str	r0, [r5, #0]
 80a08cc:	4639      	mov	r1, r7
 80a08ce:	4620      	mov	r0, r4
 80a08d0:	f7ff fe0e 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(svdf_params->fused_activation_function());
 80a08d4:	b2c0      	uxtb	r0, r0
 80a08d6:	f7ff fd83 	bl	80a03e0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
 80a08da:	7128      	strb	r0, [r5, #4]
 80a08dc:	f000 bd42 	b.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a08e0:	4628      	mov	r0, r5
 80a08e2:	f7ff fe27 	bl	80a0534 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI23TfLiteSequenceRNNParamsEEPT_v>
 80a08e6:	4605      	mov	r5, r0
  }
  const SqueezeOptions *builtin_options_as_SqueezeOptions() const {
    return builtin_options_type() == BuiltinOptions_SqueezeOptions ? static_cast<const SqueezeOptions *>(builtin_options()) : nullptr;
  }
  const SequenceRNNOptions *builtin_options_as_SequenceRNNOptions() const {
    return builtin_options_type() == BuiltinOptions_SequenceRNNOptions ? static_cast<const SequenceRNNOptions *>(builtin_options()) : nullptr;
 80a08e8:	4620      	mov	r0, r4
 80a08ea:	f7ff fdf1 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a08ee:	281f      	cmp	r0, #31
 80a08f0:	f040 8538 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a08f4:	4620      	mov	r0, r4
 80a08f6:	f7ff fe18 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_UNIDIRECTIONAL_SEQUENCE_RNN: {
      auto params = safe_allocator.Allocate<TfLiteSequenceRNNParams>();
      if (const auto* sequence_rnn_params =
 80a08fa:	4604      	mov	r4, r0
 80a08fc:	2800      	cmp	r0, #0
 80a08fe:	f000 8531 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  };
  bool time_major() const {
    return GetField<uint8_t>(VT_TIME_MAJOR, 0) != 0;
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
 80a0902:	2200      	movs	r2, #0
 80a0904:	2106      	movs	r1, #6
 80a0906:	f7ff fdf3 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_SequenceRNNOptions()) {
        params->activation =
            parse_activation(sequence_rnn_params->fused_activation_function());
 80a090a:	b2c0      	uxtb	r0, r0
 80a090c:	f7ff fd68 	bl	80a03e0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TIME_MAJOR = 4,
    VT_FUSED_ACTIVATION_FUNCTION = 6
  };
  bool time_major() const {
    return GetField<uint8_t>(VT_TIME_MAJOR, 0) != 0;
 80a0910:	2200      	movs	r2, #0
 80a0912:	7068      	strb	r0, [r5, #1]
 80a0914:	2104      	movs	r1, #4
 80a0916:	4620      	mov	r0, r4
 80a0918:	f7ff fdd0 	bl	80a04bc <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->time_major = sequence_rnn_params->time_major();
 80a091c:	3000      	adds	r0, #0
 80a091e:	bf18      	it	ne
 80a0920:	2001      	movne	r0, #1
 80a0922:	7028      	strb	r0, [r5, #0]
 80a0924:	f000 bd1e 	b.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0928:	4628      	mov	r0, r5
 80a092a:	f7ff fe07 	bl	80a053c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI26TfLiteFullyConnectedParamsEEPT_v>
 80a092e:	4605      	mov	r5, r0
  }
  const BidirectionalSequenceLSTMOptions *builtin_options_as_BidirectionalSequenceLSTMOptions() const {
    return builtin_options_type() == BuiltinOptions_BidirectionalSequenceLSTMOptions ? static_cast<const BidirectionalSequenceLSTMOptions *>(builtin_options()) : nullptr;
  }
  const BidirectionalSequenceRNNOptions *builtin_options_as_BidirectionalSequenceRNNOptions() const {
    return builtin_options_type() == BuiltinOptions_BidirectionalSequenceRNNOptions ? static_cast<const BidirectionalSequenceRNNOptions *>(builtin_options()) : nullptr;
 80a0930:	4620      	mov	r0, r4
 80a0932:	f7ff fdcd 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0936:	2846      	cmp	r0, #70	; 0x46
 80a0938:	f040 8514 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a093c:	4620      	mov	r0, r4
 80a093e:	f7ff fdf4 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_BIDIRECTIONAL_SEQUENCE_RNN: {
      auto params =
          safe_allocator.Allocate<TfLiteBidirectionalSequenceRNNParams>();
      if (const auto* bidi_sequence_rnn_params =
 80a0942:	4604      	mov	r4, r0
 80a0944:	2800      	cmp	r0, #0
 80a0946:	f000 850d 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  };
  bool time_major() const {
    return GetField<uint8_t>(VT_TIME_MAJOR, 0) != 0;
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
 80a094a:	2200      	movs	r2, #0
 80a094c:	2106      	movs	r1, #6
 80a094e:	f7ff fdcf 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_BidirectionalSequenceRNNOptions()) {
        params->activation = parse_activation(
 80a0952:	b2c0      	uxtb	r0, r0
 80a0954:	f7ff fd44 	bl	80a03e0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
    VT_TIME_MAJOR = 4,
    VT_FUSED_ACTIVATION_FUNCTION = 6,
    VT_MERGE_OUTPUTS = 8
  };
  bool time_major() const {
    return GetField<uint8_t>(VT_TIME_MAJOR, 0) != 0;
 80a0958:	2200      	movs	r2, #0
            bidi_sequence_rnn_params->fused_activation_function());
 80a095a:	7068      	strb	r0, [r5, #1]
 80a095c:	2104      	movs	r1, #4
 80a095e:	4620      	mov	r0, r4
 80a0960:	f7ff fdac 	bl	80a04bc <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->time_major = bidi_sequence_rnn_params->time_major();
 80a0964:	3000      	adds	r0, #0
 80a0966:	bf18      	it	ne
 80a0968:	2001      	movne	r0, #1
 80a096a:	7028      	strb	r0, [r5, #0]
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
  }
  bool merge_outputs() const {
    return GetField<uint8_t>(VT_MERGE_OUTPUTS, 0) != 0;
 80a096c:	2200      	movs	r2, #0
 80a096e:	2108      	movs	r1, #8
 80a0970:	4620      	mov	r0, r4
 80a0972:	f7ff fda3 	bl	80a04bc <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->merge_outputs = bidi_sequence_rnn_params->merge_outputs();
 80a0976:	3000      	adds	r0, #0
 80a0978:	bf18      	it	ne
 80a097a:	2001      	movne	r0, #1
 80a097c:	70a8      	strb	r0, [r5, #2]
 80a097e:	f000 bcf1 	b.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0982:	4628      	mov	r0, r5
 80a0984:	f7ff fdee 	bl	80a0564 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
 80a0988:	4605      	mov	r5, r0
  }
  const SVDFOptions *builtin_options_as_SVDFOptions() const {
    return builtin_options_type() == BuiltinOptions_SVDFOptions ? static_cast<const SVDFOptions *>(builtin_options()) : nullptr;
  }
  const RNNOptions *builtin_options_as_RNNOptions() const {
    return builtin_options_type() == BuiltinOptions_RNNOptions ? static_cast<const RNNOptions *>(builtin_options()) : nullptr;
 80a098a:	4620      	mov	r0, r4
 80a098c:	f7ff fda0 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0990:	2807      	cmp	r0, #7
 80a0992:	f040 84e7 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0996:	4620      	mov	r0, r4
 80a0998:	f7ff fdc7 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_RNN: {
      auto params = safe_allocator.Allocate<TfLiteRNNParams>();
      if (const auto* rnn_params = op->builtin_options_as_RNNOptions()) {
 80a099c:	2800      	cmp	r0, #0
 80a099e:	f000 84e1 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef RNNOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSED_ACTIVATION_FUNCTION = 4
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
 80a09a2:	2200      	movs	r2, #0
 80a09a4:	2104      	movs	r1, #4
 80a09a6:	f7ff fda3 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(rnn_params->fused_activation_function());
 80a09aa:	b2c0      	uxtb	r0, r0
 80a09ac:	f7ff fd18 	bl	80a03e0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
 80a09b0:	7028      	strb	r0, [r5, #0]
 80a09b2:	f000 bcd7 	b.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a09b6:	4628      	mov	r0, r5
 80a09b8:	f7ff fdd4 	bl	80a0564 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
 80a09bc:	4605      	mov	r5, r0
  }
  const SpaceToDepthOptions *builtin_options_as_SpaceToDepthOptions() const {
    return builtin_options_type() == BuiltinOptions_SpaceToDepthOptions ? static_cast<const SpaceToDepthOptions *>(builtin_options()) : nullptr;
  }
  const EmbeddingLookupSparseOptions *builtin_options_as_EmbeddingLookupSparseOptions() const {
    return builtin_options_type() == BuiltinOptions_EmbeddingLookupSparseOptions ? static_cast<const EmbeddingLookupSparseOptions *>(builtin_options()) : nullptr;
 80a09be:	4620      	mov	r0, r4
 80a09c0:	f7ff fd86 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a09c4:	2814      	cmp	r0, #20
 80a09c6:	f040 84cd 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a09ca:	4620      	mov	r0, r4
 80a09cc:	f7ff fdad 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_EMBEDDING_LOOKUP_SPARSE: {
      auto params =
          safe_allocator.Allocate<TfLiteEmbeddingLookupSparseParams>();
      if (const auto* embedding_params =
 80a09d0:	2800      	cmp	r0, #0
 80a09d2:	f000 84c7 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef EmbeddingLookupSparseOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_COMBINER = 4
  };
  CombinerType combiner() const {
    return static_cast<CombinerType>(GetField<int8_t>(VT_COMBINER, 0));
 80a09d6:	2200      	movs	r2, #0
 80a09d8:	2104      	movs	r1, #4
 80a09da:	f7ff fd89 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
      default:
        return kTfLiteLshProjectionUnknown;
    }
  };
  auto parseCombinerType = [](CombinerType type) {
    switch (type) {
 80a09de:	2801      	cmp	r0, #1
 80a09e0:	d003      	beq.n	80a09ea <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x476>
        return kTfLiteCombinerTypeMean;
      case CombinerType_SQRTN:
        return kTfLiteCombinerTypeSqrtn;
      case CombinerType_SUM:
      default:
        return kTfLiteCombinerTypeSum;
 80a09e2:	2802      	cmp	r0, #2
 80a09e4:	bf0c      	ite	eq
 80a09e6:	2002      	moveq	r0, #2
 80a09e8:	2000      	movne	r0, #0
    case BuiltinOperator_EMBEDDING_LOOKUP_SPARSE: {
      auto params =
          safe_allocator.Allocate<TfLiteEmbeddingLookupSparseParams>();
      if (const auto* embedding_params =
              op->builtin_options_as_EmbeddingLookupSparseOptions()) {
        params->combiner = parseCombinerType(embedding_params->combiner());
 80a09ea:	7028      	strb	r0, [r5, #0]
 80a09ec:	f000 bcba 	b.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a09f0:	4628      	mov	r0, r5
 80a09f2:	f7ff fda3 	bl	80a053c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI26TfLiteFullyConnectedParamsEEPT_v>
 80a09f6:	4680      	mov	r8, r0
  }
  const RNNOptions *builtin_options_as_RNNOptions() const {
    return builtin_options_type() == BuiltinOptions_RNNOptions ? static_cast<const RNNOptions *>(builtin_options()) : nullptr;
  }
  const FullyConnectedOptions *builtin_options_as_FullyConnectedOptions() const {
    return builtin_options_type() == BuiltinOptions_FullyConnectedOptions ? static_cast<const FullyConnectedOptions *>(builtin_options()) : nullptr;
 80a09f8:	4620      	mov	r0, r4
 80a09fa:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a09fe:	f7ff fd67 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0a02:	2808      	cmp	r0, #8
 80a0a04:	4605      	mov	r5, r0
 80a0a06:	f040 82d0 	bne.w	80a0faa <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa36>
 80a0a0a:	4620      	mov	r0, r4
 80a0a0c:	f7ff fd8d 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_FULLY_CONNECTED: {
      auto params = safe_allocator.Allocate<TfLiteFullyConnectedParams>();
      if (const auto* fully_connected_params =
 80a0a10:	4604      	mov	r4, r0
 80a0a12:	2800      	cmp	r0, #0
 80a0a14:	f000 82c9 	beq.w	80a0faa <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa36>
    VT_FUSED_ACTIVATION_FUNCTION = 4,
    VT_WEIGHTS_FORMAT = 6,
    VT_KEEP_NUM_DIMS = 8
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
 80a0a18:	2200      	movs	r2, #0
 80a0a1a:	2104      	movs	r1, #4
 80a0a1c:	f7ff fd68 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_FullyConnectedOptions()) {
        params->activation = parse_activation(
 80a0a20:	b2c0      	uxtb	r0, r0
 80a0a22:	f7ff fcdd 	bl	80a03e0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  }
  FullyConnectedOptionsWeightsFormat weights_format() const {
    return static_cast<FullyConnectedOptionsWeightsFormat>(GetField<int8_t>(VT_WEIGHTS_FORMAT, 0));
  }
  bool keep_num_dims() const {
    return GetField<uint8_t>(VT_KEEP_NUM_DIMS, 0) != 0;
 80a0a26:	2200      	movs	r2, #0
            fully_connected_params->fused_activation_function());
 80a0a28:	f888 0000 	strb.w	r0, [r8]
 80a0a2c:	4629      	mov	r1, r5
 80a0a2e:	4620      	mov	r0, r4
 80a0a30:	f7ff fd44 	bl	80a04bc <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
      }

      /// Return the stored pointer.
      pointer
      get() const noexcept
      { return std::get<0>(_M_t); }
 80a0a34:	f8dd 8004 	ldr.w	r8, [sp, #4]
        params->keep_num_dims = fully_connected_params->keep_num_dims();
 80a0a38:	3000      	adds	r0, #0
 80a0a3a:	bf18      	it	ne
 80a0a3c:	2001      	movne	r0, #1
 80a0a3e:	f888 0002 	strb.w	r0, [r8, #2]
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
  }
  FullyConnectedOptionsWeightsFormat weights_format() const {
    return static_cast<FullyConnectedOptionsWeightsFormat>(GetField<int8_t>(VT_WEIGHTS_FORMAT, 0));
 80a0a42:	2200      	movs	r2, #0
 80a0a44:	2106      	movs	r1, #6
 80a0a46:	4620      	mov	r0, r4
 80a0a48:	f7ff fd52 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        switch (fully_connected_params->weights_format()) {
 80a0a4c:	b108      	cbz	r0, 80a0a52 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x4de>
 80a0a4e:	2801      	cmp	r0, #1
 80a0a50:	d102      	bne.n	80a0a58 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x4e4>
 80a0a52:	9b01      	ldr	r3, [sp, #4]
          case FullyConnectedOptionsWeightsFormat_DEFAULT:
            params->weights_format = kTfLiteFullyConnectedWeightsFormatDefault;
            break;
          case FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8:
            params->weights_format =
                kTfLiteFullyConnectedWeightsFormatShuffled4x16Int8;
 80a0a54:	7058      	strb	r0, [r3, #1]
            break;
 80a0a56:	e2a8      	b.n	80a0faa <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa36>
          default:
            error_reporter->Report("Unhandled fully-connected weights format.");
 80a0a58:	49cf      	ldr	r1, [pc, #828]	; (80a0d98 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x824>)
 80a0a5a:	4638      	mov	r0, r7
 80a0a5c:	f7ff fca6 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>

      /// Destructor, invokes the deleter if the stored pointer is not null.
      ~unique_ptr() noexcept
      {
	auto& __ptr = std::get<0>(_M_t);
	if (__ptr != nullptr)
 80a0a60:	9901      	ldr	r1, [sp, #4]
 80a0a62:	e299      	b.n	80a0f98 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa24>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0a64:	4628      	mov	r0, r5
 80a0a66:	f7ff fd79 	bl	80a055c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
 80a0a6a:	4605      	mov	r5, r0
  }
  const FullyConnectedOptions *builtin_options_as_FullyConnectedOptions() const {
    return builtin_options_type() == BuiltinOptions_FullyConnectedOptions ? static_cast<const FullyConnectedOptions *>(builtin_options()) : nullptr;
  }
  const SoftmaxOptions *builtin_options_as_SoftmaxOptions() const {
    return builtin_options_type() == BuiltinOptions_SoftmaxOptions ? static_cast<const SoftmaxOptions *>(builtin_options()) : nullptr;
 80a0a6c:	4620      	mov	r0, r4
 80a0a6e:	f7ff fd2f 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0a72:	2809      	cmp	r0, #9
 80a0a74:	f040 8476 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0a78:	4620      	mov	r0, r4
 80a0a7a:	f7ff fd56 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
    case BuiltinOperator_HASHTABLE_LOOKUP:
      // no-op.
      break;
    case BuiltinOperator_SOFTMAX: {
      auto params = safe_allocator.Allocate<TfLiteSoftmaxParams>();
      if (const auto* softmax_params =
 80a0a7e:	2800      	cmp	r0, #0
 80a0a80:	f000 8470 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef SoftmaxOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BETA = 4
  };
  float beta() const {
    return GetField<float>(VT_BETA, 0.0f);
 80a0a84:	2200      	movs	r2, #0
 80a0a86:	2104      	movs	r1, #4
 80a0a88:	f7ff fd3c 	bl	80a0504 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
              op->builtin_options_as_SoftmaxOptions()) {
        params->beta = softmax_params->beta();
 80a0a8c:	6028      	str	r0, [r5, #0]
 80a0a8e:	f000 bc69 	b.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0a92:	4628      	mov	r0, r5
 80a0a94:	f7ff fd6a 	bl	80a056c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
 80a0a98:	4605      	mov	r5, r0
  }
  const SoftmaxOptions *builtin_options_as_SoftmaxOptions() const {
    return builtin_options_type() == BuiltinOptions_SoftmaxOptions ? static_cast<const SoftmaxOptions *>(builtin_options()) : nullptr;
  }
  const ConcatenationOptions *builtin_options_as_ConcatenationOptions() const {
    return builtin_options_type() == BuiltinOptions_ConcatenationOptions ? static_cast<const ConcatenationOptions *>(builtin_options()) : nullptr;
 80a0a9a:	4620      	mov	r0, r4
 80a0a9c:	f7ff fd18 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0aa0:	280a      	cmp	r0, #10
 80a0aa2:	f040 845f 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0aa6:	4620      	mov	r0, r4
 80a0aa8:	f7ff fd3f 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_CONCATENATION: {
      auto params = safe_allocator.Allocate<TfLiteConcatenationParams>();
      if (const auto* concatenation_params =
 80a0aac:	4604      	mov	r4, r0
 80a0aae:	2800      	cmp	r0, #0
 80a0ab0:	f000 8458 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  };
  int32_t axis() const {
    return GetField<int32_t>(VT_AXIS, 0);
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
 80a0ab4:	2200      	movs	r2, #0
 80a0ab6:	2106      	movs	r1, #6
 80a0ab8:	f7ff fd1a 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_ConcatenationOptions()) {
        params->activation =
            parse_activation(concatenation_params->fused_activation_function());
 80a0abc:	b2c0      	uxtb	r0, r0
 80a0abe:	f7ff fc8f 	bl	80a03e0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXIS = 4,
    VT_FUSED_ACTIVATION_FUNCTION = 6
  };
  int32_t axis() const {
    return GetField<int32_t>(VT_AXIS, 0);
 80a0ac2:	2200      	movs	r2, #0
 80a0ac4:	7128      	strb	r0, [r5, #4]
 80a0ac6:	2104      	movs	r1, #4
 80a0ac8:	4620      	mov	r0, r4
 80a0aca:	f7ff fd07 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->axis = concatenation_params->axis();
 80a0ace:	6028      	str	r0, [r5, #0]
 80a0ad0:	f000 bc48 	b.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0ad4:	4628      	mov	r0, r5
 80a0ad6:	f7ff fd45 	bl	80a0564 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
 80a0ada:	4605      	mov	r5, r0
  }
  const EmbeddingLookupSparseOptions *builtin_options_as_EmbeddingLookupSparseOptions() const {
    return builtin_options_type() == BuiltinOptions_EmbeddingLookupSparseOptions ? static_cast<const EmbeddingLookupSparseOptions *>(builtin_options()) : nullptr;
  }
  const MulOptions *builtin_options_as_MulOptions() const {
    return builtin_options_type() == BuiltinOptions_MulOptions ? static_cast<const MulOptions *>(builtin_options()) : nullptr;
 80a0adc:	4620      	mov	r0, r4
 80a0ade:	f7ff fcf7 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0ae2:	2815      	cmp	r0, #21
 80a0ae4:	f040 843e 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0ae8:	4620      	mov	r0, r4
 80a0aea:	f7ff fd1e 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_MUL: {
      auto params = safe_allocator.Allocate<TfLiteMulParams>();
      if (const auto* schema_params = op->builtin_options_as_MulOptions()) {
 80a0aee:	2800      	cmp	r0, #0
 80a0af0:	f000 8438 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef MulOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSED_ACTIVATION_FUNCTION = 4
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
 80a0af4:	2200      	movs	r2, #0
 80a0af6:	2104      	movs	r1, #4
 80a0af8:	f7ff fcfa 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(schema_params->fused_activation_function());
 80a0afc:	b2c0      	uxtb	r0, r0
 80a0afe:	f7ff fc6f 	bl	80a03e0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
 80a0b02:	7028      	strb	r0, [r5, #0]
 80a0b04:	f000 bc2e 	b.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0b08:	4628      	mov	r0, r5
 80a0b0a:	f7ff fd2b 	bl	80a0564 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
 80a0b0e:	4605      	mov	r5, r0
  }
  const ConcatenationOptions *builtin_options_as_ConcatenationOptions() const {
    return builtin_options_type() == BuiltinOptions_ConcatenationOptions ? static_cast<const ConcatenationOptions *>(builtin_options()) : nullptr;
  }
  const AddOptions *builtin_options_as_AddOptions() const {
    return builtin_options_type() == BuiltinOptions_AddOptions ? static_cast<const AddOptions *>(builtin_options()) : nullptr;
 80a0b10:	4620      	mov	r0, r4
 80a0b12:	f7ff fcdd 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0b16:	280b      	cmp	r0, #11
 80a0b18:	f040 8424 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0b1c:	4620      	mov	r0, r4
 80a0b1e:	f7ff fd04 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_ADD: {
      auto params = safe_allocator.Allocate<TfLiteAddParams>();
      if (const auto* schema_params = op->builtin_options_as_AddOptions()) {
 80a0b22:	2800      	cmp	r0, #0
 80a0b24:	f000 841e 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef AddOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSED_ACTIVATION_FUNCTION = 4
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
 80a0b28:	2200      	movs	r2, #0
 80a0b2a:	2104      	movs	r1, #4
 80a0b2c:	f7ff fce0 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(schema_params->fused_activation_function());
 80a0b30:	b2c0      	uxtb	r0, r0
 80a0b32:	f7ff fc55 	bl	80a03e0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
 80a0b36:	7028      	strb	r0, [r5, #0]
 80a0b38:	f000 bc14 	b.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0b3c:	4628      	mov	r0, r5
 80a0b3e:	f7ff fd11 	bl	80a0564 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
 80a0b42:	4605      	mov	r5, r0
  }
  const SubOptions *builtin_options_as_SubOptions() const {
    return builtin_options_type() == BuiltinOptions_SubOptions ? static_cast<const SubOptions *>(builtin_options()) : nullptr;
  }
  const DivOptions *builtin_options_as_DivOptions() const {
    return builtin_options_type() == BuiltinOptions_DivOptions ? static_cast<const DivOptions *>(builtin_options()) : nullptr;
 80a0b44:	4620      	mov	r0, r4
 80a0b46:	f7ff fcc3 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0b4a:	281d      	cmp	r0, #29
 80a0b4c:	f040 840a 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0b50:	4620      	mov	r0, r4
 80a0b52:	f7ff fcea 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_DIV: {
      auto params = safe_allocator.Allocate<TfLiteDivParams>();
      if (const auto* schema_params = op->builtin_options_as_DivOptions()) {
 80a0b56:	2800      	cmp	r0, #0
 80a0b58:	f000 8404 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef DivOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSED_ACTIVATION_FUNCTION = 4
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
 80a0b5c:	2200      	movs	r2, #0
 80a0b5e:	2104      	movs	r1, #4
 80a0b60:	f7ff fcc6 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(schema_params->fused_activation_function());
 80a0b64:	b2c0      	uxtb	r0, r0
 80a0b66:	f7ff fc3b 	bl	80a03e0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
 80a0b6a:	7028      	strb	r0, [r5, #0]
 80a0b6c:	e3fa      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0b6e:	4628      	mov	r0, r5
 80a0b70:	f7ff fcf8 	bl	80a0564 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
 80a0b74:	4605      	mov	r5, r0
  }
  const ReducerOptions *builtin_options_as_ReducerOptions() const {
    return builtin_options_type() == BuiltinOptions_ReducerOptions ? static_cast<const ReducerOptions *>(builtin_options()) : nullptr;
  }
  const SubOptions *builtin_options_as_SubOptions() const {
    return builtin_options_type() == BuiltinOptions_SubOptions ? static_cast<const SubOptions *>(builtin_options()) : nullptr;
 80a0b76:	4620      	mov	r0, r4
 80a0b78:	f7ff fcaa 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0b7c:	281c      	cmp	r0, #28
 80a0b7e:	f040 83f1 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0b82:	4620      	mov	r0, r4
 80a0b84:	f7ff fcd1 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SUB: {
      auto params = safe_allocator.Allocate<TfLiteSubParams>();
      if (const auto* schema_params = op->builtin_options_as_SubOptions()) {
 80a0b88:	2800      	cmp	r0, #0
 80a0b8a:	f000 83eb 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef SubOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSED_ACTIVATION_FUNCTION = 4
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
 80a0b8e:	2200      	movs	r2, #0
 80a0b90:	2104      	movs	r1, #4
 80a0b92:	f7ff fcad 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(schema_params->fused_activation_function());
 80a0b96:	b2c0      	uxtb	r0, r0
 80a0b98:	f7ff fc22 	bl	80a03e0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
 80a0b9c:	7028      	strb	r0, [r5, #0]
 80a0b9e:	e3e1      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0ba0:	4628      	mov	r0, r5
 80a0ba2:	f7ff fcdf 	bl	80a0564 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
 80a0ba6:	4605      	mov	r5, r0
  }
  const AddOptions *builtin_options_as_AddOptions() const {
    return builtin_options_type() == BuiltinOptions_AddOptions ? static_cast<const AddOptions *>(builtin_options()) : nullptr;
  }
  const L2NormOptions *builtin_options_as_L2NormOptions() const {
    return builtin_options_type() == BuiltinOptions_L2NormOptions ? static_cast<const L2NormOptions *>(builtin_options()) : nullptr;
 80a0ba8:	4620      	mov	r0, r4
 80a0baa:	f7ff fc91 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0bae:	280c      	cmp	r0, #12
 80a0bb0:	f040 83d8 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0bb4:	4620      	mov	r0, r4
 80a0bb6:	f7ff fcb8 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_L2_NORMALIZATION: {
      auto params = safe_allocator.Allocate<TfLiteL2NormParams>();
      if (const auto* schema_params = op->builtin_options_as_L2NormOptions()) {
 80a0bba:	2800      	cmp	r0, #0
 80a0bbc:	f000 83d2 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef L2NormOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSED_ACTIVATION_FUNCTION = 4
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
 80a0bc0:	2200      	movs	r2, #0
 80a0bc2:	2104      	movs	r1, #4
 80a0bc4:	f7ff fc94 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(schema_params->fused_activation_function());
 80a0bc8:	b2c0      	uxtb	r0, r0
 80a0bca:	f7ff fc09 	bl	80a03e0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
 80a0bce:	7028      	strb	r0, [r5, #0]
 80a0bd0:	e3c8      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0bd2:	4628      	mov	r0, r5
 80a0bd4:	f7ff fcbe 	bl	80a0554 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteFakeQuantParamsEEPT_v>
 80a0bd8:	4605      	mov	r5, r0
  }
  const L2NormOptions *builtin_options_as_L2NormOptions() const {
    return builtin_options_type() == BuiltinOptions_L2NormOptions ? static_cast<const L2NormOptions *>(builtin_options()) : nullptr;
  }
  const LocalResponseNormalizationOptions *builtin_options_as_LocalResponseNormalizationOptions() const {
    return builtin_options_type() == BuiltinOptions_LocalResponseNormalizationOptions ? static_cast<const LocalResponseNormalizationOptions *>(builtin_options()) : nullptr;
 80a0bda:	4620      	mov	r0, r4
 80a0bdc:	f7ff fc78 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0be0:	280d      	cmp	r0, #13
 80a0be2:	f040 83bf 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0be6:	4620      	mov	r0, r4
 80a0be8:	f7ff fc9f 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_LOCAL_RESPONSE_NORMALIZATION: {
      auto params = safe_allocator.Allocate<TfLiteLocalResponseNormParams>();
      if (const auto* schema_params =
 80a0bec:	4604      	mov	r4, r0
 80a0bee:	2800      	cmp	r0, #0
 80a0bf0:	f000 83b8 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
    VT_BIAS = 6,
    VT_ALPHA = 8,
    VT_BETA = 10
  };
  int32_t radius() const {
    return GetField<int32_t>(VT_RADIUS, 0);
 80a0bf4:	2200      	movs	r2, #0
 80a0bf6:	2104      	movs	r1, #4
 80a0bf8:	f7ff fc70 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  float bias() const {
    return GetField<float>(VT_BIAS, 0.0f);
 80a0bfc:	2200      	movs	r2, #0
              op->builtin_options_as_LocalResponseNormalizationOptions()) {
        params->radius = schema_params->radius();
 80a0bfe:	6028      	str	r0, [r5, #0]
 80a0c00:	2106      	movs	r1, #6
 80a0c02:	4620      	mov	r0, r4
 80a0c04:	f7ff fc7e 	bl	80a0504 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  float alpha() const {
    return GetField<float>(VT_ALPHA, 0.0f);
 80a0c08:	2200      	movs	r2, #0
        params->bias = schema_params->bias();
 80a0c0a:	6068      	str	r0, [r5, #4]
 80a0c0c:	2108      	movs	r1, #8
 80a0c0e:	4620      	mov	r0, r4
 80a0c10:	f7ff fc78 	bl	80a0504 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  float beta() const {
    return GetField<float>(VT_BETA, 0.0f);
 80a0c14:	2200      	movs	r2, #0
        params->alpha = schema_params->alpha();
 80a0c16:	60a8      	str	r0, [r5, #8]
 80a0c18:	210a      	movs	r1, #10
 80a0c1a:	4620      	mov	r0, r4
 80a0c1c:	f7ff fc72 	bl	80a0504 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
        params->beta = schema_params->beta();
 80a0c20:	60e8      	str	r0, [r5, #12]
 80a0c22:	e39f      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0c24:	4628      	mov	r0, r5
 80a0c26:	f7ff fc95 	bl	80a0554 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteFakeQuantParamsEEPT_v>
 80a0c2a:	4680      	mov	r8, r0
  }
  const LocalResponseNormalizationOptions *builtin_options_as_LocalResponseNormalizationOptions() const {
    return builtin_options_type() == BuiltinOptions_LocalResponseNormalizationOptions ? static_cast<const LocalResponseNormalizationOptions *>(builtin_options()) : nullptr;
  }
  const LSTMOptions *builtin_options_as_LSTMOptions() const {
    return builtin_options_type() == BuiltinOptions_LSTMOptions ? static_cast<const LSTMOptions *>(builtin_options()) : nullptr;
 80a0c2c:	4620      	mov	r0, r4
 80a0c2e:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a0c32:	f7ff fc4d 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0c36:	280e      	cmp	r0, #14
 80a0c38:	d12c      	bne.n	80a0c94 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x720>
 80a0c3a:	4620      	mov	r0, r4
 80a0c3c:	f7ff fc75 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_LSTM: {
      auto params = safe_allocator.Allocate<TfLiteLSTMParams>();
      if (const auto* lstm_params = op->builtin_options_as_LSTMOptions()) {
 80a0c40:	4605      	mov	r5, r0
 80a0c42:	b338      	cbz	r0, 80a0c94 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x720>
    VT_CELL_CLIP = 6,
    VT_PROJ_CLIP = 8,
    VT_KERNEL_TYPE = 10
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
 80a0c44:	2200      	movs	r2, #0
 80a0c46:	2104      	movs	r1, #4
 80a0c48:	f7ff fc52 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(lstm_params->fused_activation_function());
 80a0c4c:	b2c0      	uxtb	r0, r0
 80a0c4e:	f7ff fbc7 	bl	80a03e0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  }
  float cell_clip() const {
    return GetField<float>(VT_CELL_CLIP, 0.0f);
 80a0c52:	2200      	movs	r2, #0
 80a0c54:	f888 0000 	strb.w	r0, [r8]
 80a0c58:	2106      	movs	r1, #6
 80a0c5a:	4628      	mov	r0, r5
 80a0c5c:	f7ff fc52 	bl	80a0504 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
      }

      /// Return the stored pointer.
      pointer
      get() const noexcept
      { return std::get<0>(_M_t); }
 80a0c60:	9c01      	ldr	r4, [sp, #4]
  }
  float proj_clip() const {
    return GetField<float>(VT_PROJ_CLIP, 0.0f);
 80a0c62:	2200      	movs	r2, #0
        params->cell_clip = lstm_params->cell_clip();
 80a0c64:	6060      	str	r0, [r4, #4]
 80a0c66:	2108      	movs	r1, #8
 80a0c68:	4628      	mov	r0, r5
 80a0c6a:	f7ff fc4b 	bl	80a0504 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
 80a0c6e:	9c01      	ldr	r4, [sp, #4]
  }
  LSTMKernelType kernel_type() const {
    return static_cast<LSTMKernelType>(GetField<int8_t>(VT_KERNEL_TYPE, 0));
 80a0c70:	2200      	movs	r2, #0
        params->proj_clip = lstm_params->proj_clip();
 80a0c72:	60a0      	str	r0, [r4, #8]
 80a0c74:	210a      	movs	r1, #10
 80a0c76:	4628      	mov	r0, r5
 80a0c78:	f7ff fc3a 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        switch (lstm_params->kernel_type()) {
 80a0c7c:	b108      	cbz	r0, 80a0c82 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x70e>
 80a0c7e:	2801      	cmp	r0, #1
 80a0c80:	d102      	bne.n	80a0c88 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x714>
          case LSTMKernelType_FULL:
            params->kernel_type = kTfLiteLSTMFullKernel;
            break;
          case LSTMKernelType_BASIC:
            params->kernel_type = kTfLiteLSTMBasicKernel;
 80a0c82:	7320      	strb	r0, [r4, #12]
        }
      } else {
        error_reporter->Report("No valid LSTM builtin options exist");
        return kTfLiteError;
      }
      *builtin_data = reinterpret_cast<void*>(params.release());
 80a0c84:	6034      	str	r4, [r6, #0]
 80a0c86:	e36e      	b.n	80a1366 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf2>
          case LSTMKernelType_BASIC:
            params->kernel_type = kTfLiteLSTMBasicKernel;
            break;
          default:
            error_reporter->Report("Unhandled LSTM kernel type: %d",
                                   lstm_params->kernel_type());
 80a0c88:	b2c2      	uxtb	r2, r0
 80a0c8a:	4944      	ldr	r1, [pc, #272]	; (80a0d9c <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x828>)
 80a0c8c:	4638      	mov	r0, r7
 80a0c8e:	f7ff fb8d 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
            return kTfLiteError;
 80a0c92:	e6e5      	b.n	80a0a60 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x4ec>
        }
      } else {
        error_reporter->Report("No valid LSTM builtin options exist");
 80a0c94:	4942      	ldr	r1, [pc, #264]	; (80a0da0 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x82c>)
 80a0c96:	e6e0      	b.n	80a0a5a <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x4e6>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0c98:	4628      	mov	r0, r5
 80a0c9a:	f7ff fc5b 	bl	80a0554 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteFakeQuantParamsEEPT_v>
 80a0c9e:	4605      	mov	r5, r0
  }
  const BidirectionalSequenceRNNOptions *builtin_options_as_BidirectionalSequenceRNNOptions() const {
    return builtin_options_type() == BuiltinOptions_BidirectionalSequenceRNNOptions ? static_cast<const BidirectionalSequenceRNNOptions *>(builtin_options()) : nullptr;
  }
  const UnidirectionalSequenceLSTMOptions *builtin_options_as_UnidirectionalSequenceLSTMOptions() const {
    return builtin_options_type() == BuiltinOptions_UnidirectionalSequenceLSTMOptions ? static_cast<const UnidirectionalSequenceLSTMOptions *>(builtin_options()) : nullptr;
 80a0ca0:	4620      	mov	r0, r4
 80a0ca2:	f7ff fc15 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0ca6:	2847      	cmp	r0, #71	; 0x47
 80a0ca8:	f040 835c 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0cac:	4620      	mov	r0, r4
 80a0cae:	f7ff fc3c 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_UNIDIRECTIONAL_SEQUENCE_LSTM: {
      auto params =
          safe_allocator.Allocate<TfLiteUnidirectionalSequenceLSTMParams>();
      if (const auto* seq_lstm_params =
 80a0cb2:	4604      	mov	r4, r0
 80a0cb4:	2800      	cmp	r0, #0
 80a0cb6:	f000 8355 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
    VT_CELL_CLIP = 6,
    VT_PROJ_CLIP = 8,
    VT_TIME_MAJOR = 10
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
 80a0cba:	2200      	movs	r2, #0
 80a0cbc:	2104      	movs	r1, #4
 80a0cbe:	f7ff fc17 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_UnidirectionalSequenceLSTMOptions()) {
        params->activation =
            parse_activation(seq_lstm_params->fused_activation_function());
 80a0cc2:	b2c0      	uxtb	r0, r0
 80a0cc4:	f7ff fb8c 	bl	80a03e0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  }
  float cell_clip() const {
    return GetField<float>(VT_CELL_CLIP, 0.0f);
 80a0cc8:	2200      	movs	r2, #0
 80a0cca:	7028      	strb	r0, [r5, #0]
 80a0ccc:	2106      	movs	r1, #6
 80a0cce:	4620      	mov	r0, r4
 80a0cd0:	f7ff fc18 	bl	80a0504 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  float proj_clip() const {
    return GetField<float>(VT_PROJ_CLIP, 0.0f);
 80a0cd4:	2200      	movs	r2, #0
        params->cell_clip = seq_lstm_params->cell_clip();
 80a0cd6:	6068      	str	r0, [r5, #4]
 80a0cd8:	2108      	movs	r1, #8
 80a0cda:	4620      	mov	r0, r4
 80a0cdc:	f7ff fc12 	bl	80a0504 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  bool time_major() const {
    return GetField<uint8_t>(VT_TIME_MAJOR, 0) != 0;
 80a0ce0:	2200      	movs	r2, #0
        params->proj_clip = seq_lstm_params->proj_clip();
 80a0ce2:	60a8      	str	r0, [r5, #8]
 80a0ce4:	210a      	movs	r1, #10
 80a0ce6:	4620      	mov	r0, r4
 80a0ce8:	f7ff fbe8 	bl	80a04bc <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->time_major = seq_lstm_params->time_major();
 80a0cec:	3000      	adds	r0, #0
 80a0cee:	bf18      	it	ne
 80a0cf0:	2001      	movne	r0, #1
 80a0cf2:	7328      	strb	r0, [r5, #12]
 80a0cf4:	e336      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0cf6:	4628      	mov	r0, r5
 80a0cf8:	f7ff fc2c 	bl	80a0554 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteFakeQuantParamsEEPT_v>
 80a0cfc:	4605      	mov	r5, r0
  }
  const FillOptions *builtin_options_as_FillOptions() const {
    return builtin_options_type() == BuiltinOptions_FillOptions ? static_cast<const FillOptions *>(builtin_options()) : nullptr;
  }
  const BidirectionalSequenceLSTMOptions *builtin_options_as_BidirectionalSequenceLSTMOptions() const {
    return builtin_options_type() == BuiltinOptions_BidirectionalSequenceLSTMOptions ? static_cast<const BidirectionalSequenceLSTMOptions *>(builtin_options()) : nullptr;
 80a0cfe:	4620      	mov	r0, r4
 80a0d00:	f7ff fbe6 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0d04:	2845      	cmp	r0, #69	; 0x45
 80a0d06:	f040 832d 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0d0a:	4620      	mov	r0, r4
 80a0d0c:	f7ff fc0d 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_BIDIRECTIONAL_SEQUENCE_LSTM: {
      auto params =
          safe_allocator.Allocate<TfLiteBidirectionalSequenceLSTMParams>();
      if (const auto* bidi_lstm_params =
 80a0d10:	4604      	mov	r4, r0
 80a0d12:	2800      	cmp	r0, #0
 80a0d14:	f000 8326 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
    VT_PROJ_CLIP = 8,
    VT_MERGE_OUTPUTS = 10,
    VT_TIME_MAJOR = 12
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
 80a0d18:	2200      	movs	r2, #0
 80a0d1a:	2104      	movs	r1, #4
 80a0d1c:	f7ff fbe8 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_BidirectionalSequenceLSTMOptions()) {
        params->activation =
            parse_activation(bidi_lstm_params->fused_activation_function());
 80a0d20:	b2c0      	uxtb	r0, r0
 80a0d22:	f7ff fb5d 	bl	80a03e0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  }
  float cell_clip() const {
    return GetField<float>(VT_CELL_CLIP, 0.0f);
 80a0d26:	2200      	movs	r2, #0
 80a0d28:	7028      	strb	r0, [r5, #0]
 80a0d2a:	2106      	movs	r1, #6
 80a0d2c:	4620      	mov	r0, r4
 80a0d2e:	f7ff fbe9 	bl	80a0504 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  float proj_clip() const {
    return GetField<float>(VT_PROJ_CLIP, 0.0f);
 80a0d32:	2200      	movs	r2, #0
        params->cell_clip = bidi_lstm_params->cell_clip();
 80a0d34:	6068      	str	r0, [r5, #4]
 80a0d36:	2108      	movs	r1, #8
 80a0d38:	4620      	mov	r0, r4
 80a0d3a:	f7ff fbe3 	bl	80a0504 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  bool merge_outputs() const {
    return GetField<uint8_t>(VT_MERGE_OUTPUTS, 0) != 0;
 80a0d3e:	2200      	movs	r2, #0
        params->proj_clip = bidi_lstm_params->proj_clip();
 80a0d40:	60a8      	str	r0, [r5, #8]
 80a0d42:	210a      	movs	r1, #10
 80a0d44:	4620      	mov	r0, r4
 80a0d46:	f7ff fbb9 	bl	80a04bc <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->merge_outputs = bidi_lstm_params->merge_outputs();
 80a0d4a:	3000      	adds	r0, #0
 80a0d4c:	bf18      	it	ne
 80a0d4e:	2001      	movne	r0, #1
 80a0d50:	7328      	strb	r0, [r5, #12]
  }
  bool time_major() const {
    return GetField<uint8_t>(VT_TIME_MAJOR, 1) != 0;
 80a0d52:	2201      	movs	r2, #1
 80a0d54:	210c      	movs	r1, #12
 80a0d56:	4620      	mov	r0, r4
 80a0d58:	f7ff fbb0 	bl	80a04bc <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->time_major = bidi_lstm_params->time_major();
 80a0d5c:	3000      	adds	r0, #0
 80a0d5e:	bf18      	it	ne
 80a0d60:	2001      	movne	r0, #1
 80a0d62:	7368      	strb	r0, [r5, #13]
 80a0d64:	e2fe      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0d66:	4628      	mov	r0, r5
 80a0d68:	f7ff fbfc 	bl	80a0564 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
 80a0d6c:	4605      	mov	r5, r0
  }
  const LSTMOptions *builtin_options_as_LSTMOptions() const {
    return builtin_options_type() == BuiltinOptions_LSTMOptions ? static_cast<const LSTMOptions *>(builtin_options()) : nullptr;
  }
  const ResizeBilinearOptions *builtin_options_as_ResizeBilinearOptions() const {
    return builtin_options_type() == BuiltinOptions_ResizeBilinearOptions ? static_cast<const ResizeBilinearOptions *>(builtin_options()) : nullptr;
 80a0d6e:	4620      	mov	r0, r4
 80a0d70:	f7ff fbae 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0d74:	280f      	cmp	r0, #15
 80a0d76:	f040 82f5 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0d7a:	4620      	mov	r0, r4
 80a0d7c:	f7ff fbd5 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_RESIZE_BILINEAR: {
      auto params = safe_allocator.Allocate<TfLiteResizeBilinearParams>();
      if (const auto* schema_params =
 80a0d80:	2800      	cmp	r0, #0
 80a0d82:	f000 82ef 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef ResizeBilinearOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ALIGN_CORNERS = 8
  };
  bool align_corners() const {
    return GetField<uint8_t>(VT_ALIGN_CORNERS, 0) != 0;
 80a0d86:	2200      	movs	r2, #0
 80a0d88:	2108      	movs	r1, #8
 80a0d8a:	f7ff fb97 	bl	80a04bc <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
              op->builtin_options_as_ResizeBilinearOptions()) {
        params->align_corners = schema_params->align_corners();
 80a0d8e:	3000      	adds	r0, #0
 80a0d90:	bf18      	it	ne
 80a0d92:	2001      	movne	r0, #1
 80a0d94:	7028      	strb	r0, [r5, #0]
 80a0d96:	e2e5      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0d98:	080b4904 	.word	0x080b4904
 80a0d9c:	080b492e 	.word	0x080b492e
 80a0da0:	080b494d 	.word	0x080b494d
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0da4:	4628      	mov	r0, r5
 80a0da6:	f7ff fbdd 	bl	80a0564 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
 80a0daa:	4605      	mov	r5, r0
  }
  const RangeOptions *builtin_options_as_RangeOptions() const {
    return builtin_options_type() == BuiltinOptions_RangeOptions ? static_cast<const RangeOptions *>(builtin_options()) : nullptr;
  }
  const ResizeNearestNeighborOptions *builtin_options_as_ResizeNearestNeighborOptions() const {
    return builtin_options_type() == BuiltinOptions_ResizeNearestNeighborOptions ? static_cast<const ResizeNearestNeighborOptions *>(builtin_options()) : nullptr;
 80a0dac:	4620      	mov	r0, r4
 80a0dae:	f7ff fb8f 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0db2:	284a      	cmp	r0, #74	; 0x4a
 80a0db4:	f040 82d6 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0db8:	4620      	mov	r0, r4
 80a0dba:	f7ff fbb6 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      // required to minimize function size. TODO(b/118447267): Simplify
      // ParseOpData function and reduce its length.
      [&]() {
        auto params =
            safe_allocator.Allocate<TfLiteResizeNearestNeighborParams>();
        if (const auto* schema_params =
 80a0dbe:	2800      	cmp	r0, #0
 80a0dc0:	f000 82d0 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef ResizeNearestNeighborOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ALIGN_CORNERS = 4
  };
  bool align_corners() const {
    return GetField<uint8_t>(VT_ALIGN_CORNERS, 0) != 0;
 80a0dc4:	2200      	movs	r2, #0
 80a0dc6:	2104      	movs	r1, #4
 80a0dc8:	f7ff fb78 	bl	80a04bc <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
                op->builtin_options_as_ResizeNearestNeighborOptions()) {
          params->align_corners = schema_params->align_corners();
 80a0dcc:	3000      	adds	r0, #0
 80a0dce:	bf18      	it	ne
 80a0dd0:	2001      	movne	r0, #1
 80a0dd2:	7028      	strb	r0, [r5, #0]
 80a0dd4:	e2c6      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0dd6:	4628      	mov	r0, r5
 80a0dd8:	f7ff fbb4 	bl	80a0544 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI19TfLiteSqueezeParamsEEPT_v>
 80a0ddc:	4680      	mov	r8, r0
  }
  const CallOptions *builtin_options_as_CallOptions() const {
    return builtin_options_type() == BuiltinOptions_CallOptions ? static_cast<const CallOptions *>(builtin_options()) : nullptr;
  }
  const ReshapeOptions *builtin_options_as_ReshapeOptions() const {
    return builtin_options_type() == BuiltinOptions_ReshapeOptions ? static_cast<const ReshapeOptions *>(builtin_options()) : nullptr;
 80a0dde:	4620      	mov	r0, r4
 80a0de0:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a0de4:	f7ff fb74 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0de8:	2811      	cmp	r0, #17
 80a0dea:	f040 80de 	bne.w	80a0faa <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa36>
 80a0dee:	4620      	mov	r0, r4
 80a0df0:	f7ff fb9b 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      }();
      break;
    }
    case BuiltinOperator_RESHAPE: {
      auto params = safe_allocator.Allocate<TfLiteReshapeParams>();
      if (const auto* schema_params = op->builtin_options_as_ReshapeOptions()) {
 80a0df4:	2800      	cmp	r0, #0
 80a0df6:	f000 80d8 	beq.w	80a0faa <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa36>
 80a0dfa:	2104      	movs	r1, #4
 80a0dfc:	f7ff fb8c 	bl	80a0518 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorIlEEEET_t>
        auto* new_shape = schema_params->new_shape();
        TF_LITE_ENSURE_STATUS(FlatBufferIntVectorToArray(
 80a0e00:	4bca      	ldr	r3, [pc, #808]	; (80a112c <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xbb8>)
 80a0e02:	4604      	mov	r4, r0
 80a0e04:	e0c2      	b.n	80a0f8c <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa18>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0e06:	4628      	mov	r0, r5
 80a0e08:	f7ff fba0 	bl	80a054c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI25TfLiteTransposeConvParamsEEPT_v>
 80a0e0c:	4605      	mov	r5, r0
  }
  const SkipGramOptions *builtin_options_as_SkipGramOptions() const {
    return builtin_options_type() == BuiltinOptions_SkipGramOptions ? static_cast<const SkipGramOptions *>(builtin_options()) : nullptr;
 80a0e0e:	4620      	mov	r0, r4
 80a0e10:	f7ff fb5e 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0e14:	2812      	cmp	r0, #18
 80a0e16:	f040 82a5 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0e1a:	4620      	mov	r0, r4
 80a0e1c:	f7ff fb85 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SKIP_GRAM: {
      auto params = safe_allocator.Allocate<TfLiteSkipGramParams>();
      if (const auto* skip_gram_params =
 80a0e20:	4604      	mov	r4, r0
 80a0e22:	2800      	cmp	r0, #0
 80a0e24:	f000 829e 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
    VT_NGRAM_SIZE = 4,
    VT_MAX_SKIP_SIZE = 6,
    VT_INCLUDE_ALL_NGRAMS = 8
  };
  int32_t ngram_size() const {
    return GetField<int32_t>(VT_NGRAM_SIZE, 0);
 80a0e28:	2200      	movs	r2, #0
 80a0e2a:	2104      	movs	r1, #4
 80a0e2c:	f7ff fb56 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t max_skip_size() const {
    return GetField<int32_t>(VT_MAX_SKIP_SIZE, 0);
 80a0e30:	2200      	movs	r2, #0
              op->builtin_options_as_SkipGramOptions()) {
        params->ngram_size = skip_gram_params->ngram_size();
 80a0e32:	6028      	str	r0, [r5, #0]
 80a0e34:	2106      	movs	r1, #6
 80a0e36:	4620      	mov	r0, r4
 80a0e38:	f7ff fb50 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  bool include_all_ngrams() const {
    return GetField<uint8_t>(VT_INCLUDE_ALL_NGRAMS, 0) != 0;
 80a0e3c:	2200      	movs	r2, #0
        params->max_skip_size = skip_gram_params->max_skip_size();
 80a0e3e:	6068      	str	r0, [r5, #4]
 80a0e40:	2108      	movs	r1, #8
 80a0e42:	4620      	mov	r0, r4
 80a0e44:	f7ff fb3a 	bl	80a04bc <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->include_all_ngrams = skip_gram_params->include_all_ngrams();
 80a0e48:	3000      	adds	r0, #0
 80a0e4a:	bf18      	it	ne
 80a0e4c:	2001      	movne	r0, #1
 80a0e4e:	7228      	strb	r0, [r5, #8]
 80a0e50:	e288      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0e52:	4628      	mov	r0, r5
 80a0e54:	f7ff fb82 	bl	80a055c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
 80a0e58:	4605      	mov	r5, r0
  }
  const SkipGramOptions *builtin_options_as_SkipGramOptions() const {
    return builtin_options_type() == BuiltinOptions_SkipGramOptions ? static_cast<const SkipGramOptions *>(builtin_options()) : nullptr;
  }
  const SpaceToDepthOptions *builtin_options_as_SpaceToDepthOptions() const {
    return builtin_options_type() == BuiltinOptions_SpaceToDepthOptions ? static_cast<const SpaceToDepthOptions *>(builtin_options()) : nullptr;
 80a0e5a:	4620      	mov	r0, r4
 80a0e5c:	f7ff fb38 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0e60:	2813      	cmp	r0, #19
 80a0e62:	f040 827f 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0e66:	4620      	mov	r0, r4
 80a0e68:	f7ff fb5f 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SPACE_TO_DEPTH: {
      auto params = safe_allocator.Allocate<TfLiteSpaceToDepthParams>();
      if (const auto* schema_params =
 80a0e6c:	2800      	cmp	r0, #0
 80a0e6e:	f000 8279 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef SpaceToDepthOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BLOCK_SIZE = 4
  };
  int32_t block_size() const {
    return GetField<int32_t>(VT_BLOCK_SIZE, 0);
 80a0e72:	2200      	movs	r2, #0
 80a0e74:	2104      	movs	r1, #4
 80a0e76:	f7ff fb31 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
              op->builtin_options_as_SpaceToDepthOptions()) {
        params->block_size = schema_params->block_size();
 80a0e7a:	6028      	str	r0, [r5, #0]
 80a0e7c:	e272      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0e7e:	4628      	mov	r0, r5
 80a0e80:	f7ff fb6c 	bl	80a055c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
 80a0e84:	4605      	mov	r5, r0
  }
  const WhileOptions *builtin_options_as_WhileOptions() const {
    return builtin_options_type() == BuiltinOptions_WhileOptions ? static_cast<const WhileOptions *>(builtin_options()) : nullptr;
  }
  const DepthToSpaceOptions *builtin_options_as_DepthToSpaceOptions() const {
    return builtin_options_type() == BuiltinOptions_DepthToSpaceOptions ? static_cast<const DepthToSpaceOptions *>(builtin_options()) : nullptr;
 80a0e86:	4620      	mov	r0, r4
 80a0e88:	f7ff fb22 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0e8c:	285e      	cmp	r0, #94	; 0x5e
 80a0e8e:	f040 8269 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0e92:	4620      	mov	r0, r4
 80a0e94:	f7ff fb49 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_DEPTH_TO_SPACE: {
      auto params = safe_allocator.Allocate<TfLiteDepthToSpaceParams>();
      if (const auto* schema_params =
 80a0e98:	2800      	cmp	r0, #0
 80a0e9a:	f000 8263 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef DepthToSpaceOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BLOCK_SIZE = 4
  };
  int32_t block_size() const {
    return GetField<int32_t>(VT_BLOCK_SIZE, 0);
 80a0e9e:	2200      	movs	r2, #0
 80a0ea0:	2104      	movs	r1, #4
 80a0ea2:	f7ff fb1b 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
              op->builtin_options_as_DepthToSpaceOptions()) {
        params->block_size = schema_params->block_size();
 80a0ea6:	6028      	str	r0, [r5, #0]
 80a0ea8:	e25c      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0eaa:	4628      	mov	r0, r5
 80a0eac:	f7ff fb56 	bl	80a055c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_GATHER: {
      auto params = safe_allocator.Allocate<TfLiteGatherParams>();
      params->axis = 0;
 80a0eb0:	f8c0 8000 	str.w	r8, [r0]
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0eb4:	4605      	mov	r5, r0
  }
  const PadOptions *builtin_options_as_PadOptions() const {
    return builtin_options_type() == BuiltinOptions_PadOptions ? static_cast<const PadOptions *>(builtin_options()) : nullptr;
  }
  const GatherOptions *builtin_options_as_GatherOptions() const {
    return builtin_options_type() == BuiltinOptions_GatherOptions ? static_cast<const GatherOptions *>(builtin_options()) : nullptr;
 80a0eb6:	4620      	mov	r0, r4
 80a0eb8:	f7ff fb0a 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0ebc:	2817      	cmp	r0, #23
 80a0ebe:	f040 8251 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0ec2:	4620      	mov	r0, r4
 80a0ec4:	f7ff fb31 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_GATHER: {
      auto params = safe_allocator.Allocate<TfLiteGatherParams>();
      params->axis = 0;
      if (const auto* gather_params = op->builtin_options_as_GatherOptions()) {
 80a0ec8:	2800      	cmp	r0, #0
 80a0eca:	f000 824b 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef GatherOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXIS = 4
  };
  int32_t axis() const {
    return GetField<int32_t>(VT_AXIS, 0);
 80a0ece:	2200      	movs	r2, #0
 80a0ed0:	2104      	movs	r1, #4
 80a0ed2:	f7ff fb03 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->axis = gather_params->axis();
 80a0ed6:	6028      	str	r0, [r5, #0]
 80a0ed8:	e244      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0eda:	4628      	mov	r0, r5
 80a0edc:	f7ff fb42 	bl	80a0564 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
 80a0ee0:	4605      	mov	r5, r0
  }
  const TransposeOptions *builtin_options_as_TransposeOptions() const {
    return builtin_options_type() == BuiltinOptions_TransposeOptions ? static_cast<const TransposeOptions *>(builtin_options()) : nullptr;
  }
  const ReducerOptions *builtin_options_as_ReducerOptions() const {
    return builtin_options_type() == BuiltinOptions_ReducerOptions ? static_cast<const ReducerOptions *>(builtin_options()) : nullptr;
 80a0ee2:	4620      	mov	r0, r4
 80a0ee4:	f7ff faf4 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0ee8:	281b      	cmp	r0, #27
 80a0eea:	f040 823b 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0eee:	4620      	mov	r0, r4
 80a0ef0:	f7ff fb1b 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
    case BuiltinOperator_REDUCE_MIN:
    case BuiltinOperator_REDUCE_PROD:
    case BuiltinOperator_REDUCE_ANY:
    case BuiltinOperator_SUM: {
      auto params = safe_allocator.Allocate<TfLiteReducerParams>();
      if (const auto* schema_params = op->builtin_options_as_ReducerOptions()) {
 80a0ef4:	2800      	cmp	r0, #0
 80a0ef6:	f000 8235 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef ReducerOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_KEEP_DIMS = 4
  };
  bool keep_dims() const {
    return GetField<uint8_t>(VT_KEEP_DIMS, 0) != 0;
 80a0efa:	2200      	movs	r2, #0
 80a0efc:	2104      	movs	r1, #4
 80a0efe:	f7ff fadd 	bl	80a04bc <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->keep_dims = schema_params->keep_dims();
 80a0f02:	3000      	adds	r0, #0
 80a0f04:	bf18      	it	ne
 80a0f06:	2001      	movne	r0, #1
 80a0f08:	7028      	strb	r0, [r5, #0]
 80a0f0a:	e22b      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0f0c:	4628      	mov	r0, r5
 80a0f0e:	f7ff fb25 	bl	80a055c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
 80a0f12:	4605      	mov	r5, r0
  }
  const TopKV2Options *builtin_options_as_TopKV2Options() const {
    return builtin_options_type() == BuiltinOptions_TopKV2Options ? static_cast<const TopKV2Options *>(builtin_options()) : nullptr;
  }
  const SplitOptions *builtin_options_as_SplitOptions() const {
    return builtin_options_type() == BuiltinOptions_SplitOptions ? static_cast<const SplitOptions *>(builtin_options()) : nullptr;
 80a0f14:	4620      	mov	r0, r4
 80a0f16:	f7ff fadb 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0f1a:	2823      	cmp	r0, #35	; 0x23
 80a0f1c:	f040 8222 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0f20:	4620      	mov	r0, r4
 80a0f22:	f7ff fb02 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SPLIT: {
      auto params = safe_allocator.Allocate<TfLiteSplitParams>();
      if (const auto* schema_params = op->builtin_options_as_SplitOptions()) {
 80a0f26:	2800      	cmp	r0, #0
 80a0f28:	f000 821c 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef SplitOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_NUM_SPLITS = 4
  };
  int32_t num_splits() const {
    return GetField<int32_t>(VT_NUM_SPLITS, 0);
 80a0f2c:	2200      	movs	r2, #0
 80a0f2e:	2104      	movs	r1, #4
 80a0f30:	f7ff fad4 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->num_splits = schema_params->num_splits();
 80a0f34:	6028      	str	r0, [r5, #0]
 80a0f36:	e215      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0f38:	4628      	mov	r0, r5
 80a0f3a:	f7ff fb0f 	bl	80a055c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
 80a0f3e:	4605      	mov	r5, r0
  }
  const AbsOptions *builtin_options_as_AbsOptions() const {
    return builtin_options_type() == BuiltinOptions_AbsOptions ? static_cast<const AbsOptions *>(builtin_options()) : nullptr;
  }
  const SplitVOptions *builtin_options_as_SplitVOptions() const {
    return builtin_options_type() == BuiltinOptions_SplitVOptions ? static_cast<const SplitVOptions *>(builtin_options()) : nullptr;
 80a0f40:	4620      	mov	r0, r4
 80a0f42:	f7ff fac5 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0f46:	284f      	cmp	r0, #79	; 0x4f
 80a0f48:	f040 820c 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0f4c:	4620      	mov	r0, r4
 80a0f4e:	f7ff faec 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SPLIT_V: {
      auto params = safe_allocator.Allocate<TfLiteSplitParams>();
      if (const auto* schema_params = op->builtin_options_as_SplitVOptions()) {
 80a0f52:	2800      	cmp	r0, #0
 80a0f54:	f000 8206 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef SplitVOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_NUM_SPLITS = 4
  };
  int32_t num_splits() const {
    return GetField<int32_t>(VT_NUM_SPLITS, 0);
 80a0f58:	2200      	movs	r2, #0
 80a0f5a:	2104      	movs	r1, #4
 80a0f5c:	f7ff fabe 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->num_splits = schema_params->num_splits();
 80a0f60:	6028      	str	r0, [r5, #0]
 80a0f62:	e1ff      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a0f64:	4628      	mov	r0, r5
 80a0f66:	f7ff faed 	bl	80a0544 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI19TfLiteSqueezeParamsEEPT_v>
 80a0f6a:	4680      	mov	r8, r0
  }
  const DivOptions *builtin_options_as_DivOptions() const {
    return builtin_options_type() == BuiltinOptions_DivOptions ? static_cast<const DivOptions *>(builtin_options()) : nullptr;
  }
  const SqueezeOptions *builtin_options_as_SqueezeOptions() const {
    return builtin_options_type() == BuiltinOptions_SqueezeOptions ? static_cast<const SqueezeOptions *>(builtin_options()) : nullptr;
 80a0f6c:	4620      	mov	r0, r4
 80a0f6e:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a0f72:	f7ff faad 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0f76:	281e      	cmp	r0, #30
 80a0f78:	d117      	bne.n	80a0faa <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa36>
 80a0f7a:	4620      	mov	r0, r4
 80a0f7c:	f7ff fad5 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SQUEEZE: {
      auto params = safe_allocator.Allocate<TfLiteSqueezeParams>();
      if (const auto* schema_params = op->builtin_options_as_SqueezeOptions()) {
 80a0f80:	b198      	cbz	r0, 80a0faa <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa36>
 80a0f82:	2104      	movs	r1, #4
 80a0f84:	f7ff fac8 	bl	80a0518 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorIlEEEET_t>
 80a0f88:	4604      	mov	r4, r0
        const auto& squeeze_dims = schema_params->squeeze_dims();
        TF_LITE_ENSURE_STATUS(FlatBufferIntVectorToArray(
 80a0f8a:	4b69      	ldr	r3, [pc, #420]	; (80a1130 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xbbc>)
 80a0f8c:	4641      	mov	r1, r8
 80a0f8e:	463a      	mov	r2, r7
 80a0f90:	f7ff fa30 	bl	80a03f4 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4>
 80a0f94:	9901      	ldr	r1, [sp, #4]
 80a0f96:	b130      	cbz	r0, 80a0fa6 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa32>

      /// Destructor, invokes the deleter if the stored pointer is not null.
      ~unique_ptr() noexcept
      {
	auto& __ptr = std::get<0>(_M_t);
	if (__ptr != nullptr)
 80a0f98:	2900      	cmp	r1, #0
 80a0f9a:	f000 80ec 	beq.w	80a1176 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xc02>
	  get_deleter()(__ptr);
 80a0f9e:	4668      	mov	r0, sp
 80a0fa0:	f7ff fa12 	bl	80a03c8 <_ZN6tflite12_GLOBAL__N_124SafeBuiltinDataAllocator18BuiltinDataDeleterclEPv>
 80a0fa4:	e0e7      	b.n	80a1176 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xc02>
            sizeof(params->squeeze_dims), squeeze_dims, params->squeeze_dims,
            error_reporter, "squeeze"));
        params->num_squeeze_dims = squeeze_dims->size();
 80a0fa6:	6823      	ldr	r3, [r4, #0]
 80a0fa8:	620b      	str	r3, [r1, #32]
      }
      *builtin_data = reinterpret_cast<void*>(params.release());
 80a0faa:	9b01      	ldr	r3, [sp, #4]
 80a0fac:	6033      	str	r3, [r6, #0]
 80a0fae:	e1da      	b.n	80a1366 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf2>
 80a0fb0:	682b      	ldr	r3, [r5, #0]
 80a0fb2:	2114      	movs	r1, #20
 80a0fb4:	681b      	ldr	r3, [r3, #0]
 80a0fb6:	4628      	mov	r0, r5
 80a0fb8:	4798      	blx	r3
 80a0fba:	4605      	mov	r5, r0
  }
  const SequenceRNNOptions *builtin_options_as_SequenceRNNOptions() const {
    return builtin_options_type() == BuiltinOptions_SequenceRNNOptions ? static_cast<const SequenceRNNOptions *>(builtin_options()) : nullptr;
  }
  const StridedSliceOptions *builtin_options_as_StridedSliceOptions() const {
    return builtin_options_type() == BuiltinOptions_StridedSliceOptions ? static_cast<const StridedSliceOptions *>(builtin_options()) : nullptr;
 80a0fbc:	4620      	mov	r0, r4
 80a0fbe:	f7ff fa87 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a0fc2:	2820      	cmp	r0, #32
 80a0fc4:	f040 81ce 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a0fc8:	4620      	mov	r0, r4
 80a0fca:	f7ff faae 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_STRIDED_SLICE: {
      auto params = safe_allocator.Allocate<TfLiteStridedSliceParams>();
      if (const auto* schema_params =
 80a0fce:	4604      	mov	r4, r0
 80a0fd0:	2800      	cmp	r0, #0
 80a0fd2:	f000 81c7 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
    VT_ELLIPSIS_MASK = 8,
    VT_NEW_AXIS_MASK = 10,
    VT_SHRINK_AXIS_MASK = 12
  };
  int32_t begin_mask() const {
    return GetField<int32_t>(VT_BEGIN_MASK, 0);
 80a0fd6:	2200      	movs	r2, #0
 80a0fd8:	2104      	movs	r1, #4
 80a0fda:	f7ff fa7f 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t end_mask() const {
    return GetField<int32_t>(VT_END_MASK, 0);
 80a0fde:	2200      	movs	r2, #0
              op->builtin_options_as_StridedSliceOptions()) {
        params->begin_mask = schema_params->begin_mask();
 80a0fe0:	6028      	str	r0, [r5, #0]
 80a0fe2:	2106      	movs	r1, #6
 80a0fe4:	4620      	mov	r0, r4
 80a0fe6:	f7ff fa79 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t ellipsis_mask() const {
    return GetField<int32_t>(VT_ELLIPSIS_MASK, 0);
 80a0fea:	2200      	movs	r2, #0
        params->end_mask = schema_params->end_mask();
 80a0fec:	6068      	str	r0, [r5, #4]
 80a0fee:	2108      	movs	r1, #8
 80a0ff0:	4620      	mov	r0, r4
 80a0ff2:	f7ff fa73 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t new_axis_mask() const {
    return GetField<int32_t>(VT_NEW_AXIS_MASK, 0);
 80a0ff6:	2200      	movs	r2, #0
        params->ellipsis_mask = schema_params->ellipsis_mask();
 80a0ff8:	60a8      	str	r0, [r5, #8]
 80a0ffa:	210a      	movs	r1, #10
 80a0ffc:	4620      	mov	r0, r4
 80a0ffe:	f7ff fa6d 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t shrink_axis_mask() const {
    return GetField<int32_t>(VT_SHRINK_AXIS_MASK, 0);
 80a1002:	2200      	movs	r2, #0
        params->new_axis_mask = schema_params->new_axis_mask();
 80a1004:	60e8      	str	r0, [r5, #12]
 80a1006:	210c      	movs	r1, #12
 80a1008:	4620      	mov	r0, r4
 80a100a:	f7ff fa67 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->shrink_axis_mask = schema_params->shrink_axis_mask();
 80a100e:	6128      	str	r0, [r5, #16]
 80a1010:	e1a8      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a1012:	4628      	mov	r0, r5
 80a1014:	f7ff faa6 	bl	80a0564 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
 80a1018:	4605      	mov	r5, r0
  }
  const MaximumMinimumOptions *builtin_options_as_MaximumMinimumOptions() const {
    return builtin_options_type() == BuiltinOptions_MaximumMinimumOptions ? static_cast<const MaximumMinimumOptions *>(builtin_options()) : nullptr;
  }
  const ArgMaxOptions *builtin_options_as_ArgMaxOptions() const {
    return builtin_options_type() == BuiltinOptions_ArgMaxOptions ? static_cast<const ArgMaxOptions *>(builtin_options()) : nullptr;
 80a101a:	4620      	mov	r0, r4
 80a101c:	f7ff fa58 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a1020:	2828      	cmp	r0, #40	; 0x28
 80a1022:	f040 819f 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a1026:	4620      	mov	r0, r4
 80a1028:	f7ff fa7f 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_ARG_MAX: {
      auto params = safe_allocator.Allocate<TfLiteArgMaxParams>();
      if (const auto* schema_params = op->builtin_options_as_ArgMaxOptions()) {
 80a102c:	2800      	cmp	r0, #0
 80a102e:	f000 8199 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef ArgMaxOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OUTPUT_TYPE = 4
  };
  TensorType output_type() const {
    return static_cast<TensorType>(GetField<int8_t>(VT_OUTPUT_TYPE, 0));
 80a1032:	2200      	movs	r2, #0
 80a1034:	2104      	movs	r1, #4
 80a1036:	f7ff fa5b 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        ConvertTensorType(schema_params->output_type(), &params->output_type,
                          error_reporter);
 80a103a:	463a      	mov	r2, r7
 80a103c:	4629      	mov	r1, r5
 80a103e:	b2c0      	uxtb	r0, r0
 80a1040:	f7ff fa0e 	bl	80a0460 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
 80a1044:	e18e      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a1046:	4628      	mov	r0, r5
 80a1048:	f7ff fa8c 	bl	80a0564 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
 80a104c:	4605      	mov	r5, r0
  }
  const PowOptions *builtin_options_as_PowOptions() const {
    return builtin_options_type() == BuiltinOptions_PowOptions ? static_cast<const PowOptions *>(builtin_options()) : nullptr;
  }
  const ArgMinOptions *builtin_options_as_ArgMinOptions() const {
    return builtin_options_type() == BuiltinOptions_ArgMinOptions ? static_cast<const ArgMinOptions *>(builtin_options()) : nullptr;
 80a104e:	4620      	mov	r0, r4
 80a1050:	f7ff fa3e 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a1054:	2839      	cmp	r0, #57	; 0x39
 80a1056:	f040 8185 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a105a:	4620      	mov	r0, r4
 80a105c:	f7ff fa65 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_ARG_MIN: {
      auto params = safe_allocator.Allocate<TfLiteArgMinParams>();
      if (const auto* schema_params = op->builtin_options_as_ArgMinOptions()) {
 80a1060:	2800      	cmp	r0, #0
 80a1062:	f000 817f 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef ArgMinOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OUTPUT_TYPE = 4
  };
  TensorType output_type() const {
    return static_cast<TensorType>(GetField<int8_t>(VT_OUTPUT_TYPE, 0));
 80a1066:	2200      	movs	r2, #0
 80a1068:	2104      	movs	r1, #4
 80a106a:	f7ff fa41 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        ConvertTensorType(schema_params->output_type(), &params->output_type,
                          error_reporter);
 80a106e:	463a      	mov	r2, r7
 80a1070:	4629      	mov	r1, r5
 80a1072:	b2c0      	uxtb	r0, r0
 80a1074:	f7ff f9f4 	bl	80a0460 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
 80a1078:	e174      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a107a:	4628      	mov	r0, r5
 80a107c:	f7ff fa66 	bl	80a054c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI25TfLiteTransposeConvParamsEEPT_v>
 80a1080:	4605      	mov	r5, r0
  }
  const SliceOptions *builtin_options_as_SliceOptions() const {
    return builtin_options_type() == BuiltinOptions_SliceOptions ? static_cast<const SliceOptions *>(builtin_options()) : nullptr;
  }
  const TransposeConvOptions *builtin_options_as_TransposeConvOptions() const {
    return builtin_options_type() == BuiltinOptions_TransposeConvOptions ? static_cast<const TransposeConvOptions *>(builtin_options()) : nullptr;
 80a1082:	4620      	mov	r0, r4
 80a1084:	f7ff fa24 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a1088:	2831      	cmp	r0, #49	; 0x31
 80a108a:	f040 816b 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a108e:	4620      	mov	r0, r4
 80a1090:	f7ff fa4b 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_TRANSPOSE_CONV: {
      auto params = safe_allocator.Allocate<TfLiteTransposeConvParams>();
      if (const auto* transpose_conv_params =
 80a1094:	4604      	mov	r4, r0
 80a1096:	2800      	cmp	r0, #0
 80a1098:	f000 8164 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
    VT_PADDING = 4,
    VT_STRIDE_W = 6,
    VT_STRIDE_H = 8
  };
  Padding padding() const {
    return static_cast<Padding>(GetField<int8_t>(VT_PADDING, 0));
 80a109c:	2200      	movs	r2, #0
 80a109e:	2104      	movs	r1, #4
 80a10a0:	f7ff fa26 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_TransposeConvOptions()) {
        params->padding = parse_padding(transpose_conv_params->padding());
 80a10a4:	b2c0      	uxtb	r0, r0
 80a10a6:	f7ff f993 	bl	80a03d0 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_7PaddingEE_clESA_.isra.0>
  }
  int32_t stride_w() const {
    return GetField<int32_t>(VT_STRIDE_W, 0);
 80a10aa:	2200      	movs	r2, #0
 80a10ac:	7028      	strb	r0, [r5, #0]
 80a10ae:	2106      	movs	r1, #6
 80a10b0:	4620      	mov	r0, r4
 80a10b2:	f7ff fa13 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t stride_h() const {
    return GetField<int32_t>(VT_STRIDE_H, 0);
 80a10b6:	2200      	movs	r2, #0
        params->stride_width = transpose_conv_params->stride_w();
 80a10b8:	6068      	str	r0, [r5, #4]
 80a10ba:	2108      	movs	r1, #8
 80a10bc:	4620      	mov	r0, r4
 80a10be:	f7ff fa0d 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->stride_height = transpose_conv_params->stride_h();
 80a10c2:	60a8      	str	r0, [r5, #8]
 80a10c4:	e14e      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a10c6:	4628      	mov	r0, r5
 80a10c8:	f7ff fa4c 	bl	80a0564 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
 80a10cc:	4605      	mov	r5, r0
  }
  const TransposeConvOptions *builtin_options_as_TransposeConvOptions() const {
    return builtin_options_type() == BuiltinOptions_TransposeConvOptions ? static_cast<const TransposeConvOptions *>(builtin_options()) : nullptr;
  }
  const SparseToDenseOptions *builtin_options_as_SparseToDenseOptions() const {
    return builtin_options_type() == BuiltinOptions_SparseToDenseOptions ? static_cast<const SparseToDenseOptions *>(builtin_options()) : nullptr;
 80a10ce:	4620      	mov	r0, r4
 80a10d0:	f7ff f9fe 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a10d4:	2832      	cmp	r0, #50	; 0x32
 80a10d6:	f040 8145 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a10da:	4620      	mov	r0, r4
 80a10dc:	f7ff fa25 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SPARSE_TO_DENSE: {
      auto params = safe_allocator.Allocate<TfLiteSparseToDenseParams>();
      if (const auto* sparse_to_dense_params =
 80a10e0:	2800      	cmp	r0, #0
 80a10e2:	f000 813f 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef SparseToDenseOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_VALIDATE_INDICES = 4
  };
  bool validate_indices() const {
    return GetField<uint8_t>(VT_VALIDATE_INDICES, 0) != 0;
 80a10e6:	2200      	movs	r2, #0
 80a10e8:	2104      	movs	r1, #4
 80a10ea:	f7ff f9e7 	bl	80a04bc <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
              op->builtin_options_as_SparseToDenseOptions()) {
        params->validate_indices = sparse_to_dense_params->validate_indices();
 80a10ee:	3000      	adds	r0, #0
 80a10f0:	bf18      	it	ne
 80a10f2:	2001      	movne	r0, #1
 80a10f4:	7028      	strb	r0, [r5, #0]
 80a10f6:	e135      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a10f8:	4628      	mov	r0, r5
 80a10fa:	f7ff fa33 	bl	80a0564 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
 80a10fe:	4605      	mov	r5, r0
  }
  const NotEqualOptions *builtin_options_as_NotEqualOptions() const {
    return builtin_options_type() == BuiltinOptions_NotEqualOptions ? static_cast<const NotEqualOptions *>(builtin_options()) : nullptr;
  }
  const ShapeOptions *builtin_options_as_ShapeOptions() const {
    return builtin_options_type() == BuiltinOptions_ShapeOptions ? static_cast<const ShapeOptions *>(builtin_options()) : nullptr;
 80a1100:	4620      	mov	r0, r4
 80a1102:	f7ff f9e5 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a1106:	2837      	cmp	r0, #55	; 0x37
 80a1108:	f040 812c 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a110c:	4620      	mov	r0, r4
 80a110e:	f7ff fa0c 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SHAPE: {
      auto params = safe_allocator.Allocate<TfLiteShapeParams>();
      if (const auto* schema_params = op->builtin_options_as_ShapeOptions()) {
 80a1112:	2800      	cmp	r0, #0
 80a1114:	f000 8126 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef ShapeOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OUT_TYPE = 4
  };
  TensorType out_type() const {
    return static_cast<TensorType>(GetField<int8_t>(VT_OUT_TYPE, 0));
 80a1118:	2200      	movs	r2, #0
 80a111a:	2104      	movs	r1, #4
 80a111c:	f7ff f9e8 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        ConvertTensorType(schema_params->out_type(), &params->out_type,
                          error_reporter);
 80a1120:	463a      	mov	r2, r7
 80a1122:	4629      	mov	r1, r5
 80a1124:	b2c0      	uxtb	r0, r0
 80a1126:	f7ff f99b 	bl	80a0460 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
 80a112a:	e11b      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a112c:	080b4971 	.word	0x080b4971
 80a1130:	080b4979 	.word	0x080b4979
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a1134:	4628      	mov	r0, r5
 80a1136:	f7ff fa19 	bl	80a056c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
 80a113a:	4605      	mov	r5, r0
  }
  const FakeQuantOptions *builtin_options_as_FakeQuantOptions() const {
    return builtin_options_type() == BuiltinOptions_FakeQuantOptions ? static_cast<const FakeQuantOptions *>(builtin_options()) : nullptr;
  }
  const PackOptions *builtin_options_as_PackOptions() const {
    return builtin_options_type() == BuiltinOptions_PackOptions ? static_cast<const PackOptions *>(builtin_options()) : nullptr;
 80a113c:	4620      	mov	r0, r4
 80a113e:	f7ff f9c7 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a1142:	283b      	cmp	r0, #59	; 0x3b
 80a1144:	f040 810e 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a1148:	4620      	mov	r0, r4
 80a114a:	f7ff f9ee 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = static_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_PACK: {
      auto params = safe_allocator.Allocate<TfLitePackParams>();
      if (const auto* pack_params = op->builtin_options_as_PackOptions()) {
 80a114e:	4604      	mov	r4, r0
 80a1150:	2800      	cmp	r0, #0
 80a1152:	f000 8107 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_VALUES_COUNT = 4,
    VT_AXIS = 6
  };
  int32_t values_count() const {
    return GetField<int32_t>(VT_VALUES_COUNT, 0);
 80a1156:	2200      	movs	r2, #0
 80a1158:	2104      	movs	r1, #4
 80a115a:	f7ff f9bf 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t axis() const {
    return GetField<int32_t>(VT_AXIS, 0);
 80a115e:	2200      	movs	r2, #0
        params->values_count = pack_params->values_count();
 80a1160:	6028      	str	r0, [r5, #0]
 80a1162:	2106      	movs	r1, #6
 80a1164:	4620      	mov	r0, r4
 80a1166:	f7ff f9b9 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->axis = pack_params->axis();
 80a116a:	6068      	str	r0, [r5, #4]
 80a116c:	e0fa      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_DELEGATE: {
      // TODO(ycling): Revisit when supporting saving delegated models.
      error_reporter->Report("DELEGATE op shouldn't exist in model.");
 80a116e:	4980      	ldr	r1, [pc, #512]	; (80a1370 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdfc>)
 80a1170:	4610      	mov	r0, r2
 80a1172:	f7ff f91b 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
      return kTfLiteError;
 80a1176:	2001      	movs	r0, #1
 80a1178:	e0f6      	b.n	80a1368 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf4>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a117a:	4628      	mov	r0, r5
 80a117c:	f7ff f9ea 	bl	80a0554 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteFakeQuantParamsEEPT_v>
 80a1180:	4605      	mov	r5, r0
  }
  const ArgMinOptions *builtin_options_as_ArgMinOptions() const {
    return builtin_options_type() == BuiltinOptions_ArgMinOptions ? static_cast<const ArgMinOptions *>(builtin_options()) : nullptr;
  }
  const FakeQuantOptions *builtin_options_as_FakeQuantOptions() const {
    return builtin_options_type() == BuiltinOptions_FakeQuantOptions ? static_cast<const FakeQuantOptions *>(builtin_options()) : nullptr;
 80a1182:	4620      	mov	r0, r4
 80a1184:	f7ff f9a4 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a1188:	283a      	cmp	r0, #58	; 0x3a
 80a118a:	f040 80eb 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a118e:	4620      	mov	r0, r4
 80a1190:	f7ff f9cb 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      error_reporter->Report("DELEGATE op shouldn't exist in model.");
      return kTfLiteError;
    }
    case BuiltinOperator_FAKE_QUANT: {
      auto params = safe_allocator.Allocate<TfLiteFakeQuantParams>();
      if (const auto* schema_params =
 80a1194:	4604      	mov	r4, r0
 80a1196:	2800      	cmp	r0, #0
 80a1198:	f000 80e4 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
    VT_MAX = 6,
    VT_NUM_BITS = 8,
    VT_NARROW_RANGE = 10
  };
  float min() const {
    return GetField<float>(VT_MIN, 0.0f);
 80a119c:	2200      	movs	r2, #0
 80a119e:	2104      	movs	r1, #4
 80a11a0:	f7ff f9b0 	bl	80a0504 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  float max() const {
    return GetField<float>(VT_MAX, 0.0f);
 80a11a4:	2200      	movs	r2, #0
              op->builtin_options_as_FakeQuantOptions()) {
        params->min = schema_params->min();
 80a11a6:	6028      	str	r0, [r5, #0]
 80a11a8:	2106      	movs	r1, #6
 80a11aa:	4620      	mov	r0, r4
 80a11ac:	f7ff f9aa 	bl	80a0504 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  int32_t num_bits() const {
    return GetField<int32_t>(VT_NUM_BITS, 0);
 80a11b0:	2200      	movs	r2, #0
        params->max = schema_params->max();
 80a11b2:	6068      	str	r0, [r5, #4]
 80a11b4:	2108      	movs	r1, #8
 80a11b6:	4620      	mov	r0, r4
 80a11b8:	f7ff f990 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  bool narrow_range() const {
    return GetField<uint8_t>(VT_NARROW_RANGE, 0) != 0;
 80a11bc:	2200      	movs	r2, #0
        params->num_bits = schema_params->num_bits();
 80a11be:	60a8      	str	r0, [r5, #8]
 80a11c0:	210a      	movs	r1, #10
 80a11c2:	4620      	mov	r0, r4
 80a11c4:	f7ff f97a 	bl	80a04bc <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->narrow_range = schema_params->narrow_range();
 80a11c8:	3000      	adds	r0, #0
 80a11ca:	bf18      	it	ne
 80a11cc:	2001      	movne	r0, #1
 80a11ce:	7328      	strb	r0, [r5, #12]
 80a11d0:	e0c8      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a11d2:	4628      	mov	r0, r5
 80a11d4:	f7ff f9c2 	bl	80a055c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
 80a11d8:	4605      	mov	r5, r0
  }
  const LogicalOrOptions *builtin_options_as_LogicalOrOptions() const {
    return builtin_options_type() == BuiltinOptions_LogicalOrOptions ? static_cast<const LogicalOrOptions *>(builtin_options()) : nullptr;
  }
  const OneHotOptions *builtin_options_as_OneHotOptions() const {
    return builtin_options_type() == BuiltinOptions_OneHotOptions ? static_cast<const OneHotOptions *>(builtin_options()) : nullptr;
 80a11da:	4620      	mov	r0, r4
 80a11dc:	f7ff f978 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a11e0:	283d      	cmp	r0, #61	; 0x3d
 80a11e2:	f040 80bf 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a11e6:	4620      	mov	r0, r4
 80a11e8:	f7ff f99f 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = static_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_ONE_HOT: {
      auto params = safe_allocator.Allocate<TfLiteOneHotParams>();
      if (const auto* schema_params = op->builtin_options_as_OneHotOptions()) {
 80a11ec:	2800      	cmp	r0, #0
 80a11ee:	f000 80b9 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef OneHotOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXIS = 4
  };
  int32_t axis() const {
    return GetField<int32_t>(VT_AXIS, 0);
 80a11f2:	2200      	movs	r2, #0
 80a11f4:	2104      	movs	r1, #4
 80a11f6:	f7ff f971 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->axis = schema_params->axis();
 80a11fa:	6028      	str	r0, [r5, #0]
 80a11fc:	e0b2      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a11fe:	4628      	mov	r0, r5
 80a1200:	f7ff f9b4 	bl	80a056c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
 80a1204:	4605      	mov	r5, r0
  }
  const LogicalNotOptions *builtin_options_as_LogicalNotOptions() const {
    return builtin_options_type() == BuiltinOptions_LogicalNotOptions ? static_cast<const LogicalNotOptions *>(builtin_options()) : nullptr;
  }
  const UnpackOptions *builtin_options_as_UnpackOptions() const {
    return builtin_options_type() == BuiltinOptions_UnpackOptions ? static_cast<const UnpackOptions *>(builtin_options()) : nullptr;
 80a1206:	4620      	mov	r0, r4
 80a1208:	f7ff f962 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a120c:	2840      	cmp	r0, #64	; 0x40
 80a120e:	f040 80a9 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a1212:	4620      	mov	r0, r4
 80a1214:	f7ff f989 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = static_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_UNPACK: {
      auto params = safe_allocator.Allocate<TfLiteUnpackParams>();
      if (const auto* unpack_params = op->builtin_options_as_UnpackOptions()) {
 80a1218:	4604      	mov	r4, r0
 80a121a:	2800      	cmp	r0, #0
 80a121c:	f000 80a2 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_NUM = 4,
    VT_AXIS = 6
  };
  int32_t num() const {
    return GetField<int32_t>(VT_NUM, 0);
 80a1220:	2200      	movs	r2, #0
 80a1222:	2104      	movs	r1, #4
 80a1224:	f7ff f95a 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t axis() const {
    return GetField<int32_t>(VT_AXIS, 0);
 80a1228:	2200      	movs	r2, #0
        params->num = unpack_params->num();
 80a122a:	6028      	str	r0, [r5, #0]
 80a122c:	2106      	movs	r1, #6
 80a122e:	4620      	mov	r0, r4
 80a1230:	f7ff f954 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->axis = unpack_params->axis();
 80a1234:	6068      	str	r0, [r5, #4]
 80a1236:	e095      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a1238:	4628      	mov	r0, r5
 80a123a:	f7ff f98f 	bl	80a055c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
 80a123e:	4605      	mov	r5, r0
  }
  const ResizeNearestNeighborOptions *builtin_options_as_ResizeNearestNeighborOptions() const {
    return builtin_options_type() == BuiltinOptions_ResizeNearestNeighborOptions ? static_cast<const ResizeNearestNeighborOptions *>(builtin_options()) : nullptr;
  }
  const LeakyReluOptions *builtin_options_as_LeakyReluOptions() const {
    return builtin_options_type() == BuiltinOptions_LeakyReluOptions ? static_cast<const LeakyReluOptions *>(builtin_options()) : nullptr;
 80a1240:	4620      	mov	r0, r4
 80a1242:	f7ff f945 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a1246:	284b      	cmp	r0, #75	; 0x4b
 80a1248:	f040 808c 	bne.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a124c:	4620      	mov	r0, r4
 80a124e:	f7ff f96c 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_LEAKY_RELU: {
      auto params = safe_allocator.Allocate<TfLiteLeakyReluParams>();
      if (const auto* leaky_relu_params =
 80a1252:	2800      	cmp	r0, #0
 80a1254:	f000 8086 	beq.w	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef LeakyReluOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ALPHA = 4
  };
  float alpha() const {
    return GetField<float>(VT_ALPHA, 0.0f);
 80a1258:	2200      	movs	r2, #0
 80a125a:	2104      	movs	r1, #4
 80a125c:	f7ff f952 	bl	80a0504 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
              op->builtin_options_as_LeakyReluOptions()) {
        params->alpha = leaky_relu_params->alpha();
 80a1260:	6028      	str	r0, [r5, #0]
 80a1262:	e07f      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a1264:	4628      	mov	r0, r5
 80a1266:	f7ff f97d 	bl	80a0564 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
 80a126a:	4605      	mov	r5, r0
  }
  const SquaredDifferenceOptions *builtin_options_as_SquaredDifferenceOptions() const {
    return builtin_options_type() == BuiltinOptions_SquaredDifferenceOptions ? static_cast<const SquaredDifferenceOptions *>(builtin_options()) : nullptr;
  }
  const MirrorPadOptions *builtin_options_as_MirrorPadOptions() const {
    return builtin_options_type() == BuiltinOptions_MirrorPadOptions ? static_cast<const MirrorPadOptions *>(builtin_options()) : nullptr;
 80a126c:	4620      	mov	r0, r4
 80a126e:	f7ff f92f 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a1272:	284d      	cmp	r0, #77	; 0x4d
 80a1274:	d176      	bne.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a1276:	4620      	mov	r0, r4
 80a1278:	f7ff f957 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_MIRROR_PAD: {
      auto params = safe_allocator.Allocate<TfLiteMirrorPaddingParams>();
      const auto* mirror_pad_params = op->builtin_options_as_MirrorPadOptions();
      if (mirror_pad_params != nullptr) {
 80a127c:	2800      	cmp	r0, #0
 80a127e:	d071      	beq.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef MirrorPadOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_MODE = 4
  };
  MirrorPadMode mode() const {
    return static_cast<MirrorPadMode>(GetField<int8_t>(VT_MODE, 0));
 80a1280:	2200      	movs	r2, #0
 80a1282:	2104      	movs	r1, #4
 80a1284:	f7ff f934 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->mode =
            mirror_pad_params->mode() == tflite::MirrorPadMode_REFLECT
                ? TfLiteMirrorPaddingMode::kTfLiteMirrorPaddingReflect
                : TfLiteMirrorPaddingMode::kTfLiteMirrorPaddingSymmetric;
 80a1288:	b2c0      	uxtb	r0, r0
 80a128a:	2800      	cmp	r0, #0
 80a128c:	bf0c      	ite	eq
 80a128e:	2301      	moveq	r3, #1
 80a1290:	2302      	movne	r3, #2
 80a1292:	702b      	strb	r3, [r5, #0]
 80a1294:	e066      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a1296:	4628      	mov	r0, r5
 80a1298:	f7ff f964 	bl	80a0564 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
 80a129c:	4605      	mov	r5, r0
  }
  const SplitVOptions *builtin_options_as_SplitVOptions() const {
    return builtin_options_type() == BuiltinOptions_SplitVOptions ? static_cast<const SplitVOptions *>(builtin_options()) : nullptr;
  }
  const UniqueOptions *builtin_options_as_UniqueOptions() const {
    return builtin_options_type() == BuiltinOptions_UniqueOptions ? static_cast<const UniqueOptions *>(builtin_options()) : nullptr;
 80a129e:	4620      	mov	r0, r4
 80a12a0:	f7ff f916 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a12a4:	2850      	cmp	r0, #80	; 0x50
 80a12a6:	d15d      	bne.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a12a8:	4620      	mov	r0, r4
 80a12aa:	f7ff f93e 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_UNIQUE: {
      auto params = safe_allocator.Allocate<TfLiteUniqueParams>();
      const auto* unique_params = op->builtin_options_as_UniqueOptions();
      if (unique_params != nullptr) {
 80a12ae:	2800      	cmp	r0, #0
 80a12b0:	d058      	beq.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  typedef UniqueOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_IDX_OUT_TYPE = 4
  };
  TensorType idx_out_type() const {
    return static_cast<TensorType>(GetField<int8_t>(VT_IDX_OUT_TYPE, 2));
 80a12b2:	2202      	movs	r2, #2
 80a12b4:	2104      	movs	r1, #4
 80a12b6:	f7ff f91b 	bl	80a04f0 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->index_out_type =
            unique_params->idx_out_type() == tflite::TensorType_INT64
                ? TfLiteType::kTfLiteInt64
                : TfLiteType::kTfLiteInt32;
 80a12ba:	b2c0      	uxtb	r0, r0
 80a12bc:	2804      	cmp	r0, #4
 80a12be:	bf0c      	ite	eq
 80a12c0:	2304      	moveq	r3, #4
 80a12c2:	2302      	movne	r3, #2
 80a12c4:	702b      	strb	r3, [r5, #0]
 80a12c6:	e04d      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
 80a12c8:	4628      	mov	r0, r5
 80a12ca:	f7ff f94f 	bl	80a056c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
 80a12ce:	4605      	mov	r5, r0
  }
  const RankOptions *builtin_options_as_RankOptions() const {
    return builtin_options_type() == BuiltinOptions_RankOptions ? static_cast<const RankOptions *>(builtin_options()) : nullptr;
  }
  const ReverseSequenceOptions *builtin_options_as_ReverseSequenceOptions() const {
    return builtin_options_type() == BuiltinOptions_ReverseSequenceOptions ? static_cast<const ReverseSequenceOptions *>(builtin_options()) : nullptr;
 80a12d0:	4620      	mov	r0, r4
 80a12d2:	f7ff f8fd 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a12d6:	2857      	cmp	r0, #87	; 0x57
 80a12d8:	d144      	bne.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a12da:	4620      	mov	r0, r4
 80a12dc:	f7ff f925 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_REVERSE_SEQUENCE: {
      auto params = safe_allocator.Allocate<TfLiteReverseSequenceParams>();
      if (const auto* reverse_seq_params =
 80a12e0:	4604      	mov	r4, r0
 80a12e2:	2800      	cmp	r0, #0
 80a12e4:	d03e      	beq.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SEQ_DIM = 4,
    VT_BATCH_DIM = 6
  };
  int32_t seq_dim() const {
    return GetField<int32_t>(VT_SEQ_DIM, 0);
 80a12e6:	2200      	movs	r2, #0
 80a12e8:	2104      	movs	r1, #4
 80a12ea:	f7ff f8f7 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t batch_dim() const {
    return GetField<int32_t>(VT_BATCH_DIM, 0);
 80a12ee:	2200      	movs	r2, #0
              op->builtin_options_as_ReverseSequenceOptions()) {
        params->seq_dim = reverse_seq_params->seq_dim();
 80a12f0:	6028      	str	r0, [r5, #0]
 80a12f2:	2106      	movs	r1, #6
 80a12f4:	4620      	mov	r0, r4
 80a12f6:	f7ff f8f1 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->batch_dim = reverse_seq_params->batch_dim();
 80a12fa:	6068      	str	r0, [r5, #4]
 80a12fc:	e032      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      }
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_IF: {
      TfLiteIfParams* params = allocator->AllocatePOD<TfLiteIfParams>();
 80a12fe:	4628      	mov	r0, r5
 80a1300:	f7ff f934 	bl	80a056c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
 80a1304:	4605      	mov	r5, r0
  }
  const HardSwishOptions *builtin_options_as_HardSwishOptions() const {
    return builtin_options_type() == BuiltinOptions_HardSwishOptions ? static_cast<const HardSwishOptions *>(builtin_options()) : nullptr;
  }
  const IfOptions *builtin_options_as_IfOptions() const {
    return builtin_options_type() == BuiltinOptions_IfOptions ? static_cast<const IfOptions *>(builtin_options()) : nullptr;
 80a1306:	4620      	mov	r0, r4
 80a1308:	f7ff f8e2 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a130c:	285c      	cmp	r0, #92	; 0x5c
 80a130e:	d129      	bne.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a1310:	4620      	mov	r0, r4
 80a1312:	f7ff f90a 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      if (const auto* if_params = op->builtin_options_as_IfOptions()) {
 80a1316:	4604      	mov	r4, r0
 80a1318:	b320      	cbz	r0, 80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_THEN_SUBGRAPH_INDEX = 4,
    VT_ELSE_SUBGRAPH_INDEX = 6
  };
  int32_t then_subgraph_index() const {
    return GetField<int32_t>(VT_THEN_SUBGRAPH_INDEX, 0);
 80a131a:	2200      	movs	r2, #0
 80a131c:	2104      	movs	r1, #4
 80a131e:	f7ff f8dd 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t else_subgraph_index() const {
    return GetField<int32_t>(VT_ELSE_SUBGRAPH_INDEX, 0);
 80a1322:	2200      	movs	r2, #0
        params->then_subgraph_index = if_params->then_subgraph_index();
 80a1324:	6028      	str	r0, [r5, #0]
 80a1326:	2106      	movs	r1, #6
 80a1328:	4620      	mov	r0, r4
 80a132a:	f7ff f8d7 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->else_subgraph_index = if_params->else_subgraph_index();
 80a132e:	6068      	str	r0, [r5, #4]
 80a1330:	e018      	b.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
      }
      *builtin_data = reinterpret_cast<void*>(params);
      break;
    }
    case BuiltinOperator_WHILE: {
      TfLiteWhileParams* params = allocator->AllocatePOD<TfLiteWhileParams>();
 80a1332:	4628      	mov	r0, r5
 80a1334:	f7ff f91a 	bl	80a056c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
 80a1338:	4605      	mov	r5, r0
  }
  const IfOptions *builtin_options_as_IfOptions() const {
    return builtin_options_type() == BuiltinOptions_IfOptions ? static_cast<const IfOptions *>(builtin_options()) : nullptr;
  }
  const WhileOptions *builtin_options_as_WhileOptions() const {
    return builtin_options_type() == BuiltinOptions_WhileOptions ? static_cast<const WhileOptions *>(builtin_options()) : nullptr;
 80a133a:	4620      	mov	r0, r4
 80a133c:	f7ff f8c8 	bl	80a04d0 <_ZNK6tflite8Operator20builtin_options_typeEv>
 80a1340:	285d      	cmp	r0, #93	; 0x5d
 80a1342:	d10f      	bne.n	80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
 80a1344:	4620      	mov	r0, r4
 80a1346:	f7ff f8f0 	bl	80a052a <_ZNK6tflite8Operator15builtin_optionsEv>
      if (const auto* while_params = op->builtin_options_as_WhileOptions()) {
 80a134a:	4604      	mov	r4, r0
 80a134c:	b150      	cbz	r0, 80a1364 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xdf0>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_COND_SUBGRAPH_INDEX = 4,
    VT_BODY_SUBGRAPH_INDEX = 6
  };
  int32_t cond_subgraph_index() const {
    return GetField<int32_t>(VT_COND_SUBGRAPH_INDEX, 0);
 80a134e:	2200      	movs	r2, #0
 80a1350:	2104      	movs	r1, #4
 80a1352:	f7ff f8c3 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t body_subgraph_index() const {
    return GetField<int32_t>(VT_BODY_SUBGRAPH_INDEX, 0);
 80a1356:	2200      	movs	r2, #0
        params->cond_subgraph_index = while_params->cond_subgraph_index();
 80a1358:	6028      	str	r0, [r5, #0]
 80a135a:	2106      	movs	r1, #6
 80a135c:	4620      	mov	r0, r4
 80a135e:	f7ff f8bd 	bl	80a04dc <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->body_subgraph_index = while_params->body_subgraph_index();
 80a1362:	6068      	str	r0, [r5, #4]
      }
      *builtin_data = reinterpret_cast<void*>(params);
 80a1364:	6035      	str	r5, [r6, #0]
    case BuiltinOperator_QUANTIZE:
    case BuiltinOperator_NON_MAX_SUPPRESSION_V4:
    case BuiltinOperator_NON_MAX_SUPPRESSION_V5:
      break;
  }
  return kTfLiteOk;
 80a1366:	2000      	movs	r0, #0
}  // NOLINT[readability/fn_size]
 80a1368:	b002      	add	sp, #8
 80a136a:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80a136e:	bf00      	nop
 80a1370:	080b4981 	.word	0x080b4981

080a1374 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration>:

namespace tflite {

TfLiteStatus GetRegistrationFromOpCode(
    const OperatorCode* opcode, const OpResolver& op_resolver,
    ErrorReporter* error_reporter, const TfLiteRegistration** registration) {
 80a1374:	e92d 43f8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, lr}
 80a1378:	461f      	mov	r7, r3
  TfLiteStatus status = kTfLiteOk;
  *registration = nullptr;
 80a137a:	2300      	movs	r3, #0

namespace tflite {

TfLiteStatus GetRegistrationFromOpCode(
    const OperatorCode* opcode, const OpResolver& op_resolver,
    ErrorReporter* error_reporter, const TfLiteRegistration** registration) {
 80a137c:	460e      	mov	r6, r1
  TfLiteStatus status = kTfLiteOk;
  *registration = nullptr;
 80a137e:	603b      	str	r3, [r7, #0]
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
 80a1380:	2104      	movs	r1, #4

namespace tflite {

TfLiteStatus GetRegistrationFromOpCode(
    const OperatorCode* opcode, const OpResolver& op_resolver,
    ErrorReporter* error_reporter, const TfLiteRegistration** registration) {
 80a1382:	4605      	mov	r5, r0
 80a1384:	4690      	mov	r8, r2
 80a1386:	f7ff f863 	bl	80a0450 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
 80a138a:	b100      	cbz	r0, 80a138e <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x1a>
 80a138c:	5628      	ldrsb	r0, [r5, r0]
    VT_BUILTIN_CODE = 4,
    VT_CUSTOM_CODE = 6,
    VT_VERSION = 8
  };
  BuiltinOperator builtin_code() const {
    return static_cast<BuiltinOperator>(GetField<int8_t>(VT_BUILTIN_CODE, 0));
 80a138e:	b2c4      	uxtb	r4, r0
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
 80a1390:	2108      	movs	r1, #8
 80a1392:	4628      	mov	r0, r5
 80a1394:	f7ff f85c 	bl	80a0450 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
 80a1398:	b110      	cbz	r0, 80a13a0 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x2c>
 80a139a:	f855 9000 	ldr.w	r9, [r5, r0]
 80a139e:	e001      	b.n	80a13a4 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x30>
 80a13a0:	f04f 0901 	mov.w	r9, #1
  TfLiteStatus status = kTfLiteOk;
  *registration = nullptr;
  auto builtin_code = opcode->builtin_code();
  int version = opcode->version();

  if (builtin_code > BuiltinOperator_MAX ||
 80a13a4:	2c79      	cmp	r4, #121	; 0x79
 80a13a6:	d905      	bls.n	80a13b4 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x40>
      builtin_code < BuiltinOperator_MIN) {
    error_reporter->Report(
        "Op builtin_code out of range: %d. Are you using old TFLite binary "
        "with newer model?",
        builtin_code);
 80a13a8:	4622      	mov	r2, r4
 80a13aa:	491b      	ldr	r1, [pc, #108]	; (80a1418 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0xa4>)
 80a13ac:	4640      	mov	r0, r8
 80a13ae:	f7fe fffd 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
 80a13b2:	e01f      	b.n	80a13f4 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x80>
    status = kTfLiteError;
  } else if (builtin_code != BuiltinOperator_CUSTOM) {
 80a13b4:	2c20      	cmp	r4, #32
 80a13b6:	d010      	beq.n	80a13da <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x66>
    *registration = op_resolver.FindOp(builtin_code, version);
 80a13b8:	6833      	ldr	r3, [r6, #0]
 80a13ba:	464a      	mov	r2, r9
 80a13bc:	681b      	ldr	r3, [r3, #0]
 80a13be:	4621      	mov	r1, r4
 80a13c0:	4630      	mov	r0, r6
 80a13c2:	4798      	blx	r3
 80a13c4:	6038      	str	r0, [r7, #0]
    if (*registration == nullptr) {
 80a13c6:	bb20      	cbnz	r0, 80a1412 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x9e>
      error_reporter->Report(
          "Didn't find op for builtin opcode '%s' version '%d'\n",
          EnumNameBuiltinOperator(builtin_code), version);
 80a13c8:	4a14      	ldr	r2, [pc, #80]	; (80a141c <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0xa8>)
 80a13ca:	464b      	mov	r3, r9
 80a13cc:	f852 2024 	ldr.w	r2, [r2, r4, lsl #2]
 80a13d0:	4913      	ldr	r1, [pc, #76]	; (80a1420 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0xac>)
 80a13d2:	4640      	mov	r0, r8
 80a13d4:	f7fe ffea 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
 80a13d8:	e00c      	b.n	80a13f4 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x80>
  }

  template<typename P> P GetPointer(voffset_t field) {
    auto field_offset = GetOptionalFieldOffset(field);
 80a13da:	2106      	movs	r1, #6
 80a13dc:	4628      	mov	r0, r5
 80a13de:	f7ff f837 	bl	80a0450 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    auto p = data_ + field_offset;
 80a13e2:	182a      	adds	r2, r5, r0
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
 80a13e4:	b110      	cbz	r0, 80a13ec <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x78>
 80a13e6:	5829      	ldr	r1, [r5, r0]
      status = kTfLiteError;
    }
  } else if (!opcode->custom_code()) {
 80a13e8:	1851      	adds	r1, r2, r1
 80a13ea:	d106      	bne.n	80a13fa <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x86>
    error_reporter->Report(
        "Operator with CUSTOM builtin_code has no custom_code.\n");
 80a13ec:	490d      	ldr	r1, [pc, #52]	; (80a1424 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0xb0>)
 80a13ee:	4640      	mov	r0, r8
 80a13f0:	f7fe ffdc 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
    status = kTfLiteError;
 80a13f4:	2001      	movs	r0, #1
 80a13f6:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
  } else {
    const char* name = opcode->custom_code()->c_str();
    *registration = op_resolver.FindOp(name, version);
 80a13fa:	6833      	ldr	r3, [r6, #0]
 80a13fc:	464a      	mov	r2, r9
 80a13fe:	685b      	ldr	r3, [r3, #4]
 80a1400:	3104      	adds	r1, #4
 80a1402:	4630      	mov	r0, r6
 80a1404:	4798      	blx	r3
 80a1406:	6038      	str	r0, [r7, #0]
      builtin_code < BuiltinOperator_MIN) {
    error_reporter->Report(
        "Op builtin_code out of range: %d. Are you using old TFLite binary "
        "with newer model?",
        builtin_code);
    status = kTfLiteError;
 80a1408:	fab0 f080 	clz	r0, r0
 80a140c:	0940      	lsrs	r0, r0, #5
 80a140e:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
namespace tflite {

TfLiteStatus GetRegistrationFromOpCode(
    const OperatorCode* opcode, const OpResolver& op_resolver,
    ErrorReporter* error_reporter, const TfLiteRegistration** registration) {
  TfLiteStatus status = kTfLiteOk;
 80a1412:	2000      	movs	r0, #0
      // while preparing ops.
      status = kTfLiteError;
    }
  }
  return status;
}
 80a1414:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
 80a1418:	080b4b98 	.word	0x080b4b98
 80a141c:	080b49ac 	.word	0x080b49ac
 80a1420:	080b4bec 	.word	0x080b4bec
 80a1424:	080b4c21 	.word	0x080b4c21

080a1428 <_ZN6tflite19ResetVariableTensorEP12TfLiteTensor>:

#include <string.h>

namespace tflite {

TfLiteStatus ResetVariableTensor(TfLiteTensor* tensor) {
 80a1428:	b530      	push	{r4, r5, lr}
  if (!tensor->is_variable) {
 80a142a:	f890 302d 	ldrb.w	r3, [r0, #45]	; 0x2d
 80a142e:	b16b      	cbz	r3, 80a144c <_ZN6tflite19ResetVariableTensorEP12TfLiteTensor+0x24>
    return kTfLiteOk;
  }
  // TODO(b/115961645): Implement - If a variable tensor has a buffer, reset it
  // to the value of the buffer.
  int value = 0;
  if (tensor->type == kTfLiteInt8) {
 80a1430:	7803      	ldrb	r3, [r0, #0]
#if __ANDROID__ || defined(__x86_64__) || defined(__i386__) || \
    defined(__i386) || defined(__x86__) || defined(__X86__) || \
    defined(_X86_) || defined(_M_IX86) || defined(_M_X64)
  memset(tensor->data.raw, value, tensor->bytes);
#else
  char* raw_ptr = tensor->data.raw;
 80a1432:	6844      	ldr	r4, [r0, #4]
    return kTfLiteOk;
  }
  // TODO(b/115961645): Implement - If a variable tensor has a buffer, reset it
  // to the value of the buffer.
  int value = 0;
  if (tensor->type == kTfLiteInt8) {
 80a1434:	2b09      	cmp	r3, #9
#if __ANDROID__ || defined(__x86_64__) || defined(__i386__) || \
    defined(__i386) || defined(__x86__) || defined(__X86__) || \
    defined(_X86_) || defined(_M_IX86) || defined(_M_X64)
  memset(tensor->data.raw, value, tensor->bytes);
#else
  char* raw_ptr = tensor->data.raw;
 80a1436:	4623      	mov	r3, r4
  }
  // TODO(b/115961645): Implement - If a variable tensor has a buffer, reset it
  // to the value of the buffer.
  int value = 0;
  if (tensor->type == kTfLiteInt8) {
    value = tensor->params.zero_point;
 80a1438:	bf0c      	ite	eq
 80a143a:	6901      	ldreq	r1, [r0, #16]
  if (!tensor->is_variable) {
    return kTfLiteOk;
  }
  // TODO(b/115961645): Implement - If a variable tensor has a buffer, reset it
  // to the value of the buffer.
  int value = 0;
 80a143c:	2100      	movne	r1, #0
    defined(__i386) || defined(__x86__) || defined(__X86__) || \
    defined(_X86_) || defined(_M_IX86) || defined(_M_X64)
  memset(tensor->data.raw, value, tensor->bytes);
#else
  char* raw_ptr = tensor->data.raw;
  for (int i = 0; i < tensor->bytes; ++i) {
 80a143e:	6985      	ldr	r5, [r0, #24]
 80a1440:	1b1a      	subs	r2, r3, r4
 80a1442:	4295      	cmp	r5, r2
 80a1444:	d902      	bls.n	80a144c <_ZN6tflite19ResetVariableTensorEP12TfLiteTensor+0x24>
    *raw_ptr = value;
 80a1446:	f803 1b01 	strb.w	r1, [r3], #1
    defined(__i386) || defined(__x86__) || defined(__X86__) || \
    defined(_X86_) || defined(_M_IX86) || defined(_M_X64)
  memset(tensor->data.raw, value, tensor->bytes);
#else
  char* raw_ptr = tensor->data.raw;
  for (int i = 0; i < tensor->bytes; ++i) {
 80a144a:	e7f8      	b.n	80a143e <_ZN6tflite19ResetVariableTensorEP12TfLiteTensor+0x16>
    *raw_ptr = value;
    raw_ptr++;
  }
#endif
  return kTfLiteOk;
}
 80a144c:	2000      	movs	r0, #0
 80a144e:	bd30      	pop	{r4, r5, pc}

080a1450 <_ZN12_GLOBAL__N_19StrCatStrEPciPKc>:
  return start;
}

// Appends a string to a string, in-place. You need to pass in the maximum
// string length as the second argument.
char* StrCatStr(char* main, int main_max_length, const char* to_append) {
 80a1450:	b530      	push	{r4, r5, lr}
 80a1452:	4604      	mov	r4, r0
 80a1454:	4623      	mov	r3, r4
  char* current = main;
  while (*current != 0) {
 80a1456:	781d      	ldrb	r5, [r3, #0]
 80a1458:	3401      	adds	r4, #1
 80a145a:	2d00      	cmp	r5, #0
 80a145c:	d1fa      	bne.n	80a1454 <_ZN12_GLOBAL__N_19StrCatStrEPciPKc+0x4>
    ++current;
  }
  char* current_end = main + (main_max_length - 1);
 80a145e:	3901      	subs	r1, #1
 80a1460:	4408      	add	r0, r1
 80a1462:	3a01      	subs	r2, #1
  while ((*to_append != 0) && (current < current_end)) {
 80a1464:	f812 1f01 	ldrb.w	r1, [r2, #1]!
 80a1468:	b121      	cbz	r1, 80a1474 <_ZN12_GLOBAL__N_19StrCatStrEPciPKc+0x24>
 80a146a:	4283      	cmp	r3, r0
 80a146c:	d202      	bcs.n	80a1474 <_ZN12_GLOBAL__N_19StrCatStrEPciPKc+0x24>
    *current = *to_append;
 80a146e:	f803 1b01 	strb.w	r1, [r3], #1
  char* current = main;
  while (*current != 0) {
    ++current;
  }
  char* current_end = main + (main_max_length - 1);
  while ((*to_append != 0) && (current < current_end)) {
 80a1472:	e7f7      	b.n	80a1464 <_ZN12_GLOBAL__N_19StrCatStrEPciPKc+0x14>
    *current = *to_append;
    ++current;
    ++to_append;
  }
  *current = 0;
 80a1474:	2200      	movs	r2, #0
 80a1476:	701a      	strb	r2, [r3, #0]
  return current;
}
 80a1478:	4618      	mov	r0, r3
 80a147a:	bd30      	pop	{r4, r5, pc}

080a147c <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci>:

// Populates the provided buffer with an ASCII representation of the number.
char* FastUInt32ToBufferLeft(uint32_t i, char* buffer, int base) {
 80a147c:	b530      	push	{r4, r5, lr}
 80a147e:	4603      	mov	r3, r0
 80a1480:	460c      	mov	r4, r1
  char* start = buffer;
  do {
    int32_t digit = i % base;
 80a1482:	fbb3 f5f2 	udiv	r5, r3, r2
    if (digit < 10) {
      character = '0' + digit;
    } else {
      character = 'a' + (digit - 10);
    }
    *buffer++ = character;
 80a1486:	4620      	mov	r0, r4

// Populates the provided buffer with an ASCII representation of the number.
char* FastUInt32ToBufferLeft(uint32_t i, char* buffer, int base) {
  char* start = buffer;
  do {
    int32_t digit = i % base;
 80a1488:	fb02 3315 	mls	r3, r2, r5, r3
    char character;
    if (digit < 10) {
 80a148c:	2b09      	cmp	r3, #9
      character = '0' + digit;
 80a148e:	bfd4      	ite	le
 80a1490:	3330      	addle	r3, #48	; 0x30
    } else {
      character = 'a' + (digit - 10);
 80a1492:	3357      	addgt	r3, #87	; 0x57
 80a1494:	b2db      	uxtb	r3, r3
    }
    *buffer++ = character;
 80a1496:	f800 3b01 	strb.w	r3, [r0], #1
    i /= base;
 80a149a:	462b      	mov	r3, r5
  } while (i > 0);
 80a149c:	b10d      	cbz	r5, 80a14a2 <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci+0x26>
 80a149e:	4604      	mov	r4, r0
 80a14a0:	e7ef      	b.n	80a1482 <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci+0x6>
  *buffer = 0;
 80a14a2:	7065      	strb	r5, [r4, #1]

// Reverses a zero-terminated string in-place.
char* ReverseStringInPlace(char* start, char* end) {
  char* p1 = start;
  char* p2 = end - 1;
  while (p1 < p2) {
 80a14a4:	42a1      	cmp	r1, r4
 80a14a6:	d206      	bcs.n	80a14b6 <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci+0x3a>
    char tmp = *p1;
 80a14a8:	780b      	ldrb	r3, [r1, #0]
    *p1++ = *p2;
 80a14aa:	7822      	ldrb	r2, [r4, #0]
 80a14ac:	f801 2b01 	strb.w	r2, [r1], #1
    *p2-- = tmp;
 80a14b0:	f804 3901 	strb.w	r3, [r4], #-1
 80a14b4:	e7f6      	b.n	80a14a4 <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci+0x28>
    i /= base;
  } while (i > 0);
  *buffer = 0;
  ReverseStringInPlace(start, buffer);
  return buffer;
}
 80a14b6:	bd30      	pop	{r4, r5, pc}

080a14b8 <DebugLogInt32>:
  return current;
}

}  // namespace

extern "C" void DebugLogInt32(int32_t i) {
 80a14b8:	b500      	push	{lr}
}

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
 80a14ba:	2800      	cmp	r0, #0
  return current;
}

}  // namespace

extern "C" void DebugLogInt32(int32_t i) {
 80a14bc:	b08d      	sub	sp, #52	; 0x34

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
    *buffer++ = '-';
 80a14be:	bfbd      	ittte	lt
 80a14c0:	232d      	movlt	r3, #45	; 0x2d
    u = -u;
 80a14c2:	4240      	neglt	r0, r0

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
    *buffer++ = '-';
 80a14c4:	f10d 0101 	addlt.w	r1, sp, #1
}

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
 80a14c8:	4669      	movge	r1, sp
    *buffer++ = '-';
    u = -u;
  }
  return FastUInt32ToBufferLeft(u, buffer, 10);
 80a14ca:	f04f 020a 	mov.w	r2, #10

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
    *buffer++ = '-';
 80a14ce:	bfb8      	it	lt
 80a14d0:	f88d 3000 	strblt.w	r3, [sp]
    u = -u;
  }
  return FastUInt32ToBufferLeft(u, buffer, 10);
 80a14d4:	f7ff ffd2 	bl	80a147c <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci>
}  // namespace

extern "C" void DebugLogInt32(int32_t i) {
  char number_string[kFastToBufferSize];
  FastInt32ToBufferLeft(i, number_string);
  DebugLog(number_string);
 80a14d8:	4668      	mov	r0, sp
 80a14da:	f000 ff4d 	bl	80a2378 <DebugLog>
}
 80a14de:	b00d      	add	sp, #52	; 0x34
 80a14e0:	f85d fb04 	ldr.w	pc, [sp], #4

080a14e4 <DebugLogFloat>:
  char number_string[kFastToBufferSize];
  FastUInt32ToBufferLeft(i, number_string, 16);
  DebugLog(number_string);
}

extern "C" void DebugLogFloat(float i) {
 80a14e4:	b5f0      	push	{r4, r5, r6, r7, lr}
  const uint32_t u = *reinterpret_cast<uint32_t*>(&f);
  const int32_t exponent =
      ((u & exponent_mask) >> exponent_shift) - exponent_bias;
  const uint32_t fraction = (u & fraction_mask);
  // Expect ~0x2B1B9D3 for fraction.
  if (u & sign_mask) {
 80a14e6:	2800      	cmp	r0, #0
  const int32_t exponent_shift = 23;
  const int32_t exponent_bias = 127;
  const uint32_t fraction_mask = 0x007fffff;
  const uint32_t u = *reinterpret_cast<uint32_t*>(&f);
  const int32_t exponent =
      ((u & exponent_mask) >> exponent_shift) - exponent_bias;
 80a14e8:	f3c0 54c7 	ubfx	r4, r0, #23, #8
  const uint32_t fraction = (u & fraction_mask);
  // Expect ~0x2B1B9D3 for fraction.
  if (u & sign_mask) {
    *current = '-';
 80a14ec:	bfb8      	it	lt
 80a14ee:	232d      	movlt	r3, #45	; 0x2d
  char number_string[kFastToBufferSize];
  FastUInt32ToBufferLeft(i, number_string, 16);
  DebugLog(number_string);
}

extern "C" void DebugLogFloat(float i) {
 80a14f0:	b09d      	sub	sp, #116	; 0x74
  const int32_t exponent_shift = 23;
  const int32_t exponent_bias = 127;
  const uint32_t fraction_mask = 0x007fffff;
  const uint32_t u = *reinterpret_cast<uint32_t*>(&f);
  const int32_t exponent =
      ((u & exponent_mask) >> exponent_shift) - exponent_bias;
 80a14f2:	f1a4 047f 	sub.w	r4, r4, #127	; 0x7f
  const uint32_t fraction = (u & fraction_mask);
  // Expect ~0x2B1B9D3 for fraction.
  if (u & sign_mask) {
    *current = '-';
 80a14f6:	bfb6      	itet	lt
 80a14f8:	f88d 3010 	strblt.w	r3, [sp, #16]
// Populates the provided buffer with ASCII representation of the float number.
// Avoids the use of any floating point instructions (since these aren't
// supported on many microcontrollers) and as a consequence prints values with
// power-of-two exponents.
char* FastFloatToBufferLeft(float f, char* buffer) {
  char* current = buffer;
 80a14fc:	ab04      	addge	r3, sp, #16
      ((u & exponent_mask) >> exponent_shift) - exponent_bias;
  const uint32_t fraction = (u & fraction_mask);
  // Expect ~0x2B1B9D3 for fraction.
  if (u & sign_mask) {
    *current = '-';
    current += 1;
 80a14fe:	f10d 0311 	addlt.w	r3, sp, #17
  }
  *current = 0;
 80a1502:	2200      	movs	r2, #0
  // These are special cases for infinities and not-a-numbers.
  if (exponent == 128) {
 80a1504:	2c80      	cmp	r4, #128	; 0x80
  const int32_t exponent_bias = 127;
  const uint32_t fraction_mask = 0x007fffff;
  const uint32_t u = *reinterpret_cast<uint32_t*>(&f);
  const int32_t exponent =
      ((u & exponent_mask) >> exponent_shift) - exponent_bias;
  const uint32_t fraction = (u & fraction_mask);
 80a1506:	f3c0 0716 	ubfx	r7, r0, #0, #23
  // Expect ~0x2B1B9D3 for fraction.
  if (u & sign_mask) {
    *current = '-';
    current += 1;
  }
  *current = 0;
 80a150a:	701a      	strb	r2, [r3, #0]
  // These are special cases for infinities and not-a-numbers.
  if (exponent == 128) {
 80a150c:	d108      	bne.n	80a1520 <DebugLogFloat+0x3c>
 80a150e:	f10d 013f 	add.w	r1, sp, #63	; 0x3f
    if (fraction == 0) {
 80a1512:	b90f      	cbnz	r7, 80a1518 <DebugLogFloat+0x34>
      current = StrCatStr(current, (current_end - current), "Inf");
 80a1514:	4a2a      	ldr	r2, [pc, #168]	; (80a15c0 <DebugLogFloat+0xdc>)
 80a1516:	e000      	b.n	80a151a <DebugLogFloat+0x36>
      return current;
    } else {
      current = StrCatStr(current, (current_end - current), "NaN");
 80a1518:	4a2a      	ldr	r2, [pc, #168]	; (80a15c4 <DebugLogFloat+0xe0>)
 80a151a:	1ac9      	subs	r1, r1, r3
 80a151c:	4618      	mov	r0, r3
 80a151e:	e047      	b.n	80a15b0 <DebugLogFloat+0xcc>
  // We can approximate this using multiply-adds and right-shifts using the
  // values in this array. The 1. portion of the number string is printed out
  // in a fixed way before the fraction, below.
  const int32_t scale_shifts_size = 13;
  const int8_t scale_shifts[13] = {3,  4,  8,  11, 13, 14, 17,
                                   18, 19, 20, 21, 22, 23};
 80a1520:	466d      	mov	r5, sp
 80a1522:	46ee      	mov	lr, sp
 80a1524:	4a28      	ldr	r2, [pc, #160]	; (80a15c8 <DebugLogFloat+0xe4>)
 80a1526:	f102 0c08 	add.w	ip, r2, #8
 80a152a:	462e      	mov	r6, r5
 80a152c:	6810      	ldr	r0, [r2, #0]
 80a152e:	6851      	ldr	r1, [r2, #4]
 80a1530:	3208      	adds	r2, #8
 80a1532:	c603      	stmia	r6!, {r0, r1}
 80a1534:	4562      	cmp	r2, ip
 80a1536:	4635      	mov	r5, r6
 80a1538:	d1f7      	bne.n	80a152a <DebugLogFloat+0x46>
 80a153a:	6810      	ldr	r0, [r2, #0]
 80a153c:	7912      	ldrb	r2, [r2, #4]
 80a153e:	6030      	str	r0, [r6, #0]
  uint32_t scaled_fraction = fraction;
 80a1540:	4638      	mov	r0, r7
  // We can approximate this using multiply-adds and right-shifts using the
  // values in this array. The 1. portion of the number string is printed out
  // in a fixed way before the fraction, below.
  const int32_t scale_shifts_size = 13;
  const int8_t scale_shifts[13] = {3,  4,  8,  11, 13, 14, 17,
                                   18, 19, 20, 21, 22, 23};
 80a1542:	7132      	strb	r2, [r6, #4]
  uint32_t scaled_fraction = fraction;
  for (int i = 0; i < scale_shifts_size; ++i) {
 80a1544:	2200      	movs	r2, #0
    scaled_fraction += (fraction >> scale_shifts[i]);
 80a1546:	f91e 1002 	ldrsb.w	r1, [lr, r2]
  // in a fixed way before the fraction, below.
  const int32_t scale_shifts_size = 13;
  const int8_t scale_shifts[13] = {3,  4,  8,  11, 13, 14, 17,
                                   18, 19, 20, 21, 22, 23};
  uint32_t scaled_fraction = fraction;
  for (int i = 0; i < scale_shifts_size; ++i) {
 80a154a:	3201      	adds	r2, #1
    scaled_fraction += (fraction >> scale_shifts[i]);
 80a154c:	fa27 f101 	lsr.w	r1, r7, r1
  // in a fixed way before the fraction, below.
  const int32_t scale_shifts_size = 13;
  const int8_t scale_shifts[13] = {3,  4,  8,  11, 13, 14, 17,
                                   18, 19, 20, 21, 22, 23};
  uint32_t scaled_fraction = fraction;
  for (int i = 0; i < scale_shifts_size; ++i) {
 80a1550:	2a0d      	cmp	r2, #13
    scaled_fraction += (fraction >> scale_shifts[i]);
 80a1552:	4408      	add	r0, r1
  // in a fixed way before the fraction, below.
  const int32_t scale_shifts_size = 13;
  const int8_t scale_shifts[13] = {3,  4,  8,  11, 13, 14, 17,
                                   18, 19, 20, 21, 22, 23};
  uint32_t scaled_fraction = fraction;
  for (int i = 0; i < scale_shifts_size; ++i) {
 80a1554:	d1f7      	bne.n	80a1546 <DebugLogFloat+0x62>
    scaled_fraction += (fraction >> scale_shifts[i]);
  }
  *current = '1';
 80a1556:	2231      	movs	r2, #49	; 0x31
 80a1558:	701a      	strb	r2, [r3, #0]
  current += 1;
  *current = '.';
 80a155a:	222e      	movs	r2, #46	; 0x2e
  current += 1;
 80a155c:	1c9e      	adds	r6, r3, #2
  for (int i = 0; i < scale_shifts_size; ++i) {
    scaled_fraction += (fraction >> scale_shifts[i]);
  }
  *current = '1';
  current += 1;
  *current = '.';
 80a155e:	705a      	strb	r2, [r3, #1]
  current += 1;
  *current = 0;
  current = StrCatUInt32(current, (current_end - current), scaled_fraction, 10);
 80a1560:	f10d 053f 	add.w	r5, sp, #63	; 0x3f
  }
  *current = '1';
  current += 1;
  *current = '.';
  current += 1;
  *current = 0;
 80a1564:	2200      	movs	r2, #0
 80a1566:	709a      	strb	r2, [r3, #2]
  current = StrCatUInt32(current, (current_end - current), scaled_fraction, 10);
 80a1568:	1baf      	subs	r7, r5, r6
}

// Converts a number to a string and appends it to another.
char* StrCatUInt32(char* main, int main_max_length, uint32_t number, int base) {
  char number_string[kFastToBufferSize];
  FastUInt32ToBufferLeft(number, number_string, base);
 80a156a:	220a      	movs	r2, #10
 80a156c:	a910      	add	r1, sp, #64	; 0x40
 80a156e:	f7ff ff85 	bl	80a147c <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci>
  return StrCatStr(main, main_max_length, number_string);
 80a1572:	aa10      	add	r2, sp, #64	; 0x40
 80a1574:	4639      	mov	r1, r7
 80a1576:	4630      	mov	r0, r6
 80a1578:	f7ff ff6a 	bl	80a1450 <_ZN12_GLOBAL__N_19StrCatStrEPciPKc>
  current += 1;
  *current = '.';
  current += 1;
  *current = 0;
  current = StrCatUInt32(current, (current_end - current), scaled_fraction, 10);
  current = StrCatStr(current, (current_end - current), "*2^");
 80a157c:	4a13      	ldr	r2, [pc, #76]	; (80a15cc <DebugLogFloat+0xe8>)
 80a157e:	1a29      	subs	r1, r5, r0
 80a1580:	f7ff ff66 	bl	80a1450 <_ZN12_GLOBAL__N_19StrCatStrEPciPKc>
 80a1584:	4606      	mov	r6, r0
}

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
 80a1586:	2c00      	cmp	r4, #0
    *buffer++ = '-';
    u = -u;
 80a1588:	bfb8      	it	lt
 80a158a:	4264      	neglt	r4, r4
  *current = '.';
  current += 1;
  *current = 0;
  current = StrCatUInt32(current, (current_end - current), scaled_fraction, 10);
  current = StrCatStr(current, (current_end - current), "*2^");
  current = StrCatInt32(current, (current_end - current), exponent);
 80a158c:	eba5 0500 	sub.w	r5, r5, r0

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
    *buffer++ = '-';
 80a1590:	bfba      	itte	lt
 80a1592:	232d      	movlt	r3, #45	; 0x2d
 80a1594:	f10d 0141 	addlt.w	r1, sp, #65	; 0x41
}

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
 80a1598:	a910      	addge	r1, sp, #64	; 0x40
    *buffer++ = '-';
    u = -u;
  }
  return FastUInt32ToBufferLeft(u, buffer, 10);
 80a159a:	f04f 020a 	mov.w	r2, #10
 80a159e:	4620      	mov	r0, r4

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
    *buffer++ = '-';
 80a15a0:	bfb8      	it	lt
 80a15a2:	f88d 3040 	strblt.w	r3, [sp, #64]	; 0x40
    u = -u;
  }
  return FastUInt32ToBufferLeft(u, buffer, 10);
 80a15a6:	f7ff ff69 	bl	80a147c <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci>

// Converts a number to a string and appends it to another.
char* StrCatInt32(char* main, int main_max_length, int32_t number) {
  char number_string[kFastToBufferSize];
  FastInt32ToBufferLeft(number, number_string);
  return StrCatStr(main, main_max_length, number_string);
 80a15aa:	4629      	mov	r1, r5
 80a15ac:	4630      	mov	r0, r6
 80a15ae:	aa10      	add	r2, sp, #64	; 0x40
 80a15b0:	f7ff ff4e 	bl	80a1450 <_ZN12_GLOBAL__N_19StrCatStrEPciPKc>
}

extern "C" void DebugLogFloat(float i) {
  char number_string[kFastToBufferSize];
  FastFloatToBufferLeft(i, number_string);
  DebugLog(number_string);
 80a15b4:	a804      	add	r0, sp, #16
 80a15b6:	f000 fedf 	bl	80a2378 <DebugLog>
}
 80a15ba:	b01d      	add	sp, #116	; 0x74
 80a15bc:	bdf0      	pop	{r4, r5, r6, r7, pc}
 80a15be:	bf00      	nop
 80a15c0:	080b5100 	.word	0x080b5100
 80a15c4:	080b5104 	.word	0x080b5104
 80a15c8:	080b50f3 	.word	0x080b50f3
 80a15cc:	080b5108 	.word	0x080b5108

080a15d0 <_ZN6tflite14AlignPointerUpEPhj>:

uint8_t* AlignPointerUp(uint8_t* data, size_t alignment) {
  size_t data_as_size_t = reinterpret_cast<size_t>(data);
  uint8_t* aligned_result = reinterpret_cast<uint8_t*>(
      ((data_as_size_t + (alignment - 1)) / alignment) * alignment);
  return aligned_result;
 80a15d0:	1e4b      	subs	r3, r1, #1
 80a15d2:	4418      	add	r0, r3
 80a15d4:	fbb0 f0f1 	udiv	r0, r0, r1
}
 80a15d8:	4348      	muls	r0, r1
 80a15da:	4770      	bx	lr

080a15dc <_ZN6tflite16AlignPointerDownEPhj>:

uint8_t* AlignPointerDown(uint8_t* data, size_t alignment) {
  size_t data_as_size_t = reinterpret_cast<size_t>(data);
  uint8_t* aligned_result =
      reinterpret_cast<uint8_t*>((data_as_size_t / alignment) * alignment);
  return aligned_result;
 80a15dc:	fbb0 f0f1 	udiv	r0, r0, r1
}
 80a15e0:	4348      	muls	r0, r1
 80a15e2:	4770      	bx	lr

080a15e4 <_ZN6tflite11AlignSizeUpEjj>:

size_t AlignSizeUp(size_t size, size_t alignment) {
  size_t aligned_size = (((size + (alignment - 1)) / alignment) * alignment);
  return aligned_size;
 80a15e4:	3801      	subs	r0, #1
 80a15e6:	4408      	add	r0, r1
 80a15e8:	fbb0 f0f1 	udiv	r0, r0, r1
}
 80a15ec:	4348      	muls	r0, r1
 80a15ee:	4770      	bx	lr

080a15f0 <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE>:

TfLiteStatus TfLiteTypeSizeOf(TfLiteType type, size_t* size,
                              ErrorReporter* reporter) {
 80a15f0:	b538      	push	{r3, r4, r5, lr}
  switch (type) {
 80a15f2:	1e43      	subs	r3, r0, #1
  size_t aligned_size = (((size + (alignment - 1)) / alignment) * alignment);
  return aligned_size;
}

TfLiteStatus TfLiteTypeSizeOf(TfLiteType type, size_t* size,
                              ErrorReporter* reporter) {
 80a15f4:	4604      	mov	r4, r0
 80a15f6:	4615      	mov	r5, r2
  switch (type) {
 80a15f8:	2b08      	cmp	r3, #8
 80a15fa:	d810      	bhi.n	80a161e <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE+0x2e>
 80a15fc:	e8df f003 	tbb	[pc, r3]
 80a1600:	0d0b0909 	.word	0x0d0b0909
 80a1604:	0d050b0f 	.word	0x0d050b0f
 80a1608:	0b          	.byte	0x0b
 80a1609:	00          	.byte	0x00
    case kTfLiteFloat32:
      *size = sizeof(float);
      break;
    case kTfLiteInt16:
      *size = sizeof(int16_t);
 80a160a:	2302      	movs	r3, #2
 80a160c:	600b      	str	r3, [r1, #0]
    default:
      reporter->Report("Type %s (%d) not is not supported",
                       TfLiteTypeGetName(type), type);
      return kTfLiteError;
  }
  return kTfLiteOk;
 80a160e:	2000      	movs	r0, #0
    case kTfLiteFloat32:
      *size = sizeof(float);
      break;
    case kTfLiteInt16:
      *size = sizeof(int16_t);
      break;
 80a1610:	bd38      	pop	{r3, r4, r5, pc}
    case kTfLiteInt32:
      *size = sizeof(int32_t);
 80a1612:	2304      	movs	r3, #4
 80a1614:	e7fa      	b.n	80a160c <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE+0x1c>
      break;
    case kTfLiteInt64:
      *size = sizeof(int64_t);
      break;
    case kTfLiteBool:
      *size = sizeof(bool);
 80a1616:	2301      	movs	r3, #1
 80a1618:	e7f8      	b.n	80a160c <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE+0x1c>
      break;
    case kTfLiteComplex64:
      *size = sizeof(float) * 2;
 80a161a:	2308      	movs	r3, #8
 80a161c:	e7f6      	b.n	80a160c <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE+0x1c>
      break;
    default:
      reporter->Report("Type %s (%d) not is not supported",
 80a161e:	f7fe fd7d 	bl	80a011c <TfLiteTypeGetName>
                       TfLiteTypeGetName(type), type);
 80a1622:	4623      	mov	r3, r4
 80a1624:	4602      	mov	r2, r0
 80a1626:	4903      	ldr	r1, [pc, #12]	; (80a1634 <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE+0x44>)
 80a1628:	4628      	mov	r0, r5
 80a162a:	f7fe febf 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
 80a162e:	2001      	movs	r0, #1
      return kTfLiteError;
  }
  return kTfLiteOk;
}
 80a1630:	bd38      	pop	{r3, r4, r5, pc}
 80a1632:	bf00      	nop
 80a1634:	080b510c 	.word	0x080b510c

080a1638 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE>:

TfLiteStatus BytesRequiredForTensor(const tflite::Tensor& flatbuffer_tensor,
                                    size_t* bytes, size_t* type_size,
                                    ErrorReporter* error_reporter) {
 80a1638:	e92d 41f3 	stmdb	sp!, {r0, r1, r4, r5, r6, r7, r8, lr}
 80a163c:	4616      	mov	r6, r2
 80a163e:	460d      	mov	r5, r1
// "tables" use an offset table (possibly shared) that allows fields to be
// omitted and added at will, but uses an extra indirection to read.
class Table {
 public:
  const uint8_t *GetVTable() const {
    return data_ - ReadScalar<soffset_t>(data_);
 80a1640:	6801      	ldr	r1, [r0, #0]
 80a1642:	461f      	mov	r7, r3
 80a1644:	1a41      	subs	r1, r0, r1
 80a1646:	f8b1 e000 	ldrh.w	lr, [r1]
  int element_count = 1;
  for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
 80a164a:	2300      	movs	r3, #0
}

TfLiteStatus BytesRequiredForTensor(const tflite::Tensor& flatbuffer_tensor,
                                    size_t* bytes, size_t* type_size,
                                    ErrorReporter* error_reporter) {
  int element_count = 1;
 80a164c:	2401      	movs	r4, #1
    auto vtable = GetVTable();
    // The first element is the size of the vtable (fields + type id + itself).
    auto vtsize = ReadScalar<voffset_t>(vtable);
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
 80a164e:	f1be 0f04 	cmp.w	lr, #4
 80a1652:	bf8c      	ite	hi
 80a1654:	888a      	ldrhhi	r2, [r1, #4]
 80a1656:	2200      	movls	r2, #0
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
  }

  template<typename P> P GetPointer(voffset_t field) {
    auto field_offset = GetOptionalFieldOffset(field);
    auto p = data_ + field_offset;
 80a1658:	eb00 0802 	add.w	r8, r0, r2
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
 80a165c:	b362      	cbz	r2, 80a16b8 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x80>
 80a165e:	f850 c002 	ldr.w	ip, [r0, r2]
 80a1662:	eb08 020c 	add.w	r2, r8, ip
  for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
 80a1666:	f858 c00c 	ldr.w	ip, [r8, ip]
 80a166a:	4563      	cmp	r3, ip
 80a166c:	d205      	bcs.n	80a167a <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x42>
template<typename T> struct IndirectHelper {
  typedef T return_type;
  typedef T mutable_return_type;
  static const size_t element_stride = sizeof(T);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    return EndianScalar((reinterpret_cast<const T *>(p))[i]);
 80a166e:	eb02 0283 	add.w	r2, r2, r3, lsl #2
    element_count *= flatbuffer_tensor.shape()->Get(n);
 80a1672:	6852      	ldr	r2, [r2, #4]

TfLiteStatus BytesRequiredForTensor(const tflite::Tensor& flatbuffer_tensor,
                                    size_t* bytes, size_t* type_size,
                                    ErrorReporter* error_reporter) {
  int element_count = 1;
  for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
 80a1674:	3301      	adds	r3, #1
    element_count *= flatbuffer_tensor.shape()->Get(n);
 80a1676:	4354      	muls	r4, r2

TfLiteStatus BytesRequiredForTensor(const tflite::Tensor& flatbuffer_tensor,
                                    size_t* bytes, size_t* type_size,
                                    ErrorReporter* error_reporter) {
  int element_count = 1;
  for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
 80a1678:	e7e9      	b.n	80a164e <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x16>
    auto vtable = GetVTable();
    // The first element is the size of the vtable (fields + type id + itself).
    auto vtsize = ReadScalar<voffset_t>(vtable);
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
 80a167a:	f1be 0f06 	cmp.w	lr, #6
 80a167e:	d903      	bls.n	80a1688 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x50>
 80a1680:	88ca      	ldrh	r2, [r1, #6]
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
 80a1682:	b11a      	cbz	r2, 80a168c <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x54>
 80a1684:	5680      	ldrsb	r0, [r0, r2]
 80a1686:	e002      	b.n	80a168e <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x56>
 80a1688:	2000      	movs	r0, #0
 80a168a:	e000      	b.n	80a168e <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x56>
 80a168c:	4610      	mov	r0, r2
    element_count *= flatbuffer_tensor.shape()->Get(n);
  }

  TfLiteType tf_lite_type;
  TF_LITE_ENSURE_STATUS(ConvertTensorType(flatbuffer_tensor.type(),
 80a168e:	463a      	mov	r2, r7
 80a1690:	f10d 0107 	add.w	r1, sp, #7
 80a1694:	b2c0      	uxtb	r0, r0
 80a1696:	f7fe fee3 	bl	80a0460 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
 80a169a:	b108      	cbz	r0, 80a16a0 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x68>
 80a169c:	2001      	movs	r0, #1
 80a169e:	e00d      	b.n	80a16bc <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x84>
                                          &tf_lite_type, error_reporter));
  TF_LITE_ENSURE_STATUS(
 80a16a0:	463a      	mov	r2, r7
 80a16a2:	4631      	mov	r1, r6
 80a16a4:	f89d 0007 	ldrb.w	r0, [sp, #7]
 80a16a8:	f7ff ffa2 	bl	80a15f0 <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE>
 80a16ac:	2800      	cmp	r0, #0
 80a16ae:	d1f5      	bne.n	80a169c <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x64>
      TfLiteTypeSizeOf(tf_lite_type, type_size, error_reporter));
  *bytes = element_count * (*type_size);
 80a16b0:	6833      	ldr	r3, [r6, #0]
 80a16b2:	435c      	muls	r4, r3
 80a16b4:	602c      	str	r4, [r5, #0]
  return kTfLiteOk;
 80a16b6:	e001      	b.n	80a16bc <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x84>
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
 80a16b8:	6813      	ldr	r3, [r2, #0]
 80a16ba:	deff      	udf	#255	; 0xff
}
 80a16bc:	b002      	add	sp, #8
 80a16be:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}

080a16c2 <_ZNK6tflite6Tensor11is_variableEv>:
    return GetPointer<const flatbuffers::String *>(VT_NAME);
  }
  const QuantizationParameters *quantization() const {
    return GetPointer<const QuantizationParameters *>(VT_QUANTIZATION);
  }
  bool is_variable() const {
 80a16c2:	b510      	push	{r4, lr}
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
 80a16c4:	210e      	movs	r1, #14
 80a16c6:	4604      	mov	r4, r0
 80a16c8:	f7fe fec2 	bl	80a0450 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
 80a16cc:	b100      	cbz	r0, 80a16d0 <_ZNK6tflite6Tensor11is_variableEv+0xe>
 80a16ce:	5c20      	ldrb	r0, [r4, r0]
    return GetField<uint8_t>(VT_IS_VARIABLE, 0) != 0;
  }
 80a16d0:	3000      	adds	r0, #0
 80a16d2:	bf18      	it	ne
 80a16d4:	2001      	movne	r0, #1
 80a16d6:	bd10      	pop	{r4, pc}

080a16d8 <_ZNK11flatbuffers6VectorIfE3GetEm>:
  uoffset_t Length() const { return size(); }

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
 80a16d8:	b508      	push	{r3, lr}
    FLATBUFFERS_ASSERT(i < size());
 80a16da:	6803      	ldr	r3, [r0, #0]
 80a16dc:	4299      	cmp	r1, r3
 80a16de:	d305      	bcc.n	80a16ec <_ZNK11flatbuffers6VectorIfE3GetEm+0x14>
 80a16e0:	4b04      	ldr	r3, [pc, #16]	; (80a16f4 <_ZNK11flatbuffers6VectorIfE3GetEm+0x1c>)
 80a16e2:	4a05      	ldr	r2, [pc, #20]	; (80a16f8 <_ZNK11flatbuffers6VectorIfE3GetEm+0x20>)
 80a16e4:	21ed      	movs	r1, #237	; 0xed
 80a16e6:	4805      	ldr	r0, [pc, #20]	; (80a16fc <_ZNK11flatbuffers6VectorIfE3GetEm+0x24>)
 80a16e8:	f00e fcb2 	bl	80b0050 <__assert_func>
template<typename T> struct IndirectHelper {
  typedef T return_type;
  typedef T mutable_return_type;
  static const size_t element_stride = sizeof(T);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    return EndianScalar((reinterpret_cast<const T *>(p))[i]);
 80a16ec:	eb00 0081 	add.w	r0, r0, r1, lsl #2
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
    return IndirectHelper<T>::Read(Data(), i);
  }
 80a16f0:	6840      	ldr	r0, [r0, #4]
 80a16f2:	bd08      	pop	{r3, pc}
 80a16f4:	080b4829 	.word	0x080b4829
 80a16f8:	080b54f8 	.word	0x080b54f8
 80a16fc:	080b4834 	.word	0x080b4834

080a1700 <_ZNK11flatbuffers6VectorIlE3GetEm>:
  uoffset_t Length() const { return size(); }

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
 80a1700:	b508      	push	{r3, lr}
    FLATBUFFERS_ASSERT(i < size());
 80a1702:	6803      	ldr	r3, [r0, #0]
 80a1704:	4299      	cmp	r1, r3
 80a1706:	d305      	bcc.n	80a1714 <_ZNK11flatbuffers6VectorIlE3GetEm+0x14>
 80a1708:	4b04      	ldr	r3, [pc, #16]	; (80a171c <_ZNK11flatbuffers6VectorIlE3GetEm+0x1c>)
 80a170a:	4a05      	ldr	r2, [pc, #20]	; (80a1720 <_ZNK11flatbuffers6VectorIlE3GetEm+0x20>)
 80a170c:	21ed      	movs	r1, #237	; 0xed
 80a170e:	4805      	ldr	r0, [pc, #20]	; (80a1724 <_ZNK11flatbuffers6VectorIlE3GetEm+0x24>)
 80a1710:	f00e fc9e 	bl	80b0050 <__assert_func>
template<typename T> struct IndirectHelper {
  typedef T return_type;
  typedef T mutable_return_type;
  static const size_t element_stride = sizeof(T);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    return EndianScalar((reinterpret_cast<const T *>(p))[i]);
 80a1714:	eb00 0081 	add.w	r0, r0, r1, lsl #2
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
    return IndirectHelper<T>::Read(Data(), i);
  }
 80a1718:	6840      	ldr	r0, [r0, #4]
 80a171a:	bd08      	pop	{r3, pc}
 80a171c:	080b4829 	.word	0x080b4829
 80a1720:	080b512e 	.word	0x080b512e
 80a1724:	080b4834 	.word	0x080b4834

080a1728 <_ZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEm>:
  uoffset_t Length() const { return size(); }

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
 80a1728:	b508      	push	{r3, lr}
    FLATBUFFERS_ASSERT(i < size());
 80a172a:	6803      	ldr	r3, [r0, #0]
 80a172c:	4299      	cmp	r1, r3
 80a172e:	d305      	bcc.n	80a173c <_ZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEm+0x14>
 80a1730:	4b06      	ldr	r3, [pc, #24]	; (80a174c <_ZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEm+0x24>)
 80a1732:	4a07      	ldr	r2, [pc, #28]	; (80a1750 <_ZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEm+0x28>)
 80a1734:	21ed      	movs	r1, #237	; 0xed
 80a1736:	4807      	ldr	r0, [pc, #28]	; (80a1754 <_ZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEm+0x2c>)
 80a1738:	f00e fc8a 	bl	80b0050 <__assert_func>
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
 80a173c:	3004      	adds	r0, #4
template<typename T> struct IndirectHelper<Offset<T>> {
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
 80a173e:	eb00 0281 	add.w	r2, r0, r1, lsl #2
  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
    return IndirectHelper<T>::Read(Data(), i);
 80a1742:	f850 0021 	ldr.w	r0, [r0, r1, lsl #2]
  }
 80a1746:	4410      	add	r0, r2
 80a1748:	bd08      	pop	{r3, pc}
 80a174a:	bf00      	nop
 80a174c:	080b4829 	.word	0x080b4829
 80a1750:	080b52f6 	.word	0x080b52f6
 80a1754:	080b4834 	.word	0x080b4834

080a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>:
  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
  }

  template<typename P> P GetPointer(voffset_t field) {
 80a1758:	b510      	push	{r4, lr}
 80a175a:	4604      	mov	r4, r0
    auto field_offset = GetOptionalFieldOffset(field);
 80a175c:	f7fe fe78 	bl	80a0450 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    auto p = data_ + field_offset;
 80a1760:	1822      	adds	r2, r4, r0
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
 80a1762:	b108      	cbz	r0, 80a1768 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t+0x10>
 80a1764:	5823      	ldr	r3, [r4, r0]
 80a1766:	18d0      	adds	r0, r2, r3
  }
 80a1768:	bd10      	pop	{r4, pc}
	...

080a176c <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE>:
// requirement for SIMD extensions.
constexpr int kBufferAlignment = 16;

}  // namespace

MicroAllocator::MicroAllocator(TfLiteContext* context, const Model* model,
 80a176c:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
 80a1770:	460f      	mov	r7, r1
 80a1772:	9e07      	ldr	r6, [sp, #28]
 80a1774:	9906      	ldr	r1, [sp, #24]
// though we have enough information about lifetimes of the tensors to do so.
// This makes it pretty wasteful, so we should use a more intelligent method.
class SimpleMemoryAllocator {
 public:
  SimpleMemoryAllocator(uint8_t* buffer, size_t buffer_size)
      : data_size_(0), data_size_max_(buffer_size), data_(buffer) {}
 80a1776:	2500      	movs	r5, #0
 80a1778:	6081      	str	r1, [r0, #8]
 80a177a:	60c3      	str	r3, [r0, #12]
    : model_(model),
      memory_allocator_(tensor_arena, arena_size),
      error_reporter_(error_reporter),
      context_(context),
      arena_(tensor_arena),
      arena_size_(arena_size) {
 80a177c:	6183      	str	r3, [r0, #24]
 80a177e:	61c1      	str	r1, [r0, #28]
 80a1780:	6002      	str	r2, [r0, #0]
 80a1782:	6045      	str	r5, [r0, #4]
 80a1784:	6106      	str	r6, [r0, #16]
 80a1786:	6147      	str	r7, [r0, #20]
// requirement for SIMD extensions.
constexpr int kBufferAlignment = 16;

}  // namespace

MicroAllocator::MicroAllocator(TfLiteContext* context, const Model* model,
 80a1788:	4604      	mov	r4, r0
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
 80a178a:	2108      	movs	r1, #8
 80a178c:	4610      	mov	r0, r2
 80a178e:	f7ff ffe3 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
      error_reporter_(error_reporter),
      context_(context),
      arena_(tensor_arena),
      arena_size_(arena_size) {
  auto* subgraphs = model->subgraphs();
  if (subgraphs->size() != 1) {
 80a1792:	6803      	ldr	r3, [r0, #0]
 80a1794:	2b01      	cmp	r3, #1
 80a1796:	d004      	beq.n	80a17a2 <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE+0x36>
    error_reporter->Report("Only 1 subgraph is currently supported.\n");
 80a1798:	4917      	ldr	r1, [pc, #92]	; (80a17f8 <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE+0x8c>)
 80a179a:	4630      	mov	r0, r6
 80a179c:	f7fe fe06 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return;
 80a17a0:	e026      	b.n	80a17f0 <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE+0x84>
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
 80a17a2:	6843      	ldr	r3, [r0, #4]
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
 80a17a4:	1d06      	adds	r6, r0, #4
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
 80a17a6:	441e      	add	r6, r3
  }
  subgraph_ = (*subgraphs)[0];
 80a17a8:	6226      	str	r6, [r4, #32]
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
 80a17aa:	2104      	movs	r1, #4
 80a17ac:	4630      	mov	r0, r6
 80a17ae:	f7ff ffd3 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
 80a17b2:	210a      	movs	r1, #10
 80a17b4:	4680      	mov	r8, r0
  tensors_ = subgraph_->tensors();
 80a17b6:	62a0      	str	r0, [r4, #40]	; 0x28
 80a17b8:	4630      	mov	r0, r6
 80a17ba:	f7ff ffcd 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
  operators_ = subgraph_->operators();
 80a17be:	6260      	str	r0, [r4, #36]	; 0x24
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
 80a17c0:	f8d8 3000 	ldr.w	r3, [r8]

  context_->tensors_size = tensors_->size();
  context_->tensors =
      reinterpret_cast<TfLiteTensor*>(memory_allocator_.AllocateFromTail(
 80a17c4:	2204      	movs	r2, #4
  }
  subgraph_ = (*subgraphs)[0];
  tensors_ = subgraph_->tensors();
  operators_ = subgraph_->operators();

  context_->tensors_size = tensors_->size();
 80a17c6:	603b      	str	r3, [r7, #0]
  context_->tensors =
      reinterpret_cast<TfLiteTensor*>(memory_allocator_.AllocateFromTail(
          sizeof(TfLiteTensor) * context_->tensors_size, 4));
 80a17c8:	6967      	ldr	r7, [r4, #20]
  tensors_ = subgraph_->tensors();
  operators_ = subgraph_->operators();

  context_->tensors_size = tensors_->size();
  context_->tensors =
      reinterpret_cast<TfLiteTensor*>(memory_allocator_.AllocateFromTail(
 80a17ca:	2638      	movs	r6, #56	; 0x38
 80a17cc:	6839      	ldr	r1, [r7, #0]
 80a17ce:	18a0      	adds	r0, r4, r2
 80a17d0:	4371      	muls	r1, r6
 80a17d2:	f000 fdbc 	bl	80a234e <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>
          sizeof(TfLiteTensor) * context_->tensors_size, 4));

  // Null all inputs so we can later perform a null check to avoid re-allocating
  // registered pre-allocated inputs.
  for (size_t i = 0; i < context_->tensors_size; ++i) {
 80a17d6:	462b      	mov	r3, r5
    context_->tensors[i].data.raw = nullptr;
 80a17d8:	4629      	mov	r1, r5
  operators_ = subgraph_->operators();

  context_->tensors_size = tensors_->size();
  context_->tensors =
      reinterpret_cast<TfLiteTensor*>(memory_allocator_.AllocateFromTail(
          sizeof(TfLiteTensor) * context_->tensors_size, 4));
 80a17da:	60b8      	str	r0, [r7, #8]

  // Null all inputs so we can later perform a null check to avoid re-allocating
  // registered pre-allocated inputs.
  for (size_t i = 0; i < context_->tensors_size; ++i) {
 80a17dc:	6962      	ldr	r2, [r4, #20]
 80a17de:	6810      	ldr	r0, [r2, #0]
 80a17e0:	4283      	cmp	r3, r0
 80a17e2:	d205      	bcs.n	80a17f0 <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE+0x84>
    context_->tensors[i].data.raw = nullptr;
 80a17e4:	6892      	ldr	r2, [r2, #8]
 80a17e6:	fb06 2203 	mla	r2, r6, r3, r2
      reinterpret_cast<TfLiteTensor*>(memory_allocator_.AllocateFromTail(
          sizeof(TfLiteTensor) * context_->tensors_size, 4));

  // Null all inputs so we can later perform a null check to avoid re-allocating
  // registered pre-allocated inputs.
  for (size_t i = 0; i < context_->tensors_size; ++i) {
 80a17ea:	3301      	adds	r3, #1
    context_->tensors[i].data.raw = nullptr;
 80a17ec:	6051      	str	r1, [r2, #4]
      reinterpret_cast<TfLiteTensor*>(memory_allocator_.AllocateFromTail(
          sizeof(TfLiteTensor) * context_->tensors_size, 4));

  // Null all inputs so we can later perform a null check to avoid re-allocating
  // registered pre-allocated inputs.
  for (size_t i = 0; i < context_->tensors_size; ++i) {
 80a17ee:	e7f5      	b.n	80a17dc <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE+0x70>
    context_->tensors[i].data.raw = nullptr;
  }
}
 80a17f0:	4620      	mov	r0, r4
 80a17f2:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80a17f6:	bf00      	nop
 80a17f8:	080b53ec 	.word	0x080b53ec

080a17fc <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh>:

TfLiteStatus MicroAllocator::InitializeRuntimeTensor(
    const tflite::Tensor& flatbuffer_tensor,
    const flatbuffers::Vector<flatbuffers::Offset<Buffer>>* buffers,
    ErrorReporter* error_reporter, TfLiteTensor* result,
    uint8_t* preallocated_buffer) {
 80a17fc:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a1800:	460d      	mov	r5, r1
 80a1802:	b087      	sub	sp, #28
 80a1804:	4607      	mov	r7, r0
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
 80a1806:	2106      	movs	r1, #6
 80a1808:	4628      	mov	r0, r5
 80a180a:	4616      	mov	r6, r2
 80a180c:	4698      	mov	r8, r3
 80a180e:	9c10      	ldr	r4, [sp, #64]	; 0x40
 80a1810:	f8dd 9044 	ldr.w	r9, [sp, #68]	; 0x44
 80a1814:	f7fe fe1c 	bl	80a0450 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
 80a1818:	b100      	cbz	r0, 80a181c <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x20>
 80a181a:	5628      	ldrsb	r0, [r5, r0]
  // Make sure the serialized type is one we know how to deal with, and convert
  // it from a flatbuffer enum into a constant used by the kernel C API.
  TF_LITE_ENSURE_STATUS(ConvertTensorType(flatbuffer_tensor.type(),
 80a181c:	4642      	mov	r2, r8
 80a181e:	4621      	mov	r1, r4
 80a1820:	b2c0      	uxtb	r0, r0
 80a1822:	f7fe fe1d 	bl	80a0460 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
 80a1826:	4682      	mov	sl, r0
 80a1828:	2800      	cmp	r0, #0
 80a182a:	f040 80df 	bne.w	80a19ec <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1f0>
                                          &result->type, error_reporter));
  // Make sure we remember if the serialized tensor is designated as a variable.
  result->is_variable = flatbuffer_tensor.is_variable();
 80a182e:	4628      	mov	r0, r5
 80a1830:	f7ff ff47 	bl	80a16c2 <_ZNK6tflite6Tensor11is_variableEv>
  // We need to figure out where the actual contents of this tensor are stored
  // in memory. We'll check to see if there's a serialized buffer (pretty much
  // the same as a constant op in TensorFlow) associated with this tensor first,
  // and if there is update the runtime structure to point to its location in
  // memory.
  result->data.raw = nullptr;
 80a1834:	f8c4 a004 	str.w	sl, [r4, #4]
  // Make sure the serialized type is one we know how to deal with, and convert
  // it from a flatbuffer enum into a constant used by the kernel C API.
  TF_LITE_ENSURE_STATUS(ConvertTensorType(flatbuffer_tensor.type(),
                                          &result->type, error_reporter));
  // Make sure we remember if the serialized tensor is designated as a variable.
  result->is_variable = flatbuffer_tensor.is_variable();
 80a1838:	f884 002d 	strb.w	r0, [r4, #45]	; 0x2d
  // in memory. We'll check to see if there's a serialized buffer (pretty much
  // the same as a constant op in TensorFlow) associated with this tensor first,
  // and if there is update the runtime structure to point to its location in
  // memory.
  result->data.raw = nullptr;
  result->bytes = 0;
 80a183c:	f8c4 a018 	str.w	sl, [r4, #24]
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
 80a1840:	2108      	movs	r1, #8
 80a1842:	4628      	mov	r0, r5
 80a1844:	f7fe fe04 	bl	80a0450 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
 80a1848:	b100      	cbz	r0, 80a184c <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x50>
 80a184a:	5828      	ldr	r0, [r5, r0]

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
 80a184c:	6833      	ldr	r3, [r6, #0]
 80a184e:	4283      	cmp	r3, r0
 80a1850:	d802      	bhi.n	80a1858 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x5c>
 80a1852:	4b74      	ldr	r3, [pc, #464]	; (80a1a24 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x228>)
 80a1854:	4a74      	ldr	r2, [pc, #464]	; (80a1a28 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x22c>)
 80a1856:	e0ab      	b.n	80a19b0 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1b4>
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
 80a1858:	3604      	adds	r6, #4
template<typename T> struct IndirectHelper<Offset<T>> {
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
 80a185a:	eb06 0380 	add.w	r3, r6, r0, lsl #2
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
 80a185e:	f856 0020 	ldr.w	r0, [r6, r0, lsl #2]
  // First see if there's any buffer information in the serialized tensor.
  if (auto* buffer = (*buffers)[flatbuffer_tensor.buffer()]) {
 80a1862:	1818      	adds	r0, r3, r0
 80a1864:	d009      	beq.n	80a187a <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x7e>
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
 80a1866:	2104      	movs	r1, #4
 80a1868:	f7ff ff76 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
    // If we've found a buffer, does it have any data?
    if (auto* array = buffer->data()) {
 80a186c:	b128      	cbz	r0, 80a187a <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x7e>
      // If it has any data, is the data size larger than zero?
      if (size_t array_size = array->size()) {
 80a186e:	6803      	ldr	r3, [r0, #0]
 80a1870:	b11b      	cbz	r3, 80a187a <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x7e>
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
 80a1872:	3004      	adds	r0, #4
        // We've found a buffer with valid data, so update the runtime tensor
        // data structure to point to it.
        result->data.raw =
            const_cast<char*>(reinterpret_cast<const char*>(array->data()));
        // We set the data from a serialized buffer, so record tha.
        result->allocation_type = kTfLiteMmapRo;
 80a1874:	2301      	movs	r3, #1
      // If it has any data, is the data size larger than zero?
      if (size_t array_size = array->size()) {
        // We've found a buffer with valid data, so update the runtime tensor
        // data structure to point to it.
        result->data.raw =
            const_cast<char*>(reinterpret_cast<const char*>(array->data()));
 80a1876:	6060      	str	r0, [r4, #4]
        // We set the data from a serialized buffer, so record tha.
        result->allocation_type = kTfLiteMmapRo;
 80a1878:	7523      	strb	r3, [r4, #20]
    // it less ambiguous.
  }

  // TODO(petewarden): Some of these paths aren't getting enough testing
  // coverage, so we should figure out some tests that exercise them.
  if (!result->data.raw) {
 80a187a:	6863      	ldr	r3, [r4, #4]
 80a187c:	b933      	cbnz	r3, 80a188c <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x90>
    // The tensor contents haven't been set from a serialized buffer, so
    // make a note that they will be allocated from memory. The actual
    // allocation won't happen until later.
    result->allocation_type = kTfLiteArenaRw;
 80a187e:	2302      	movs	r3, #2
 80a1880:	7523      	strb	r3, [r4, #20]
    if (preallocated_buffer != nullptr) {
 80a1882:	f1b9 0f00 	cmp.w	r9, #0
 80a1886:	d001      	beq.n	80a188c <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x90>
      // If the client is supplying memory for the contents of the tensor
      // themselves, use it.
      // TODO(petewarden): Should we store the fact this is a client-allocated
      // buffer?
      result->data.raw = reinterpret_cast<char*>(preallocated_buffer);
 80a1888:	f8c4 9004 	str.w	r9, [r4, #4]
    }
  }

  // Figure out what the size in bytes of the buffer is and store it.
  size_t type_size;
  TF_LITE_ENSURE_STATUS(BytesRequiredForTensor(
 80a188c:	4643      	mov	r3, r8
 80a188e:	aa05      	add	r2, sp, #20
 80a1890:	f104 0118 	add.w	r1, r4, #24
 80a1894:	4628      	mov	r0, r5
 80a1896:	f7ff fecf 	bl	80a1638 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE>
 80a189a:	4606      	mov	r6, r0
 80a189c:	2800      	cmp	r0, #0
 80a189e:	f040 80a5 	bne.w	80a19ec <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1f0>
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
 80a18a2:	2104      	movs	r1, #4
 80a18a4:	4628      	mov	r0, r5
 80a18a6:	f7ff ff57 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
      flatbuffer_tensor, &result->bytes, &type_size, error_reporter));
  // Copy the shape of the tensor from the serialized data into the runtime
  // form. We have to allocate memory for this.
  result->dims =
      reinterpret_cast<TfLiteIntArray*>(memory_allocator_.AllocateFromTail(
 80a18aa:	6801      	ldr	r1, [r0, #0]
 80a18ac:	f107 0b04 	add.w	fp, r7, #4
 80a18b0:	3101      	adds	r1, #1
 80a18b2:	2204      	movs	r2, #4
 80a18b4:	0089      	lsls	r1, r1, #2
 80a18b6:	4658      	mov	r0, fp
 80a18b8:	f000 fd49 	bl	80a234e <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>
 80a18bc:	2104      	movs	r1, #4
 80a18be:	4607      	mov	r7, r0
          sizeof(int) * (flatbuffer_tensor.shape()->Length() + 1),
          sizeof(int)));
 80a18c0:	60a0      	str	r0, [r4, #8]
 80a18c2:	4628      	mov	r0, r5
 80a18c4:	f7ff ff48 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
  result->dims->size = flatbuffer_tensor.shape()->Length();
 80a18c8:	6803      	ldr	r3, [r0, #0]
 80a18ca:	603b      	str	r3, [r7, #0]
  for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
 80a18cc:	4637      	mov	r7, r6
 80a18ce:	2104      	movs	r1, #4
 80a18d0:	4628      	mov	r0, r5
 80a18d2:	f7ff ff41 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
 80a18d6:	6803      	ldr	r3, [r0, #0]
 80a18d8:	42bb      	cmp	r3, r7
 80a18da:	d90a      	bls.n	80a18f2 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0xf6>
    result->dims->data[n] = flatbuffer_tensor.shape()->Get(n);
 80a18dc:	4639      	mov	r1, r7
 80a18de:	f7ff ff0f 	bl	80a1700 <_ZNK11flatbuffers6VectorIlE3GetEm>
 80a18e2:	f8d4 8008 	ldr.w	r8, [r4, #8]
 80a18e6:	eb08 0887 	add.w	r8, r8, r7, lsl #2
 80a18ea:	f8c8 0004 	str.w	r0, [r8, #4]
  result->dims =
      reinterpret_cast<TfLiteIntArray*>(memory_allocator_.AllocateFromTail(
          sizeof(int) * (flatbuffer_tensor.shape()->Length() + 1),
          sizeof(int)));
  result->dims->size = flatbuffer_tensor.shape()->Length();
  for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
 80a18ee:	3701      	adds	r7, #1
 80a18f0:	e7ed      	b.n	80a18ce <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0xd2>
 80a18f2:	210c      	movs	r1, #12
 80a18f4:	4628      	mov	r0, r5
 80a18f6:	f7ff ff2f 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
  }
  // Copy the quantization information from the serialized data.
  const auto* src_quantization = flatbuffer_tensor.quantization();
  if (src_quantization && src_quantization->scale() &&
      (src_quantization->scale()->size() > 0) &&
      src_quantization->zero_point() &&
 80a18fa:	4607      	mov	r7, r0
 80a18fc:	2800      	cmp	r0, #0
 80a18fe:	d065      	beq.n	80a19cc <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1d0>
 80a1900:	2108      	movs	r1, #8
 80a1902:	f7ff ff29 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
  for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
    result->dims->data[n] = flatbuffer_tensor.shape()->Get(n);
  }
  // Copy the quantization information from the serialized data.
  const auto* src_quantization = flatbuffer_tensor.quantization();
  if (src_quantization && src_quantization->scale() &&
 80a1906:	4680      	mov	r8, r0
 80a1908:	2800      	cmp	r0, #0
 80a190a:	d05f      	beq.n	80a19cc <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1d0>
 80a190c:	6803      	ldr	r3, [r0, #0]
 80a190e:	2b00      	cmp	r3, #0
 80a1910:	d05c      	beq.n	80a19cc <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1d0>
 80a1912:	210a      	movs	r1, #10
 80a1914:	4638      	mov	r0, r7
 80a1916:	f7ff ff1f 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
      (src_quantization->scale()->size() > 0) &&
 80a191a:	2800      	cmp	r0, #0
 80a191c:	d056      	beq.n	80a19cc <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1d0>
      src_quantization->zero_point() &&
 80a191e:	6803      	ldr	r3, [r0, #0]
 80a1920:	2b00      	cmp	r3, #0
 80a1922:	d053      	beq.n	80a19cc <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1d0>
      (src_quantization->zero_point()->size() > 0)) {
    result->params.scale = src_quantization->scale()->Get(0);
 80a1924:	4640      	mov	r0, r8
 80a1926:	2100      	movs	r1, #0
 80a1928:	f7ff fed6 	bl	80a16d8 <_ZNK11flatbuffers6VectorIfE3GetEm>
 80a192c:	f104 090f 	add.w	r9, r4, #15
 80a1930:	60e0      	str	r0, [r4, #12]
    // This magic handles issues with little-endianness.
    for (unsigned int b = 0; b < sizeof(int64_t); ++b)
 80a1932:	f04f 0800 	mov.w	r8, #0
 80a1936:	210a      	movs	r1, #10
 80a1938:	4638      	mov	r0, r7
 80a193a:	f7ff ff0d 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
      *(reinterpret_cast<char*>(&result->params.zero_point) + b) =
          *(reinterpret_cast<const char*>(
                src_quantization->zero_point()->Data()) +
            b);
 80a193e:	4440      	add	r0, r8
 80a1940:	7903      	ldrb	r3, [r0, #4]
      (src_quantization->scale()->size() > 0) &&
      src_quantization->zero_point() &&
      (src_quantization->zero_point()->size() > 0)) {
    result->params.scale = src_quantization->scale()->Get(0);
    // This magic handles issues with little-endianness.
    for (unsigned int b = 0; b < sizeof(int64_t); ++b)
 80a1942:	f108 0801 	add.w	r8, r8, #1
 80a1946:	f1b8 0f08 	cmp.w	r8, #8
      *(reinterpret_cast<char*>(&result->params.zero_point) + b) =
          *(reinterpret_cast<const char*>(
                src_quantization->zero_point()->Data()) +
            b);
 80a194a:	f809 3f01 	strb.w	r3, [r9, #1]!
      (src_quantization->scale()->size() > 0) &&
      src_quantization->zero_point() &&
      (src_quantization->zero_point()->size() > 0)) {
    result->params.scale = src_quantization->scale()->Get(0);
    // This magic handles issues with little-endianness.
    for (unsigned int b = 0; b < sizeof(int64_t); ++b)
 80a194e:	d1f2      	bne.n	80a1936 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x13a>
 80a1950:	4641      	mov	r1, r8
 80a1952:	4638      	mov	r0, r7
 80a1954:	f7ff ff00 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
    // Populate per-channel quantization params.
    int channels = src_quantization->scale()->size();
    TfLiteAffineQuantization* quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            memory_allocator_.AllocateFromTail(sizeof(TfLiteAffineQuantization),
                                               sizeof(int)));
 80a1958:	2204      	movs	r2, #4
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
 80a195a:	f8d0 8000 	ldr.w	r8, [r0]
 80a195e:	210c      	movs	r1, #12
 80a1960:	4658      	mov	r0, fp
 80a1962:	f000 fcf4 	bl	80a234e <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>
    int* zero_point_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
 80a1966:	ea4f 0388 	mov.w	r3, r8, lsl #2
 80a196a:	f103 0904 	add.w	r9, r3, #4
            channels * sizeof(int) + sizeof(int), sizeof(int)));
 80a196e:	4649      	mov	r1, r9
    // Populate per-channel quantization params.
    int channels = src_quantization->scale()->size();
    TfLiteAffineQuantization* quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            memory_allocator_.AllocateFromTail(sizeof(TfLiteAffineQuantization),
                                               sizeof(int)));
 80a1970:	9001      	str	r0, [sp, #4]
    int* zero_point_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(int) + sizeof(int), sizeof(int)));
 80a1972:	2204      	movs	r2, #4
 80a1974:	4658      	mov	r0, fp
 80a1976:	f000 fcea 	bl	80a234e <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>
    int* scale_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(float) + sizeof(int), sizeof(int)));
 80a197a:	4649      	mov	r1, r9
        reinterpret_cast<TfLiteAffineQuantization*>(
            memory_allocator_.AllocateFromTail(sizeof(TfLiteAffineQuantization),
                                               sizeof(int)));
    int* zero_point_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(int) + sizeof(int), sizeof(int)));
 80a197c:	4682      	mov	sl, r0
    int* scale_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(float) + sizeof(int), sizeof(int)));
 80a197e:	2204      	movs	r2, #4
 80a1980:	4658      	mov	r0, fp
 80a1982:	f000 fce4 	bl	80a234e <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>
 80a1986:	4683      	mov	fp, r0
    zero_point_array[0] = channels;
 80a1988:	f8ca 8000 	str.w	r8, [sl]
 80a198c:	f8cd a00c 	str.w	sl, [sp, #12]
    scale_array[0] = channels;
 80a1990:	f8c0 8000 	str.w	r8, [r0]
 80a1994:	9002      	str	r0, [sp, #8]
    int* zero_point_data = &zero_point_array[1];
    float* scale_data = reinterpret_cast<float*>(&scale_array[1]);
    for (int i = 0; i < channels; i++) {
 80a1996:	f04f 0900 	mov.w	r9, #0
 80a199a:	45c8      	cmp	r8, r9
 80a199c:	dd0c      	ble.n	80a19b8 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1bc>
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
 80a199e:	210a      	movs	r1, #10
 80a19a0:	4638      	mov	r0, r7
 80a19a2:	f7ff fed9 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
 80a19a6:	6801      	ldr	r1, [r0, #0]
 80a19a8:	4589      	cmp	r9, r1
 80a19aa:	d321      	bcc.n	80a19f0 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1f4>
 80a19ac:	4b1d      	ldr	r3, [pc, #116]	; (80a1a24 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x228>)
 80a19ae:	4a1f      	ldr	r2, [pc, #124]	; (80a1a2c <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x230>)
 80a19b0:	21ed      	movs	r1, #237	; 0xed
 80a19b2:	481f      	ldr	r0, [pc, #124]	; (80a1a30 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x234>)
 80a19b4:	f00e fb4c 	bl	80b0050 <__assert_func>
      zero_point_data[i] = src_quantization->zero_point()->Get(i);
      scale_data[i] = src_quantization->scale()->Get(i);
    }
    quantization->scale = reinterpret_cast<TfLiteFloatArray*>(scale_array);
 80a19b8:	9b01      	ldr	r3, [sp, #4]
 80a19ba:	f8c3 b000 	str.w	fp, [r3]
    quantization->zero_point =
        reinterpret_cast<TfLiteIntArray*>(zero_point_array);
 80a19be:	f8c3 a004 	str.w	sl, [r3, #4]

    result->quantization = {kTfLiteAffineQuantization, quantization};
 80a19c2:	2301      	movs	r3, #1
 80a19c4:	f884 3030 	strb.w	r3, [r4, #48]	; 0x30
 80a19c8:	9b01      	ldr	r3, [sp, #4]
 80a19ca:	6363      	str	r3, [r4, #52]	; 0x34
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
 80a19cc:	210a      	movs	r1, #10
 80a19ce:	4628      	mov	r0, r5
 80a19d0:	f7ff fec2 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
  }
  // Copy the name, if there is one.
  if (flatbuffer_tensor.name()->c_str() != nullptr) {
 80a19d4:	3004      	adds	r0, #4
    result->name = flatbuffer_tensor.name()->c_str();
  } else {
    result->name = "<No name>";
 80a19d6:	bf0a      	itet	eq
 80a19d8:	4b16      	ldreq	r3, [pc, #88]	; (80a1a34 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x238>)

    result->quantization = {kTfLiteAffineQuantization, quantization};
  }
  // Copy the name, if there is one.
  if (flatbuffer_tensor.name()->c_str() != nullptr) {
    result->name = flatbuffer_tensor.name()->c_str();
 80a19da:	6220      	strne	r0, [r4, #32]
  } else {
    result->name = "<No name>";
 80a19dc:	6223      	streq	r3, [r4, #32]
  }
  // These aren't used by the micro flavor of TFL, so set them to defaults.
  result->allocation = nullptr;
 80a19de:	2300      	movs	r3, #0
 80a19e0:	61e3      	str	r3, [r4, #28]
  result->delegate = nullptr;
 80a19e2:	6263      	str	r3, [r4, #36]	; 0x24
  result->buffer_handle = 0;
 80a19e4:	62a3      	str	r3, [r4, #40]	; 0x28
  result->data_is_stale = false;
 80a19e6:	f884 302c 	strb.w	r3, [r4, #44]	; 0x2c
 80a19ea:	e016      	b.n	80a1a1a <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x21e>
    const flatbuffers::Vector<flatbuffers::Offset<Buffer>>* buffers,
    ErrorReporter* error_reporter, TfLiteTensor* result,
    uint8_t* preallocated_buffer) {
  // Make sure the serialized type is one we know how to deal with, and convert
  // it from a flatbuffer enum into a constant used by the kernel C API.
  TF_LITE_ENSURE_STATUS(ConvertTensorType(flatbuffer_tensor.type(),
 80a19ec:	2601      	movs	r6, #1
 80a19ee:	e014      	b.n	80a1a1a <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x21e>
template<typename T> struct IndirectHelper {
  typedef T return_type;
  typedef T mutable_return_type;
  static const size_t element_stride = sizeof(T);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    return EndianScalar((reinterpret_cast<const T *>(p))[i]);
 80a19f0:	eb00 00c9 	add.w	r0, r0, r9, lsl #3
    zero_point_array[0] = channels;
    scale_array[0] = channels;
    int* zero_point_data = &zero_point_array[1];
    float* scale_data = reinterpret_cast<float*>(&scale_array[1]);
    for (int i = 0; i < channels; i++) {
      zero_point_data[i] = src_quantization->zero_point()->Get(i);
 80a19f4:	9b03      	ldr	r3, [sp, #12]
 80a19f6:	6841      	ldr	r1, [r0, #4]
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
 80a19f8:	4638      	mov	r0, r7
 80a19fa:	f843 1f04 	str.w	r1, [r3, #4]!
 80a19fe:	2108      	movs	r1, #8
 80a1a00:	9303      	str	r3, [sp, #12]
 80a1a02:	f7ff fea9 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
      scale_data[i] = src_quantization->scale()->Get(i);
 80a1a06:	4649      	mov	r1, r9
 80a1a08:	f7ff fe66 	bl	80a16d8 <_ZNK11flatbuffers6VectorIfE3GetEm>
 80a1a0c:	9b02      	ldr	r3, [sp, #8]
            channels * sizeof(float) + sizeof(int), sizeof(int)));
    zero_point_array[0] = channels;
    scale_array[0] = channels;
    int* zero_point_data = &zero_point_array[1];
    float* scale_data = reinterpret_cast<float*>(&scale_array[1]);
    for (int i = 0; i < channels; i++) {
 80a1a0e:	f109 0901 	add.w	r9, r9, #1
      zero_point_data[i] = src_quantization->zero_point()->Get(i);
      scale_data[i] = src_quantization->scale()->Get(i);
 80a1a12:	f843 0f04 	str.w	r0, [r3, #4]!
 80a1a16:	9302      	str	r3, [sp, #8]
 80a1a18:	e7bf      	b.n	80a199a <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x19e>
  result->allocation = nullptr;
  result->delegate = nullptr;
  result->buffer_handle = 0;
  result->data_is_stale = false;
  return kTfLiteOk;
}
 80a1a1a:	4630      	mov	r0, r6
 80a1a1c:	b007      	add	sp, #28
 80a1a1e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80a1a22:	bf00      	nop
 80a1a24:	080b4829 	.word	0x080b4829
 80a1a28:	080b55c0 	.word	0x080b55c0
 80a1a2c:	080b56b6 	.word	0x080b56b6
 80a1a30:	080b4834 	.word	0x080b4834
 80a1a34:	080b5415 	.word	0x080b5415

080a1a38 <_ZN6tflite14MicroAllocator15AllocateTensorsEv>:
  const auto* tensor = tensors_->Get(tensor_index);
  return InitializeRuntimeTensor(*tensor, buffers, error_reporter_,
                                 &context_->tensors[tensor_index], buffer);
}

TfLiteStatus MicroAllocator::AllocateTensors() {
 80a1a38:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
 80a1a3c:	6a83      	ldr	r3, [r0, #40]	; 0x28

  // It would be better not to allocate this memory for the lifetime of the
  // model, but we don't have a straightforward way to avoid it.
  TensorInfo* tensor_info =
      reinterpret_cast<TensorInfo*>(memory_allocator_.AllocateFromTail(
          sizeof(TensorInfo) * tensors_size, sizeof(TensorInfo)));
 80a1a3e:	2214      	movs	r2, #20
 80a1a40:	f8d3 9000 	ldr.w	r9, [r3]
  const auto* tensor = tensors_->Get(tensor_index);
  return InitializeRuntimeTensor(*tensor, buffers, error_reporter_,
                                 &context_->tensors[tensor_index], buffer);
}

TfLiteStatus MicroAllocator::AllocateTensors() {
 80a1a44:	b091      	sub	sp, #68	; 0x44

  // It would be better not to allocate this memory for the lifetime of the
  // model, but we don't have a straightforward way to avoid it.
  TensorInfo* tensor_info =
      reinterpret_cast<TensorInfo*>(memory_allocator_.AllocateFromTail(
          sizeof(TensorInfo) * tensors_size, sizeof(TensorInfo)));
 80a1a46:	fb02 f109 	mul.w	r1, r2, r9
  const auto* tensor = tensors_->Get(tensor_index);
  return InitializeRuntimeTensor(*tensor, buffers, error_reporter_,
                                 &context_->tensors[tensor_index], buffer);
}

TfLiteStatus MicroAllocator::AllocateTensors() {
 80a1a4a:	4604      	mov	r4, r0

  // It would be better not to allocate this memory for the lifetime of the
  // model, but we don't have a straightforward way to avoid it.
  TensorInfo* tensor_info =
      reinterpret_cast<TensorInfo*>(memory_allocator_.AllocateFromTail(
          sizeof(TensorInfo) * tensors_size, sizeof(TensorInfo)));
 80a1a4c:	3004      	adds	r0, #4
 80a1a4e:	f000 fc7e 	bl	80a234e <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>
 80a1a52:	4605      	mov	r5, r0
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
 80a1a54:	210c      	movs	r1, #12
 80a1a56:	6820      	ldr	r0, [r4, #0]
 80a1a58:	f7ff fe7e 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
 80a1a5c:	f105 0710 	add.w	r7, r5, #16

  const flatbuffers::Vector<flatbuffers::Offset<Buffer>>* buffers =
      model_->buffers();

  // Set up the runtime data structures for all tensors.
  for (size_t i = 0; i < tensors_size; ++i) {
 80a1a60:	f04f 0800 	mov.w	r8, #0
 80a1a64:	463e      	mov	r6, r7
    TensorInfo* current = &tensor_info[i];
    current->flatbuffer_tensor = &(*(tensors_->Get(i)));
    current->runtime_tensor = &context_->tensors[i];
    const bool is_variable = current->flatbuffer_tensor->is_variable();
    if (is_variable) {
      current->first_created = 0;
 80a1a66:	46c3      	mov	fp, r8
 80a1a68:	9002      	str	r0, [sp, #8]
      current->last_used = operators_->size();
    } else {
      current->first_created = -1;
 80a1a6a:	f04f 3aff 	mov.w	sl, #4294967295	; 0xffffffff

  const flatbuffers::Vector<flatbuffers::Offset<Buffer>>* buffers =
      model_->buffers();

  // Set up the runtime data structures for all tensors.
  for (size_t i = 0; i < tensors_size; ++i) {
 80a1a6e:	45c8      	cmp	r8, r9
 80a1a70:	d104      	bne.n	80a1a7c <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x44>
 80a1a72:	2600      	movs	r6, #0
  }

  // First go through the inputs and figure out if they need to be allocated.
  for (size_t i = 0; i < subgraph_->inputs()->size(); ++i) {
    const int tensor_index = subgraph_->inputs()->Get(i);
    TensorInfo* current = &tensor_info[tensor_index];
 80a1a74:	f04f 0914 	mov.w	r9, #20
    // Check for pre-allocated inputs.
    current->needs_allocating = (current->runtime_tensor->data.raw == nullptr);
    current->first_created = 0;
 80a1a78:	46b0      	mov	r8, r6
 80a1a7a:	e03e      	b.n	80a1afa <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0xc2>
      model_->buffers();

  // Set up the runtime data structures for all tensors.
  for (size_t i = 0; i < tensors_size; ++i) {
    TensorInfo* current = &tensor_info[i];
    current->flatbuffer_tensor = &(*(tensors_->Get(i)));
 80a1a7c:	6aa0      	ldr	r0, [r4, #40]	; 0x28
 80a1a7e:	4641      	mov	r1, r8
 80a1a80:	f7ff fe52 	bl	80a1728 <_ZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEm>
 80a1a84:	f846 0c10 	str.w	r0, [r6, #-16]
    current->runtime_tensor = &context_->tensors[i];
 80a1a88:	6963      	ldr	r3, [r4, #20]
 80a1a8a:	2238      	movs	r2, #56	; 0x38
 80a1a8c:	689b      	ldr	r3, [r3, #8]
 80a1a8e:	fb02 3308 	mla	r3, r2, r8, r3
 80a1a92:	f846 3c0c 	str.w	r3, [r6, #-12]
 80a1a96:	9303      	str	r3, [sp, #12]
    const bool is_variable = current->flatbuffer_tensor->is_variable();
 80a1a98:	f7ff fe13 	bl	80a16c2 <_ZNK6tflite6Tensor11is_variableEv>
    if (is_variable) {
 80a1a9c:	9b03      	ldr	r3, [sp, #12]
 80a1a9e:	b130      	cbz	r0, 80a1aae <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x76>
      current->first_created = 0;
 80a1aa0:	f846 bc08 	str.w	fp, [r6, #-8]
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
 80a1aa4:	6a62      	ldr	r2, [r4, #36]	; 0x24
      current->last_used = operators_->size();
 80a1aa6:	6812      	ldr	r2, [r2, #0]
 80a1aa8:	f846 2c04 	str.w	r2, [r6, #-4]
 80a1aac:	e003      	b.n	80a1ab6 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x7e>
    } else {
      current->first_created = -1;
 80a1aae:	f846 ac08 	str.w	sl, [r6, #-8]
      current->last_used = -1;
 80a1ab2:	f846 ac04 	str.w	sl, [r6, #-4]
    }
    current->needs_allocating = false;
 80a1ab6:	f886 b000 	strb.w	fp, [r6]
    // Preallocated inputs have already been set up earlier, so skip them.
    const bool is_preallocated_input =
        (current->runtime_tensor->data.raw != nullptr);
    if (!is_preallocated_input) {
 80a1aba:	685a      	ldr	r2, [r3, #4]
 80a1abc:	b11a      	cbz	r2, 80a1ac6 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x8e>

  const flatbuffers::Vector<flatbuffers::Offset<Buffer>>* buffers =
      model_->buffers();

  // Set up the runtime data structures for all tensors.
  for (size_t i = 0; i < tensors_size; ++i) {
 80a1abe:	f108 0801 	add.w	r8, r8, #1
 80a1ac2:	3614      	adds	r6, #20
 80a1ac4:	e7d3      	b.n	80a1a6e <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x36>
    current->needs_allocating = false;
    // Preallocated inputs have already been set up earlier, so skip them.
    const bool is_preallocated_input =
        (current->runtime_tensor->data.raw != nullptr);
    if (!is_preallocated_input) {
      TF_LITE_ENSURE_STATUS(InitializeRuntimeTensor(
 80a1ac6:	9201      	str	r2, [sp, #4]
 80a1ac8:	9300      	str	r3, [sp, #0]
 80a1aca:	6923      	ldr	r3, [r4, #16]
 80a1acc:	9a02      	ldr	r2, [sp, #8]
 80a1ace:	f856 1c10 	ldr.w	r1, [r6, #-16]
 80a1ad2:	4620      	mov	r0, r4
 80a1ad4:	f7ff fe92 	bl	80a17fc <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh>
 80a1ad8:	2800      	cmp	r0, #0
 80a1ada:	d0f0      	beq.n	80a1abe <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x86>
 80a1adc:	e101      	b.n	80a1ce2 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2aa>
    }
  }

  // First go through the inputs and figure out if they need to be allocated.
  for (size_t i = 0; i < subgraph_->inputs()->size(); ++i) {
    const int tensor_index = subgraph_->inputs()->Get(i);
 80a1ade:	4631      	mov	r1, r6
 80a1ae0:	f7ff fe0e 	bl	80a1700 <_ZNK11flatbuffers6VectorIlE3GetEm>
    TensorInfo* current = &tensor_info[tensor_index];
 80a1ae4:	fb09 5000 	mla	r0, r9, r0, r5
    // Check for pre-allocated inputs.
    current->needs_allocating = (current->runtime_tensor->data.raw == nullptr);
 80a1ae8:	6843      	ldr	r3, [r0, #4]
          current->runtime_tensor, nullptr));
    }
  }

  // First go through the inputs and figure out if they need to be allocated.
  for (size_t i = 0; i < subgraph_->inputs()->size(); ++i) {
 80a1aea:	3601      	adds	r6, #1
    const int tensor_index = subgraph_->inputs()->Get(i);
    TensorInfo* current = &tensor_info[tensor_index];
    // Check for pre-allocated inputs.
    current->needs_allocating = (current->runtime_tensor->data.raw == nullptr);
 80a1aec:	685b      	ldr	r3, [r3, #4]
    current->first_created = 0;
 80a1aee:	f8c0 8008 	str.w	r8, [r0, #8]
  // First go through the inputs and figure out if they need to be allocated.
  for (size_t i = 0; i < subgraph_->inputs()->size(); ++i) {
    const int tensor_index = subgraph_->inputs()->Get(i);
    TensorInfo* current = &tensor_info[tensor_index];
    // Check for pre-allocated inputs.
    current->needs_allocating = (current->runtime_tensor->data.raw == nullptr);
 80a1af2:	fab3 f383 	clz	r3, r3
 80a1af6:	095b      	lsrs	r3, r3, #5
 80a1af8:	7403      	strb	r3, [r0, #16]
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
 80a1afa:	2106      	movs	r1, #6
 80a1afc:	6a20      	ldr	r0, [r4, #32]
 80a1afe:	f7ff fe2b 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
          current->runtime_tensor, nullptr));
    }
  }

  // First go through the inputs and figure out if they need to be allocated.
  for (size_t i = 0; i < subgraph_->inputs()->size(); ++i) {
 80a1b02:	6803      	ldr	r3, [r0, #0]
 80a1b04:	429e      	cmp	r6, r3
 80a1b06:	d3ea      	bcc.n	80a1ade <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0xa6>
 80a1b08:	2600      	movs	r6, #0

  // Mark all outputs as persistent to the end of the invocation.
  for (size_t i = 0; i < subgraph_->outputs()->size(); ++i) {
    const int tensor_index = subgraph_->outputs()->Get(i);
    TensorInfo* current = &tensor_info[tensor_index];
    current->last_used = operators_->size() - 1;
 80a1b0a:	f04f 0914 	mov.w	r9, #20
 80a1b0e:	2108      	movs	r1, #8
 80a1b10:	6a20      	ldr	r0, [r4, #32]
 80a1b12:	f7ff fe21 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
    current->needs_allocating = (current->runtime_tensor->data.raw == nullptr);
    current->first_created = 0;
  }

  // Mark all outputs as persistent to the end of the invocation.
  for (size_t i = 0; i < subgraph_->outputs()->size(); ++i) {
 80a1b16:	6803      	ldr	r3, [r0, #0]
 80a1b18:	f8d4 8024 	ldr.w	r8, [r4, #36]	; 0x24
 80a1b1c:	429e      	cmp	r6, r3
 80a1b1e:	d20a      	bcs.n	80a1b36 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0xfe>
    const int tensor_index = subgraph_->outputs()->Get(i);
 80a1b20:	4631      	mov	r1, r6
 80a1b22:	f7ff fded 	bl	80a1700 <_ZNK11flatbuffers6VectorIlE3GetEm>
    TensorInfo* current = &tensor_info[tensor_index];
    current->last_used = operators_->size() - 1;
 80a1b26:	fb09 5000 	mla	r0, r9, r0, r5
 80a1b2a:	f8d8 3000 	ldr.w	r3, [r8]
    current->needs_allocating = (current->runtime_tensor->data.raw == nullptr);
    current->first_created = 0;
  }

  // Mark all outputs as persistent to the end of the invocation.
  for (size_t i = 0; i < subgraph_->outputs()->size(); ++i) {
 80a1b2e:	3601      	adds	r6, #1
    const int tensor_index = subgraph_->outputs()->Get(i);
    TensorInfo* current = &tensor_info[tensor_index];
    current->last_used = operators_->size() - 1;
 80a1b30:	3b01      	subs	r3, #1
 80a1b32:	60c3      	str	r3, [r0, #12]
    current->needs_allocating = (current->runtime_tensor->data.raw == nullptr);
    current->first_created = 0;
  }

  // Mark all outputs as persistent to the end of the invocation.
  for (size_t i = 0; i < subgraph_->outputs()->size(); ++i) {
 80a1b34:	e7eb      	b.n	80a1b0e <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0xd6>
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
 80a1b36:	f8d8 6000 	ldr.w	r6, [r8]
  // Figure out when the first and last use of each tensor is.
  for (int i = (operators_->size() - 1); i >= 0; --i) {
    const auto* op = operators_->Get(i);
    for (size_t n = 0; n < op->inputs()->size(); ++n) {
      const int tensor_index = op->inputs()->Get(n);
      TensorInfo* current = &tensor_info[tensor_index];
 80a1b3a:	f04f 0914 	mov.w	r9, #20
    TensorInfo* current = &tensor_info[tensor_index];
    current->last_used = operators_->size() - 1;
  }

  // Figure out when the first and last use of each tensor is.
  for (int i = (operators_->size() - 1); i >= 0; --i) {
 80a1b3e:	f106 38ff 	add.w	r8, r6, #4294967295	; 0xffffffff
 80a1b42:	f106 4680 	add.w	r6, r6, #1073741824	; 0x40000000
 80a1b46:	3e01      	subs	r6, #1
 80a1b48:	00b6      	lsls	r6, r6, #2
 80a1b4a:	f1b8 0f00 	cmp.w	r8, #0
 80a1b4e:	da03      	bge.n	80a1b58 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x120>
 80a1b50:	462b      	mov	r3, r5
 80a1b52:	2200      	movs	r2, #0
          "Logic error in memory planner, tensor %d has an invalid lifetime",
          i);
      return kTfLiteError;
    }
    if (!is_read_only && !is_preallocated_input) {
      current->needs_allocating = true;
 80a1b54:	2001      	movs	r0, #1
 80a1b56:	e04c      	b.n	80a1bf2 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x1ba>
    current->last_used = operators_->size() - 1;
  }

  // Figure out when the first and last use of each tensor is.
  for (int i = (operators_->size() - 1); i >= 0; --i) {
    const auto* op = operators_->Get(i);
 80a1b58:	6a63      	ldr	r3, [r4, #36]	; 0x24

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
 80a1b5a:	681a      	ldr	r2, [r3, #0]
 80a1b5c:	4590      	cmp	r8, r2
 80a1b5e:	d305      	bcc.n	80a1b6c <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x134>
 80a1b60:	4b66      	ldr	r3, [pc, #408]	; (80a1cfc <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2c4>)
 80a1b62:	4a67      	ldr	r2, [pc, #412]	; (80a1d00 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2c8>)
 80a1b64:	21ed      	movs	r1, #237	; 0xed
 80a1b66:	4867      	ldr	r0, [pc, #412]	; (80a1d04 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2cc>)
 80a1b68:	f00e fa72 	bl	80b0050 <__assert_func>
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
 80a1b6c:	3304      	adds	r3, #4
template<typename T> struct IndirectHelper<Offset<T>> {
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
 80a1b6e:	eb03 0b06 	add.w	fp, r3, r6
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
 80a1b72:	599b      	ldr	r3, [r3, r6]
    for (size_t n = 0; n < op->inputs()->size(); ++n) {
 80a1b74:	f04f 0a00 	mov.w	sl, #0
 80a1b78:	449b      	add	fp, r3
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
 80a1b7a:	2106      	movs	r1, #6
 80a1b7c:	4658      	mov	r0, fp
 80a1b7e:	f7ff fdeb 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
 80a1b82:	6803      	ldr	r3, [r0, #0]
 80a1b84:	459a      	cmp	sl, r3
 80a1b86:	d302      	bcc.n	80a1b8e <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x156>
 80a1b88:	f04f 0a00 	mov.w	sl, #0
 80a1b8c:	e01a      	b.n	80a1bc4 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x18c>
      const int tensor_index = op->inputs()->Get(n);
 80a1b8e:	4651      	mov	r1, sl
 80a1b90:	f7ff fdb6 	bl	80a1700 <_ZNK11flatbuffers6VectorIlE3GetEm>
      TensorInfo* current = &tensor_info[tensor_index];
 80a1b94:	fb09 5000 	mla	r0, r9, r0, r5
      if ((current->last_used == -1) || (current->last_used > i)) {
 80a1b98:	68c3      	ldr	r3, [r0, #12]
 80a1b9a:	1c59      	adds	r1, r3, #1
 80a1b9c:	d001      	beq.n	80a1ba2 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x16a>
 80a1b9e:	4598      	cmp	r8, r3
 80a1ba0:	da01      	bge.n	80a1ba6 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x16e>
        current->last_used = i;
 80a1ba2:	f8c0 800c 	str.w	r8, [r0, #12]
  }

  // Figure out when the first and last use of each tensor is.
  for (int i = (operators_->size() - 1); i >= 0; --i) {
    const auto* op = operators_->Get(i);
    for (size_t n = 0; n < op->inputs()->size(); ++n) {
 80a1ba6:	f10a 0a01 	add.w	sl, sl, #1
 80a1baa:	e7e6      	b.n	80a1b7a <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x142>
      if ((current->last_used == -1) || (current->last_used > i)) {
        current->last_used = i;
      }
    }
    for (size_t n = 0; n < op->outputs()->size(); ++n) {
      const int tensor_index = op->outputs()->Get(n);
 80a1bac:	4651      	mov	r1, sl
 80a1bae:	f7ff fda7 	bl	80a1700 <_ZNK11flatbuffers6VectorIlE3GetEm>
      TensorInfo* current = &tensor_info[tensor_index];
 80a1bb2:	fb09 5000 	mla	r0, r9, r0, r5
      if ((current->first_created == -1) || (current->first_created < i)) {
 80a1bb6:	6883      	ldr	r3, [r0, #8]
 80a1bb8:	1c5a      	adds	r2, r3, #1
 80a1bba:	d00b      	beq.n	80a1bd4 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x19c>
 80a1bbc:	4598      	cmp	r8, r3
 80a1bbe:	dc09      	bgt.n	80a1bd4 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x19c>
      TensorInfo* current = &tensor_info[tensor_index];
      if ((current->last_used == -1) || (current->last_used > i)) {
        current->last_used = i;
      }
    }
    for (size_t n = 0; n < op->outputs()->size(); ++n) {
 80a1bc0:	f10a 0a01 	add.w	sl, sl, #1
 80a1bc4:	2108      	movs	r1, #8
 80a1bc6:	4658      	mov	r0, fp
 80a1bc8:	f7ff fdc6 	bl	80a1758 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
 80a1bcc:	6803      	ldr	r3, [r0, #0]
 80a1bce:	459a      	cmp	sl, r3
 80a1bd0:	d3ec      	bcc.n	80a1bac <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x174>
 80a1bd2:	e002      	b.n	80a1bda <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x1a2>
      const int tensor_index = op->outputs()->Get(n);
      TensorInfo* current = &tensor_info[tensor_index];
      if ((current->first_created == -1) || (current->first_created < i)) {
        current->first_created = i;
 80a1bd4:	f8c0 8008 	str.w	r8, [r0, #8]
 80a1bd8:	e7f2      	b.n	80a1bc0 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x188>
    TensorInfo* current = &tensor_info[tensor_index];
    current->last_used = operators_->size() - 1;
  }

  // Figure out when the first and last use of each tensor is.
  for (int i = (operators_->size() - 1); i >= 0; --i) {
 80a1bda:	f108 38ff 	add.w	r8, r8, #4294967295	; 0xffffffff
 80a1bde:	3e04      	subs	r6, #4
 80a1be0:	e7b3      	b.n	80a1b4a <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x112>

  // Work out which tensors need to be allocated.
  for (size_t i = 0; i < tensors_->size(); ++i) {
    TensorInfo* current = &tensor_info[i];
    const bool is_read_only =
        (current->first_created == -1) && (current->last_used != -1);
 80a1be2:	6899      	ldr	r1, [r3, #8]
 80a1be4:	3101      	adds	r1, #1
 80a1be6:	68d9      	ldr	r1, [r3, #12]
 80a1be8:	d175      	bne.n	80a1cd6 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x29e>
 80a1bea:	3101      	adds	r1, #1
 80a1bec:	d075      	beq.n	80a1cda <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2a2>
      }
    }
  }

  // Work out which tensors need to be allocated.
  for (size_t i = 0; i < tensors_->size(); ++i) {
 80a1bee:	3201      	adds	r2, #1
 80a1bf0:	3314      	adds	r3, #20
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
 80a1bf2:	6aa1      	ldr	r1, [r4, #40]	; 0x28
 80a1bf4:	6809      	ldr	r1, [r1, #0]
 80a1bf6:	428a      	cmp	r2, r1
 80a1bf8:	d3f3      	bcc.n	80a1be2 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x1aa>
    if (!is_read_only && !is_preallocated_input) {
      current->needs_allocating = true;
    }
  }

  uint8_t* aligned_arena = AlignPointerUp(arena_, kBufferAlignment);
 80a1bfa:	2110      	movs	r1, #16
 80a1bfc:	69a0      	ldr	r0, [r4, #24]
 80a1bfe:	f7ff fce7 	bl	80a15d0 <_ZN6tflite14AlignPointerUpEPhj>
  const size_t alignment_loss = (aligned_arena - arena_);

  int remaining_arena_size =
      arena_size_ - (memory_allocator_.GetDataSize() + alignment_loss);
 80a1c02:	69e3      	ldr	r3, [r4, #28]
 80a1c04:	6866      	ldr	r6, [r4, #4]
  GreedyMemoryPlanner planner(aligned_arena, remaining_arena_size);
 80a1c06:	4601      	mov	r1, r0

  uint8_t* aligned_arena = AlignPointerUp(arena_, kBufferAlignment);
  const size_t alignment_loss = (aligned_arena - arena_);

  int remaining_arena_size =
      arena_size_ - (memory_allocator_.GetDataSize() + alignment_loss);
 80a1c08:	1b9e      	subs	r6, r3, r6
 80a1c0a:	69a3      	ldr	r3, [r4, #24]
    if (!is_read_only && !is_preallocated_input) {
      current->needs_allocating = true;
    }
  }

  uint8_t* aligned_arena = AlignPointerUp(arena_, kBufferAlignment);
 80a1c0c:	4680      	mov	r8, r0
  const size_t alignment_loss = (aligned_arena - arena_);

  int remaining_arena_size =
      arena_size_ - (memory_allocator_.GetDataSize() + alignment_loss);
 80a1c0e:	1ac3      	subs	r3, r0, r3
 80a1c10:	1af6      	subs	r6, r6, r3
  GreedyMemoryPlanner planner(aligned_arena, remaining_arena_size);
 80a1c12:	4632      	mov	r2, r6
 80a1c14:	a806      	add	r0, sp, #24
 80a1c16:	f00d fc51 	bl	80af4bc <_ZN6tflite19GreedyMemoryPlannerC1EPhi>

  // Add the tensors to our allocation plan.
  for (size_t i = 0; i < tensors_->size(); ++i) {
 80a1c1a:	f04f 0900 	mov.w	r9, #0
 80a1c1e:	6aa3      	ldr	r3, [r4, #40]	; 0x28
 80a1c20:	681b      	ldr	r3, [r3, #0]
 80a1c22:	4599      	cmp	r9, r3
 80a1c24:	d21b      	bcs.n	80a1c5e <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x226>
    TensorInfo* current = &tensor_info[i];
    if (current->needs_allocating) {
 80a1c26:	783b      	ldrb	r3, [r7, #0]
 80a1c28:	b1ab      	cbz	r3, 80a1c56 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x21e>
      size_t bytes_required;
      size_t type_size;
      TF_LITE_ENSURE_STATUS(BytesRequiredForTensor(*current->flatbuffer_tensor,
 80a1c2a:	6923      	ldr	r3, [r4, #16]
 80a1c2c:	aa05      	add	r2, sp, #20
 80a1c2e:	a904      	add	r1, sp, #16
 80a1c30:	f857 0c10 	ldr.w	r0, [r7, #-16]
 80a1c34:	f7ff fd00 	bl	80a1638 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE>
 80a1c38:	bb00      	cbnz	r0, 80a1c7c <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x244>
                                                   &bytes_required, &type_size,
                                                   error_reporter_));
      size_t aligned_bytes_required =
          AlignSizeUp(bytes_required, kBufferAlignment);
 80a1c3a:	2110      	movs	r1, #16
 80a1c3c:	9804      	ldr	r0, [sp, #16]
 80a1c3e:	f7ff fcd1 	bl	80a15e4 <_ZN6tflite11AlignSizeUpEjj>
      planner.AddBuffer(error_reporter_, aligned_bytes_required,
                        current->first_created, current->last_used);
 80a1c42:	f857 3c04 	ldr.w	r3, [r7, #-4]
 80a1c46:	4602      	mov	r2, r0
 80a1c48:	9300      	str	r3, [sp, #0]
 80a1c4a:	f857 3c08 	ldr.w	r3, [r7, #-8]
 80a1c4e:	6921      	ldr	r1, [r4, #16]
 80a1c50:	a806      	add	r0, sp, #24
 80a1c52:	f00d fbf7 	bl	80af444 <_ZN6tflite19GreedyMemoryPlanner9AddBufferEPNS_13ErrorReporterEiii>
  int remaining_arena_size =
      arena_size_ - (memory_allocator_.GetDataSize() + alignment_loss);
  GreedyMemoryPlanner planner(aligned_arena, remaining_arena_size);

  // Add the tensors to our allocation plan.
  for (size_t i = 0; i < tensors_->size(); ++i) {
 80a1c56:	f109 0901 	add.w	r9, r9, #1
 80a1c5a:	3714      	adds	r7, #20
 80a1c5c:	e7df      	b.n	80a1c1e <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x1e6>
                        current->first_created, current->last_used);
    }
  }

  // Make sure we have enough room.
  if (planner.GetMaximumMemorySize() > remaining_arena_size) {
 80a1c5e:	a806      	add	r0, sp, #24
 80a1c60:	f00d fd15 	bl	80af68e <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv>
 80a1c64:	4286      	cmp	r6, r0
 80a1c66:	da0b      	bge.n	80a1c80 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x248>
    error_reporter_->Report(
 80a1c68:	a806      	add	r0, sp, #24
 80a1c6a:	6924      	ldr	r4, [r4, #16]
 80a1c6c:	f00d fd0f 	bl	80af68e <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv>
        "Arena size is too small for activation buffers. Needed %d but only %d "
        "was available.",
        planner.GetMaximumMemorySize(), remaining_arena_size);
 80a1c70:	4633      	mov	r3, r6
 80a1c72:	4602      	mov	r2, r0
 80a1c74:	4924      	ldr	r1, [pc, #144]	; (80a1d08 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2d0>)
 80a1c76:	4620      	mov	r0, r4
 80a1c78:	f7fe fb98 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return kTfLiteError;
 80a1c7c:	2401      	movs	r4, #1
 80a1c7e:	e026      	b.n	80a1cce <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x296>
 80a1c80:	2600      	movs	r6, #0
 80a1c82:	4637      	mov	r7, r6
 80a1c84:	6aa3      	ldr	r3, [r4, #40]	; 0x28
  }

  // Figure out the actual memory addresses for each buffer, based on the plan.
  int planner_index = 0;
  for (size_t i = 0; i < tensors_->size(); ++i) {
 80a1c86:	681b      	ldr	r3, [r3, #0]
 80a1c88:	429e      	cmp	r6, r3
 80a1c8a:	d21f      	bcs.n	80a1ccc <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x294>
    TensorInfo* current = &tensor_info[i];
    if (current->needs_allocating) {
 80a1c8c:	7c2b      	ldrb	r3, [r5, #16]
 80a1c8e:	b163      	cbz	r3, 80a1caa <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x272>
      int offset;
      TF_LITE_ENSURE_STATUS(
 80a1c90:	ab05      	add	r3, sp, #20
 80a1c92:	463a      	mov	r2, r7
 80a1c94:	6921      	ldr	r1, [r4, #16]
 80a1c96:	a806      	add	r0, sp, #24
 80a1c98:	f00d fd14 	bl	80af6c4 <_ZN6tflite19GreedyMemoryPlanner18GetOffsetForBufferEPNS_13ErrorReporterEiPi>
 80a1c9c:	2800      	cmp	r0, #0
 80a1c9e:	d1ed      	bne.n	80a1c7c <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x244>
          planner.GetOffsetForBuffer(error_reporter_, planner_index, &offset));
      current->runtime_tensor->data.uint8 = aligned_arena + offset;
 80a1ca0:	9b05      	ldr	r3, [sp, #20]
 80a1ca2:	686a      	ldr	r2, [r5, #4]
 80a1ca4:	4443      	add	r3, r8
 80a1ca6:	6053      	str	r3, [r2, #4]
      ++planner_index;
 80a1ca8:	3701      	adds	r7, #1
    }
    // Set default value for variable tensors:
    if (current->flatbuffer_tensor->is_variable()) {
 80a1caa:	6828      	ldr	r0, [r5, #0]
 80a1cac:	f7ff fd09 	bl	80a16c2 <_ZNK6tflite6Tensor11is_variableEv>
 80a1cb0:	b148      	cbz	r0, 80a1cc6 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x28e>
      if (current->runtime_tensor->data.uint8 == nullptr) {
 80a1cb2:	6868      	ldr	r0, [r5, #4]
 80a1cb4:	6843      	ldr	r3, [r0, #4]
 80a1cb6:	b923      	cbnz	r3, 80a1cc2 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x28a>
        error_reporter_->Report("Variable is not allocated");
 80a1cb8:	4914      	ldr	r1, [pc, #80]	; (80a1d0c <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2d4>)
 80a1cba:	6920      	ldr	r0, [r4, #16]
 80a1cbc:	f7fe fb76 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
 80a1cc0:	e7dc      	b.n	80a1c7c <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x244>
        return kTfLiteError;
      }
      tflite::ResetVariableTensor(current->runtime_tensor);
 80a1cc2:	f7ff fbb1 	bl	80a1428 <_ZN6tflite19ResetVariableTensorEP12TfLiteTensor>
    return kTfLiteError;
  }

  // Figure out the actual memory addresses for each buffer, based on the plan.
  int planner_index = 0;
  for (size_t i = 0; i < tensors_->size(); ++i) {
 80a1cc6:	3601      	adds	r6, #1
 80a1cc8:	3514      	adds	r5, #20
 80a1cca:	e7db      	b.n	80a1c84 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x24c>
      }
      tflite::ResetVariableTensor(current->runtime_tensor);
    }
  }

  return kTfLiteOk;
 80a1ccc:	2400      	movs	r4, #0
  uint8_t* aligned_arena = AlignPointerUp(arena_, kBufferAlignment);
  const size_t alignment_loss = (aligned_arena - arena_);

  int remaining_arena_size =
      arena_size_ - (memory_allocator_.GetDataSize() + alignment_loss);
  GreedyMemoryPlanner planner(aligned_arena, remaining_arena_size);
 80a1cce:	a806      	add	r0, sp, #24
 80a1cd0:	f00d fbae 	bl	80af430 <_ZN6tflite19GreedyMemoryPlannerD1Ev>
 80a1cd4:	e00e      	b.n	80a1cf4 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2bc>
        (current->first_created == -1) && (current->last_used != -1);
    const bool is_preallocated_input =
        (current->runtime_tensor->data.raw != nullptr);
    const bool has_partial_lifetime =
        !is_read_only &&
        ((current->first_created == -1) || (current->last_used == -1));
 80a1cd6:	3101      	adds	r1, #1
 80a1cd8:	d105      	bne.n	80a1ce6 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2ae>
    if (has_partial_lifetime) {
      error_reporter_->Report(
          "Logic error in memory planner, tensor %d has an invalid lifetime",
          i);
 80a1cda:	490d      	ldr	r1, [pc, #52]	; (80a1d10 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2d8>)
 80a1cdc:	6920      	ldr	r0, [r4, #16]
 80a1cde:	f7fe fb65 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
      return kTfLiteError;
 80a1ce2:	2401      	movs	r4, #1
 80a1ce4:	e006      	b.n	80a1cf4 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2bc>
  for (size_t i = 0; i < tensors_->size(); ++i) {
    TensorInfo* current = &tensor_info[i];
    const bool is_read_only =
        (current->first_created == -1) && (current->last_used != -1);
    const bool is_preallocated_input =
        (current->runtime_tensor->data.raw != nullptr);
 80a1ce6:	6859      	ldr	r1, [r3, #4]
      error_reporter_->Report(
          "Logic error in memory planner, tensor %d has an invalid lifetime",
          i);
      return kTfLiteError;
    }
    if (!is_read_only && !is_preallocated_input) {
 80a1ce8:	6849      	ldr	r1, [r1, #4]
 80a1cea:	2900      	cmp	r1, #0
 80a1cec:	f47f af7f 	bne.w	80a1bee <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x1b6>
      current->needs_allocating = true;
 80a1cf0:	7418      	strb	r0, [r3, #16]
 80a1cf2:	e77c      	b.n	80a1bee <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x1b6>
      tflite::ResetVariableTensor(current->runtime_tensor);
    }
  }

  return kTfLiteOk;
}
 80a1cf4:	4620      	mov	r0, r4
 80a1cf6:	b011      	add	sp, #68	; 0x44
 80a1cf8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80a1cfc:	080b4829 	.word	0x080b4829
 80a1d00:	080b51fc 	.word	0x080b51fc
 80a1d04:	080b4834 	.word	0x080b4834
 80a1d08:	080b5448 	.word	0x080b5448
 80a1d0c:	080b549d 	.word	0x080b549d
 80a1d10:	080b54b7 	.word	0x080b54b7

080a1d14 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list>:
namespace tflite {
namespace {
void DebugLogPrintf(const char* format, va_list args) {
  const int output_cache_size = 64;
  char output_cache[output_cache_size + 1];
  int output_cache_index = 0;
 80a1d14:	2300      	movs	r3, #0
  }
  DebugLog("\r\n");
}
}  // namespace

int MicroErrorReporter::Report(const char* format, va_list args) {
 80a1d16:	b5f0      	push	{r4, r5, r6, r7, lr}
 80a1d18:	460d      	mov	r5, r1
 80a1d1a:	4614      	mov	r4, r2
    } else {
      output_cache[output_cache_index] = *current;
      output_cache_index += 1;
    }
    if (output_cache_index >= output_cache_size) {
      output_cache[output_cache_index] = 0;
 80a1d1c:	461f      	mov	r7, r3
  }
  DebugLog("\r\n");
}
}  // namespace

int MicroErrorReporter::Report(const char* format, va_list args) {
 80a1d1e:	b093      	sub	sp, #76	; 0x4c
void DebugLogPrintf(const char* format, va_list args) {
  const int output_cache_size = 64;
  char output_cache[output_cache_size + 1];
  int output_cache_index = 0;
  const char* current = format;
  while (*current != 0) {
 80a1d20:	782a      	ldrb	r2, [r5, #0]
 80a1d22:	2a00      	cmp	r2, #0
 80a1d24:	d03f      	beq.n	80a1da6 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x92>
    if (*current == '%') {
 80a1d26:	2a25      	cmp	r2, #37	; 0x25
 80a1d28:	d12c      	bne.n	80a1d84 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x70>
      const char next = *(current + 1);
 80a1d2a:	786e      	ldrb	r6, [r5, #1]
      if ((next == 'd') || (next == 's') || (next == 'f')) {
 80a1d2c:	f006 02fd 	and.w	r2, r6, #253	; 0xfd
 80a1d30:	2a64      	cmp	r2, #100	; 0x64
 80a1d32:	d001      	beq.n	80a1d38 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x24>
 80a1d34:	2e73      	cmp	r6, #115	; 0x73
 80a1d36:	d12a      	bne.n	80a1d8e <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x7a>
        current += 1;
 80a1d38:	3501      	adds	r5, #1
        if (output_cache_index > 0) {
 80a1d3a:	b133      	cbz	r3, 80a1d4a <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x36>
          output_cache[output_cache_index] = 0;
 80a1d3c:	aa12      	add	r2, sp, #72	; 0x48
 80a1d3e:	4413      	add	r3, r2
          DebugLog(output_cache);
 80a1d40:	a801      	add	r0, sp, #4
    if (*current == '%') {
      const char next = *(current + 1);
      if ((next == 'd') || (next == 's') || (next == 'f')) {
        current += 1;
        if (output_cache_index > 0) {
          output_cache[output_cache_index] = 0;
 80a1d42:	f803 7c44 	strb.w	r7, [r3, #-68]
          DebugLog(output_cache);
 80a1d46:	f000 fb17 	bl	80a2378 <DebugLog>
          output_cache_index = 0;
        }
        if (next == 'd') {
 80a1d4a:	2e64      	cmp	r6, #100	; 0x64
 80a1d4c:	d104      	bne.n	80a1d58 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x44>
          DebugLogInt32(va_arg(args, int));
 80a1d4e:	6820      	ldr	r0, [r4, #0]
 80a1d50:	1d26      	adds	r6, r4, #4
 80a1d52:	f7ff fbb1 	bl	80a14b8 <DebugLogInt32>
 80a1d56:	e005      	b.n	80a1d64 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x50>
        } else if (next == 's') {
 80a1d58:	2e73      	cmp	r6, #115	; 0x73
 80a1d5a:	d105      	bne.n	80a1d68 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x54>
          DebugLog(va_arg(args, char*));
 80a1d5c:	6820      	ldr	r0, [r4, #0]
 80a1d5e:	1d26      	adds	r6, r4, #4
 80a1d60:	f000 fb0a 	bl	80a2378 <DebugLog>
 80a1d64:	4634      	mov	r4, r6
 80a1d66:	e01b      	b.n	80a1da0 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x8c>
        } else if (next == 'f') {
 80a1d68:	2e66      	cmp	r6, #102	; 0x66
 80a1d6a:	d119      	bne.n	80a1da0 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x8c>
          DebugLogFloat(va_arg(args, double));
 80a1d6c:	1de2      	adds	r2, r4, #7
 80a1d6e:	f022 0207 	bic.w	r2, r2, #7
 80a1d72:	e9d2 0100 	ldrd	r0, r1, [r2]
 80a1d76:	f102 0408 	add.w	r4, r2, #8
 80a1d7a:	f011 fad9 	bl	80b3330 <__aeabi_d2f>
 80a1d7e:	f7ff fbb1 	bl	80a14e4 <DebugLogFloat>
 80a1d82:	e00d      	b.n	80a1da0 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x8c>
        }
      }
    } else {
      output_cache[output_cache_index] = *current;
 80a1d84:	a912      	add	r1, sp, #72	; 0x48
 80a1d86:	4419      	add	r1, r3
 80a1d88:	f801 2c44 	strb.w	r2, [r1, #-68]
      output_cache_index += 1;
 80a1d8c:	3301      	adds	r3, #1
    }
    if (output_cache_index >= output_cache_size) {
 80a1d8e:	2b3f      	cmp	r3, #63	; 0x3f
 80a1d90:	dd07      	ble.n	80a1da2 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x8e>
      output_cache[output_cache_index] = 0;
 80a1d92:	aa12      	add	r2, sp, #72	; 0x48
 80a1d94:	4413      	add	r3, r2
      DebugLog(output_cache);
 80a1d96:	a801      	add	r0, sp, #4
    } else {
      output_cache[output_cache_index] = *current;
      output_cache_index += 1;
    }
    if (output_cache_index >= output_cache_size) {
      output_cache[output_cache_index] = 0;
 80a1d98:	f803 7c44 	strb.w	r7, [r3, #-68]
      DebugLog(output_cache);
 80a1d9c:	f000 faec 	bl	80a2378 <DebugLog>
      output_cache_index = 0;
 80a1da0:	2300      	movs	r3, #0
    }
    current += 1;
 80a1da2:	3501      	adds	r5, #1
 80a1da4:	e7bc      	b.n	80a1d20 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0xc>
  }
  if (output_cache_index > 0) {
 80a1da6:	b133      	cbz	r3, 80a1db6 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0xa2>
    output_cache[output_cache_index] = 0;
 80a1da8:	a912      	add	r1, sp, #72	; 0x48
 80a1daa:	440b      	add	r3, r1
    DebugLog(output_cache);
 80a1dac:	a801      	add	r0, sp, #4
      output_cache_index = 0;
    }
    current += 1;
  }
  if (output_cache_index > 0) {
    output_cache[output_cache_index] = 0;
 80a1dae:	f803 2c44 	strb.w	r2, [r3, #-68]
    DebugLog(output_cache);
 80a1db2:	f000 fae1 	bl	80a2378 <DebugLog>
    output_cache_index = 0;
  }
  DebugLog("\r\n");
 80a1db6:	4803      	ldr	r0, [pc, #12]	; (80a1dc4 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0xb0>)
 80a1db8:	f000 fade 	bl	80a2378 <DebugLog>
}  // namespace

int MicroErrorReporter::Report(const char* format, va_list args) {
  DebugLogPrintf(format, args);
  return 0;
}
 80a1dbc:	2000      	movs	r0, #0
 80a1dbe:	b013      	add	sp, #76	; 0x4c
 80a1dc0:	bdf0      	pop	{r4, r5, r6, r7, pc}
 80a1dc2:	bf00      	nop
 80a1dc4:	080b57a4 	.word	0x080b57a4

080a1dc8 <_ZN6tflite12_GLOBAL__N_118StackDataAllocator8AllocateEj>:
namespace {
const int kStackDataAllocatorSize = 128;
class StackDataAllocator : public BuiltinDataAllocator {
 public:
  void* Allocate(size_t size) override {
    if (size > kStackDataAllocatorSize) {
 80a1dc8:	2980      	cmp	r1, #128	; 0x80
      return nullptr;
    } else {
      return data_;
 80a1dca:	bf94      	ite	ls
 80a1dcc:	3004      	addls	r0, #4
const int kStackDataAllocatorSize = 128;
class StackDataAllocator : public BuiltinDataAllocator {
 public:
  void* Allocate(size_t size) override {
    if (size > kStackDataAllocatorSize) {
      return nullptr;
 80a1dce:	2000      	movhi	r0, #0
    } else {
      return data_;
    }
  }
 80a1dd0:	4770      	bx	lr

080a1dd2 <_ZN6tflite12_GLOBAL__N_118StackDataAllocator10DeallocateEPv>:
  void Deallocate(void* data) override {
 80a1dd2:	4770      	bx	lr

080a1dd4 <_ZN6tflite12_GLOBAL__N_118StackDataAllocatorD1Ev>:
#include "tensorflow/lite/experimental/micro/compatibility.h"

namespace tflite {
namespace {
const int kStackDataAllocatorSize = 128;
class StackDataAllocator : public BuiltinDataAllocator {
 80a1dd4:	4770      	bx	lr

080a1dd6 <_ZN6tflite12_GLOBAL__N_113ReportOpErrorEP13TfLiteContextPKcz>:
  } else {
    return EnumNameBuiltinOperator(BuiltinOperator(registration->builtin_code));
  }
}

void ReportOpError(struct TfLiteContext* context, const char* format, ...) {
 80a1dd6:	b40e      	push	{r1, r2, r3}
 80a1dd8:	b503      	push	{r0, r1, lr}
  MicroInterpreter* interpreter =
      static_cast<MicroInterpreter*>(context->impl_);
 80a1dda:	68c3      	ldr	r3, [r0, #12]
  } else {
    return EnumNameBuiltinOperator(BuiltinOperator(registration->builtin_code));
  }
}

void ReportOpError(struct TfLiteContext* context, const char* format, ...) {
 80a1ddc:	aa03      	add	r2, sp, #12
 80a1dde:	6898      	ldr	r0, [r3, #8]
 80a1de0:	f852 1b04 	ldr.w	r1, [r2], #4
  MicroInterpreter* interpreter =
      static_cast<MicroInterpreter*>(context->impl_);
  va_list args;
  va_start(args, format);
  interpreter->error_reporter()->Report(format, args);
 80a1de4:	6803      	ldr	r3, [r0, #0]

void ReportOpError(struct TfLiteContext* context, const char* format, ...) {
  MicroInterpreter* interpreter =
      static_cast<MicroInterpreter*>(context->impl_);
  va_list args;
  va_start(args, format);
 80a1de6:	9201      	str	r2, [sp, #4]
  interpreter->error_reporter()->Report(format, args);
 80a1de8:	689b      	ldr	r3, [r3, #8]
 80a1dea:	4798      	blx	r3
  va_end(args);
}
 80a1dec:	b002      	add	sp, #8
 80a1dee:	f85d eb04 	ldr.w	lr, [sp], #4
 80a1df2:	b003      	add	sp, #12
 80a1df4:	4770      	bx	lr

080a1df6 <_ZN6tflite12_GLOBAL__N_118StackDataAllocatorD0Ev>:
#include "tensorflow/lite/experimental/micro/compatibility.h"

namespace tflite {
namespace {
const int kStackDataAllocatorSize = 128;
class StackDataAllocator : public BuiltinDataAllocator {
 80a1df6:	b510      	push	{r4, lr}
 80a1df8:	4604      	mov	r4, r0
 80a1dfa:	2184      	movs	r1, #132	; 0x84
 80a1dfc:	f00e fdfd 	bl	80b09fa <_ZdlPvj>
 80a1e00:	4620      	mov	r0, r4
 80a1e02:	bd10      	pop	{r4, pc}

080a1e04 <_ZN6tflite16MicroInterpreter15AllocateTensorsEv>:
TfLiteStatus MicroInterpreter::RegisterPreallocatedInput(uint8_t* buffer,
                                                         size_t input_index) {
  return allocator_.RegisterPreallocatedInput(buffer, input_index);
}

TfLiteStatus MicroInterpreter::AllocateTensors() {
 80a1e04:	b510      	push	{r4, lr}
 80a1e06:	4604      	mov	r4, r0
  TfLiteStatus status = allocator_.AllocateTensors();
 80a1e08:	3044      	adds	r0, #68	; 0x44
 80a1e0a:	f7ff fe15 	bl	80a1a38 <_ZN6tflite14MicroAllocator15AllocateTensorsEv>
 80a1e0e:	2301      	movs	r3, #1
  TF_LITE_ENSURE_OK(&context_, status);
 80a1e10:	b910      	cbnz	r0, 80a1e18 <_ZN6tflite16MicroInterpreter15AllocateTensorsEv+0x14>
  tensors_allocated_ = true;
 80a1e12:	f884 3070 	strb.w	r3, [r4, #112]	; 0x70
  return kTfLiteOk;
 80a1e16:	bd10      	pop	{r4, pc}
  return allocator_.RegisterPreallocatedInput(buffer, input_index);
}

TfLiteStatus MicroInterpreter::AllocateTensors() {
  TfLiteStatus status = allocator_.AllocateTensors();
  TF_LITE_ENSURE_OK(&context_, status);
 80a1e18:	4618      	mov	r0, r3
  tensors_allocated_ = true;
  return kTfLiteOk;
}
 80a1e1a:	bd10      	pop	{r4, pc}

080a1e1c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>:
// "tables" use an offset table (possibly shared) that allows fields to be
// omitted and added at will, but uses an extra indirection to read.
class Table {
 public:
  const uint8_t *GetVTable() const {
    return data_ - ReadScalar<soffset_t>(data_);
 80a1e1c:	6803      	ldr	r3, [r0, #0]
 80a1e1e:	1ac3      	subs	r3, r0, r3
    auto vtable = GetVTable();
    // The first element is the size of the vtable (fields + type id + itself).
    auto vtsize = ReadScalar<voffset_t>(vtable);
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
 80a1e20:	881a      	ldrh	r2, [r3, #0]
 80a1e22:	428a      	cmp	r2, r1
 80a1e24:	bf8c      	ite	hi
 80a1e26:	5a5b      	ldrhhi	r3, [r3, r1]
 80a1e28:	2300      	movls	r3, #0
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
  }

  template<typename P> P GetPointer(voffset_t field) {
    auto field_offset = GetOptionalFieldOffset(field);
    auto p = data_ + field_offset;
 80a1e2a:	18c2      	adds	r2, r0, r3
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
 80a1e2c:	b113      	cbz	r3, 80a1e34 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t+0x18>
 80a1e2e:	58c3      	ldr	r3, [r0, r3]
 80a1e30:	18d0      	adds	r0, r2, r3
 80a1e32:	4770      	bx	lr
 80a1e34:	4618      	mov	r0, r3
  }
 80a1e36:	4770      	bx	lr

080a1e38 <_ZN6tflite16MicroInterpreterC1EPKNS_5ModelERKNS_10OpResolverEPhjPNS_13ErrorReporterE>:
  va_end(args);
}

}  // namespace

MicroInterpreter::MicroInterpreter(const Model* model,
 80a1e38:	e92d 41f3 	stmdb	sp!, {r0, r1, r4, r5, r6, r7, r8, lr}
 80a1e3c:	4604      	mov	r4, r0
 80a1e3e:	460e      	mov	r6, r1
 80a1e40:	4698      	mov	r8, r3
 80a1e42:	9d09      	ldr	r5, [sp, #36]	; 0x24
      op_resolver_(op_resolver),
      error_reporter_(error_reporter),
      context_(),
      allocator_(&context_, model_, tensor_arena, tensor_arena_size,
                 error_reporter_),
      tensors_allocated_(false) {
 80a1e44:	f100 070c 	add.w	r7, r0, #12
 80a1e48:	6021      	str	r1, [r4, #0]
 80a1e4a:	6042      	str	r2, [r0, #4]
 80a1e4c:	6085      	str	r5, [r0, #8]
 80a1e4e:	2238      	movs	r2, #56	; 0x38
 80a1e50:	2100      	movs	r1, #0
 80a1e52:	4638      	mov	r0, r7
 80a1e54:	f011 fe44 	bl	80b3ae0 <memset>
 80a1e58:	9b08      	ldr	r3, [sp, #32]
 80a1e5a:	4632      	mov	r2, r6
 80a1e5c:	4639      	mov	r1, r7
 80a1e5e:	9300      	str	r3, [sp, #0]
 80a1e60:	9501      	str	r5, [sp, #4]
 80a1e62:	4643      	mov	r3, r8
 80a1e64:	f104 0044 	add.w	r0, r4, #68	; 0x44
 80a1e68:	2700      	movs	r7, #0
 80a1e6a:	f7ff fc7f 	bl	80a176c <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE>
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
 80a1e6e:	4630      	mov	r0, r6
 80a1e70:	f884 7070 	strb.w	r7, [r4, #112]	; 0x70
 80a1e74:	2108      	movs	r1, #8
 80a1e76:	f7ff ffd1 	bl	80a1e1c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
  auto* subgraphs = model->subgraphs();
  if (subgraphs->size() != 1) {
 80a1e7a:	6806      	ldr	r6, [r0, #0]
 80a1e7c:	2e01      	cmp	r6, #1
 80a1e7e:	d007      	beq.n	80a1e90 <_ZN6tflite16MicroInterpreterC1EPKNS_5ModelERKNS_10OpResolverEPhjPNS_13ErrorReporterE+0x58>
    error_reporter->Report("Only 1 subgraph is currently supported.\n");
 80a1e80:	490f      	ldr	r1, [pc, #60]	; (80a1ec0 <_ZN6tflite16MicroInterpreterC1EPKNS_5ModelERKNS_10OpResolverEPhjPNS_13ErrorReporterE+0x88>)
 80a1e82:	4628      	mov	r0, r5
 80a1e84:	f7fe fa92 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
    initialization_status_ = kTfLiteError;
 80a1e88:	2301      	movs	r3, #1
 80a1e8a:	f884 3071 	strb.w	r3, [r4, #113]	; 0x71
    return;
 80a1e8e:	e013      	b.n	80a1eb8 <_ZN6tflite16MicroInterpreterC1EPKNS_5ModelERKNS_10OpResolverEPhjPNS_13ErrorReporterE+0x80>
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
 80a1e90:	6843      	ldr	r3, [r0, #4]
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
 80a1e92:	1d05      	adds	r5, r0, #4
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
 80a1e94:	441d      	add	r5, r3
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
 80a1e96:	2104      	movs	r1, #4
  }
  subgraph_ = (*subgraphs)[0];
 80a1e98:	67e5      	str	r5, [r4, #124]	; 0x7c
 80a1e9a:	4628      	mov	r0, r5
 80a1e9c:	f7ff ffbe 	bl	80a1e1c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
 80a1ea0:	210a      	movs	r1, #10
  tensors_ = subgraph_->tensors();
 80a1ea2:	6760      	str	r0, [r4, #116]	; 0x74
 80a1ea4:	4628      	mov	r0, r5
 80a1ea6:	f7ff ffb9 	bl	80a1e1c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
  operators_ = subgraph_->operators();

  context_.impl_ = static_cast<void*>(this);
  context_.ReportError = ReportOpError;
 80a1eaa:	4b06      	ldr	r3, [pc, #24]	; (80a1ec4 <_ZN6tflite16MicroInterpreterC1EPKNS_5ModelERKNS_10OpResolverEPhjPNS_13ErrorReporterE+0x8c>)
    initialization_status_ = kTfLiteError;
    return;
  }
  subgraph_ = (*subgraphs)[0];
  tensors_ = subgraph_->tensors();
  operators_ = subgraph_->operators();
 80a1eac:	67a0      	str	r0, [r4, #120]	; 0x78

  context_.impl_ = static_cast<void*>(this);
 80a1eae:	61a4      	str	r4, [r4, #24]
  context_.ReportError = ReportOpError;
 80a1eb0:	6223      	str	r3, [r4, #32]
  context_.recommended_num_threads = 1;
 80a1eb2:	6326      	str	r6, [r4, #48]	; 0x30
      if (thisTensor->allocation_type == kTfLiteMmapRo)
        CorrectTensorEndianness(thisTensor);
    }
  }

  initialization_status_ = kTfLiteOk;
 80a1eb4:	f884 7071 	strb.w	r7, [r4, #113]	; 0x71
}
 80a1eb8:	4620      	mov	r0, r4
 80a1eba:	b002      	add	sp, #8
 80a1ebc:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80a1ec0:	080b53ec 	.word	0x080b53ec
 80a1ec4:	080a1dd7 	.word	0x080a1dd7

080a1ec8 <_ZN6tflite16MicroInterpreter6outputEj>:
    return nullptr;
  }
  return &(context_.tensors[inputs->Get(index)]);
}

TfLiteTensor* MicroInterpreter::output(size_t index) {
 80a1ec8:	b538      	push	{r3, r4, r5, lr}
 80a1eca:	460d      	mov	r5, r1
 80a1ecc:	4604      	mov	r4, r0
 80a1ece:	2108      	movs	r1, #8
 80a1ed0:	6fc0      	ldr	r0, [r0, #124]	; 0x7c
 80a1ed2:	f7ff ffa3 	bl	80a1e1c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
 80a1ed6:	6803      	ldr	r3, [r0, #0]
  const flatbuffers::Vector<int32_t>* outputs = subgraph_->outputs();
  const size_t length = outputs->size();
  if ((index < 0) || (index >= outputs->size())) {
 80a1ed8:	42ab      	cmp	r3, r5
 80a1eda:	d806      	bhi.n	80a1eea <_ZN6tflite16MicroInterpreter6outputEj+0x22>
    error_reporter_->Report("Output index %d out of range (length is %d)",
                            index, length);
 80a1edc:	462a      	mov	r2, r5
 80a1ede:	4907      	ldr	r1, [pc, #28]	; (80a1efc <_ZN6tflite16MicroInterpreter6outputEj+0x34>)
 80a1ee0:	68a0      	ldr	r0, [r4, #8]
 80a1ee2:	f7fe fa63 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return nullptr;
 80a1ee6:	2000      	movs	r0, #0
 80a1ee8:	bd38      	pop	{r3, r4, r5, pc}
  }
  return &(context_.tensors[outputs->Get(index)]);
 80a1eea:	4629      	mov	r1, r5
 80a1eec:	f7ff fc08 	bl	80a1700 <_ZNK11flatbuffers6VectorIlE3GetEm>
 80a1ef0:	6962      	ldr	r2, [r4, #20]
 80a1ef2:	2338      	movs	r3, #56	; 0x38
 80a1ef4:	fb03 2000 	mla	r0, r3, r0, r2
}
 80a1ef8:	bd38      	pop	{r3, r4, r5, pc}
 80a1efa:	bf00      	nop
 80a1efc:	080b57d3 	.word	0x080b57d3

080a1f00 <_ZN6tflite16MicroInterpreter5inputEj>:
    }
  }
  return status;
}

TfLiteTensor* MicroInterpreter::input(size_t index) {
 80a1f00:	b538      	push	{r3, r4, r5, lr}
 80a1f02:	460d      	mov	r5, r1
 80a1f04:	4604      	mov	r4, r0
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
 80a1f06:	2106      	movs	r1, #6
 80a1f08:	6fc0      	ldr	r0, [r0, #124]	; 0x7c
 80a1f0a:	f7ff ff87 	bl	80a1e1c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
 80a1f0e:	6803      	ldr	r3, [r0, #0]
  const flatbuffers::Vector<int32_t>* inputs = subgraph_->inputs();
  const size_t length = inputs->size();
  if ((index < 0) || (index >= length)) {
 80a1f10:	429d      	cmp	r5, r3
 80a1f12:	d306      	bcc.n	80a1f22 <_ZN6tflite16MicroInterpreter5inputEj+0x22>
    error_reporter_->Report("Input index %d out of range (length is %d)", index,
                            length);
 80a1f14:	462a      	mov	r2, r5
 80a1f16:	4907      	ldr	r1, [pc, #28]	; (80a1f34 <_ZN6tflite16MicroInterpreter5inputEj+0x34>)
 80a1f18:	68a0      	ldr	r0, [r4, #8]
 80a1f1a:	f7fe fa47 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return nullptr;
 80a1f1e:	2000      	movs	r0, #0
 80a1f20:	bd38      	pop	{r3, r4, r5, pc}
  }
  return &(context_.tensors[inputs->Get(index)]);
 80a1f22:	4629      	mov	r1, r5
 80a1f24:	f7ff fbec 	bl	80a1700 <_ZNK11flatbuffers6VectorIlE3GetEm>
 80a1f28:	6962      	ldr	r2, [r4, #20]
 80a1f2a:	2338      	movs	r3, #56	; 0x38
 80a1f2c:	fb03 2000 	mla	r0, r3, r0, r2
}
 80a1f30:	bd38      	pop	{r3, r4, r5, pc}
 80a1f32:	bf00      	nop
 80a1f34:	080b57ff 	.word	0x080b57ff

080a1f38 <_ZN6tflite16MicroInterpreter6InvokeEv>:
  TF_LITE_ENSURE_OK(&context_, status);
  tensors_allocated_ = true;
  return kTfLiteOk;
}

TfLiteStatus MicroInterpreter::Invoke() {
 80a1f38:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  if (initialization_status_ != kTfLiteOk) {
 80a1f3c:	f890 5071 	ldrb.w	r5, [r0, #113]	; 0x71
  TF_LITE_ENSURE_OK(&context_, status);
  tensors_allocated_ = true;
  return kTfLiteOk;
}

TfLiteStatus MicroInterpreter::Invoke() {
 80a1f40:	b0c5      	sub	sp, #276	; 0x114
 80a1f42:	4604      	mov	r4, r0
  if (initialization_status_ != kTfLiteOk) {
 80a1f44:	b125      	cbz	r5, 80a1f50 <_ZN6tflite16MicroInterpreter6InvokeEv+0x18>
    error_reporter_->Report("Invoke() called after initialization failed\n");
 80a1f46:	4976      	ldr	r1, [pc, #472]	; (80a2120 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1e8>)
 80a1f48:	6880      	ldr	r0, [r0, #8]
 80a1f4a:	f7fe fa2f 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
 80a1f4e:	e0a2      	b.n	80a2096 <_ZN6tflite16MicroInterpreter6InvokeEv+0x15e>
    return kTfLiteError;
  }

  // Ensure tensors are allocated before the interpreter is invoked to avoid
  // difficult to debug segfaults.
  if (!tensors_allocated_) {
 80a1f50:	f890 3070 	ldrb.w	r3, [r0, #112]	; 0x70
 80a1f54:	b90b      	cbnz	r3, 80a1f5a <_ZN6tflite16MicroInterpreter6InvokeEv+0x22>
    AllocateTensors();
 80a1f56:	f7ff ff55 	bl	80a1e04 <_ZN6tflite16MicroInterpreter15AllocateTensorsEv>
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
 80a1f5a:	2106      	movs	r1, #6
 80a1f5c:	6820      	ldr	r0, [r4, #0]
 80a1f5e:	f7ff ff5d 	bl	80a1e1c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
  }
  TfLiteStatus status = kTfLiteOk;
  auto opcodes = model_->operator_codes();
  for (size_t i = 0; i < operators_->size(); ++i) {
 80a1f62:	2600      	movs	r6, #0
 80a1f64:	4683      	mov	fp, r0
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
 80a1f66:	46b2      	mov	sl, r6
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
 80a1f68:	1d03      	adds	r3, r0, #4
 80a1f6a:	9304      	str	r3, [sp, #16]
 80a1f6c:	6fa3      	ldr	r3, [r4, #120]	; 0x78
 80a1f6e:	681a      	ldr	r2, [r3, #0]
 80a1f70:	4296      	cmp	r6, r2
 80a1f72:	f080 80d1 	bcs.w	80a2118 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1e0>
 80a1f76:	3304      	adds	r3, #4
template<typename T> struct IndirectHelper<Offset<T>> {
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
 80a1f78:	eb03 0286 	add.w	r2, r3, r6, lsl #2
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
 80a1f7c:	f853 3026 	ldr.w	r3, [r3, r6, lsl #2]
 80a1f80:	18d7      	adds	r7, r2, r3
// "tables" use an offset table (possibly shared) that allows fields to be
// omitted and added at will, but uses an extra indirection to read.
class Table {
 public:
  const uint8_t *GetVTable() const {
    return data_ - ReadScalar<soffset_t>(data_);
 80a1f82:	58d3      	ldr	r3, [r2, r3]
 80a1f84:	1afb      	subs	r3, r7, r3
    auto vtable = GetVTable();
    // The first element is the size of the vtable (fields + type id + itself).
    auto vtsize = ReadScalar<voffset_t>(vtable);
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
 80a1f86:	881a      	ldrh	r2, [r3, #0]
 80a1f88:	2a04      	cmp	r2, #4
 80a1f8a:	d904      	bls.n	80a1f96 <_ZN6tflite16MicroInterpreter6InvokeEv+0x5e>
 80a1f8c:	889b      	ldrh	r3, [r3, #4]
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
 80a1f8e:	b12b      	cbz	r3, 80a1f9c <_ZN6tflite16MicroInterpreter6InvokeEv+0x64>
 80a1f90:	f857 8003 	ldr.w	r8, [r7, r3]
 80a1f94:	e003      	b.n	80a1f9e <_ZN6tflite16MicroInterpreter6InvokeEv+0x66>
 80a1f96:	f04f 0800 	mov.w	r8, #0
 80a1f9a:	e000      	b.n	80a1f9e <_ZN6tflite16MicroInterpreter6InvokeEv+0x66>
 80a1f9c:	4698      	mov	r8, r3
    const auto* op = operators_->Get(i);
    size_t index = op->opcode_index();
    if (index < 0 || index >= opcodes->size()) {
 80a1f9e:	f8db 3000 	ldr.w	r3, [fp]
 80a1fa2:	4543      	cmp	r3, r8
 80a1fa4:	d802      	bhi.n	80a1fac <_ZN6tflite16MicroInterpreter6InvokeEv+0x74>
      error_reporter_->Report("Missing registration for opcode_index %d\n",
                              index);
 80a1fa6:	4642      	mov	r2, r8
 80a1fa8:	495e      	ldr	r1, [pc, #376]	; (80a2124 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1ec>)
 80a1faa:	e025      	b.n	80a1ff8 <_ZN6tflite16MicroInterpreter6InvokeEv+0xc0>
template<typename T> struct IndirectHelper<Offset<T>> {
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
 80a1fac:	9b04      	ldr	r3, [sp, #16]
      return kTfLiteError;
    }
    auto opcode = (*opcodes)[index];
    const TfLiteRegistration* registration = nullptr;
    status = GetRegistrationFromOpCode(opcode, op_resolver_, error_reporter_,
                                       &registration);
 80a1fae:	68a2      	ldr	r2, [r4, #8]
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
 80a1fb0:	f853 e028 	ldr.w	lr, [r3, r8, lsl #2]
template<typename T> struct IndirectHelper<Offset<T>> {
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
 80a1fb4:	eb03 0088 	add.w	r0, r3, r8, lsl #2
      error_reporter_->Report("Missing registration for opcode_index %d\n",
                              index);
      return kTfLiteError;
    }
    auto opcode = (*opcodes)[index];
    const TfLiteRegistration* registration = nullptr;
 80a1fb8:	ab44      	add	r3, sp, #272	; 0x110
 80a1fba:	f843 adf4 	str.w	sl, [r3, #-244]!
    status = GetRegistrationFromOpCode(opcode, op_resolver_, error_reporter_,
                                       &registration);
 80a1fbe:	6861      	ldr	r1, [r4, #4]
 80a1fc0:	4470      	add	r0, lr
 80a1fc2:	f7ff f9d7 	bl	80a1374 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration>
    if (status != kTfLiteOk) {
 80a1fc6:	2800      	cmp	r0, #0
 80a1fc8:	d167      	bne.n	80a209a <_ZN6tflite16MicroInterpreter6InvokeEv+0x162>
      return status;
    }
    if (registration == nullptr) {
 80a1fca:	9b07      	ldr	r3, [sp, #28]
 80a1fcc:	b913      	cbnz	r3, 80a1fd4 <_ZN6tflite16MicroInterpreter6InvokeEv+0x9c>
      error_reporter_->Report("Skipping op for opcode_index %d\n", index);
 80a1fce:	4642      	mov	r2, r8
 80a1fd0:	4955      	ldr	r1, [pc, #340]	; (80a2128 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1f0>)
 80a1fd2:	e011      	b.n	80a1ff8 <_ZN6tflite16MicroInterpreter6InvokeEv+0xc0>
      return kTfLiteError;
    }
    BuiltinOperator op_type =
        static_cast<BuiltinOperator>(registration->builtin_code);
 80a1fd4:	f893 8014 	ldrb.w	r8, [r3, #20]

    if (op_type != BuiltinOperator_CUSTOM && op->custom_options()) {
 80a1fd8:	f1b8 0f20 	cmp.w	r8, #32
 80a1fdc:	d010      	beq.n	80a2000 <_ZN6tflite16MicroInterpreter6InvokeEv+0xc8>
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
 80a1fde:	210e      	movs	r1, #14
 80a1fe0:	4638      	mov	r0, r7
 80a1fe2:	f7ff ff1b 	bl	80a1e1c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
 80a1fe6:	b158      	cbz	r0, 80a2000 <_ZN6tflite16MicroInterpreter6InvokeEv+0xc8>
  };
  return names;
}

inline const char *EnumNameBuiltinOperator(BuiltinOperator e) {
  if (e < BuiltinOperator_ADD || e > BuiltinOperator_NON_MAX_SUPPRESSION_V5) return "";
 80a1fe8:	f1b8 0f79 	cmp.w	r8, #121	; 0x79
 80a1fec:	f200 8092 	bhi.w	80a2114 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1dc>
  const size_t index = static_cast<size_t>(e);
  return EnumNamesBuiltinOperator()[index];
 80a1ff0:	4b4e      	ldr	r3, [pc, #312]	; (80a212c <_ZN6tflite16MicroInterpreter6InvokeEv+0x1f4>)
 80a1ff2:	f853 2028 	ldr.w	r2, [r3, r8, lsl #2]
      error_reporter_->Report(
          "Unsupported behavior: found builtin operator %s with custom "
          "options.\n",
          EnumNameBuiltinOperator(op_type));
 80a1ff6:	494e      	ldr	r1, [pc, #312]	; (80a2130 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1f8>)
 80a1ff8:	68a0      	ldr	r0, [r4, #8]
 80a1ffa:	f7fe f9d7 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
 80a1ffe:	e04a      	b.n	80a2096 <_ZN6tflite16MicroInterpreter6InvokeEv+0x15e>
#include "tensorflow/lite/experimental/micro/compatibility.h"

namespace tflite {
namespace {
const int kStackDataAllocatorSize = 128;
class StackDataAllocator : public BuiltinDataAllocator {
 80a2000:	4b4c      	ldr	r3, [pc, #304]	; (80a2134 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1fc>)
 80a2002:	210e      	movs	r1, #14
 80a2004:	4638      	mov	r0, r7
 80a2006:	9323      	str	r3, [sp, #140]	; 0x8c
      return kTfLiteError;
    }
    StackDataAllocator stack_data_allocator;
    const char* custom_data = nullptr;
    size_t custom_data_size = 0;
    unsigned char* builtin_data = nullptr;
 80a2008:	f8cd a020 	str.w	sl, [sp, #32]
 80a200c:	f7ff ff06 	bl	80a1e1c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
    if (op->custom_options()) {
 80a2010:	2800      	cmp	r0, #0
 80a2012:	d044      	beq.n	80a209e <_ZN6tflite16MicroInterpreter6InvokeEv+0x166>
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
 80a2014:	1d03      	adds	r3, r0, #4
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
 80a2016:	f8d0 9000 	ldr.w	r9, [r0]
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
 80a201a:	9303      	str	r3, [sp, #12]
                                        (void**)(&builtin_data)));
    }

    const char* init_data;
    size_t init_data_size;
    if (registration->builtin_code == BuiltinOperator_CUSTOM) {
 80a201c:	9807      	ldr	r0, [sp, #28]
 80a201e:	6942      	ldr	r2, [r0, #20]
    } else {
      init_data = reinterpret_cast<const char*>(builtin_data);
      init_data_size = 0;
    }
    void* user_data = nullptr;
    if (registration->init) {
 80a2020:	6803      	ldr	r3, [r0, #0]
                                        (void**)(&builtin_data)));
    }

    const char* init_data;
    size_t init_data_size;
    if (registration->builtin_code == BuiltinOperator_CUSTOM) {
 80a2022:	2a20      	cmp	r2, #32
      init_data = custom_data;
      init_data_size = custom_data_size;
    } else {
      init_data = reinterpret_cast<const char*>(builtin_data);
 80a2024:	bf19      	ittee	ne
 80a2026:	9908      	ldrne	r1, [sp, #32]
      init_data_size = 0;
 80a2028:	2200      	movne	r2, #0

    const char* init_data;
    size_t init_data_size;
    if (registration->builtin_code == BuiltinOperator_CUSTOM) {
      init_data = custom_data;
      init_data_size = custom_data_size;
 80a202a:	464a      	moveq	r2, r9
    }

    const char* init_data;
    size_t init_data_size;
    if (registration->builtin_code == BuiltinOperator_CUSTOM) {
      init_data = custom_data;
 80a202c:	9903      	ldreq	r1, [sp, #12]
    } else {
      init_data = reinterpret_cast<const char*>(builtin_data);
      init_data_size = 0;
    }
    void* user_data = nullptr;
    if (registration->init) {
 80a202e:	2b00      	cmp	r3, #0
 80a2030:	d042      	beq.n	80a20b8 <_ZN6tflite16MicroInterpreter6InvokeEv+0x180>
      user_data = registration->init(&context_, init_data, init_data_size);
 80a2032:	f104 000c 	add.w	r0, r4, #12
 80a2036:	4798      	blx	r3
 80a2038:	4680      	mov	r8, r0
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
 80a203a:	2106      	movs	r1, #6
 80a203c:	4638      	mov	r0, r7
 80a203e:	f7ff feed 	bl	80a1e1c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
 80a2042:	2108      	movs	r1, #8
 80a2044:	9005      	str	r0, [sp, #20]
 80a2046:	4638      	mov	r0, r7
 80a2048:	f7ff fee8 	bl	80a1e1c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
    TfLiteIntArray* temporaries_array =
        reinterpret_cast<TfLiteIntArray*>(temporaries_data);
    temporaries_array->size = 0;

    TfLiteNode node;
    node.inputs = inputs_array;
 80a204c:	9a05      	ldr	r2, [sp, #20]
    node.outputs = outputs_array;
    node.temporaries = temporaries_array;
    node.user_data = user_data;
    node.builtin_data = reinterpret_cast<void*>(builtin_data);
    node.custom_initial_data = custom_data;
 80a204e:	9b03      	ldr	r3, [sp, #12]
    TfLiteIntArray* temporaries_array =
        reinterpret_cast<TfLiteIntArray*>(temporaries_data);
    temporaries_array->size = 0;

    TfLiteNode node;
    node.inputs = inputs_array;
 80a2050:	9209      	str	r2, [sp, #36]	; 0x24
    node.outputs = outputs_array;
    node.temporaries = temporaries_array;
 80a2052:	aa12      	add	r2, sp, #72	; 0x48
 80a2054:	920c      	str	r2, [sp, #48]	; 0x30
    node.user_data = user_data;
    node.builtin_data = reinterpret_cast<void*>(builtin_data);
 80a2056:	9a08      	ldr	r2, [sp, #32]
    node.custom_initial_data = custom_data;
 80a2058:	930f      	str	r3, [sp, #60]	; 0x3c
    node.custom_initial_data_size = custom_data_size;
    node.delegate = nullptr;
    if (registration->prepare) {
 80a205a:	9b07      	ldr	r3, [sp, #28]

    TfLiteNode node;
    node.inputs = inputs_array;
    node.outputs = outputs_array;
    node.temporaries = temporaries_array;
    node.user_data = user_data;
 80a205c:	f8cd 8034 	str.w	r8, [sp, #52]	; 0x34
    node.builtin_data = reinterpret_cast<void*>(builtin_data);
 80a2060:	920e      	str	r2, [sp, #56]	; 0x38
    node.custom_initial_data = custom_data;
    node.custom_initial_data_size = custom_data_size;
 80a2062:	f8cd 9040 	str.w	r9, [sp, #64]	; 0x40
    node.delegate = nullptr;
 80a2066:	f8cd a044 	str.w	sl, [sp, #68]	; 0x44
    if (registration->prepare) {
 80a206a:	689b      	ldr	r3, [r3, #8]

    const int kMaxTemporaries = 16;
    int temporaries_data[kMaxTemporaries + 1];
    TfLiteIntArray* temporaries_array =
        reinterpret_cast<TfLiteIntArray*>(temporaries_data);
    temporaries_array->size = 0;
 80a206c:	f8cd a048 	str.w	sl, [sp, #72]	; 0x48

    TfLiteNode node;
    node.inputs = inputs_array;
    node.outputs = outputs_array;
 80a2070:	900a      	str	r0, [sp, #40]	; 0x28
    node.user_data = user_data;
    node.builtin_data = reinterpret_cast<void*>(builtin_data);
    node.custom_initial_data = custom_data;
    node.custom_initial_data_size = custom_data_size;
    node.delegate = nullptr;
    if (registration->prepare) {
 80a2072:	b35b      	cbz	r3, 80a20cc <_ZN6tflite16MicroInterpreter6InvokeEv+0x194>
      TfLiteStatus prepare_status = registration->prepare(&context_, &node);
 80a2074:	a909      	add	r1, sp, #36	; 0x24
 80a2076:	f104 000c 	add.w	r0, r4, #12
 80a207a:	4798      	blx	r3
      if (prepare_status != kTfLiteOk) {
 80a207c:	4601      	mov	r1, r0
 80a207e:	b328      	cbz	r0, 80a20cc <_ZN6tflite16MicroInterpreter6InvokeEv+0x194>
        error_reporter_->Report(
 80a2080:	9a07      	ldr	r2, [sp, #28]
 80a2082:	68a0      	ldr	r0, [r4, #8]

  TF_LITE_REMOVE_VIRTUAL_DELETE
};

const char* OpNameFromRegistration(const TfLiteRegistration* registration) {
  if (registration->builtin_code == BuiltinOperator_CUSTOM) {
 80a2084:	6953      	ldr	r3, [r2, #20]
 80a2086:	2b20      	cmp	r3, #32
 80a2088:	d118      	bne.n	80a20bc <_ZN6tflite16MicroInterpreter6InvokeEv+0x184>
    return registration->custom_name;
 80a208a:	6992      	ldr	r2, [r2, #24]
    if (registration->prepare) {
      TfLiteStatus prepare_status = registration->prepare(&context_, &node);
      if (prepare_status != kTfLiteOk) {
        error_reporter_->Report(
            "Node %s (number %d) failed to prepare with status %d",
            OpNameFromRegistration(registration), i, prepare_status);
 80a208c:	4633      	mov	r3, r6
 80a208e:	9100      	str	r1, [sp, #0]
 80a2090:	4929      	ldr	r1, [pc, #164]	; (80a2138 <_ZN6tflite16MicroInterpreter6InvokeEv+0x200>)
    if (registration->invoke) {
      TfLiteStatus invoke_status = registration->invoke(&context_, &node);
      if (invoke_status != kTfLiteOk) {
        error_reporter_->Report(
            "Node %s (number %d) failed to invoke with status %d",
            OpNameFromRegistration(registration), i, invoke_status);
 80a2092:	f7fe f98b 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
    if (op_type != BuiltinOperator_CUSTOM && op->custom_options()) {
      error_reporter_->Report(
          "Unsupported behavior: found builtin operator %s with custom "
          "options.\n",
          EnumNameBuiltinOperator(op_type));
      return kTfLiteError;
 80a2096:	2501      	movs	r5, #1
 80a2098:	e03e      	b.n	80a2118 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1e0>
      return kTfLiteError;
    }
    auto opcode = (*opcodes)[index];
    const TfLiteRegistration* registration = nullptr;
    status = GetRegistrationFromOpCode(opcode, op_resolver_, error_reporter_,
                                       &registration);
 80a209a:	4605      	mov	r5, r0
 80a209c:	e03c      	b.n	80a2118 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1e0>
    unsigned char* builtin_data = nullptr;
    if (op->custom_options()) {
      custom_data = reinterpret_cast<const char*>(op->custom_options()->data());
      custom_data_size = op->custom_options()->size();
    } else {
      TF_LITE_ENSURE_STATUS(ParseOpData(op, op_type, error_reporter_,
 80a209e:	ab08      	add	r3, sp, #32
 80a20a0:	9300      	str	r3, [sp, #0]
 80a20a2:	68a2      	ldr	r2, [r4, #8]
 80a20a4:	ab23      	add	r3, sp, #140	; 0x8c
 80a20a6:	4641      	mov	r1, r8
 80a20a8:	4638      	mov	r0, r7
 80a20aa:	f7fe fa63 	bl	80a0574 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv>
 80a20ae:	2800      	cmp	r0, #0
 80a20b0:	d1f1      	bne.n	80a2096 <_ZN6tflite16MicroInterpreter6InvokeEv+0x15e>
          EnumNameBuiltinOperator(op_type));
      return kTfLiteError;
    }
    StackDataAllocator stack_data_allocator;
    const char* custom_data = nullptr;
    size_t custom_data_size = 0;
 80a20b2:	4681      	mov	r9, r0
          "options.\n",
          EnumNameBuiltinOperator(op_type));
      return kTfLiteError;
    }
    StackDataAllocator stack_data_allocator;
    const char* custom_data = nullptr;
 80a20b4:	9003      	str	r0, [sp, #12]
 80a20b6:	e7b1      	b.n	80a201c <_ZN6tflite16MicroInterpreter6InvokeEv+0xe4>
      init_data_size = custom_data_size;
    } else {
      init_data = reinterpret_cast<const char*>(builtin_data);
      init_data_size = 0;
    }
    void* user_data = nullptr;
 80a20b8:	4698      	mov	r8, r3
 80a20ba:	e7be      	b.n	80a203a <_ZN6tflite16MicroInterpreter6InvokeEv+0x102>
  };
  return names;
}

inline const char *EnumNameBuiltinOperator(BuiltinOperator e) {
  if (e < BuiltinOperator_ADD || e > BuiltinOperator_NON_MAX_SUPPRESSION_V5) return "";
 80a20bc:	b2db      	uxtb	r3, r3
 80a20be:	2b79      	cmp	r3, #121	; 0x79
  const size_t index = static_cast<size_t>(e);
  return EnumNamesBuiltinOperator()[index];
 80a20c0:	bf96      	itet	ls
 80a20c2:	4a1a      	ldrls	r2, [pc, #104]	; (80a212c <_ZN6tflite16MicroInterpreter6InvokeEv+0x1f4>)
  };
  return names;
}

inline const char *EnumNameBuiltinOperator(BuiltinOperator e) {
  if (e < BuiltinOperator_ADD || e > BuiltinOperator_NON_MAX_SUPPRESSION_V5) return "";
 80a20c4:	4a1d      	ldrhi	r2, [pc, #116]	; (80a213c <_ZN6tflite16MicroInterpreter6InvokeEv+0x204>)
  const size_t index = static_cast<size_t>(e);
  return EnumNamesBuiltinOperator()[index];
 80a20c6:	f852 2023 	ldrls.w	r2, [r2, r3, lsl #2]
 80a20ca:	e7df      	b.n	80a208c <_ZN6tflite16MicroInterpreter6InvokeEv+0x154>
            OpNameFromRegistration(registration), i, prepare_status);
        return kTfLiteError;
      }
    }

    if (registration->invoke) {
 80a20cc:	9b07      	ldr	r3, [sp, #28]
 80a20ce:	68db      	ldr	r3, [r3, #12]
 80a20d0:	b1bb      	cbz	r3, 80a2102 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1ca>
      TfLiteStatus invoke_status = registration->invoke(&context_, &node);
 80a20d2:	a909      	add	r1, sp, #36	; 0x24
 80a20d4:	f104 000c 	add.w	r0, r4, #12
 80a20d8:	4798      	blx	r3
      if (invoke_status != kTfLiteOk) {
 80a20da:	4601      	mov	r1, r0
 80a20dc:	b188      	cbz	r0, 80a2102 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1ca>
        error_reporter_->Report(
 80a20de:	9a07      	ldr	r2, [sp, #28]
 80a20e0:	68a0      	ldr	r0, [r4, #8]

  TF_LITE_REMOVE_VIRTUAL_DELETE
};

const char* OpNameFromRegistration(const TfLiteRegistration* registration) {
  if (registration->builtin_code == BuiltinOperator_CUSTOM) {
 80a20e2:	6953      	ldr	r3, [r2, #20]
 80a20e4:	2b20      	cmp	r3, #32
 80a20e6:	d101      	bne.n	80a20ec <_ZN6tflite16MicroInterpreter6InvokeEv+0x1b4>
    return registration->custom_name;
 80a20e8:	6992      	ldr	r2, [r2, #24]
 80a20ea:	e006      	b.n	80a20fa <_ZN6tflite16MicroInterpreter6InvokeEv+0x1c2>
  };
  return names;
}

inline const char *EnumNameBuiltinOperator(BuiltinOperator e) {
  if (e < BuiltinOperator_ADD || e > BuiltinOperator_NON_MAX_SUPPRESSION_V5) return "";
 80a20ec:	b2db      	uxtb	r3, r3
 80a20ee:	2b79      	cmp	r3, #121	; 0x79
  const size_t index = static_cast<size_t>(e);
  return EnumNamesBuiltinOperator()[index];
 80a20f0:	bf96      	itet	ls
 80a20f2:	4a0e      	ldrls	r2, [pc, #56]	; (80a212c <_ZN6tflite16MicroInterpreter6InvokeEv+0x1f4>)
  };
  return names;
}

inline const char *EnumNameBuiltinOperator(BuiltinOperator e) {
  if (e < BuiltinOperator_ADD || e > BuiltinOperator_NON_MAX_SUPPRESSION_V5) return "";
 80a20f4:	4a11      	ldrhi	r2, [pc, #68]	; (80a213c <_ZN6tflite16MicroInterpreter6InvokeEv+0x204>)
  const size_t index = static_cast<size_t>(e);
  return EnumNamesBuiltinOperator()[index];
 80a20f6:	f852 2023 	ldrls.w	r2, [r2, r3, lsl #2]
    if (registration->invoke) {
      TfLiteStatus invoke_status = registration->invoke(&context_, &node);
      if (invoke_status != kTfLiteOk) {
        error_reporter_->Report(
            "Node %s (number %d) failed to invoke with status %d",
            OpNameFromRegistration(registration), i, invoke_status);
 80a20fa:	9100      	str	r1, [sp, #0]
 80a20fc:	4633      	mov	r3, r6
 80a20fe:	4910      	ldr	r1, [pc, #64]	; (80a2140 <_ZN6tflite16MicroInterpreter6InvokeEv+0x208>)
 80a2100:	e7c7      	b.n	80a2092 <_ZN6tflite16MicroInterpreter6InvokeEv+0x15a>
        return kTfLiteError;
      }
    }

    if (registration->free) {
 80a2102:	9b07      	ldr	r3, [sp, #28]
 80a2104:	685b      	ldr	r3, [r3, #4]
 80a2106:	b11b      	cbz	r3, 80a2110 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1d8>
      registration->free(&context_, user_data);
 80a2108:	4641      	mov	r1, r8
 80a210a:	f104 000c 	add.w	r0, r4, #12
 80a210e:	4798      	blx	r3
  if (!tensors_allocated_) {
    AllocateTensors();
  }
  TfLiteStatus status = kTfLiteOk;
  auto opcodes = model_->operator_codes();
  for (size_t i = 0; i < operators_->size(); ++i) {
 80a2110:	3601      	adds	r6, #1
 80a2112:	e72b      	b.n	80a1f6c <_ZN6tflite16MicroInterpreter6InvokeEv+0x34>
  };
  return names;
}

inline const char *EnumNameBuiltinOperator(BuiltinOperator e) {
  if (e < BuiltinOperator_ADD || e > BuiltinOperator_NON_MAX_SUPPRESSION_V5) return "";
 80a2114:	4a09      	ldr	r2, [pc, #36]	; (80a213c <_ZN6tflite16MicroInterpreter6InvokeEv+0x204>)
 80a2116:	e76e      	b.n	80a1ff6 <_ZN6tflite16MicroInterpreter6InvokeEv+0xbe>
    if (registration->free) {
      registration->free(&context_, user_data);
    }
  }
  return status;
}
 80a2118:	4628      	mov	r0, r5
 80a211a:	b045      	add	sp, #276	; 0x114
 80a211c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80a2120:	080b582a 	.word	0x080b582a
 80a2124:	080b5857 	.word	0x080b5857
 80a2128:	080b5881 	.word	0x080b5881
 80a212c:	080b49ac 	.word	0x080b49ac
 80a2130:	080b58a2 	.word	0x080b58a2
 80a2134:	080b595c 	.word	0x080b595c
 80a2138:	080b58e8 	.word	0x080b58e8
 80a213c:	080b57a6 	.word	0x080b57a6
 80a2140:	080b591d 	.word	0x080b591d

080a2144 <_ZNK6tflite22MicroMutableOpResolver6FindOpENS_15BuiltinOperatorEi>:

namespace tflite {

const TfLiteRegistration* MicroMutableOpResolver::FindOp(
    tflite::BuiltinOperator op, int version) const {
  for (int i = 0; i < registrations_len_; ++i) {
 80a2144:	f241 0304 	movw	r3, #4100	; 0x1004
#include "tensorflow/lite/experimental/micro/micro_mutable_op_resolver.h"

namespace tflite {

const TfLiteRegistration* MicroMutableOpResolver::FindOp(
    tflite::BuiltinOperator op, int version) const {
 80a2148:	b570      	push	{r4, r5, r6, lr}
  for (int i = 0; i < registrations_len_; ++i) {
 80a214a:	58c5      	ldr	r5, [r0, r3]
 80a214c:	4603      	mov	r3, r0
 80a214e:	2400      	movs	r4, #0
 80a2150:	42ac      	cmp	r4, r5
 80a2152:	da0c      	bge.n	80a216e <_ZNK6tflite22MicroMutableOpResolver6FindOpENS_15BuiltinOperatorEi+0x2a>
    const TfLiteRegistration& registration = registrations_[i];
    if ((registration.builtin_code == op) &&
 80a2154:	699e      	ldr	r6, [r3, #24]
 80a2156:	428e      	cmp	r6, r1
 80a2158:	d106      	bne.n	80a2168 <_ZNK6tflite22MicroMutableOpResolver6FindOpENS_15BuiltinOperatorEi+0x24>
 80a215a:	6a1e      	ldr	r6, [r3, #32]
 80a215c:	4296      	cmp	r6, r2
 80a215e:	d103      	bne.n	80a2168 <_ZNK6tflite22MicroMutableOpResolver6FindOpENS_15BuiltinOperatorEi+0x24>
namespace tflite {

const TfLiteRegistration* MicroMutableOpResolver::FindOp(
    tflite::BuiltinOperator op, int version) const {
  for (int i = 0; i < registrations_len_; ++i) {
    const TfLiteRegistration& registration = registrations_[i];
 80a2160:	eb00 1044 	add.w	r0, r0, r4, lsl #5
 80a2164:	3004      	adds	r0, #4
 80a2166:	bd70      	pop	{r4, r5, r6, pc}

namespace tflite {

const TfLiteRegistration* MicroMutableOpResolver::FindOp(
    tflite::BuiltinOperator op, int version) const {
  for (int i = 0; i < registrations_len_; ++i) {
 80a2168:	3401      	adds	r4, #1
 80a216a:	3320      	adds	r3, #32
 80a216c:	e7f0      	b.n	80a2150 <_ZNK6tflite22MicroMutableOpResolver6FindOpENS_15BuiltinOperatorEi+0xc>
    if ((registration.builtin_code == op) &&
        (registration.version == version)) {
      return &registration;
    }
  }
  return nullptr;
 80a216e:	2000      	movs	r0, #0
}
 80a2170:	bd70      	pop	{r4, r5, r6, pc}

080a2172 <_ZNK6tflite22MicroMutableOpResolver6FindOpEPKci>:

const TfLiteRegistration* MicroMutableOpResolver::FindOp(const char* op,
                                                         int version) const {
 80a2172:	e92d 43f8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, lr}
  for (int i = 0; i < registrations_len_; ++i) {
 80a2176:	f241 0304 	movw	r3, #4100	; 0x1004
  }
  return nullptr;
}

const TfLiteRegistration* MicroMutableOpResolver::FindOp(const char* op,
                                                         int version) const {
 80a217a:	4604      	mov	r4, r0
 80a217c:	4688      	mov	r8, r1
 80a217e:	4691      	mov	r9, r2
 80a2180:	4605      	mov	r5, r0
  for (int i = 0; i < registrations_len_; ++i) {
 80a2182:	58c7      	ldr	r7, [r0, r3]
 80a2184:	2600      	movs	r6, #0
 80a2186:	42be      	cmp	r6, r7
 80a2188:	da12      	bge.n	80a21b0 <_ZNK6tflite22MicroMutableOpResolver6FindOpEPKci+0x3e>
    const TfLiteRegistration& registration = registrations_[i];
    if ((registration.builtin_code == BuiltinOperator_CUSTOM) &&
 80a218a:	69ab      	ldr	r3, [r5, #24]
 80a218c:	2b20      	cmp	r3, #32
 80a218e:	d10c      	bne.n	80a21aa <_ZNK6tflite22MicroMutableOpResolver6FindOpEPKci+0x38>
        (strcmp(registration.custom_name, op) == 0) &&
 80a2190:	4641      	mov	r1, r8
 80a2192:	69e8      	ldr	r0, [r5, #28]
 80a2194:	f011 fcd4 	bl	80b3b40 <strcmp>

const TfLiteRegistration* MicroMutableOpResolver::FindOp(const char* op,
                                                         int version) const {
  for (int i = 0; i < registrations_len_; ++i) {
    const TfLiteRegistration& registration = registrations_[i];
    if ((registration.builtin_code == BuiltinOperator_CUSTOM) &&
 80a2198:	b938      	cbnz	r0, 80a21aa <_ZNK6tflite22MicroMutableOpResolver6FindOpEPKci+0x38>
        (strcmp(registration.custom_name, op) == 0) &&
 80a219a:	6a2b      	ldr	r3, [r5, #32]
 80a219c:	454b      	cmp	r3, r9
 80a219e:	d104      	bne.n	80a21aa <_ZNK6tflite22MicroMutableOpResolver6FindOpEPKci+0x38>
}

const TfLiteRegistration* MicroMutableOpResolver::FindOp(const char* op,
                                                         int version) const {
  for (int i = 0; i < registrations_len_; ++i) {
    const TfLiteRegistration& registration = registrations_[i];
 80a21a0:	eb04 1046 	add.w	r0, r4, r6, lsl #5
 80a21a4:	3004      	adds	r0, #4
 80a21a6:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
  return nullptr;
}

const TfLiteRegistration* MicroMutableOpResolver::FindOp(const char* op,
                                                         int version) const {
  for (int i = 0; i < registrations_len_; ++i) {
 80a21aa:	3601      	adds	r6, #1
 80a21ac:	3520      	adds	r5, #32
 80a21ae:	e7ea      	b.n	80a2186 <_ZNK6tflite22MicroMutableOpResolver6FindOpEPKci+0x14>
        (strcmp(registration.custom_name, op) == 0) &&
        (registration.version == version)) {
      return &registration;
    }
  }
  return nullptr;
 80a21b0:	2000      	movs	r0, #0
}
 80a21b2:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}

080a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>:

void MicroMutableOpResolver::AddBuiltin(tflite::BuiltinOperator op,
                                        TfLiteRegistration* registration,
                                        int min_version, int max_version) {
 80a21b6:	e92d 47f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, lr}
 80a21ba:	4680      	mov	r8, r0
 80a21bc:	4689      	mov	r9, r1
 80a21be:	4692      	mov	sl, r2
 80a21c0:	461f      	mov	r7, r3
  for (int version = min_version; version <= max_version; ++version) {
    if (registrations_len_ >= TFLITE_REGISTRATIONS_MAX) {
 80a21c2:	f500 5c80 	add.w	ip, r0, #4096	; 0x1000
 80a21c6:	f10c 0c04 	add.w	ip, ip, #4
}

void MicroMutableOpResolver::AddBuiltin(tflite::BuiltinOperator op,
                                        TfLiteRegistration* registration,
                                        int min_version, int max_version) {
  for (int version = min_version; version <= max_version; ++version) {
 80a21ca:	9b08      	ldr	r3, [sp, #32]
 80a21cc:	429f      	cmp	r7, r3
 80a21ce:	dc15      	bgt.n	80a21fc <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii+0x46>
    if (registrations_len_ >= TFLITE_REGISTRATIONS_MAX) {
 80a21d0:	f8dc 6000 	ldr.w	r6, [ip]
 80a21d4:	2e7f      	cmp	r6, #127	; 0x7f
 80a21d6:	dc11      	bgt.n	80a21fc <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii+0x46>
      return;
    }
    TfLiteRegistration* new_registration = &registrations_[registrations_len_];
    registrations_len_ += 1;

    *new_registration = *registration;
 80a21d8:	4655      	mov	r5, sl
    if (registrations_len_ >= TFLITE_REGISTRATIONS_MAX) {
      // TODO(petewarden) - Add error reporting hooks so we can report this!
      return;
    }
    TfLiteRegistration* new_registration = &registrations_[registrations_len_];
    registrations_len_ += 1;
 80a21da:	1c73      	adds	r3, r6, #1

    *new_registration = *registration;
 80a21dc:	eb08 1646 	add.w	r6, r8, r6, lsl #5
    if (registrations_len_ >= TFLITE_REGISTRATIONS_MAX) {
      // TODO(petewarden) - Add error reporting hooks so we can report this!
      return;
    }
    TfLiteRegistration* new_registration = &registrations_[registrations_len_];
    registrations_len_ += 1;
 80a21e0:	f8cc 3000 	str.w	r3, [ip]

    *new_registration = *registration;
 80a21e4:	1d34      	adds	r4, r6, #4
 80a21e6:	cd0f      	ldmia	r5!, {r0, r1, r2, r3}
 80a21e8:	c40f      	stmia	r4!, {r0, r1, r2, r3}
 80a21ea:	e895 000f 	ldmia.w	r5, {r0, r1, r2, r3}
 80a21ee:	e884 000f 	stmia.w	r4, {r0, r1, r2, r3}
    new_registration->builtin_code = op;
    new_registration->version = version;
 80a21f2:	6237      	str	r7, [r6, #32]
    }
    TfLiteRegistration* new_registration = &registrations_[registrations_len_];
    registrations_len_ += 1;

    *new_registration = *registration;
    new_registration->builtin_code = op;
 80a21f4:	f8c6 9018 	str.w	r9, [r6, #24]
}

void MicroMutableOpResolver::AddBuiltin(tflite::BuiltinOperator op,
                                        TfLiteRegistration* registration,
                                        int min_version, int max_version) {
  for (int version = min_version; version <= max_version; ++version) {
 80a21f8:	3701      	adds	r7, #1
 80a21fa:	e7e6      	b.n	80a21ca <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii+0x14>
 80a21fc:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}

080a2200 <_ZN6tflite12ElementCountERK14TfLiteIntArray>:
static const int8_t kAsymmetricInt8Max = 127;
static const int kSymmetricInt8Scale = kAsymmetricInt8Max;

}  // namespace

int ElementCount(const TfLiteIntArray& dims) {
 80a2200:	b510      	push	{r4, lr}
 80a2202:	4603      	mov	r3, r0
  int result = 1;
  for (int i = 0; i < dims.size; ++i) {
 80a2204:	6804      	ldr	r4, [r0, #0]
 80a2206:	2200      	movs	r2, #0
static const int kSymmetricInt8Scale = kAsymmetricInt8Max;

}  // namespace

int ElementCount(const TfLiteIntArray& dims) {
  int result = 1;
 80a2208:	2001      	movs	r0, #1
  for (int i = 0; i < dims.size; ++i) {
 80a220a:	42a2      	cmp	r2, r4
 80a220c:	da04      	bge.n	80a2218 <_ZN6tflite12ElementCountERK14TfLiteIntArray+0x18>
    result *= dims.data[i];
 80a220e:	f853 1f04 	ldr.w	r1, [r3, #4]!

}  // namespace

int ElementCount(const TfLiteIntArray& dims) {
  int result = 1;
  for (int i = 0; i < dims.size; ++i) {
 80a2212:	3201      	adds	r2, #1
    result *= dims.data[i];
 80a2214:	4348      	muls	r0, r1

}  // namespace

int ElementCount(const TfLiteIntArray& dims) {
  int result = 1;
  for (int i = 0; i < dims.size; ++i) {
 80a2216:	e7f8      	b.n	80a220a <_ZN6tflite12ElementCountERK14TfLiteIntArray+0xa>
    result *= dims.data[i];
  }
  return result;
}
 80a2218:	bd10      	pop	{r4, pc}
	...

080a221c <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf>:

void SignedSymmetricPerChannelQuantize(const float* values,
                                       TfLiteIntArray* dims,
                                       int quantized_dimension,
                                       int8_t* quantized_values,
                                       float* scaling_factors) {
 80a221c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a2220:	468b      	mov	fp, r1
 80a2222:	b08b      	sub	sp, #44	; 0x2c
 80a2224:	9006      	str	r0, [sp, #24]
  int input_size = ElementCount(*dims);
 80a2226:	4608      	mov	r0, r1

void SignedSymmetricPerChannelQuantize(const float* values,
                                       TfLiteIntArray* dims,
                                       int quantized_dimension,
                                       int8_t* quantized_values,
                                       float* scaling_factors) {
 80a2228:	9307      	str	r3, [sp, #28]
 80a222a:	9204      	str	r2, [sp, #16]
  int input_size = ElementCount(*dims);
 80a222c:	f7ff ffe8 	bl	80a2200 <_ZN6tflite12ElementCountERK14TfLiteIntArray>
  int channel_count = dims->data[quantized_dimension];
 80a2230:	9b04      	ldr	r3, [sp, #16]
  int per_channel_size = input_size / channel_count;
  for (int channel = 0; channel < channel_count; channel++) {
 80a2232:	f04f 0800 	mov.w	r8, #0
                                       TfLiteIntArray* dims,
                                       int quantized_dimension,
                                       int8_t* quantized_values,
                                       float* scaling_factors) {
  int input_size = ElementCount(*dims);
  int channel_count = dims->data[quantized_dimension];
 80a2236:	eb0b 0383 	add.w	r3, fp, r3, lsl #2
 80a223a:	685b      	ldr	r3, [r3, #4]
 80a223c:	9302      	str	r3, [sp, #8]
  int per_channel_size = input_size / channel_count;
 80a223e:	fb90 faf3 	sdiv	sl, r0, r3
 80a2242:	9b14      	ldr	r3, [sp, #80]	; 0x50
 80a2244:	3b04      	subs	r3, #4
 80a2246:	9301      	str	r3, [sp, #4]
  for (int channel = 0; channel < channel_count; channel++) {
 80a2248:	9b02      	ldr	r3, [sp, #8]
 80a224a:	4598      	cmp	r8, r3
 80a224c:	da63      	bge.n	80a2316 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0xfa>
 80a224e:	465a      	mov	r2, fp
 80a2250:	2300      	movs	r3, #0
 80a2252:	2501      	movs	r5, #1
    float min = 0;
    float max = 0;
    int stride = 1;
    for (int i = 0; i < quantized_dimension; i++) {
 80a2254:	9904      	ldr	r1, [sp, #16]
 80a2256:	428b      	cmp	r3, r1
 80a2258:	da04      	bge.n	80a2264 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x48>
      stride *= dims->data[i];
 80a225a:	f852 1f04 	ldr.w	r1, [r2, #4]!
  int per_channel_size = input_size / channel_count;
  for (int channel = 0; channel < channel_count; channel++) {
    float min = 0;
    float max = 0;
    int stride = 1;
    for (int i = 0; i < quantized_dimension; i++) {
 80a225e:	3301      	adds	r3, #1
      stride *= dims->data[i];
 80a2260:	434d      	muls	r5, r1
  int per_channel_size = input_size / channel_count;
  for (int channel = 0; channel < channel_count; channel++) {
    float min = 0;
    float max = 0;
    int stride = 1;
    for (int i = 0; i < quantized_dimension; i++) {
 80a2262:	e7f7      	b.n	80a2254 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x38>
      stride *= dims->data[i];
    }
    int channel_stride = per_channel_size / stride;
    // Calculate scales for each channel.
    for (int i = 0; i < per_channel_size; i++) {
      int idx = channel * channel_stride + i * stride;
 80a2264:	fb9a f4f5 	sdiv	r4, sl, r5
 80a2268:	fb08 f404 	mul.w	r4, r8, r4
 80a226c:	00ab      	lsls	r3, r5, #2
 80a226e:	9a06      	ldr	r2, [sp, #24]
 80a2270:	9305      	str	r3, [sp, #20]
 80a2272:	1b63      	subs	r3, r4, r5
 80a2274:	eb02 0383 	add.w	r3, r2, r3, lsl #2
 80a2278:	f04f 0900 	mov.w	r9, #0
  int input_size = ElementCount(*dims);
  int channel_count = dims->data[quantized_dimension];
  int per_channel_size = input_size / channel_count;
  for (int channel = 0; channel < channel_count; channel++) {
    float min = 0;
    float max = 0;
 80a227c:	2600      	movs	r6, #0
 80a227e:	9303      	str	r3, [sp, #12]
                                       float* scaling_factors) {
  int input_size = ElementCount(*dims);
  int channel_count = dims->data[quantized_dimension];
  int per_channel_size = input_size / channel_count;
  for (int channel = 0; channel < channel_count; channel++) {
    float min = 0;
 80a2280:	4637      	mov	r7, r6
    for (int i = 0; i < quantized_dimension; i++) {
      stride *= dims->data[i];
    }
    int channel_stride = per_channel_size / stride;
    // Calculate scales for each channel.
    for (int i = 0; i < per_channel_size; i++) {
 80a2282:	464b      	mov	r3, r9
 80a2284:	9a05      	ldr	r2, [sp, #20]
 80a2286:	4553      	cmp	r3, sl
 80a2288:	4491      	add	r9, r2
 80a228a:	9309      	str	r3, [sp, #36]	; 0x24
 80a228c:	da11      	bge.n	80a22b2 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x96>
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
 80a228e:	9b03      	ldr	r3, [sp, #12]
 80a2290:	4638      	mov	r0, r7
 80a2292:	f853 2009 	ldr.w	r2, [r3, r9]
 80a2296:	4611      	mov	r1, r2
 80a2298:	9208      	str	r2, [sp, #32]
 80a229a:	f00e fedf 	bl	80b105c <fminf>
      max = fmaxf(max, values[idx]);
 80a229e:	9a08      	ldr	r2, [sp, #32]
    }
    int channel_stride = per_channel_size / stride;
    // Calculate scales for each channel.
    for (int i = 0; i < per_channel_size; i++) {
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
 80a22a0:	4607      	mov	r7, r0
      max = fmaxf(max, values[idx]);
 80a22a2:	4611      	mov	r1, r2
 80a22a4:	4630      	mov	r0, r6
 80a22a6:	f00e fec5 	bl	80b1034 <fmaxf>
    for (int i = 0; i < quantized_dimension; i++) {
      stride *= dims->data[i];
    }
    int channel_stride = per_channel_size / stride;
    // Calculate scales for each channel.
    for (int i = 0; i < per_channel_size; i++) {
 80a22aa:	9b09      	ldr	r3, [sp, #36]	; 0x24
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
 80a22ac:	4606      	mov	r6, r0
    for (int i = 0; i < quantized_dimension; i++) {
      stride *= dims->data[i];
    }
    int channel_stride = per_channel_size / stride;
    // Calculate scales for each channel.
    for (int i = 0; i < per_channel_size; i++) {
 80a22ae:	3301      	adds	r3, #1
 80a22b0:	e7e8      	b.n	80a2284 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x68>
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
 80a22b2:	f026 4100 	bic.w	r1, r6, #2147483648	; 0x80000000
 80a22b6:	f027 4000 	bic.w	r0, r7, #2147483648	; 0x80000000
 80a22ba:	f00e febb 	bl	80b1034 <fmaxf>
 80a22be:	4917      	ldr	r1, [pc, #92]	; (80a231c <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x100>)
 80a22c0:	f011 fa48 	bl	80b3754 <__aeabi_fdiv>
 80a22c4:	2600      	movs	r6, #0
    for (int i = 0; i < per_channel_size; i++) {
 80a22c6:	4637      	mov	r7, r6
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
 80a22c8:	9b01      	ldr	r3, [sp, #4]
      int idx = channel * channel_stride + i * stride;
      const int32_t quantized_value =
          static_cast<int32_t>(roundf(values[idx] / scaling_factors[channel]));
      // Clamp: just in case some odd numeric offset.
      quantized_values[idx] = fminf(
          kSymmetricInt8Scale, fmaxf(-kSymmetricInt8Scale, quantized_value));
 80a22ca:	1b64      	subs	r4, r4, r5
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
 80a22cc:	f843 0f04 	str.w	r0, [r3, #4]!
 80a22d0:	9301      	str	r3, [sp, #4]
      int idx = channel * channel_stride + i * stride;
      const int32_t quantized_value =
          static_cast<int32_t>(roundf(values[idx] / scaling_factors[channel]));
      // Clamp: just in case some odd numeric offset.
      quantized_values[idx] = fminf(
          kSymmetricInt8Scale, fmaxf(-kSymmetricInt8Scale, quantized_value));
 80a22d2:	9b07      	ldr	r3, [sp, #28]
 80a22d4:	441c      	add	r4, r3
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
    for (int i = 0; i < per_channel_size; i++) {
 80a22d6:	4557      	cmp	r7, sl
 80a22d8:	442e      	add	r6, r5
 80a22da:	da19      	bge.n	80a2310 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0xf4>
      int idx = channel * channel_stride + i * stride;
      const int32_t quantized_value =
          static_cast<int32_t>(roundf(values[idx] / scaling_factors[channel]));
 80a22dc:	9b01      	ldr	r3, [sp, #4]
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
    for (int i = 0; i < per_channel_size; i++) {
 80a22de:	3701      	adds	r7, #1
      int idx = channel * channel_stride + i * stride;
      const int32_t quantized_value =
          static_cast<int32_t>(roundf(values[idx] / scaling_factors[channel]));
 80a22e0:	6819      	ldr	r1, [r3, #0]
 80a22e2:	9b03      	ldr	r3, [sp, #12]
 80a22e4:	f853 0026 	ldr.w	r0, [r3, r6, lsl #2]
 80a22e8:	f011 fa34 	bl	80b3754 <__aeabi_fdiv>
 80a22ec:	f00e fee4 	bl	80b10b8 <roundf>
      // Clamp: just in case some odd numeric offset.
      quantized_values[idx] = fminf(
 80a22f0:	f011 fb58 	bl	80b39a4 <__aeabi_f2iz>
 80a22f4:	f011 f926 	bl	80b3544 <__aeabi_i2f>
 80a22f8:	4601      	mov	r1, r0
 80a22fa:	4809      	ldr	r0, [pc, #36]	; (80a2320 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x104>)
 80a22fc:	f00e fe9a 	bl	80b1034 <fmaxf>
 80a2300:	4601      	mov	r1, r0
 80a2302:	4806      	ldr	r0, [pc, #24]	; (80a231c <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x100>)
 80a2304:	f00e feaa 	bl	80b105c <fminf>
          kSymmetricInt8Scale, fmaxf(-kSymmetricInt8Scale, quantized_value));
 80a2308:	f011 fb4c 	bl	80b39a4 <__aeabi_f2iz>
 80a230c:	55a0      	strb	r0, [r4, r6]
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
    for (int i = 0; i < per_channel_size; i++) {
 80a230e:	e7e2      	b.n	80a22d6 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0xba>
                                       int8_t* quantized_values,
                                       float* scaling_factors) {
  int input_size = ElementCount(*dims);
  int channel_count = dims->data[quantized_dimension];
  int per_channel_size = input_size / channel_count;
  for (int channel = 0; channel < channel_count; channel++) {
 80a2310:	f108 0801 	add.w	r8, r8, #1
 80a2314:	e798      	b.n	80a2248 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x2c>
      // Clamp: just in case some odd numeric offset.
      quantized_values[idx] = fminf(
          kSymmetricInt8Scale, fmaxf(-kSymmetricInt8Scale, quantized_value));
    }
  }
}
 80a2316:	b00b      	add	sp, #44	; 0x2c
 80a2318:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80a231c:	42fe0000 	.word	0x42fe0000
 80a2320:	c2fe0000 	.word	0xc2fe0000

080a2324 <_ZN6tflite19SymmetricDequantizeEPKaifPf>:
                          scaling_factor);
}

void SymmetricDequantize(const int8_t* values, const int size,
                         const float dequantization_scale,
                         float* dequantized_values) {
 80a2324:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
 80a2328:	4607      	mov	r7, r0
 80a232a:	4688      	mov	r8, r1
 80a232c:	4616      	mov	r6, r2
 80a232e:	461d      	mov	r5, r3
  for (int i = 0; i < size; ++i) {
 80a2330:	2400      	movs	r4, #0
 80a2332:	4544      	cmp	r4, r8
 80a2334:	da09      	bge.n	80a234a <_ZN6tflite19SymmetricDequantizeEPKaifPf+0x26>
    dequantized_values[i] = values[i] * dequantization_scale;
 80a2336:	5738      	ldrsb	r0, [r7, r4]
 80a2338:	f011 f904 	bl	80b3544 <__aeabi_i2f>
 80a233c:	4631      	mov	r1, r6
 80a233e:	f011 f955 	bl	80b35ec <__aeabi_fmul>
 80a2342:	f845 0024 	str.w	r0, [r5, r4, lsl #2]
}

void SymmetricDequantize(const int8_t* values, const int size,
                         const float dequantization_scale,
                         float* dequantized_values) {
  for (int i = 0; i < size; ++i) {
 80a2346:	3401      	adds	r4, #1
 80a2348:	e7f3      	b.n	80a2332 <_ZN6tflite19SymmetricDequantizeEPKaifPf+0xe>
 80a234a:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}

080a234e <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>:
#include "tensorflow/lite/experimental/micro/memory_helpers.h"

namespace tflite {

uint8_t* SimpleMemoryAllocator::AllocateFromTail(size_t size,
                                                 size_t alignment) {
 80a234e:	b538      	push	{r3, r4, r5, lr}
  uint8_t* previous_free = (data_ + data_size_max_) - data_size_;
 80a2350:	6804      	ldr	r4, [r0, #0]
 80a2352:	6843      	ldr	r3, [r0, #4]
#include "tensorflow/lite/experimental/micro/memory_helpers.h"

namespace tflite {

uint8_t* SimpleMemoryAllocator::AllocateFromTail(size_t size,
                                                 size_t alignment) {
 80a2354:	4605      	mov	r5, r0
  uint8_t* previous_free = (data_ + data_size_max_) - data_size_;
 80a2356:	1b1b      	subs	r3, r3, r4
 80a2358:	6884      	ldr	r4, [r0, #8]
 80a235a:	441c      	add	r4, r3
  uint8_t* current_data = previous_free - size;
  uint8_t* aligned_result = AlignPointerDown(current_data, alignment);
 80a235c:	1a60      	subs	r0, r4, r1
 80a235e:	4611      	mov	r1, r2
 80a2360:	f7ff f93c 	bl	80a15dc <_ZN6tflite16AlignPointerDownEPhj>
  size_t aligned_size = (previous_free - aligned_result);
  if ((data_size_ + aligned_size) > data_size_max_) {
 80a2364:	682b      	ldr	r3, [r5, #0]
 80a2366:	1a24      	subs	r4, r4, r0
 80a2368:	441c      	add	r4, r3
 80a236a:	686b      	ldr	r3, [r5, #4]
 80a236c:	429c      	cmp	r4, r3
    // TODO(petewarden): Add error reporting beyond returning null!
    return nullptr;
  }
  data_size_ += aligned_size;
 80a236e:	bf94      	ite	ls
 80a2370:	602c      	strls	r4, [r5, #0]
  uint8_t* current_data = previous_free - size;
  uint8_t* aligned_result = AlignPointerDown(current_data, alignment);
  size_t aligned_size = (previous_free - aligned_result);
  if ((data_size_ + aligned_size) > data_size_max_) {
    // TODO(petewarden): Add error reporting beyond returning null!
    return nullptr;
 80a2372:	2000      	movhi	r0, #0
  }
  data_size_ += aligned_size;
  return aligned_result;
}
 80a2374:	bd38      	pop	{r3, r4, r5, pc}
	...

080a2378 <DebugLog>:
#define DEBUG_SERIAL_OBJECT (Serial)
#endif

// On Arduino platforms, we set up a serial port and write to it for debug
// logging.
extern "C" void DebugLog(const char* s) {
 80a2378:	b538      	push	{r3, r4, r5, lr}
  static bool is_initialized = false;
  if (!is_initialized) {
 80a237a:	4c09      	ldr	r4, [pc, #36]	; (80a23a0 <DebugLog+0x28>)
#define DEBUG_SERIAL_OBJECT (Serial)
#endif

// On Arduino platforms, we set up a serial port and write to it for debug
// logging.
extern "C" void DebugLog(const char* s) {
 80a237c:	4605      	mov	r5, r0
  static bool is_initialized = false;
  if (!is_initialized) {
 80a237e:	7823      	ldrb	r3, [r4, #0]
 80a2380:	b93b      	cbnz	r3, 80a2392 <DebugLog+0x1a>
    DEBUG_SERIAL_OBJECT.begin(9600);
 80a2382:	f00e f921 	bl	80b05c8 <_Z16_fetch_usbserialv>
 80a2386:	f44f 5116 	mov.w	r1, #9600	; 0x2580
 80a238a:	f00e f903 	bl	80b0594 <_ZN9USBSerial5beginEl>
    is_initialized = true;
 80a238e:	2301      	movs	r3, #1
 80a2390:	7023      	strb	r3, [r4, #0]
  }
  DEBUG_SERIAL_OBJECT.print(s);
 80a2392:	f00e f919 	bl	80b05c8 <_Z16_fetch_usbserialv>
 80a2396:	4629      	mov	r1, r5
}
 80a2398:	e8bd 4038 	ldmia.w	sp!, {r3, r4, r5, lr}
  static bool is_initialized = false;
  if (!is_initialized) {
    DEBUG_SERIAL_OBJECT.begin(9600);
    is_initialized = true;
  }
  DEBUG_SERIAL_OBJECT.print(s);
 80a239c:	f00d bfa4 	b.w	80b02e8 <_ZN5Print5printEPKc>
 80a23a0:	20001e4d 	.word	0x20001e4d

080a23a4 <_GLOBAL__sub_I_DebugLog>:
 80a23a4:	f00d bcb6 	b.w	80afd14 <HAL_Pin_Map>

080a23a8 <_ZN6tflite3ops5micro3add4InitEP13TfLiteContextPKcj>:
  int32 output_offset;
};

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
 80a23a8:	2000      	movs	r0, #0
 80a23aa:	4770      	bx	lr

080a23ac <_ZN6tflite3ops5micro3add4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
 80a23ac:	4770      	bx	lr

080a23ae <_ZN6tflite3ops5micro3add7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
 80a23ae:	2000      	movs	r0, #0
 80a23b0:	4770      	bx	lr

080a23b2 <_ZN6tflite12RuntimeShapeD1Ev>:
  bool operator==(const RuntimeShape& comp) const {
    return this->size_ == comp.size_ &&
           std::memcmp(DimsData(), comp.DimsData(), size_ * sizeof(int32)) == 0;
  }

  ~RuntimeShape() {
 80a23b2:	b510      	push	{r4, lr}
    if (size_ > kMaxSmallSize) {
 80a23b4:	6803      	ldr	r3, [r0, #0]
  bool operator==(const RuntimeShape& comp) const {
    return this->size_ == comp.size_ &&
           std::memcmp(DimsData(), comp.DimsData(), size_ * sizeof(int32)) == 0;
  }

  ~RuntimeShape() {
 80a23b6:	4604      	mov	r4, r0
    if (size_ > kMaxSmallSize) {
 80a23b8:	2b04      	cmp	r3, #4
 80a23ba:	dd03      	ble.n	80a23c4 <_ZN6tflite12RuntimeShapeD1Ev+0x12>
#ifdef TF_LITE_STATIC_MEMORY
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
 80a23bc:	6840      	ldr	r0, [r0, #4]
 80a23be:	b108      	cbz	r0, 80a23c4 <_ZN6tflite12RuntimeShapeD1Ev+0x12>
 80a23c0:	f7fd fe71 	bl	80a00a6 <_ZdaPv>
#endif  // TF_LITE_STATIC_MEMORY
    }
  }
 80a23c4:	4620      	mov	r0, r4
 80a23c6:	bd10      	pop	{r4, pc}

080a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>:

  inline int32 DimensionsCount() const { return size_; }
  inline int32 Dims(int i) const {
    TFLITE_DCHECK_GE(i, 0);
 80a23c8:	2900      	cmp	r1, #0
#endif  // TF_LITE_STATIC_MEMORY
    }
  }

  inline int32 DimensionsCount() const { return size_; }
  inline int32 Dims(int i) const {
 80a23ca:	b508      	push	{r3, lr}
    TFLITE_DCHECK_GE(i, 0);
 80a23cc:	da01      	bge.n	80a23d2 <_ZNK6tflite12RuntimeShape4DimsEi+0xa>
 80a23ce:	f00d fe2f 	bl	80b0030 <abort>
    TFLITE_DCHECK_LT(i, size_);
 80a23d2:	6803      	ldr	r3, [r0, #0]
 80a23d4:	428b      	cmp	r3, r1
 80a23d6:	ddfa      	ble.n	80a23ce <_ZNK6tflite12RuntimeShape4DimsEi+0x6>
    return size_ > kMaxSmallSize ? dims_pointer_[i] : dims_[i];
 80a23d8:	2b04      	cmp	r3, #4
 80a23da:	bfc7      	ittee	gt
 80a23dc:	6843      	ldrgt	r3, [r0, #4]
 80a23de:	f853 0021 	ldrgt.w	r0, [r3, r1, lsl #2]
 80a23e2:	eb00 0081 	addle.w	r0, r0, r1, lsl #2
 80a23e6:	6840      	ldrle	r0, [r0, #4]
  }
 80a23e8:	bd08      	pop	{r3, pc}

080a23ea <_ZN6tflite12RuntimeShape6SetDimEil>:
  inline void SetDim(int i, int32 val) {
    TFLITE_DCHECK_GE(i, 0);
 80a23ea:	2900      	cmp	r1, #0
  inline int32 Dims(int i) const {
    TFLITE_DCHECK_GE(i, 0);
    TFLITE_DCHECK_LT(i, size_);
    return size_ > kMaxSmallSize ? dims_pointer_[i] : dims_[i];
  }
  inline void SetDim(int i, int32 val) {
 80a23ec:	b508      	push	{r3, lr}
    TFLITE_DCHECK_GE(i, 0);
 80a23ee:	da01      	bge.n	80a23f4 <_ZN6tflite12RuntimeShape6SetDimEil+0xa>
 80a23f0:	f00d fe1e 	bl	80b0030 <abort>
    TFLITE_DCHECK_LT(i, size_);
 80a23f4:	6803      	ldr	r3, [r0, #0]
 80a23f6:	428b      	cmp	r3, r1
 80a23f8:	ddfa      	ble.n	80a23f0 <_ZN6tflite12RuntimeShape6SetDimEil+0x6>
    if (size_ > kMaxSmallSize) {
 80a23fa:	2b04      	cmp	r3, #4
      dims_pointer_[i] = val;
 80a23fc:	bfcb      	itete	gt
 80a23fe:	6843      	ldrgt	r3, [r0, #4]
    } else {
      dims_[i] = val;
 80a2400:	eb00 0081 	addle.w	r0, r0, r1, lsl #2
  }
  inline void SetDim(int i, int32 val) {
    TFLITE_DCHECK_GE(i, 0);
    TFLITE_DCHECK_LT(i, size_);
    if (size_ > kMaxSmallSize) {
      dims_pointer_[i] = val;
 80a2404:	f843 2021 	strgt.w	r2, [r3, r1, lsl #2]
    } else {
      dims_[i] = val;
 80a2408:	6042      	strle	r2, [r0, #4]
 80a240a:	bd08      	pop	{r3, pc}

080a240c <_ZN6tflite12RuntimeShape6ResizeEi>:
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  // The caller must ensure that the shape is no bigger than 4-D.
  inline const int32* DimsDataUpTo4D() const { return dims_; }

  inline void Resize(int dimensions_count) {
 80a240c:	b538      	push	{r3, r4, r5, lr}
    if (size_ > kMaxSmallSize) {
 80a240e:	6803      	ldr	r3, [r0, #0]
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  // The caller must ensure that the shape is no bigger than 4-D.
  inline const int32* DimsDataUpTo4D() const { return dims_; }

  inline void Resize(int dimensions_count) {
 80a2410:	4605      	mov	r5, r0
    if (size_ > kMaxSmallSize) {
 80a2412:	2b04      	cmp	r3, #4
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  // The caller must ensure that the shape is no bigger than 4-D.
  inline const int32* DimsDataUpTo4D() const { return dims_; }

  inline void Resize(int dimensions_count) {
 80a2414:	460c      	mov	r4, r1
    if (size_ > kMaxSmallSize) {
 80a2416:	dd03      	ble.n	80a2420 <_ZN6tflite12RuntimeShape6ResizeEi+0x14>
#ifdef TF_LITE_STATIC_MEMORY
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
 80a2418:	6840      	ldr	r0, [r0, #4]
 80a241a:	b108      	cbz	r0, 80a2420 <_ZN6tflite12RuntimeShape6ResizeEi+0x14>
 80a241c:	f7fd fe43 	bl	80a00a6 <_ZdaPv>
#endif  // TF_LITE_STATIC_MEMORY
    }
    size_ = dimensions_count;
    if (dimensions_count > kMaxSmallSize) {
 80a2420:	2c04      	cmp	r4, #4
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
#endif  // TF_LITE_STATIC_MEMORY
    }
    size_ = dimensions_count;
 80a2422:	602c      	str	r4, [r5, #0]
    if (dimensions_count > kMaxSmallSize) {
 80a2424:	dd08      	ble.n	80a2438 <_ZN6tflite12RuntimeShape6ResizeEi+0x2c>
#ifdef TF_LITE_STATIC_MEMORY
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      dims_pointer_ = new int32[dimensions_count];
 80a2426:	f1b4 5ffe 	cmp.w	r4, #532676608	; 0x1fc00000
 80a242a:	bfd4      	ite	le
 80a242c:	00a0      	lslle	r0, r4, #2
 80a242e:	f04f 30ff 	movgt.w	r0, #4294967295	; 0xffffffff
 80a2432:	f7fd fe34 	bl	80a009e <_Znaj>
 80a2436:	6068      	str	r0, [r5, #4]
 80a2438:	bd38      	pop	{r3, r4, r5, pc}

080a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>:

 private:
  // For use only by ExtendedShape(), written to guarantee (return-value) copy
  // elision in C++17.
  // This creates a shape padded to the desired size with the specified value.
  RuntimeShape(int new_shape_size, const RuntimeShape& shape, int pad_value)
 80a243a:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
      : size_(0) {
 80a243e:	2500      	movs	r5, #0
 80a2440:	6005      	str	r5, [r0, #0]

 private:
  // For use only by ExtendedShape(), written to guarantee (return-value) copy
  // elision in C++17.
  // This creates a shape padded to the desired size with the specified value.
  RuntimeShape(int new_shape_size, const RuntimeShape& shape, int pad_value)
 80a2442:	4698      	mov	r8, r3
      : size_(0) {
    // If the following check fails, it is likely because a 4D-only kernel is
    // being used with an array of larger dimension count.
    TFLITE_CHECK_GE(new_shape_size, shape.DimensionsCount());
 80a2444:	6813      	ldr	r3, [r2, #0]

 private:
  // For use only by ExtendedShape(), written to guarantee (return-value) copy
  // elision in C++17.
  // This creates a shape padded to the desired size with the specified value.
  RuntimeShape(int new_shape_size, const RuntimeShape& shape, int pad_value)
 80a2446:	4606      	mov	r6, r0
      : size_(0) {
    // If the following check fails, it is likely because a 4D-only kernel is
    // being used with an array of larger dimension count.
    TFLITE_CHECK_GE(new_shape_size, shape.DimensionsCount());
 80a2448:	428b      	cmp	r3, r1

 private:
  // For use only by ExtendedShape(), written to guarantee (return-value) copy
  // elision in C++17.
  // This creates a shape padded to the desired size with the specified value.
  RuntimeShape(int new_shape_size, const RuntimeShape& shape, int pad_value)
 80a244a:	460f      	mov	r7, r1
 80a244c:	4614      	mov	r4, r2
      : size_(0) {
    // If the following check fails, it is likely because a 4D-only kernel is
    // being used with an array of larger dimension count.
    TFLITE_CHECK_GE(new_shape_size, shape.DimensionsCount());
 80a244e:	dd01      	ble.n	80a2454 <_ZN6tflite12RuntimeShapeC1EiRKS0_i+0x1a>
 80a2450:	f00d fdee 	bl	80b0030 <abort>
    Resize(new_shape_size);
 80a2454:	f7ff ffda 	bl	80a240c <_ZN6tflite12RuntimeShape6ResizeEi>
    const int size_increase = new_shape_size - shape.DimensionsCount();
 80a2458:	6820      	ldr	r0, [r4, #0]
 80a245a:	1a3f      	subs	r7, r7, r0
    for (int i = 0; i < size_increase; ++i) {
 80a245c:	42bd      	cmp	r5, r7
 80a245e:	da06      	bge.n	80a246e <_ZN6tflite12RuntimeShapeC1EiRKS0_i+0x34>
      SetDim(i, pad_value);
 80a2460:	4629      	mov	r1, r5
 80a2462:	4642      	mov	r2, r8
 80a2464:	4630      	mov	r0, r6
 80a2466:	f7ff ffc0 	bl	80a23ea <_ZN6tflite12RuntimeShape6SetDimEil>
    // If the following check fails, it is likely because a 4D-only kernel is
    // being used with an array of larger dimension count.
    TFLITE_CHECK_GE(new_shape_size, shape.DimensionsCount());
    Resize(new_shape_size);
    const int size_increase = new_shape_size - shape.DimensionsCount();
    for (int i = 0; i < size_increase; ++i) {
 80a246a:	3501      	adds	r5, #1
 80a246c:	e7f6      	b.n	80a245c <_ZN6tflite12RuntimeShapeC1EiRKS0_i+0x22>
      dims_[i] = val;
    }
  }

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80a246e:	6833      	ldr	r3, [r6, #0]
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80a2470:	6822      	ldr	r2, [r4, #0]
      dims_[i] = val;
    }
  }

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80a2472:	2b04      	cmp	r3, #4
 80a2474:	bfcc      	ite	gt
 80a2476:	6870      	ldrgt	r0, [r6, #4]
 80a2478:	1d30      	addle	r0, r6, #4
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80a247a:	2a04      	cmp	r2, #4
    Resize(new_shape_size);
    const int size_increase = new_shape_size - shape.DimensionsCount();
    for (int i = 0; i < size_increase; ++i) {
      SetDim(i, pad_value);
    }
    std::memcpy(DimsData() + size_increase, shape.DimsData(),
 80a247c:	eb00 0087 	add.w	r0, r0, r7, lsl #2

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80a2480:	bfcc      	ite	gt
 80a2482:	6861      	ldrgt	r1, [r4, #4]
 80a2484:	1d21      	addle	r1, r4, #4
    const int size_increase = new_shape_size - shape.DimensionsCount();
    for (int i = 0; i < size_increase; ++i) {
      SetDim(i, pad_value);
    }
    std::memcpy(DimsData() + size_increase, shape.DimsData(),
                sizeof(int32) * shape.DimensionsCount());
 80a2486:	0092      	lsls	r2, r2, #2
 80a2488:	f011 fb1f 	bl	80b3aca <memcpy>
  }
 80a248c:	4630      	mov	r0, r6
 80a248e:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}

080a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>:
    }
  }
  return offset;
}

inline int Offset(const RuntimeShape& shape, int i0, int i1, int i2, int i3) {
 80a2492:	b570      	push	{r4, r5, r6, lr}
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), 4);
 80a2494:	6805      	ldr	r5, [r0, #0]
    }
  }
  return offset;
}

inline int Offset(const RuntimeShape& shape, int i0, int i1, int i2, int i3) {
 80a2496:	9c04      	ldr	r4, [sp, #16]
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), 4);
 80a2498:	2d04      	cmp	r5, #4
 80a249a:	d001      	beq.n	80a24a0 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xe>
 80a249c:	f00d fdc8 	bl	80b0030 <abort>
  const int* dims_data = reinterpret_cast<const int*>(shape.DimsDataUpTo4D());
  TFLITE_DCHECK(i0 >= 0 && i0 < dims_data[0]);
 80a24a0:	2900      	cmp	r1, #0
 80a24a2:	dbfb      	blt.n	80a249c <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
 80a24a4:	6845      	ldr	r5, [r0, #4]
 80a24a6:	42a9      	cmp	r1, r5
 80a24a8:	daf8      	bge.n	80a249c <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
  TFLITE_DCHECK(i1 >= 0 && i1 < dims_data[1]);
 80a24aa:	2a00      	cmp	r2, #0
 80a24ac:	dbf6      	blt.n	80a249c <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
 80a24ae:	6886      	ldr	r6, [r0, #8]
 80a24b0:	42b2      	cmp	r2, r6
 80a24b2:	daf3      	bge.n	80a249c <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
  TFLITE_DCHECK(i2 >= 0 && i2 < dims_data[2]);
 80a24b4:	2b00      	cmp	r3, #0
 80a24b6:	dbf1      	blt.n	80a249c <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
 80a24b8:	68c5      	ldr	r5, [r0, #12]
 80a24ba:	42ab      	cmp	r3, r5
 80a24bc:	daee      	bge.n	80a249c <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
  TFLITE_DCHECK(i3 >= 0 && i3 < dims_data[3]);
 80a24be:	2c00      	cmp	r4, #0
 80a24c0:	dbec      	blt.n	80a249c <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
 80a24c2:	6900      	ldr	r0, [r0, #16]
 80a24c4:	4284      	cmp	r4, r0
 80a24c6:	dae9      	bge.n	80a249c <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
  return ((i0 * dims_data[1] + i1) * dims_data[2] + i2) * dims_data[3] + i3;
 80a24c8:	fb06 2101 	mla	r1, r6, r1, r2
 80a24cc:	fb05 3301 	mla	r3, r5, r1, r3
}
 80a24d0:	fb00 4003 	mla	r0, r0, r3, r4
 80a24d4:	bd70      	pop	{r4, r5, r6, pc}

080a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>:
  return shape.FlatSize();
}

inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0,
                            const RuntimeShape& check_shape_1) {
 80a24d6:	e92d 43f8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, lr}
 80a24da:	6805      	ldr	r5, [r0, #0]
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
 80a24dc:	680b      	ldr	r3, [r1, #0]
  return shape.FlatSize();
}

inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0,
                            const RuntimeShape& check_shape_1) {
 80a24de:	4604      	mov	r4, r0
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
 80a24e0:	429d      	cmp	r5, r3
  return shape.FlatSize();
}

inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0,
                            const RuntimeShape& check_shape_1) {
 80a24e2:	4688      	mov	r8, r1
 80a24e4:	4617      	mov	r7, r2
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
 80a24e6:	d101      	bne.n	80a24ec <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x16>
 80a24e8:	2600      	movs	r6, #0
 80a24ea:	e00d      	b.n	80a2508 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x32>
 80a24ec:	f00d fda0 	bl	80b0030 <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
 80a24f0:	4631      	mov	r1, r6
 80a24f2:	4620      	mov	r0, r4
 80a24f4:	f7ff ff68 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a24f8:	4631      	mov	r1, r6
 80a24fa:	4681      	mov	r9, r0
 80a24fc:	4640      	mov	r0, r8
 80a24fe:	f7ff ff63 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a2502:	4581      	cmp	r9, r0
 80a2504:	d1f2      	bne.n	80a24ec <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x16>
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0,
                            const RuntimeShape& check_shape_1) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
 80a2506:	3601      	adds	r6, #1
 80a2508:	42ae      	cmp	r6, r5
 80a250a:	dbf1      	blt.n	80a24f0 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x1a>

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
 80a250c:	683b      	ldr	r3, [r7, #0]
 80a250e:	429d      	cmp	r5, r3
 80a2510:	d1ec      	bne.n	80a24ec <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x16>
 80a2512:	2600      	movs	r6, #0
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
 80a2514:	42b5      	cmp	r5, r6
 80a2516:	dd0c      	ble.n	80a2532 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x5c>
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
 80a2518:	4631      	mov	r1, r6
 80a251a:	4620      	mov	r0, r4
 80a251c:	f7ff ff54 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a2520:	4631      	mov	r1, r6
 80a2522:	4680      	mov	r8, r0
 80a2524:	4638      	mov	r0, r7
 80a2526:	f7ff ff4f 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a252a:	4580      	cmp	r8, r0
 80a252c:	d1de      	bne.n	80a24ec <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x16>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
 80a252e:	3601      	adds	r6, #1
 80a2530:	e7f0      	b.n	80a2514 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x3e>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80a2532:	2d04      	cmp	r5, #4
 80a2534:	bfcc      	ite	gt
 80a2536:	6864      	ldrgt	r4, [r4, #4]
 80a2538:	3404      	addle	r4, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a253a:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
 80a253c:	2001      	movs	r0, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a253e:	429d      	cmp	r5, r3
 80a2540:	dd04      	ble.n	80a254c <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x76>
      buffer_size *= dims_data[i];
 80a2542:	f854 2023 	ldr.w	r2, [r4, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a2546:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
 80a2548:	4350      	muls	r0, r2
 80a254a:	e7f8      	b.n	80a253e <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x68>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
  }
  return MatchingFlatSize(shape, check_shape_1);
}
 80a254c:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}

080a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>:
// This function implements the same computation as the ARMv7 NEON VQRDMULH
// instruction.
template <>
inline std::int32_t SaturatingRoundingDoublingHighMul(std::int32_t a,
                                                      std::int32_t b) {
  bool overflow = a == b && a == std::numeric_limits<std::int32_t>::min();
 80a2550:	4288      	cmp	r0, r1
  }
#endif
}

inline int32 MultiplyByQuantizedMultiplierSmallerThanOneExp(
    int32 x, int32 quantized_multiplier, int left_shift) {
 80a2552:	b570      	push	{r4, r5, r6, lr}
 80a2554:	d104      	bne.n	80a2560 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x10>
 80a2556:	f100 4300 	add.w	r3, r0, #2147483648	; 0x80000000
 80a255a:	425e      	negs	r6, r3
 80a255c:	415e      	adcs	r6, r3
 80a255e:	e000      	b.n	80a2562 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x12>
 80a2560:	2600      	movs	r6, #0
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
 80a2562:	fb80 4501 	smull	r4, r5, r0, r1
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
 80a2566:	2c00      	cmp	r4, #0
 80a2568:	f175 0300 	sbcs.w	r3, r5, #0
 80a256c:	4b1c      	ldr	r3, [pc, #112]	; (80a25e0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x90>)
 80a256e:	bfa8      	it	ge
 80a2570:	f04f 4380 	movge.w	r3, #1073741824	; 0x40000000
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
 80a2574:	b97e      	cbnz	r6, 80a2596 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x46>
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
 80a2576:	18e4      	adds	r4, r4, r3
 80a2578:	eb45 75e3 	adc.w	r5, r5, r3, asr #31
 80a257c:	2c00      	cmp	r4, #0
 80a257e:	f175 0300 	sbcs.w	r3, r5, #0
 80a2582:	da04      	bge.n	80a258e <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x3e>
 80a2584:	f06f 4000 	mvn.w	r0, #2147483648	; 0x80000000
 80a2588:	2100      	movs	r1, #0
 80a258a:	1824      	adds	r4, r4, r0
 80a258c:	414d      	adcs	r5, r1
 80a258e:	0fe4      	lsrs	r4, r4, #31
 80a2590:	ea44 0445 	orr.w	r4, r4, r5, lsl #1
 80a2594:	e001      	b.n	80a259a <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x4a>
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
 80a2596:	f06f 4400 	mvn.w	r4, #2147483648	; 0x80000000
  using gemmlowp::RoundingDivideByPOT;
  using gemmlowp::SaturatingRoundingDoublingHighMul;
  return RoundingDivideByPOT(
 80a259a:	4255      	negs	r5, r2

// Correctly-rounded-to-nearest division by a power-of-two.
// Also known as a rounding arithmetic right shift.
template <typename IntegerType>
inline IntegerType RoundingDivideByPOT(IntegerType x, int exponent) {
  assert(exponent >= 0);
 80a259c:	2d00      	cmp	r5, #0
 80a259e:	da04      	bge.n	80a25aa <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x5a>
 80a25a0:	4b10      	ldr	r3, [pc, #64]	; (80a25e4 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x94>)
 80a25a2:	4a11      	ldr	r2, [pc, #68]	; (80a25e8 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x98>)
 80a25a4:	f44f 71b3 	mov.w	r1, #358	; 0x166
 80a25a8:	e005      	b.n	80a25b6 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x66>
  assert(exponent <= 31);
 80a25aa:	2d1f      	cmp	r5, #31
 80a25ac:	dd06      	ble.n	80a25bc <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x6c>
 80a25ae:	f240 1167 	movw	r1, #359	; 0x167
 80a25b2:	4b0e      	ldr	r3, [pc, #56]	; (80a25ec <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x9c>)
 80a25b4:	4a0c      	ldr	r2, [pc, #48]	; (80a25e8 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x98>)
 80a25b6:	480e      	ldr	r0, [pc, #56]	; (80a25f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0xa0>)
 80a25b8:	f00d fd4a 	bl	80b0050 <__assert_func>
  const IntegerType mask = Dup<IntegerType>((1ll << exponent) - 1);
 80a25bc:	462a      	mov	r2, r5
 80a25be:	2001      	movs	r0, #1
 80a25c0:	2100      	movs	r1, #0
 80a25c2:	f010 fa0f 	bl	80b29e4 <__aeabi_llsl>
 80a25c6:	3801      	subs	r0, #1
      SaturatingRoundingDoublingHighMul(x, quantized_multiplier), -left_shift);
 80a25c8:	ea00 0304 	and.w	r3, r0, r4
 80a25cc:	1040      	asrs	r0, r0, #1
 80a25ce:	eb00 70d4 	add.w	r0, r0, r4, lsr #31
 80a25d2:	412c      	asrs	r4, r5
}
 80a25d4:	4283      	cmp	r3, r0
 80a25d6:	bfd4      	ite	le
 80a25d8:	4620      	movle	r0, r4
 80a25da:	1c60      	addgt	r0, r4, #1
 80a25dc:	bd70      	pop	{r4, r5, r6, pc}
 80a25de:	bf00      	nop
 80a25e0:	c0000001 	.word	0xc0000001
 80a25e4:	080b596c 	.word	0x080b596c
 80a25e8:	080b5a5b 	.word	0x080b5a5b
 80a25ec:	080b5a19 	.word	0x080b5a19
 80a25f0:	080b597a 	.word	0x080b597a

080a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>:
// DO NOT USE THIS FUNCTION FOR NEW FUNCTIONALITY BEYOND IMPLEMENTING
// BROADCASTING.
//
// Same as Offset(), except takes as NdArrayDesc<N> instead of Dims<N>.
inline int SubscriptToIndex(const NdArrayDesc<4>& desc, int i0, int i1, int i2,
                            int i3) {
 80a25f4:	b570      	push	{r4, r5, r6, lr}
  TFLITE_DCHECK(i0 >= 0 && i0 < desc.extents[0]);
 80a25f6:	2900      	cmp	r1, #0
// DO NOT USE THIS FUNCTION FOR NEW FUNCTIONALITY BEYOND IMPLEMENTING
// BROADCASTING.
//
// Same as Offset(), except takes as NdArrayDesc<N> instead of Dims<N>.
inline int SubscriptToIndex(const NdArrayDesc<4>& desc, int i0, int i1, int i2,
                            int i3) {
 80a25f8:	9c04      	ldr	r4, [sp, #16]
  TFLITE_DCHECK(i0 >= 0 && i0 < desc.extents[0]);
 80a25fa:	db02      	blt.n	80a2602 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
 80a25fc:	6805      	ldr	r5, [r0, #0]
 80a25fe:	42a9      	cmp	r1, r5
 80a2600:	db01      	blt.n	80a2606 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0x12>
 80a2602:	f00d fd15 	bl	80b0030 <abort>
  TFLITE_DCHECK(i1 >= 0 && i1 < desc.extents[1]);
 80a2606:	2a00      	cmp	r2, #0
 80a2608:	dbfb      	blt.n	80a2602 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
 80a260a:	6845      	ldr	r5, [r0, #4]
 80a260c:	42aa      	cmp	r2, r5
 80a260e:	daf8      	bge.n	80a2602 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
  TFLITE_DCHECK(i2 >= 0 && i2 < desc.extents[2]);
 80a2610:	2b00      	cmp	r3, #0
 80a2612:	dbf6      	blt.n	80a2602 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
 80a2614:	6885      	ldr	r5, [r0, #8]
 80a2616:	42ab      	cmp	r3, r5
 80a2618:	daf3      	bge.n	80a2602 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
  TFLITE_DCHECK(i3 >= 0 && i3 < desc.extents[3]);
 80a261a:	2c00      	cmp	r4, #0
 80a261c:	dbf1      	blt.n	80a2602 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
 80a261e:	68c5      	ldr	r5, [r0, #12]
 80a2620:	42ac      	cmp	r4, r5
 80a2622:	daee      	bge.n	80a2602 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
  return i0 * desc.strides[0] + i1 * desc.strides[1] + i2 * desc.strides[2] +
         i3 * desc.strides[3];
 80a2624:	6945      	ldr	r5, [r0, #20]
 80a2626:	6906      	ldr	r6, [r0, #16]
 80a2628:	4355      	muls	r5, r2
 80a262a:	fb06 5101 	mla	r1, r6, r1, r5
 80a262e:	6982      	ldr	r2, [r0, #24]
 80a2630:	69c0      	ldr	r0, [r0, #28]
 80a2632:	fb02 1303 	mla	r3, r2, r3, r1
}
 80a2636:	fb00 3004 	mla	r0, r0, r4, r3
 80a263a:	bd70      	pop	{r4, r5, r6, pc}

080a263c <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph>:
}

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const uint8* input1_data,
                const RuntimeShape& input2_shape, const uint8* input2_data,
                const RuntimeShape& output_shape, uint8* output_data) {
 80a263c:	e92d 4ff8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a2640:	4604      	mov	r4, r0
 80a2642:	4690      	mov	r8, r2
 80a2644:	4608      	mov	r0, r1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
 80a2646:	6b22      	ldr	r2, [r4, #48]	; 0x30
 80a2648:	6ae1      	ldr	r1, [r4, #44]	; 0x2c
}

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const uint8* input1_data,
                const RuntimeShape& input2_shape, const uint8* input2_data,
                const RuntimeShape& output_shape, uint8* output_data) {
 80a264a:	9f0a      	ldr	r7, [sp, #40]	; 0x28
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
 80a264c:	4291      	cmp	r1, r2
}

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const uint8* input1_data,
                const RuntimeShape& input2_shape, const uint8* input2_data,
                const RuntimeShape& output_shape, uint8* output_data) {
 80a264e:	9e0c      	ldr	r6, [sp, #48]	; 0x30
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
 80a2650:	dd01      	ble.n	80a2656 <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1a>

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const uint8* input1_data,
                const RuntimeShape& input2_shape, const uint8* input2_data,
                const RuntimeShape& output_shape, uint8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
 80a2652:	f00d fced 	bl	80b0030 <abort>
                   params.quantized_activation_max);
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a2656:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
 80a2658:	4619      	mov	r1, r3
 80a265a:	f7ff ff3c 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>

  TFLITE_DCHECK_GT(params.input1_offset, -256);
 80a265e:	6862      	ldr	r2, [r4, #4]
                const RuntimeShape& input2_shape, const uint8* input2_data,
                const RuntimeShape& output_shape, uint8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a2660:	4681      	mov	r9, r0

  TFLITE_DCHECK_GT(params.input1_offset, -256);
 80a2662:	f112 0fff 	cmn.w	r2, #255	; 0xff
 80a2666:	dbf4      	blt.n	80a2652 <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x16>
  TFLITE_DCHECK_GT(params.input2_offset, -256);
  TFLITE_DCHECK_LT(params.input1_offset, 256);
 80a2668:	2aff      	cmp	r2, #255	; 0xff
                   params.quantized_activation_max);
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);

  TFLITE_DCHECK_GT(params.input1_offset, -256);
  TFLITE_DCHECK_GT(params.input2_offset, -256);
 80a266a:	68a3      	ldr	r3, [r4, #8]
  TFLITE_DCHECK_LT(params.input1_offset, 256);
 80a266c:	dcf1      	bgt.n	80a2652 <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x16>
  TFLITE_DCHECK_LT(params.input2_offset, 256);
 80a266e:	33ff      	adds	r3, #255	; 0xff
 80a2670:	f5b3 7fff 	cmp.w	r3, #510	; 0x1fe
 80a2674:	d8ed      	bhi.n	80a2652 <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x16>
 80a2676:	2500      	movs	r5, #0
  TFLITE_DCHECK_GT(params.input1_offset, -256);
  TFLITE_DCHECK_GT(params.input2_offset, -256);
  TFLITE_DCHECK_LT(params.input1_offset, 256);
  TFLITE_DCHECK_LT(params.input2_offset, 256);

  for (int i = 0; i < size; ++i) {
 80a2678:	45a9      	cmp	r9, r5
 80a267a:	dd28      	ble.n	80a26ce <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x92>
    const int32 input1_val = params.input1_offset + input1_data[i];
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
 80a267c:	f817 a005 	ldrb.w	sl, [r7, r5]
 80a2680:	68a3      	ldr	r3, [r4, #8]
  TFLITE_DCHECK_LT(params.input2_offset, 256);

  for (int i = 0; i < size; ++i) {
    const int32 input1_val = params.input1_offset + input1_data[i];
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
 80a2682:	69a0      	ldr	r0, [r4, #24]
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
 80a2684:	4453      	add	r3, sl
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, params.input1_multiplier, params.input1_shift);
 80a2686:	f818 2005 	ldrb.w	r2, [r8, r5]

  for (int i = 0; i < size; ++i) {
    const int32 input1_val = params.input1_offset + input1_data[i];
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
 80a268a:	fa03 fa00 	lsl.w	sl, r3, r0
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, params.input1_multiplier, params.input1_shift);
 80a268e:	6863      	ldr	r3, [r4, #4]
 80a2690:	69e1      	ldr	r1, [r4, #28]
 80a2692:	4413      	add	r3, r2
 80a2694:	fa03 f000 	lsl.w	r0, r3, r0
 80a2698:	6a22      	ldr	r2, [r4, #32]
 80a269a:	f7ff ff59 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, params.input2_multiplier, params.input2_shift);
 80a269e:	6aa2      	ldr	r2, [r4, #40]	; 0x28
 80a26a0:	6a61      	ldr	r1, [r4, #36]	; 0x24
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, params.input1_multiplier, params.input1_shift);
 80a26a2:	4683      	mov	fp, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, params.input2_multiplier, params.input2_shift);
 80a26a4:	4650      	mov	r0, sl
 80a26a6:	f7ff ff53 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 raw_sum = scaled_input1_val + scaled_input2_val;
    const int32 raw_output =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
 80a26aa:	6962      	ldr	r2, [r4, #20]
 80a26ac:	6921      	ldr	r1, [r4, #16]
 80a26ae:	4458      	add	r0, fp
 80a26b0:	f7ff ff4e 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
            raw_sum, params.output_multiplier, params.output_shift) +
        params.output_offset;
    const int32 clamped_output =
        std::min(params.quantized_activation_max,
                 std::max(params.quantized_activation_min, raw_output));
    output_data[i] = static_cast<uint8>(clamped_output);
 80a26b4:	68e3      	ldr	r3, [r4, #12]
 80a26b6:	4418      	add	r0, r3
 80a26b8:	6ae3      	ldr	r3, [r4, #44]	; 0x2c
 80a26ba:	4298      	cmp	r0, r3
 80a26bc:	bfb8      	it	lt
 80a26be:	4618      	movlt	r0, r3
 80a26c0:	6b23      	ldr	r3, [r4, #48]	; 0x30
 80a26c2:	4283      	cmp	r3, r0
 80a26c4:	bfa8      	it	ge
 80a26c6:	4603      	movge	r3, r0
 80a26c8:	5573      	strb	r3, [r6, r5]
  TFLITE_DCHECK_GT(params.input1_offset, -256);
  TFLITE_DCHECK_GT(params.input2_offset, -256);
  TFLITE_DCHECK_LT(params.input1_offset, 256);
  TFLITE_DCHECK_LT(params.input2_offset, 256);

  for (int i = 0; i < size; ++i) {
 80a26ca:	3501      	adds	r5, #1
 80a26cc:	e7d4      	b.n	80a2678 <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x3c>
 80a26ce:	e8bd 8ff8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a26d2 <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa>:
}

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const int8_t* input1_data,
                const RuntimeShape& input2_shape, const int8_t* input2_data,
                const RuntimeShape& output_shape, int8_t* output_data) {
 80a26d2:	e92d 4ff8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a26d6:	4604      	mov	r4, r0
 80a26d8:	4690      	mov	r8, r2
 80a26da:	4608      	mov	r0, r1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
 80a26dc:	6b22      	ldr	r2, [r4, #48]	; 0x30
 80a26de:	6ae1      	ldr	r1, [r4, #44]	; 0x2c
}

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const int8_t* input1_data,
                const RuntimeShape& input2_shape, const int8_t* input2_data,
                const RuntimeShape& output_shape, int8_t* output_data) {
 80a26e0:	9f0a      	ldr	r7, [sp, #40]	; 0x28
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
 80a26e2:	4291      	cmp	r1, r2
}

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const int8_t* input1_data,
                const RuntimeShape& input2_shape, const int8_t* input2_data,
                const RuntimeShape& output_shape, int8_t* output_data) {
 80a26e4:	9e0c      	ldr	r6, [sp, #48]	; 0x30
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
 80a26e6:	dd01      	ble.n	80a26ec <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x1a>

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const int8_t* input1_data,
                const RuntimeShape& input2_shape, const int8_t* input2_data,
                const RuntimeShape& output_shape, int8_t* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
 80a26e8:	f00d fca2 	bl	80b0030 <abort>
                   params.quantized_activation_max);
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a26ec:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
 80a26ee:	4619      	mov	r1, r3
 80a26f0:	f7ff fef1 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>

  const int32_t int8_max_value = std::numeric_limits<int8_t>::max();
  TFLITE_DCHECK_GE(params.input1_offset, -1 * int8_max_value);
 80a26f4:	6862      	ldr	r2, [r4, #4]
                const RuntimeShape& input2_shape, const int8_t* input2_data,
                const RuntimeShape& output_shape, int8_t* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a26f6:	4681      	mov	r9, r0

  const int32_t int8_max_value = std::numeric_limits<int8_t>::max();
  TFLITE_DCHECK_GE(params.input1_offset, -1 * int8_max_value);
 80a26f8:	f112 0f7f 	cmn.w	r2, #127	; 0x7f
 80a26fc:	dbf4      	blt.n	80a26e8 <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x16>
  TFLITE_DCHECK_GE(params.input2_offset, -1 * int8_max_value);
  TFLITE_DCHECK_LE(params.input1_offset, int8_max_value);
 80a26fe:	2a7f      	cmp	r2, #127	; 0x7f
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);

  const int32_t int8_max_value = std::numeric_limits<int8_t>::max();
  TFLITE_DCHECK_GE(params.input1_offset, -1 * int8_max_value);
  TFLITE_DCHECK_GE(params.input2_offset, -1 * int8_max_value);
 80a2700:	68a3      	ldr	r3, [r4, #8]
  TFLITE_DCHECK_LE(params.input1_offset, int8_max_value);
 80a2702:	dcf1      	bgt.n	80a26e8 <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x16>
  TFLITE_DCHECK_LE(params.input2_offset, int8_max_value);
 80a2704:	337f      	adds	r3, #127	; 0x7f
 80a2706:	2bfe      	cmp	r3, #254	; 0xfe
 80a2708:	d8ee      	bhi.n	80a26e8 <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x16>
 80a270a:	2500      	movs	r5, #0
  TFLITE_DCHECK_GE(params.input1_offset, -1 * int8_max_value);
  TFLITE_DCHECK_GE(params.input2_offset, -1 * int8_max_value);
  TFLITE_DCHECK_LE(params.input1_offset, int8_max_value);
  TFLITE_DCHECK_LE(params.input2_offset, int8_max_value);

  for (int i = 0; i < size; ++i) {
 80a270c:	45a9      	cmp	r9, r5
 80a270e:	dd28      	ble.n	80a2762 <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x90>
    const int32 input1_val = params.input1_offset + input1_data[i];
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
 80a2710:	f917 a005 	ldrsb.w	sl, [r7, r5]
 80a2714:	68a3      	ldr	r3, [r4, #8]
  TFLITE_DCHECK_LE(params.input2_offset, int8_max_value);

  for (int i = 0; i < size; ++i) {
    const int32 input1_val = params.input1_offset + input1_data[i];
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
 80a2716:	69a0      	ldr	r0, [r4, #24]
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
 80a2718:	4453      	add	r3, sl
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, params.input1_multiplier, params.input1_shift);
 80a271a:	f918 2005 	ldrsb.w	r2, [r8, r5]

  for (int i = 0; i < size; ++i) {
    const int32 input1_val = params.input1_offset + input1_data[i];
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
 80a271e:	fa03 fa00 	lsl.w	sl, r3, r0
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, params.input1_multiplier, params.input1_shift);
 80a2722:	6863      	ldr	r3, [r4, #4]
 80a2724:	69e1      	ldr	r1, [r4, #28]
 80a2726:	4413      	add	r3, r2
 80a2728:	fa03 f000 	lsl.w	r0, r3, r0
 80a272c:	6a22      	ldr	r2, [r4, #32]
 80a272e:	f7ff ff0f 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, params.input2_multiplier, params.input2_shift);
 80a2732:	6aa2      	ldr	r2, [r4, #40]	; 0x28
 80a2734:	6a61      	ldr	r1, [r4, #36]	; 0x24
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, params.input1_multiplier, params.input1_shift);
 80a2736:	4683      	mov	fp, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, params.input2_multiplier, params.input2_shift);
 80a2738:	4650      	mov	r0, sl
 80a273a:	f7ff ff09 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 raw_sum = scaled_input1_val + scaled_input2_val;
    const int32 raw_output =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
 80a273e:	6962      	ldr	r2, [r4, #20]
 80a2740:	6921      	ldr	r1, [r4, #16]
 80a2742:	4458      	add	r0, fp
 80a2744:	f7ff ff04 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
            raw_sum, params.output_multiplier, params.output_shift) +
        params.output_offset;
    const int32 clamped_output =
        std::min(params.quantized_activation_max,
                 std::max(params.quantized_activation_min, raw_output));
    output_data[i] = static_cast<int8_t>(clamped_output);
 80a2748:	68e3      	ldr	r3, [r4, #12]
 80a274a:	4418      	add	r0, r3
 80a274c:	6ae3      	ldr	r3, [r4, #44]	; 0x2c
 80a274e:	4298      	cmp	r0, r3
 80a2750:	bfb8      	it	lt
 80a2752:	4618      	movlt	r0, r3
 80a2754:	6b23      	ldr	r3, [r4, #48]	; 0x30
 80a2756:	4283      	cmp	r3, r0
 80a2758:	bfa8      	it	ge
 80a275a:	4603      	movge	r3, r0
 80a275c:	5573      	strb	r3, [r6, r5]
  TFLITE_DCHECK_GE(params.input1_offset, -1 * int8_max_value);
  TFLITE_DCHECK_GE(params.input2_offset, -1 * int8_max_value);
  TFLITE_DCHECK_LE(params.input1_offset, int8_max_value);
  TFLITE_DCHECK_LE(params.input2_offset, int8_max_value);

  for (int i = 0; i < size; ++i) {
 80a275e:	3501      	adds	r5, #1
 80a2760:	e7d4      	b.n	80a270c <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x3a>
 80a2762:	e8bd 8ff8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a2766 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE>:
//
// Returns true iff there is some sort of broadcast, which includes five-fold
// patterns and falling back to generic broadcast.
inline bool ProcessBroadcastShapes(const RuntimeShape& shape0,
                                   const RuntimeShape& shape1,
                                   tflite::ArithmeticParams* params) {
 80a2766:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
 80a276a:	6804      	ldr	r4, [r0, #0]
 80a276c:	680b      	ldr	r3, [r1, #0]
 80a276e:	b091      	sub	sp, #68	; 0x44
 80a2770:	429c      	cmp	r4, r3
 80a2772:	bfb8      	it	lt
 80a2774:	461c      	movlt	r4, r3
      dims_pointer_ = new int32[dimensions_count];
#endif  // TF_LITE_STATIC_MEMORY
    }
  }

  RuntimeShape(int shape_size, int32 value) : size_(0) {
 80a2776:	af10      	add	r7, sp, #64	; 0x40
 80a2778:	2600      	movs	r6, #0
  const int dims_count =
      std::max(shape0.DimensionsCount(), shape1.DimensionsCount());

  params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
 80a277a:	2304      	movs	r3, #4
 80a277c:	f847 6d3c 	str.w	r6, [r7, #-60]!
//
// Returns true iff there is some sort of broadcast, which includes five-fold
// patterns and falling back to generic broadcast.
inline bool ProcessBroadcastShapes(const RuntimeShape& shape0,
                                   const RuntimeShape& shape1,
                                   tflite::ArithmeticParams* params) {
 80a2780:	4681      	mov	r9, r0
 80a2782:	4688      	mov	r8, r1
  const int dims_count =
      std::max(shape0.DimensionsCount(), shape1.DimensionsCount());

  params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
 80a2784:	7013      	strb	r3, [r2, #0]
    Resize(shape_size);
 80a2786:	4621      	mov	r1, r4
 80a2788:	4638      	mov	r0, r7
//
// Returns true iff there is some sort of broadcast, which includes five-fold
// patterns and falling back to generic broadcast.
inline bool ProcessBroadcastShapes(const RuntimeShape& shape0,
                                   const RuntimeShape& shape1,
                                   tflite::ArithmeticParams* params) {
 80a278a:	4615      	mov	r5, r2
 80a278c:	f7ff fe3e 	bl	80a240c <_ZN6tflite12RuntimeShape6ResizeEi>
    for (int i = 0; i < shape_size; ++i) {
 80a2790:	42a6      	cmp	r6, r4
 80a2792:	da06      	bge.n	80a27a2 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x3c>
      SetDim(i, value);
 80a2794:	4631      	mov	r1, r6
 80a2796:	2201      	movs	r2, #1
 80a2798:	4638      	mov	r0, r7
 80a279a:	f7ff fe26 	bl	80a23ea <_ZN6tflite12RuntimeShape6SetDimEil>
    }
  }

  RuntimeShape(int shape_size, int32 value) : size_(0) {
    Resize(shape_size);
    for (int i = 0; i < shape_size; ++i) {
 80a279e:	3601      	adds	r6, #1
 80a27a0:	e7f6      	b.n	80a2790 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x2a>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
 80a27a2:	2301      	movs	r3, #1
 80a27a4:	464a      	mov	r2, r9
 80a27a6:	4621      	mov	r1, r4
 80a27a8:	a806      	add	r0, sp, #24
 80a27aa:	f7ff fe46 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a27ae:	2301      	movs	r3, #1
 80a27b0:	4642      	mov	r2, r8
 80a27b2:	4621      	mov	r1, r4
 80a27b4:	a80b      	add	r0, sp, #44	; 0x2c
 80a27b6:	f7ff fe40 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
    }
    std::memcpy(DimsData(), other.DimsData(), sizeof(int32) * size_);
  }

  bool operator==(const RuntimeShape& comp) const {
    return this->size_ == comp.size_ &&
 80a27ba:	9a06      	ldr	r2, [sp, #24]
 80a27bc:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80a27be:	429a      	cmp	r2, r3
 80a27c0:	d10d      	bne.n	80a27de <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x78>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80a27c2:	2a04      	cmp	r2, #4
 80a27c4:	bfc7      	ittee	gt
 80a27c6:	9807      	ldrgt	r0, [sp, #28]
 80a27c8:	990c      	ldrgt	r1, [sp, #48]	; 0x30
 80a27ca:	a807      	addle	r0, sp, #28
 80a27cc:	a90c      	addle	r1, sp, #48	; 0x30
    std::memcpy(DimsData(), other.DimsData(), sizeof(int32) * size_);
  }

  bool operator==(const RuntimeShape& comp) const {
    return this->size_ == comp.size_ &&
           std::memcmp(DimsData(), comp.DimsData(), size_ * sizeof(int32)) == 0;
 80a27ce:	0092      	lsls	r2, r2, #2
 80a27d0:	f011 f96c 	bl	80b3aac <memcmp>
    }
    std::memcpy(DimsData(), other.DimsData(), sizeof(int32) * size_);
  }

  bool operator==(const RuntimeShape& comp) const {
    return this->size_ == comp.size_ &&
 80a27d4:	b918      	cbnz	r0, 80a27de <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x78>
  auto extended_shape0 = RuntimeShape::ExtendedShape(dims_count, shape0);
  auto extended_shape1 = RuntimeShape::ExtendedShape(dims_count, shape1);

  // Check for "exact" match, implicitly accepting any scalar shapes.
  if (extended_shape0 == extended_shape1) {
    params->broadcast_category = BroadcastableOpCategory::kNonBroadcast;
 80a27d6:	2301      	movs	r3, #1
 80a27d8:	702b      	strb	r3, [r5, #0]

  if (params->broadcast_category !=
          BroadcastableOpCategory::kFirstInputBroadcastsFast &&
      params->broadcast_category !=
          BroadcastableOpCategory::kSecondInputBroadcastsFast) {
    return false;
 80a27da:	2400      	movs	r4, #0
 80a27dc:	e08c      	b.n	80a28f8 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x192>
  if (extended_shape0 == extended_shape1) {
    params->broadcast_category = BroadcastableOpCategory::kNonBroadcast;
    return false;
  }

  for (int i = dims_count - 1; i >= 0; --i) {
 80a27de:	3c01      	subs	r4, #1
 80a27e0:	4626      	mov	r6, r4
 80a27e2:	2e00      	cmp	r6, #0
 80a27e4:	db13      	blt.n	80a280e <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0xa8>
    if (extended_shape0.Dims(i) == extended_shape1.Dims(i)) {
 80a27e6:	4631      	mov	r1, r6
 80a27e8:	a806      	add	r0, sp, #24
 80a27ea:	f7ff fded 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a27ee:	4631      	mov	r1, r6
 80a27f0:	4680      	mov	r8, r0
 80a27f2:	a80b      	add	r0, sp, #44	; 0x2c
 80a27f4:	f7ff fde8 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a27f8:	4580      	cmp	r8, r0
 80a27fa:	d04d      	beq.n	80a2898 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x132>
      continue;
    } else if (extended_shape0.Dims(i) == 1) {
 80a27fc:	f1b8 0f01 	cmp.w	r8, #1
 80a2800:	d101      	bne.n	80a2806 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0xa0>
      params->broadcast_category =
          BroadcastableOpCategory::kFirstInputBroadcastsFast;
 80a2802:	2302      	movs	r3, #2
 80a2804:	e002      	b.n	80a280c <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0xa6>
      break;
    } else if (extended_shape1.Dims(i) == 1) {
 80a2806:	2801      	cmp	r0, #1
 80a2808:	d143      	bne.n	80a2892 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x12c>
      params->broadcast_category =
          BroadcastableOpCategory::kSecondInputBroadcastsFast;
 80a280a:	2303      	movs	r3, #3
 80a280c:	702b      	strb	r3, [r5, #0]
      params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
      return true;
    }
  }

  if (params->broadcast_category !=
 80a280e:	782b      	ldrb	r3, [r5, #0]
 80a2810:	1e9a      	subs	r2, r3, #2
 80a2812:	2a01      	cmp	r2, #1
 80a2814:	d8e1      	bhi.n	80a27da <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x74>
  // From this point it is assumed contractually that corresponding dimensions
  // in shape0 and shape1 are either (a) equal or (b) one or other equals 1.
  const bool swap_inputs = params->broadcast_category ==
                           BroadcastableOpCategory::kSecondInputBroadcastsFast;
  const RuntimeShape* shape_a =
      swap_inputs ? &extended_shape1 : &extended_shape0;
 80a2816:	2b03      	cmp	r3, #3
  const RuntimeShape* shape_b =
      swap_inputs ? &extended_shape0 : &extended_shape1;

  int i = dims_count - 1;
  params->broadcast_shape[0] = 1;
 80a2818:	f04f 0301 	mov.w	r3, #1
  // From this point it is assumed contractually that corresponding dimensions
  // in shape0 and shape1 are either (a) equal or (b) one or other equals 1.
  const bool swap_inputs = params->broadcast_category ==
                           BroadcastableOpCategory::kSecondInputBroadcastsFast;
  const RuntimeShape* shape_a =
      swap_inputs ? &extended_shape1 : &extended_shape0;
 80a281c:	bf07      	ittee	eq
 80a281e:	f10d 082c 	addeq.w	r8, sp, #44	; 0x2c
  const RuntimeShape* shape_b =
      swap_inputs ? &extended_shape0 : &extended_shape1;
 80a2822:	ae06      	addeq	r6, sp, #24
  // From this point it is assumed contractually that corresponding dimensions
  // in shape0 and shape1 are either (a) equal or (b) one or other equals 1.
  const bool swap_inputs = params->broadcast_category ==
                           BroadcastableOpCategory::kSecondInputBroadcastsFast;
  const RuntimeShape* shape_a =
      swap_inputs ? &extended_shape1 : &extended_shape0;
 80a2824:	f10d 0818 	addne.w	r8, sp, #24
  const RuntimeShape* shape_b =
      swap_inputs ? &extended_shape0 : &extended_shape1;
 80a2828:	ae0b      	addne	r6, sp, #44	; 0x2c

  int i = dims_count - 1;
  params->broadcast_shape[0] = 1;
 80a282a:	63eb      	str	r3, [r5, #60]	; 0x3c
  params->broadcast_shape[1] = 1;
 80a282c:	642b      	str	r3, [r5, #64]	; 0x40
  params->broadcast_shape[2] = 1;
 80a282e:	646b      	str	r3, [r5, #68]	; 0x44
  params->broadcast_shape[3] = 1;
 80a2830:	64ab      	str	r3, [r5, #72]	; 0x48
  params->broadcast_shape[4] = 1;
 80a2832:	64eb      	str	r3, [r5, #76]	; 0x4c
  // y_0 is greedy: include dims if both or neither equal 1: in other words,
  // test for equality rather than (shape_a->Dims(i) != 1).
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
 80a2834:	2c00      	cmp	r4, #0
 80a2836:	db5e      	blt.n	80a28f6 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x190>
 80a2838:	4621      	mov	r1, r4
 80a283a:	4640      	mov	r0, r8
 80a283c:	f7ff fdc4 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a2840:	4621      	mov	r1, r4
 80a2842:	4681      	mov	r9, r0
 80a2844:	4630      	mov	r0, r6
 80a2846:	f7ff fdbf 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a284a:	4581      	cmp	r9, r0
 80a284c:	d026      	beq.n	80a289c <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x136>
    params->broadcast_shape[4] *= shape_b->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).  If it is input_b
  // that has the unit dimension, the next two loops are not entered.
  while (i >= 0 && shape_a->Dims(i) == 1) {
 80a284e:	4621      	mov	r1, r4
 80a2850:	4640      	mov	r0, r8
 80a2852:	f7ff fdb9 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a2856:	2801      	cmp	r0, #1
 80a2858:	d026      	beq.n	80a28a8 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x142>
    params->broadcast_shape[3] *= shape_b->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
 80a285a:	4621      	mov	r1, r4
 80a285c:	4640      	mov	r0, r8
 80a285e:	f7ff fdb3 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a2862:	4621      	mov	r1, r4
 80a2864:	4681      	mov	r9, r0
 80a2866:	4630      	mov	r0, r6
 80a2868:	f7ff fdae 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a286c:	4581      	cmp	r9, r0
 80a286e:	d027      	beq.n	80a28c0 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x15a>
    params->broadcast_shape[2] *= shape_a->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).
  while (i >= 0 && shape_b->Dims(i) == 1) {
 80a2870:	4621      	mov	r1, r4
 80a2872:	4630      	mov	r0, r6
 80a2874:	f7ff fda8 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a2878:	2801      	cmp	r0, #1
 80a287a:	d029      	beq.n	80a28d0 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x16a>
    params->broadcast_shape[1] *= shape_a->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
 80a287c:	4621      	mov	r1, r4
 80a287e:	4640      	mov	r0, r8
 80a2880:	f7ff fda2 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a2884:	4621      	mov	r1, r4
 80a2886:	4681      	mov	r9, r0
 80a2888:	4630      	mov	r0, r6
 80a288a:	f7ff fd9d 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a288e:	4581      	cmp	r9, r0
 80a2890:	d02a      	beq.n	80a28e8 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x182>
          BroadcastableOpCategory::kSecondInputBroadcastsFast;
      break;
    } else {
      // This case is erroneous: there is a dimension that does not match and
      // is not a broadcast from one shape to the other.
      params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
 80a2892:	2304      	movs	r3, #4
 80a2894:	702b      	strb	r3, [r5, #0]
 80a2896:	e02e      	b.n	80a28f6 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x190>
  if (extended_shape0 == extended_shape1) {
    params->broadcast_category = BroadcastableOpCategory::kNonBroadcast;
    return false;
  }

  for (int i = dims_count - 1; i >= 0; --i) {
 80a2898:	3e01      	subs	r6, #1
 80a289a:	e7a2      	b.n	80a27e2 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x7c>
  params->broadcast_shape[3] = 1;
  params->broadcast_shape[4] = 1;
  // y_0 is greedy: include dims if both or neither equal 1: in other words,
  // test for equality rather than (shape_a->Dims(i) != 1).
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
    params->broadcast_shape[4] *= shape_b->Dims(i);
 80a289c:	6ceb      	ldr	r3, [r5, #76]	; 0x4c
    --i;
 80a289e:	3c01      	subs	r4, #1
  params->broadcast_shape[3] = 1;
  params->broadcast_shape[4] = 1;
  // y_0 is greedy: include dims if both or neither equal 1: in other words,
  // test for equality rather than (shape_a->Dims(i) != 1).
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
    params->broadcast_shape[4] *= shape_b->Dims(i);
 80a28a0:	fb09 f303 	mul.w	r3, r9, r3
 80a28a4:	64eb      	str	r3, [r5, #76]	; 0x4c
  params->broadcast_shape[2] = 1;
  params->broadcast_shape[3] = 1;
  params->broadcast_shape[4] = 1;
  // y_0 is greedy: include dims if both or neither equal 1: in other words,
  // test for equality rather than (shape_a->Dims(i) != 1).
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
 80a28a6:	e7c5      	b.n	80a2834 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0xce>
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).  If it is input_b
  // that has the unit dimension, the next two loops are not entered.
  while (i >= 0 && shape_a->Dims(i) == 1) {
    params->broadcast_shape[3] *= shape_b->Dims(i);
 80a28a8:	4621      	mov	r1, r4
 80a28aa:	4630      	mov	r0, r6
 80a28ac:	f7ff fd8c 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a28b0:	6cab      	ldr	r3, [r5, #72]	; 0x48
    params->broadcast_shape[4] *= shape_b->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).  If it is input_b
  // that has the unit dimension, the next two loops are not entered.
  while (i >= 0 && shape_a->Dims(i) == 1) {
 80a28b2:	f114 34ff 	adds.w	r4, r4, #4294967295	; 0xffffffff
    params->broadcast_shape[3] *= shape_b->Dims(i);
 80a28b6:	fb00 f003 	mul.w	r0, r0, r3
 80a28ba:	64a8      	str	r0, [r5, #72]	; 0x48
    params->broadcast_shape[4] *= shape_b->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).  If it is input_b
  // that has the unit dimension, the next two loops are not entered.
  while (i >= 0 && shape_a->Dims(i) == 1) {
 80a28bc:	d2c7      	bcs.n	80a284e <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0xe8>
 80a28be:	e01a      	b.n	80a28f6 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x190>
    params->broadcast_shape[3] *= shape_b->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
    params->broadcast_shape[2] *= shape_a->Dims(i);
 80a28c0:	6c6b      	ldr	r3, [r5, #68]	; 0x44
  // that has the unit dimension, the next two loops are not entered.
  while (i >= 0 && shape_a->Dims(i) == 1) {
    params->broadcast_shape[3] *= shape_b->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
 80a28c2:	f114 34ff 	adds.w	r4, r4, #4294967295	; 0xffffffff
    params->broadcast_shape[2] *= shape_a->Dims(i);
 80a28c6:	fb09 f303 	mul.w	r3, r9, r3
 80a28ca:	646b      	str	r3, [r5, #68]	; 0x44
  // that has the unit dimension, the next two loops are not entered.
  while (i >= 0 && shape_a->Dims(i) == 1) {
    params->broadcast_shape[3] *= shape_b->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
 80a28cc:	d2c5      	bcs.n	80a285a <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0xf4>
 80a28ce:	e012      	b.n	80a28f6 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x190>
    params->broadcast_shape[2] *= shape_a->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).
  while (i >= 0 && shape_b->Dims(i) == 1) {
    params->broadcast_shape[1] *= shape_a->Dims(i);
 80a28d0:	4621      	mov	r1, r4
 80a28d2:	4640      	mov	r0, r8
 80a28d4:	f7ff fd78 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a28d8:	6c2b      	ldr	r3, [r5, #64]	; 0x40
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
    params->broadcast_shape[2] *= shape_a->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).
  while (i >= 0 && shape_b->Dims(i) == 1) {
 80a28da:	f114 34ff 	adds.w	r4, r4, #4294967295	; 0xffffffff
    params->broadcast_shape[1] *= shape_a->Dims(i);
 80a28de:	fb00 f003 	mul.w	r0, r0, r3
 80a28e2:	6428      	str	r0, [r5, #64]	; 0x40
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
    params->broadcast_shape[2] *= shape_a->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).
  while (i >= 0 && shape_b->Dims(i) == 1) {
 80a28e4:	d2c4      	bcs.n	80a2870 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x10a>
 80a28e6:	e006      	b.n	80a28f6 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x190>
    params->broadcast_shape[1] *= shape_a->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
    params->broadcast_shape[0] *= shape_b->Dims(i);
 80a28e8:	6beb      	ldr	r3, [r5, #60]	; 0x3c
  // Here either input_a or input_b has dim of 1 (if i >= 0).
  while (i >= 0 && shape_b->Dims(i) == 1) {
    params->broadcast_shape[1] *= shape_a->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
 80a28ea:	f114 34ff 	adds.w	r4, r4, #4294967295	; 0xffffffff
    params->broadcast_shape[0] *= shape_b->Dims(i);
 80a28ee:	fb09 f303 	mul.w	r3, r9, r3
 80a28f2:	63eb      	str	r3, [r5, #60]	; 0x3c
  // Here either input_a or input_b has dim of 1 (if i >= 0).
  while (i >= 0 && shape_b->Dims(i) == 1) {
    params->broadcast_shape[1] *= shape_a->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
 80a28f4:	d2c2      	bcs.n	80a287c <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x116>
  // Rarer case is when the broadcast dimensions cannot be handled by a fivefold
  // loop.
  if (i >= 0) {
    params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
  }
  return true;
 80a28f6:	2401      	movs	r4, #1

  params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
  RuntimeShape scalar_shape(dims_count, 1);

  auto extended_shape0 = RuntimeShape::ExtendedShape(dims_count, shape0);
  auto extended_shape1 = RuntimeShape::ExtendedShape(dims_count, shape1);
 80a28f8:	a80b      	add	r0, sp, #44	; 0x2c
 80a28fa:	f7ff fd5a 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
      std::max(shape0.DimensionsCount(), shape1.DimensionsCount());

  params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
  RuntimeShape scalar_shape(dims_count, 1);

  auto extended_shape0 = RuntimeShape::ExtendedShape(dims_count, shape0);
 80a28fe:	a806      	add	r0, sp, #24
 80a2900:	f7ff fd57 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                                   tflite::ArithmeticParams* params) {
  const int dims_count =
      std::max(shape0.DimensionsCount(), shape1.DimensionsCount());

  params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
  RuntimeShape scalar_shape(dims_count, 1);
 80a2904:	4638      	mov	r0, r7
 80a2906:	f7ff fd54 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  // loop.
  if (i >= 0) {
    params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
  }
  return true;
}
 80a290a:	4620      	mov	r0, r4
 80a290c:	b011      	add	sp, #68	; 0x44
 80a290e:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}

080a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>:
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
}

inline RuntimeShape GetTensorShape(const TfLiteTensor* tensor) {
 80a2912:	b570      	push	{r4, r5, r6, lr}
 80a2914:	4604      	mov	r4, r0
  if (tensor == nullptr) {
 80a2916:	b909      	cbnz	r1, 80a291c <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor+0xa>
  // larger shapes are separately allocated.
  static constexpr int kMaxSmallSize = 4;

  RuntimeShape& operator=(RuntimeShape const&) = delete;

  RuntimeShape() : size_(0) {}
 80a2918:	6001      	str	r1, [r0, #0]
 80a291a:	e010      	b.n	80a293e <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor+0x2c>
    return RuntimeShape();
  }

  TfLiteIntArray* dims = tensor->dims;
  const int dims_size = dims->size;
 80a291c:	688d      	ldr	r5, [r1, #8]
    for (int i = 0; i < shape_size; ++i) {
      SetDim(i, value);
    }
  }

  RuntimeShape(int dimensions_count, const int32* dims_data) : size_(0) {
 80a291e:	2300      	movs	r3, #0
 80a2920:	f855 6b04 	ldr.w	r6, [r5], #4
 80a2924:	6003      	str	r3, [r0, #0]
#endif  // TF_LITE_STATIC_MEMORY
    }
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
    Resize(dimensions_count);
 80a2926:	4631      	mov	r1, r6
 80a2928:	f7ff fd70 	bl	80a240c <_ZN6tflite12RuntimeShape6ResizeEi>
      dims_[i] = val;
    }
  }

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80a292c:	6823      	ldr	r3, [r4, #0]
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
    Resize(dimensions_count);
    int32* dst_dims = DimsData();
    std::memcpy(dst_dims, dims_data, dimensions_count * sizeof(int32));
 80a292e:	00b2      	lsls	r2, r6, #2
      dims_[i] = val;
    }
  }

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80a2930:	2b04      	cmp	r3, #4
 80a2932:	bfcc      	ite	gt
 80a2934:	6860      	ldrgt	r0, [r4, #4]
 80a2936:	1d20      	addle	r0, r4, #4
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
    Resize(dimensions_count);
    int32* dst_dims = DimsData();
    std::memcpy(dst_dims, dims_data, dimensions_count * sizeof(int32));
 80a2938:	4629      	mov	r1, r5
 80a293a:	f011 f8c6 	bl	80b3aca <memcpy>
  const int32_t* dims_data = reinterpret_cast<const int32_t*>(dims->data);
  return RuntimeShape(dims_size, dims_data);
}
 80a293e:	4620      	mov	r0, r4
 80a2940:	bd70      	pop	{r4, r5, r6, pc}

080a2942 <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE>:

TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteAddParams* params,
                             const TfLiteTensor* input1,
                             const TfLiteTensor* input2, TfLiteTensor* output,
                             OpData* data) {
 80a2942:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  data->requires_broadcast = !HaveSameShapes(input1, input2);
 80a2946:	4610      	mov	r0, r2
}

TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteAddParams* params,
                             const TfLiteTensor* input1,
                             const TfLiteTensor* input2, TfLiteTensor* output,
                             OpData* data) {
 80a2948:	b085      	sub	sp, #20
 80a294a:	4689      	mov	r9, r1
  data->requires_broadcast = !HaveSameShapes(input1, input2);
 80a294c:	4619      	mov	r1, r3
}

TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteAddParams* params,
                             const TfLiteTensor* input1,
                             const TfLiteTensor* input2, TfLiteTensor* output,
                             OpData* data) {
 80a294e:	461e      	mov	r6, r3
 80a2950:	9d0e      	ldr	r5, [sp, #56]	; 0x38
 80a2952:	9c0f      	ldr	r4, [sp, #60]	; 0x3c
 80a2954:	4617      	mov	r7, r2
  data->requires_broadcast = !HaveSameShapes(input1, input2);
 80a2956:	f00d f8d9 	bl	80afb0c <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
 80a295a:	f080 0001 	eor.w	r0, r0, #1
 80a295e:	7020      	strb	r0, [r4, #0]

  if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt8) {
 80a2960:	782b      	ldrb	r3, [r5, #0]
 80a2962:	2b03      	cmp	r3, #3
 80a2964:	d001      	beq.n	80a296a <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE+0x28>
 80a2966:	2b09      	cmp	r3, #9
 80a2968:	d162      	bne.n	80a2a30 <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE+0xee>
    // 8bit -> 8bit general quantized path, with general rescalings
    data->input1_offset = -input1->params.zero_point;
 80a296a:	693b      	ldr	r3, [r7, #16]
 80a296c:	425b      	negs	r3, r3
 80a296e:	62a3      	str	r3, [r4, #40]	; 0x28
    data->input2_offset = -input2->params.zero_point;
 80a2970:	6933      	ldr	r3, [r6, #16]
 80a2972:	425b      	negs	r3, r3
 80a2974:	62e3      	str	r3, [r4, #44]	; 0x2c
    data->output_offset = output->params.zero_point;
 80a2976:	692b      	ldr	r3, [r5, #16]
 80a2978:	6323      	str	r3, [r4, #48]	; 0x30
    data->left_shift = 20;
 80a297a:	2314      	movs	r3, #20
 80a297c:	6263      	str	r3, [r4, #36]	; 0x24
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
 80a297e:	f8d7 800c 	ldr.w	r8, [r7, #12]
 80a2982:	f8d6 a00c 	ldr.w	sl, [r6, #12]
 80a2986:	4640      	mov	r0, r8
 80a2988:	4651      	mov	r1, sl
 80a298a:	f010 ffcd 	bl	80b3928 <__aeabi_fcmplt>
 80a298e:	b108      	cbz	r0, 80a2994 <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE+0x52>
    const double twice_max_input_scale =
        2 * std::max(input1->params.scale, input2->params.scale);
 80a2990:	4651      	mov	r1, sl
 80a2992:	e000      	b.n	80a2996 <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE+0x54>
 80a2994:	4641      	mov	r1, r8
 80a2996:	4608      	mov	r0, r1
 80a2998:	f010 fd20 	bl	80b33dc <__addsf3>
 80a299c:	f010 f992 	bl	80b2cc4 <__aeabi_f2d>
 80a29a0:	4606      	mov	r6, r0
 80a29a2:	460f      	mov	r7, r1
    const double real_input1_multiplier =
        input1->params.scale / twice_max_input_scale;
    const double real_input2_multiplier =
        input2->params.scale / twice_max_input_scale;
 80a29a4:	4650      	mov	r0, sl
 80a29a6:	f010 f98d 	bl	80b2cc4 <__aeabi_f2d>
 80a29aa:	4632      	mov	r2, r6
 80a29ac:	463b      	mov	r3, r7
 80a29ae:	f010 fb07 	bl	80b2fc0 <__aeabi_ddiv>
 80a29b2:	e9cd 0100 	strd	r0, r1, [sp]
    const double real_output_multiplier =
        twice_max_input_scale /
        ((1 << data->left_shift) * output->params.scale);
 80a29b6:	f04f 4193 	mov.w	r1, #1233125376	; 0x49800000
 80a29ba:	68e8      	ldr	r0, [r5, #12]
 80a29bc:	f010 fe16 	bl	80b35ec <__aeabi_fmul>
 80a29c0:	f010 f980 	bl	80b2cc4 <__aeabi_f2d>
 80a29c4:	4602      	mov	r2, r0
 80a29c6:	460b      	mov	r3, r1
 80a29c8:	4630      	mov	r0, r6
 80a29ca:	4639      	mov	r1, r7
 80a29cc:	f010 faf8 	bl	80b2fc0 <__aeabi_ddiv>
 80a29d0:	e9cd 0102 	strd	r0, r1, [sp, #8]

    QuantizeMultiplierSmallerThanOneExp(
        real_input1_multiplier, &data->input1_multiplier, &data->input1_shift);
 80a29d4:	4640      	mov	r0, r8
 80a29d6:	f010 f975 	bl	80b2cc4 <__aeabi_f2d>
        input2->params.scale / twice_max_input_scale;
    const double real_output_multiplier =
        twice_max_input_scale /
        ((1 << data->left_shift) * output->params.scale);

    QuantizeMultiplierSmallerThanOneExp(
 80a29da:	f104 0b04 	add.w	fp, r4, #4
        real_input1_multiplier, &data->input1_multiplier, &data->input1_shift);
 80a29de:	4632      	mov	r2, r6
 80a29e0:	463b      	mov	r3, r7
        input2->params.scale / twice_max_input_scale;
    const double real_output_multiplier =
        twice_max_input_scale /
        ((1 << data->left_shift) * output->params.scale);

    QuantizeMultiplierSmallerThanOneExp(
 80a29e2:	f104 0a14 	add.w	sl, r4, #20
        real_input1_multiplier, &data->input1_multiplier, &data->input1_shift);
 80a29e6:	f010 faeb 	bl	80b2fc0 <__aeabi_ddiv>
 80a29ea:	465b      	mov	r3, fp
 80a29ec:	4652      	mov	r2, sl
 80a29ee:	f00d f8e7 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>

    QuantizeMultiplierSmallerThanOneExp(
        real_input2_multiplier, &data->input2_multiplier, &data->input2_shift);
 80a29f2:	e9dd 0100 	ldrd	r0, r1, [sp]
 80a29f6:	f104 0308 	add.w	r3, r4, #8
 80a29fa:	f104 0218 	add.w	r2, r4, #24
 80a29fe:	f00d f8df 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>

    QuantizeMultiplierSmallerThanOneExp(
        real_output_multiplier, &data->output_multiplier, &data->output_shift);
 80a2a02:	e9dd 0102 	ldrd	r0, r1, [sp, #8]
 80a2a06:	f104 0320 	add.w	r3, r4, #32
 80a2a0a:	f104 021c 	add.w	r2, r4, #28
 80a2a0e:	f00d f8d7 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>

    if (output->type == kTfLiteUInt8) {
 80a2a12:	782b      	ldrb	r3, [r5, #0]
      CalculateActivationRangeUint8(params->activation, output,
                                    &data->output_activation_min,
                                    &data->output_activation_max);
 80a2a14:	4629      	mov	r1, r5
        real_input2_multiplier, &data->input2_multiplier, &data->input2_shift);

    QuantizeMultiplierSmallerThanOneExp(
        real_output_multiplier, &data->output_multiplier, &data->output_shift);

    if (output->type == kTfLiteUInt8) {
 80a2a16:	2b03      	cmp	r3, #3
 80a2a18:	f104 020c 	add.w	r2, r4, #12
 80a2a1c:	f104 0310 	add.w	r3, r4, #16
 80a2a20:	f899 0000 	ldrb.w	r0, [r9]
 80a2a24:	d102      	bne.n	80a2a2c <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE+0xea>
      CalculateActivationRangeUint8(params->activation, output,
                                    &data->output_activation_min,
                                    &data->output_activation_max);
 80a2a26:	f00c ff71 	bl	80af90c <_ZN6tflite29CalculateActivationRangeUint8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>
 80a2a2a:	e001      	b.n	80a2a30 <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE+0xee>
    } else {
      CalculateActivationRangeInt8(params->activation, output,
                                   &data->output_activation_min,
                                   &data->output_activation_max);
 80a2a2c:	f00d f862 	bl	80afaf4 <_ZN6tflite28CalculateActivationRangeInt8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>
    }
  }

  return kTfLiteOk;
}
 80a2a30:	2000      	movs	r0, #0
 80a2a32:	b005      	add	sp, #20
 80a2a34:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a2a38 <_ZN6tflite3ops5micro12Register_ADDEv>:
}  // namespace add

TfLiteRegistration* Register_ADD() {
  static TfLiteRegistration r = {add::Init, add::Free, add::Prepare, add::Eval};
  return &r;
}
 80a2a38:	4800      	ldr	r0, [pc, #0]	; (80a2a3c <_ZN6tflite3ops5micro12Register_ADDEv+0x4>)
 80a2a3a:	4770      	bx	lr
 80a2a3c:	20000008 	.word	0x20000008

080a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>:
    }
  }
}

template <int N>
inline void NdArrayDescsForElementwiseBroadcast(
 80a2a40:	e92d 47f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, lr}
 80a2a44:	460e      	mov	r6, r1
 80a2a46:	b08a      	sub	sp, #40	; 0x28
 80a2a48:	461d      	mov	r5, r3
    const RuntimeShape& input0_shape, const RuntimeShape& input1_shape,
    NdArrayDesc<N>* desc0_out, NdArrayDesc<N>* desc1_out) {
  TFLITE_DCHECK(desc0_out != nullptr);
 80a2a4a:	4614      	mov	r4, r2
 80a2a4c:	b90a      	cbnz	r2, 80a2a52 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0x12>
 80a2a4e:	f00d faef 	bl	80b0030 <abort>
  TFLITE_DCHECK(desc1_out != nullptr);
 80a2a52:	2b00      	cmp	r3, #0
 80a2a54:	d0fb      	beq.n	80a2a4e <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0xe>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
 80a2a56:	4602      	mov	r2, r0
 80a2a58:	2301      	movs	r3, #1
 80a2a5a:	2104      	movs	r1, #4
 80a2a5c:	4668      	mov	r0, sp
 80a2a5e:	f7ff fcec 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
 80a2a62:	f04f 0901 	mov.w	r9, #1
 80a2a66:	4632      	mov	r2, r6
 80a2a68:	2301      	movs	r3, #1
 80a2a6a:	2104      	movs	r1, #4
 80a2a6c:	a805      	add	r0, sp, #20
 80a2a6e:	f7ff fce4 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a2a72:	46a0      	mov	r8, r4
 80a2a74:	462f      	mov	r7, r5

  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
 80a2a76:	46ca      	mov	sl, r9
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
 80a2a78:	2603      	movs	r6, #3
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
 80a2a7a:	4631      	mov	r1, r6
 80a2a7c:	4668      	mov	r0, sp
 80a2a7e:	f7ff fca3 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
    desc0_out->strides[i] = desc0_stride;
 80a2a82:	f8c8 a01c 	str.w	sl, [r8, #28]

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
 80a2a86:	f8c8 000c 	str.w	r0, [r8, #12]
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
 80a2a8a:	4631      	mov	r1, r6
 80a2a8c:	4668      	mov	r0, sp
 80a2a8e:	f7ff fc9b 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
 80a2a92:	4631      	mov	r1, r6
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
 80a2a94:	fb00 fa0a 	mul.w	sl, r0, sl
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
 80a2a98:	a805      	add	r0, sp, #20
 80a2a9a:	f7ff fc95 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
    desc1_out->strides[i] = desc1_stride;
 80a2a9e:	f8c7 901c 	str.w	r9, [r7, #28]
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
 80a2aa2:	60f8      	str	r0, [r7, #12]
    desc1_out->strides[i] = desc1_stride;
    desc1_stride *= extended_input1_shape.Dims(i);
 80a2aa4:	4631      	mov	r1, r6
 80a2aa6:	a805      	add	r0, sp, #20
 80a2aa8:	f7ff fc8e 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
 80a2aac:	3e01      	subs	r6, #1
 80a2aae:	1c73      	adds	r3, r6, #1
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
    desc1_out->strides[i] = desc1_stride;
    desc1_stride *= extended_input1_shape.Dims(i);
 80a2ab0:	fb00 f909 	mul.w	r9, r0, r9
 80a2ab4:	f1a8 0804 	sub.w	r8, r8, #4
 80a2ab8:	f1a7 0704 	sub.w	r7, r7, #4
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
 80a2abc:	d1dd      	bne.n	80a2a7a <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0x3a>
 80a2abe:	2600      	movs	r6, #0
 80a2ac0:	46b0      	mov	r8, r6
 80a2ac2:	3510      	adds	r5, #16
 80a2ac4:	3410      	adds	r4, #16

  // Walk over each dimension. If the extents are equal do nothing.
  // Otherwise, set the desc with extent 1 to have extent equal to the other and
  // stride 0.
  for (int i = 0; i < N; ++i) {
    const int extent0 = extended_input0_shape.Dims(i);
 80a2ac6:	4631      	mov	r1, r6
 80a2ac8:	4668      	mov	r0, sp
 80a2aca:	f7ff fc7d 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
    const int extent1 = extended_input1_shape.Dims(i);
 80a2ace:	4631      	mov	r1, r6

  // Walk over each dimension. If the extents are equal do nothing.
  // Otherwise, set the desc with extent 1 to have extent equal to the other and
  // stride 0.
  for (int i = 0; i < N; ++i) {
    const int extent0 = extended_input0_shape.Dims(i);
 80a2ad0:	4607      	mov	r7, r0
    const int extent1 = extended_input1_shape.Dims(i);
 80a2ad2:	a805      	add	r0, sp, #20
 80a2ad4:	f7ff fc78 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
    if (extent0 != extent1) {
 80a2ad8:	4287      	cmp	r7, r0
 80a2ada:	d00c      	beq.n	80a2af6 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0xb6>
      if (extent0 == 1) {
 80a2adc:	2f01      	cmp	r7, #1
 80a2ade:	d104      	bne.n	80a2aea <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0xaa>
        desc0_out->strides[i] = 0;
 80a2ae0:	f8c4 8000 	str.w	r8, [r4]
        desc0_out->extents[i] = extent1;
 80a2ae4:	f844 0c10 	str.w	r0, [r4, #-16]
 80a2ae8:	e005      	b.n	80a2af6 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0xb6>
      } else {
        TFLITE_DCHECK_EQ(extent1, 1);
 80a2aea:	2801      	cmp	r0, #1
 80a2aec:	d1af      	bne.n	80a2a4e <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0xe>
        desc1_out->strides[i] = 0;
 80a2aee:	f8c5 8000 	str.w	r8, [r5]
        desc1_out->extents[i] = extent0;
 80a2af2:	f845 7c10 	str.w	r7, [r5, #-16]
  }

  // Walk over each dimension. If the extents are equal do nothing.
  // Otherwise, set the desc with extent 1 to have extent equal to the other and
  // stride 0.
  for (int i = 0; i < N; ++i) {
 80a2af6:	3601      	adds	r6, #1
 80a2af8:	2e04      	cmp	r6, #4
 80a2afa:	f105 0504 	add.w	r5, r5, #4
 80a2afe:	f104 0404 	add.w	r4, r4, #4
 80a2b02:	d1e0      	bne.n	80a2ac6 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0x86>
    NdArrayDesc<N>* desc0_out, NdArrayDesc<N>* desc1_out) {
  TFLITE_DCHECK(desc0_out != nullptr);
  TFLITE_DCHECK(desc1_out != nullptr);

  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);
 80a2b04:	a805      	add	r0, sp, #20
 80a2b06:	f7ff fc54 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
    const RuntimeShape& input0_shape, const RuntimeShape& input1_shape,
    NdArrayDesc<N>* desc0_out, NdArrayDesc<N>* desc1_out) {
  TFLITE_DCHECK(desc0_out != nullptr);
  TFLITE_DCHECK(desc1_out != nullptr);

  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
 80a2b0a:	4668      	mov	r0, sp
 80a2b0c:	f7ff fc51 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
        desc1_out->strides[i] = 0;
        desc1_out->extents[i] = extent0;
      }
    }
  }
}
 80a2b10:	b00a      	add	sp, #40	; 0x28
 80a2b12:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}

080a2b16 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf>:
                               const RuntimeShape& input1_shape,
                               const float* input1_data,
                               const RuntimeShape& input2_shape,
                               const float* input2_data,
                               const RuntimeShape& output_shape,
                               float* output_data) {
 80a2b16:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a2b1a:	b09d      	sub	sp, #116	; 0x74
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
 80a2b1c:	ad0c      	add	r5, sp, #48	; 0x30
                               const RuntimeShape& input1_shape,
                               const float* input1_data,
                               const RuntimeShape& input2_shape,
                               const float* input2_data,
                               const RuntimeShape& output_shape,
                               float* output_data) {
 80a2b1e:	4606      	mov	r6, r0
 80a2b20:	4617      	mov	r7, r2
 80a2b22:	4608      	mov	r0, r1
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
 80a2b24:	462a      	mov	r2, r5
                               const RuntimeShape& input1_shape,
                               const float* input1_data,
                               const RuntimeShape& input2_shape,
                               const float* input2_data,
                               const RuntimeShape& output_shape,
                               float* output_data) {
 80a2b26:	4619      	mov	r1, r3
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
 80a2b28:	ab14      	add	r3, sp, #80	; 0x50
 80a2b2a:	f7ff ff89 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
 80a2b2e:	2301      	movs	r3, #1
 80a2b30:	9a27      	ldr	r2, [sp, #156]	; 0x9c
 80a2b32:	2104      	movs	r1, #4
 80a2b34:	a807      	add	r0, sp, #28
 80a2b36:	f7ff fc80 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
 80a2b3a:	2400      	movs	r4, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              ActivationFunctionWithMinMax(
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
 80a2b3c:	9503      	str	r5, [sp, #12]
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
 80a2b3e:	2100      	movs	r1, #0
 80a2b40:	a807      	add	r0, sp, #28
 80a2b42:	f7ff fc41 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a2b46:	4284      	cmp	r4, r0
 80a2b48:	da5b      	bge.n	80a2c02 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0xec>
 80a2b4a:	2500      	movs	r5, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
 80a2b4c:	f10d 091c 	add.w	r9, sp, #28
 80a2b50:	2101      	movs	r1, #1
 80a2b52:	4648      	mov	r0, r9
 80a2b54:	f7ff fc38 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a2b58:	4285      	cmp	r5, r0
 80a2b5a:	da50      	bge.n	80a2bfe <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0xe8>
 80a2b5c:	f04f 0800 	mov.w	r8, #0
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
 80a2b60:	2102      	movs	r1, #2
 80a2b62:	4648      	mov	r0, r9
 80a2b64:	f7ff fc30 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a2b68:	4580      	cmp	r8, r0
 80a2b6a:	da46      	bge.n	80a2bfa <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0xe4>
 80a2b6c:	f04f 0a00 	mov.w	sl, #0
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
 80a2b70:	2103      	movs	r1, #3
 80a2b72:	4648      	mov	r0, r9
 80a2b74:	f7ff fc28 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a2b78:	4582      	cmp	sl, r0
 80a2b7a:	da3b      	bge.n	80a2bf4 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0xde>
          output_data[Offset(extended_output_shape, b, y, x, c)] =
 80a2b7c:	4643      	mov	r3, r8
 80a2b7e:	462a      	mov	r2, r5
 80a2b80:	4621      	mov	r1, r4
 80a2b82:	f8cd a000 	str.w	sl, [sp]
 80a2b86:	4648      	mov	r0, r9
 80a2b88:	f7ff fc83 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              ActivationFunctionWithMinMax(
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
 80a2b8c:	4643      	mov	r3, r8
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          output_data[Offset(extended_output_shape, b, y, x, c)] =
 80a2b8e:	9002      	str	r0, [sp, #8]
              ActivationFunctionWithMinMax(
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
 80a2b90:	462a      	mov	r2, r5
 80a2b92:	4621      	mov	r1, r4
 80a2b94:	f8cd a000 	str.w	sl, [sp]
 80a2b98:	9803      	ldr	r0, [sp, #12]
 80a2b9a:	f7ff fd2b 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                      input2_data[SubscriptToIndex(desc2, b, y, x, c)],
 80a2b9e:	462a      	mov	r2, r5
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              ActivationFunctionWithMinMax(
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
 80a2ba0:	4683      	mov	fp, r0
                      input2_data[SubscriptToIndex(desc2, b, y, x, c)],
 80a2ba2:	4643      	mov	r3, r8
 80a2ba4:	4621      	mov	r1, r4
 80a2ba6:	f8cd a000 	str.w	sl, [sp]
 80a2baa:	a814      	add	r0, sp, #80	; 0x50
 80a2bac:	f7ff fd22 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              ActivationFunctionWithMinMax(
 80a2bb0:	9b26      	ldr	r3, [sp, #152]	; 0x98
 80a2bb2:	f853 1020 	ldr.w	r1, [r3, r0, lsl #2]
 80a2bb6:	f857 002b 	ldr.w	r0, [r7, fp, lsl #2]
 80a2bba:	f010 fc0f 	bl	80b33dc <__addsf3>
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
                      input2_data[SubscriptToIndex(desc2, b, y, x, c)],
                  params.float_activation_min, params.float_activation_max);
 80a2bbe:	6b72      	ldr	r2, [r6, #52]	; 0x34
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              ActivationFunctionWithMinMax(
 80a2bc0:	9005      	str	r0, [sp, #20]
 80a2bc2:	4611      	mov	r1, r2
 80a2bc4:	9204      	str	r2, [sp, #16]
 80a2bc6:	f010 feaf 	bl	80b3928 <__aeabi_fcmplt>
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
                      input2_data[SubscriptToIndex(desc2, b, y, x, c)],
                  params.float_activation_min, params.float_activation_max);
 80a2bca:	f8d6 b038 	ldr.w	fp, [r6, #56]	; 0x38
 80a2bce:	9b05      	ldr	r3, [sp, #20]
 80a2bd0:	b108      	cbz	r0, 80a2bd6 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0xc0>
	return __b;
 80a2bd2:	9a04      	ldr	r2, [sp, #16]
 80a2bd4:	4613      	mov	r3, r2
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
 80a2bd6:	4619      	mov	r1, r3
 80a2bd8:	4658      	mov	r0, fp
 80a2bda:	9304      	str	r3, [sp, #16]
 80a2bdc:	f010 fea4 	bl	80b3928 <__aeabi_fcmplt>
 80a2be0:	9b04      	ldr	r3, [sp, #16]
 80a2be2:	b100      	cbz	r0, 80a2be6 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0xd0>
	return __b;
 80a2be4:	465b      	mov	r3, fp
 80a2be6:	9a28      	ldr	r2, [sp, #160]	; 0xa0
 80a2be8:	9902      	ldr	r1, [sp, #8]
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
 80a2bea:	f10a 0a01 	add.w	sl, sl, #1
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              ActivationFunctionWithMinMax(
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
                      input2_data[SubscriptToIndex(desc2, b, y, x, c)],
                  params.float_activation_min, params.float_activation_max);
 80a2bee:	f842 3021 	str.w	r3, [r2, r1, lsl #2]
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
 80a2bf2:	e7bd      	b.n	80a2b70 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0x5a>
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
 80a2bf4:	f108 0801 	add.w	r8, r8, #1
 80a2bf8:	e7b2      	b.n	80a2b60 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0x4a>
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
 80a2bfa:	3501      	adds	r5, #1
 80a2bfc:	e7a6      	b.n	80a2b4c <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0x36>
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
 80a2bfe:	3401      	adds	r4, #1
 80a2c00:	e79d      	b.n	80a2b3e <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0x28>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
  const RuntimeShape extended_output_shape =
      RuntimeShape::ExtendedShape(4, output_shape);
 80a2c02:	a807      	add	r0, sp, #28
 80a2c04:	f7ff fbd5 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                  params.float_activation_min, params.float_activation_max);
        }
      }
    }
  }
}
 80a2c08:	b01d      	add	sp, #116	; 0x74
 80a2c0a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

080a2c10 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_>:
  return kTfLiteOk;
}

void EvalAdd(TfLiteContext* context, TfLiteNode* node, TfLiteAddParams* params,
             const OpData* data, const TfLiteTensor* input1,
             const TfLiteTensor* input2, TfLiteTensor* output) {
 80a2c10:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
 80a2c14:	7812      	ldrb	r2, [r2, #0]
  return kTfLiteOk;
}

void EvalAdd(TfLiteContext* context, TfLiteNode* node, TfLiteAddParams* params,
             const OpData* data, const TfLiteTensor* input1,
             const TfLiteTensor* input2, TfLiteTensor* output) {
 80a2c16:	b0ab      	sub	sp, #172	; 0xac
// Calculates the useful range of an activation layer given its activation
// tensor.a
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
 80a2c18:	2a01      	cmp	r2, #1
 80a2c1a:	9e34      	ldr	r6, [sp, #208]	; 0xd0
 80a2c1c:	9d35      	ldr	r5, [sp, #212]	; 0xd4
 80a2c1e:	9c36      	ldr	r4, [sp, #216]	; 0xd8
 80a2c20:	d007      	beq.n	80a2c32 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x22>
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
 80a2c22:	2a03      	cmp	r2, #3
 80a2c24:	d007      	beq.n	80a2c36 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x26>
    *activation_min = 0;
    *activation_max = 6;
  } else if (activation == kTfLiteActRelu1) {
 80a2c26:	2a02      	cmp	r2, #2
 80a2c28:	d008      	beq.n	80a2c3c <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x2c>
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
    *activation_max = std::numeric_limits<T>::max();
 80a2c2a:	4a37      	ldr	r2, [pc, #220]	; (80a2d08 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0xf8>)
    *activation_max = 6;
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
 80a2c2c:	f46f 0100 	mvn.w	r1, #8388608	; 0x800000
 80a2c30:	e007      	b.n	80a2c42 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x32>
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
 80a2c32:	4a35      	ldr	r2, [pc, #212]	; (80a2d08 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0xf8>)
 80a2c34:	e000      	b.n	80a2c38 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x28>
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
    *activation_max = 6;
 80a2c36:	4a35      	ldr	r2, [pc, #212]	; (80a2d0c <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0xfc>)
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
 80a2c38:	2100      	movs	r1, #0
 80a2c3a:	e002      	b.n	80a2c42 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x32>
    *activation_max = 6;
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
 80a2c3c:	4934      	ldr	r1, [pc, #208]	; (80a2d10 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x100>)
    *activation_max = 1;
 80a2c3e:	f04f 527e 	mov.w	r2, #1065353216	; 0x3f800000
#define TF_LITE_ADD(opname)                                                   \
  reference_ops::opname(op_params, GetTensorShape(input1),                    \
                        GetTensorData<float>(input1), GetTensorShape(input2), \
                        GetTensorData<float>(input2), GetTensorShape(output), \
                        GetTensorData<float>(output))
  if (data->requires_broadcast) {
 80a2c42:	781b      	ldrb	r3, [r3, #0]
  int output_shift;
};

template <typename P>
inline void SetActivationParams(float min, float max, P* params) {
  params->float_activation_min = min;
 80a2c44:	9123      	str	r1, [sp, #140]	; 0x8c
  params->float_activation_max = max;
 80a2c46:	9224      	str	r2, [sp, #144]	; 0x90
 80a2c48:	af11      	add	r7, sp, #68	; 0x44
    TF_LITE_ADD(BroadcastAdd4DSlow);
 80a2c4a:	4631      	mov	r1, r6
 80a2c4c:	a807      	add	r0, sp, #28
#define TF_LITE_ADD(opname)                                                   \
  reference_ops::opname(op_params, GetTensorShape(input1),                    \
                        GetTensorData<float>(input1), GetTensorShape(input2), \
                        GetTensorData<float>(input2), GetTensorShape(output), \
                        GetTensorData<float>(output))
  if (data->requires_broadcast) {
 80a2c4e:	b1cb      	cbz	r3, 80a2c84 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x74>
    TF_LITE_ADD(BroadcastAdd4DSlow);
 80a2c50:	f7ff fe5f 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a2c54:	b106      	cbz	r6, 80a2c58 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x48>
 80a2c56:	6876      	ldr	r6, [r6, #4]
 80a2c58:	4629      	mov	r1, r5
 80a2c5a:	a80c      	add	r0, sp, #48	; 0x30
 80a2c5c:	f7ff fe59 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a2c60:	b105      	cbz	r5, 80a2c64 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x54>
 80a2c62:	686d      	ldr	r5, [r5, #4]
 80a2c64:	4621      	mov	r1, r4
 80a2c66:	4638      	mov	r0, r7
 80a2c68:	f7ff fe53 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a2c6c:	b104      	cbz	r4, 80a2c70 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x60>
 80a2c6e:	6864      	ldr	r4, [r4, #4]
 80a2c70:	9402      	str	r4, [sp, #8]
 80a2c72:	e88d 00a0 	stmia.w	sp, {r5, r7}
 80a2c76:	ab0c      	add	r3, sp, #48	; 0x30
 80a2c78:	4632      	mov	r2, r6
 80a2c7a:	a907      	add	r1, sp, #28
 80a2c7c:	a816      	add	r0, sp, #88	; 0x58
 80a2c7e:	f7ff ff4a 	bl	80a2b16 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf>
 80a2c82:	e035      	b.n	80a2cf0 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0xe0>
  } else {
    TF_LITE_ADD(Add);
 80a2c84:	f7ff fe45 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a2c88:	b106      	cbz	r6, 80a2c8c <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x7c>
 80a2c8a:	6876      	ldr	r6, [r6, #4]
 80a2c8c:	4629      	mov	r1, r5
 80a2c8e:	a80c      	add	r0, sp, #48	; 0x30
 80a2c90:	f7ff fe3f 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a2c94:	b105      	cbz	r5, 80a2c98 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x88>
 80a2c96:	686d      	ldr	r5, [r5, #4]
 80a2c98:	4621      	mov	r1, r4
 80a2c9a:	4638      	mov	r0, r7
 80a2c9c:	f7ff fe39 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a2ca0:	b104      	cbz	r4, 80a2ca4 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x94>
 80a2ca2:	6864      	ldr	r4, [r4, #4]

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const float* input1_data,
                const RuntimeShape& input2_shape, const float* input2_data,
                const RuntimeShape& output_shape, float* output_data) {
  const int size = MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a2ca4:	463a      	mov	r2, r7
 80a2ca6:	a90c      	add	r1, sp, #48	; 0x30
 80a2ca8:	a807      	add	r0, sp, #28
 80a2caa:	f7ff fc14 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a2cae:	4683      	mov	fp, r0
  for (int i = 0; i < size; i++) {
 80a2cb0:	f04f 0800 	mov.w	r8, #0
 80a2cb4:	45c3      	cmp	fp, r8
 80a2cb6:	dd1b      	ble.n	80a2cf0 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0xe0>
    auto x = input1_data[i] + input2_data[i];
 80a2cb8:	f855 1028 	ldr.w	r1, [r5, r8, lsl #2]
 80a2cbc:	f856 0028 	ldr.w	r0, [r6, r8, lsl #2]
 80a2cc0:	f010 fb8c 	bl	80b33dc <__addsf3>
    output_data[i] = ActivationFunctionWithMinMax(
        x, params.float_activation_min, params.float_activation_max);
 80a2cc4:	9b23      	ldr	r3, [sp, #140]	; 0x8c
                const RuntimeShape& input1_shape, const float* input1_data,
                const RuntimeShape& input2_shape, const float* input2_data,
                const RuntimeShape& output_shape, float* output_data) {
  const int size = MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int i = 0; i < size; i++) {
    auto x = input1_data[i] + input2_data[i];
 80a2cc6:	4681      	mov	r9, r0
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
 80a2cc8:	4619      	mov	r1, r3
    output_data[i] = ActivationFunctionWithMinMax(
        x, params.float_activation_min, params.float_activation_max);
 80a2cca:	f8dd a090 	ldr.w	sl, [sp, #144]	; 0x90
 80a2cce:	9305      	str	r3, [sp, #20]
 80a2cd0:	f010 fe2a 	bl	80b3928 <__aeabi_fcmplt>
 80a2cd4:	b108      	cbz	r0, 80a2cda <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0xca>
	return __b;
 80a2cd6:	9b05      	ldr	r3, [sp, #20]
 80a2cd8:	4699      	mov	r9, r3
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
 80a2cda:	4649      	mov	r1, r9
 80a2cdc:	4650      	mov	r0, sl
 80a2cde:	f010 fe23 	bl	80b3928 <__aeabi_fcmplt>
 80a2ce2:	b100      	cbz	r0, 80a2ce6 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0xd6>
	return __b;
 80a2ce4:	46d1      	mov	r9, sl
 80a2ce6:	f844 9028 	str.w	r9, [r4, r8, lsl #2]
inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const float* input1_data,
                const RuntimeShape& input2_shape, const float* input2_data,
                const RuntimeShape& output_shape, float* output_data) {
  const int size = MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int i = 0; i < size; i++) {
 80a2cea:	f108 0801 	add.w	r8, r8, #1
 80a2cee:	e7e1      	b.n	80a2cb4 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0xa4>
 80a2cf0:	4638      	mov	r0, r7
 80a2cf2:	f7ff fb5e 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a2cf6:	a80c      	add	r0, sp, #48	; 0x30
 80a2cf8:	f7ff fb5b 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a2cfc:	a807      	add	r0, sp, #28
 80a2cfe:	f7ff fb58 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  }
#undef TF_LITE_ADD
}
 80a2d02:	b02b      	add	sp, #172	; 0xac
 80a2d04:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80a2d08:	7f7fffff 	.word	0x7f7fffff
 80a2d0c:	40c00000 	.word	0x40c00000
 80a2d10:	bf800000 	.word	0xbf800000

080a2d14 <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa>:
                               const RuntimeShape& input1_shape,
                               const int8_t* input1_data,
                               const RuntimeShape& input2_shape,
                               const int8_t* input2_data,
                               const RuntimeShape& output_shape,
                               int8_t* output_data) {
 80a2d14:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a2d18:	b09b      	sub	sp, #108	; 0x6c
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
 80a2d1a:	ae0a      	add	r6, sp, #40	; 0x28
                               const RuntimeShape& input1_shape,
                               const int8_t* input1_data,
                               const RuntimeShape& input2_shape,
                               const int8_t* input2_data,
                               const RuntimeShape& output_shape,
                               int8_t* output_data) {
 80a2d1c:	4604      	mov	r4, r0
 80a2d1e:	4693      	mov	fp, r2
 80a2d20:	4608      	mov	r0, r1
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
 80a2d22:	4632      	mov	r2, r6
                               const RuntimeShape& input1_shape,
                               const int8_t* input1_data,
                               const RuntimeShape& input2_shape,
                               const int8_t* input2_data,
                               const RuntimeShape& output_shape,
                               int8_t* output_data) {
 80a2d24:	4619      	mov	r1, r3
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
 80a2d26:	ab12      	add	r3, sp, #72	; 0x48
 80a2d28:	f7ff fe8a 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
 80a2d2c:	2301      	movs	r3, #1
 80a2d2e:	9a25      	ldr	r2, [sp, #148]	; 0x94
 80a2d30:	2104      	movs	r1, #4
 80a2d32:	a805      	add	r0, sp, #20
 80a2d34:	f7ff fb81 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
 80a2d38:	2500      	movs	r5, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          const int32_t input1_val =
              params.input1_offset +
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a2d3a:	9602      	str	r6, [sp, #8]
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
 80a2d3c:	2100      	movs	r1, #0
 80a2d3e:	a805      	add	r0, sp, #20
 80a2d40:	f7ff fb42 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a2d44:	4285      	cmp	r5, r0
 80a2d46:	da65      	bge.n	80a2e14 <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x100>
 80a2d48:	2600      	movs	r6, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
 80a2d4a:	f10d 0814 	add.w	r8, sp, #20
 80a2d4e:	2101      	movs	r1, #1
 80a2d50:	4640      	mov	r0, r8
 80a2d52:	f7ff fb39 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a2d56:	4286      	cmp	r6, r0
 80a2d58:	da5a      	bge.n	80a2e10 <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0xfc>
 80a2d5a:	2700      	movs	r7, #0
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
 80a2d5c:	2102      	movs	r1, #2
 80a2d5e:	4640      	mov	r0, r8
 80a2d60:	f7ff fb32 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a2d64:	4287      	cmp	r7, r0
 80a2d66:	da51      	bge.n	80a2e0c <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0xf8>
 80a2d68:	f04f 0900 	mov.w	r9, #0
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
 80a2d6c:	2103      	movs	r1, #3
 80a2d6e:	4640      	mov	r0, r8
 80a2d70:	f7ff fb2a 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a2d74:	4581      	cmp	r9, r0
 80a2d76:	da47      	bge.n	80a2e08 <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0xf4>
          const int32_t input1_val =
              params.input1_offset +
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a2d78:	f8cd 9000 	str.w	r9, [sp]
 80a2d7c:	463b      	mov	r3, r7
 80a2d7e:	4632      	mov	r2, r6
 80a2d80:	4629      	mov	r1, r5
 80a2d82:	9802      	ldr	r0, [sp, #8]
 80a2d84:	f7ff fc36 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
 80a2d88:	6863      	ldr	r3, [r4, #4]
 80a2d8a:	f91b a000 	ldrsb.w	sl, [fp, r0]
          const int32_t input2_val =
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a2d8e:	f8cd 9000 	str.w	r9, [sp]
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          const int32_t input1_val =
              params.input1_offset +
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a2d92:	449a      	add	sl, r3
          const int32_t input2_val =
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a2d94:	4632      	mov	r2, r6
 80a2d96:	463b      	mov	r3, r7
 80a2d98:	4629      	mov	r1, r5
 80a2d9a:	a812      	add	r0, sp, #72	; 0x48
 80a2d9c:	f7ff fc2a 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32_t shifted_input1_val =
              input1_val * (1 << params.left_shift);
          const int32_t shifted_input2_val =
              input2_val * (1 << params.left_shift);
 80a2da0:	9b24      	ldr	r3, [sp, #144]	; 0x90
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32_t input2_val =
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32_t shifted_input1_val =
              input1_val * (1 << params.left_shift);
 80a2da2:	f8d4 e018 	ldr.w	lr, [r4, #24]
          const int32_t shifted_input2_val =
              input2_val * (1 << params.left_shift);
 80a2da6:	561a      	ldrsb	r2, [r3, r0]
 80a2da8:	68a3      	ldr	r3, [r4, #8]
          const int32_t scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
 80a2daa:	fa0a f00e 	lsl.w	r0, sl, lr
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32_t shifted_input1_val =
              input1_val * (1 << params.left_shift);
          const int32_t shifted_input2_val =
              input2_val * (1 << params.left_shift);
 80a2dae:	4413      	add	r3, r2
 80a2db0:	fa03 f30e 	lsl.w	r3, r3, lr
          const int32_t scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
 80a2db4:	6a22      	ldr	r2, [r4, #32]
 80a2db6:	69e1      	ldr	r1, [r4, #28]
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32_t shifted_input1_val =
              input1_val * (1 << params.left_shift);
          const int32_t shifted_input2_val =
              input2_val * (1 << params.left_shift);
 80a2db8:	9303      	str	r3, [sp, #12]
          const int32_t scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
 80a2dba:	f7ff fbc9 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32_t scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, params.input2_multiplier,
                  params.input2_shift);
 80a2dbe:	9b03      	ldr	r3, [sp, #12]
          const int32_t shifted_input2_val =
              input2_val * (1 << params.left_shift);
          const int32_t scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
 80a2dc0:	4682      	mov	sl, r0
          const int32_t scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, params.input2_multiplier,
                  params.input2_shift);
 80a2dc2:	6aa2      	ldr	r2, [r4, #40]	; 0x28
 80a2dc4:	6a61      	ldr	r1, [r4, #36]	; 0x24
 80a2dc6:	4618      	mov	r0, r3
 80a2dc8:	f7ff fbc2 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32_t raw_sum = scaled_input1_val + scaled_input2_val;
          const int32_t raw_output =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
 80a2dcc:	6962      	ldr	r2, [r4, #20]
 80a2dce:	6921      	ldr	r1, [r4, #16]
 80a2dd0:	4450      	add	r0, sl
 80a2dd2:	f7ff fbbd 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
 80a2dd6:	68e3      	ldr	r3, [r4, #12]
                  raw_sum, params.output_multiplier, params.output_shift) +
              params.output_offset;
          const int32_t clamped_output =
              std::min(params.quantized_activation_max,
                       std::max(params.quantized_activation_min, raw_output));
          output_data[Offset(extended_output_shape, b, y, x, c)] =
 80a2dd8:	f8cd 9000 	str.w	r9, [sp]
 80a2ddc:	4418      	add	r0, r3
 80a2dde:	6ae3      	ldr	r3, [r4, #44]	; 0x2c
 80a2de0:	4632      	mov	r2, r6
 80a2de2:	4283      	cmp	r3, r0
 80a2de4:	bfb8      	it	lt
 80a2de6:	4603      	movlt	r3, r0
 80a2de8:	6b20      	ldr	r0, [r4, #48]	; 0x30
 80a2dea:	4629      	mov	r1, r5
 80a2dec:	4283      	cmp	r3, r0
 80a2dee:	bfa8      	it	ge
 80a2df0:	4603      	movge	r3, r0
 80a2df2:	4640      	mov	r0, r8
 80a2df4:	469a      	mov	sl, r3
 80a2df6:	463b      	mov	r3, r7
 80a2df8:	f7ff fb4b 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<int8_t>(clamped_output);
 80a2dfc:	9b26      	ldr	r3, [sp, #152]	; 0x98
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
 80a2dfe:	f109 0901 	add.w	r9, r9, #1
              params.output_offset;
          const int32_t clamped_output =
              std::min(params.quantized_activation_max,
                       std::max(params.quantized_activation_min, raw_output));
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              static_cast<int8_t>(clamped_output);
 80a2e02:	f803 a000 	strb.w	sl, [r3, r0]
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
 80a2e06:	e7b1      	b.n	80a2d6c <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x58>
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
 80a2e08:	3701      	adds	r7, #1
 80a2e0a:	e7a7      	b.n	80a2d5c <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x48>
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
 80a2e0c:	3601      	adds	r6, #1
 80a2e0e:	e79c      	b.n	80a2d4a <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x36>
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
 80a2e10:	3501      	adds	r5, #1
 80a2e12:	e793      	b.n	80a2d3c <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x28>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
  const RuntimeShape extended_output_shape =
      RuntimeShape::ExtendedShape(4, output_shape);
 80a2e14:	a805      	add	r0, sp, #20
 80a2e16:	f7ff facc 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
              static_cast<int8_t>(clamped_output);
        }
      }
    }
  }
}
 80a2e1a:	b01b      	add	sp, #108	; 0x6c
 80a2e1c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a2e20 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph>:
                               const RuntimeShape& input1_shape,
                               const uint8* input1_data,
                               const RuntimeShape& input2_shape,
                               const uint8* input2_data,
                               const RuntimeShape& output_shape,
                               uint8* output_data) {
 80a2e20:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a2e24:	b09b      	sub	sp, #108	; 0x6c
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
 80a2e26:	ae0a      	add	r6, sp, #40	; 0x28
                               const RuntimeShape& input1_shape,
                               const uint8* input1_data,
                               const RuntimeShape& input2_shape,
                               const uint8* input2_data,
                               const RuntimeShape& output_shape,
                               uint8* output_data) {
 80a2e28:	4604      	mov	r4, r0
 80a2e2a:	4693      	mov	fp, r2
 80a2e2c:	4608      	mov	r0, r1
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
 80a2e2e:	4632      	mov	r2, r6
                               const RuntimeShape& input1_shape,
                               const uint8* input1_data,
                               const RuntimeShape& input2_shape,
                               const uint8* input2_data,
                               const RuntimeShape& output_shape,
                               uint8* output_data) {
 80a2e30:	4619      	mov	r1, r3
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
 80a2e32:	ab12      	add	r3, sp, #72	; 0x48
 80a2e34:	f7ff fe04 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
 80a2e38:	2301      	movs	r3, #1
 80a2e3a:	9a25      	ldr	r2, [sp, #148]	; 0x94
 80a2e3c:	2104      	movs	r1, #4
 80a2e3e:	a805      	add	r0, sp, #20
 80a2e40:	f7ff fafb 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
 80a2e44:	2500      	movs	r5, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          const int32 input1_val =
              params.input1_offset +
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a2e46:	9602      	str	r6, [sp, #8]
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
 80a2e48:	2100      	movs	r1, #0
 80a2e4a:	a805      	add	r0, sp, #20
 80a2e4c:	f7ff fabc 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a2e50:	4285      	cmp	r5, r0
 80a2e52:	da65      	bge.n	80a2f20 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x100>
 80a2e54:	2600      	movs	r6, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
 80a2e56:	f10d 0814 	add.w	r8, sp, #20
 80a2e5a:	2101      	movs	r1, #1
 80a2e5c:	4640      	mov	r0, r8
 80a2e5e:	f7ff fab3 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a2e62:	4286      	cmp	r6, r0
 80a2e64:	da5a      	bge.n	80a2f1c <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0xfc>
 80a2e66:	2700      	movs	r7, #0
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
 80a2e68:	2102      	movs	r1, #2
 80a2e6a:	4640      	mov	r0, r8
 80a2e6c:	f7ff faac 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a2e70:	4287      	cmp	r7, r0
 80a2e72:	da51      	bge.n	80a2f18 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0xf8>
 80a2e74:	f04f 0900 	mov.w	r9, #0
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
 80a2e78:	2103      	movs	r1, #3
 80a2e7a:	4640      	mov	r0, r8
 80a2e7c:	f7ff faa4 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a2e80:	4581      	cmp	r9, r0
 80a2e82:	da47      	bge.n	80a2f14 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0xf4>
          const int32 input1_val =
              params.input1_offset +
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a2e84:	f8cd 9000 	str.w	r9, [sp]
 80a2e88:	463b      	mov	r3, r7
 80a2e8a:	4632      	mov	r2, r6
 80a2e8c:	4629      	mov	r1, r5
 80a2e8e:	9802      	ldr	r0, [sp, #8]
 80a2e90:	f7ff fbb0 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
 80a2e94:	6863      	ldr	r3, [r4, #4]
 80a2e96:	f81b a000 	ldrb.w	sl, [fp, r0]
          const int32 input2_val =
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a2e9a:	f8cd 9000 	str.w	r9, [sp]
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          const int32 input1_val =
              params.input1_offset +
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a2e9e:	449a      	add	sl, r3
          const int32 input2_val =
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a2ea0:	4632      	mov	r2, r6
 80a2ea2:	463b      	mov	r3, r7
 80a2ea4:	4629      	mov	r1, r5
 80a2ea6:	a812      	add	r0, sp, #72	; 0x48
 80a2ea8:	f7ff fba4 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val =
              input1_val * (1 << params.left_shift);
          const int32 shifted_input2_val =
              input2_val * (1 << params.left_shift);
 80a2eac:	9b24      	ldr	r3, [sp, #144]	; 0x90
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val =
              input1_val * (1 << params.left_shift);
 80a2eae:	f8d4 e018 	ldr.w	lr, [r4, #24]
          const int32 shifted_input2_val =
              input2_val * (1 << params.left_shift);
 80a2eb2:	5c1a      	ldrb	r2, [r3, r0]
 80a2eb4:	68a3      	ldr	r3, [r4, #8]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
 80a2eb6:	fa0a f00e 	lsl.w	r0, sl, lr
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val =
              input1_val * (1 << params.left_shift);
          const int32 shifted_input2_val =
              input2_val * (1 << params.left_shift);
 80a2eba:	4413      	add	r3, r2
 80a2ebc:	fa03 f30e 	lsl.w	r3, r3, lr
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
 80a2ec0:	6a22      	ldr	r2, [r4, #32]
 80a2ec2:	69e1      	ldr	r1, [r4, #28]
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val =
              input1_val * (1 << params.left_shift);
          const int32 shifted_input2_val =
              input2_val * (1 << params.left_shift);
 80a2ec4:	9303      	str	r3, [sp, #12]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
 80a2ec6:	f7ff fb43 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, params.input2_multiplier,
                  params.input2_shift);
 80a2eca:	9b03      	ldr	r3, [sp, #12]
          const int32 shifted_input2_val =
              input2_val * (1 << params.left_shift);
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
 80a2ecc:	4682      	mov	sl, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, params.input2_multiplier,
                  params.input2_shift);
 80a2ece:	6aa2      	ldr	r2, [r4, #40]	; 0x28
 80a2ed0:	6a61      	ldr	r1, [r4, #36]	; 0x24
 80a2ed2:	4618      	mov	r0, r3
 80a2ed4:	f7ff fb3c 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 raw_sum = scaled_input1_val + scaled_input2_val;
          const int32 raw_output =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
 80a2ed8:	6962      	ldr	r2, [r4, #20]
 80a2eda:	6921      	ldr	r1, [r4, #16]
 80a2edc:	4450      	add	r0, sl
 80a2ede:	f7ff fb37 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
 80a2ee2:	68e3      	ldr	r3, [r4, #12]
                  raw_sum, params.output_multiplier, params.output_shift) +
              params.output_offset;
          const int32 clamped_output =
              std::min(params.quantized_activation_max,
                       std::max(params.quantized_activation_min, raw_output));
          output_data[Offset(extended_output_shape, b, y, x, c)] =
 80a2ee4:	f8cd 9000 	str.w	r9, [sp]
 80a2ee8:	4418      	add	r0, r3
 80a2eea:	6ae3      	ldr	r3, [r4, #44]	; 0x2c
 80a2eec:	4632      	mov	r2, r6
 80a2eee:	4283      	cmp	r3, r0
 80a2ef0:	bfb8      	it	lt
 80a2ef2:	4603      	movlt	r3, r0
 80a2ef4:	6b20      	ldr	r0, [r4, #48]	; 0x30
 80a2ef6:	4629      	mov	r1, r5
 80a2ef8:	4283      	cmp	r3, r0
 80a2efa:	bfa8      	it	ge
 80a2efc:	4603      	movge	r3, r0
 80a2efe:	4640      	mov	r0, r8
 80a2f00:	469a      	mov	sl, r3
 80a2f02:	463b      	mov	r3, r7
 80a2f04:	f7ff fac5 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<uint8>(clamped_output);
 80a2f08:	9b26      	ldr	r3, [sp, #152]	; 0x98
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
 80a2f0a:	f109 0901 	add.w	r9, r9, #1
              params.output_offset;
          const int32 clamped_output =
              std::min(params.quantized_activation_max,
                       std::max(params.quantized_activation_min, raw_output));
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              static_cast<uint8>(clamped_output);
 80a2f0e:	f803 a000 	strb.w	sl, [r3, r0]
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
 80a2f12:	e7b1      	b.n	80a2e78 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x58>
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
 80a2f14:	3701      	adds	r7, #1
 80a2f16:	e7a7      	b.n	80a2e68 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x48>
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
 80a2f18:	3601      	adds	r6, #1
 80a2f1a:	e79c      	b.n	80a2e56 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x36>
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
 80a2f1c:	3501      	adds	r5, #1
 80a2f1e:	e793      	b.n	80a2e48 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x28>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
  const RuntimeShape extended_output_shape =
      RuntimeShape::ExtendedShape(4, output_shape);
 80a2f20:	a805      	add	r0, sp, #20
 80a2f22:	f7ff fa46 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
              static_cast<uint8>(clamped_output);
        }
      }
    }
  }
}
 80a2f26:	b01b      	add	sp, #108	; 0x6c
 80a2f28:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a2f2c <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4>:

TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
 80a2f2c:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
 80a2f30:	4614      	mov	r4, r2
 80a2f32:	461e      	mov	r6, r3
                              const TfLiteTensor* input1,
                              const TfLiteTensor* input2,
                              TfLiteTensor* output) {
  if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt8) {
    tflite::ArithmeticParams op_params;
    op_params.left_shift = data->left_shift;
 80a2f34:	6a43      	ldr	r3, [r0, #36]	; 0x24
    TF_LITE_ADD(Add);
  }
#undef TF_LITE_ADD
}

TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
 80a2f36:	b0a8      	sub	sp, #160	; 0xa0
                              const TfLiteTensor* input1,
                              const TfLiteTensor* input2,
                              TfLiteTensor* output) {
  if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt8) {
    tflite::ArithmeticParams op_params;
    op_params.left_shift = data->left_shift;
 80a2f38:	931a      	str	r3, [sp, #104]	; 0x68
    op_params.input1_offset = data->input1_offset;
 80a2f3a:	6a83      	ldr	r3, [r0, #40]	; 0x28
  params->float_activation_max = max;
}

template <typename P>
inline void SetActivationParams(int32 min, int32 max, P* params) {
  params->quantized_activation_min = min;
 80a2f3c:	68c2      	ldr	r2, [r0, #12]
 80a2f3e:	9315      	str	r3, [sp, #84]	; 0x54
    op_params.input1_multiplier = data->input1_multiplier;
 80a2f40:	6943      	ldr	r3, [r0, #20]
    TF_LITE_ADD(Add);
  }
#undef TF_LITE_ADD
}

TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
 80a2f42:	460d      	mov	r5, r1
                              TfLiteTensor* output) {
  if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt8) {
    tflite::ArithmeticParams op_params;
    op_params.left_shift = data->left_shift;
    op_params.input1_offset = data->input1_offset;
    op_params.input1_multiplier = data->input1_multiplier;
 80a2f44:	931b      	str	r3, [sp, #108]	; 0x6c
    op_params.input1_shift = data->input1_shift;
 80a2f46:	6843      	ldr	r3, [r0, #4]
 80a2f48:	921f      	str	r2, [sp, #124]	; 0x7c
 80a2f4a:	931c      	str	r3, [sp, #112]	; 0x70
    op_params.input2_offset = data->input2_offset;
 80a2f4c:	6ac3      	ldr	r3, [r0, #44]	; 0x2c
 80a2f4e:	af05      	add	r7, sp, #20
 80a2f50:	9316      	str	r3, [sp, #88]	; 0x58
    op_params.input2_multiplier = data->input2_multiplier;
 80a2f52:	6983      	ldr	r3, [r0, #24]
 80a2f54:	931d      	str	r3, [sp, #116]	; 0x74
    op_params.input2_shift = data->input2_shift;
 80a2f56:	6883      	ldr	r3, [r0, #8]
 80a2f58:	931e      	str	r3, [sp, #120]	; 0x78
    op_params.output_offset = data->output_offset;
 80a2f5a:	6b03      	ldr	r3, [r0, #48]	; 0x30
 80a2f5c:	9317      	str	r3, [sp, #92]	; 0x5c
    op_params.output_multiplier = data->output_multiplier;
 80a2f5e:	69c3      	ldr	r3, [r0, #28]
 80a2f60:	9318      	str	r3, [sp, #96]	; 0x60
    op_params.output_shift = data->output_shift;
 80a2f62:	6a03      	ldr	r3, [r0, #32]
 80a2f64:	9319      	str	r3, [sp, #100]	; 0x64
    SetActivationParams(data->output_activation_min,
                        data->output_activation_max, &op_params);
 80a2f66:	6903      	ldr	r3, [r0, #16]
    bool need_broadcast = reference_ops::ProcessBroadcastShapes(
        GetTensorShape(input1), GetTensorShape(input2), &op_params);
 80a2f68:	a80f      	add	r0, sp, #60	; 0x3c
  params->quantized_activation_max = max;
 80a2f6a:	9320      	str	r3, [sp, #128]	; 0x80
 80a2f6c:	f7ff fcd1 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a2f70:	4621      	mov	r1, r4
 80a2f72:	a80a      	add	r0, sp, #40	; 0x28
 80a2f74:	f7ff fccd 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a2f78:	a90a      	add	r1, sp, #40	; 0x28
 80a2f7a:	aa14      	add	r2, sp, #80	; 0x50
 80a2f7c:	a80f      	add	r0, sp, #60	; 0x3c
 80a2f7e:	f7ff fbf2 	bl	80a2766 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE>
 80a2f82:	4680      	mov	r8, r0
 80a2f84:	a80a      	add	r0, sp, #40	; 0x28
 80a2f86:	f7ff fa14 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a2f8a:	a80f      	add	r0, sp, #60	; 0x3c
 80a2f8c:	f7ff fa11 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
#define TF_LITE_ADD(type, opname, dtype)                             \
  type::opname(op_params, GetTensorShape(input1),                    \
               GetTensorData<dtype>(input1), GetTensorShape(input2), \
               GetTensorData<dtype>(input2), GetTensorShape(output), \
               GetTensorData<dtype>(output));
    if (output->type == kTfLiteInt8) {
 80a2f90:	7833      	ldrb	r3, [r6, #0]
      if (need_broadcast) {
        TF_LITE_ADD(reference_integer_ops, BroadcastAdd4DSlow, int8_t);
 80a2f92:	4629      	mov	r1, r5
#define TF_LITE_ADD(type, opname, dtype)                             \
  type::opname(op_params, GetTensorShape(input1),                    \
               GetTensorData<dtype>(input1), GetTensorShape(input2), \
               GetTensorData<dtype>(input2), GetTensorShape(output), \
               GetTensorData<dtype>(output));
    if (output->type == kTfLiteInt8) {
 80a2f94:	2b09      	cmp	r3, #9
      if (need_broadcast) {
        TF_LITE_ADD(reference_integer_ops, BroadcastAdd4DSlow, int8_t);
 80a2f96:	a80f      	add	r0, sp, #60	; 0x3c
#define TF_LITE_ADD(type, opname, dtype)                             \
  type::opname(op_params, GetTensorShape(input1),                    \
               GetTensorData<dtype>(input1), GetTensorShape(input2), \
               GetTensorData<dtype>(input2), GetTensorShape(output), \
               GetTensorData<dtype>(output));
    if (output->type == kTfLiteInt8) {
 80a2f98:	d134      	bne.n	80a3004 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0xd8>
      if (need_broadcast) {
 80a2f9a:	f1b8 0f00 	cmp.w	r8, #0
 80a2f9e:	d018      	beq.n	80a2fd2 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0xa6>
        TF_LITE_ADD(reference_integer_ops, BroadcastAdd4DSlow, int8_t);
 80a2fa0:	f7ff fcb7 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a2fa4:	b105      	cbz	r5, 80a2fa8 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x7c>
 80a2fa6:	686d      	ldr	r5, [r5, #4]
 80a2fa8:	4621      	mov	r1, r4
 80a2faa:	a80a      	add	r0, sp, #40	; 0x28
 80a2fac:	f7ff fcb1 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a2fb0:	b104      	cbz	r4, 80a2fb4 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x88>
 80a2fb2:	6864      	ldr	r4, [r4, #4]
 80a2fb4:	4631      	mov	r1, r6
 80a2fb6:	4638      	mov	r0, r7
 80a2fb8:	f7ff fcab 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a2fbc:	6873      	ldr	r3, [r6, #4]
 80a2fbe:	e88d 0090 	stmia.w	sp, {r4, r7}
 80a2fc2:	9302      	str	r3, [sp, #8]
 80a2fc4:	462a      	mov	r2, r5
 80a2fc6:	ab0a      	add	r3, sp, #40	; 0x28
 80a2fc8:	a90f      	add	r1, sp, #60	; 0x3c
 80a2fca:	a814      	add	r0, sp, #80	; 0x50
 80a2fcc:	f7ff fea2 	bl	80a2d14 <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa>
 80a2fd0:	e04c      	b.n	80a306c <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x140>
      } else {
        TF_LITE_ADD(reference_integer_ops, Add, int8_t);
 80a2fd2:	f7ff fc9e 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a2fd6:	b105      	cbz	r5, 80a2fda <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0xae>
 80a2fd8:	686d      	ldr	r5, [r5, #4]
 80a2fda:	4621      	mov	r1, r4
 80a2fdc:	a80a      	add	r0, sp, #40	; 0x28
 80a2fde:	f7ff fc98 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a2fe2:	b104      	cbz	r4, 80a2fe6 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0xba>
 80a2fe4:	6864      	ldr	r4, [r4, #4]
 80a2fe6:	4631      	mov	r1, r6
 80a2fe8:	4638      	mov	r0, r7
 80a2fea:	f7ff fc92 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a2fee:	6873      	ldr	r3, [r6, #4]
 80a2ff0:	e88d 0090 	stmia.w	sp, {r4, r7}
 80a2ff4:	9302      	str	r3, [sp, #8]
 80a2ff6:	462a      	mov	r2, r5
 80a2ff8:	ab0a      	add	r3, sp, #40	; 0x28
 80a2ffa:	a90f      	add	r1, sp, #60	; 0x3c
 80a2ffc:	a814      	add	r0, sp, #80	; 0x50
 80a2ffe:	f7ff fb68 	bl	80a26d2 <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa>
 80a3002:	e033      	b.n	80a306c <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x140>
      }
    } else {
      if (need_broadcast) {
 80a3004:	f1b8 0f00 	cmp.w	r8, #0
 80a3008:	d018      	beq.n	80a303c <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x110>
        TF_LITE_ADD(reference_ops, BroadcastAdd4DSlow, uint8_t);
 80a300a:	f7ff fc82 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a300e:	b105      	cbz	r5, 80a3012 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0xe6>
 80a3010:	686d      	ldr	r5, [r5, #4]
 80a3012:	4621      	mov	r1, r4
 80a3014:	a80a      	add	r0, sp, #40	; 0x28
 80a3016:	f7ff fc7c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a301a:	b104      	cbz	r4, 80a301e <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0xf2>
 80a301c:	6864      	ldr	r4, [r4, #4]
 80a301e:	4631      	mov	r1, r6
 80a3020:	4638      	mov	r0, r7
 80a3022:	f7ff fc76 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a3026:	6873      	ldr	r3, [r6, #4]
 80a3028:	e88d 0090 	stmia.w	sp, {r4, r7}
 80a302c:	9302      	str	r3, [sp, #8]
 80a302e:	462a      	mov	r2, r5
 80a3030:	ab0a      	add	r3, sp, #40	; 0x28
 80a3032:	a90f      	add	r1, sp, #60	; 0x3c
 80a3034:	a814      	add	r0, sp, #80	; 0x50
 80a3036:	f7ff fef3 	bl	80a2e20 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph>
 80a303a:	e017      	b.n	80a306c <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x140>
      } else {
        TF_LITE_ADD(reference_ops, Add, uint8_t);
 80a303c:	f7ff fc69 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a3040:	b105      	cbz	r5, 80a3044 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x118>
 80a3042:	686d      	ldr	r5, [r5, #4]
 80a3044:	4621      	mov	r1, r4
 80a3046:	a80a      	add	r0, sp, #40	; 0x28
 80a3048:	f7ff fc63 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a304c:	b104      	cbz	r4, 80a3050 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x124>
 80a304e:	6864      	ldr	r4, [r4, #4]
 80a3050:	4631      	mov	r1, r6
 80a3052:	4638      	mov	r0, r7
 80a3054:	f7ff fc5d 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a3058:	6873      	ldr	r3, [r6, #4]
 80a305a:	e88d 0090 	stmia.w	sp, {r4, r7}
 80a305e:	9302      	str	r3, [sp, #8]
 80a3060:	462a      	mov	r2, r5
 80a3062:	ab0a      	add	r3, sp, #40	; 0x28
 80a3064:	a90f      	add	r1, sp, #60	; 0x3c
 80a3066:	a814      	add	r0, sp, #80	; 0x50
 80a3068:	f7ff fae8 	bl	80a263c <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph>
 80a306c:	4638      	mov	r0, r7
 80a306e:	f7ff f9a0 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a3072:	a80a      	add	r0, sp, #40	; 0x28
 80a3074:	f7ff f99d 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a3078:	a80f      	add	r0, sp, #60	; 0x3c
 80a307a:	f7ff f99a 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
    }
#undef TF_LITE_ADD
  }

  return kTfLiteOk;
}
 80a307e:	b028      	add	sp, #160	; 0xa0
 80a3080:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}

080a3084 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_>:

TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
                              TfLiteAddParams* params, const OpData* data,
                              const TfLiteTensor* input1,
                              const TfLiteTensor* input2,
                              TfLiteTensor* output) {
 80a3084:	b508      	push	{r3, lr}
 80a3086:	4618      	mov	r0, r3
 80a3088:	9b04      	ldr	r3, [sp, #16]
  if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt8) {
 80a308a:	781a      	ldrb	r2, [r3, #0]
 80a308c:	2a03      	cmp	r2, #3
 80a308e:	d001      	beq.n	80a3094 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x10>
 80a3090:	2a09      	cmp	r2, #9
 80a3092:	d103      	bne.n	80a309c <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x18>
 80a3094:	9a03      	ldr	r2, [sp, #12]
 80a3096:	9902      	ldr	r1, [sp, #8]
 80a3098:	f7ff ff48 	bl	80a2f2c <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4>
    }
#undef TF_LITE_ADD
  }

  return kTfLiteOk;
}
 80a309c:	2000      	movs	r0, #0
 80a309e:	bd08      	pop	{r3, pc}

080a30a0 <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80a30a0:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a30a4:	460f      	mov	r7, r1
 80a30a6:	680a      	ldr	r2, [r1, #0]
 80a30a8:	f8d0 8008 	ldr.w	r8, [r0, #8]
  auto* params = reinterpret_cast<TfLiteAddParams*>(node->builtin_data);
 80a30ac:	f8d1 9014 	ldr.w	r9, [r1, #20]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a30b0:	6851      	ldr	r1, [r2, #4]
 80a30b2:	6892      	ldr	r2, [r2, #8]
 80a30b4:	2338      	movs	r3, #56	; 0x38
 80a30b6:	fb03 8202 	mla	r2, r3, r2, r8
  }

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80a30ba:	b095      	sub	sp, #84	; 0x54
 80a30bc:	9204      	str	r2, [sp, #16]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a30be:	687a      	ldr	r2, [r7, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a30c0:	fb03 8b01 	mla	fp, r3, r1, r8
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a30c4:	6854      	ldr	r4, [r2, #4]
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  OpData data;
  TF_LITE_ENSURE_STATUS(
 80a30c6:	f10d 0a1c 	add.w	sl, sp, #28
 80a30ca:	4363      	muls	r3, r4
 80a30cc:	eb08 0403 	add.w	r4, r8, r3
 80a30d0:	9305      	str	r3, [sp, #20]
 80a30d2:	e88d 0410 	stmia.w	sp, {r4, sl}
 80a30d6:	9b04      	ldr	r3, [sp, #16]
 80a30d8:	465a      	mov	r2, fp
 80a30da:	4649      	mov	r1, r9
  }

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80a30dc:	4606      	mov	r6, r0
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  OpData data;
  TF_LITE_ENSURE_STATUS(
 80a30de:	f7ff fc30 	bl	80a2942 <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE>
 80a30e2:	4605      	mov	r5, r0
 80a30e4:	bb38      	cbnz	r0, 80a3136 <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0x96>
      CalculateOpData(context, params, input1, input2, output, &data));

  if (output->type == kTfLiteFloat32) {
 80a30e6:	9b05      	ldr	r3, [sp, #20]
 80a30e8:	f818 3003 	ldrb.w	r3, [r8, r3]
 80a30ec:	2b01      	cmp	r3, #1
 80a30ee:	d10b      	bne.n	80a3108 <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0x68>
    EvalAdd(context, node, params, &data, input1, input2, output);
 80a30f0:	9b04      	ldr	r3, [sp, #16]
 80a30f2:	9402      	str	r4, [sp, #8]
 80a30f4:	9301      	str	r3, [sp, #4]
 80a30f6:	f8cd b000 	str.w	fp, [sp]
 80a30fa:	4653      	mov	r3, sl
 80a30fc:	464a      	mov	r2, r9
 80a30fe:	4639      	mov	r1, r7
 80a3100:	4630      	mov	r0, r6
 80a3102:	f7ff fd85 	bl	80a2c10 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_>
 80a3106:	e017      	b.n	80a3138 <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0x98>
  } else if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt8) {
 80a3108:	2b03      	cmp	r3, #3
 80a310a:	d001      	beq.n	80a3110 <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0x70>
 80a310c:	2b09      	cmp	r3, #9
 80a310e:	d10e      	bne.n	80a312e <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0x8e>
    TF_LITE_ENSURE_OK(context, EvalAddQuantized(context, node, params, &data,
 80a3110:	9b04      	ldr	r3, [sp, #16]
 80a3112:	9402      	str	r4, [sp, #8]
 80a3114:	9301      	str	r3, [sp, #4]
 80a3116:	f8cd b000 	str.w	fp, [sp]
 80a311a:	4653      	mov	r3, sl
 80a311c:	464a      	mov	r2, r9
 80a311e:	4639      	mov	r1, r7
 80a3120:	4630      	mov	r0, r6
 80a3122:	f7ff ffaf 	bl	80a3084 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_>
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  OpData data;
  TF_LITE_ENSURE_STATUS(
 80a3126:	1c05      	adds	r5, r0, #0
 80a3128:	bf18      	it	ne
 80a312a:	2501      	movne	r5, #1
 80a312c:	e004      	b.n	80a3138 <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0x98>
  } else if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt8) {
    TF_LITE_ENSURE_OK(context, EvalAddQuantized(context, node, params, &data,
                                                input1, input2, output));
  } else {
    context->ReportError(context,
                         "Inputs and outputs not all float|uint8|int8 types.");
 80a312e:	6973      	ldr	r3, [r6, #20]
 80a3130:	4903      	ldr	r1, [pc, #12]	; (80a3140 <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0xa0>)
 80a3132:	4630      	mov	r0, r6
 80a3134:	4798      	blx	r3
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  OpData data;
  TF_LITE_ENSURE_STATUS(
 80a3136:	2501      	movs	r5, #1
                         "Inputs and outputs not all float|uint8|int8 types.");
    return kTfLiteError;
  }

  return kTfLiteOk;
}
 80a3138:	4628      	mov	r0, r5
 80a313a:	b015      	add	sp, #84	; 0x54
 80a313c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80a3140:	080b5a28 	.word	0x080b5a28

080a3144 <_ZN6tflite3ops5micro14AllOpsResolverC1Ev>:
#define TFLITE_REGISTRATIONS_MAX (128)
#endif

namespace tflite {

class MicroMutableOpResolver : public OpResolver {
 80a3144:	f241 0304 	movw	r3, #4100	; 0x1004
TfLiteRegistration* Register_UNPACK();
TfLiteRegistration* Register_NEG();
TfLiteRegistration* Register_ADD();
TfLiteRegistration* Register_QUANTIZE();
TfLiteRegistration* Register_DEQUANTIZE();
AllOpsResolver::AllOpsResolver() {
 80a3148:	b5f7      	push	{r0, r1, r2, r4, r5, r6, r7, lr}
 80a314a:	2700      	movs	r7, #0
 80a314c:	50c7      	str	r7, [r0, r3]
 80a314e:	4bbd      	ldr	r3, [pc, #756]	; (80a3444 <_ZN6tflite3ops5micro14AllOpsResolverC1Ev+0x300>)
 80a3150:	4605      	mov	r5, r0
 80a3152:	6003      	str	r3, [r0, #0]
  AddBuiltin(BuiltinOperator_DEPTHWISE_CONV_2D, Register_DEPTHWISE_CONV_2D());
 80a3154:	f00b fb6c 	bl	80ae830 <_ZN6tflite3ops5micro26Register_DEPTHWISE_CONV_2DEv>
 80a3158:	2401      	movs	r4, #1
 80a315a:	4623      	mov	r3, r4
 80a315c:	4602      	mov	r2, r0
 80a315e:	2104      	movs	r1, #4
 80a3160:	4628      	mov	r0, r5
 80a3162:	9400      	str	r4, [sp, #0]
 80a3164:	f7ff f827 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_FULLY_CONNECTED, Register_FULLY_CONNECTED(),
 80a3168:	f006 f908 	bl	80a937c <_ZN6tflite3ops5micro24Register_FULLY_CONNECTEDEv>
             /* min_version */ 1,
             /* max_version */ 4);
 80a316c:	2604      	movs	r6, #4
 80a316e:	4623      	mov	r3, r4
 80a3170:	4602      	mov	r2, r0
 80a3172:	2109      	movs	r1, #9
 80a3174:	4628      	mov	r0, r5
 80a3176:	9600      	str	r6, [sp, #0]
 80a3178:	f7ff f81d 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_MAX_POOL_2D, Register_MAX_POOL_2D());
 80a317c:	f007 ffcc 	bl	80ab118 <_ZN6tflite3ops5micro20Register_MAX_POOL_2DEv>
 80a3180:	4623      	mov	r3, r4
 80a3182:	4602      	mov	r2, r0
 80a3184:	2111      	movs	r1, #17
 80a3186:	4628      	mov	r0, r5
 80a3188:	9400      	str	r4, [sp, #0]
 80a318a:	f7ff f814 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_SOFTMAX, Register_SOFTMAX());
 80a318e:	f008 fe4b 	bl	80abe28 <_ZN6tflite3ops5micro16Register_SOFTMAXEv>
 80a3192:	4623      	mov	r3, r4
 80a3194:	4602      	mov	r2, r0
 80a3196:	2119      	movs	r1, #25
 80a3198:	4628      	mov	r0, r5
 80a319a:	9400      	str	r4, [sp, #0]
 80a319c:	f7ff f80b 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LOGISTIC, Register_LOGISTIC());
 80a31a0:	f006 fb20 	bl	80a97e4 <_ZN6tflite3ops5micro17Register_LOGISTICEv>
 80a31a4:	4623      	mov	r3, r4
 80a31a6:	4602      	mov	r2, r0
 80a31a8:	210e      	movs	r1, #14
 80a31aa:	4628      	mov	r0, r5
 80a31ac:	9400      	str	r4, [sp, #0]
 80a31ae:	f7ff f802 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_SVDF, Register_SVDF());
 80a31b2:	f00a ff0d 	bl	80adfd0 <_ZN6tflite3ops5micro13Register_SVDFEv>
 80a31b6:	4623      	mov	r3, r4
 80a31b8:	4602      	mov	r2, r0
 80a31ba:	211b      	movs	r1, #27
 80a31bc:	4628      	mov	r0, r5
 80a31be:	9400      	str	r4, [sp, #0]
 80a31c0:	f7fe fff9 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_CONV_2D, Register_CONV_2D());
 80a31c4:	f005 fac0 	bl	80a8748 <_ZN6tflite3ops5micro16Register_CONV_2DEv>
 80a31c8:	4623      	mov	r3, r4
 80a31ca:	4602      	mov	r2, r0
 80a31cc:	2103      	movs	r1, #3
 80a31ce:	4628      	mov	r0, r5
 80a31d0:	9400      	str	r4, [sp, #0]
 80a31d2:	f7fe fff0 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_AVERAGE_POOL_2D, Register_AVERAGE_POOL_2D());
 80a31d6:	f007 ff9b 	bl	80ab110 <_ZN6tflite3ops5micro24Register_AVERAGE_POOL_2DEv>
 80a31da:	4623      	mov	r3, r4
 80a31dc:	4602      	mov	r2, r0
 80a31de:	4621      	mov	r1, r4
 80a31e0:	4628      	mov	r0, r5
 80a31e2:	9400      	str	r4, [sp, #0]
 80a31e4:	f7fe ffe7 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_ABS, Register_ABS());
 80a31e8:	f005 fd08 	bl	80a8bfc <_ZN6tflite3ops5micro12Register_ABSEv>
 80a31ec:	4623      	mov	r3, r4
 80a31ee:	4602      	mov	r2, r0
 80a31f0:	2165      	movs	r1, #101	; 0x65
 80a31f2:	4628      	mov	r0, r5
 80a31f4:	9400      	str	r4, [sp, #0]
 80a31f6:	f7fe ffde 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_SIN, Register_SIN());
 80a31fa:	f005 fd03 	bl	80a8c04 <_ZN6tflite3ops5micro12Register_SINEv>
 80a31fe:	4623      	mov	r3, r4
 80a3200:	4602      	mov	r2, r0
 80a3202:	2142      	movs	r1, #66	; 0x42
 80a3204:	4628      	mov	r0, r5
 80a3206:	9400      	str	r4, [sp, #0]
 80a3208:	f7fe ffd5 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_COS, Register_COS());
 80a320c:	f005 fcfe 	bl	80a8c0c <_ZN6tflite3ops5micro12Register_COSEv>
 80a3210:	4623      	mov	r3, r4
 80a3212:	4602      	mov	r2, r0
 80a3214:	216c      	movs	r1, #108	; 0x6c
 80a3216:	4628      	mov	r0, r5
 80a3218:	9400      	str	r4, [sp, #0]
 80a321a:	f7fe ffcc 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LOG, Register_LOG());
 80a321e:	f005 fcf9 	bl	80a8c14 <_ZN6tflite3ops5micro12Register_LOGEv>
 80a3222:	4623      	mov	r3, r4
 80a3224:	4602      	mov	r2, r0
 80a3226:	2149      	movs	r1, #73	; 0x49
 80a3228:	4628      	mov	r0, r5
 80a322a:	9400      	str	r4, [sp, #0]
 80a322c:	f7fe ffc3 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_SQRT, Register_SQRT());
 80a3230:	f005 fcf4 	bl	80a8c1c <_ZN6tflite3ops5micro13Register_SQRTEv>
 80a3234:	4623      	mov	r3, r4
 80a3236:	4602      	mov	r2, r0
 80a3238:	214b      	movs	r1, #75	; 0x4b
 80a323a:	4628      	mov	r0, r5
 80a323c:	9400      	str	r4, [sp, #0]
 80a323e:	f7fe ffba 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_RSQRT, Register_RSQRT());
 80a3242:	f005 fcef 	bl	80a8c24 <_ZN6tflite3ops5micro14Register_RSQRTEv>
 80a3246:	4623      	mov	r3, r4
 80a3248:	4602      	mov	r2, r0
 80a324a:	214c      	movs	r1, #76	; 0x4c
 80a324c:	4628      	mov	r0, r5
 80a324e:	9400      	str	r4, [sp, #0]
 80a3250:	f7fe ffb1 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_SQUARE, Register_SQUARE());
 80a3254:	f005 fcea 	bl	80a8c2c <_ZN6tflite3ops5micro15Register_SQUAREEv>
 80a3258:	4623      	mov	r3, r4
 80a325a:	4602      	mov	r2, r0
 80a325c:	215c      	movs	r1, #92	; 0x5c
 80a325e:	4628      	mov	r0, r5
 80a3260:	9400      	str	r4, [sp, #0]
 80a3262:	f7fe ffa8 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_PRELU, Register_PRELU());
 80a3266:	f007 ff5d 	bl	80ab124 <_ZN6tflite3ops5micro14Register_PRELUEv>
 80a326a:	4623      	mov	r3, r4
 80a326c:	4602      	mov	r2, r0
 80a326e:	2136      	movs	r1, #54	; 0x36
 80a3270:	4628      	mov	r0, r5
 80a3272:	9400      	str	r4, [sp, #0]
 80a3274:	f7fe ff9f 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_FLOOR, Register_FLOOR());
 80a3278:	f005 fd4c 	bl	80a8d14 <_ZN6tflite3ops5micro14Register_FLOOREv>
 80a327c:	4623      	mov	r3, r4
 80a327e:	4602      	mov	r2, r0
 80a3280:	2108      	movs	r1, #8
 80a3282:	4628      	mov	r0, r5
 80a3284:	9400      	str	r4, [sp, #0]
 80a3286:	f7fe ff96 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_MAXIMUM, Register_MAXIMUM());
 80a328a:	f006 faed 	bl	80a9868 <_ZN6tflite3ops5micro16Register_MAXIMUMEv>
 80a328e:	4623      	mov	r3, r4
 80a3290:	4602      	mov	r2, r0
 80a3292:	2137      	movs	r1, #55	; 0x37
 80a3294:	4628      	mov	r0, r5
 80a3296:	9400      	str	r4, [sp, #0]
 80a3298:	f7fe ff8d 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_MINIMUM, Register_MINIMUM());
 80a329c:	f006 fae8 	bl	80a9870 <_ZN6tflite3ops5micro16Register_MINIMUMEv>
 80a32a0:	4623      	mov	r3, r4
 80a32a2:	4602      	mov	r2, r0
 80a32a4:	2139      	movs	r1, #57	; 0x39
 80a32a6:	4628      	mov	r0, r5
 80a32a8:	9400      	str	r4, [sp, #0]
 80a32aa:	f7fe ff84 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_ARG_MAX, Register_ARG_MAX());
 80a32ae:	f000 f8cd 	bl	80a344c <_ZN6tflite3ops5micro16Register_ARG_MAXEv>
 80a32b2:	4623      	mov	r3, r4
 80a32b4:	4602      	mov	r2, r0
 80a32b6:	2138      	movs	r1, #56	; 0x38
 80a32b8:	4628      	mov	r0, r5
 80a32ba:	9400      	str	r4, [sp, #0]
 80a32bc:	f7fe ff7b 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_ARG_MIN, Register_ARG_MIN());
 80a32c0:	f000 f8c8 	bl	80a3454 <_ZN6tflite3ops5micro16Register_ARG_MINEv>
 80a32c4:	4623      	mov	r3, r4
 80a32c6:	4602      	mov	r2, r0
 80a32c8:	214f      	movs	r1, #79	; 0x4f
 80a32ca:	4628      	mov	r0, r5
 80a32cc:	9400      	str	r4, [sp, #0]
 80a32ce:	f7fe ff72 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LOGICAL_OR, Register_LOGICAL_OR());
 80a32d2:	f006 f861 	bl	80a9398 <_ZN6tflite3ops5micro19Register_LOGICAL_OREv>
 80a32d6:	4623      	mov	r3, r4
 80a32d8:	4602      	mov	r2, r0
 80a32da:	2154      	movs	r1, #84	; 0x54
 80a32dc:	4628      	mov	r0, r5
 80a32de:	9400      	str	r4, [sp, #0]
 80a32e0:	f7fe ff69 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LOGICAL_AND, Register_LOGICAL_AND());
 80a32e4:	f006 f85c 	bl	80a93a0 <_ZN6tflite3ops5micro20Register_LOGICAL_ANDEv>
 80a32e8:	4623      	mov	r3, r4
 80a32ea:	4602      	mov	r2, r0
 80a32ec:	2156      	movs	r1, #86	; 0x56
 80a32ee:	4628      	mov	r0, r5
 80a32f0:	9400      	str	r4, [sp, #0]
 80a32f2:	f7fe ff60 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LOGICAL_NOT, Register_LOGICAL_NOT());
 80a32f6:	f005 fc9d 	bl	80a8c34 <_ZN6tflite3ops5micro20Register_LOGICAL_NOTEv>
 80a32fa:	4623      	mov	r3, r4
 80a32fc:	4602      	mov	r2, r0
 80a32fe:	2157      	movs	r1, #87	; 0x57
 80a3300:	4628      	mov	r0, r5
 80a3302:	9400      	str	r4, [sp, #0]
 80a3304:	f7fe ff57 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_RESHAPE, Register_RESHAPE());
 80a3308:	f008 fae6 	bl	80ab8d8 <_ZN6tflite3ops5micro16Register_RESHAPEEv>
 80a330c:	4623      	mov	r3, r4
 80a330e:	4602      	mov	r2, r0
 80a3310:	2116      	movs	r1, #22
 80a3312:	4628      	mov	r0, r5
 80a3314:	9400      	str	r4, [sp, #0]
 80a3316:	f7fe ff4e 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_EQUAL, Register_EQUAL());
 80a331a:	f000 fd61 	bl	80a3de0 <_ZN6tflite3ops5micro14Register_EQUALEv>
 80a331e:	4623      	mov	r3, r4
 80a3320:	4602      	mov	r2, r0
 80a3322:	2147      	movs	r1, #71	; 0x47
 80a3324:	4628      	mov	r0, r5
 80a3326:	9400      	str	r4, [sp, #0]
 80a3328:	f7fe ff45 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_NOT_EQUAL, Register_NOT_EQUAL());
 80a332c:	f000 fd5c 	bl	80a3de8 <_ZN6tflite3ops5micro18Register_NOT_EQUALEv>
 80a3330:	4623      	mov	r3, r4
 80a3332:	4602      	mov	r2, r0
 80a3334:	2148      	movs	r1, #72	; 0x48
 80a3336:	4628      	mov	r0, r5
 80a3338:	9400      	str	r4, [sp, #0]
 80a333a:	f7fe ff3c 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_GREATER, Register_GREATER());
 80a333e:	f000 fd57 	bl	80a3df0 <_ZN6tflite3ops5micro16Register_GREATEREv>
 80a3342:	4623      	mov	r3, r4
 80a3344:	4602      	mov	r2, r0
 80a3346:	213d      	movs	r1, #61	; 0x3d
 80a3348:	4628      	mov	r0, r5
 80a334a:	9400      	str	r4, [sp, #0]
 80a334c:	f7fe ff33 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_GREATER_EQUAL, Register_GREATER_EQUAL());
 80a3350:	f000 fd52 	bl	80a3df8 <_ZN6tflite3ops5micro22Register_GREATER_EQUALEv>
 80a3354:	4623      	mov	r3, r4
 80a3356:	4602      	mov	r2, r0
 80a3358:	213e      	movs	r1, #62	; 0x3e
 80a335a:	4628      	mov	r0, r5
 80a335c:	9400      	str	r4, [sp, #0]
 80a335e:	f7fe ff2a 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LESS, Register_LESS());
 80a3362:	f000 fd4d 	bl	80a3e00 <_ZN6tflite3ops5micro13Register_LESSEv>
 80a3366:	4623      	mov	r3, r4
 80a3368:	4602      	mov	r2, r0
 80a336a:	213a      	movs	r1, #58	; 0x3a
 80a336c:	4628      	mov	r0, r5
 80a336e:	9400      	str	r4, [sp, #0]
 80a3370:	f7fe ff21 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LESS_EQUAL, Register_LESS_EQUAL());
 80a3374:	f000 fd48 	bl	80a3e08 <_ZN6tflite3ops5micro19Register_LESS_EQUALEv>
 80a3378:	4623      	mov	r3, r4
 80a337a:	4602      	mov	r2, r0
 80a337c:	213f      	movs	r1, #63	; 0x3f
 80a337e:	4628      	mov	r0, r5
 80a3380:	9400      	str	r4, [sp, #0]
 80a3382:	f7fe ff18 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_CEIL, Register_CEIL());
 80a3386:	f000 fd27 	bl	80a3dd8 <_ZN6tflite3ops5micro13Register_CEILEv>
 80a338a:	4623      	mov	r3, r4
 80a338c:	4602      	mov	r2, r0
 80a338e:	2168      	movs	r1, #104	; 0x68
 80a3390:	4628      	mov	r0, r5
 80a3392:	9400      	str	r4, [sp, #0]
 80a3394:	f7fe ff0f 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_ROUND, Register_ROUND());
 80a3398:	f008 fbc8 	bl	80abb2c <_ZN6tflite3ops5micro14Register_ROUNDEv>
 80a339c:	4623      	mov	r3, r4
 80a339e:	4602      	mov	r2, r0
 80a33a0:	2174      	movs	r1, #116	; 0x74
 80a33a2:	4628      	mov	r0, r5
 80a33a4:	9400      	str	r4, [sp, #0]
 80a33a6:	f7fe ff06 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_STRIDED_SLICE, Register_STRIDED_SLICE());
 80a33aa:	f009 fd85 	bl	80aceb8 <_ZN6tflite3ops5micro22Register_STRIDED_SLICEEv>
 80a33ae:	4623      	mov	r3, r4
 80a33b0:	4602      	mov	r2, r0
 80a33b2:	212d      	movs	r1, #45	; 0x2d
 80a33b4:	4628      	mov	r0, r5
 80a33b6:	9400      	str	r4, [sp, #0]
 80a33b8:	f7fe fefd 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_PACK, Register_PACK());
 80a33bc:	f007 f8de 	bl	80aa57c <_ZN6tflite3ops5micro13Register_PACKEv>
 80a33c0:	4623      	mov	r3, r4
 80a33c2:	4602      	mov	r2, r0
 80a33c4:	2153      	movs	r1, #83	; 0x53
 80a33c6:	4628      	mov	r0, r5
 80a33c8:	9400      	str	r4, [sp, #0]
 80a33ca:	f7fe fef4 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_SPLIT, Register_SPLIT(),
 80a33ce:	f009 f867 	bl	80ac4a0 <_ZN6tflite3ops5micro14Register_SPLITEv>
             /* min_version */ 1,
             /* max_version */ 3);
 80a33d2:	2303      	movs	r3, #3
 80a33d4:	4602      	mov	r2, r0
 80a33d6:	9300      	str	r3, [sp, #0]
 80a33d8:	2131      	movs	r1, #49	; 0x31
 80a33da:	4623      	mov	r3, r4
 80a33dc:	4628      	mov	r0, r5
 80a33de:	f7fe feea 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_UNPACK, Register_UNPACK());
 80a33e2:	f00a ffdb 	bl	80ae39c <_ZN6tflite3ops5micro15Register_UNPACKEv>
 80a33e6:	4623      	mov	r3, r4
 80a33e8:	4602      	mov	r2, r0
 80a33ea:	2158      	movs	r1, #88	; 0x58
 80a33ec:	4628      	mov	r0, r5
 80a33ee:	9400      	str	r4, [sp, #0]
 80a33f0:	f7fe fee1 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_NEG, Register_NEG());
 80a33f4:	f006 fe84 	bl	80aa100 <_ZN6tflite3ops5micro12Register_NEGEv>
 80a33f8:	4623      	mov	r3, r4
 80a33fa:	4602      	mov	r2, r0
 80a33fc:	213b      	movs	r1, #59	; 0x3b
 80a33fe:	4628      	mov	r0, r5
 80a3400:	9400      	str	r4, [sp, #0]
 80a3402:	f7fe fed8 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_ADD, Register_ADD());
 80a3406:	f7ff fb17 	bl	80a2a38 <_ZN6tflite3ops5micro12Register_ADDEv>
 80a340a:	4623      	mov	r3, r4
 80a340c:	4602      	mov	r2, r0
 80a340e:	4639      	mov	r1, r7
 80a3410:	4628      	mov	r0, r5
 80a3412:	9400      	str	r4, [sp, #0]
 80a3414:	f7fe fecf 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_QUANTIZE, Register_QUANTIZE(), 1, 4);
 80a3418:	f008 f97a 	bl	80ab710 <_ZN6tflite3ops5micro17Register_QUANTIZEEv>
 80a341c:	4623      	mov	r3, r4
 80a341e:	4602      	mov	r2, r0
 80a3420:	2172      	movs	r1, #114	; 0x72
 80a3422:	4628      	mov	r0, r5
 80a3424:	9600      	str	r6, [sp, #0]
 80a3426:	f7fe fec6 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_DEQUANTIZE, Register_DEQUANTIZE(), 1, 4);
 80a342a:	f005 fa93 	bl	80a8954 <_ZN6tflite3ops5micro19Register_DEQUANTIZEEv>
 80a342e:	9600      	str	r6, [sp, #0]
 80a3430:	4602      	mov	r2, r0
 80a3432:	4623      	mov	r3, r4
 80a3434:	4628      	mov	r0, r5
 80a3436:	2106      	movs	r1, #6
 80a3438:	f7fe febd 	bl	80a21b6 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
}
 80a343c:	4628      	mov	r0, r5
 80a343e:	b003      	add	sp, #12
 80a3440:	bdf0      	pop	{r4, r5, r6, r7, pc}
 80a3442:	bf00      	nop
 80a3444:	080b5ac0 	.word	0x080b5ac0

080a3448 <_ZN6tflite3ops5micro11arg_min_max7PrepareEP13TfLiteContextP10TfLiteNode>:
constexpr int kAxis = 1;
constexpr int kOutputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
 80a3448:	2000      	movs	r0, #0
 80a344a:	4770      	bx	lr

080a344c <_ZN6tflite3ops5micro16Register_ARG_MAXEv>:

TfLiteRegistration* Register_ARG_MAX() {
  static TfLiteRegistration r = {nullptr, nullptr, arg_min_max::Prepare,
                                 arg_min_max::ArgMaxEval};
  return &r;
}
 80a344c:	4800      	ldr	r0, [pc, #0]	; (80a3450 <_ZN6tflite3ops5micro16Register_ARG_MAXEv+0x4>)
 80a344e:	4770      	bx	lr
 80a3450:	20000028 	.word	0x20000028

080a3454 <_ZN6tflite3ops5micro16Register_ARG_MINEv>:

TfLiteRegistration* Register_ARG_MIN() {
  static TfLiteRegistration r = {nullptr, nullptr, arg_min_max::Prepare,
                                 arg_min_max::ArgMinEval};
  return &r;
}
 80a3454:	4800      	ldr	r0, [pc, #0]	; (80a3458 <_ZN6tflite3ops5micro16Register_ARG_MINEv+0x4>)
 80a3456:	4770      	bx	lr
 80a3458:	20000048 	.word	0x20000048

080a345c <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>:
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
 80a345c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a3460:	6805      	ldr	r5, [r0, #0]
 80a3462:	b08b      	sub	sp, #44	; 0x2c
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
 80a3464:	2d00      	cmp	r5, #0
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
 80a3466:	4606      	mov	r6, r0
 80a3468:	9104      	str	r1, [sp, #16]
 80a346a:	461f      	mov	r7, r3
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
 80a346c:	dc01      	bgt.n	80a3472 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x16>
 80a346e:	f00c fddf 	bl	80b0030 <abort>
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
 80a3472:	6839      	ldr	r1, [r7, #0]
 80a3474:	1e6b      	subs	r3, r5, #1
 80a3476:	428b      	cmp	r3, r1
 80a3478:	d1f9      	bne.n	80a346e <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
 80a347a:	6814      	ldr	r4, [r2, #0]
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80a347c:	f04f 0800 	mov.w	r8, #0
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
  if (axis < 0) {
 80a3480:	2c00      	cmp	r4, #0
    axis += input1_shape.DimensionsCount();
 80a3482:	bfb8      	it	lt
 80a3484:	1964      	addlt	r4, r4, r5
  }
  const int axis_size = input1_shape.Dims(axis);
 80a3486:	4621      	mov	r1, r4
 80a3488:	f7fe ff9e 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a348c:	4681      	mov	r9, r0

  int outer_size = 1;
 80a348e:	f04f 0b01 	mov.w	fp, #1
  for (int i = 0; i < axis; ++i) {
 80a3492:	4544      	cmp	r4, r8
 80a3494:	dd0f      	ble.n	80a34b6 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x5a>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
 80a3496:	4641      	mov	r1, r8
 80a3498:	4630      	mov	r0, r6
 80a349a:	f7fe ff95 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a349e:	4641      	mov	r1, r8
 80a34a0:	4682      	mov	sl, r0
 80a34a2:	4638      	mov	r0, r7
 80a34a4:	f7fe ff90 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a34a8:	4582      	cmp	sl, r0
 80a34aa:	d1e0      	bne.n	80a346e <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    outer_size *= input1_shape.Dims(i);
 80a34ac:	fb0a fb0b 	mul.w	fp, sl, fp
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80a34b0:	f108 0801 	add.w	r8, r8, #1
 80a34b4:	e7ed      	b.n	80a3492 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x36>
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
 80a34b6:	f104 0801 	add.w	r8, r4, #1
  for (int i = 0; i < axis; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
 80a34ba:	2401      	movs	r4, #1
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
 80a34bc:	45a8      	cmp	r8, r5
 80a34be:	da10      	bge.n	80a34e2 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x86>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
 80a34c0:	4641      	mov	r1, r8
 80a34c2:	4630      	mov	r0, r6
 80a34c4:	f7fe ff80 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a34c8:	f108 31ff 	add.w	r1, r8, #4294967295	; 0xffffffff
 80a34cc:	4682      	mov	sl, r0
 80a34ce:	4638      	mov	r0, r7
 80a34d0:	f7fe ff7a 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a34d4:	4582      	cmp	sl, r0
 80a34d6:	d1ca      	bne.n	80a346e <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    inner_size *= input1_shape.Dims(i);
 80a34d8:	fb0a f404 	mul.w	r4, sl, r4
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
 80a34dc:	f108 0801 	add.w	r8, r8, #1
 80a34e0:	e7ec      	b.n	80a34bc <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x60>
 80a34e2:	2600      	movs	r6, #0
 80a34e4:	46b2      	mov	sl, r6
 80a34e6:	00a3      	lsls	r3, r4, #2
 80a34e8:	9301      	str	r3, [sp, #4]
 80a34ea:	9b14      	ldr	r3, [sp, #80]	; 0x50
 80a34ec:	9a04      	ldr	r2, [sp, #16]
 80a34ee:	9300      	str	r3, [sp, #0]
 80a34f0:	ea4f 0389 	mov.w	r3, r9, lsl #2
 80a34f4:	4363      	muls	r3, r4
 80a34f6:	9305      	str	r3, [sp, #20]
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
 80a34f8:	45d3      	cmp	fp, sl
 80a34fa:	dd37      	ble.n	80a356c <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x110>
 80a34fc:	fb06 4304 	mla	r3, r6, r4, r4
 80a3500:	2500      	movs	r5, #0
 80a3502:	9306      	str	r3, [sp, #24]
    for (int inner = 0; inner < inner_size; ++inner) {
 80a3504:	42ac      	cmp	r4, r5
 80a3506:	dd27      	ble.n	80a3558 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xfc>
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
 80a3508:	f852 3025 	ldr.w	r3, [r2, r5, lsl #2]
 80a350c:	9904      	ldr	r1, [sp, #16]
 80a350e:	9302      	str	r3, [sp, #8]
 80a3510:	9b06      	ldr	r3, [sp, #24]
 80a3512:	2700      	movs	r7, #0
 80a3514:	18eb      	adds	r3, r5, r3
 80a3516:	eb01 0383 	add.w	r3, r1, r3, lsl #2
 80a351a:	9307      	str	r3, [sp, #28]
      T2 min_max_index = 0;
 80a351c:	463b      	mov	r3, r7
      for (int i = 1; i < axis_size; ++i) {
 80a351e:	f04f 0801 	mov.w	r8, #1
 80a3522:	45c8      	cmp	r8, r9
 80a3524:	da13      	bge.n	80a354e <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xf2>
 80a3526:	9308      	str	r3, [sp, #32]
 80a3528:	9b07      	ldr	r3, [sp, #28]
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
 80a352a:	9802      	ldr	r0, [sp, #8]
 80a352c:	59db      	ldr	r3, [r3, r7]
 80a352e:	9209      	str	r2, [sp, #36]	; 0x24
 80a3530:	4619      	mov	r1, r3
 80a3532:	9303      	str	r3, [sp, #12]
 80a3534:	f010 f9f8 	bl	80b3928 <__aeabi_fcmplt>
 80a3538:	9b08      	ldr	r3, [sp, #32]
 80a353a:	9a09      	ldr	r2, [sp, #36]	; 0x24
 80a353c:	b110      	cbz	r0, 80a3544 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xe8>
          min_max_value = curr_value;
 80a353e:	9b03      	ldr	r3, [sp, #12]
 80a3540:	9302      	str	r3, [sp, #8]
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
 80a3542:	4643      	mov	r3, r8
 80a3544:	9901      	ldr	r1, [sp, #4]
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
 80a3546:	f108 0801 	add.w	r8, r8, #1
 80a354a:	440f      	add	r7, r1
 80a354c:	e7e9      	b.n	80a3522 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xc6>
        if (cmp(curr_value, min_max_value)) {
          min_max_value = curr_value;
          min_max_index = static_cast<T2>(i);
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
 80a354e:	9900      	ldr	r1, [sp, #0]
 80a3550:	f841 3025 	str.w	r3, [r1, r5, lsl #2]
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
 80a3554:	3501      	adds	r5, #1
 80a3556:	e7d5      	b.n	80a3504 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xa8>
 80a3558:	e89d 000a 	ldmia.w	sp, {r1, r3}
 80a355c:	440b      	add	r3, r1
 80a355e:	9300      	str	r3, [sp, #0]
 80a3560:	9b05      	ldr	r3, [sp, #20]
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
 80a3562:	f10a 0a01 	add.w	sl, sl, #1
 80a3566:	441a      	add	r2, r3
 80a3568:	444e      	add	r6, r9
 80a356a:	e7c5      	b.n	80a34f8 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x9c>
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
    }
  }
}
 80a356c:	b00b      	add	sp, #44	; 0x2c
 80a356e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a3572 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>:
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
 80a3572:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a3576:	6805      	ldr	r5, [r0, #0]
 80a3578:	b08b      	sub	sp, #44	; 0x2c
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
 80a357a:	2d00      	cmp	r5, #0
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
 80a357c:	4606      	mov	r6, r0
 80a357e:	9104      	str	r1, [sp, #16]
 80a3580:	461f      	mov	r7, r3
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
 80a3582:	dc01      	bgt.n	80a3588 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x16>
 80a3584:	f00c fd54 	bl	80b0030 <abort>
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
 80a3588:	6839      	ldr	r1, [r7, #0]
 80a358a:	1e6b      	subs	r3, r5, #1
 80a358c:	428b      	cmp	r3, r1
 80a358e:	d1f9      	bne.n	80a3584 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
 80a3590:	6814      	ldr	r4, [r2, #0]
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80a3592:	f04f 0800 	mov.w	r8, #0
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
  if (axis < 0) {
 80a3596:	2c00      	cmp	r4, #0
    axis += input1_shape.DimensionsCount();
 80a3598:	bfb8      	it	lt
 80a359a:	1964      	addlt	r4, r4, r5
  }
  const int axis_size = input1_shape.Dims(axis);
 80a359c:	4621      	mov	r1, r4
 80a359e:	f7fe ff13 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a35a2:	4681      	mov	r9, r0

  int outer_size = 1;
 80a35a4:	f04f 0b01 	mov.w	fp, #1
  for (int i = 0; i < axis; ++i) {
 80a35a8:	4544      	cmp	r4, r8
 80a35aa:	dd0f      	ble.n	80a35cc <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x5a>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
 80a35ac:	4641      	mov	r1, r8
 80a35ae:	4630      	mov	r0, r6
 80a35b0:	f7fe ff0a 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a35b4:	4641      	mov	r1, r8
 80a35b6:	4682      	mov	sl, r0
 80a35b8:	4638      	mov	r0, r7
 80a35ba:	f7fe ff05 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a35be:	4582      	cmp	sl, r0
 80a35c0:	d1e0      	bne.n	80a3584 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    outer_size *= input1_shape.Dims(i);
 80a35c2:	fb0a fb0b 	mul.w	fp, sl, fp
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80a35c6:	f108 0801 	add.w	r8, r8, #1
 80a35ca:	e7ed      	b.n	80a35a8 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x36>
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
 80a35cc:	f104 0801 	add.w	r8, r4, #1
  for (int i = 0; i < axis; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
 80a35d0:	2401      	movs	r4, #1
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
 80a35d2:	45a8      	cmp	r8, r5
 80a35d4:	da10      	bge.n	80a35f8 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x86>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
 80a35d6:	4641      	mov	r1, r8
 80a35d8:	4630      	mov	r0, r6
 80a35da:	f7fe fef5 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a35de:	f108 31ff 	add.w	r1, r8, #4294967295	; 0xffffffff
 80a35e2:	4682      	mov	sl, r0
 80a35e4:	4638      	mov	r0, r7
 80a35e6:	f7fe feef 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a35ea:	4582      	cmp	sl, r0
 80a35ec:	d1ca      	bne.n	80a3584 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    inner_size *= input1_shape.Dims(i);
 80a35ee:	fb0a f404 	mul.w	r4, sl, r4
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
 80a35f2:	f108 0801 	add.w	r8, r8, #1
 80a35f6:	e7ec      	b.n	80a35d2 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x60>
 80a35f8:	2600      	movs	r6, #0
 80a35fa:	46b2      	mov	sl, r6
 80a35fc:	00a3      	lsls	r3, r4, #2
 80a35fe:	9301      	str	r3, [sp, #4]
 80a3600:	9b14      	ldr	r3, [sp, #80]	; 0x50
 80a3602:	9a04      	ldr	r2, [sp, #16]
 80a3604:	9300      	str	r3, [sp, #0]
 80a3606:	ea4f 0389 	mov.w	r3, r9, lsl #2
 80a360a:	4363      	muls	r3, r4
 80a360c:	9305      	str	r3, [sp, #20]
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
 80a360e:	45d3      	cmp	fp, sl
 80a3610:	dd37      	ble.n	80a3682 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x110>
 80a3612:	fb06 4304 	mla	r3, r6, r4, r4
 80a3616:	2500      	movs	r5, #0
 80a3618:	9306      	str	r3, [sp, #24]
    for (int inner = 0; inner < inner_size; ++inner) {
 80a361a:	42ac      	cmp	r4, r5
 80a361c:	dd27      	ble.n	80a366e <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xfc>
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
 80a361e:	f852 3025 	ldr.w	r3, [r2, r5, lsl #2]
 80a3622:	9904      	ldr	r1, [sp, #16]
 80a3624:	9302      	str	r3, [sp, #8]
 80a3626:	9b06      	ldr	r3, [sp, #24]
 80a3628:	2700      	movs	r7, #0
 80a362a:	18eb      	adds	r3, r5, r3
 80a362c:	eb01 0383 	add.w	r3, r1, r3, lsl #2
 80a3630:	9307      	str	r3, [sp, #28]
      T2 min_max_index = 0;
 80a3632:	463b      	mov	r3, r7
      for (int i = 1; i < axis_size; ++i) {
 80a3634:	f04f 0801 	mov.w	r8, #1
 80a3638:	45c8      	cmp	r8, r9
 80a363a:	da13      	bge.n	80a3664 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xf2>
 80a363c:	9308      	str	r3, [sp, #32]
 80a363e:	9b07      	ldr	r3, [sp, #28]
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
 80a3640:	9802      	ldr	r0, [sp, #8]
 80a3642:	59db      	ldr	r3, [r3, r7]
 80a3644:	9209      	str	r2, [sp, #36]	; 0x24
 80a3646:	4619      	mov	r1, r3
 80a3648:	9303      	str	r3, [sp, #12]
 80a364a:	f010 f98b 	bl	80b3964 <__aeabi_fcmpgt>
 80a364e:	9b08      	ldr	r3, [sp, #32]
 80a3650:	9a09      	ldr	r2, [sp, #36]	; 0x24
 80a3652:	b110      	cbz	r0, 80a365a <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xe8>
          min_max_value = curr_value;
 80a3654:	9b03      	ldr	r3, [sp, #12]
 80a3656:	9302      	str	r3, [sp, #8]
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
 80a3658:	4643      	mov	r3, r8
 80a365a:	9901      	ldr	r1, [sp, #4]
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
 80a365c:	f108 0801 	add.w	r8, r8, #1
 80a3660:	440f      	add	r7, r1
 80a3662:	e7e9      	b.n	80a3638 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xc6>
        if (cmp(curr_value, min_max_value)) {
          min_max_value = curr_value;
          min_max_index = static_cast<T2>(i);
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
 80a3664:	9900      	ldr	r1, [sp, #0]
 80a3666:	f841 3025 	str.w	r3, [r1, r5, lsl #2]
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
 80a366a:	3501      	adds	r5, #1
 80a366c:	e7d5      	b.n	80a361a <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xa8>
 80a366e:	e89d 000a 	ldmia.w	sp, {r1, r3}
 80a3672:	440b      	add	r3, r1
 80a3674:	9300      	str	r3, [sp, #0]
 80a3676:	9b05      	ldr	r3, [sp, #20]
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
 80a3678:	f10a 0a01 	add.w	sl, sl, #1
 80a367c:	441a      	add	r2, r3
 80a367e:	444e      	add	r6, r9
 80a3680:	e7c5      	b.n	80a360e <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x9c>
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
    }
  }
}
 80a3682:	b00b      	add	sp, #44	; 0x2c
 80a3684:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a3688 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>:
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
 80a3688:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a368c:	6806      	ldr	r6, [r0, #0]
 80a368e:	b087      	sub	sp, #28
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
 80a3690:	2e00      	cmp	r6, #0
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
 80a3692:	4681      	mov	r9, r0
 80a3694:	460f      	mov	r7, r1
 80a3696:	469a      	mov	sl, r3
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
 80a3698:	dc01      	bgt.n	80a369e <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x16>
 80a369a:	f00c fcc9 	bl	80b0030 <abort>
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
 80a369e:	f8da 1000 	ldr.w	r1, [sl]
 80a36a2:	1e73      	subs	r3, r6, #1
 80a36a4:	428b      	cmp	r3, r1
 80a36a6:	d1f8      	bne.n	80a369a <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
 80a36a8:	6814      	ldr	r4, [r2, #0]
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80a36aa:	f04f 0b00 	mov.w	fp, #0
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
  if (axis < 0) {
 80a36ae:	2c00      	cmp	r4, #0
    axis += input1_shape.DimensionsCount();
 80a36b0:	bfb8      	it	lt
 80a36b2:	19a4      	addlt	r4, r4, r6
  }
  const int axis_size = input1_shape.Dims(axis);
 80a36b4:	4621      	mov	r1, r4
 80a36b6:	f7fe fe87 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a36ba:	4605      	mov	r5, r0

  int outer_size = 1;
 80a36bc:	f04f 0801 	mov.w	r8, #1
  for (int i = 0; i < axis; ++i) {
 80a36c0:	455c      	cmp	r4, fp
 80a36c2:	dd10      	ble.n	80a36e6 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x5e>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
 80a36c4:	4659      	mov	r1, fp
 80a36c6:	4648      	mov	r0, r9
 80a36c8:	f7fe fe7e 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a36cc:	4659      	mov	r1, fp
 80a36ce:	9001      	str	r0, [sp, #4]
 80a36d0:	4650      	mov	r0, sl
 80a36d2:	f7fe fe79 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a36d6:	9b01      	ldr	r3, [sp, #4]
 80a36d8:	4283      	cmp	r3, r0
 80a36da:	d1de      	bne.n	80a369a <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    outer_size *= input1_shape.Dims(i);
 80a36dc:	fb03 f808 	mul.w	r8, r3, r8
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80a36e0:	f10b 0b01 	add.w	fp, fp, #1
 80a36e4:	e7ec      	b.n	80a36c0 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x38>
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
 80a36e6:	f104 0b01 	add.w	fp, r4, #1
  for (int i = 0; i < axis; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
 80a36ea:	2401      	movs	r4, #1
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
 80a36ec:	45b3      	cmp	fp, r6
 80a36ee:	da10      	bge.n	80a3712 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x8a>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
 80a36f0:	4659      	mov	r1, fp
 80a36f2:	4648      	mov	r0, r9
 80a36f4:	f7fe fe68 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a36f8:	f10b 31ff 	add.w	r1, fp, #4294967295	; 0xffffffff
 80a36fc:	9001      	str	r0, [sp, #4]
 80a36fe:	4650      	mov	r0, sl
 80a3700:	f7fe fe62 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a3704:	9b01      	ldr	r3, [sp, #4]
 80a3706:	4283      	cmp	r3, r0
 80a3708:	d1c7      	bne.n	80a369a <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    inner_size *= input1_shape.Dims(i);
 80a370a:	435c      	muls	r4, r3
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
 80a370c:	f10b 0b01 	add.w	fp, fp, #1
 80a3710:	e7ec      	b.n	80a36ec <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x64>
 80a3712:	2200      	movs	r2, #0
 80a3714:	4694      	mov	ip, r2
 80a3716:	00a3      	lsls	r3, r4, #2
 80a3718:	9302      	str	r3, [sp, #8]
 80a371a:	fb05 f304 	mul.w	r3, r5, r4
 80a371e:	f8dd e040 	ldr.w	lr, [sp, #64]	; 0x40
 80a3722:	9304      	str	r3, [sp, #16]
 80a3724:	9701      	str	r7, [sp, #4]
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
 80a3726:	45e0      	cmp	r8, ip
 80a3728:	dd29      	ble.n	80a377e <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xf6>
 80a372a:	fb02 4304 	mla	r3, r2, r4, r4
 80a372e:	9305      	str	r3, [sp, #20]
 80a3730:	2300      	movs	r3, #0
    for (int inner = 0; inner < inner_size; ++inner) {
 80a3732:	429c      	cmp	r4, r3
 80a3734:	dd19      	ble.n	80a376a <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xe2>
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
 80a3736:	9901      	ldr	r1, [sp, #4]
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
 80a3738:	2001      	movs	r0, #1
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
 80a373a:	f811 a003 	ldrb.w	sl, [r1, r3]
 80a373e:	9905      	ldr	r1, [sp, #20]
 80a3740:	1859      	adds	r1, r3, r1
 80a3742:	1879      	adds	r1, r7, r1
 80a3744:	9103      	str	r1, [sp, #12]
 80a3746:	2100      	movs	r1, #0
      T2 min_max_index = 0;
 80a3748:	4689      	mov	r9, r1
      for (int i = 1; i < axis_size; ++i) {
 80a374a:	42a8      	cmp	r0, r5
 80a374c:	da09      	bge.n	80a3762 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xda>
 80a374e:	9e03      	ldr	r6, [sp, #12]
 80a3750:	f816 b001 	ldrb.w	fp, [r6, r1]
 80a3754:	4421      	add	r1, r4
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
 80a3756:	45da      	cmp	sl, fp
 80a3758:	bf3c      	itt	cc
 80a375a:	4681      	movcc	r9, r0
 80a375c:	46da      	movcc	sl, fp
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
 80a375e:	3001      	adds	r0, #1
 80a3760:	e7f3      	b.n	80a374a <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xc2>
        if (cmp(curr_value, min_max_value)) {
          min_max_value = curr_value;
          min_max_index = static_cast<T2>(i);
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
 80a3762:	f84e 9023 	str.w	r9, [lr, r3, lsl #2]
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
 80a3766:	3301      	adds	r3, #1
 80a3768:	e7e3      	b.n	80a3732 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xaa>
 80a376a:	9b02      	ldr	r3, [sp, #8]
 80a376c:	9901      	ldr	r1, [sp, #4]
 80a376e:	449e      	add	lr, r3
 80a3770:	9b04      	ldr	r3, [sp, #16]
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
 80a3772:	f10c 0c01 	add.w	ip, ip, #1
 80a3776:	4419      	add	r1, r3
 80a3778:	9101      	str	r1, [sp, #4]
 80a377a:	442a      	add	r2, r5
 80a377c:	e7d3      	b.n	80a3726 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x9e>
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
    }
  }
}
 80a377e:	b007      	add	sp, #28
 80a3780:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a3784 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>:
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
 80a3784:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a3788:	6806      	ldr	r6, [r0, #0]
 80a378a:	b087      	sub	sp, #28
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
 80a378c:	2e00      	cmp	r6, #0
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
 80a378e:	4681      	mov	r9, r0
 80a3790:	460f      	mov	r7, r1
 80a3792:	469a      	mov	sl, r3
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
 80a3794:	dc01      	bgt.n	80a379a <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x16>
 80a3796:	f00c fc4b 	bl	80b0030 <abort>
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
 80a379a:	f8da 1000 	ldr.w	r1, [sl]
 80a379e:	1e73      	subs	r3, r6, #1
 80a37a0:	428b      	cmp	r3, r1
 80a37a2:	d1f8      	bne.n	80a3796 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
 80a37a4:	6814      	ldr	r4, [r2, #0]
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80a37a6:	f04f 0b00 	mov.w	fp, #0
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
  if (axis < 0) {
 80a37aa:	2c00      	cmp	r4, #0
    axis += input1_shape.DimensionsCount();
 80a37ac:	bfb8      	it	lt
 80a37ae:	19a4      	addlt	r4, r4, r6
  }
  const int axis_size = input1_shape.Dims(axis);
 80a37b0:	4621      	mov	r1, r4
 80a37b2:	f7fe fe09 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a37b6:	4605      	mov	r5, r0

  int outer_size = 1;
 80a37b8:	f04f 0801 	mov.w	r8, #1
  for (int i = 0; i < axis; ++i) {
 80a37bc:	455c      	cmp	r4, fp
 80a37be:	dd10      	ble.n	80a37e2 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x5e>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
 80a37c0:	4659      	mov	r1, fp
 80a37c2:	4648      	mov	r0, r9
 80a37c4:	f7fe fe00 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a37c8:	4659      	mov	r1, fp
 80a37ca:	9001      	str	r0, [sp, #4]
 80a37cc:	4650      	mov	r0, sl
 80a37ce:	f7fe fdfb 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a37d2:	9b01      	ldr	r3, [sp, #4]
 80a37d4:	4283      	cmp	r3, r0
 80a37d6:	d1de      	bne.n	80a3796 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    outer_size *= input1_shape.Dims(i);
 80a37d8:	fb03 f808 	mul.w	r8, r3, r8
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80a37dc:	f10b 0b01 	add.w	fp, fp, #1
 80a37e0:	e7ec      	b.n	80a37bc <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x38>
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
 80a37e2:	f104 0b01 	add.w	fp, r4, #1
  for (int i = 0; i < axis; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
 80a37e6:	2401      	movs	r4, #1
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
 80a37e8:	45b3      	cmp	fp, r6
 80a37ea:	da10      	bge.n	80a380e <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x8a>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
 80a37ec:	4659      	mov	r1, fp
 80a37ee:	4648      	mov	r0, r9
 80a37f0:	f7fe fdea 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a37f4:	f10b 31ff 	add.w	r1, fp, #4294967295	; 0xffffffff
 80a37f8:	9001      	str	r0, [sp, #4]
 80a37fa:	4650      	mov	r0, sl
 80a37fc:	f7fe fde4 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a3800:	9b01      	ldr	r3, [sp, #4]
 80a3802:	4283      	cmp	r3, r0
 80a3804:	d1c7      	bne.n	80a3796 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    inner_size *= input1_shape.Dims(i);
 80a3806:	435c      	muls	r4, r3
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
 80a3808:	f10b 0b01 	add.w	fp, fp, #1
 80a380c:	e7ec      	b.n	80a37e8 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x64>
 80a380e:	2200      	movs	r2, #0
 80a3810:	4694      	mov	ip, r2
 80a3812:	00a3      	lsls	r3, r4, #2
 80a3814:	9302      	str	r3, [sp, #8]
 80a3816:	fb05 f304 	mul.w	r3, r5, r4
 80a381a:	f8dd e040 	ldr.w	lr, [sp, #64]	; 0x40
 80a381e:	9304      	str	r3, [sp, #16]
 80a3820:	9701      	str	r7, [sp, #4]
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
 80a3822:	45e0      	cmp	r8, ip
 80a3824:	dd29      	ble.n	80a387a <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xf6>
 80a3826:	fb02 4304 	mla	r3, r2, r4, r4
 80a382a:	9305      	str	r3, [sp, #20]
 80a382c:	2300      	movs	r3, #0
    for (int inner = 0; inner < inner_size; ++inner) {
 80a382e:	429c      	cmp	r4, r3
 80a3830:	dd19      	ble.n	80a3866 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xe2>
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
 80a3832:	9901      	ldr	r1, [sp, #4]
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
 80a3834:	2001      	movs	r0, #1
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
 80a3836:	f811 a003 	ldrb.w	sl, [r1, r3]
 80a383a:	9905      	ldr	r1, [sp, #20]
 80a383c:	1859      	adds	r1, r3, r1
 80a383e:	1879      	adds	r1, r7, r1
 80a3840:	9103      	str	r1, [sp, #12]
 80a3842:	2100      	movs	r1, #0
      T2 min_max_index = 0;
 80a3844:	4689      	mov	r9, r1
      for (int i = 1; i < axis_size; ++i) {
 80a3846:	42a8      	cmp	r0, r5
 80a3848:	da09      	bge.n	80a385e <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xda>
 80a384a:	9e03      	ldr	r6, [sp, #12]
 80a384c:	f816 b001 	ldrb.w	fp, [r6, r1]
 80a3850:	4421      	add	r1, r4
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
 80a3852:	45da      	cmp	sl, fp
 80a3854:	bf84      	itt	hi
 80a3856:	4681      	movhi	r9, r0
 80a3858:	46da      	movhi	sl, fp
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
 80a385a:	3001      	adds	r0, #1
 80a385c:	e7f3      	b.n	80a3846 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xc2>
        if (cmp(curr_value, min_max_value)) {
          min_max_value = curr_value;
          min_max_index = static_cast<T2>(i);
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
 80a385e:	f84e 9023 	str.w	r9, [lr, r3, lsl #2]
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
 80a3862:	3301      	adds	r3, #1
 80a3864:	e7e3      	b.n	80a382e <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xaa>
 80a3866:	9b02      	ldr	r3, [sp, #8]
 80a3868:	9901      	ldr	r1, [sp, #4]
 80a386a:	449e      	add	lr, r3
 80a386c:	9b04      	ldr	r3, [sp, #16]
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
 80a386e:	f10c 0c01 	add.w	ip, ip, #1
 80a3872:	4419      	add	r1, r3
 80a3874:	9101      	str	r1, [sp, #4]
 80a3876:	442a      	add	r2, r5
 80a3878:	e7d3      	b.n	80a3822 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x9e>
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
    }
  }
}
 80a387a:	b007      	add	sp, #28
 80a387c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a3880 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>:
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
 80a3880:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a3884:	6806      	ldr	r6, [r0, #0]
 80a3886:	b087      	sub	sp, #28
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
 80a3888:	2e00      	cmp	r6, #0
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
 80a388a:	4681      	mov	r9, r0
 80a388c:	460f      	mov	r7, r1
 80a388e:	469a      	mov	sl, r3
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
 80a3890:	dc01      	bgt.n	80a3896 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x16>
 80a3892:	f00c fbcd 	bl	80b0030 <abort>
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
 80a3896:	f8da 1000 	ldr.w	r1, [sl]
 80a389a:	1e73      	subs	r3, r6, #1
 80a389c:	428b      	cmp	r3, r1
 80a389e:	d1f8      	bne.n	80a3892 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
 80a38a0:	6814      	ldr	r4, [r2, #0]
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80a38a2:	f04f 0b00 	mov.w	fp, #0
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
  if (axis < 0) {
 80a38a6:	2c00      	cmp	r4, #0
    axis += input1_shape.DimensionsCount();
 80a38a8:	bfb8      	it	lt
 80a38aa:	19a4      	addlt	r4, r4, r6
  }
  const int axis_size = input1_shape.Dims(axis);
 80a38ac:	4621      	mov	r1, r4
 80a38ae:	f7fe fd8b 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a38b2:	4605      	mov	r5, r0

  int outer_size = 1;
 80a38b4:	f04f 0801 	mov.w	r8, #1
  for (int i = 0; i < axis; ++i) {
 80a38b8:	455c      	cmp	r4, fp
 80a38ba:	dd10      	ble.n	80a38de <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x5e>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
 80a38bc:	4659      	mov	r1, fp
 80a38be:	4648      	mov	r0, r9
 80a38c0:	f7fe fd82 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a38c4:	4659      	mov	r1, fp
 80a38c6:	9001      	str	r0, [sp, #4]
 80a38c8:	4650      	mov	r0, sl
 80a38ca:	f7fe fd7d 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a38ce:	9b01      	ldr	r3, [sp, #4]
 80a38d0:	4283      	cmp	r3, r0
 80a38d2:	d1de      	bne.n	80a3892 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    outer_size *= input1_shape.Dims(i);
 80a38d4:	fb03 f808 	mul.w	r8, r3, r8
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80a38d8:	f10b 0b01 	add.w	fp, fp, #1
 80a38dc:	e7ec      	b.n	80a38b8 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x38>
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
 80a38de:	f104 0b01 	add.w	fp, r4, #1
  for (int i = 0; i < axis; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
 80a38e2:	2401      	movs	r4, #1
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
 80a38e4:	45b3      	cmp	fp, r6
 80a38e6:	da10      	bge.n	80a390a <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x8a>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
 80a38e8:	4659      	mov	r1, fp
 80a38ea:	4648      	mov	r0, r9
 80a38ec:	f7fe fd6c 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a38f0:	f10b 31ff 	add.w	r1, fp, #4294967295	; 0xffffffff
 80a38f4:	9001      	str	r0, [sp, #4]
 80a38f6:	4650      	mov	r0, sl
 80a38f8:	f7fe fd66 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a38fc:	9b01      	ldr	r3, [sp, #4]
 80a38fe:	4283      	cmp	r3, r0
 80a3900:	d1c7      	bne.n	80a3892 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    inner_size *= input1_shape.Dims(i);
 80a3902:	435c      	muls	r4, r3
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
 80a3904:	f10b 0b01 	add.w	fp, fp, #1
 80a3908:	e7ec      	b.n	80a38e4 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x64>
 80a390a:	2200      	movs	r2, #0
 80a390c:	4694      	mov	ip, r2
 80a390e:	00a3      	lsls	r3, r4, #2
 80a3910:	9302      	str	r3, [sp, #8]
 80a3912:	fb05 f304 	mul.w	r3, r5, r4
 80a3916:	f8dd e040 	ldr.w	lr, [sp, #64]	; 0x40
 80a391a:	9304      	str	r3, [sp, #16]
 80a391c:	9701      	str	r7, [sp, #4]
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
 80a391e:	45e0      	cmp	r8, ip
 80a3920:	dd29      	ble.n	80a3976 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xf6>
 80a3922:	fb02 4304 	mla	r3, r2, r4, r4
 80a3926:	9305      	str	r3, [sp, #20]
 80a3928:	2300      	movs	r3, #0
    for (int inner = 0; inner < inner_size; ++inner) {
 80a392a:	429c      	cmp	r4, r3
 80a392c:	dd19      	ble.n	80a3962 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xe2>
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
 80a392e:	9901      	ldr	r1, [sp, #4]
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
 80a3930:	2001      	movs	r0, #1
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
 80a3932:	f911 a003 	ldrsb.w	sl, [r1, r3]
 80a3936:	9905      	ldr	r1, [sp, #20]
 80a3938:	1859      	adds	r1, r3, r1
 80a393a:	1879      	adds	r1, r7, r1
 80a393c:	9103      	str	r1, [sp, #12]
 80a393e:	2100      	movs	r1, #0
      T2 min_max_index = 0;
 80a3940:	4689      	mov	r9, r1
      for (int i = 1; i < axis_size; ++i) {
 80a3942:	42a8      	cmp	r0, r5
 80a3944:	da09      	bge.n	80a395a <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xda>
 80a3946:	9e03      	ldr	r6, [sp, #12]
 80a3948:	f916 b001 	ldrsb.w	fp, [r6, r1]
 80a394c:	4421      	add	r1, r4
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
 80a394e:	45da      	cmp	sl, fp
 80a3950:	bfbc      	itt	lt
 80a3952:	4681      	movlt	r9, r0
 80a3954:	46da      	movlt	sl, fp
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
 80a3956:	3001      	adds	r0, #1
 80a3958:	e7f3      	b.n	80a3942 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xc2>
        if (cmp(curr_value, min_max_value)) {
          min_max_value = curr_value;
          min_max_index = static_cast<T2>(i);
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
 80a395a:	f84e 9023 	str.w	r9, [lr, r3, lsl #2]
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
 80a395e:	3301      	adds	r3, #1
 80a3960:	e7e3      	b.n	80a392a <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xaa>
 80a3962:	9b02      	ldr	r3, [sp, #8]
 80a3964:	9901      	ldr	r1, [sp, #4]
 80a3966:	449e      	add	lr, r3
 80a3968:	9b04      	ldr	r3, [sp, #16]
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
 80a396a:	f10c 0c01 	add.w	ip, ip, #1
 80a396e:	4419      	add	r1, r3
 80a3970:	9101      	str	r1, [sp, #4]
 80a3972:	442a      	add	r2, r5
 80a3974:	e7d3      	b.n	80a391e <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x9e>
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
    }
  }
}
 80a3976:	b007      	add	sp, #28
 80a3978:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a397c <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>:
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
 80a397c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a3980:	6806      	ldr	r6, [r0, #0]
 80a3982:	b087      	sub	sp, #28
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
 80a3984:	2e00      	cmp	r6, #0
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
 80a3986:	4681      	mov	r9, r0
 80a3988:	460f      	mov	r7, r1
 80a398a:	469a      	mov	sl, r3
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
 80a398c:	dc01      	bgt.n	80a3992 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x16>
 80a398e:	f00c fb4f 	bl	80b0030 <abort>
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
 80a3992:	f8da 1000 	ldr.w	r1, [sl]
 80a3996:	1e73      	subs	r3, r6, #1
 80a3998:	428b      	cmp	r3, r1
 80a399a:	d1f8      	bne.n	80a398e <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
 80a399c:	6814      	ldr	r4, [r2, #0]
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80a399e:	f04f 0b00 	mov.w	fp, #0
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
  if (axis < 0) {
 80a39a2:	2c00      	cmp	r4, #0
    axis += input1_shape.DimensionsCount();
 80a39a4:	bfb8      	it	lt
 80a39a6:	19a4      	addlt	r4, r4, r6
  }
  const int axis_size = input1_shape.Dims(axis);
 80a39a8:	4621      	mov	r1, r4
 80a39aa:	f7fe fd0d 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a39ae:	4605      	mov	r5, r0

  int outer_size = 1;
 80a39b0:	f04f 0801 	mov.w	r8, #1
  for (int i = 0; i < axis; ++i) {
 80a39b4:	455c      	cmp	r4, fp
 80a39b6:	dd10      	ble.n	80a39da <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x5e>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
 80a39b8:	4659      	mov	r1, fp
 80a39ba:	4648      	mov	r0, r9
 80a39bc:	f7fe fd04 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a39c0:	4659      	mov	r1, fp
 80a39c2:	9001      	str	r0, [sp, #4]
 80a39c4:	4650      	mov	r0, sl
 80a39c6:	f7fe fcff 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a39ca:	9b01      	ldr	r3, [sp, #4]
 80a39cc:	4283      	cmp	r3, r0
 80a39ce:	d1de      	bne.n	80a398e <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    outer_size *= input1_shape.Dims(i);
 80a39d0:	fb03 f808 	mul.w	r8, r3, r8
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80a39d4:	f10b 0b01 	add.w	fp, fp, #1
 80a39d8:	e7ec      	b.n	80a39b4 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x38>
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
 80a39da:	f104 0b01 	add.w	fp, r4, #1
  for (int i = 0; i < axis; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
 80a39de:	2401      	movs	r4, #1
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
 80a39e0:	45b3      	cmp	fp, r6
 80a39e2:	da10      	bge.n	80a3a06 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x8a>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
 80a39e4:	4659      	mov	r1, fp
 80a39e6:	4648      	mov	r0, r9
 80a39e8:	f7fe fcee 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a39ec:	f10b 31ff 	add.w	r1, fp, #4294967295	; 0xffffffff
 80a39f0:	9001      	str	r0, [sp, #4]
 80a39f2:	4650      	mov	r0, sl
 80a39f4:	f7fe fce8 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a39f8:	9b01      	ldr	r3, [sp, #4]
 80a39fa:	4283      	cmp	r3, r0
 80a39fc:	d1c7      	bne.n	80a398e <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    inner_size *= input1_shape.Dims(i);
 80a39fe:	435c      	muls	r4, r3
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
 80a3a00:	f10b 0b01 	add.w	fp, fp, #1
 80a3a04:	e7ec      	b.n	80a39e0 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x64>
 80a3a06:	2200      	movs	r2, #0
 80a3a08:	4694      	mov	ip, r2
 80a3a0a:	00a3      	lsls	r3, r4, #2
 80a3a0c:	9302      	str	r3, [sp, #8]
 80a3a0e:	fb05 f304 	mul.w	r3, r5, r4
 80a3a12:	f8dd e040 	ldr.w	lr, [sp, #64]	; 0x40
 80a3a16:	9304      	str	r3, [sp, #16]
 80a3a18:	9701      	str	r7, [sp, #4]
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
 80a3a1a:	45e0      	cmp	r8, ip
 80a3a1c:	dd29      	ble.n	80a3a72 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xf6>
 80a3a1e:	fb02 4304 	mla	r3, r2, r4, r4
 80a3a22:	9305      	str	r3, [sp, #20]
 80a3a24:	2300      	movs	r3, #0
    for (int inner = 0; inner < inner_size; ++inner) {
 80a3a26:	429c      	cmp	r4, r3
 80a3a28:	dd19      	ble.n	80a3a5e <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xe2>
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
 80a3a2a:	9901      	ldr	r1, [sp, #4]
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
 80a3a2c:	2001      	movs	r0, #1
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
 80a3a2e:	f911 a003 	ldrsb.w	sl, [r1, r3]
 80a3a32:	9905      	ldr	r1, [sp, #20]
 80a3a34:	1859      	adds	r1, r3, r1
 80a3a36:	1879      	adds	r1, r7, r1
 80a3a38:	9103      	str	r1, [sp, #12]
 80a3a3a:	2100      	movs	r1, #0
      T2 min_max_index = 0;
 80a3a3c:	4689      	mov	r9, r1
      for (int i = 1; i < axis_size; ++i) {
 80a3a3e:	42a8      	cmp	r0, r5
 80a3a40:	da09      	bge.n	80a3a56 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xda>
 80a3a42:	9e03      	ldr	r6, [sp, #12]
 80a3a44:	f916 b001 	ldrsb.w	fp, [r6, r1]
 80a3a48:	4421      	add	r1, r4
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
 80a3a4a:	45da      	cmp	sl, fp
 80a3a4c:	bfc4      	itt	gt
 80a3a4e:	4681      	movgt	r9, r0
 80a3a50:	46da      	movgt	sl, fp
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
 80a3a52:	3001      	adds	r0, #1
 80a3a54:	e7f3      	b.n	80a3a3e <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xc2>
        if (cmp(curr_value, min_max_value)) {
          min_max_value = curr_value;
          min_max_index = static_cast<T2>(i);
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
 80a3a56:	f84e 9023 	str.w	r9, [lr, r3, lsl #2]
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
 80a3a5a:	3301      	adds	r3, #1
 80a3a5c:	e7e3      	b.n	80a3a26 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xaa>
 80a3a5e:	9b02      	ldr	r3, [sp, #8]
 80a3a60:	9901      	ldr	r1, [sp, #4]
 80a3a62:	449e      	add	lr, r3
 80a3a64:	9b04      	ldr	r3, [sp, #16]
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
 80a3a66:	f10c 0c01 	add.w	ip, ip, #1
 80a3a6a:	4419      	add	r1, r3
 80a3a6c:	9101      	str	r1, [sp, #4]
 80a3a6e:	442a      	add	r2, r5
 80a3a70:	e7d3      	b.n	80a3a1a <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x9e>
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
    }
  }
}
 80a3a72:	b007      	add	sp, #28
 80a3a74:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a3a78 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb>:
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
                             output_shape, output_data, micro::Less());
  }
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node, bool is_arg_max) {
 80a3a78:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
 80a3a7c:	680e      	ldr	r6, [r1, #0]
 80a3a7e:	4604      	mov	r4, r0
 80a3a80:	4617      	mov	r7, r2
 80a3a82:	6882      	ldr	r2, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a3a84:	68b0      	ldr	r0, [r6, #8]
 80a3a86:	2338      	movs	r3, #56	; 0x38
 80a3a88:	4358      	muls	r0, r3
 80a3a8a:	eb02 0800 	add.w	r8, r2, r0

#define TF_LITE_ARG_MIN_MAX(data_type, axis_type, output_type)            \
  ArgMinMaxHelper(GetTensorShape(input), GetTensorData<data_type>(input), \
                  GetTensorData<axis_type>(axis), GetTensorShape(output), \
                  GetTensorData<output_type>(output), is_arg_max)
  if (axis->type == kTfLiteInt32) {
 80a3a8e:	5c10      	ldrb	r0, [r2, r0]
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
                             output_shape, output_data, micro::Less());
  }
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node, bool is_arg_max) {
 80a3a90:	b08e      	sub	sp, #56	; 0x38

#define TF_LITE_ARG_MIN_MAX(data_type, axis_type, output_type)            \
  ArgMinMaxHelper(GetTensorShape(input), GetTensorData<data_type>(input), \
                  GetTensorData<axis_type>(axis), GetTensorShape(output), \
                  GetTensorData<output_type>(output), is_arg_max)
  if (axis->type == kTfLiteInt32) {
 80a3a92:	2802      	cmp	r0, #2
 80a3a94:	d16b      	bne.n	80a3b6e <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xf6>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a3a96:	6849      	ldr	r1, [r1, #4]
 80a3a98:	6849      	ldr	r1, [r1, #4]
 80a3a9a:	4359      	muls	r1, r3
    if (output->type == kTfLiteInt32) {
 80a3a9c:	5c50      	ldrb	r0, [r2, r1]
 80a3a9e:	1855      	adds	r5, r2, r1
 80a3aa0:	2802      	cmp	r0, #2
 80a3aa2:	d164      	bne.n	80a3b6e <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xf6>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a3aa4:	6871      	ldr	r1, [r6, #4]
 80a3aa6:	434b      	muls	r3, r1
      switch (input->type) {
 80a3aa8:	5cd0      	ldrb	r0, [r2, r3]
 80a3aaa:	18d6      	adds	r6, r2, r3
 80a3aac:	2803      	cmp	r0, #3
 80a3aae:	d01d      	beq.n	80a3aec <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0x74>
 80a3ab0:	2809      	cmp	r0, #9
 80a3ab2:	d035      	beq.n	80a3b20 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xa8>
 80a3ab4:	2801      	cmp	r0, #1
 80a3ab6:	d154      	bne.n	80a3b62 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xea>
        case kTfLiteFloat32:
          TF_LITE_ARG_MIN_MAX(float, int32_t, int32_t);
 80a3ab8:	4631      	mov	r1, r6
 80a3aba:	a804      	add	r0, sp, #16
 80a3abc:	f7fe ff29 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a3ac0:	4629      	mov	r1, r5
 80a3ac2:	a809      	add	r0, sp, #36	; 0x24
 80a3ac4:	6874      	ldr	r4, [r6, #4]
 80a3ac6:	f8d8 6004 	ldr.w	r6, [r8, #4]
 80a3aca:	f7fe ff22 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a3ace:	686b      	ldr	r3, [r5, #4]
 80a3ad0:	aa03      	add	r2, sp, #12
inline void ArgMinMaxHelper(const RuntimeShape& input1_shape,
                            const T1* input1_data, const T3* input2_data,
                            const RuntimeShape& output_shape, T2* output_data,
                            bool is_arg_max) {
  if (is_arg_max) {
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
 80a3ad2:	9201      	str	r2, [sp, #4]
 80a3ad4:	9300      	str	r3, [sp, #0]
 80a3ad6:	4632      	mov	r2, r6
 80a3ad8:	ab09      	add	r3, sp, #36	; 0x24
 80a3ada:	4621      	mov	r1, r4
 80a3adc:	a804      	add	r0, sp, #16
template <typename T1, typename T2, typename T3>
inline void ArgMinMaxHelper(const RuntimeShape& input1_shape,
                            const T1* input1_data, const T3* input2_data,
                            const RuntimeShape& output_shape, T2* output_data,
                            bool is_arg_max) {
  if (is_arg_max) {
 80a3ade:	b117      	cbz	r7, 80a3ae6 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0x6e>
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
 80a3ae0:	f7ff fcbc 	bl	80a345c <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>
 80a3ae4:	e035      	b.n	80a3b52 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xda>
                             output_shape, output_data, micro::Greater());
  } else {
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
 80a3ae6:	f7ff fd44 	bl	80a3572 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>
 80a3aea:	e032      	b.n	80a3b52 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xda>
      switch (input->type) {
        case kTfLiteFloat32:
          TF_LITE_ARG_MIN_MAX(float, int32_t, int32_t);
          break;
        case kTfLiteUInt8:
          TF_LITE_ARG_MIN_MAX(uint8_t, int32_t, int32_t);
 80a3aec:	4631      	mov	r1, r6
 80a3aee:	a804      	add	r0, sp, #16
 80a3af0:	f7fe ff0f 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a3af4:	4629      	mov	r1, r5
 80a3af6:	a809      	add	r0, sp, #36	; 0x24
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a3af8:	6874      	ldr	r4, [r6, #4]
 80a3afa:	f8d8 6004 	ldr.w	r6, [r8, #4]
 80a3afe:	f7fe ff08 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a3b02:	686b      	ldr	r3, [r5, #4]
 80a3b04:	aa03      	add	r2, sp, #12
inline void ArgMinMaxHelper(const RuntimeShape& input1_shape,
                            const T1* input1_data, const T3* input2_data,
                            const RuntimeShape& output_shape, T2* output_data,
                            bool is_arg_max) {
  if (is_arg_max) {
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
 80a3b06:	9201      	str	r2, [sp, #4]
 80a3b08:	9300      	str	r3, [sp, #0]
 80a3b0a:	4632      	mov	r2, r6
 80a3b0c:	ab09      	add	r3, sp, #36	; 0x24
 80a3b0e:	4621      	mov	r1, r4
 80a3b10:	a804      	add	r0, sp, #16
template <typename T1, typename T2, typename T3>
inline void ArgMinMaxHelper(const RuntimeShape& input1_shape,
                            const T1* input1_data, const T3* input2_data,
                            const RuntimeShape& output_shape, T2* output_data,
                            bool is_arg_max) {
  if (is_arg_max) {
 80a3b12:	b117      	cbz	r7, 80a3b1a <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xa2>
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
 80a3b14:	f7ff fdb8 	bl	80a3688 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>
 80a3b18:	e01b      	b.n	80a3b52 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xda>
                             output_shape, output_data, micro::Greater());
  } else {
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
 80a3b1a:	f7ff fe33 	bl	80a3784 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>
 80a3b1e:	e018      	b.n	80a3b52 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xda>
          break;
        case kTfLiteUInt8:
          TF_LITE_ARG_MIN_MAX(uint8_t, int32_t, int32_t);
          break;
        case kTfLiteInt8:
          TF_LITE_ARG_MIN_MAX(int8_t, int32_t, int32_t);
 80a3b20:	4631      	mov	r1, r6
 80a3b22:	a804      	add	r0, sp, #16
 80a3b24:	f7fe fef5 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a3b28:	4629      	mov	r1, r5
 80a3b2a:	a809      	add	r0, sp, #36	; 0x24
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a3b2c:	6874      	ldr	r4, [r6, #4]
 80a3b2e:	f8d8 6004 	ldr.w	r6, [r8, #4]
 80a3b32:	f7fe feee 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a3b36:	686b      	ldr	r3, [r5, #4]
 80a3b38:	aa03      	add	r2, sp, #12
inline void ArgMinMaxHelper(const RuntimeShape& input1_shape,
                            const T1* input1_data, const T3* input2_data,
                            const RuntimeShape& output_shape, T2* output_data,
                            bool is_arg_max) {
  if (is_arg_max) {
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
 80a3b3a:	9201      	str	r2, [sp, #4]
 80a3b3c:	9300      	str	r3, [sp, #0]
 80a3b3e:	4632      	mov	r2, r6
 80a3b40:	ab09      	add	r3, sp, #36	; 0x24
 80a3b42:	4621      	mov	r1, r4
 80a3b44:	a804      	add	r0, sp, #16
template <typename T1, typename T2, typename T3>
inline void ArgMinMaxHelper(const RuntimeShape& input1_shape,
                            const T1* input1_data, const T3* input2_data,
                            const RuntimeShape& output_shape, T2* output_data,
                            bool is_arg_max) {
  if (is_arg_max) {
 80a3b46:	b117      	cbz	r7, 80a3b4e <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xd6>
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
 80a3b48:	f7ff fe9a 	bl	80a3880 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>
 80a3b4c:	e001      	b.n	80a3b52 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xda>
                             output_shape, output_data, micro::Greater());
  } else {
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
 80a3b4e:	f7ff ff15 	bl	80a397c <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>
          break;
        case kTfLiteUInt8:
          TF_LITE_ARG_MIN_MAX(uint8_t, int32_t, int32_t);
          break;
        case kTfLiteInt8:
          TF_LITE_ARG_MIN_MAX(int8_t, int32_t, int32_t);
 80a3b52:	a809      	add	r0, sp, #36	; 0x24
 80a3b54:	f7fe fc2d 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a3b58:	a804      	add	r0, sp, #16
 80a3b5a:	f7fe fc2a 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
    return kTfLiteError;
  }

#undef TF_LITE_ARG_MIN_MAX

  return kTfLiteOk;
 80a3b5e:	2000      	movs	r0, #0
        case kTfLiteUInt8:
          TF_LITE_ARG_MIN_MAX(uint8_t, int32_t, int32_t);
          break;
        case kTfLiteInt8:
          TF_LITE_ARG_MIN_MAX(int8_t, int32_t, int32_t);
          break;
 80a3b60:	e00d      	b.n	80a3b7e <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0x106>
        default:
          context->ReportError(context,
 80a3b62:	6965      	ldr	r5, [r4, #20]
 80a3b64:	f7fc fada 	bl	80a011c <TfLiteTypeGetName>
                               "Only float32, uint8 and int8 are "
                               "supported currently, got %s.",
                               TfLiteTypeGetName(input->type));
 80a3b68:	4906      	ldr	r1, [pc, #24]	; (80a3b84 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0x10c>)
 80a3b6a:	4602      	mov	r2, r0
 80a3b6c:	e004      	b.n	80a3b78 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0x100>
                           "Only int32 are supported currently, got %s.",
                           TfLiteTypeGetName(output->type));
      return kTfLiteError;
    }
  } else {
    context->ReportError(context, "Only int32 are supported currently, got %s.",
 80a3b6e:	6965      	ldr	r5, [r4, #20]
 80a3b70:	f7fc fad4 	bl	80a011c <TfLiteTypeGetName>
                         TfLiteTypeGetName(axis->type));
 80a3b74:	4602      	mov	r2, r0
 80a3b76:	4904      	ldr	r1, [pc, #16]	; (80a3b88 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0x110>)
 80a3b78:	4620      	mov	r0, r4
 80a3b7a:	47a8      	blx	r5
    return kTfLiteError;
 80a3b7c:	2001      	movs	r0, #1
  }

#undef TF_LITE_ARG_MIN_MAX

  return kTfLiteOk;
}
 80a3b7e:	b00e      	add	sp, #56	; 0x38
 80a3b80:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80a3b84:	080b5ad0 	.word	0x080b5ad0
 80a3b88:	080b5b0e 	.word	0x080b5b0e

080a3b8c <_ZN6tflite3ops5micro11arg_min_max10ArgMinEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus ArgMinEval(TfLiteContext* context, TfLiteNode* node) {
  return Eval(context, node, false);
 80a3b8c:	2200      	movs	r2, #0
 80a3b8e:	f7ff bf73 	b.w	80a3a78 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb>

080a3b92 <_ZN6tflite3ops5micro11arg_min_max10ArgMaxEvalEP13TfLiteContextP10TfLiteNode>:
}

TfLiteStatus ArgMaxEval(TfLiteContext* context, TfLiteNode* node) {
  return Eval(context, node, true);
 80a3b92:	2201      	movs	r2, #1
 80a3b94:	f7ff bf70 	b.w	80a3a78 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb>

080a3b98 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode>:
namespace ceil {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 80a3b98:	b5f0      	push	{r4, r5, r6, r7, lr}
 80a3b9a:	680b      	ldr	r3, [r1, #0]
 80a3b9c:	b085      	sub	sp, #20
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
 80a3b9e:	681e      	ldr	r6, [r3, #0]
 80a3ba0:	4605      	mov	r5, r0
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
 80a3ba2:	2e01      	cmp	r6, #1
 80a3ba4:	d009      	beq.n	80a3bba <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x22>
 80a3ba6:	4b3b      	ldr	r3, [pc, #236]	; (80a3c94 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xfc>)
 80a3ba8:	2401      	movs	r4, #1
 80a3baa:	9301      	str	r3, [sp, #4]
 80a3bac:	4b3a      	ldr	r3, [pc, #232]	; (80a3c98 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x100>)
 80a3bae:	9403      	str	r4, [sp, #12]
 80a3bb0:	9300      	str	r3, [sp, #0]
 80a3bb2:	9602      	str	r6, [sp, #8]
 80a3bb4:	6945      	ldr	r5, [r0, #20]
 80a3bb6:	2321      	movs	r3, #33	; 0x21
 80a3bb8:	e01e      	b.n	80a3bf8 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
 80a3bba:	f8d1 e004 	ldr.w	lr, [r1, #4]
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
 80a3bbe:	f8de 4000 	ldr.w	r4, [lr]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
 80a3bc2:	2c01      	cmp	r4, #1
 80a3bc4:	d008      	beq.n	80a3bd8 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x40>
 80a3bc6:	4b33      	ldr	r3, [pc, #204]	; (80a3c94 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xfc>)
 80a3bc8:	9603      	str	r6, [sp, #12]
 80a3bca:	9301      	str	r3, [sp, #4]
 80a3bcc:	4b33      	ldr	r3, [pc, #204]	; (80a3c9c <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x104>)
 80a3bce:	9402      	str	r4, [sp, #8]
 80a3bd0:	9300      	str	r3, [sp, #0]
 80a3bd2:	6944      	ldr	r4, [r0, #20]
 80a3bd4:	2322      	movs	r3, #34	; 0x22
 80a3bd6:	e022      	b.n	80a3c1e <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x86>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a3bd8:	6859      	ldr	r1, [r3, #4]
 80a3bda:	2338      	movs	r3, #56	; 0x38
 80a3bdc:	4359      	muls	r1, r3
 80a3bde:	6882      	ldr	r2, [r0, #8]
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
 80a3be0:	5c56      	ldrb	r6, [r2, r1]
 80a3be2:	1857      	adds	r7, r2, r1
 80a3be4:	2e01      	cmp	r6, #1
 80a3be6:	d00b      	beq.n	80a3c00 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x68>
 80a3be8:	4b2d      	ldr	r3, [pc, #180]	; (80a3ca0 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x108>)
 80a3bea:	9403      	str	r4, [sp, #12]
 80a3bec:	9301      	str	r3, [sp, #4]
 80a3bee:	4b2d      	ldr	r3, [pc, #180]	; (80a3ca4 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x10c>)
 80a3bf0:	9602      	str	r6, [sp, #8]
 80a3bf2:	9300      	str	r3, [sp, #0]
 80a3bf4:	6945      	ldr	r5, [r0, #20]
 80a3bf6:	2323      	movs	r3, #35	; 0x23
 80a3bf8:	4a2b      	ldr	r2, [pc, #172]	; (80a3ca8 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x110>)
 80a3bfa:	492c      	ldr	r1, [pc, #176]	; (80a3cac <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x114>)
 80a3bfc:	47a8      	blx	r5
 80a3bfe:	e042      	b.n	80a3c86 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xee>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a3c00:	f8de 1004 	ldr.w	r1, [lr, #4]
 80a3c04:	434b      	muls	r3, r1
  TF_LITE_ENSURE_EQ(context, output->type, input->type);
 80a3c06:	5cd4      	ldrb	r4, [r2, r3]
 80a3c08:	18d1      	adds	r1, r2, r3
 80a3c0a:	2c01      	cmp	r4, #1
 80a3c0c:	d00a      	beq.n	80a3c24 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x8c>
 80a3c0e:	4b25      	ldr	r3, [pc, #148]	; (80a3ca4 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x10c>)
 80a3c10:	9603      	str	r6, [sp, #12]
 80a3c12:	9301      	str	r3, [sp, #4]
 80a3c14:	4b26      	ldr	r3, [pc, #152]	; (80a3cb0 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x118>)
 80a3c16:	9402      	str	r4, [sp, #8]
 80a3c18:	9300      	str	r3, [sp, #0]
 80a3c1a:	6944      	ldr	r4, [r0, #20]
 80a3c1c:	2324      	movs	r3, #36	; 0x24
 80a3c1e:	4a22      	ldr	r2, [pc, #136]	; (80a3ca8 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x110>)
 80a3c20:	4922      	ldr	r1, [pc, #136]	; (80a3cac <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x114>)
 80a3c22:	e02f      	b.n	80a3c84 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xec>
  TF_LITE_ENSURE_EQ(context, output->bytes, input->bytes);
 80a3c24:	698b      	ldr	r3, [r1, #24]
 80a3c26:	69ba      	ldr	r2, [r7, #24]
 80a3c28:	4293      	cmp	r3, r2
 80a3c2a:	d008      	beq.n	80a3c3e <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xa6>
 80a3c2c:	9302      	str	r3, [sp, #8]
 80a3c2e:	4b21      	ldr	r3, [pc, #132]	; (80a3cb4 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x11c>)
 80a3c30:	9203      	str	r2, [sp, #12]
 80a3c32:	9301      	str	r3, [sp, #4]
 80a3c34:	4b20      	ldr	r3, [pc, #128]	; (80a3cb8 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x120>)
 80a3c36:	9300      	str	r3, [sp, #0]
 80a3c38:	6945      	ldr	r5, [r0, #20]
 80a3c3a:	2325      	movs	r3, #37	; 0x25
 80a3c3c:	e7dc      	b.n	80a3bf8 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
  TF_LITE_ENSURE_EQ(context, output->dims->size, input->dims->size);
 80a3c3e:	688b      	ldr	r3, [r1, #8]
 80a3c40:	68ba      	ldr	r2, [r7, #8]
 80a3c42:	681e      	ldr	r6, [r3, #0]
 80a3c44:	6811      	ldr	r1, [r2, #0]
 80a3c46:	428e      	cmp	r6, r1
 80a3c48:	d008      	beq.n	80a3c5c <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xc4>
 80a3c4a:	4b1c      	ldr	r3, [pc, #112]	; (80a3cbc <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x124>)
 80a3c4c:	9103      	str	r1, [sp, #12]
 80a3c4e:	9301      	str	r3, [sp, #4]
 80a3c50:	4b1b      	ldr	r3, [pc, #108]	; (80a3cc0 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x128>)
 80a3c52:	9602      	str	r6, [sp, #8]
 80a3c54:	9300      	str	r3, [sp, #0]
 80a3c56:	6945      	ldr	r5, [r0, #20]
 80a3c58:	2326      	movs	r3, #38	; 0x26
 80a3c5a:	e7cd      	b.n	80a3bf8 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
 80a3c5c:	2100      	movs	r1, #0
  for (int i = 0; i < output->dims->size; ++i) {
 80a3c5e:	42b1      	cmp	r1, r6
 80a3c60:	da15      	bge.n	80a3c8e <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xf6>
    TF_LITE_ENSURE_EQ(context, output->dims->data[i], input->dims->data[i]);
 80a3c62:	f853 0f04 	ldr.w	r0, [r3, #4]!
 80a3c66:	f852 4f04 	ldr.w	r4, [r2, #4]!
 80a3c6a:	42a0      	cmp	r0, r4
 80a3c6c:	d00d      	beq.n	80a3c8a <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xf2>
 80a3c6e:	9002      	str	r0, [sp, #8]
 80a3c70:	4628      	mov	r0, r5
 80a3c72:	4b14      	ldr	r3, [pc, #80]	; (80a3cc4 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x12c>)
 80a3c74:	9403      	str	r4, [sp, #12]
 80a3c76:	9301      	str	r3, [sp, #4]
 80a3c78:	4b13      	ldr	r3, [pc, #76]	; (80a3cc8 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x130>)
 80a3c7a:	4a0b      	ldr	r2, [pc, #44]	; (80a3ca8 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x110>)
 80a3c7c:	9300      	str	r3, [sp, #0]
 80a3c7e:	696c      	ldr	r4, [r5, #20]
 80a3c80:	490a      	ldr	r1, [pc, #40]	; (80a3cac <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x114>)
 80a3c82:	2328      	movs	r3, #40	; 0x28
 80a3c84:	47a0      	blx	r4
 80a3c86:	2001      	movs	r0, #1
 80a3c88:	e002      	b.n	80a3c90 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xf8>
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
  TF_LITE_ENSURE_EQ(context, output->type, input->type);
  TF_LITE_ENSURE_EQ(context, output->bytes, input->bytes);
  TF_LITE_ENSURE_EQ(context, output->dims->size, input->dims->size);
  for (int i = 0; i < output->dims->size; ++i) {
 80a3c8a:	3101      	adds	r1, #1
 80a3c8c:	e7e7      	b.n	80a3c5e <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xc6>
    TF_LITE_ENSURE_EQ(context, output->dims->data[i], input->dims->data[i]);
  }
  return kTfLiteOk;
 80a3c8e:	2000      	movs	r0, #0
}
 80a3c90:	b005      	add	sp, #20
 80a3c92:	bdf0      	pop	{r4, r5, r6, r7, pc}
 80a3c94:	080b75ad 	.word	0x080b75ad
 80a3c98:	080b5bfa 	.word	0x080b5bfa
 80a3c9c:	080b5c0a 	.word	0x080b5c0a
 80a3ca0:	080b644f 	.word	0x080b644f
 80a3ca4:	080b5c1b 	.word	0x080b5c1b
 80a3ca8:	080b5b3a 	.word	0x080b5b3a
 80a3cac:	080b5be0 	.word	0x080b5be0
 80a3cb0:	080b5c27 	.word	0x080b5c27
 80a3cb4:	080b5c34 	.word	0x080b5c34
 80a3cb8:	080b5c41 	.word	0x080b5c41
 80a3cbc:	080b5c4f 	.word	0x080b5c4f
 80a3cc0:	080b5c61 	.word	0x080b5c61
 80a3cc4:	080b5c74 	.word	0x080b5c74
 80a3cc8:	080b5c89 	.word	0x080b5c89

080a3ccc <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl>:
      dims_pointer_ = new int32[dimensions_count];
#endif  // TF_LITE_STATIC_MEMORY
    }
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
 80a3ccc:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
  }
  // The caller must ensure that the shape is no bigger than 4-D.
  inline const int32* DimsDataUpTo4D() const { return dims_; }

  inline void Resize(int dimensions_count) {
    if (size_ > kMaxSmallSize) {
 80a3cce:	6803      	ldr	r3, [r0, #0]
      dims_pointer_ = new int32[dimensions_count];
#endif  // TF_LITE_STATIC_MEMORY
    }
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
 80a3cd0:	4604      	mov	r4, r0
  }
  // The caller must ensure that the shape is no bigger than 4-D.
  inline const int32* DimsDataUpTo4D() const { return dims_; }

  inline void Resize(int dimensions_count) {
    if (size_ > kMaxSmallSize) {
 80a3cd2:	2b04      	cmp	r3, #4
      dims_pointer_ = new int32[dimensions_count];
#endif  // TF_LITE_STATIC_MEMORY
    }
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
 80a3cd4:	460d      	mov	r5, r1
 80a3cd6:	4617      	mov	r7, r2
  }
  // The caller must ensure that the shape is no bigger than 4-D.
  inline const int32* DimsDataUpTo4D() const { return dims_; }

  inline void Resize(int dimensions_count) {
    if (size_ > kMaxSmallSize) {
 80a3cd8:	dd03      	ble.n	80a3ce2 <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl+0x16>
#ifdef TF_LITE_STATIC_MEMORY
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
 80a3cda:	6840      	ldr	r0, [r0, #4]
 80a3cdc:	b108      	cbz	r0, 80a3ce2 <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl+0x16>
 80a3cde:	f7fc f9e2 	bl	80a00a6 <_ZdaPv>
#endif  // TF_LITE_STATIC_MEMORY
    }
    size_ = dimensions_count;
    if (dimensions_count > kMaxSmallSize) {
 80a3ce2:	2d04      	cmp	r5, #4
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
#endif  // TF_LITE_STATIC_MEMORY
    }
    size_ = dimensions_count;
 80a3ce4:	6025      	str	r5, [r4, #0]
 80a3ce6:	ea4f 0685 	mov.w	r6, r5, lsl #2
    if (dimensions_count > kMaxSmallSize) {
 80a3cea:	dd08      	ble.n	80a3cfe <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl+0x32>
#ifdef TF_LITE_STATIC_MEMORY
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      dims_pointer_ = new int32[dimensions_count];
 80a3cec:	f1b5 5ffe 	cmp.w	r5, #532676608	; 0x1fc00000
 80a3cf0:	bfd4      	ite	le
 80a3cf2:	4630      	movle	r0, r6
 80a3cf4:	f04f 30ff 	movgt.w	r0, #4294967295	; 0xffffffff
 80a3cf8:	f7fc f9d1 	bl	80a009e <_Znaj>
 80a3cfc:	6060      	str	r0, [r4, #4]
      dims_[i] = val;
    }
  }

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80a3cfe:	6823      	ldr	r3, [r4, #0]
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
    Resize(dimensions_count);
    int32* dst_dims = DimsData();
    std::memcpy(dst_dims, dims_data, dimensions_count * sizeof(int32));
 80a3d00:	4632      	mov	r2, r6
      dims_[i] = val;
    }
  }

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80a3d02:	2b04      	cmp	r3, #4
 80a3d04:	bfcc      	ite	gt
 80a3d06:	6860      	ldrgt	r0, [r4, #4]
 80a3d08:	1d20      	addle	r0, r4, #4
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
    Resize(dimensions_count);
    int32* dst_dims = DimsData();
    std::memcpy(dst_dims, dims_data, dimensions_count * sizeof(int32));
 80a3d0a:	4639      	mov	r1, r7
  }
 80a3d0c:	e8bd 40f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, lr}
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
    Resize(dimensions_count);
    int32* dst_dims = DimsData();
    std::memcpy(dst_dims, dims_data, dimensions_count * sizeof(int32));
 80a3d10:	f00f bedb 	b.w	80b3aca <memcpy>

080a3d14 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80a3d14:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a3d18:	680a      	ldr	r2, [r1, #0]
 80a3d1a:	6883      	ldr	r3, [r0, #8]
 80a3d1c:	6855      	ldr	r5, [r2, #4]
 80a3d1e:	2238      	movs	r2, #56	; 0x38
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a3d20:	6849      	ldr	r1, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a3d22:	fb02 3505 	mla	r5, r2, r5, r3
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a3d26:	684c      	ldr	r4, [r1, #4]
 80a3d28:	b08a      	sub	sp, #40	; 0x28
 80a3d2a:	fb02 3404 	mla	r4, r2, r4, r3
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
}

inline RuntimeShape GetTensorShape(const TfLiteTensor* tensor) {
  if (tensor == nullptr) {
 80a3d2e:	b90d      	cbnz	r5, 80a3d34 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x20>
  // larger shapes are separately allocated.
  static constexpr int kMaxSmallSize = 4;

  RuntimeShape& operator=(RuntimeShape const&) = delete;

  RuntimeShape() : size_(0) {}
 80a3d30:	9500      	str	r5, [sp, #0]
 80a3d32:	e009      	b.n	80a3d48 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x34>
    return RuntimeShape();
  }

  TfLiteIntArray* dims = tensor->dims;
  const int dims_size = dims->size;
 80a3d34:	68aa      	ldr	r2, [r5, #8]
    for (int i = 0; i < shape_size; ++i) {
      SetDim(i, value);
    }
  }

  RuntimeShape(int dimensions_count, const int32* dims_data) : size_(0) {
 80a3d36:	a80a      	add	r0, sp, #40	; 0x28
 80a3d38:	2300      	movs	r3, #0
 80a3d3a:	f852 1b04 	ldr.w	r1, [r2], #4
 80a3d3e:	f840 3d28 	str.w	r3, [r0, #-40]!
    ReplaceWith(dimensions_count, dims_data);
 80a3d42:	f7ff ffc3 	bl	80a3ccc <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a3d46:	686d      	ldr	r5, [r5, #4]
}

inline RuntimeShape GetTensorShape(const TfLiteTensor* tensor) {
  if (tensor == nullptr) {
 80a3d48:	b90c      	cbnz	r4, 80a3d4e <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x3a>
  // larger shapes are separately allocated.
  static constexpr int kMaxSmallSize = 4;

  RuntimeShape& operator=(RuntimeShape const&) = delete;

  RuntimeShape() : size_(0) {}
 80a3d4a:	9405      	str	r4, [sp, #20]
 80a3d4c:	e009      	b.n	80a3d62 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x4e>
    return RuntimeShape();
  }

  TfLiteIntArray* dims = tensor->dims;
  const int dims_size = dims->size;
 80a3d4e:	68a2      	ldr	r2, [r4, #8]
    for (int i = 0; i < shape_size; ++i) {
      SetDim(i, value);
    }
  }

  RuntimeShape(int dimensions_count, const int32* dims_data) : size_(0) {
 80a3d50:	a80a      	add	r0, sp, #40	; 0x28
 80a3d52:	2300      	movs	r3, #0
 80a3d54:	f852 1b04 	ldr.w	r1, [r2], #4
 80a3d58:	f840 3d14 	str.w	r3, [r0, #-20]!
    ReplaceWith(dimensions_count, dims_data);
 80a3d5c:	f7ff ffb6 	bl	80a3ccc <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a3d60:	6864      	ldr	r4, [r4, #4]
 80a3d62:	9f00      	ldr	r7, [sp, #0]

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
 80a3d64:	9b05      	ldr	r3, [sp, #20]
 80a3d66:	429f      	cmp	r7, r3
 80a3d68:	d101      	bne.n	80a3d6e <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x5a>
 80a3d6a:	2600      	movs	r6, #0
 80a3d6c:	e00d      	b.n	80a3d8a <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x76>
 80a3d6e:	f00c f95f 	bl	80b0030 <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
 80a3d72:	4631      	mov	r1, r6
 80a3d74:	4668      	mov	r0, sp
 80a3d76:	f7fe fb27 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a3d7a:	4631      	mov	r1, r6
 80a3d7c:	4680      	mov	r8, r0
 80a3d7e:	a805      	add	r0, sp, #20
 80a3d80:	f7fe fb22 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a3d84:	4580      	cmp	r8, r0
 80a3d86:	d1f2      	bne.n	80a3d6e <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x5a>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
 80a3d88:	3601      	adds	r6, #1
 80a3d8a:	42b7      	cmp	r7, r6
 80a3d8c:	dcf1      	bgt.n	80a3d72 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x5e>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80a3d8e:	2f04      	cmp	r7, #4
 80a3d90:	bfcc      	ite	gt
 80a3d92:	9a01      	ldrgt	r2, [sp, #4]
 80a3d94:	aa01      	addle	r2, sp, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a3d96:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
 80a3d98:	f04f 0801 	mov.w	r8, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a3d9c:	429f      	cmp	r7, r3
 80a3d9e:	dc01      	bgt.n	80a3da4 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x90>
 80a3da0:	2600      	movs	r6, #0
 80a3da2:	e005      	b.n	80a3db0 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x9c>
      buffer_size *= dims_data[i];
 80a3da4:	f852 1023 	ldr.w	r1, [r2, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a3da8:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
 80a3daa:	fb01 f808 	mul.w	r8, r1, r8
 80a3dae:	e7f5      	b.n	80a3d9c <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x88>

inline void Ceil(const RuntimeShape& input_shape, const float* input_data,
                 const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; ++i) {
 80a3db0:	4546      	cmp	r6, r8
 80a3db2:	da07      	bge.n	80a3dc4 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0xb0>
  using ::ceil;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  ceil(float __x)
  { return __builtin_ceilf(__x); }
 80a3db4:	f855 0026 	ldr.w	r0, [r5, r6, lsl #2]
 80a3db8:	f00d f876 	bl	80b0ea8 <ceilf>
    output_data[i] = std::ceil(input_data[i]);
 80a3dbc:	f844 0026 	str.w	r0, [r4, r6, lsl #2]

inline void Ceil(const RuntimeShape& input_shape, const float* input_data,
                 const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; ++i) {
 80a3dc0:	3601      	adds	r6, #1
 80a3dc2:	e7f5      	b.n	80a3db0 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x9c>
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  reference_ops::Ceil(GetTensorShape(input), GetTensorData<float>(input),
                      GetTensorShape(output), GetTensorData<float>(output));
 80a3dc4:	a805      	add	r0, sp, #20
 80a3dc6:	f7fe faf4 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  reference_ops::Ceil(GetTensorShape(input), GetTensorData<float>(input),
 80a3dca:	4668      	mov	r0, sp
 80a3dcc:	f7fe faf1 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                      GetTensorShape(output), GetTensorData<float>(output));

  return kTfLiteOk;
}
 80a3dd0:	2000      	movs	r0, #0
 80a3dd2:	b00a      	add	sp, #40	; 0x28
 80a3dd4:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}

080a3dd8 <_ZN6tflite3ops5micro13Register_CEILEv>:

TfLiteRegistration* Register_CEIL() {
  static TfLiteRegistration r = {/*init=*/nullptr,
                                 /*free=*/nullptr, ceil::Prepare, ceil::Eval};
  return &r;
}
 80a3dd8:	4800      	ldr	r0, [pc, #0]	; (80a3ddc <_ZN6tflite3ops5micro13Register_CEILEv+0x4>)
 80a3dda:	4770      	bx	lr
 80a3ddc:	20000068 	.word	0x20000068

080a3de0 <_ZN6tflite3ops5micro14Register_EQUALEv>:

TfLiteRegistration* Register_EQUAL() {
  static TfLiteRegistration r = {nullptr, nullptr, nullptr,
                                 comparisons::EqualEval};
  return &r;
}
 80a3de0:	4800      	ldr	r0, [pc, #0]	; (80a3de4 <_ZN6tflite3ops5micro14Register_EQUALEv+0x4>)
 80a3de2:	4770      	bx	lr
 80a3de4:	200000a8 	.word	0x200000a8

080a3de8 <_ZN6tflite3ops5micro18Register_NOT_EQUALEv>:

TfLiteRegistration* Register_NOT_EQUAL() {
  static TfLiteRegistration r = {nullptr, nullptr, nullptr,
                                 comparisons::NotEqualEval};
  return &r;
}
 80a3de8:	4800      	ldr	r0, [pc, #0]	; (80a3dec <_ZN6tflite3ops5micro18Register_NOT_EQUALEv+0x4>)
 80a3dea:	4770      	bx	lr
 80a3dec:	200000e8 	.word	0x200000e8

080a3df0 <_ZN6tflite3ops5micro16Register_GREATEREv>:

TfLiteRegistration* Register_GREATER() {
  static TfLiteRegistration r = {nullptr, nullptr, nullptr,
                                 comparisons::GreaterEval};
  return &r;
}
 80a3df0:	4800      	ldr	r0, [pc, #0]	; (80a3df4 <_ZN6tflite3ops5micro16Register_GREATEREv+0x4>)
 80a3df2:	4770      	bx	lr
 80a3df4:	20000088 	.word	0x20000088

080a3df8 <_ZN6tflite3ops5micro22Register_GREATER_EQUALEv>:

TfLiteRegistration* Register_GREATER_EQUAL() {
  static TfLiteRegistration r = {nullptr, nullptr, nullptr,
                                 comparisons::GreaterEqualEval};
  return &r;
}
 80a3df8:	4800      	ldr	r0, [pc, #0]	; (80a3dfc <_ZN6tflite3ops5micro22Register_GREATER_EQUALEv+0x4>)
 80a3dfa:	4770      	bx	lr
 80a3dfc:	20000108 	.word	0x20000108

080a3e00 <_ZN6tflite3ops5micro13Register_LESSEv>:

TfLiteRegistration* Register_LESS() {
  static TfLiteRegistration r = {nullptr, nullptr, nullptr,
                                 comparisons::LessEval};
  return &r;
}
 80a3e00:	4800      	ldr	r0, [pc, #0]	; (80a3e04 <_ZN6tflite3ops5micro13Register_LESSEv+0x4>)
 80a3e02:	4770      	bx	lr
 80a3e04:	200000c8 	.word	0x200000c8

080a3e08 <_ZN6tflite3ops5micro19Register_LESS_EQUALEv>:

TfLiteRegistration* Register_LESS_EQUAL() {
  static TfLiteRegistration r = {nullptr, nullptr, nullptr,
                                 comparisons::LessEqualEval};
  return &r;
}
 80a3e08:	4800      	ldr	r0, [pc, #0]	; (80a3e0c <_ZN6tflite3ops5micro19Register_LESS_EQUALEv+0x4>)
 80a3e0a:	4770      	bx	lr
 80a3e0c:	20000128 	.word	0x20000128

080a3e10 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a3e10:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a3e14:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a3e16:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a3e18:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a3e1a:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a3e1c:	4691      	mov	r9, r2
 80a3e1e:	460c      	mov	r4, r1
 80a3e20:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a3e22:	dd01      	ble.n	80a3e28 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a3e24:	f00c f904 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a3e28:	682b      	ldr	r3, [r5, #0]
 80a3e2a:	2b04      	cmp	r3, #4
 80a3e2c:	dcfa      	bgt.n	80a3e24 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a3e2e:	6813      	ldr	r3, [r2, #0]
 80a3e30:	2b04      	cmp	r3, #4
 80a3e32:	dcf7      	bgt.n	80a3e24 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
 80a3e34:	2301      	movs	r3, #1
 80a3e36:	2104      	movs	r1, #4
 80a3e38:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a3e3a:	f10d 0820 	add.w	r8, sp, #32
 80a3e3e:	f7fe fafc 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a3e42:	4620      	mov	r0, r4
 80a3e44:	ab10      	add	r3, sp, #64	; 0x40
 80a3e46:	4642      	mov	r2, r8
 80a3e48:	4629      	mov	r1, r5
 80a3e4a:	f7fe fdf9 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a3e4e:	2400      	movs	r4, #0
 80a3e50:	2100      	movs	r1, #0
 80a3e52:	a803      	add	r0, sp, #12
 80a3e54:	f7fe fab8 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a3e58:	4284      	cmp	r4, r0
 80a3e5a:	da3d      	bge.n	80a3ed8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
 80a3e5c:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a3e5e:	2101      	movs	r1, #1
 80a3e60:	a803      	add	r0, sp, #12
 80a3e62:	f7fe fab1 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a3e66:	4285      	cmp	r5, r0
 80a3e68:	da34      	bge.n	80a3ed4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
 80a3e6a:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a3e6c:	2102      	movs	r1, #2
 80a3e6e:	a803      	add	r0, sp, #12
 80a3e70:	f7fe faaa 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a3e74:	4286      	cmp	r6, r0
 80a3e76:	da2b      	bge.n	80a3ed0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
 80a3e78:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a3e7a:	2103      	movs	r1, #3
 80a3e7c:	a803      	add	r0, sp, #12
 80a3e7e:	f7fe faa3 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a3e82:	4287      	cmp	r7, r0
 80a3e84:	da22      	bge.n	80a3ecc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xbc>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a3e86:	9700      	str	r7, [sp, #0]
 80a3e88:	4633      	mov	r3, r6
 80a3e8a:	462a      	mov	r2, r5
 80a3e8c:	4621      	mov	r1, r4
 80a3e8e:	a803      	add	r0, sp, #12
 80a3e90:	f7fe faff 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a3e94:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a3e96:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a3e98:	4633      	mov	r3, r6
 80a3e9a:	462a      	mov	r2, r5
 80a3e9c:	4621      	mov	r1, r4
 80a3e9e:	4640      	mov	r0, r8
 80a3ea0:	f7fe fba8 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a3ea4:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a3ea6:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a3ea8:	4633      	mov	r3, r6
 80a3eaa:	462a      	mov	r2, r5
 80a3eac:	4621      	mov	r1, r4
 80a3eae:	a810      	add	r0, sp, #64	; 0x40
 80a3eb0:	f7fe fba0 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a3eb4:	9a22      	ldr	r2, [sp, #136]	; 0x88
 80a3eb6:	f819 300b 	ldrb.w	r3, [r9, fp]
 80a3eba:	5c12      	ldrb	r2, [r2, r0]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a3ebc:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
 80a3ebe:	1a9a      	subs	r2, r3, r2
 80a3ec0:	4253      	negs	r3, r2
 80a3ec2:	4153      	adcs	r3, r2
 80a3ec4:	9a24      	ldr	r2, [sp, #144]	; 0x90
 80a3ec6:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a3eca:	e7d6      	b.n	80a3e7a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a3ecc:	3601      	adds	r6, #1
 80a3ece:	e7cd      	b.n	80a3e6c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a3ed0:	3501      	adds	r5, #1
 80a3ed2:	e7c4      	b.n	80a3e5e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a3ed4:	3401      	adds	r4, #1
 80a3ed6:	e7bb      	b.n	80a3e50 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a3ed8:	a803      	add	r0, sp, #12
 80a3eda:	f7fe fa6a 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a3ede:	b019      	add	sp, #100	; 0x64
 80a3ee0:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a3ee4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a3ee4:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a3ee8:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a3eea:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a3eec:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a3eee:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a3ef0:	4691      	mov	r9, r2
 80a3ef2:	460c      	mov	r4, r1
 80a3ef4:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a3ef6:	dd01      	ble.n	80a3efc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a3ef8:	f00c f89a 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a3efc:	682b      	ldr	r3, [r5, #0]
 80a3efe:	2b04      	cmp	r3, #4
 80a3f00:	dcfa      	bgt.n	80a3ef8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a3f02:	6813      	ldr	r3, [r2, #0]
 80a3f04:	2b04      	cmp	r3, #4
 80a3f06:	dcf7      	bgt.n	80a3ef8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
 80a3f08:	2301      	movs	r3, #1
 80a3f0a:	2104      	movs	r1, #4
 80a3f0c:	a803      	add	r0, sp, #12
 80a3f0e:	f7fe fa94 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a3f12:	4620      	mov	r0, r4
 80a3f14:	ab10      	add	r3, sp, #64	; 0x40
 80a3f16:	aa08      	add	r2, sp, #32
 80a3f18:	4629      	mov	r1, r5
 80a3f1a:	f7fe fd91 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a3f1e:	2400      	movs	r4, #0
 80a3f20:	2100      	movs	r1, #0
 80a3f22:	a803      	add	r0, sp, #12
 80a3f24:	f7fe fa50 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a3f28:	4284      	cmp	r4, r0
 80a3f2a:	da41      	bge.n	80a3fb0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
 80a3f2c:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a3f2e:	2101      	movs	r1, #1
 80a3f30:	a803      	add	r0, sp, #12
 80a3f32:	f7fe fa49 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a3f36:	4285      	cmp	r5, r0
 80a3f38:	da38      	bge.n	80a3fac <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
 80a3f3a:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a3f3c:	2102      	movs	r1, #2
 80a3f3e:	a803      	add	r0, sp, #12
 80a3f40:	f7fe fa42 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a3f44:	4286      	cmp	r6, r0
 80a3f46:	da2f      	bge.n	80a3fa8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
 80a3f48:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a3f4a:	2103      	movs	r1, #3
 80a3f4c:	a803      	add	r0, sp, #12
 80a3f4e:	f7fe fa3b 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a3f52:	4287      	cmp	r7, r0
 80a3f54:	da26      	bge.n	80a3fa4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a3f56:	4633      	mov	r3, r6
 80a3f58:	462a      	mov	r2, r5
 80a3f5a:	4621      	mov	r1, r4
 80a3f5c:	9700      	str	r7, [sp, #0]
 80a3f5e:	a803      	add	r0, sp, #12
 80a3f60:	f7fe fa97 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a3f64:	4633      	mov	r3, r6
 80a3f66:	462a      	mov	r2, r5
 80a3f68:	4621      	mov	r1, r4

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a3f6a:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a3f6c:	9700      	str	r7, [sp, #0]
 80a3f6e:	a808      	add	r0, sp, #32
 80a3f70:	f7fe fb40 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a3f74:	4633      	mov	r3, r6
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a3f76:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a3f78:	4621      	mov	r1, r4
 80a3f7a:	9700      	str	r7, [sp, #0]
 80a3f7c:	462a      	mov	r2, r5
 80a3f7e:	a810      	add	r0, sp, #64	; 0x40
 80a3f80:	f7fe fb38 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a3f84:	9b22      	ldr	r3, [sp, #136]	; 0x88
 80a3f86:	f04f 0801 	mov.w	r8, #1
 80a3f8a:	f853 1020 	ldr.w	r1, [r3, r0, lsl #2]
 80a3f8e:	f859 002b 	ldr.w	r0, [r9, fp, lsl #2]
 80a3f92:	f00f fcbf 	bl	80b3914 <__aeabi_fcmpeq>
 80a3f96:	b900      	cbnz	r0, 80a3f9a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xb6>
 80a3f98:	4680      	mov	r8, r0
 80a3f9a:	9b24      	ldr	r3, [sp, #144]	; 0x90
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a3f9c:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
 80a3f9e:	f803 800a 	strb.w	r8, [r3, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a3fa2:	e7d2      	b.n	80a3f4a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x66>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a3fa4:	3601      	adds	r6, #1
 80a3fa6:	e7c9      	b.n	80a3f3c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x58>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a3fa8:	3501      	adds	r5, #1
 80a3faa:	e7c0      	b.n	80a3f2e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4a>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a3fac:	3401      	adds	r4, #1
 80a3fae:	e7b7      	b.n	80a3f20 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x3c>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a3fb0:	a803      	add	r0, sp, #12
 80a3fb2:	f7fe f9fe 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a3fb6:	b019      	add	sp, #100	; 0x64
 80a3fb8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a3fbc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a3fbc:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a3fc0:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a3fc2:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a3fc4:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a3fc6:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a3fc8:	4691      	mov	r9, r2
 80a3fca:	460c      	mov	r4, r1
 80a3fcc:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a3fce:	dd01      	ble.n	80a3fd4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a3fd0:	f00c f82e 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a3fd4:	682b      	ldr	r3, [r5, #0]
 80a3fd6:	2b04      	cmp	r3, #4
 80a3fd8:	dcfa      	bgt.n	80a3fd0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a3fda:	6813      	ldr	r3, [r2, #0]
 80a3fdc:	2b04      	cmp	r3, #4
 80a3fde:	dcf7      	bgt.n	80a3fd0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
 80a3fe0:	2301      	movs	r3, #1
 80a3fe2:	2104      	movs	r1, #4
 80a3fe4:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a3fe6:	f10d 0820 	add.w	r8, sp, #32
 80a3fea:	f7fe fa26 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a3fee:	4620      	mov	r0, r4
 80a3ff0:	ab10      	add	r3, sp, #64	; 0x40
 80a3ff2:	4642      	mov	r2, r8
 80a3ff4:	4629      	mov	r1, r5
 80a3ff6:	f7fe fd23 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a3ffa:	2400      	movs	r4, #0
 80a3ffc:	2100      	movs	r1, #0
 80a3ffe:	a803      	add	r0, sp, #12
 80a4000:	f7fe f9e2 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4004:	4284      	cmp	r4, r0
 80a4006:	da3e      	bge.n	80a4086 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xca>
 80a4008:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a400a:	2101      	movs	r1, #1
 80a400c:	a803      	add	r0, sp, #12
 80a400e:	f7fe f9db 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4012:	4285      	cmp	r5, r0
 80a4014:	da35      	bge.n	80a4082 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc6>
 80a4016:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4018:	2102      	movs	r1, #2
 80a401a:	a803      	add	r0, sp, #12
 80a401c:	f7fe f9d4 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4020:	4286      	cmp	r6, r0
 80a4022:	da2c      	bge.n	80a407e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc2>
 80a4024:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4026:	2103      	movs	r1, #3
 80a4028:	a803      	add	r0, sp, #12
 80a402a:	f7fe f9cd 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a402e:	4287      	cmp	r7, r0
 80a4030:	da23      	bge.n	80a407a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xbe>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4032:	9700      	str	r7, [sp, #0]
 80a4034:	4633      	mov	r3, r6
 80a4036:	462a      	mov	r2, r5
 80a4038:	4621      	mov	r1, r4
 80a403a:	a803      	add	r0, sp, #12
 80a403c:	f7fe fa29 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4040:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4042:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4044:	4633      	mov	r3, r6
 80a4046:	462a      	mov	r2, r5
 80a4048:	4621      	mov	r1, r4
 80a404a:	4640      	mov	r0, r8
 80a404c:	f7fe fad2 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a4050:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4052:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a4054:	4633      	mov	r3, r6
 80a4056:	462a      	mov	r2, r5
 80a4058:	4621      	mov	r1, r4
 80a405a:	a810      	add	r0, sp, #64	; 0x40
 80a405c:	f7fe faca 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4060:	9a22      	ldr	r2, [sp, #136]	; 0x88
 80a4062:	f859 302b 	ldr.w	r3, [r9, fp, lsl #2]
 80a4066:	f852 2020 	ldr.w	r2, [r2, r0, lsl #2]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a406a:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
 80a406c:	1a9a      	subs	r2, r3, r2
 80a406e:	4253      	negs	r3, r2
 80a4070:	4153      	adcs	r3, r2
 80a4072:	9a24      	ldr	r2, [sp, #144]	; 0x90
 80a4074:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4078:	e7d5      	b.n	80a4026 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a407a:	3601      	adds	r6, #1
 80a407c:	e7cc      	b.n	80a4018 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a407e:	3501      	adds	r5, #1
 80a4080:	e7c3      	b.n	80a400a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4082:	3401      	adds	r4, #1
 80a4084:	e7ba      	b.n	80a3ffc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a4086:	a803      	add	r0, sp, #12
 80a4088:	f7fe f993 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a408c:	b019      	add	sp, #100	; 0x64
 80a408e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a4092 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4092:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a4096:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4098:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a409a:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a409c:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a409e:	4692      	mov	sl, r2
 80a40a0:	460c      	mov	r4, r1
 80a40a2:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a40a4:	dd01      	ble.n	80a40aa <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a40a6:	f00b ffc3 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a40aa:	682b      	ldr	r3, [r5, #0]
 80a40ac:	2b04      	cmp	r3, #4
 80a40ae:	dcfa      	bgt.n	80a40a6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a40b0:	6813      	ldr	r3, [r2, #0]
 80a40b2:	2b04      	cmp	r3, #4
 80a40b4:	dcf7      	bgt.n	80a40a6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
 80a40b6:	2301      	movs	r3, #1
 80a40b8:	2104      	movs	r1, #4
 80a40ba:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a40bc:	f10d 0820 	add.w	r8, sp, #32
 80a40c0:	f7fe f9bb 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a40c4:	4620      	mov	r0, r4
 80a40c6:	ab10      	add	r3, sp, #64	; 0x40
 80a40c8:	4642      	mov	r2, r8
 80a40ca:	4629      	mov	r1, r5
 80a40cc:	f7fe fcb8 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a40d0:	2400      	movs	r4, #0
 80a40d2:	2100      	movs	r1, #0
 80a40d4:	a803      	add	r0, sp, #12
 80a40d6:	f7fe f977 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a40da:	4284      	cmp	r4, r0
 80a40dc:	da45      	bge.n	80a416a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd8>
 80a40de:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a40e0:	2101      	movs	r1, #1
 80a40e2:	a803      	add	r0, sp, #12
 80a40e4:	f7fe f970 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a40e8:	4285      	cmp	r5, r0
 80a40ea:	da3c      	bge.n	80a4166 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd4>
 80a40ec:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a40ee:	2102      	movs	r1, #2
 80a40f0:	a803      	add	r0, sp, #12
 80a40f2:	f7fe f969 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a40f6:	4286      	cmp	r6, r0
 80a40f8:	da33      	bge.n	80a4162 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd0>
 80a40fa:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a40fc:	2103      	movs	r1, #3
 80a40fe:	a803      	add	r0, sp, #12
 80a4100:	f7fe f962 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4104:	4287      	cmp	r7, r0
 80a4106:	da2a      	bge.n	80a415e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4108:	9700      	str	r7, [sp, #0]
 80a410a:	4633      	mov	r3, r6
 80a410c:	462a      	mov	r2, r5
 80a410e:	4621      	mov	r1, r4
 80a4110:	a803      	add	r0, sp, #12
 80a4112:	f7fe f9be 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4116:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4118:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a411a:	4633      	mov	r3, r6
 80a411c:	462a      	mov	r2, r5
 80a411e:	4621      	mov	r1, r4
 80a4120:	4640      	mov	r0, r8
 80a4122:	f7fe fa67 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a4126:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4128:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a412a:	4633      	mov	r3, r6
 80a412c:	462a      	mov	r2, r5
 80a412e:	4621      	mov	r1, r4
 80a4130:	a810      	add	r0, sp, #64	; 0x40
 80a4132:	f7fe fa5f 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4136:	9b22      	ldr	r3, [sp, #136]	; 0x88
 80a4138:	eb0a 09c9 	add.w	r9, sl, r9, lsl #3
 80a413c:	eb03 03c0 	add.w	r3, r3, r0, lsl #3

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4140:	e9d3 2300 	ldrd	r2, r3, [r3]
 80a4144:	e9d9 0100 	ldrd	r0, r1, [r9]
 80a4148:	4299      	cmp	r1, r3
 80a414a:	bf08      	it	eq
 80a414c:	4290      	cmpeq	r0, r2
 80a414e:	9a24      	ldr	r2, [sp, #144]	; 0x90
 80a4150:	bf0c      	ite	eq
 80a4152:	2301      	moveq	r3, #1
 80a4154:	2300      	movne	r3, #0
 80a4156:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a415a:	3701      	adds	r7, #1
 80a415c:	e7ce      	b.n	80a40fc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a415e:	3601      	adds	r6, #1
 80a4160:	e7c5      	b.n	80a40ee <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4162:	3501      	adds	r5, #1
 80a4164:	e7bc      	b.n	80a40e0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4166:	3401      	adds	r4, #1
 80a4168:	e7b3      	b.n	80a40d2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a416a:	a803      	add	r0, sp, #12
 80a416c:	f7fe f921 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a4170:	b019      	add	sp, #100	; 0x64
 80a4172:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a4176 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4176:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a417a:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a417c:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a417e:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4180:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4182:	4691      	mov	r9, r2
 80a4184:	460c      	mov	r4, r1
 80a4186:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4188:	dd01      	ble.n	80a418e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a418a:	f00b ff51 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a418e:	682b      	ldr	r3, [r5, #0]
 80a4190:	2b04      	cmp	r3, #4
 80a4192:	dcfa      	bgt.n	80a418a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a4194:	6813      	ldr	r3, [r2, #0]
 80a4196:	2b04      	cmp	r3, #4
 80a4198:	dcf7      	bgt.n	80a418a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
 80a419a:	2301      	movs	r3, #1
 80a419c:	2104      	movs	r1, #4
 80a419e:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a41a0:	f10d 0820 	add.w	r8, sp, #32
 80a41a4:	f7fe f949 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a41a8:	4620      	mov	r0, r4
 80a41aa:	ab10      	add	r3, sp, #64	; 0x40
 80a41ac:	4642      	mov	r2, r8
 80a41ae:	4629      	mov	r1, r5
 80a41b0:	f7fe fc46 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a41b4:	2400      	movs	r4, #0
 80a41b6:	2100      	movs	r1, #0
 80a41b8:	a803      	add	r0, sp, #12
 80a41ba:	f7fe f905 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a41be:	4284      	cmp	r4, r0
 80a41c0:	da3b      	bge.n	80a423a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
 80a41c2:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a41c4:	2101      	movs	r1, #1
 80a41c6:	a803      	add	r0, sp, #12
 80a41c8:	f7fe f8fe 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a41cc:	4285      	cmp	r5, r0
 80a41ce:	da32      	bge.n	80a4236 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
 80a41d0:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a41d2:	2102      	movs	r1, #2
 80a41d4:	a803      	add	r0, sp, #12
 80a41d6:	f7fe f8f7 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a41da:	4286      	cmp	r6, r0
 80a41dc:	da29      	bge.n	80a4232 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xbc>
 80a41de:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a41e0:	2103      	movs	r1, #3
 80a41e2:	a803      	add	r0, sp, #12
 80a41e4:	f7fe f8f0 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a41e8:	4287      	cmp	r7, r0
 80a41ea:	da20      	bge.n	80a422e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xb8>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a41ec:	9700      	str	r7, [sp, #0]
 80a41ee:	4633      	mov	r3, r6
 80a41f0:	462a      	mov	r2, r5
 80a41f2:	4621      	mov	r1, r4
 80a41f4:	a803      	add	r0, sp, #12
 80a41f6:	f7fe f94c 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a41fa:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a41fc:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a41fe:	4633      	mov	r3, r6
 80a4200:	462a      	mov	r2, r5
 80a4202:	4621      	mov	r1, r4
 80a4204:	4640      	mov	r0, r8
 80a4206:	f7fe f9f5 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a420a:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a420c:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a420e:	4633      	mov	r3, r6
 80a4210:	462a      	mov	r2, r5
 80a4212:	4621      	mov	r1, r4
 80a4214:	a810      	add	r0, sp, #64	; 0x40
 80a4216:	f7fe f9ed 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a421a:	9b22      	ldr	r3, [sp, #136]	; 0x88
 80a421c:	f819 200b 	ldrb.w	r2, [r9, fp]
 80a4220:	5c1b      	ldrb	r3, [r3, r0]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4222:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4224:	4053      	eors	r3, r2
 80a4226:	9a24      	ldr	r2, [sp, #144]	; 0x90
 80a4228:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a422c:	e7d8      	b.n	80a41e0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a422e:	3601      	adds	r6, #1
 80a4230:	e7cf      	b.n	80a41d2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4232:	3501      	adds	r5, #1
 80a4234:	e7c6      	b.n	80a41c4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4236:	3401      	adds	r4, #1
 80a4238:	e7bd      	b.n	80a41b6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a423a:	a803      	add	r0, sp, #12
 80a423c:	f7fe f8b9 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a4240:	b019      	add	sp, #100	; 0x64
 80a4242:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a4246 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4246:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a424a:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a424c:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a424e:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4250:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4252:	4691      	mov	r9, r2
 80a4254:	460c      	mov	r4, r1
 80a4256:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4258:	dd01      	ble.n	80a425e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a425a:	f00b fee9 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a425e:	682b      	ldr	r3, [r5, #0]
 80a4260:	2b04      	cmp	r3, #4
 80a4262:	dcfa      	bgt.n	80a425a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a4264:	6813      	ldr	r3, [r2, #0]
 80a4266:	2b04      	cmp	r3, #4
 80a4268:	dcf7      	bgt.n	80a425a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
 80a426a:	2301      	movs	r3, #1
 80a426c:	2104      	movs	r1, #4
 80a426e:	a803      	add	r0, sp, #12
 80a4270:	f7fe f8e3 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a4274:	4620      	mov	r0, r4
 80a4276:	ab10      	add	r3, sp, #64	; 0x40
 80a4278:	aa08      	add	r2, sp, #32
 80a427a:	4629      	mov	r1, r5
 80a427c:	f7fe fbe0 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4280:	2400      	movs	r4, #0
 80a4282:	2100      	movs	r1, #0
 80a4284:	a803      	add	r0, sp, #12
 80a4286:	f7fe f89f 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a428a:	4284      	cmp	r4, r0
 80a428c:	da42      	bge.n	80a4314 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xce>
 80a428e:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4290:	2101      	movs	r1, #1
 80a4292:	a803      	add	r0, sp, #12
 80a4294:	f7fe f898 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4298:	4285      	cmp	r5, r0
 80a429a:	da39      	bge.n	80a4310 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xca>
 80a429c:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a429e:	2102      	movs	r1, #2
 80a42a0:	a803      	add	r0, sp, #12
 80a42a2:	f7fe f891 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a42a6:	4286      	cmp	r6, r0
 80a42a8:	da30      	bge.n	80a430c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc6>
 80a42aa:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a42ac:	2103      	movs	r1, #3
 80a42ae:	a803      	add	r0, sp, #12
 80a42b0:	f7fe f88a 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a42b4:	4287      	cmp	r7, r0
 80a42b6:	da27      	bge.n	80a4308 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc2>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a42b8:	4633      	mov	r3, r6
 80a42ba:	462a      	mov	r2, r5
 80a42bc:	4621      	mov	r1, r4
 80a42be:	9700      	str	r7, [sp, #0]
 80a42c0:	a803      	add	r0, sp, #12
 80a42c2:	f7fe f8e6 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a42c6:	4633      	mov	r3, r6
 80a42c8:	462a      	mov	r2, r5
 80a42ca:	4621      	mov	r1, r4

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a42cc:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a42ce:	9700      	str	r7, [sp, #0]
 80a42d0:	a808      	add	r0, sp, #32
 80a42d2:	f7fe f98f 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a42d6:	4633      	mov	r3, r6
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a42d8:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a42da:	4621      	mov	r1, r4
 80a42dc:	9700      	str	r7, [sp, #0]
 80a42de:	462a      	mov	r2, r5
 80a42e0:	a810      	add	r0, sp, #64	; 0x40
 80a42e2:	f7fe f987 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a42e6:	9b22      	ldr	r3, [sp, #136]	; 0x88
 80a42e8:	f04f 0801 	mov.w	r8, #1
 80a42ec:	f853 1020 	ldr.w	r1, [r3, r0, lsl #2]
 80a42f0:	f859 002b 	ldr.w	r0, [r9, fp, lsl #2]
 80a42f4:	f00f fb0e 	bl	80b3914 <__aeabi_fcmpeq>
 80a42f8:	b108      	cbz	r0, 80a42fe <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xb8>
 80a42fa:	f04f 0800 	mov.w	r8, #0
 80a42fe:	9b24      	ldr	r3, [sp, #144]	; 0x90
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4300:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4302:	f803 800a 	strb.w	r8, [r3, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4306:	e7d1      	b.n	80a42ac <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x66>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4308:	3601      	adds	r6, #1
 80a430a:	e7c8      	b.n	80a429e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x58>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a430c:	3501      	adds	r5, #1
 80a430e:	e7bf      	b.n	80a4290 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4a>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4310:	3401      	adds	r4, #1
 80a4312:	e7b6      	b.n	80a4282 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x3c>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a4314:	a803      	add	r0, sp, #12
 80a4316:	f7fe f84c 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a431a:	b019      	add	sp, #100	; 0x64
 80a431c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a4320 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4320:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a4324:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4326:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4328:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a432a:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a432c:	4691      	mov	r9, r2
 80a432e:	460c      	mov	r4, r1
 80a4330:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4332:	dd01      	ble.n	80a4338 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a4334:	f00b fe7c 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a4338:	682b      	ldr	r3, [r5, #0]
 80a433a:	2b04      	cmp	r3, #4
 80a433c:	dcfa      	bgt.n	80a4334 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a433e:	6813      	ldr	r3, [r2, #0]
 80a4340:	2b04      	cmp	r3, #4
 80a4342:	dcf7      	bgt.n	80a4334 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
 80a4344:	2301      	movs	r3, #1
 80a4346:	2104      	movs	r1, #4
 80a4348:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a434a:	f10d 0820 	add.w	r8, sp, #32
 80a434e:	f7fe f874 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a4352:	4620      	mov	r0, r4
 80a4354:	ab10      	add	r3, sp, #64	; 0x40
 80a4356:	4642      	mov	r2, r8
 80a4358:	4629      	mov	r1, r5
 80a435a:	f7fe fb71 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a435e:	2400      	movs	r4, #0
 80a4360:	2100      	movs	r1, #0
 80a4362:	a803      	add	r0, sp, #12
 80a4364:	f7fe f830 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4368:	4284      	cmp	r4, r0
 80a436a:	da3e      	bge.n	80a43ea <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xca>
 80a436c:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a436e:	2101      	movs	r1, #1
 80a4370:	a803      	add	r0, sp, #12
 80a4372:	f7fe f829 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4376:	4285      	cmp	r5, r0
 80a4378:	da35      	bge.n	80a43e6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc6>
 80a437a:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a437c:	2102      	movs	r1, #2
 80a437e:	a803      	add	r0, sp, #12
 80a4380:	f7fe f822 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4384:	4286      	cmp	r6, r0
 80a4386:	da2c      	bge.n	80a43e2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc2>
 80a4388:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a438a:	2103      	movs	r1, #3
 80a438c:	a803      	add	r0, sp, #12
 80a438e:	f7fe f81b 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4392:	4287      	cmp	r7, r0
 80a4394:	da23      	bge.n	80a43de <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xbe>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4396:	9700      	str	r7, [sp, #0]
 80a4398:	4633      	mov	r3, r6
 80a439a:	462a      	mov	r2, r5
 80a439c:	4621      	mov	r1, r4
 80a439e:	a803      	add	r0, sp, #12
 80a43a0:	f7fe f877 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a43a4:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a43a6:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a43a8:	4633      	mov	r3, r6
 80a43aa:	462a      	mov	r2, r5
 80a43ac:	4621      	mov	r1, r4
 80a43ae:	4640      	mov	r0, r8
 80a43b0:	f7fe f920 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a43b4:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a43b6:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a43b8:	4633      	mov	r3, r6
 80a43ba:	462a      	mov	r2, r5
 80a43bc:	4621      	mov	r1, r4
 80a43be:	a810      	add	r0, sp, #64	; 0x40
 80a43c0:	f7fe f918 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a43c4:	9a22      	ldr	r2, [sp, #136]	; 0x88
 80a43c6:	f859 302b 	ldr.w	r3, [r9, fp, lsl #2]
 80a43ca:	f852 2020 	ldr.w	r2, [r2, r0, lsl #2]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a43ce:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
 80a43d0:	1a9b      	subs	r3, r3, r2
 80a43d2:	9a24      	ldr	r2, [sp, #144]	; 0x90
 80a43d4:	bf18      	it	ne
 80a43d6:	2301      	movne	r3, #1
 80a43d8:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a43dc:	e7d5      	b.n	80a438a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a43de:	3601      	adds	r6, #1
 80a43e0:	e7cc      	b.n	80a437c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a43e2:	3501      	adds	r5, #1
 80a43e4:	e7c3      	b.n	80a436e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a43e6:	3401      	adds	r4, #1
 80a43e8:	e7ba      	b.n	80a4360 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a43ea:	a803      	add	r0, sp, #12
 80a43ec:	f7fd ffe1 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a43f0:	b019      	add	sp, #100	; 0x64
 80a43f2:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a43f6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a43f6:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a43fa:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a43fc:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a43fe:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4400:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4402:	4692      	mov	sl, r2
 80a4404:	460c      	mov	r4, r1
 80a4406:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4408:	dd01      	ble.n	80a440e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a440a:	f00b fe11 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a440e:	682b      	ldr	r3, [r5, #0]
 80a4410:	2b04      	cmp	r3, #4
 80a4412:	dcfa      	bgt.n	80a440a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a4414:	6813      	ldr	r3, [r2, #0]
 80a4416:	2b04      	cmp	r3, #4
 80a4418:	dcf7      	bgt.n	80a440a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
 80a441a:	2301      	movs	r3, #1
 80a441c:	2104      	movs	r1, #4
 80a441e:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a4420:	f10d 0820 	add.w	r8, sp, #32
 80a4424:	f7fe f809 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a4428:	4620      	mov	r0, r4
 80a442a:	ab10      	add	r3, sp, #64	; 0x40
 80a442c:	4642      	mov	r2, r8
 80a442e:	4629      	mov	r1, r5
 80a4430:	f7fe fb06 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4434:	2400      	movs	r4, #0
 80a4436:	2100      	movs	r1, #0
 80a4438:	a803      	add	r0, sp, #12
 80a443a:	f7fd ffc5 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a443e:	4284      	cmp	r4, r0
 80a4440:	da45      	bge.n	80a44ce <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd8>
 80a4442:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4444:	2101      	movs	r1, #1
 80a4446:	a803      	add	r0, sp, #12
 80a4448:	f7fd ffbe 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a444c:	4285      	cmp	r5, r0
 80a444e:	da3c      	bge.n	80a44ca <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd4>
 80a4450:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4452:	2102      	movs	r1, #2
 80a4454:	a803      	add	r0, sp, #12
 80a4456:	f7fd ffb7 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a445a:	4286      	cmp	r6, r0
 80a445c:	da33      	bge.n	80a44c6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd0>
 80a445e:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4460:	2103      	movs	r1, #3
 80a4462:	a803      	add	r0, sp, #12
 80a4464:	f7fd ffb0 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4468:	4287      	cmp	r7, r0
 80a446a:	da2a      	bge.n	80a44c2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a446c:	9700      	str	r7, [sp, #0]
 80a446e:	4633      	mov	r3, r6
 80a4470:	462a      	mov	r2, r5
 80a4472:	4621      	mov	r1, r4
 80a4474:	a803      	add	r0, sp, #12
 80a4476:	f7fe f80c 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a447a:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a447c:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a447e:	4633      	mov	r3, r6
 80a4480:	462a      	mov	r2, r5
 80a4482:	4621      	mov	r1, r4
 80a4484:	4640      	mov	r0, r8
 80a4486:	f7fe f8b5 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a448a:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a448c:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a448e:	4633      	mov	r3, r6
 80a4490:	462a      	mov	r2, r5
 80a4492:	4621      	mov	r1, r4
 80a4494:	a810      	add	r0, sp, #64	; 0x40
 80a4496:	f7fe f8ad 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a449a:	9b22      	ldr	r3, [sp, #136]	; 0x88
 80a449c:	eb0a 09c9 	add.w	r9, sl, r9, lsl #3
 80a44a0:	eb03 03c0 	add.w	r3, r3, r0, lsl #3

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a44a4:	e9d3 2300 	ldrd	r2, r3, [r3]
 80a44a8:	e9d9 0100 	ldrd	r0, r1, [r9]
 80a44ac:	4299      	cmp	r1, r3
 80a44ae:	bf08      	it	eq
 80a44b0:	4290      	cmpeq	r0, r2
 80a44b2:	9a24      	ldr	r2, [sp, #144]	; 0x90
 80a44b4:	bf14      	ite	ne
 80a44b6:	2301      	movne	r3, #1
 80a44b8:	2300      	moveq	r3, #0
 80a44ba:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a44be:	3701      	adds	r7, #1
 80a44c0:	e7ce      	b.n	80a4460 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a44c2:	3601      	adds	r6, #1
 80a44c4:	e7c5      	b.n	80a4452 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a44c6:	3501      	adds	r5, #1
 80a44c8:	e7bc      	b.n	80a4444 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a44ca:	3401      	adds	r4, #1
 80a44cc:	e7b3      	b.n	80a4436 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a44ce:	a803      	add	r0, sp, #12
 80a44d0:	f7fd ff6f 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a44d4:	b019      	add	sp, #100	; 0x64
 80a44d6:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a44da <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a44da:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a44de:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a44e0:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a44e2:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a44e4:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a44e6:	4691      	mov	r9, r2
 80a44e8:	460c      	mov	r4, r1
 80a44ea:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a44ec:	dd01      	ble.n	80a44f2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a44ee:	f00b fd9f 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a44f2:	682b      	ldr	r3, [r5, #0]
 80a44f4:	2b04      	cmp	r3, #4
 80a44f6:	dcfa      	bgt.n	80a44ee <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a44f8:	6813      	ldr	r3, [r2, #0]
 80a44fa:	2b04      	cmp	r3, #4
 80a44fc:	dcf7      	bgt.n	80a44ee <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
 80a44fe:	2301      	movs	r3, #1
 80a4500:	2104      	movs	r1, #4
 80a4502:	a803      	add	r0, sp, #12
 80a4504:	f7fd ff99 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a4508:	4620      	mov	r0, r4
 80a450a:	ab10      	add	r3, sp, #64	; 0x40
 80a450c:	aa08      	add	r2, sp, #32
 80a450e:	4629      	mov	r1, r5
 80a4510:	f7fe fa96 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4514:	2400      	movs	r4, #0
 80a4516:	2100      	movs	r1, #0
 80a4518:	a803      	add	r0, sp, #12
 80a451a:	f7fd ff55 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a451e:	4284      	cmp	r4, r0
 80a4520:	da41      	bge.n	80a45a6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
 80a4522:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4524:	2101      	movs	r1, #1
 80a4526:	a803      	add	r0, sp, #12
 80a4528:	f7fd ff4e 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a452c:	4285      	cmp	r5, r0
 80a452e:	da38      	bge.n	80a45a2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
 80a4530:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4532:	2102      	movs	r1, #2
 80a4534:	a803      	add	r0, sp, #12
 80a4536:	f7fd ff47 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a453a:	4286      	cmp	r6, r0
 80a453c:	da2f      	bge.n	80a459e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
 80a453e:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4540:	2103      	movs	r1, #3
 80a4542:	a803      	add	r0, sp, #12
 80a4544:	f7fd ff40 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4548:	4287      	cmp	r7, r0
 80a454a:	da26      	bge.n	80a459a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a454c:	4633      	mov	r3, r6
 80a454e:	462a      	mov	r2, r5
 80a4550:	4621      	mov	r1, r4
 80a4552:	9700      	str	r7, [sp, #0]
 80a4554:	a803      	add	r0, sp, #12
 80a4556:	f7fd ff9c 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a455a:	4633      	mov	r3, r6
 80a455c:	462a      	mov	r2, r5
 80a455e:	4621      	mov	r1, r4

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4560:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4562:	9700      	str	r7, [sp, #0]
 80a4564:	a808      	add	r0, sp, #32
 80a4566:	f7fe f845 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a456a:	4633      	mov	r3, r6
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a456c:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a456e:	4621      	mov	r1, r4
 80a4570:	9700      	str	r7, [sp, #0]
 80a4572:	462a      	mov	r2, r5
 80a4574:	a810      	add	r0, sp, #64	; 0x40
 80a4576:	f7fe f83d 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a457a:	9b22      	ldr	r3, [sp, #136]	; 0x88
 80a457c:	f04f 0801 	mov.w	r8, #1
 80a4580:	f853 1020 	ldr.w	r1, [r3, r0, lsl #2]
 80a4584:	f859 002b 	ldr.w	r0, [r9, fp, lsl #2]
 80a4588:	f00f f9ec 	bl	80b3964 <__aeabi_fcmpgt>
 80a458c:	b900      	cbnz	r0, 80a4590 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xb6>
 80a458e:	4680      	mov	r8, r0
 80a4590:	9b24      	ldr	r3, [sp, #144]	; 0x90
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4592:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4594:	f803 800a 	strb.w	r8, [r3, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4598:	e7d2      	b.n	80a4540 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x66>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a459a:	3601      	adds	r6, #1
 80a459c:	e7c9      	b.n	80a4532 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x58>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a459e:	3501      	adds	r5, #1
 80a45a0:	e7c0      	b.n	80a4524 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4a>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a45a2:	3401      	adds	r4, #1
 80a45a4:	e7b7      	b.n	80a4516 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x3c>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a45a6:	a803      	add	r0, sp, #12
 80a45a8:	f7fd ff03 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a45ac:	b019      	add	sp, #100	; 0x64
 80a45ae:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a45b2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a45b2:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a45b6:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a45b8:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a45ba:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a45bc:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a45be:	4691      	mov	r9, r2
 80a45c0:	460c      	mov	r4, r1
 80a45c2:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a45c4:	dd01      	ble.n	80a45ca <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a45c6:	f00b fd33 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a45ca:	682b      	ldr	r3, [r5, #0]
 80a45cc:	2b04      	cmp	r3, #4
 80a45ce:	dcfa      	bgt.n	80a45c6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a45d0:	6813      	ldr	r3, [r2, #0]
 80a45d2:	2b04      	cmp	r3, #4
 80a45d4:	dcf7      	bgt.n	80a45c6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
 80a45d6:	2301      	movs	r3, #1
 80a45d8:	2104      	movs	r1, #4
 80a45da:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a45dc:	f10d 0820 	add.w	r8, sp, #32
 80a45e0:	f7fd ff2b 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a45e4:	4620      	mov	r0, r4
 80a45e6:	ab10      	add	r3, sp, #64	; 0x40
 80a45e8:	4642      	mov	r2, r8
 80a45ea:	4629      	mov	r1, r5
 80a45ec:	f7fe fa28 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a45f0:	2400      	movs	r4, #0
 80a45f2:	2100      	movs	r1, #0
 80a45f4:	a803      	add	r0, sp, #12
 80a45f6:	f7fd fee7 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a45fa:	4284      	cmp	r4, r0
 80a45fc:	da3f      	bge.n	80a467e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
 80a45fe:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4600:	2101      	movs	r1, #1
 80a4602:	a803      	add	r0, sp, #12
 80a4604:	f7fd fee0 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4608:	4285      	cmp	r5, r0
 80a460a:	da36      	bge.n	80a467a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
 80a460c:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a460e:	2102      	movs	r1, #2
 80a4610:	a803      	add	r0, sp, #12
 80a4612:	f7fd fed9 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4616:	4286      	cmp	r6, r0
 80a4618:	da2d      	bge.n	80a4676 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
 80a461a:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a461c:	2103      	movs	r1, #3
 80a461e:	a803      	add	r0, sp, #12
 80a4620:	f7fd fed2 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4624:	4287      	cmp	r7, r0
 80a4626:	da24      	bge.n	80a4672 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4628:	9700      	str	r7, [sp, #0]
 80a462a:	4633      	mov	r3, r6
 80a462c:	462a      	mov	r2, r5
 80a462e:	4621      	mov	r1, r4
 80a4630:	a803      	add	r0, sp, #12
 80a4632:	f7fd ff2e 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4636:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4638:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a463a:	4633      	mov	r3, r6
 80a463c:	462a      	mov	r2, r5
 80a463e:	4621      	mov	r1, r4
 80a4640:	4640      	mov	r0, r8
 80a4642:	f7fd ffd7 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a4646:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4648:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a464a:	4633      	mov	r3, r6
 80a464c:	462a      	mov	r2, r5
 80a464e:	4621      	mov	r1, r4
 80a4650:	a810      	add	r0, sp, #64	; 0x40
 80a4652:	f7fd ffcf 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4656:	9a22      	ldr	r2, [sp, #136]	; 0x88
 80a4658:	f859 302b 	ldr.w	r3, [r9, fp, lsl #2]
 80a465c:	f852 2020 	ldr.w	r2, [r2, r0, lsl #2]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4660:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4662:	4293      	cmp	r3, r2
 80a4664:	9a24      	ldr	r2, [sp, #144]	; 0x90
 80a4666:	bfd4      	ite	le
 80a4668:	2300      	movle	r3, #0
 80a466a:	2301      	movgt	r3, #1
 80a466c:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4670:	e7d4      	b.n	80a461c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4672:	3601      	adds	r6, #1
 80a4674:	e7cb      	b.n	80a460e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4676:	3501      	adds	r5, #1
 80a4678:	e7c2      	b.n	80a4600 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a467a:	3401      	adds	r4, #1
 80a467c:	e7b9      	b.n	80a45f2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a467e:	a803      	add	r0, sp, #12
 80a4680:	f7fd fe97 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a4684:	b019      	add	sp, #100	; 0x64
 80a4686:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a468a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a468a:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a468e:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4690:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4692:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4694:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4696:	4692      	mov	sl, r2
 80a4698:	460c      	mov	r4, r1
 80a469a:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a469c:	dd01      	ble.n	80a46a2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a469e:	f00b fcc7 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a46a2:	682b      	ldr	r3, [r5, #0]
 80a46a4:	2b04      	cmp	r3, #4
 80a46a6:	dcfa      	bgt.n	80a469e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a46a8:	6813      	ldr	r3, [r2, #0]
 80a46aa:	2b04      	cmp	r3, #4
 80a46ac:	dcf7      	bgt.n	80a469e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
 80a46ae:	2301      	movs	r3, #1
 80a46b0:	2104      	movs	r1, #4
 80a46b2:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a46b4:	f10d 0820 	add.w	r8, sp, #32
 80a46b8:	f7fd febf 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a46bc:	4620      	mov	r0, r4
 80a46be:	ab10      	add	r3, sp, #64	; 0x40
 80a46c0:	4642      	mov	r2, r8
 80a46c2:	4629      	mov	r1, r5
 80a46c4:	f7fe f9bc 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a46c8:	2400      	movs	r4, #0
 80a46ca:	2100      	movs	r1, #0
 80a46cc:	a803      	add	r0, sp, #12
 80a46ce:	f7fd fe7b 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a46d2:	4284      	cmp	r4, r0
 80a46d4:	da44      	bge.n	80a4760 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd6>
 80a46d6:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a46d8:	2101      	movs	r1, #1
 80a46da:	a803      	add	r0, sp, #12
 80a46dc:	f7fd fe74 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a46e0:	4285      	cmp	r5, r0
 80a46e2:	da3b      	bge.n	80a475c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd2>
 80a46e4:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a46e6:	2102      	movs	r1, #2
 80a46e8:	a803      	add	r0, sp, #12
 80a46ea:	f7fd fe6d 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a46ee:	4286      	cmp	r6, r0
 80a46f0:	da32      	bge.n	80a4758 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xce>
 80a46f2:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a46f4:	2103      	movs	r1, #3
 80a46f6:	a803      	add	r0, sp, #12
 80a46f8:	f7fd fe66 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a46fc:	4287      	cmp	r7, r0
 80a46fe:	da29      	bge.n	80a4754 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xca>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4700:	9700      	str	r7, [sp, #0]
 80a4702:	4633      	mov	r3, r6
 80a4704:	462a      	mov	r2, r5
 80a4706:	4621      	mov	r1, r4
 80a4708:	a803      	add	r0, sp, #12
 80a470a:	f7fd fec2 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a470e:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4710:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4712:	4633      	mov	r3, r6
 80a4714:	462a      	mov	r2, r5
 80a4716:	4621      	mov	r1, r4
 80a4718:	4640      	mov	r0, r8
 80a471a:	f7fd ff6b 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a471e:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4720:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a4722:	4633      	mov	r3, r6
 80a4724:	462a      	mov	r2, r5
 80a4726:	4621      	mov	r1, r4
 80a4728:	a810      	add	r0, sp, #64	; 0x40
 80a472a:	f7fd ff63 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a472e:	9b22      	ldr	r3, [sp, #136]	; 0x88
 80a4730:	eb0a 09c9 	add.w	r9, sl, r9, lsl #3
 80a4734:	eb03 00c0 	add.w	r0, r3, r0, lsl #3

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4738:	c803      	ldmia	r0, {r0, r1}
 80a473a:	e9d9 2300 	ldrd	r2, r3, [r9]
 80a473e:	4290      	cmp	r0, r2
 80a4740:	eb71 0303 	sbcs.w	r3, r1, r3
 80a4744:	9a24      	ldr	r2, [sp, #144]	; 0x90
 80a4746:	bfb4      	ite	lt
 80a4748:	2301      	movlt	r3, #1
 80a474a:	2300      	movge	r3, #0
 80a474c:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4750:	3701      	adds	r7, #1
 80a4752:	e7cf      	b.n	80a46f4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4754:	3601      	adds	r6, #1
 80a4756:	e7c6      	b.n	80a46e6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4758:	3501      	adds	r5, #1
 80a475a:	e7bd      	b.n	80a46d8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a475c:	3401      	adds	r4, #1
 80a475e:	e7b4      	b.n	80a46ca <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a4760:	a803      	add	r0, sp, #12
 80a4762:	f7fd fe26 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a4766:	b019      	add	sp, #100	; 0x64
 80a4768:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a476c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a476c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a4770:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4772:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4774:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4776:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4778:	4691      	mov	r9, r2
 80a477a:	460c      	mov	r4, r1
 80a477c:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a477e:	dd01      	ble.n	80a4784 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a4780:	f00b fc56 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a4784:	682b      	ldr	r3, [r5, #0]
 80a4786:	2b04      	cmp	r3, #4
 80a4788:	dcfa      	bgt.n	80a4780 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a478a:	6813      	ldr	r3, [r2, #0]
 80a478c:	2b04      	cmp	r3, #4
 80a478e:	dcf7      	bgt.n	80a4780 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
 80a4790:	2301      	movs	r3, #1
 80a4792:	2104      	movs	r1, #4
 80a4794:	a803      	add	r0, sp, #12
 80a4796:	f7fd fe50 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a479a:	4620      	mov	r0, r4
 80a479c:	ab10      	add	r3, sp, #64	; 0x40
 80a479e:	aa08      	add	r2, sp, #32
 80a47a0:	4629      	mov	r1, r5
 80a47a2:	f7fe f94d 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a47a6:	2400      	movs	r4, #0
 80a47a8:	2100      	movs	r1, #0
 80a47aa:	a803      	add	r0, sp, #12
 80a47ac:	f7fd fe0c 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a47b0:	4284      	cmp	r4, r0
 80a47b2:	da41      	bge.n	80a4838 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
 80a47b4:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a47b6:	2101      	movs	r1, #1
 80a47b8:	a803      	add	r0, sp, #12
 80a47ba:	f7fd fe05 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a47be:	4285      	cmp	r5, r0
 80a47c0:	da38      	bge.n	80a4834 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
 80a47c2:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a47c4:	2102      	movs	r1, #2
 80a47c6:	a803      	add	r0, sp, #12
 80a47c8:	f7fd fdfe 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a47cc:	4286      	cmp	r6, r0
 80a47ce:	da2f      	bge.n	80a4830 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
 80a47d0:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a47d2:	2103      	movs	r1, #3
 80a47d4:	a803      	add	r0, sp, #12
 80a47d6:	f7fd fdf7 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a47da:	4287      	cmp	r7, r0
 80a47dc:	da26      	bge.n	80a482c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a47de:	4633      	mov	r3, r6
 80a47e0:	462a      	mov	r2, r5
 80a47e2:	4621      	mov	r1, r4
 80a47e4:	9700      	str	r7, [sp, #0]
 80a47e6:	a803      	add	r0, sp, #12
 80a47e8:	f7fd fe53 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a47ec:	4633      	mov	r3, r6
 80a47ee:	462a      	mov	r2, r5
 80a47f0:	4621      	mov	r1, r4

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a47f2:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a47f4:	9700      	str	r7, [sp, #0]
 80a47f6:	a808      	add	r0, sp, #32
 80a47f8:	f7fd fefc 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a47fc:	4633      	mov	r3, r6
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a47fe:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a4800:	4621      	mov	r1, r4
 80a4802:	9700      	str	r7, [sp, #0]
 80a4804:	462a      	mov	r2, r5
 80a4806:	a810      	add	r0, sp, #64	; 0x40
 80a4808:	f7fd fef4 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a480c:	9b22      	ldr	r3, [sp, #136]	; 0x88
 80a480e:	f04f 0801 	mov.w	r8, #1
 80a4812:	f853 1020 	ldr.w	r1, [r3, r0, lsl #2]
 80a4816:	f859 002b 	ldr.w	r0, [r9, fp, lsl #2]
 80a481a:	f00f f899 	bl	80b3950 <__aeabi_fcmpge>
 80a481e:	b900      	cbnz	r0, 80a4822 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xb6>
 80a4820:	4680      	mov	r8, r0
 80a4822:	9b24      	ldr	r3, [sp, #144]	; 0x90
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4824:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4826:	f803 800a 	strb.w	r8, [r3, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a482a:	e7d2      	b.n	80a47d2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x66>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a482c:	3601      	adds	r6, #1
 80a482e:	e7c9      	b.n	80a47c4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x58>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4830:	3501      	adds	r5, #1
 80a4832:	e7c0      	b.n	80a47b6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4a>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4834:	3401      	adds	r4, #1
 80a4836:	e7b7      	b.n	80a47a8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x3c>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a4838:	a803      	add	r0, sp, #12
 80a483a:	f7fd fdba 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a483e:	b019      	add	sp, #100	; 0x64
 80a4840:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a4844 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4844:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a4848:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a484a:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a484c:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a484e:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4850:	4691      	mov	r9, r2
 80a4852:	460c      	mov	r4, r1
 80a4854:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4856:	dd01      	ble.n	80a485c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a4858:	f00b fbea 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a485c:	682b      	ldr	r3, [r5, #0]
 80a485e:	2b04      	cmp	r3, #4
 80a4860:	dcfa      	bgt.n	80a4858 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a4862:	6813      	ldr	r3, [r2, #0]
 80a4864:	2b04      	cmp	r3, #4
 80a4866:	dcf7      	bgt.n	80a4858 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
 80a4868:	2301      	movs	r3, #1
 80a486a:	2104      	movs	r1, #4
 80a486c:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a486e:	f10d 0820 	add.w	r8, sp, #32
 80a4872:	f7fd fde2 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a4876:	4620      	mov	r0, r4
 80a4878:	ab10      	add	r3, sp, #64	; 0x40
 80a487a:	4642      	mov	r2, r8
 80a487c:	4629      	mov	r1, r5
 80a487e:	f7fe f8df 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4882:	2400      	movs	r4, #0
 80a4884:	2100      	movs	r1, #0
 80a4886:	a803      	add	r0, sp, #12
 80a4888:	f7fd fd9e 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a488c:	4284      	cmp	r4, r0
 80a488e:	da3f      	bge.n	80a4910 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
 80a4890:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4892:	2101      	movs	r1, #1
 80a4894:	a803      	add	r0, sp, #12
 80a4896:	f7fd fd97 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a489a:	4285      	cmp	r5, r0
 80a489c:	da36      	bge.n	80a490c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
 80a489e:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a48a0:	2102      	movs	r1, #2
 80a48a2:	a803      	add	r0, sp, #12
 80a48a4:	f7fd fd90 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a48a8:	4286      	cmp	r6, r0
 80a48aa:	da2d      	bge.n	80a4908 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
 80a48ac:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a48ae:	2103      	movs	r1, #3
 80a48b0:	a803      	add	r0, sp, #12
 80a48b2:	f7fd fd89 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a48b6:	4287      	cmp	r7, r0
 80a48b8:	da24      	bge.n	80a4904 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a48ba:	9700      	str	r7, [sp, #0]
 80a48bc:	4633      	mov	r3, r6
 80a48be:	462a      	mov	r2, r5
 80a48c0:	4621      	mov	r1, r4
 80a48c2:	a803      	add	r0, sp, #12
 80a48c4:	f7fd fde5 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a48c8:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a48ca:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a48cc:	4633      	mov	r3, r6
 80a48ce:	462a      	mov	r2, r5
 80a48d0:	4621      	mov	r1, r4
 80a48d2:	4640      	mov	r0, r8
 80a48d4:	f7fd fe8e 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a48d8:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a48da:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a48dc:	4633      	mov	r3, r6
 80a48de:	462a      	mov	r2, r5
 80a48e0:	4621      	mov	r1, r4
 80a48e2:	a810      	add	r0, sp, #64	; 0x40
 80a48e4:	f7fd fe86 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a48e8:	9a22      	ldr	r2, [sp, #136]	; 0x88
 80a48ea:	f859 302b 	ldr.w	r3, [r9, fp, lsl #2]
 80a48ee:	f852 2020 	ldr.w	r2, [r2, r0, lsl #2]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a48f2:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
 80a48f4:	4293      	cmp	r3, r2
 80a48f6:	9a24      	ldr	r2, [sp, #144]	; 0x90
 80a48f8:	bfb4      	ite	lt
 80a48fa:	2300      	movlt	r3, #0
 80a48fc:	2301      	movge	r3, #1
 80a48fe:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4902:	e7d4      	b.n	80a48ae <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4904:	3601      	adds	r6, #1
 80a4906:	e7cb      	b.n	80a48a0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4908:	3501      	adds	r5, #1
 80a490a:	e7c2      	b.n	80a4892 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a490c:	3401      	adds	r4, #1
 80a490e:	e7b9      	b.n	80a4884 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a4910:	a803      	add	r0, sp, #12
 80a4912:	f7fd fd4e 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a4916:	b019      	add	sp, #100	; 0x64
 80a4918:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a491c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a491c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a4920:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4922:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4924:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4926:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4928:	4692      	mov	sl, r2
 80a492a:	460c      	mov	r4, r1
 80a492c:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a492e:	dd01      	ble.n	80a4934 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a4930:	f00b fb7e 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a4934:	682b      	ldr	r3, [r5, #0]
 80a4936:	2b04      	cmp	r3, #4
 80a4938:	dcfa      	bgt.n	80a4930 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a493a:	6813      	ldr	r3, [r2, #0]
 80a493c:	2b04      	cmp	r3, #4
 80a493e:	dcf7      	bgt.n	80a4930 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
 80a4940:	2301      	movs	r3, #1
 80a4942:	2104      	movs	r1, #4
 80a4944:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a4946:	f10d 0820 	add.w	r8, sp, #32
 80a494a:	f7fd fd76 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a494e:	4620      	mov	r0, r4
 80a4950:	ab10      	add	r3, sp, #64	; 0x40
 80a4952:	4642      	mov	r2, r8
 80a4954:	4629      	mov	r1, r5
 80a4956:	f7fe f873 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a495a:	2400      	movs	r4, #0
 80a495c:	2100      	movs	r1, #0
 80a495e:	a803      	add	r0, sp, #12
 80a4960:	f7fd fd32 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4964:	4284      	cmp	r4, r0
 80a4966:	da45      	bge.n	80a49f4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd8>
 80a4968:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a496a:	2101      	movs	r1, #1
 80a496c:	a803      	add	r0, sp, #12
 80a496e:	f7fd fd2b 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4972:	4285      	cmp	r5, r0
 80a4974:	da3c      	bge.n	80a49f0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd4>
 80a4976:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4978:	2102      	movs	r1, #2
 80a497a:	a803      	add	r0, sp, #12
 80a497c:	f7fd fd24 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4980:	4286      	cmp	r6, r0
 80a4982:	da33      	bge.n	80a49ec <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd0>
 80a4984:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4986:	2103      	movs	r1, #3
 80a4988:	a803      	add	r0, sp, #12
 80a498a:	f7fd fd1d 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a498e:	4287      	cmp	r7, r0
 80a4990:	da2a      	bge.n	80a49e8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4992:	9700      	str	r7, [sp, #0]
 80a4994:	4633      	mov	r3, r6
 80a4996:	462a      	mov	r2, r5
 80a4998:	4621      	mov	r1, r4
 80a499a:	a803      	add	r0, sp, #12
 80a499c:	f7fd fd79 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a49a0:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a49a2:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a49a4:	4633      	mov	r3, r6
 80a49a6:	462a      	mov	r2, r5
 80a49a8:	4621      	mov	r1, r4
 80a49aa:	4640      	mov	r0, r8
 80a49ac:	f7fd fe22 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a49b0:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a49b2:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a49b4:	4633      	mov	r3, r6
 80a49b6:	462a      	mov	r2, r5
 80a49b8:	4621      	mov	r1, r4
 80a49ba:	a810      	add	r0, sp, #64	; 0x40
 80a49bc:	f7fd fe1a 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a49c0:	9b22      	ldr	r3, [sp, #136]	; 0x88
 80a49c2:	eb0a 09c9 	add.w	r9, sl, r9, lsl #3
 80a49c6:	eb03 03c0 	add.w	r3, r3, r0, lsl #3

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a49ca:	e9d3 2300 	ldrd	r2, r3, [r3]
 80a49ce:	e9d9 0100 	ldrd	r0, r1, [r9]
 80a49d2:	4290      	cmp	r0, r2
 80a49d4:	eb71 0303 	sbcs.w	r3, r1, r3
 80a49d8:	9a24      	ldr	r2, [sp, #144]	; 0x90
 80a49da:	bfac      	ite	ge
 80a49dc:	2301      	movge	r3, #1
 80a49de:	2300      	movlt	r3, #0
 80a49e0:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a49e4:	3701      	adds	r7, #1
 80a49e6:	e7ce      	b.n	80a4986 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a49e8:	3601      	adds	r6, #1
 80a49ea:	e7c5      	b.n	80a4978 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a49ec:	3501      	adds	r5, #1
 80a49ee:	e7bc      	b.n	80a496a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a49f0:	3401      	adds	r4, #1
 80a49f2:	e7b3      	b.n	80a495c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a49f4:	a803      	add	r0, sp, #12
 80a49f6:	f7fd fcdc 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a49fa:	b019      	add	sp, #100	; 0x64
 80a49fc:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a4a00 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4a00:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a4a04:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4a06:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4a08:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4a0a:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4a0c:	4691      	mov	r9, r2
 80a4a0e:	460c      	mov	r4, r1
 80a4a10:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4a12:	dd01      	ble.n	80a4a18 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a4a14:	f00b fb0c 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a4a18:	682b      	ldr	r3, [r5, #0]
 80a4a1a:	2b04      	cmp	r3, #4
 80a4a1c:	dcfa      	bgt.n	80a4a14 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a4a1e:	6813      	ldr	r3, [r2, #0]
 80a4a20:	2b04      	cmp	r3, #4
 80a4a22:	dcf7      	bgt.n	80a4a14 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
 80a4a24:	2301      	movs	r3, #1
 80a4a26:	2104      	movs	r1, #4
 80a4a28:	a803      	add	r0, sp, #12
 80a4a2a:	f7fd fd06 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a4a2e:	4620      	mov	r0, r4
 80a4a30:	ab10      	add	r3, sp, #64	; 0x40
 80a4a32:	aa08      	add	r2, sp, #32
 80a4a34:	4629      	mov	r1, r5
 80a4a36:	f7fe f803 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4a3a:	2400      	movs	r4, #0
 80a4a3c:	2100      	movs	r1, #0
 80a4a3e:	a803      	add	r0, sp, #12
 80a4a40:	f7fd fcc2 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4a44:	4284      	cmp	r4, r0
 80a4a46:	da41      	bge.n	80a4acc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
 80a4a48:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4a4a:	2101      	movs	r1, #1
 80a4a4c:	a803      	add	r0, sp, #12
 80a4a4e:	f7fd fcbb 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4a52:	4285      	cmp	r5, r0
 80a4a54:	da38      	bge.n	80a4ac8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
 80a4a56:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4a58:	2102      	movs	r1, #2
 80a4a5a:	a803      	add	r0, sp, #12
 80a4a5c:	f7fd fcb4 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4a60:	4286      	cmp	r6, r0
 80a4a62:	da2f      	bge.n	80a4ac4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
 80a4a64:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4a66:	2103      	movs	r1, #3
 80a4a68:	a803      	add	r0, sp, #12
 80a4a6a:	f7fd fcad 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4a6e:	4287      	cmp	r7, r0
 80a4a70:	da26      	bge.n	80a4ac0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4a72:	4633      	mov	r3, r6
 80a4a74:	462a      	mov	r2, r5
 80a4a76:	4621      	mov	r1, r4
 80a4a78:	9700      	str	r7, [sp, #0]
 80a4a7a:	a803      	add	r0, sp, #12
 80a4a7c:	f7fd fd09 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4a80:	4633      	mov	r3, r6
 80a4a82:	462a      	mov	r2, r5
 80a4a84:	4621      	mov	r1, r4

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4a86:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4a88:	9700      	str	r7, [sp, #0]
 80a4a8a:	a808      	add	r0, sp, #32
 80a4a8c:	f7fd fdb2 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a4a90:	4633      	mov	r3, r6
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4a92:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a4a94:	4621      	mov	r1, r4
 80a4a96:	9700      	str	r7, [sp, #0]
 80a4a98:	462a      	mov	r2, r5
 80a4a9a:	a810      	add	r0, sp, #64	; 0x40
 80a4a9c:	f7fd fdaa 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4aa0:	9b22      	ldr	r3, [sp, #136]	; 0x88
 80a4aa2:	f04f 0801 	mov.w	r8, #1
 80a4aa6:	f853 1020 	ldr.w	r1, [r3, r0, lsl #2]
 80a4aaa:	f859 002b 	ldr.w	r0, [r9, fp, lsl #2]
 80a4aae:	f00e ff3b 	bl	80b3928 <__aeabi_fcmplt>
 80a4ab2:	b900      	cbnz	r0, 80a4ab6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xb6>
 80a4ab4:	4680      	mov	r8, r0
 80a4ab6:	9b24      	ldr	r3, [sp, #144]	; 0x90
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4ab8:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4aba:	f803 800a 	strb.w	r8, [r3, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4abe:	e7d2      	b.n	80a4a66 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x66>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4ac0:	3601      	adds	r6, #1
 80a4ac2:	e7c9      	b.n	80a4a58 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x58>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4ac4:	3501      	adds	r5, #1
 80a4ac6:	e7c0      	b.n	80a4a4a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4a>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4ac8:	3401      	adds	r4, #1
 80a4aca:	e7b7      	b.n	80a4a3c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x3c>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a4acc:	a803      	add	r0, sp, #12
 80a4ace:	f7fd fc70 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a4ad2:	b019      	add	sp, #100	; 0x64
 80a4ad4:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a4ad8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4ad8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a4adc:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4ade:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4ae0:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4ae2:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4ae4:	4691      	mov	r9, r2
 80a4ae6:	460c      	mov	r4, r1
 80a4ae8:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4aea:	dd01      	ble.n	80a4af0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a4aec:	f00b faa0 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a4af0:	682b      	ldr	r3, [r5, #0]
 80a4af2:	2b04      	cmp	r3, #4
 80a4af4:	dcfa      	bgt.n	80a4aec <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a4af6:	6813      	ldr	r3, [r2, #0]
 80a4af8:	2b04      	cmp	r3, #4
 80a4afa:	dcf7      	bgt.n	80a4aec <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
 80a4afc:	2301      	movs	r3, #1
 80a4afe:	2104      	movs	r1, #4
 80a4b00:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a4b02:	f10d 0820 	add.w	r8, sp, #32
 80a4b06:	f7fd fc98 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a4b0a:	4620      	mov	r0, r4
 80a4b0c:	ab10      	add	r3, sp, #64	; 0x40
 80a4b0e:	4642      	mov	r2, r8
 80a4b10:	4629      	mov	r1, r5
 80a4b12:	f7fd ff95 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4b16:	2400      	movs	r4, #0
 80a4b18:	2100      	movs	r1, #0
 80a4b1a:	a803      	add	r0, sp, #12
 80a4b1c:	f7fd fc54 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4b20:	4284      	cmp	r4, r0
 80a4b22:	da3f      	bge.n	80a4ba4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
 80a4b24:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4b26:	2101      	movs	r1, #1
 80a4b28:	a803      	add	r0, sp, #12
 80a4b2a:	f7fd fc4d 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4b2e:	4285      	cmp	r5, r0
 80a4b30:	da36      	bge.n	80a4ba0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
 80a4b32:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4b34:	2102      	movs	r1, #2
 80a4b36:	a803      	add	r0, sp, #12
 80a4b38:	f7fd fc46 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4b3c:	4286      	cmp	r6, r0
 80a4b3e:	da2d      	bge.n	80a4b9c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
 80a4b40:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4b42:	2103      	movs	r1, #3
 80a4b44:	a803      	add	r0, sp, #12
 80a4b46:	f7fd fc3f 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4b4a:	4287      	cmp	r7, r0
 80a4b4c:	da24      	bge.n	80a4b98 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4b4e:	9700      	str	r7, [sp, #0]
 80a4b50:	4633      	mov	r3, r6
 80a4b52:	462a      	mov	r2, r5
 80a4b54:	4621      	mov	r1, r4
 80a4b56:	a803      	add	r0, sp, #12
 80a4b58:	f7fd fc9b 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4b5c:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4b5e:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4b60:	4633      	mov	r3, r6
 80a4b62:	462a      	mov	r2, r5
 80a4b64:	4621      	mov	r1, r4
 80a4b66:	4640      	mov	r0, r8
 80a4b68:	f7fd fd44 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a4b6c:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4b6e:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a4b70:	4633      	mov	r3, r6
 80a4b72:	462a      	mov	r2, r5
 80a4b74:	4621      	mov	r1, r4
 80a4b76:	a810      	add	r0, sp, #64	; 0x40
 80a4b78:	f7fd fd3c 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4b7c:	9a22      	ldr	r2, [sp, #136]	; 0x88
 80a4b7e:	f859 302b 	ldr.w	r3, [r9, fp, lsl #2]
 80a4b82:	f852 2020 	ldr.w	r2, [r2, r0, lsl #2]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4b86:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4b88:	4293      	cmp	r3, r2
 80a4b8a:	9a24      	ldr	r2, [sp, #144]	; 0x90
 80a4b8c:	bfac      	ite	ge
 80a4b8e:	2300      	movge	r3, #0
 80a4b90:	2301      	movlt	r3, #1
 80a4b92:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4b96:	e7d4      	b.n	80a4b42 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4b98:	3601      	adds	r6, #1
 80a4b9a:	e7cb      	b.n	80a4b34 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4b9c:	3501      	adds	r5, #1
 80a4b9e:	e7c2      	b.n	80a4b26 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4ba0:	3401      	adds	r4, #1
 80a4ba2:	e7b9      	b.n	80a4b18 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a4ba4:	a803      	add	r0, sp, #12
 80a4ba6:	f7fd fc04 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a4baa:	b019      	add	sp, #100	; 0x64
 80a4bac:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a4bb0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4bb0:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a4bb4:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4bb6:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4bb8:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4bba:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4bbc:	4692      	mov	sl, r2
 80a4bbe:	460c      	mov	r4, r1
 80a4bc0:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4bc2:	dd01      	ble.n	80a4bc8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a4bc4:	f00b fa34 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a4bc8:	682b      	ldr	r3, [r5, #0]
 80a4bca:	2b04      	cmp	r3, #4
 80a4bcc:	dcfa      	bgt.n	80a4bc4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a4bce:	6813      	ldr	r3, [r2, #0]
 80a4bd0:	2b04      	cmp	r3, #4
 80a4bd2:	dcf7      	bgt.n	80a4bc4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
 80a4bd4:	2301      	movs	r3, #1
 80a4bd6:	2104      	movs	r1, #4
 80a4bd8:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a4bda:	f10d 0820 	add.w	r8, sp, #32
 80a4bde:	f7fd fc2c 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a4be2:	4620      	mov	r0, r4
 80a4be4:	ab10      	add	r3, sp, #64	; 0x40
 80a4be6:	4642      	mov	r2, r8
 80a4be8:	4629      	mov	r1, r5
 80a4bea:	f7fd ff29 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4bee:	2400      	movs	r4, #0
 80a4bf0:	2100      	movs	r1, #0
 80a4bf2:	a803      	add	r0, sp, #12
 80a4bf4:	f7fd fbe8 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4bf8:	4284      	cmp	r4, r0
 80a4bfa:	da45      	bge.n	80a4c88 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd8>
 80a4bfc:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4bfe:	2101      	movs	r1, #1
 80a4c00:	a803      	add	r0, sp, #12
 80a4c02:	f7fd fbe1 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4c06:	4285      	cmp	r5, r0
 80a4c08:	da3c      	bge.n	80a4c84 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd4>
 80a4c0a:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4c0c:	2102      	movs	r1, #2
 80a4c0e:	a803      	add	r0, sp, #12
 80a4c10:	f7fd fbda 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4c14:	4286      	cmp	r6, r0
 80a4c16:	da33      	bge.n	80a4c80 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd0>
 80a4c18:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4c1a:	2103      	movs	r1, #3
 80a4c1c:	a803      	add	r0, sp, #12
 80a4c1e:	f7fd fbd3 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4c22:	4287      	cmp	r7, r0
 80a4c24:	da2a      	bge.n	80a4c7c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4c26:	9700      	str	r7, [sp, #0]
 80a4c28:	4633      	mov	r3, r6
 80a4c2a:	462a      	mov	r2, r5
 80a4c2c:	4621      	mov	r1, r4
 80a4c2e:	a803      	add	r0, sp, #12
 80a4c30:	f7fd fc2f 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4c34:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4c36:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4c38:	4633      	mov	r3, r6
 80a4c3a:	462a      	mov	r2, r5
 80a4c3c:	4621      	mov	r1, r4
 80a4c3e:	4640      	mov	r0, r8
 80a4c40:	f7fd fcd8 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a4c44:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4c46:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a4c48:	4633      	mov	r3, r6
 80a4c4a:	462a      	mov	r2, r5
 80a4c4c:	4621      	mov	r1, r4
 80a4c4e:	a810      	add	r0, sp, #64	; 0x40
 80a4c50:	f7fd fcd0 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4c54:	9b22      	ldr	r3, [sp, #136]	; 0x88
 80a4c56:	eb0a 09c9 	add.w	r9, sl, r9, lsl #3
 80a4c5a:	eb03 03c0 	add.w	r3, r3, r0, lsl #3

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4c5e:	e9d3 2300 	ldrd	r2, r3, [r3]
 80a4c62:	e9d9 0100 	ldrd	r0, r1, [r9]
 80a4c66:	4290      	cmp	r0, r2
 80a4c68:	eb71 0303 	sbcs.w	r3, r1, r3
 80a4c6c:	9a24      	ldr	r2, [sp, #144]	; 0x90
 80a4c6e:	bfb4      	ite	lt
 80a4c70:	2301      	movlt	r3, #1
 80a4c72:	2300      	movge	r3, #0
 80a4c74:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4c78:	3701      	adds	r7, #1
 80a4c7a:	e7ce      	b.n	80a4c1a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4c7c:	3601      	adds	r6, #1
 80a4c7e:	e7c5      	b.n	80a4c0c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4c80:	3501      	adds	r5, #1
 80a4c82:	e7bc      	b.n	80a4bfe <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4c84:	3401      	adds	r4, #1
 80a4c86:	e7b3      	b.n	80a4bf0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a4c88:	a803      	add	r0, sp, #12
 80a4c8a:	f7fd fb92 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a4c8e:	b019      	add	sp, #100	; 0x64
 80a4c90:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a4c94 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4c94:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a4c98:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4c9a:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4c9c:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4c9e:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4ca0:	4691      	mov	r9, r2
 80a4ca2:	460c      	mov	r4, r1
 80a4ca4:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4ca6:	dd01      	ble.n	80a4cac <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a4ca8:	f00b f9c2 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a4cac:	682b      	ldr	r3, [r5, #0]
 80a4cae:	2b04      	cmp	r3, #4
 80a4cb0:	dcfa      	bgt.n	80a4ca8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a4cb2:	6813      	ldr	r3, [r2, #0]
 80a4cb4:	2b04      	cmp	r3, #4
 80a4cb6:	dcf7      	bgt.n	80a4ca8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
 80a4cb8:	2301      	movs	r3, #1
 80a4cba:	2104      	movs	r1, #4
 80a4cbc:	a803      	add	r0, sp, #12
 80a4cbe:	f7fd fbbc 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a4cc2:	4620      	mov	r0, r4
 80a4cc4:	ab10      	add	r3, sp, #64	; 0x40
 80a4cc6:	aa08      	add	r2, sp, #32
 80a4cc8:	4629      	mov	r1, r5
 80a4cca:	f7fd feb9 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4cce:	2400      	movs	r4, #0
 80a4cd0:	2100      	movs	r1, #0
 80a4cd2:	a803      	add	r0, sp, #12
 80a4cd4:	f7fd fb78 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4cd8:	4284      	cmp	r4, r0
 80a4cda:	da41      	bge.n	80a4d60 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
 80a4cdc:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4cde:	2101      	movs	r1, #1
 80a4ce0:	a803      	add	r0, sp, #12
 80a4ce2:	f7fd fb71 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4ce6:	4285      	cmp	r5, r0
 80a4ce8:	da38      	bge.n	80a4d5c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
 80a4cea:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4cec:	2102      	movs	r1, #2
 80a4cee:	a803      	add	r0, sp, #12
 80a4cf0:	f7fd fb6a 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4cf4:	4286      	cmp	r6, r0
 80a4cf6:	da2f      	bge.n	80a4d58 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
 80a4cf8:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4cfa:	2103      	movs	r1, #3
 80a4cfc:	a803      	add	r0, sp, #12
 80a4cfe:	f7fd fb63 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4d02:	4287      	cmp	r7, r0
 80a4d04:	da26      	bge.n	80a4d54 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4d06:	4633      	mov	r3, r6
 80a4d08:	462a      	mov	r2, r5
 80a4d0a:	4621      	mov	r1, r4
 80a4d0c:	9700      	str	r7, [sp, #0]
 80a4d0e:	a803      	add	r0, sp, #12
 80a4d10:	f7fd fbbf 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4d14:	4633      	mov	r3, r6
 80a4d16:	462a      	mov	r2, r5
 80a4d18:	4621      	mov	r1, r4

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4d1a:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4d1c:	9700      	str	r7, [sp, #0]
 80a4d1e:	a808      	add	r0, sp, #32
 80a4d20:	f7fd fc68 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a4d24:	4633      	mov	r3, r6
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4d26:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a4d28:	4621      	mov	r1, r4
 80a4d2a:	9700      	str	r7, [sp, #0]
 80a4d2c:	462a      	mov	r2, r5
 80a4d2e:	a810      	add	r0, sp, #64	; 0x40
 80a4d30:	f7fd fc60 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4d34:	9b22      	ldr	r3, [sp, #136]	; 0x88
 80a4d36:	f04f 0801 	mov.w	r8, #1
 80a4d3a:	f853 1020 	ldr.w	r1, [r3, r0, lsl #2]
 80a4d3e:	f859 002b 	ldr.w	r0, [r9, fp, lsl #2]
 80a4d42:	f00e fdfb 	bl	80b393c <__aeabi_fcmple>
 80a4d46:	b900      	cbnz	r0, 80a4d4a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xb6>
 80a4d48:	4680      	mov	r8, r0
 80a4d4a:	9b24      	ldr	r3, [sp, #144]	; 0x90
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4d4c:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4d4e:	f803 800a 	strb.w	r8, [r3, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4d52:	e7d2      	b.n	80a4cfa <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x66>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4d54:	3601      	adds	r6, #1
 80a4d56:	e7c9      	b.n	80a4cec <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x58>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4d58:	3501      	adds	r5, #1
 80a4d5a:	e7c0      	b.n	80a4cde <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4a>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4d5c:	3401      	adds	r4, #1
 80a4d5e:	e7b7      	b.n	80a4cd0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x3c>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a4d60:	a803      	add	r0, sp, #12
 80a4d62:	f7fd fb26 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a4d66:	b019      	add	sp, #100	; 0x64
 80a4d68:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a4d6c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4d6c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a4d70:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4d72:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4d74:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4d76:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4d78:	4691      	mov	r9, r2
 80a4d7a:	460c      	mov	r4, r1
 80a4d7c:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4d7e:	dd01      	ble.n	80a4d84 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a4d80:	f00b f956 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a4d84:	682b      	ldr	r3, [r5, #0]
 80a4d86:	2b04      	cmp	r3, #4
 80a4d88:	dcfa      	bgt.n	80a4d80 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a4d8a:	6813      	ldr	r3, [r2, #0]
 80a4d8c:	2b04      	cmp	r3, #4
 80a4d8e:	dcf7      	bgt.n	80a4d80 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
 80a4d90:	2301      	movs	r3, #1
 80a4d92:	2104      	movs	r1, #4
 80a4d94:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a4d96:	f10d 0820 	add.w	r8, sp, #32
 80a4d9a:	f7fd fb4e 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a4d9e:	4620      	mov	r0, r4
 80a4da0:	ab10      	add	r3, sp, #64	; 0x40
 80a4da2:	4642      	mov	r2, r8
 80a4da4:	4629      	mov	r1, r5
 80a4da6:	f7fd fe4b 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4daa:	2400      	movs	r4, #0
 80a4dac:	2100      	movs	r1, #0
 80a4dae:	a803      	add	r0, sp, #12
 80a4db0:	f7fd fb0a 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4db4:	4284      	cmp	r4, r0
 80a4db6:	da3f      	bge.n	80a4e38 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
 80a4db8:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4dba:	2101      	movs	r1, #1
 80a4dbc:	a803      	add	r0, sp, #12
 80a4dbe:	f7fd fb03 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4dc2:	4285      	cmp	r5, r0
 80a4dc4:	da36      	bge.n	80a4e34 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
 80a4dc6:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4dc8:	2102      	movs	r1, #2
 80a4dca:	a803      	add	r0, sp, #12
 80a4dcc:	f7fd fafc 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4dd0:	4286      	cmp	r6, r0
 80a4dd2:	da2d      	bge.n	80a4e30 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
 80a4dd4:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4dd6:	2103      	movs	r1, #3
 80a4dd8:	a803      	add	r0, sp, #12
 80a4dda:	f7fd faf5 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4dde:	4287      	cmp	r7, r0
 80a4de0:	da24      	bge.n	80a4e2c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4de2:	9700      	str	r7, [sp, #0]
 80a4de4:	4633      	mov	r3, r6
 80a4de6:	462a      	mov	r2, r5
 80a4de8:	4621      	mov	r1, r4
 80a4dea:	a803      	add	r0, sp, #12
 80a4dec:	f7fd fb51 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4df0:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4df2:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4df4:	4633      	mov	r3, r6
 80a4df6:	462a      	mov	r2, r5
 80a4df8:	4621      	mov	r1, r4
 80a4dfa:	4640      	mov	r0, r8
 80a4dfc:	f7fd fbfa 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a4e00:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4e02:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a4e04:	4633      	mov	r3, r6
 80a4e06:	462a      	mov	r2, r5
 80a4e08:	4621      	mov	r1, r4
 80a4e0a:	a810      	add	r0, sp, #64	; 0x40
 80a4e0c:	f7fd fbf2 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4e10:	9a22      	ldr	r2, [sp, #136]	; 0x88
 80a4e12:	f859 302b 	ldr.w	r3, [r9, fp, lsl #2]
 80a4e16:	f852 2020 	ldr.w	r2, [r2, r0, lsl #2]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4e1a:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4e1c:	4293      	cmp	r3, r2
 80a4e1e:	9a24      	ldr	r2, [sp, #144]	; 0x90
 80a4e20:	bfcc      	ite	gt
 80a4e22:	2300      	movgt	r3, #0
 80a4e24:	2301      	movle	r3, #1
 80a4e26:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4e2a:	e7d4      	b.n	80a4dd6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4e2c:	3601      	adds	r6, #1
 80a4e2e:	e7cb      	b.n	80a4dc8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4e30:	3501      	adds	r5, #1
 80a4e32:	e7c2      	b.n	80a4dba <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4e34:	3401      	adds	r4, #1
 80a4e36:	e7b9      	b.n	80a4dac <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a4e38:	a803      	add	r0, sp, #12
 80a4e3a:	f7fd faba 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a4e3e:	b019      	add	sp, #100	; 0x64
 80a4e40:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a4e44 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4e44:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a4e48:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4e4a:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4e4c:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4e4e:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
 80a4e50:	4692      	mov	sl, r2
 80a4e52:	460c      	mov	r4, r1
 80a4e54:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4e56:	dd01      	ble.n	80a4e5c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
 80a4e58:	f00b f8ea 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a4e5c:	682b      	ldr	r3, [r5, #0]
 80a4e5e:	2b04      	cmp	r3, #4
 80a4e60:	dcfa      	bgt.n	80a4e58 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a4e62:	6813      	ldr	r3, [r2, #0]
 80a4e64:	2b04      	cmp	r3, #4
 80a4e66:	dcf7      	bgt.n	80a4e58 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
 80a4e68:	2301      	movs	r3, #1
 80a4e6a:	2104      	movs	r1, #4
 80a4e6c:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a4e6e:	f10d 0820 	add.w	r8, sp, #32
 80a4e72:	f7fd fae2 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a4e76:	4620      	mov	r0, r4
 80a4e78:	ab10      	add	r3, sp, #64	; 0x40
 80a4e7a:	4642      	mov	r2, r8
 80a4e7c:	4629      	mov	r1, r5
 80a4e7e:	f7fd fddf 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4e82:	2400      	movs	r4, #0
 80a4e84:	2100      	movs	r1, #0
 80a4e86:	a803      	add	r0, sp, #12
 80a4e88:	f7fd fa9e 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4e8c:	4284      	cmp	r4, r0
 80a4e8e:	da44      	bge.n	80a4f1a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd6>
 80a4e90:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4e92:	2101      	movs	r1, #1
 80a4e94:	a803      	add	r0, sp, #12
 80a4e96:	f7fd fa97 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4e9a:	4285      	cmp	r5, r0
 80a4e9c:	da3b      	bge.n	80a4f16 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd2>
 80a4e9e:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4ea0:	2102      	movs	r1, #2
 80a4ea2:	a803      	add	r0, sp, #12
 80a4ea4:	f7fd fa90 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4ea8:	4286      	cmp	r6, r0
 80a4eaa:	da32      	bge.n	80a4f12 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xce>
 80a4eac:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4eae:	2103      	movs	r1, #3
 80a4eb0:	a803      	add	r0, sp, #12
 80a4eb2:	f7fd fa89 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4eb6:	4287      	cmp	r7, r0
 80a4eb8:	da29      	bge.n	80a4f0e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xca>
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4eba:	9700      	str	r7, [sp, #0]
 80a4ebc:	4633      	mov	r3, r6
 80a4ebe:	462a      	mov	r2, r5
 80a4ec0:	4621      	mov	r1, r4
 80a4ec2:	a803      	add	r0, sp, #12
 80a4ec4:	f7fd fae5 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4ec8:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4eca:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4ecc:	4633      	mov	r3, r6
 80a4ece:	462a      	mov	r2, r5
 80a4ed0:	4621      	mov	r1, r4
 80a4ed2:	4640      	mov	r0, r8
 80a4ed4:	f7fd fb8e 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a4ed8:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4eda:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
 80a4edc:	4633      	mov	r3, r6
 80a4ede:	462a      	mov	r2, r5
 80a4ee0:	4621      	mov	r1, r4
 80a4ee2:	a810      	add	r0, sp, #64	; 0x40
 80a4ee4:	f7fd fb86 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
 80a4ee8:	9b22      	ldr	r3, [sp, #136]	; 0x88
 80a4eea:	eb0a 09c9 	add.w	r9, sl, r9, lsl #3
 80a4eee:	eb03 00c0 	add.w	r0, r3, r0, lsl #3

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
 80a4ef2:	c803      	ldmia	r0, {r0, r1}
 80a4ef4:	e9d9 2300 	ldrd	r2, r3, [r9]
 80a4ef8:	4290      	cmp	r0, r2
 80a4efa:	eb71 0303 	sbcs.w	r3, r1, r3
 80a4efe:	9a24      	ldr	r2, [sp, #144]	; 0x90
 80a4f00:	bfac      	ite	ge
 80a4f02:	2301      	movge	r3, #1
 80a4f04:	2300      	movlt	r3, #0
 80a4f06:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4f0a:	3701      	adds	r7, #1
 80a4f0c:	e7cf      	b.n	80a4eae <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4f0e:	3601      	adds	r6, #1
 80a4f10:	e7c6      	b.n	80a4ea0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4f12:	3501      	adds	r5, #1
 80a4f14:	e7bd      	b.n	80a4e92 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4f16:	3401      	adds	r4, #1
 80a4f18:	e7b4      	b.n	80a4e84 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a4f1a:	a803      	add	r0, sp, #12
 80a4f1c:	f7fd fa49 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
 80a4f20:	b019      	add	sp, #100	; 0x64
 80a4f22:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a4f26 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a4f26:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a4f2a:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4f2c:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a4f2e:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4f30:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a4f32:	9208      	str	r2, [sp, #32]
 80a4f34:	4604      	mov	r4, r0
 80a4f36:	460e      	mov	r6, r1
 80a4f38:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a4f3a:	dd01      	ble.n	80a4f40 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
 80a4f3c:	f00b f878 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a4f40:	683b      	ldr	r3, [r7, #0]
 80a4f42:	2b04      	cmp	r3, #4
 80a4f44:	dcfa      	bgt.n	80a4f3c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a4f46:	6813      	ldr	r3, [r2, #0]
 80a4f48:	2b04      	cmp	r3, #4
 80a4f4a:	dcf7      	bgt.n	80a4f3c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
 80a4f4c:	2301      	movs	r3, #1
 80a4f4e:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a4f50:	ad10      	add	r5, sp, #64	; 0x40
 80a4f52:	a80b      	add	r0, sp, #44	; 0x2c
 80a4f54:	f7fd fa71 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a4f58:	ab18      	add	r3, sp, #96	; 0x60
 80a4f5a:	462a      	mov	r2, r5
 80a4f5c:	4639      	mov	r1, r7
 80a4f5e:	4630      	mov	r0, r6
 80a4f60:	f7fd fd6e 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a4f64:	6863      	ldr	r3, [r4, #4]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
 80a4f66:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
 80a4f6a:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a4f6c:	68a3      	ldr	r3, [r4, #8]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a4f6e:	9509      	str	r5, [sp, #36]	; 0x24
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a4f70:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
 80a4f72:	68e3      	ldr	r3, [r4, #12]
 80a4f74:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
 80a4f76:	6923      	ldr	r3, [r4, #16]
 80a4f78:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
 80a4f7a:	6963      	ldr	r3, [r4, #20]
 80a4f7c:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
 80a4f7e:	69a3      	ldr	r3, [r4, #24]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4f80:	2400      	movs	r4, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a4f82:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a4f84:	2100      	movs	r1, #0
 80a4f86:	a80b      	add	r0, sp, #44	; 0x2c
 80a4f88:	f7fd fa1e 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4f8c:	4284      	cmp	r4, r0
 80a4f8e:	da59      	bge.n	80a5044 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
 80a4f90:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a4f92:	af0b      	add	r7, sp, #44	; 0x2c
 80a4f94:	2101      	movs	r1, #1
 80a4f96:	4638      	mov	r0, r7
 80a4f98:	f7fd fa16 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4f9c:	4285      	cmp	r5, r0
 80a4f9e:	da4f      	bge.n	80a5040 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
 80a4fa0:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a4fa2:	2102      	movs	r1, #2
 80a4fa4:	4638      	mov	r0, r7
 80a4fa6:	f7fd fa0f 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4faa:	4286      	cmp	r6, r0
 80a4fac:	da46      	bge.n	80a503c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
 80a4fae:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a4fb2:	2103      	movs	r1, #3
 80a4fb4:	4638      	mov	r0, r7
 80a4fb6:	f7fd fa07 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a4fba:	4580      	cmp	r8, r0
 80a4fbc:	da3c      	bge.n	80a5038 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a4fbe:	f8cd 8000 	str.w	r8, [sp]
 80a4fc2:	4633      	mov	r3, r6
 80a4fc4:	462a      	mov	r2, r5
 80a4fc6:	4621      	mov	r1, r4
 80a4fc8:	9809      	ldr	r0, [sp, #36]	; 0x24
 80a4fca:	f7fd fb13 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
 80a4fce:	9b08      	ldr	r3, [sp, #32]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a4fd0:	462a      	mov	r2, r5
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a4fd2:	f813 9000 	ldrb.w	r9, [r3, r0]
 80a4fd6:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a4fd8:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a4fdc:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a4fde:	4621      	mov	r1, r4
 80a4fe0:	4633      	mov	r3, r6
 80a4fe2:	a818      	add	r0, sp, #96	; 0x60
 80a4fe4:	f7fd fb06 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a4fe8:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a4fea:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a4fec:	f813 b000 	ldrb.w	fp, [r3, r0]
 80a4ff0:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a4ff2:	9903      	ldr	r1, [sp, #12]
 80a4ff4:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a4ff8:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a4ffa:	f7fd faa9 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a4ffe:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a5002:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
 80a5004:	9a07      	ldr	r2, [sp, #28]
 80a5006:	9906      	ldr	r1, [sp, #24]
 80a5008:	4658      	mov	r0, fp
 80a500a:	f7fd faa1 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
 80a500e:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
 80a5010:	f8cd 8000 	str.w	r8, [sp]
 80a5014:	4633      	mov	r3, r6
 80a5016:	462a      	mov	r2, r5
 80a5018:	4621      	mov	r1, r4
 80a501a:	4638      	mov	r0, r7
 80a501c:	f7fd fa39 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80a5020:	ebcb 0309 	rsb	r3, fp, r9
 80a5024:	f1d3 0900 	rsbs	r9, r3, #0
 80a5028:	eb49 0903 	adc.w	r9, r9, r3
 80a502c:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a502e:	f108 0801 	add.w	r8, r8, #1
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
          output_data[Offset(output_shape, b, y, x, c)] =
 80a5032:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a5036:	e7bc      	b.n	80a4fb2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a5038:	3601      	adds	r6, #1
 80a503a:	e7b2      	b.n	80a4fa2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a503c:	3501      	adds	r5, #1
 80a503e:	e7a8      	b.n	80a4f92 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a5040:	3401      	adds	r4, #1
 80a5042:	e79f      	b.n	80a4f84 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a5044:	a80b      	add	r0, sp, #44	; 0x2c
 80a5046:	f7fd f9b4 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
 80a504a:	b021      	add	sp, #132	; 0x84
 80a504c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a5050 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a5050:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a5054:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a5056:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a5058:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a505a:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a505c:	9208      	str	r2, [sp, #32]
 80a505e:	4604      	mov	r4, r0
 80a5060:	460e      	mov	r6, r1
 80a5062:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a5064:	dd01      	ble.n	80a506a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
 80a5066:	f00a ffe3 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a506a:	683b      	ldr	r3, [r7, #0]
 80a506c:	2b04      	cmp	r3, #4
 80a506e:	dcfa      	bgt.n	80a5066 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a5070:	6813      	ldr	r3, [r2, #0]
 80a5072:	2b04      	cmp	r3, #4
 80a5074:	dcf7      	bgt.n	80a5066 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
 80a5076:	2301      	movs	r3, #1
 80a5078:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a507a:	ad10      	add	r5, sp, #64	; 0x40
 80a507c:	a80b      	add	r0, sp, #44	; 0x2c
 80a507e:	f7fd f9dc 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a5082:	ab18      	add	r3, sp, #96	; 0x60
 80a5084:	462a      	mov	r2, r5
 80a5086:	4639      	mov	r1, r7
 80a5088:	4630      	mov	r0, r6
 80a508a:	f7fd fcd9 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a508e:	6863      	ldr	r3, [r4, #4]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
 80a5090:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
 80a5094:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a5096:	68a3      	ldr	r3, [r4, #8]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a5098:	9509      	str	r5, [sp, #36]	; 0x24
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a509a:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
 80a509c:	68e3      	ldr	r3, [r4, #12]
 80a509e:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
 80a50a0:	6923      	ldr	r3, [r4, #16]
 80a50a2:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
 80a50a4:	6963      	ldr	r3, [r4, #20]
 80a50a6:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
 80a50a8:	69a3      	ldr	r3, [r4, #24]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a50aa:	2400      	movs	r4, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a50ac:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a50ae:	2100      	movs	r1, #0
 80a50b0:	a80b      	add	r0, sp, #44	; 0x2c
 80a50b2:	f7fd f989 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a50b6:	4284      	cmp	r4, r0
 80a50b8:	da59      	bge.n	80a516e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
 80a50ba:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a50bc:	af0b      	add	r7, sp, #44	; 0x2c
 80a50be:	2101      	movs	r1, #1
 80a50c0:	4638      	mov	r0, r7
 80a50c2:	f7fd f981 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a50c6:	4285      	cmp	r5, r0
 80a50c8:	da4f      	bge.n	80a516a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
 80a50ca:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a50cc:	2102      	movs	r1, #2
 80a50ce:	4638      	mov	r0, r7
 80a50d0:	f7fd f97a 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a50d4:	4286      	cmp	r6, r0
 80a50d6:	da46      	bge.n	80a5166 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
 80a50d8:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a50dc:	2103      	movs	r1, #3
 80a50de:	4638      	mov	r0, r7
 80a50e0:	f7fd f972 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a50e4:	4580      	cmp	r8, r0
 80a50e6:	da3c      	bge.n	80a5162 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a50e8:	f8cd 8000 	str.w	r8, [sp]
 80a50ec:	4633      	mov	r3, r6
 80a50ee:	462a      	mov	r2, r5
 80a50f0:	4621      	mov	r1, r4
 80a50f2:	9809      	ldr	r0, [sp, #36]	; 0x24
 80a50f4:	f7fd fa7e 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
 80a50f8:	9b08      	ldr	r3, [sp, #32]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a50fa:	462a      	mov	r2, r5
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a50fc:	f913 9000 	ldrsb.w	r9, [r3, r0]
 80a5100:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a5102:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a5106:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a5108:	4621      	mov	r1, r4
 80a510a:	4633      	mov	r3, r6
 80a510c:	a818      	add	r0, sp, #96	; 0x60
 80a510e:	f7fd fa71 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a5112:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a5114:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a5116:	f913 b000 	ldrsb.w	fp, [r3, r0]
 80a511a:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a511c:	9903      	ldr	r1, [sp, #12]
 80a511e:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a5122:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a5124:	f7fd fa14 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a5128:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a512c:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
 80a512e:	9a07      	ldr	r2, [sp, #28]
 80a5130:	9906      	ldr	r1, [sp, #24]
 80a5132:	4658      	mov	r0, fp
 80a5134:	f7fd fa0c 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
 80a5138:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
 80a513a:	f8cd 8000 	str.w	r8, [sp]
 80a513e:	4633      	mov	r3, r6
 80a5140:	462a      	mov	r2, r5
 80a5142:	4621      	mov	r1, r4
 80a5144:	4638      	mov	r0, r7
 80a5146:	f7fd f9a4 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80a514a:	ebcb 0309 	rsb	r3, fp, r9
 80a514e:	f1d3 0900 	rsbs	r9, r3, #0
 80a5152:	eb49 0903 	adc.w	r9, r9, r3
 80a5156:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a5158:	f108 0801 	add.w	r8, r8, #1
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
          output_data[Offset(output_shape, b, y, x, c)] =
 80a515c:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a5160:	e7bc      	b.n	80a50dc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a5162:	3601      	adds	r6, #1
 80a5164:	e7b2      	b.n	80a50cc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a5166:	3501      	adds	r5, #1
 80a5168:	e7a8      	b.n	80a50bc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a516a:	3401      	adds	r4, #1
 80a516c:	e79f      	b.n	80a50ae <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a516e:	a80b      	add	r0, sp, #44	; 0x2c
 80a5170:	f7fd f91f 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
 80a5174:	b021      	add	sp, #132	; 0x84
 80a5176:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

080a517c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode>:
              op_params, GetTensorShape(input1), GetTensorData<type>(input1), \
              GetTensorShape(input2), GetTensorData<type>(input2),            \
              GetTensorShape(output), GetTensorData<bool>(output));           \
  }

TfLiteStatus EqualEval(TfLiteContext* context, TfLiteNode* node) {
 80a517c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a5180:	680a      	ldr	r2, [r1, #0]
 80a5182:	f8d0 9008 	ldr.w	r9, [r0, #8]
 80a5186:	4682      	mov	sl, r0
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a5188:	6850      	ldr	r0, [r2, #4]
 80a518a:	2338      	movs	r3, #56	; 0x38
 80a518c:	6895      	ldr	r5, [r2, #8]
 80a518e:	fb03 f800 	mul.w	r8, r3, r0
 80a5192:	fb03 9505 	mla	r5, r3, r5, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a5196:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a5198:	eb09 0608 	add.w	r6, r9, r8
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a519c:	6854      	ldr	r4, [r2, #4]
 80a519e:	b0ab      	sub	sp, #172	; 0xac
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
 80a51a0:	4629      	mov	r1, r5
 80a51a2:	4630      	mov	r0, r6
 80a51a4:	fb03 9404 	mla	r4, r3, r4, r9
 80a51a8:	f00a fcb0 	bl	80afb0c <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
  switch (input1->type) {
 80a51ac:	f819 2008 	ldrb.w	r2, [r9, r8]

TfLiteStatus EqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
 80a51b0:	f080 0001 	eor.w	r0, r0, #1
  switch (input1->type) {
 80a51b4:	1e53      	subs	r3, r2, #1

TfLiteStatus EqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
 80a51b6:	b2c7      	uxtb	r7, r0
  switch (input1->type) {
 80a51b8:	2b08      	cmp	r3, #8
 80a51ba:	f200 826f 	bhi.w	80a569c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x520>
 80a51be:	e8df f013 	tbh	[pc, r3, lsl #1]
 80a51c2:	0057      	.short	0x0057
 80a51c4:	014200a8 	.word	0x014200a8
 80a51c8:	026d00f4 	.word	0x026d00f4
 80a51cc:	026d0009 	.word	0x026d0009
 80a51d0:	01d2026d 	.word	0x01d2026d
 80a51d4:	f10d 0874 	add.w	r8, sp, #116	; 0x74
    case kTfLiteBool:
      TF_LITE_COMPARISON(bool, Equal, requires_broadcast);
 80a51d8:	4631      	mov	r1, r6
 80a51da:	b1cf      	cbz	r7, 80a5210 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x94>
 80a51dc:	a813      	add	r0, sp, #76	; 0x4c
 80a51de:	f7fd fb98 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a51e2:	4629      	mov	r1, r5
 80a51e4:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a51e6:	6876      	ldr	r6, [r6, #4]
 80a51e8:	f7fd fb93 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a51ec:	b105      	cbz	r5, 80a51f0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x74>
 80a51ee:	686d      	ldr	r5, [r5, #4]
 80a51f0:	4621      	mov	r1, r4
 80a51f2:	4640      	mov	r0, r8
 80a51f4:	f7fd fb8d 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a51f8:	b104      	cbz	r4, 80a51fc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x80>
 80a51fa:	6864      	ldr	r4, [r4, #4]
      bool* output_data) {                                                     \
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
 80a51fc:	9402      	str	r4, [sp, #8]
 80a51fe:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a5202:	ab18      	add	r3, sp, #96	; 0x60
 80a5204:	4632      	mov	r2, r6
 80a5206:	a913      	add	r1, sp, #76	; 0x4c
 80a5208:	a822      	add	r0, sp, #136	; 0x88
 80a520a:	f7fe fe01 	bl	80a3e10 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a520e:	e1e8      	b.n	80a55e2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x466>
 80a5210:	a818      	add	r0, sp, #96	; 0x60
 80a5212:	f7fd fb7e 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5216:	4629      	mov	r1, r5
 80a5218:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a521a:	6876      	ldr	r6, [r6, #4]
 80a521c:	f7fd fb79 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5220:	b105      	cbz	r5, 80a5224 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0xa8>
 80a5222:	686d      	ldr	r5, [r5, #4]
 80a5224:	4621      	mov	r1, r4
 80a5226:	a822      	add	r0, sp, #136	; 0x88
 80a5228:	f7fd fb73 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a522c:	b104      	cbz	r4, 80a5230 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0xb4>
 80a522e:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a5230:	4641      	mov	r1, r8
 80a5232:	aa22      	add	r2, sp, #136	; 0x88
 80a5234:	a818      	add	r0, sp, #96	; 0x60
 80a5236:	f7fd f94e 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a523a:	3d01      	subs	r5, #1
 80a523c:	1e73      	subs	r3, r6, #1
 80a523e:	17c1      	asrs	r1, r0, #31
 80a5240:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5242:	2600      	movs	r6, #0
 80a5244:	2700      	movs	r7, #0
 80a5246:	4286      	cmp	r6, r0
 80a5248:	eb77 0201 	sbcs.w	r2, r7, r1
 80a524c:	f280 822d 	bge.w	80a56aa <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x52e>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a5250:	f813 2f01 	ldrb.w	r2, [r3, #1]!
 80a5254:	f815 ef01 	ldrb.w	lr, [r5, #1]!
 80a5258:	ebce 0c02 	rsb	ip, lr, r2
 80a525c:	f1dc 0200 	rsbs	r2, ip, #0
 80a5260:	eb42 020c 	adc.w	r2, r2, ip
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5264:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a5266:	f804 2f01 	strb.w	r2, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a526a:	f147 0700 	adc.w	r7, r7, #0
 80a526e:	e7ea      	b.n	80a5246 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0xca>
 80a5270:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteFloat32:
      TF_LITE_COMPARISON(float, Equal, requires_broadcast);
 80a5274:	4631      	mov	r1, r6
 80a5276:	b1cf      	cbz	r7, 80a52ac <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x130>
 80a5278:	a813      	add	r0, sp, #76	; 0x4c
 80a527a:	f7fd fb4a 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a527e:	4629      	mov	r1, r5
 80a5280:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a5282:	6876      	ldr	r6, [r6, #4]
 80a5284:	f7fd fb45 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5288:	b105      	cbz	r5, 80a528c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x110>
 80a528a:	686d      	ldr	r5, [r5, #4]
 80a528c:	4621      	mov	r1, r4
 80a528e:	4640      	mov	r0, r8
 80a5290:	f7fd fb3f 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a5294:	b104      	cbz	r4, 80a5298 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x11c>
 80a5296:	6864      	ldr	r4, [r4, #4]
      bool* output_data) {                                                     \
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
 80a5298:	9402      	str	r4, [sp, #8]
 80a529a:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a529e:	ab18      	add	r3, sp, #96	; 0x60
 80a52a0:	4632      	mov	r2, r6
 80a52a2:	a913      	add	r1, sp, #76	; 0x4c
 80a52a4:	a822      	add	r0, sp, #136	; 0x88
 80a52a6:	f7fe fe1d 	bl	80a3ee4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a52aa:	e19a      	b.n	80a55e2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x466>
 80a52ac:	a818      	add	r0, sp, #96	; 0x60
 80a52ae:	f7fd fb30 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a52b2:	6873      	ldr	r3, [r6, #4]
 80a52b4:	4629      	mov	r1, r5
 80a52b6:	4640      	mov	r0, r8
 80a52b8:	9305      	str	r3, [sp, #20]
 80a52ba:	f7fd fb2a 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a52be:	b105      	cbz	r5, 80a52c2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x146>
 80a52c0:	686d      	ldr	r5, [r5, #4]
 80a52c2:	4621      	mov	r1, r4
 80a52c4:	a822      	add	r0, sp, #136	; 0x88
 80a52c6:	f7fd fb24 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a52ca:	b104      	cbz	r4, 80a52ce <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x152>
 80a52cc:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a52ce:	aa22      	add	r2, sp, #136	; 0x88
 80a52d0:	4641      	mov	r1, r8
 80a52d2:	a818      	add	r0, sp, #96	; 0x60
 80a52d4:	f7fd f8ff 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a52d8:	4682      	mov	sl, r0
 80a52da:	ea4f 7be0 	mov.w	fp, r0, asr #31
 80a52de:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
 80a52e0:	2600      	movs	r6, #0
 80a52e2:	2700      	movs	r7, #0
 80a52e4:	4556      	cmp	r6, sl
 80a52e6:	eb77 030b 	sbcs.w	r3, r7, fp
 80a52ea:	f280 81de 	bge.w	80a56aa <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x52e>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a52ee:	9a05      	ldr	r2, [sp, #20]
 80a52f0:	f855 1026 	ldr.w	r1, [r5, r6, lsl #2]
 80a52f4:	f852 0026 	ldr.w	r0, [r2, r6, lsl #2]
 80a52f8:	3401      	adds	r4, #1
 80a52fa:	f04f 0901 	mov.w	r9, #1
 80a52fe:	f00e fb09 	bl	80b3914 <__aeabi_fcmpeq>
 80a5302:	b900      	cbnz	r0, 80a5306 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x18a>
 80a5304:	4681      	mov	r9, r0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5306:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a5308:	f884 9000 	strb.w	r9, [r4]
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a530c:	f147 0700 	adc.w	r7, r7, #0
 80a5310:	e7e8      	b.n	80a52e4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x168>
 80a5312:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, Equal, requires_broadcast);
 80a5316:	4631      	mov	r1, r6
 80a5318:	b1cf      	cbz	r7, 80a534e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x1d2>
 80a531a:	a813      	add	r0, sp, #76	; 0x4c
 80a531c:	f7fd faf9 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5320:	4629      	mov	r1, r5
 80a5322:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a5324:	6876      	ldr	r6, [r6, #4]
 80a5326:	f7fd faf4 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a532a:	b105      	cbz	r5, 80a532e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x1b2>
 80a532c:	686d      	ldr	r5, [r5, #4]
 80a532e:	4621      	mov	r1, r4
 80a5330:	4640      	mov	r0, r8
 80a5332:	f7fd faee 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a5336:	b104      	cbz	r4, 80a533a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x1be>
 80a5338:	6864      	ldr	r4, [r4, #4]
      bool* output_data) {                                                     \
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
 80a533a:	9402      	str	r4, [sp, #8]
 80a533c:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a5340:	ab18      	add	r3, sp, #96	; 0x60
 80a5342:	4632      	mov	r2, r6
 80a5344:	a913      	add	r1, sp, #76	; 0x4c
 80a5346:	a822      	add	r0, sp, #136	; 0x88
 80a5348:	f7fe fe38 	bl	80a3fbc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a534c:	e149      	b.n	80a55e2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x466>
 80a534e:	a818      	add	r0, sp, #96	; 0x60
 80a5350:	f7fd fadf 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5354:	4629      	mov	r1, r5
 80a5356:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a5358:	6876      	ldr	r6, [r6, #4]
 80a535a:	f7fd fada 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a535e:	b105      	cbz	r5, 80a5362 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x1e6>
 80a5360:	686d      	ldr	r5, [r5, #4]
 80a5362:	4621      	mov	r1, r4
 80a5364:	a822      	add	r0, sp, #136	; 0x88
 80a5366:	f7fd fad4 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a536a:	b104      	cbz	r4, 80a536e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x1f2>
 80a536c:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a536e:	aa22      	add	r2, sp, #136	; 0x88
 80a5370:	4641      	mov	r1, r8
 80a5372:	a818      	add	r0, sp, #96	; 0x60
 80a5374:	f7fd f8af 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a5378:	3c01      	subs	r4, #1
 80a537a:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
 80a537c:	2200      	movs	r2, #0
 80a537e:	2300      	movs	r3, #0
 80a5380:	4282      	cmp	r2, r0
 80a5382:	eb73 0701 	sbcs.w	r7, r3, r1
 80a5386:	f280 8190 	bge.w	80a56aa <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x52e>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a538a:	f856 7022 	ldr.w	r7, [r6, r2, lsl #2]
 80a538e:	f855 e022 	ldr.w	lr, [r5, r2, lsl #2]
 80a5392:	ebce 0e07 	rsb	lr, lr, r7
 80a5396:	f1de 0700 	rsbs	r7, lr, #0
 80a539a:	eb47 070e 	adc.w	r7, r7, lr
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a539e:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a53a0:	f804 7f01 	strb.w	r7, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a53a4:	f143 0300 	adc.w	r3, r3, #0
 80a53a8:	e7ea      	b.n	80a5380 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x204>
 80a53aa:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, Equal, requires_broadcast);
 80a53ae:	4631      	mov	r1, r6
 80a53b0:	b1cf      	cbz	r7, 80a53e6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x26a>
 80a53b2:	a813      	add	r0, sp, #76	; 0x4c
 80a53b4:	f7fd faad 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a53b8:	4629      	mov	r1, r5
 80a53ba:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a53bc:	6876      	ldr	r6, [r6, #4]
 80a53be:	f7fd faa8 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a53c2:	b105      	cbz	r5, 80a53c6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x24a>
 80a53c4:	686d      	ldr	r5, [r5, #4]
 80a53c6:	4621      	mov	r1, r4
 80a53c8:	4640      	mov	r0, r8
 80a53ca:	f7fd faa2 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a53ce:	b104      	cbz	r4, 80a53d2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x256>
 80a53d0:	6864      	ldr	r4, [r4, #4]
      bool* output_data) {                                                     \
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
 80a53d2:	9402      	str	r4, [sp, #8]
 80a53d4:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a53d8:	ab18      	add	r3, sp, #96	; 0x60
 80a53da:	4632      	mov	r2, r6
 80a53dc:	a913      	add	r1, sp, #76	; 0x4c
 80a53de:	a822      	add	r0, sp, #136	; 0x88
 80a53e0:	f7fe fe57 	bl	80a4092 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a53e4:	e0fd      	b.n	80a55e2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x466>
 80a53e6:	a818      	add	r0, sp, #96	; 0x60
 80a53e8:	f7fd fa93 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a53ec:	4629      	mov	r1, r5
 80a53ee:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a53f0:	6876      	ldr	r6, [r6, #4]
 80a53f2:	f7fd fa8e 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a53f6:	b105      	cbz	r5, 80a53fa <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x27e>
 80a53f8:	686d      	ldr	r5, [r5, #4]
 80a53fa:	4621      	mov	r1, r4
 80a53fc:	a822      	add	r0, sp, #136	; 0x88
 80a53fe:	f7fd fa88 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a5402:	b104      	cbz	r4, 80a5406 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x28a>
 80a5404:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a5406:	aa22      	add	r2, sp, #136	; 0x88
 80a5408:	4641      	mov	r1, r8
 80a540a:	a818      	add	r0, sp, #96	; 0x60
 80a540c:	f7fd f863 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a5410:	3d08      	subs	r5, #8
 80a5412:	17c1      	asrs	r1, r0, #31
 80a5414:	f1a6 0e08 	sub.w	lr, r6, #8
 80a5418:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
 80a541a:	2200      	movs	r2, #0
 80a541c:	2300      	movs	r3, #0
 80a541e:	4282      	cmp	r2, r0
 80a5420:	eb73 0601 	sbcs.w	r6, r3, r1
 80a5424:	f280 8141 	bge.w	80a56aa <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x52e>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a5428:	e9f5 6702 	ldrd	r6, r7, [r5, #8]!
 80a542c:	e9fe ab02 	ldrd	sl, fp, [lr, #8]!
 80a5430:	45bb      	cmp	fp, r7
 80a5432:	bf06      	itte	eq
 80a5434:	45b2      	cmpeq	sl, r6
 80a5436:	2601      	moveq	r6, #1
 80a5438:	2600      	movne	r6, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a543a:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a543c:	f804 6f01 	strb.w	r6, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5440:	f143 0300 	adc.w	r3, r3, #0
 80a5444:	e7eb      	b.n	80a541e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x2a2>
            GetTensorData<input_dtype>(input2), GetTensorShape(output),        \
            GetTensorData<bool>(output));                                      \
      }                                                                        \
    }                                                                          \
  }
TF_LITE_QUANTIZE_COMPARISON(Equal);
 80a5446:	6933      	ldr	r3, [r6, #16]
 80a5448:	68f0      	ldr	r0, [r6, #12]
 80a544a:	f1c3 0900 	rsb	r9, r3, #0
 80a544e:	692b      	ldr	r3, [r5, #16]
 80a5450:	f1c3 0800 	rsb	r8, r3, #0
 80a5454:	f00d fc36 	bl	80b2cc4 <__aeabi_f2d>
 80a5458:	ab10      	add	r3, sp, #64	; 0x40
 80a545a:	aa0f      	add	r2, sp, #60	; 0x3c
 80a545c:	f00a fbb0 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a5460:	68e8      	ldr	r0, [r5, #12]
 80a5462:	f00d fc2f 	bl	80b2cc4 <__aeabi_f2d>
 80a5466:	ab12      	add	r3, sp, #72	; 0x48
 80a5468:	aa11      	add	r2, sp, #68	; 0x44
 80a546a:	f00a fba9 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a546e:	2308      	movs	r3, #8
 80a5470:	9322      	str	r3, [sp, #136]	; 0x88
 80a5472:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80a5474:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
 80a5478:	9324      	str	r3, [sp, #144]	; 0x90
 80a547a:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80a547c:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
 80a5480:	9325      	str	r3, [sp, #148]	; 0x94
 80a5482:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80a5484:	f10d 0874 	add.w	r8, sp, #116	; 0x74
 80a5488:	9327      	str	r3, [sp, #156]	; 0x9c
 80a548a:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80a548c:	4631      	mov	r1, r6
 80a548e:	9328      	str	r3, [sp, #160]	; 0xa0
 80a5490:	a813      	add	r0, sp, #76	; 0x4c
 80a5492:	b1bf      	cbz	r7, 80a54c4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x348>
 80a5494:	f7fd fa3d 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5498:	4629      	mov	r1, r5
 80a549a:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a549c:	6876      	ldr	r6, [r6, #4]
 80a549e:	f7fd fa38 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a54a2:	4621      	mov	r1, r4
 80a54a4:	4640      	mov	r0, r8
 80a54a6:	686d      	ldr	r5, [r5, #4]
 80a54a8:	f7fd fa33 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a54ac:	b104      	cbz	r4, 80a54b0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x334>
 80a54ae:	6864      	ldr	r4, [r4, #4]
      bool* output_data) {                                                     \
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
 80a54b0:	9402      	str	r4, [sp, #8]
 80a54b2:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a54b6:	ab18      	add	r3, sp, #96	; 0x60
 80a54b8:	4632      	mov	r2, r6
 80a54ba:	a913      	add	r1, sp, #76	; 0x4c
 80a54bc:	a822      	add	r0, sp, #136	; 0x88
 80a54be:	f7ff fd32 	bl	80a4f26 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a54c2:	e08e      	b.n	80a55e2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x466>
 80a54c4:	f7fd fa25 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a54c8:	6873      	ldr	r3, [r6, #4]
 80a54ca:	4629      	mov	r1, r5
 80a54cc:	a818      	add	r0, sp, #96	; 0x60
 80a54ce:	9305      	str	r3, [sp, #20]
 80a54d0:	f7fd fa1f 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a54d4:	4621      	mov	r1, r4
 80a54d6:	4640      	mov	r0, r8
 80a54d8:	f8d5 b004 	ldr.w	fp, [r5, #4]
 80a54dc:	f7fd fa19 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a54e0:	b104      	cbz	r4, 80a54e4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x368>
 80a54e2:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a54e4:	9b23      	ldr	r3, [sp, #140]	; 0x8c
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a54e6:	aa1d      	add	r2, sp, #116	; 0x74
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a54e8:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a54ea:	9b24      	ldr	r3, [sp, #144]	; 0x90
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a54ec:	a918      	add	r1, sp, #96	; 0x60
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a54ee:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
 80a54f0:	9b25      	ldr	r3, [sp, #148]	; 0x94
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a54f2:	a813      	add	r0, sp, #76	; 0x4c
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
 80a54f4:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
 80a54f6:	9b26      	ldr	r3, [sp, #152]	; 0x98
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
 80a54f8:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
 80a54fa:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
 80a54fc:	9b27      	ldr	r3, [sp, #156]	; 0x9c
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a54fe:	f04f 0800 	mov.w	r8, #0
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
 80a5502:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;
 80a5504:	9b28      	ldr	r3, [sp, #160]	; 0xa0

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5506:	f04f 0900 	mov.w	r9, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a550a:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a550c:	f7fc ffe3 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a5510:	4602      	mov	r2, r0
 80a5512:	17c3      	asrs	r3, r0, #31
 80a5514:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5518:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
 80a551c:	4590      	cmp	r8, r2
 80a551e:	eb79 0303 	sbcs.w	r3, r9, r3
 80a5522:	f280 80b0 	bge.w	80a5686 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x50a>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a5526:	f81b 5008 	ldrb.w	r5, [fp, r8]
 80a552a:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a552c:	9a08      	ldr	r2, [sp, #32]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a552e:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a5530:	9b05      	ldr	r3, [sp, #20]
 80a5532:	9907      	ldr	r1, [sp, #28]
 80a5534:	f813 0008 	ldrb.w	r0, [r3, r8]
 80a5538:	9b06      	ldr	r3, [sp, #24]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a553a:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a553c:	4418      	add	r0, r3
 80a553e:	40b8      	lsls	r0, r7
 80a5540:	f7fd f806 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a5544:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a5546:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a5548:	990a      	ldr	r1, [sp, #40]	; 0x28
 80a554a:	4628      	mov	r0, r5
 80a554c:	f7fd f800 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
 80a5550:	ebc0 020a 	rsb	r2, r0, sl
 80a5554:	4250      	negs	r0, r2
 80a5556:	4150      	adcs	r0, r2
 80a5558:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a555c:	f118 0801 	adds.w	r8, r8, #1
 80a5560:	f149 0900 	adc.w	r9, r9, #0
 80a5564:	e7d8      	b.n	80a5518 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x39c>
 80a5566:	6933      	ldr	r3, [r6, #16]
 80a5568:	68f0      	ldr	r0, [r6, #12]
 80a556a:	f1c3 0900 	rsb	r9, r3, #0
 80a556e:	692b      	ldr	r3, [r5, #16]
 80a5570:	f1c3 0800 	rsb	r8, r3, #0
 80a5574:	f00d fba6 	bl	80b2cc4 <__aeabi_f2d>
 80a5578:	ab10      	add	r3, sp, #64	; 0x40
 80a557a:	aa0f      	add	r2, sp, #60	; 0x3c
 80a557c:	f00a fb20 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a5580:	68e8      	ldr	r0, [r5, #12]
 80a5582:	f00d fb9f 	bl	80b2cc4 <__aeabi_f2d>
 80a5586:	ab12      	add	r3, sp, #72	; 0x48
 80a5588:	aa11      	add	r2, sp, #68	; 0x44
 80a558a:	f00a fb19 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a558e:	2308      	movs	r3, #8
 80a5590:	9322      	str	r3, [sp, #136]	; 0x88
 80a5592:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80a5594:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
 80a5598:	9324      	str	r3, [sp, #144]	; 0x90
 80a559a:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80a559c:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
 80a55a0:	9325      	str	r3, [sp, #148]	; 0x94
 80a55a2:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80a55a4:	f10d 0874 	add.w	r8, sp, #116	; 0x74
 80a55a8:	9327      	str	r3, [sp, #156]	; 0x9c
 80a55aa:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80a55ac:	4631      	mov	r1, r6
 80a55ae:	9328      	str	r3, [sp, #160]	; 0xa0
 80a55b0:	a813      	add	r0, sp, #76	; 0x4c
 80a55b2:	b1c7      	cbz	r7, 80a55e6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x46a>
 80a55b4:	f7fd f9ad 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a55b8:	4629      	mov	r1, r5
 80a55ba:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a55bc:	6876      	ldr	r6, [r6, #4]
 80a55be:	f7fd f9a8 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a55c2:	4621      	mov	r1, r4
 80a55c4:	4640      	mov	r0, r8
 80a55c6:	686d      	ldr	r5, [r5, #4]
 80a55c8:	f7fd f9a3 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a55cc:	b104      	cbz	r4, 80a55d0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x454>
 80a55ce:	6864      	ldr	r4, [r4, #4]
      bool* output_data) {                                                     \
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
 80a55d0:	9402      	str	r4, [sp, #8]
 80a55d2:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a55d6:	ab18      	add	r3, sp, #96	; 0x60
 80a55d8:	4632      	mov	r2, r6
 80a55da:	a913      	add	r1, sp, #76	; 0x4c
 80a55dc:	a822      	add	r0, sp, #136	; 0x88
 80a55de:	f7ff fd37 	bl	80a5050 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a55e2:	4640      	mov	r0, r8
 80a55e4:	e050      	b.n	80a5688 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x50c>
 80a55e6:	f7fd f994 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a55ea:	6873      	ldr	r3, [r6, #4]
 80a55ec:	4629      	mov	r1, r5
 80a55ee:	a818      	add	r0, sp, #96	; 0x60
 80a55f0:	9305      	str	r3, [sp, #20]
 80a55f2:	f7fd f98e 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a55f6:	4621      	mov	r1, r4
 80a55f8:	4640      	mov	r0, r8
 80a55fa:	f8d5 b004 	ldr.w	fp, [r5, #4]
 80a55fe:	f7fd f988 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a5602:	b104      	cbz	r4, 80a5606 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x48a>
 80a5604:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a5606:	9b23      	ldr	r3, [sp, #140]	; 0x8c
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a5608:	aa1d      	add	r2, sp, #116	; 0x74
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a560a:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a560c:	9b24      	ldr	r3, [sp, #144]	; 0x90
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a560e:	a918      	add	r1, sp, #96	; 0x60
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a5610:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
 80a5612:	9b25      	ldr	r3, [sp, #148]	; 0x94
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a5614:	a813      	add	r0, sp, #76	; 0x4c
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
 80a5616:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
 80a5618:	9b26      	ldr	r3, [sp, #152]	; 0x98
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
 80a561a:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
 80a561c:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
 80a561e:	9b27      	ldr	r3, [sp, #156]	; 0x9c
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5620:	f04f 0800 	mov.w	r8, #0
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
 80a5624:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;
 80a5626:	9b28      	ldr	r3, [sp, #160]	; 0xa0

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5628:	f04f 0900 	mov.w	r9, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a562c:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a562e:	f7fc ff52 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a5632:	4602      	mov	r2, r0
 80a5634:	17c3      	asrs	r3, r0, #31
 80a5636:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
  for (int64_t i = 0; i < flatsize; ++i) {
 80a563a:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
 80a563e:	4590      	cmp	r8, r2
 80a5640:	eb79 0303 	sbcs.w	r3, r9, r3
 80a5644:	da1f      	bge.n	80a5686 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x50a>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a5646:	f91b 5008 	ldrsb.w	r5, [fp, r8]
 80a564a:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a564c:	9a08      	ldr	r2, [sp, #32]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a564e:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a5650:	9b05      	ldr	r3, [sp, #20]
 80a5652:	9907      	ldr	r1, [sp, #28]
 80a5654:	f913 0008 	ldrsb.w	r0, [r3, r8]
 80a5658:	9b06      	ldr	r3, [sp, #24]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a565a:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a565c:	4418      	add	r0, r3
 80a565e:	40b8      	lsls	r0, r7
 80a5660:	f7fc ff76 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a5664:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a5666:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a5668:	990a      	ldr	r1, [sp, #40]	; 0x28
 80a566a:	4628      	mov	r0, r5
 80a566c:	f7fc ff70 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
 80a5670:	ebc0 030a 	rsb	r3, r0, sl
 80a5674:	4258      	negs	r0, r3
 80a5676:	4158      	adcs	r0, r3
 80a5678:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a567c:	f118 0801 	adds.w	r8, r8, #1
 80a5680:	f149 0900 	adc.w	r9, r9, #0
 80a5684:	e7d9      	b.n	80a563a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x4be>
 80a5686:	a81d      	add	r0, sp, #116	; 0x74
 80a5688:	f7fc fe93 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a568c:	a818      	add	r0, sp, #96	; 0x60
 80a568e:	f7fc fe90 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a5692:	a813      	add	r0, sp, #76	; 0x4c
 80a5694:	f7fc fe8d 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(
          context, "Does not support type %d, requires bool|float|int|uint8",
          input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
 80a5698:	2000      	movs	r0, #0
 80a569a:	e00e      	b.n	80a56ba <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x53e>
                                 requires_broadcast);
      break;
    default:
      context->ReportError(
          context, "Does not support type %d, requires bool|float|int|uint8",
          input1->type);
 80a569c:	4650      	mov	r0, sl
 80a569e:	f8da 3014 	ldr.w	r3, [sl, #20]
 80a56a2:	4907      	ldr	r1, [pc, #28]	; (80a56c0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x544>)
 80a56a4:	4798      	blx	r3
      return kTfLiteError;
 80a56a6:	2001      	movs	r0, #1
 80a56a8:	e007      	b.n	80a56ba <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x53e>
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, Equal, requires_broadcast);
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, Equal, requires_broadcast);
 80a56aa:	a822      	add	r0, sp, #136	; 0x88
 80a56ac:	f7fc fe81 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a56b0:	4640      	mov	r0, r8
 80a56b2:	f7fc fe7e 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a56b6:	a818      	add	r0, sp, #96	; 0x60
 80a56b8:	e7ec      	b.n	80a5694 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x518>
          context, "Does not support type %d, requires bool|float|int|uint8",
          input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
 80a56ba:	b02b      	add	sp, #172	; 0xac
 80a56bc:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80a56c0:	080b5c9f 	.word	0x080b5c9f

080a56c4 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a56c4:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a56c8:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a56ca:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a56cc:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a56ce:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a56d0:	9208      	str	r2, [sp, #32]
 80a56d2:	4604      	mov	r4, r0
 80a56d4:	460e      	mov	r6, r1
 80a56d6:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a56d8:	dd01      	ble.n	80a56de <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
 80a56da:	f00a fca9 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a56de:	683b      	ldr	r3, [r7, #0]
 80a56e0:	2b04      	cmp	r3, #4
 80a56e2:	dcfa      	bgt.n	80a56da <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a56e4:	6813      	ldr	r3, [r2, #0]
 80a56e6:	2b04      	cmp	r3, #4
 80a56e8:	dcf7      	bgt.n	80a56da <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
 80a56ea:	2301      	movs	r3, #1
 80a56ec:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a56ee:	ad10      	add	r5, sp, #64	; 0x40
 80a56f0:	a80b      	add	r0, sp, #44	; 0x2c
 80a56f2:	f7fc fea2 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a56f6:	ab18      	add	r3, sp, #96	; 0x60
 80a56f8:	462a      	mov	r2, r5
 80a56fa:	4639      	mov	r1, r7
 80a56fc:	4630      	mov	r0, r6
 80a56fe:	f7fd f99f 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a5702:	6863      	ldr	r3, [r4, #4]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
 80a5704:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
 80a5708:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a570a:	68a3      	ldr	r3, [r4, #8]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a570c:	9509      	str	r5, [sp, #36]	; 0x24
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a570e:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
 80a5710:	68e3      	ldr	r3, [r4, #12]
 80a5712:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
 80a5714:	6923      	ldr	r3, [r4, #16]
 80a5716:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
 80a5718:	6963      	ldr	r3, [r4, #20]
 80a571a:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
 80a571c:	69a3      	ldr	r3, [r4, #24]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a571e:	2400      	movs	r4, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a5720:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a5722:	2100      	movs	r1, #0
 80a5724:	a80b      	add	r0, sp, #44	; 0x2c
 80a5726:	f7fc fe4f 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a572a:	4284      	cmp	r4, r0
 80a572c:	da58      	bge.n	80a57e0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11c>
 80a572e:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a5730:	af0b      	add	r7, sp, #44	; 0x2c
 80a5732:	2101      	movs	r1, #1
 80a5734:	4638      	mov	r0, r7
 80a5736:	f7fc fe47 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a573a:	4285      	cmp	r5, r0
 80a573c:	da4e      	bge.n	80a57dc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x118>
 80a573e:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a5740:	2102      	movs	r1, #2
 80a5742:	4638      	mov	r0, r7
 80a5744:	f7fc fe40 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a5748:	4286      	cmp	r6, r0
 80a574a:	da45      	bge.n	80a57d8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x114>
 80a574c:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a5750:	2103      	movs	r1, #3
 80a5752:	4638      	mov	r0, r7
 80a5754:	f7fc fe38 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a5758:	4580      	cmp	r8, r0
 80a575a:	da3b      	bge.n	80a57d4 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x110>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a575c:	f8cd 8000 	str.w	r8, [sp]
 80a5760:	4633      	mov	r3, r6
 80a5762:	462a      	mov	r2, r5
 80a5764:	4621      	mov	r1, r4
 80a5766:	9809      	ldr	r0, [sp, #36]	; 0x24
 80a5768:	f7fc ff44 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
 80a576c:	9b08      	ldr	r3, [sp, #32]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a576e:	462a      	mov	r2, r5
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a5770:	f813 9000 	ldrb.w	r9, [r3, r0]
 80a5774:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a5776:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a577a:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a577c:	4621      	mov	r1, r4
 80a577e:	4633      	mov	r3, r6
 80a5780:	a818      	add	r0, sp, #96	; 0x60
 80a5782:	f7fc ff37 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a5786:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a5788:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a578a:	f813 b000 	ldrb.w	fp, [r3, r0]
 80a578e:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a5790:	9903      	ldr	r1, [sp, #12]
 80a5792:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a5796:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a5798:	f7fc feda 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a579c:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a57a0:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
 80a57a2:	9a07      	ldr	r2, [sp, #28]
 80a57a4:	9906      	ldr	r1, [sp, #24]
 80a57a6:	4658      	mov	r0, fp
 80a57a8:	f7fc fed2 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
 80a57ac:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
 80a57ae:	f8cd 8000 	str.w	r8, [sp]
 80a57b2:	4633      	mov	r3, r6
 80a57b4:	462a      	mov	r2, r5
 80a57b6:	4621      	mov	r1, r4
 80a57b8:	4638      	mov	r0, r7
 80a57ba:	f7fc fe6a 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80a57be:	ebb9 090b 	subs.w	r9, r9, fp
 80a57c2:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
 80a57c4:	bf18      	it	ne
 80a57c6:	f04f 0901 	movne.w	r9, #1
 80a57ca:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a57ce:	f108 0801 	add.w	r8, r8, #1
 80a57d2:	e7bd      	b.n	80a5750 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a57d4:	3601      	adds	r6, #1
 80a57d6:	e7b3      	b.n	80a5740 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a57d8:	3501      	adds	r5, #1
 80a57da:	e7a9      	b.n	80a5730 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a57dc:	3401      	adds	r4, #1
 80a57de:	e7a0      	b.n	80a5722 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a57e0:	a80b      	add	r0, sp, #44	; 0x2c
 80a57e2:	f7fc fde6 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
 80a57e6:	b021      	add	sp, #132	; 0x84
 80a57e8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a57ec <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a57ec:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a57f0:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a57f2:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a57f4:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a57f6:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a57f8:	9208      	str	r2, [sp, #32]
 80a57fa:	4604      	mov	r4, r0
 80a57fc:	460e      	mov	r6, r1
 80a57fe:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a5800:	dd01      	ble.n	80a5806 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
 80a5802:	f00a fc15 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a5806:	683b      	ldr	r3, [r7, #0]
 80a5808:	2b04      	cmp	r3, #4
 80a580a:	dcfa      	bgt.n	80a5802 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a580c:	6813      	ldr	r3, [r2, #0]
 80a580e:	2b04      	cmp	r3, #4
 80a5810:	dcf7      	bgt.n	80a5802 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
 80a5812:	2301      	movs	r3, #1
 80a5814:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a5816:	ad10      	add	r5, sp, #64	; 0x40
 80a5818:	a80b      	add	r0, sp, #44	; 0x2c
 80a581a:	f7fc fe0e 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a581e:	ab18      	add	r3, sp, #96	; 0x60
 80a5820:	462a      	mov	r2, r5
 80a5822:	4639      	mov	r1, r7
 80a5824:	4630      	mov	r0, r6
 80a5826:	f7fd f90b 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a582a:	6863      	ldr	r3, [r4, #4]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
 80a582c:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
 80a5830:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a5832:	68a3      	ldr	r3, [r4, #8]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a5834:	9509      	str	r5, [sp, #36]	; 0x24
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a5836:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
 80a5838:	68e3      	ldr	r3, [r4, #12]
 80a583a:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
 80a583c:	6923      	ldr	r3, [r4, #16]
 80a583e:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
 80a5840:	6963      	ldr	r3, [r4, #20]
 80a5842:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
 80a5844:	69a3      	ldr	r3, [r4, #24]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a5846:	2400      	movs	r4, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a5848:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a584a:	2100      	movs	r1, #0
 80a584c:	a80b      	add	r0, sp, #44	; 0x2c
 80a584e:	f7fc fdbb 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a5852:	4284      	cmp	r4, r0
 80a5854:	da58      	bge.n	80a5908 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11c>
 80a5856:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a5858:	af0b      	add	r7, sp, #44	; 0x2c
 80a585a:	2101      	movs	r1, #1
 80a585c:	4638      	mov	r0, r7
 80a585e:	f7fc fdb3 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a5862:	4285      	cmp	r5, r0
 80a5864:	da4e      	bge.n	80a5904 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x118>
 80a5866:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a5868:	2102      	movs	r1, #2
 80a586a:	4638      	mov	r0, r7
 80a586c:	f7fc fdac 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a5870:	4286      	cmp	r6, r0
 80a5872:	da45      	bge.n	80a5900 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x114>
 80a5874:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a5878:	2103      	movs	r1, #3
 80a587a:	4638      	mov	r0, r7
 80a587c:	f7fc fda4 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a5880:	4580      	cmp	r8, r0
 80a5882:	da3b      	bge.n	80a58fc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x110>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a5884:	f8cd 8000 	str.w	r8, [sp]
 80a5888:	4633      	mov	r3, r6
 80a588a:	462a      	mov	r2, r5
 80a588c:	4621      	mov	r1, r4
 80a588e:	9809      	ldr	r0, [sp, #36]	; 0x24
 80a5890:	f7fc feb0 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
 80a5894:	9b08      	ldr	r3, [sp, #32]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a5896:	462a      	mov	r2, r5
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a5898:	f913 9000 	ldrsb.w	r9, [r3, r0]
 80a589c:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a589e:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a58a2:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a58a4:	4621      	mov	r1, r4
 80a58a6:	4633      	mov	r3, r6
 80a58a8:	a818      	add	r0, sp, #96	; 0x60
 80a58aa:	f7fc fea3 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a58ae:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a58b0:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a58b2:	f913 b000 	ldrsb.w	fp, [r3, r0]
 80a58b6:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a58b8:	9903      	ldr	r1, [sp, #12]
 80a58ba:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a58be:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a58c0:	f7fc fe46 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a58c4:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a58c8:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
 80a58ca:	9a07      	ldr	r2, [sp, #28]
 80a58cc:	9906      	ldr	r1, [sp, #24]
 80a58ce:	4658      	mov	r0, fp
 80a58d0:	f7fc fe3e 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
 80a58d4:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
 80a58d6:	f8cd 8000 	str.w	r8, [sp]
 80a58da:	4633      	mov	r3, r6
 80a58dc:	462a      	mov	r2, r5
 80a58de:	4621      	mov	r1, r4
 80a58e0:	4638      	mov	r0, r7
 80a58e2:	f7fc fdd6 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80a58e6:	ebb9 090b 	subs.w	r9, r9, fp
 80a58ea:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
 80a58ec:	bf18      	it	ne
 80a58ee:	f04f 0901 	movne.w	r9, #1
 80a58f2:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a58f6:	f108 0801 	add.w	r8, r8, #1
 80a58fa:	e7bd      	b.n	80a5878 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a58fc:	3601      	adds	r6, #1
 80a58fe:	e7b3      	b.n	80a5868 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a5900:	3501      	adds	r5, #1
 80a5902:	e7a9      	b.n	80a5858 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a5904:	3401      	adds	r4, #1
 80a5906:	e7a0      	b.n	80a584a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a5908:	a80b      	add	r0, sp, #44	; 0x2c
 80a590a:	f7fc fd52 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
 80a590e:	b021      	add	sp, #132	; 0x84
 80a5910:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a5914 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode>:

// TODO(renjieliu): Refactor the logic to avoid duplications.
TfLiteStatus NotEqualEval(TfLiteContext* context, TfLiteNode* node) {
 80a5914:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a5918:	680a      	ldr	r2, [r1, #0]
 80a591a:	f8d0 9008 	ldr.w	r9, [r0, #8]
 80a591e:	4682      	mov	sl, r0
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a5920:	6850      	ldr	r0, [r2, #4]
 80a5922:	2338      	movs	r3, #56	; 0x38
 80a5924:	6895      	ldr	r5, [r2, #8]
 80a5926:	fb03 f800 	mul.w	r8, r3, r0
 80a592a:	fb03 9505 	mla	r5, r3, r5, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a592e:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a5930:	eb09 0608 	add.w	r6, r9, r8
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a5934:	6854      	ldr	r4, [r2, #4]
 80a5936:	b0ab      	sub	sp, #172	; 0xac
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
 80a5938:	4629      	mov	r1, r5
 80a593a:	4630      	mov	r0, r6
 80a593c:	fb03 9404 	mla	r4, r3, r4, r9
 80a5940:	f00a f8e4 	bl	80afb0c <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
  switch (input1->type) {
 80a5944:	f819 2008 	ldrb.w	r2, [r9, r8]
// TODO(renjieliu): Refactor the logic to avoid duplications.
TfLiteStatus NotEqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
 80a5948:	f080 0001 	eor.w	r0, r0, #1
  switch (input1->type) {
 80a594c:	1e53      	subs	r3, r2, #1
// TODO(renjieliu): Refactor the logic to avoid duplications.
TfLiteStatus NotEqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
 80a594e:	b2c7      	uxtb	r7, r0
  switch (input1->type) {
 80a5950:	2b08      	cmp	r3, #8
 80a5952:	f200 826a 	bhi.w	80a5e2a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x516>
 80a5956:	e8df f013 	tbh	[pc, r3, lsl #1]
 80a595a:	0053      	.short	0x0053
 80a595c:	013d00a5 	.word	0x013d00a5
 80a5960:	026800ef 	.word	0x026800ef
 80a5964:	02680009 	.word	0x02680009
 80a5968:	01cd0268 	.word	0x01cd0268
 80a596c:	f10d 0874 	add.w	r8, sp, #116	; 0x74
    case kTfLiteBool:
      TF_LITE_COMPARISON(bool, NotEqual, requires_broadcast);
 80a5970:	4631      	mov	r1, r6
 80a5972:	b1cf      	cbz	r7, 80a59a8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x94>
 80a5974:	a813      	add	r0, sp, #76	; 0x4c
 80a5976:	f7fc ffcc 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a597a:	4629      	mov	r1, r5
 80a597c:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a597e:	6876      	ldr	r6, [r6, #4]
 80a5980:	f7fc ffc7 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5984:	b105      	cbz	r5, 80a5988 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x74>
 80a5986:	686d      	ldr	r5, [r5, #4]
 80a5988:	4621      	mov	r1, r4
 80a598a:	4640      	mov	r0, r8
 80a598c:	f7fc ffc1 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a5990:	b104      	cbz	r4, 80a5994 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x80>
 80a5992:	6864      	ldr	r4, [r4, #4]
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
 80a5994:	9402      	str	r4, [sp, #8]
 80a5996:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a599a:	ab18      	add	r3, sp, #96	; 0x60
 80a599c:	4632      	mov	r2, r6
 80a599e:	a913      	add	r1, sp, #76	; 0x4c
 80a59a0:	a822      	add	r0, sp, #136	; 0x88
 80a59a2:	f7fe fbe8 	bl	80a4176 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a59a6:	e1e3      	b.n	80a5d70 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x45c>
 80a59a8:	a818      	add	r0, sp, #96	; 0x60
 80a59aa:	f7fc ffb2 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a59ae:	4629      	mov	r1, r5
 80a59b0:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a59b2:	6876      	ldr	r6, [r6, #4]
 80a59b4:	f7fc ffad 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a59b8:	b105      	cbz	r5, 80a59bc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0xa8>
 80a59ba:	686d      	ldr	r5, [r5, #4]
 80a59bc:	4621      	mov	r1, r4
 80a59be:	a822      	add	r0, sp, #136	; 0x88
 80a59c0:	f7fc ffa7 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a59c4:	b104      	cbz	r4, 80a59c8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0xb4>
 80a59c6:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a59c8:	4641      	mov	r1, r8
 80a59ca:	aa22      	add	r2, sp, #136	; 0x88
 80a59cc:	a818      	add	r0, sp, #96	; 0x60
 80a59ce:	f7fc fd82 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a59d2:	3d01      	subs	r5, #1
 80a59d4:	1e73      	subs	r3, r6, #1
 80a59d6:	17c1      	asrs	r1, r0, #31
 80a59d8:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
 80a59da:	2600      	movs	r6, #0
 80a59dc:	2700      	movs	r7, #0
 80a59de:	4286      	cmp	r6, r0
 80a59e0:	eb77 0201 	sbcs.w	r2, r7, r1
 80a59e4:	f280 8228 	bge.w	80a5e38 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x524>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a59e8:	f813 ef01 	ldrb.w	lr, [r3, #1]!
 80a59ec:	f815 2f01 	ldrb.w	r2, [r5, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a59f0:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a59f2:	ea8e 0202 	eor.w	r2, lr, r2
 80a59f6:	f804 2f01 	strb.w	r2, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a59fa:	f147 0700 	adc.w	r7, r7, #0
 80a59fe:	e7ee      	b.n	80a59de <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0xca>
 80a5a00:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteFloat32:
      TF_LITE_COMPARISON(float, NotEqual, requires_broadcast);
 80a5a04:	4631      	mov	r1, r6
 80a5a06:	b1cf      	cbz	r7, 80a5a3c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x128>
 80a5a08:	a813      	add	r0, sp, #76	; 0x4c
 80a5a0a:	f7fc ff82 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5a0e:	4629      	mov	r1, r5
 80a5a10:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a5a12:	6876      	ldr	r6, [r6, #4]
 80a5a14:	f7fc ff7d 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5a18:	b105      	cbz	r5, 80a5a1c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x108>
 80a5a1a:	686d      	ldr	r5, [r5, #4]
 80a5a1c:	4621      	mov	r1, r4
 80a5a1e:	4640      	mov	r0, r8
 80a5a20:	f7fc ff77 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a5a24:	b104      	cbz	r4, 80a5a28 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x114>
 80a5a26:	6864      	ldr	r4, [r4, #4]
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
 80a5a28:	9402      	str	r4, [sp, #8]
 80a5a2a:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a5a2e:	ab18      	add	r3, sp, #96	; 0x60
 80a5a30:	4632      	mov	r2, r6
 80a5a32:	a913      	add	r1, sp, #76	; 0x4c
 80a5a34:	a822      	add	r0, sp, #136	; 0x88
 80a5a36:	f7fe fc06 	bl	80a4246 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a5a3a:	e199      	b.n	80a5d70 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x45c>
 80a5a3c:	a818      	add	r0, sp, #96	; 0x60
 80a5a3e:	f7fc ff68 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a5a42:	6873      	ldr	r3, [r6, #4]
 80a5a44:	4629      	mov	r1, r5
 80a5a46:	4640      	mov	r0, r8
 80a5a48:	9305      	str	r3, [sp, #20]
 80a5a4a:	f7fc ff62 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5a4e:	b105      	cbz	r5, 80a5a52 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x13e>
 80a5a50:	686d      	ldr	r5, [r5, #4]
 80a5a52:	4621      	mov	r1, r4
 80a5a54:	a822      	add	r0, sp, #136	; 0x88
 80a5a56:	f7fc ff5c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a5a5a:	b104      	cbz	r4, 80a5a5e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x14a>
 80a5a5c:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a5a5e:	aa22      	add	r2, sp, #136	; 0x88
 80a5a60:	4641      	mov	r1, r8
 80a5a62:	a818      	add	r0, sp, #96	; 0x60
 80a5a64:	f7fc fd37 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a5a68:	4682      	mov	sl, r0
 80a5a6a:	ea4f 7be0 	mov.w	fp, r0, asr #31
 80a5a6e:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5a70:	2600      	movs	r6, #0
 80a5a72:	2700      	movs	r7, #0
 80a5a74:	4556      	cmp	r6, sl
 80a5a76:	eb77 030b 	sbcs.w	r3, r7, fp
 80a5a7a:	f280 81dd 	bge.w	80a5e38 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x524>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a5a7e:	9a05      	ldr	r2, [sp, #20]
 80a5a80:	f855 1026 	ldr.w	r1, [r5, r6, lsl #2]
 80a5a84:	f852 0026 	ldr.w	r0, [r2, r6, lsl #2]
 80a5a88:	3401      	adds	r4, #1
 80a5a8a:	f04f 0901 	mov.w	r9, #1
 80a5a8e:	f00d ff41 	bl	80b3914 <__aeabi_fcmpeq>
 80a5a92:	b108      	cbz	r0, 80a5a98 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x184>
 80a5a94:	f04f 0900 	mov.w	r9, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5a98:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a5a9a:	f884 9000 	strb.w	r9, [r4]
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5a9e:	f147 0700 	adc.w	r7, r7, #0
 80a5aa2:	e7e7      	b.n	80a5a74 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x160>
 80a5aa4:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, NotEqual, requires_broadcast);
 80a5aa8:	4631      	mov	r1, r6
 80a5aaa:	b1cf      	cbz	r7, 80a5ae0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x1cc>
 80a5aac:	a813      	add	r0, sp, #76	; 0x4c
 80a5aae:	f7fc ff30 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5ab2:	4629      	mov	r1, r5
 80a5ab4:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a5ab6:	6876      	ldr	r6, [r6, #4]
 80a5ab8:	f7fc ff2b 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5abc:	b105      	cbz	r5, 80a5ac0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x1ac>
 80a5abe:	686d      	ldr	r5, [r5, #4]
 80a5ac0:	4621      	mov	r1, r4
 80a5ac2:	4640      	mov	r0, r8
 80a5ac4:	f7fc ff25 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a5ac8:	b104      	cbz	r4, 80a5acc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x1b8>
 80a5aca:	6864      	ldr	r4, [r4, #4]
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
 80a5acc:	9402      	str	r4, [sp, #8]
 80a5ace:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a5ad2:	ab18      	add	r3, sp, #96	; 0x60
 80a5ad4:	4632      	mov	r2, r6
 80a5ad6:	a913      	add	r1, sp, #76	; 0x4c
 80a5ad8:	a822      	add	r0, sp, #136	; 0x88
 80a5ada:	f7fe fc21 	bl	80a4320 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a5ade:	e147      	b.n	80a5d70 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x45c>
 80a5ae0:	a818      	add	r0, sp, #96	; 0x60
 80a5ae2:	f7fc ff16 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5ae6:	4629      	mov	r1, r5
 80a5ae8:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a5aea:	6876      	ldr	r6, [r6, #4]
 80a5aec:	f7fc ff11 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5af0:	b105      	cbz	r5, 80a5af4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x1e0>
 80a5af2:	686d      	ldr	r5, [r5, #4]
 80a5af4:	4621      	mov	r1, r4
 80a5af6:	a822      	add	r0, sp, #136	; 0x88
 80a5af8:	f7fc ff0b 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a5afc:	b104      	cbz	r4, 80a5b00 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x1ec>
 80a5afe:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a5b00:	aa22      	add	r2, sp, #136	; 0x88
 80a5b02:	4641      	mov	r1, r8
 80a5b04:	a818      	add	r0, sp, #96	; 0x60
 80a5b06:	f7fc fce6 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a5b0a:	3c01      	subs	r4, #1
 80a5b0c:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5b0e:	2200      	movs	r2, #0
 80a5b10:	2300      	movs	r3, #0
 80a5b12:	4282      	cmp	r2, r0
 80a5b14:	eb73 0701 	sbcs.w	r7, r3, r1
 80a5b18:	f280 818e 	bge.w	80a5e38 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x524>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a5b1c:	f856 7022 	ldr.w	r7, [r6, r2, lsl #2]
 80a5b20:	f855 e022 	ldr.w	lr, [r5, r2, lsl #2]
 80a5b24:	ebb7 070e 	subs.w	r7, r7, lr
 80a5b28:	bf18      	it	ne
 80a5b2a:	2701      	movne	r7, #1
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5b2c:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a5b2e:	f804 7f01 	strb.w	r7, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5b32:	f143 0300 	adc.w	r3, r3, #0
 80a5b36:	e7ec      	b.n	80a5b12 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x1fe>
 80a5b38:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, NotEqual, requires_broadcast);
 80a5b3c:	4631      	mov	r1, r6
 80a5b3e:	b1cf      	cbz	r7, 80a5b74 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x260>
 80a5b40:	a813      	add	r0, sp, #76	; 0x4c
 80a5b42:	f7fc fee6 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5b46:	4629      	mov	r1, r5
 80a5b48:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a5b4a:	6876      	ldr	r6, [r6, #4]
 80a5b4c:	f7fc fee1 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5b50:	b105      	cbz	r5, 80a5b54 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x240>
 80a5b52:	686d      	ldr	r5, [r5, #4]
 80a5b54:	4621      	mov	r1, r4
 80a5b56:	4640      	mov	r0, r8
 80a5b58:	f7fc fedb 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a5b5c:	b104      	cbz	r4, 80a5b60 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x24c>
 80a5b5e:	6864      	ldr	r4, [r4, #4]
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
 80a5b60:	9402      	str	r4, [sp, #8]
 80a5b62:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a5b66:	ab18      	add	r3, sp, #96	; 0x60
 80a5b68:	4632      	mov	r2, r6
 80a5b6a:	a913      	add	r1, sp, #76	; 0x4c
 80a5b6c:	a822      	add	r0, sp, #136	; 0x88
 80a5b6e:	f7fe fc42 	bl	80a43f6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a5b72:	e0fd      	b.n	80a5d70 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x45c>
 80a5b74:	a818      	add	r0, sp, #96	; 0x60
 80a5b76:	f7fc fecc 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5b7a:	4629      	mov	r1, r5
 80a5b7c:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a5b7e:	6876      	ldr	r6, [r6, #4]
 80a5b80:	f7fc fec7 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5b84:	b105      	cbz	r5, 80a5b88 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x274>
 80a5b86:	686d      	ldr	r5, [r5, #4]
 80a5b88:	4621      	mov	r1, r4
 80a5b8a:	a822      	add	r0, sp, #136	; 0x88
 80a5b8c:	f7fc fec1 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a5b90:	b104      	cbz	r4, 80a5b94 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x280>
 80a5b92:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a5b94:	aa22      	add	r2, sp, #136	; 0x88
 80a5b96:	4641      	mov	r1, r8
 80a5b98:	a818      	add	r0, sp, #96	; 0x60
 80a5b9a:	f7fc fc9c 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a5b9e:	3d08      	subs	r5, #8
 80a5ba0:	17c1      	asrs	r1, r0, #31
 80a5ba2:	f1a6 0e08 	sub.w	lr, r6, #8
 80a5ba6:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5ba8:	2200      	movs	r2, #0
 80a5baa:	2300      	movs	r3, #0
 80a5bac:	4282      	cmp	r2, r0
 80a5bae:	eb73 0601 	sbcs.w	r6, r3, r1
 80a5bb2:	f280 8141 	bge.w	80a5e38 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x524>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a5bb6:	e9f5 6702 	ldrd	r6, r7, [r5, #8]!
 80a5bba:	e9fe ab02 	ldrd	sl, fp, [lr, #8]!
 80a5bbe:	45bb      	cmp	fp, r7
 80a5bc0:	bf0a      	itet	eq
 80a5bc2:	45b2      	cmpeq	sl, r6
 80a5bc4:	2601      	movne	r6, #1
 80a5bc6:	2600      	moveq	r6, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5bc8:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a5bca:	f804 6f01 	strb.w	r6, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5bce:	f143 0300 	adc.w	r3, r3, #0
 80a5bd2:	e7eb      	b.n	80a5bac <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x298>
            GetTensorData<bool>(output));                                      \
      }                                                                        \
    }                                                                          \
  }
TF_LITE_QUANTIZE_COMPARISON(Equal);
TF_LITE_QUANTIZE_COMPARISON(NotEqual);
 80a5bd4:	6933      	ldr	r3, [r6, #16]
 80a5bd6:	68f0      	ldr	r0, [r6, #12]
 80a5bd8:	f1c3 0900 	rsb	r9, r3, #0
 80a5bdc:	692b      	ldr	r3, [r5, #16]
 80a5bde:	f1c3 0800 	rsb	r8, r3, #0
 80a5be2:	f00d f86f 	bl	80b2cc4 <__aeabi_f2d>
 80a5be6:	ab10      	add	r3, sp, #64	; 0x40
 80a5be8:	aa0f      	add	r2, sp, #60	; 0x3c
 80a5bea:	f009 ffe9 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a5bee:	68e8      	ldr	r0, [r5, #12]
 80a5bf0:	f00d f868 	bl	80b2cc4 <__aeabi_f2d>
 80a5bf4:	ab12      	add	r3, sp, #72	; 0x48
 80a5bf6:	aa11      	add	r2, sp, #68	; 0x44
 80a5bf8:	f009 ffe2 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a5bfc:	2308      	movs	r3, #8
 80a5bfe:	9322      	str	r3, [sp, #136]	; 0x88
 80a5c00:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80a5c02:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
 80a5c06:	9324      	str	r3, [sp, #144]	; 0x90
 80a5c08:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80a5c0a:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
 80a5c0e:	9325      	str	r3, [sp, #148]	; 0x94
 80a5c10:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80a5c12:	f10d 0874 	add.w	r8, sp, #116	; 0x74
 80a5c16:	9327      	str	r3, [sp, #156]	; 0x9c
 80a5c18:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80a5c1a:	4631      	mov	r1, r6
 80a5c1c:	9328      	str	r3, [sp, #160]	; 0xa0
 80a5c1e:	a813      	add	r0, sp, #76	; 0x4c
 80a5c20:	b1bf      	cbz	r7, 80a5c52 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x33e>
 80a5c22:	f7fc fe76 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5c26:	4629      	mov	r1, r5
 80a5c28:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a5c2a:	6876      	ldr	r6, [r6, #4]
 80a5c2c:	f7fc fe71 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5c30:	4621      	mov	r1, r4
 80a5c32:	4640      	mov	r0, r8
 80a5c34:	686d      	ldr	r5, [r5, #4]
 80a5c36:	f7fc fe6c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a5c3a:	b104      	cbz	r4, 80a5c3e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x32a>
 80a5c3c:	6864      	ldr	r4, [r4, #4]
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
 80a5c3e:	9402      	str	r4, [sp, #8]
 80a5c40:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a5c44:	ab18      	add	r3, sp, #96	; 0x60
 80a5c46:	4632      	mov	r2, r6
 80a5c48:	a913      	add	r1, sp, #76	; 0x4c
 80a5c4a:	a822      	add	r0, sp, #136	; 0x88
 80a5c4c:	f7ff fd3a 	bl	80a56c4 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a5c50:	e08e      	b.n	80a5d70 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x45c>
 80a5c52:	f7fc fe5e 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a5c56:	6873      	ldr	r3, [r6, #4]
 80a5c58:	4629      	mov	r1, r5
 80a5c5a:	a818      	add	r0, sp, #96	; 0x60
 80a5c5c:	9305      	str	r3, [sp, #20]
 80a5c5e:	f7fc fe58 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5c62:	4621      	mov	r1, r4
 80a5c64:	4640      	mov	r0, r8
 80a5c66:	f8d5 b004 	ldr.w	fp, [r5, #4]
 80a5c6a:	f7fc fe52 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a5c6e:	b104      	cbz	r4, 80a5c72 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x35e>
 80a5c70:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a5c72:	9b23      	ldr	r3, [sp, #140]	; 0x8c
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a5c74:	aa1d      	add	r2, sp, #116	; 0x74
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a5c76:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a5c78:	9b24      	ldr	r3, [sp, #144]	; 0x90
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a5c7a:	a918      	add	r1, sp, #96	; 0x60
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a5c7c:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
 80a5c7e:	9b25      	ldr	r3, [sp, #148]	; 0x94
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a5c80:	a813      	add	r0, sp, #76	; 0x4c
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
 80a5c82:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
 80a5c84:	9b26      	ldr	r3, [sp, #152]	; 0x98
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
 80a5c86:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
 80a5c88:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
 80a5c8a:	9b27      	ldr	r3, [sp, #156]	; 0x9c
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5c8c:	f04f 0800 	mov.w	r8, #0
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
 80a5c90:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;
 80a5c92:	9b28      	ldr	r3, [sp, #160]	; 0xa0

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5c94:	f04f 0900 	mov.w	r9, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a5c98:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a5c9a:	f7fc fc1c 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a5c9e:	4602      	mov	r2, r0
 80a5ca0:	17c3      	asrs	r3, r0, #31
 80a5ca2:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5ca6:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
 80a5caa:	4590      	cmp	r8, r2
 80a5cac:	eb79 0303 	sbcs.w	r3, r9, r3
 80a5cb0:	f280 80b0 	bge.w	80a5e14 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x500>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a5cb4:	f81b 5008 	ldrb.w	r5, [fp, r8]
 80a5cb8:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a5cba:	9a08      	ldr	r2, [sp, #32]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a5cbc:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a5cbe:	9b05      	ldr	r3, [sp, #20]
 80a5cc0:	9907      	ldr	r1, [sp, #28]
 80a5cc2:	f813 0008 	ldrb.w	r0, [r3, r8]
 80a5cc6:	9b06      	ldr	r3, [sp, #24]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a5cc8:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a5cca:	4418      	add	r0, r3
 80a5ccc:	40b8      	lsls	r0, r7
 80a5cce:	f7fc fc3f 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a5cd2:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a5cd4:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a5cd6:	990a      	ldr	r1, [sp, #40]	; 0x28
 80a5cd8:	4628      	mov	r0, r5
 80a5cda:	f7fc fc39 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
 80a5cde:	ebba 0000 	subs.w	r0, sl, r0
 80a5ce2:	bf18      	it	ne
 80a5ce4:	2001      	movne	r0, #1
 80a5ce6:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5cea:	f118 0801 	adds.w	r8, r8, #1
 80a5cee:	f149 0900 	adc.w	r9, r9, #0
 80a5cf2:	e7d8      	b.n	80a5ca6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x392>
 80a5cf4:	6933      	ldr	r3, [r6, #16]
 80a5cf6:	68f0      	ldr	r0, [r6, #12]
 80a5cf8:	f1c3 0900 	rsb	r9, r3, #0
 80a5cfc:	692b      	ldr	r3, [r5, #16]
 80a5cfe:	f1c3 0800 	rsb	r8, r3, #0
 80a5d02:	f00c ffdf 	bl	80b2cc4 <__aeabi_f2d>
 80a5d06:	ab10      	add	r3, sp, #64	; 0x40
 80a5d08:	aa0f      	add	r2, sp, #60	; 0x3c
 80a5d0a:	f009 ff59 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a5d0e:	68e8      	ldr	r0, [r5, #12]
 80a5d10:	f00c ffd8 	bl	80b2cc4 <__aeabi_f2d>
 80a5d14:	ab12      	add	r3, sp, #72	; 0x48
 80a5d16:	aa11      	add	r2, sp, #68	; 0x44
 80a5d18:	f009 ff52 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a5d1c:	2308      	movs	r3, #8
 80a5d1e:	9322      	str	r3, [sp, #136]	; 0x88
 80a5d20:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80a5d22:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
 80a5d26:	9324      	str	r3, [sp, #144]	; 0x90
 80a5d28:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80a5d2a:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
 80a5d2e:	9325      	str	r3, [sp, #148]	; 0x94
 80a5d30:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80a5d32:	f10d 0874 	add.w	r8, sp, #116	; 0x74
 80a5d36:	9327      	str	r3, [sp, #156]	; 0x9c
 80a5d38:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80a5d3a:	4631      	mov	r1, r6
 80a5d3c:	9328      	str	r3, [sp, #160]	; 0xa0
 80a5d3e:	a813      	add	r0, sp, #76	; 0x4c
 80a5d40:	b1c7      	cbz	r7, 80a5d74 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x460>
 80a5d42:	f7fc fde6 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5d46:	4629      	mov	r1, r5
 80a5d48:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a5d4a:	6876      	ldr	r6, [r6, #4]
 80a5d4c:	f7fc fde1 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5d50:	4621      	mov	r1, r4
 80a5d52:	4640      	mov	r0, r8
 80a5d54:	686d      	ldr	r5, [r5, #4]
 80a5d56:	f7fc fddc 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a5d5a:	b104      	cbz	r4, 80a5d5e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x44a>
 80a5d5c:	6864      	ldr	r4, [r4, #4]
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
 80a5d5e:	9402      	str	r4, [sp, #8]
 80a5d60:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a5d64:	ab18      	add	r3, sp, #96	; 0x60
 80a5d66:	4632      	mov	r2, r6
 80a5d68:	a913      	add	r1, sp, #76	; 0x4c
 80a5d6a:	a822      	add	r0, sp, #136	; 0x88
 80a5d6c:	f7ff fd3e 	bl	80a57ec <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a5d70:	4640      	mov	r0, r8
 80a5d72:	e050      	b.n	80a5e16 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x502>
 80a5d74:	f7fc fdcd 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a5d78:	6873      	ldr	r3, [r6, #4]
 80a5d7a:	4629      	mov	r1, r5
 80a5d7c:	a818      	add	r0, sp, #96	; 0x60
 80a5d7e:	9305      	str	r3, [sp, #20]
 80a5d80:	f7fc fdc7 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a5d84:	4621      	mov	r1, r4
 80a5d86:	4640      	mov	r0, r8
 80a5d88:	f8d5 b004 	ldr.w	fp, [r5, #4]
 80a5d8c:	f7fc fdc1 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a5d90:	b104      	cbz	r4, 80a5d94 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x480>
 80a5d92:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a5d94:	9b23      	ldr	r3, [sp, #140]	; 0x8c
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a5d96:	aa1d      	add	r2, sp, #116	; 0x74
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a5d98:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a5d9a:	9b24      	ldr	r3, [sp, #144]	; 0x90
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a5d9c:	a918      	add	r1, sp, #96	; 0x60
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a5d9e:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
 80a5da0:	9b25      	ldr	r3, [sp, #148]	; 0x94
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a5da2:	a813      	add	r0, sp, #76	; 0x4c
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
 80a5da4:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
 80a5da6:	9b26      	ldr	r3, [sp, #152]	; 0x98
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
 80a5da8:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
 80a5daa:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
 80a5dac:	9b27      	ldr	r3, [sp, #156]	; 0x9c
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5dae:	f04f 0800 	mov.w	r8, #0
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
 80a5db2:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;
 80a5db4:	9b28      	ldr	r3, [sp, #160]	; 0xa0

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5db6:	f04f 0900 	mov.w	r9, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a5dba:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a5dbc:	f7fc fb8b 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a5dc0:	4602      	mov	r2, r0
 80a5dc2:	17c3      	asrs	r3, r0, #31
 80a5dc4:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5dc8:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
 80a5dcc:	4590      	cmp	r8, r2
 80a5dce:	eb79 0303 	sbcs.w	r3, r9, r3
 80a5dd2:	da1f      	bge.n	80a5e14 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x500>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a5dd4:	f91b 5008 	ldrsb.w	r5, [fp, r8]
 80a5dd8:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a5dda:	9a08      	ldr	r2, [sp, #32]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a5ddc:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a5dde:	9b05      	ldr	r3, [sp, #20]
 80a5de0:	9907      	ldr	r1, [sp, #28]
 80a5de2:	f913 0008 	ldrsb.w	r0, [r3, r8]
 80a5de6:	9b06      	ldr	r3, [sp, #24]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a5de8:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a5dea:	4418      	add	r0, r3
 80a5dec:	40b8      	lsls	r0, r7
 80a5dee:	f7fc fbaf 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a5df2:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a5df4:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a5df6:	990a      	ldr	r1, [sp, #40]	; 0x28
 80a5df8:	4628      	mov	r0, r5
 80a5dfa:	f7fc fba9 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
 80a5dfe:	ebba 0000 	subs.w	r0, sl, r0
 80a5e02:	bf18      	it	ne
 80a5e04:	2001      	movne	r0, #1
 80a5e06:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a5e0a:	f118 0801 	adds.w	r8, r8, #1
 80a5e0e:	f149 0900 	adc.w	r9, r9, #0
 80a5e12:	e7d9      	b.n	80a5dc8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x4b4>
 80a5e14:	a81d      	add	r0, sp, #116	; 0x74
 80a5e16:	f7fc facc 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a5e1a:	a818      	add	r0, sp, #96	; 0x60
 80a5e1c:	f7fc fac9 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a5e20:	a813      	add	r0, sp, #76	; 0x4c
 80a5e22:	f7fc fac6 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(
          context, "Does not support type %d, requires bool|float|int|uint8",
          input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
 80a5e26:	2000      	movs	r0, #0
 80a5e28:	e00e      	b.n	80a5e48 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x534>
                                    requires_broadcast);
      break;
    default:
      context->ReportError(
          context, "Does not support type %d, requires bool|float|int|uint8",
          input1->type);
 80a5e2a:	4650      	mov	r0, sl
 80a5e2c:	f8da 3014 	ldr.w	r3, [sl, #20]
 80a5e30:	4907      	ldr	r1, [pc, #28]	; (80a5e50 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x53c>)
 80a5e32:	4798      	blx	r3
      return kTfLiteError;
 80a5e34:	2001      	movs	r0, #1
 80a5e36:	e007      	b.n	80a5e48 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x534>
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, NotEqual, requires_broadcast);
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, NotEqual, requires_broadcast);
 80a5e38:	a822      	add	r0, sp, #136	; 0x88
 80a5e3a:	f7fc faba 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a5e3e:	4640      	mov	r0, r8
 80a5e40:	f7fc fab7 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a5e44:	a818      	add	r0, sp, #96	; 0x60
 80a5e46:	e7ec      	b.n	80a5e22 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x50e>
          context, "Does not support type %d, requires bool|float|int|uint8",
          input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
 80a5e48:	b02b      	add	sp, #172	; 0xac
 80a5e4a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80a5e4e:	bf00      	nop
 80a5e50:	080b5c9f 	.word	0x080b5c9f

080a5e54 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a5e54:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a5e58:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a5e5a:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a5e5c:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a5e5e:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a5e60:	9208      	str	r2, [sp, #32]
 80a5e62:	4604      	mov	r4, r0
 80a5e64:	460e      	mov	r6, r1
 80a5e66:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a5e68:	dd01      	ble.n	80a5e6e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
 80a5e6a:	f00a f8e1 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a5e6e:	683b      	ldr	r3, [r7, #0]
 80a5e70:	2b04      	cmp	r3, #4
 80a5e72:	dcfa      	bgt.n	80a5e6a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a5e74:	6813      	ldr	r3, [r2, #0]
 80a5e76:	2b04      	cmp	r3, #4
 80a5e78:	dcf7      	bgt.n	80a5e6a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
 80a5e7a:	2301      	movs	r3, #1
 80a5e7c:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a5e7e:	ad10      	add	r5, sp, #64	; 0x40
 80a5e80:	a80b      	add	r0, sp, #44	; 0x2c
 80a5e82:	f7fc fada 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a5e86:	ab18      	add	r3, sp, #96	; 0x60
 80a5e88:	462a      	mov	r2, r5
 80a5e8a:	4639      	mov	r1, r7
 80a5e8c:	4630      	mov	r0, r6
 80a5e8e:	f7fc fdd7 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a5e92:	6863      	ldr	r3, [r4, #4]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
 80a5e94:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
 80a5e98:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a5e9a:	68a3      	ldr	r3, [r4, #8]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a5e9c:	9509      	str	r5, [sp, #36]	; 0x24
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a5e9e:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
 80a5ea0:	68e3      	ldr	r3, [r4, #12]
 80a5ea2:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
 80a5ea4:	6923      	ldr	r3, [r4, #16]
 80a5ea6:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
 80a5ea8:	6963      	ldr	r3, [r4, #20]
 80a5eaa:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
 80a5eac:	69a3      	ldr	r3, [r4, #24]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a5eae:	2400      	movs	r4, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a5eb0:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a5eb2:	2100      	movs	r1, #0
 80a5eb4:	a80b      	add	r0, sp, #44	; 0x2c
 80a5eb6:	f7fc fa87 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a5eba:	4284      	cmp	r4, r0
 80a5ebc:	da59      	bge.n	80a5f72 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
 80a5ebe:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a5ec0:	af0b      	add	r7, sp, #44	; 0x2c
 80a5ec2:	2101      	movs	r1, #1
 80a5ec4:	4638      	mov	r0, r7
 80a5ec6:	f7fc fa7f 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a5eca:	4285      	cmp	r5, r0
 80a5ecc:	da4f      	bge.n	80a5f6e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
 80a5ece:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a5ed0:	2102      	movs	r1, #2
 80a5ed2:	4638      	mov	r0, r7
 80a5ed4:	f7fc fa78 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a5ed8:	4286      	cmp	r6, r0
 80a5eda:	da46      	bge.n	80a5f6a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
 80a5edc:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a5ee0:	2103      	movs	r1, #3
 80a5ee2:	4638      	mov	r0, r7
 80a5ee4:	f7fc fa70 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a5ee8:	4580      	cmp	r8, r0
 80a5eea:	da3c      	bge.n	80a5f66 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a5eec:	f8cd 8000 	str.w	r8, [sp]
 80a5ef0:	4633      	mov	r3, r6
 80a5ef2:	462a      	mov	r2, r5
 80a5ef4:	4621      	mov	r1, r4
 80a5ef6:	9809      	ldr	r0, [sp, #36]	; 0x24
 80a5ef8:	f7fc fb7c 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
 80a5efc:	9b08      	ldr	r3, [sp, #32]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a5efe:	462a      	mov	r2, r5
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a5f00:	f813 9000 	ldrb.w	r9, [r3, r0]
 80a5f04:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a5f06:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a5f0a:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a5f0c:	4621      	mov	r1, r4
 80a5f0e:	4633      	mov	r3, r6
 80a5f10:	a818      	add	r0, sp, #96	; 0x60
 80a5f12:	f7fc fb6f 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a5f16:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a5f18:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a5f1a:	f813 b000 	ldrb.w	fp, [r3, r0]
 80a5f1e:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a5f20:	9903      	ldr	r1, [sp, #12]
 80a5f22:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a5f26:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a5f28:	f7fc fb12 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a5f2c:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a5f30:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
 80a5f32:	9a07      	ldr	r2, [sp, #28]
 80a5f34:	9906      	ldr	r1, [sp, #24]
 80a5f36:	4658      	mov	r0, fp
 80a5f38:	f7fc fb0a 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
 80a5f3c:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
 80a5f3e:	f8cd 8000 	str.w	r8, [sp]
 80a5f42:	4633      	mov	r3, r6
 80a5f44:	462a      	mov	r2, r5
 80a5f46:	4621      	mov	r1, r4
 80a5f48:	4638      	mov	r0, r7
 80a5f4a:	f7fc faa2 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80a5f4e:	45d9      	cmp	r9, fp
 80a5f50:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
 80a5f52:	bfd4      	ite	le
 80a5f54:	f04f 0900 	movle.w	r9, #0
 80a5f58:	f04f 0901 	movgt.w	r9, #1
 80a5f5c:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a5f60:	f108 0801 	add.w	r8, r8, #1
 80a5f64:	e7bc      	b.n	80a5ee0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a5f66:	3601      	adds	r6, #1
 80a5f68:	e7b2      	b.n	80a5ed0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a5f6a:	3501      	adds	r5, #1
 80a5f6c:	e7a8      	b.n	80a5ec0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a5f6e:	3401      	adds	r4, #1
 80a5f70:	e79f      	b.n	80a5eb2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a5f72:	a80b      	add	r0, sp, #44	; 0x2c
 80a5f74:	f7fc fa1d 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
 80a5f78:	b021      	add	sp, #132	; 0x84
 80a5f7a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a5f7e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a5f7e:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a5f82:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a5f84:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a5f86:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a5f88:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a5f8a:	9208      	str	r2, [sp, #32]
 80a5f8c:	4604      	mov	r4, r0
 80a5f8e:	460e      	mov	r6, r1
 80a5f90:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a5f92:	dd01      	ble.n	80a5f98 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
 80a5f94:	f00a f84c 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a5f98:	683b      	ldr	r3, [r7, #0]
 80a5f9a:	2b04      	cmp	r3, #4
 80a5f9c:	dcfa      	bgt.n	80a5f94 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a5f9e:	6813      	ldr	r3, [r2, #0]
 80a5fa0:	2b04      	cmp	r3, #4
 80a5fa2:	dcf7      	bgt.n	80a5f94 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
 80a5fa4:	2301      	movs	r3, #1
 80a5fa6:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a5fa8:	ad10      	add	r5, sp, #64	; 0x40
 80a5faa:	a80b      	add	r0, sp, #44	; 0x2c
 80a5fac:	f7fc fa45 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a5fb0:	ab18      	add	r3, sp, #96	; 0x60
 80a5fb2:	462a      	mov	r2, r5
 80a5fb4:	4639      	mov	r1, r7
 80a5fb6:	4630      	mov	r0, r6
 80a5fb8:	f7fc fd42 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a5fbc:	6863      	ldr	r3, [r4, #4]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
 80a5fbe:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
 80a5fc2:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a5fc4:	68a3      	ldr	r3, [r4, #8]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a5fc6:	9509      	str	r5, [sp, #36]	; 0x24
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a5fc8:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
 80a5fca:	68e3      	ldr	r3, [r4, #12]
 80a5fcc:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
 80a5fce:	6923      	ldr	r3, [r4, #16]
 80a5fd0:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
 80a5fd2:	6963      	ldr	r3, [r4, #20]
 80a5fd4:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
 80a5fd6:	69a3      	ldr	r3, [r4, #24]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a5fd8:	2400      	movs	r4, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a5fda:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a5fdc:	2100      	movs	r1, #0
 80a5fde:	a80b      	add	r0, sp, #44	; 0x2c
 80a5fe0:	f7fc f9f2 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a5fe4:	4284      	cmp	r4, r0
 80a5fe6:	da59      	bge.n	80a609c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
 80a5fe8:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a5fea:	af0b      	add	r7, sp, #44	; 0x2c
 80a5fec:	2101      	movs	r1, #1
 80a5fee:	4638      	mov	r0, r7
 80a5ff0:	f7fc f9ea 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a5ff4:	4285      	cmp	r5, r0
 80a5ff6:	da4f      	bge.n	80a6098 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
 80a5ff8:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a5ffa:	2102      	movs	r1, #2
 80a5ffc:	4638      	mov	r0, r7
 80a5ffe:	f7fc f9e3 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a6002:	4286      	cmp	r6, r0
 80a6004:	da46      	bge.n	80a6094 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
 80a6006:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a600a:	2103      	movs	r1, #3
 80a600c:	4638      	mov	r0, r7
 80a600e:	f7fc f9db 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a6012:	4580      	cmp	r8, r0
 80a6014:	da3c      	bge.n	80a6090 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a6016:	f8cd 8000 	str.w	r8, [sp]
 80a601a:	4633      	mov	r3, r6
 80a601c:	462a      	mov	r2, r5
 80a601e:	4621      	mov	r1, r4
 80a6020:	9809      	ldr	r0, [sp, #36]	; 0x24
 80a6022:	f7fc fae7 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
 80a6026:	9b08      	ldr	r3, [sp, #32]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a6028:	462a      	mov	r2, r5
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a602a:	f913 9000 	ldrsb.w	r9, [r3, r0]
 80a602e:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a6030:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a6034:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a6036:	4621      	mov	r1, r4
 80a6038:	4633      	mov	r3, r6
 80a603a:	a818      	add	r0, sp, #96	; 0x60
 80a603c:	f7fc fada 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6040:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a6042:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6044:	f913 b000 	ldrsb.w	fp, [r3, r0]
 80a6048:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a604a:	9903      	ldr	r1, [sp, #12]
 80a604c:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6050:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a6052:	f7fc fa7d 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6056:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a605a:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
 80a605c:	9a07      	ldr	r2, [sp, #28]
 80a605e:	9906      	ldr	r1, [sp, #24]
 80a6060:	4658      	mov	r0, fp
 80a6062:	f7fc fa75 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
 80a6066:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
 80a6068:	f8cd 8000 	str.w	r8, [sp]
 80a606c:	4633      	mov	r3, r6
 80a606e:	462a      	mov	r2, r5
 80a6070:	4621      	mov	r1, r4
 80a6072:	4638      	mov	r0, r7
 80a6074:	f7fc fa0d 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80a6078:	45d9      	cmp	r9, fp
 80a607a:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
 80a607c:	bfd4      	ite	le
 80a607e:	f04f 0900 	movle.w	r9, #0
 80a6082:	f04f 0901 	movgt.w	r9, #1
 80a6086:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a608a:	f108 0801 	add.w	r8, r8, #1
 80a608e:	e7bc      	b.n	80a600a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a6090:	3601      	adds	r6, #1
 80a6092:	e7b2      	b.n	80a5ffa <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a6094:	3501      	adds	r5, #1
 80a6096:	e7a8      	b.n	80a5fea <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a6098:	3401      	adds	r4, #1
 80a609a:	e79f      	b.n	80a5fdc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a609c:	a80b      	add	r0, sp, #44	; 0x2c
 80a609e:	f7fc f988 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
 80a60a2:	b021      	add	sp, #132	; 0x84
 80a60a4:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a60a8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus GreaterEval(TfLiteContext* context, TfLiteNode* node) {
 80a60a8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a60ac:	680a      	ldr	r2, [r1, #0]
 80a60ae:	f8d0 9008 	ldr.w	r9, [r0, #8]
 80a60b2:	4682      	mov	sl, r0
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a60b4:	6850      	ldr	r0, [r2, #4]
 80a60b6:	2338      	movs	r3, #56	; 0x38
 80a60b8:	6895      	ldr	r5, [r2, #8]
 80a60ba:	fb03 f800 	mul.w	r8, r3, r0
 80a60be:	fb03 9505 	mla	r5, r3, r5, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a60c2:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a60c4:	eb09 0608 	add.w	r6, r9, r8
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a60c8:	6854      	ldr	r4, [r2, #4]
 80a60ca:	b0ab      	sub	sp, #172	; 0xac
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
 80a60cc:	4629      	mov	r1, r5
 80a60ce:	4630      	mov	r0, r6
 80a60d0:	fb03 9404 	mla	r4, r3, r4, r9
 80a60d4:	f009 fd1a 	bl	80afb0c <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
  switch (input1->type) {
 80a60d8:	f819 2008 	ldrb.w	r2, [r9, r8]

TfLiteStatus GreaterEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
 80a60dc:	f080 0001 	eor.w	r0, r0, #1
  switch (input1->type) {
 80a60e0:	1e53      	subs	r3, r2, #1

TfLiteStatus GreaterEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
 80a60e2:	b2c7      	uxtb	r7, r0
  switch (input1->type) {
 80a60e4:	2b08      	cmp	r3, #8
 80a60e6:	f200 8220 	bhi.w	80a652a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x482>
 80a60ea:	e8df f013 	tbh	[pc, r3, lsl #1]
 80a60ee:	0009      	.short	0x0009
 80a60f0:	00f3005a 	.word	0x00f3005a
 80a60f4:	021e00a4 	.word	0x021e00a4
 80a60f8:	021e021e 	.word	0x021e021e
 80a60fc:	0183021e 	.word	0x0183021e
 80a6100:	f10d 0874 	add.w	r8, sp, #116	; 0x74
    case kTfLiteFloat32:
      TF_LITE_COMPARISON(float, Greater, requires_broadcast);
 80a6104:	4631      	mov	r1, r6
 80a6106:	b1cf      	cbz	r7, 80a613c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x94>
 80a6108:	a813      	add	r0, sp, #76	; 0x4c
 80a610a:	f7fc fc02 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a610e:	4629      	mov	r1, r5
 80a6110:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a6112:	6876      	ldr	r6, [r6, #4]
 80a6114:	f7fc fbfd 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6118:	b105      	cbz	r5, 80a611c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x74>
 80a611a:	686d      	ldr	r5, [r5, #4]
 80a611c:	4621      	mov	r1, r4
 80a611e:	4640      	mov	r0, r8
 80a6120:	f7fc fbf7 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a6124:	b104      	cbz	r4, 80a6128 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x80>
 80a6126:	6864      	ldr	r4, [r4, #4]
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
 80a6128:	9402      	str	r4, [sp, #8]
 80a612a:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a612e:	ab18      	add	r3, sp, #96	; 0x60
 80a6130:	4632      	mov	r2, r6
 80a6132:	a913      	add	r1, sp, #76	; 0x4c
 80a6134:	a822      	add	r0, sp, #136	; 0x88
 80a6136:	f7fe f9d0 	bl	80a44da <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a613a:	e199      	b.n	80a6470 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3c8>
 80a613c:	a818      	add	r0, sp, #96	; 0x60
 80a613e:	f7fc fbe8 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a6142:	6873      	ldr	r3, [r6, #4]
 80a6144:	4629      	mov	r1, r5
 80a6146:	4640      	mov	r0, r8
 80a6148:	9305      	str	r3, [sp, #20]
 80a614a:	f7fc fbe2 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a614e:	b105      	cbz	r5, 80a6152 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0xaa>
 80a6150:	686d      	ldr	r5, [r5, #4]
 80a6152:	4621      	mov	r1, r4
 80a6154:	a822      	add	r0, sp, #136	; 0x88
 80a6156:	f7fc fbdc 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a615a:	b104      	cbz	r4, 80a615e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0xb6>
 80a615c:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a615e:	aa22      	add	r2, sp, #136	; 0x88
 80a6160:	4641      	mov	r1, r8
 80a6162:	a818      	add	r0, sp, #96	; 0x60
 80a6164:	f7fc f9b7 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a6168:	4682      	mov	sl, r0
 80a616a:	ea4f 7be0 	mov.w	fp, r0, asr #31
 80a616e:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
 80a6170:	2600      	movs	r6, #0
 80a6172:	2700      	movs	r7, #0
 80a6174:	4556      	cmp	r6, sl
 80a6176:	eb77 030b 	sbcs.w	r3, r7, fp
 80a617a:	f280 81dd 	bge.w	80a6538 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x490>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a617e:	9a05      	ldr	r2, [sp, #20]
 80a6180:	f855 1026 	ldr.w	r1, [r5, r6, lsl #2]
 80a6184:	f852 0026 	ldr.w	r0, [r2, r6, lsl #2]
 80a6188:	3401      	adds	r4, #1
 80a618a:	f04f 0901 	mov.w	r9, #1
 80a618e:	f00d fbe9 	bl	80b3964 <__aeabi_fcmpgt>
 80a6192:	b900      	cbnz	r0, 80a6196 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0xee>
 80a6194:	4681      	mov	r9, r0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a6196:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a6198:	f884 9000 	strb.w	r9, [r4]
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a619c:	f147 0700 	adc.w	r7, r7, #0
 80a61a0:	e7e8      	b.n	80a6174 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0xcc>
 80a61a2:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, Greater, requires_broadcast);
 80a61a6:	4631      	mov	r1, r6
 80a61a8:	b1cf      	cbz	r7, 80a61de <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x136>
 80a61aa:	a813      	add	r0, sp, #76	; 0x4c
 80a61ac:	f7fc fbb1 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a61b0:	4629      	mov	r1, r5
 80a61b2:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a61b4:	6876      	ldr	r6, [r6, #4]
 80a61b6:	f7fc fbac 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a61ba:	b105      	cbz	r5, 80a61be <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x116>
 80a61bc:	686d      	ldr	r5, [r5, #4]
 80a61be:	4621      	mov	r1, r4
 80a61c0:	4640      	mov	r0, r8
 80a61c2:	f7fc fba6 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a61c6:	b104      	cbz	r4, 80a61ca <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x122>
 80a61c8:	6864      	ldr	r4, [r4, #4]
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
 80a61ca:	9402      	str	r4, [sp, #8]
 80a61cc:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a61d0:	ab18      	add	r3, sp, #96	; 0x60
 80a61d2:	4632      	mov	r2, r6
 80a61d4:	a913      	add	r1, sp, #76	; 0x4c
 80a61d6:	a822      	add	r0, sp, #136	; 0x88
 80a61d8:	f7fe f9eb 	bl	80a45b2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a61dc:	e148      	b.n	80a6470 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3c8>
 80a61de:	a818      	add	r0, sp, #96	; 0x60
 80a61e0:	f7fc fb97 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a61e4:	4629      	mov	r1, r5
 80a61e6:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a61e8:	6876      	ldr	r6, [r6, #4]
 80a61ea:	f7fc fb92 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a61ee:	b105      	cbz	r5, 80a61f2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x14a>
 80a61f0:	686d      	ldr	r5, [r5, #4]
 80a61f2:	4621      	mov	r1, r4
 80a61f4:	a822      	add	r0, sp, #136	; 0x88
 80a61f6:	f7fc fb8c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a61fa:	b104      	cbz	r4, 80a61fe <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x156>
 80a61fc:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a61fe:	aa22      	add	r2, sp, #136	; 0x88
 80a6200:	4641      	mov	r1, r8
 80a6202:	a818      	add	r0, sp, #96	; 0x60
 80a6204:	f7fc f967 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a6208:	3c01      	subs	r4, #1
 80a620a:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
 80a620c:	2200      	movs	r2, #0
 80a620e:	2300      	movs	r3, #0
 80a6210:	4282      	cmp	r2, r0
 80a6212:	eb73 0701 	sbcs.w	r7, r3, r1
 80a6216:	f280 818f 	bge.w	80a6538 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x490>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a621a:	f856 7022 	ldr.w	r7, [r6, r2, lsl #2]
 80a621e:	f855 e022 	ldr.w	lr, [r5, r2, lsl #2]
 80a6222:	4577      	cmp	r7, lr
 80a6224:	bfd4      	ite	le
 80a6226:	2700      	movle	r7, #0
 80a6228:	2701      	movgt	r7, #1
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a622a:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a622c:	f804 7f01 	strb.w	r7, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a6230:	f143 0300 	adc.w	r3, r3, #0
 80a6234:	e7ec      	b.n	80a6210 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x168>
 80a6236:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, Greater, requires_broadcast);
 80a623a:	4631      	mov	r1, r6
 80a623c:	b1cf      	cbz	r7, 80a6272 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x1ca>
 80a623e:	a813      	add	r0, sp, #76	; 0x4c
 80a6240:	f7fc fb67 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6244:	4629      	mov	r1, r5
 80a6246:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a6248:	6876      	ldr	r6, [r6, #4]
 80a624a:	f7fc fb62 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a624e:	b105      	cbz	r5, 80a6252 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x1aa>
 80a6250:	686d      	ldr	r5, [r5, #4]
 80a6252:	4621      	mov	r1, r4
 80a6254:	4640      	mov	r0, r8
 80a6256:	f7fc fb5c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a625a:	b104      	cbz	r4, 80a625e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x1b6>
 80a625c:	6864      	ldr	r4, [r4, #4]
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
 80a625e:	9402      	str	r4, [sp, #8]
 80a6260:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a6264:	ab18      	add	r3, sp, #96	; 0x60
 80a6266:	4632      	mov	r2, r6
 80a6268:	a913      	add	r1, sp, #76	; 0x4c
 80a626a:	a822      	add	r0, sp, #136	; 0x88
 80a626c:	f7fe fa0d 	bl	80a468a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a6270:	e0fe      	b.n	80a6470 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3c8>
 80a6272:	a818      	add	r0, sp, #96	; 0x60
 80a6274:	f7fc fb4d 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6278:	4629      	mov	r1, r5
 80a627a:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a627c:	6876      	ldr	r6, [r6, #4]
 80a627e:	f7fc fb48 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6282:	b105      	cbz	r5, 80a6286 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x1de>
 80a6284:	686d      	ldr	r5, [r5, #4]
 80a6286:	4621      	mov	r1, r4
 80a6288:	a822      	add	r0, sp, #136	; 0x88
 80a628a:	f7fc fb42 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a628e:	b104      	cbz	r4, 80a6292 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x1ea>
 80a6290:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a6292:	aa22      	add	r2, sp, #136	; 0x88
 80a6294:	4641      	mov	r1, r8
 80a6296:	a818      	add	r0, sp, #96	; 0x60
 80a6298:	f7fc f91d 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a629c:	3d08      	subs	r5, #8
 80a629e:	17c1      	asrs	r1, r0, #31
 80a62a0:	f1a6 0e08 	sub.w	lr, r6, #8
 80a62a4:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
 80a62a6:	2200      	movs	r2, #0
 80a62a8:	2300      	movs	r3, #0
 80a62aa:	4282      	cmp	r2, r0
 80a62ac:	eb73 0601 	sbcs.w	r6, r3, r1
 80a62b0:	f280 8142 	bge.w	80a6538 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x490>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a62b4:	e9fe 6702 	ldrd	r6, r7, [lr, #8]!
 80a62b8:	e9f5 ab02 	ldrd	sl, fp, [r5, #8]!
 80a62bc:	45b2      	cmp	sl, r6
 80a62be:	eb7b 0607 	sbcs.w	r6, fp, r7
 80a62c2:	bfb4      	ite	lt
 80a62c4:	2601      	movlt	r6, #1
 80a62c6:	2600      	movge	r6, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a62c8:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a62ca:	f804 6f01 	strb.w	r6, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a62ce:	f143 0300 	adc.w	r3, r3, #0
 80a62d2:	e7ea      	b.n	80a62aa <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x202>
      }                                                                        \
    }                                                                          \
  }
TF_LITE_QUANTIZE_COMPARISON(Equal);
TF_LITE_QUANTIZE_COMPARISON(NotEqual);
TF_LITE_QUANTIZE_COMPARISON(Greater);
 80a62d4:	6933      	ldr	r3, [r6, #16]
 80a62d6:	68f0      	ldr	r0, [r6, #12]
 80a62d8:	f1c3 0900 	rsb	r9, r3, #0
 80a62dc:	692b      	ldr	r3, [r5, #16]
 80a62de:	f1c3 0800 	rsb	r8, r3, #0
 80a62e2:	f00c fcef 	bl	80b2cc4 <__aeabi_f2d>
 80a62e6:	ab10      	add	r3, sp, #64	; 0x40
 80a62e8:	aa0f      	add	r2, sp, #60	; 0x3c
 80a62ea:	f009 fc69 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a62ee:	68e8      	ldr	r0, [r5, #12]
 80a62f0:	f00c fce8 	bl	80b2cc4 <__aeabi_f2d>
 80a62f4:	ab12      	add	r3, sp, #72	; 0x48
 80a62f6:	aa11      	add	r2, sp, #68	; 0x44
 80a62f8:	f009 fc62 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a62fc:	2308      	movs	r3, #8
 80a62fe:	9322      	str	r3, [sp, #136]	; 0x88
 80a6300:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80a6302:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
 80a6306:	9324      	str	r3, [sp, #144]	; 0x90
 80a6308:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80a630a:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
 80a630e:	9325      	str	r3, [sp, #148]	; 0x94
 80a6310:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80a6312:	f10d 0874 	add.w	r8, sp, #116	; 0x74
 80a6316:	9327      	str	r3, [sp, #156]	; 0x9c
 80a6318:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80a631a:	4631      	mov	r1, r6
 80a631c:	9328      	str	r3, [sp, #160]	; 0xa0
 80a631e:	a813      	add	r0, sp, #76	; 0x4c
 80a6320:	b1bf      	cbz	r7, 80a6352 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x2aa>
 80a6322:	f7fc faf6 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6326:	4629      	mov	r1, r5
 80a6328:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a632a:	6876      	ldr	r6, [r6, #4]
 80a632c:	f7fc faf1 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6330:	4621      	mov	r1, r4
 80a6332:	4640      	mov	r0, r8
 80a6334:	686d      	ldr	r5, [r5, #4]
 80a6336:	f7fc faec 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a633a:	b104      	cbz	r4, 80a633e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x296>
 80a633c:	6864      	ldr	r4, [r4, #4]
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
 80a633e:	9402      	str	r4, [sp, #8]
 80a6340:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a6344:	ab18      	add	r3, sp, #96	; 0x60
 80a6346:	4632      	mov	r2, r6
 80a6348:	a913      	add	r1, sp, #76	; 0x4c
 80a634a:	a822      	add	r0, sp, #136	; 0x88
 80a634c:	f7ff fd82 	bl	80a5e54 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a6350:	e08e      	b.n	80a6470 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3c8>
 80a6352:	f7fc fade 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a6356:	6873      	ldr	r3, [r6, #4]
 80a6358:	4629      	mov	r1, r5
 80a635a:	a818      	add	r0, sp, #96	; 0x60
 80a635c:	9305      	str	r3, [sp, #20]
 80a635e:	f7fc fad8 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6362:	4621      	mov	r1, r4
 80a6364:	4640      	mov	r0, r8
 80a6366:	f8d5 b004 	ldr.w	fp, [r5, #4]
 80a636a:	f7fc fad2 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a636e:	b104      	cbz	r4, 80a6372 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x2ca>
 80a6370:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a6372:	9b23      	ldr	r3, [sp, #140]	; 0x8c
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a6374:	aa1d      	add	r2, sp, #116	; 0x74
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a6376:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a6378:	9b24      	ldr	r3, [sp, #144]	; 0x90
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a637a:	a918      	add	r1, sp, #96	; 0x60
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a637c:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
 80a637e:	9b25      	ldr	r3, [sp, #148]	; 0x94
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a6380:	a813      	add	r0, sp, #76	; 0x4c
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
 80a6382:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
 80a6384:	9b26      	ldr	r3, [sp, #152]	; 0x98
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
 80a6386:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
 80a6388:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
 80a638a:	9b27      	ldr	r3, [sp, #156]	; 0x9c
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a638c:	f04f 0800 	mov.w	r8, #0
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
 80a6390:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;
 80a6392:	9b28      	ldr	r3, [sp, #160]	; 0xa0

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a6394:	f04f 0900 	mov.w	r9, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a6398:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a639a:	f7fc f89c 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a639e:	4602      	mov	r2, r0
 80a63a0:	17c3      	asrs	r3, r0, #31
 80a63a2:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
  for (int64_t i = 0; i < flatsize; ++i) {
 80a63a6:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
 80a63aa:	4590      	cmp	r8, r2
 80a63ac:	eb79 0303 	sbcs.w	r3, r9, r3
 80a63b0:	f280 80b0 	bge.w	80a6514 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x46c>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a63b4:	f81b 5008 	ldrb.w	r5, [fp, r8]
 80a63b8:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a63ba:	9a08      	ldr	r2, [sp, #32]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a63bc:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a63be:	9b05      	ldr	r3, [sp, #20]
 80a63c0:	9907      	ldr	r1, [sp, #28]
 80a63c2:	f813 0008 	ldrb.w	r0, [r3, r8]
 80a63c6:	9b06      	ldr	r3, [sp, #24]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a63c8:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a63ca:	4418      	add	r0, r3
 80a63cc:	40b8      	lsls	r0, r7
 80a63ce:	f7fc f8bf 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a63d2:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a63d4:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a63d6:	990a      	ldr	r1, [sp, #40]	; 0x28
 80a63d8:	4628      	mov	r0, r5
 80a63da:	f7fc f8b9 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
 80a63de:	4582      	cmp	sl, r0
 80a63e0:	bfd4      	ite	le
 80a63e2:	2000      	movle	r0, #0
 80a63e4:	2001      	movgt	r0, #1
 80a63e6:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a63ea:	f118 0801 	adds.w	r8, r8, #1
 80a63ee:	f149 0900 	adc.w	r9, r9, #0
 80a63f2:	e7d8      	b.n	80a63a6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x2fe>
 80a63f4:	6933      	ldr	r3, [r6, #16]
 80a63f6:	68f0      	ldr	r0, [r6, #12]
 80a63f8:	f1c3 0900 	rsb	r9, r3, #0
 80a63fc:	692b      	ldr	r3, [r5, #16]
 80a63fe:	f1c3 0800 	rsb	r8, r3, #0
 80a6402:	f00c fc5f 	bl	80b2cc4 <__aeabi_f2d>
 80a6406:	ab10      	add	r3, sp, #64	; 0x40
 80a6408:	aa0f      	add	r2, sp, #60	; 0x3c
 80a640a:	f009 fbd9 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a640e:	68e8      	ldr	r0, [r5, #12]
 80a6410:	f00c fc58 	bl	80b2cc4 <__aeabi_f2d>
 80a6414:	ab12      	add	r3, sp, #72	; 0x48
 80a6416:	aa11      	add	r2, sp, #68	; 0x44
 80a6418:	f009 fbd2 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a641c:	2308      	movs	r3, #8
 80a641e:	9322      	str	r3, [sp, #136]	; 0x88
 80a6420:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80a6422:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
 80a6426:	9324      	str	r3, [sp, #144]	; 0x90
 80a6428:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80a642a:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
 80a642e:	9325      	str	r3, [sp, #148]	; 0x94
 80a6430:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80a6432:	f10d 0874 	add.w	r8, sp, #116	; 0x74
 80a6436:	9327      	str	r3, [sp, #156]	; 0x9c
 80a6438:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80a643a:	4631      	mov	r1, r6
 80a643c:	9328      	str	r3, [sp, #160]	; 0xa0
 80a643e:	a813      	add	r0, sp, #76	; 0x4c
 80a6440:	b1c7      	cbz	r7, 80a6474 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3cc>
 80a6442:	f7fc fa66 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6446:	4629      	mov	r1, r5
 80a6448:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a644a:	6876      	ldr	r6, [r6, #4]
 80a644c:	f7fc fa61 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6450:	4621      	mov	r1, r4
 80a6452:	4640      	mov	r0, r8
 80a6454:	686d      	ldr	r5, [r5, #4]
 80a6456:	f7fc fa5c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a645a:	b104      	cbz	r4, 80a645e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3b6>
 80a645c:	6864      	ldr	r4, [r4, #4]
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
 80a645e:	9402      	str	r4, [sp, #8]
 80a6460:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a6464:	ab18      	add	r3, sp, #96	; 0x60
 80a6466:	4632      	mov	r2, r6
 80a6468:	a913      	add	r1, sp, #76	; 0x4c
 80a646a:	a822      	add	r0, sp, #136	; 0x88
 80a646c:	f7ff fd87 	bl	80a5f7e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a6470:	4640      	mov	r0, r8
 80a6472:	e050      	b.n	80a6516 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x46e>
 80a6474:	f7fc fa4d 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a6478:	6873      	ldr	r3, [r6, #4]
 80a647a:	4629      	mov	r1, r5
 80a647c:	a818      	add	r0, sp, #96	; 0x60
 80a647e:	9305      	str	r3, [sp, #20]
 80a6480:	f7fc fa47 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6484:	4621      	mov	r1, r4
 80a6486:	4640      	mov	r0, r8
 80a6488:	f8d5 b004 	ldr.w	fp, [r5, #4]
 80a648c:	f7fc fa41 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a6490:	b104      	cbz	r4, 80a6494 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3ec>
 80a6492:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a6494:	9b23      	ldr	r3, [sp, #140]	; 0x8c
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a6496:	aa1d      	add	r2, sp, #116	; 0x74
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a6498:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a649a:	9b24      	ldr	r3, [sp, #144]	; 0x90
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a649c:	a918      	add	r1, sp, #96	; 0x60
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a649e:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
 80a64a0:	9b25      	ldr	r3, [sp, #148]	; 0x94
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a64a2:	a813      	add	r0, sp, #76	; 0x4c
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
 80a64a4:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
 80a64a6:	9b26      	ldr	r3, [sp, #152]	; 0x98
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
 80a64a8:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
 80a64aa:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
 80a64ac:	9b27      	ldr	r3, [sp, #156]	; 0x9c
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a64ae:	f04f 0800 	mov.w	r8, #0
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
 80a64b2:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;
 80a64b4:	9b28      	ldr	r3, [sp, #160]	; 0xa0

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a64b6:	f04f 0900 	mov.w	r9, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a64ba:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a64bc:	f7fc f80b 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a64c0:	4602      	mov	r2, r0
 80a64c2:	17c3      	asrs	r3, r0, #31
 80a64c4:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
  for (int64_t i = 0; i < flatsize; ++i) {
 80a64c8:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
 80a64cc:	4590      	cmp	r8, r2
 80a64ce:	eb79 0303 	sbcs.w	r3, r9, r3
 80a64d2:	da1f      	bge.n	80a6514 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x46c>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a64d4:	f91b 5008 	ldrsb.w	r5, [fp, r8]
 80a64d8:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a64da:	9a08      	ldr	r2, [sp, #32]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a64dc:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a64de:	9b05      	ldr	r3, [sp, #20]
 80a64e0:	9907      	ldr	r1, [sp, #28]
 80a64e2:	f913 0008 	ldrsb.w	r0, [r3, r8]
 80a64e6:	9b06      	ldr	r3, [sp, #24]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a64e8:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a64ea:	4418      	add	r0, r3
 80a64ec:	40b8      	lsls	r0, r7
 80a64ee:	f7fc f82f 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a64f2:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a64f4:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a64f6:	990a      	ldr	r1, [sp, #40]	; 0x28
 80a64f8:	4628      	mov	r0, r5
 80a64fa:	f7fc f829 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
 80a64fe:	4582      	cmp	sl, r0
 80a6500:	bfd4      	ite	le
 80a6502:	2000      	movle	r0, #0
 80a6504:	2001      	movgt	r0, #1
 80a6506:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a650a:	f118 0801 	adds.w	r8, r8, #1
 80a650e:	f149 0900 	adc.w	r9, r9, #0
 80a6512:	e7d9      	b.n	80a64c8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x420>
 80a6514:	a81d      	add	r0, sp, #116	; 0x74
 80a6516:	f7fb ff4c 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a651a:	a818      	add	r0, sp, #96	; 0x60
 80a651c:	f7fb ff49 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a6520:	a813      	add	r0, sp, #76	; 0x4c
 80a6522:	f7fb ff46 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
 80a6526:	2000      	movs	r0, #0
 80a6528:	e00e      	b.n	80a6548 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x4a0>
                                   requires_broadcast);
      break;
    default:
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
 80a652a:	4650      	mov	r0, sl
 80a652c:	f8da 3014 	ldr.w	r3, [sl, #20]
 80a6530:	4907      	ldr	r1, [pc, #28]	; (80a6550 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x4a8>)
 80a6532:	4798      	blx	r3
      return kTfLiteError;
 80a6534:	2001      	movs	r0, #1
 80a6536:	e007      	b.n	80a6548 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x4a0>
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, Greater, requires_broadcast);
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, Greater, requires_broadcast);
 80a6538:	a822      	add	r0, sp, #136	; 0x88
 80a653a:	f7fb ff3a 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a653e:	4640      	mov	r0, r8
 80a6540:	f7fb ff37 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a6544:	a818      	add	r0, sp, #96	; 0x60
 80a6546:	e7ec      	b.n	80a6522 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x47a>
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
 80a6548:	b02b      	add	sp, #172	; 0xac
 80a654a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80a654e:	bf00      	nop
 80a6550:	080b5cd7 	.word	0x080b5cd7

080a6554 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a6554:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a6558:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a655a:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a655c:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a655e:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a6560:	9208      	str	r2, [sp, #32]
 80a6562:	4604      	mov	r4, r0
 80a6564:	460e      	mov	r6, r1
 80a6566:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a6568:	dd01      	ble.n	80a656e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
 80a656a:	f009 fd61 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a656e:	683b      	ldr	r3, [r7, #0]
 80a6570:	2b04      	cmp	r3, #4
 80a6572:	dcfa      	bgt.n	80a656a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a6574:	6813      	ldr	r3, [r2, #0]
 80a6576:	2b04      	cmp	r3, #4
 80a6578:	dcf7      	bgt.n	80a656a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
 80a657a:	2301      	movs	r3, #1
 80a657c:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a657e:	ad10      	add	r5, sp, #64	; 0x40
 80a6580:	a80b      	add	r0, sp, #44	; 0x2c
 80a6582:	f7fb ff5a 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a6586:	ab18      	add	r3, sp, #96	; 0x60
 80a6588:	462a      	mov	r2, r5
 80a658a:	4639      	mov	r1, r7
 80a658c:	4630      	mov	r0, r6
 80a658e:	f7fc fa57 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a6592:	6863      	ldr	r3, [r4, #4]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
 80a6594:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
 80a6598:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a659a:	68a3      	ldr	r3, [r4, #8]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a659c:	9509      	str	r5, [sp, #36]	; 0x24
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a659e:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
 80a65a0:	68e3      	ldr	r3, [r4, #12]
 80a65a2:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
 80a65a4:	6923      	ldr	r3, [r4, #16]
 80a65a6:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
 80a65a8:	6963      	ldr	r3, [r4, #20]
 80a65aa:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
 80a65ac:	69a3      	ldr	r3, [r4, #24]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a65ae:	2400      	movs	r4, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a65b0:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a65b2:	2100      	movs	r1, #0
 80a65b4:	a80b      	add	r0, sp, #44	; 0x2c
 80a65b6:	f7fb ff07 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a65ba:	4284      	cmp	r4, r0
 80a65bc:	da59      	bge.n	80a6672 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
 80a65be:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a65c0:	af0b      	add	r7, sp, #44	; 0x2c
 80a65c2:	2101      	movs	r1, #1
 80a65c4:	4638      	mov	r0, r7
 80a65c6:	f7fb feff 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a65ca:	4285      	cmp	r5, r0
 80a65cc:	da4f      	bge.n	80a666e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
 80a65ce:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a65d0:	2102      	movs	r1, #2
 80a65d2:	4638      	mov	r0, r7
 80a65d4:	f7fb fef8 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a65d8:	4286      	cmp	r6, r0
 80a65da:	da46      	bge.n	80a666a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
 80a65dc:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a65e0:	2103      	movs	r1, #3
 80a65e2:	4638      	mov	r0, r7
 80a65e4:	f7fb fef0 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a65e8:	4580      	cmp	r8, r0
 80a65ea:	da3c      	bge.n	80a6666 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a65ec:	f8cd 8000 	str.w	r8, [sp]
 80a65f0:	4633      	mov	r3, r6
 80a65f2:	462a      	mov	r2, r5
 80a65f4:	4621      	mov	r1, r4
 80a65f6:	9809      	ldr	r0, [sp, #36]	; 0x24
 80a65f8:	f7fb fffc 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
 80a65fc:	9b08      	ldr	r3, [sp, #32]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a65fe:	462a      	mov	r2, r5
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a6600:	f813 9000 	ldrb.w	r9, [r3, r0]
 80a6604:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a6606:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a660a:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a660c:	4621      	mov	r1, r4
 80a660e:	4633      	mov	r3, r6
 80a6610:	a818      	add	r0, sp, #96	; 0x60
 80a6612:	f7fb ffef 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6616:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a6618:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a661a:	f813 b000 	ldrb.w	fp, [r3, r0]
 80a661e:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a6620:	9903      	ldr	r1, [sp, #12]
 80a6622:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6626:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a6628:	f7fb ff92 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a662c:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a6630:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
 80a6632:	9a07      	ldr	r2, [sp, #28]
 80a6634:	9906      	ldr	r1, [sp, #24]
 80a6636:	4658      	mov	r0, fp
 80a6638:	f7fb ff8a 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
 80a663c:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
 80a663e:	f8cd 8000 	str.w	r8, [sp]
 80a6642:	4633      	mov	r3, r6
 80a6644:	462a      	mov	r2, r5
 80a6646:	4621      	mov	r1, r4
 80a6648:	4638      	mov	r0, r7
 80a664a:	f7fb ff22 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80a664e:	45d9      	cmp	r9, fp
 80a6650:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
 80a6652:	bfb4      	ite	lt
 80a6654:	f04f 0900 	movlt.w	r9, #0
 80a6658:	f04f 0901 	movge.w	r9, #1
 80a665c:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a6660:	f108 0801 	add.w	r8, r8, #1
 80a6664:	e7bc      	b.n	80a65e0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a6666:	3601      	adds	r6, #1
 80a6668:	e7b2      	b.n	80a65d0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a666a:	3501      	adds	r5, #1
 80a666c:	e7a8      	b.n	80a65c0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a666e:	3401      	adds	r4, #1
 80a6670:	e79f      	b.n	80a65b2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a6672:	a80b      	add	r0, sp, #44	; 0x2c
 80a6674:	f7fb fe9d 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
 80a6678:	b021      	add	sp, #132	; 0x84
 80a667a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a667e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a667e:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a6682:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a6684:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a6686:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a6688:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a668a:	9208      	str	r2, [sp, #32]
 80a668c:	4604      	mov	r4, r0
 80a668e:	460e      	mov	r6, r1
 80a6690:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a6692:	dd01      	ble.n	80a6698 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
 80a6694:	f009 fccc 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a6698:	683b      	ldr	r3, [r7, #0]
 80a669a:	2b04      	cmp	r3, #4
 80a669c:	dcfa      	bgt.n	80a6694 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a669e:	6813      	ldr	r3, [r2, #0]
 80a66a0:	2b04      	cmp	r3, #4
 80a66a2:	dcf7      	bgt.n	80a6694 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
 80a66a4:	2301      	movs	r3, #1
 80a66a6:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a66a8:	ad10      	add	r5, sp, #64	; 0x40
 80a66aa:	a80b      	add	r0, sp, #44	; 0x2c
 80a66ac:	f7fb fec5 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a66b0:	ab18      	add	r3, sp, #96	; 0x60
 80a66b2:	462a      	mov	r2, r5
 80a66b4:	4639      	mov	r1, r7
 80a66b6:	4630      	mov	r0, r6
 80a66b8:	f7fc f9c2 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a66bc:	6863      	ldr	r3, [r4, #4]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
 80a66be:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
 80a66c2:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a66c4:	68a3      	ldr	r3, [r4, #8]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a66c6:	9509      	str	r5, [sp, #36]	; 0x24
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a66c8:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
 80a66ca:	68e3      	ldr	r3, [r4, #12]
 80a66cc:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
 80a66ce:	6923      	ldr	r3, [r4, #16]
 80a66d0:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
 80a66d2:	6963      	ldr	r3, [r4, #20]
 80a66d4:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
 80a66d6:	69a3      	ldr	r3, [r4, #24]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a66d8:	2400      	movs	r4, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a66da:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a66dc:	2100      	movs	r1, #0
 80a66de:	a80b      	add	r0, sp, #44	; 0x2c
 80a66e0:	f7fb fe72 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a66e4:	4284      	cmp	r4, r0
 80a66e6:	da59      	bge.n	80a679c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
 80a66e8:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a66ea:	af0b      	add	r7, sp, #44	; 0x2c
 80a66ec:	2101      	movs	r1, #1
 80a66ee:	4638      	mov	r0, r7
 80a66f0:	f7fb fe6a 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a66f4:	4285      	cmp	r5, r0
 80a66f6:	da4f      	bge.n	80a6798 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
 80a66f8:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a66fa:	2102      	movs	r1, #2
 80a66fc:	4638      	mov	r0, r7
 80a66fe:	f7fb fe63 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a6702:	4286      	cmp	r6, r0
 80a6704:	da46      	bge.n	80a6794 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
 80a6706:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a670a:	2103      	movs	r1, #3
 80a670c:	4638      	mov	r0, r7
 80a670e:	f7fb fe5b 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a6712:	4580      	cmp	r8, r0
 80a6714:	da3c      	bge.n	80a6790 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a6716:	f8cd 8000 	str.w	r8, [sp]
 80a671a:	4633      	mov	r3, r6
 80a671c:	462a      	mov	r2, r5
 80a671e:	4621      	mov	r1, r4
 80a6720:	9809      	ldr	r0, [sp, #36]	; 0x24
 80a6722:	f7fb ff67 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
 80a6726:	9b08      	ldr	r3, [sp, #32]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a6728:	462a      	mov	r2, r5
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a672a:	f913 9000 	ldrsb.w	r9, [r3, r0]
 80a672e:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a6730:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a6734:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a6736:	4621      	mov	r1, r4
 80a6738:	4633      	mov	r3, r6
 80a673a:	a818      	add	r0, sp, #96	; 0x60
 80a673c:	f7fb ff5a 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6740:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a6742:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6744:	f913 b000 	ldrsb.w	fp, [r3, r0]
 80a6748:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a674a:	9903      	ldr	r1, [sp, #12]
 80a674c:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6750:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a6752:	f7fb fefd 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6756:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a675a:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
 80a675c:	9a07      	ldr	r2, [sp, #28]
 80a675e:	9906      	ldr	r1, [sp, #24]
 80a6760:	4658      	mov	r0, fp
 80a6762:	f7fb fef5 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
 80a6766:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
 80a6768:	f8cd 8000 	str.w	r8, [sp]
 80a676c:	4633      	mov	r3, r6
 80a676e:	462a      	mov	r2, r5
 80a6770:	4621      	mov	r1, r4
 80a6772:	4638      	mov	r0, r7
 80a6774:	f7fb fe8d 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80a6778:	45d9      	cmp	r9, fp
 80a677a:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
 80a677c:	bfb4      	ite	lt
 80a677e:	f04f 0900 	movlt.w	r9, #0
 80a6782:	f04f 0901 	movge.w	r9, #1
 80a6786:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a678a:	f108 0801 	add.w	r8, r8, #1
 80a678e:	e7bc      	b.n	80a670a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a6790:	3601      	adds	r6, #1
 80a6792:	e7b2      	b.n	80a66fa <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a6794:	3501      	adds	r5, #1
 80a6796:	e7a8      	b.n	80a66ea <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a6798:	3401      	adds	r4, #1
 80a679a:	e79f      	b.n	80a66dc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a679c:	a80b      	add	r0, sp, #44	; 0x2c
 80a679e:	f7fb fe08 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
 80a67a2:	b021      	add	sp, #132	; 0x84
 80a67a4:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a67a8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus GreaterEqualEval(TfLiteContext* context, TfLiteNode* node) {
 80a67a8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a67ac:	680a      	ldr	r2, [r1, #0]
 80a67ae:	f8d0 9008 	ldr.w	r9, [r0, #8]
 80a67b2:	4682      	mov	sl, r0
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a67b4:	6850      	ldr	r0, [r2, #4]
 80a67b6:	2338      	movs	r3, #56	; 0x38
 80a67b8:	6895      	ldr	r5, [r2, #8]
 80a67ba:	fb03 f800 	mul.w	r8, r3, r0
 80a67be:	fb03 9505 	mla	r5, r3, r5, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a67c2:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a67c4:	eb09 0608 	add.w	r6, r9, r8
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a67c8:	6854      	ldr	r4, [r2, #4]
 80a67ca:	b0ab      	sub	sp, #172	; 0xac
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
 80a67cc:	4629      	mov	r1, r5
 80a67ce:	4630      	mov	r0, r6
 80a67d0:	fb03 9404 	mla	r4, r3, r4, r9
 80a67d4:	f009 f99a 	bl	80afb0c <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
  switch (input1->type) {
 80a67d8:	f819 2008 	ldrb.w	r2, [r9, r8]

TfLiteStatus GreaterEqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
 80a67dc:	f080 0001 	eor.w	r0, r0, #1
  switch (input1->type) {
 80a67e0:	1e53      	subs	r3, r2, #1

TfLiteStatus GreaterEqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
 80a67e2:	b2c7      	uxtb	r7, r0
  switch (input1->type) {
 80a67e4:	2b08      	cmp	r3, #8
 80a67e6:	f200 8220 	bhi.w	80a6c2a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x482>
 80a67ea:	e8df f013 	tbh	[pc, r3, lsl #1]
 80a67ee:	0009      	.short	0x0009
 80a67f0:	00f3005a 	.word	0x00f3005a
 80a67f4:	021e00a4 	.word	0x021e00a4
 80a67f8:	021e021e 	.word	0x021e021e
 80a67fc:	0183021e 	.word	0x0183021e
 80a6800:	f10d 0874 	add.w	r8, sp, #116	; 0x74
    case kTfLiteFloat32:
      TF_LITE_COMPARISON(float, GreaterEqual, requires_broadcast);
 80a6804:	4631      	mov	r1, r6
 80a6806:	b1cf      	cbz	r7, 80a683c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x94>
 80a6808:	a813      	add	r0, sp, #76	; 0x4c
 80a680a:	f7fc f882 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a680e:	4629      	mov	r1, r5
 80a6810:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a6812:	6876      	ldr	r6, [r6, #4]
 80a6814:	f7fc f87d 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6818:	b105      	cbz	r5, 80a681c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x74>
 80a681a:	686d      	ldr	r5, [r5, #4]
 80a681c:	4621      	mov	r1, r4
 80a681e:	4640      	mov	r0, r8
 80a6820:	f7fc f877 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a6824:	b104      	cbz	r4, 80a6828 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x80>
 80a6826:	6864      	ldr	r4, [r4, #4]
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
 80a6828:	9402      	str	r4, [sp, #8]
 80a682a:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a682e:	ab18      	add	r3, sp, #96	; 0x60
 80a6830:	4632      	mov	r2, r6
 80a6832:	a913      	add	r1, sp, #76	; 0x4c
 80a6834:	a822      	add	r0, sp, #136	; 0x88
 80a6836:	f7fd ff99 	bl	80a476c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a683a:	e199      	b.n	80a6b70 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3c8>
 80a683c:	a818      	add	r0, sp, #96	; 0x60
 80a683e:	f7fc f868 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a6842:	6873      	ldr	r3, [r6, #4]
 80a6844:	4629      	mov	r1, r5
 80a6846:	4640      	mov	r0, r8
 80a6848:	9305      	str	r3, [sp, #20]
 80a684a:	f7fc f862 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a684e:	b105      	cbz	r5, 80a6852 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0xaa>
 80a6850:	686d      	ldr	r5, [r5, #4]
 80a6852:	4621      	mov	r1, r4
 80a6854:	a822      	add	r0, sp, #136	; 0x88
 80a6856:	f7fc f85c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a685a:	b104      	cbz	r4, 80a685e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0xb6>
 80a685c:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a685e:	aa22      	add	r2, sp, #136	; 0x88
 80a6860:	4641      	mov	r1, r8
 80a6862:	a818      	add	r0, sp, #96	; 0x60
 80a6864:	f7fb fe37 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a6868:	4682      	mov	sl, r0
 80a686a:	ea4f 7be0 	mov.w	fp, r0, asr #31
 80a686e:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
 80a6870:	2600      	movs	r6, #0
 80a6872:	2700      	movs	r7, #0
 80a6874:	4556      	cmp	r6, sl
 80a6876:	eb77 030b 	sbcs.w	r3, r7, fp
 80a687a:	f280 81dd 	bge.w	80a6c38 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x490>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a687e:	9a05      	ldr	r2, [sp, #20]
 80a6880:	f855 1026 	ldr.w	r1, [r5, r6, lsl #2]
 80a6884:	f852 0026 	ldr.w	r0, [r2, r6, lsl #2]
 80a6888:	3401      	adds	r4, #1
 80a688a:	f04f 0901 	mov.w	r9, #1
 80a688e:	f00d f85f 	bl	80b3950 <__aeabi_fcmpge>
 80a6892:	b900      	cbnz	r0, 80a6896 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0xee>
 80a6894:	4681      	mov	r9, r0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a6896:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a6898:	f884 9000 	strb.w	r9, [r4]
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a689c:	f147 0700 	adc.w	r7, r7, #0
 80a68a0:	e7e8      	b.n	80a6874 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0xcc>
 80a68a2:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, GreaterEqual, requires_broadcast);
 80a68a6:	4631      	mov	r1, r6
 80a68a8:	b1cf      	cbz	r7, 80a68de <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x136>
 80a68aa:	a813      	add	r0, sp, #76	; 0x4c
 80a68ac:	f7fc f831 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a68b0:	4629      	mov	r1, r5
 80a68b2:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a68b4:	6876      	ldr	r6, [r6, #4]
 80a68b6:	f7fc f82c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a68ba:	b105      	cbz	r5, 80a68be <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x116>
 80a68bc:	686d      	ldr	r5, [r5, #4]
 80a68be:	4621      	mov	r1, r4
 80a68c0:	4640      	mov	r0, r8
 80a68c2:	f7fc f826 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a68c6:	b104      	cbz	r4, 80a68ca <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x122>
 80a68c8:	6864      	ldr	r4, [r4, #4]
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
 80a68ca:	9402      	str	r4, [sp, #8]
 80a68cc:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a68d0:	ab18      	add	r3, sp, #96	; 0x60
 80a68d2:	4632      	mov	r2, r6
 80a68d4:	a913      	add	r1, sp, #76	; 0x4c
 80a68d6:	a822      	add	r0, sp, #136	; 0x88
 80a68d8:	f7fd ffb4 	bl	80a4844 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a68dc:	e148      	b.n	80a6b70 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3c8>
 80a68de:	a818      	add	r0, sp, #96	; 0x60
 80a68e0:	f7fc f817 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a68e4:	4629      	mov	r1, r5
 80a68e6:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a68e8:	6876      	ldr	r6, [r6, #4]
 80a68ea:	f7fc f812 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a68ee:	b105      	cbz	r5, 80a68f2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x14a>
 80a68f0:	686d      	ldr	r5, [r5, #4]
 80a68f2:	4621      	mov	r1, r4
 80a68f4:	a822      	add	r0, sp, #136	; 0x88
 80a68f6:	f7fc f80c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a68fa:	b104      	cbz	r4, 80a68fe <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x156>
 80a68fc:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a68fe:	aa22      	add	r2, sp, #136	; 0x88
 80a6900:	4641      	mov	r1, r8
 80a6902:	a818      	add	r0, sp, #96	; 0x60
 80a6904:	f7fb fde7 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a6908:	3c01      	subs	r4, #1
 80a690a:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
 80a690c:	2200      	movs	r2, #0
 80a690e:	2300      	movs	r3, #0
 80a6910:	4282      	cmp	r2, r0
 80a6912:	eb73 0701 	sbcs.w	r7, r3, r1
 80a6916:	f280 818f 	bge.w	80a6c38 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x490>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a691a:	f856 7022 	ldr.w	r7, [r6, r2, lsl #2]
 80a691e:	f855 e022 	ldr.w	lr, [r5, r2, lsl #2]
 80a6922:	4577      	cmp	r7, lr
 80a6924:	bfb4      	ite	lt
 80a6926:	2700      	movlt	r7, #0
 80a6928:	2701      	movge	r7, #1
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a692a:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a692c:	f804 7f01 	strb.w	r7, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a6930:	f143 0300 	adc.w	r3, r3, #0
 80a6934:	e7ec      	b.n	80a6910 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x168>
 80a6936:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, GreaterEqual, requires_broadcast);
 80a693a:	4631      	mov	r1, r6
 80a693c:	b1cf      	cbz	r7, 80a6972 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x1ca>
 80a693e:	a813      	add	r0, sp, #76	; 0x4c
 80a6940:	f7fb ffe7 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6944:	4629      	mov	r1, r5
 80a6946:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a6948:	6876      	ldr	r6, [r6, #4]
 80a694a:	f7fb ffe2 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a694e:	b105      	cbz	r5, 80a6952 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x1aa>
 80a6950:	686d      	ldr	r5, [r5, #4]
 80a6952:	4621      	mov	r1, r4
 80a6954:	4640      	mov	r0, r8
 80a6956:	f7fb ffdc 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a695a:	b104      	cbz	r4, 80a695e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x1b6>
 80a695c:	6864      	ldr	r4, [r4, #4]
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
 80a695e:	9402      	str	r4, [sp, #8]
 80a6960:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a6964:	ab18      	add	r3, sp, #96	; 0x60
 80a6966:	4632      	mov	r2, r6
 80a6968:	a913      	add	r1, sp, #76	; 0x4c
 80a696a:	a822      	add	r0, sp, #136	; 0x88
 80a696c:	f7fd ffd6 	bl	80a491c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a6970:	e0fe      	b.n	80a6b70 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3c8>
 80a6972:	a818      	add	r0, sp, #96	; 0x60
 80a6974:	f7fb ffcd 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6978:	4629      	mov	r1, r5
 80a697a:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a697c:	6876      	ldr	r6, [r6, #4]
 80a697e:	f7fb ffc8 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6982:	b105      	cbz	r5, 80a6986 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x1de>
 80a6984:	686d      	ldr	r5, [r5, #4]
 80a6986:	4621      	mov	r1, r4
 80a6988:	a822      	add	r0, sp, #136	; 0x88
 80a698a:	f7fb ffc2 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a698e:	b104      	cbz	r4, 80a6992 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x1ea>
 80a6990:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a6992:	aa22      	add	r2, sp, #136	; 0x88
 80a6994:	4641      	mov	r1, r8
 80a6996:	a818      	add	r0, sp, #96	; 0x60
 80a6998:	f7fb fd9d 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a699c:	3d08      	subs	r5, #8
 80a699e:	17c1      	asrs	r1, r0, #31
 80a69a0:	f1a6 0e08 	sub.w	lr, r6, #8
 80a69a4:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
 80a69a6:	2200      	movs	r2, #0
 80a69a8:	2300      	movs	r3, #0
 80a69aa:	4282      	cmp	r2, r0
 80a69ac:	eb73 0601 	sbcs.w	r6, r3, r1
 80a69b0:	f280 8142 	bge.w	80a6c38 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x490>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a69b4:	e9fe ab02 	ldrd	sl, fp, [lr, #8]!
 80a69b8:	e9f5 6702 	ldrd	r6, r7, [r5, #8]!
 80a69bc:	45b2      	cmp	sl, r6
 80a69be:	eb7b 0607 	sbcs.w	r6, fp, r7
 80a69c2:	bfac      	ite	ge
 80a69c4:	2601      	movge	r6, #1
 80a69c6:	2600      	movlt	r6, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a69c8:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a69ca:	f804 6f01 	strb.w	r6, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a69ce:	f143 0300 	adc.w	r3, r3, #0
 80a69d2:	e7ea      	b.n	80a69aa <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x202>
    }                                                                          \
  }
TF_LITE_QUANTIZE_COMPARISON(Equal);
TF_LITE_QUANTIZE_COMPARISON(NotEqual);
TF_LITE_QUANTIZE_COMPARISON(Greater);
TF_LITE_QUANTIZE_COMPARISON(GreaterEqual);
 80a69d4:	6933      	ldr	r3, [r6, #16]
 80a69d6:	68f0      	ldr	r0, [r6, #12]
 80a69d8:	f1c3 0900 	rsb	r9, r3, #0
 80a69dc:	692b      	ldr	r3, [r5, #16]
 80a69de:	f1c3 0800 	rsb	r8, r3, #0
 80a69e2:	f00c f96f 	bl	80b2cc4 <__aeabi_f2d>
 80a69e6:	ab10      	add	r3, sp, #64	; 0x40
 80a69e8:	aa0f      	add	r2, sp, #60	; 0x3c
 80a69ea:	f009 f8e9 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a69ee:	68e8      	ldr	r0, [r5, #12]
 80a69f0:	f00c f968 	bl	80b2cc4 <__aeabi_f2d>
 80a69f4:	ab12      	add	r3, sp, #72	; 0x48
 80a69f6:	aa11      	add	r2, sp, #68	; 0x44
 80a69f8:	f009 f8e2 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a69fc:	2308      	movs	r3, #8
 80a69fe:	9322      	str	r3, [sp, #136]	; 0x88
 80a6a00:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80a6a02:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
 80a6a06:	9324      	str	r3, [sp, #144]	; 0x90
 80a6a08:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80a6a0a:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
 80a6a0e:	9325      	str	r3, [sp, #148]	; 0x94
 80a6a10:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80a6a12:	f10d 0874 	add.w	r8, sp, #116	; 0x74
 80a6a16:	9327      	str	r3, [sp, #156]	; 0x9c
 80a6a18:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80a6a1a:	4631      	mov	r1, r6
 80a6a1c:	9328      	str	r3, [sp, #160]	; 0xa0
 80a6a1e:	a813      	add	r0, sp, #76	; 0x4c
 80a6a20:	b1bf      	cbz	r7, 80a6a52 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x2aa>
 80a6a22:	f7fb ff76 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6a26:	4629      	mov	r1, r5
 80a6a28:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a6a2a:	6876      	ldr	r6, [r6, #4]
 80a6a2c:	f7fb ff71 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6a30:	4621      	mov	r1, r4
 80a6a32:	4640      	mov	r0, r8
 80a6a34:	686d      	ldr	r5, [r5, #4]
 80a6a36:	f7fb ff6c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a6a3a:	b104      	cbz	r4, 80a6a3e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x296>
 80a6a3c:	6864      	ldr	r4, [r4, #4]
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
 80a6a3e:	9402      	str	r4, [sp, #8]
 80a6a40:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a6a44:	ab18      	add	r3, sp, #96	; 0x60
 80a6a46:	4632      	mov	r2, r6
 80a6a48:	a913      	add	r1, sp, #76	; 0x4c
 80a6a4a:	a822      	add	r0, sp, #136	; 0x88
 80a6a4c:	f7ff fd82 	bl	80a6554 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a6a50:	e08e      	b.n	80a6b70 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3c8>
 80a6a52:	f7fb ff5e 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a6a56:	6873      	ldr	r3, [r6, #4]
 80a6a58:	4629      	mov	r1, r5
 80a6a5a:	a818      	add	r0, sp, #96	; 0x60
 80a6a5c:	9305      	str	r3, [sp, #20]
 80a6a5e:	f7fb ff58 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6a62:	4621      	mov	r1, r4
 80a6a64:	4640      	mov	r0, r8
 80a6a66:	f8d5 b004 	ldr.w	fp, [r5, #4]
 80a6a6a:	f7fb ff52 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a6a6e:	b104      	cbz	r4, 80a6a72 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x2ca>
 80a6a70:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a6a72:	9b23      	ldr	r3, [sp, #140]	; 0x8c
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a6a74:	aa1d      	add	r2, sp, #116	; 0x74
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a6a76:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a6a78:	9b24      	ldr	r3, [sp, #144]	; 0x90
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a6a7a:	a918      	add	r1, sp, #96	; 0x60
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a6a7c:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
 80a6a7e:	9b25      	ldr	r3, [sp, #148]	; 0x94
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a6a80:	a813      	add	r0, sp, #76	; 0x4c
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
 80a6a82:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
 80a6a84:	9b26      	ldr	r3, [sp, #152]	; 0x98
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
 80a6a86:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
 80a6a88:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
 80a6a8a:	9b27      	ldr	r3, [sp, #156]	; 0x9c
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a6a8c:	f04f 0800 	mov.w	r8, #0
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
 80a6a90:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;
 80a6a92:	9b28      	ldr	r3, [sp, #160]	; 0xa0

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a6a94:	f04f 0900 	mov.w	r9, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a6a98:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a6a9a:	f7fb fd1c 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a6a9e:	4602      	mov	r2, r0
 80a6aa0:	17c3      	asrs	r3, r0, #31
 80a6aa2:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
  for (int64_t i = 0; i < flatsize; ++i) {
 80a6aa6:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
 80a6aaa:	4590      	cmp	r8, r2
 80a6aac:	eb79 0303 	sbcs.w	r3, r9, r3
 80a6ab0:	f280 80b0 	bge.w	80a6c14 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x46c>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6ab4:	f81b 5008 	ldrb.w	r5, [fp, r8]
 80a6ab8:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a6aba:	9a08      	ldr	r2, [sp, #32]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6abc:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a6abe:	9b05      	ldr	r3, [sp, #20]
 80a6ac0:	9907      	ldr	r1, [sp, #28]
 80a6ac2:	f813 0008 	ldrb.w	r0, [r3, r8]
 80a6ac6:	9b06      	ldr	r3, [sp, #24]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6ac8:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a6aca:	4418      	add	r0, r3
 80a6acc:	40b8      	lsls	r0, r7
 80a6ace:	f7fb fd3f 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a6ad2:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a6ad4:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a6ad6:	990a      	ldr	r1, [sp, #40]	; 0x28
 80a6ad8:	4628      	mov	r0, r5
 80a6ada:	f7fb fd39 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
 80a6ade:	4582      	cmp	sl, r0
 80a6ae0:	bfb4      	ite	lt
 80a6ae2:	2000      	movlt	r0, #0
 80a6ae4:	2001      	movge	r0, #1
 80a6ae6:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a6aea:	f118 0801 	adds.w	r8, r8, #1
 80a6aee:	f149 0900 	adc.w	r9, r9, #0
 80a6af2:	e7d8      	b.n	80a6aa6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x2fe>
 80a6af4:	6933      	ldr	r3, [r6, #16]
 80a6af6:	68f0      	ldr	r0, [r6, #12]
 80a6af8:	f1c3 0900 	rsb	r9, r3, #0
 80a6afc:	692b      	ldr	r3, [r5, #16]
 80a6afe:	f1c3 0800 	rsb	r8, r3, #0
 80a6b02:	f00c f8df 	bl	80b2cc4 <__aeabi_f2d>
 80a6b06:	ab10      	add	r3, sp, #64	; 0x40
 80a6b08:	aa0f      	add	r2, sp, #60	; 0x3c
 80a6b0a:	f009 f859 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a6b0e:	68e8      	ldr	r0, [r5, #12]
 80a6b10:	f00c f8d8 	bl	80b2cc4 <__aeabi_f2d>
 80a6b14:	ab12      	add	r3, sp, #72	; 0x48
 80a6b16:	aa11      	add	r2, sp, #68	; 0x44
 80a6b18:	f009 f852 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a6b1c:	2308      	movs	r3, #8
 80a6b1e:	9322      	str	r3, [sp, #136]	; 0x88
 80a6b20:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80a6b22:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
 80a6b26:	9324      	str	r3, [sp, #144]	; 0x90
 80a6b28:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80a6b2a:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
 80a6b2e:	9325      	str	r3, [sp, #148]	; 0x94
 80a6b30:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80a6b32:	f10d 0874 	add.w	r8, sp, #116	; 0x74
 80a6b36:	9327      	str	r3, [sp, #156]	; 0x9c
 80a6b38:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80a6b3a:	4631      	mov	r1, r6
 80a6b3c:	9328      	str	r3, [sp, #160]	; 0xa0
 80a6b3e:	a813      	add	r0, sp, #76	; 0x4c
 80a6b40:	b1c7      	cbz	r7, 80a6b74 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3cc>
 80a6b42:	f7fb fee6 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6b46:	4629      	mov	r1, r5
 80a6b48:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a6b4a:	6876      	ldr	r6, [r6, #4]
 80a6b4c:	f7fb fee1 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6b50:	4621      	mov	r1, r4
 80a6b52:	4640      	mov	r0, r8
 80a6b54:	686d      	ldr	r5, [r5, #4]
 80a6b56:	f7fb fedc 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a6b5a:	b104      	cbz	r4, 80a6b5e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3b6>
 80a6b5c:	6864      	ldr	r4, [r4, #4]
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
 80a6b5e:	9402      	str	r4, [sp, #8]
 80a6b60:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a6b64:	ab18      	add	r3, sp, #96	; 0x60
 80a6b66:	4632      	mov	r2, r6
 80a6b68:	a913      	add	r1, sp, #76	; 0x4c
 80a6b6a:	a822      	add	r0, sp, #136	; 0x88
 80a6b6c:	f7ff fd87 	bl	80a667e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a6b70:	4640      	mov	r0, r8
 80a6b72:	e050      	b.n	80a6c16 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x46e>
 80a6b74:	f7fb fecd 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a6b78:	6873      	ldr	r3, [r6, #4]
 80a6b7a:	4629      	mov	r1, r5
 80a6b7c:	a818      	add	r0, sp, #96	; 0x60
 80a6b7e:	9305      	str	r3, [sp, #20]
 80a6b80:	f7fb fec7 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6b84:	4621      	mov	r1, r4
 80a6b86:	4640      	mov	r0, r8
 80a6b88:	f8d5 b004 	ldr.w	fp, [r5, #4]
 80a6b8c:	f7fb fec1 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a6b90:	b104      	cbz	r4, 80a6b94 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3ec>
 80a6b92:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a6b94:	9b23      	ldr	r3, [sp, #140]	; 0x8c
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a6b96:	aa1d      	add	r2, sp, #116	; 0x74
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a6b98:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a6b9a:	9b24      	ldr	r3, [sp, #144]	; 0x90
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a6b9c:	a918      	add	r1, sp, #96	; 0x60
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a6b9e:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
 80a6ba0:	9b25      	ldr	r3, [sp, #148]	; 0x94
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a6ba2:	a813      	add	r0, sp, #76	; 0x4c
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
 80a6ba4:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
 80a6ba6:	9b26      	ldr	r3, [sp, #152]	; 0x98
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
 80a6ba8:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
 80a6baa:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
 80a6bac:	9b27      	ldr	r3, [sp, #156]	; 0x9c
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a6bae:	f04f 0800 	mov.w	r8, #0
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
 80a6bb2:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;
 80a6bb4:	9b28      	ldr	r3, [sp, #160]	; 0xa0

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a6bb6:	f04f 0900 	mov.w	r9, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a6bba:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a6bbc:	f7fb fc8b 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a6bc0:	4602      	mov	r2, r0
 80a6bc2:	17c3      	asrs	r3, r0, #31
 80a6bc4:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
  for (int64_t i = 0; i < flatsize; ++i) {
 80a6bc8:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
 80a6bcc:	4590      	cmp	r8, r2
 80a6bce:	eb79 0303 	sbcs.w	r3, r9, r3
 80a6bd2:	da1f      	bge.n	80a6c14 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x46c>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6bd4:	f91b 5008 	ldrsb.w	r5, [fp, r8]
 80a6bd8:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a6bda:	9a08      	ldr	r2, [sp, #32]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6bdc:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a6bde:	9b05      	ldr	r3, [sp, #20]
 80a6be0:	9907      	ldr	r1, [sp, #28]
 80a6be2:	f913 0008 	ldrsb.w	r0, [r3, r8]
 80a6be6:	9b06      	ldr	r3, [sp, #24]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6be8:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a6bea:	4418      	add	r0, r3
 80a6bec:	40b8      	lsls	r0, r7
 80a6bee:	f7fb fcaf 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a6bf2:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a6bf4:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a6bf6:	990a      	ldr	r1, [sp, #40]	; 0x28
 80a6bf8:	4628      	mov	r0, r5
 80a6bfa:	f7fb fca9 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
 80a6bfe:	4582      	cmp	sl, r0
 80a6c00:	bfb4      	ite	lt
 80a6c02:	2000      	movlt	r0, #0
 80a6c04:	2001      	movge	r0, #1
 80a6c06:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a6c0a:	f118 0801 	adds.w	r8, r8, #1
 80a6c0e:	f149 0900 	adc.w	r9, r9, #0
 80a6c12:	e7d9      	b.n	80a6bc8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x420>
 80a6c14:	a81d      	add	r0, sp, #116	; 0x74
 80a6c16:	f7fb fbcc 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a6c1a:	a818      	add	r0, sp, #96	; 0x60
 80a6c1c:	f7fb fbc9 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a6c20:	a813      	add	r0, sp, #76	; 0x4c
 80a6c22:	f7fb fbc6 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
 80a6c26:	2000      	movs	r0, #0
 80a6c28:	e00e      	b.n	80a6c48 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x4a0>
                                        requires_broadcast);
      break;
    default:
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
 80a6c2a:	4650      	mov	r0, sl
 80a6c2c:	f8da 3014 	ldr.w	r3, [sl, #20]
 80a6c30:	4907      	ldr	r1, [pc, #28]	; (80a6c50 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x4a8>)
 80a6c32:	4798      	blx	r3
      return kTfLiteError;
 80a6c34:	2001      	movs	r0, #1
 80a6c36:	e007      	b.n	80a6c48 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x4a0>
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, GreaterEqual, requires_broadcast);
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, GreaterEqual, requires_broadcast);
 80a6c38:	a822      	add	r0, sp, #136	; 0x88
 80a6c3a:	f7fb fbba 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a6c3e:	4640      	mov	r0, r8
 80a6c40:	f7fb fbb7 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a6c44:	a818      	add	r0, sp, #96	; 0x60
 80a6c46:	e7ec      	b.n	80a6c22 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x47a>
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
 80a6c48:	b02b      	add	sp, #172	; 0xac
 80a6c4a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80a6c4e:	bf00      	nop
 80a6c50:	080b5cd7 	.word	0x080b5cd7

080a6c54 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a6c54:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a6c58:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a6c5a:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a6c5c:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a6c5e:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a6c60:	9208      	str	r2, [sp, #32]
 80a6c62:	4604      	mov	r4, r0
 80a6c64:	460e      	mov	r6, r1
 80a6c66:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a6c68:	dd01      	ble.n	80a6c6e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
 80a6c6a:	f009 f9e1 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a6c6e:	683b      	ldr	r3, [r7, #0]
 80a6c70:	2b04      	cmp	r3, #4
 80a6c72:	dcfa      	bgt.n	80a6c6a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a6c74:	6813      	ldr	r3, [r2, #0]
 80a6c76:	2b04      	cmp	r3, #4
 80a6c78:	dcf7      	bgt.n	80a6c6a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
 80a6c7a:	2301      	movs	r3, #1
 80a6c7c:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a6c7e:	ad10      	add	r5, sp, #64	; 0x40
 80a6c80:	a80b      	add	r0, sp, #44	; 0x2c
 80a6c82:	f7fb fbda 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a6c86:	ab18      	add	r3, sp, #96	; 0x60
 80a6c88:	462a      	mov	r2, r5
 80a6c8a:	4639      	mov	r1, r7
 80a6c8c:	4630      	mov	r0, r6
 80a6c8e:	f7fb fed7 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a6c92:	6863      	ldr	r3, [r4, #4]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
 80a6c94:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
 80a6c98:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a6c9a:	68a3      	ldr	r3, [r4, #8]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a6c9c:	9509      	str	r5, [sp, #36]	; 0x24
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a6c9e:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
 80a6ca0:	68e3      	ldr	r3, [r4, #12]
 80a6ca2:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
 80a6ca4:	6923      	ldr	r3, [r4, #16]
 80a6ca6:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
 80a6ca8:	6963      	ldr	r3, [r4, #20]
 80a6caa:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
 80a6cac:	69a3      	ldr	r3, [r4, #24]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a6cae:	2400      	movs	r4, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a6cb0:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a6cb2:	2100      	movs	r1, #0
 80a6cb4:	a80b      	add	r0, sp, #44	; 0x2c
 80a6cb6:	f7fb fb87 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a6cba:	4284      	cmp	r4, r0
 80a6cbc:	da59      	bge.n	80a6d72 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
 80a6cbe:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a6cc0:	af0b      	add	r7, sp, #44	; 0x2c
 80a6cc2:	2101      	movs	r1, #1
 80a6cc4:	4638      	mov	r0, r7
 80a6cc6:	f7fb fb7f 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a6cca:	4285      	cmp	r5, r0
 80a6ccc:	da4f      	bge.n	80a6d6e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
 80a6cce:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a6cd0:	2102      	movs	r1, #2
 80a6cd2:	4638      	mov	r0, r7
 80a6cd4:	f7fb fb78 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a6cd8:	4286      	cmp	r6, r0
 80a6cda:	da46      	bge.n	80a6d6a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
 80a6cdc:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a6ce0:	2103      	movs	r1, #3
 80a6ce2:	4638      	mov	r0, r7
 80a6ce4:	f7fb fb70 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a6ce8:	4580      	cmp	r8, r0
 80a6cea:	da3c      	bge.n	80a6d66 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a6cec:	f8cd 8000 	str.w	r8, [sp]
 80a6cf0:	4633      	mov	r3, r6
 80a6cf2:	462a      	mov	r2, r5
 80a6cf4:	4621      	mov	r1, r4
 80a6cf6:	9809      	ldr	r0, [sp, #36]	; 0x24
 80a6cf8:	f7fb fc7c 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
 80a6cfc:	9b08      	ldr	r3, [sp, #32]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a6cfe:	462a      	mov	r2, r5
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a6d00:	f813 9000 	ldrb.w	r9, [r3, r0]
 80a6d04:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a6d06:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a6d0a:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a6d0c:	4621      	mov	r1, r4
 80a6d0e:	4633      	mov	r3, r6
 80a6d10:	a818      	add	r0, sp, #96	; 0x60
 80a6d12:	f7fb fc6f 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6d16:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a6d18:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6d1a:	f813 b000 	ldrb.w	fp, [r3, r0]
 80a6d1e:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a6d20:	9903      	ldr	r1, [sp, #12]
 80a6d22:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6d26:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a6d28:	f7fb fc12 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6d2c:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a6d30:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
 80a6d32:	9a07      	ldr	r2, [sp, #28]
 80a6d34:	9906      	ldr	r1, [sp, #24]
 80a6d36:	4658      	mov	r0, fp
 80a6d38:	f7fb fc0a 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
 80a6d3c:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
 80a6d3e:	f8cd 8000 	str.w	r8, [sp]
 80a6d42:	4633      	mov	r3, r6
 80a6d44:	462a      	mov	r2, r5
 80a6d46:	4621      	mov	r1, r4
 80a6d48:	4638      	mov	r0, r7
 80a6d4a:	f7fb fba2 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80a6d4e:	45d9      	cmp	r9, fp
 80a6d50:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
 80a6d52:	bfac      	ite	ge
 80a6d54:	f04f 0900 	movge.w	r9, #0
 80a6d58:	f04f 0901 	movlt.w	r9, #1
 80a6d5c:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a6d60:	f108 0801 	add.w	r8, r8, #1
 80a6d64:	e7bc      	b.n	80a6ce0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a6d66:	3601      	adds	r6, #1
 80a6d68:	e7b2      	b.n	80a6cd0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a6d6a:	3501      	adds	r5, #1
 80a6d6c:	e7a8      	b.n	80a6cc0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a6d6e:	3401      	adds	r4, #1
 80a6d70:	e79f      	b.n	80a6cb2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a6d72:	a80b      	add	r0, sp, #44	; 0x2c
 80a6d74:	f7fb fb1d 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
 80a6d78:	b021      	add	sp, #132	; 0x84
 80a6d7a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a6d7e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a6d7e:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a6d82:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a6d84:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a6d86:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a6d88:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a6d8a:	9208      	str	r2, [sp, #32]
 80a6d8c:	4604      	mov	r4, r0
 80a6d8e:	460e      	mov	r6, r1
 80a6d90:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a6d92:	dd01      	ble.n	80a6d98 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
 80a6d94:	f009 f94c 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a6d98:	683b      	ldr	r3, [r7, #0]
 80a6d9a:	2b04      	cmp	r3, #4
 80a6d9c:	dcfa      	bgt.n	80a6d94 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a6d9e:	6813      	ldr	r3, [r2, #0]
 80a6da0:	2b04      	cmp	r3, #4
 80a6da2:	dcf7      	bgt.n	80a6d94 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
 80a6da4:	2301      	movs	r3, #1
 80a6da6:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a6da8:	ad10      	add	r5, sp, #64	; 0x40
 80a6daa:	a80b      	add	r0, sp, #44	; 0x2c
 80a6dac:	f7fb fb45 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a6db0:	ab18      	add	r3, sp, #96	; 0x60
 80a6db2:	462a      	mov	r2, r5
 80a6db4:	4639      	mov	r1, r7
 80a6db6:	4630      	mov	r0, r6
 80a6db8:	f7fb fe42 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a6dbc:	6863      	ldr	r3, [r4, #4]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
 80a6dbe:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
 80a6dc2:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a6dc4:	68a3      	ldr	r3, [r4, #8]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a6dc6:	9509      	str	r5, [sp, #36]	; 0x24
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a6dc8:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
 80a6dca:	68e3      	ldr	r3, [r4, #12]
 80a6dcc:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
 80a6dce:	6923      	ldr	r3, [r4, #16]
 80a6dd0:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
 80a6dd2:	6963      	ldr	r3, [r4, #20]
 80a6dd4:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
 80a6dd6:	69a3      	ldr	r3, [r4, #24]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a6dd8:	2400      	movs	r4, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a6dda:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a6ddc:	2100      	movs	r1, #0
 80a6dde:	a80b      	add	r0, sp, #44	; 0x2c
 80a6de0:	f7fb faf2 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a6de4:	4284      	cmp	r4, r0
 80a6de6:	da59      	bge.n	80a6e9c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
 80a6de8:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a6dea:	af0b      	add	r7, sp, #44	; 0x2c
 80a6dec:	2101      	movs	r1, #1
 80a6dee:	4638      	mov	r0, r7
 80a6df0:	f7fb faea 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a6df4:	4285      	cmp	r5, r0
 80a6df6:	da4f      	bge.n	80a6e98 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
 80a6df8:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a6dfa:	2102      	movs	r1, #2
 80a6dfc:	4638      	mov	r0, r7
 80a6dfe:	f7fb fae3 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a6e02:	4286      	cmp	r6, r0
 80a6e04:	da46      	bge.n	80a6e94 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
 80a6e06:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a6e0a:	2103      	movs	r1, #3
 80a6e0c:	4638      	mov	r0, r7
 80a6e0e:	f7fb fadb 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a6e12:	4580      	cmp	r8, r0
 80a6e14:	da3c      	bge.n	80a6e90 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a6e16:	f8cd 8000 	str.w	r8, [sp]
 80a6e1a:	4633      	mov	r3, r6
 80a6e1c:	462a      	mov	r2, r5
 80a6e1e:	4621      	mov	r1, r4
 80a6e20:	9809      	ldr	r0, [sp, #36]	; 0x24
 80a6e22:	f7fb fbe7 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
 80a6e26:	9b08      	ldr	r3, [sp, #32]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a6e28:	462a      	mov	r2, r5
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a6e2a:	f913 9000 	ldrsb.w	r9, [r3, r0]
 80a6e2e:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a6e30:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a6e34:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a6e36:	4621      	mov	r1, r4
 80a6e38:	4633      	mov	r3, r6
 80a6e3a:	a818      	add	r0, sp, #96	; 0x60
 80a6e3c:	f7fb fbda 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6e40:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a6e42:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6e44:	f913 b000 	ldrsb.w	fp, [r3, r0]
 80a6e48:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a6e4a:	9903      	ldr	r1, [sp, #12]
 80a6e4c:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6e50:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a6e52:	f7fb fb7d 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a6e56:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a6e5a:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
 80a6e5c:	9a07      	ldr	r2, [sp, #28]
 80a6e5e:	9906      	ldr	r1, [sp, #24]
 80a6e60:	4658      	mov	r0, fp
 80a6e62:	f7fb fb75 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
 80a6e66:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
 80a6e68:	f8cd 8000 	str.w	r8, [sp]
 80a6e6c:	4633      	mov	r3, r6
 80a6e6e:	462a      	mov	r2, r5
 80a6e70:	4621      	mov	r1, r4
 80a6e72:	4638      	mov	r0, r7
 80a6e74:	f7fb fb0d 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80a6e78:	45d9      	cmp	r9, fp
 80a6e7a:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
 80a6e7c:	bfac      	ite	ge
 80a6e7e:	f04f 0900 	movge.w	r9, #0
 80a6e82:	f04f 0901 	movlt.w	r9, #1
 80a6e86:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a6e8a:	f108 0801 	add.w	r8, r8, #1
 80a6e8e:	e7bc      	b.n	80a6e0a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a6e90:	3601      	adds	r6, #1
 80a6e92:	e7b2      	b.n	80a6dfa <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a6e94:	3501      	adds	r5, #1
 80a6e96:	e7a8      	b.n	80a6dea <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a6e98:	3401      	adds	r4, #1
 80a6e9a:	e79f      	b.n	80a6ddc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a6e9c:	a80b      	add	r0, sp, #44	; 0x2c
 80a6e9e:	f7fb fa88 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
 80a6ea2:	b021      	add	sp, #132	; 0x84
 80a6ea4:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a6ea8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus LessEval(TfLiteContext* context, TfLiteNode* node) {
 80a6ea8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a6eac:	680a      	ldr	r2, [r1, #0]
 80a6eae:	f8d0 9008 	ldr.w	r9, [r0, #8]
 80a6eb2:	4682      	mov	sl, r0
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a6eb4:	6850      	ldr	r0, [r2, #4]
 80a6eb6:	2338      	movs	r3, #56	; 0x38
 80a6eb8:	6895      	ldr	r5, [r2, #8]
 80a6eba:	fb03 f800 	mul.w	r8, r3, r0
 80a6ebe:	fb03 9505 	mla	r5, r3, r5, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a6ec2:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a6ec4:	eb09 0608 	add.w	r6, r9, r8
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a6ec8:	6854      	ldr	r4, [r2, #4]
 80a6eca:	b0ab      	sub	sp, #172	; 0xac
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
 80a6ecc:	4629      	mov	r1, r5
 80a6ece:	4630      	mov	r0, r6
 80a6ed0:	fb03 9404 	mla	r4, r3, r4, r9
 80a6ed4:	f008 fe1a 	bl	80afb0c <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
  switch (input1->type) {
 80a6ed8:	f819 2008 	ldrb.w	r2, [r9, r8]

TfLiteStatus LessEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
 80a6edc:	f080 0001 	eor.w	r0, r0, #1
  switch (input1->type) {
 80a6ee0:	1e53      	subs	r3, r2, #1

TfLiteStatus LessEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
 80a6ee2:	b2c7      	uxtb	r7, r0
  switch (input1->type) {
 80a6ee4:	2b08      	cmp	r3, #8
 80a6ee6:	f200 8220 	bhi.w	80a732a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x482>
 80a6eea:	e8df f013 	tbh	[pc, r3, lsl #1]
 80a6eee:	0009      	.short	0x0009
 80a6ef0:	00f3005a 	.word	0x00f3005a
 80a6ef4:	021e00a4 	.word	0x021e00a4
 80a6ef8:	021e021e 	.word	0x021e021e
 80a6efc:	0183021e 	.word	0x0183021e
 80a6f00:	f10d 0874 	add.w	r8, sp, #116	; 0x74
    case kTfLiteFloat32:
      TF_LITE_COMPARISON(float, Less, requires_broadcast);
 80a6f04:	4631      	mov	r1, r6
 80a6f06:	b1cf      	cbz	r7, 80a6f3c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x94>
 80a6f08:	a813      	add	r0, sp, #76	; 0x4c
 80a6f0a:	f7fb fd02 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6f0e:	4629      	mov	r1, r5
 80a6f10:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a6f12:	6876      	ldr	r6, [r6, #4]
 80a6f14:	f7fb fcfd 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6f18:	b105      	cbz	r5, 80a6f1c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x74>
 80a6f1a:	686d      	ldr	r5, [r5, #4]
 80a6f1c:	4621      	mov	r1, r4
 80a6f1e:	4640      	mov	r0, r8
 80a6f20:	f7fb fcf7 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a6f24:	b104      	cbz	r4, 80a6f28 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x80>
 80a6f26:	6864      	ldr	r4, [r4, #4]
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
 80a6f28:	9402      	str	r4, [sp, #8]
 80a6f2a:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a6f2e:	ab18      	add	r3, sp, #96	; 0x60
 80a6f30:	4632      	mov	r2, r6
 80a6f32:	a913      	add	r1, sp, #76	; 0x4c
 80a6f34:	a822      	add	r0, sp, #136	; 0x88
 80a6f36:	f7fd fd63 	bl	80a4a00 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a6f3a:	e199      	b.n	80a7270 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3c8>
 80a6f3c:	a818      	add	r0, sp, #96	; 0x60
 80a6f3e:	f7fb fce8 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a6f42:	6873      	ldr	r3, [r6, #4]
 80a6f44:	4629      	mov	r1, r5
 80a6f46:	4640      	mov	r0, r8
 80a6f48:	9305      	str	r3, [sp, #20]
 80a6f4a:	f7fb fce2 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6f4e:	b105      	cbz	r5, 80a6f52 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0xaa>
 80a6f50:	686d      	ldr	r5, [r5, #4]
 80a6f52:	4621      	mov	r1, r4
 80a6f54:	a822      	add	r0, sp, #136	; 0x88
 80a6f56:	f7fb fcdc 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a6f5a:	b104      	cbz	r4, 80a6f5e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0xb6>
 80a6f5c:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a6f5e:	aa22      	add	r2, sp, #136	; 0x88
 80a6f60:	4641      	mov	r1, r8
 80a6f62:	a818      	add	r0, sp, #96	; 0x60
 80a6f64:	f7fb fab7 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a6f68:	4682      	mov	sl, r0
 80a6f6a:	ea4f 7be0 	mov.w	fp, r0, asr #31
 80a6f6e:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
 80a6f70:	2600      	movs	r6, #0
 80a6f72:	2700      	movs	r7, #0
 80a6f74:	4556      	cmp	r6, sl
 80a6f76:	eb77 030b 	sbcs.w	r3, r7, fp
 80a6f7a:	f280 81dd 	bge.w	80a7338 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x490>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a6f7e:	9a05      	ldr	r2, [sp, #20]
 80a6f80:	f855 1026 	ldr.w	r1, [r5, r6, lsl #2]
 80a6f84:	f852 0026 	ldr.w	r0, [r2, r6, lsl #2]
 80a6f88:	3401      	adds	r4, #1
 80a6f8a:	f04f 0901 	mov.w	r9, #1
 80a6f8e:	f00c fccb 	bl	80b3928 <__aeabi_fcmplt>
 80a6f92:	b900      	cbnz	r0, 80a6f96 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0xee>
 80a6f94:	4681      	mov	r9, r0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a6f96:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a6f98:	f884 9000 	strb.w	r9, [r4]
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a6f9c:	f147 0700 	adc.w	r7, r7, #0
 80a6fa0:	e7e8      	b.n	80a6f74 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0xcc>
 80a6fa2:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, Less, requires_broadcast);
 80a6fa6:	4631      	mov	r1, r6
 80a6fa8:	b1cf      	cbz	r7, 80a6fde <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x136>
 80a6faa:	a813      	add	r0, sp, #76	; 0x4c
 80a6fac:	f7fb fcb1 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6fb0:	4629      	mov	r1, r5
 80a6fb2:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a6fb4:	6876      	ldr	r6, [r6, #4]
 80a6fb6:	f7fb fcac 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6fba:	b105      	cbz	r5, 80a6fbe <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x116>
 80a6fbc:	686d      	ldr	r5, [r5, #4]
 80a6fbe:	4621      	mov	r1, r4
 80a6fc0:	4640      	mov	r0, r8
 80a6fc2:	f7fb fca6 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a6fc6:	b104      	cbz	r4, 80a6fca <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x122>
 80a6fc8:	6864      	ldr	r4, [r4, #4]
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
 80a6fca:	9402      	str	r4, [sp, #8]
 80a6fcc:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a6fd0:	ab18      	add	r3, sp, #96	; 0x60
 80a6fd2:	4632      	mov	r2, r6
 80a6fd4:	a913      	add	r1, sp, #76	; 0x4c
 80a6fd6:	a822      	add	r0, sp, #136	; 0x88
 80a6fd8:	f7fd fd7e 	bl	80a4ad8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a6fdc:	e148      	b.n	80a7270 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3c8>
 80a6fde:	a818      	add	r0, sp, #96	; 0x60
 80a6fe0:	f7fb fc97 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6fe4:	4629      	mov	r1, r5
 80a6fe6:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a6fe8:	6876      	ldr	r6, [r6, #4]
 80a6fea:	f7fb fc92 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a6fee:	b105      	cbz	r5, 80a6ff2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x14a>
 80a6ff0:	686d      	ldr	r5, [r5, #4]
 80a6ff2:	4621      	mov	r1, r4
 80a6ff4:	a822      	add	r0, sp, #136	; 0x88
 80a6ff6:	f7fb fc8c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a6ffa:	b104      	cbz	r4, 80a6ffe <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x156>
 80a6ffc:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a6ffe:	aa22      	add	r2, sp, #136	; 0x88
 80a7000:	4641      	mov	r1, r8
 80a7002:	a818      	add	r0, sp, #96	; 0x60
 80a7004:	f7fb fa67 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a7008:	3c01      	subs	r4, #1
 80a700a:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
 80a700c:	2200      	movs	r2, #0
 80a700e:	2300      	movs	r3, #0
 80a7010:	4282      	cmp	r2, r0
 80a7012:	eb73 0701 	sbcs.w	r7, r3, r1
 80a7016:	f280 818f 	bge.w	80a7338 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x490>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a701a:	f856 7022 	ldr.w	r7, [r6, r2, lsl #2]
 80a701e:	f855 e022 	ldr.w	lr, [r5, r2, lsl #2]
 80a7022:	4577      	cmp	r7, lr
 80a7024:	bfac      	ite	ge
 80a7026:	2700      	movge	r7, #0
 80a7028:	2701      	movlt	r7, #1
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a702a:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a702c:	f804 7f01 	strb.w	r7, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a7030:	f143 0300 	adc.w	r3, r3, #0
 80a7034:	e7ec      	b.n	80a7010 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x168>
 80a7036:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, Less, requires_broadcast);
 80a703a:	4631      	mov	r1, r6
 80a703c:	b1cf      	cbz	r7, 80a7072 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x1ca>
 80a703e:	a813      	add	r0, sp, #76	; 0x4c
 80a7040:	f7fb fc67 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a7044:	4629      	mov	r1, r5
 80a7046:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a7048:	6876      	ldr	r6, [r6, #4]
 80a704a:	f7fb fc62 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a704e:	b105      	cbz	r5, 80a7052 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x1aa>
 80a7050:	686d      	ldr	r5, [r5, #4]
 80a7052:	4621      	mov	r1, r4
 80a7054:	4640      	mov	r0, r8
 80a7056:	f7fb fc5c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a705a:	b104      	cbz	r4, 80a705e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x1b6>
 80a705c:	6864      	ldr	r4, [r4, #4]
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
 80a705e:	9402      	str	r4, [sp, #8]
 80a7060:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a7064:	ab18      	add	r3, sp, #96	; 0x60
 80a7066:	4632      	mov	r2, r6
 80a7068:	a913      	add	r1, sp, #76	; 0x4c
 80a706a:	a822      	add	r0, sp, #136	; 0x88
 80a706c:	f7fd fda0 	bl	80a4bb0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a7070:	e0fe      	b.n	80a7270 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3c8>
 80a7072:	a818      	add	r0, sp, #96	; 0x60
 80a7074:	f7fb fc4d 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a7078:	4629      	mov	r1, r5
 80a707a:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a707c:	6876      	ldr	r6, [r6, #4]
 80a707e:	f7fb fc48 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a7082:	b105      	cbz	r5, 80a7086 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x1de>
 80a7084:	686d      	ldr	r5, [r5, #4]
 80a7086:	4621      	mov	r1, r4
 80a7088:	a822      	add	r0, sp, #136	; 0x88
 80a708a:	f7fb fc42 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a708e:	b104      	cbz	r4, 80a7092 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x1ea>
 80a7090:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a7092:	aa22      	add	r2, sp, #136	; 0x88
 80a7094:	4641      	mov	r1, r8
 80a7096:	a818      	add	r0, sp, #96	; 0x60
 80a7098:	f7fb fa1d 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a709c:	3d08      	subs	r5, #8
 80a709e:	17c1      	asrs	r1, r0, #31
 80a70a0:	f1a6 0e08 	sub.w	lr, r6, #8
 80a70a4:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
 80a70a6:	2200      	movs	r2, #0
 80a70a8:	2300      	movs	r3, #0
 80a70aa:	4282      	cmp	r2, r0
 80a70ac:	eb73 0601 	sbcs.w	r6, r3, r1
 80a70b0:	f280 8142 	bge.w	80a7338 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x490>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a70b4:	e9fe ab02 	ldrd	sl, fp, [lr, #8]!
 80a70b8:	e9f5 6702 	ldrd	r6, r7, [r5, #8]!
 80a70bc:	45b2      	cmp	sl, r6
 80a70be:	eb7b 0607 	sbcs.w	r6, fp, r7
 80a70c2:	bfb4      	ite	lt
 80a70c4:	2601      	movlt	r6, #1
 80a70c6:	2600      	movge	r6, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a70c8:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a70ca:	f804 6f01 	strb.w	r6, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a70ce:	f143 0300 	adc.w	r3, r3, #0
 80a70d2:	e7ea      	b.n	80a70aa <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x202>
  }
TF_LITE_QUANTIZE_COMPARISON(Equal);
TF_LITE_QUANTIZE_COMPARISON(NotEqual);
TF_LITE_QUANTIZE_COMPARISON(Greater);
TF_LITE_QUANTIZE_COMPARISON(GreaterEqual);
TF_LITE_QUANTIZE_COMPARISON(Less);
 80a70d4:	6933      	ldr	r3, [r6, #16]
 80a70d6:	68f0      	ldr	r0, [r6, #12]
 80a70d8:	f1c3 0900 	rsb	r9, r3, #0
 80a70dc:	692b      	ldr	r3, [r5, #16]
 80a70de:	f1c3 0800 	rsb	r8, r3, #0
 80a70e2:	f00b fdef 	bl	80b2cc4 <__aeabi_f2d>
 80a70e6:	ab10      	add	r3, sp, #64	; 0x40
 80a70e8:	aa0f      	add	r2, sp, #60	; 0x3c
 80a70ea:	f008 fd69 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a70ee:	68e8      	ldr	r0, [r5, #12]
 80a70f0:	f00b fde8 	bl	80b2cc4 <__aeabi_f2d>
 80a70f4:	ab12      	add	r3, sp, #72	; 0x48
 80a70f6:	aa11      	add	r2, sp, #68	; 0x44
 80a70f8:	f008 fd62 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a70fc:	2308      	movs	r3, #8
 80a70fe:	9322      	str	r3, [sp, #136]	; 0x88
 80a7100:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80a7102:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
 80a7106:	9324      	str	r3, [sp, #144]	; 0x90
 80a7108:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80a710a:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
 80a710e:	9325      	str	r3, [sp, #148]	; 0x94
 80a7110:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80a7112:	f10d 0874 	add.w	r8, sp, #116	; 0x74
 80a7116:	9327      	str	r3, [sp, #156]	; 0x9c
 80a7118:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80a711a:	4631      	mov	r1, r6
 80a711c:	9328      	str	r3, [sp, #160]	; 0xa0
 80a711e:	a813      	add	r0, sp, #76	; 0x4c
 80a7120:	b1bf      	cbz	r7, 80a7152 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x2aa>
 80a7122:	f7fb fbf6 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a7126:	4629      	mov	r1, r5
 80a7128:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a712a:	6876      	ldr	r6, [r6, #4]
 80a712c:	f7fb fbf1 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a7130:	4621      	mov	r1, r4
 80a7132:	4640      	mov	r0, r8
 80a7134:	686d      	ldr	r5, [r5, #4]
 80a7136:	f7fb fbec 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a713a:	b104      	cbz	r4, 80a713e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x296>
 80a713c:	6864      	ldr	r4, [r4, #4]
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
 80a713e:	9402      	str	r4, [sp, #8]
 80a7140:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a7144:	ab18      	add	r3, sp, #96	; 0x60
 80a7146:	4632      	mov	r2, r6
 80a7148:	a913      	add	r1, sp, #76	; 0x4c
 80a714a:	a822      	add	r0, sp, #136	; 0x88
 80a714c:	f7ff fd82 	bl	80a6c54 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a7150:	e08e      	b.n	80a7270 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3c8>
 80a7152:	f7fb fbde 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a7156:	6873      	ldr	r3, [r6, #4]
 80a7158:	4629      	mov	r1, r5
 80a715a:	a818      	add	r0, sp, #96	; 0x60
 80a715c:	9305      	str	r3, [sp, #20]
 80a715e:	f7fb fbd8 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a7162:	4621      	mov	r1, r4
 80a7164:	4640      	mov	r0, r8
 80a7166:	f8d5 b004 	ldr.w	fp, [r5, #4]
 80a716a:	f7fb fbd2 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a716e:	b104      	cbz	r4, 80a7172 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x2ca>
 80a7170:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a7172:	9b23      	ldr	r3, [sp, #140]	; 0x8c
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a7174:	aa1d      	add	r2, sp, #116	; 0x74
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a7176:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a7178:	9b24      	ldr	r3, [sp, #144]	; 0x90
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a717a:	a918      	add	r1, sp, #96	; 0x60
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a717c:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
 80a717e:	9b25      	ldr	r3, [sp, #148]	; 0x94
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a7180:	a813      	add	r0, sp, #76	; 0x4c
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
 80a7182:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
 80a7184:	9b26      	ldr	r3, [sp, #152]	; 0x98
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
 80a7186:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
 80a7188:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
 80a718a:	9b27      	ldr	r3, [sp, #156]	; 0x9c
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a718c:	f04f 0800 	mov.w	r8, #0
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
 80a7190:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;
 80a7192:	9b28      	ldr	r3, [sp, #160]	; 0xa0

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a7194:	f04f 0900 	mov.w	r9, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a7198:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a719a:	f7fb f99c 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a719e:	4602      	mov	r2, r0
 80a71a0:	17c3      	asrs	r3, r0, #31
 80a71a2:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
  for (int64_t i = 0; i < flatsize; ++i) {
 80a71a6:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
 80a71aa:	4590      	cmp	r8, r2
 80a71ac:	eb79 0303 	sbcs.w	r3, r9, r3
 80a71b0:	f280 80b0 	bge.w	80a7314 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x46c>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a71b4:	f81b 5008 	ldrb.w	r5, [fp, r8]
 80a71b8:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a71ba:	9a08      	ldr	r2, [sp, #32]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a71bc:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a71be:	9b05      	ldr	r3, [sp, #20]
 80a71c0:	9907      	ldr	r1, [sp, #28]
 80a71c2:	f813 0008 	ldrb.w	r0, [r3, r8]
 80a71c6:	9b06      	ldr	r3, [sp, #24]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a71c8:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a71ca:	4418      	add	r0, r3
 80a71cc:	40b8      	lsls	r0, r7
 80a71ce:	f7fb f9bf 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a71d2:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a71d4:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a71d6:	990a      	ldr	r1, [sp, #40]	; 0x28
 80a71d8:	4628      	mov	r0, r5
 80a71da:	f7fb f9b9 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
 80a71de:	4582      	cmp	sl, r0
 80a71e0:	bfac      	ite	ge
 80a71e2:	2000      	movge	r0, #0
 80a71e4:	2001      	movlt	r0, #1
 80a71e6:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a71ea:	f118 0801 	adds.w	r8, r8, #1
 80a71ee:	f149 0900 	adc.w	r9, r9, #0
 80a71f2:	e7d8      	b.n	80a71a6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x2fe>
 80a71f4:	6933      	ldr	r3, [r6, #16]
 80a71f6:	68f0      	ldr	r0, [r6, #12]
 80a71f8:	f1c3 0900 	rsb	r9, r3, #0
 80a71fc:	692b      	ldr	r3, [r5, #16]
 80a71fe:	f1c3 0800 	rsb	r8, r3, #0
 80a7202:	f00b fd5f 	bl	80b2cc4 <__aeabi_f2d>
 80a7206:	ab10      	add	r3, sp, #64	; 0x40
 80a7208:	aa0f      	add	r2, sp, #60	; 0x3c
 80a720a:	f008 fcd9 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a720e:	68e8      	ldr	r0, [r5, #12]
 80a7210:	f00b fd58 	bl	80b2cc4 <__aeabi_f2d>
 80a7214:	ab12      	add	r3, sp, #72	; 0x48
 80a7216:	aa11      	add	r2, sp, #68	; 0x44
 80a7218:	f008 fcd2 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a721c:	2308      	movs	r3, #8
 80a721e:	9322      	str	r3, [sp, #136]	; 0x88
 80a7220:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80a7222:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
 80a7226:	9324      	str	r3, [sp, #144]	; 0x90
 80a7228:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80a722a:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
 80a722e:	9325      	str	r3, [sp, #148]	; 0x94
 80a7230:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80a7232:	f10d 0874 	add.w	r8, sp, #116	; 0x74
 80a7236:	9327      	str	r3, [sp, #156]	; 0x9c
 80a7238:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80a723a:	4631      	mov	r1, r6
 80a723c:	9328      	str	r3, [sp, #160]	; 0xa0
 80a723e:	a813      	add	r0, sp, #76	; 0x4c
 80a7240:	b1c7      	cbz	r7, 80a7274 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3cc>
 80a7242:	f7fb fb66 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a7246:	4629      	mov	r1, r5
 80a7248:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a724a:	6876      	ldr	r6, [r6, #4]
 80a724c:	f7fb fb61 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a7250:	4621      	mov	r1, r4
 80a7252:	4640      	mov	r0, r8
 80a7254:	686d      	ldr	r5, [r5, #4]
 80a7256:	f7fb fb5c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a725a:	b104      	cbz	r4, 80a725e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3b6>
 80a725c:	6864      	ldr	r4, [r4, #4]
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
 80a725e:	9402      	str	r4, [sp, #8]
 80a7260:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a7264:	ab18      	add	r3, sp, #96	; 0x60
 80a7266:	4632      	mov	r2, r6
 80a7268:	a913      	add	r1, sp, #76	; 0x4c
 80a726a:	a822      	add	r0, sp, #136	; 0x88
 80a726c:	f7ff fd87 	bl	80a6d7e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a7270:	4640      	mov	r0, r8
 80a7272:	e050      	b.n	80a7316 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x46e>
 80a7274:	f7fb fb4d 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a7278:	6873      	ldr	r3, [r6, #4]
 80a727a:	4629      	mov	r1, r5
 80a727c:	a818      	add	r0, sp, #96	; 0x60
 80a727e:	9305      	str	r3, [sp, #20]
 80a7280:	f7fb fb47 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a7284:	4621      	mov	r1, r4
 80a7286:	4640      	mov	r0, r8
 80a7288:	f8d5 b004 	ldr.w	fp, [r5, #4]
 80a728c:	f7fb fb41 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a7290:	b104      	cbz	r4, 80a7294 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3ec>
 80a7292:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a7294:	9b23      	ldr	r3, [sp, #140]	; 0x8c
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a7296:	aa1d      	add	r2, sp, #116	; 0x74
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a7298:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a729a:	9b24      	ldr	r3, [sp, #144]	; 0x90
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a729c:	a918      	add	r1, sp, #96	; 0x60
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a729e:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
 80a72a0:	9b25      	ldr	r3, [sp, #148]	; 0x94
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a72a2:	a813      	add	r0, sp, #76	; 0x4c
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
 80a72a4:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
 80a72a6:	9b26      	ldr	r3, [sp, #152]	; 0x98
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
 80a72a8:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
 80a72aa:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
 80a72ac:	9b27      	ldr	r3, [sp, #156]	; 0x9c
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a72ae:	f04f 0800 	mov.w	r8, #0
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
 80a72b2:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;
 80a72b4:	9b28      	ldr	r3, [sp, #160]	; 0xa0

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a72b6:	f04f 0900 	mov.w	r9, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a72ba:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a72bc:	f7fb f90b 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a72c0:	4602      	mov	r2, r0
 80a72c2:	17c3      	asrs	r3, r0, #31
 80a72c4:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
  for (int64_t i = 0; i < flatsize; ++i) {
 80a72c8:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
 80a72cc:	4590      	cmp	r8, r2
 80a72ce:	eb79 0303 	sbcs.w	r3, r9, r3
 80a72d2:	da1f      	bge.n	80a7314 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x46c>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a72d4:	f91b 5008 	ldrsb.w	r5, [fp, r8]
 80a72d8:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a72da:	9a08      	ldr	r2, [sp, #32]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a72dc:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a72de:	9b05      	ldr	r3, [sp, #20]
 80a72e0:	9907      	ldr	r1, [sp, #28]
 80a72e2:	f913 0008 	ldrsb.w	r0, [r3, r8]
 80a72e6:	9b06      	ldr	r3, [sp, #24]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a72e8:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a72ea:	4418      	add	r0, r3
 80a72ec:	40b8      	lsls	r0, r7
 80a72ee:	f7fb f92f 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a72f2:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a72f4:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a72f6:	990a      	ldr	r1, [sp, #40]	; 0x28
 80a72f8:	4628      	mov	r0, r5
 80a72fa:	f7fb f929 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
 80a72fe:	4582      	cmp	sl, r0
 80a7300:	bfac      	ite	ge
 80a7302:	2000      	movge	r0, #0
 80a7304:	2001      	movlt	r0, #1
 80a7306:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a730a:	f118 0801 	adds.w	r8, r8, #1
 80a730e:	f149 0900 	adc.w	r9, r9, #0
 80a7312:	e7d9      	b.n	80a72c8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x420>
 80a7314:	a81d      	add	r0, sp, #116	; 0x74
 80a7316:	f7fb f84c 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a731a:	a818      	add	r0, sp, #96	; 0x60
 80a731c:	f7fb f849 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a7320:	a813      	add	r0, sp, #76	; 0x4c
 80a7322:	f7fb f846 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
 80a7326:	2000      	movs	r0, #0
 80a7328:	e00e      	b.n	80a7348 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x4a0>
                                requires_broadcast);
      break;
    default:
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
 80a732a:	4650      	mov	r0, sl
 80a732c:	f8da 3014 	ldr.w	r3, [sl, #20]
 80a7330:	4907      	ldr	r1, [pc, #28]	; (80a7350 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x4a8>)
 80a7332:	4798      	blx	r3
      return kTfLiteError;
 80a7334:	2001      	movs	r0, #1
 80a7336:	e007      	b.n	80a7348 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x4a0>
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, Less, requires_broadcast);
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, Less, requires_broadcast);
 80a7338:	a822      	add	r0, sp, #136	; 0x88
 80a733a:	f7fb f83a 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a733e:	4640      	mov	r0, r8
 80a7340:	f7fb f837 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a7344:	a818      	add	r0, sp, #96	; 0x60
 80a7346:	e7ec      	b.n	80a7322 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x47a>
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
 80a7348:	b02b      	add	sp, #172	; 0xac
 80a734a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80a734e:	bf00      	nop
 80a7350:	080b5cd7 	.word	0x080b5cd7

080a7354 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a7354:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a7358:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a735a:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a735c:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a735e:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a7360:	9208      	str	r2, [sp, #32]
 80a7362:	4604      	mov	r4, r0
 80a7364:	460e      	mov	r6, r1
 80a7366:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a7368:	dd01      	ble.n	80a736e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
 80a736a:	f008 fe61 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a736e:	683b      	ldr	r3, [r7, #0]
 80a7370:	2b04      	cmp	r3, #4
 80a7372:	dcfa      	bgt.n	80a736a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a7374:	6813      	ldr	r3, [r2, #0]
 80a7376:	2b04      	cmp	r3, #4
 80a7378:	dcf7      	bgt.n	80a736a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
 80a737a:	2301      	movs	r3, #1
 80a737c:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a737e:	ad10      	add	r5, sp, #64	; 0x40
 80a7380:	a80b      	add	r0, sp, #44	; 0x2c
 80a7382:	f7fb f85a 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a7386:	ab18      	add	r3, sp, #96	; 0x60
 80a7388:	462a      	mov	r2, r5
 80a738a:	4639      	mov	r1, r7
 80a738c:	4630      	mov	r0, r6
 80a738e:	f7fb fb57 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a7392:	6863      	ldr	r3, [r4, #4]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
 80a7394:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
 80a7398:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a739a:	68a3      	ldr	r3, [r4, #8]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a739c:	9509      	str	r5, [sp, #36]	; 0x24
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a739e:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
 80a73a0:	68e3      	ldr	r3, [r4, #12]
 80a73a2:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
 80a73a4:	6923      	ldr	r3, [r4, #16]
 80a73a6:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
 80a73a8:	6963      	ldr	r3, [r4, #20]
 80a73aa:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
 80a73ac:	69a3      	ldr	r3, [r4, #24]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a73ae:	2400      	movs	r4, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a73b0:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a73b2:	2100      	movs	r1, #0
 80a73b4:	a80b      	add	r0, sp, #44	; 0x2c
 80a73b6:	f7fb f807 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a73ba:	4284      	cmp	r4, r0
 80a73bc:	da59      	bge.n	80a7472 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
 80a73be:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a73c0:	af0b      	add	r7, sp, #44	; 0x2c
 80a73c2:	2101      	movs	r1, #1
 80a73c4:	4638      	mov	r0, r7
 80a73c6:	f7fa ffff 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a73ca:	4285      	cmp	r5, r0
 80a73cc:	da4f      	bge.n	80a746e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
 80a73ce:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a73d0:	2102      	movs	r1, #2
 80a73d2:	4638      	mov	r0, r7
 80a73d4:	f7fa fff8 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a73d8:	4286      	cmp	r6, r0
 80a73da:	da46      	bge.n	80a746a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
 80a73dc:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a73e0:	2103      	movs	r1, #3
 80a73e2:	4638      	mov	r0, r7
 80a73e4:	f7fa fff0 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a73e8:	4580      	cmp	r8, r0
 80a73ea:	da3c      	bge.n	80a7466 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a73ec:	f8cd 8000 	str.w	r8, [sp]
 80a73f0:	4633      	mov	r3, r6
 80a73f2:	462a      	mov	r2, r5
 80a73f4:	4621      	mov	r1, r4
 80a73f6:	9809      	ldr	r0, [sp, #36]	; 0x24
 80a73f8:	f7fb f8fc 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
 80a73fc:	9b08      	ldr	r3, [sp, #32]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a73fe:	462a      	mov	r2, r5
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a7400:	f813 9000 	ldrb.w	r9, [r3, r0]
 80a7404:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a7406:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a740a:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a740c:	4621      	mov	r1, r4
 80a740e:	4633      	mov	r3, r6
 80a7410:	a818      	add	r0, sp, #96	; 0x60
 80a7412:	f7fb f8ef 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a7416:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a7418:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a741a:	f813 b000 	ldrb.w	fp, [r3, r0]
 80a741e:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a7420:	9903      	ldr	r1, [sp, #12]
 80a7422:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a7426:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a7428:	f7fb f892 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a742c:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a7430:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
 80a7432:	9a07      	ldr	r2, [sp, #28]
 80a7434:	9906      	ldr	r1, [sp, #24]
 80a7436:	4658      	mov	r0, fp
 80a7438:	f7fb f88a 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
 80a743c:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
 80a743e:	f8cd 8000 	str.w	r8, [sp]
 80a7442:	4633      	mov	r3, r6
 80a7444:	462a      	mov	r2, r5
 80a7446:	4621      	mov	r1, r4
 80a7448:	4638      	mov	r0, r7
 80a744a:	f7fb f822 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80a744e:	45d9      	cmp	r9, fp
 80a7450:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
 80a7452:	bfcc      	ite	gt
 80a7454:	f04f 0900 	movgt.w	r9, #0
 80a7458:	f04f 0901 	movle.w	r9, #1
 80a745c:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a7460:	f108 0801 	add.w	r8, r8, #1
 80a7464:	e7bc      	b.n	80a73e0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a7466:	3601      	adds	r6, #1
 80a7468:	e7b2      	b.n	80a73d0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a746a:	3501      	adds	r5, #1
 80a746c:	e7a8      	b.n	80a73c0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a746e:	3401      	adds	r4, #1
 80a7470:	e79f      	b.n	80a73b2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a7472:	a80b      	add	r0, sp, #44	; 0x2c
 80a7474:	f7fa ff9d 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
 80a7478:	b021      	add	sp, #132	; 0x84
 80a747a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a747e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a747e:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a7482:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a7484:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a7486:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a7488:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
 80a748a:	9208      	str	r2, [sp, #32]
 80a748c:	4604      	mov	r4, r0
 80a748e:	460e      	mov	r6, r1
 80a7490:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a7492:	dd01      	ble.n	80a7498 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
 80a7494:	f008 fdcc 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a7498:	683b      	ldr	r3, [r7, #0]
 80a749a:	2b04      	cmp	r3, #4
 80a749c:	dcfa      	bgt.n	80a7494 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a749e:	6813      	ldr	r3, [r2, #0]
 80a74a0:	2b04      	cmp	r3, #4
 80a74a2:	dcf7      	bgt.n	80a7494 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
 80a74a4:	2301      	movs	r3, #1
 80a74a6:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a74a8:	ad10      	add	r5, sp, #64	; 0x40
 80a74aa:	a80b      	add	r0, sp, #44	; 0x2c
 80a74ac:	f7fa ffc5 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a74b0:	ab18      	add	r3, sp, #96	; 0x60
 80a74b2:	462a      	mov	r2, r5
 80a74b4:	4639      	mov	r1, r7
 80a74b6:	4630      	mov	r0, r6
 80a74b8:	f7fb fac2 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a74bc:	6863      	ldr	r3, [r4, #4]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
 80a74be:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
 80a74c2:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a74c4:	68a3      	ldr	r3, [r4, #8]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a74c6:	9509      	str	r5, [sp, #36]	; 0x24
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a74c8:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
 80a74ca:	68e3      	ldr	r3, [r4, #12]
 80a74cc:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
 80a74ce:	6923      	ldr	r3, [r4, #16]
 80a74d0:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
 80a74d2:	6963      	ldr	r3, [r4, #20]
 80a74d4:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
 80a74d6:	69a3      	ldr	r3, [r4, #24]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a74d8:	2400      	movs	r4, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a74da:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a74dc:	2100      	movs	r1, #0
 80a74de:	a80b      	add	r0, sp, #44	; 0x2c
 80a74e0:	f7fa ff72 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a74e4:	4284      	cmp	r4, r0
 80a74e6:	da59      	bge.n	80a759c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
 80a74e8:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a74ea:	af0b      	add	r7, sp, #44	; 0x2c
 80a74ec:	2101      	movs	r1, #1
 80a74ee:	4638      	mov	r0, r7
 80a74f0:	f7fa ff6a 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a74f4:	4285      	cmp	r5, r0
 80a74f6:	da4f      	bge.n	80a7598 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
 80a74f8:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a74fa:	2102      	movs	r1, #2
 80a74fc:	4638      	mov	r0, r7
 80a74fe:	f7fa ff63 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a7502:	4286      	cmp	r6, r0
 80a7504:	da46      	bge.n	80a7594 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
 80a7506:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a750a:	2103      	movs	r1, #3
 80a750c:	4638      	mov	r0, r7
 80a750e:	f7fa ff5b 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a7512:	4580      	cmp	r8, r0
 80a7514:	da3c      	bge.n	80a7590 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a7516:	f8cd 8000 	str.w	r8, [sp]
 80a751a:	4633      	mov	r3, r6
 80a751c:	462a      	mov	r2, r5
 80a751e:	4621      	mov	r1, r4
 80a7520:	9809      	ldr	r0, [sp, #36]	; 0x24
 80a7522:	f7fb f867 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
 80a7526:	9b08      	ldr	r3, [sp, #32]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a7528:	462a      	mov	r2, r5
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a752a:	f913 9000 	ldrsb.w	r9, [r3, r0]
 80a752e:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a7530:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
 80a7534:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
 80a7536:	4621      	mov	r1, r4
 80a7538:	4633      	mov	r3, r6
 80a753a:	a818      	add	r0, sp, #96	; 0x60
 80a753c:	f7fb f85a 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a7540:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a7542:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a7544:	f913 b000 	ldrsb.w	fp, [r3, r0]
 80a7548:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a754a:	9903      	ldr	r1, [sp, #12]
 80a754c:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a7550:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a7552:	f7fa fffd 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a7556:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
 80a755a:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
 80a755c:	9a07      	ldr	r2, [sp, #28]
 80a755e:	9906      	ldr	r1, [sp, #24]
 80a7560:	4658      	mov	r0, fp
 80a7562:	f7fa fff5 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
 80a7566:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
 80a7568:	f8cd 8000 	str.w	r8, [sp]
 80a756c:	4633      	mov	r3, r6
 80a756e:	462a      	mov	r2, r5
 80a7570:	4621      	mov	r1, r4
 80a7572:	4638      	mov	r0, r7
 80a7574:	f7fa ff8d 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80a7578:	45d9      	cmp	r9, fp
 80a757a:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
 80a757c:	bfcc      	ite	gt
 80a757e:	f04f 0900 	movgt.w	r9, #0
 80a7582:	f04f 0901 	movle.w	r9, #1
 80a7586:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a758a:	f108 0801 	add.w	r8, r8, #1
 80a758e:	e7bc      	b.n	80a750a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a7590:	3601      	adds	r6, #1
 80a7592:	e7b2      	b.n	80a74fa <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a7594:	3501      	adds	r5, #1
 80a7596:	e7a8      	b.n	80a74ea <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a7598:	3401      	adds	r4, #1
 80a759a:	e79f      	b.n	80a74dc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a759c:	a80b      	add	r0, sp, #44	; 0x2c
 80a759e:	f7fa ff08 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
 80a75a2:	b021      	add	sp, #132	; 0x84
 80a75a4:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a75a8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus LessEqualEval(TfLiteContext* context, TfLiteNode* node) {
 80a75a8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a75ac:	680a      	ldr	r2, [r1, #0]
 80a75ae:	f8d0 9008 	ldr.w	r9, [r0, #8]
 80a75b2:	4682      	mov	sl, r0
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a75b4:	6850      	ldr	r0, [r2, #4]
 80a75b6:	2338      	movs	r3, #56	; 0x38
 80a75b8:	6895      	ldr	r5, [r2, #8]
 80a75ba:	fb03 f800 	mul.w	r8, r3, r0
 80a75be:	fb03 9505 	mla	r5, r3, r5, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a75c2:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a75c4:	eb09 0608 	add.w	r6, r9, r8
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a75c8:	6854      	ldr	r4, [r2, #4]
 80a75ca:	b0ab      	sub	sp, #172	; 0xac
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
 80a75cc:	4629      	mov	r1, r5
 80a75ce:	4630      	mov	r0, r6
 80a75d0:	fb03 9404 	mla	r4, r3, r4, r9
 80a75d4:	f008 fa9a 	bl	80afb0c <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
  switch (input1->type) {
 80a75d8:	f819 2008 	ldrb.w	r2, [r9, r8]

TfLiteStatus LessEqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
 80a75dc:	f080 0001 	eor.w	r0, r0, #1
  switch (input1->type) {
 80a75e0:	1e53      	subs	r3, r2, #1

TfLiteStatus LessEqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
 80a75e2:	b2c7      	uxtb	r7, r0
  switch (input1->type) {
 80a75e4:	2b08      	cmp	r3, #8
 80a75e6:	f200 8220 	bhi.w	80a7a2a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x482>
 80a75ea:	e8df f013 	tbh	[pc, r3, lsl #1]
 80a75ee:	0009      	.short	0x0009
 80a75f0:	00f3005a 	.word	0x00f3005a
 80a75f4:	021e00a4 	.word	0x021e00a4
 80a75f8:	021e021e 	.word	0x021e021e
 80a75fc:	0183021e 	.word	0x0183021e
 80a7600:	f10d 0874 	add.w	r8, sp, #116	; 0x74
    case kTfLiteFloat32:
      TF_LITE_COMPARISON(float, LessEqual, requires_broadcast);
 80a7604:	4631      	mov	r1, r6
 80a7606:	b1cf      	cbz	r7, 80a763c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x94>
 80a7608:	a813      	add	r0, sp, #76	; 0x4c
 80a760a:	f7fb f982 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a760e:	4629      	mov	r1, r5
 80a7610:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a7612:	6876      	ldr	r6, [r6, #4]
 80a7614:	f7fb f97d 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a7618:	b105      	cbz	r5, 80a761c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x74>
 80a761a:	686d      	ldr	r5, [r5, #4]
 80a761c:	4621      	mov	r1, r4
 80a761e:	4640      	mov	r0, r8
 80a7620:	f7fb f977 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a7624:	b104      	cbz	r4, 80a7628 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x80>
 80a7626:	6864      	ldr	r4, [r4, #4]
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
TFLITE_COMPARISON_OP(LessEqual);
 80a7628:	9402      	str	r4, [sp, #8]
 80a762a:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a762e:	ab18      	add	r3, sp, #96	; 0x60
 80a7630:	4632      	mov	r2, r6
 80a7632:	a913      	add	r1, sp, #76	; 0x4c
 80a7634:	a822      	add	r0, sp, #136	; 0x88
 80a7636:	f7fd fb2d 	bl	80a4c94 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a763a:	e199      	b.n	80a7970 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3c8>
 80a763c:	a818      	add	r0, sp, #96	; 0x60
 80a763e:	f7fb f968 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a7642:	6873      	ldr	r3, [r6, #4]
 80a7644:	4629      	mov	r1, r5
 80a7646:	4640      	mov	r0, r8
 80a7648:	9305      	str	r3, [sp, #20]
 80a764a:	f7fb f962 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a764e:	b105      	cbz	r5, 80a7652 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0xaa>
 80a7650:	686d      	ldr	r5, [r5, #4]
 80a7652:	4621      	mov	r1, r4
 80a7654:	a822      	add	r0, sp, #136	; 0x88
 80a7656:	f7fb f95c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a765a:	b104      	cbz	r4, 80a765e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0xb6>
 80a765c:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a765e:	aa22      	add	r2, sp, #136	; 0x88
 80a7660:	4641      	mov	r1, r8
 80a7662:	a818      	add	r0, sp, #96	; 0x60
 80a7664:	f7fa ff37 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a7668:	4682      	mov	sl, r0
 80a766a:	ea4f 7be0 	mov.w	fp, r0, asr #31
 80a766e:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
 80a7670:	2600      	movs	r6, #0
 80a7672:	2700      	movs	r7, #0
 80a7674:	4556      	cmp	r6, sl
 80a7676:	eb77 030b 	sbcs.w	r3, r7, fp
 80a767a:	f280 81dd 	bge.w	80a7a38 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x490>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a767e:	9a05      	ldr	r2, [sp, #20]
 80a7680:	f855 1026 	ldr.w	r1, [r5, r6, lsl #2]
 80a7684:	f852 0026 	ldr.w	r0, [r2, r6, lsl #2]
 80a7688:	3401      	adds	r4, #1
 80a768a:	f04f 0901 	mov.w	r9, #1
 80a768e:	f00c f955 	bl	80b393c <__aeabi_fcmple>
 80a7692:	b900      	cbnz	r0, 80a7696 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0xee>
 80a7694:	4681      	mov	r9, r0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a7696:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a7698:	f884 9000 	strb.w	r9, [r4]
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a769c:	f147 0700 	adc.w	r7, r7, #0
 80a76a0:	e7e8      	b.n	80a7674 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0xcc>
 80a76a2:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, LessEqual, requires_broadcast);
 80a76a6:	4631      	mov	r1, r6
 80a76a8:	b1cf      	cbz	r7, 80a76de <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x136>
 80a76aa:	a813      	add	r0, sp, #76	; 0x4c
 80a76ac:	f7fb f931 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a76b0:	4629      	mov	r1, r5
 80a76b2:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a76b4:	6876      	ldr	r6, [r6, #4]
 80a76b6:	f7fb f92c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a76ba:	b105      	cbz	r5, 80a76be <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x116>
 80a76bc:	686d      	ldr	r5, [r5, #4]
 80a76be:	4621      	mov	r1, r4
 80a76c0:	4640      	mov	r0, r8
 80a76c2:	f7fb f926 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a76c6:	b104      	cbz	r4, 80a76ca <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x122>
 80a76c8:	6864      	ldr	r4, [r4, #4]
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
TFLITE_COMPARISON_OP(LessEqual);
 80a76ca:	9402      	str	r4, [sp, #8]
 80a76cc:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a76d0:	ab18      	add	r3, sp, #96	; 0x60
 80a76d2:	4632      	mov	r2, r6
 80a76d4:	a913      	add	r1, sp, #76	; 0x4c
 80a76d6:	a822      	add	r0, sp, #136	; 0x88
 80a76d8:	f7fd fb48 	bl	80a4d6c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a76dc:	e148      	b.n	80a7970 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3c8>
 80a76de:	a818      	add	r0, sp, #96	; 0x60
 80a76e0:	f7fb f917 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a76e4:	4629      	mov	r1, r5
 80a76e6:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a76e8:	6876      	ldr	r6, [r6, #4]
 80a76ea:	f7fb f912 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a76ee:	b105      	cbz	r5, 80a76f2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x14a>
 80a76f0:	686d      	ldr	r5, [r5, #4]
 80a76f2:	4621      	mov	r1, r4
 80a76f4:	a822      	add	r0, sp, #136	; 0x88
 80a76f6:	f7fb f90c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a76fa:	b104      	cbz	r4, 80a76fe <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x156>
 80a76fc:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a76fe:	aa22      	add	r2, sp, #136	; 0x88
 80a7700:	4641      	mov	r1, r8
 80a7702:	a818      	add	r0, sp, #96	; 0x60
 80a7704:	f7fa fee7 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a7708:	3c01      	subs	r4, #1
 80a770a:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
 80a770c:	2200      	movs	r2, #0
 80a770e:	2300      	movs	r3, #0
 80a7710:	4282      	cmp	r2, r0
 80a7712:	eb73 0701 	sbcs.w	r7, r3, r1
 80a7716:	f280 818f 	bge.w	80a7a38 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x490>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a771a:	f856 7022 	ldr.w	r7, [r6, r2, lsl #2]
 80a771e:	f855 e022 	ldr.w	lr, [r5, r2, lsl #2]
 80a7722:	4577      	cmp	r7, lr
 80a7724:	bfcc      	ite	gt
 80a7726:	2700      	movgt	r7, #0
 80a7728:	2701      	movle	r7, #1
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a772a:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a772c:	f804 7f01 	strb.w	r7, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a7730:	f143 0300 	adc.w	r3, r3, #0
 80a7734:	e7ec      	b.n	80a7710 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x168>
 80a7736:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, LessEqual, requires_broadcast);
 80a773a:	4631      	mov	r1, r6
 80a773c:	b1cf      	cbz	r7, 80a7772 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x1ca>
 80a773e:	a813      	add	r0, sp, #76	; 0x4c
 80a7740:	f7fb f8e7 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a7744:	4629      	mov	r1, r5
 80a7746:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a7748:	6876      	ldr	r6, [r6, #4]
 80a774a:	f7fb f8e2 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a774e:	b105      	cbz	r5, 80a7752 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x1aa>
 80a7750:	686d      	ldr	r5, [r5, #4]
 80a7752:	4621      	mov	r1, r4
 80a7754:	4640      	mov	r0, r8
 80a7756:	f7fb f8dc 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a775a:	b104      	cbz	r4, 80a775e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x1b6>
 80a775c:	6864      	ldr	r4, [r4, #4]
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
TFLITE_COMPARISON_OP(LessEqual);
 80a775e:	9402      	str	r4, [sp, #8]
 80a7760:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a7764:	ab18      	add	r3, sp, #96	; 0x60
 80a7766:	4632      	mov	r2, r6
 80a7768:	a913      	add	r1, sp, #76	; 0x4c
 80a776a:	a822      	add	r0, sp, #136	; 0x88
 80a776c:	f7fd fb6a 	bl	80a4e44 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a7770:	e0fe      	b.n	80a7970 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3c8>
 80a7772:	a818      	add	r0, sp, #96	; 0x60
 80a7774:	f7fb f8cd 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a7778:	4629      	mov	r1, r5
 80a777a:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a777c:	6876      	ldr	r6, [r6, #4]
 80a777e:	f7fb f8c8 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a7782:	b105      	cbz	r5, 80a7786 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x1de>
 80a7784:	686d      	ldr	r5, [r5, #4]
 80a7786:	4621      	mov	r1, r4
 80a7788:	a822      	add	r0, sp, #136	; 0x88
 80a778a:	f7fb f8c2 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a778e:	b104      	cbz	r4, 80a7792 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x1ea>
 80a7790:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a7792:	aa22      	add	r2, sp, #136	; 0x88
 80a7794:	4641      	mov	r1, r8
 80a7796:	a818      	add	r0, sp, #96	; 0x60
 80a7798:	f7fa fe9d 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a779c:	3d08      	subs	r5, #8
 80a779e:	17c1      	asrs	r1, r0, #31
 80a77a0:	f1a6 0e08 	sub.w	lr, r6, #8
 80a77a4:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
 80a77a6:	2200      	movs	r2, #0
 80a77a8:	2300      	movs	r3, #0
 80a77aa:	4282      	cmp	r2, r0
 80a77ac:	eb73 0601 	sbcs.w	r6, r3, r1
 80a77b0:	f280 8142 	bge.w	80a7a38 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x490>
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a77b4:	e9fe 6702 	ldrd	r6, r7, [lr, #8]!
 80a77b8:	e9f5 ab02 	ldrd	sl, fp, [r5, #8]!
 80a77bc:	45b2      	cmp	sl, r6
 80a77be:	eb7b 0607 	sbcs.w	r6, fp, r7
 80a77c2:	bfac      	ite	ge
 80a77c4:	2601      	movge	r6, #1
 80a77c6:	2600      	movlt	r6, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a77c8:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
 80a77ca:	f804 6f01 	strb.w	r6, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a77ce:	f143 0300 	adc.w	r3, r3, #0
 80a77d2:	e7ea      	b.n	80a77aa <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x202>
TF_LITE_QUANTIZE_COMPARISON(Equal);
TF_LITE_QUANTIZE_COMPARISON(NotEqual);
TF_LITE_QUANTIZE_COMPARISON(Greater);
TF_LITE_QUANTIZE_COMPARISON(GreaterEqual);
TF_LITE_QUANTIZE_COMPARISON(Less);
TF_LITE_QUANTIZE_COMPARISON(LessEqual);
 80a77d4:	6933      	ldr	r3, [r6, #16]
 80a77d6:	68f0      	ldr	r0, [r6, #12]
 80a77d8:	f1c3 0900 	rsb	r9, r3, #0
 80a77dc:	692b      	ldr	r3, [r5, #16]
 80a77de:	f1c3 0800 	rsb	r8, r3, #0
 80a77e2:	f00b fa6f 	bl	80b2cc4 <__aeabi_f2d>
 80a77e6:	ab10      	add	r3, sp, #64	; 0x40
 80a77e8:	aa0f      	add	r2, sp, #60	; 0x3c
 80a77ea:	f008 f9e9 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a77ee:	68e8      	ldr	r0, [r5, #12]
 80a77f0:	f00b fa68 	bl	80b2cc4 <__aeabi_f2d>
 80a77f4:	ab12      	add	r3, sp, #72	; 0x48
 80a77f6:	aa11      	add	r2, sp, #68	; 0x44
 80a77f8:	f008 f9e2 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a77fc:	2308      	movs	r3, #8
 80a77fe:	9322      	str	r3, [sp, #136]	; 0x88
 80a7800:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80a7802:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
 80a7806:	9324      	str	r3, [sp, #144]	; 0x90
 80a7808:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80a780a:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
 80a780e:	9325      	str	r3, [sp, #148]	; 0x94
 80a7810:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80a7812:	f10d 0874 	add.w	r8, sp, #116	; 0x74
 80a7816:	9327      	str	r3, [sp, #156]	; 0x9c
 80a7818:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80a781a:	4631      	mov	r1, r6
 80a781c:	9328      	str	r3, [sp, #160]	; 0xa0
 80a781e:	a813      	add	r0, sp, #76	; 0x4c
 80a7820:	b1bf      	cbz	r7, 80a7852 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x2aa>
 80a7822:	f7fb f876 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a7826:	4629      	mov	r1, r5
 80a7828:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a782a:	6876      	ldr	r6, [r6, #4]
 80a782c:	f7fb f871 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a7830:	4621      	mov	r1, r4
 80a7832:	4640      	mov	r0, r8
 80a7834:	686d      	ldr	r5, [r5, #4]
 80a7836:	f7fb f86c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a783a:	b104      	cbz	r4, 80a783e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x296>
 80a783c:	6864      	ldr	r4, [r4, #4]
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
TFLITE_COMPARISON_OP(LessEqual);
 80a783e:	9402      	str	r4, [sp, #8]
 80a7840:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a7844:	ab18      	add	r3, sp, #96	; 0x60
 80a7846:	4632      	mov	r2, r6
 80a7848:	a913      	add	r1, sp, #76	; 0x4c
 80a784a:	a822      	add	r0, sp, #136	; 0x88
 80a784c:	f7ff fd82 	bl	80a7354 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a7850:	e08e      	b.n	80a7970 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3c8>
 80a7852:	f7fb f85e 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a7856:	6873      	ldr	r3, [r6, #4]
 80a7858:	4629      	mov	r1, r5
 80a785a:	a818      	add	r0, sp, #96	; 0x60
 80a785c:	9305      	str	r3, [sp, #20]
 80a785e:	f7fb f858 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a7862:	4621      	mov	r1, r4
 80a7864:	4640      	mov	r0, r8
 80a7866:	f8d5 b004 	ldr.w	fp, [r5, #4]
 80a786a:	f7fb f852 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a786e:	b104      	cbz	r4, 80a7872 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x2ca>
 80a7870:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a7872:	9b23      	ldr	r3, [sp, #140]	; 0x8c
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a7874:	aa1d      	add	r2, sp, #116	; 0x74
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a7876:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a7878:	9b24      	ldr	r3, [sp, #144]	; 0x90
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a787a:	a918      	add	r1, sp, #96	; 0x60
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a787c:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
 80a787e:	9b25      	ldr	r3, [sp, #148]	; 0x94
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a7880:	a813      	add	r0, sp, #76	; 0x4c
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
 80a7882:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
 80a7884:	9b26      	ldr	r3, [sp, #152]	; 0x98
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
 80a7886:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
 80a7888:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
 80a788a:	9b27      	ldr	r3, [sp, #156]	; 0x9c
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a788c:	f04f 0800 	mov.w	r8, #0
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
 80a7890:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;
 80a7892:	9b28      	ldr	r3, [sp, #160]	; 0xa0

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a7894:	f04f 0900 	mov.w	r9, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a7898:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a789a:	f7fa fe1c 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a789e:	4602      	mov	r2, r0
 80a78a0:	17c3      	asrs	r3, r0, #31
 80a78a2:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
  for (int64_t i = 0; i < flatsize; ++i) {
 80a78a6:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
 80a78aa:	4590      	cmp	r8, r2
 80a78ac:	eb79 0303 	sbcs.w	r3, r9, r3
 80a78b0:	f280 80b0 	bge.w	80a7a14 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x46c>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a78b4:	f81b 5008 	ldrb.w	r5, [fp, r8]
 80a78b8:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a78ba:	9a08      	ldr	r2, [sp, #32]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a78bc:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a78be:	9b05      	ldr	r3, [sp, #20]
 80a78c0:	9907      	ldr	r1, [sp, #28]
 80a78c2:	f813 0008 	ldrb.w	r0, [r3, r8]
 80a78c6:	9b06      	ldr	r3, [sp, #24]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a78c8:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a78ca:	4418      	add	r0, r3
 80a78cc:	40b8      	lsls	r0, r7
 80a78ce:	f7fa fe3f 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a78d2:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a78d4:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a78d6:	990a      	ldr	r1, [sp, #40]	; 0x28
 80a78d8:	4628      	mov	r0, r5
 80a78da:	f7fa fe39 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
 80a78de:	4582      	cmp	sl, r0
 80a78e0:	bfcc      	ite	gt
 80a78e2:	2000      	movgt	r0, #0
 80a78e4:	2001      	movle	r0, #1
 80a78e6:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a78ea:	f118 0801 	adds.w	r8, r8, #1
 80a78ee:	f149 0900 	adc.w	r9, r9, #0
 80a78f2:	e7d8      	b.n	80a78a6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x2fe>
 80a78f4:	6933      	ldr	r3, [r6, #16]
 80a78f6:	68f0      	ldr	r0, [r6, #12]
 80a78f8:	f1c3 0900 	rsb	r9, r3, #0
 80a78fc:	692b      	ldr	r3, [r5, #16]
 80a78fe:	f1c3 0800 	rsb	r8, r3, #0
 80a7902:	f00b f9df 	bl	80b2cc4 <__aeabi_f2d>
 80a7906:	ab10      	add	r3, sp, #64	; 0x40
 80a7908:	aa0f      	add	r2, sp, #60	; 0x3c
 80a790a:	f008 f959 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a790e:	68e8      	ldr	r0, [r5, #12]
 80a7910:	f00b f9d8 	bl	80b2cc4 <__aeabi_f2d>
 80a7914:	ab12      	add	r3, sp, #72	; 0x48
 80a7916:	aa11      	add	r2, sp, #68	; 0x44
 80a7918:	f008 f952 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
 80a791c:	2308      	movs	r3, #8
 80a791e:	9322      	str	r3, [sp, #136]	; 0x88
 80a7920:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80a7922:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
 80a7926:	9324      	str	r3, [sp, #144]	; 0x90
 80a7928:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80a792a:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
 80a792e:	9325      	str	r3, [sp, #148]	; 0x94
 80a7930:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80a7932:	f10d 0874 	add.w	r8, sp, #116	; 0x74
 80a7936:	9327      	str	r3, [sp, #156]	; 0x9c
 80a7938:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80a793a:	4631      	mov	r1, r6
 80a793c:	9328      	str	r3, [sp, #160]	; 0xa0
 80a793e:	a813      	add	r0, sp, #76	; 0x4c
 80a7940:	b1c7      	cbz	r7, 80a7974 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3cc>
 80a7942:	f7fa ffe6 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a7946:	4629      	mov	r1, r5
 80a7948:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a794a:	6876      	ldr	r6, [r6, #4]
 80a794c:	f7fa ffe1 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a7950:	4621      	mov	r1, r4
 80a7952:	4640      	mov	r0, r8
 80a7954:	686d      	ldr	r5, [r5, #4]
 80a7956:	f7fa ffdc 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a795a:	b104      	cbz	r4, 80a795e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3b6>
 80a795c:	6864      	ldr	r4, [r4, #4]
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
TFLITE_COMPARISON_OP(LessEqual);
 80a795e:	9402      	str	r4, [sp, #8]
 80a7960:	e88d 0120 	stmia.w	sp, {r5, r8}
 80a7964:	ab18      	add	r3, sp, #96	; 0x60
 80a7966:	4632      	mov	r2, r6
 80a7968:	a913      	add	r1, sp, #76	; 0x4c
 80a796a:	a822      	add	r0, sp, #136	; 0x88
 80a796c:	f7ff fd87 	bl	80a747e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
 80a7970:	4640      	mov	r0, r8
 80a7972:	e050      	b.n	80a7a16 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x46e>
 80a7974:	f7fa ffcd 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a7978:	6873      	ldr	r3, [r6, #4]
 80a797a:	4629      	mov	r1, r5
 80a797c:	a818      	add	r0, sp, #96	; 0x60
 80a797e:	9305      	str	r3, [sp, #20]
 80a7980:	f7fa ffc7 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a7984:	4621      	mov	r1, r4
 80a7986:	4640      	mov	r0, r8
 80a7988:	f8d5 b004 	ldr.w	fp, [r5, #4]
 80a798c:	f7fa ffc1 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a7990:	b104      	cbz	r4, 80a7994 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3ec>
 80a7992:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a7994:	9b23      	ldr	r3, [sp, #140]	; 0x8c
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a7996:	aa1d      	add	r2, sp, #116	; 0x74
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
 80a7998:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
 80a799a:	9b24      	ldr	r3, [sp, #144]	; 0x90
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a799c:	a918      	add	r1, sp, #96	; 0x60
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
 80a799e:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
 80a79a0:	9b25      	ldr	r3, [sp, #148]	; 0x94
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a79a2:	a813      	add	r0, sp, #76	; 0x4c
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
 80a79a4:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
 80a79a6:	9b26      	ldr	r3, [sp, #152]	; 0x98
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
 80a79a8:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
 80a79aa:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
 80a79ac:	9b27      	ldr	r3, [sp, #156]	; 0x9c
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a79ae:	f04f 0800 	mov.w	r8, #0
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
 80a79b2:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;
 80a79b4:	9b28      	ldr	r3, [sp, #160]	; 0xa0

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a79b6:	f04f 0900 	mov.w	r9, #0
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
 80a79ba:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
 80a79bc:	f7fa fd8b 	bl	80a24d6 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
 80a79c0:	4602      	mov	r2, r0
 80a79c2:	17c3      	asrs	r3, r0, #31
 80a79c4:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
  for (int64_t i = 0; i < flatsize; ++i) {
 80a79c8:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
 80a79cc:	4590      	cmp	r8, r2
 80a79ce:	eb79 0303 	sbcs.w	r3, r9, r3
 80a79d2:	da1f      	bge.n	80a7a14 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x46c>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a79d4:	f91b 5008 	ldrsb.w	r5, [fp, r8]
 80a79d8:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a79da:	9a08      	ldr	r2, [sp, #32]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a79dc:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a79de:	9b05      	ldr	r3, [sp, #20]
 80a79e0:	9907      	ldr	r1, [sp, #28]
 80a79e2:	f913 0008 	ldrsb.w	r0, [r3, r8]
 80a79e6:	9b06      	ldr	r3, [sp, #24]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
 80a79e8:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a79ea:	4418      	add	r0, r3
 80a79ec:	40b8      	lsls	r0, r7
 80a79ee:	f7fa fdaf 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a79f2:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
 80a79f4:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
 80a79f6:	990a      	ldr	r1, [sp, #40]	; 0x28
 80a79f8:	4628      	mov	r0, r5
 80a79fa:	f7fa fda9 	bl	80a2550 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
 80a79fe:	4582      	cmp	sl, r0
 80a7a00:	bfcc      	ite	gt
 80a7a02:	2000      	movgt	r0, #0
 80a7a04:	2001      	movle	r0, #1
 80a7a06:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
 80a7a0a:	f118 0801 	adds.w	r8, r8, #1
 80a7a0e:	f149 0900 	adc.w	r9, r9, #0
 80a7a12:	e7d9      	b.n	80a79c8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x420>
 80a7a14:	a81d      	add	r0, sp, #116	; 0x74
 80a7a16:	f7fa fccc 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a7a1a:	a818      	add	r0, sp, #96	; 0x60
 80a7a1c:	f7fa fcc9 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a7a20:	a813      	add	r0, sp, #76	; 0x4c
 80a7a22:	f7fa fcc6 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
 80a7a26:	2000      	movs	r0, #0
 80a7a28:	e00e      	b.n	80a7a48 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x4a0>
                                     requires_broadcast);
      break;
    default:
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
 80a7a2a:	4650      	mov	r0, sl
 80a7a2c:	f8da 3014 	ldr.w	r3, [sl, #20]
 80a7a30:	4907      	ldr	r1, [pc, #28]	; (80a7a50 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x4a8>)
 80a7a32:	4798      	blx	r3
      return kTfLiteError;
 80a7a34:	2001      	movs	r0, #1
 80a7a36:	e007      	b.n	80a7a48 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x4a0>
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, LessEqual, requires_broadcast);
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, LessEqual, requires_broadcast);
 80a7a38:	a822      	add	r0, sp, #136	; 0x88
 80a7a3a:	f7fa fcba 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a7a3e:	4640      	mov	r0, r8
 80a7a40:	f7fa fcb7 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a7a44:	a818      	add	r0, sp, #96	; 0x60
 80a7a46:	e7ec      	b.n	80a7a22 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x47a>
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
 80a7a48:	b02b      	add	sp, #172	; 0xac
 80a7a4a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80a7a4e:	bf00      	nop
 80a7a50:	080b5cd7 	.word	0x080b5cd7

080a7a54 <_ZN6tflite3ops5micro4conv4InitEP13TfLiteContextPKcj>:
  return kTfLiteOk;
}

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
 80a7a54:	2000      	movs	r0, #0
 80a7a56:	4770      	bx	lr

080a7a58 <_ZN6tflite3ops5micro4conv4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
 80a7a58:	4770      	bx	lr

080a7a5a <_ZN6tflite3ops5micro4conv7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
 80a7a5a:	2000      	movs	r0, #0
 80a7a5c:	4770      	bx	lr

080a7a5e <_ZNK6tflite12RuntimeShape8FlatSizeEv>:
    BuildFrom<const std::initializer_list<int>>(init_list);
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
 80a7a5e:	b510      	push	{r4, lr}

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80a7a60:	6801      	ldr	r1, [r0, #0]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a7a62:	2200      	movs	r2, #0

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80a7a64:	2904      	cmp	r1, #4
 80a7a66:	bfcc      	ite	gt
 80a7a68:	6843      	ldrgt	r3, [r0, #4]
 80a7a6a:	1d03      	addle	r3, r0, #4
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
 80a7a6c:	2001      	movs	r0, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a7a6e:	428a      	cmp	r2, r1
 80a7a70:	da04      	bge.n	80a7a7c <_ZNK6tflite12RuntimeShape8FlatSizeEv+0x1e>
      buffer_size *= dims_data[i];
 80a7a72:	f853 4022 	ldr.w	r4, [r3, r2, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a7a76:	3201      	adds	r2, #1
      buffer_size *= dims_data[i];
 80a7a78:	4360      	muls	r0, r4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a7a7a:	e7f8      	b.n	80a7a6e <_ZNK6tflite12RuntimeShape8FlatSizeEv+0x10>
      buffer_size *= dims_data[i];
    }
    return buffer_size;
  }
 80a7a7c:	bd10      	pop	{r4, pc}

080a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>:
  return MatchingArraySize(array1, index1, args...);
}

// Get common shape dim, DCHECKing that they all agree.
inline int MatchingDim(const RuntimeShape& shape1, int index1,
                       const RuntimeShape& shape2, int index2) {
 80a7a7e:	b570      	push	{r4, r5, r6, lr}
 80a7a80:	4615      	mov	r5, r2
 80a7a82:	461e      	mov	r6, r3
  TFLITE_DCHECK_EQ(shape1.Dims(index1), shape2.Dims(index2));
 80a7a84:	f7fa fca0 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a7a88:	4631      	mov	r1, r6
 80a7a8a:	4604      	mov	r4, r0
 80a7a8c:	4628      	mov	r0, r5
 80a7a8e:	f7fa fc9b 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a7a92:	4284      	cmp	r4, r0
 80a7a94:	d001      	beq.n	80a7a9a <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i+0x1c>
 80a7a96:	f008 facb 	bl	80b0030 <abort>
  return shape1.Dims(index1);
}
 80a7a9a:	bd70      	pop	{r4, r5, r6, pc}

080a7a9c <_ZN6tflite29MultiplyByQuantizedMultiplierElli>:
  return SaturatingRoundingDoublingHighMul(x * (1 << left_shift),
                                           quantized_multiplier);
}

inline int32 MultiplyByQuantizedMultiplier(int32 x, int32 quantized_multiplier,
                                           int shift) {
 80a7a9c:	b570      	push	{r4, r5, r6, lr}
  using gemmlowp::RoundingDivideByPOT;
  using gemmlowp::SaturatingRoundingDoublingHighMul;
  int left_shift = shift > 0 ? shift : 0;
 80a7a9e:	ea22 74e2 	bic.w	r4, r2, r2, asr #31
  int right_shift = shift > 0 ? 0 : -shift;
 80a7aa2:	2a00      	cmp	r2, #0
  return RoundingDivideByPOT(SaturatingRoundingDoublingHighMul(
 80a7aa4:	fa00 f004 	lsl.w	r0, r0, r4
inline int32 MultiplyByQuantizedMultiplier(int32 x, int32 quantized_multiplier,
                                           int shift) {
  using gemmlowp::RoundingDivideByPOT;
  using gemmlowp::SaturatingRoundingDoublingHighMul;
  int left_shift = shift > 0 ? shift : 0;
  int right_shift = shift > 0 ? 0 : -shift;
 80a7aa8:	bfd4      	ite	le
 80a7aaa:	4256      	negle	r6, r2
 80a7aac:	2600      	movgt	r6, #0
// This function implements the same computation as the ARMv7 NEON VQRDMULH
// instruction.
template <>
inline std::int32_t SaturatingRoundingDoublingHighMul(std::int32_t a,
                                                      std::int32_t b) {
  bool overflow = a == b && a == std::numeric_limits<std::int32_t>::min();
 80a7aae:	4288      	cmp	r0, r1
 80a7ab0:	d104      	bne.n	80a7abc <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x20>
 80a7ab2:	f101 4300 	add.w	r3, r1, #2147483648	; 0x80000000
 80a7ab6:	425a      	negs	r2, r3
 80a7ab8:	415a      	adcs	r2, r3
 80a7aba:	e000      	b.n	80a7abe <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x22>
 80a7abc:	2200      	movs	r2, #0
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
 80a7abe:	fb80 4501 	smull	r4, r5, r0, r1
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
 80a7ac2:	2c00      	cmp	r4, #0
 80a7ac4:	f175 0300 	sbcs.w	r3, r5, #0
 80a7ac8:	4b18      	ldr	r3, [pc, #96]	; (80a7b2c <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x90>)
 80a7aca:	bfa8      	it	ge
 80a7acc:	f04f 4380 	movge.w	r3, #1073741824	; 0x40000000
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
 80a7ad0:	b97a      	cbnz	r2, 80a7af2 <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x56>
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
 80a7ad2:	18e4      	adds	r4, r4, r3
 80a7ad4:	eb45 75e3 	adc.w	r5, r5, r3, asr #31
 80a7ad8:	2c00      	cmp	r4, #0
 80a7ada:	f175 0300 	sbcs.w	r3, r5, #0
 80a7ade:	da04      	bge.n	80a7aea <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x4e>
 80a7ae0:	f06f 4200 	mvn.w	r2, #2147483648	; 0x80000000
 80a7ae4:	2300      	movs	r3, #0
 80a7ae6:	18a4      	adds	r4, r4, r2
 80a7ae8:	415d      	adcs	r5, r3
 80a7aea:	0fe0      	lsrs	r0, r4, #31
 80a7aec:	ea40 0445 	orr.w	r4, r0, r5, lsl #1
 80a7af0:	e001      	b.n	80a7af6 <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x5a>
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
 80a7af2:	f06f 4400 	mvn.w	r4, #2147483648	; 0x80000000
// Correctly-rounded-to-nearest division by a power-of-two.
// Also known as a rounding arithmetic right shift.
template <typename IntegerType>
inline IntegerType RoundingDivideByPOT(IntegerType x, int exponent) {
  assert(exponent >= 0);
  assert(exponent <= 31);
 80a7af6:	2e1f      	cmp	r6, #31
 80a7af8:	dd06      	ble.n	80a7b08 <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x6c>
 80a7afa:	4b0d      	ldr	r3, [pc, #52]	; (80a7b30 <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x94>)
 80a7afc:	4a0d      	ldr	r2, [pc, #52]	; (80a7b34 <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x98>)
 80a7afe:	f240 1167 	movw	r1, #359	; 0x167
 80a7b02:	480d      	ldr	r0, [pc, #52]	; (80a7b38 <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x9c>)
 80a7b04:	f008 faa4 	bl	80b0050 <__assert_func>
  const IntegerType mask = Dup<IntegerType>((1ll << exponent) - 1);
 80a7b08:	4632      	mov	r2, r6
 80a7b0a:	2001      	movs	r0, #1
 80a7b0c:	2100      	movs	r1, #0
 80a7b0e:	f00a ff69 	bl	80b29e4 <__aeabi_llsl>
 80a7b12:	3801      	subs	r0, #1
  return RoundingDivideByPOT(SaturatingRoundingDoublingHighMul(
                                 x * (1 << left_shift), quantized_multiplier),
                             right_shift);
 80a7b14:	ea00 0304 	and.w	r3, r0, r4
 80a7b18:	1040      	asrs	r0, r0, #1
 80a7b1a:	eb00 70d4 	add.w	r0, r0, r4, lsr #31
 80a7b1e:	4134      	asrs	r4, r6
}
 80a7b20:	4283      	cmp	r3, r0
 80a7b22:	bfd4      	ite	le
 80a7b24:	4620      	movle	r0, r4
 80a7b26:	1c60      	addgt	r0, r4, #1
 80a7b28:	bd70      	pop	{r4, r5, r6, pc}
 80a7b2a:	bf00      	nop
 80a7b2c:	c0000001 	.word	0xc0000001
 80a7b30:	080b5a19 	.word	0x080b5a19
 80a7b34:	080b5e7e 	.word	0x080b5e7e
 80a7b38:	080b597a 	.word	0x080b597a

080a7b3c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_>:
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const float* input_data, const RuntimeShape& filter_shape,
                 const float* filter_data, const RuntimeShape& bias_shape,
                 const float* bias_data, const RuntimeShape& output_shape,
                 float* output_data, const RuntimeShape& im2col_shape,
                 float* im2col_data) {
 80a7b3c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a7b40:	4698      	mov	r8, r3
  const int stride_width = params.stride_width;
 80a7b42:	f9b0 300a 	ldrsh.w	r3, [r0, #10]
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const float* input_data, const RuntimeShape& filter_shape,
                 const float* filter_data, const RuntimeShape& bias_shape,
                 const float* bias_data, const RuntimeShape& output_shape,
                 float* output_data, const RuntimeShape& im2col_shape,
                 float* im2col_data) {
 80a7b46:	b09d      	sub	sp, #116	; 0x74
  const int stride_width = params.stride_width;
 80a7b48:	930c      	str	r3, [sp, #48]	; 0x30
  const int stride_height = params.stride_height;
 80a7b4a:	f9b0 300c 	ldrsh.w	r3, [r0, #12]
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const float* input_data, const RuntimeShape& filter_shape,
                 const float* filter_data, const RuntimeShape& bias_shape,
                 const float* bias_data, const RuntimeShape& output_shape,
                 float* output_data, const RuntimeShape& im2col_shape,
                 float* im2col_data) {
 80a7b4e:	468a      	mov	sl, r1
  const int stride_width = params.stride_width;
  const int stride_height = params.stride_height;
 80a7b50:	930d      	str	r3, [sp, #52]	; 0x34
  const int dilation_width_factor = params.dilation_width_factor;
 80a7b52:	f9b0 300e 	ldrsh.w	r3, [r0, #14]
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const float* input_data, const RuntimeShape& filter_shape,
                 const float* filter_data, const RuntimeShape& bias_shape,
                 const float* bias_data, const RuntimeShape& output_shape,
                 float* output_data, const RuntimeShape& im2col_shape,
                 float* im2col_data) {
 80a7b56:	921b      	str	r2, [sp, #108]	; 0x6c
  const int stride_width = params.stride_width;
  const int stride_height = params.stride_height;
  const int dilation_width_factor = params.dilation_width_factor;
 80a7b58:	930e      	str	r3, [sp, #56]	; 0x38
  const int dilation_height_factor = params.dilation_height_factor;
 80a7b5a:	f9b0 3010 	ldrsh.w	r3, [r0, #16]
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const float* input_data, const RuntimeShape& filter_shape,
                 const float* filter_data, const RuntimeShape& bias_shape,
                 const float* bias_data, const RuntimeShape& output_shape,
                 float* output_data, const RuntimeShape& im2col_shape,
                 float* im2col_data) {
 80a7b5e:	f8dd 90a4 	ldr.w	r9, [sp, #164]	; 0xa4
  const int stride_width = params.stride_width;
  const int stride_height = params.stride_height;
  const int dilation_width_factor = params.dilation_width_factor;
  const int dilation_height_factor = params.dilation_height_factor;
 80a7b62:	930f      	str	r3, [sp, #60]	; 0x3c
  const int pad_width = params.padding_values.width;
 80a7b64:	f9b0 3002 	ldrsh.w	r3, [r0, #2]
 80a7b68:	9310      	str	r3, [sp, #64]	; 0x40
  const int pad_height = params.padding_values.height;
 80a7b6a:	f9b0 3004 	ldrsh.w	r3, [r0, #4]
 80a7b6e:	9311      	str	r3, [sp, #68]	; 0x44
  const float output_activation_min = params.float_activation_min;
 80a7b70:	6b03      	ldr	r3, [r0, #48]	; 0x30
 80a7b72:	9309      	str	r3, [sp, #36]	; 0x24
  const float output_activation_max = params.float_activation_max;
 80a7b74:	6b43      	ldr	r3, [r0, #52]	; 0x34
 80a7b76:	930a      	str	r3, [sp, #40]	; 0x28
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
 80a7b78:	680b      	ldr	r3, [r1, #0]
 80a7b7a:	2b04      	cmp	r3, #4
 80a7b7c:	d001      	beq.n	80a7b82 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x46>
 80a7b7e:	f008 fa57 	bl	80b0030 <abort>
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
 80a7b82:	f8d8 3000 	ldr.w	r3, [r8]
 80a7b86:	2b04      	cmp	r3, #4
 80a7b88:	d1f9      	bne.n	80a7b7e <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x42>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
 80a7b8a:	f8d9 3000 	ldr.w	r3, [r9]
 80a7b8e:	2b04      	cmp	r3, #4
 80a7b90:	d1f5      	bne.n	80a7b7e <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x42>

  (void)im2col_data;   // only used in optimized code.
  (void)im2col_shape;  // only used in optimized code.
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80a7b92:	2300      	movs	r3, #0
 80a7b94:	4619      	mov	r1, r3
 80a7b96:	464a      	mov	r2, r9
 80a7b98:	4650      	mov	r0, sl
 80a7b9a:	f7ff ff70 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
 80a7b9e:	2303      	movs	r3, #3
 80a7ba0:	4619      	mov	r1, r3
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  (void)im2col_data;   // only used in optimized code.
  (void)im2col_shape;  // only used in optimized code.
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80a7ba2:	9012      	str	r0, [sp, #72]	; 0x48
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
 80a7ba4:	4642      	mov	r2, r8
 80a7ba6:	4650      	mov	r0, sl
 80a7ba8:	f7ff ff69 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
 80a7bac:	2303      	movs	r3, #3
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  (void)im2col_data;   // only used in optimized code.
  (void)im2col_shape;  // only used in optimized code.
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
 80a7bae:	9013      	str	r0, [sp, #76]	; 0x4c
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
 80a7bb0:	464a      	mov	r2, r9
 80a7bb2:	2100      	movs	r1, #0
 80a7bb4:	4640      	mov	r0, r8
 80a7bb6:	f7ff ff62 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  if (bias_data) {
 80a7bba:	9b28      	ldr	r3, [sp, #160]	; 0xa0

  (void)im2col_data;   // only used in optimized code.
  (void)im2col_shape;  // only used in optimized code.
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
 80a7bbc:	900b      	str	r0, [sp, #44]	; 0x2c
  if (bias_data) {
 80a7bbe:	b12b      	cbz	r3, 80a7bcc <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x90>
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
 80a7bc0:	9827      	ldr	r0, [sp, #156]	; 0x9c
 80a7bc2:	f7ff ff4c 	bl	80a7a5e <_ZNK6tflite12RuntimeShape8FlatSizeEv>
 80a7bc6:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80a7bc8:	4283      	cmp	r3, r0
 80a7bca:	d1d8      	bne.n	80a7b7e <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x42>
  }
  const int input_height = input_shape.Dims(1);
 80a7bcc:	2101      	movs	r1, #1
 80a7bce:	4650      	mov	r0, sl
 80a7bd0:	f7fa fbfa 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
 80a7bd4:	2102      	movs	r1, #2
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
 80a7bd6:	9014      	str	r0, [sp, #80]	; 0x50
  const int input_width = input_shape.Dims(2);
 80a7bd8:	4650      	mov	r0, sl
 80a7bda:	f7fa fbf5 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_height = filter_shape.Dims(1);
 80a7bde:	2101      	movs	r1, #1
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
 80a7be0:	9015      	str	r0, [sp, #84]	; 0x54
  const int filter_height = filter_shape.Dims(1);
 80a7be2:	4640      	mov	r0, r8
 80a7be4:	f7fa fbf0 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_width = filter_shape.Dims(2);
 80a7be8:	2102      	movs	r1, #2
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
 80a7bea:	9016      	str	r0, [sp, #88]	; 0x58
  const int filter_width = filter_shape.Dims(2);
 80a7bec:	4640      	mov	r0, r8
 80a7bee:	f7fa fbeb 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
 80a7bf2:	2101      	movs	r1, #1
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
 80a7bf4:	9017      	str	r0, [sp, #92]	; 0x5c
  const int output_height = output_shape.Dims(1);
 80a7bf6:	4648      	mov	r0, r9
 80a7bf8:	f7fa fbe6 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
 80a7bfc:	2102      	movs	r1, #2
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
 80a7bfe:	9018      	str	r0, [sp, #96]	; 0x60
  const int output_width = output_shape.Dims(2);
 80a7c00:	4648      	mov	r0, r9
 80a7c02:	f7fa fbe1 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  for (int batch = 0; batch < batches; ++batch) {
 80a7c06:	f04f 0b00 	mov.w	fp, #0
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
 80a7c0a:	9019      	str	r0, [sp, #100]	; 0x64
  for (int batch = 0; batch < batches; ++batch) {
 80a7c0c:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80a7c0e:	459b      	cmp	fp, r3
 80a7c10:	f280 8097 	bge.w	80a7d42 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x206>
 80a7c14:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80a7c16:	425b      	negs	r3, r3
 80a7c18:	9308      	str	r3, [sp, #32]
 80a7c1a:	2300      	movs	r3, #0
 80a7c1c:	9302      	str	r3, [sp, #8]
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80a7c1e:	9b02      	ldr	r3, [sp, #8]
 80a7c20:	9a18      	ldr	r2, [sp, #96]	; 0x60
 80a7c22:	4293      	cmp	r3, r2
 80a7c24:	f280 808a 	bge.w	80a7d3c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x200>
 80a7c28:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80a7c2a:	425b      	negs	r3, r3
 80a7c2c:	9307      	str	r3, [sp, #28]
 80a7c2e:	2300      	movs	r3, #0
 80a7c30:	9303      	str	r3, [sp, #12]
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80a7c32:	9b03      	ldr	r3, [sp, #12]
 80a7c34:	9a19      	ldr	r2, [sp, #100]	; 0x64
 80a7c36:	4293      	cmp	r3, r2
 80a7c38:	da78      	bge.n	80a7d2c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x1f0>
 80a7c3a:	2400      	movs	r4, #0
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
 80a7c3c:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80a7c3e:	429c      	cmp	r4, r3
 80a7c40:	da6c      	bge.n	80a7d1c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x1e0>
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          float total = 0.f;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
 80a7c42:	2300      	movs	r3, #0
 80a7c44:	9304      	str	r3, [sp, #16]
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
 80a7c46:	9d08      	ldr	r5, [sp, #32]
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          float total = 0.f;
 80a7c48:	2300      	movs	r3, #0
 80a7c4a:	9306      	str	r3, [sp, #24]
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
 80a7c4c:	9b04      	ldr	r3, [sp, #16]
 80a7c4e:	9a16      	ldr	r2, [sp, #88]	; 0x58
 80a7c50:	4293      	cmp	r3, r2
 80a7c52:	da3f      	bge.n	80a7cd4 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x198>
 80a7c54:	2300      	movs	r3, #0
 80a7c56:	9e07      	ldr	r6, [sp, #28]
 80a7c58:	9305      	str	r3, [sp, #20]
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
 80a7c5a:	9b05      	ldr	r3, [sp, #20]
 80a7c5c:	9a17      	ldr	r2, [sp, #92]	; 0x5c
 80a7c5e:	4293      	cmp	r3, r2
 80a7c60:	da32      	bge.n	80a7cc8 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x18c>
 80a7c62:	2700      	movs	r7, #0
              for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
 80a7c64:	9b13      	ldr	r3, [sp, #76]	; 0x4c
 80a7c66:	429f      	cmp	r7, r3
 80a7c68:	da28      	bge.n	80a7cbc <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x180>
                const int in_x = in_x_origin + dilation_width_factor * filter_x;
                const int in_y =
                    in_y_origin + dilation_height_factor * filter_y;
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
 80a7c6a:	2e00      	cmp	r6, #0
 80a7c6c:	db24      	blt.n	80a7cb8 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x17c>
 80a7c6e:	9b15      	ldr	r3, [sp, #84]	; 0x54
 80a7c70:	42b3      	cmp	r3, r6
 80a7c72:	dd21      	ble.n	80a7cb8 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x17c>
 80a7c74:	2d00      	cmp	r5, #0
 80a7c76:	db1f      	blt.n	80a7cb8 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x17c>
 80a7c78:	9b14      	ldr	r3, [sp, #80]	; 0x50
 80a7c7a:	42ab      	cmp	r3, r5
 80a7c7c:	dd1c      	ble.n	80a7cb8 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x17c>
                    (in_y < input_height)) {
                  float input_value = input_data[Offset(
 80a7c7e:	4633      	mov	r3, r6
 80a7c80:	462a      	mov	r2, r5
 80a7c82:	4659      	mov	r1, fp
 80a7c84:	9700      	str	r7, [sp, #0]
 80a7c86:	4650      	mov	r0, sl
 80a7c88:	f7fa fc03 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                      input_shape, batch, in_y, in_x, in_channel)];
                  float filter_value =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
 80a7c8c:	9b05      	ldr	r3, [sp, #20]
                    in_y_origin + dilation_height_factor * filter_y;
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height)) {
                  float input_value = input_data[Offset(
 80a7c8e:	901a      	str	r0, [sp, #104]	; 0x68
                      input_shape, batch, in_y, in_x, in_channel)];
                  float filter_value =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
 80a7c90:	9a04      	ldr	r2, [sp, #16]
 80a7c92:	4621      	mov	r1, r4
 80a7c94:	9700      	str	r7, [sp, #0]
 80a7c96:	4640      	mov	r0, r8
 80a7c98:	f7fa fbfb 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                                         filter_x, in_channel)];
                  total += (input_value * filter_value);
 80a7c9c:	9b26      	ldr	r3, [sp, #152]	; 0x98
 80a7c9e:	9a1a      	ldr	r2, [sp, #104]	; 0x68
 80a7ca0:	f853 1020 	ldr.w	r1, [r3, r0, lsl #2]
 80a7ca4:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
 80a7ca6:	f853 0022 	ldr.w	r0, [r3, r2, lsl #2]
 80a7caa:	f00b fc9f 	bl	80b35ec <__aeabi_fmul>
 80a7cae:	4601      	mov	r1, r0
 80a7cb0:	9806      	ldr	r0, [sp, #24]
 80a7cb2:	f00b fb93 	bl	80b33dc <__addsf3>
 80a7cb6:	9006      	str	r0, [sp, #24]
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          float total = 0.f;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
              for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
 80a7cb8:	3701      	adds	r7, #1
 80a7cba:	e7d3      	b.n	80a7c64 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x128>
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          float total = 0.f;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
 80a7cbc:	9b05      	ldr	r3, [sp, #20]
 80a7cbe:	3301      	adds	r3, #1
 80a7cc0:	9305      	str	r3, [sp, #20]
 80a7cc2:	9b0e      	ldr	r3, [sp, #56]	; 0x38
 80a7cc4:	441e      	add	r6, r3
 80a7cc6:	e7c8      	b.n	80a7c5a <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x11e>
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          float total = 0.f;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
 80a7cc8:	9b04      	ldr	r3, [sp, #16]
 80a7cca:	3301      	adds	r3, #1
 80a7ccc:	9304      	str	r3, [sp, #16]
 80a7cce:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80a7cd0:	441d      	add	r5, r3
 80a7cd2:	e7bb      	b.n	80a7c4c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x110>
                }
              }
            }
          }
          float bias_value = 0.0f;
          if (bias_data) {
 80a7cd4:	9b28      	ldr	r3, [sp, #160]	; 0xa0
 80a7cd6:	b113      	cbz	r3, 80a7cde <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x1a2>
            bias_value = bias_data[out_channel];
 80a7cd8:	f853 5024 	ldr.w	r5, [r3, r4, lsl #2]
 80a7cdc:	e000      	b.n	80a7ce0 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x1a4>
                  total += (input_value * filter_value);
                }
              }
            }
          }
          float bias_value = 0.0f;
 80a7cde:	2500      	movs	r5, #0
          if (bias_data) {
            bias_value = bias_data[out_channel];
          }
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
 80a7ce0:	9b03      	ldr	r3, [sp, #12]
 80a7ce2:	9a02      	ldr	r2, [sp, #8]
 80a7ce4:	4659      	mov	r1, fp
 80a7ce6:	9400      	str	r4, [sp, #0]
 80a7ce8:	4648      	mov	r0, r9
 80a7cea:	f7fa fbd2 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              ActivationFunctionWithMinMax(total + bias_value,
 80a7cee:	4629      	mov	r1, r5
          }
          float bias_value = 0.0f;
          if (bias_data) {
            bias_value = bias_data[out_channel];
          }
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
 80a7cf0:	4606      	mov	r6, r0
              ActivationFunctionWithMinMax(total + bias_value,
 80a7cf2:	9806      	ldr	r0, [sp, #24]
 80a7cf4:	f00b fb72 	bl	80b33dc <__addsf3>
 80a7cf8:	4605      	mov	r5, r0
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
 80a7cfa:	4601      	mov	r1, r0
 80a7cfc:	9809      	ldr	r0, [sp, #36]	; 0x24
 80a7cfe:	f00b fe31 	bl	80b3964 <__aeabi_fcmpgt>
 80a7d02:	b100      	cbz	r0, 80a7d06 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x1ca>
	return __b;
 80a7d04:	9d09      	ldr	r5, [sp, #36]	; 0x24
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
 80a7d06:	4629      	mov	r1, r5
 80a7d08:	980a      	ldr	r0, [sp, #40]	; 0x28
 80a7d0a:	f00b fe0d 	bl	80b3928 <__aeabi_fcmplt>
 80a7d0e:	b100      	cbz	r0, 80a7d12 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x1d6>
	return __b;
 80a7d10:	9d0a      	ldr	r5, [sp, #40]	; 0x28
                                           output_activation_min,
                                           output_activation_max);
 80a7d12:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
 80a7d14:	3401      	adds	r4, #1
            bias_value = bias_data[out_channel];
          }
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
              ActivationFunctionWithMinMax(total + bias_value,
                                           output_activation_min,
                                           output_activation_max);
 80a7d16:	f843 5026 	str.w	r5, [r3, r6, lsl #2]
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
 80a7d1a:	e78f      	b.n	80a7c3c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x100>
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80a7d1c:	9b03      	ldr	r3, [sp, #12]
 80a7d1e:	9a0c      	ldr	r2, [sp, #48]	; 0x30
 80a7d20:	3301      	adds	r3, #1
 80a7d22:	9303      	str	r3, [sp, #12]
 80a7d24:	9b07      	ldr	r3, [sp, #28]
 80a7d26:	4413      	add	r3, r2
 80a7d28:	9307      	str	r3, [sp, #28]
 80a7d2a:	e782      	b.n	80a7c32 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0xf6>
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80a7d2c:	9b02      	ldr	r3, [sp, #8]
 80a7d2e:	9a0d      	ldr	r2, [sp, #52]	; 0x34
 80a7d30:	3301      	adds	r3, #1
 80a7d32:	9302      	str	r3, [sp, #8]
 80a7d34:	9b08      	ldr	r3, [sp, #32]
 80a7d36:	4413      	add	r3, r2
 80a7d38:	9308      	str	r3, [sp, #32]
 80a7d3a:	e770      	b.n	80a7c1e <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0xe2>
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
 80a7d3c:	f10b 0b01 	add.w	fp, fp, #1
 80a7d40:	e764      	b.n	80a7c0c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0xd0>
                                           output_activation_max);
        }
      }
    }
  }
}
 80a7d42:	b01d      	add	sp, #116	; 0x74
 80a7d44:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a7d48 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv>:
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const uint8* input_data, const RuntimeShape& filter_shape,
                 const uint8* filter_data, const RuntimeShape& bias_shape,
                 const int32* bias_data, const RuntimeShape& output_shape,
                 uint8* output_data, const RuntimeShape& im2col_shape,
                 uint8* im2col_data, void* cpu_backend_context) {
 80a7d48:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a7d4c:	469a      	mov	sl, r3
  (void)cpu_backend_context;  // only used in optimized code.
  (void)im2col_data;   // only used in optimized code.
  (void)im2col_shape;  // only used in optimized code.
  const int stride_width = params.stride_width;
 80a7d4e:	f9b0 300a 	ldrsh.w	r3, [r0, #10]
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const uint8* input_data, const RuntimeShape& filter_shape,
                 const uint8* filter_data, const RuntimeShape& bias_shape,
                 const int32* bias_data, const RuntimeShape& output_shape,
                 uint8* output_data, const RuntimeShape& im2col_shape,
                 uint8* im2col_data, void* cpu_backend_context) {
 80a7d52:	b0a3      	sub	sp, #140	; 0x8c
  (void)cpu_backend_context;  // only used in optimized code.
  (void)im2col_data;   // only used in optimized code.
  (void)im2col_shape;  // only used in optimized code.
  const int stride_width = params.stride_width;
 80a7d54:	930d      	str	r3, [sp, #52]	; 0x34
  const int stride_height = params.stride_height;
 80a7d56:	f9b0 300c 	ldrsh.w	r3, [r0, #12]
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const uint8* input_data, const RuntimeShape& filter_shape,
                 const uint8* filter_data, const RuntimeShape& bias_shape,
                 const int32* bias_data, const RuntimeShape& output_shape,
                 uint8* output_data, const RuntimeShape& im2col_shape,
                 uint8* im2col_data, void* cpu_backend_context) {
 80a7d5a:	9221      	str	r2, [sp, #132]	; 0x84
  (void)cpu_backend_context;  // only used in optimized code.
  (void)im2col_data;   // only used in optimized code.
  (void)im2col_shape;  // only used in optimized code.
  const int stride_width = params.stride_width;
  const int stride_height = params.stride_height;
 80a7d5c:	930e      	str	r3, [sp, #56]	; 0x38
  const int dilation_width_factor = params.dilation_width_factor;
 80a7d5e:	f9b0 300e 	ldrsh.w	r3, [r0, #14]
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const uint8* input_data, const RuntimeShape& filter_shape,
                 const uint8* filter_data, const RuntimeShape& bias_shape,
                 const int32* bias_data, const RuntimeShape& output_shape,
                 uint8* output_data, const RuntimeShape& im2col_shape,
                 uint8* im2col_data, void* cpu_backend_context) {
 80a7d62:	4689      	mov	r9, r1
  (void)cpu_backend_context;  // only used in optimized code.
  (void)im2col_data;   // only used in optimized code.
  (void)im2col_shape;  // only used in optimized code.
  const int stride_width = params.stride_width;
  const int stride_height = params.stride_height;
  const int dilation_width_factor = params.dilation_width_factor;
 80a7d64:	930f      	str	r3, [sp, #60]	; 0x3c
  const int dilation_height_factor = params.dilation_height_factor;
 80a7d66:	f9b0 3010 	ldrsh.w	r3, [r0, #16]
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const uint8* input_data, const RuntimeShape& filter_shape,
                 const uint8* filter_data, const RuntimeShape& bias_shape,
                 const int32* bias_data, const RuntimeShape& output_shape,
                 uint8* output_data, const RuntimeShape& im2col_shape,
                 uint8* im2col_data, void* cpu_backend_context) {
 80a7d6a:	f8dd 80bc 	ldr.w	r8, [sp, #188]	; 0xbc
  (void)im2col_data;   // only used in optimized code.
  (void)im2col_shape;  // only used in optimized code.
  const int stride_width = params.stride_width;
  const int stride_height = params.stride_height;
  const int dilation_width_factor = params.dilation_width_factor;
  const int dilation_height_factor = params.dilation_height_factor;
 80a7d6e:	9310      	str	r3, [sp, #64]	; 0x40
  const int pad_width = params.padding_values.width;
 80a7d70:	f9b0 3002 	ldrsh.w	r3, [r0, #2]
 80a7d74:	9311      	str	r3, [sp, #68]	; 0x44
  const int pad_height = params.padding_values.height;
 80a7d76:	f9b0 3004 	ldrsh.w	r3, [r0, #4]
 80a7d7a:	9312      	str	r3, [sp, #72]	; 0x48
  const int32 input_offset = params.input_offset;
 80a7d7c:	6943      	ldr	r3, [r0, #20]
 80a7d7e:	9313      	str	r3, [sp, #76]	; 0x4c
  const int32 filter_offset = params.weights_offset;
 80a7d80:	6983      	ldr	r3, [r0, #24]
 80a7d82:	9314      	str	r3, [sp, #80]	; 0x50
  const int32 output_offset = params.output_offset;
 80a7d84:	69c3      	ldr	r3, [r0, #28]
 80a7d86:	9315      	str	r3, [sp, #84]	; 0x54
  const int32 output_multiplier = params.output_multiplier;
 80a7d88:	6a03      	ldr	r3, [r0, #32]
 80a7d8a:	9316      	str	r3, [sp, #88]	; 0x58
  const int output_shift = params.output_shift;
 80a7d8c:	6a43      	ldr	r3, [r0, #36]	; 0x24
 80a7d8e:	9317      	str	r3, [sp, #92]	; 0x5c
  const int32 output_activation_min = params.quantized_activation_min;
 80a7d90:	6a83      	ldr	r3, [r0, #40]	; 0x28
 80a7d92:	930a      	str	r3, [sp, #40]	; 0x28
  const int32 output_activation_max = params.quantized_activation_max;
 80a7d94:	6ac3      	ldr	r3, [r0, #44]	; 0x2c
 80a7d96:	930b      	str	r3, [sp, #44]	; 0x2c
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
 80a7d98:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
 80a7d9a:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80a7d9c:	4293      	cmp	r3, r2
 80a7d9e:	dd01      	ble.n	80a7da4 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x5c>
 80a7da0:	f008 f946 	bl	80b0030 <abort>

  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
 80a7da4:	680b      	ldr	r3, [r1, #0]
 80a7da6:	2b04      	cmp	r3, #4
 80a7da8:	d1fa      	bne.n	80a7da0 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x58>
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
 80a7daa:	f8da 3000 	ldr.w	r3, [sl]
 80a7dae:	2b04      	cmp	r3, #4
 80a7db0:	d1f6      	bne.n	80a7da0 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x58>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
 80a7db2:	f8d8 3000 	ldr.w	r3, [r8]
 80a7db6:	2b04      	cmp	r3, #4
 80a7db8:	d1f2      	bne.n	80a7da0 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x58>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80a7dba:	2300      	movs	r3, #0
 80a7dbc:	4619      	mov	r1, r3
 80a7dbe:	4642      	mov	r2, r8
 80a7dc0:	4648      	mov	r0, r9
 80a7dc2:	f7ff fe5c 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
 80a7dc6:	2303      	movs	r3, #3
 80a7dc8:	4619      	mov	r1, r3
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);

  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80a7dca:	9018      	str	r0, [sp, #96]	; 0x60
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
 80a7dcc:	4652      	mov	r2, sl
 80a7dce:	4648      	mov	r0, r9
 80a7dd0:	f7ff fe55 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
 80a7dd4:	2303      	movs	r3, #3

  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
 80a7dd6:	9019      	str	r0, [sp, #100]	; 0x64
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
 80a7dd8:	4642      	mov	r2, r8
 80a7dda:	2100      	movs	r1, #0
 80a7ddc:	4650      	mov	r0, sl
 80a7dde:	f7ff fe4e 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  if (bias_data) {
 80a7de2:	9b2e      	ldr	r3, [sp, #184]	; 0xb8
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
 80a7de4:	900c      	str	r0, [sp, #48]	; 0x30
  if (bias_data) {
 80a7de6:	b12b      	cbz	r3, 80a7df4 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0xac>
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
 80a7de8:	982d      	ldr	r0, [sp, #180]	; 0xb4
 80a7dea:	f7ff fe38 	bl	80a7a5e <_ZNK6tflite12RuntimeShape8FlatSizeEv>
 80a7dee:	9b0c      	ldr	r3, [sp, #48]	; 0x30
 80a7df0:	4283      	cmp	r3, r0
 80a7df2:	d1d5      	bne.n	80a7da0 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x58>
  }
  const int input_height = input_shape.Dims(1);
 80a7df4:	2101      	movs	r1, #1
 80a7df6:	4648      	mov	r0, r9
 80a7df8:	f7fa fae6 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
 80a7dfc:	2102      	movs	r1, #2
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
 80a7dfe:	901a      	str	r0, [sp, #104]	; 0x68
  const int input_width = input_shape.Dims(2);
 80a7e00:	4648      	mov	r0, r9
 80a7e02:	f7fa fae1 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_height = filter_shape.Dims(1);
 80a7e06:	2101      	movs	r1, #1
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
 80a7e08:	901b      	str	r0, [sp, #108]	; 0x6c
  const int filter_height = filter_shape.Dims(1);
 80a7e0a:	4650      	mov	r0, sl
 80a7e0c:	f7fa fadc 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_width = filter_shape.Dims(2);
 80a7e10:	2102      	movs	r1, #2
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
 80a7e12:	901c      	str	r0, [sp, #112]	; 0x70
  const int filter_width = filter_shape.Dims(2);
 80a7e14:	4650      	mov	r0, sl
 80a7e16:	f7fa fad7 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
 80a7e1a:	2101      	movs	r1, #1
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
 80a7e1c:	901d      	str	r0, [sp, #116]	; 0x74
  const int output_height = output_shape.Dims(1);
 80a7e1e:	4640      	mov	r0, r8
 80a7e20:	f7fa fad2 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
 80a7e24:	2102      	movs	r1, #2
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
 80a7e26:	901e      	str	r0, [sp, #120]	; 0x78
  const int output_width = output_shape.Dims(2);
 80a7e28:	4640      	mov	r0, r8
 80a7e2a:	f7fa facd 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  for (int batch = 0; batch < batches; ++batch) {
 80a7e2e:	f04f 0b00 	mov.w	fp, #0
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
 80a7e32:	901f      	str	r0, [sp, #124]	; 0x7c
  for (int batch = 0; batch < batches; ++batch) {
 80a7e34:	9b18      	ldr	r3, [sp, #96]	; 0x60
 80a7e36:	459b      	cmp	fp, r3
 80a7e38:	f280 8093 	bge.w	80a7f62 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x21a>
 80a7e3c:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80a7e3e:	425b      	negs	r3, r3
 80a7e40:	9309      	str	r3, [sp, #36]	; 0x24
 80a7e42:	2300      	movs	r3, #0
 80a7e44:	9304      	str	r3, [sp, #16]
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80a7e46:	9b04      	ldr	r3, [sp, #16]
 80a7e48:	9a1e      	ldr	r2, [sp, #120]	; 0x78
 80a7e4a:	4293      	cmp	r3, r2
 80a7e4c:	f280 8086 	bge.w	80a7f5c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x214>
 80a7e50:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80a7e52:	425b      	negs	r3, r3
 80a7e54:	9308      	str	r3, [sp, #32]
 80a7e56:	2300      	movs	r3, #0
 80a7e58:	9305      	str	r3, [sp, #20]
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80a7e5a:	9b05      	ldr	r3, [sp, #20]
 80a7e5c:	9a1f      	ldr	r2, [sp, #124]	; 0x7c
 80a7e5e:	4293      	cmp	r3, r2
 80a7e60:	da74      	bge.n	80a7f4c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x204>
 80a7e62:	2400      	movs	r4, #0
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
 80a7e64:	9b0c      	ldr	r3, [sp, #48]	; 0x30
 80a7e66:	429c      	cmp	r4, r3
 80a7e68:	da68      	bge.n	80a7f3c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x1f4>
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
 80a7e6a:	2500      	movs	r5, #0
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
 80a7e6c:	9e09      	ldr	r6, [sp, #36]	; 0x24
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
 80a7e6e:	9506      	str	r5, [sp, #24]
 80a7e70:	9b06      	ldr	r3, [sp, #24]
 80a7e72:	9a1c      	ldr	r2, [sp, #112]	; 0x70
 80a7e74:	4293      	cmp	r3, r2
 80a7e76:	da41      	bge.n	80a7efc <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x1b4>
 80a7e78:	2300      	movs	r3, #0
 80a7e7a:	9f08      	ldr	r7, [sp, #32]
 80a7e7c:	9307      	str	r3, [sp, #28]
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
 80a7e7e:	9b07      	ldr	r3, [sp, #28]
 80a7e80:	9a1d      	ldr	r2, [sp, #116]	; 0x74
 80a7e82:	4293      	cmp	r3, r2
 80a7e84:	da34      	bge.n	80a7ef0 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x1a8>
 80a7e86:	2300      	movs	r3, #0
 80a7e88:	9303      	str	r3, [sp, #12]
              for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
 80a7e8a:	9b03      	ldr	r3, [sp, #12]
 80a7e8c:	9a19      	ldr	r2, [sp, #100]	; 0x64
 80a7e8e:	4293      	cmp	r3, r2
 80a7e90:	da28      	bge.n	80a7ee4 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x19c>
                const int in_x = in_x_origin + dilation_width_factor * filter_x;
                const int in_y =
                    in_y_origin + dilation_height_factor * filter_y;
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
 80a7e92:	2f00      	cmp	r7, #0
 80a7e94:	db23      	blt.n	80a7ede <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x196>
 80a7e96:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
 80a7e98:	42bb      	cmp	r3, r7
 80a7e9a:	dd20      	ble.n	80a7ede <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x196>
 80a7e9c:	2e00      	cmp	r6, #0
 80a7e9e:	db1e      	blt.n	80a7ede <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x196>
 80a7ea0:	9b1a      	ldr	r3, [sp, #104]	; 0x68
 80a7ea2:	42b3      	cmp	r3, r6
 80a7ea4:	dd1b      	ble.n	80a7ede <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x196>
                    (in_y < input_height)) {
                  int32 input_val = input_data[Offset(input_shape, batch, in_y,
 80a7ea6:	9b03      	ldr	r3, [sp, #12]
 80a7ea8:	4632      	mov	r2, r6
 80a7eaa:	9300      	str	r3, [sp, #0]
 80a7eac:	4659      	mov	r1, fp
 80a7eae:	463b      	mov	r3, r7
 80a7eb0:	4648      	mov	r0, r9
 80a7eb2:	f7fa faee 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                                                      in_x, in_channel)];
                  int32 filter_val =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
 80a7eb6:	9b03      	ldr	r3, [sp, #12]
                    in_y_origin + dilation_height_factor * filter_y;
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height)) {
                  int32 input_val = input_data[Offset(input_shape, batch, in_y,
 80a7eb8:	9020      	str	r0, [sp, #128]	; 0x80
                                                      in_x, in_channel)];
                  int32 filter_val =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
 80a7eba:	9300      	str	r3, [sp, #0]
 80a7ebc:	9a06      	ldr	r2, [sp, #24]
 80a7ebe:	9b07      	ldr	r3, [sp, #28]
 80a7ec0:	4621      	mov	r1, r4
 80a7ec2:	4650      	mov	r0, sl
 80a7ec4:	f7fa fae5 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                                         filter_x, in_channel)];
                  acc +=
                      (filter_val + filter_offset) * (input_val + input_offset);
 80a7ec8:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
 80a7eca:	9a14      	ldr	r2, [sp, #80]	; 0x50
 80a7ecc:	5c1b      	ldrb	r3, [r3, r0]
 80a7ece:	9920      	ldr	r1, [sp, #128]	; 0x80
 80a7ed0:	4413      	add	r3, r2
 80a7ed2:	9a21      	ldr	r2, [sp, #132]	; 0x84
 80a7ed4:	5c52      	ldrb	r2, [r2, r1]
 80a7ed6:	9913      	ldr	r1, [sp, #76]	; 0x4c
 80a7ed8:	440a      	add	r2, r1
 80a7eda:	fb02 5503 	mla	r5, r2, r3, r5
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
              for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
 80a7ede:	9b03      	ldr	r3, [sp, #12]
 80a7ee0:	3301      	adds	r3, #1
 80a7ee2:	e7d1      	b.n	80a7e88 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x140>
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
 80a7ee4:	9b07      	ldr	r3, [sp, #28]
 80a7ee6:	3301      	adds	r3, #1
 80a7ee8:	9307      	str	r3, [sp, #28]
 80a7eea:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80a7eec:	441f      	add	r7, r3
 80a7eee:	e7c6      	b.n	80a7e7e <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x136>
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
 80a7ef0:	9b06      	ldr	r3, [sp, #24]
 80a7ef2:	3301      	adds	r3, #1
 80a7ef4:	9306      	str	r3, [sp, #24]
 80a7ef6:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80a7ef8:	441e      	add	r6, r3
 80a7efa:	e7b9      	b.n	80a7e70 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x128>
                      (filter_val + filter_offset) * (input_val + input_offset);
                }
              }
            }
          }
          if (bias_data) {
 80a7efc:	9b2e      	ldr	r3, [sp, #184]	; 0xb8
 80a7efe:	b113      	cbz	r3, 80a7f06 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x1be>
            acc += bias_data[out_channel];
 80a7f00:	f853 3024 	ldr.w	r3, [r3, r4, lsl #2]
 80a7f04:	441d      	add	r5, r3
          }
          acc = MultiplyByQuantizedMultiplier(acc, output_multiplier,
 80a7f06:	9a17      	ldr	r2, [sp, #92]	; 0x5c
 80a7f08:	9916      	ldr	r1, [sp, #88]	; 0x58
 80a7f0a:	4628      	mov	r0, r5
 80a7f0c:	f7ff fdc6 	bl	80a7a9c <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
                                              output_shift);
          acc += output_offset;
 80a7f10:	9b15      	ldr	r3, [sp, #84]	; 0x54
          acc = std::max(acc, output_activation_min);
          acc = std::min(acc, output_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
 80a7f12:	9400      	str	r4, [sp, #0]
          if (bias_data) {
            acc += bias_data[out_channel];
          }
          acc = MultiplyByQuantizedMultiplier(acc, output_multiplier,
                                              output_shift);
          acc += output_offset;
 80a7f14:	4418      	add	r0, r3
 80a7f16:	9b0a      	ldr	r3, [sp, #40]	; 0x28
          acc = std::max(acc, output_activation_min);
          acc = std::min(acc, output_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
 80a7f18:	9a04      	ldr	r2, [sp, #16]
 80a7f1a:	4283      	cmp	r3, r0
 80a7f1c:	bfb8      	it	lt
 80a7f1e:	4603      	movlt	r3, r0
 80a7f20:	461d      	mov	r5, r3
 80a7f22:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80a7f24:	4659      	mov	r1, fp
 80a7f26:	429d      	cmp	r5, r3
 80a7f28:	bfa8      	it	ge
 80a7f2a:	461d      	movge	r5, r3
 80a7f2c:	4640      	mov	r0, r8
 80a7f2e:	9b05      	ldr	r3, [sp, #20]
 80a7f30:	f7fa faaf 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<uint8>(acc);
 80a7f34:	9b30      	ldr	r3, [sp, #192]	; 0xc0
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
 80a7f36:	3401      	adds	r4, #1
                                              output_shift);
          acc += output_offset;
          acc = std::max(acc, output_activation_min);
          acc = std::min(acc, output_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
              static_cast<uint8>(acc);
 80a7f38:	541d      	strb	r5, [r3, r0]
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
 80a7f3a:	e793      	b.n	80a7e64 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x11c>
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80a7f3c:	9b05      	ldr	r3, [sp, #20]
 80a7f3e:	9a0d      	ldr	r2, [sp, #52]	; 0x34
 80a7f40:	3301      	adds	r3, #1
 80a7f42:	9305      	str	r3, [sp, #20]
 80a7f44:	9b08      	ldr	r3, [sp, #32]
 80a7f46:	4413      	add	r3, r2
 80a7f48:	9308      	str	r3, [sp, #32]
 80a7f4a:	e786      	b.n	80a7e5a <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x112>
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80a7f4c:	9b04      	ldr	r3, [sp, #16]
 80a7f4e:	9a0e      	ldr	r2, [sp, #56]	; 0x38
 80a7f50:	3301      	adds	r3, #1
 80a7f52:	9304      	str	r3, [sp, #16]
 80a7f54:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80a7f56:	4413      	add	r3, r2
 80a7f58:	9309      	str	r3, [sp, #36]	; 0x24
 80a7f5a:	e774      	b.n	80a7e46 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0xfe>
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
 80a7f5c:	f10b 0b01 	add.w	fp, fp, #1
 80a7f60:	e768      	b.n	80a7e34 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0xec>
              static_cast<uint8>(acc);
        }
      }
    }
  }
}
 80a7f62:	b023      	add	sp, #140	; 0x8c
 80a7f64:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a7f68 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa>:
    const ConvParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
 80a7f68:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a7f6c:	4699      	mov	r9, r3
  // Get parameters.
  const int32 input_offset = params.input_offset;  // r = s(q - Z)
 80a7f6e:	6943      	ldr	r3, [r0, #20]
    const ConvParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
 80a7f70:	b09d      	sub	sp, #116	; 0x74
  // Get parameters.
  const int32 input_offset = params.input_offset;  // r = s(q - Z)
 80a7f72:	9309      	str	r3, [sp, #36]	; 0x24
  const int stride_width = params.stride_width;
 80a7f74:	f9b0 300a 	ldrsh.w	r3, [r0, #10]
    const ConvParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
 80a7f78:	911a      	str	r1, [sp, #104]	; 0x68
  // Get parameters.
  const int32 input_offset = params.input_offset;  // r = s(q - Z)
  const int stride_width = params.stride_width;
 80a7f7a:	930a      	str	r3, [sp, #40]	; 0x28
  const int stride_height = params.stride_height;
 80a7f7c:	f9b0 300c 	ldrsh.w	r3, [r0, #12]
    const ConvParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
 80a7f80:	921b      	str	r2, [sp, #108]	; 0x6c
  // Get parameters.
  const int32 input_offset = params.input_offset;  // r = s(q - Z)
  const int stride_width = params.stride_width;
  const int stride_height = params.stride_height;
 80a7f82:	930b      	str	r3, [sp, #44]	; 0x2c
  const int dilation_width_factor = params.dilation_width_factor;
 80a7f84:	f9b0 300e 	ldrsh.w	r3, [r0, #14]
    const ConvParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
 80a7f88:	f8dd b09c 	ldr.w	fp, [sp, #156]	; 0x9c
  // Get parameters.
  const int32 input_offset = params.input_offset;  // r = s(q - Z)
  const int stride_width = params.stride_width;
  const int stride_height = params.stride_height;
  const int dilation_width_factor = params.dilation_width_factor;
 80a7f8c:	930c      	str	r3, [sp, #48]	; 0x30
  const int dilation_height_factor = params.dilation_height_factor;
 80a7f8e:	f9b0 3010 	ldrsh.w	r3, [r0, #16]
 80a7f92:	930d      	str	r3, [sp, #52]	; 0x34
  const int pad_width = params.padding_values.width;
 80a7f94:	f9b0 3002 	ldrsh.w	r3, [r0, #2]
 80a7f98:	930e      	str	r3, [sp, #56]	; 0x38
  const int pad_height = params.padding_values.height;
 80a7f9a:	f9b0 3004 	ldrsh.w	r3, [r0, #4]
 80a7f9e:	930f      	str	r3, [sp, #60]	; 0x3c
  const int32 output_offset = params.output_offset;
 80a7fa0:	69c3      	ldr	r3, [r0, #28]
 80a7fa2:	9310      	str	r3, [sp, #64]	; 0x40
  const int32 output_activation_min = std::numeric_limits<int8_t>::min();
  const int32 output_activation_max = std::numeric_limits<int8_t>::max();

  // Sanity check.
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
 80a7fa4:	f8d9 3000 	ldr.w	r3, [r9]
 80a7fa8:	2b04      	cmp	r3, #4
 80a7faa:	d001      	beq.n	80a7fb0 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x48>
 80a7fac:	f008 f840 	bl	80b0030 <abort>
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
 80a7fb0:	f8db 3000 	ldr.w	r3, [fp]
 80a7fb4:	2b04      	cmp	r3, #4
 80a7fb6:	d1f9      	bne.n	80a7fac <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x44>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
 80a7fb8:	9b2b      	ldr	r3, [sp, #172]	; 0xac
 80a7fba:	681b      	ldr	r3, [r3, #0]
 80a7fbc:	2b04      	cmp	r3, #4
 80a7fbe:	d1f5      	bne.n	80a7fac <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x44>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80a7fc0:	2300      	movs	r3, #0
 80a7fc2:	4619      	mov	r1, r3
 80a7fc4:	9a2b      	ldr	r2, [sp, #172]	; 0xac
 80a7fc6:	4648      	mov	r0, r9
 80a7fc8:	f7ff fd59 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
 80a7fcc:	2303      	movs	r3, #3
 80a7fce:	4619      	mov	r1, r3
  // Sanity check.
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80a7fd0:	9011      	str	r0, [sp, #68]	; 0x44
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
 80a7fd2:	465a      	mov	r2, fp
 80a7fd4:	4648      	mov	r0, r9
 80a7fd6:	f7ff fd52 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
 80a7fda:	2303      	movs	r3, #3
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
 80a7fdc:	9012      	str	r0, [sp, #72]	; 0x48
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
 80a7fde:	9a2b      	ldr	r2, [sp, #172]	; 0xac
 80a7fe0:	2100      	movs	r1, #0
 80a7fe2:	4658      	mov	r0, fp
 80a7fe4:	f7ff fd4b 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  if (bias_data) {
 80a7fe8:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
 80a7fea:	9008      	str	r0, [sp, #32]
  if (bias_data) {
 80a7fec:	b12b      	cbz	r3, 80a7ffa <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x92>
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
 80a7fee:	9829      	ldr	r0, [sp, #164]	; 0xa4
 80a7ff0:	f7ff fd35 	bl	80a7a5e <_ZNK6tflite12RuntimeShape8FlatSizeEv>
 80a7ff4:	9b08      	ldr	r3, [sp, #32]
 80a7ff6:	4283      	cmp	r3, r0
 80a7ff8:	d1d8      	bne.n	80a7fac <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x44>
  }

  // Check dimensions of the tensors.
  const int input_height = input_shape.Dims(1);
 80a7ffa:	2101      	movs	r1, #1
 80a7ffc:	4648      	mov	r0, r9
 80a7ffe:	f7fa f9e3 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
 80a8002:	2102      	movs	r1, #2
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }

  // Check dimensions of the tensors.
  const int input_height = input_shape.Dims(1);
 80a8004:	9013      	str	r0, [sp, #76]	; 0x4c
  const int input_width = input_shape.Dims(2);
 80a8006:	4648      	mov	r0, r9
 80a8008:	f7fa f9de 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_height = filter_shape.Dims(1);
 80a800c:	2101      	movs	r1, #1
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }

  // Check dimensions of the tensors.
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
 80a800e:	9014      	str	r0, [sp, #80]	; 0x50
  const int filter_height = filter_shape.Dims(1);
 80a8010:	4658      	mov	r0, fp
 80a8012:	f7fa f9d9 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_width = filter_shape.Dims(2);
 80a8016:	2102      	movs	r1, #2
  }

  // Check dimensions of the tensors.
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
 80a8018:	9015      	str	r0, [sp, #84]	; 0x54
  const int filter_width = filter_shape.Dims(2);
 80a801a:	4658      	mov	r0, fp
 80a801c:	f7fa f9d4 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
 80a8020:	2101      	movs	r1, #1

  // Check dimensions of the tensors.
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
 80a8022:	9016      	str	r0, [sp, #88]	; 0x58
  const int output_height = output_shape.Dims(1);
 80a8024:	982b      	ldr	r0, [sp, #172]	; 0xac
 80a8026:	f7fa f9cf 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
 80a802a:	2102      	movs	r1, #2
  // Check dimensions of the tensors.
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
 80a802c:	9017      	str	r0, [sp, #92]	; 0x5c
  const int output_width = output_shape.Dims(2);
 80a802e:	982b      	ldr	r0, [sp, #172]	; 0xac
 80a8030:	f7fa f9ca 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  for (int batch = 0; batch < batches; ++batch) {
 80a8034:	f04f 0a00 	mov.w	sl, #0
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
 80a8038:	9018      	str	r0, [sp, #96]	; 0x60
  for (int batch = 0; batch < batches; ++batch) {
 80a803a:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80a803c:	459a      	cmp	sl, r3
 80a803e:	f280 8091 	bge.w	80a8164 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1fc>
 80a8042:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80a8044:	425b      	negs	r3, r3
 80a8046:	9307      	str	r3, [sp, #28]
 80a8048:	2300      	movs	r3, #0
 80a804a:	9302      	str	r3, [sp, #8]
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80a804c:	9b02      	ldr	r3, [sp, #8]
 80a804e:	9a17      	ldr	r2, [sp, #92]	; 0x5c
 80a8050:	4293      	cmp	r3, r2
 80a8052:	da6b      	bge.n	80a812c <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1c4>
 80a8054:	9b0e      	ldr	r3, [sp, #56]	; 0x38
 80a8056:	425b      	negs	r3, r3
 80a8058:	9306      	str	r3, [sp, #24]
 80a805a:	2300      	movs	r3, #0
 80a805c:	9303      	str	r3, [sp, #12]
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80a805e:	9b03      	ldr	r3, [sp, #12]
 80a8060:	9a18      	ldr	r2, [sp, #96]	; 0x60
 80a8062:	4293      	cmp	r3, r2
 80a8064:	da5a      	bge.n	80a811c <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1b4>
 80a8066:	2400      	movs	r4, #0
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
 80a8068:	9b08      	ldr	r3, [sp, #32]
 80a806a:	429c      	cmp	r4, r3
 80a806c:	da4e      	bge.n	80a810c <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1a4>
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
 80a806e:	2500      	movs	r5, #0
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
 80a8070:	9f07      	ldr	r7, [sp, #28]
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
 80a8072:	9504      	str	r5, [sp, #16]
 80a8074:	9b04      	ldr	r3, [sp, #16]
 80a8076:	9a15      	ldr	r2, [sp, #84]	; 0x54
 80a8078:	4293      	cmp	r3, r2
 80a807a:	da24      	bge.n	80a80c6 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x15e>
 80a807c:	2300      	movs	r3, #0
 80a807e:	f8dd 8018 	ldr.w	r8, [sp, #24]
 80a8082:	9305      	str	r3, [sp, #20]
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
 80a8084:	9b05      	ldr	r3, [sp, #20]
 80a8086:	9a16      	ldr	r2, [sp, #88]	; 0x58
 80a8088:	4293      	cmp	r3, r2
 80a808a:	da16      	bge.n	80a80ba <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x152>
 80a808c:	2600      	movs	r6, #0
              for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
 80a808e:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80a8090:	429e      	cmp	r6, r3
 80a8092:	da0c      	bge.n	80a80ae <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x146>
                const int in_x = in_x_origin + dilation_width_factor * filter_x;
                const int in_y =
                    in_y_origin + dilation_height_factor * filter_y;
                // Zero padding by omitting the areas outside the image.
                const bool is_point_inside_image =
                    (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
 80a8094:	f1b8 0f00 	cmp.w	r8, #0
 80a8098:	db07      	blt.n	80a80aa <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x142>
 80a809a:	9b14      	ldr	r3, [sp, #80]	; 0x50
 80a809c:	4543      	cmp	r3, r8
 80a809e:	dd04      	ble.n	80a80aa <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x142>
 80a80a0:	2f00      	cmp	r7, #0
 80a80a2:	db02      	blt.n	80a80aa <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x142>
 80a80a4:	9b13      	ldr	r3, [sp, #76]	; 0x4c
 80a80a6:	42bb      	cmp	r3, r7
 80a80a8:	dc43      	bgt.n	80a8132 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1ca>
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
              for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
 80a80aa:	3601      	adds	r6, #1
 80a80ac:	e7ef      	b.n	80a808e <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x126>
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
 80a80ae:	9b05      	ldr	r3, [sp, #20]
 80a80b0:	3301      	adds	r3, #1
 80a80b2:	9305      	str	r3, [sp, #20]
 80a80b4:	9b0c      	ldr	r3, [sp, #48]	; 0x30
 80a80b6:	4498      	add	r8, r3
 80a80b8:	e7e4      	b.n	80a8084 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x11c>
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
 80a80ba:	9b04      	ldr	r3, [sp, #16]
 80a80bc:	3301      	adds	r3, #1
 80a80be:	9304      	str	r3, [sp, #16]
 80a80c0:	9b0d      	ldr	r3, [sp, #52]	; 0x34
 80a80c2:	441f      	add	r7, r3
 80a80c4:	e7d6      	b.n	80a8074 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x10c>
                }
              }
            }
          }

          if (bias_data) {
 80a80c6:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
 80a80c8:	b113      	cbz	r3, 80a80d0 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x168>
            acc += bias_data[out_channel];
 80a80ca:	f853 3024 	ldr.w	r3, [r3, r4, lsl #2]
 80a80ce:	441d      	add	r5, r3
          }
          acc = MultiplyByQuantizedMultiplier(
 80a80d0:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
 80a80d2:	4628      	mov	r0, r5
 80a80d4:	f853 2024 	ldr.w	r2, [r3, r4, lsl #2]
 80a80d8:	9b1a      	ldr	r3, [sp, #104]	; 0x68
 80a80da:	f06f 057f 	mvn.w	r5, #127	; 0x7f
 80a80de:	f853 1024 	ldr.w	r1, [r3, r4, lsl #2]
 80a80e2:	f7ff fcdb 	bl	80a7a9c <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
              acc, output_multiplier[out_channel], output_shift[out_channel]);
          acc += output_offset;
 80a80e6:	9b10      	ldr	r3, [sp, #64]	; 0x40
          acc = std::max(acc, output_activation_min);
          acc = std::min(acc, output_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
 80a80e8:	9400      	str	r4, [sp, #0]
          if (bias_data) {
            acc += bias_data[out_channel];
          }
          acc = MultiplyByQuantizedMultiplier(
              acc, output_multiplier[out_channel], output_shift[out_channel]);
          acc += output_offset;
 80a80ea:	4418      	add	r0, r3
 80a80ec:	4285      	cmp	r5, r0
 80a80ee:	bfb8      	it	lt
 80a80f0:	4605      	movlt	r5, r0
          acc = std::max(acc, output_activation_min);
          acc = std::min(acc, output_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
 80a80f2:	9b03      	ldr	r3, [sp, #12]
 80a80f4:	9a02      	ldr	r2, [sp, #8]
 80a80f6:	4651      	mov	r1, sl
 80a80f8:	982b      	ldr	r0, [sp, #172]	; 0xac
 80a80fa:	f7fa f9ca 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<int8_t>(acc);
 80a80fe:	2d7f      	cmp	r5, #127	; 0x7f
 80a8100:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
 80a8102:	bfa8      	it	ge
 80a8104:	257f      	movge	r5, #127	; 0x7f
 80a8106:	541d      	strb	r5, [r3, r0]
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
 80a8108:	3401      	adds	r4, #1
 80a810a:	e7ad      	b.n	80a8068 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x100>
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80a810c:	9b03      	ldr	r3, [sp, #12]
 80a810e:	9a0a      	ldr	r2, [sp, #40]	; 0x28
 80a8110:	3301      	adds	r3, #1
 80a8112:	9303      	str	r3, [sp, #12]
 80a8114:	9b06      	ldr	r3, [sp, #24]
 80a8116:	4413      	add	r3, r2
 80a8118:	9306      	str	r3, [sp, #24]
 80a811a:	e7a0      	b.n	80a805e <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0xf6>
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80a811c:	9b02      	ldr	r3, [sp, #8]
 80a811e:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
 80a8120:	3301      	adds	r3, #1
 80a8122:	9302      	str	r3, [sp, #8]
 80a8124:	9b07      	ldr	r3, [sp, #28]
 80a8126:	4413      	add	r3, r2
 80a8128:	9307      	str	r3, [sp, #28]
 80a812a:	e78f      	b.n	80a804c <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0xe4>
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
 80a812c:	f10a 0a01 	add.w	sl, sl, #1
 80a8130:	e783      	b.n	80a803a <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0xd2>
                // Zero padding by omitting the areas outside the image.
                const bool is_point_inside_image =
                    (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height);
                if (is_point_inside_image) {
                  int32 input_val = input_data[Offset(input_shape, batch, in_y,
 80a8132:	4643      	mov	r3, r8
 80a8134:	463a      	mov	r2, r7
 80a8136:	4651      	mov	r1, sl
 80a8138:	9600      	str	r6, [sp, #0]
 80a813a:	4648      	mov	r0, r9
 80a813c:	f7fa f9a9 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                                                      in_x, in_channel)];
                  int32 filter_val =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
 80a8140:	9b05      	ldr	r3, [sp, #20]
                // Zero padding by omitting the areas outside the image.
                const bool is_point_inside_image =
                    (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height);
                if (is_point_inside_image) {
                  int32 input_val = input_data[Offset(input_shape, batch, in_y,
 80a8142:	9019      	str	r0, [sp, #100]	; 0x64
                                                      in_x, in_channel)];
                  int32 filter_val =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
 80a8144:	9a04      	ldr	r2, [sp, #16]
 80a8146:	9600      	str	r6, [sp, #0]
 80a8148:	4621      	mov	r1, r4
 80a814a:	4658      	mov	r0, fp
 80a814c:	f7fa f9a1 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                  // long as the filter size (filter_y * filter_x * in_channel)
                  // does not exceed 2^16, which is the case in all the models
                  // we have seen so far.
                  // TODO(jianlijianli): Add a check to make sure the
                  // accumulator depth is smaller than 2^16.
                  acc += filter_val * (input_val + input_offset);
 80a8150:	9a19      	ldr	r2, [sp, #100]	; 0x64
 80a8152:	9b26      	ldr	r3, [sp, #152]	; 0x98
 80a8154:	569b      	ldrsb	r3, [r3, r2]
 80a8156:	9a09      	ldr	r2, [sp, #36]	; 0x24
 80a8158:	4413      	add	r3, r2
 80a815a:	9a28      	ldr	r2, [sp, #160]	; 0xa0
 80a815c:	5612      	ldrsb	r2, [r2, r0]
 80a815e:	fb02 5503 	mla	r5, r2, r3, r5
 80a8162:	e7a2      	b.n	80a80aa <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x142>
              static_cast<int8_t>(acc);
        }
      }
    }
  }
}
 80a8164:	b01d      	add	sp, #116	; 0x74
 80a8166:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a816a <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>:

// Matching GetWindowedOutputSize in TensorFlow.
inline int ComputeOutSize(TfLitePadding padding, int image_size,
                          int filter_size, int stride, int dilation_rate = 1) {
  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;
  switch (padding) {
 80a816a:	2801      	cmp	r0, #1
 80a816c:	d008      	beq.n	80a8180 <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii+0x16>
 80a816e:	2802      	cmp	r0, #2
 80a8170:	d10b      	bne.n	80a818a <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii+0x20>
    case kTfLitePaddingSame:
      return (image_size + stride - 1) / stride;
    case kTfLitePaddingValid:
      return (image_size + stride - effective_filter_size) / stride;
 80a8172:	9800      	ldr	r0, [sp, #0]
 80a8174:	3a01      	subs	r2, #1
 80a8176:	4350      	muls	r0, r2
 80a8178:	4419      	add	r1, r3
 80a817a:	3001      	adds	r0, #1
 80a817c:	1a09      	subs	r1, r1, r0
 80a817e:	e001      	b.n	80a8184 <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii+0x1a>
inline int ComputeOutSize(TfLitePadding padding, int image_size,
                          int filter_size, int stride, int dilation_rate = 1) {
  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;
  switch (padding) {
    case kTfLitePaddingSame:
      return (image_size + stride - 1) / stride;
 80a8180:	4419      	add	r1, r3
 80a8182:	3901      	subs	r1, #1
    case kTfLitePaddingValid:
      return (image_size + stride - effective_filter_size) / stride;
 80a8184:	fb91 f0f3 	sdiv	r0, r1, r3
 80a8188:	4770      	bx	lr
    default:
      return 0;
 80a818a:	2000      	movs	r0, #0
  }
}
 80a818c:	4770      	bx	lr
	...

080a8190 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE>:

TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, int width, int height,
                             int filter_width, int filter_height, int out_width,
                             int out_height, const TfLiteType data_type,
                             OpData* data) {
 80a8190:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a8194:	469a      	mov	sl, r3
  bool has_bias = node->inputs->size == 3;
 80a8196:	680b      	ldr	r3, [r1, #0]

TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, int width, int height,
                             int filter_width, int filter_height, int out_width,
                             int out_height, const TfLiteType data_type,
                             OpData* data) {
 80a8198:	b08d      	sub	sp, #52	; 0x34
  bool has_bias = node->inputs->size == 3;
  // Check number of inputs/outputs
  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
 80a819a:	681b      	ldr	r3, [r3, #0]

TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, int width, int height,
                             int filter_width, int filter_height, int out_width,
                             int out_height, const TfLiteType data_type,
                             OpData* data) {
 80a819c:	4680      	mov	r8, r0
  bool has_bias = node->inputs->size == 3;
  // Check number of inputs/outputs
  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
 80a819e:	3b02      	subs	r3, #2
 80a81a0:	2b01      	cmp	r3, #1

TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, int width, int height,
                             int filter_width, int filter_height, int out_width,
                             int out_height, const TfLiteType data_type,
                             OpData* data) {
 80a81a2:	4689      	mov	r9, r1
 80a81a4:	4616      	mov	r6, r2
 80a81a6:	9c1c      	ldr	r4, [sp, #112]	; 0x70
  bool has_bias = node->inputs->size == 3;
  // Check number of inputs/outputs
  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
 80a81a8:	d908      	bls.n	80a81bc <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x2c>
 80a81aa:	4b49      	ldr	r3, [pc, #292]	; (80a82d0 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x140>)
 80a81ac:	4a49      	ldr	r2, [pc, #292]	; (80a82d4 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x144>)
 80a81ae:	9300      	str	r3, [sp, #0]
 80a81b0:	6944      	ldr	r4, [r0, #20]
 80a81b2:	234f      	movs	r3, #79	; 0x4f
 80a81b4:	4948      	ldr	r1, [pc, #288]	; (80a82d8 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x148>)
 80a81b6:	47a0      	blx	r4
 80a81b8:	2001      	movs	r0, #1
 80a81ba:	e085      	b.n	80a82c8 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x138>
  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);
 80a81bc:	684b      	ldr	r3, [r1, #4]
 80a81be:	681b      	ldr	r3, [r3, #0]
 80a81c0:	2b01      	cmp	r3, #1
 80a81c2:	d00c      	beq.n	80a81de <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x4e>
 80a81c4:	9302      	str	r3, [sp, #8]
 80a81c6:	4b45      	ldr	r3, [pc, #276]	; (80a82dc <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x14c>)
 80a81c8:	2401      	movs	r4, #1
 80a81ca:	9301      	str	r3, [sp, #4]
 80a81cc:	4b44      	ldr	r3, [pc, #272]	; (80a82e0 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x150>)
 80a81ce:	9403      	str	r4, [sp, #12]
 80a81d0:	9300      	str	r3, [sp, #0]
 80a81d2:	6945      	ldr	r5, [r0, #20]
 80a81d4:	2350      	movs	r3, #80	; 0x50
 80a81d6:	4a3f      	ldr	r2, [pc, #252]	; (80a82d4 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x144>)
 80a81d8:	4942      	ldr	r1, [pc, #264]	; (80a82e4 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x154>)
 80a81da:	47a8      	blx	r5
 80a81dc:	e7ec      	b.n	80a81b8 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x28>
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
 80a81de:	6893      	ldr	r3, [r2, #8]
  // Check number of inputs/outputs
  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);

  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
 80a81e0:	f892 b000 	ldrb.w	fp, [r2]
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
 80a81e4:	68d5      	ldr	r5, [r2, #12]
 80a81e6:	9309      	str	r3, [sp, #36]	; 0x24
 80a81e8:	6853      	ldr	r3, [r2, #4]
 80a81ea:	6917      	ldr	r7, [r2, #16]

inline TfLitePaddingValues ComputePaddingHeightWidth(
    int stride_height, int stride_width, int dilation_rate_height,
    int dilation_rate_width, int in_height, int in_width, int filter_height,
    int filter_width, TfLitePadding padding, int* out_height, int* out_width) {
  *out_width = ComputeOutSize(padding, in_width, filter_width, stride_width,
 80a81ec:	4651      	mov	r1, sl
 80a81ee:	9500      	str	r5, [sp, #0]
 80a81f0:	9a17      	ldr	r2, [sp, #92]	; 0x5c
 80a81f2:	4658      	mov	r0, fp
 80a81f4:	930a      	str	r3, [sp, #40]	; 0x28
 80a81f6:	f7ff ffb8 	bl	80a816a <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>
                              dilation_rate_width);
  *out_height = ComputeOutSize(padding, in_height, filter_height, stride_height,
 80a81fa:	9700      	str	r7, [sp, #0]

inline TfLitePaddingValues ComputePaddingHeightWidth(
    int stride_height, int stride_width, int dilation_rate_height,
    int dilation_rate_width, int in_height, int in_width, int filter_height,
    int filter_width, TfLitePadding padding, int* out_height, int* out_width) {
  *out_width = ComputeOutSize(padding, in_width, filter_width, stride_width,
 80a81fc:	900b      	str	r0, [sp, #44]	; 0x2c
                              dilation_rate_width);
  *out_height = ComputeOutSize(padding, in_height, filter_height, stride_height,
 80a81fe:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80a8200:	9a18      	ldr	r2, [sp, #96]	; 0x60
 80a8202:	9916      	ldr	r1, [sp, #88]	; 0x58
 80a8204:	4658      	mov	r0, fp
 80a8206:	f7ff ffb0 	bl	80a816a <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>
inline int ComputePaddingWithOffset(int stride, int dilation_rate, int in_size,
                                    int filter_size, int out_size,
                                    int* offset) {
  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;
  int total_padding =
      ((out_size - 1) * stride + effective_filter_size - in_size);
 80a820a:	9b18      	ldr	r3, [sp, #96]	; 0x60
 80a820c:	3801      	subs	r0, #1
 80a820e:	3b01      	subs	r3, #1
 80a8210:	435f      	muls	r7, r3
 80a8212:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80a8214:	3701      	adds	r7, #1
 80a8216:	fb03 7000 	mla	r0, r3, r0, r7
 80a821a:	9b16      	ldr	r3, [sp, #88]	; 0x58
 80a821c:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
 80a821e:	1ac0      	subs	r0, r0, r3
 80a8220:	9b17      	ldr	r3, [sp, #92]	; 0x5c
  total_padding = total_padding > 0 ? total_padding : 0;
 80a8222:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
inline int ComputePaddingWithOffset(int stride, int dilation_rate, int in_size,
                                    int filter_size, int out_size,
                                    int* offset) {
  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;
  int total_padding =
      ((out_size - 1) * stride + effective_filter_size - in_size);
 80a8226:	3b01      	subs	r3, #1
 80a8228:	436b      	muls	r3, r5
 80a822a:	1e55      	subs	r5, r2, #1
 80a822c:	9a0a      	ldr	r2, [sp, #40]	; 0x28
 80a822e:	3301      	adds	r3, #1
 80a8230:	fb02 3505 	mla	r5, r2, r5, r3
 80a8234:	ebca 0a05 	rsb	sl, sl, r5
  total_padding = total_padding > 0 ? total_padding : 0;
 80a8238:	ea2a 7aea 	bic.w	sl, sl, sl, asr #31
 80a823c:	ea4f 036a 	mov.w	r3, sl, asr #1
 80a8240:	6023      	str	r3, [r4, #0]
 80a8242:	1043      	asrs	r3, r0, #1
 80a8244:	6063      	str	r3, [r4, #4]

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training.
  if (data_type != kTfLiteFloat32) {
 80a8246:	f89d 306c 	ldrb.w	r3, [sp, #108]	; 0x6c
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
 80a824a:	f00a 0501 	and.w	r5, sl, #1
 80a824e:	f000 0001 	and.w	r0, r0, #1

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training.
  if (data_type != kTfLiteFloat32) {
 80a8252:	2b01      	cmp	r3, #1
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
 80a8254:	60a5      	str	r5, [r4, #8]
 80a8256:	60e0      	str	r0, [r4, #12]

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training.
  if (data_type != kTfLiteFloat32) {
 80a8258:	d035      	beq.n	80a82c6 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x136>
 80a825a:	f8d9 5000 	ldr.w	r5, [r9]
 80a825e:	f8d8 0008 	ldr.w	r0, [r8, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a8262:	6869      	ldr	r1, [r5, #4]
 80a8264:	68aa      	ldr	r2, [r5, #8]
}

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
 80a8266:	68ed      	ldr	r5, [r5, #12]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a8268:	2338      	movs	r3, #56	; 0x38

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
 80a826a:	1c6f      	adds	r7, r5, #1
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a826c:	fb03 0101 	mla	r1, r3, r1, r0
 80a8270:	fb03 0202 	mla	r2, r3, r2, r0
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a8274:	bf18      	it	ne
 80a8276:	fb03 0305 	mlane	r3, r3, r5, r0
    const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
    const TfLiteTensor* bias =
        GetOptionalInputTensor(context, node, kBiasTensor);
    TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
 80a827a:	f504 758c 	add.w	r5, r4, #280	; 0x118
 80a827e:	9507      	str	r5, [sp, #28]
 80a8280:	f104 0518 	add.w	r5, r4, #24
 80a8284:	9506      	str	r5, [sp, #24]
 80a8286:	f504 7507 	add.w	r5, r4, #540	; 0x21c
 80a828a:	9505      	str	r5, [sp, #20]
 80a828c:	f504 7506 	add.w	r5, r4, #536	; 0x218
 80a8290:	9504      	str	r5, [sp, #16]
 80a8292:	f104 0514 	add.w	r5, r4, #20
 80a8296:	f104 0410 	add.w	r4, r4, #16
 80a829a:	9402      	str	r4, [sp, #8]
 80a829c:	f106 0614 	add.w	r6, r6, #20
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a82a0:	f8d9 4004 	ldr.w	r4, [r9, #4]
 80a82a4:	9503      	str	r5, [sp, #12]
 80a82a6:	9601      	str	r6, [sp, #4]
 80a82a8:	6864      	ldr	r4, [r4, #4]
 80a82aa:	f04f 0538 	mov.w	r5, #56	; 0x38
 80a82ae:	fb05 0004 	mla	r0, r5, r4, r0
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
  }
  return nullptr;
 80a82b2:	bf08      	it	eq
 80a82b4:	2300      	moveq	r3, #0
 80a82b6:	9000      	str	r0, [sp, #0]
 80a82b8:	4640      	mov	r0, r8
 80a82ba:	f007 fb33 	bl	80af924 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_>
                             int filter_width, int filter_height, int out_width,
                             int out_height, const TfLiteType data_type,
                             OpData* data) {
  bool has_bias = node->inputs->size == 3;
  // Check number of inputs/outputs
  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
 80a82be:	3000      	adds	r0, #0
 80a82c0:	bf18      	it	ne
 80a82c2:	2001      	movne	r0, #1
 80a82c4:	e000      	b.n	80a82c8 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x138>
        &data->output_multiplier, &data->output_shift,
        &data->output_activation_min, &data->output_activation_max,
        data->per_channel_output_multiplier,
        reinterpret_cast<int*>(data->per_channel_output_shift)));
  }
  return kTfLiteOk;
 80a82c6:	2000      	movs	r0, #0
}
 80a82c8:	b00d      	add	sp, #52	; 0x34
 80a82ca:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80a82ce:	bf00      	nop
 80a82d0:	080b5dc7 	.word	0x080b5dc7
 80a82d4:	080b5d0a 	.word	0x080b5d0a
 80a82d8:	080b5db0 	.word	0x080b5db0
 80a82dc:	080b75ad 	.word	0x080b75ad
 80a82e0:	080b5deb 	.word	0x080b5deb
 80a82e4:	080b5be0 	.word	0x080b5be0

080a82e8 <_ZN6tflite3ops5micro4conv13EvalQuantizedEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_>:

void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
 80a82e8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a82ec:	b0b1      	sub	sp, #196	; 0xc4
 80a82ee:	f8dd 80e8 	ldr.w	r8, [sp, #232]	; 0xe8
 80a82f2:	9f3b      	ldr	r7, [sp, #236]	; 0xec
  const int32_t input_offset = -input->params.zero_point;
 80a82f4:	f8d8 1010 	ldr.w	r1, [r8, #16]
  int32_t output_activation_min;
  int32_t output_activation_max;
};

inline PaddingType RuntimePaddingType(TfLitePadding padding) {
  switch (padding) {
 80a82f8:	7810      	ldrb	r0, [r2, #0]

void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
 80a82fa:	9e3f      	ldr	r6, [sp, #252]	; 0xfc
  const int32_t input_offset = -input->params.zero_point;
 80a82fc:	f1c1 0e00 	rsb	lr, r1, #0
  const int32_t filter_offset = -filter->params.zero_point;
 80a8300:	6939      	ldr	r1, [r7, #16]
  int32_t output_activation_min;
  int32_t output_activation_max;
};

inline PaddingType RuntimePaddingType(TfLitePadding padding) {
  switch (padding) {
 80a8302:	2801      	cmp	r0, #1

void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
 80a8304:	9c3c      	ldr	r4, [sp, #240]	; 0xf0
 80a8306:	9d3d      	ldr	r5, [sp, #244]	; 0xf4
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
 80a8308:	f1c1 0100 	rsb	r1, r1, #0
  const int32_t output_offset = output->params.zero_point;
 80a830c:	f8d6 c010 	ldr.w	ip, [r6, #16]
  int32_t output_activation_min;
  int32_t output_activation_max;
};

inline PaddingType RuntimePaddingType(TfLitePadding padding) {
  switch (padding) {
 80a8310:	d003      	beq.n	80a831a <_ZN6tflite3ops5micro4conv13EvalQuantizedEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x32>
      return PaddingType::kSame;
    case TfLitePadding::kTfLitePaddingValid:
      return PaddingType::kValid;
    case TfLitePadding::kTfLitePaddingUnknown:
    default:
      return PaddingType::kNone;
 80a8312:	2802      	cmp	r0, #2
 80a8314:	bf0c      	ite	eq
 80a8316:	2002      	moveq	r0, #2
 80a8318:	2000      	movne	r0, #0
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(params->padding);
 80a831a:	f88d 0088 	strb.w	r0, [sp, #136]	; 0x88
  op_params.padding_values.width = data->padding.width;
 80a831e:	6818      	ldr	r0, [r3, #0]
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
 80a8320:	9128      	str	r1, [sp, #160]	; 0xa0
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(params->padding);
  op_params.padding_values.width = data->padding.width;
 80a8322:	f8ad 008a 	strh.w	r0, [sp, #138]	; 0x8a
  op_params.padding_values.height = data->padding.height;
 80a8326:	6858      	ldr	r0, [r3, #4]
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
 80a8328:	4641      	mov	r1, r8
  const int32_t output_offset = output->params.zero_point;

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(params->padding);
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
 80a832a:	f8ad 008c 	strh.w	r0, [sp, #140]	; 0x8c
  op_params.stride_width = params->stride_width;
 80a832e:	6850      	ldr	r0, [r2, #4]
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = input_offset;
 80a8330:	f8cd e09c 	str.w	lr, [sp, #156]	; 0x9c

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(params->padding);
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
 80a8334:	f8ad 0092 	strh.w	r0, [sp, #146]	; 0x92
  op_params.stride_height = params->stride_height;
 80a8338:	6890      	ldr	r0, [r2, #8]
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
 80a833a:	f8cd c0a4 	str.w	ip, [sp, #164]	; 0xa4
  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(params->padding);
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
 80a833e:	f8ad 0094 	strh.w	r0, [sp, #148]	; 0x94
  op_params.dilation_width_factor = params->dilation_width_factor;
 80a8342:	68d0      	ldr	r0, [r2, #12]
  op_params.dilation_height_factor = params->dilation_height_factor;
 80a8344:	6912      	ldr	r2, [r2, #16]
  op_params.padding_type = RuntimePaddingType(params->padding);
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
 80a8346:	f8ad 0096 	strh.w	r0, [sp, #150]	; 0x96
  op_params.dilation_height_factor = params->dilation_height_factor;
 80a834a:	f8ad 2098 	strh.w	r2, [sp, #152]	; 0x98
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
 80a834e:	691a      	ldr	r2, [r3, #16]
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
 80a8350:	a809      	add	r0, sp, #36	; 0x24
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
 80a8352:	922a      	str	r2, [sp, #168]	; 0xa8
  op_params.output_shift = -data->output_shift;
 80a8354:	695a      	ldr	r2, [r3, #20]
 80a8356:	4252      	negs	r2, r2
 80a8358:	922b      	str	r2, [sp, #172]	; 0xac
  op_params.quantized_activation_min = data->output_activation_min;
 80a835a:	f8d3 2218 	ldr.w	r2, [r3, #536]	; 0x218
  op_params.quantized_activation_max = data->output_activation_max;
 80a835e:	f8d3 321c 	ldr.w	r3, [r3, #540]	; 0x21c
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
 80a8362:	922c      	str	r2, [sp, #176]	; 0xb0
  op_params.quantized_activation_max = data->output_activation_max;
 80a8364:	932d      	str	r3, [sp, #180]	; 0xb4
  reference_ops::Conv(op_params, GetTensorShape(input),
 80a8366:	f7fa fad4 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                      GetTensorData<uint8_t>(input), GetTensorShape(filter),
 80a836a:	4639      	mov	r1, r7
 80a836c:	a80e      	add	r0, sp, #56	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a836e:	f8d8 8004 	ldr.w	r8, [r8, #4]
 80a8372:	f7fa face 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a8376:	f8d7 9004 	ldr.w	r9, [r7, #4]
                      GetTensorData<uint8_t>(filter), GetTensorShape(bias),
 80a837a:	af13      	add	r7, sp, #76	; 0x4c
 80a837c:	4621      	mov	r1, r4
 80a837e:	4638      	mov	r0, r7
 80a8380:	f7fa fac7 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a8384:	b114      	cbz	r4, 80a838c <_ZN6tflite3ops5micro4conv13EvalQuantizedEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xa4>
 80a8386:	f8d4 a004 	ldr.w	sl, [r4, #4]
 80a838a:	e000      	b.n	80a838e <_ZN6tflite3ops5micro4conv13EvalQuantizedEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xa6>
 80a838c:	46a2      	mov	sl, r4
                      GetTensorData<int32_t>(bias), GetTensorShape(output),
 80a838e:	ac18      	add	r4, sp, #96	; 0x60
 80a8390:	4631      	mov	r1, r6
 80a8392:	4620      	mov	r0, r4
 80a8394:	f7fa fabd 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a8398:	f8d6 b004 	ldr.w	fp, [r6, #4]
                      GetTensorData<uint8_t>(output), GetTensorShape(im2col),
 80a839c:	ae1d      	add	r6, sp, #116	; 0x74
 80a839e:	4629      	mov	r1, r5
 80a83a0:	4630      	mov	r0, r6
 80a83a2:	f7fa fab6 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a83a6:	b105      	cbz	r5, 80a83aa <_ZN6tflite3ops5micro4conv13EvalQuantizedEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xc2>
 80a83a8:	686d      	ldr	r5, [r5, #4]
                      GetTensorData<uint8_t>(im2col), nullptr);
 80a83aa:	2300      	movs	r3, #0
 80a83ac:	4642      	mov	r2, r8
 80a83ae:	a909      	add	r1, sp, #36	; 0x24
 80a83b0:	9307      	str	r3, [sp, #28]
 80a83b2:	a822      	add	r0, sp, #136	; 0x88
 80a83b4:	ab0e      	add	r3, sp, #56	; 0x38
 80a83b6:	9506      	str	r5, [sp, #24]
 80a83b8:	9605      	str	r6, [sp, #20]
 80a83ba:	f8cd b010 	str.w	fp, [sp, #16]
 80a83be:	9403      	str	r4, [sp, #12]
 80a83c0:	f8cd a008 	str.w	sl, [sp, #8]
 80a83c4:	9701      	str	r7, [sp, #4]
 80a83c6:	f8cd 9000 	str.w	r9, [sp]
 80a83ca:	f7ff fcbd 	bl	80a7d48 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv>
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<uint8_t>(input), GetTensorShape(filter),
                      GetTensorData<uint8_t>(filter), GetTensorShape(bias),
                      GetTensorData<int32_t>(bias), GetTensorShape(output),
                      GetTensorData<uint8_t>(output), GetTensorShape(im2col),
 80a83ce:	4630      	mov	r0, r6
 80a83d0:	f7f9 ffef 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<uint8_t>(input), GetTensorShape(filter),
                      GetTensorData<uint8_t>(filter), GetTensorShape(bias),
                      GetTensorData<int32_t>(bias), GetTensorShape(output),
 80a83d4:	4620      	mov	r0, r4
 80a83d6:	f7f9 ffec 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<uint8_t>(input), GetTensorShape(filter),
                      GetTensorData<uint8_t>(filter), GetTensorShape(bias),
 80a83da:	4638      	mov	r0, r7
 80a83dc:	f7f9 ffe9 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<uint8_t>(input), GetTensorShape(filter),
 80a83e0:	a80e      	add	r0, sp, #56	; 0x38
 80a83e2:	f7f9 ffe6 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
 80a83e6:	a809      	add	r0, sp, #36	; 0x24
 80a83e8:	f7f9 ffe3 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                      GetTensorData<uint8_t>(input), GetTensorShape(filter),
                      GetTensorData<uint8_t>(filter), GetTensorShape(bias),
                      GetTensorData<int32_t>(bias), GetTensorShape(output),
                      GetTensorData<uint8_t>(output), GetTensorShape(im2col),
                      GetTensorData<uint8_t>(im2col), nullptr);
}
 80a83ec:	b031      	add	sp, #196	; 0xc4
 80a83ee:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a83f2 <_ZN6tflite3ops5micro4conv23EvalQuantizedPerChannelEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_>:
void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, OpData* data,
                             const TfLiteTensor* input,
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output,
                             TfLiteTensor* im2col) {
 80a83f2:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
 80a83f6:	4699      	mov	r9, r3
void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, OpData* data,
                             const TfLiteTensor* input,
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output,
                             TfLiteTensor* im2col) {
 80a83f8:	b0ad      	sub	sp, #180	; 0xb4
 80a83fa:	9f36      	ldr	r7, [sp, #216]	; 0xd8
 80a83fc:	ac37      	add	r4, sp, #220	; 0xdc
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
 80a83fe:	6939      	ldr	r1, [r7, #16]
void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, OpData* data,
                             const TfLiteTensor* input,
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output,
                             TfLiteTensor* im2col) {
 80a8400:	e894 0430 	ldmia.w	r4, {r4, r5, sl}
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
 80a8404:	4249      	negs	r1, r1
 80a8406:	9123      	str	r1, [sp, #140]	; 0x8c
  op_params.output_offset = output->params.zero_point;
 80a8408:	f8da 1010 	ldr.w	r1, [sl, #16]
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;

  reference_integer_ops::ConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
 80a840c:	a80a      	add	r0, sp, #40	; 0x28
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output,
                             TfLiteTensor* im2col) {
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
 80a840e:	9125      	str	r1, [sp, #148]	; 0x94
  op_params.stride_height = params->stride_height;
 80a8410:	6891      	ldr	r1, [r2, #8]
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;

  reference_integer_ops::ConvPerChannel(
 80a8412:	f503 768c 	add.w	r6, r3, #280	; 0x118
                             const TfLiteTensor* bias, TfLiteTensor* output,
                             TfLiteTensor* im2col) {
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = params->stride_height;
 80a8416:	f8ad 1084 	strh.w	r1, [sp, #132]	; 0x84
  op_params.stride_width = params->stride_width;
 80a841a:	6851      	ldr	r1, [r2, #4]
 80a841c:	f8ad 1082 	strh.w	r1, [sp, #130]	; 0x82
  op_params.dilation_height_factor = params->dilation_height_factor;
 80a8420:	6911      	ldr	r1, [r2, #16]
  op_params.dilation_width_factor = params->dilation_width_factor;
 80a8422:	68d2      	ldr	r2, [r2, #12]
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.dilation_height_factor = params->dilation_height_factor;
 80a8424:	f8ad 1088 	strh.w	r1, [sp, #136]	; 0x88
  op_params.dilation_width_factor = params->dilation_width_factor;
 80a8428:	f8ad 2086 	strh.w	r2, [sp, #134]	; 0x86
  op_params.padding_values.height = data->padding.height;
 80a842c:	685a      	ldr	r2, [r3, #4]
  op_params.padding_values.width = data->padding.width;

  reference_integer_ops::ConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
 80a842e:	4639      	mov	r1, r7
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
 80a8430:	f8ad 207c 	strh.w	r2, [sp, #124]	; 0x7c
  op_params.padding_values.width = data->padding.width;
 80a8434:	f859 2b18 	ldr.w	r2, [r9], #24
 80a8438:	f8ad 207a 	strh.w	r2, [sp, #122]	; 0x7a

  reference_integer_ops::ConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
 80a843c:	f7fa fa69 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a8440:	f8d7 b004 	ldr.w	fp, [r7, #4]
      GetTensorData<int8>(input), GetTensorShape(filter),
 80a8444:	af0f      	add	r7, sp, #60	; 0x3c
 80a8446:	4621      	mov	r1, r4
 80a8448:	4638      	mov	r0, r7
 80a844a:	f7fa fa62 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a844e:	b104      	cbz	r4, 80a8452 <_ZN6tflite3ops5micro4conv23EvalQuantizedPerChannelEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_+0x60>
 80a8450:	6864      	ldr	r4, [r4, #4]
      GetTensorData<int8>(filter), GetTensorShape(bias),
 80a8452:	f10d 0850 	add.w	r8, sp, #80	; 0x50
 80a8456:	4629      	mov	r1, r5
 80a8458:	4640      	mov	r0, r8
 80a845a:	f7fa fa5a 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a845e:	b10d      	cbz	r5, 80a8464 <_ZN6tflite3ops5micro4conv23EvalQuantizedPerChannelEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_+0x72>
 80a8460:	686b      	ldr	r3, [r5, #4]
 80a8462:	e000      	b.n	80a8466 <_ZN6tflite3ops5micro4conv23EvalQuantizedPerChannelEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_+0x74>
 80a8464:	462b      	mov	r3, r5
      GetTensorData<int32>(bias), GetTensorShape(output),
 80a8466:	ad19      	add	r5, sp, #100	; 0x64
 80a8468:	4651      	mov	r1, sl
 80a846a:	4628      	mov	r0, r5
 80a846c:	9309      	str	r3, [sp, #36]	; 0x24
 80a846e:	f7fa fa50 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorData<int8>(output));
 80a8472:	f8da 2004 	ldr.w	r2, [sl, #4]
 80a8476:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80a8478:	4649      	mov	r1, r9
 80a847a:	9206      	str	r2, [sp, #24]
 80a847c:	9304      	str	r3, [sp, #16]
 80a847e:	4632      	mov	r2, r6
 80a8480:	ab0a      	add	r3, sp, #40	; 0x28
 80a8482:	a81e      	add	r0, sp, #120	; 0x78
 80a8484:	9505      	str	r5, [sp, #20]
 80a8486:	f8cd 800c 	str.w	r8, [sp, #12]
 80a848a:	9402      	str	r4, [sp, #8]
 80a848c:	9701      	str	r7, [sp, #4]
 80a848e:	f8cd b000 	str.w	fp, [sp]
 80a8492:	f7ff fd69 	bl	80a7f68 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa>
  reference_integer_ops::ConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
      GetTensorData<int8>(filter), GetTensorShape(bias),
      GetTensorData<int32>(bias), GetTensorShape(output),
 80a8496:	4628      	mov	r0, r5
 80a8498:	f7f9 ff8b 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>

  reference_integer_ops::ConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
      GetTensorData<int8>(filter), GetTensorShape(bias),
 80a849c:	4640      	mov	r0, r8
 80a849e:	f7f9 ff88 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.padding_values.width = data->padding.width;

  reference_integer_ops::ConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
 80a84a2:	4638      	mov	r0, r7
 80a84a4:	f7f9 ff85 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;

  reference_integer_ops::ConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
 80a84a8:	a80a      	add	r0, sp, #40	; 0x28
 80a84aa:	f7f9 ff82 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
      GetTensorData<int8>(input), GetTensorShape(filter),
      GetTensorData<int8>(filter), GetTensorShape(bias),
      GetTensorData<int32>(bias), GetTensorShape(output),
      GetTensorData<int8>(output));
}
 80a84ae:	b02d      	add	sp, #180	; 0xb4
 80a84b0:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a84b4 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_>:

void EvalFloat(TfLiteContext* context, TfLiteNode* node,
               TfLiteConvParams* params, OpData* data,
               const TfLiteTensor* input, const TfLiteTensor* filter,
               const TfLiteTensor* bias, TfLiteTensor* im2col,
               TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
 80a84b4:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
 80a84b8:	7d11      	ldrb	r1, [r2, #20]

void EvalFloat(TfLiteContext* context, TfLiteNode* node,
               TfLiteConvParams* params, OpData* data,
               const TfLiteTensor* input, const TfLiteTensor* filter,
               const TfLiteTensor* bias, TfLiteTensor* im2col,
               TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
 80a84ba:	b0b1      	sub	sp, #196	; 0xc4
 80a84bc:	ac3a      	add	r4, sp, #232	; 0xe8
// Calculates the useful range of an activation layer given its activation
// tensor.a
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
 80a84be:	2901      	cmp	r1, #1
 80a84c0:	e894 0170 	ldmia.w	r4, {r4, r5, r6, r8}
 80a84c4:	9f3f      	ldr	r7, [sp, #252]	; 0xfc
 80a84c6:	d007      	beq.n	80a84d8 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x24>
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
 80a84c8:	2903      	cmp	r1, #3
 80a84ca:	d007      	beq.n	80a84dc <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x28>
    *activation_min = 0;
    *activation_max = 6;
  } else if (activation == kTfLiteActRelu1) {
 80a84cc:	2902      	cmp	r1, #2
 80a84ce:	d009      	beq.n	80a84e4 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x30>
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
    *activation_max = std::numeric_limits<T>::max();
 80a84d0:	483a      	ldr	r0, [pc, #232]	; (80a85bc <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x108>)
    *activation_max = 6;
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
 80a84d2:	f46f 0e00 	mvn.w	lr, #8388608	; 0x800000
 80a84d6:	e009      	b.n	80a84ec <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x38>
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
 80a84d8:	4838      	ldr	r0, [pc, #224]	; (80a85bc <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x108>)
 80a84da:	e000      	b.n	80a84de <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x2a>
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
    *activation_max = 6;
 80a84dc:	4838      	ldr	r0, [pc, #224]	; (80a85c0 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x10c>)
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
 80a84de:	f04f 0e00 	mov.w	lr, #0
 80a84e2:	e003      	b.n	80a84ec <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x38>
    *activation_max = 6;
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
 80a84e4:	f8df e0dc 	ldr.w	lr, [pc, #220]	; 80a85c4 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x110>
    *activation_max = 1;
 80a84e8:	f04f 507e 	mov.w	r0, #1065353216	; 0x3f800000
  int32_t output_activation_min;
  int32_t output_activation_max;
};

inline PaddingType RuntimePaddingType(TfLitePadding padding) {
  switch (padding) {
 80a84ec:	7811      	ldrb	r1, [r2, #0]
 80a84ee:	2901      	cmp	r1, #1
 80a84f0:	d003      	beq.n	80a84fa <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x46>
      return PaddingType::kSame;
    case TfLitePadding::kTfLitePaddingValid:
      return PaddingType::kValid;
    case TfLitePadding::kTfLitePaddingUnknown:
    default:
      return PaddingType::kNone;
 80a84f2:	2902      	cmp	r1, #2
 80a84f4:	bf0c      	ite	eq
 80a84f6:	2102      	moveq	r1, #2
 80a84f8:	2100      	movne	r1, #0
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(params->padding);
 80a84fa:	f88d 1088 	strb.w	r1, [sp, #136]	; 0x88
  op_params.padding_values.width = data->padding.width;
 80a84fe:	6819      	ldr	r1, [r3, #0]
  op_params.padding_values.height = data->padding.height;
 80a8500:	685b      	ldr	r3, [r3, #4]
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(params->padding);
  op_params.padding_values.width = data->padding.width;
 80a8502:	f8ad 108a 	strh.w	r1, [sp, #138]	; 0x8a
  op_params.padding_values.height = data->padding.height;
 80a8506:	f8ad 308c 	strh.w	r3, [sp, #140]	; 0x8c
  op_params.stride_width = params->stride_width;
 80a850a:	6853      	ldr	r3, [r2, #4]
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
 80a850c:	902f      	str	r0, [sp, #188]	; 0xbc

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(params->padding);
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
 80a850e:	f8ad 3092 	strh.w	r3, [sp, #146]	; 0x92
  op_params.stride_height = params->stride_height;
 80a8512:	6893      	ldr	r3, [r2, #8]
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  reference_ops::Conv(op_params, GetTensorShape(input),
 80a8514:	4621      	mov	r1, r4
  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(params->padding);
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
 80a8516:	f8ad 3094 	strh.w	r3, [sp, #148]	; 0x94
  op_params.dilation_width_factor = params->dilation_width_factor;
 80a851a:	68d3      	ldr	r3, [r2, #12]
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  reference_ops::Conv(op_params, GetTensorShape(input),
 80a851c:	a809      	add	r0, sp, #36	; 0x24
  op_params.padding_type = RuntimePaddingType(params->padding);
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
 80a851e:	f8ad 3096 	strh.w	r3, [sp, #150]	; 0x96
  op_params.dilation_height_factor = params->dilation_height_factor;
 80a8522:	6913      	ldr	r3, [r2, #16]
  op_params.float_activation_min = output_activation_min;
 80a8524:	f8cd e0b8 	str.w	lr, [sp, #184]	; 0xb8
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
 80a8528:	f8ad 3098 	strh.w	r3, [sp, #152]	; 0x98
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  reference_ops::Conv(op_params, GetTensorShape(input),
 80a852c:	f7fa f9f1 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a8530:	b104      	cbz	r4, 80a8534 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x80>
 80a8532:	6864      	ldr	r4, [r4, #4]
                      GetTensorData<float>(input), GetTensorShape(filter),
 80a8534:	4629      	mov	r1, r5
 80a8536:	a80e      	add	r0, sp, #56	; 0x38
 80a8538:	f7fa f9eb 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a853c:	b105      	cbz	r5, 80a8540 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x8c>
 80a853e:	686d      	ldr	r5, [r5, #4]
                      GetTensorData<float>(filter), GetTensorShape(bias),
 80a8540:	f10d 094c 	add.w	r9, sp, #76	; 0x4c
 80a8544:	4631      	mov	r1, r6
 80a8546:	4648      	mov	r0, r9
 80a8548:	f7fa f9e3 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a854c:	b106      	cbz	r6, 80a8550 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x9c>
 80a854e:	6876      	ldr	r6, [r6, #4]
                      GetTensorData<float>(bias), GetTensorShape(output),
 80a8550:	f10d 0a60 	add.w	sl, sp, #96	; 0x60
 80a8554:	4639      	mov	r1, r7
 80a8556:	4650      	mov	r0, sl
 80a8558:	f7fa f9db 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a855c:	b107      	cbz	r7, 80a8560 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xac>
 80a855e:	687f      	ldr	r7, [r7, #4]
                      GetTensorData<float>(output), GetTensorShape(im2col),
 80a8560:	f10d 0b74 	add.w	fp, sp, #116	; 0x74
 80a8564:	4641      	mov	r1, r8
 80a8566:	4658      	mov	r0, fp
 80a8568:	f7fa f9d3 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a856c:	f1b8 0f00 	cmp.w	r8, #0
 80a8570:	d002      	beq.n	80a8578 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xc4>
 80a8572:	f8d8 3004 	ldr.w	r3, [r8, #4]
 80a8576:	e000      	b.n	80a857a <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xc6>
 80a8578:	4643      	mov	r3, r8
                      GetTensorData<float>(im2col));
 80a857a:	4622      	mov	r2, r4
 80a857c:	a909      	add	r1, sp, #36	; 0x24
 80a857e:	9306      	str	r3, [sp, #24]
 80a8580:	a822      	add	r0, sp, #136	; 0x88
 80a8582:	ab0e      	add	r3, sp, #56	; 0x38
 80a8584:	f8cd b014 	str.w	fp, [sp, #20]
 80a8588:	9704      	str	r7, [sp, #16]
 80a858a:	f8cd a00c 	str.w	sl, [sp, #12]
 80a858e:	9602      	str	r6, [sp, #8]
 80a8590:	e88d 0220 	stmia.w	sp, {r5, r9}
 80a8594:	f7ff fad2 	bl	80a7b3c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_>

  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<float>(input), GetTensorShape(filter),
                      GetTensorData<float>(filter), GetTensorShape(bias),
                      GetTensorData<float>(bias), GetTensorShape(output),
                      GetTensorData<float>(output), GetTensorShape(im2col),
 80a8598:	4658      	mov	r0, fp
 80a859a:	f7f9 ff0a 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.float_activation_max = output_activation_max;

  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<float>(input), GetTensorShape(filter),
                      GetTensorData<float>(filter), GetTensorShape(bias),
                      GetTensorData<float>(bias), GetTensorShape(output),
 80a859e:	4650      	mov	r0, sl
 80a85a0:	f7f9 ff07 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<float>(input), GetTensorShape(filter),
                      GetTensorData<float>(filter), GetTensorShape(bias),
 80a85a4:	4648      	mov	r0, r9
 80a85a6:	f7f9 ff04 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<float>(input), GetTensorShape(filter),
 80a85aa:	a80e      	add	r0, sp, #56	; 0x38
 80a85ac:	f7f9 ff01 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  reference_ops::Conv(op_params, GetTensorShape(input),
 80a85b0:	a809      	add	r0, sp, #36	; 0x24
 80a85b2:	f7f9 fefe 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                      GetTensorData<float>(input), GetTensorShape(filter),
                      GetTensorData<float>(filter), GetTensorShape(bias),
                      GetTensorData<float>(bias), GetTensorShape(output),
                      GetTensorData<float>(output), GetTensorShape(im2col),
                      GetTensorData<float>(im2col));
}
 80a85b6:	b031      	add	sp, #196	; 0xc4
 80a85b8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80a85bc:	7f7fffff 	.word	0x7f7fffff
 80a85c0:	40c00000 	.word	0x40c00000
 80a85c4:	bf800000 	.word	0xbf800000

080a85c8 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80a85c8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a85cc:	680a      	ldr	r2, [r1, #0]
 80a85ce:	4688      	mov	r8, r1
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a85d0:	6851      	ldr	r1, [r2, #4]
 80a85d2:	2338      	movs	r3, #56	; 0x38
 80a85d4:	fb03 fa01 	mul.w	sl, r3, r1
 80a85d8:	6887      	ldr	r7, [r0, #8]
 80a85da:	6896      	ldr	r6, [r2, #8]
}

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
 80a85dc:	68d2      	ldr	r2, [r2, #12]
 80a85de:	f5ad 7d15 	sub.w	sp, sp, #596	; 0x254
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a85e2:	eb07 010a 	add.w	r1, r7, sl
 80a85e6:	910b      	str	r1, [sp, #44]	; 0x2c

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
 80a85e8:	1c51      	adds	r1, r2, #1
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a85ea:	fb03 7606 	mla	r6, r3, r6, r7
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a85ee:	bf18      	it	ne
 80a85f0:	fb03 7302 	mlane	r3, r3, r2, r7
 80a85f4:	4605      	mov	r5, r0
  int filter_height = filter->dims->data[1];
  int output_width = output->dims->data[2];
  int output_height = output->dims->data[1];

  OpData data;
  if (input->type != kTfLiteFloat32) {
 80a85f6:	f817 000a 	ldrb.w	r0, [r7, sl]
  }
  return nullptr;
 80a85fa:	bf08      	it	eq
 80a85fc:	2300      	moveq	r3, #0
 80a85fe:	2801      	cmp	r0, #1
 80a8600:	930a      	str	r3, [sp, #40]	; 0x28
 80a8602:	d024      	beq.n	80a864e <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x86>
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
 80a8604:	f896 4030 	ldrb.w	r4, [r6, #48]	; 0x30
 80a8608:	2c01      	cmp	r4, #1
 80a860a:	d00e      	beq.n	80a862a <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x62>
 80a860c:	4b46      	ldr	r3, [pc, #280]	; (80a8728 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x160>)
 80a860e:	2601      	movs	r6, #1
 80a8610:	9301      	str	r3, [sp, #4]
 80a8612:	4b46      	ldr	r3, [pc, #280]	; (80a872c <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x164>)
 80a8614:	9402      	str	r4, [sp, #8]
 80a8616:	9300      	str	r3, [sp, #0]
 80a8618:	696c      	ldr	r4, [r5, #20]
 80a861a:	9603      	str	r6, [sp, #12]
 80a861c:	23dd      	movs	r3, #221	; 0xdd
 80a861e:	4a44      	ldr	r2, [pc, #272]	; (80a8730 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x168>)
 80a8620:	4944      	ldr	r1, [pc, #272]	; (80a8734 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x16c>)
 80a8622:	4628      	mov	r0, r5
 80a8624:	47a0      	blx	r4
 80a8626:	4634      	mov	r4, r6
 80a8628:	e079      	b.n	80a871e <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x156>
                      kTfLiteAffineQuantization);

    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
 80a862a:	6b73      	ldr	r3, [r6, #52]	; 0x34
    TF_LITE_ENSURE(context, affine_quantization);
 80a862c:	b923      	cbnz	r3, 80a8638 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x70>
 80a862e:	4b42      	ldr	r3, [pc, #264]	; (80a8738 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x170>)
 80a8630:	696e      	ldr	r6, [r5, #20]
 80a8632:	9300      	str	r3, [sp, #0]
 80a8634:	23e2      	movs	r3, #226	; 0xe2
 80a8636:	e005      	b.n	80a8644 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x7c>
    TF_LITE_ENSURE(context, affine_quantization->scale);
 80a8638:	681b      	ldr	r3, [r3, #0]
 80a863a:	b943      	cbnz	r3, 80a864e <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x86>
 80a863c:	4b3f      	ldr	r3, [pc, #252]	; (80a873c <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x174>)
 80a863e:	696e      	ldr	r6, [r5, #20]
 80a8640:	9300      	str	r3, [sp, #0]
 80a8642:	23e3      	movs	r3, #227	; 0xe3
 80a8644:	4a3a      	ldr	r2, [pc, #232]	; (80a8730 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x168>)
 80a8646:	493e      	ldr	r1, [pc, #248]	; (80a8740 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x178>)
 80a8648:	4628      	mov	r0, r5
 80a864a:	47b0      	blx	r6
 80a864c:	e067      	b.n	80a871e <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x156>
                      GetTensorData<float>(output), GetTensorShape(im2col),
                      GetTensorData<float>(im2col));
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
 80a864e:	f8d8 3014 	ldr.w	r3, [r8, #20]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a8652:	f04f 0938 	mov.w	r9, #56	; 0x38
 80a8656:	9309      	str	r3, [sp, #36]	; 0x24
 80a8658:	f8d8 3004 	ldr.w	r3, [r8, #4]
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
  }

  TF_LITE_ENSURE_STATUS(CalculateOpData(
 80a865c:	f10d 0b30 	add.w	fp, sp, #48	; 0x30
 80a8660:	685b      	ldr	r3, [r3, #4]
  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);

  int input_width = input->dims->data[2];
  int input_height = input->dims->data[1];
  int filter_width = filter->dims->data[2];
 80a8662:	68b2      	ldr	r2, [r6, #8]
 80a8664:	fb09 7903 	mla	r9, r9, r3, r7
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);

  int input_width = input->dims->data[2];
 80a8668:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
  int input_height = input->dims->data[1];
  int filter_width = filter->dims->data[2];
  int filter_height = filter->dims->data[1];
  int output_width = output->dims->data[2];
 80a866a:	f8d9 1008 	ldr.w	r1, [r9, #8]
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);

  int input_width = input->dims->data[2];
 80a866e:	689b      	ldr	r3, [r3, #8]
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
  }

  TF_LITE_ENSURE_STATUS(CalculateOpData(
 80a8670:	f8cd b018 	str.w	fp, [sp, #24]
 80a8674:	9005      	str	r0, [sp, #20]
 80a8676:	6888      	ldr	r0, [r1, #8]
 80a8678:	9004      	str	r0, [sp, #16]
 80a867a:	68c9      	ldr	r1, [r1, #12]
 80a867c:	4628      	mov	r0, r5
 80a867e:	9103      	str	r1, [sp, #12]
 80a8680:	6891      	ldr	r1, [r2, #8]
 80a8682:	9102      	str	r1, [sp, #8]
 80a8684:	68d2      	ldr	r2, [r2, #12]
 80a8686:	4641      	mov	r1, r8
 80a8688:	9201      	str	r2, [sp, #4]
 80a868a:	689a      	ldr	r2, [r3, #8]
 80a868c:	9200      	str	r2, [sp, #0]
 80a868e:	68db      	ldr	r3, [r3, #12]
 80a8690:	9a09      	ldr	r2, [sp, #36]	; 0x24
 80a8692:	f7ff fd7d 	bl	80a8190 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE>
 80a8696:	4604      	mov	r4, r0
 80a8698:	2800      	cmp	r0, #0
 80a869a:	d13f      	bne.n	80a871c <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x154>
      context, node, params, input_width, input_height, filter_width,
      filter_height, output_width, output_height, input->type, &data));

  switch (input->type) {  // Already know in/out types are same.
 80a869c:	f817 000a 	ldrb.w	r0, [r7, sl]
 80a86a0:	2803      	cmp	r0, #3
 80a86a2:	d022      	beq.n	80a86ea <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x122>
 80a86a4:	2809      	cmp	r0, #9
 80a86a6:	d011      	beq.n	80a86cc <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x104>
 80a86a8:	2801      	cmp	r0, #1
 80a86aa:	d12e      	bne.n	80a870a <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x142>
    case kTfLiteFloat32:
      EvalFloat(context, node, params, &data, input, filter, bias, nullptr,
                nullptr, output);
 80a86ac:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80a86ae:	f8cd 9014 	str.w	r9, [sp, #20]
 80a86b2:	9302      	str	r3, [sp, #8]
 80a86b4:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80a86b6:	9404      	str	r4, [sp, #16]
 80a86b8:	9300      	str	r3, [sp, #0]
 80a86ba:	9403      	str	r4, [sp, #12]
 80a86bc:	9601      	str	r6, [sp, #4]
 80a86be:	465b      	mov	r3, fp
 80a86c0:	9a09      	ldr	r2, [sp, #36]	; 0x24
 80a86c2:	4641      	mov	r1, r8
 80a86c4:	4628      	mov	r0, r5
 80a86c6:	f7ff fef5 	bl	80a84b4 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_>
      break;
 80a86ca:	e028      	b.n	80a871e <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x156>
    case kTfLiteInt8:
      EvalQuantizedPerChannel(context, node, params, &data, input, filter, bias,
                              output, nullptr);
 80a86cc:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80a86ce:	9404      	str	r4, [sp, #16]
 80a86d0:	9302      	str	r3, [sp, #8]
 80a86d2:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80a86d4:	f8cd 900c 	str.w	r9, [sp, #12]
 80a86d8:	9300      	str	r3, [sp, #0]
 80a86da:	9601      	str	r6, [sp, #4]
 80a86dc:	465b      	mov	r3, fp
 80a86de:	9a09      	ldr	r2, [sp, #36]	; 0x24
 80a86e0:	4641      	mov	r1, r8
 80a86e2:	4628      	mov	r0, r5
 80a86e4:	f7ff fe85 	bl	80a83f2 <_ZN6tflite3ops5micro4conv23EvalQuantizedPerChannelEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_>
      break;
 80a86e8:	e019      	b.n	80a871e <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x156>
    case kTfLiteUInt8:
      EvalQuantized(context, node, params, &data, input, filter, bias, nullptr,
                    nullptr, output);
 80a86ea:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80a86ec:	f8cd 9014 	str.w	r9, [sp, #20]
 80a86f0:	9302      	str	r3, [sp, #8]
 80a86f2:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80a86f4:	9404      	str	r4, [sp, #16]
 80a86f6:	9300      	str	r3, [sp, #0]
 80a86f8:	9403      	str	r4, [sp, #12]
 80a86fa:	9601      	str	r6, [sp, #4]
 80a86fc:	465b      	mov	r3, fp
 80a86fe:	9a09      	ldr	r2, [sp, #36]	; 0x24
 80a8700:	4641      	mov	r1, r8
 80a8702:	4628      	mov	r0, r5
 80a8704:	f7ff fdf0 	bl	80a82e8 <_ZN6tflite3ops5micro4conv13EvalQuantizedEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_>
      break;
 80a8708:	e009      	b.n	80a871e <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x156>
    default:
      context->ReportError(context, "Type %s (%d) not supported.",
 80a870a:	696c      	ldr	r4, [r5, #20]
 80a870c:	f7f7 fd06 	bl	80a011c <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type), input->type);
 80a8710:	f817 300a 	ldrb.w	r3, [r7, sl]
 80a8714:	4602      	mov	r2, r0
 80a8716:	490b      	ldr	r1, [pc, #44]	; (80a8744 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x17c>)
 80a8718:	4628      	mov	r0, r5
 80a871a:	47a0      	blx	r4
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
  }

  TF_LITE_ENSURE_STATUS(CalculateOpData(
 80a871c:	2401      	movs	r4, #1
      context->ReportError(context, "Type %s (%d) not supported.",
                           TfLiteTypeGetName(input->type), input->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
 80a871e:	4620      	mov	r0, r4
 80a8720:	f50d 7d15 	add.w	sp, sp, #596	; 0x254
 80a8724:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80a8728:	080b5dff 	.word	0x080b5dff
 80a872c:	080b5e19 	.word	0x080b5e19
 80a8730:	080b5d0a 	.word	0x080b5d0a
 80a8734:	080b5be0 	.word	0x080b5be0
 80a8738:	080b5e33 	.word	0x080b5e33
 80a873c:	080b5e47 	.word	0x080b5e47
 80a8740:	080b5db0 	.word	0x080b5db0
 80a8744:	080b5e62 	.word	0x080b5e62

080a8748 <_ZN6tflite3ops5micro16Register_CONV_2DEv>:

TfLiteRegistration* Register_CONV_2D() {
  static TfLiteRegistration r = {conv::Init, conv::Free, conv::Prepare,
                                 conv::Eval};
  return &r;
}
 80a8748:	4800      	ldr	r0, [pc, #0]	; (80a874c <_ZN6tflite3ops5micro16Register_CONV_2DEv+0x4>)
 80a874a:	4770      	bx	lr
 80a874c:	20000148 	.word	0x20000148

080a8750 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode>:
namespace tflite {
namespace ops {
namespace micro {
namespace dequantize {

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 80a8750:	b5f0      	push	{r4, r5, r6, r7, lr}
 80a8752:	680b      	ldr	r3, [r1, #0]
 80a8754:	b085      	sub	sp, #20
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
 80a8756:	681e      	ldr	r6, [r3, #0]
 80a8758:	4605      	mov	r5, r0
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
 80a875a:	2e01      	cmp	r6, #1
 80a875c:	d00c      	beq.n	80a8778 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x28>
 80a875e:	4b20      	ldr	r3, [pc, #128]	; (80a87e0 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x90>)
 80a8760:	2401      	movs	r4, #1
 80a8762:	9301      	str	r3, [sp, #4]
 80a8764:	4b1f      	ldr	r3, [pc, #124]	; (80a87e4 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x94>)
 80a8766:	9403      	str	r4, [sp, #12]
 80a8768:	9300      	str	r3, [sp, #0]
 80a876a:	9602      	str	r6, [sp, #8]
 80a876c:	6945      	ldr	r5, [r0, #20]
 80a876e:	231d      	movs	r3, #29
 80a8770:	4a1d      	ldr	r2, [pc, #116]	; (80a87e8 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x98>)
 80a8772:	491e      	ldr	r1, [pc, #120]	; (80a87ec <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x9c>)
 80a8774:	47a8      	blx	r5
 80a8776:	e02d      	b.n	80a87d4 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x84>
 80a8778:	684f      	ldr	r7, [r1, #4]
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
 80a877a:	683c      	ldr	r4, [r7, #0]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
 80a877c:	2c01      	cmp	r4, #1
 80a877e:	d00b      	beq.n	80a8798 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x48>
 80a8780:	4b17      	ldr	r3, [pc, #92]	; (80a87e0 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x90>)
 80a8782:	9603      	str	r6, [sp, #12]
 80a8784:	9301      	str	r3, [sp, #4]
 80a8786:	4b1a      	ldr	r3, [pc, #104]	; (80a87f0 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0xa0>)
 80a8788:	9402      	str	r4, [sp, #8]
 80a878a:	9300      	str	r3, [sp, #0]
 80a878c:	6944      	ldr	r4, [r0, #20]
 80a878e:	231e      	movs	r3, #30
 80a8790:	4a15      	ldr	r2, [pc, #84]	; (80a87e8 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x98>)
 80a8792:	4916      	ldr	r1, [pc, #88]	; (80a87ec <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x9c>)
 80a8794:	47a0      	blx	r4
 80a8796:	e01d      	b.n	80a87d4 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x84>

  // TODO(b/140515557): Add cached dequant to improve hybrid model performance.
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  TF_LITE_ENSURE(context,
 80a8798:	685a      	ldr	r2, [r3, #4]
 80a879a:	2338      	movs	r3, #56	; 0x38
 80a879c:	435a      	muls	r2, r3
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);

  // TODO(b/140515557): Add cached dequant to improve hybrid model performance.
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
 80a879e:	6881      	ldr	r1, [r0, #8]
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  TF_LITE_ENSURE(context,
 80a87a0:	5c8a      	ldrb	r2, [r1, r2]
 80a87a2:	2a03      	cmp	r2, #3
 80a87a4:	d009      	beq.n	80a87ba <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x6a>
 80a87a6:	2a09      	cmp	r2, #9
 80a87a8:	d007      	beq.n	80a87ba <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x6a>
 80a87aa:	4b12      	ldr	r3, [pc, #72]	; (80a87f4 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0xa4>)
 80a87ac:	4a0e      	ldr	r2, [pc, #56]	; (80a87e8 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x98>)
 80a87ae:	9300      	str	r3, [sp, #0]
 80a87b0:	6945      	ldr	r5, [r0, #20]
 80a87b2:	2325      	movs	r3, #37	; 0x25
 80a87b4:	4910      	ldr	r1, [pc, #64]	; (80a87f8 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0xa8>)
 80a87b6:	47a8      	blx	r5
 80a87b8:	e00c      	b.n	80a87d4 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x84>
                 input->type == kTfLiteUInt8 || input->type == kTfLiteInt8);
  TF_LITE_ENSURE(context, output->type == kTfLiteFloat32);
 80a87ba:	687a      	ldr	r2, [r7, #4]
 80a87bc:	4353      	muls	r3, r2
 80a87be:	5ccb      	ldrb	r3, [r1, r3]
 80a87c0:	2b01      	cmp	r3, #1
 80a87c2:	d009      	beq.n	80a87d8 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x88>
 80a87c4:	4b0d      	ldr	r3, [pc, #52]	; (80a87fc <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0xac>)
 80a87c6:	4a08      	ldr	r2, [pc, #32]	; (80a87e8 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x98>)
 80a87c8:	9300      	str	r3, [sp, #0]
 80a87ca:	696c      	ldr	r4, [r5, #20]
 80a87cc:	2326      	movs	r3, #38	; 0x26
 80a87ce:	490a      	ldr	r1, [pc, #40]	; (80a87f8 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0xa8>)
 80a87d0:	4628      	mov	r0, r5
 80a87d2:	47a0      	blx	r4
 80a87d4:	2001      	movs	r0, #1
 80a87d6:	e000      	b.n	80a87da <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x8a>

  return kTfLiteOk;
 80a87d8:	2000      	movs	r0, #0
}
 80a87da:	b005      	add	sp, #20
 80a87dc:	bdf0      	pop	{r4, r5, r6, r7, pc}
 80a87de:	bf00      	nop
 80a87e0:	080b75ad 	.word	0x080b75ad
 80a87e4:	080b5bfa 	.word	0x080b5bfa
 80a87e8:	080b5ed8 	.word	0x080b5ed8
 80a87ec:	080b5be0 	.word	0x080b5be0
 80a87f0:	080b5c0a 	.word	0x080b5c0a
 80a87f4:	080b5f84 	.word	0x080b5f84
 80a87f8:	080b5db0 	.word	0x080b5db0
 80a87fc:	080b5fbe 	.word	0x080b5fbe

080a8800 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_>:
}

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
 80a8800:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
 80a8804:	6806      	ldr	r6, [r0, #0]
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
 80a8806:	680b      	ldr	r3, [r1, #0]
}

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
 80a8808:	4604      	mov	r4, r0
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
 80a880a:	429e      	cmp	r6, r3
}

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
 80a880c:	460f      	mov	r7, r1
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
 80a880e:	d101      	bne.n	80a8814 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_+0x14>
 80a8810:	2500      	movs	r5, #0
 80a8812:	e00d      	b.n	80a8830 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_+0x30>
 80a8814:	f007 fc0c 	bl	80b0030 <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
 80a8818:	4629      	mov	r1, r5
 80a881a:	4620      	mov	r0, r4
 80a881c:	f7f9 fdd4 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a8820:	4629      	mov	r1, r5
 80a8822:	4680      	mov	r8, r0
 80a8824:	4638      	mov	r0, r7
 80a8826:	f7f9 fdcf 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a882a:	4580      	cmp	r8, r0
 80a882c:	d1f2      	bne.n	80a8814 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_+0x14>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
 80a882e:	3501      	adds	r5, #1
 80a8830:	42b5      	cmp	r5, r6
 80a8832:	dbf1      	blt.n	80a8818 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_+0x18>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80a8834:	2e04      	cmp	r6, #4
 80a8836:	bfcc      	ite	gt
 80a8838:	6864      	ldrgt	r4, [r4, #4]
 80a883a:	3404      	addle	r4, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a883c:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
 80a883e:	2001      	movs	r0, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a8840:	429e      	cmp	r6, r3
 80a8842:	dd04      	ble.n	80a884e <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_+0x4e>
      buffer_size *= dims_data[i];
 80a8844:	f854 2023 	ldr.w	r2, [r4, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a8848:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
 80a884a:	4350      	muls	r0, r2
 80a884c:	e7f8      	b.n	80a8840 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_+0x40>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
  }
  return shape.FlatSize();
}
 80a884e:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
	...

080a8854 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80a8854:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
 80a8858:	680b      	ldr	r3, [r1, #0]
 80a885a:	f8d0 a008 	ldr.w	sl, [r0, #8]
 80a885e:	685a      	ldr	r2, [r3, #4]
 80a8860:	2338      	movs	r3, #56	; 0x38
 80a8862:	fb03 f802 	mul.w	r8, r3, r2
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
 80a8866:	684a      	ldr	r2, [r1, #4]

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
 80a8868:	eb0a 0508 	add.w	r5, sl, r8
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
 80a886c:	6854      	ldr	r4, [r2, #4]
  TF_LITE_ENSURE(context, output->type == kTfLiteFloat32);

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80a886e:	b08b      	sub	sp, #44	; 0x2c
 80a8870:	4683      	mov	fp, r0
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  tflite::DequantizationParams op_params;
  op_params.zero_point = input->params.zero_point;
  op_params.scale = input->params.scale;
 80a8872:	68e8      	ldr	r0, [r5, #12]
  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
 80a8874:	fb03 a404 	mla	r4, r3, r4, sl

  tflite::DequantizationParams op_params;
  op_params.zero_point = input->params.zero_point;
  op_params.scale = input->params.scale;
 80a8878:	f00a fa24 	bl	80b2cc4 <__aeabi_f2d>
 80a887c:	4606      	mov	r6, r0
  switch (input->type) {
 80a887e:	f81a 0008 	ldrb.w	r0, [sl, r8]
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  tflite::DequantizationParams op_params;
  op_params.zero_point = input->params.zero_point;
 80a8882:	f8d5 9010 	ldr.w	r9, [r5, #16]
  op_params.scale = input->params.scale;
  switch (input->type) {
 80a8886:	2803      	cmp	r0, #3
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  tflite::DequantizationParams op_params;
  op_params.zero_point = input->params.zero_point;
  op_params.scale = input->params.scale;
 80a8888:	460f      	mov	r7, r1
  switch (input->type) {
 80a888a:	d002      	beq.n	80a8892 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0x3e>
 80a888c:	2809      	cmp	r0, #9
 80a888e:	d024      	beq.n	80a88da <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0x86>
 80a8890:	e04f      	b.n	80a8932 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0xde>
    case kTfLiteUInt8:
      reference_ops::Dequantize(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
 80a8892:	4629      	mov	r1, r5
 80a8894:	4668      	mov	r0, sp
 80a8896:	f7fa f83c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
          GetTensorShape(output), GetTensorData<float>(output));
 80a889a:	4621      	mov	r1, r4
 80a889c:	a805      	add	r0, sp, #20
 80a889e:	f8d5 8004 	ldr.w	r8, [r5, #4]
 80a88a2:	f7fa f836 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a88a6:	b104      	cbz	r4, 80a88aa <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0x56>
 80a88a8:	6864      	ldr	r4, [r4, #4]
inline void Dequantize(const tflite::DequantizationParams& op_params,
                       const RuntimeShape& input_shape, const T* input_data,
                       const RuntimeShape& output_shape, float* output_data) {
  int32 zero_point = op_params.zero_point;
  const double scale = op_params.scale;
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
 80a88aa:	a905      	add	r1, sp, #20
 80a88ac:	4668      	mov	r0, sp
 80a88ae:	f7ff ffa7 	bl	80a8800 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_>
 80a88b2:	4682      	mov	sl, r0

  for (int i = 0; i < flat_size; i++) {
 80a88b4:	2500      	movs	r5, #0
 80a88b6:	45aa      	cmp	sl, r5
 80a88b8:	dd33      	ble.n	80a8922 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0xce>
    const int32 val = input_data[i];
    const float result = static_cast<float>(scale * (val - zero_point));
    output_data[i] = result;
 80a88ba:	f818 0005 	ldrb.w	r0, [r8, r5]
 80a88be:	ebc9 0000 	rsb	r0, r9, r0
 80a88c2:	f00a f9ed 	bl	80b2ca0 <__aeabi_i2d>
 80a88c6:	4632      	mov	r2, r6
 80a88c8:	463b      	mov	r3, r7
 80a88ca:	f00a fa4f 	bl	80b2d6c <__aeabi_dmul>
 80a88ce:	f00a fd2f 	bl	80b3330 <__aeabi_d2f>
 80a88d2:	f844 0025 	str.w	r0, [r4, r5, lsl #2]
                       const RuntimeShape& output_shape, float* output_data) {
  int32 zero_point = op_params.zero_point;
  const double scale = op_params.scale;
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; i++) {
 80a88d6:	3501      	adds	r5, #1
 80a88d8:	e7ed      	b.n	80a88b6 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0x62>
      break;
    case kTfLiteInt8:
      reference_ops::Dequantize(
          op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
 80a88da:	4629      	mov	r1, r5
 80a88dc:	4668      	mov	r0, sp
 80a88de:	f7fa f818 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
          GetTensorShape(output), GetTensorData<float>(output));
 80a88e2:	4621      	mov	r1, r4
 80a88e4:	a805      	add	r0, sp, #20
 80a88e6:	f8d5 8004 	ldr.w	r8, [r5, #4]
 80a88ea:	f7fa f812 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a88ee:	b104      	cbz	r4, 80a88f2 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0x9e>
 80a88f0:	6864      	ldr	r4, [r4, #4]
inline void Dequantize(const tflite::DequantizationParams& op_params,
                       const RuntimeShape& input_shape, const T* input_data,
                       const RuntimeShape& output_shape, float* output_data) {
  int32 zero_point = op_params.zero_point;
  const double scale = op_params.scale;
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
 80a88f2:	a905      	add	r1, sp, #20
 80a88f4:	4668      	mov	r0, sp
 80a88f6:	f7ff ff83 	bl	80a8800 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_>
 80a88fa:	4682      	mov	sl, r0

  for (int i = 0; i < flat_size; i++) {
 80a88fc:	2500      	movs	r5, #0
 80a88fe:	45aa      	cmp	sl, r5
 80a8900:	dd0f      	ble.n	80a8922 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0xce>
    const int32 val = input_data[i];
    const float result = static_cast<float>(scale * (val - zero_point));
    output_data[i] = result;
 80a8902:	f918 0005 	ldrsb.w	r0, [r8, r5]
 80a8906:	ebc9 0000 	rsb	r0, r9, r0
 80a890a:	f00a f9c9 	bl	80b2ca0 <__aeabi_i2d>
 80a890e:	4632      	mov	r2, r6
 80a8910:	463b      	mov	r3, r7
 80a8912:	f00a fa2b 	bl	80b2d6c <__aeabi_dmul>
 80a8916:	f00a fd0b 	bl	80b3330 <__aeabi_d2f>
 80a891a:	f844 0025 	str.w	r0, [r4, r5, lsl #2]
                       const RuntimeShape& output_shape, float* output_data) {
  int32 zero_point = op_params.zero_point;
  const double scale = op_params.scale;
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; i++) {
 80a891e:	3501      	adds	r5, #1
 80a8920:	e7ed      	b.n	80a88fe <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0xaa>
 80a8922:	a805      	add	r0, sp, #20
 80a8924:	f7f9 fd45 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(output), GetTensorData<float>(output));
      break;
    case kTfLiteInt8:
      reference_ops::Dequantize(
          op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
 80a8928:	4668      	mov	r0, sp
 80a892a:	f7f9 fd42 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(context, "Type %s (%d) not supported.",
                           TfLiteTypeGetName(input->type), input->type);
      return kTfLiteError;
  }

  return kTfLiteOk;
 80a892e:	2000      	movs	r0, #0
      break;
    case kTfLiteInt8:
      reference_ops::Dequantize(
          op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
          GetTensorShape(output), GetTensorData<float>(output));
      break;
 80a8930:	e00a      	b.n	80a8948 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0xf4>
    default:
      context->ReportError(context, "Type %s (%d) not supported.",
 80a8932:	f8db 4014 	ldr.w	r4, [fp, #20]
 80a8936:	f7f7 fbf1 	bl	80a011c <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type), input->type);
 80a893a:	f81a 3008 	ldrb.w	r3, [sl, r8]
 80a893e:	4602      	mov	r2, r0
 80a8940:	4903      	ldr	r1, [pc, #12]	; (80a8950 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0xfc>)
 80a8942:	4658      	mov	r0, fp
 80a8944:	47a0      	blx	r4
      return kTfLiteError;
 80a8946:	2001      	movs	r0, #1
  }

  return kTfLiteOk;
}
 80a8948:	b00b      	add	sp, #44	; 0x2c
 80a894a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80a894e:	bf00      	nop
 80a8950:	080b5e62 	.word	0x080b5e62

080a8954 <_ZN6tflite3ops5micro19Register_DEQUANTIZEEv>:

TfLiteRegistration* Register_DEQUANTIZE() {
  static TfLiteRegistration r = {nullptr, nullptr, dequantize::Prepare,
                                 dequantize::Eval};
  return &r;
}
 80a8954:	4800      	ldr	r0, [pc, #0]	; (80a8958 <_ZN6tflite3ops5micro19Register_DEQUANTIZEEv+0x4>)
 80a8956:	4770      	bx	lr
 80a8958:	20000168 	.word	0x20000168

080a895c <_ZSt3absf>:
#endif

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  abs(float __x)
  { return __builtin_fabsf(__x); }
 80a895c:	f020 4000 	bic.w	r0, r0, #2147483648	; 0x80000000
 80a8960:	4770      	bx	lr

080a8962 <_ZZN6tflite3ops5micro11elementwise12_GLOBAL__N_110SquareEvalEP13TfLiteContextP10TfLiteNodeENUlfE_4_FUNEf>:
TfLiteStatus RsqrtEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalNumeric(context, node, [](float f) { return 1.f / std::sqrt(f); });
}

TfLiteStatus SquareEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalNumeric(context, node, [](float f) { return f * f; });
 80a8962:	b508      	push	{r3, lr}
 80a8964:	4601      	mov	r1, r0
 80a8966:	f00a fe41 	bl	80b35ec <__aeabi_fmul>
 80a896a:	bd08      	pop	{r3, pc}

080a896c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode>:
  return type == kTfLiteBool;
}

typedef bool (*IsSupportedType)(TfLiteType);
template <IsSupportedType>
TfLiteStatus GenericPrepare(TfLiteContext* context, TfLiteNode* node) {
 80a896c:	e92d 41ff 	stmdb	sp!, {r0, r1, r2, r3, r4, r5, r6, r7, r8, lr}
 80a8970:	680b      	ldr	r3, [r1, #0]
 80a8972:	4605      	mov	r5, r0
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
 80a8974:	681e      	ldr	r6, [r3, #0]
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
 80a8976:	2e01      	cmp	r6, #1
 80a8978:	d009      	beq.n	80a898e <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x22>
 80a897a:	4b21      	ldr	r3, [pc, #132]	; (80a8a00 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x94>)
 80a897c:	2401      	movs	r4, #1
 80a897e:	9301      	str	r3, [sp, #4]
 80a8980:	4b20      	ldr	r3, [pc, #128]	; (80a8a04 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x98>)
 80a8982:	9403      	str	r4, [sp, #12]
 80a8984:	9300      	str	r3, [sp, #0]
 80a8986:	9602      	str	r6, [sp, #8]
 80a8988:	6945      	ldr	r5, [r0, #20]
 80a898a:	2327      	movs	r3, #39	; 0x27
 80a898c:	e022      	b.n	80a89d4 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x68>
 80a898e:	6849      	ldr	r1, [r1, #4]
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
 80a8990:	680c      	ldr	r4, [r1, #0]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
 80a8992:	2c01      	cmp	r4, #1
 80a8994:	d00c      	beq.n	80a89b0 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x44>
 80a8996:	4b1a      	ldr	r3, [pc, #104]	; (80a8a00 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x94>)
 80a8998:	9603      	str	r6, [sp, #12]
 80a899a:	9301      	str	r3, [sp, #4]
 80a899c:	4b1a      	ldr	r3, [pc, #104]	; (80a8a08 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x9c>)
 80a899e:	9402      	str	r4, [sp, #8]
 80a89a0:	9300      	str	r3, [sp, #0]
 80a89a2:	6944      	ldr	r4, [r0, #20]
 80a89a4:	2328      	movs	r3, #40	; 0x28
 80a89a6:	4a19      	ldr	r2, [pc, #100]	; (80a8a0c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa0>)
 80a89a8:	4919      	ldr	r1, [pc, #100]	; (80a8a10 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa4>)
 80a89aa:	47a0      	blx	r4
 80a89ac:	4630      	mov	r0, r6
 80a89ae:	e023      	b.n	80a89f8 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x8c>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a89b0:	685e      	ldr	r6, [r3, #4]
  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, output->type);
 80a89b2:	6849      	ldr	r1, [r1, #4]
 80a89b4:	2238      	movs	r2, #56	; 0x38
 80a89b6:	4356      	muls	r6, r2
 80a89b8:	434a      	muls	r2, r1
 80a89ba:	6887      	ldr	r7, [r0, #8]
 80a89bc:	5dbb      	ldrb	r3, [r7, r6]
 80a89be:	5cba      	ldrb	r2, [r7, r2]
 80a89c0:	4293      	cmp	r3, r2
 80a89c2:	d00b      	beq.n	80a89dc <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x70>
 80a89c4:	9302      	str	r3, [sp, #8]
 80a89c6:	4b13      	ldr	r3, [pc, #76]	; (80a8a14 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa8>)
 80a89c8:	9203      	str	r2, [sp, #12]
 80a89ca:	9301      	str	r3, [sp, #4]
 80a89cc:	4b12      	ldr	r3, [pc, #72]	; (80a8a18 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xac>)
 80a89ce:	9300      	str	r3, [sp, #0]
 80a89d0:	6945      	ldr	r5, [r0, #20]
 80a89d2:	232b      	movs	r3, #43	; 0x2b
 80a89d4:	4a0d      	ldr	r2, [pc, #52]	; (80a8a0c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa0>)
 80a89d6:	490e      	ldr	r1, [pc, #56]	; (80a8a10 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa4>)
 80a89d8:	47a8      	blx	r5
 80a89da:	e00a      	b.n	80a89f2 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x86>
  if (!IsSupportedType(input->type)) {
 80a89dc:	b95b      	cbnz	r3, 80a89f6 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x8a>
    context->ReportError(context, "Input data type %s (%d) is not supported.",
 80a89de:	f8d0 8014 	ldr.w	r8, [r0, #20]
 80a89e2:	4618      	mov	r0, r3
 80a89e4:	f7f7 fb9a 	bl	80a011c <TfLiteTypeGetName>
 80a89e8:	5dbb      	ldrb	r3, [r7, r6]
 80a89ea:	4602      	mov	r2, r0
 80a89ec:	490b      	ldr	r1, [pc, #44]	; (80a8a1c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xb0>)
 80a89ee:	4628      	mov	r0, r5
 80a89f0:	47c0      	blx	r8
                         TfLiteTypeGetName(input->type), input->type);
    return kTfLiteError;
 80a89f2:	4620      	mov	r0, r4
 80a89f4:	e000      	b.n	80a89f8 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x8c>
  }
  return kTfLiteOk;
 80a89f6:	2000      	movs	r0, #0
}
 80a89f8:	b004      	add	sp, #16
 80a89fa:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80a89fe:	bf00      	nop
 80a8a00:	080b75ad 	.word	0x080b75ad
 80a8a04:	080b5bfa 	.word	0x080b5bfa
 80a8a08:	080b5c0a 	.word	0x080b5c0a
 80a8a0c:	080b5fdd 	.word	0x080b5fdd
 80a8a10:	080b5be0 	.word	0x080b5be0
 80a8a14:	080b5c27 	.word	0x080b5c27
 80a8a18:	080b5c1b 	.word	0x080b5c1b
 80a8a1c:	080b608a 	.word	0x080b608a

080a8a20 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsNumericSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode>:
 80a8a20:	f7ff bfa4 	b.w	80a896c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode>

080a8a24 <_ZSt3sinf>:
  using ::sin;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  sin(float __x)
  { return __builtin_sinf(__x); }
 80a8a24:	f008 bb6c 	b.w	80b1100 <sinf>

080a8a28 <_ZSt3cosf>:
  using ::cos;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  cos(float __x)
  { return __builtin_cosf(__x); }
 80a8a28:	f008 ba82 	b.w	80b0f30 <cosf>

080a8a2c <_ZSt3logf>:
  using ::log;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  log(float __x)
  { return __builtin_logf(__x); }
 80a8a2c:	f008 bc96 	b.w	80b135c <logf>

080a8a30 <_ZSt4sqrtf>:
  using ::sqrt;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  sqrt(float __x)
  { return __builtin_sqrtf(__x); }
 80a8a30:	f008 bd08 	b.w	80b1444 <sqrtf>

080a8a34 <_ZZN6tflite3ops5micro11elementwise12_GLOBAL__N_19RsqrtEvalEP13TfLiteContextP10TfLiteNodeENUlfE_4_FUNEf>:
TfLiteStatus SqrtEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalNumeric(context, node, std::sqrt);
}

TfLiteStatus RsqrtEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalNumeric(context, node, [](float f) { return 1.f / std::sqrt(f); });
 80a8a34:	b508      	push	{r3, lr}
 80a8a36:	f008 fd05 	bl	80b1444 <sqrtf>
 80a8a3a:	4601      	mov	r1, r0
 80a8a3c:	f04f 507e 	mov.w	r0, #1065353216	; 0x3f800000
 80a8a40:	f00a fe88 	bl	80b3754 <__aeabi_fdiv>
 80a8a44:	bd08      	pop	{r3, pc}
	...

080a8a48 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>:
  }
  return kTfLiteOk;
}

template <typename T>
inline TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node,
 80a8a48:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a8a4c:	4615      	mov	r5, r2
 80a8a4e:	680a      	ldr	r2, [r1, #0]
 80a8a50:	6883      	ldr	r3, [r0, #8]
 80a8a52:	6854      	ldr	r4, [r2, #4]
 80a8a54:	2238      	movs	r2, #56	; 0x38
 80a8a56:	4354      	muls	r4, r2
                             T func(T), TfLiteType expected_type) {
  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
 80a8a58:	5d1f      	ldrb	r7, [r3, r4]
  }
  return kTfLiteOk;
}

template <typename T>
inline TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node,
 80a8a5a:	b085      	sub	sp, #20
                             T func(T), TfLiteType expected_type) {
  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
 80a8a5c:	2f01      	cmp	r7, #1
 80a8a5e:	eb03 0204 	add.w	r2, r3, r4
 80a8a62:	d00d      	beq.n	80a8a80 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x38>
 80a8a64:	4b21      	ldr	r3, [pc, #132]	; (80a8aec <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0xa4>)
 80a8a66:	2401      	movs	r4, #1
 80a8a68:	9301      	str	r3, [sp, #4]
 80a8a6a:	4b21      	ldr	r3, [pc, #132]	; (80a8af0 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0xa8>)
 80a8a6c:	9403      	str	r4, [sp, #12]
 80a8a6e:	9300      	str	r3, [sp, #0]
 80a8a70:	9702      	str	r7, [sp, #8]
 80a8a72:	6945      	ldr	r5, [r0, #20]
 80a8a74:	2339      	movs	r3, #57	; 0x39
 80a8a76:	4a1f      	ldr	r2, [pc, #124]	; (80a8af4 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0xac>)
 80a8a78:	491f      	ldr	r1, [pc, #124]	; (80a8af8 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0xb0>)
 80a8a7a:	47a8      	blx	r5
 80a8a7c:	4620      	mov	r0, r4
 80a8a7e:	e032      	b.n	80a8ae6 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x9e>
 80a8a80:	6890      	ldr	r0, [r2, #8]
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
 80a8a82:	2400      	movs	r4, #0
 80a8a84:	f8d0 c000 	ldr.w	ip, [r0]
inline int NumIntermediates(const TfLiteNode* node) {
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
 80a8a88:	2601      	movs	r6, #1
 80a8a8a:	2700      	movs	r7, #0
  for (int i = 0; i < dims->size; ++i) {
 80a8a8c:	45a4      	cmp	ip, r4
 80a8a8e:	dd0c      	ble.n	80a8aaa <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x62>
    count *= dims->data[i];
 80a8a90:	f850 af04 	ldr.w	sl, [r0, #4]!
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
 80a8a94:	3401      	adds	r4, #1
    count *= dims->data[i];
 80a8a96:	ea4f 79ea 	mov.w	r9, sl, asr #31
 80a8a9a:	fb06 fe09 	mul.w	lr, r6, r9
 80a8a9e:	fb0a ee07 	mla	lr, sl, r7, lr
 80a8aa2:	fba6 670a 	umull	r6, r7, r6, sl
 80a8aa6:	4477      	add	r7, lr
 80a8aa8:	e7f0      	b.n	80a8a8c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x44>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a8aaa:	6849      	ldr	r1, [r1, #4]
 80a8aac:	2038      	movs	r0, #56	; 0x38
 80a8aae:	6849      	ldr	r1, [r1, #4]
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a8ab0:	f8d2 b004 	ldr.w	fp, [r2, #4]
 80a8ab4:	fb00 3301 	mla	r3, r0, r1, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a8ab8:	b10b      	cbz	r3, 80a8abe <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x76>
 80a8aba:	685c      	ldr	r4, [r3, #4]
 80a8abc:	e000      	b.n	80a8ac0 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x78>
 80a8abe:	461c      	mov	r4, r3
  const int64_t num_elements = NumElements(input);
  const T* in_data = GetTensorData<T>(input);
  T* out_data = GetTensorData<T>(output);
  for (int64_t i = 0; i < num_elements; ++i) {
 80a8ac0:	f04f 0800 	mov.w	r8, #0
 80a8ac4:	f04f 0900 	mov.w	r9, #0
 80a8ac8:	45b0      	cmp	r8, r6
 80a8aca:	eb79 0307 	sbcs.w	r3, r9, r7
 80a8ace:	da09      	bge.n	80a8ae4 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x9c>
    out_data[i] = func(in_data[i]);
 80a8ad0:	f85b 0028 	ldr.w	r0, [fp, r8, lsl #2]
 80a8ad4:	47a8      	blx	r5
 80a8ad6:	f844 0028 	str.w	r0, [r4, r8, lsl #2]
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
  const int64_t num_elements = NumElements(input);
  const T* in_data = GetTensorData<T>(input);
  T* out_data = GetTensorData<T>(output);
  for (int64_t i = 0; i < num_elements; ++i) {
 80a8ada:	f118 0801 	adds.w	r8, r8, #1
 80a8ade:	f149 0900 	adc.w	r9, r9, #0
 80a8ae2:	e7f1      	b.n	80a8ac8 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x80>
    out_data[i] = func(in_data[i]);
  }
  return kTfLiteOk;
 80a8ae4:	2000      	movs	r0, #0
}
 80a8ae6:	b005      	add	sp, #20
 80a8ae8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80a8aec:	080b60b4 	.word	0x080b60b4
 80a8af0:	080b5c1b 	.word	0x080b5c1b
 80a8af4:	080b5fdd 	.word	0x080b5fdd
 80a8af8:	080b5be0 	.word	0x080b5be0

080a8afc <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_110SquareEvalEP13TfLiteContextP10TfLiteNode>:

inline TfLiteStatus EvalNumeric(TfLiteContext* context, TfLiteNode* node,
                                float float_func(float)) {
  return EvalImpl<float>(context, node, float_func, kTfLiteFloat32);
 80a8afc:	4a01      	ldr	r2, [pc, #4]	; (80a8b04 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_110SquareEvalEP13TfLiteContextP10TfLiteNode+0x8>)
 80a8afe:	f7ff bfa3 	b.w	80a8a48 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
 80a8b02:	bf00      	nop
 80a8b04:	080a8963 	.word	0x080a8963

080a8b08 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17AbsEvalEP13TfLiteContextP10TfLiteNode>:
 80a8b08:	4a01      	ldr	r2, [pc, #4]	; (80a8b10 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17AbsEvalEP13TfLiteContextP10TfLiteNode+0x8>)
 80a8b0a:	f7ff bf9d 	b.w	80a8a48 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
 80a8b0e:	bf00      	nop
 80a8b10:	080a895d 	.word	0x080a895d

080a8b14 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17SinEvalEP13TfLiteContextP10TfLiteNode>:
 80a8b14:	4a01      	ldr	r2, [pc, #4]	; (80a8b1c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17SinEvalEP13TfLiteContextP10TfLiteNode+0x8>)
 80a8b16:	f7ff bf97 	b.w	80a8a48 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
 80a8b1a:	bf00      	nop
 80a8b1c:	080a8a25 	.word	0x080a8a25

080a8b20 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17CosEvalEP13TfLiteContextP10TfLiteNode>:
 80a8b20:	4a01      	ldr	r2, [pc, #4]	; (80a8b28 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17CosEvalEP13TfLiteContextP10TfLiteNode+0x8>)
 80a8b22:	f7ff bf91 	b.w	80a8a48 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
 80a8b26:	bf00      	nop
 80a8b28:	080a8a29 	.word	0x080a8a29

080a8b2c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_19RsqrtEvalEP13TfLiteContextP10TfLiteNode>:
 80a8b2c:	4a01      	ldr	r2, [pc, #4]	; (80a8b34 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_19RsqrtEvalEP13TfLiteContextP10TfLiteNode+0x8>)
 80a8b2e:	f7ff bf8b 	b.w	80a8a48 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
 80a8b32:	bf00      	nop
 80a8b34:	080a8a35 	.word	0x080a8a35

080a8b38 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17LogEvalEP13TfLiteContextP10TfLiteNode>:
 80a8b38:	4a01      	ldr	r2, [pc, #4]	; (80a8b40 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17LogEvalEP13TfLiteContextP10TfLiteNode+0x8>)
 80a8b3a:	f7ff bf85 	b.w	80a8a48 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
 80a8b3e:	bf00      	nop
 80a8b40:	080a8a2d 	.word	0x080a8a2d

080a8b44 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18SqrtEvalEP13TfLiteContextP10TfLiteNode>:
 80a8b44:	4a01      	ldr	r2, [pc, #4]	; (80a8b4c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18SqrtEvalEP13TfLiteContextP10TfLiteNode+0x8>)
 80a8b46:	f7ff bf7f 	b.w	80a8a48 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
 80a8b4a:	bf00      	nop
 80a8b4c:	080a8a31 	.word	0x080a8a31

080a8b50 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus SquareEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalNumeric(context, node, [](float f) { return f * f; });
}

TfLiteStatus LogicalNotEval(TfLiteContext* context, TfLiteNode* node) {
 80a8b50:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a8b54:	680a      	ldr	r2, [r1, #0]
 80a8b56:	460f      	mov	r7, r1
 80a8b58:	6851      	ldr	r1, [r2, #4]
 80a8b5a:	2238      	movs	r2, #56	; 0x38
 80a8b5c:	434a      	muls	r2, r1
 80a8b5e:	6883      	ldr	r3, [r0, #8]
 80a8b60:	b085      	sub	sp, #20
 80a8b62:	189e      	adds	r6, r3, r2
template <typename T>
inline TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node,
                             T func(T), TfLiteType expected_type) {
  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
 80a8b64:	5c9a      	ldrb	r2, [r3, r2]
 80a8b66:	2a06      	cmp	r2, #6
 80a8b68:	d00d      	beq.n	80a8b86 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x36>
 80a8b6a:	2306      	movs	r3, #6
 80a8b6c:	9303      	str	r3, [sp, #12]
 80a8b6e:	4b1f      	ldr	r3, [pc, #124]	; (80a8bec <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x9c>)
 80a8b70:	9202      	str	r2, [sp, #8]
 80a8b72:	9301      	str	r3, [sp, #4]
 80a8b74:	4b1e      	ldr	r3, [pc, #120]	; (80a8bf0 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0xa0>)
 80a8b76:	4a1f      	ldr	r2, [pc, #124]	; (80a8bf4 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0xa4>)
 80a8b78:	9300      	str	r3, [sp, #0]
 80a8b7a:	6944      	ldr	r4, [r0, #20]
 80a8b7c:	2339      	movs	r3, #57	; 0x39
 80a8b7e:	491e      	ldr	r1, [pc, #120]	; (80a8bf8 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0xa8>)
 80a8b80:	47a0      	blx	r4
 80a8b82:	2001      	movs	r0, #1
 80a8b84:	e02f      	b.n	80a8be6 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x96>
 80a8b86:	68b2      	ldr	r2, [r6, #8]
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
 80a8b88:	2400      	movs	r4, #0
 80a8b8a:	f8d2 c000 	ldr.w	ip, [r2]
inline int NumIntermediates(const TfLiteNode* node) {
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
 80a8b8e:	2001      	movs	r0, #1
 80a8b90:	2100      	movs	r1, #0
  for (int i = 0; i < dims->size; ++i) {
 80a8b92:	45a4      	cmp	ip, r4
 80a8b94:	dd0c      	ble.n	80a8bb0 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x60>
    count *= dims->data[i];
 80a8b96:	f852 ef04 	ldr.w	lr, [r2, #4]!
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
 80a8b9a:	3401      	adds	r4, #1
    count *= dims->data[i];
 80a8b9c:	ea4f 79ee 	mov.w	r9, lr, asr #31
 80a8ba0:	fb00 f509 	mul.w	r5, r0, r9
 80a8ba4:	fb0e 5501 	mla	r5, lr, r1, r5
 80a8ba8:	fba0 010e 	umull	r0, r1, r0, lr
 80a8bac:	4429      	add	r1, r5
 80a8bae:	e7f0      	b.n	80a8b92 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x42>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a8bb0:	687a      	ldr	r2, [r7, #4]
 80a8bb2:	2438      	movs	r4, #56	; 0x38
 80a8bb4:	6852      	ldr	r2, [r2, #4]
 80a8bb6:	fb04 3302 	mla	r3, r4, r2, r3
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a8bba:	6872      	ldr	r2, [r6, #4]

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a8bbc:	b103      	cbz	r3, 80a8bc0 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x70>
 80a8bbe:	685b      	ldr	r3, [r3, #4]
 80a8bc0:	3a01      	subs	r2, #1
 80a8bc2:	3b01      	subs	r3, #1
  const int64_t num_elements = NumElements(input);
  const T* in_data = GetTensorData<T>(input);
  T* out_data = GetTensorData<T>(output);
  for (int64_t i = 0; i < num_elements; ++i) {
 80a8bc4:	2400      	movs	r4, #0
 80a8bc6:	2500      	movs	r5, #0
 80a8bc8:	4284      	cmp	r4, r0
 80a8bca:	eb75 0601 	sbcs.w	r6, r5, r1
 80a8bce:	da09      	bge.n	80a8be4 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x94>
    out_data[i] = func(in_data[i]);
 80a8bd0:	f812 6f01 	ldrb.w	r6, [r2, #1]!
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
  const int64_t num_elements = NumElements(input);
  const T* in_data = GetTensorData<T>(input);
  T* out_data = GetTensorData<T>(output);
  for (int64_t i = 0; i < num_elements; ++i) {
 80a8bd4:	3401      	adds	r4, #1
    out_data[i] = func(in_data[i]);
 80a8bd6:	f086 0601 	eor.w	r6, r6, #1
 80a8bda:	f803 6f01 	strb.w	r6, [r3, #1]!
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
  const int64_t num_elements = NumElements(input);
  const T* in_data = GetTensorData<T>(input);
  T* out_data = GetTensorData<T>(output);
  for (int64_t i = 0; i < num_elements; ++i) {
 80a8bde:	f145 0500 	adc.w	r5, r5, #0
 80a8be2:	e7f1      	b.n	80a8bc8 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x78>
    out_data[i] = func(in_data[i]);
  }
  return kTfLiteOk;
 80a8be4:	2000      	movs	r0, #0
  return EvalNumeric(context, node, [](float f) { return f * f; });
}

TfLiteStatus LogicalNotEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalLogical(context, node, [](bool v) { return !v; });
}
 80a8be6:	b005      	add	sp, #20
 80a8be8:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}
 80a8bec:	080b60b4 	.word	0x080b60b4
 80a8bf0:	080b5c1b 	.word	0x080b5c1b
 80a8bf4:	080b5fdd 	.word	0x080b5fdd
 80a8bf8:	080b5be0 	.word	0x080b5be0

080a8bfc <_ZN6tflite3ops5micro12Register_ABSEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::AbsEval};
  return &r;
}
 80a8bfc:	4800      	ldr	r0, [pc, #0]	; (80a8c00 <_ZN6tflite3ops5micro12Register_ABSEv+0x4>)
 80a8bfe:	4770      	bx	lr
 80a8c00:	20000268 	.word	0x20000268

080a8c04 <_ZN6tflite3ops5micro12Register_SINEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::SinEval};
  return &r;
}
 80a8c04:	4800      	ldr	r0, [pc, #0]	; (80a8c08 <_ZN6tflite3ops5micro12Register_SINEv+0x4>)
 80a8c06:	4770      	bx	lr
 80a8c08:	20000248 	.word	0x20000248

080a8c0c <_ZN6tflite3ops5micro12Register_COSEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::CosEval};
  return &r;
}
 80a8c0c:	4800      	ldr	r0, [pc, #0]	; (80a8c10 <_ZN6tflite3ops5micro12Register_COSEv+0x4>)
 80a8c0e:	4770      	bx	lr
 80a8c10:	200001c8 	.word	0x200001c8

080a8c14 <_ZN6tflite3ops5micro12Register_LOGEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::LogEval};
  return &r;
}
 80a8c14:	4800      	ldr	r0, [pc, #0]	; (80a8c18 <_ZN6tflite3ops5micro12Register_LOGEv+0x4>)
 80a8c16:	4770      	bx	lr
 80a8c18:	20000188 	.word	0x20000188

080a8c1c <_ZN6tflite3ops5micro13Register_SQRTEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::SqrtEval};
  return &r;
}
 80a8c1c:	4800      	ldr	r0, [pc, #0]	; (80a8c20 <_ZN6tflite3ops5micro13Register_SQRTEv+0x4>)
 80a8c1e:	4770      	bx	lr
 80a8c20:	20000228 	.word	0x20000228

080a8c24 <_ZN6tflite3ops5micro14Register_RSQRTEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::RsqrtEval};
  return &r;
}
 80a8c24:	4800      	ldr	r0, [pc, #0]	; (80a8c28 <_ZN6tflite3ops5micro14Register_RSQRTEv+0x4>)
 80a8c26:	4770      	bx	lr
 80a8c28:	20000208 	.word	0x20000208

080a8c2c <_ZN6tflite3ops5micro15Register_SQUAREEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::SquareEval};
  return &r;
}
 80a8c2c:	4800      	ldr	r0, [pc, #0]	; (80a8c30 <_ZN6tflite3ops5micro15Register_SQUAREEv+0x4>)
 80a8c2e:	4770      	bx	lr
 80a8c30:	200001a8 	.word	0x200001a8

080a8c34 <_ZN6tflite3ops5micro20Register_LOGICAL_NOTEv>:
  static TfLiteRegistration r = {
      /*init=*/nullptr, /*free=*/nullptr,
      elementwise::GenericPrepare<elementwise::IsLogicalSupportedType>,
      elementwise::LogicalNotEval};
  return &r;
}
 80a8c34:	4800      	ldr	r0, [pc, #0]	; (80a8c38 <_ZN6tflite3ops5micro20Register_LOGICAL_NOTEv+0x4>)
 80a8c36:	4770      	bx	lr
 80a8c38:	200001e8 	.word	0x200001e8

080a8c3c <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode>:
namespace floor {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80a8c3c:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a8c40:	680b      	ldr	r3, [r1, #0]
 80a8c42:	2438      	movs	r4, #56	; 0x38
 80a8c44:	685b      	ldr	r3, [r3, #4]
 80a8c46:	6882      	ldr	r2, [r0, #8]
 80a8c48:	4363      	muls	r3, r4
 80a8c4a:	18d5      	adds	r5, r2, r3
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
 80a8c4c:	5cd3      	ldrb	r3, [r2, r3]
namespace floor {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80a8c4e:	b08e      	sub	sp, #56	; 0x38
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
 80a8c50:	2b01      	cmp	r3, #1
 80a8c52:	d00d      	beq.n	80a8c70 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x34>
 80a8c54:	9302      	str	r3, [sp, #8]
 80a8c56:	4b2b      	ldr	r3, [pc, #172]	; (80a8d04 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xc8>)
 80a8c58:	2401      	movs	r4, #1
 80a8c5a:	9301      	str	r3, [sp, #4]
 80a8c5c:	4b2a      	ldr	r3, [pc, #168]	; (80a8d08 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xcc>)
 80a8c5e:	9403      	str	r4, [sp, #12]
 80a8c60:	9300      	str	r3, [sp, #0]
 80a8c62:	6945      	ldr	r5, [r0, #20]
 80a8c64:	231f      	movs	r3, #31
 80a8c66:	4a29      	ldr	r2, [pc, #164]	; (80a8d0c <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xd0>)
 80a8c68:	4929      	ldr	r1, [pc, #164]	; (80a8d10 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xd4>)
 80a8c6a:	47a8      	blx	r5
 80a8c6c:	4620      	mov	r0, r4
 80a8c6e:	e045      	b.n	80a8cfc <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xc0>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a8c70:	684b      	ldr	r3, [r1, #4]
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  reference_ops::Floor(GetTensorShape(input), GetTensorData<float>(input),
 80a8c72:	a804      	add	r0, sp, #16
 80a8c74:	685b      	ldr	r3, [r3, #4]
 80a8c76:	4629      	mov	r1, r5
 80a8c78:	fb04 2403 	mla	r4, r4, r3, r2
 80a8c7c:	f7f9 fe49 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                       GetTensorShape(output), GetTensorData<float>(output));
 80a8c80:	4621      	mov	r1, r4
 80a8c82:	a809      	add	r0, sp, #36	; 0x24
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a8c84:	f8d5 8004 	ldr.w	r8, [r5, #4]
 80a8c88:	f7f9 fe43 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a8c8c:	b104      	cbz	r4, 80a8c90 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x54>
 80a8c8e:	6864      	ldr	r4, [r4, #4]
 80a8c90:	9e04      	ldr	r6, [sp, #16]

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
 80a8c92:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80a8c94:	429e      	cmp	r6, r3
 80a8c96:	d101      	bne.n	80a8c9c <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x60>
 80a8c98:	2500      	movs	r5, #0
 80a8c9a:	e00d      	b.n	80a8cb8 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x7c>
 80a8c9c:	f007 f9c8 	bl	80b0030 <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
 80a8ca0:	4629      	mov	r1, r5
 80a8ca2:	a804      	add	r0, sp, #16
 80a8ca4:	f7f9 fb90 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a8ca8:	4629      	mov	r1, r5
 80a8caa:	4607      	mov	r7, r0
 80a8cac:	a809      	add	r0, sp, #36	; 0x24
 80a8cae:	f7f9 fb8b 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a8cb2:	4287      	cmp	r7, r0
 80a8cb4:	d1f2      	bne.n	80a8c9c <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x60>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
 80a8cb6:	3501      	adds	r5, #1
 80a8cb8:	42ae      	cmp	r6, r5
 80a8cba:	dcf1      	bgt.n	80a8ca0 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x64>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80a8cbc:	2e04      	cmp	r6, #4
 80a8cbe:	bfcc      	ite	gt
 80a8cc0:	9a05      	ldrgt	r2, [sp, #20]
 80a8cc2:	aa05      	addle	r2, sp, #20
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a8cc4:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
 80a8cc6:	2701      	movs	r7, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a8cc8:	429e      	cmp	r6, r3
 80a8cca:	dc01      	bgt.n	80a8cd0 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x94>
 80a8ccc:	2500      	movs	r5, #0
 80a8cce:	e004      	b.n	80a8cda <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x9e>
      buffer_size *= dims_data[i];
 80a8cd0:	f852 1023 	ldr.w	r1, [r2, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a8cd4:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
 80a8cd6:	434f      	muls	r7, r1
 80a8cd8:	e7f6      	b.n	80a8cc8 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x8c>

inline void Floor(const RuntimeShape& input_shape, const float* input_data,
                  const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; i++) {
 80a8cda:	42bd      	cmp	r5, r7
 80a8cdc:	da07      	bge.n	80a8cee <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xb2>
  using ::floor;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  floor(float __x)
  { return __builtin_floorf(__x); }
 80a8cde:	f858 0025 	ldr.w	r0, [r8, r5, lsl #2]
 80a8ce2:	f008 f961 	bl	80b0fa8 <floorf>
    int offset = i;
    output_data[offset] = std::floor(input_data[offset]);
 80a8ce6:	f844 0025 	str.w	r0, [r4, r5, lsl #2]

inline void Floor(const RuntimeShape& input_shape, const float* input_data,
                  const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; i++) {
 80a8cea:	3501      	adds	r5, #1
 80a8cec:	e7f5      	b.n	80a8cda <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x9e>
 80a8cee:	a809      	add	r0, sp, #36	; 0x24
 80a8cf0:	f7f9 fb5f 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  reference_ops::Floor(GetTensorShape(input), GetTensorData<float>(input),
 80a8cf4:	a804      	add	r0, sp, #16
 80a8cf6:	f7f9 fb5c 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                       GetTensorShape(output), GetTensorData<float>(output));
  return kTfLiteOk;
 80a8cfa:	2000      	movs	r0, #0
}
 80a8cfc:	b00e      	add	sp, #56	; 0x38
 80a8cfe:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80a8d02:	bf00      	nop
 80a8d04:	080b644f 	.word	0x080b644f
 80a8d08:	080b5c1b 	.word	0x080b5c1b
 80a8d0c:	080b60c2 	.word	0x080b60c2
 80a8d10:	080b5be0 	.word	0x080b5be0

080a8d14 <_ZN6tflite3ops5micro14Register_FLOOREv>:
TfLiteRegistration* Register_FLOOR() {
  static TfLiteRegistration r = {/*init=*/nullptr,
                                 /*free=*/nullptr, /*prepare=*/nullptr,
                                 floor::Eval};
  return &r;
}
 80a8d14:	4800      	ldr	r0, [pc, #0]	; (80a8d18 <_ZN6tflite3ops5micro14Register_FLOOREv+0x4>)
 80a8d16:	4770      	bx	lr
 80a8d18:	20000288 	.word	0x20000288

080a8d1c <_ZN6tflite3ops5micro15fully_connected4InitEP13TfLiteContextPKcj>:

}  // namespace

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
 80a8d1c:	2000      	movs	r0, #0
 80a8d1e:	4770      	bx	lr

080a8d20 <_ZN6tflite3ops5micro15fully_connected4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
 80a8d20:	4770      	bx	lr

080a8d22 <_ZN6tflite3ops5micro15fully_connected7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
 80a8d22:	2000      	movs	r0, #0
 80a8d24:	4770      	bx	lr

080a8d26 <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi>:
}

// Data is required to be contiguous, and so many operators can use either the
// full array flat size or the flat size with one dimension skipped (commonly
// the depth).
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
 80a8d26:	b538      	push	{r3, r4, r5, lr}
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
 80a8d28:	2900      	cmp	r1, #0
 80a8d2a:	6804      	ldr	r4, [r0, #0]
 80a8d2c:	db01      	blt.n	80a8d32 <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi+0xc>
 80a8d2e:	42a1      	cmp	r1, r4
 80a8d30:	db01      	blt.n	80a8d36 <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi+0x10>
 80a8d32:	f007 f97d 	bl	80b0030 <abort>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80a8d36:	2c04      	cmp	r4, #4
 80a8d38:	bfcc      	ite	gt
 80a8d3a:	6843      	ldrgt	r3, [r0, #4]
 80a8d3c:	1d03      	addle	r3, r0, #4
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
  const auto* dims_data = shape.DimsData();
  int flat_size = 1;
  for (int i = 0; i < dims_count; ++i) {
 80a8d3e:	2200      	movs	r2, #0
// the depth).
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
  const auto* dims_data = shape.DimsData();
  int flat_size = 1;
 80a8d40:	2001      	movs	r0, #1
  for (int i = 0; i < dims_count; ++i) {
 80a8d42:	42a2      	cmp	r2, r4
 80a8d44:	da07      	bge.n	80a8d56 <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi+0x30>
    flat_size *= (i == skip_dim) ? 1 : dims_data[i];
 80a8d46:	428a      	cmp	r2, r1
 80a8d48:	bf14      	ite	ne
 80a8d4a:	f853 5022 	ldrne.w	r5, [r3, r2, lsl #2]
 80a8d4e:	2501      	moveq	r5, #1
 80a8d50:	4368      	muls	r0, r5
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
  const auto* dims_data = shape.DimsData();
  int flat_size = 1;
  for (int i = 0; i < dims_count; ++i) {
 80a8d52:	3201      	adds	r2, #1
 80a8d54:	e7f5      	b.n	80a8d42 <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi+0x1c>
    flat_size *= (i == skip_dim) ? 1 : dims_data[i];
  }
  return flat_size;
}
 80a8d56:	bd38      	pop	{r3, r4, r5, pc}

080a8d58 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph>:
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    uint8* output_data) {
 80a8d58:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a8d5c:	461d      	mov	r5, r3
  const int32 input_offset = params.input_offset;
 80a8d5e:	6803      	ldr	r3, [r0, #0]
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    uint8* output_data) {
 80a8d60:	b08b      	sub	sp, #44	; 0x2c
  const int32 input_offset = params.input_offset;
 80a8d62:	9302      	str	r3, [sp, #8]
  const int32 filter_offset = params.weights_offset;
 80a8d64:	6843      	ldr	r3, [r0, #4]
 80a8d66:	682e      	ldr	r6, [r5, #0]
 80a8d68:	9303      	str	r3, [sp, #12]
  const int32 output_offset = params.output_offset;
 80a8d6a:	6883      	ldr	r3, [r0, #8]
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;
  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);
 80a8d6c:	2e01      	cmp	r6, #1
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    uint8* output_data) {
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
 80a8d6e:	9304      	str	r3, [sp, #16]
  const int32 output_multiplier = params.output_multiplier;
 80a8d70:	68c3      	ldr	r3, [r0, #12]
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    uint8* output_data) {
 80a8d72:	4614      	mov	r4, r2
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
 80a8d74:	9305      	str	r3, [sp, #20]
  const int output_shift = params.output_shift;
 80a8d76:	6903      	ldr	r3, [r0, #16]
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    uint8* output_data) {
 80a8d78:	9f17      	ldr	r7, [sp, #92]	; 0x5c
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
 80a8d7a:	9306      	str	r3, [sp, #24]
  const int32 output_activation_min = params.quantized_activation_min;
 80a8d7c:	6943      	ldr	r3, [r0, #20]
  const int32 output_activation_max = params.quantized_activation_max;
 80a8d7e:	f8d0 b018 	ldr.w	fp, [r0, #24]
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
 80a8d82:	9301      	str	r3, [sp, #4]
  const int32 output_activation_max = params.quantized_activation_max;
  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);
 80a8d84:	dc01      	bgt.n	80a8d8a <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0x32>
 80a8d86:	f007 f953 	bl	80b0030 <abort>
 80a8d8a:	683b      	ldr	r3, [r7, #0]
  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);
 80a8d8c:	2b00      	cmp	r3, #0
 80a8d8e:	ddfa      	ble.n	80a8d86 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0x2e>

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
 80a8d90:	9a01      	ldr	r2, [sp, #4]
 80a8d92:	455a      	cmp	r2, fp
 80a8d94:	dcf7      	bgt.n	80a8d86 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0x2e>
  // but the current --variable_batch hack consists in overwriting the 3rd
  // dimension with the runtime batch size, as we don't keep track for each
  // array of which dimension is the batch dimension in it.
  const int output_dim_count = output_shape.DimensionsCount();
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
 80a8d96:	f103 38ff 	add.w	r8, r3, #4294967295	; 0xffffffff
 80a8d9a:	4641      	mov	r1, r8
 80a8d9c:	4638      	mov	r0, r7
 80a8d9e:	f7ff ffc2 	bl	80a8d26 <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi>
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
 80a8da2:	4643      	mov	r3, r8
 80a8da4:	463a      	mov	r2, r7
 80a8da6:	1eb1      	subs	r1, r6, #2
  // but the current --variable_batch hack consists in overwriting the 3rd
  // dimension with the runtime batch size, as we don't keep track for each
  // array of which dimension is the batch dimension in it.
  const int output_dim_count = output_shape.DimensionsCount();
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
 80a8da8:	9007      	str	r0, [sp, #28]
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
 80a8daa:	4628      	mov	r0, r5
 80a8dac:	f7fe fe67 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
 80a8db0:	1e71      	subs	r1, r6, #1
  // array of which dimension is the batch dimension in it.
  const int output_dim_count = output_shape.DimensionsCount();
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
 80a8db2:	4682      	mov	sl, r0
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
 80a8db4:	4628      	mov	r0, r5
 80a8db6:	f7f9 fb07 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a8dba:	4606      	mov	r6, r0
 80a8dbc:	f8dd 8060 	ldr.w	r8, [sp, #96]	; 0x60
 80a8dc0:	4267      	negs	r7, r4
  for (int b = 0; b < batches; ++b) {
 80a8dc2:	f04f 0900 	mov.w	r9, #0
 80a8dc6:	9b07      	ldr	r3, [sp, #28]
 80a8dc8:	4599      	cmp	r9, r3
 80a8dca:	da39      	bge.n	80a8e40 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0xe8>
 80a8dcc:	9b14      	ldr	r3, [sp, #80]	; 0x50
 80a8dce:	2500      	movs	r5, #0
    for (int out_c = 0; out_c < output_depth; ++out_c) {
 80a8dd0:	4555      	cmp	r5, sl
 80a8dd2:	da2f      	bge.n	80a8e34 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0xdc>
 80a8dd4:	469c      	mov	ip, r3
 80a8dd6:	46a6      	mov	lr, r4
      int32 acc = 0;
 80a8dd8:	2000      	movs	r0, #0
      for (int d = 0; d < accum_depth; ++d) {
 80a8dda:	eb0e 0207 	add.w	r2, lr, r7
 80a8dde:	4296      	cmp	r6, r2
 80a8de0:	dd0f      	ble.n	80a8e02 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0xaa>
        int32 input_val = input_data[b * accum_depth + d];
 80a8de2:	f81e 2b01 	ldrb.w	r2, [lr], #1
        int32 filter_val = filter_data[out_c * accum_depth + d];
        acc += (filter_val + filter_offset) * (input_val + input_offset);
 80a8de6:	9903      	ldr	r1, [sp, #12]
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
      int32 acc = 0;
      for (int d = 0; d < accum_depth; ++d) {
        int32 input_val = input_data[b * accum_depth + d];
 80a8de8:	9208      	str	r2, [sp, #32]
        int32 filter_val = filter_data[out_c * accum_depth + d];
 80a8dea:	f81c 2b01 	ldrb.w	r2, [ip], #1
        acc += (filter_val + filter_offset) * (input_val + input_offset);
 80a8dee:	440a      	add	r2, r1
 80a8df0:	9209      	str	r2, [sp, #36]	; 0x24
 80a8df2:	9902      	ldr	r1, [sp, #8]
 80a8df4:	9a08      	ldr	r2, [sp, #32]
 80a8df6:	440a      	add	r2, r1
 80a8df8:	4611      	mov	r1, r2
 80a8dfa:	9a09      	ldr	r2, [sp, #36]	; 0x24
 80a8dfc:	fb01 0002 	mla	r0, r1, r2, r0
                                       output_shape, output_dim_count - 1);
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
      int32 acc = 0;
      for (int d = 0; d < accum_depth; ++d) {
 80a8e00:	e7eb      	b.n	80a8dda <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0x82>
        int32 input_val = input_data[b * accum_depth + d];
        int32 filter_val = filter_data[out_c * accum_depth + d];
        acc += (filter_val + filter_offset) * (input_val + input_offset);
      }
      if (bias_data) {
 80a8e02:	9a16      	ldr	r2, [sp, #88]	; 0x58
 80a8e04:	b112      	cbz	r2, 80a8e0c <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0xb4>
        acc += bias_data[out_c];
 80a8e06:	f852 2025 	ldr.w	r2, [r2, r5, lsl #2]
 80a8e0a:	4410      	add	r0, r2
      }
      acc = MultiplyByQuantizedMultiplier(acc, output_multiplier, output_shift);
 80a8e0c:	9a06      	ldr	r2, [sp, #24]
 80a8e0e:	9905      	ldr	r1, [sp, #20]
 80a8e10:	9308      	str	r3, [sp, #32]
 80a8e12:	f7fe fe43 	bl	80a7a9c <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
      acc += output_offset;
 80a8e16:	9b04      	ldr	r3, [sp, #16]
 80a8e18:	4418      	add	r0, r3
 80a8e1a:	9b01      	ldr	r3, [sp, #4]
 80a8e1c:	4298      	cmp	r0, r3
 80a8e1e:	bfb8      	it	lt
 80a8e20:	4618      	movlt	r0, r3
      acc = std::max(acc, output_activation_min);
      acc = std::min(acc, output_activation_max);
      output_data[out_c + output_depth * b] = static_cast<uint8>(acc);
 80a8e22:	4558      	cmp	r0, fp
 80a8e24:	bfa8      	it	ge
 80a8e26:	4658      	movge	r0, fp
 80a8e28:	9b08      	ldr	r3, [sp, #32]
 80a8e2a:	f808 0005 	strb.w	r0, [r8, r5]
 80a8e2e:	4433      	add	r3, r6
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
 80a8e30:	3501      	adds	r5, #1
 80a8e32:	e7cd      	b.n	80a8dd0 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0x78>
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
 80a8e34:	f109 0901 	add.w	r9, r9, #1
 80a8e38:	44d0      	add	r8, sl
 80a8e3a:	4434      	add	r4, r6
 80a8e3c:	1bbf      	subs	r7, r7, r6
 80a8e3e:	e7c2      	b.n	80a8dc6 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0x6e>
      acc = std::max(acc, output_activation_min);
      acc = std::min(acc, output_activation_max);
      output_data[out_c + output_depth * b] = static_cast<uint8>(acc);
    }
  }
}
 80a8e40:	b00b      	add	sp, #44	; 0x2c
 80a8e42:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a8e46 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps>:
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int16* output_data) {
 80a8e46:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a8e4a:	461e      	mov	r6, r3
  const int32 input_offset = params.input_offset;
 80a8e4c:	6803      	ldr	r3, [r0, #0]
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int16* output_data) {
 80a8e4e:	b08b      	sub	sp, #44	; 0x2c
  const int32 input_offset = params.input_offset;
 80a8e50:	9302      	str	r3, [sp, #8]
  const int32 filter_offset = params.weights_offset;
 80a8e52:	6843      	ldr	r3, [r0, #4]
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;
 80a8e54:	f8d0 a018 	ldr.w	sl, [r0, #24]
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int16* output_data) {
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
 80a8e58:	9303      	str	r3, [sp, #12]
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
 80a8e5a:	68c3      	ldr	r3, [r0, #12]
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int16* output_data) {
 80a8e5c:	4614      	mov	r4, r2
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
 80a8e5e:	9304      	str	r3, [sp, #16]
  const int output_shift = params.output_shift;
 80a8e60:	6903      	ldr	r3, [r0, #16]
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int16* output_data) {
 80a8e62:	f8dd 805c 	ldr.w	r8, [sp, #92]	; 0x5c
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
 80a8e66:	9305      	str	r3, [sp, #20]
  const int32 output_activation_min = params.quantized_activation_min;
 80a8e68:	6943      	ldr	r3, [r0, #20]
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int16* output_data) {
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
 80a8e6a:	6885      	ldr	r5, [r0, #8]
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
 80a8e6c:	4553      	cmp	r3, sl
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
 80a8e6e:	9301      	str	r3, [sp, #4]
  const int32 output_activation_max = params.quantized_activation_max;

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
 80a8e70:	dd01      	ble.n	80a8e76 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0x30>
 80a8e72:	f007 f8dd 	bl	80b0030 <abort>
  TFLITE_DCHECK_EQ(output_offset, 0);
 80a8e76:	2d00      	cmp	r5, #0
 80a8e78:	d1fb      	bne.n	80a8e72 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0x2c>
  // but the current --variable_batch hack consists in overwriting the 3rd
  // dimension with the runtime batch size, as we don't keep track for each
  // array of which dimension is the batch dimension in it.
  const int output_dim_count = output_shape.DimensionsCount();
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
 80a8e7a:	f8d8 3000 	ldr.w	r3, [r8]
 80a8e7e:	4640      	mov	r0, r8
 80a8e80:	f103 39ff 	add.w	r9, r3, #4294967295	; 0xffffffff
 80a8e84:	4649      	mov	r1, r9
 80a8e86:	f7ff ff4e 	bl	80a8d26 <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi>
 80a8e8a:	6837      	ldr	r7, [r6, #0]
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
 80a8e8c:	464b      	mov	r3, r9
 80a8e8e:	4642      	mov	r2, r8
 80a8e90:	1eb9      	subs	r1, r7, #2
  // but the current --variable_batch hack consists in overwriting the 3rd
  // dimension with the runtime batch size, as we don't keep track for each
  // array of which dimension is the batch dimension in it.
  const int output_dim_count = output_shape.DimensionsCount();
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
 80a8e92:	9006      	str	r0, [sp, #24]
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
 80a8e94:	4630      	mov	r0, r6
 80a8e96:	f7fe fdf2 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
 80a8e9a:	1e79      	subs	r1, r7, #1
  // array of which dimension is the batch dimension in it.
  const int output_dim_count = output_shape.DimensionsCount();
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
 80a8e9c:	4683      	mov	fp, r0
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
 80a8e9e:	4630      	mov	r0, r6
 80a8ea0:	f7f9 fa92 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a8ea4:	4607      	mov	r7, r0
 80a8ea6:	ea4f 034b 	mov.w	r3, fp, lsl #1
 80a8eaa:	f8dd 9060 	ldr.w	r9, [sp, #96]	; 0x60
 80a8eae:	9308      	str	r3, [sp, #32]
 80a8eb0:	f1c4 0800 	rsb	r8, r4, #0
  for (int b = 0; b < batches; ++b) {
 80a8eb4:	9b06      	ldr	r3, [sp, #24]
 80a8eb6:	429d      	cmp	r5, r3
 80a8eb8:	da36      	bge.n	80a8f28 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0xe2>
 80a8eba:	9b14      	ldr	r3, [sp, #80]	; 0x50
 80a8ebc:	2600      	movs	r6, #0
    for (int out_c = 0; out_c < output_depth; ++out_c) {
 80a8ebe:	455e      	cmp	r6, fp
 80a8ec0:	da2b      	bge.n	80a8f1a <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0xd4>
      // Internal accumulation.
      // Initialize accumulator with the bias-value.
      int32 accum = bias_data[out_c];
 80a8ec2:	469c      	mov	ip, r3
 80a8ec4:	46a6      	mov	lr, r4
 80a8ec6:	9a16      	ldr	r2, [sp, #88]	; 0x58
 80a8ec8:	f852 0026 	ldr.w	r0, [r2, r6, lsl #2]
      // Accumulation loop.
      for (int d = 0; d < accum_depth; ++d) {
 80a8ecc:	eb08 020e 	add.w	r2, r8, lr
 80a8ed0:	4297      	cmp	r7, r2
 80a8ed2:	dd10      	ble.n	80a8ef6 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0xb0>
        int16 input_val = input_data[b * accum_depth + d] + input_offset;
 80a8ed4:	f81e 2b01 	ldrb.w	r2, [lr], #1
        int16 filter_val = filter_data[out_c * accum_depth + d] + filter_offset;
        accum += filter_val * input_val;
 80a8ed8:	9903      	ldr	r1, [sp, #12]
      // Internal accumulation.
      // Initialize accumulator with the bias-value.
      int32 accum = bias_data[out_c];
      // Accumulation loop.
      for (int d = 0; d < accum_depth; ++d) {
        int16 input_val = input_data[b * accum_depth + d] + input_offset;
 80a8eda:	9207      	str	r2, [sp, #28]
        int16 filter_val = filter_data[out_c * accum_depth + d] + filter_offset;
 80a8edc:	f81c 2b01 	ldrb.w	r2, [ip], #1
        accum += filter_val * input_val;
 80a8ee0:	440a      	add	r2, r1
 80a8ee2:	b212      	sxth	r2, r2
 80a8ee4:	9209      	str	r2, [sp, #36]	; 0x24
 80a8ee6:	9902      	ldr	r1, [sp, #8]
 80a8ee8:	9a07      	ldr	r2, [sp, #28]
 80a8eea:	440a      	add	r2, r1
 80a8eec:	b211      	sxth	r1, r2
 80a8eee:	9a09      	ldr	r2, [sp, #36]	; 0x24
 80a8ef0:	fb01 0002 	mla	r0, r1, r2, r0
    for (int out_c = 0; out_c < output_depth; ++out_c) {
      // Internal accumulation.
      // Initialize accumulator with the bias-value.
      int32 accum = bias_data[out_c];
      // Accumulation loop.
      for (int d = 0; d < accum_depth; ++d) {
 80a8ef4:	e7ea      	b.n	80a8ecc <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0x86>
      // Down-scale the final int32 accumulator to the scale used by our
      // (16-bit, typically 3 integer bits) fixed-point format. The quantized
      // multiplier and shift here have been pre-computed offline
      // (e.g. by toco).
      accum =
          MultiplyByQuantizedMultiplier(accum, output_multiplier, output_shift);
 80a8ef6:	9a05      	ldr	r2, [sp, #20]
 80a8ef8:	9904      	ldr	r1, [sp, #16]
 80a8efa:	9307      	str	r3, [sp, #28]
 80a8efc:	f7fe fdce 	bl	80a7a9c <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
 80a8f00:	9b01      	ldr	r3, [sp, #4]
 80a8f02:	4298      	cmp	r0, r3
 80a8f04:	bfb8      	it	lt
 80a8f06:	4618      	movlt	r0, r3
      // Saturate, cast to int16, and store to output array.
      accum = std::max(accum, output_activation_min - output_offset);
      accum = std::min(accum, output_activation_max - output_offset);
      accum += output_offset;
      output_data[out_c + output_depth * b] = accum;
 80a8f08:	4550      	cmp	r0, sl
 80a8f0a:	bfa8      	it	ge
 80a8f0c:	4650      	movge	r0, sl
 80a8f0e:	9b07      	ldr	r3, [sp, #28]
 80a8f10:	f829 0016 	strh.w	r0, [r9, r6, lsl #1]
 80a8f14:	443b      	add	r3, r7
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
 80a8f16:	3601      	adds	r6, #1
 80a8f18:	e7d1      	b.n	80a8ebe <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0x78>
 80a8f1a:	9b08      	ldr	r3, [sp, #32]
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
 80a8f1c:	3501      	adds	r5, #1
 80a8f1e:	4499      	add	r9, r3
 80a8f20:	443c      	add	r4, r7
 80a8f22:	ebc7 0808 	rsb	r8, r7, r8
 80a8f26:	e7c5      	b.n	80a8eb4 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0x6e>
      accum = std::min(accum, output_activation_max - output_offset);
      accum += output_offset;
      output_data[out_c + output_depth * b] = accum;
    }
  }
}
 80a8f28:	b00b      	add	sp, #44	; 0x2c
 80a8f2a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a8f2e <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa>:
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const int8_t* input_data, const RuntimeShape& filter_shape,
    const int8_t* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8_t* output_data) {
 80a8f2e:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a8f32:	461d      	mov	r5, r3
  const int32 input_offset = params.input_offset;
 80a8f34:	6803      	ldr	r3, [r0, #0]
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const int8_t* input_data, const RuntimeShape& filter_shape,
    const int8_t* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8_t* output_data) {
 80a8f36:	b08b      	sub	sp, #44	; 0x2c
  const int32 input_offset = params.input_offset;
 80a8f38:	9302      	str	r3, [sp, #8]
  const int32 filter_offset = params.weights_offset;
 80a8f3a:	6843      	ldr	r3, [r0, #4]
 80a8f3c:	682e      	ldr	r6, [r5, #0]
 80a8f3e:	9303      	str	r3, [sp, #12]
  const int32 output_offset = params.output_offset;
 80a8f40:	6883      	ldr	r3, [r0, #8]
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;
  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);
 80a8f42:	2e01      	cmp	r6, #1
    const int8_t* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8_t* output_data) {
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
 80a8f44:	9304      	str	r3, [sp, #16]
  const int32 output_multiplier = params.output_multiplier;
 80a8f46:	68c3      	ldr	r3, [r0, #12]
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const int8_t* input_data, const RuntimeShape& filter_shape,
    const int8_t* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8_t* output_data) {
 80a8f48:	4614      	mov	r4, r2
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
 80a8f4a:	9305      	str	r3, [sp, #20]
  const int output_shift = params.output_shift;
 80a8f4c:	6903      	ldr	r3, [r0, #16]
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const int8_t* input_data, const RuntimeShape& filter_shape,
    const int8_t* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8_t* output_data) {
 80a8f4e:	9f17      	ldr	r7, [sp, #92]	; 0x5c
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
 80a8f50:	9306      	str	r3, [sp, #24]
  const int32 output_activation_min = params.quantized_activation_min;
 80a8f52:	6943      	ldr	r3, [r0, #20]
  const int32 output_activation_max = params.quantized_activation_max;
 80a8f54:	f8d0 b018 	ldr.w	fp, [r0, #24]
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
 80a8f58:	9301      	str	r3, [sp, #4]
  const int32 output_activation_max = params.quantized_activation_max;
  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);
 80a8f5a:	dc01      	bgt.n	80a8f60 <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x32>
 80a8f5c:	f007 f868 	bl	80b0030 <abort>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 2);
 80a8f60:	683b      	ldr	r3, [r7, #0]
 80a8f62:	2b02      	cmp	r3, #2
 80a8f64:	d1fa      	bne.n	80a8f5c <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x2e>

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
 80a8f66:	9b01      	ldr	r3, [sp, #4]
 80a8f68:	455b      	cmp	r3, fp
 80a8f6a:	dcf7      	bgt.n	80a8f5c <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x2e>
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = output_shape.Dims(0);
 80a8f6c:	2100      	movs	r1, #0
 80a8f6e:	4638      	mov	r0, r7
 80a8f70:	f7f9 fa2a 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_depth = output_shape.Dims(1);
 80a8f74:	2101      	movs	r1, #1
  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 2);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = output_shape.Dims(0);
 80a8f76:	9007      	str	r0, [sp, #28]
  const int output_depth = output_shape.Dims(1);
 80a8f78:	4638      	mov	r0, r7
 80a8f7a:	f7f9 fa25 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  TFLITE_DCHECK_LE(output_depth, filter_shape.Dims(filter_dim_count - 2));
 80a8f7e:	1eb1      	subs	r1, r6, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 2);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = output_shape.Dims(0);
  const int output_depth = output_shape.Dims(1);
 80a8f80:	4681      	mov	r9, r0
  TFLITE_DCHECK_LE(output_depth, filter_shape.Dims(filter_dim_count - 2));
 80a8f82:	4628      	mov	r0, r5
 80a8f84:	f7f9 fa20 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a8f88:	4581      	cmp	r9, r0
 80a8f8a:	dce7      	bgt.n	80a8f5c <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x2e>
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
 80a8f8c:	1e71      	subs	r1, r6, #1
 80a8f8e:	4628      	mov	r0, r5
 80a8f90:	f7f9 fa1a 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a8f94:	4606      	mov	r6, r0
 80a8f96:	f8dd 8060 	ldr.w	r8, [sp, #96]	; 0x60
 80a8f9a:	4267      	negs	r7, r4
  for (int b = 0; b < batches; ++b) {
 80a8f9c:	f04f 0a00 	mov.w	sl, #0
 80a8fa0:	9b07      	ldr	r3, [sp, #28]
 80a8fa2:	459a      	cmp	sl, r3
 80a8fa4:	da39      	bge.n	80a901a <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0xec>
 80a8fa6:	9b14      	ldr	r3, [sp, #80]	; 0x50
 80a8fa8:	2500      	movs	r5, #0
    for (int out_c = 0; out_c < output_depth; ++out_c) {
 80a8faa:	454d      	cmp	r5, r9
 80a8fac:	da2f      	bge.n	80a900e <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0xe0>
 80a8fae:	469c      	mov	ip, r3
 80a8fb0:	46a6      	mov	lr, r4
      int32 acc = 0;
 80a8fb2:	2000      	movs	r0, #0
      for (int d = 0; d < accum_depth; ++d) {
 80a8fb4:	eb07 020e 	add.w	r2, r7, lr
 80a8fb8:	4296      	cmp	r6, r2
 80a8fba:	dd0f      	ble.n	80a8fdc <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0xae>
        int32 input_val = input_data[b * accum_depth + d];
 80a8fbc:	f91e 2b01 	ldrsb.w	r2, [lr], #1
        int32 filter_val = filter_data[out_c * accum_depth + d];
        acc += (filter_val + filter_offset) * (input_val + input_offset);
 80a8fc0:	9903      	ldr	r1, [sp, #12]
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
      int32 acc = 0;
      for (int d = 0; d < accum_depth; ++d) {
        int32 input_val = input_data[b * accum_depth + d];
 80a8fc2:	9208      	str	r2, [sp, #32]
        int32 filter_val = filter_data[out_c * accum_depth + d];
 80a8fc4:	f91c 2b01 	ldrsb.w	r2, [ip], #1
        acc += (filter_val + filter_offset) * (input_val + input_offset);
 80a8fc8:	440a      	add	r2, r1
 80a8fca:	9209      	str	r2, [sp, #36]	; 0x24
 80a8fcc:	9902      	ldr	r1, [sp, #8]
 80a8fce:	9a08      	ldr	r2, [sp, #32]
 80a8fd0:	440a      	add	r2, r1
 80a8fd2:	4611      	mov	r1, r2
 80a8fd4:	9a09      	ldr	r2, [sp, #36]	; 0x24
 80a8fd6:	fb01 0002 	mla	r0, r1, r2, r0
  TFLITE_DCHECK_LE(output_depth, filter_shape.Dims(filter_dim_count - 2));
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
      int32 acc = 0;
      for (int d = 0; d < accum_depth; ++d) {
 80a8fda:	e7eb      	b.n	80a8fb4 <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x86>
        int32 input_val = input_data[b * accum_depth + d];
        int32 filter_val = filter_data[out_c * accum_depth + d];
        acc += (filter_val + filter_offset) * (input_val + input_offset);
      }
      if (bias_data) {
 80a8fdc:	9a16      	ldr	r2, [sp, #88]	; 0x58
 80a8fde:	b112      	cbz	r2, 80a8fe6 <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0xb8>
        acc += bias_data[out_c];
 80a8fe0:	f852 2025 	ldr.w	r2, [r2, r5, lsl #2]
 80a8fe4:	4410      	add	r0, r2
      }
      acc = MultiplyByQuantizedMultiplier(acc, output_multiplier, output_shift);
 80a8fe6:	9a06      	ldr	r2, [sp, #24]
 80a8fe8:	9905      	ldr	r1, [sp, #20]
 80a8fea:	9308      	str	r3, [sp, #32]
 80a8fec:	f7fe fd56 	bl	80a7a9c <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
      acc += output_offset;
 80a8ff0:	9b04      	ldr	r3, [sp, #16]
 80a8ff2:	4418      	add	r0, r3
 80a8ff4:	9b01      	ldr	r3, [sp, #4]
 80a8ff6:	4298      	cmp	r0, r3
 80a8ff8:	bfb8      	it	lt
 80a8ffa:	4618      	movlt	r0, r3
      acc = std::max(acc, output_activation_min);
      acc = std::min(acc, output_activation_max);
      output_data[out_c + output_depth * b] = static_cast<int8_t>(acc);
 80a8ffc:	4558      	cmp	r0, fp
 80a8ffe:	bfa8      	it	ge
 80a9000:	4658      	movge	r0, fp
 80a9002:	9b08      	ldr	r3, [sp, #32]
 80a9004:	f808 0005 	strb.w	r0, [r8, r5]
 80a9008:	4433      	add	r3, r6
  const int batches = output_shape.Dims(0);
  const int output_depth = output_shape.Dims(1);
  TFLITE_DCHECK_LE(output_depth, filter_shape.Dims(filter_dim_count - 2));
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
 80a900a:	3501      	adds	r5, #1
 80a900c:	e7cd      	b.n	80a8faa <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x7c>
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = output_shape.Dims(0);
  const int output_depth = output_shape.Dims(1);
  TFLITE_DCHECK_LE(output_depth, filter_shape.Dims(filter_dim_count - 2));
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
 80a900e:	f10a 0a01 	add.w	sl, sl, #1
 80a9012:	44c8      	add	r8, r9
 80a9014:	4434      	add	r4, r6
 80a9016:	1bbf      	subs	r7, r7, r6
 80a9018:	e7c2      	b.n	80a8fa0 <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x72>
      acc = std::max(acc, output_activation_min);
      acc = std::min(acc, output_activation_max);
      output_data[out_c + output_depth * b] = static_cast<int8_t>(acc);
    }
  }
}
 80a901a:	b00b      	add	sp, #44	; 0x2c
 80a901c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a9020 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode>:
      GetTensorShape(bias), GetTensorData<float>(bias), GetTensorShape(output),
      GetTensorData<float>(output));
  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80a9020:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a9024:	680c      	ldr	r4, [r1, #0]
  auto* params =
      reinterpret_cast<TfLiteFullyConnectedParams*>(node->builtin_data);
 80a9026:	694b      	ldr	r3, [r1, #20]
      GetTensorShape(bias), GetTensorData<float>(bias), GetTensorShape(output),
      GetTensorData<float>(output));
  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80a9028:	b0b3      	sub	sp, #204	; 0xcc
 80a902a:	f8d0 9008 	ldr.w	r9, [r0, #8]
 80a902e:	4680      	mov	r8, r0
  auto* params =
      reinterpret_cast<TfLiteFullyConnectedParams*>(node->builtin_data);
 80a9030:	9306      	str	r3, [sp, #24]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a9032:	68a0      	ldr	r0, [r4, #8]
 80a9034:	6863      	ldr	r3, [r4, #4]
}

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
 80a9036:	68e4      	ldr	r4, [r4, #12]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a9038:	2238      	movs	r2, #56	; 0x38
 80a903a:	fb02 fa00 	mul.w	sl, r2, r0

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
 80a903e:	1c60      	adds	r0, r4, #1
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a9040:	fb02 f303 	mul.w	r3, r2, r3
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a9044:	bf18      	it	ne
 80a9046:	fb02 9404 	mlane	r4, r2, r4, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a904a:	684a      	ldr	r2, [r1, #4]
 80a904c:	f04f 0b38 	mov.w	fp, #56	; 0x38
 80a9050:	6852      	ldr	r2, [r2, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a9052:	eb09 0703 	add.w	r7, r9, r3
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a9056:	fb0b fb02 	mul.w	fp, fp, r2
                             TfLiteType data_type, const TfLiteTensor* input,
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output,
                             OpData* data) {
  TfLiteStatus status = kTfLiteOk;
  if (data_type != kTfLiteFloat32) {
 80a905a:	f819 3003 	ldrb.w	r3, [r9, r3]
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
  }
  return nullptr;
 80a905e:	bf08      	it	eq
 80a9060:	2400      	moveq	r4, #0
 80a9062:	2b01      	cmp	r3, #1
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a9064:	eb09 060a 	add.w	r6, r9, sl
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a9068:	eb09 050b 	add.w	r5, r9, fp
 80a906c:	d021      	beq.n	80a90b2 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x92>
    double real_multiplier = 0.0;
 80a906e:	ab32      	add	r3, sp, #200	; 0xc8
 80a9070:	2000      	movs	r0, #0
 80a9072:	2100      	movs	r1, #0
 80a9074:	e963 010a 	strd	r0, r1, [r3, #-40]!	; 0x28
    TF_LITE_ENSURE_STATUS(GetQuantizedConvolutionMultipler(
 80a9078:	4632      	mov	r2, r6
 80a907a:	9301      	str	r3, [sp, #4]
 80a907c:	9500      	str	r5, [sp, #0]
 80a907e:	4623      	mov	r3, r4
 80a9080:	4639      	mov	r1, r7
 80a9082:	4640      	mov	r0, r8
 80a9084:	f006 fbb4 	bl	80af7f0 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd>
 80a9088:	2800      	cmp	r0, #0
 80a908a:	d132      	bne.n	80a90f2 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0xd2>
        context, input, filter, bias, output, &real_multiplier));
    int exponent;
    QuantizeMultiplier(real_multiplier, &data->output_multiplier, &exponent);
 80a908c:	e9dd 0128 	ldrd	r0, r1, [sp, #160]	; 0xa0
 80a9090:	ab23      	add	r3, sp, #140	; 0x8c
 80a9092:	aa0f      	add	r2, sp, #60	; 0x3c
 80a9094:	f006 fd44 	bl	80afb20 <_ZN6tflite18QuantizeMultiplierEdPlPi>
    data->output_shift = -exponent;
 80a9098:	9b23      	ldr	r3, [sp, #140]	; 0x8c
    TF_LITE_ENSURE_STATUS(CalculateActivationRangeQuantized(
 80a909a:	462a      	mov	r2, r5
    double real_multiplier = 0.0;
    TF_LITE_ENSURE_STATUS(GetQuantizedConvolutionMultipler(
        context, input, filter, bias, output, &real_multiplier));
    int exponent;
    QuantizeMultiplier(real_multiplier, &data->output_multiplier, &exponent);
    data->output_shift = -exponent;
 80a909c:	425b      	negs	r3, r3
 80a909e:	9310      	str	r3, [sp, #64]	; 0x40
    TF_LITE_ENSURE_STATUS(CalculateActivationRangeQuantized(
 80a90a0:	9b06      	ldr	r3, [sp, #24]
 80a90a2:	4640      	mov	r0, r8
 80a90a4:	7819      	ldrb	r1, [r3, #0]
 80a90a6:	ab12      	add	r3, sp, #72	; 0x48
 80a90a8:	9300      	str	r3, [sp, #0]
 80a90aa:	ab11      	add	r3, sp, #68	; 0x44
 80a90ac:	f006 fbfe 	bl	80af8ac <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_>
 80a90b0:	b9f8      	cbnz	r0, 80a90f2 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0xd2>
  OpData local_data_object;
  OpData* data = &local_data_object;
  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, data_type, input,
                                        filter, bias, output, data));

  switch (filter->type) {  // Already know in/out types are same.
 80a90b2:	f819 200a 	ldrb.w	r2, [r9, sl]
 80a90b6:	2a03      	cmp	r2, #3
 80a90b8:	d176      	bne.n	80a91a8 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x188>
                           TfLiteFullyConnectedParams* params, OpData* data,
                           const TfLiteTensor* input,
                           const TfLiteTensor* filter, const TfLiteTensor* bias,
                           TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
 80a90ba:	6933      	ldr	r3, [r6, #16]
  const int32_t output_offset = output->params.zero_point;

  tflite::FullyConnectedParams op_params;
  op_params.input_offset = input_offset;
 80a90bc:	693a      	ldr	r2, [r7, #16]
                           TfLiteFullyConnectedParams* params, OpData* data,
                           const TfLiteTensor* input,
                           const TfLiteTensor* filter, const TfLiteTensor* bias,
                           TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
 80a90be:	425b      	negs	r3, r3
  const int32_t output_offset = output->params.zero_point;
 80a90c0:	6929      	ldr	r1, [r5, #16]

  tflite::FullyConnectedParams op_params;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
 80a90c2:	9329      	str	r3, [sp, #164]	; 0xa4
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
 80a90c4:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;

  tflite::FullyConnectedParams op_params;
  op_params.input_offset = input_offset;
 80a90c6:	4252      	negs	r2, r2
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
 80a90c8:	932b      	str	r3, [sp, #172]	; 0xac
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;
 80a90ca:	9b10      	ldr	r3, [sp, #64]	; 0x40
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;

  tflite::FullyConnectedParams op_params;
  op_params.input_offset = input_offset;
 80a90cc:	9228      	str	r2, [sp, #160]	; 0xa0
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;
 80a90ce:	425b      	negs	r3, r3
 80a90d0:	932c      	str	r3, [sp, #176]	; 0xb0
  op_params.quantized_activation_min = data->output_activation_min;
 80a90d2:	9b11      	ldr	r3, [sp, #68]	; 0x44
  const int32_t output_offset = output->params.zero_point;

  tflite::FullyConnectedParams op_params;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
 80a90d4:	912a      	str	r1, [sp, #168]	; 0xa8
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
 80a90d6:	932d      	str	r3, [sp, #180]	; 0xb4
  op_params.quantized_activation_max = data->output_activation_max;
 80a90d8:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80a90da:	932e      	str	r3, [sp, #184]	; 0xb8
  reference_ops::FullyConnected(                                       \
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input), \
      GetTensorShape(filter), GetTensorData<uint8_t>(filter),          \
      GetTensorShape(bias), GetTensorData<int32_t>(bias),              \
      GetTensorShape(output), GetTensorData<output_data_type>(output))
  switch (output->type) {
 80a90dc:	f819 300b 	ldrb.w	r3, [r9, fp]
 80a90e0:	2b03      	cmp	r3, #3
 80a90e2:	d008      	beq.n	80a90f6 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0xd6>
 80a90e4:	2b07      	cmp	r3, #7
 80a90e6:	d02c      	beq.n	80a9142 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x122>
      TF_LITE_FULLY_CONNECTED(int16_t);
      break;
    default:
      context->ReportError(
          context,
          "Quantized FullyConnected expects output data type uint8 or int16");
 80a90e8:	f8d8 3014 	ldr.w	r3, [r8, #20]
 80a90ec:	499e      	ldr	r1, [pc, #632]	; (80a9368 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x348>)
 80a90ee:	4640      	mov	r0, r8
 80a90f0:	4798      	blx	r3
                           output);

    default:
      context->ReportError(context, "Type %d not currently supported.",
                           filter->type);
      return kTfLiteError;
 80a90f2:	2001      	movs	r0, #1
 80a90f4:	e135      	b.n	80a9362 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x342>
      GetTensorShape(filter), GetTensorData<uint8_t>(filter),          \
      GetTensorShape(bias), GetTensorData<int32_t>(bias),              \
      GetTensorShape(output), GetTensorData<output_data_type>(output))
  switch (output->type) {
    case kTfLiteUInt8:
      TF_LITE_FULLY_CONNECTED(uint8_t);
 80a90f6:	4639      	mov	r1, r7
 80a90f8:	a814      	add	r0, sp, #80	; 0x50
 80a90fa:	f7f9 fc0a 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a90fe:	4631      	mov	r1, r6
 80a9100:	a819      	add	r0, sp, #100	; 0x64
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a9102:	f8d7 8004 	ldr.w	r8, [r7, #4]
 80a9106:	f7f9 fc04 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a910a:	f8d6 9004 	ldr.w	r9, [r6, #4]
 80a910e:	ae1e      	add	r6, sp, #120	; 0x78
 80a9110:	4621      	mov	r1, r4
 80a9112:	4630      	mov	r0, r6
 80a9114:	f7f9 fbfd 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9118:	b104      	cbz	r4, 80a911c <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0xfc>
 80a911a:	6864      	ldr	r4, [r4, #4]
 80a911c:	af23      	add	r7, sp, #140	; 0x8c
 80a911e:	4629      	mov	r1, r5
 80a9120:	4638      	mov	r0, r7
 80a9122:	f7f9 fbf6 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9126:	686b      	ldr	r3, [r5, #4]
 80a9128:	4642      	mov	r2, r8
 80a912a:	9304      	str	r3, [sp, #16]
 80a912c:	9703      	str	r7, [sp, #12]
 80a912e:	9402      	str	r4, [sp, #8]
 80a9130:	9601      	str	r6, [sp, #4]
 80a9132:	f8cd 9000 	str.w	r9, [sp]
 80a9136:	ab19      	add	r3, sp, #100	; 0x64
 80a9138:	a914      	add	r1, sp, #80	; 0x50
 80a913a:	a828      	add	r0, sp, #160	; 0xa0
 80a913c:	f7ff fe0c 	bl	80a8d58 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph>
 80a9140:	e024      	b.n	80a918c <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x16c>
      break;
    case kTfLiteInt16:
      TF_LITE_FULLY_CONNECTED(int16_t);
 80a9142:	4639      	mov	r1, r7
 80a9144:	a814      	add	r0, sp, #80	; 0x50
 80a9146:	f7f9 fbe4 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a914a:	4631      	mov	r1, r6
 80a914c:	a819      	add	r0, sp, #100	; 0x64
 80a914e:	f8d7 8004 	ldr.w	r8, [r7, #4]
 80a9152:	f7f9 fbde 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9156:	f8d6 9004 	ldr.w	r9, [r6, #4]
 80a915a:	ae1e      	add	r6, sp, #120	; 0x78
 80a915c:	4621      	mov	r1, r4
 80a915e:	4630      	mov	r0, r6
 80a9160:	f7f9 fbd7 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9164:	b104      	cbz	r4, 80a9168 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x148>
 80a9166:	6864      	ldr	r4, [r4, #4]
 80a9168:	af23      	add	r7, sp, #140	; 0x8c
 80a916a:	4629      	mov	r1, r5
 80a916c:	4638      	mov	r0, r7
 80a916e:	f7f9 fbd0 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9172:	686b      	ldr	r3, [r5, #4]
 80a9174:	4642      	mov	r2, r8
 80a9176:	9304      	str	r3, [sp, #16]
 80a9178:	9703      	str	r7, [sp, #12]
 80a917a:	9402      	str	r4, [sp, #8]
 80a917c:	9601      	str	r6, [sp, #4]
 80a917e:	f8cd 9000 	str.w	r9, [sp]
 80a9182:	ab19      	add	r3, sp, #100	; 0x64
 80a9184:	a914      	add	r1, sp, #80	; 0x50
 80a9186:	a828      	add	r0, sp, #160	; 0xa0
 80a9188:	f7ff fe5d 	bl	80a8e46 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps>
 80a918c:	4638      	mov	r0, r7
 80a918e:	f7f9 f910 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a9192:	4630      	mov	r0, r6
 80a9194:	f7f9 f90d 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a9198:	a819      	add	r0, sp, #100	; 0x64
 80a919a:	f7f9 f90a 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a919e:	a814      	add	r0, sp, #80	; 0x50
 80a91a0:	f7f9 f907 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
          context,
          "Quantized FullyConnected expects output data type uint8 or int16");
      return kTfLiteError;
  }

  return kTfLiteOk;
 80a91a4:	2000      	movs	r0, #0
 80a91a6:	e0dc      	b.n	80a9362 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x342>
  OpData local_data_object;
  OpData* data = &local_data_object;
  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, data_type, input,
                                        filter, bias, output, data));

  switch (filter->type) {  // Already know in/out types are same.
 80a91a8:	2a09      	cmp	r2, #9
 80a91aa:	d136      	bne.n	80a921a <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x1fa>
                               TfLiteFullyConnectedParams* params, OpData* data,
                               const TfLiteTensor* input,
                               const TfLiteTensor* filter,
                               const TfLiteTensor* bias, TfLiteTensor* output) {
  FullyConnectedParams op_params;
  op_params.input_offset = -input->params.zero_point;
 80a91ac:	693b      	ldr	r3, [r7, #16]
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  reference_integer_ops::FullyConnected(
      op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
 80a91ae:	4639      	mov	r1, r7
                               TfLiteFullyConnectedParams* params, OpData* data,
                               const TfLiteTensor* input,
                               const TfLiteTensor* filter,
                               const TfLiteTensor* bias, TfLiteTensor* output) {
  FullyConnectedParams op_params;
  op_params.input_offset = -input->params.zero_point;
 80a91b0:	425b      	negs	r3, r3
 80a91b2:	9328      	str	r3, [sp, #160]	; 0xa0
  op_params.weights_offset = -filter->params.zero_point;
 80a91b4:	6933      	ldr	r3, [r6, #16]
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  reference_integer_ops::FullyConnected(
      op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
 80a91b6:	a814      	add	r0, sp, #80	; 0x50
                               const TfLiteTensor* input,
                               const TfLiteTensor* filter,
                               const TfLiteTensor* bias, TfLiteTensor* output) {
  FullyConnectedParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.weights_offset = -filter->params.zero_point;
 80a91b8:	425b      	negs	r3, r3
 80a91ba:	9329      	str	r3, [sp, #164]	; 0xa4
  op_params.output_offset = output->params.zero_point;
 80a91bc:	692b      	ldr	r3, [r5, #16]
 80a91be:	932a      	str	r3, [sp, #168]	; 0xa8
  op_params.output_multiplier = data->output_multiplier;
 80a91c0:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80a91c2:	932b      	str	r3, [sp, #172]	; 0xac
  // TODO(b/138810107): Figure out whether output shift should be inverted
  op_params.output_shift = -data->output_shift;
 80a91c4:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80a91c6:	425b      	negs	r3, r3
 80a91c8:	932c      	str	r3, [sp, #176]	; 0xb0
  op_params.quantized_activation_min = data->output_activation_min;
 80a91ca:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80a91cc:	932d      	str	r3, [sp, #180]	; 0xb4
  op_params.quantized_activation_max = data->output_activation_max;
 80a91ce:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80a91d0:	932e      	str	r3, [sp, #184]	; 0xb8

  reference_integer_ops::FullyConnected(
      op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
 80a91d2:	f7f9 fb9e 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(filter), GetTensorData<int8_t>(filter),
 80a91d6:	4631      	mov	r1, r6
 80a91d8:	a819      	add	r0, sp, #100	; 0x64
 80a91da:	f8d7 8004 	ldr.w	r8, [r7, #4]
 80a91de:	f7f9 fb98 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a91e2:	f8d6 9004 	ldr.w	r9, [r6, #4]
      GetTensorShape(bias), GetTensorData<int32_t>(bias),
 80a91e6:	ae1e      	add	r6, sp, #120	; 0x78
 80a91e8:	4621      	mov	r1, r4
 80a91ea:	4630      	mov	r0, r6
 80a91ec:	f7f9 fb91 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a91f0:	b104      	cbz	r4, 80a91f4 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x1d4>
 80a91f2:	6864      	ldr	r4, [r4, #4]
      GetTensorShape(output), GetTensorData<int8_t>(output));
 80a91f4:	af23      	add	r7, sp, #140	; 0x8c
 80a91f6:	4629      	mov	r1, r5
 80a91f8:	4638      	mov	r0, r7
 80a91fa:	f7f9 fb8a 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a91fe:	686b      	ldr	r3, [r5, #4]
 80a9200:	4642      	mov	r2, r8
 80a9202:	9304      	str	r3, [sp, #16]
 80a9204:	9703      	str	r7, [sp, #12]
 80a9206:	9402      	str	r4, [sp, #8]
 80a9208:	9601      	str	r6, [sp, #4]
 80a920a:	f8cd 9000 	str.w	r9, [sp]
 80a920e:	ab19      	add	r3, sp, #100	; 0x64
 80a9210:	a914      	add	r1, sp, #80	; 0x50
 80a9212:	a828      	add	r0, sp, #160	; 0xa0
 80a9214:	f7ff fe8b 	bl	80a8f2e <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa>
 80a9218:	e7b8      	b.n	80a918c <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x16c>
  OpData local_data_object;
  OpData* data = &local_data_object;
  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, data_type, input,
                                        filter, bias, output, data));

  switch (filter->type) {  // Already know in/out types are same.
 80a921a:	2a01      	cmp	r2, #1
 80a921c:	f040 809b 	bne.w	80a9356 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x336>
 80a9220:	9b06      	ldr	r3, [sp, #24]
 80a9222:	781b      	ldrb	r3, [r3, #0]
// Calculates the useful range of an activation layer given its activation
// tensor.a
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
 80a9224:	2b01      	cmp	r3, #1
 80a9226:	d008      	beq.n	80a923a <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x21a>
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
 80a9228:	2b03      	cmp	r3, #3
 80a922a:	d009      	beq.n	80a9240 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x220>
    *activation_min = 0;
    *activation_max = 6;
  } else if (activation == kTfLiteActRelu1) {
 80a922c:	2b02      	cmp	r3, #2
 80a922e:	d00c      	beq.n	80a924a <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x22a>
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
    *activation_max = std::numeric_limits<T>::max();
 80a9230:	f8df 813c 	ldr.w	r8, [pc, #316]	; 80a9370 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x350>
    *activation_max = 6;
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
 80a9234:	f46f 0900 	mvn.w	r9, #8388608	; 0x800000
 80a9238:	e00b      	b.n	80a9252 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x232>
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
 80a923a:	f8df 8134 	ldr.w	r8, [pc, #308]	; 80a9370 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x350>
 80a923e:	e001      	b.n	80a9244 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x224>
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
    *activation_max = 6;
 80a9240:	f8df 8130 	ldr.w	r8, [pc, #304]	; 80a9374 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x354>
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
 80a9244:	f04f 0900 	mov.w	r9, #0
 80a9248:	e003      	b.n	80a9252 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x232>
    *activation_max = 6;
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
 80a924a:	f8df 912c 	ldr.w	r9, [pc, #300]	; 80a9378 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x358>
    *activation_max = 1;
 80a924e:	f04f 587e 	mov.w	r8, #1065353216	; 0x3f800000
                           &output_activation_max);
  tflite::FullyConnectedParams op_params;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  tflite::reference_ops::FullyConnected(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
 80a9252:	4639      	mov	r1, r7
 80a9254:	a819      	add	r0, sp, #100	; 0x64
 80a9256:	f7f9 fb5c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(filter), GetTensorData<float>(filter),
 80a925a:	4631      	mov	r1, r6
 80a925c:	a81e      	add	r0, sp, #120	; 0x78
 80a925e:	687f      	ldr	r7, [r7, #4]
 80a9260:	f7f9 fb57 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9264:	6873      	ldr	r3, [r6, #4]
      GetTensorShape(bias), GetTensorData<float>(bias), GetTensorShape(output),
 80a9266:	4621      	mov	r1, r4
 80a9268:	a823      	add	r0, sp, #140	; 0x8c
 80a926a:	930a      	str	r3, [sp, #40]	; 0x28
 80a926c:	f7f9 fb51 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9270:	b104      	cbz	r4, 80a9274 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x254>
 80a9272:	6864      	ldr	r4, [r4, #4]
 80a9274:	4629      	mov	r1, r5
 80a9276:	a828      	add	r0, sp, #160	; 0xa0
 80a9278:	f7f9 fb4b 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a927c:	b105      	cbz	r5, 80a9280 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x260>
 80a927e:	686d      	ldr	r5, [r5, #4]
  // but the current --variable_batch hack consists in overwriting the 3rd
  // dimension with the runtime batch size, as we don't keep track for each
  // array of which dimension is the batch dimension in it.
  const int output_dims_count = output_shape.DimensionsCount();
  const int weights_dims_count = weights_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);
 80a9280:	9b28      	ldr	r3, [sp, #160]	; 0xa0
 80a9282:	a828      	add	r0, sp, #160	; 0xa0
 80a9284:	f103 3aff 	add.w	sl, r3, #4294967295	; 0xffffffff
 80a9288:	4651      	mov	r1, sl
 80a928a:	f7ff fd4c 	bl	80a8d26 <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi>
 80a928e:	9e1e      	ldr	r6, [sp, #120]	; 0x78
  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,
                                       output_shape, output_dims_count - 1);
 80a9290:	4653      	mov	r3, sl
 80a9292:	1eb1      	subs	r1, r6, #2
  // but the current --variable_batch hack consists in overwriting the 3rd
  // dimension with the runtime batch size, as we don't keep track for each
  // array of which dimension is the batch dimension in it.
  const int output_dims_count = output_shape.DimensionsCount();
  const int weights_dims_count = weights_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);
 80a9294:	900b      	str	r0, [sp, #44]	; 0x2c
  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,
                                       output_shape, output_dims_count - 1);
 80a9296:	aa28      	add	r2, sp, #160	; 0xa0
 80a9298:	a81e      	add	r0, sp, #120	; 0x78
 80a929a:	f7fe fbf0 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);
 80a929e:	1e71      	subs	r1, r6, #1
  // array of which dimension is the batch dimension in it.
  const int output_dims_count = output_shape.DimensionsCount();
  const int weights_dims_count = weights_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);
  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,
                                       output_shape, output_dims_count - 1);
 80a92a0:	9006      	str	r0, [sp, #24]
  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);
 80a92a2:	a81e      	add	r0, sp, #120	; 0x78
 80a92a4:	f7f9 f890 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a92a8:	463e      	mov	r6, r7
 80a92aa:	9b06      	ldr	r3, [sp, #24]
 80a92ac:	9007      	str	r0, [sp, #28]
 80a92ae:	009b      	lsls	r3, r3, #2
 80a92b0:	9309      	str	r3, [sp, #36]	; 0x24
 80a92b2:	0083      	lsls	r3, r0, #2
 80a92b4:	9308      	str	r3, [sp, #32]
  for (int b = 0; b < batches; ++b) {
 80a92b6:	f04f 0a00 	mov.w	sl, #0
 80a92ba:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80a92bc:	4553      	cmp	r3, sl
 80a92be:	dd3f      	ble.n	80a9340 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x320>
 80a92c0:	f8dd b028 	ldr.w	fp, [sp, #40]	; 0x28
 80a92c4:	2700      	movs	r7, #0
    for (int out_c = 0; out_c < output_depth; ++out_c) {
 80a92c6:	9b06      	ldr	r3, [sp, #24]
 80a92c8:	42bb      	cmp	r3, r7
 80a92ca:	dd32      	ble.n	80a9332 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x312>
 80a92cc:	2300      	movs	r3, #0
 80a92ce:	2200      	movs	r2, #0
      float total = 0.f;
      for (int d = 0; d < accum_depth; ++d) {
 80a92d0:	9907      	ldr	r1, [sp, #28]
 80a92d2:	4299      	cmp	r1, r3
 80a92d4:	dd10      	ble.n	80a92f8 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x2d8>
        total += input_data[b * accum_depth + d] *
                 weights_data[out_c * accum_depth + d];
 80a92d6:	f85b 1023 	ldr.w	r1, [fp, r3, lsl #2]
 80a92da:	f856 0023 	ldr.w	r0, [r6, r3, lsl #2]
 80a92de:	930c      	str	r3, [sp, #48]	; 0x30
 80a92e0:	920d      	str	r2, [sp, #52]	; 0x34
 80a92e2:	f00a f983 	bl	80b35ec <__aeabi_fmul>
 80a92e6:	9a0d      	ldr	r2, [sp, #52]	; 0x34
 80a92e8:	4601      	mov	r1, r0
 80a92ea:	4610      	mov	r0, r2
 80a92ec:	f00a f876 	bl	80b33dc <__addsf3>
                                       output_shape, output_dims_count - 1);
  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
      float total = 0.f;
      for (int d = 0; d < accum_depth; ++d) {
 80a92f0:	9b0c      	ldr	r3, [sp, #48]	; 0x30
        total += input_data[b * accum_depth + d] *
                 weights_data[out_c * accum_depth + d];
 80a92f2:	4602      	mov	r2, r0
                                       output_shape, output_dims_count - 1);
  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
      float total = 0.f;
      for (int d = 0; d < accum_depth; ++d) {
 80a92f4:	3301      	adds	r3, #1
 80a92f6:	e7eb      	b.n	80a92d0 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x2b0>
        total += input_data[b * accum_depth + d] *
                 weights_data[out_c * accum_depth + d];
      }
      float bias_value = 0.0f;
      if (bias_data) {
 80a92f8:	b114      	cbz	r4, 80a9300 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x2e0>
        bias_value = bias_data[out_c];
 80a92fa:	f854 1027 	ldr.w	r1, [r4, r7, lsl #2]
 80a92fe:	e000      	b.n	80a9302 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x2e2>
      float total = 0.f;
      for (int d = 0; d < accum_depth; ++d) {
        total += input_data[b * accum_depth + d] *
                 weights_data[out_c * accum_depth + d];
      }
      float bias_value = 0.0f;
 80a9300:	2100      	movs	r1, #0
      if (bias_data) {
        bias_value = bias_data[out_c];
      }
      output_data[out_c + output_depth * b] = ActivationFunctionWithMinMax(
 80a9302:	4610      	mov	r0, r2
 80a9304:	f00a f86a 	bl	80b33dc <__addsf3>
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
 80a9308:	4649      	mov	r1, r9
 80a930a:	900c      	str	r0, [sp, #48]	; 0x30
 80a930c:	f00a fb0c 	bl	80b3928 <__aeabi_fcmplt>
 80a9310:	9b0c      	ldr	r3, [sp, #48]	; 0x30
 80a9312:	b100      	cbz	r0, 80a9316 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x2f6>
	return __b;
 80a9314:	464b      	mov	r3, r9
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
 80a9316:	4618      	mov	r0, r3
 80a9318:	4641      	mov	r1, r8
 80a931a:	930c      	str	r3, [sp, #48]	; 0x30
 80a931c:	f00a fb22 	bl	80b3964 <__aeabi_fcmpgt>
 80a9320:	9b0c      	ldr	r3, [sp, #48]	; 0x30
 80a9322:	b100      	cbz	r0, 80a9326 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x306>
	return __b;
 80a9324:	4643      	mov	r3, r8
          total + bias_value, output_activation_min, output_activation_max);
 80a9326:	f845 3027 	str.w	r3, [r5, r7, lsl #2]
 80a932a:	9b08      	ldr	r3, [sp, #32]
  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);
  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,
                                       output_shape, output_dims_count - 1);
  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
 80a932c:	3701      	adds	r7, #1
 80a932e:	449b      	add	fp, r3
 80a9330:	e7c9      	b.n	80a92c6 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x2a6>
 80a9332:	9b09      	ldr	r3, [sp, #36]	; 0x24
  const int weights_dims_count = weights_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);
  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,
                                       output_shape, output_dims_count - 1);
  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);
  for (int b = 0; b < batches; ++b) {
 80a9334:	f10a 0a01 	add.w	sl, sl, #1
 80a9338:	441d      	add	r5, r3
 80a933a:	9b08      	ldr	r3, [sp, #32]
 80a933c:	441e      	add	r6, r3
 80a933e:	e7bc      	b.n	80a92ba <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x29a>
 80a9340:	a828      	add	r0, sp, #160	; 0xa0
 80a9342:	f7f9 f836 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a9346:	a823      	add	r0, sp, #140	; 0x8c
 80a9348:	f7f9 f833 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  tflite::FullyConnectedParams op_params;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  tflite::reference_ops::FullyConnected(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
      GetTensorShape(filter), GetTensorData<float>(filter),
 80a934c:	a81e      	add	r0, sp, #120	; 0x78
 80a934e:	f7f9 f830 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                           &output_activation_max);
  tflite::FullyConnectedParams op_params;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  tflite::reference_ops::FullyConnected(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
 80a9352:	a819      	add	r0, sp, #100	; 0x64
 80a9354:	e724      	b.n	80a91a0 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x180>
      return EvalQuantized(context, node, params, data, input, filter, bias,
                           output);

    default:
      context->ReportError(context, "Type %d not currently supported.",
                           filter->type);
 80a9356:	f8d8 3014 	ldr.w	r3, [r8, #20]
 80a935a:	4904      	ldr	r1, [pc, #16]	; (80a936c <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x34c>)
 80a935c:	4640      	mov	r0, r8
 80a935e:	4798      	blx	r3
 80a9360:	e6c7      	b.n	80a90f2 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0xd2>
      return kTfLiteError;
  }
  return kTfLiteOk;
}
 80a9362:	b033      	add	sp, #204	; 0xcc
 80a9364:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80a9368:	080b6169 	.word	0x080b6169
 80a936c:	080b61aa 	.word	0x080b61aa
 80a9370:	7f7fffff 	.word	0x7f7fffff
 80a9374:	40c00000 	.word	0x40c00000
 80a9378:	bf800000 	.word	0xbf800000

080a937c <_ZN6tflite3ops5micro24Register_FULLY_CONNECTEDEv>:
TfLiteRegistration* Register_FULLY_CONNECTED() {
  static TfLiteRegistration r = {fully_connected::Init, fully_connected::Free,
                                 fully_connected::Prepare,
                                 fully_connected::Eval};
  return &r;
}
 80a937c:	4800      	ldr	r0, [pc, #0]	; (80a9380 <_ZN6tflite3ops5micro24Register_FULLY_CONNECTEDEv+0x4>)
 80a937e:	4770      	bx	lr
 80a9380:	200002a8 	.word	0x200002a8

080a9384 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_19LogicalOrEbb>:
  }

  return kTfLiteOk;
}

bool LogicalOr(bool x, bool y) { return x || y; }
 80a9384:	2800      	cmp	r0, #0
 80a9386:	bf0c      	ite	eq
 80a9388:	4608      	moveq	r0, r1
 80a938a:	2001      	movne	r0, #1
 80a938c:	4770      	bx	lr

080a938e <_ZN6tflite3ops5micro7logical12_GLOBAL__N_110LogicalAndEbb>:

TfLiteStatus LogicalOrEval(TfLiteContext* context, TfLiteNode* node) {
  return LogicalImpl(context, node, LogicalOr);
}

bool LogicalAnd(bool x, bool y) { return x && y; }
 80a938e:	2800      	cmp	r0, #0
 80a9390:	bf14      	ite	ne
 80a9392:	4608      	movne	r0, r1
 80a9394:	2000      	moveq	r0, #0
 80a9396:	4770      	bx	lr

080a9398 <_ZN6tflite3ops5micro19Register_LOGICAL_OREv>:
  // Init, Free, Prepare, Eval are satisfying the Interface required by
  // TfLiteRegistration.
  static TfLiteRegistration r = {/* init */ nullptr, /* free */ nullptr,
                                 /* prepare */ nullptr, logical::LogicalOrEval};
  return &r;
}
 80a9398:	4800      	ldr	r0, [pc, #0]	; (80a939c <_ZN6tflite3ops5micro19Register_LOGICAL_OREv+0x4>)
 80a939a:	4770      	bx	lr
 80a939c:	200002c8 	.word	0x200002c8

080a93a0 <_ZN6tflite3ops5micro20Register_LOGICAL_ANDEv>:
  // TfLiteRegistration.
  static TfLiteRegistration r = {/* init */ nullptr, /* free */ nullptr,
                                 /* prepare */ nullptr,
                                 logical::LogicalAndEval};
  return &r;
}
 80a93a0:	4800      	ldr	r0, [pc, #0]	; (80a93a4 <_ZN6tflite3ops5micro20Register_LOGICAL_ANDEv+0x4>)
 80a93a2:	4770      	bx	lr
 80a93a4:	200002e8 	.word	0x200002e8

080a93a8 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E>:
}

// R: Result type. T1: Input 1 type. T2: Input 2 type.
// TODO(renjieliu): Refactor other binary functions to use this one.
template <typename R, typename T1, typename T2>
inline void BinaryFunction(const RuntimeShape& input1_shape,
 80a93a8:	e92d 4ff8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a93ac:	4699      	mov	r9, r3
 80a93ae:	6807      	ldr	r7, [r0, #0]
}

inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0,
                            const RuntimeShape& check_shape_1) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
 80a93b0:	6813      	ldr	r3, [r2, #0]
 80a93b2:	4604      	mov	r4, r0
 80a93b4:	429f      	cmp	r7, r3
 80a93b6:	4688      	mov	r8, r1
 80a93b8:	4616      	mov	r6, r2
 80a93ba:	9d0a      	ldr	r5, [sp, #40]	; 0x28
 80a93bc:	d102      	bne.n	80a93c4 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1c>
 80a93be:	f04f 0a00 	mov.w	sl, #0
 80a93c2:	e00e      	b.n	80a93e2 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x3a>
 80a93c4:	f006 fe34 	bl	80b0030 <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
 80a93c8:	4651      	mov	r1, sl
 80a93ca:	4620      	mov	r0, r4
 80a93cc:	f7f8 fffc 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a93d0:	4651      	mov	r1, sl
 80a93d2:	4683      	mov	fp, r0
 80a93d4:	4630      	mov	r0, r6
 80a93d6:	f7f8 fff7 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a93da:	4583      	cmp	fp, r0
 80a93dc:	d1f2      	bne.n	80a93c4 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1c>
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0,
                            const RuntimeShape& check_shape_1) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
 80a93de:	f10a 0a01 	add.w	sl, sl, #1
 80a93e2:	4557      	cmp	r7, sl
 80a93e4:	dcf0      	bgt.n	80a93c8 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x20>

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
 80a93e6:	682b      	ldr	r3, [r5, #0]
 80a93e8:	429f      	cmp	r7, r3
 80a93ea:	d1eb      	bne.n	80a93c4 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1c>
 80a93ec:	f04f 0a00 	mov.w	sl, #0
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
 80a93f0:	4557      	cmp	r7, sl
 80a93f2:	dd0d      	ble.n	80a9410 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x68>
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
 80a93f4:	4651      	mov	r1, sl
 80a93f6:	4620      	mov	r0, r4
 80a93f8:	f7f8 ffe6 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a93fc:	4651      	mov	r1, sl
 80a93fe:	4606      	mov	r6, r0
 80a9400:	4628      	mov	r0, r5
 80a9402:	f7f8 ffe1 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a9406:	4286      	cmp	r6, r0
 80a9408:	d1dc      	bne.n	80a93c4 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1c>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
 80a940a:	f10a 0a01 	add.w	sl, sl, #1
 80a940e:	e7ef      	b.n	80a93f0 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x48>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80a9410:	2f04      	cmp	r7, #4
 80a9412:	bfcc      	ite	gt
 80a9414:	6864      	ldrgt	r4, [r4, #4]
 80a9416:	3404      	addle	r4, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a9418:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
 80a941a:	f04f 0a01 	mov.w	sl, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a941e:	429f      	cmp	r7, r3
 80a9420:	dc01      	bgt.n	80a9426 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x7e>
 80a9422:	2400      	movs	r4, #0
 80a9424:	e005      	b.n	80a9432 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x8a>
      buffer_size *= dims_data[i];
 80a9426:	f854 2023 	ldr.w	r2, [r4, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a942a:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
 80a942c:	fb02 fa0a 	mul.w	sl, r2, sl
 80a9430:	e7f5      	b.n	80a941e <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x76>
                           const T2* input2_data,
                           const RuntimeShape& output_shape, R* output_data,
                           R (*func)(T1, T2)) {
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int i = 0; i < flat_size; ++i) {
 80a9432:	4554      	cmp	r4, sl
 80a9434:	da09      	bge.n	80a944a <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0xa2>
    output_data[i] = func(input1_data[i], input2_data[i]);
 80a9436:	f819 1004 	ldrb.w	r1, [r9, r4]
 80a943a:	f818 0004 	ldrb.w	r0, [r8, r4]
 80a943e:	9b0c      	ldr	r3, [sp, #48]	; 0x30
 80a9440:	4798      	blx	r3
 80a9442:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80a9444:	5518      	strb	r0, [r3, r4]
                           const T2* input2_data,
                           const RuntimeShape& output_shape, R* output_data,
                           R (*func)(T1, T2)) {
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int i = 0; i < flat_size; ++i) {
 80a9446:	3401      	adds	r4, #1
 80a9448:	e7f3      	b.n	80a9432 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x8a>
 80a944a:	e8bd 8ff8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a944e <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E>:
//
// Also appears to duplicte MinimumMaximum.
//
// R: Result type. T1: Input 1 type. T2: Input 2 type.
template <typename R, typename T1, typename T2>
inline void BroadcastBinaryFunction4DSlow(
 80a944e:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a9452:	469a      	mov	sl, r3
    const RuntimeShape& unextended_input1_shape, const T1* input1_data,
    const RuntimeShape& unextended_input2_shape, const T2* input2_data,
    const RuntimeShape& unextended_output_shape, R* output_data,
    R (*func)(T1, T2)) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a9454:	6803      	ldr	r3, [r0, #0]
//
// Also appears to duplicte MinimumMaximum.
//
// R: Result type. T1: Input 1 type. T2: Input 2 type.
template <typename R, typename T1, typename T2>
inline void BroadcastBinaryFunction4DSlow(
 80a9456:	b0a5      	sub	sp, #148	; 0x94
    const RuntimeShape& unextended_input1_shape, const T1* input1_data,
    const RuntimeShape& unextended_input2_shape, const T2* input2_data,
    const RuntimeShape& unextended_output_shape, R* output_data,
    R (*func)(T1, T2)) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a9458:	2b04      	cmp	r3, #4
//
// Also appears to duplicte MinimumMaximum.
//
// R: Result type. T1: Input 1 type. T2: Input 2 type.
template <typename R, typename T1, typename T2>
inline void BroadcastBinaryFunction4DSlow(
 80a945a:	4614      	mov	r4, r2
 80a945c:	4605      	mov	r5, r0
 80a945e:	9103      	str	r1, [sp, #12]
 80a9460:	9a2e      	ldr	r2, [sp, #184]	; 0xb8
    const RuntimeShape& unextended_input1_shape, const T1* input1_data,
    const RuntimeShape& unextended_input2_shape, const T2* input2_data,
    const RuntimeShape& unextended_output_shape, R* output_data,
    R (*func)(T1, T2)) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a9462:	dd01      	ble.n	80a9468 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1a>
 80a9464:	f006 fde4 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a9468:	6823      	ldr	r3, [r4, #0]
 80a946a:	2b04      	cmp	r3, #4
 80a946c:	dcfa      	bgt.n	80a9464 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a946e:	6813      	ldr	r3, [r2, #0]
 80a9470:	2b04      	cmp	r3, #4
 80a9472:	dcf7      	bgt.n	80a9464 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
 80a9474:	2301      	movs	r3, #1
 80a9476:	2104      	movs	r1, #4
 80a9478:	a805      	add	r0, sp, #20
 80a947a:	f7f8 ffde 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a947e:	462a      	mov	r2, r5
 80a9480:	2301      	movs	r3, #1
 80a9482:	2104      	movs	r1, #4
 80a9484:	a80a      	add	r0, sp, #40	; 0x28
 80a9486:	f7f8 ffd8 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a948a:	f10d 0970 	add.w	r9, sp, #112	; 0x70
 80a948e:	4622      	mov	r2, r4
 80a9490:	2301      	movs	r3, #1
 80a9492:	2104      	movs	r1, #4
 80a9494:	a80f      	add	r0, sp, #60	; 0x3c
  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
 80a9496:	f04f 0b01 	mov.w	fp, #1
 80a949a:	f7f8 ffce 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>

  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
 80a949e:	465e      	mov	r6, fp
 80a94a0:	464f      	mov	r7, r9
 80a94a2:	f10d 0890 	add.w	r8, sp, #144	; 0x90
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
 80a94a6:	2403      	movs	r4, #3
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
 80a94a8:	ad14      	add	r5, sp, #80	; 0x50
 80a94aa:	4621      	mov	r1, r4
 80a94ac:	a80a      	add	r0, sp, #40	; 0x28
 80a94ae:	f7f8 ff8b 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
 80a94b2:	4621      	mov	r1, r4

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
 80a94b4:	f845 0024 	str.w	r0, [r5, r4, lsl #2]
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
 80a94b8:	a80a      	add	r0, sp, #40	; 0x28
  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
 80a94ba:	f849 6d04 	str.w	r6, [r9, #-4]!
    desc0_stride *= extended_input0_shape.Dims(i);
 80a94be:	f7f8 ff83 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
 80a94c2:	4621      	mov	r1, r4
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
 80a94c4:	4346      	muls	r6, r0
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
 80a94c6:	a80f      	add	r0, sp, #60	; 0x3c
 80a94c8:	f7f8 ff7e 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a94cc:	ab1c      	add	r3, sp, #112	; 0x70
 80a94ce:	f843 0024 	str.w	r0, [r3, r4, lsl #2]
    desc1_out->strides[i] = desc1_stride;
    desc1_stride *= extended_input1_shape.Dims(i);
 80a94d2:	4621      	mov	r1, r4
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
    desc1_out->strides[i] = desc1_stride;
 80a94d4:	f848 bd04 	str.w	fp, [r8, #-4]!
    desc1_stride *= extended_input1_shape.Dims(i);
 80a94d8:	a80f      	add	r0, sp, #60	; 0x3c
 80a94da:	f7f8 ff75 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
 80a94de:	f114 34ff 	adds.w	r4, r4, #4294967295	; 0xffffffff
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
    desc1_out->strides[i] = desc1_stride;
    desc1_stride *= extended_input1_shape.Dims(i);
 80a94e2:	fb00 fb0b 	mul.w	fp, r0, fp
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
 80a94e6:	d2e0      	bcs.n	80a94aa <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x5c>
 80a94e8:	2400      	movs	r4, #0
      if (extent0 == 1) {
        desc0_out->strides[i] = 0;
        desc0_out->extents[i] = extent1;
      } else {
        TFLITE_DCHECK_EQ(extent1, 1);
        desc1_out->strides[i] = 0;
 80a94ea:	46a0      	mov	r8, r4

  // Walk over each dimension. If the extents are equal do nothing.
  // Otherwise, set the desc with extent 1 to have extent equal to the other and
  // stride 0.
  for (int i = 0; i < N; ++i) {
    const int extent0 = extended_input0_shape.Dims(i);
 80a94ec:	4621      	mov	r1, r4
 80a94ee:	a80a      	add	r0, sp, #40	; 0x28
 80a94f0:	f7f8 ff6a 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
    const int extent1 = extended_input1_shape.Dims(i);
 80a94f4:	4621      	mov	r1, r4

  // Walk over each dimension. If the extents are equal do nothing.
  // Otherwise, set the desc with extent 1 to have extent equal to the other and
  // stride 0.
  for (int i = 0; i < N; ++i) {
    const int extent0 = extended_input0_shape.Dims(i);
 80a94f6:	4606      	mov	r6, r0
    const int extent1 = extended_input1_shape.Dims(i);
 80a94f8:	a80f      	add	r0, sp, #60	; 0x3c
 80a94fa:	f7f8 ff65 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
    if (extent0 != extent1) {
 80a94fe:	4286      	cmp	r6, r0
 80a9500:	d010      	beq.n	80a9524 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0xd6>
      if (extent0 == 1) {
 80a9502:	2e01      	cmp	r6, #1
 80a9504:	ea4f 0384 	mov.w	r3, r4, lsl #2
 80a9508:	d105      	bne.n	80a9516 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0xc8>
        desc0_out->strides[i] = 0;
 80a950a:	442b      	add	r3, r5
 80a950c:	f8c3 8010 	str.w	r8, [r3, #16]
        desc0_out->extents[i] = extent1;
 80a9510:	f845 0024 	str.w	r0, [r5, r4, lsl #2]
 80a9514:	e006      	b.n	80a9524 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0xd6>
      } else {
        TFLITE_DCHECK_EQ(extent1, 1);
 80a9516:	2801      	cmp	r0, #1
 80a9518:	d1a4      	bne.n	80a9464 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
        desc1_out->strides[i] = 0;
 80a951a:	443b      	add	r3, r7
 80a951c:	f8c3 8010 	str.w	r8, [r3, #16]
        desc1_out->extents[i] = extent0;
 80a9520:	f847 6024 	str.w	r6, [r7, r4, lsl #2]
  }

  // Walk over each dimension. If the extents are equal do nothing.
  // Otherwise, set the desc with extent 1 to have extent equal to the other and
  // stride 0.
  for (int i = 0; i < N; ++i) {
 80a9524:	3401      	adds	r4, #1
 80a9526:	2c04      	cmp	r4, #4
 80a9528:	d1e0      	bne.n	80a94ec <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x9e>
    NdArrayDesc<N>* desc0_out, NdArrayDesc<N>* desc1_out) {
  TFLITE_DCHECK(desc0_out != nullptr);
  TFLITE_DCHECK(desc1_out != nullptr);

  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);
 80a952a:	a80f      	add	r0, sp, #60	; 0x3c
 80a952c:	f7f8 ff41 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
    const RuntimeShape& input0_shape, const RuntimeShape& input1_shape,
    NdArrayDesc<N>* desc0_out, NdArrayDesc<N>* desc1_out) {
  TFLITE_DCHECK(desc0_out != nullptr);
  TFLITE_DCHECK(desc1_out != nullptr);

  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
 80a9530:	a80a      	add	r0, sp, #40	; 0x28
 80a9532:	f7f8 ff3e 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a9536:	2400      	movs	r4, #0
 80a9538:	2100      	movs	r1, #0
 80a953a:	a805      	add	r0, sp, #20
 80a953c:	f7f8 ff44 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a9540:	4284      	cmp	r4, r0
 80a9542:	da5d      	bge.n	80a9600 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1b2>
 80a9544:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a9546:	f10d 0914 	add.w	r9, sp, #20
 80a954a:	2101      	movs	r1, #1
 80a954c:	4648      	mov	r0, r9
 80a954e:	f7f8 ff3b 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a9552:	4285      	cmp	r5, r0
 80a9554:	da52      	bge.n	80a95fc <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1ae>
 80a9556:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a9558:	2102      	movs	r1, #2
 80a955a:	4648      	mov	r0, r9
 80a955c:	f7f8 ff34 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a9560:	4286      	cmp	r6, r0
 80a9562:	da49      	bge.n	80a95f8 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1aa>
 80a9564:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a9566:	2103      	movs	r1, #3
 80a9568:	4648      	mov	r0, r9
 80a956a:	f7f8 ff2d 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a956e:	4287      	cmp	r7, r0
 80a9570:	da40      	bge.n	80a95f4 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1a6>
  }
  return offset;
}

inline int Offset(const RuntimeShape& shape, int i0, int i1, int i2, int i3) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), 4);
 80a9572:	9b05      	ldr	r3, [sp, #20]
 80a9574:	2b04      	cmp	r3, #4
 80a9576:	f47f af75 	bne.w	80a9464 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  const int* dims_data = reinterpret_cast<const int*>(shape.DimsDataUpTo4D());
  TFLITE_DCHECK(i0 >= 0 && i0 < dims_data[0]);
 80a957a:	2c00      	cmp	r4, #0
 80a957c:	f6ff af72 	blt.w	80a9464 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
 80a9580:	9b06      	ldr	r3, [sp, #24]
 80a9582:	429c      	cmp	r4, r3
 80a9584:	f6bf af6e 	bge.w	80a9464 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  TFLITE_DCHECK(i1 >= 0 && i1 < dims_data[1]);
 80a9588:	2d00      	cmp	r5, #0
 80a958a:	f6ff af6b 	blt.w	80a9464 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
 80a958e:	9b07      	ldr	r3, [sp, #28]
 80a9590:	429d      	cmp	r5, r3
 80a9592:	f6bf af67 	bge.w	80a9464 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  TFLITE_DCHECK(i2 >= 0 && i2 < dims_data[2]);
 80a9596:	2e00      	cmp	r6, #0
 80a9598:	f6ff af64 	blt.w	80a9464 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
 80a959c:	9908      	ldr	r1, [sp, #32]
 80a959e:	428e      	cmp	r6, r1
 80a95a0:	f6bf af60 	bge.w	80a9464 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  TFLITE_DCHECK(i3 >= 0 && i3 < dims_data[3]);
 80a95a4:	2f00      	cmp	r7, #0
 80a95a6:	f6ff af5d 	blt.w	80a9464 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
 80a95aa:	9a09      	ldr	r2, [sp, #36]	; 0x24
 80a95ac:	4297      	cmp	r7, r2
 80a95ae:	f6bf af59 	bge.w	80a9464 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  return ((i0 * dims_data[1] + i1) * dims_data[2] + i2) * dims_data[3] + i3;
 80a95b2:	fb03 5304 	mla	r3, r3, r4, r5
 80a95b6:	fb01 6303 	mla	r3, r1, r3, r6
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a95ba:	9700      	str	r7, [sp, #0]
 80a95bc:	fb02 7803 	mla	r8, r2, r3, r7
 80a95c0:	4621      	mov	r1, r4
 80a95c2:	4633      	mov	r3, r6
 80a95c4:	462a      	mov	r2, r5
 80a95c6:	a814      	add	r0, sp, #80	; 0x50
 80a95c8:	f7f9 f814 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
 80a95cc:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a95ce:	4683      	mov	fp, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
 80a95d0:	4633      	mov	r3, r6
 80a95d2:	462a      	mov	r2, r5
 80a95d4:	4621      	mov	r1, r4
 80a95d6:	a81c      	add	r0, sp, #112	; 0x70
 80a95d8:	f7f9 f80c 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = func(in1_val, in2_val);
 80a95dc:	9b03      	ldr	r3, [sp, #12]
 80a95de:	f81a 1000 	ldrb.w	r1, [sl, r0]
 80a95e2:	f813 000b 	ldrb.w	r0, [r3, fp]
 80a95e6:	9b30      	ldr	r3, [sp, #192]	; 0xc0
 80a95e8:	4798      	blx	r3
 80a95ea:	9b2f      	ldr	r3, [sp, #188]	; 0xbc
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a95ec:	3701      	adds	r7, #1
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = func(in1_val, in2_val);
 80a95ee:	f803 0008 	strb.w	r0, [r3, r8]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a95f2:	e7b8      	b.n	80a9566 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x118>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a95f4:	3601      	adds	r6, #1
 80a95f6:	e7af      	b.n	80a9558 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x10a>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a95f8:	3501      	adds	r5, #1
 80a95fa:	e7a4      	b.n	80a9546 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0xf8>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a95fc:	3401      	adds	r4, #1
 80a95fe:	e79b      	b.n	80a9538 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0xea>
    R (*func)(T1, T2)) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a9600:	a805      	add	r0, sp, #20
 80a9602:	f7f8 fed6 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = func(in1_val, in2_val);
        }
      }
    }
  }
}
 80a9606:	b025      	add	sp, #148	; 0x94
 80a9608:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a960c <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE>:
constexpr int kInputTensor1 = 0;
constexpr int kInputTensor2 = 1;
constexpr int kOutputTensor = 0;

TfLiteStatus LogicalImpl(TfLiteContext* context, TfLiteNode* node,
                         bool (*func)(bool, bool)) {
 80a960c:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
 80a9610:	680b      	ldr	r3, [r1, #0]
 80a9612:	4617      	mov	r7, r2
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a9614:	685c      	ldr	r4, [r3, #4]
 80a9616:	6882      	ldr	r2, [r0, #8]
 80a9618:	689d      	ldr	r5, [r3, #8]
 80a961a:	2038      	movs	r0, #56	; 0x38
 80a961c:	fb00 2404 	mla	r4, r0, r4, r2
 80a9620:	fb00 2505 	mla	r5, r0, r5, r2
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a9624:	684b      	ldr	r3, [r1, #4]
 80a9626:	b094      	sub	sp, #80	; 0x50
 80a9628:	6859      	ldr	r1, [r3, #4]
 80a962a:	ae0f      	add	r6, sp, #60	; 0x3c
 80a962c:	fb00 2801 	mla	r8, r0, r1, r2
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  if (HaveSameShapes(input1, input2)) {
 80a9630:	4629      	mov	r1, r5
 80a9632:	4620      	mov	r0, r4
 80a9634:	f006 fa6a 	bl	80afb0c <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
    reference_ops::BinaryFunction<bool, bool, bool>(
        GetTensorShape(input1), GetTensorData<bool>(input1),
 80a9638:	4621      	mov	r1, r4
                         bool (*func)(bool, bool)) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  if (HaveSameShapes(input1, input2)) {
 80a963a:	b1f8      	cbz	r0, 80a967c <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x70>
    reference_ops::BinaryFunction<bool, bool, bool>(
        GetTensorShape(input1), GetTensorData<bool>(input1),
 80a963c:	a805      	add	r0, sp, #20
 80a963e:	f7f9 f968 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a9642:	b104      	cbz	r4, 80a9646 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x3a>
 80a9644:	6864      	ldr	r4, [r4, #4]
        GetTensorShape(input2), GetTensorData<bool>(input2),
 80a9646:	4629      	mov	r1, r5
 80a9648:	a80a      	add	r0, sp, #40	; 0x28
 80a964a:	f7f9 f962 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a964e:	b105      	cbz	r5, 80a9652 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x46>
 80a9650:	686d      	ldr	r5, [r5, #4]
        GetTensorShape(output), GetTensorData<bool>(output), func);
 80a9652:	4641      	mov	r1, r8
 80a9654:	4630      	mov	r0, r6
 80a9656:	f7f9 f95c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a965a:	f1b8 0f00 	cmp.w	r8, #0
 80a965e:	d002      	beq.n	80a9666 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x5a>
 80a9660:	f8d8 2004 	ldr.w	r2, [r8, #4]
 80a9664:	e000      	b.n	80a9668 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x5c>
 80a9666:	4642      	mov	r2, r8
 80a9668:	9201      	str	r2, [sp, #4]
 80a966a:	9702      	str	r7, [sp, #8]
 80a966c:	9600      	str	r6, [sp, #0]
 80a966e:	462b      	mov	r3, r5
 80a9670:	aa0a      	add	r2, sp, #40	; 0x28
 80a9672:	4621      	mov	r1, r4
 80a9674:	a805      	add	r0, sp, #20
 80a9676:	f7ff fe97 	bl	80a93a8 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E>
 80a967a:	e01e      	b.n	80a96ba <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0xae>
  } else {
    reference_ops::BroadcastBinaryFunction4DSlow<bool, bool, bool>(
        GetTensorShape(input1), GetTensorData<bool>(input1),
 80a967c:	a805      	add	r0, sp, #20
 80a967e:	f7f9 f948 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a9682:	b104      	cbz	r4, 80a9686 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x7a>
 80a9684:	6864      	ldr	r4, [r4, #4]
        GetTensorShape(input2), GetTensorData<bool>(input2),
 80a9686:	4629      	mov	r1, r5
 80a9688:	a80a      	add	r0, sp, #40	; 0x28
 80a968a:	f7f9 f942 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a968e:	b105      	cbz	r5, 80a9692 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x86>
 80a9690:	686d      	ldr	r5, [r5, #4]
        GetTensorShape(output), GetTensorData<bool>(output), func);
 80a9692:	4641      	mov	r1, r8
 80a9694:	4630      	mov	r0, r6
 80a9696:	f7f9 f93c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a969a:	f1b8 0f00 	cmp.w	r8, #0
 80a969e:	d002      	beq.n	80a96a6 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x9a>
 80a96a0:	f8d8 1004 	ldr.w	r1, [r8, #4]
 80a96a4:	e000      	b.n	80a96a8 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x9c>
 80a96a6:	4641      	mov	r1, r8
 80a96a8:	9101      	str	r1, [sp, #4]
 80a96aa:	9702      	str	r7, [sp, #8]
 80a96ac:	9600      	str	r6, [sp, #0]
 80a96ae:	462b      	mov	r3, r5
 80a96b0:	aa0a      	add	r2, sp, #40	; 0x28
 80a96b2:	4621      	mov	r1, r4
 80a96b4:	a805      	add	r0, sp, #20
 80a96b6:	f7ff feca 	bl	80a944e <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E>
 80a96ba:	4630      	mov	r0, r6
 80a96bc:	f7f8 fe79 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
        GetTensorShape(input2), GetTensorData<bool>(input2),
        GetTensorShape(output), GetTensorData<bool>(output), func);
  } else {
    reference_ops::BroadcastBinaryFunction4DSlow<bool, bool, bool>(
        GetTensorShape(input1), GetTensorData<bool>(input1),
        GetTensorShape(input2), GetTensorData<bool>(input2),
 80a96c0:	a80a      	add	r0, sp, #40	; 0x28
 80a96c2:	f7f8 fe76 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
        GetTensorShape(input1), GetTensorData<bool>(input1),
        GetTensorShape(input2), GetTensorData<bool>(input2),
        GetTensorShape(output), GetTensorData<bool>(output), func);
  } else {
    reference_ops::BroadcastBinaryFunction4DSlow<bool, bool, bool>(
        GetTensorShape(input1), GetTensorData<bool>(input1),
 80a96c6:	a805      	add	r0, sp, #20
 80a96c8:	f7f8 fe73 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
        GetTensorShape(input2), GetTensorData<bool>(input2),
        GetTensorShape(output), GetTensorData<bool>(output), func);
  }

  return kTfLiteOk;
}
 80a96cc:	2000      	movs	r0, #0
 80a96ce:	b014      	add	sp, #80	; 0x50
 80a96d0:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}

080a96d4 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_113LogicalOrEvalEP13TfLiteContextP10TfLiteNode>:

bool LogicalOr(bool x, bool y) { return x || y; }

TfLiteStatus LogicalOrEval(TfLiteContext* context, TfLiteNode* node) {
  return LogicalImpl(context, node, LogicalOr);
 80a96d4:	4a01      	ldr	r2, [pc, #4]	; (80a96dc <_ZN6tflite3ops5micro7logical12_GLOBAL__N_113LogicalOrEvalEP13TfLiteContextP10TfLiteNode+0x8>)
 80a96d6:	f7ff bf99 	b.w	80a960c <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE>
 80a96da:	bf00      	nop
 80a96dc:	080a9385 	.word	0x080a9385

080a96e0 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_114LogicalAndEvalEP13TfLiteContextP10TfLiteNode>:
}

bool LogicalAnd(bool x, bool y) { return x && y; }

TfLiteStatus LogicalAndEval(TfLiteContext* context, TfLiteNode* node) {
  return LogicalImpl(context, node, LogicalAnd);
 80a96e0:	4a01      	ldr	r2, [pc, #4]	; (80a96e8 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_114LogicalAndEvalEP13TfLiteContextP10TfLiteNode+0x8>)
 80a96e2:	f7ff bf93 	b.w	80a960c <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE>
 80a96e6:	bf00      	nop
 80a96e8:	080a938f 	.word	0x080a938f

080a96ec <_ZN6tflite3ops5micro11activations7PrepareEP13TfLiteContextP10TfLiteNode>:
constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
 80a96ec:	2000      	movs	r0, #0
 80a96ee:	4770      	bx	lr

080a96f0 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf>:

namespace tflite {
namespace reference_ops {

inline void Logistic(const RuntimeShape& input_shape, const float* input_data,
                     const RuntimeShape& output_shape, float* output_data) {
 80a96f0:	e92d 47f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, lr}
 80a96f4:	461f      	mov	r7, r3
 80a96f6:	6806      	ldr	r6, [r0, #0]

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
 80a96f8:	6813      	ldr	r3, [r2, #0]
 80a96fa:	4604      	mov	r4, r0
 80a96fc:	429e      	cmp	r6, r3
 80a96fe:	4688      	mov	r8, r1
 80a9700:	4691      	mov	r9, r2
 80a9702:	d101      	bne.n	80a9708 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x18>
 80a9704:	2500      	movs	r5, #0
 80a9706:	e00d      	b.n	80a9724 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x34>
 80a9708:	f006 fc92 	bl	80b0030 <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
 80a970c:	4629      	mov	r1, r5
 80a970e:	4620      	mov	r0, r4
 80a9710:	f7f8 fe5a 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a9714:	4629      	mov	r1, r5
 80a9716:	4682      	mov	sl, r0
 80a9718:	4648      	mov	r0, r9
 80a971a:	f7f8 fe55 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a971e:	4582      	cmp	sl, r0
 80a9720:	d1f2      	bne.n	80a9708 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x18>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
 80a9722:	3501      	adds	r5, #1
 80a9724:	42ae      	cmp	r6, r5
 80a9726:	dcf1      	bgt.n	80a970c <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x1c>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80a9728:	2e04      	cmp	r6, #4
 80a972a:	bfcc      	ite	gt
 80a972c:	6864      	ldrgt	r4, [r4, #4]
 80a972e:	3404      	addle	r4, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a9730:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
 80a9732:	2501      	movs	r5, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a9734:	429e      	cmp	r6, r3
 80a9736:	dc01      	bgt.n	80a973c <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x4c>
 80a9738:	2400      	movs	r4, #0
 80a973a:	e004      	b.n	80a9746 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x56>
      buffer_size *= dims_data[i];
 80a973c:	f854 2023 	ldr.w	r2, [r4, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80a9740:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
 80a9742:	4355      	muls	r5, r2
 80a9744:	e7f6      	b.n	80a9734 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x44>
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; i++) {
 80a9746:	42ac      	cmp	r4, r5
 80a9748:	da12      	bge.n	80a9770 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x80>
  using ::exp;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  exp(float __x)
  { return __builtin_expf(__x); }
 80a974a:	f858 0024 	ldr.w	r0, [r8, r4, lsl #2]
 80a974e:	f100 4000 	add.w	r0, r0, #2147483648	; 0x80000000
 80a9752:	f007 fd91 	bl	80b1278 <expf>
    float val = input_data[i];
    float result = 1.f / (1.f + std::exp(-val));
    output_data[i] = result;
 80a9756:	f04f 517e 	mov.w	r1, #1065353216	; 0x3f800000
 80a975a:	f009 fe3f 	bl	80b33dc <__addsf3>
 80a975e:	4601      	mov	r1, r0
 80a9760:	f04f 507e 	mov.w	r0, #1065353216	; 0x3f800000
 80a9764:	f009 fff6 	bl	80b3754 <__aeabi_fdiv>
 80a9768:	f847 0024 	str.w	r0, [r7, r4, lsl #2]

inline void Logistic(const RuntimeShape& input_shape, const float* input_data,
                     const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; i++) {
 80a976c:	3401      	adds	r4, #1
 80a976e:	e7ea      	b.n	80a9746 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x56>
 80a9770:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}

080a9774 <_ZN6tflite3ops5micro11activations4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80a9774:	b570      	push	{r4, r5, r6, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a9776:	680a      	ldr	r2, [r1, #0]
 80a9778:	2438      	movs	r4, #56	; 0x38
 80a977a:	6852      	ldr	r2, [r2, #4]
 80a977c:	6883      	ldr	r3, [r0, #8]
 80a977e:	4362      	muls	r2, r4
 80a9780:	4605      	mov	r5, r0
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (input->type) {
 80a9782:	5c98      	ldrb	r0, [r3, r2]

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80a9784:	b08a      	sub	sp, #40	; 0x28
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (input->type) {
 80a9786:	2801      	cmp	r0, #1
 80a9788:	eb03 0602 	add.w	r6, r3, r2
 80a978c:	d11d      	bne.n	80a97ca <_ZN6tflite3ops5micro11activations4EvalEP13TfLiteContextP10TfLiteNode+0x56>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a978e:	684a      	ldr	r2, [r1, #4]
    case kTfLiteFloat32: {
      reference_ops::Logistic(
          GetTensorShape(input), GetTensorData<float>(input),
 80a9790:	4668      	mov	r0, sp
 80a9792:	6852      	ldr	r2, [r2, #4]
 80a9794:	4631      	mov	r1, r6
 80a9796:	fb04 3402 	mla	r4, r4, r2, r3
 80a979a:	f7f9 f8ba 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
          GetTensorShape(output), GetTensorData<float>(output));
 80a979e:	4621      	mov	r1, r4
 80a97a0:	a805      	add	r0, sp, #20
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a97a2:	6875      	ldr	r5, [r6, #4]
 80a97a4:	f7f9 f8b5 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80a97a8:	b10c      	cbz	r4, 80a97ae <_ZN6tflite3ops5micro11activations4EvalEP13TfLiteContextP10TfLiteNode+0x3a>
 80a97aa:	6863      	ldr	r3, [r4, #4]
 80a97ac:	e000      	b.n	80a97b0 <_ZN6tflite3ops5micro11activations4EvalEP13TfLiteContextP10TfLiteNode+0x3c>
 80a97ae:	4623      	mov	r3, r4
 80a97b0:	aa05      	add	r2, sp, #20
 80a97b2:	4629      	mov	r1, r5
 80a97b4:	4668      	mov	r0, sp
 80a97b6:	f7ff ff9b 	bl	80a96f0 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf>
 80a97ba:	a805      	add	r0, sp, #20
 80a97bc:	f7f8 fdf9 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (input->type) {
    case kTfLiteFloat32: {
      reference_ops::Logistic(
          GetTensorShape(input), GetTensorData<float>(input),
 80a97c0:	4668      	mov	r0, sp
 80a97c2:	f7f8 fdf6 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
          GetTensorShape(output), GetTensorData<float>(output));
      return kTfLiteOk;
 80a97c6:	2000      	movs	r0, #0
 80a97c8:	e007      	b.n	80a97da <_ZN6tflite3ops5micro11activations4EvalEP13TfLiteContextP10TfLiteNode+0x66>
    }
    default: {
      // TODO(b/141211002): Also support other data types once we have supported
      // temporary tensors in TFLM.
      context->ReportError(context,
 80a97ca:	696c      	ldr	r4, [r5, #20]
 80a97cc:	f7f6 fca6 	bl	80a011c <TfLiteTypeGetName>
                           "Only float32 is supported currently, got %s",
                           TfLiteTypeGetName(input->type));
 80a97d0:	4903      	ldr	r1, [pc, #12]	; (80a97e0 <_ZN6tflite3ops5micro11activations4EvalEP13TfLiteContextP10TfLiteNode+0x6c>)
 80a97d2:	4602      	mov	r2, r0
 80a97d4:	4628      	mov	r0, r5
 80a97d6:	47a0      	blx	r4
      return kTfLiteError;
 80a97d8:	2001      	movs	r0, #1
    }
  }
}
 80a97da:	b00a      	add	sp, #40	; 0x28
 80a97dc:	bd70      	pop	{r4, r5, r6, pc}
 80a97de:	bf00      	nop
 80a97e0:	080b61cb 	.word	0x080b61cb

080a97e4 <_ZN6tflite3ops5micro17Register_LOGISTICEv>:
TfLiteRegistration* Register_LOGISTIC() {
  static TfLiteRegistration r = {/*init=*/nullptr,
                                 /*free=*/nullptr, activations::Prepare,
                                 activations::Eval};
  return &r;
}
 80a97e4:	4800      	ldr	r0, [pc, #0]	; (80a97e8 <_ZN6tflite3ops5micro17Register_LOGISTICEv+0x4>)
 80a97e6:	4770      	bx	lr
 80a97e8:	20000308 	.word	0x20000308

080a97ec <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MaximumOp2opIfEET_S6_S6_>:
  TfLiteTensor* output;
};

struct MaximumOp {
  template <typename data_type>
  static data_type op(data_type el1, data_type el2) {
 80a97ec:	b538      	push	{r3, r4, r5, lr}
 80a97ee:	4604      	mov	r4, r0
 80a97f0:	460d      	mov	r5, r1
    return el1 > el2 ? el1 : el2;
 80a97f2:	f00a f8b7 	bl	80b3964 <__aeabi_fcmpgt>
 80a97f6:	b908      	cbnz	r0, 80a97fc <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MaximumOp2opIfEET_S6_S6_+0x10>
 80a97f8:	4628      	mov	r0, r5
 80a97fa:	bd38      	pop	{r3, r4, r5, pc}
 80a97fc:	4620      	mov	r0, r4
  }
 80a97fe:	bd38      	pop	{r3, r4, r5, pc}

080a9800 <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MaximumOp2opIhEET_S6_S6_>:
 80a9800:	4288      	cmp	r0, r1
 80a9802:	bf38      	it	cc
 80a9804:	4608      	movcc	r0, r1
 80a9806:	4770      	bx	lr

080a9808 <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MaximumOp2opIaEET_S6_S6_>:
 80a9808:	4288      	cmp	r0, r1
 80a980a:	bfb8      	it	lt
 80a980c:	4608      	movlt	r0, r1
 80a980e:	4770      	bx	lr

080a9810 <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MaximumOp2opIlEET_S6_S6_>:
 80a9810:	4288      	cmp	r0, r1
 80a9812:	bfb8      	it	lt
 80a9814:	4608      	movlt	r0, r1
 80a9816:	4770      	bx	lr

080a9818 <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MaximumOp2opIxEET_S6_S6_>:
  TfLiteTensor* output;
};

struct MaximumOp {
  template <typename data_type>
  static data_type op(data_type el1, data_type el2) {
 80a9818:	b500      	push	{lr}
    return el1 > el2 ? el1 : el2;
 80a981a:	4290      	cmp	r0, r2
 80a981c:	eb71 0e03 	sbcs.w	lr, r1, r3
 80a9820:	bfbc      	itt	lt
 80a9822:	4610      	movlt	r0, r2
 80a9824:	4619      	movlt	r1, r3
  }
 80a9826:	f85d fb04 	ldr.w	pc, [sp], #4

080a982a <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MinimumOp2opIfEET_S6_S6_>:
};

struct MinimumOp {
  template <typename data_type>
  static data_type op(data_type el1, data_type el2) {
 80a982a:	b538      	push	{r3, r4, r5, lr}
 80a982c:	4604      	mov	r4, r0
 80a982e:	460d      	mov	r5, r1
    return el1 < el2 ? el1 : el2;
 80a9830:	f00a f87a 	bl	80b3928 <__aeabi_fcmplt>
 80a9834:	b908      	cbnz	r0, 80a983a <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MinimumOp2opIfEET_S6_S6_+0x10>
 80a9836:	4628      	mov	r0, r5
 80a9838:	bd38      	pop	{r3, r4, r5, pc}
 80a983a:	4620      	mov	r0, r4
  }
 80a983c:	bd38      	pop	{r3, r4, r5, pc}

080a983e <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MinimumOp2opIhEET_S6_S6_>:
 80a983e:	4288      	cmp	r0, r1
 80a9840:	bf28      	it	cs
 80a9842:	4608      	movcs	r0, r1
 80a9844:	4770      	bx	lr

080a9846 <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MinimumOp2opIaEET_S6_S6_>:
 80a9846:	4288      	cmp	r0, r1
 80a9848:	bfa8      	it	ge
 80a984a:	4608      	movge	r0, r1
 80a984c:	4770      	bx	lr

080a984e <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MinimumOp2opIlEET_S6_S6_>:
 80a984e:	4288      	cmp	r0, r1
 80a9850:	bfa8      	it	ge
 80a9852:	4608      	movge	r0, r1
 80a9854:	4770      	bx	lr

080a9856 <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MinimumOp2opIxEET_S6_S6_>:
  }
};

struct MinimumOp {
  template <typename data_type>
  static data_type op(data_type el1, data_type el2) {
 80a9856:	b500      	push	{lr}
    return el1 < el2 ? el1 : el2;
 80a9858:	4282      	cmp	r2, r0
 80a985a:	eb73 0e01 	sbcs.w	lr, r3, r1
 80a985e:	bfbc      	itt	lt
 80a9860:	4610      	movlt	r0, r2
 80a9862:	4619      	movlt	r1, r3
  }
 80a9864:	f85d fb04 	ldr.w	pc, [sp], #4

080a9868 <_ZN6tflite3ops5micro16Register_MAXIMUMEv>:
      /* free */ nullptr,
      /* prepare */ nullptr,
      maximum_minimum::Eval<maximum_minimum::kReference,
                            maximum_minimum::MaximumOp>};
  return &r;
}
 80a9868:	4800      	ldr	r0, [pc, #0]	; (80a986c <_ZN6tflite3ops5micro16Register_MAXIMUMEv+0x4>)
 80a986a:	4770      	bx	lr
 80a986c:	20000348 	.word	0x20000348

080a9870 <_ZN6tflite3ops5micro16Register_MINIMUMEv>:
      /* free */ nullptr,
      /* prepare */ nullptr,
      maximum_minimum::Eval<maximum_minimum::kReference,
                            maximum_minimum::MinimumOp>};
  return &r;
}
 80a9870:	4800      	ldr	r0, [pc, #0]	; (80a9874 <_ZN6tflite3ops5micro16Register_MINIMUMEv+0x4>)
 80a9872:	4770      	bx	lr
 80a9874:	20000328 	.word	0x20000328

080a9878 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>:

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
 80a9878:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a987c:	4699      	mov	r9, r3
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a987e:	6803      	ldr	r3, [r0, #0]

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
 80a9880:	b09b      	sub	sp, #108	; 0x6c
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a9882:	2b04      	cmp	r3, #4

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
 80a9884:	4616      	mov	r6, r2
 80a9886:	4604      	mov	r4, r0
 80a9888:	9102      	str	r1, [sp, #8]
 80a988a:	9a24      	ldr	r2, [sp, #144]	; 0x90
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a988c:	dd01      	ble.n	80a9892 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x1a>
 80a988e:	f006 fbcf 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a9892:	6833      	ldr	r3, [r6, #0]
 80a9894:	2b04      	cmp	r3, #4
 80a9896:	dcfa      	bgt.n	80a988e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a9898:	6813      	ldr	r3, [r2, #0]
 80a989a:	2b04      	cmp	r3, #4
 80a989c:	dcf7      	bgt.n	80a988e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
 80a989e:	2301      	movs	r3, #1
 80a98a0:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a98a2:	ad0a      	add	r5, sp, #40	; 0x28
 80a98a4:	a805      	add	r0, sp, #20
 80a98a6:	f7f8 fdc8 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a98aa:	4620      	mov	r0, r4
 80a98ac:	ab12      	add	r3, sp, #72	; 0x48
 80a98ae:	462a      	mov	r2, r5
 80a98b0:	4631      	mov	r1, r6
 80a98b2:	f7f9 f8c5 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a98b6:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a98b8:	9503      	str	r5, [sp, #12]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a98ba:	2100      	movs	r1, #0
 80a98bc:	a805      	add	r0, sp, #20
 80a98be:	f7f8 fd83 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a98c2:	4284      	cmp	r4, r0
 80a98c4:	da43      	bge.n	80a994e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd6>
 80a98c6:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a98c8:	af05      	add	r7, sp, #20
 80a98ca:	2101      	movs	r1, #1
 80a98cc:	4638      	mov	r0, r7
 80a98ce:	f7f8 fd7b 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a98d2:	4285      	cmp	r5, r0
 80a98d4:	da39      	bge.n	80a994a <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd2>
 80a98d6:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a98d8:	2102      	movs	r1, #2
 80a98da:	4638      	mov	r0, r7
 80a98dc:	f7f8 fd74 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a98e0:	4286      	cmp	r6, r0
 80a98e2:	da30      	bge.n	80a9946 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xce>
 80a98e4:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a98e8:	2103      	movs	r1, #3
 80a98ea:	4638      	mov	r0, r7
 80a98ec:	f7f8 fd6c 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a98f0:	4580      	cmp	r8, r0
 80a98f2:	da26      	bge.n	80a9942 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xca>
          auto out_idx = Offset(output_shape, b, y, x, c);
 80a98f4:	f8cd 8000 	str.w	r8, [sp]
 80a98f8:	4633      	mov	r3, r6
 80a98fa:	462a      	mov	r2, r5
 80a98fc:	4621      	mov	r1, r4
 80a98fe:	4638      	mov	r0, r7
 80a9900:	f7f8 fdc7 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a9904:	f8cd 8000 	str.w	r8, [sp]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
 80a9908:	4682      	mov	sl, r0
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a990a:	4633      	mov	r3, r6
 80a990c:	462a      	mov	r2, r5
 80a990e:	4621      	mov	r1, r4
 80a9910:	9803      	ldr	r0, [sp, #12]
 80a9912:	f7f8 fe6f 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
 80a9916:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a991a:	4683      	mov	fp, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
 80a991c:	4633      	mov	r3, r6
 80a991e:	462a      	mov	r2, r5
 80a9920:	4621      	mov	r1, r4
 80a9922:	a812      	add	r0, sp, #72	; 0x48
 80a9924:	f7f8 fe66 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
 80a9928:	9b02      	ldr	r3, [sp, #8]
 80a992a:	f859 1020 	ldr.w	r1, [r9, r0, lsl #2]
 80a992e:	f853 002b 	ldr.w	r0, [r3, fp, lsl #2]
 80a9932:	9b26      	ldr	r3, [sp, #152]	; 0x98
 80a9934:	4798      	blx	r3
 80a9936:	9b25      	ldr	r3, [sp, #148]	; 0x94
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a9938:	f108 0801 	add.w	r8, r8, #1
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
 80a993c:	f843 002a 	str.w	r0, [r3, sl, lsl #2]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a9940:	e7d2      	b.n	80a98e8 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x70>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a9942:	3601      	adds	r6, #1
 80a9944:	e7c8      	b.n	80a98d8 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x60>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a9946:	3501      	adds	r5, #1
 80a9948:	e7be      	b.n	80a98c8 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x50>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a994a:	3401      	adds	r4, #1
 80a994c:	e7b5      	b.n	80a98ba <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x42>
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a994e:	a805      	add	r0, sp, #20
 80a9950:	f7f8 fd2f 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = op(in1_val, in2_val);
        }
      }
    }
  }
}
 80a9954:	b01b      	add	sp, #108	; 0x6c
 80a9956:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a995a <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>:

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
 80a995a:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a995e:	4699      	mov	r9, r3
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a9960:	6803      	ldr	r3, [r0, #0]

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
 80a9962:	b09b      	sub	sp, #108	; 0x6c
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a9964:	2b04      	cmp	r3, #4

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
 80a9966:	4616      	mov	r6, r2
 80a9968:	4604      	mov	r4, r0
 80a996a:	9102      	str	r1, [sp, #8]
 80a996c:	9a24      	ldr	r2, [sp, #144]	; 0x90
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a996e:	dd01      	ble.n	80a9974 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x1a>
 80a9970:	f006 fb5e 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a9974:	6833      	ldr	r3, [r6, #0]
 80a9976:	2b04      	cmp	r3, #4
 80a9978:	dcfa      	bgt.n	80a9970 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a997a:	6813      	ldr	r3, [r2, #0]
 80a997c:	2b04      	cmp	r3, #4
 80a997e:	dcf7      	bgt.n	80a9970 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
 80a9980:	2301      	movs	r3, #1
 80a9982:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a9984:	ad0a      	add	r5, sp, #40	; 0x28
 80a9986:	a805      	add	r0, sp, #20
 80a9988:	f7f8 fd57 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a998c:	4620      	mov	r0, r4
 80a998e:	ab12      	add	r3, sp, #72	; 0x48
 80a9990:	462a      	mov	r2, r5
 80a9992:	4631      	mov	r1, r6
 80a9994:	f7f9 f854 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a9998:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a999a:	9503      	str	r5, [sp, #12]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a999c:	2100      	movs	r1, #0
 80a999e:	a805      	add	r0, sp, #20
 80a99a0:	f7f8 fd12 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a99a4:	4284      	cmp	r4, r0
 80a99a6:	da43      	bge.n	80a9a30 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd6>
 80a99a8:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a99aa:	af05      	add	r7, sp, #20
 80a99ac:	2101      	movs	r1, #1
 80a99ae:	4638      	mov	r0, r7
 80a99b0:	f7f8 fd0a 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a99b4:	4285      	cmp	r5, r0
 80a99b6:	da39      	bge.n	80a9a2c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd2>
 80a99b8:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a99ba:	2102      	movs	r1, #2
 80a99bc:	4638      	mov	r0, r7
 80a99be:	f7f8 fd03 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a99c2:	4286      	cmp	r6, r0
 80a99c4:	da30      	bge.n	80a9a28 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xce>
 80a99c6:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a99ca:	2103      	movs	r1, #3
 80a99cc:	4638      	mov	r0, r7
 80a99ce:	f7f8 fcfb 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a99d2:	4580      	cmp	r8, r0
 80a99d4:	da26      	bge.n	80a9a24 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xca>
          auto out_idx = Offset(output_shape, b, y, x, c);
 80a99d6:	f8cd 8000 	str.w	r8, [sp]
 80a99da:	4633      	mov	r3, r6
 80a99dc:	462a      	mov	r2, r5
 80a99de:	4621      	mov	r1, r4
 80a99e0:	4638      	mov	r0, r7
 80a99e2:	f7f8 fd56 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a99e6:	f8cd 8000 	str.w	r8, [sp]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
 80a99ea:	4682      	mov	sl, r0
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a99ec:	4633      	mov	r3, r6
 80a99ee:	462a      	mov	r2, r5
 80a99f0:	4621      	mov	r1, r4
 80a99f2:	9803      	ldr	r0, [sp, #12]
 80a99f4:	f7f8 fdfe 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
 80a99f8:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a99fc:	4683      	mov	fp, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
 80a99fe:	4633      	mov	r3, r6
 80a9a00:	462a      	mov	r2, r5
 80a9a02:	4621      	mov	r1, r4
 80a9a04:	a812      	add	r0, sp, #72	; 0x48
 80a9a06:	f7f8 fdf5 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
 80a9a0a:	9b02      	ldr	r3, [sp, #8]
 80a9a0c:	f819 1000 	ldrb.w	r1, [r9, r0]
 80a9a10:	f813 000b 	ldrb.w	r0, [r3, fp]
 80a9a14:	9b26      	ldr	r3, [sp, #152]	; 0x98
 80a9a16:	4798      	blx	r3
 80a9a18:	9b25      	ldr	r3, [sp, #148]	; 0x94
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a9a1a:	f108 0801 	add.w	r8, r8, #1
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
 80a9a1e:	f803 000a 	strb.w	r0, [r3, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a9a22:	e7d2      	b.n	80a99ca <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x70>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a9a24:	3601      	adds	r6, #1
 80a9a26:	e7c8      	b.n	80a99ba <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x60>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a9a28:	3501      	adds	r5, #1
 80a9a2a:	e7be      	b.n	80a99aa <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x50>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a9a2c:	3401      	adds	r4, #1
 80a9a2e:	e7b5      	b.n	80a999c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x42>
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a9a30:	a805      	add	r0, sp, #20
 80a9a32:	f7f8 fcbe 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = op(in1_val, in2_val);
        }
      }
    }
  }
}
 80a9a36:	b01b      	add	sp, #108	; 0x6c
 80a9a38:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a9a3c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>:

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
 80a9a3c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a9a40:	4699      	mov	r9, r3
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a9a42:	6803      	ldr	r3, [r0, #0]

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
 80a9a44:	b09b      	sub	sp, #108	; 0x6c
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a9a46:	2b04      	cmp	r3, #4

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
 80a9a48:	4616      	mov	r6, r2
 80a9a4a:	4604      	mov	r4, r0
 80a9a4c:	9102      	str	r1, [sp, #8]
 80a9a4e:	9a24      	ldr	r2, [sp, #144]	; 0x90
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a9a50:	dd01      	ble.n	80a9a56 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x1a>
 80a9a52:	f006 faed 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a9a56:	6833      	ldr	r3, [r6, #0]
 80a9a58:	2b04      	cmp	r3, #4
 80a9a5a:	dcfa      	bgt.n	80a9a52 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a9a5c:	6813      	ldr	r3, [r2, #0]
 80a9a5e:	2b04      	cmp	r3, #4
 80a9a60:	dcf7      	bgt.n	80a9a52 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
 80a9a62:	2301      	movs	r3, #1
 80a9a64:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a9a66:	ad0a      	add	r5, sp, #40	; 0x28
 80a9a68:	a805      	add	r0, sp, #20
 80a9a6a:	f7f8 fce6 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a9a6e:	4620      	mov	r0, r4
 80a9a70:	ab12      	add	r3, sp, #72	; 0x48
 80a9a72:	462a      	mov	r2, r5
 80a9a74:	4631      	mov	r1, r6
 80a9a76:	f7f8 ffe3 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a9a7a:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a9a7c:	9503      	str	r5, [sp, #12]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a9a7e:	2100      	movs	r1, #0
 80a9a80:	a805      	add	r0, sp, #20
 80a9a82:	f7f8 fca1 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a9a86:	4284      	cmp	r4, r0
 80a9a88:	da43      	bge.n	80a9b12 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd6>
 80a9a8a:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a9a8c:	af05      	add	r7, sp, #20
 80a9a8e:	2101      	movs	r1, #1
 80a9a90:	4638      	mov	r0, r7
 80a9a92:	f7f8 fc99 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a9a96:	4285      	cmp	r5, r0
 80a9a98:	da39      	bge.n	80a9b0e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd2>
 80a9a9a:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a9a9c:	2102      	movs	r1, #2
 80a9a9e:	4638      	mov	r0, r7
 80a9aa0:	f7f8 fc92 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a9aa4:	4286      	cmp	r6, r0
 80a9aa6:	da30      	bge.n	80a9b0a <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xce>
 80a9aa8:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a9aac:	2103      	movs	r1, #3
 80a9aae:	4638      	mov	r0, r7
 80a9ab0:	f7f8 fc8a 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a9ab4:	4580      	cmp	r8, r0
 80a9ab6:	da26      	bge.n	80a9b06 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xca>
          auto out_idx = Offset(output_shape, b, y, x, c);
 80a9ab8:	f8cd 8000 	str.w	r8, [sp]
 80a9abc:	4633      	mov	r3, r6
 80a9abe:	462a      	mov	r2, r5
 80a9ac0:	4621      	mov	r1, r4
 80a9ac2:	4638      	mov	r0, r7
 80a9ac4:	f7f8 fce5 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a9ac8:	f8cd 8000 	str.w	r8, [sp]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
 80a9acc:	4682      	mov	sl, r0
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a9ace:	4633      	mov	r3, r6
 80a9ad0:	462a      	mov	r2, r5
 80a9ad2:	4621      	mov	r1, r4
 80a9ad4:	9803      	ldr	r0, [sp, #12]
 80a9ad6:	f7f8 fd8d 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
 80a9ada:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a9ade:	4683      	mov	fp, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
 80a9ae0:	4633      	mov	r3, r6
 80a9ae2:	462a      	mov	r2, r5
 80a9ae4:	4621      	mov	r1, r4
 80a9ae6:	a812      	add	r0, sp, #72	; 0x48
 80a9ae8:	f7f8 fd84 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
 80a9aec:	9b02      	ldr	r3, [sp, #8]
 80a9aee:	f919 1000 	ldrsb.w	r1, [r9, r0]
 80a9af2:	f913 000b 	ldrsb.w	r0, [r3, fp]
 80a9af6:	9b26      	ldr	r3, [sp, #152]	; 0x98
 80a9af8:	4798      	blx	r3
 80a9afa:	9b25      	ldr	r3, [sp, #148]	; 0x94
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a9afc:	f108 0801 	add.w	r8, r8, #1
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
 80a9b00:	f803 000a 	strb.w	r0, [r3, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a9b04:	e7d2      	b.n	80a9aac <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x70>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a9b06:	3601      	adds	r6, #1
 80a9b08:	e7c8      	b.n	80a9a9c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x60>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a9b0a:	3501      	adds	r5, #1
 80a9b0c:	e7be      	b.n	80a9a8c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x50>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a9b0e:	3401      	adds	r4, #1
 80a9b10:	e7b5      	b.n	80a9a7e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x42>
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a9b12:	a805      	add	r0, sp, #20
 80a9b14:	f7f8 fc4d 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = op(in1_val, in2_val);
        }
      }
    }
  }
}
 80a9b18:	b01b      	add	sp, #108	; 0x6c
 80a9b1a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a9b1e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>:

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
 80a9b1e:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a9b22:	4699      	mov	r9, r3
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a9b24:	6803      	ldr	r3, [r0, #0]

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
 80a9b26:	b09b      	sub	sp, #108	; 0x6c
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a9b28:	2b04      	cmp	r3, #4

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
 80a9b2a:	4616      	mov	r6, r2
 80a9b2c:	4604      	mov	r4, r0
 80a9b2e:	9102      	str	r1, [sp, #8]
 80a9b30:	9a24      	ldr	r2, [sp, #144]	; 0x90
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a9b32:	dd01      	ble.n	80a9b38 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x1a>
 80a9b34:	f006 fa7c 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a9b38:	6833      	ldr	r3, [r6, #0]
 80a9b3a:	2b04      	cmp	r3, #4
 80a9b3c:	dcfa      	bgt.n	80a9b34 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a9b3e:	6813      	ldr	r3, [r2, #0]
 80a9b40:	2b04      	cmp	r3, #4
 80a9b42:	dcf7      	bgt.n	80a9b34 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
 80a9b44:	2301      	movs	r3, #1
 80a9b46:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a9b48:	ad0a      	add	r5, sp, #40	; 0x28
 80a9b4a:	a805      	add	r0, sp, #20
 80a9b4c:	f7f8 fc75 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a9b50:	4620      	mov	r0, r4
 80a9b52:	ab12      	add	r3, sp, #72	; 0x48
 80a9b54:	462a      	mov	r2, r5
 80a9b56:	4631      	mov	r1, r6
 80a9b58:	f7f8 ff72 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a9b5c:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a9b5e:	9503      	str	r5, [sp, #12]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a9b60:	2100      	movs	r1, #0
 80a9b62:	a805      	add	r0, sp, #20
 80a9b64:	f7f8 fc30 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a9b68:	4284      	cmp	r4, r0
 80a9b6a:	da43      	bge.n	80a9bf4 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd6>
 80a9b6c:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a9b6e:	af05      	add	r7, sp, #20
 80a9b70:	2101      	movs	r1, #1
 80a9b72:	4638      	mov	r0, r7
 80a9b74:	f7f8 fc28 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a9b78:	4285      	cmp	r5, r0
 80a9b7a:	da39      	bge.n	80a9bf0 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd2>
 80a9b7c:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a9b7e:	2102      	movs	r1, #2
 80a9b80:	4638      	mov	r0, r7
 80a9b82:	f7f8 fc21 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a9b86:	4286      	cmp	r6, r0
 80a9b88:	da30      	bge.n	80a9bec <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xce>
 80a9b8a:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a9b8e:	2103      	movs	r1, #3
 80a9b90:	4638      	mov	r0, r7
 80a9b92:	f7f8 fc19 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a9b96:	4580      	cmp	r8, r0
 80a9b98:	da26      	bge.n	80a9be8 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xca>
          auto out_idx = Offset(output_shape, b, y, x, c);
 80a9b9a:	f8cd 8000 	str.w	r8, [sp]
 80a9b9e:	4633      	mov	r3, r6
 80a9ba0:	462a      	mov	r2, r5
 80a9ba2:	4621      	mov	r1, r4
 80a9ba4:	4638      	mov	r0, r7
 80a9ba6:	f7f8 fc74 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a9baa:	f8cd 8000 	str.w	r8, [sp]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
 80a9bae:	4682      	mov	sl, r0
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a9bb0:	4633      	mov	r3, r6
 80a9bb2:	462a      	mov	r2, r5
 80a9bb4:	4621      	mov	r1, r4
 80a9bb6:	9803      	ldr	r0, [sp, #12]
 80a9bb8:	f7f8 fd1c 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
 80a9bbc:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a9bc0:	4683      	mov	fp, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
 80a9bc2:	4633      	mov	r3, r6
 80a9bc4:	462a      	mov	r2, r5
 80a9bc6:	4621      	mov	r1, r4
 80a9bc8:	a812      	add	r0, sp, #72	; 0x48
 80a9bca:	f7f8 fd13 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
 80a9bce:	9b02      	ldr	r3, [sp, #8]
 80a9bd0:	f859 1020 	ldr.w	r1, [r9, r0, lsl #2]
 80a9bd4:	f853 002b 	ldr.w	r0, [r3, fp, lsl #2]
 80a9bd8:	9b26      	ldr	r3, [sp, #152]	; 0x98
 80a9bda:	4798      	blx	r3
 80a9bdc:	9b25      	ldr	r3, [sp, #148]	; 0x94
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a9bde:	f108 0801 	add.w	r8, r8, #1
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
 80a9be2:	f843 002a 	str.w	r0, [r3, sl, lsl #2]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a9be6:	e7d2      	b.n	80a9b8e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x70>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a9be8:	3601      	adds	r6, #1
 80a9bea:	e7c8      	b.n	80a9b7e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x60>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a9bec:	3501      	adds	r5, #1
 80a9bee:	e7be      	b.n	80a9b6e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x50>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a9bf0:	3401      	adds	r4, #1
 80a9bf2:	e7b5      	b.n	80a9b60 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x42>
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a9bf4:	a805      	add	r0, sp, #20
 80a9bf6:	f7f8 fbdc 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = op(in1_val, in2_val);
        }
      }
    }
  }
}
 80a9bfa:	b01b      	add	sp, #108	; 0x6c
 80a9bfc:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a9c00 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>:

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
 80a9c00:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80a9c04:	469b      	mov	fp, r3
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a9c06:	6803      	ldr	r3, [r0, #0]

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
 80a9c08:	b09d      	sub	sp, #116	; 0x74
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a9c0a:	2b04      	cmp	r3, #4

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
 80a9c0c:	4616      	mov	r6, r2
 80a9c0e:	4604      	mov	r4, r0
 80a9c10:	9104      	str	r1, [sp, #16]
 80a9c12:	9a26      	ldr	r2, [sp, #152]	; 0x98
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80a9c14:	dd01      	ble.n	80a9c1a <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x1a>
 80a9c16:	f006 fa0b 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80a9c1a:	6833      	ldr	r3, [r6, #0]
 80a9c1c:	2b04      	cmp	r3, #4
 80a9c1e:	dcfa      	bgt.n	80a9c16 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80a9c20:	6813      	ldr	r3, [r2, #0]
 80a9c22:	2b04      	cmp	r3, #4
 80a9c24:	dcf7      	bgt.n	80a9c16 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
 80a9c26:	2301      	movs	r3, #1
 80a9c28:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
 80a9c2a:	ad0c      	add	r5, sp, #48	; 0x30
 80a9c2c:	a807      	add	r0, sp, #28
 80a9c2e:	f7f8 fc04 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80a9c32:	4620      	mov	r0, r4
 80a9c34:	ab14      	add	r3, sp, #80	; 0x50
 80a9c36:	462a      	mov	r2, r5
 80a9c38:	4631      	mov	r1, r6
 80a9c3a:	f7f8 ff01 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a9c3e:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a9c40:	9505      	str	r5, [sp, #20]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a9c42:	2100      	movs	r1, #0
 80a9c44:	a807      	add	r0, sp, #28
 80a9c46:	f7f8 fbbf 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a9c4a:	4284      	cmp	r4, r0
 80a9c4c:	da4a      	bge.n	80a9ce4 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xe4>
 80a9c4e:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a9c50:	af07      	add	r7, sp, #28
 80a9c52:	2101      	movs	r1, #1
 80a9c54:	4638      	mov	r0, r7
 80a9c56:	f7f8 fbb7 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a9c5a:	4285      	cmp	r5, r0
 80a9c5c:	da40      	bge.n	80a9ce0 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xe0>
 80a9c5e:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a9c60:	9703      	str	r7, [sp, #12]
 80a9c62:	2102      	movs	r1, #2
 80a9c64:	9803      	ldr	r0, [sp, #12]
 80a9c66:	f7f8 fbaf 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a9c6a:	4286      	cmp	r6, r0
 80a9c6c:	da36      	bge.n	80a9cdc <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xdc>
 80a9c6e:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a9c72:	2103      	movs	r1, #3
 80a9c74:	9803      	ldr	r0, [sp, #12]
 80a9c76:	f7f8 fba7 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80a9c7a:	4580      	cmp	r8, r0
 80a9c7c:	da2c      	bge.n	80a9cd8 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd8>
          auto out_idx = Offset(output_shape, b, y, x, c);
 80a9c7e:	f8cd 8000 	str.w	r8, [sp]
 80a9c82:	4633      	mov	r3, r6
 80a9c84:	462a      	mov	r2, r5
 80a9c86:	4621      	mov	r1, r4
 80a9c88:	9803      	ldr	r0, [sp, #12]
 80a9c8a:	f7f8 fc02 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a9c8e:	f8cd 8000 	str.w	r8, [sp]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
 80a9c92:	4681      	mov	r9, r0
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a9c94:	4633      	mov	r3, r6
 80a9c96:	462a      	mov	r2, r5
 80a9c98:	4621      	mov	r1, r4
 80a9c9a:	9805      	ldr	r0, [sp, #20]
 80a9c9c:	f7f8 fcaa 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
 80a9ca0:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80a9ca4:	4682      	mov	sl, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
 80a9ca6:	4633      	mov	r3, r6
 80a9ca8:	462a      	mov	r2, r5
 80a9caa:	4621      	mov	r1, r4
 80a9cac:	a814      	add	r0, sp, #80	; 0x50
 80a9cae:	f7f8 fca1 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
 80a9cb2:	9b27      	ldr	r3, [sp, #156]	; 0x9c
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
 80a9cb4:	eb0b 00c0 	add.w	r0, fp, r0, lsl #3
          output_data[out_idx] = op(in1_val, in2_val);
 80a9cb8:	eb03 09c9 	add.w	r9, r3, r9, lsl #3
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
 80a9cbc:	9b04      	ldr	r3, [sp, #16]
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
 80a9cbe:	9f28      	ldr	r7, [sp, #160]	; 0xa0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
 80a9cc0:	eb03 0aca 	add.w	sl, r3, sl, lsl #3
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
 80a9cc4:	e9d0 2300 	ldrd	r2, r3, [r0]
 80a9cc8:	e9da 0100 	ldrd	r0, r1, [sl]
 80a9ccc:	47b8      	blx	r7
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a9cce:	f108 0801 	add.w	r8, r8, #1
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
 80a9cd2:	e9c9 0100 	strd	r0, r1, [r9]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80a9cd6:	e7cc      	b.n	80a9c72 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x72>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80a9cd8:	3601      	adds	r6, #1
 80a9cda:	e7c2      	b.n	80a9c62 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x62>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80a9cdc:	3501      	adds	r5, #1
 80a9cde:	e7b7      	b.n	80a9c50 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x50>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80a9ce0:	3401      	adds	r4, #1
 80a9ce2:	e7ae      	b.n	80a9c42 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x42>
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80a9ce4:	a807      	add	r0, sp, #28
 80a9ce6:	f7f8 fb64 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = op(in1_val, in2_val);
        }
      }
    }
  }
}
 80a9cea:	b01d      	add	sp, #116	; 0x74
 80a9cec:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080a9cf0 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode>:
      GetTensorData<data_type>(op_context.output),
      op_type::template op<data_type>);
}

template <KernelType kernel_type, typename OpType>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80a9cf0:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
 80a9cf4:	680b      	ldr	r3, [r1, #0]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a9cf6:	2238      	movs	r2, #56	; 0x38
 80a9cf8:	685c      	ldr	r4, [r3, #4]
 80a9cfa:	689d      	ldr	r5, [r3, #8]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a9cfc:	684b      	ldr	r3, [r1, #4]
 80a9cfe:	f8d0 8008 	ldr.w	r8, [r0, #8]
 80a9d02:	685f      	ldr	r7, [r3, #4]
 80a9d04:	4681      	mov	r9, r0
 80a9d06:	4357      	muls	r7, r2
  OpContext op_context(context, node);

  if (kernel_type == kReference) {
    switch (op_context.output->type) {
 80a9d08:	f818 0007 	ldrb.w	r0, [r8, r7]
      GetTensorData<data_type>(op_context.output),
      op_type::template op<data_type>);
}

template <KernelType kernel_type, typename OpType>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80a9d0c:	b095      	sub	sp, #84	; 0x54
  OpContext op_context(context, node);

  if (kernel_type == kReference) {
    switch (op_context.output->type) {
 80a9d0e:	1e43      	subs	r3, r0, #1
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a9d10:	fb02 8404 	mla	r4, r2, r4, r8
 80a9d14:	fb02 8505 	mla	r5, r2, r5, r8
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a9d18:	eb08 0607 	add.w	r6, r8, r7
 80a9d1c:	2b08      	cmp	r3, #8
 80a9d1e:	f200 80a2 	bhi.w	80a9e66 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x176>
 80a9d22:	e8df f003 	tbb	[pc, r3]
 80a9d26:	5c05      	.short	0x5c05
 80a9d28:	a0a07922 	.word	0xa0a07922
 80a9d2c:	a0a0      	.short	0xa0a0
 80a9d2e:	3f          	.byte	0x3f
 80a9d2f:	00          	.byte	0x00
}  // namespace

template <typename data_type, typename op_type>
void TFLiteOperation(TfLiteContext* context, TfLiteNode* node,
                     const OpContext& op_context) {
  reference_ops::MaximumMinimumBroadcast4DSlow(
 80a9d30:	4621      	mov	r1, r4
 80a9d32:	a80f      	add	r0, sp, #60	; 0x3c
 80a9d34:	f7f8 fded 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80a9d38:	b104      	cbz	r4, 80a9d3c <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x4c>
 80a9d3a:	6864      	ldr	r4, [r4, #4]
 80a9d3c:	4629      	mov	r1, r5
 80a9d3e:	a80a      	add	r0, sp, #40	; 0x28
 80a9d40:	f7f8 fde7 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9d44:	b105      	cbz	r5, 80a9d48 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x58>
 80a9d46:	686d      	ldr	r5, [r5, #4]
 80a9d48:	af05      	add	r7, sp, #20
 80a9d4a:	4631      	mov	r1, r6
 80a9d4c:	4638      	mov	r0, r7
 80a9d4e:	f7f8 fde0 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9d52:	4b4c      	ldr	r3, [pc, #304]	; (80a9e84 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x194>)
 80a9d54:	aa0a      	add	r2, sp, #40	; 0x28
 80a9d56:	9302      	str	r3, [sp, #8]
 80a9d58:	6873      	ldr	r3, [r6, #4]
 80a9d5a:	4621      	mov	r1, r4
 80a9d5c:	9301      	str	r3, [sp, #4]
 80a9d5e:	9700      	str	r7, [sp, #0]
 80a9d60:	462b      	mov	r3, r5
 80a9d62:	a80f      	add	r0, sp, #60	; 0x3c
 80a9d64:	f7ff fd88 	bl	80a9878 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
 80a9d68:	e072      	b.n	80a9e50 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
 80a9d6a:	4621      	mov	r1, r4
 80a9d6c:	a80f      	add	r0, sp, #60	; 0x3c
 80a9d6e:	f7f8 fdd0 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9d72:	b104      	cbz	r4, 80a9d76 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x86>
 80a9d74:	6864      	ldr	r4, [r4, #4]
 80a9d76:	4629      	mov	r1, r5
 80a9d78:	a80a      	add	r0, sp, #40	; 0x28
 80a9d7a:	f7f8 fdca 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9d7e:	b105      	cbz	r5, 80a9d82 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x92>
 80a9d80:	686d      	ldr	r5, [r5, #4]
 80a9d82:	af05      	add	r7, sp, #20
 80a9d84:	4631      	mov	r1, r6
 80a9d86:	4638      	mov	r0, r7
 80a9d88:	f7f8 fdc3 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9d8c:	4b3e      	ldr	r3, [pc, #248]	; (80a9e88 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x198>)
 80a9d8e:	aa0a      	add	r2, sp, #40	; 0x28
 80a9d90:	9302      	str	r3, [sp, #8]
 80a9d92:	6873      	ldr	r3, [r6, #4]
 80a9d94:	4621      	mov	r1, r4
 80a9d96:	9301      	str	r3, [sp, #4]
 80a9d98:	9700      	str	r7, [sp, #0]
 80a9d9a:	462b      	mov	r3, r5
 80a9d9c:	a80f      	add	r0, sp, #60	; 0x3c
 80a9d9e:	f7ff fddc 	bl	80a995a <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
 80a9da2:	e055      	b.n	80a9e50 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
 80a9da4:	4621      	mov	r1, r4
 80a9da6:	a80f      	add	r0, sp, #60	; 0x3c
 80a9da8:	f7f8 fdb3 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9dac:	b104      	cbz	r4, 80a9db0 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xc0>
 80a9dae:	6864      	ldr	r4, [r4, #4]
 80a9db0:	4629      	mov	r1, r5
 80a9db2:	a80a      	add	r0, sp, #40	; 0x28
 80a9db4:	f7f8 fdad 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9db8:	b105      	cbz	r5, 80a9dbc <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xcc>
 80a9dba:	686d      	ldr	r5, [r5, #4]
 80a9dbc:	af05      	add	r7, sp, #20
 80a9dbe:	4631      	mov	r1, r6
 80a9dc0:	4638      	mov	r0, r7
 80a9dc2:	f7f8 fda6 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9dc6:	4b31      	ldr	r3, [pc, #196]	; (80a9e8c <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x19c>)
 80a9dc8:	aa0a      	add	r2, sp, #40	; 0x28
 80a9dca:	9302      	str	r3, [sp, #8]
 80a9dcc:	6873      	ldr	r3, [r6, #4]
 80a9dce:	4621      	mov	r1, r4
 80a9dd0:	9301      	str	r3, [sp, #4]
 80a9dd2:	9700      	str	r7, [sp, #0]
 80a9dd4:	462b      	mov	r3, r5
 80a9dd6:	a80f      	add	r0, sp, #60	; 0x3c
 80a9dd8:	f7ff fe30 	bl	80a9a3c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
 80a9ddc:	e038      	b.n	80a9e50 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
 80a9dde:	4621      	mov	r1, r4
 80a9de0:	a80f      	add	r0, sp, #60	; 0x3c
 80a9de2:	f7f8 fd96 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9de6:	b104      	cbz	r4, 80a9dea <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xfa>
 80a9de8:	6864      	ldr	r4, [r4, #4]
 80a9dea:	4629      	mov	r1, r5
 80a9dec:	a80a      	add	r0, sp, #40	; 0x28
 80a9dee:	f7f8 fd90 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9df2:	b105      	cbz	r5, 80a9df6 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x106>
 80a9df4:	686d      	ldr	r5, [r5, #4]
 80a9df6:	af05      	add	r7, sp, #20
 80a9df8:	4631      	mov	r1, r6
 80a9dfa:	4638      	mov	r0, r7
 80a9dfc:	f7f8 fd89 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9e00:	4b23      	ldr	r3, [pc, #140]	; (80a9e90 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x1a0>)
 80a9e02:	aa0a      	add	r2, sp, #40	; 0x28
 80a9e04:	9302      	str	r3, [sp, #8]
 80a9e06:	6873      	ldr	r3, [r6, #4]
 80a9e08:	4621      	mov	r1, r4
 80a9e0a:	9301      	str	r3, [sp, #4]
 80a9e0c:	9700      	str	r7, [sp, #0]
 80a9e0e:	462b      	mov	r3, r5
 80a9e10:	a80f      	add	r0, sp, #60	; 0x3c
 80a9e12:	f7ff fe84 	bl	80a9b1e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
 80a9e16:	e01b      	b.n	80a9e50 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
 80a9e18:	4621      	mov	r1, r4
 80a9e1a:	a80f      	add	r0, sp, #60	; 0x3c
 80a9e1c:	f7f8 fd79 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9e20:	b104      	cbz	r4, 80a9e24 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x134>
 80a9e22:	6864      	ldr	r4, [r4, #4]
 80a9e24:	4629      	mov	r1, r5
 80a9e26:	a80a      	add	r0, sp, #40	; 0x28
 80a9e28:	f7f8 fd73 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9e2c:	b105      	cbz	r5, 80a9e30 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x140>
 80a9e2e:	686d      	ldr	r5, [r5, #4]
 80a9e30:	af05      	add	r7, sp, #20
 80a9e32:	4631      	mov	r1, r6
 80a9e34:	4638      	mov	r0, r7
 80a9e36:	f7f8 fd6c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9e3a:	4b16      	ldr	r3, [pc, #88]	; (80a9e94 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x1a4>)
 80a9e3c:	aa0a      	add	r2, sp, #40	; 0x28
 80a9e3e:	9302      	str	r3, [sp, #8]
 80a9e40:	6873      	ldr	r3, [r6, #4]
 80a9e42:	4621      	mov	r1, r4
 80a9e44:	9301      	str	r3, [sp, #4]
 80a9e46:	9700      	str	r7, [sp, #0]
 80a9e48:	462b      	mov	r3, r5
 80a9e4a:	a80f      	add	r0, sp, #60	; 0x3c
 80a9e4c:	f7ff fed8 	bl	80a9c00 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
 80a9e50:	4638      	mov	r0, r7
 80a9e52:	f7f8 faae 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a9e56:	a80a      	add	r0, sp, #40	; 0x28
 80a9e58:	f7f8 faab 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80a9e5c:	a80f      	add	r0, sp, #60	; 0x3c
 80a9e5e:	f7f8 faa8 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  } else {
    context->ReportError(context,
                         "Kernel type not supported by Maximum/Minimum.");
    return kTfLiteError;
  }
  return kTfLiteOk;
 80a9e62:	2000      	movs	r0, #0
 80a9e64:	e00a      	b.n	80a9e7c <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x18c>
        break;
      case kTfLiteInt64:
        TFLiteOperation<int64_t, OpType>(context, node, op_context);
        break;
      default:
        context->ReportError(
 80a9e66:	f8d9 4014 	ldr.w	r4, [r9, #20]
 80a9e6a:	f7f6 f957 	bl	80a011c <TfLiteTypeGetName>
 80a9e6e:	f818 3007 	ldrb.w	r3, [r8, r7]
 80a9e72:	4602      	mov	r2, r0
 80a9e74:	4908      	ldr	r1, [pc, #32]	; (80a9e98 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x1a8>)
 80a9e76:	4648      	mov	r0, r9
 80a9e78:	47a0      	blx	r4
            context, "Type %s (%d) is not supported by Maximum/Minimum.",
            TfLiteTypeGetName(op_context.output->type),
            op_context.output->type);
        return kTfLiteError;
 80a9e7a:	2001      	movs	r0, #1
    context->ReportError(context,
                         "Kernel type not supported by Maximum/Minimum.");
    return kTfLiteError;
  }
  return kTfLiteOk;
}
 80a9e7c:	b015      	add	sp, #84	; 0x54
 80a9e7e:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}
 80a9e82:	bf00      	nop
 80a9e84:	080a982b 	.word	0x080a982b
 80a9e88:	080a983f 	.word	0x080a983f
 80a9e8c:	080a9847 	.word	0x080a9847
 80a9e90:	080a984f 	.word	0x080a984f
 80a9e94:	080a9857 	.word	0x080a9857
 80a9e98:	080b61f7 	.word	0x080b61f7

080a9e9c <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode>:
      GetTensorData<data_type>(op_context.output),
      op_type::template op<data_type>);
}

template <KernelType kernel_type, typename OpType>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80a9e9c:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
 80a9ea0:	680b      	ldr	r3, [r1, #0]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a9ea2:	2238      	movs	r2, #56	; 0x38
 80a9ea4:	685c      	ldr	r4, [r3, #4]
 80a9ea6:	689d      	ldr	r5, [r3, #8]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a9ea8:	684b      	ldr	r3, [r1, #4]
 80a9eaa:	f8d0 8008 	ldr.w	r8, [r0, #8]
 80a9eae:	685f      	ldr	r7, [r3, #4]
 80a9eb0:	4681      	mov	r9, r0
 80a9eb2:	4357      	muls	r7, r2
  OpContext op_context(context, node);

  if (kernel_type == kReference) {
    switch (op_context.output->type) {
 80a9eb4:	f818 0007 	ldrb.w	r0, [r8, r7]
      GetTensorData<data_type>(op_context.output),
      op_type::template op<data_type>);
}

template <KernelType kernel_type, typename OpType>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80a9eb8:	b095      	sub	sp, #84	; 0x54
  OpContext op_context(context, node);

  if (kernel_type == kReference) {
    switch (op_context.output->type) {
 80a9eba:	1e43      	subs	r3, r0, #1
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80a9ebc:	fb02 8404 	mla	r4, r2, r4, r8
 80a9ec0:	fb02 8505 	mla	r5, r2, r5, r8
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80a9ec4:	eb08 0607 	add.w	r6, r8, r7
 80a9ec8:	2b08      	cmp	r3, #8
 80a9eca:	f200 80a2 	bhi.w	80aa012 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x176>
 80a9ece:	e8df f003 	tbb	[pc, r3]
 80a9ed2:	5c05      	.short	0x5c05
 80a9ed4:	a0a07922 	.word	0xa0a07922
 80a9ed8:	a0a0      	.short	0xa0a0
 80a9eda:	3f          	.byte	0x3f
 80a9edb:	00          	.byte	0x00
}  // namespace

template <typename data_type, typename op_type>
void TFLiteOperation(TfLiteContext* context, TfLiteNode* node,
                     const OpContext& op_context) {
  reference_ops::MaximumMinimumBroadcast4DSlow(
 80a9edc:	4621      	mov	r1, r4
 80a9ede:	a80f      	add	r0, sp, #60	; 0x3c
 80a9ee0:	f7f8 fd17 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9ee4:	b104      	cbz	r4, 80a9ee8 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x4c>
 80a9ee6:	6864      	ldr	r4, [r4, #4]
 80a9ee8:	4629      	mov	r1, r5
 80a9eea:	a80a      	add	r0, sp, #40	; 0x28
 80a9eec:	f7f8 fd11 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9ef0:	b105      	cbz	r5, 80a9ef4 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x58>
 80a9ef2:	686d      	ldr	r5, [r5, #4]
 80a9ef4:	af05      	add	r7, sp, #20
 80a9ef6:	4631      	mov	r1, r6
 80a9ef8:	4638      	mov	r0, r7
 80a9efa:	f7f8 fd0a 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9efe:	4b4c      	ldr	r3, [pc, #304]	; (80aa030 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x194>)
 80a9f00:	aa0a      	add	r2, sp, #40	; 0x28
 80a9f02:	9302      	str	r3, [sp, #8]
 80a9f04:	6873      	ldr	r3, [r6, #4]
 80a9f06:	4621      	mov	r1, r4
 80a9f08:	9301      	str	r3, [sp, #4]
 80a9f0a:	9700      	str	r7, [sp, #0]
 80a9f0c:	462b      	mov	r3, r5
 80a9f0e:	a80f      	add	r0, sp, #60	; 0x3c
 80a9f10:	f7ff fcb2 	bl	80a9878 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
 80a9f14:	e072      	b.n	80a9ffc <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
 80a9f16:	4621      	mov	r1, r4
 80a9f18:	a80f      	add	r0, sp, #60	; 0x3c
 80a9f1a:	f7f8 fcfa 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9f1e:	b104      	cbz	r4, 80a9f22 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x86>
 80a9f20:	6864      	ldr	r4, [r4, #4]
 80a9f22:	4629      	mov	r1, r5
 80a9f24:	a80a      	add	r0, sp, #40	; 0x28
 80a9f26:	f7f8 fcf4 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9f2a:	b105      	cbz	r5, 80a9f2e <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x92>
 80a9f2c:	686d      	ldr	r5, [r5, #4]
 80a9f2e:	af05      	add	r7, sp, #20
 80a9f30:	4631      	mov	r1, r6
 80a9f32:	4638      	mov	r0, r7
 80a9f34:	f7f8 fced 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9f38:	4b3e      	ldr	r3, [pc, #248]	; (80aa034 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x198>)
 80a9f3a:	aa0a      	add	r2, sp, #40	; 0x28
 80a9f3c:	9302      	str	r3, [sp, #8]
 80a9f3e:	6873      	ldr	r3, [r6, #4]
 80a9f40:	4621      	mov	r1, r4
 80a9f42:	9301      	str	r3, [sp, #4]
 80a9f44:	9700      	str	r7, [sp, #0]
 80a9f46:	462b      	mov	r3, r5
 80a9f48:	a80f      	add	r0, sp, #60	; 0x3c
 80a9f4a:	f7ff fd06 	bl	80a995a <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
 80a9f4e:	e055      	b.n	80a9ffc <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
 80a9f50:	4621      	mov	r1, r4
 80a9f52:	a80f      	add	r0, sp, #60	; 0x3c
 80a9f54:	f7f8 fcdd 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9f58:	b104      	cbz	r4, 80a9f5c <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xc0>
 80a9f5a:	6864      	ldr	r4, [r4, #4]
 80a9f5c:	4629      	mov	r1, r5
 80a9f5e:	a80a      	add	r0, sp, #40	; 0x28
 80a9f60:	f7f8 fcd7 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9f64:	b105      	cbz	r5, 80a9f68 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xcc>
 80a9f66:	686d      	ldr	r5, [r5, #4]
 80a9f68:	af05      	add	r7, sp, #20
 80a9f6a:	4631      	mov	r1, r6
 80a9f6c:	4638      	mov	r0, r7
 80a9f6e:	f7f8 fcd0 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9f72:	4b31      	ldr	r3, [pc, #196]	; (80aa038 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x19c>)
 80a9f74:	aa0a      	add	r2, sp, #40	; 0x28
 80a9f76:	9302      	str	r3, [sp, #8]
 80a9f78:	6873      	ldr	r3, [r6, #4]
 80a9f7a:	4621      	mov	r1, r4
 80a9f7c:	9301      	str	r3, [sp, #4]
 80a9f7e:	9700      	str	r7, [sp, #0]
 80a9f80:	462b      	mov	r3, r5
 80a9f82:	a80f      	add	r0, sp, #60	; 0x3c
 80a9f84:	f7ff fd5a 	bl	80a9a3c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
 80a9f88:	e038      	b.n	80a9ffc <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
 80a9f8a:	4621      	mov	r1, r4
 80a9f8c:	a80f      	add	r0, sp, #60	; 0x3c
 80a9f8e:	f7f8 fcc0 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9f92:	b104      	cbz	r4, 80a9f96 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xfa>
 80a9f94:	6864      	ldr	r4, [r4, #4]
 80a9f96:	4629      	mov	r1, r5
 80a9f98:	a80a      	add	r0, sp, #40	; 0x28
 80a9f9a:	f7f8 fcba 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9f9e:	b105      	cbz	r5, 80a9fa2 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x106>
 80a9fa0:	686d      	ldr	r5, [r5, #4]
 80a9fa2:	af05      	add	r7, sp, #20
 80a9fa4:	4631      	mov	r1, r6
 80a9fa6:	4638      	mov	r0, r7
 80a9fa8:	f7f8 fcb3 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9fac:	4b23      	ldr	r3, [pc, #140]	; (80aa03c <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x1a0>)
 80a9fae:	aa0a      	add	r2, sp, #40	; 0x28
 80a9fb0:	9302      	str	r3, [sp, #8]
 80a9fb2:	6873      	ldr	r3, [r6, #4]
 80a9fb4:	4621      	mov	r1, r4
 80a9fb6:	9301      	str	r3, [sp, #4]
 80a9fb8:	9700      	str	r7, [sp, #0]
 80a9fba:	462b      	mov	r3, r5
 80a9fbc:	a80f      	add	r0, sp, #60	; 0x3c
 80a9fbe:	f7ff fdae 	bl	80a9b1e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
 80a9fc2:	e01b      	b.n	80a9ffc <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
 80a9fc4:	4621      	mov	r1, r4
 80a9fc6:	a80f      	add	r0, sp, #60	; 0x3c
 80a9fc8:	f7f8 fca3 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9fcc:	b104      	cbz	r4, 80a9fd0 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x134>
 80a9fce:	6864      	ldr	r4, [r4, #4]
 80a9fd0:	4629      	mov	r1, r5
 80a9fd2:	a80a      	add	r0, sp, #40	; 0x28
 80a9fd4:	f7f8 fc9d 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9fd8:	b105      	cbz	r5, 80a9fdc <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x140>
 80a9fda:	686d      	ldr	r5, [r5, #4]
 80a9fdc:	af05      	add	r7, sp, #20
 80a9fde:	4631      	mov	r1, r6
 80a9fe0:	4638      	mov	r0, r7
 80a9fe2:	f7f8 fc96 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80a9fe6:	4b16      	ldr	r3, [pc, #88]	; (80aa040 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x1a4>)
 80a9fe8:	aa0a      	add	r2, sp, #40	; 0x28
 80a9fea:	9302      	str	r3, [sp, #8]
 80a9fec:	6873      	ldr	r3, [r6, #4]
 80a9fee:	4621      	mov	r1, r4
 80a9ff0:	9301      	str	r3, [sp, #4]
 80a9ff2:	9700      	str	r7, [sp, #0]
 80a9ff4:	462b      	mov	r3, r5
 80a9ff6:	a80f      	add	r0, sp, #60	; 0x3c
 80a9ff8:	f7ff fe02 	bl	80a9c00 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
 80a9ffc:	4638      	mov	r0, r7
 80a9ffe:	f7f8 f9d8 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80aa002:	a80a      	add	r0, sp, #40	; 0x28
 80aa004:	f7f8 f9d5 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80aa008:	a80f      	add	r0, sp, #60	; 0x3c
 80aa00a:	f7f8 f9d2 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  } else {
    context->ReportError(context,
                         "Kernel type not supported by Maximum/Minimum.");
    return kTfLiteError;
  }
  return kTfLiteOk;
 80aa00e:	2000      	movs	r0, #0
 80aa010:	e00a      	b.n	80aa028 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x18c>
        break;
      case kTfLiteInt64:
        TFLiteOperation<int64_t, OpType>(context, node, op_context);
        break;
      default:
        context->ReportError(
 80aa012:	f8d9 4014 	ldr.w	r4, [r9, #20]
 80aa016:	f7f6 f881 	bl	80a011c <TfLiteTypeGetName>
 80aa01a:	f818 3007 	ldrb.w	r3, [r8, r7]
 80aa01e:	4602      	mov	r2, r0
 80aa020:	4908      	ldr	r1, [pc, #32]	; (80aa044 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x1a8>)
 80aa022:	4648      	mov	r0, r9
 80aa024:	47a0      	blx	r4
            context, "Type %s (%d) is not supported by Maximum/Minimum.",
            TfLiteTypeGetName(op_context.output->type),
            op_context.output->type);
        return kTfLiteError;
 80aa026:	2001      	movs	r0, #1
    context->ReportError(context,
                         "Kernel type not supported by Maximum/Minimum.");
    return kTfLiteError;
  }
  return kTfLiteOk;
}
 80aa028:	b015      	add	sp, #84	; 0x54
 80aa02a:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}
 80aa02e:	bf00      	nop
 80aa030:	080a97ed 	.word	0x080a97ed
 80aa034:	080a9801 	.word	0x080a9801
 80aa038:	080a9809 	.word	0x080a9809
 80aa03c:	080a9811 	.word	0x080a9811
 80aa040:	080a9819 	.word	0x080a9819
 80aa044:	080b61f7 	.word	0x080b61f7

080aa048 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode>:
namespace neg {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80aa048:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80aa04c:	680b      	ldr	r3, [r1, #0]
 80aa04e:	2738      	movs	r7, #56	; 0x38
 80aa050:	685b      	ldr	r3, [r3, #4]
 80aa052:	6884      	ldr	r4, [r0, #8]
 80aa054:	437b      	muls	r3, r7
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  switch (input->type) {
 80aa056:	5ce2      	ldrb	r2, [r4, r3]
namespace neg {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80aa058:	b08a      	sub	sp, #40	; 0x28
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  switch (input->type) {
 80aa05a:	2a01      	cmp	r2, #1
 80aa05c:	eb04 0603 	add.w	r6, r4, r3
 80aa060:	d145      	bne.n	80aa0ee <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0xa6>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80aa062:	684b      	ldr	r3, [r1, #4]
    // TODO(wangtz): handle for kTfLiteInt8
    case kTfLiteFloat32:
      reference_ops::Negate(GetTensorShape(input), GetTensorData<float>(input),
 80aa064:	4668      	mov	r0, sp
 80aa066:	685b      	ldr	r3, [r3, #4]
 80aa068:	4631      	mov	r1, r6
 80aa06a:	fb07 4403 	mla	r4, r7, r3, r4
 80aa06e:	f7f8 fc50 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                            GetTensorShape(output),
 80aa072:	4621      	mov	r1, r4
 80aa074:	a805      	add	r0, sp, #20
 80aa076:	6877      	ldr	r7, [r6, #4]
 80aa078:	f7f8 fc4b 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80aa07c:	b104      	cbz	r4, 80aa080 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x38>
 80aa07e:	6864      	ldr	r4, [r4, #4]
 80aa080:	9e00      	ldr	r6, [sp, #0]

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
 80aa082:	9b05      	ldr	r3, [sp, #20]
 80aa084:	429e      	cmp	r6, r3
 80aa086:	d101      	bne.n	80aa08c <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x44>
 80aa088:	2500      	movs	r5, #0
 80aa08a:	e00d      	b.n	80aa0a8 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x60>
 80aa08c:	f005 ffd0 	bl	80b0030 <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
 80aa090:	4629      	mov	r1, r5
 80aa092:	4668      	mov	r0, sp
 80aa094:	f7f8 f998 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80aa098:	4629      	mov	r1, r5
 80aa09a:	4680      	mov	r8, r0
 80aa09c:	a805      	add	r0, sp, #20
 80aa09e:	f7f8 f993 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80aa0a2:	4580      	cmp	r8, r0
 80aa0a4:	d1f2      	bne.n	80aa08c <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x44>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
 80aa0a6:	3501      	adds	r5, #1
 80aa0a8:	42ae      	cmp	r6, r5
 80aa0aa:	dcf1      	bgt.n	80aa090 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x48>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80aa0ac:	2e04      	cmp	r6, #4
 80aa0ae:	bfcc      	ite	gt
 80aa0b0:	9a01      	ldrgt	r2, [sp, #4]
 80aa0b2:	aa01      	addle	r2, sp, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80aa0b4:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
 80aa0b6:	2101      	movs	r1, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80aa0b8:	429e      	cmp	r6, r3
 80aa0ba:	dc01      	bgt.n	80aa0c0 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x78>
 80aa0bc:	2300      	movs	r3, #0
 80aa0be:	e004      	b.n	80aa0ca <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x82>
      buffer_size *= dims_data[i];
 80aa0c0:	f852 0023 	ldr.w	r0, [r2, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80aa0c4:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
 80aa0c6:	4341      	muls	r1, r0
 80aa0c8:	e7f6      	b.n	80aa0b8 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x70>
template <typename T>
inline void Negate(const RuntimeShape& input_shape, const T* input_data,
                   const RuntimeShape& output_shape, T* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; ++i) {
 80aa0ca:	428b      	cmp	r3, r1
 80aa0cc:	da07      	bge.n	80aa0de <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x96>
    output_data[i] = -input_data[i];
 80aa0ce:	f857 2023 	ldr.w	r2, [r7, r3, lsl #2]
 80aa0d2:	f102 4200 	add.w	r2, r2, #2147483648	; 0x80000000
 80aa0d6:	f844 2023 	str.w	r2, [r4, r3, lsl #2]
template <typename T>
inline void Negate(const RuntimeShape& input_shape, const T* input_data,
                   const RuntimeShape& output_shape, T* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; ++i) {
 80aa0da:	3301      	adds	r3, #1
 80aa0dc:	e7f5      	b.n	80aa0ca <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x82>
 80aa0de:	a805      	add	r0, sp, #20
 80aa0e0:	f7f8 f967 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  switch (input->type) {
    // TODO(wangtz): handle for kTfLiteInt8
    case kTfLiteFloat32:
      reference_ops::Negate(GetTensorShape(input), GetTensorData<float>(input),
 80aa0e4:	4668      	mov	r0, sp
 80aa0e6:	f7f8 f964 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
    default:
      context->ReportError(
          context, "Neg only currently supports float32, got %d.", input->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
 80aa0ea:	2000      	movs	r0, #0
 80aa0ec:	e003      	b.n	80aa0f6 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0xae>
                            GetTensorShape(output),
                            GetTensorData<float>(output));
      break;
    default:
      context->ReportError(
          context, "Neg only currently supports float32, got %d.", input->type);
 80aa0ee:	6943      	ldr	r3, [r0, #20]
 80aa0f0:	4902      	ldr	r1, [pc, #8]	; (80aa0fc <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0xb4>)
 80aa0f2:	4798      	blx	r3
      return kTfLiteError;
 80aa0f4:	2001      	movs	r0, #1
  }
  return kTfLiteOk;
}
 80aa0f6:	b00a      	add	sp, #40	; 0x28
 80aa0f8:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80aa0fc:	080b6229 	.word	0x080b6229

080aa100 <_ZN6tflite3ops5micro12Register_NEGEv>:

TfLiteRegistration* Register_NEG() {
  static TfLiteRegistration r = {/*init=*/nullptr, /*free=*/nullptr,
                                 /*prepare=*/nullptr, neg::Eval};
  return &r;
}
 80aa100:	4800      	ldr	r0, [pc, #0]	; (80aa104 <_ZN6tflite3ops5micro12Register_NEGEv+0x4>)
 80aa102:	4770      	bx	lr
 80aa104:	20000368 	.word	0x20000368

080aa108 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_17PrepareEP13TfLiteContextP10TfLiteNode>:

constexpr int kOutputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
 80aa108:	2000      	movs	r0, #0
 80aa10a:	4770      	bx	lr

080aa10c <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode>:
  }

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80aa10c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80aa110:	684b      	ldr	r3, [r1, #4]
 80aa112:	b085      	sub	sp, #20
 80aa114:	6885      	ldr	r5, [r0, #8]
 80aa116:	9000      	str	r0, [sp, #0]
 80aa118:	6858      	ldr	r0, [r3, #4]
 80aa11a:	2338      	movs	r3, #56	; 0x38
 80aa11c:	4358      	muls	r0, r3
 80aa11e:	182b      	adds	r3, r5, r0
  const TfLitePackParams* data =
      reinterpret_cast<TfLitePackParams*>(node->builtin_data);

  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (output->type) {
 80aa120:	5c28      	ldrb	r0, [r5, r0]
  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLitePackParams* data =
      reinterpret_cast<TfLitePackParams*>(node->builtin_data);
 80aa122:	694a      	ldr	r2, [r1, #20]

  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (output->type) {
 80aa124:	1e46      	subs	r6, r0, #1
 80aa126:	2e08      	cmp	r6, #8
 80aa128:	f200 8219 	bhi.w	80aa55e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x452>
 80aa12c:	e8df f016 	tbh	[pc, r6, lsl #1]
 80aa130:	013e0009 	.word	0x013e0009
 80aa134:	01a90076 	.word	0x01a90076
 80aa138:	02170217 	.word	0x02170217
 80aa13c:	02170217 	.word	0x02170217
 80aa140:	00d9      	.short	0x00d9
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
 80aa142:	689e      	ldr	r6, [r3, #8]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
 80aa144:	6808      	ldr	r0, [r1, #0]
 80aa146:	46b4      	mov	ip, r6
  const TfLiteIntArray* input_dims = input0->dims;
 80aa148:	6840      	ldr	r0, [r0, #4]
 80aa14a:	2738      	movs	r7, #56	; 0x38
 80aa14c:	fb07 5500 	mla	r5, r7, r0, r5

  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (output->type) {
    case kTfLiteFloat32: {
      return PackImpl<float>(context, node, output, data->values_count,
 80aa150:	f8d2 8000 	ldr.w	r8, [r2]
                             data->axis);
 80aa154:	6852      	ldr	r2, [r2, #4]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
 80aa156:	f8d6 e000 	ldr.w	lr, [r6]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
 80aa15a:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
 80aa15c:	68ad      	ldr	r5, [r5, #8]
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
    axis += dimensions;
 80aa15e:	bfb8      	it	lt
 80aa160:	4472      	addlt	r2, lr
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80aa162:	2000      	movs	r0, #0

  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
 80aa164:	2701      	movs	r7, #1
  for (int i = 0; i < axis; ++i) {
 80aa166:	4282      	cmp	r2, r0
 80aa168:	dd05      	ble.n	80aa176 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x6a>
    outer_size *= output_dims->data[i];
 80aa16a:	f85c 9f04 	ldr.w	r9, [ip, #4]!
  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80aa16e:	3001      	adds	r0, #1
    outer_size *= output_dims->data[i];
 80aa170:	fb09 f707 	mul.w	r7, r9, r7
 80aa174:	e7f7      	b.n	80aa166 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
 80aa176:	1c50      	adds	r0, r2, #1
  }
  int copy_size = 1;
 80aa178:	2201      	movs	r2, #1
  for (int i = axis + 1; i < dimensions; ++i) {
 80aa17a:	4586      	cmp	lr, r0
 80aa17c:	f100 0c01 	add.w	ip, r0, #1
 80aa180:	dd04      	ble.n	80aa18c <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x80>
    copy_size *= output_dims->data[i];
 80aa182:	f856 002c 	ldr.w	r0, [r6, ip, lsl #2]
 80aa186:	4342      	muls	r2, r0
 80aa188:	4660      	mov	r0, ip
 80aa18a:	e7f6      	b.n	80aa17a <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x6e>
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
 80aa18c:	f8d5 c000 	ldr.w	ip, [r5]
 80aa190:	4628      	mov	r0, r5
 80aa192:	2600      	movs	r6, #0
 80aa194:	2501      	movs	r5, #1
 80aa196:	45b4      	cmp	ip, r6
 80aa198:	dd05      	ble.n	80aa1a6 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x9a>
    input_size *= input_dims->data[i];
 80aa19a:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= output_dims->data[i];
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
 80aa19e:	3601      	adds	r6, #1
    input_size *= input_dims->data[i];
 80aa1a0:	fb0e f505 	mul.w	r5, lr, r5
 80aa1a4:	e7f7      	b.n	80aa196 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x8a>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);
 80aa1a6:	fb02 f007 	mul.w	r0, r2, r7
 80aa1aa:	4285      	cmp	r5, r0
 80aa1ac:	d001      	beq.n	80aa1b2 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xa6>
 80aa1ae:	f005 ff3f 	bl	80b0030 <abort>
 80aa1b2:	2500      	movs	r5, #0

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
 80aa1b4:	462e      	mov	r6, r5
 80aa1b6:	685b      	ldr	r3, [r3, #4]
 80aa1b8:	ea4f 0c82 	mov.w	ip, r2, lsl #2
 80aa1bc:	9301      	str	r3, [sp, #4]
 80aa1be:	fb02 f308 	mul.w	r3, r2, r8
 80aa1c2:	009b      	lsls	r3, r3, #2
 80aa1c4:	9302      	str	r3, [sp, #8]
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
 80aa1c6:	f04f 0938 	mov.w	r9, #56	; 0x38
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
 80aa1ca:	45b0      	cmp	r8, r6
 80aa1cc:	dc01      	bgt.n	80aa1d2 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc6>
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (output->type) {
    case kTfLiteFloat32: {
      return PackImpl<float>(context, node, output, data->values_count,
                             data->axis);
 80aa1ce:	2000      	movs	r0, #0
 80aa1d0:	e1ce      	b.n	80aa570 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x464>
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
 80aa1d2:	680b      	ldr	r3, [r1, #0]
 80aa1d4:	eb03 0386 	add.w	r3, r3, r6, lsl #2
 80aa1d8:	6858      	ldr	r0, [r3, #4]
 80aa1da:	9b00      	ldr	r3, [sp, #0]
 80aa1dc:	689b      	ldr	r3, [r3, #8]
 80aa1de:	fb09 3300 	mla	r3, r9, r0, r3
 80aa1e2:	b103      	cbz	r3, 80aa1e6 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xda>
 80aa1e4:	685b      	ldr	r3, [r3, #4]
 80aa1e6:	f04f 0e00 	mov.w	lr, #0
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80aa1ea:	46f2      	mov	sl, lr
 80aa1ec:	4557      	cmp	r7, sl
 80aa1ee:	dd12      	ble.n	80aa216 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x10a>
      const T* input_ptr = input_data + copy_size * k;
      int loc = k * values_count * copy_size + i * copy_size;
      T* output_ptr = output_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
 80aa1f0:	9c01      	ldr	r4, [sp, #4]
 80aa1f2:	eb05 0b0e 	add.w	fp, r5, lr
  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80aa1f6:	2000      	movs	r0, #0
      const T* input_ptr = input_data + copy_size * k;
      int loc = k * values_count * copy_size + i * copy_size;
      T* output_ptr = output_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
 80aa1f8:	44a3      	add	fp, r4
 80aa1fa:	4282      	cmp	r2, r0
 80aa1fc:	dd05      	ble.n	80aa20a <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xfe>
 80aa1fe:	f853 4020 	ldr.w	r4, [r3, r0, lsl #2]
 80aa202:	f84b 4020 	str.w	r4, [fp, r0, lsl #2]
 80aa206:	3001      	adds	r0, #1
 80aa208:	e7f7      	b.n	80aa1fa <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xee>
 80aa20a:	9802      	ldr	r0, [sp, #8]
  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80aa20c:	f10a 0a01 	add.w	sl, sl, #1
 80aa210:	4463      	add	r3, ip
 80aa212:	4486      	add	lr, r0
 80aa214:	e7ea      	b.n	80aa1ec <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xe0>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
 80aa216:	3601      	adds	r6, #1
 80aa218:	4465      	add	r5, ip
 80aa21a:	e7d6      	b.n	80aa1ca <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xbe>
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
 80aa21c:	689f      	ldr	r7, [r3, #8]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
 80aa21e:	6808      	ldr	r0, [r1, #0]
 80aa220:	46bc      	mov	ip, r7
  const TfLiteIntArray* input_dims = input0->dims;
 80aa222:	6840      	ldr	r0, [r0, #4]
 80aa224:	2638      	movs	r6, #56	; 0x38
 80aa226:	fb06 5500 	mla	r5, r6, r0, r5
    case kTfLiteFloat32: {
      return PackImpl<float>(context, node, output, data->values_count,
                             data->axis);
    }
    case kTfLiteUInt8: {
      return PackImpl<uint8_t>(context, node, output, data->values_count,
 80aa22a:	f8d2 9000 	ldr.w	r9, [r2]
                               data->axis);
 80aa22e:	6852      	ldr	r2, [r2, #4]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
 80aa230:	f8d7 e000 	ldr.w	lr, [r7]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
 80aa234:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
 80aa236:	68ae      	ldr	r6, [r5, #8]
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
    axis += dimensions;
 80aa238:	bfb8      	it	lt
 80aa23a:	4472      	addlt	r2, lr
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80aa23c:	2000      	movs	r0, #0

  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
 80aa23e:	2501      	movs	r5, #1
  for (int i = 0; i < axis; ++i) {
 80aa240:	4282      	cmp	r2, r0
 80aa242:	dd05      	ble.n	80aa250 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x144>
    outer_size *= output_dims->data[i];
 80aa244:	f85c 8f04 	ldr.w	r8, [ip, #4]!
  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80aa248:	3001      	adds	r0, #1
    outer_size *= output_dims->data[i];
 80aa24a:	fb08 f505 	mul.w	r5, r8, r5
 80aa24e:	e7f7      	b.n	80aa240 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x134>
 80aa250:	1c50      	adds	r0, r2, #1
  }
  int copy_size = 1;
 80aa252:	2201      	movs	r2, #1
  for (int i = axis + 1; i < dimensions; ++i) {
 80aa254:	4586      	cmp	lr, r0
 80aa256:	f100 0c01 	add.w	ip, r0, #1
 80aa25a:	dd04      	ble.n	80aa266 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x15a>
    copy_size *= output_dims->data[i];
 80aa25c:	f857 002c 	ldr.w	r0, [r7, ip, lsl #2]
 80aa260:	4342      	muls	r2, r0
 80aa262:	4660      	mov	r0, ip
 80aa264:	e7f6      	b.n	80aa254 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x148>
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
 80aa266:	f8d6 c000 	ldr.w	ip, [r6]
 80aa26a:	4630      	mov	r0, r6
 80aa26c:	2700      	movs	r7, #0
 80aa26e:	2601      	movs	r6, #1
 80aa270:	45bc      	cmp	ip, r7
 80aa272:	dd05      	ble.n	80aa280 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x174>
    input_size *= input_dims->data[i];
 80aa274:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= output_dims->data[i];
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
 80aa278:	3701      	adds	r7, #1
    input_size *= input_dims->data[i];
 80aa27a:	fb0e f606 	mul.w	r6, lr, r6
 80aa27e:	e7f7      	b.n	80aa270 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x164>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);
 80aa280:	fb02 f005 	mul.w	r0, r2, r5
 80aa284:	4286      	cmp	r6, r0
 80aa286:	d192      	bne.n	80aa1ae <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xa2>
 80aa288:	685e      	ldr	r6, [r3, #4]
 80aa28a:	fb02 f309 	mul.w	r3, r2, r9

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
 80aa28e:	2000      	movs	r0, #0
 80aa290:	9301      	str	r3, [sp, #4]
 80aa292:	4581      	cmp	r9, r0
 80aa294:	dd9b      	ble.n	80aa1ce <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc2>
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
 80aa296:	680b      	ldr	r3, [r1, #0]
 80aa298:	2438      	movs	r4, #56	; 0x38
 80aa29a:	eb03 0380 	add.w	r3, r3, r0, lsl #2
 80aa29e:	685f      	ldr	r7, [r3, #4]
 80aa2a0:	9b00      	ldr	r3, [sp, #0]
 80aa2a2:	689b      	ldr	r3, [r3, #8]
 80aa2a4:	fb04 3307 	mla	r3, r4, r7, r3
 80aa2a8:	b103      	cbz	r3, 80aa2ac <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1a0>
 80aa2aa:	685b      	ldr	r3, [r3, #4]
 80aa2ac:	46b6      	mov	lr, r6
 80aa2ae:	425f      	negs	r7, r3
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80aa2b0:	f04f 0c00 	mov.w	ip, #0
 80aa2b4:	4565      	cmp	r5, ip
 80aa2b6:	dd11      	ble.n	80aa2dc <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1d0>
 80aa2b8:	46f2      	mov	sl, lr
 80aa2ba:	4698      	mov	r8, r3
      const T* input_ptr = input_data + copy_size * k;
      int loc = k * values_count * copy_size + i * copy_size;
      T* output_ptr = output_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
 80aa2bc:	eb07 0b08 	add.w	fp, r7, r8
 80aa2c0:	455a      	cmp	r2, fp
 80aa2c2:	dd04      	ble.n	80aa2ce <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1c2>
 80aa2c4:	f818 bb01 	ldrb.w	fp, [r8], #1
 80aa2c8:	f80a bb01 	strb.w	fp, [sl], #1
 80aa2cc:	e7f6      	b.n	80aa2bc <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1b0>
 80aa2ce:	9c01      	ldr	r4, [sp, #4]
  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80aa2d0:	f10c 0c01 	add.w	ip, ip, #1
 80aa2d4:	4413      	add	r3, r2
 80aa2d6:	44a6      	add	lr, r4
 80aa2d8:	1abf      	subs	r7, r7, r2
 80aa2da:	e7eb      	b.n	80aa2b4 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1a8>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
 80aa2dc:	3001      	adds	r0, #1
 80aa2de:	4416      	add	r6, r2
 80aa2e0:	e7d7      	b.n	80aa292 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x186>
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
 80aa2e2:	689f      	ldr	r7, [r3, #8]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
 80aa2e4:	6808      	ldr	r0, [r1, #0]
 80aa2e6:	46bc      	mov	ip, r7
  const TfLiteIntArray* input_dims = input0->dims;
 80aa2e8:	6840      	ldr	r0, [r0, #4]
 80aa2ea:	2638      	movs	r6, #56	; 0x38
 80aa2ec:	fb06 5500 	mla	r5, r6, r0, r5
    case kTfLiteUInt8: {
      return PackImpl<uint8_t>(context, node, output, data->values_count,
                               data->axis);
    }
    case kTfLiteInt8: {
      return PackImpl<int8_t>(context, node, output, data->values_count,
 80aa2f0:	f8d2 9000 	ldr.w	r9, [r2]
                              data->axis);
 80aa2f4:	6852      	ldr	r2, [r2, #4]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
 80aa2f6:	f8d7 e000 	ldr.w	lr, [r7]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
 80aa2fa:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
 80aa2fc:	68ae      	ldr	r6, [r5, #8]
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
    axis += dimensions;
 80aa2fe:	bfb8      	it	lt
 80aa300:	4472      	addlt	r2, lr
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80aa302:	2000      	movs	r0, #0

  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
 80aa304:	2501      	movs	r5, #1
  for (int i = 0; i < axis; ++i) {
 80aa306:	4282      	cmp	r2, r0
 80aa308:	dd05      	ble.n	80aa316 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x20a>
    outer_size *= output_dims->data[i];
 80aa30a:	f85c 8f04 	ldr.w	r8, [ip, #4]!
  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80aa30e:	3001      	adds	r0, #1
    outer_size *= output_dims->data[i];
 80aa310:	fb08 f505 	mul.w	r5, r8, r5
 80aa314:	e7f7      	b.n	80aa306 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1fa>
 80aa316:	1c50      	adds	r0, r2, #1
  }
  int copy_size = 1;
 80aa318:	2201      	movs	r2, #1
  for (int i = axis + 1; i < dimensions; ++i) {
 80aa31a:	4586      	cmp	lr, r0
 80aa31c:	f100 0c01 	add.w	ip, r0, #1
 80aa320:	dd04      	ble.n	80aa32c <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x220>
    copy_size *= output_dims->data[i];
 80aa322:	f857 002c 	ldr.w	r0, [r7, ip, lsl #2]
 80aa326:	4342      	muls	r2, r0
 80aa328:	4660      	mov	r0, ip
 80aa32a:	e7f6      	b.n	80aa31a <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x20e>
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
 80aa32c:	f8d6 c000 	ldr.w	ip, [r6]
 80aa330:	4630      	mov	r0, r6
 80aa332:	2700      	movs	r7, #0
 80aa334:	2601      	movs	r6, #1
 80aa336:	45bc      	cmp	ip, r7
 80aa338:	dd05      	ble.n	80aa346 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x23a>
    input_size *= input_dims->data[i];
 80aa33a:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= output_dims->data[i];
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
 80aa33e:	3701      	adds	r7, #1
    input_size *= input_dims->data[i];
 80aa340:	fb0e f606 	mul.w	r6, lr, r6
 80aa344:	e7f7      	b.n	80aa336 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x22a>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);
 80aa346:	fb02 f005 	mul.w	r0, r2, r5
 80aa34a:	4286      	cmp	r6, r0
 80aa34c:	f47f af2f 	bne.w	80aa1ae <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xa2>
 80aa350:	685e      	ldr	r6, [r3, #4]
 80aa352:	fb02 f309 	mul.w	r3, r2, r9

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
 80aa356:	2000      	movs	r0, #0
 80aa358:	9301      	str	r3, [sp, #4]
 80aa35a:	4581      	cmp	r9, r0
 80aa35c:	f77f af37 	ble.w	80aa1ce <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc2>
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
 80aa360:	680b      	ldr	r3, [r1, #0]
 80aa362:	2438      	movs	r4, #56	; 0x38
 80aa364:	eb03 0380 	add.w	r3, r3, r0, lsl #2
 80aa368:	685f      	ldr	r7, [r3, #4]
 80aa36a:	9b00      	ldr	r3, [sp, #0]
 80aa36c:	689b      	ldr	r3, [r3, #8]
 80aa36e:	fb04 3307 	mla	r3, r4, r7, r3
 80aa372:	b103      	cbz	r3, 80aa376 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x26a>
 80aa374:	685b      	ldr	r3, [r3, #4]
 80aa376:	46b6      	mov	lr, r6
 80aa378:	425f      	negs	r7, r3
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80aa37a:	f04f 0c00 	mov.w	ip, #0
 80aa37e:	4565      	cmp	r5, ip
 80aa380:	dd11      	ble.n	80aa3a6 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x29a>
 80aa382:	46f2      	mov	sl, lr
 80aa384:	4698      	mov	r8, r3
      const T* input_ptr = input_data + copy_size * k;
      int loc = k * values_count * copy_size + i * copy_size;
      T* output_ptr = output_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
 80aa386:	eb08 0b07 	add.w	fp, r8, r7
 80aa38a:	455a      	cmp	r2, fp
 80aa38c:	dd04      	ble.n	80aa398 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x28c>
 80aa38e:	f918 bb01 	ldrsb.w	fp, [r8], #1
 80aa392:	f80a bb01 	strb.w	fp, [sl], #1
 80aa396:	e7f6      	b.n	80aa386 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x27a>
 80aa398:	9c01      	ldr	r4, [sp, #4]
  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80aa39a:	f10c 0c01 	add.w	ip, ip, #1
 80aa39e:	4413      	add	r3, r2
 80aa3a0:	44a6      	add	lr, r4
 80aa3a2:	1abf      	subs	r7, r7, r2
 80aa3a4:	e7eb      	b.n	80aa37e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x272>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
 80aa3a6:	3001      	adds	r0, #1
 80aa3a8:	4416      	add	r6, r2
 80aa3aa:	e7d6      	b.n	80aa35a <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x24e>
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
 80aa3ac:	689e      	ldr	r6, [r3, #8]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
 80aa3ae:	6808      	ldr	r0, [r1, #0]
 80aa3b0:	46b4      	mov	ip, r6
  const TfLiteIntArray* input_dims = input0->dims;
 80aa3b2:	6840      	ldr	r0, [r0, #4]
 80aa3b4:	2738      	movs	r7, #56	; 0x38
 80aa3b6:	fb07 5500 	mla	r5, r7, r0, r5
    case kTfLiteInt8: {
      return PackImpl<int8_t>(context, node, output, data->values_count,
                              data->axis);
    }
    case kTfLiteInt32: {
      return PackImpl<int32_t>(context, node, output, data->values_count,
 80aa3ba:	f8d2 8000 	ldr.w	r8, [r2]
                               data->axis);
 80aa3be:	6852      	ldr	r2, [r2, #4]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
 80aa3c0:	f8d6 e000 	ldr.w	lr, [r6]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
 80aa3c4:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
 80aa3c6:	68ad      	ldr	r5, [r5, #8]
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
    axis += dimensions;
 80aa3c8:	bfb8      	it	lt
 80aa3ca:	4472      	addlt	r2, lr
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80aa3cc:	2000      	movs	r0, #0

  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
 80aa3ce:	2701      	movs	r7, #1
  for (int i = 0; i < axis; ++i) {
 80aa3d0:	4282      	cmp	r2, r0
 80aa3d2:	dd05      	ble.n	80aa3e0 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2d4>
    outer_size *= output_dims->data[i];
 80aa3d4:	f85c 9f04 	ldr.w	r9, [ip, #4]!
  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80aa3d8:	3001      	adds	r0, #1
    outer_size *= output_dims->data[i];
 80aa3da:	fb09 f707 	mul.w	r7, r9, r7
 80aa3de:	e7f7      	b.n	80aa3d0 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2c4>
 80aa3e0:	1c50      	adds	r0, r2, #1
  }
  int copy_size = 1;
 80aa3e2:	2201      	movs	r2, #1
  for (int i = axis + 1; i < dimensions; ++i) {
 80aa3e4:	4586      	cmp	lr, r0
 80aa3e6:	f100 0c01 	add.w	ip, r0, #1
 80aa3ea:	dd04      	ble.n	80aa3f6 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2ea>
    copy_size *= output_dims->data[i];
 80aa3ec:	f856 002c 	ldr.w	r0, [r6, ip, lsl #2]
 80aa3f0:	4342      	muls	r2, r0
 80aa3f2:	4660      	mov	r0, ip
 80aa3f4:	e7f6      	b.n	80aa3e4 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2d8>
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
 80aa3f6:	f8d5 c000 	ldr.w	ip, [r5]
 80aa3fa:	4628      	mov	r0, r5
 80aa3fc:	2600      	movs	r6, #0
 80aa3fe:	2501      	movs	r5, #1
 80aa400:	45b4      	cmp	ip, r6
 80aa402:	dd05      	ble.n	80aa410 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x304>
    input_size *= input_dims->data[i];
 80aa404:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= output_dims->data[i];
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
 80aa408:	3601      	adds	r6, #1
    input_size *= input_dims->data[i];
 80aa40a:	fb0e f505 	mul.w	r5, lr, r5
 80aa40e:	e7f7      	b.n	80aa400 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2f4>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);
 80aa410:	fb02 f007 	mul.w	r0, r2, r7
 80aa414:	4285      	cmp	r5, r0
 80aa416:	f47f aeca 	bne.w	80aa1ae <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xa2>
 80aa41a:	2500      	movs	r5, #0

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
 80aa41c:	462e      	mov	r6, r5
 80aa41e:	685b      	ldr	r3, [r3, #4]
 80aa420:	ea4f 0c82 	mov.w	ip, r2, lsl #2
 80aa424:	9301      	str	r3, [sp, #4]
 80aa426:	fb02 f308 	mul.w	r3, r2, r8
 80aa42a:	009b      	lsls	r3, r3, #2
 80aa42c:	9302      	str	r3, [sp, #8]
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
 80aa42e:	f04f 0938 	mov.w	r9, #56	; 0x38
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
 80aa432:	45b0      	cmp	r8, r6
 80aa434:	f77f aecb 	ble.w	80aa1ce <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc2>
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
 80aa438:	680b      	ldr	r3, [r1, #0]
 80aa43a:	eb03 0386 	add.w	r3, r3, r6, lsl #2
 80aa43e:	6858      	ldr	r0, [r3, #4]
 80aa440:	9b00      	ldr	r3, [sp, #0]
 80aa442:	689b      	ldr	r3, [r3, #8]
 80aa444:	fb09 3300 	mla	r3, r9, r0, r3
 80aa448:	b103      	cbz	r3, 80aa44c <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x340>
 80aa44a:	685b      	ldr	r3, [r3, #4]
 80aa44c:	f04f 0e00 	mov.w	lr, #0
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80aa450:	46f2      	mov	sl, lr
 80aa452:	4557      	cmp	r7, sl
 80aa454:	dd12      	ble.n	80aa47c <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x370>
      const T* input_ptr = input_data + copy_size * k;
      int loc = k * values_count * copy_size + i * copy_size;
      T* output_ptr = output_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
 80aa456:	9c01      	ldr	r4, [sp, #4]
 80aa458:	eb05 0b0e 	add.w	fp, r5, lr
  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80aa45c:	2000      	movs	r0, #0
      const T* input_ptr = input_data + copy_size * k;
      int loc = k * values_count * copy_size + i * copy_size;
      T* output_ptr = output_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
 80aa45e:	44a3      	add	fp, r4
 80aa460:	4282      	cmp	r2, r0
 80aa462:	dd05      	ble.n	80aa470 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x364>
 80aa464:	f853 4020 	ldr.w	r4, [r3, r0, lsl #2]
 80aa468:	f84b 4020 	str.w	r4, [fp, r0, lsl #2]
 80aa46c:	3001      	adds	r0, #1
 80aa46e:	e7f7      	b.n	80aa460 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x354>
 80aa470:	9802      	ldr	r0, [sp, #8]
  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80aa472:	f10a 0a01 	add.w	sl, sl, #1
 80aa476:	4463      	add	r3, ip
 80aa478:	4486      	add	lr, r0
 80aa47a:	e7ea      	b.n	80aa452 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x346>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
 80aa47c:	3601      	adds	r6, #1
 80aa47e:	4465      	add	r5, ip
 80aa480:	e7d7      	b.n	80aa432 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x326>
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
 80aa482:	689e      	ldr	r6, [r3, #8]
    case kTfLiteInt32: {
      return PackImpl<int32_t>(context, node, output, data->values_count,
                               data->axis);
    }
    case kTfLiteInt64: {
      return PackImpl<int64_t>(context, node, output, data->values_count,
 80aa484:	6810      	ldr	r0, [r2, #0]
 80aa486:	46b4      	mov	ip, r6
 80aa488:	9001      	str	r0, [sp, #4]

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
 80aa48a:	6808      	ldr	r0, [r1, #0]
  const TfLiteIntArray* input_dims = input0->dims;
 80aa48c:	f04f 0e38 	mov.w	lr, #56	; 0x38
 80aa490:	6840      	ldr	r0, [r0, #4]
      return PackImpl<int32_t>(context, node, output, data->values_count,
                               data->axis);
    }
    case kTfLiteInt64: {
      return PackImpl<int64_t>(context, node, output, data->values_count,
                               data->axis);
 80aa492:	6852      	ldr	r2, [r2, #4]
template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
 80aa494:	fb0e 5500 	mla	r5, lr, r0, r5
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
 80aa498:	6837      	ldr	r7, [r6, #0]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
 80aa49a:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
 80aa49c:	68ad      	ldr	r5, [r5, #8]
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
    axis += dimensions;
 80aa49e:	bfb8      	it	lt
 80aa4a0:	19d2      	addlt	r2, r2, r7
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80aa4a2:	2000      	movs	r0, #0

  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
 80aa4a4:	f04f 0e01 	mov.w	lr, #1
  for (int i = 0; i < axis; ++i) {
 80aa4a8:	4282      	cmp	r2, r0
 80aa4aa:	dd05      	ble.n	80aa4b8 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3ac>
    outer_size *= output_dims->data[i];
 80aa4ac:	f85c 8f04 	ldr.w	r8, [ip, #4]!
  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80aa4b0:	3001      	adds	r0, #1
    outer_size *= output_dims->data[i];
 80aa4b2:	fb08 fe0e 	mul.w	lr, r8, lr
 80aa4b6:	e7f7      	b.n	80aa4a8 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x39c>
 80aa4b8:	1c50      	adds	r0, r2, #1
  }
  int copy_size = 1;
 80aa4ba:	2201      	movs	r2, #1
  for (int i = axis + 1; i < dimensions; ++i) {
 80aa4bc:	4287      	cmp	r7, r0
 80aa4be:	f100 0c01 	add.w	ip, r0, #1
 80aa4c2:	dd04      	ble.n	80aa4ce <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3c2>
    copy_size *= output_dims->data[i];
 80aa4c4:	f856 002c 	ldr.w	r0, [r6, ip, lsl #2]
 80aa4c8:	4342      	muls	r2, r0
 80aa4ca:	4660      	mov	r0, ip
 80aa4cc:	e7f6      	b.n	80aa4bc <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3b0>
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
 80aa4ce:	f8d5 c000 	ldr.w	ip, [r5]
 80aa4d2:	4628      	mov	r0, r5
 80aa4d4:	2600      	movs	r6, #0
 80aa4d6:	2501      	movs	r5, #1
 80aa4d8:	45b4      	cmp	ip, r6
 80aa4da:	dd04      	ble.n	80aa4e6 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3da>
    input_size *= input_dims->data[i];
 80aa4dc:	f850 7f04 	ldr.w	r7, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= output_dims->data[i];
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
 80aa4e0:	3601      	adds	r6, #1
    input_size *= input_dims->data[i];
 80aa4e2:	437d      	muls	r5, r7
 80aa4e4:	e7f8      	b.n	80aa4d8 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3cc>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);
 80aa4e6:	fb02 f00e 	mul.w	r0, r2, lr
 80aa4ea:	4285      	cmp	r5, r0
 80aa4ec:	f47f ae5f 	bne.w	80aa1ae <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xa2>
 80aa4f0:	2000      	movs	r0, #0

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
 80aa4f2:	4605      	mov	r5, r0
 80aa4f4:	685b      	ldr	r3, [r3, #4]
 80aa4f6:	00d7      	lsls	r7, r2, #3
 80aa4f8:	9302      	str	r3, [sp, #8]
 80aa4fa:	9b01      	ldr	r3, [sp, #4]
 80aa4fc:	4353      	muls	r3, r2
 80aa4fe:	00db      	lsls	r3, r3, #3
 80aa500:	9303      	str	r3, [sp, #12]
 80aa502:	9b01      	ldr	r3, [sp, #4]
 80aa504:	42ab      	cmp	r3, r5
 80aa506:	f77f ae62 	ble.w	80aa1ce <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc2>
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
 80aa50a:	680b      	ldr	r3, [r1, #0]
 80aa50c:	2438      	movs	r4, #56	; 0x38
 80aa50e:	eb03 0385 	add.w	r3, r3, r5, lsl #2
 80aa512:	685e      	ldr	r6, [r3, #4]
 80aa514:	9b00      	ldr	r3, [sp, #0]
 80aa516:	689b      	ldr	r3, [r3, #8]
 80aa518:	fb04 3306 	mla	r3, r4, r6, r3
 80aa51c:	b103      	cbz	r3, 80aa520 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x414>
 80aa51e:	685b      	ldr	r3, [r3, #4]
 80aa520:	f04f 0c00 	mov.w	ip, #0
 80aa524:	461e      	mov	r6, r3
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80aa526:	46e1      	mov	r9, ip
 80aa528:	45ce      	cmp	lr, r9
 80aa52a:	dd15      	ble.n	80aa558 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x44c>
 80aa52c:	46b3      	mov	fp, r6
 80aa52e:	9c02      	ldr	r4, [sp, #8]
 80aa530:	eb00 080c 	add.w	r8, r0, ip
 80aa534:	44a0      	add	r8, r4
      const T* input_ptr = input_data + copy_size * k;
      int loc = k * values_count * copy_size + i * copy_size;
      T* output_ptr = output_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
 80aa536:	f04f 0a00 	mov.w	sl, #0
 80aa53a:	4552      	cmp	r2, sl
 80aa53c:	dd06      	ble.n	80aa54c <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x440>
 80aa53e:	e8fb 3402 	ldrd	r3, r4, [fp], #8
 80aa542:	f10a 0a01 	add.w	sl, sl, #1
 80aa546:	e8e8 3402 	strd	r3, r4, [r8], #8
 80aa54a:	e7f6      	b.n	80aa53a <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x42e>
 80aa54c:	9c03      	ldr	r4, [sp, #12]
  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80aa54e:	f109 0901 	add.w	r9, r9, #1
 80aa552:	443e      	add	r6, r7
 80aa554:	44a4      	add	ip, r4
 80aa556:	e7e7      	b.n	80aa528 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x41c>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
 80aa558:	3501      	adds	r5, #1
 80aa55a:	4438      	add	r0, r7
 80aa55c:	e7d1      	b.n	80aa502 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3f6>
    case kTfLiteInt64: {
      return PackImpl<int64_t>(context, node, output, data->values_count,
                               data->axis);
    }
    default: {
      context->ReportError(context, "Type '%s' is not supported by pack.",
 80aa55e:	9b00      	ldr	r3, [sp, #0]
 80aa560:	695d      	ldr	r5, [r3, #20]
 80aa562:	f7f5 fddb 	bl	80a011c <TfLiteTypeGetName>
                           TfLiteTypeGetName(output->type));
 80aa566:	4904      	ldr	r1, [pc, #16]	; (80aa578 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x46c>)
 80aa568:	4602      	mov	r2, r0
 80aa56a:	9800      	ldr	r0, [sp, #0]
 80aa56c:	47a8      	blx	r5
      return kTfLiteError;
 80aa56e:	2001      	movs	r0, #1
    }
  }

  return kTfLiteOk;
}
 80aa570:	b005      	add	sp, #20
 80aa572:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80aa576:	bf00      	nop
 80aa578:	080b6256 	.word	0x080b6256

080aa57c <_ZN6tflite3ops5micro13Register_PACKEv>:
}  // namespace pack

TfLiteRegistration* Register_PACK() {
  static TfLiteRegistration r = {nullptr, nullptr, pack::Prepare, pack::Eval};
  return &r;
}
 80aa57c:	4800      	ldr	r0, [pc, #0]	; (80aa580 <_ZN6tflite3ops5micro13Register_PACKEv+0x4>)
 80aa57e:	4770      	bx	lr
 80aa580:	20000388 	.word	0x20000388

080aa584 <_ZN6tflite3ops5micro7pooling4InitEP13TfLiteContextPKcj>:

}  // namespace

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
 80aa584:	2000      	movs	r0, #0
 80aa586:	4770      	bx	lr

080aa588 <_ZN6tflite3ops5micro7pooling4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
 80aa588:	4770      	bx	lr

080aa58a <_ZN6tflite3ops5micro7pooling7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
 80aa58a:	2000      	movs	r0, #0
 80aa58c:	4770      	bx	lr

080aa58e <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf>:
namespace reference_ops {

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape,
                        const float* input_data,
                        const RuntimeShape& output_shape, float* output_data) {
 80aa58e:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80aa592:	469a      	mov	sl, r3
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
 80aa594:	680b      	ldr	r3, [r1, #0]
namespace reference_ops {

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape,
                        const float* input_data,
                        const RuntimeShape& output_shape, float* output_data) {
 80aa596:	b097      	sub	sp, #92	; 0x5c
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
 80aa598:	2b04      	cmp	r3, #4
namespace reference_ops {

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape,
                        const float* input_data,
                        const RuntimeShape& output_shape, float* output_data) {
 80aa59a:	4604      	mov	r4, r0
 80aa59c:	4689      	mov	r9, r1
 80aa59e:	9214      	str	r2, [sp, #80]	; 0x50
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
 80aa5a0:	d001      	beq.n	80aa5a6 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x18>
 80aa5a2:	f005 fd45 	bl	80b0030 <abort>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
 80aa5a6:	f8da 3000 	ldr.w	r3, [sl]
 80aa5aa:	2b04      	cmp	r3, #4
 80aa5ac:	d1f9      	bne.n	80aa5a2 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x14>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80aa5ae:	2300      	movs	r3, #0
 80aa5b0:	4619      	mov	r1, r3
 80aa5b2:	4652      	mov	r2, sl
 80aa5b4:	4648      	mov	r0, r9
 80aa5b6:	f7fd fa62 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
 80aa5ba:	2303      	movs	r3, #3
 80aa5bc:	4619      	mov	r1, r3
 80aa5be:	4652      	mov	r2, sl
                        const RuntimeShape& input_shape,
                        const float* input_data,
                        const RuntimeShape& output_shape, float* output_data) {
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80aa5c0:	900a      	str	r0, [sp, #40]	; 0x28
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
 80aa5c2:	4648      	mov	r0, r9
 80aa5c4:	f7fd fa5b 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
 80aa5c8:	2101      	movs	r1, #1
                        const float* input_data,
                        const RuntimeShape& output_shape, float* output_data) {
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
 80aa5ca:	900b      	str	r0, [sp, #44]	; 0x2c
  const int input_height = input_shape.Dims(1);
 80aa5cc:	4648      	mov	r0, r9
 80aa5ce:	f7f7 fefb 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
 80aa5d2:	2102      	movs	r1, #2
                        const RuntimeShape& output_shape, float* output_data) {
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
 80aa5d4:	900c      	str	r0, [sp, #48]	; 0x30
  const int input_width = input_shape.Dims(2);
 80aa5d6:	4648      	mov	r0, r9
 80aa5d8:	f7f7 fef6 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
 80aa5dc:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
 80aa5de:	900d      	str	r0, [sp, #52]	; 0x34
  const int output_height = output_shape.Dims(1);
 80aa5e0:	4650      	mov	r0, sl
 80aa5e2:	f7f7 fef1 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
 80aa5e6:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
 80aa5e8:	900e      	str	r0, [sp, #56]	; 0x38
  const int output_width = output_shape.Dims(2);
 80aa5ea:	4650      	mov	r0, sl
 80aa5ec:	f7f7 feec 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int stride_height = params.stride_height;
 80aa5f0:	68e3      	ldr	r3, [r4, #12]
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
 80aa5f2:	9010      	str	r0, [sp, #64]	; 0x40
  const int stride_height = params.stride_height;
 80aa5f4:	9311      	str	r3, [sp, #68]	; 0x44
  const int stride_width = params.stride_width;
 80aa5f6:	6923      	ldr	r3, [r4, #16]
  for (int batch = 0; batch < batches; ++batch) {
 80aa5f8:	f04f 0b00 	mov.w	fp, #0
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
 80aa5fc:	9312      	str	r3, [sp, #72]	; 0x48
  for (int batch = 0; batch < batches; ++batch) {
 80aa5fe:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80aa600:	459b      	cmp	fp, r3
 80aa602:	f280 8092 	bge.w	80aa72a <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x19c>
 80aa606:	2300      	movs	r3, #0
 80aa608:	9305      	str	r3, [sp, #20]
 80aa60a:	9303      	str	r3, [sp, #12]
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80aa60c:	9b03      	ldr	r3, [sp, #12]
 80aa60e:	9a0e      	ldr	r2, [sp, #56]	; 0x38
 80aa610:	4293      	cmp	r3, r2
 80aa612:	f280 8087 	bge.w	80aa724 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x196>
 80aa616:	2300      	movs	r3, #0
 80aa618:	9306      	str	r3, [sp, #24]
 80aa61a:	9304      	str	r3, [sp, #16]
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80aa61c:	9b04      	ldr	r3, [sp, #16]
 80aa61e:	9a10      	ldr	r2, [sp, #64]	; 0x40
 80aa620:	4293      	cmp	r3, r2
 80aa622:	da77      	bge.n	80aa714 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x186>
 80aa624:	2300      	movs	r3, #0
 80aa626:	9302      	str	r3, [sp, #8]
        for (int channel = 0; channel < depth; ++channel) {
 80aa628:	9b02      	ldr	r3, [sp, #8]
 80aa62a:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
 80aa62c:	4293      	cmp	r3, r2
 80aa62e:	da69      	bge.n	80aa704 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x176>
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
 80aa630:	9a06      	ldr	r2, [sp, #24]
 80aa632:	f9b4 3002 	ldrsh.w	r3, [r4, #2]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
 80aa636:	f9b4 6004 	ldrsh.w	r6, [r4, #4]
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
 80aa63a:	1ad3      	subs	r3, r2, r3
 80aa63c:	9308      	str	r3, [sp, #32]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
 80aa63e:	9b05      	ldr	r3, [sp, #20]
 80aa640:	9a08      	ldr	r2, [sp, #32]
 80aa642:	1b9e      	subs	r6, r3, r6
 80aa644:	9b08      	ldr	r3, [sp, #32]
 80aa646:	4275      	negs	r5, r6
 80aa648:	425b      	negs	r3, r3
 80aa64a:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
 80aa64e:	9309      	str	r3, [sp, #36]	; 0x24
 80aa650:	9b0d      	ldr	r3, [sp, #52]	; 0x34
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float total = 0.f;
          float filter_count = 0;
 80aa652:	2700      	movs	r7, #0
 80aa654:	1a9a      	subs	r2, r3, r2
 80aa656:	69a3      	ldr	r3, [r4, #24]
 80aa658:	ea25 75e5 	bic.w	r5, r5, r5, asr #31
 80aa65c:	429a      	cmp	r2, r3
 80aa65e:	bfa8      	it	ge
 80aa660:	461a      	movge	r2, r3
 80aa662:	9b0c      	ldr	r3, [sp, #48]	; 0x30
 80aa664:	920f      	str	r2, [sp, #60]	; 0x3c
 80aa666:	6962      	ldr	r2, [r4, #20]
 80aa668:	1b9b      	subs	r3, r3, r6
 80aa66a:	4293      	cmp	r3, r2
 80aa66c:	bfa8      	it	ge
 80aa66e:	4613      	movge	r3, r2
          const int filter_x_end =
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float total = 0.f;
 80aa670:	9707      	str	r7, [sp, #28]
 80aa672:	9313      	str	r3, [sp, #76]	; 0x4c
          float filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
 80aa674:	9b13      	ldr	r3, [sp, #76]	; 0x4c
 80aa676:	429d      	cmp	r5, r3
 80aa678:	da21      	bge.n	80aa6be <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x130>
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              total +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
 80aa67a:	1973      	adds	r3, r6, r5
 80aa67c:	f8dd 8024 	ldr.w	r8, [sp, #36]	; 0x24
 80aa680:	9315      	str	r3, [sp, #84]	; 0x54
              std::min(params.filter_height, input_height - in_y_origin);
          float total = 0.f;
          float filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
 80aa682:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80aa684:	4598      	cmp	r8, r3
 80aa686:	da18      	bge.n	80aa6ba <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x12c>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              total +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
 80aa688:	9b02      	ldr	r3, [sp, #8]
 80aa68a:	9a15      	ldr	r2, [sp, #84]	; 0x54
 80aa68c:	9300      	str	r3, [sp, #0]
 80aa68e:	9b08      	ldr	r3, [sp, #32]
 80aa690:	4659      	mov	r1, fp
 80aa692:	4443      	add	r3, r8
 80aa694:	4648      	mov	r0, r9
 80aa696:	f7f7 fefc 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80aa69a:	9b14      	ldr	r3, [sp, #80]	; 0x50
              std::min(params.filter_height, input_height - in_y_origin);
          float total = 0.f;
          float filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
 80aa69c:	f108 0801 	add.w	r8, r8, #1
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              total +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
 80aa6a0:	f853 1020 	ldr.w	r1, [r3, r0, lsl #2]
 80aa6a4:	9807      	ldr	r0, [sp, #28]
 80aa6a6:	f008 fe99 	bl	80b33dc <__addsf3>
              filter_count++;
 80aa6aa:	f04f 517e 	mov.w	r1, #1065353216	; 0x3f800000
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              total +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
 80aa6ae:	9007      	str	r0, [sp, #28]
              filter_count++;
 80aa6b0:	4638      	mov	r0, r7
 80aa6b2:	f008 fe93 	bl	80b33dc <__addsf3>
 80aa6b6:	4607      	mov	r7, r0
              std::min(params.filter_height, input_height - in_y_origin);
          float total = 0.f;
          float filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
 80aa6b8:	e7e3      	b.n	80aa682 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0xf4>
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float total = 0.f;
          float filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
 80aa6ba:	3501      	adds	r5, #1
 80aa6bc:	e7da      	b.n	80aa674 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0xe6>
              total +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
              filter_count++;
            }
          }
          const float average = total / filter_count;
 80aa6be:	4639      	mov	r1, r7
 80aa6c0:	9807      	ldr	r0, [sp, #28]
 80aa6c2:	f009 f847 	bl	80b3754 <__aeabi_fdiv>
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
 80aa6c6:	9b02      	ldr	r3, [sp, #8]
              total +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
              filter_count++;
            }
          }
          const float average = total / filter_count;
 80aa6c8:	4605      	mov	r5, r0
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
 80aa6ca:	9300      	str	r3, [sp, #0]
 80aa6cc:	4659      	mov	r1, fp
 80aa6ce:	9b04      	ldr	r3, [sp, #16]
 80aa6d0:	9a03      	ldr	r2, [sp, #12]
 80aa6d2:	4650      	mov	r0, sl
 80aa6d4:	f7f7 fedd 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              ActivationFunctionWithMinMax(average, params.float_activation_min,
 80aa6d8:	f8d4 8024 	ldr.w	r8, [r4, #36]	; 0x24
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
              filter_count++;
            }
          }
          const float average = total / filter_count;
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
 80aa6dc:	4607      	mov	r7, r0
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
 80aa6de:	4641      	mov	r1, r8
 80aa6e0:	4628      	mov	r0, r5
              ActivationFunctionWithMinMax(average, params.float_activation_min,
                                           params.float_activation_max);
 80aa6e2:	6aa6      	ldr	r6, [r4, #40]	; 0x28
 80aa6e4:	f009 f920 	bl	80b3928 <__aeabi_fcmplt>
 80aa6e8:	b100      	cbz	r0, 80aa6ec <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x15e>
	return __b;
 80aa6ea:	4645      	mov	r5, r8
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
 80aa6ec:	4629      	mov	r1, r5
 80aa6ee:	4630      	mov	r0, r6
 80aa6f0:	f009 f91a 	bl	80b3928 <__aeabi_fcmplt>
 80aa6f4:	b100      	cbz	r0, 80aa6f8 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x16a>
	return __b;
 80aa6f6:	4635      	mov	r5, r6
 80aa6f8:	9b20      	ldr	r3, [sp, #128]	; 0x80
 80aa6fa:	f843 5027 	str.w	r5, [r3, r7, lsl #2]
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
 80aa6fe:	9b02      	ldr	r3, [sp, #8]
 80aa700:	3301      	adds	r3, #1
 80aa702:	e790      	b.n	80aa626 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x98>
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80aa704:	9b04      	ldr	r3, [sp, #16]
 80aa706:	9a12      	ldr	r2, [sp, #72]	; 0x48
 80aa708:	3301      	adds	r3, #1
 80aa70a:	9304      	str	r3, [sp, #16]
 80aa70c:	9b06      	ldr	r3, [sp, #24]
 80aa70e:	4413      	add	r3, r2
 80aa710:	9306      	str	r3, [sp, #24]
 80aa712:	e783      	b.n	80aa61c <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x8e>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80aa714:	9b03      	ldr	r3, [sp, #12]
 80aa716:	9a11      	ldr	r2, [sp, #68]	; 0x44
 80aa718:	3301      	adds	r3, #1
 80aa71a:	9303      	str	r3, [sp, #12]
 80aa71c:	9b05      	ldr	r3, [sp, #20]
 80aa71e:	4413      	add	r3, r2
 80aa720:	9305      	str	r3, [sp, #20]
 80aa722:	e773      	b.n	80aa60c <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x7e>
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
 80aa724:	f10b 0b01 	add.w	fp, fp, #1
 80aa728:	e769      	b.n	80aa5fe <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x70>
                                           params.float_activation_max);
        }
      }
    }
  }
}
 80aa72a:	b017      	add	sp, #92	; 0x5c
 80aa72c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080aa730 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph>:

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape,
                        const uint8* input_data,
                        const RuntimeShape& output_shape, uint8* output_data) {
 80aa730:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80aa734:	b097      	sub	sp, #92	; 0x5c
 80aa736:	9214      	str	r2, [sp, #80]	; 0x50
 80aa738:	461f      	mov	r7, r3
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
 80aa73a:	69c2      	ldr	r2, [r0, #28]
 80aa73c:	6a03      	ldr	r3, [r0, #32]
}

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape,
                        const uint8* input_data,
                        const RuntimeShape& output_shape, uint8* output_data) {
 80aa73e:	4604      	mov	r4, r0
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
 80aa740:	429a      	cmp	r2, r3
}

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape,
                        const uint8* input_data,
                        const RuntimeShape& output_shape, uint8* output_data) {
 80aa742:	460e      	mov	r6, r1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
 80aa744:	dd01      	ble.n	80aa74a <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x1a>

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape,
                        const uint8* input_data,
                        const RuntimeShape& output_shape, uint8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
 80aa746:	f005 fc73 	bl	80b0030 <abort>
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
 80aa74a:	680b      	ldr	r3, [r1, #0]
 80aa74c:	2b04      	cmp	r3, #4
 80aa74e:	d1fa      	bne.n	80aa746 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x16>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
 80aa750:	683b      	ldr	r3, [r7, #0]
 80aa752:	2b04      	cmp	r3, #4
 80aa754:	d1f7      	bne.n	80aa746 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x16>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80aa756:	2300      	movs	r3, #0
 80aa758:	4619      	mov	r1, r3
 80aa75a:	463a      	mov	r2, r7
 80aa75c:	4630      	mov	r0, r6
 80aa75e:	f7fd f98e 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
 80aa762:	2303      	movs	r3, #3
 80aa764:	4619      	mov	r1, r3
 80aa766:	463a      	mov	r2, r7
                        const RuntimeShape& output_shape, uint8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80aa768:	900a      	str	r0, [sp, #40]	; 0x28
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
 80aa76a:	4630      	mov	r0, r6
 80aa76c:	f7fd f987 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
 80aa770:	2101      	movs	r1, #1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
 80aa772:	900b      	str	r0, [sp, #44]	; 0x2c
  const int input_height = input_shape.Dims(1);
 80aa774:	4630      	mov	r0, r6
 80aa776:	f7f7 fe27 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
 80aa77a:	2102      	movs	r1, #2
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
 80aa77c:	900c      	str	r0, [sp, #48]	; 0x30
  const int input_width = input_shape.Dims(2);
 80aa77e:	4630      	mov	r0, r6
 80aa780:	f7f7 fe22 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
 80aa784:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
 80aa786:	900d      	str	r0, [sp, #52]	; 0x34
  const int output_height = output_shape.Dims(1);
 80aa788:	4638      	mov	r0, r7
 80aa78a:	f7f7 fe1d 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
 80aa78e:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
 80aa790:	900e      	str	r0, [sp, #56]	; 0x38
  const int output_width = output_shape.Dims(2);
 80aa792:	4638      	mov	r0, r7
 80aa794:	f7f7 fe18 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int stride_height = params.stride_height;
 80aa798:	68e3      	ldr	r3, [r4, #12]
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
 80aa79a:	900f      	str	r0, [sp, #60]	; 0x3c
  const int stride_height = params.stride_height;
 80aa79c:	9310      	str	r3, [sp, #64]	; 0x40
  const int stride_width = params.stride_width;
 80aa79e:	6923      	ldr	r3, [r4, #16]
  for (int batch = 0; batch < batches; ++batch) {
 80aa7a0:	f04f 0a00 	mov.w	sl, #0
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
 80aa7a4:	9311      	str	r3, [sp, #68]	; 0x44
  for (int batch = 0; batch < batches; ++batch) {
 80aa7a6:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80aa7a8:	459a      	cmp	sl, r3
 80aa7aa:	f280 8088 	bge.w	80aa8be <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x18e>
 80aa7ae:	2300      	movs	r3, #0
 80aa7b0:	9305      	str	r3, [sp, #20]
 80aa7b2:	9303      	str	r3, [sp, #12]
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80aa7b4:	9b03      	ldr	r3, [sp, #12]
 80aa7b6:	9a0e      	ldr	r2, [sp, #56]	; 0x38
 80aa7b8:	4293      	cmp	r3, r2
 80aa7ba:	da7d      	bge.n	80aa8b8 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x188>
 80aa7bc:	2300      	movs	r3, #0
 80aa7be:	9306      	str	r3, [sp, #24]
 80aa7c0:	9304      	str	r3, [sp, #16]
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80aa7c2:	9b04      	ldr	r3, [sp, #16]
 80aa7c4:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
 80aa7c6:	4293      	cmp	r3, r2
 80aa7c8:	da6e      	bge.n	80aa8a8 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x178>
 80aa7ca:	2300      	movs	r3, #0
 80aa7cc:	9302      	str	r3, [sp, #8]
        for (int channel = 0; channel < depth; ++channel) {
 80aa7ce:	9b02      	ldr	r3, [sp, #8]
 80aa7d0:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
 80aa7d2:	4293      	cmp	r3, r2
 80aa7d4:	da60      	bge.n	80aa898 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x168>
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
 80aa7d6:	9a06      	ldr	r2, [sp, #24]
 80aa7d8:	f9b4 3002 	ldrsh.w	r3, [r4, #2]
          const int filter_x_end =
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
 80aa7dc:	2500      	movs	r5, #0
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
 80aa7de:	1ad3      	subs	r3, r2, r3
 80aa7e0:	9308      	str	r3, [sp, #32]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
 80aa7e2:	9a05      	ldr	r2, [sp, #20]
 80aa7e4:	f9b4 3004 	ldrsh.w	r3, [r4, #4]
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
 80aa7e8:	46ab      	mov	fp, r5
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
 80aa7ea:	1ad3      	subs	r3, r2, r3
 80aa7ec:	9309      	str	r3, [sp, #36]	; 0x24
 80aa7ee:	9b08      	ldr	r3, [sp, #32]
 80aa7f0:	9a08      	ldr	r2, [sp, #32]
 80aa7f2:	425b      	negs	r3, r3
 80aa7f4:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
 80aa7f8:	9307      	str	r3, [sp, #28]
 80aa7fa:	9b0d      	ldr	r3, [sp, #52]	; 0x34
 80aa7fc:	1a9a      	subs	r2, r3, r2
 80aa7fe:	69a3      	ldr	r3, [r4, #24]
 80aa800:	429a      	cmp	r2, r3
 80aa802:	bfa8      	it	ge
 80aa804:	461a      	movge	r2, r3
 80aa806:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80aa808:	9213      	str	r2, [sp, #76]	; 0x4c
 80aa80a:	f1c3 0800 	rsb	r8, r3, #0
 80aa80e:	9a09      	ldr	r2, [sp, #36]	; 0x24
 80aa810:	9b0c      	ldr	r3, [sp, #48]	; 0x30
 80aa812:	ea28 78e8 	bic.w	r8, r8, r8, asr #31
 80aa816:	1a9a      	subs	r2, r3, r2
 80aa818:	6963      	ldr	r3, [r4, #20]
 80aa81a:	429a      	cmp	r2, r3
 80aa81c:	bfa8      	it	ge
 80aa81e:	461a      	movge	r2, r3
 80aa820:	9212      	str	r2, [sp, #72]	; 0x48
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
 80aa822:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80aa824:	4598      	cmp	r8, r3
 80aa826:	da1e      	bge.n	80aa866 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x136>
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
 80aa828:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80aa82a:	f8dd 901c 	ldr.w	r9, [sp, #28]
 80aa82e:	4443      	add	r3, r8
 80aa830:	ebc9 0b0b 	rsb	fp, r9, fp
 80aa834:	9315      	str	r3, [sp, #84]	; 0x54
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
 80aa836:	9a13      	ldr	r2, [sp, #76]	; 0x4c
 80aa838:	eb0b 0309 	add.w	r3, fp, r9
 80aa83c:	4591      	cmp	r9, r2
 80aa83e:	da0e      	bge.n	80aa85e <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x12e>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
 80aa840:	9b02      	ldr	r3, [sp, #8]
 80aa842:	9a15      	ldr	r2, [sp, #84]	; 0x54
 80aa844:	9300      	str	r3, [sp, #0]
 80aa846:	9b08      	ldr	r3, [sp, #32]
 80aa848:	4651      	mov	r1, sl
 80aa84a:	444b      	add	r3, r9
 80aa84c:	4630      	mov	r0, r6
 80aa84e:	f7f7 fe20 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80aa852:	9b14      	ldr	r3, [sp, #80]	; 0x50
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
 80aa854:	f109 0901 	add.w	r9, r9, #1
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
 80aa858:	5c1b      	ldrb	r3, [r3, r0]
 80aa85a:	441d      	add	r5, r3
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
 80aa85c:	e7eb      	b.n	80aa836 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x106>
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
 80aa85e:	f108 0801 	add.w	r8, r8, #1
 80aa862:	469b      	mov	fp, r3
 80aa864:	e7dd      	b.n	80aa822 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xf2>
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
              filter_count++;
            }
          }
          acc = (acc + filter_count / 2) / filter_count;
 80aa866:	eb05 056b 	add.w	r5, r5, fp, asr #1
 80aa86a:	fb95 fbfb 	sdiv	fp, r5, fp
 80aa86e:	69e5      	ldr	r5, [r4, #28]
 80aa870:	6a23      	ldr	r3, [r4, #32]
 80aa872:	455d      	cmp	r5, fp
 80aa874:	bfb8      	it	lt
 80aa876:	465d      	movlt	r5, fp
 80aa878:	429d      	cmp	r5, r3
 80aa87a:	bfa8      	it	ge
 80aa87c:	461d      	movge	r5, r3
          acc = std::max(acc, params.quantized_activation_min);
          acc = std::min(acc, params.quantized_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
 80aa87e:	9b02      	ldr	r3, [sp, #8]
 80aa880:	9a03      	ldr	r2, [sp, #12]
 80aa882:	9300      	str	r3, [sp, #0]
 80aa884:	4651      	mov	r1, sl
 80aa886:	9b04      	ldr	r3, [sp, #16]
 80aa888:	4638      	mov	r0, r7
 80aa88a:	f7f7 fe02 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<uint8>(acc);
 80aa88e:	9b20      	ldr	r3, [sp, #128]	; 0x80
 80aa890:	541d      	strb	r5, [r3, r0]
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
 80aa892:	9b02      	ldr	r3, [sp, #8]
 80aa894:	3301      	adds	r3, #1
 80aa896:	e799      	b.n	80aa7cc <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x9c>
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80aa898:	9b04      	ldr	r3, [sp, #16]
 80aa89a:	9a11      	ldr	r2, [sp, #68]	; 0x44
 80aa89c:	3301      	adds	r3, #1
 80aa89e:	9304      	str	r3, [sp, #16]
 80aa8a0:	9b06      	ldr	r3, [sp, #24]
 80aa8a2:	4413      	add	r3, r2
 80aa8a4:	9306      	str	r3, [sp, #24]
 80aa8a6:	e78c      	b.n	80aa7c2 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x92>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80aa8a8:	9b03      	ldr	r3, [sp, #12]
 80aa8aa:	9a10      	ldr	r2, [sp, #64]	; 0x40
 80aa8ac:	3301      	adds	r3, #1
 80aa8ae:	9303      	str	r3, [sp, #12]
 80aa8b0:	9b05      	ldr	r3, [sp, #20]
 80aa8b2:	4413      	add	r3, r2
 80aa8b4:	9305      	str	r3, [sp, #20]
 80aa8b6:	e77d      	b.n	80aa7b4 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x84>
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
 80aa8b8:	f10a 0a01 	add.w	sl, sl, #1
 80aa8bc:	e773      	b.n	80aa7a6 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x76>
              static_cast<uint8>(acc);
        }
      }
    }
  }
}
 80aa8be:	b017      	add	sp, #92	; 0x5c
 80aa8c0:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080aa8c4 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf>:
  }
}

inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const float* input_data, const RuntimeShape& output_shape,
                    float* output_data) {
 80aa8c4:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80aa8c8:	461e      	mov	r6, r3
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
 80aa8ca:	680b      	ldr	r3, [r1, #0]
  }
}

inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const float* input_data, const RuntimeShape& output_shape,
                    float* output_data) {
 80aa8cc:	b099      	sub	sp, #100	; 0x64
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
 80aa8ce:	2b04      	cmp	r3, #4
  }
}

inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const float* input_data, const RuntimeShape& output_shape,
                    float* output_data) {
 80aa8d0:	4604      	mov	r4, r0
 80aa8d2:	460d      	mov	r5, r1
 80aa8d4:	9209      	str	r2, [sp, #36]	; 0x24
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
 80aa8d6:	d001      	beq.n	80aa8dc <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x18>
 80aa8d8:	f005 fbaa 	bl	80b0030 <abort>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
 80aa8dc:	6833      	ldr	r3, [r6, #0]
 80aa8de:	2b04      	cmp	r3, #4
 80aa8e0:	d1fa      	bne.n	80aa8d8 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x14>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80aa8e2:	2300      	movs	r3, #0
 80aa8e4:	4619      	mov	r1, r3
 80aa8e6:	4632      	mov	r2, r6
 80aa8e8:	4628      	mov	r0, r5
 80aa8ea:	f7fd f8c8 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
 80aa8ee:	2303      	movs	r3, #3
 80aa8f0:	4619      	mov	r1, r3
 80aa8f2:	4632      	mov	r2, r6
inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const float* input_data, const RuntimeShape& output_shape,
                    float* output_data) {
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80aa8f4:	900a      	str	r0, [sp, #40]	; 0x28
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
 80aa8f6:	4628      	mov	r0, r5
 80aa8f8:	f7fd f8c1 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
 80aa8fc:	2101      	movs	r1, #1
                    const float* input_data, const RuntimeShape& output_shape,
                    float* output_data) {
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
 80aa8fe:	900b      	str	r0, [sp, #44]	; 0x2c
  const int input_height = input_shape.Dims(1);
 80aa900:	4628      	mov	r0, r5
 80aa902:	f7f7 fd61 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
 80aa906:	2102      	movs	r1, #2
                    float* output_data) {
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
 80aa908:	900c      	str	r0, [sp, #48]	; 0x30
  const int input_width = input_shape.Dims(2);
 80aa90a:	4628      	mov	r0, r5
 80aa90c:	f7f7 fd5c 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
 80aa910:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
 80aa912:	900d      	str	r0, [sp, #52]	; 0x34
  const int output_height = output_shape.Dims(1);
 80aa914:	4630      	mov	r0, r6
 80aa916:	f7f7 fd57 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
 80aa91a:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
 80aa91c:	900e      	str	r0, [sp, #56]	; 0x38
  const int output_width = output_shape.Dims(2);
 80aa91e:	4630      	mov	r0, r6
 80aa920:	f7f7 fd52 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int stride_height = params.stride_height;
 80aa924:	68e3      	ldr	r3, [r4, #12]
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
 80aa926:	900f      	str	r0, [sp, #60]	; 0x3c
  const int stride_height = params.stride_height;
 80aa928:	9310      	str	r3, [sp, #64]	; 0x40
  const int stride_width = params.stride_width;
 80aa92a:	6923      	ldr	r3, [r4, #16]
  for (int batch = 0; batch < batches; ++batch) {
 80aa92c:	f04f 0b00 	mov.w	fp, #0
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
 80aa930:	9311      	str	r3, [sp, #68]	; 0x44
  for (int batch = 0; batch < batches; ++batch) {
 80aa932:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80aa934:	459b      	cmp	fp, r3
 80aa936:	f280 8096 	bge.w	80aaa66 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x1a2>
 80aa93a:	2300      	movs	r3, #0
 80aa93c:	9306      	str	r3, [sp, #24]
 80aa93e:	9304      	str	r3, [sp, #16]
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80aa940:	9b04      	ldr	r3, [sp, #16]
 80aa942:	9a0e      	ldr	r2, [sp, #56]	; 0x38
 80aa944:	4293      	cmp	r3, r2
 80aa946:	f280 808b 	bge.w	80aaa60 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x19c>
 80aa94a:	2300      	movs	r3, #0
 80aa94c:	9307      	str	r3, [sp, #28]
 80aa94e:	9305      	str	r3, [sp, #20]
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80aa950:	9b05      	ldr	r3, [sp, #20]
 80aa952:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
 80aa954:	4293      	cmp	r3, r2
 80aa956:	da7b      	bge.n	80aaa50 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x18c>
 80aa958:	2300      	movs	r3, #0
 80aa95a:	9303      	str	r3, [sp, #12]
        for (int channel = 0; channel < depth; ++channel) {
 80aa95c:	9b03      	ldr	r3, [sp, #12]
 80aa95e:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
 80aa960:	4293      	cmp	r3, r2
 80aa962:	da6d      	bge.n	80aaa40 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x17c>
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
 80aa964:	9b07      	ldr	r3, [sp, #28]
 80aa966:	f9b4 8002 	ldrsh.w	r8, [r4, #2]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
 80aa96a:	f9b4 9004 	ldrsh.w	r9, [r4, #4]
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
 80aa96e:	ebc8 0803 	rsb	r8, r8, r3
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
 80aa972:	9b06      	ldr	r3, [sp, #24]
 80aa974:	ebc9 0903 	rsb	r9, r9, r3
 80aa978:	f1c8 0300 	rsb	r3, r8, #0
 80aa97c:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
 80aa980:	9308      	str	r3, [sp, #32]
 80aa982:	9b0d      	ldr	r3, [sp, #52]	; 0x34
 80aa984:	f1c9 0700 	rsb	r7, r9, #0
 80aa988:	ebc8 0203 	rsb	r2, r8, r3
 80aa98c:	69a3      	ldr	r3, [r4, #24]
 80aa98e:	ea27 77e7 	bic.w	r7, r7, r7, asr #31
 80aa992:	429a      	cmp	r2, r3
 80aa994:	bfa8      	it	ge
 80aa996:	461a      	movge	r2, r3
 80aa998:	9b0c      	ldr	r3, [sp, #48]	; 0x30
 80aa99a:	9212      	str	r2, [sp, #72]	; 0x48
 80aa99c:	ebc9 0203 	rsb	r2, r9, r3
 80aa9a0:	6963      	ldr	r3, [r4, #20]
 80aa9a2:	429a      	cmp	r2, r3
 80aa9a4:	bfa8      	it	ge
 80aa9a6:	461a      	movge	r2, r3
          const int filter_x_end =
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float max = std::numeric_limits<float>::lowest();
 80aa9a8:	f46f 0300 	mvn.w	r3, #8388608	; 0x800000
 80aa9ac:	9213      	str	r2, [sp, #76]	; 0x4c
 80aa9ae:	9317      	str	r3, [sp, #92]	; 0x5c
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
 80aa9b0:	9b13      	ldr	r3, [sp, #76]	; 0x4c
 80aa9b2:	429f      	cmp	r7, r3
 80aa9b4:	da24      	bge.n	80aaa00 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x13c>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
 80aa9b6:	eb07 0309 	add.w	r3, r7, r9
 80aa9ba:	f8dd a020 	ldr.w	sl, [sp, #32]
 80aa9be:	9314      	str	r3, [sp, #80]	; 0x50
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float max = std::numeric_limits<float>::lowest();
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
 80aa9c0:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80aa9c2:	459a      	cmp	sl, r3
 80aa9c4:	da1a      	bge.n	80aa9fc <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x138>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
 80aa9c6:	9b03      	ldr	r3, [sp, #12]
 80aa9c8:	9a14      	ldr	r2, [sp, #80]	; 0x50
 80aa9ca:	9300      	str	r3, [sp, #0]
 80aa9cc:	4659      	mov	r1, fp
 80aa9ce:	eb08 030a 	add.w	r3, r8, sl
 80aa9d2:	4628      	mov	r0, r5
 80aa9d4:	f7f7 fd5d 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80aa9d8:	9b09      	ldr	r3, [sp, #36]	; 0x24
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
 80aa9da:	9917      	ldr	r1, [sp, #92]	; 0x5c
 80aa9dc:	eb03 0380 	add.w	r3, r3, r0, lsl #2
 80aa9e0:	9315      	str	r3, [sp, #84]	; 0x54
 80aa9e2:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80aa9e4:	f853 0020 	ldr.w	r0, [r3, r0, lsl #2]
 80aa9e8:	f008 ffbc 	bl	80b3964 <__aeabi_fcmpgt>
 80aa9ec:	9b15      	ldr	r3, [sp, #84]	; 0x54
 80aa9ee:	b900      	cbnz	r0, 80aa9f2 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x12e>
	return __b;
      return __a;
 80aa9f0:	ab17      	add	r3, sp, #92	; 0x5c
 80aa9f2:	681b      	ldr	r3, [r3, #0]
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float max = std::numeric_limits<float>::lowest();
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
 80aa9f4:	f10a 0a01 	add.w	sl, sl, #1
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
 80aa9f8:	9317      	str	r3, [sp, #92]	; 0x5c
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float max = std::numeric_limits<float>::lowest();
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
 80aa9fa:	e7e1      	b.n	80aa9c0 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0xfc>
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float max = std::numeric_limits<float>::lowest();
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
 80aa9fc:	3701      	adds	r7, #1
 80aa9fe:	e7d7      	b.n	80aa9b0 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0xec>
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
            }
          }
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
 80aaa00:	9b03      	ldr	r3, [sp, #12]
 80aaa02:	4659      	mov	r1, fp
 80aaa04:	9300      	str	r3, [sp, #0]
 80aaa06:	9a04      	ldr	r2, [sp, #16]
 80aaa08:	9b05      	ldr	r3, [sp, #20]
 80aaa0a:	4630      	mov	r0, r6
 80aaa0c:	f7f7 fd41 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              ActivationFunctionWithMinMax(max, params.float_activation_min,
 80aaa10:	f8dd 905c 	ldr.w	r9, [sp, #92]	; 0x5c
 80aaa14:	f8d4 a024 	ldr.w	sl, [r4, #36]	; 0x24
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
            }
          }
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
 80aaa18:	4680      	mov	r8, r0
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
 80aaa1a:	4651      	mov	r1, sl
 80aaa1c:	4648      	mov	r0, r9
              ActivationFunctionWithMinMax(max, params.float_activation_min,
                                           params.float_activation_max);
 80aaa1e:	6aa7      	ldr	r7, [r4, #40]	; 0x28
 80aaa20:	f008 ff82 	bl	80b3928 <__aeabi_fcmplt>
 80aaa24:	b100      	cbz	r0, 80aaa28 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x164>
	return __b;
 80aaa26:	46d1      	mov	r9, sl
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
 80aaa28:	4649      	mov	r1, r9
 80aaa2a:	4638      	mov	r0, r7
 80aaa2c:	f008 ff7c 	bl	80b3928 <__aeabi_fcmplt>
 80aaa30:	b100      	cbz	r0, 80aaa34 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x170>
	return __b;
 80aaa32:	46b9      	mov	r9, r7
 80aaa34:	9b22      	ldr	r3, [sp, #136]	; 0x88
 80aaa36:	f843 9028 	str.w	r9, [r3, r8, lsl #2]
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
 80aaa3a:	9b03      	ldr	r3, [sp, #12]
 80aaa3c:	3301      	adds	r3, #1
 80aaa3e:	e78c      	b.n	80aa95a <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x96>
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80aaa40:	9b05      	ldr	r3, [sp, #20]
 80aaa42:	9a11      	ldr	r2, [sp, #68]	; 0x44
 80aaa44:	3301      	adds	r3, #1
 80aaa46:	9305      	str	r3, [sp, #20]
 80aaa48:	9b07      	ldr	r3, [sp, #28]
 80aaa4a:	4413      	add	r3, r2
 80aaa4c:	9307      	str	r3, [sp, #28]
 80aaa4e:	e77f      	b.n	80aa950 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x8c>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80aaa50:	9b04      	ldr	r3, [sp, #16]
 80aaa52:	9a10      	ldr	r2, [sp, #64]	; 0x40
 80aaa54:	3301      	adds	r3, #1
 80aaa56:	9304      	str	r3, [sp, #16]
 80aaa58:	9b06      	ldr	r3, [sp, #24]
 80aaa5a:	4413      	add	r3, r2
 80aaa5c:	9306      	str	r3, [sp, #24]
 80aaa5e:	e76f      	b.n	80aa940 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x7c>
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
 80aaa60:	f10b 0b01 	add.w	fp, fp, #1
 80aaa64:	e765      	b.n	80aa932 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x6e>
                                           params.float_activation_max);
        }
      }
    }
  }
}
 80aaa66:	b019      	add	sp, #100	; 0x64
 80aaa68:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080aaa6c <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph>:

inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const uint8* input_data, const RuntimeShape& output_shape,
                    uint8* output_data) {
 80aaa6c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80aaa70:	b097      	sub	sp, #92	; 0x5c
 80aaa72:	9208      	str	r2, [sp, #32]
 80aaa74:	461e      	mov	r6, r3
  TFLITE_DCHECK_LE(params.quantized_activation_min,
 80aaa76:	69c2      	ldr	r2, [r0, #28]
 80aaa78:	6a03      	ldr	r3, [r0, #32]
  }
}

inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const uint8* input_data, const RuntimeShape& output_shape,
                    uint8* output_data) {
 80aaa7a:	4604      	mov	r4, r0
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
 80aaa7c:	429a      	cmp	r2, r3
  }
}

inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const uint8* input_data, const RuntimeShape& output_shape,
                    uint8* output_data) {
 80aaa7e:	460d      	mov	r5, r1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
 80aaa80:	dd01      	ble.n	80aaa86 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x1a>
}

inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const uint8* input_data, const RuntimeShape& output_shape,
                    uint8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
 80aaa82:	f005 fad5 	bl	80b0030 <abort>
                   params.quantized_activation_max);
  TFLITE_DCHECK_GE(params.quantized_activation_min, 0);
 80aaa86:	2a00      	cmp	r2, #0
 80aaa88:	dbfb      	blt.n	80aaa82 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x16>
  TFLITE_DCHECK_LE(params.quantized_activation_max, 255);
 80aaa8a:	2bff      	cmp	r3, #255	; 0xff
 80aaa8c:	dcf9      	bgt.n	80aaa82 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x16>
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
 80aaa8e:	680b      	ldr	r3, [r1, #0]
 80aaa90:	2b04      	cmp	r3, #4
 80aaa92:	d1f6      	bne.n	80aaa82 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x16>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
 80aaa94:	6833      	ldr	r3, [r6, #0]
 80aaa96:	2b04      	cmp	r3, #4
 80aaa98:	d1f3      	bne.n	80aaa82 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x16>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80aaa9a:	2300      	movs	r3, #0
 80aaa9c:	4619      	mov	r1, r3
 80aaa9e:	4632      	mov	r2, r6
 80aaaa0:	4628      	mov	r0, r5
 80aaaa2:	f7fc ffec 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
 80aaaa6:	2303      	movs	r3, #3
 80aaaa8:	4619      	mov	r1, r3
 80aaaaa:	4632      	mov	r2, r6
                   params.quantized_activation_max);
  TFLITE_DCHECK_GE(params.quantized_activation_min, 0);
  TFLITE_DCHECK_LE(params.quantized_activation_max, 255);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80aaaac:	9009      	str	r0, [sp, #36]	; 0x24
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
 80aaaae:	4628      	mov	r0, r5
 80aaab0:	f7fc ffe5 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
 80aaab4:	2101      	movs	r1, #1
  TFLITE_DCHECK_GE(params.quantized_activation_min, 0);
  TFLITE_DCHECK_LE(params.quantized_activation_max, 255);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
 80aaab6:	900b      	str	r0, [sp, #44]	; 0x2c
  const int input_height = input_shape.Dims(1);
 80aaab8:	4628      	mov	r0, r5
 80aaaba:	f7f7 fc85 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
 80aaabe:	2102      	movs	r1, #2
  TFLITE_DCHECK_LE(params.quantized_activation_max, 255);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
 80aaac0:	900c      	str	r0, [sp, #48]	; 0x30
  const int input_width = input_shape.Dims(2);
 80aaac2:	4628      	mov	r0, r5
 80aaac4:	f7f7 fc80 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
 80aaac8:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
 80aaaca:	900d      	str	r0, [sp, #52]	; 0x34
  const int output_height = output_shape.Dims(1);
 80aaacc:	4630      	mov	r0, r6
 80aaace:	f7f7 fc7b 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
 80aaad2:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
 80aaad4:	900e      	str	r0, [sp, #56]	; 0x38
  const int output_width = output_shape.Dims(2);
 80aaad6:	4630      	mov	r0, r6
 80aaad8:	f7f7 fc76 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int stride_height = params.stride_height;
 80aaadc:	68e3      	ldr	r3, [r4, #12]
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
 80aaade:	900f      	str	r0, [sp, #60]	; 0x3c
  const int stride_height = params.stride_height;
 80aaae0:	9310      	str	r3, [sp, #64]	; 0x40
  const int stride_width = params.stride_width;
 80aaae2:	6923      	ldr	r3, [r4, #16]
  for (int batch = 0; batch < batches; ++batch) {
 80aaae4:	f04f 0b00 	mov.w	fp, #0
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
 80aaae8:	9311      	str	r3, [sp, #68]	; 0x44
  for (int batch = 0; batch < batches; ++batch) {
 80aaaea:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80aaaec:	459b      	cmp	fp, r3
 80aaaee:	f280 808d 	bge.w	80aac0c <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x1a0>
 80aaaf2:	2300      	movs	r3, #0
 80aaaf4:	9305      	str	r3, [sp, #20]
 80aaaf6:	9302      	str	r3, [sp, #8]
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80aaaf8:	9b02      	ldr	r3, [sp, #8]
 80aaafa:	9a0e      	ldr	r2, [sp, #56]	; 0x38
 80aaafc:	4293      	cmp	r3, r2
 80aaafe:	f280 8082 	bge.w	80aac06 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x19a>
 80aab02:	2300      	movs	r3, #0
 80aab04:	9304      	str	r3, [sp, #16]
 80aab06:	9303      	str	r3, [sp, #12]
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80aab08:	9b03      	ldr	r3, [sp, #12]
 80aab0a:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
 80aab0c:	4293      	cmp	r3, r2
 80aab0e:	da72      	bge.n	80aabf6 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x18a>
 80aab10:	f04f 0800 	mov.w	r8, #0
        for (int channel = 0; channel < depth; ++channel) {
 80aab14:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80aab16:	4598      	cmp	r8, r3
 80aab18:	da65      	bge.n	80aabe6 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x17a>
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
 80aab1a:	f9b4 9002 	ldrsh.w	r9, [r4, #2]
 80aab1e:	9b04      	ldr	r3, [sp, #16]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
 80aab20:	f9b4 a004 	ldrsh.w	sl, [r4, #4]
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
 80aab24:	ebc9 0303 	rsb	r3, r9, r3
 80aab28:	9307      	str	r3, [sp, #28]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
 80aab2a:	9b05      	ldr	r3, [sp, #20]
 80aab2c:	9a07      	ldr	r2, [sp, #28]
 80aab2e:	ebca 0a03 	rsb	sl, sl, r3
 80aab32:	9b07      	ldr	r3, [sp, #28]
 80aab34:	f1ca 0700 	rsb	r7, sl, #0
 80aab38:	425b      	negs	r3, r3
 80aab3a:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
 80aab3e:	9306      	str	r3, [sp, #24]
 80aab40:	9b0d      	ldr	r3, [sp, #52]	; 0x34
 80aab42:	ea27 77e7 	bic.w	r7, r7, r7, asr #31
 80aab46:	1a9a      	subs	r2, r3, r2
 80aab48:	69a3      	ldr	r3, [r4, #24]
 80aab4a:	429a      	cmp	r2, r3
 80aab4c:	bfa8      	it	ge
 80aab4e:	461a      	movge	r2, r3
 80aab50:	9b0c      	ldr	r3, [sp, #48]	; 0x30
 80aab52:	920a      	str	r2, [sp, #40]	; 0x28
 80aab54:	ebca 0203 	rsb	r2, sl, r3
 80aab58:	6963      	ldr	r3, [r4, #20]
 80aab5a:	429a      	cmp	r2, r3
 80aab5c:	bfa8      	it	ge
 80aab5e:	461a      	movge	r2, r3
          const int filter_x_end =
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          uint8 max = 0;
 80aab60:	f04f 0300 	mov.w	r3, #0
 80aab64:	9212      	str	r2, [sp, #72]	; 0x48
 80aab66:	f88d 3057 	strb.w	r3, [sp, #87]	; 0x57
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
 80aab6a:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80aab6c:	429f      	cmp	r7, r3
 80aab6e:	da22      	bge.n	80aabb6 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x14a>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
 80aab70:	eb0a 0307 	add.w	r3, sl, r7
 80aab74:	f8dd 9018 	ldr.w	r9, [sp, #24]
 80aab78:	9313      	str	r3, [sp, #76]	; 0x4c
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          uint8 max = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
 80aab7a:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80aab7c:	4599      	cmp	r9, r3
 80aab7e:	da18      	bge.n	80aabb2 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x146>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
 80aab80:	9b07      	ldr	r3, [sp, #28]
 80aab82:	9a13      	ldr	r2, [sp, #76]	; 0x4c
 80aab84:	444b      	add	r3, r9
 80aab86:	4659      	mov	r1, fp
 80aab88:	f8cd 8000 	str.w	r8, [sp]
 80aab8c:	4628      	mov	r0, r5
 80aab8e:	f7f7 fc80 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
 80aab92:	9a08      	ldr	r2, [sp, #32]
 80aab94:	9b08      	ldr	r3, [sp, #32]
 80aab96:	5c11      	ldrb	r1, [r2, r0]
 80aab98:	f89d 2057 	ldrb.w	r2, [sp, #87]	; 0x57
 80aab9c:	4403      	add	r3, r0
 80aab9e:	4291      	cmp	r1, r2
	return __b;
      return __a;
 80aaba0:	bf98      	it	ls
 80aaba2:	f10d 0357 	addls.w	r3, sp, #87	; 0x57
 80aaba6:	781b      	ldrb	r3, [r3, #0]
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          uint8 max = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
 80aaba8:	f109 0901 	add.w	r9, r9, #1
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
 80aabac:	f88d 3057 	strb.w	r3, [sp, #87]	; 0x57
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          uint8 max = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
 80aabb0:	e7e3      	b.n	80aab7a <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x10e>
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          uint8 max = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
 80aabb2:	3701      	adds	r7, #1
 80aabb4:	e7d9      	b.n	80aab6a <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xfe>
 80aabb6:	7f27      	ldrb	r7, [r4, #28]
 80aabb8:	f89d 3057 	ldrb.w	r3, [sp, #87]	; 0x57
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
            }
          }
          max = std::max<uint8>(max, params.quantized_activation_min);
          max = std::min<uint8>(max, params.quantized_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
 80aabbc:	f8cd 8000 	str.w	r8, [sp]
 80aabc0:	429f      	cmp	r7, r3
 80aabc2:	bf38      	it	cc
 80aabc4:	461f      	movcc	r7, r3
 80aabc6:	f894 3020 	ldrb.w	r3, [r4, #32]
 80aabca:	9a02      	ldr	r2, [sp, #8]
 80aabcc:	429f      	cmp	r7, r3
 80aabce:	bf28      	it	cs
 80aabd0:	461f      	movcs	r7, r3
 80aabd2:	4659      	mov	r1, fp
 80aabd4:	9b03      	ldr	r3, [sp, #12]
 80aabd6:	4630      	mov	r0, r6
 80aabd8:	f7f7 fc5b 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<uint8>(max);
 80aabdc:	9b20      	ldr	r3, [sp, #128]	; 0x80
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
 80aabde:	f108 0801 	add.w	r8, r8, #1
            }
          }
          max = std::max<uint8>(max, params.quantized_activation_min);
          max = std::min<uint8>(max, params.quantized_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
              static_cast<uint8>(max);
 80aabe2:	541f      	strb	r7, [r3, r0]
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
 80aabe4:	e796      	b.n	80aab14 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xa8>
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80aabe6:	9b03      	ldr	r3, [sp, #12]
 80aabe8:	9a11      	ldr	r2, [sp, #68]	; 0x44
 80aabea:	3301      	adds	r3, #1
 80aabec:	9303      	str	r3, [sp, #12]
 80aabee:	9b04      	ldr	r3, [sp, #16]
 80aabf0:	4413      	add	r3, r2
 80aabf2:	9304      	str	r3, [sp, #16]
 80aabf4:	e788      	b.n	80aab08 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x9c>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80aabf6:	9b02      	ldr	r3, [sp, #8]
 80aabf8:	9a10      	ldr	r2, [sp, #64]	; 0x40
 80aabfa:	3301      	adds	r3, #1
 80aabfc:	9302      	str	r3, [sp, #8]
 80aabfe:	9b05      	ldr	r3, [sp, #20]
 80aac00:	4413      	add	r3, r2
 80aac02:	9305      	str	r3, [sp, #20]
 80aac04:	e778      	b.n	80aaaf8 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x8c>
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
 80aac06:	f10b 0b01 	add.w	fp, fp, #1
 80aac0a:	e76e      	b.n	80aaaea <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x7e>
              static_cast<uint8>(max);
        }
      }
    }
  }
}
 80aac0c:	b017      	add	sp, #92	; 0x5c
 80aac0e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080aac12 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa>:
namespace tflite {
namespace reference_integer_ops {

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape, const int8* input_data,
                        const RuntimeShape& output_shape, int8* output_data) {
 80aac12:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80aac16:	b097      	sub	sp, #92	; 0x5c
 80aac18:	9214      	str	r2, [sp, #80]	; 0x50
 80aac1a:	461f      	mov	r7, r3
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
 80aac1c:	69c2      	ldr	r2, [r0, #28]
 80aac1e:	6a03      	ldr	r3, [r0, #32]
namespace tflite {
namespace reference_integer_ops {

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape, const int8* input_data,
                        const RuntimeShape& output_shape, int8* output_data) {
 80aac20:	4604      	mov	r4, r0
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
 80aac22:	429a      	cmp	r2, r3
namespace tflite {
namespace reference_integer_ops {

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape, const int8* input_data,
                        const RuntimeShape& output_shape, int8* output_data) {
 80aac24:	4689      	mov	r9, r1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
 80aac26:	dd01      	ble.n	80aac2c <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x1a>
namespace reference_integer_ops {

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape, const int8* input_data,
                        const RuntimeShape& output_shape, int8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
 80aac28:	f005 fa02 	bl	80b0030 <abort>
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
 80aac2c:	680b      	ldr	r3, [r1, #0]
 80aac2e:	2b04      	cmp	r3, #4
 80aac30:	d1fa      	bne.n	80aac28 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x16>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
 80aac32:	683b      	ldr	r3, [r7, #0]
 80aac34:	2b04      	cmp	r3, #4
 80aac36:	d1f7      	bne.n	80aac28 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x16>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80aac38:	2300      	movs	r3, #0
 80aac3a:	4619      	mov	r1, r3
 80aac3c:	463a      	mov	r2, r7
 80aac3e:	4648      	mov	r0, r9
 80aac40:	f7fc ff1d 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
 80aac44:	2303      	movs	r3, #3
 80aac46:	4619      	mov	r1, r3
 80aac48:	463a      	mov	r2, r7
                        const RuntimeShape& output_shape, int8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80aac4a:	900a      	str	r0, [sp, #40]	; 0x28
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
 80aac4c:	4648      	mov	r0, r9
 80aac4e:	f7fc ff16 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
 80aac52:	2101      	movs	r1, #1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
 80aac54:	900b      	str	r0, [sp, #44]	; 0x2c
  const int input_height = input_shape.Dims(1);
 80aac56:	4648      	mov	r0, r9
 80aac58:	f7f7 fbb6 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
 80aac5c:	2102      	movs	r1, #2
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
 80aac5e:	900c      	str	r0, [sp, #48]	; 0x30
  const int input_width = input_shape.Dims(2);
 80aac60:	4648      	mov	r0, r9
 80aac62:	f7f7 fbb1 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
 80aac66:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
 80aac68:	900d      	str	r0, [sp, #52]	; 0x34
  const int output_height = output_shape.Dims(1);
 80aac6a:	4638      	mov	r0, r7
 80aac6c:	f7f7 fbac 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
 80aac70:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
 80aac72:	900e      	str	r0, [sp, #56]	; 0x38
  const int output_width = output_shape.Dims(2);
 80aac74:	4638      	mov	r0, r7
 80aac76:	f7f7 fba7 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int stride_height = params.stride_height;
 80aac7a:	68e3      	ldr	r3, [r4, #12]
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
 80aac7c:	900f      	str	r0, [sp, #60]	; 0x3c
  const int stride_height = params.stride_height;
 80aac7e:	9310      	str	r3, [sp, #64]	; 0x40
  const int stride_width = params.stride_width;
 80aac80:	6923      	ldr	r3, [r4, #16]
  for (int batch = 0; batch < batches; ++batch) {
 80aac82:	f04f 0a00 	mov.w	sl, #0
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
 80aac86:	9311      	str	r3, [sp, #68]	; 0x44
  for (int batch = 0; batch < batches; ++batch) {
 80aac88:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80aac8a:	459a      	cmp	sl, r3
 80aac8c:	f280 808d 	bge.w	80aadaa <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x198>
 80aac90:	2300      	movs	r3, #0
 80aac92:	9305      	str	r3, [sp, #20]
 80aac94:	9303      	str	r3, [sp, #12]
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80aac96:	9b03      	ldr	r3, [sp, #12]
 80aac98:	9a0e      	ldr	r2, [sp, #56]	; 0x38
 80aac9a:	4293      	cmp	r3, r2
 80aac9c:	f280 8082 	bge.w	80aada4 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x192>
 80aaca0:	2300      	movs	r3, #0
 80aaca2:	9306      	str	r3, [sp, #24]
 80aaca4:	9304      	str	r3, [sp, #16]
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80aaca6:	9b04      	ldr	r3, [sp, #16]
 80aaca8:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
 80aacaa:	4293      	cmp	r3, r2
 80aacac:	da72      	bge.n	80aad94 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x182>
 80aacae:	2300      	movs	r3, #0
 80aacb0:	9302      	str	r3, [sp, #8]
        for (int channel = 0; channel < depth; ++channel) {
 80aacb2:	9b02      	ldr	r3, [sp, #8]
 80aacb4:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
 80aacb6:	4293      	cmp	r3, r2
 80aacb8:	da64      	bge.n	80aad84 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x172>
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
 80aacba:	9a06      	ldr	r2, [sp, #24]
 80aacbc:	f9b4 3002 	ldrsh.w	r3, [r4, #2]
          const int filter_x_end =
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
 80aacc0:	2500      	movs	r5, #0
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
 80aacc2:	1ad3      	subs	r3, r2, r3
 80aacc4:	9308      	str	r3, [sp, #32]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
 80aacc6:	9a05      	ldr	r2, [sp, #20]
 80aacc8:	f9b4 3004 	ldrsh.w	r3, [r4, #4]
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
 80aaccc:	46ab      	mov	fp, r5
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
 80aacce:	1ad3      	subs	r3, r2, r3
 80aacd0:	9309      	str	r3, [sp, #36]	; 0x24
 80aacd2:	9b08      	ldr	r3, [sp, #32]
 80aacd4:	9a08      	ldr	r2, [sp, #32]
 80aacd6:	425b      	negs	r3, r3
 80aacd8:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
 80aacdc:	9307      	str	r3, [sp, #28]
 80aacde:	9b0d      	ldr	r3, [sp, #52]	; 0x34
 80aace0:	1a9a      	subs	r2, r3, r2
 80aace2:	69a3      	ldr	r3, [r4, #24]
 80aace4:	429a      	cmp	r2, r3
 80aace6:	bfa8      	it	ge
 80aace8:	461a      	movge	r2, r3
 80aacea:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80aacec:	9213      	str	r2, [sp, #76]	; 0x4c
 80aacee:	425e      	negs	r6, r3
 80aacf0:	9a09      	ldr	r2, [sp, #36]	; 0x24
 80aacf2:	9b0c      	ldr	r3, [sp, #48]	; 0x30
 80aacf4:	ea26 76e6 	bic.w	r6, r6, r6, asr #31
 80aacf8:	1a9a      	subs	r2, r3, r2
 80aacfa:	6963      	ldr	r3, [r4, #20]
 80aacfc:	429a      	cmp	r2, r3
 80aacfe:	bfa8      	it	ge
 80aad00:	461a      	movge	r2, r3
 80aad02:	9212      	str	r2, [sp, #72]	; 0x48
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
 80aad04:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80aad06:	429e      	cmp	r6, r3
 80aad08:	da1d      	bge.n	80aad46 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x134>
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
 80aad0a:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80aad0c:	f8dd 801c 	ldr.w	r8, [sp, #28]
 80aad10:	4433      	add	r3, r6
 80aad12:	ebc8 0b0b 	rsb	fp, r8, fp
 80aad16:	9315      	str	r3, [sp, #84]	; 0x54
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
 80aad18:	9a13      	ldr	r2, [sp, #76]	; 0x4c
 80aad1a:	eb0b 0308 	add.w	r3, fp, r8
 80aad1e:	4590      	cmp	r8, r2
 80aad20:	da0e      	bge.n	80aad40 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x12e>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
 80aad22:	9b02      	ldr	r3, [sp, #8]
 80aad24:	9a15      	ldr	r2, [sp, #84]	; 0x54
 80aad26:	9300      	str	r3, [sp, #0]
 80aad28:	9b08      	ldr	r3, [sp, #32]
 80aad2a:	4651      	mov	r1, sl
 80aad2c:	4443      	add	r3, r8
 80aad2e:	4648      	mov	r0, r9
 80aad30:	f7f7 fbaf 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80aad34:	9b14      	ldr	r3, [sp, #80]	; 0x50
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
 80aad36:	f108 0801 	add.w	r8, r8, #1
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
 80aad3a:	561b      	ldrsb	r3, [r3, r0]
 80aad3c:	441d      	add	r5, r3
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
 80aad3e:	e7eb      	b.n	80aad18 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x106>
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
 80aad40:	3601      	adds	r6, #1
 80aad42:	469b      	mov	fp, r3
 80aad44:	e7de      	b.n	80aad04 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0xf2>
              filter_count++;
            }
          }
          // Round to the closest integer value.
          acc = acc > 0 ? (acc + filter_count / 2) / filter_count
                        : (acc - filter_count / 2) / filter_count;
 80aad46:	2d00      	cmp	r5, #0
 80aad48:	bfdb      	ittet	le
 80aad4a:	2302      	movle	r3, #2
 80aad4c:	fb9b f3f3 	sdivle	r3, fp, r3
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
              filter_count++;
            }
          }
          // Round to the closest integer value.
          acc = acc > 0 ? (acc + filter_count / 2) / filter_count
 80aad50:	eb05 056b 	addgt.w	r5, r5, fp, asr #1
                        : (acc - filter_count / 2) / filter_count;
 80aad54:	1aed      	suble	r5, r5, r3
 80aad56:	fb95 fbfb 	sdiv	fp, r5, fp
 80aad5a:	69e5      	ldr	r5, [r4, #28]
          acc = std::max(acc, params.quantized_activation_min);
          acc = std::min(acc, params.quantized_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
 80aad5c:	9b02      	ldr	r3, [sp, #8]
 80aad5e:	45ab      	cmp	fp, r5
 80aad60:	bfb8      	it	lt
 80aad62:	46ab      	movlt	fp, r5
 80aad64:	6a25      	ldr	r5, [r4, #32]
 80aad66:	9300      	str	r3, [sp, #0]
 80aad68:	455d      	cmp	r5, fp
 80aad6a:	9b04      	ldr	r3, [sp, #16]
 80aad6c:	9a03      	ldr	r2, [sp, #12]
 80aad6e:	4651      	mov	r1, sl
 80aad70:	4638      	mov	r0, r7
 80aad72:	bfa8      	it	ge
 80aad74:	465d      	movge	r5, fp
 80aad76:	f7f7 fb8c 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<int8>(acc);
 80aad7a:	9b20      	ldr	r3, [sp, #128]	; 0x80
 80aad7c:	541d      	strb	r5, [r3, r0]
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
 80aad7e:	9b02      	ldr	r3, [sp, #8]
 80aad80:	3301      	adds	r3, #1
 80aad82:	e795      	b.n	80aacb0 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x9e>
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80aad84:	9b04      	ldr	r3, [sp, #16]
 80aad86:	9a11      	ldr	r2, [sp, #68]	; 0x44
 80aad88:	3301      	adds	r3, #1
 80aad8a:	9304      	str	r3, [sp, #16]
 80aad8c:	9b06      	ldr	r3, [sp, #24]
 80aad8e:	4413      	add	r3, r2
 80aad90:	9306      	str	r3, [sp, #24]
 80aad92:	e788      	b.n	80aaca6 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x94>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80aad94:	9b03      	ldr	r3, [sp, #12]
 80aad96:	9a10      	ldr	r2, [sp, #64]	; 0x40
 80aad98:	3301      	adds	r3, #1
 80aad9a:	9303      	str	r3, [sp, #12]
 80aad9c:	9b05      	ldr	r3, [sp, #20]
 80aad9e:	4413      	add	r3, r2
 80aada0:	9305      	str	r3, [sp, #20]
 80aada2:	e778      	b.n	80aac96 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x84>
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
 80aada4:	f10a 0a01 	add.w	sl, sl, #1
 80aada8:	e76e      	b.n	80aac88 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x76>
              static_cast<int8>(acc);
        }
      }
    }
  }
}
 80aadaa:	b017      	add	sp, #92	; 0x5c
 80aadac:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080aadb0 <_ZN6tflite3ops5micro7pooling12_GLOBAL__N_115CalculateOpDataEPK13TfLiteContextPK16TfLitePoolParamsPK12TfLiteTensorSC_PNS3_6OpDataE.isra.2>:

struct OpData {
  TfLitePaddingValues padding;
};

TfLiteStatus CalculateOpData(const TfLiteContext* context,
 80aadb0:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80aadb4:	688b      	ldr	r3, [r1, #8]
 80aadb6:	b085      	sub	sp, #20

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
inline int SizeOfDimension(const TfLiteTensor* t, int dim) {
  return t->dims->data[dim];
 80aadb8:	689f      	ldr	r7, [r3, #8]
 80aadba:	f8d3 b00c 	ldr.w	fp, [r3, #12]

  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      /*dilation_rate_height=*/1,
      /*dilation_rate_width=*/1, height, width, params->filter_height,
      params->filter_width, params->padding, &out_height, &out_width);
 80aadbe:	68c3      	ldr	r3, [r0, #12]
  int width = SizeOfDimension(input, 2);

  int out_height, out_width;

  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
 80aadc0:	6846      	ldr	r6, [r0, #4]
      /*dilation_rate_height=*/1,
      /*dilation_rate_width=*/1, height, width, params->filter_height,
      params->filter_width, params->padding, &out_height, &out_width);
 80aadc2:	f890 a000 	ldrb.w	sl, [r0]
 80aadc6:	9302      	str	r3, [sp, #8]

inline TfLitePaddingValues ComputePaddingHeightWidth(
    int stride_height, int stride_width, int dilation_rate_height,
    int dilation_rate_width, int in_height, int in_width, int filter_height,
    int filter_width, TfLitePadding padding, int* out_height, int* out_width) {
  *out_width = ComputeOutSize(padding, in_width, filter_width, stride_width,
 80aadc8:	2401      	movs	r4, #1
  int width = SizeOfDimension(input, 2);

  int out_height, out_width;

  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
 80aadca:	f8d0 9008 	ldr.w	r9, [r0, #8]
      /*dilation_rate_height=*/1,
      /*dilation_rate_width=*/1, height, width, params->filter_height,
 80aadce:	f8d0 8010 	ldr.w	r8, [r0, #16]

struct OpData {
  TfLitePaddingValues padding;
};

TfLiteStatus CalculateOpData(const TfLiteContext* context,
 80aadd2:	4615      	mov	r5, r2
 80aadd4:	9400      	str	r4, [sp, #0]
 80aadd6:	4633      	mov	r3, r6
 80aadd8:	9a02      	ldr	r2, [sp, #8]
 80aadda:	4659      	mov	r1, fp
 80aaddc:	4650      	mov	r0, sl
 80aadde:	f7fd f9c4 	bl	80a816a <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>
                              dilation_rate_width);
  *out_height = ComputeOutSize(padding, in_height, filter_height, stride_height,
 80aade2:	9400      	str	r4, [sp, #0]

inline TfLitePaddingValues ComputePaddingHeightWidth(
    int stride_height, int stride_width, int dilation_rate_height,
    int dilation_rate_width, int in_height, int in_width, int filter_height,
    int filter_width, TfLitePadding padding, int* out_height, int* out_width) {
  *out_width = ComputeOutSize(padding, in_width, filter_width, stride_width,
 80aade4:	9003      	str	r0, [sp, #12]
                              dilation_rate_width);
  *out_height = ComputeOutSize(padding, in_height, filter_height, stride_height,
 80aade6:	464b      	mov	r3, r9
 80aade8:	4642      	mov	r2, r8
 80aadea:	4639      	mov	r1, r7
 80aadec:	4650      	mov	r0, sl
 80aadee:	f7fd f9bc 	bl	80a816a <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>
inline int ComputePaddingWithOffset(int stride, int dilation_rate, int in_size,
                                    int filter_size, int out_size,
                                    int* offset) {
  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;
  int total_padding =
      ((out_size - 1) * stride + effective_filter_size - in_size);
 80aadf2:	9b03      	ldr	r3, [sp, #12]
 80aadf4:	3801      	subs	r0, #1
 80aadf6:	1e5c      	subs	r4, r3, #1
 80aadf8:	9b02      	ldr	r3, [sp, #8]
 80aadfa:	fb09 8800 	mla	r8, r9, r0, r8
 80aadfe:	fb06 3604 	mla	r6, r6, r4, r3
 80aae02:	ebcb 0606 	rsb	r6, fp, r6
 80aae06:	ebc7 0708 	rsb	r7, r7, r8
  total_padding = total_padding > 0 ? total_padding : 0;
 80aae0a:	ea26 76e6 	bic.w	r6, r6, r6, asr #31

  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      /*dilation_rate_height=*/1,
      /*dilation_rate_width=*/1, height, width, params->filter_height,
      params->filter_width, params->padding, &out_height, &out_width);
 80aae0e:	1073      	asrs	r3, r6, #1
 80aae10:	ea27 77e7 	bic.w	r7, r7, r7, asr #31
 80aae14:	602b      	str	r3, [r5, #0]
 80aae16:	f006 0601 	and.w	r6, r6, #1
 80aae1a:	107b      	asrs	r3, r7, #1

  return kTfLiteOk;
}
 80aae1c:	2000      	movs	r0, #0

  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      /*dilation_rate_height=*/1,
      /*dilation_rate_width=*/1, height, width, params->filter_height,
      params->filter_width, params->padding, &out_height, &out_width);
 80aae1e:	f007 0701 	and.w	r7, r7, #1
 80aae22:	606b      	str	r3, [r5, #4]
 80aae24:	60ae      	str	r6, [r5, #8]
 80aae26:	60ef      	str	r7, [r5, #12]

  return kTfLiteOk;
}
 80aae28:	b005      	add	sp, #20
 80aae2a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

080aae30 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode>:
      return kTfLiteError;
  }
  return kTfLiteOk;
}

TfLiteStatus MaxEval(TfLiteContext* context, TfLiteNode* node) {
 80aae30:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80aae34:	680b      	ldr	r3, [r1, #0]
 80aae36:	2438      	movs	r4, #56	; 0x38
 80aae38:	685b      	ldr	r3, [r3, #4]
 80aae3a:	f8d0 a008 	ldr.w	sl, [r0, #8]
 80aae3e:	fb04 f803 	mul.w	r8, r4, r3
  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);
 80aae42:	694d      	ldr	r5, [r1, #20]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80aae44:	684b      	ldr	r3, [r1, #4]
      return kTfLiteError;
  }
  return kTfLiteOk;
}

TfLiteStatus MaxEval(TfLiteContext* context, TfLiteNode* node) {
 80aae46:	b09f      	sub	sp, #124	; 0x7c
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80aae48:	eb0a 0708 	add.w	r7, sl, r8
 80aae4c:	4681      	mov	r9, r0
  OpData data;

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, input, output, &data));
 80aae4e:	aa05      	add	r2, sp, #20
 80aae50:	4639      	mov	r1, r7
 80aae52:	4628      	mov	r0, r5
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80aae54:	f8d3 b004 	ldr.w	fp, [r3, #4]
 80aae58:	f7ff ffaa 	bl	80aadb0 <_ZN6tflite3ops5micro7pooling12_GLOBAL__N_115CalculateOpDataEPK13TfLiteContextPK16TfLitePoolParamsPK12TfLiteTensorSC_PNS3_6OpDataE.isra.2>
 80aae5c:	4606      	mov	r6, r0
 80aae5e:	2800      	cmp	r0, #0
 80aae60:	d179      	bne.n	80aaf56 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x126>

  switch (input->type) {
 80aae62:	f81a 0008 	ldrb.w	r0, [sl, r8]
 80aae66:	fb04 a40b 	mla	r4, r4, fp, sl
 80aae6a:	2801      	cmp	r0, #1
 80aae6c:	d002      	beq.n	80aae74 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x44>
 80aae6e:	2803      	cmp	r0, #3
 80aae70:	d036      	beq.n	80aaee0 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0xb0>
 80aae72:	e068      	b.n	80aaf46 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x116>

void MaxEvalFloat(TfLiteContext* context, TfLiteNode* node,
                  TfLitePoolParams* params, OpData* data,
                  const TfLiteTensor* input, TfLiteTensor* output) {
  float activation_min, activation_max;
  CalculateActivationRange(params->activation, &activation_min,
 80aae74:	7d2b      	ldrb	r3, [r5, #20]
// Calculates the useful range of an activation layer given its activation
// tensor.a
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
 80aae76:	2b01      	cmp	r3, #1
 80aae78:	d007      	beq.n	80aae8a <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x5a>
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
 80aae7a:	2b03      	cmp	r3, #3
 80aae7c:	d007      	beq.n	80aae8e <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x5e>
    *activation_min = 0;
    *activation_max = 6;
  } else if (activation == kTfLiteActRelu1) {
 80aae7e:	2b02      	cmp	r3, #2
 80aae80:	d008      	beq.n	80aae94 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x64>
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
    *activation_max = std::numeric_limits<T>::max();
 80aae82:	4b37      	ldr	r3, [pc, #220]	; (80aaf60 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x130>)
    *activation_max = 6;
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
 80aae84:	f46f 0200 	mvn.w	r2, #8388608	; 0x800000
 80aae88:	e007      	b.n	80aae9a <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x6a>
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
 80aae8a:	4b35      	ldr	r3, [pc, #212]	; (80aaf60 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x130>)
 80aae8c:	e000      	b.n	80aae90 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x60>
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
    *activation_max = 6;
 80aae8e:	4b35      	ldr	r3, [pc, #212]	; (80aaf64 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x134>)
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
 80aae90:	2200      	movs	r2, #0
 80aae92:	e002      	b.n	80aae9a <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x6a>
    *activation_max = 6;
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
 80aae94:	4a34      	ldr	r2, [pc, #208]	; (80aaf68 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x138>)
    *activation_max = 1;
 80aae96:	f04f 537e 	mov.w	r3, #1065353216	; 0x3f800000
                           &activation_max);

  tflite::PoolParams op_params;
  op_params.stride_height = params->stride_height;
 80aae9a:	68a9      	ldr	r1, [r5, #8]
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.float_activation_min = activation_min;
  op_params.float_activation_max = activation_max;
  reference_ops::MaxPool(op_params, GetTensorShape(input),
 80aae9c:	a809      	add	r0, sp, #36	; 0x24
  float activation_min, activation_max;
  CalculateActivationRange(params->activation, &activation_min,
                           &activation_max);

  tflite::PoolParams op_params;
  op_params.stride_height = params->stride_height;
 80aae9e:	9116      	str	r1, [sp, #88]	; 0x58
  op_params.stride_width = params->stride_width;
 80aaea0:	6869      	ldr	r1, [r5, #4]
 80aaea2:	9117      	str	r1, [sp, #92]	; 0x5c
  op_params.filter_height = params->filter_height;
 80aaea4:	6929      	ldr	r1, [r5, #16]
 80aaea6:	9118      	str	r1, [sp, #96]	; 0x60
  op_params.filter_width = params->filter_width;
 80aaea8:	68e9      	ldr	r1, [r5, #12]
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.float_activation_min = activation_min;
 80aaeaa:	921c      	str	r2, [sp, #112]	; 0x70

  tflite::PoolParams op_params;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.filter_height = params->filter_height;
  op_params.filter_width = params->filter_width;
 80aaeac:	9119      	str	r1, [sp, #100]	; 0x64
  op_params.padding_values.height = data->padding.height;
 80aaeae:	9906      	ldr	r1, [sp, #24]
  op_params.padding_values.width = data->padding.width;
  op_params.float_activation_min = activation_min;
  op_params.float_activation_max = activation_max;
 80aaeb0:	931d      	str	r3, [sp, #116]	; 0x74
  tflite::PoolParams op_params;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.filter_height = params->filter_height;
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
 80aaeb2:	f8ad 1050 	strh.w	r1, [sp, #80]	; 0x50
  op_params.padding_values.width = data->padding.width;
 80aaeb6:	9905      	ldr	r1, [sp, #20]
 80aaeb8:	f8ad 104e 	strh.w	r1, [sp, #78]	; 0x4e
  op_params.float_activation_min = activation_min;
  op_params.float_activation_max = activation_max;
  reference_ops::MaxPool(op_params, GetTensorShape(input),
 80aaebc:	4639      	mov	r1, r7
 80aaebe:	f7f7 fd28 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                         GetTensorData<float>(input), GetTensorShape(output),
 80aaec2:	4621      	mov	r1, r4
 80aaec4:	a80e      	add	r0, sp, #56	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80aaec6:	687d      	ldr	r5, [r7, #4]
 80aaec8:	f7f7 fd23 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80aaecc:	b104      	cbz	r4, 80aaed0 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0xa0>
 80aaece:	6864      	ldr	r4, [r4, #4]
                         GetTensorData<float>(output));
 80aaed0:	9400      	str	r4, [sp, #0]
 80aaed2:	ab0e      	add	r3, sp, #56	; 0x38
 80aaed4:	462a      	mov	r2, r5
 80aaed6:	a909      	add	r1, sp, #36	; 0x24
 80aaed8:	a813      	add	r0, sp, #76	; 0x4c
 80aaeda:	f7ff fcf3 	bl	80aa8c4 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf>
 80aaede:	e02b      	b.n	80aaf38 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x108>
void MaxEvalQuantizedUInt8(TfLiteContext* context, TfLiteNode* node,
                           TfLitePoolParams* params, OpData* data,
                           const TfLiteTensor* input, TfLiteTensor* output) {
  int32_t activation_min, activation_max;
  CalculateActivationRangeUint8(params->activation, output, &activation_min,
                                &activation_max);
 80aaee0:	7d28      	ldrb	r0, [r5, #20]
 80aaee2:	ab04      	add	r3, sp, #16
 80aaee4:	aa03      	add	r2, sp, #12
 80aaee6:	4621      	mov	r1, r4
 80aaee8:	f004 fd10 	bl	80af90c <_ZN6tflite29CalculateActivationRangeUint8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>

  tflite::PoolParams op_params;
  op_params.stride_height = params->stride_height;
 80aaeec:	68ab      	ldr	r3, [r5, #8]
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
  reference_ops::MaxPool(op_params, GetTensorShape(input),
 80aaeee:	4639      	mov	r1, r7
  int32_t activation_min, activation_max;
  CalculateActivationRangeUint8(params->activation, output, &activation_min,
                                &activation_max);

  tflite::PoolParams op_params;
  op_params.stride_height = params->stride_height;
 80aaef0:	9316      	str	r3, [sp, #88]	; 0x58
  op_params.stride_width = params->stride_width;
 80aaef2:	686b      	ldr	r3, [r5, #4]
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
  reference_ops::MaxPool(op_params, GetTensorShape(input),
 80aaef4:	a809      	add	r0, sp, #36	; 0x24
  CalculateActivationRangeUint8(params->activation, output, &activation_min,
                                &activation_max);

  tflite::PoolParams op_params;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
 80aaef6:	9317      	str	r3, [sp, #92]	; 0x5c
  op_params.filter_height = params->filter_height;
 80aaef8:	692b      	ldr	r3, [r5, #16]
 80aaefa:	9318      	str	r3, [sp, #96]	; 0x60
  op_params.filter_width = params->filter_width;
 80aaefc:	68eb      	ldr	r3, [r5, #12]
 80aaefe:	9319      	str	r3, [sp, #100]	; 0x64
  op_params.padding_values.height = data->padding.height;
 80aaf00:	9b06      	ldr	r3, [sp, #24]
 80aaf02:	f8ad 3050 	strh.w	r3, [sp, #80]	; 0x50
  op_params.padding_values.width = data->padding.width;
 80aaf06:	9b05      	ldr	r3, [sp, #20]
 80aaf08:	f8ad 304e 	strh.w	r3, [sp, #78]	; 0x4e
  op_params.quantized_activation_min = activation_min;
 80aaf0c:	9b03      	ldr	r3, [sp, #12]
 80aaf0e:	931a      	str	r3, [sp, #104]	; 0x68
  op_params.quantized_activation_max = activation_max;
 80aaf10:	9b04      	ldr	r3, [sp, #16]
 80aaf12:	931b      	str	r3, [sp, #108]	; 0x6c
  reference_ops::MaxPool(op_params, GetTensorShape(input),
 80aaf14:	f7f7 fcfd 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                         GetTensorData<uint8_t>(input), GetTensorShape(output),
 80aaf18:	4621      	mov	r1, r4
 80aaf1a:	a80e      	add	r0, sp, #56	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80aaf1c:	687d      	ldr	r5, [r7, #4]
 80aaf1e:	f7f7 fcf8 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80aaf22:	b10c      	cbz	r4, 80aaf28 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0xf8>
 80aaf24:	6863      	ldr	r3, [r4, #4]
 80aaf26:	e000      	b.n	80aaf2a <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0xfa>
 80aaf28:	4633      	mov	r3, r6
                         GetTensorData<uint8_t>(output));
 80aaf2a:	9300      	str	r3, [sp, #0]
 80aaf2c:	462a      	mov	r2, r5
 80aaf2e:	ab0e      	add	r3, sp, #56	; 0x38
 80aaf30:	a909      	add	r1, sp, #36	; 0x24
 80aaf32:	a813      	add	r0, sp, #76	; 0x4c
 80aaf34:	f7ff fd9a 	bl	80aaa6c <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph>
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
  reference_ops::MaxPool(op_params, GetTensorShape(input),
                         GetTensorData<uint8_t>(input), GetTensorShape(output),
 80aaf38:	a80e      	add	r0, sp, #56	; 0x38
 80aaf3a:	f7f7 fa3a 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
  reference_ops::MaxPool(op_params, GetTensorShape(input),
 80aaf3e:	a809      	add	r0, sp, #36	; 0x24
 80aaf40:	f7f7 fa37 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80aaf44:	e008      	b.n	80aaf58 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x128>
      break;
    case kTfLiteUInt8:
      MaxEvalQuantizedUInt8(context, node, params, &data, input, output);
      break;
    default:
      context->ReportError(context, "Type %s not currently supported.",
 80aaf46:	f8d9 4014 	ldr.w	r4, [r9, #20]
 80aaf4a:	f7f5 f8e7 	bl	80a011c <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type));
 80aaf4e:	4907      	ldr	r1, [pc, #28]	; (80aaf6c <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x13c>)
 80aaf50:	4602      	mov	r2, r0
 80aaf52:	4648      	mov	r0, r9
 80aaf54:	47a0      	blx	r4
  OpData data;

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, input, output, &data));
 80aaf56:	2601      	movs	r6, #1
      context->ReportError(context, "Type %s not currently supported.",
                           TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
  return kTfLiteOk;
}
 80aaf58:	4630      	mov	r0, r6
 80aaf5a:	b01f      	add	sp, #124	; 0x7c
 80aaf5c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80aaf60:	7f7fffff 	.word	0x7f7fffff
 80aaf64:	40c00000 	.word	0x40c00000
 80aaf68:	bf800000 	.word	0xbf800000
 80aaf6c:	080b627a 	.word	0x080b627a

080aaf70 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

TfLiteStatus AverageEval(TfLiteContext* context, TfLiteNode* node) {
 80aaf70:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80aaf74:	680b      	ldr	r3, [r1, #0]
 80aaf76:	2438      	movs	r4, #56	; 0x38
 80aaf78:	685b      	ldr	r3, [r3, #4]
 80aaf7a:	f8d0 a008 	ldr.w	sl, [r0, #8]
 80aaf7e:	fb04 f803 	mul.w	r8, r4, r3
  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);
 80aaf82:	694d      	ldr	r5, [r1, #20]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80aaf84:	684b      	ldr	r3, [r1, #4]

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

TfLiteStatus AverageEval(TfLiteContext* context, TfLiteNode* node) {
 80aaf86:	b09f      	sub	sp, #124	; 0x7c
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80aaf88:	eb0a 0608 	add.w	r6, sl, r8
 80aaf8c:	4681      	mov	r9, r0
  OpData data;

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, input, output, &data));
 80aaf8e:	aa05      	add	r2, sp, #20
 80aaf90:	4631      	mov	r1, r6
 80aaf92:	4628      	mov	r0, r5
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80aaf94:	f8d3 b004 	ldr.w	fp, [r3, #4]
 80aaf98:	f7ff ff0a 	bl	80aadb0 <_ZN6tflite3ops5micro7pooling12_GLOBAL__N_115CalculateOpDataEPK13TfLiteContextPK16TfLitePoolParamsPK12TfLiteTensorSC_PNS3_6OpDataE.isra.2>
 80aaf9c:	4607      	mov	r7, r0
 80aaf9e:	2800      	cmp	r0, #0
 80aafa0:	f040 80a9 	bne.w	80ab0f6 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x186>

  // Inputs and outputs share the same type, guarenteed by the converter.
  switch (input->type) {
 80aafa4:	f81a 0008 	ldrb.w	r0, [sl, r8]
 80aafa8:	fb04 a40b 	mla	r4, r4, fp, sl
 80aafac:	2803      	cmp	r0, #3
 80aafae:	d03a      	beq.n	80ab026 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0xb6>
 80aafb0:	2809      	cmp	r0, #9
 80aafb2:	d065      	beq.n	80ab080 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x110>
 80aafb4:	2801      	cmp	r0, #1
 80aafb6:	f040 8096 	bne.w	80ab0e6 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x176>

void AverageEvalFloat(const TfLiteContext* context, const TfLiteNode* node,
                      const TfLitePoolParams* params, const OpData* data,
                      const TfLiteTensor* input, TfLiteTensor* output) {
  float activation_min, activation_max;
  CalculateActivationRange(params->activation, &activation_min,
 80aafba:	7d2b      	ldrb	r3, [r5, #20]
// Calculates the useful range of an activation layer given its activation
// tensor.a
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
 80aafbc:	2b01      	cmp	r3, #1
 80aafbe:	d007      	beq.n	80aafd0 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x60>
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
 80aafc0:	2b03      	cmp	r3, #3
 80aafc2:	d007      	beq.n	80aafd4 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x64>
    *activation_min = 0;
    *activation_max = 6;
  } else if (activation == kTfLiteActRelu1) {
 80aafc4:	2b02      	cmp	r3, #2
 80aafc6:	d008      	beq.n	80aafda <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x6a>
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
    *activation_max = std::numeric_limits<T>::max();
 80aafc8:	4b4d      	ldr	r3, [pc, #308]	; (80ab100 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x190>)
    *activation_max = 6;
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
 80aafca:	f46f 0200 	mvn.w	r2, #8388608	; 0x800000
 80aafce:	e007      	b.n	80aafe0 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x70>
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
 80aafd0:	4b4b      	ldr	r3, [pc, #300]	; (80ab100 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x190>)
 80aafd2:	e000      	b.n	80aafd6 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x66>
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
    *activation_max = 6;
 80aafd4:	4b4b      	ldr	r3, [pc, #300]	; (80ab104 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x194>)
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
 80aafd6:	2200      	movs	r2, #0
 80aafd8:	e002      	b.n	80aafe0 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x70>
    *activation_max = 6;
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
 80aafda:	4a4b      	ldr	r2, [pc, #300]	; (80ab108 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x198>)
    *activation_max = 1;
 80aafdc:	f04f 537e 	mov.w	r3, #1065353216	; 0x3f800000
                           &activation_max);

  PoolParams op_params;
  op_params.stride_height = params->stride_height;
 80aafe0:	68a9      	ldr	r1, [r5, #8]
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.float_activation_min = activation_min;
  op_params.float_activation_max = activation_max;
  reference_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
 80aafe2:	a809      	add	r0, sp, #36	; 0x24
  float activation_min, activation_max;
  CalculateActivationRange(params->activation, &activation_min,
                           &activation_max);

  PoolParams op_params;
  op_params.stride_height = params->stride_height;
 80aafe4:	9116      	str	r1, [sp, #88]	; 0x58
  op_params.stride_width = params->stride_width;
 80aafe6:	6869      	ldr	r1, [r5, #4]
 80aafe8:	9117      	str	r1, [sp, #92]	; 0x5c
  op_params.filter_height = params->filter_height;
 80aafea:	6929      	ldr	r1, [r5, #16]
 80aafec:	9118      	str	r1, [sp, #96]	; 0x60
  op_params.filter_width = params->filter_width;
 80aafee:	68e9      	ldr	r1, [r5, #12]
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.float_activation_min = activation_min;
 80aaff0:	921c      	str	r2, [sp, #112]	; 0x70

  PoolParams op_params;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.filter_height = params->filter_height;
  op_params.filter_width = params->filter_width;
 80aaff2:	9119      	str	r1, [sp, #100]	; 0x64
  op_params.padding_values.height = data->padding.height;
 80aaff4:	9906      	ldr	r1, [sp, #24]
  op_params.padding_values.width = data->padding.width;
  op_params.float_activation_min = activation_min;
  op_params.float_activation_max = activation_max;
 80aaff6:	931d      	str	r3, [sp, #116]	; 0x74
  PoolParams op_params;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.filter_height = params->filter_height;
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
 80aaff8:	f8ad 1050 	strh.w	r1, [sp, #80]	; 0x50
  op_params.padding_values.width = data->padding.width;
 80aaffc:	9905      	ldr	r1, [sp, #20]
 80aaffe:	f8ad 104e 	strh.w	r1, [sp, #78]	; 0x4e
  op_params.float_activation_min = activation_min;
  op_params.float_activation_max = activation_max;
  reference_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
 80ab002:	4631      	mov	r1, r6
 80ab004:	f7f7 fc85 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(output), GetTensorData<float>(output));
 80ab008:	4621      	mov	r1, r4
 80ab00a:	a80e      	add	r0, sp, #56	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80ab00c:	6875      	ldr	r5, [r6, #4]
 80ab00e:	f7f7 fc80 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80ab012:	b104      	cbz	r4, 80ab016 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0xa6>
 80ab014:	6864      	ldr	r4, [r4, #4]
 80ab016:	9400      	str	r4, [sp, #0]
 80ab018:	ab0e      	add	r3, sp, #56	; 0x38
 80ab01a:	462a      	mov	r2, r5
 80ab01c:	a909      	add	r1, sp, #36	; 0x24
 80ab01e:	a813      	add	r0, sp, #76	; 0x4c
 80ab020:	f7ff fab5 	bl	80aa58e <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf>
 80ab024:	e058      	b.n	80ab0d8 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x168>
void AverageEvalUint8(const TfLiteContext* context, const TfLiteNode* node,
                      const TfLitePoolParams* params, const OpData* data,
                      const TfLiteTensor* input, TfLiteTensor* output) {
  int32_t activation_min, activation_max;
  CalculateActivationRangeUint8(params->activation, output, &activation_min,
                                &activation_max);
 80ab026:	7d28      	ldrb	r0, [r5, #20]
 80ab028:	ab04      	add	r3, sp, #16
 80ab02a:	aa03      	add	r2, sp, #12
 80ab02c:	4621      	mov	r1, r4
 80ab02e:	f004 fc6d 	bl	80af90c <_ZN6tflite29CalculateActivationRangeUint8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>

  PoolParams op_params;
  op_params.stride_height = params->stride_height;
 80ab032:	68ab      	ldr	r3, [r5, #8]
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
  reference_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
 80ab034:	4631      	mov	r1, r6
  int32_t activation_min, activation_max;
  CalculateActivationRangeUint8(params->activation, output, &activation_min,
                                &activation_max);

  PoolParams op_params;
  op_params.stride_height = params->stride_height;
 80ab036:	9316      	str	r3, [sp, #88]	; 0x58
  op_params.stride_width = params->stride_width;
 80ab038:	686b      	ldr	r3, [r5, #4]
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
  reference_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
 80ab03a:	a809      	add	r0, sp, #36	; 0x24
  CalculateActivationRangeUint8(params->activation, output, &activation_min,
                                &activation_max);

  PoolParams op_params;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
 80ab03c:	9317      	str	r3, [sp, #92]	; 0x5c
  op_params.filter_height = params->filter_height;
 80ab03e:	692b      	ldr	r3, [r5, #16]
 80ab040:	9318      	str	r3, [sp, #96]	; 0x60
  op_params.filter_width = params->filter_width;
 80ab042:	68eb      	ldr	r3, [r5, #12]
 80ab044:	9319      	str	r3, [sp, #100]	; 0x64
  op_params.padding_values.height = data->padding.height;
 80ab046:	9b06      	ldr	r3, [sp, #24]
 80ab048:	f8ad 3050 	strh.w	r3, [sp, #80]	; 0x50
  op_params.padding_values.width = data->padding.width;
 80ab04c:	9b05      	ldr	r3, [sp, #20]
 80ab04e:	f8ad 304e 	strh.w	r3, [sp, #78]	; 0x4e
  op_params.quantized_activation_min = activation_min;
 80ab052:	9b03      	ldr	r3, [sp, #12]
 80ab054:	931a      	str	r3, [sp, #104]	; 0x68
  op_params.quantized_activation_max = activation_max;
 80ab056:	9b04      	ldr	r3, [sp, #16]
 80ab058:	931b      	str	r3, [sp, #108]	; 0x6c
  reference_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
 80ab05a:	f7f7 fc5a 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(output), GetTensorData<uint8_t>(output));
 80ab05e:	4621      	mov	r1, r4
 80ab060:	a80e      	add	r0, sp, #56	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80ab062:	6875      	ldr	r5, [r6, #4]
 80ab064:	f7f7 fc55 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80ab068:	b10c      	cbz	r4, 80ab06e <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0xfe>
 80ab06a:	6863      	ldr	r3, [r4, #4]
 80ab06c:	e000      	b.n	80ab070 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x100>
 80ab06e:	463b      	mov	r3, r7
 80ab070:	9300      	str	r3, [sp, #0]
 80ab072:	462a      	mov	r2, r5
 80ab074:	ab0e      	add	r3, sp, #56	; 0x38
 80ab076:	a909      	add	r1, sp, #36	; 0x24
 80ab078:	a813      	add	r0, sp, #76	; 0x4c
 80ab07a:	f7ff fb59 	bl	80aa730 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph>
 80ab07e:	e02b      	b.n	80ab0d8 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x168>
void AverageEvalInt8(const TfLiteContext* context, const TfLiteNode* node,
                     const TfLitePoolParams* params, const OpData* data,
                     const TfLiteTensor* input, TfLiteTensor* output) {
  int32_t activation_min, activation_max;
  CalculateActivationRangeInt8(params->activation, output, &activation_min,
                               &activation_max);
 80ab080:	7d28      	ldrb	r0, [r5, #20]
 80ab082:	ab04      	add	r3, sp, #16
 80ab084:	aa03      	add	r2, sp, #12
 80ab086:	4621      	mov	r1, r4
 80ab088:	f004 fd34 	bl	80afaf4 <_ZN6tflite28CalculateActivationRangeInt8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>

  PoolParams op_params;
  op_params.stride_height = params->stride_height;
 80ab08c:	68ab      	ldr	r3, [r5, #8]
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
  reference_integer_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
 80ab08e:	4631      	mov	r1, r6
  int32_t activation_min, activation_max;
  CalculateActivationRangeInt8(params->activation, output, &activation_min,
                               &activation_max);

  PoolParams op_params;
  op_params.stride_height = params->stride_height;
 80ab090:	9316      	str	r3, [sp, #88]	; 0x58
  op_params.stride_width = params->stride_width;
 80ab092:	686b      	ldr	r3, [r5, #4]
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
  reference_integer_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
 80ab094:	a809      	add	r0, sp, #36	; 0x24
  CalculateActivationRangeInt8(params->activation, output, &activation_min,
                               &activation_max);

  PoolParams op_params;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
 80ab096:	9317      	str	r3, [sp, #92]	; 0x5c
  op_params.filter_height = params->filter_height;
 80ab098:	692b      	ldr	r3, [r5, #16]
 80ab09a:	9318      	str	r3, [sp, #96]	; 0x60
  op_params.filter_width = params->filter_width;
 80ab09c:	68eb      	ldr	r3, [r5, #12]
 80ab09e:	9319      	str	r3, [sp, #100]	; 0x64
  op_params.padding_values.height = data->padding.height;
 80ab0a0:	9b06      	ldr	r3, [sp, #24]
 80ab0a2:	f8ad 3050 	strh.w	r3, [sp, #80]	; 0x50
  op_params.padding_values.width = data->padding.width;
 80ab0a6:	9b05      	ldr	r3, [sp, #20]
 80ab0a8:	f8ad 304e 	strh.w	r3, [sp, #78]	; 0x4e
  op_params.quantized_activation_min = activation_min;
 80ab0ac:	9b03      	ldr	r3, [sp, #12]
 80ab0ae:	931a      	str	r3, [sp, #104]	; 0x68
  op_params.quantized_activation_max = activation_max;
 80ab0b0:	9b04      	ldr	r3, [sp, #16]
 80ab0b2:	931b      	str	r3, [sp, #108]	; 0x6c
  reference_integer_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
 80ab0b4:	f7f7 fc2d 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(output), GetTensorData<int8_t>(output));
 80ab0b8:	4621      	mov	r1, r4
 80ab0ba:	a80e      	add	r0, sp, #56	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80ab0bc:	6875      	ldr	r5, [r6, #4]
 80ab0be:	f7f7 fc28 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80ab0c2:	b10c      	cbz	r4, 80ab0c8 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x158>
 80ab0c4:	6863      	ldr	r3, [r4, #4]
 80ab0c6:	e000      	b.n	80ab0ca <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x15a>
 80ab0c8:	463b      	mov	r3, r7
 80ab0ca:	9300      	str	r3, [sp, #0]
 80ab0cc:	462a      	mov	r2, r5
 80ab0ce:	ab0e      	add	r3, sp, #56	; 0x38
 80ab0d0:	a909      	add	r1, sp, #36	; 0x24
 80ab0d2:	a813      	add	r0, sp, #76	; 0x4c
 80ab0d4:	f7ff fd9d 	bl	80aac12 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa>
 80ab0d8:	a80e      	add	r0, sp, #56	; 0x38
 80ab0da:	f7f7 f96a 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
  reference_integer_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
 80ab0de:	a809      	add	r0, sp, #36	; 0x24
 80ab0e0:	f7f7 f967 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80ab0e4:	e008      	b.n	80ab0f8 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x188>
      break;
    case kTfLiteInt8:
      AverageEvalInt8(context, node, params, &data, input, output);
      break;
    default:
      context->ReportError(context, "Input type %s is not currently supported",
 80ab0e6:	f8d9 4014 	ldr.w	r4, [r9, #20]
 80ab0ea:	f7f5 f817 	bl	80a011c <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type));
 80ab0ee:	4907      	ldr	r1, [pc, #28]	; (80ab10c <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x19c>)
 80ab0f0:	4602      	mov	r2, r0
 80ab0f2:	4648      	mov	r0, r9
 80ab0f4:	47a0      	blx	r4
  OpData data;

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, input, output, &data));
 80ab0f6:	2701      	movs	r7, #1
      context->ReportError(context, "Input type %s is not currently supported",
                           TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
  return kTfLiteOk;
}
 80ab0f8:	4638      	mov	r0, r7
 80ab0fa:	b01f      	add	sp, #124	; 0x7c
 80ab0fc:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80ab100:	7f7fffff 	.word	0x7f7fffff
 80ab104:	40c00000 	.word	0x40c00000
 80ab108:	bf800000 	.word	0xbf800000
 80ab10c:	080b629b 	.word	0x080b629b

080ab110 <_ZN6tflite3ops5micro24Register_AVERAGE_POOL_2DEv>:
      pooling::Free,
      pooling::Prepare,
      pooling::AverageEval,
  };
  return &r;
}
 80ab110:	4800      	ldr	r0, [pc, #0]	; (80ab114 <_ZN6tflite3ops5micro24Register_AVERAGE_POOL_2DEv+0x4>)
 80ab112:	4770      	bx	lr
 80ab114:	200003a8 	.word	0x200003a8

080ab118 <_ZN6tflite3ops5micro20Register_MAX_POOL_2DEv>:

TfLiteRegistration* Register_MAX_POOL_2D() {
  static TfLiteRegistration r = {pooling::Init, pooling::Free, pooling::Prepare,
                                 pooling::MaxEval};
  return &r;
}
 80ab118:	4800      	ldr	r0, [pc, #0]	; (80ab11c <_ZN6tflite3ops5micro20Register_MAX_POOL_2DEv+0x4>)
 80ab11a:	4770      	bx	lr
 80ab11c:	200003c8 	.word	0x200003c8

080ab120 <_ZN6tflite3ops5micro11activations12PreluPrepareEP13TfLiteContextP10TfLiteNode>:
namespace micro {
namespace activations {

TfLiteStatus PreluPrepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
 80ab120:	2000      	movs	r0, #0
 80ab122:	4770      	bx	lr

080ab124 <_ZN6tflite3ops5micro14Register_PRELUEv>:

TfLiteRegistration* Register_PRELU() {
  static TfLiteRegistration r = {nullptr, nullptr, activations::PreluPrepare,
                                 activations::PreluEval};
  return &r;
}
 80ab124:	4800      	ldr	r0, [pc, #0]	; (80ab128 <_ZN6tflite3ops5micro14Register_PRELUEv+0x4>)
 80ab126:	4770      	bx	lr
 80ab128:	200003e8 	.word	0x200003e8

080ab12c <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf>:
}

inline void BroadcastPrelu4DSlowFloat(
    const RuntimeShape& unextended_input1_shape, const float* input1_data,
    const RuntimeShape& unextended_input2_shape, const float* input2_data,
    const RuntimeShape& unextended_output_shape, float* output_data) {
 80ab12c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80ab130:	4699      	mov	r9, r3
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80ab132:	6803      	ldr	r3, [r0, #0]
}

inline void BroadcastPrelu4DSlowFloat(
    const RuntimeShape& unextended_input1_shape, const float* input1_data,
    const RuntimeShape& unextended_input2_shape, const float* input2_data,
    const RuntimeShape& unextended_output_shape, float* output_data) {
 80ab134:	b09b      	sub	sp, #108	; 0x6c
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80ab136:	2b04      	cmp	r3, #4
}

inline void BroadcastPrelu4DSlowFloat(
    const RuntimeShape& unextended_input1_shape, const float* input1_data,
    const RuntimeShape& unextended_input2_shape, const float* input2_data,
    const RuntimeShape& unextended_output_shape, float* output_data) {
 80ab138:	4615      	mov	r5, r2
 80ab13a:	4604      	mov	r4, r0
 80ab13c:	4688      	mov	r8, r1
 80ab13e:	9a24      	ldr	r2, [sp, #144]	; 0x90
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
 80ab140:	dd01      	ble.n	80ab146 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x1a>
 80ab142:	f004 ff75 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
 80ab146:	682b      	ldr	r3, [r5, #0]
 80ab148:	2b04      	cmp	r3, #4
 80ab14a:	dcfa      	bgt.n	80ab142 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80ab14c:	6813      	ldr	r3, [r2, #0]
 80ab14e:	2b04      	cmp	r3, #4
 80ab150:	dcf7      	bgt.n	80ab142 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x16>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
 80ab152:	2301      	movs	r3, #1
 80ab154:	2104      	movs	r1, #4
 80ab156:	a805      	add	r0, sp, #20
 80ab158:	f7f7 f96f 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);
 80ab15c:	4620      	mov	r0, r4
 80ab15e:	ab12      	add	r3, sp, #72	; 0x48
 80ab160:	aa0a      	add	r2, sp, #40	; 0x28
 80ab162:	4629      	mov	r1, r5
 80ab164:	f7f7 fc6c 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80ab168:	2400      	movs	r4, #0
 80ab16a:	2100      	movs	r1, #0
 80ab16c:	a805      	add	r0, sp, #20
 80ab16e:	f7f7 f92b 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80ab172:	4284      	cmp	r4, r0
 80ab174:	da47      	bge.n	80ab206 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0xda>
 80ab176:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80ab178:	2101      	movs	r1, #1
 80ab17a:	a805      	add	r0, sp, #20
 80ab17c:	f7f7 f924 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80ab180:	4285      	cmp	r5, r0
 80ab182:	da3e      	bge.n	80ab202 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0xd6>
 80ab184:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80ab186:	2102      	movs	r1, #2
 80ab188:	a805      	add	r0, sp, #20
 80ab18a:	f7f7 f91d 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80ab18e:	4286      	cmp	r6, r0
 80ab190:	da35      	bge.n	80ab1fe <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0xd2>
 80ab192:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80ab194:	2103      	movs	r1, #3
 80ab196:	a805      	add	r0, sp, #20
 80ab198:	f7f7 f916 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80ab19c:	4287      	cmp	r7, r0
 80ab19e:	da2c      	bge.n	80ab1fa <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0xce>
          auto out_idx = Offset(output_shape, b, y, x, c);
 80ab1a0:	4633      	mov	r3, r6
 80ab1a2:	462a      	mov	r2, r5
 80ab1a4:	4621      	mov	r1, r4
 80ab1a6:	9700      	str	r7, [sp, #0]
 80ab1a8:	a805      	add	r0, sp, #20
 80ab1aa:	f7f7 f972 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80ab1ae:	4633      	mov	r3, r6
 80ab1b0:	462a      	mov	r2, r5
 80ab1b2:	4621      	mov	r1, r4

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
 80ab1b4:	4682      	mov	sl, r0
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80ab1b6:	9700      	str	r7, [sp, #0]
 80ab1b8:	a80a      	add	r0, sp, #40	; 0x28
 80ab1ba:	f7f7 fa1b 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
 80ab1be:	4633      	mov	r3, r6
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
 80ab1c0:	4683      	mov	fp, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
 80ab1c2:	462a      	mov	r2, r5
 80ab1c4:	4621      	mov	r1, r4
 80ab1c6:	9700      	str	r7, [sp, #0]
 80ab1c8:	a812      	add	r0, sp, #72	; 0x48
 80ab1ca:	f7f7 fa13 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
 80ab1ce:	f858 b02b 	ldr.w	fp, [r8, fp, lsl #2]
          auto in2_val = input2_data[in2_idx];
 80ab1d2:	f859 3020 	ldr.w	r3, [r9, r0, lsl #2]
          output_data[out_idx] = in1_val >= 0.0 ? in1_val : in1_val * in2_val;
 80ab1d6:	2100      	movs	r1, #0
 80ab1d8:	4658      	mov	r0, fp
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
 80ab1da:	9303      	str	r3, [sp, #12]
          output_data[out_idx] = in1_val >= 0.0 ? in1_val : in1_val * in2_val;
 80ab1dc:	f008 fbb8 	bl	80b3950 <__aeabi_fcmpge>
 80ab1e0:	9b03      	ldr	r3, [sp, #12]
 80ab1e2:	b920      	cbnz	r0, 80ab1ee <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0xc2>
 80ab1e4:	4619      	mov	r1, r3
 80ab1e6:	4658      	mov	r0, fp
 80ab1e8:	f008 fa00 	bl	80b35ec <__aeabi_fmul>
 80ab1ec:	e000      	b.n	80ab1f0 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0xc4>
 80ab1ee:	4658      	mov	r0, fp
 80ab1f0:	9b25      	ldr	r3, [sp, #148]	; 0x94
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80ab1f2:	3701      	adds	r7, #1
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = in1_val >= 0.0 ? in1_val : in1_val * in2_val;
 80ab1f4:	f843 002a 	str.w	r0, [r3, sl, lsl #2]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
 80ab1f8:	e7cc      	b.n	80ab194 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x68>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
 80ab1fa:	3601      	adds	r6, #1
 80ab1fc:	e7c3      	b.n	80ab186 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x5a>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
 80ab1fe:	3501      	adds	r5, #1
 80ab200:	e7ba      	b.n	80ab178 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x4c>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
 80ab202:	3401      	adds	r4, #1
 80ab204:	e7b1      	b.n	80ab16a <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x3e>
    const RuntimeShape& unextended_output_shape, float* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80ab206:	a805      	add	r0, sp, #20
 80ab208:	f7f7 f8d3 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = in1_val >= 0.0 ? in1_val : in1_val * in2_val;
        }
      }
    }
  }
}
 80ab20c:	b01b      	add	sp, #108	; 0x6c
 80ab20e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

080ab214 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph>:
                                 const RuntimeShape& input_shape,
                                 const uint8* input_data,
                                 const RuntimeShape& alpha_shape,
                                 const uint8* alpha_data,
                                 const RuntimeShape& output_shape,
                                 uint8* output_data) {
 80ab214:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80ab218:	461d      	mov	r5, r3
  TFLITE_DCHECK_LE(input_shape.DimensionsCount(), 4);
 80ab21a:	680b      	ldr	r3, [r1, #0]
                                 const RuntimeShape& input_shape,
                                 const uint8* input_data,
                                 const RuntimeShape& alpha_shape,
                                 const uint8* alpha_data,
                                 const RuntimeShape& output_shape,
                                 uint8* output_data) {
 80ab21c:	b09d      	sub	sp, #116	; 0x74
  TFLITE_DCHECK_LE(input_shape.DimensionsCount(), 4);
 80ab21e:	2b04      	cmp	r3, #4
                                 const RuntimeShape& input_shape,
                                 const uint8* input_data,
                                 const RuntimeShape& alpha_shape,
                                 const uint8* alpha_data,
                                 const RuntimeShape& output_shape,
                                 uint8* output_data) {
 80ab220:	9204      	str	r2, [sp, #16]
 80ab222:	4681      	mov	r9, r0
 80ab224:	460c      	mov	r4, r1
 80ab226:	9a27      	ldr	r2, [sp, #156]	; 0x9c
  TFLITE_DCHECK_LE(input_shape.DimensionsCount(), 4);
 80ab228:	dd01      	ble.n	80ab22e <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1a>
 80ab22a:	f004 ff01 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(alpha_shape.DimensionsCount(), 4);
 80ab22e:	682b      	ldr	r3, [r5, #0]
 80ab230:	2b04      	cmp	r3, #4
 80ab232:	dcfa      	bgt.n	80ab22a <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x16>
  TFLITE_DCHECK_LE(output_shape.DimensionsCount(), 4);
 80ab234:	6813      	ldr	r3, [r2, #0]
 80ab236:	2b04      	cmp	r3, #4
 80ab238:	dcf7      	bgt.n	80ab22a <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x16>
 80ab23a:	2301      	movs	r3, #1
 80ab23c:	2104      	movs	r1, #4
 80ab23e:	a807      	add	r0, sp, #28
 80ab240:	f7f7 f8fb 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  const RuntimeShape extended_output_shape =
      RuntimeShape::ExtendedShape(4, output_shape);
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input_shape, alpha_shape, &desc1, &desc2);
 80ab244:	4629      	mov	r1, r5
 80ab246:	ab14      	add	r3, sp, #80	; 0x50
 80ab248:	aa0c      	add	r2, sp, #48	; 0x30
 80ab24a:	4620      	mov	r0, r4
 80ab24c:	f7f7 fbf8 	bl	80a2a40 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>

  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
 80ab250:	2500      	movs	r5, #0
 80ab252:	2100      	movs	r1, #0
 80ab254:	a807      	add	r0, sp, #28
 80ab256:	f7f7 f8b7 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80ab25a:	4285      	cmp	r5, r0
 80ab25c:	f280 80ac 	bge.w	80ab3b8 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1a4>
 80ab260:	2600      	movs	r6, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
 80ab262:	2101      	movs	r1, #1
 80ab264:	a807      	add	r0, sp, #28
 80ab266:	f7f7 f8af 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80ab26a:	4286      	cmp	r6, r0
 80ab26c:	f280 80a2 	bge.w	80ab3b4 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1a0>
 80ab270:	2700      	movs	r7, #0
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
 80ab272:	f10d 0a1c 	add.w	sl, sp, #28
 80ab276:	2102      	movs	r1, #2
 80ab278:	4650      	mov	r0, sl
 80ab27a:	f7f7 f8a5 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80ab27e:	4287      	cmp	r7, r0
 80ab280:	f280 8096 	bge.w	80ab3b0 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x19c>
 80ab284:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
 80ab288:	2103      	movs	r1, #3
 80ab28a:	4650      	mov	r0, sl
 80ab28c:	f7f7 f89c 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80ab290:	4580      	cmp	r8, r0
 80ab292:	f280 808b 	bge.w	80ab3ac <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x198>
          int output_index = Offset(extended_output_shape, b, y, x, c);
 80ab296:	463b      	mov	r3, r7
 80ab298:	4632      	mov	r2, r6
 80ab29a:	4629      	mov	r1, r5
 80ab29c:	f8cd 8000 	str.w	r8, [sp]
 80ab2a0:	4650      	mov	r0, sl
 80ab2a2:	f7f7 f8f6 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          int input_index = SubscriptToIndex(desc1, b, y, x, c);
 80ab2a6:	463b      	mov	r3, r7

  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          int output_index = Offset(extended_output_shape, b, y, x, c);
 80ab2a8:	4683      	mov	fp, r0
          int input_index = SubscriptToIndex(desc1, b, y, x, c);
 80ab2aa:	f8cd 8000 	str.w	r8, [sp]
 80ab2ae:	4632      	mov	r2, r6
 80ab2b0:	4629      	mov	r1, r5
 80ab2b2:	a80c      	add	r0, sp, #48	; 0x30
 80ab2b4:	f7f7 f99e 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 input_value =
              params.input_offset + input_data[input_index];
 80ab2b8:	9b04      	ldr	r3, [sp, #16]
 80ab2ba:	f8d9 4000 	ldr.w	r4, [r9]
 80ab2be:	5c1b      	ldrb	r3, [r3, r0]
          if (input_value >= 0) {
 80ab2c0:	191c      	adds	r4, r3, r4
 80ab2c2:	d403      	bmi.n	80ab2cc <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0xb8>
            output_data[output_index] = input_data[input_index];
 80ab2c4:	9a28      	ldr	r2, [sp, #160]	; 0xa0
 80ab2c6:	f802 300b 	strb.w	r3, [r2, fp]
 80ab2ca:	e06c      	b.n	80ab3a6 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x192>
          } else {
            auto alpha_index = SubscriptToIndex(desc2, b, y, x, c);
 80ab2cc:	463b      	mov	r3, r7
 80ab2ce:	4632      	mov	r2, r6
 80ab2d0:	f8cd 8000 	str.w	r8, [sp]
 80ab2d4:	4629      	mov	r1, r5
 80ab2d6:	a814      	add	r0, sp, #80	; 0x50
 80ab2d8:	f7f7 f98c 	bl	80a25f4 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
            const int32 alpha_value =
                params.alpha_offset + alpha_data[alpha_index];
            const int32 unclamped_output =
                params.output_offset +
 80ab2dc:	f8d9 3008 	ldr.w	r3, [r9, #8]
                MultiplyByQuantizedMultiplierSmallerThanOneExp(
                    input_value * alpha_value, params.output_multiplier,
                    params.output_shift);
 80ab2e0:	f8d9 e010 	ldr.w	lr, [r9, #16]
          } else {
            auto alpha_index = SubscriptToIndex(desc2, b, y, x, c);
            const int32 alpha_value =
                params.alpha_offset + alpha_data[alpha_index];
            const int32 unclamped_output =
                params.output_offset +
 80ab2e4:	9303      	str	r3, [sp, #12]
                MultiplyByQuantizedMultiplierSmallerThanOneExp(
 80ab2e6:	9b26      	ldr	r3, [sp, #152]	; 0x98
 80ab2e8:	5c1a      	ldrb	r2, [r3, r0]
 80ab2ea:	f8d9 3004 	ldr.w	r3, [r9, #4]
 80ab2ee:	441a      	add	r2, r3
 80ab2f0:	4362      	muls	r2, r4
                    input_value * alpha_value, params.output_multiplier,
 80ab2f2:	f8d9 300c 	ldr.w	r3, [r9, #12]
// This function implements the same computation as the ARMv7 NEON VQRDMULH
// instruction.
template <>
inline std::int32_t SaturatingRoundingDoublingHighMul(std::int32_t a,
                                                      std::int32_t b) {
  bool overflow = a == b && a == std::numeric_limits<std::int32_t>::min();
 80ab2f6:	429a      	cmp	r2, r3
 80ab2f8:	d104      	bne.n	80ab304 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0xf0>
 80ab2fa:	f102 4100 	add.w	r1, r2, #2147483648	; 0x80000000
 80ab2fe:	424c      	negs	r4, r1
 80ab300:	414c      	adcs	r4, r1
 80ab302:	e000      	b.n	80ab306 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0xf2>
 80ab304:	2400      	movs	r4, #0
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
 80ab306:	fb82 2303 	smull	r2, r3, r2, r3
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
 80ab30a:	2a00      	cmp	r2, #0
 80ab30c:	f173 0100 	sbcs.w	r1, r3, #0
 80ab310:	482c      	ldr	r0, [pc, #176]	; (80ab3c4 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1b0>)
 80ab312:	bfa8      	it	ge
 80ab314:	f04f 4080 	movge.w	r0, #1073741824	; 0x40000000
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
 80ab318:	b994      	cbnz	r4, 80ab340 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x12c>
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
 80ab31a:	1812      	adds	r2, r2, r0
 80ab31c:	eb43 73e0 	adc.w	r3, r3, r0, asr #31
 80ab320:	2a00      	cmp	r2, #0
 80ab322:	f173 0100 	sbcs.w	r1, r3, #0
 80ab326:	da07      	bge.n	80ab338 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x124>
 80ab328:	f06f 4000 	mvn.w	r0, #2147483648	; 0x80000000
 80ab32c:	1880      	adds	r0, r0, r2
 80ab32e:	f04f 0100 	mov.w	r1, #0
 80ab332:	4159      	adcs	r1, r3
 80ab334:	4602      	mov	r2, r0
 80ab336:	460b      	mov	r3, r1
 80ab338:	0fd4      	lsrs	r4, r2, #31
 80ab33a:	ea44 0443 	orr.w	r4, r4, r3, lsl #1
 80ab33e:	e001      	b.n	80ab344 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x130>
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
 80ab340:	f06f 4400 	mvn.w	r4, #2147483648	; 0x80000000

inline int32 MultiplyByQuantizedMultiplierSmallerThanOneExp(
    int32 x, int32 quantized_multiplier, int left_shift) {
  using gemmlowp::RoundingDivideByPOT;
  using gemmlowp::SaturatingRoundingDoublingHighMul;
  return RoundingDivideByPOT(
 80ab344:	f1ce 0300 	rsb	r3, lr, #0

// Correctly-rounded-to-nearest division by a power-of-two.
// Also known as a rounding arithmetic right shift.
template <typename IntegerType>
inline IntegerType RoundingDivideByPOT(IntegerType x, int exponent) {
  assert(exponent >= 0);
 80ab348:	2b00      	cmp	r3, #0
 80ab34a:	da04      	bge.n	80ab356 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x142>
 80ab34c:	4b1e      	ldr	r3, [pc, #120]	; (80ab3c8 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1b4>)
 80ab34e:	4a1f      	ldr	r2, [pc, #124]	; (80ab3cc <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1b8>)
 80ab350:	f44f 71b3 	mov.w	r1, #358	; 0x166
 80ab354:	e005      	b.n	80ab362 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x14e>
  assert(exponent <= 31);
 80ab356:	2b1f      	cmp	r3, #31
 80ab358:	dd06      	ble.n	80ab368 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x154>
 80ab35a:	f240 1167 	movw	r1, #359	; 0x167
 80ab35e:	4b1c      	ldr	r3, [pc, #112]	; (80ab3d0 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1bc>)
 80ab360:	4a1a      	ldr	r2, [pc, #104]	; (80ab3cc <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1b8>)
 80ab362:	481c      	ldr	r0, [pc, #112]	; (80ab3d4 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1c0>)
 80ab364:	f004 fe74 	bl	80b0050 <__assert_func>
  const IntegerType mask = Dup<IntegerType>((1ll << exponent) - 1);
 80ab368:	461a      	mov	r2, r3
 80ab36a:	2001      	movs	r0, #1
 80ab36c:	2100      	movs	r1, #0
 80ab36e:	9305      	str	r3, [sp, #20]
 80ab370:	f007 fb38 	bl	80b29e4 <__aeabi_llsl>
          } else {
            auto alpha_index = SubscriptToIndex(desc2, b, y, x, c);
            const int32 alpha_value =
                params.alpha_offset + alpha_data[alpha_index];
            const int32 unclamped_output =
                params.output_offset +
 80ab374:	9b05      	ldr	r3, [sp, #20]
 80ab376:	3801      	subs	r0, #1
 80ab378:	ea00 0204 	and.w	r2, r0, r4
 80ab37c:	1040      	asrs	r0, r0, #1
 80ab37e:	fa44 f303 	asr.w	r3, r4, r3
 80ab382:	eb00 70d4 	add.w	r0, r0, r4, lsr #31
 80ab386:	4282      	cmp	r2, r0
 80ab388:	bfd4      	ite	le
 80ab38a:	461c      	movle	r4, r3
 80ab38c:	1c5c      	addgt	r4, r3, #1
 80ab38e:	9b03      	ldr	r3, [sp, #12]
 80ab390:	441c      	add	r4, r3
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
 80ab392:	2c00      	cmp	r4, #0
 80ab394:	dd03      	ble.n	80ab39e <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x18a>
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
 80ab396:	2cfe      	cmp	r4, #254	; 0xfe
 80ab398:	dd02      	ble.n	80ab3a0 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x18c>
	return __b;
      return __a;
 80ab39a:	24ff      	movs	r4, #255	; 0xff
 80ab39c:	e000      	b.n	80ab3a0 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x18c>
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
	return __b;
      return __a;
 80ab39e:	2400      	movs	r4, #0
                    params.output_shift);
            const int32 quantized_min = std::numeric_limits<uint8_t>::min();
            const int32 quantized_max = std::numeric_limits<uint8_t>::max();
            const int32 clamped_output = std::min(
                quantized_max, std::max(quantized_min, unclamped_output));
            output_data[output_index] = static_cast<uint8>(clamped_output);
 80ab3a0:	9b28      	ldr	r3, [sp, #160]	; 0xa0
 80ab3a2:	f803 400b 	strb.w	r4, [r3, fp]
  NdArrayDescsForElementwiseBroadcast(input_shape, alpha_shape, &desc1, &desc2);

  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
 80ab3a6:	f108 0801 	add.w	r8, r8, #1
 80ab3aa:	e76d      	b.n	80ab288 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x74>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input_shape, alpha_shape, &desc1, &desc2);

  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
 80ab3ac:	3701      	adds	r7, #1
 80ab3ae:	e760      	b.n	80ab272 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x5e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input_shape, alpha_shape, &desc1, &desc2);

  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
 80ab3b0:	3601      	adds	r6, #1
 80ab3b2:	e756      	b.n	80ab262 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x4e>
      RuntimeShape::ExtendedShape(4, output_shape);
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input_shape, alpha_shape, &desc1, &desc2);

  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
 80ab3b4:	3501      	adds	r5, #1
 80ab3b6:	e74c      	b.n	80ab252 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x3e>
                                 uint8* output_data) {
  TFLITE_DCHECK_LE(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(alpha_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(output_shape.DimensionsCount(), 4);
  const RuntimeShape extended_output_shape =
      RuntimeShape::ExtendedShape(4, output_shape);
 80ab3b8:	a807      	add	r0, sp, #28
 80ab3ba:	f7f6 fffa 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
          }
        }
      }
    }
  }
}
 80ab3be:	b01d      	add	sp, #116	; 0x74
 80ab3c0:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80ab3c4:	c0000001 	.word	0xc0000001
 80ab3c8:	080b596c 	.word	0x080b596c
 80ab3cc:	080b62fc 	.word	0x080b62fc
 80ab3d0:	080b5a19 	.word	0x080b5a19
 80ab3d4:	080b597a 	.word	0x080b597a

080ab3d8 <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus PreluEval(TfLiteContext* context, TfLiteNode* node) {
 80ab3d8:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
 80ab3dc:	680b      	ldr	r3, [r1, #0]
 80ab3de:	6887      	ldr	r7, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ab3e0:	689c      	ldr	r4, [r3, #8]
 80ab3e2:	4681      	mov	r9, r0
 80ab3e4:	6858      	ldr	r0, [r3, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80ab3e6:	684b      	ldr	r3, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ab3e8:	2238      	movs	r2, #56	; 0x38
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80ab3ea:	685b      	ldr	r3, [r3, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ab3ec:	fb02 f800 	mul.w	r8, r2, r0
 80ab3f0:	fb02 7404 	mla	r4, r2, r4, r7
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80ab3f4:	435a      	muls	r2, r3
 80ab3f6:	b09b      	sub	sp, #108	; 0x6c
  const TfLiteTensor* input = GetInput(context, node, 0);
  const TfLiteTensor* alpha = GetInput(context, node, 1);
  TfLiteTensor* output = GetOutput(context, node, 0);
  int32_t output_multiplier = 0;
 80ab3f8:	2300      	movs	r3, #0
 80ab3fa:	9304      	str	r3, [sp, #16]
  int output_shift = 0;
 80ab3fc:	9305      	str	r3, [sp, #20]
  if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt16) {
 80ab3fe:	5cbb      	ldrb	r3, [r7, r2]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ab400:	eb07 0608 	add.w	r6, r7, r8
 80ab404:	f003 03fb 	and.w	r3, r3, #251	; 0xfb
 80ab408:	2b03      	cmp	r3, #3
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80ab40a:	eb07 0502 	add.w	r5, r7, r2
 80ab40e:	d10c      	bne.n	80ab42a <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0x52>
    double real_multiplier =
        input->params.scale * alpha->params.scale / output->params.scale;
    QuantizeMultiplierSmallerThanOneExp(real_multiplier, &output_multiplier,
                                        &output_shift);
 80ab410:	68e1      	ldr	r1, [r4, #12]
 80ab412:	68f0      	ldr	r0, [r6, #12]
 80ab414:	f008 f8ea 	bl	80b35ec <__aeabi_fmul>
 80ab418:	68e9      	ldr	r1, [r5, #12]
 80ab41a:	f008 f99b 	bl	80b3754 <__aeabi_fdiv>
 80ab41e:	f007 fc51 	bl	80b2cc4 <__aeabi_f2d>
 80ab422:	ab05      	add	r3, sp, #20
 80ab424:	aa04      	add	r2, sp, #16
 80ab426:	f004 fbcb 	bl	80afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
  }
  switch (input->type) {
 80ab42a:	f817 0008 	ldrb.w	r0, [r7, r8]
 80ab42e:	2801      	cmp	r0, #1
 80ab430:	d028      	beq.n	80ab484 <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0xac>
 80ab432:	2803      	cmp	r0, #3
 80ab434:	d14a      	bne.n	80ab4cc <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0xf4>
          GetTensorShape(output), GetTensorData<float>(output));
      return kTfLiteOk;
    } break;
    case kTfLiteUInt8: {
      PreluParams op_params;
      op_params.input_offset = -input->params.zero_point;
 80ab436:	6933      	ldr	r3, [r6, #16]
      op_params.alpha_offset = -alpha->params.zero_point;
      op_params.output_offset = output->params.zero_point;
      op_params.output_multiplier = output_multiplier;
      op_params.output_shift = output_shift;
      reference_ops::BroadcastPrelu4DSlow(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
 80ab438:	4631      	mov	r1, r6
          GetTensorShape(output), GetTensorData<float>(output));
      return kTfLiteOk;
    } break;
    case kTfLiteUInt8: {
      PreluParams op_params;
      op_params.input_offset = -input->params.zero_point;
 80ab43a:	425b      	negs	r3, r3
 80ab43c:	9306      	str	r3, [sp, #24]
      op_params.alpha_offset = -alpha->params.zero_point;
 80ab43e:	6923      	ldr	r3, [r4, #16]
      op_params.output_offset = output->params.zero_point;
      op_params.output_multiplier = output_multiplier;
      op_params.output_shift = output_shift;
      reference_ops::BroadcastPrelu4DSlow(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
 80ab440:	a80b      	add	r0, sp, #44	; 0x2c
      return kTfLiteOk;
    } break;
    case kTfLiteUInt8: {
      PreluParams op_params;
      op_params.input_offset = -input->params.zero_point;
      op_params.alpha_offset = -alpha->params.zero_point;
 80ab442:	425b      	negs	r3, r3
 80ab444:	9307      	str	r3, [sp, #28]
      op_params.output_offset = output->params.zero_point;
 80ab446:	692b      	ldr	r3, [r5, #16]
 80ab448:	9308      	str	r3, [sp, #32]
      op_params.output_multiplier = output_multiplier;
 80ab44a:	9b04      	ldr	r3, [sp, #16]
 80ab44c:	9309      	str	r3, [sp, #36]	; 0x24
      op_params.output_shift = output_shift;
 80ab44e:	9b05      	ldr	r3, [sp, #20]
 80ab450:	930a      	str	r3, [sp, #40]	; 0x28
      reference_ops::BroadcastPrelu4DSlow(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
 80ab452:	f7f7 fa5e 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
          GetTensorShape(alpha), GetTensorData<uint8_t>(alpha),
 80ab456:	4621      	mov	r1, r4
 80ab458:	a810      	add	r0, sp, #64	; 0x40
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80ab45a:	6876      	ldr	r6, [r6, #4]
 80ab45c:	f7f7 fa59 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80ab460:	6867      	ldr	r7, [r4, #4]
          GetTensorShape(output), GetTensorData<uint8_t>(output));
 80ab462:	ac15      	add	r4, sp, #84	; 0x54
 80ab464:	4629      	mov	r1, r5
 80ab466:	4620      	mov	r0, r4
 80ab468:	f7f7 fa53 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80ab46c:	686b      	ldr	r3, [r5, #4]
 80ab46e:	a806      	add	r0, sp, #24
 80ab470:	9302      	str	r3, [sp, #8]
 80ab472:	9401      	str	r4, [sp, #4]
 80ab474:	9700      	str	r7, [sp, #0]
 80ab476:	ab10      	add	r3, sp, #64	; 0x40
 80ab478:	4632      	mov	r2, r6
 80ab47a:	a90b      	add	r1, sp, #44	; 0x2c
 80ab47c:	f7ff feca 	bl	80ab214 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph>
 80ab480:	4620      	mov	r0, r4
 80ab482:	e019      	b.n	80ab4b8 <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0xe0>
                                        &output_shift);
  }
  switch (input->type) {
    case kTfLiteFloat32: {
      BroadcastPrelu4DSlowFloat(
          GetTensorShape(input), GetTensorData<float>(input),
 80ab484:	4631      	mov	r1, r6
 80ab486:	a80b      	add	r0, sp, #44	; 0x2c
 80ab488:	f7f7 fa43 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
          GetTensorShape(alpha), GetTensorData<float>(alpha),
 80ab48c:	4621      	mov	r1, r4
 80ab48e:	a810      	add	r0, sp, #64	; 0x40
 80ab490:	6877      	ldr	r7, [r6, #4]
 80ab492:	f7f7 fa3e 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80ab496:	b104      	cbz	r4, 80ab49a <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0xc2>
 80ab498:	6864      	ldr	r4, [r4, #4]
          GetTensorShape(output), GetTensorData<float>(output));
 80ab49a:	ae15      	add	r6, sp, #84	; 0x54
 80ab49c:	4629      	mov	r1, r5
 80ab49e:	4630      	mov	r0, r6
 80ab4a0:	f7f7 fa37 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80ab4a4:	686b      	ldr	r3, [r5, #4]
 80ab4a6:	a80b      	add	r0, sp, #44	; 0x2c
 80ab4a8:	9301      	str	r3, [sp, #4]
 80ab4aa:	9600      	str	r6, [sp, #0]
 80ab4ac:	4623      	mov	r3, r4
 80ab4ae:	aa10      	add	r2, sp, #64	; 0x40
 80ab4b0:	4639      	mov	r1, r7
 80ab4b2:	f7ff fe3b 	bl	80ab12c <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf>
 80ab4b6:	4630      	mov	r0, r6
      op_params.output_multiplier = output_multiplier;
      op_params.output_shift = output_shift;
      reference_ops::BroadcastPrelu4DSlow(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(alpha), GetTensorData<uint8_t>(alpha),
          GetTensorShape(output), GetTensorData<uint8_t>(output));
 80ab4b8:	f7f6 ff7b 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
      op_params.output_offset = output->params.zero_point;
      op_params.output_multiplier = output_multiplier;
      op_params.output_shift = output_shift;
      reference_ops::BroadcastPrelu4DSlow(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(alpha), GetTensorData<uint8_t>(alpha),
 80ab4bc:	a810      	add	r0, sp, #64	; 0x40
 80ab4be:	f7f6 ff78 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
      op_params.alpha_offset = -alpha->params.zero_point;
      op_params.output_offset = output->params.zero_point;
      op_params.output_multiplier = output_multiplier;
      op_params.output_shift = output_shift;
      reference_ops::BroadcastPrelu4DSlow(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
 80ab4c2:	a80b      	add	r0, sp, #44	; 0x2c
 80ab4c4:	f7f6 ff75 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
          GetTensorShape(alpha), GetTensorData<uint8_t>(alpha),
          GetTensorShape(output), GetTensorData<uint8_t>(output));
      return kTfLiteOk;
 80ab4c8:	2000      	movs	r0, #0
 80ab4ca:	e008      	b.n	80ab4de <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0x106>
    } break;
    default:
      context->ReportError(
 80ab4cc:	f8d9 4014 	ldr.w	r4, [r9, #20]
 80ab4d0:	f7f4 fe24 	bl	80a011c <TfLiteTypeGetName>
          context, "Only float32 and uint8 are supported currently, got %d.",
          TfLiteTypeGetName(input->type));
 80ab4d4:	4903      	ldr	r1, [pc, #12]	; (80ab4e4 <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0x10c>)
 80ab4d6:	4602      	mov	r2, r0
 80ab4d8:	4648      	mov	r0, r9
 80ab4da:	47a0      	blx	r4
      return kTfLiteError;
 80ab4dc:	2001      	movs	r0, #1
  }
}
 80ab4de:	b01b      	add	sp, #108	; 0x6c
 80ab4e0:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}
 80ab4e4:	080b62c4 	.word	0x080b62c4

080ab4e8 <_ZN6tflite3ops5micro8quantize4InitEP13TfLiteContextPKcj>:
namespace micro {
namespace quantize {

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
 80ab4e8:	2000      	movs	r0, #0
 80ab4ea:	4770      	bx	lr

080ab4ec <_ZN6tflite3ops5micro8quantize4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
 80ab4ec:	4770      	bx	lr
	...

080ab4f0 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 80ab4f0:	b5f0      	push	{r4, r5, r6, r7, lr}
 80ab4f2:	680f      	ldr	r7, [r1, #0]
 80ab4f4:	b085      	sub	sp, #20
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
 80ab4f6:	683c      	ldr	r4, [r7, #0]
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
 80ab4f8:	2c01      	cmp	r4, #1
 80ab4fa:	d009      	beq.n	80ab510 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x20>
 80ab4fc:	4a32      	ldr	r2, [pc, #200]	; (80ab5c8 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xd8>)
 80ab4fe:	2501      	movs	r5, #1
 80ab500:	9201      	str	r2, [sp, #4]
 80ab502:	4a32      	ldr	r2, [pc, #200]	; (80ab5cc <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xdc>)
 80ab504:	9503      	str	r5, [sp, #12]
 80ab506:	9402      	str	r4, [sp, #8]
 80ab508:	9200      	str	r2, [sp, #0]
 80ab50a:	6944      	ldr	r4, [r0, #20]
 80ab50c:	2322      	movs	r3, #34	; 0x22
 80ab50e:	e021      	b.n	80ab554 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x64>
 80ab510:	684a      	ldr	r2, [r1, #4]
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
 80ab512:	6815      	ldr	r5, [r2, #0]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
 80ab514:	2d01      	cmp	r5, #1
 80ab516:	d00b      	beq.n	80ab530 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x40>
 80ab518:	4a2b      	ldr	r2, [pc, #172]	; (80ab5c8 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xd8>)
 80ab51a:	9403      	str	r4, [sp, #12]
 80ab51c:	9201      	str	r2, [sp, #4]
 80ab51e:	4a2c      	ldr	r2, [pc, #176]	; (80ab5d0 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe0>)
 80ab520:	9502      	str	r5, [sp, #8]
 80ab522:	9200      	str	r2, [sp, #0]
 80ab524:	6945      	ldr	r5, [r0, #20]
 80ab526:	2323      	movs	r3, #35	; 0x23
 80ab528:	4a2a      	ldr	r2, [pc, #168]	; (80ab5d4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe4>)
 80ab52a:	492b      	ldr	r1, [pc, #172]	; (80ab5d8 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe8>)
 80ab52c:	47a8      	blx	r5
 80ab52e:	e046      	b.n	80ab5be <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xce>

  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
 80ab530:	6852      	ldr	r2, [r2, #4]
 80ab532:	2138      	movs	r1, #56	; 0x38
 80ab534:	434a      	muls	r2, r1

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);

  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
 80ab536:	6886      	ldr	r6, [r0, #8]
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
 80ab538:	eb06 0e02 	add.w	lr, r6, r2

  // TODO(b/128934713): Add support for fixed-point per-channel quantization.
  // Currently this only support affine per-layer quantization.
  TF_LITE_ENSURE_EQ(context, output->quantization.type,
 80ab53c:	f89e 4030 	ldrb.w	r4, [lr, #48]	; 0x30
 80ab540:	2c01      	cmp	r4, #1
 80ab542:	d00c      	beq.n	80ab55e <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x6e>
 80ab544:	4a25      	ldr	r2, [pc, #148]	; (80ab5dc <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xec>)
 80ab546:	9503      	str	r5, [sp, #12]
 80ab548:	9201      	str	r2, [sp, #4]
 80ab54a:	4a25      	ldr	r2, [pc, #148]	; (80ab5e0 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xf0>)
 80ab54c:	9402      	str	r4, [sp, #8]
 80ab54e:	9200      	str	r2, [sp, #0]
 80ab550:	6944      	ldr	r4, [r0, #20]
 80ab552:	232b      	movs	r3, #43	; 0x2b
 80ab554:	4a1f      	ldr	r2, [pc, #124]	; (80ab5d4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe4>)
 80ab556:	4920      	ldr	r1, [pc, #128]	; (80ab5d8 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe8>)
 80ab558:	47a0      	blx	r4
 80ab55a:	4628      	mov	r0, r5
 80ab55c:	e032      	b.n	80ab5c4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xd4>
                    kTfLiteAffineQuantization);
  const auto* affine_quantization =
      reinterpret_cast<TfLiteAffineQuantization*>(output->quantization.params);
 80ab55e:	f8de 5034 	ldr.w	r5, [lr, #52]	; 0x34
  TF_LITE_ENSURE(context, affine_quantization);
 80ab562:	b925      	cbnz	r5, 80ab56e <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x7e>
 80ab564:	4a1f      	ldr	r2, [pc, #124]	; (80ab5e4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xf4>)
 80ab566:	232e      	movs	r3, #46	; 0x2e
 80ab568:	9200      	str	r2, [sp, #0]
 80ab56a:	6945      	ldr	r5, [r0, #20]
 80ab56c:	e024      	b.n	80ab5b8 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xc8>
  TF_LITE_ENSURE(context, affine_quantization->scale);
 80ab56e:	682d      	ldr	r5, [r5, #0]
 80ab570:	b925      	cbnz	r5, 80ab57c <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x8c>
 80ab572:	4a1d      	ldr	r2, [pc, #116]	; (80ab5e8 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xf8>)
 80ab574:	232f      	movs	r3, #47	; 0x2f
 80ab576:	9200      	str	r2, [sp, #0]
 80ab578:	6945      	ldr	r5, [r0, #20]
 80ab57a:	e01d      	b.n	80ab5b8 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xc8>
  TF_LITE_ENSURE(context, affine_quantization->scale->size == 1);
 80ab57c:	682d      	ldr	r5, [r5, #0]
 80ab57e:	2d01      	cmp	r5, #1
 80ab580:	d004      	beq.n	80ab58c <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x9c>
 80ab582:	4a1a      	ldr	r2, [pc, #104]	; (80ab5ec <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xfc>)
 80ab584:	2330      	movs	r3, #48	; 0x30
 80ab586:	9200      	str	r2, [sp, #0]
 80ab588:	6945      	ldr	r5, [r0, #20]
 80ab58a:	e015      	b.n	80ab5b8 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xc8>

  TF_LITE_ENSURE(context, input->type == kTfLiteFloat32);
 80ab58c:	687c      	ldr	r4, [r7, #4]
 80ab58e:	4361      	muls	r1, r4
 80ab590:	5c74      	ldrb	r4, [r6, r1]
 80ab592:	2c01      	cmp	r4, #1
 80ab594:	d007      	beq.n	80ab5a6 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xb6>
 80ab596:	4a16      	ldr	r2, [pc, #88]	; (80ab5f0 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x100>)
 80ab598:	2332      	movs	r3, #50	; 0x32
 80ab59a:	9200      	str	r2, [sp, #0]
 80ab59c:	6944      	ldr	r4, [r0, #20]
 80ab59e:	4a0d      	ldr	r2, [pc, #52]	; (80ab5d4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe4>)
 80ab5a0:	4914      	ldr	r1, [pc, #80]	; (80ab5f4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x104>)
 80ab5a2:	47a0      	blx	r4
 80ab5a4:	e7d9      	b.n	80ab55a <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x6a>
  TF_LITE_ENSURE(context,
 80ab5a6:	5cb2      	ldrb	r2, [r6, r2]
 80ab5a8:	2a03      	cmp	r2, #3
 80ab5aa:	d00a      	beq.n	80ab5c2 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xd2>
 80ab5ac:	2a09      	cmp	r2, #9
 80ab5ae:	d008      	beq.n	80ab5c2 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xd2>
 80ab5b0:	4a11      	ldr	r2, [pc, #68]	; (80ab5f8 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x108>)
 80ab5b2:	2334      	movs	r3, #52	; 0x34
 80ab5b4:	9200      	str	r2, [sp, #0]
 80ab5b6:	6945      	ldr	r5, [r0, #20]
 80ab5b8:	4a06      	ldr	r2, [pc, #24]	; (80ab5d4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe4>)
 80ab5ba:	490e      	ldr	r1, [pc, #56]	; (80ab5f4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x104>)
 80ab5bc:	47a8      	blx	r5
 80ab5be:	4620      	mov	r0, r4
 80ab5c0:	e000      	b.n	80ab5c4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xd4>
                 output->type == kTfLiteUInt8 || output->type == kTfLiteInt8);

  return kTfLiteOk;
 80ab5c2:	2000      	movs	r0, #0
}
 80ab5c4:	b005      	add	sp, #20
 80ab5c6:	bdf0      	pop	{r4, r5, r6, r7, pc}
 80ab5c8:	080b75ad 	.word	0x080b75ad
 80ab5cc:	080b5bfa 	.word	0x080b5bfa
 80ab5d0:	080b5c0a 	.word	0x080b5c0a
 80ab5d4:	080b6356 	.word	0x080b6356
 80ab5d8:	080b5be0 	.word	0x080b5be0
 80ab5dc:	080b5dff 	.word	0x080b5dff
 80ab5e0:	080b6400 	.word	0x080b6400
 80ab5e4:	080b5e33 	.word	0x080b5e33
 80ab5e8:	080b5e47 	.word	0x080b5e47
 80ab5ec:	080b641a 	.word	0x080b641a
 80ab5f0:	080b6440 	.word	0x080b6440
 80ab5f4:	080b5db0 	.word	0x080b5db0
 80ab5f8:	080b645e 	.word	0x080b645e

080ab5fc <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80ab5fc:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
 80ab600:	680b      	ldr	r3, [r1, #0]
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
 80ab602:	6849      	ldr	r1, [r1, #4]

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
 80ab604:	2238      	movs	r2, #56	; 0x38
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
 80ab606:	684d      	ldr	r5, [r1, #4]

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
 80ab608:	685b      	ldr	r3, [r3, #4]
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
 80ab60a:	4355      	muls	r5, r2

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
 80ab60c:	6887      	ldr	r7, [r0, #8]
 80ab60e:	4353      	muls	r3, r2
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
 80ab610:	197e      	adds	r6, r7, r5
                 output->type == kTfLiteUInt8 || output->type == kTfLiteInt8);

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80ab612:	b08d      	sub	sp, #52	; 0x34
 80ab614:	4683      	mov	fp, r0
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  tflite::QuantizationParams op_params;
  op_params.zero_point = output->params.zero_point;
  op_params.scale = output->params.scale;
 80ab616:	68f0      	ldr	r0, [r6, #12]

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
 80ab618:	18fc      	adds	r4, r7, r3
 80ab61a:	9301      	str	r3, [sp, #4]
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  tflite::QuantizationParams op_params;
  op_params.zero_point = output->params.zero_point;
  op_params.scale = output->params.scale;
 80ab61c:	f007 fb52 	bl	80b2cc4 <__aeabi_f2d>
  switch (output->type) {
 80ab620:	5d7a      	ldrb	r2, [r7, r5]
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  tflite::QuantizationParams op_params;
  op_params.zero_point = output->params.zero_point;
 80ab622:	f8d6 a010 	ldr.w	sl, [r6, #16]
  op_params.scale = output->params.scale;
  switch (output->type) {
 80ab626:	2a03      	cmp	r2, #3
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  tflite::QuantizationParams op_params;
  op_params.zero_point = output->params.zero_point;
  op_params.scale = output->params.scale;
 80ab628:	4680      	mov	r8, r0
 80ab62a:	4689      	mov	r9, r1
  switch (output->type) {
 80ab62c:	d02e      	beq.n	80ab68c <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x90>
 80ab62e:	2a09      	cmp	r2, #9
 80ab630:	9b01      	ldr	r3, [sp, #4]
 80ab632:	d15d      	bne.n	80ab6f0 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0xf4>
    case kTfLiteInt8:
      reference_ops::AffineQuantize(
          op_params, GetTensorShape(input), GetTensorData<float>(input),
 80ab634:	4621      	mov	r1, r4
 80ab636:	a802      	add	r0, sp, #8
 80ab638:	f7f7 f96b 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80ab63c:	b104      	cbz	r4, 80ab640 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x44>
 80ab63e:	6864      	ldr	r4, [r4, #4]
          GetTensorShape(output), GetTensorData<int8_t>(output));
 80ab640:	4631      	mov	r1, r6
 80ab642:	a807      	add	r0, sp, #28
 80ab644:	f7f7 f965 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                           const RuntimeShape& input_shape,
                           const float* input_data,
                           const RuntimeShape& output_shape, T* output_data) {
  const int32 zero_point = op_params.zero_point;
  const double scale = static_cast<double>(op_params.scale);
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
 80ab648:	a907      	add	r1, sp, #28
 80ab64a:	a802      	add	r0, sp, #8
 80ab64c:	f7fd f8d8 	bl	80a8800 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_>
 80ab650:	4607      	mov	r7, r0
 80ab652:	6876      	ldr	r6, [r6, #4]
  static constexpr int32 min_val = std::numeric_limits<T>::min();
  static constexpr int32 max_val = std::numeric_limits<T>::max();

  for (int i = 0; i < flat_size; i++) {
 80ab654:	2500      	movs	r5, #0
 80ab656:	42af      	cmp	r7, r5
 80ab658:	dd42      	ble.n	80ab6e0 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0xe4>
}
inline double TfLiteRound(const double x) { return ::round(x); }
#else
template <class T>
inline T TfLiteRound(const T x) {
  return std::round(x);
 80ab65a:	f854 0025 	ldr.w	r0, [r4, r5, lsl #2]
 80ab65e:	f007 fb31 	bl	80b2cc4 <__aeabi_f2d>
 80ab662:	4642      	mov	r2, r8
 80ab664:	464b      	mov	r3, r9
 80ab666:	f007 fcab 	bl	80b2fc0 <__aeabi_ddiv>
 80ab66a:	f005 fbcd 	bl	80b0e08 <round>
    const float val = input_data[i];
    int32 unclamped = static_cast<int32>(TfLiteRound(val / scale)) + zero_point;
 80ab66e:	f007 fe17 	bl	80b32a0 <__aeabi_d2iz>
 80ab672:	4450      	add	r0, sl
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
 80ab674:	f110 0f80 	cmn.w	r0, #128	; 0x80
 80ab678:	db03      	blt.n	80ab682 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x86>
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
 80ab67a:	287f      	cmp	r0, #127	; 0x7f
 80ab67c:	bfa8      	it	ge
 80ab67e:	207f      	movge	r0, #127	; 0x7f
 80ab680:	e001      	b.n	80ab686 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x8a>
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
	return __b;
 80ab682:	f06f 007f 	mvn.w	r0, #127	; 0x7f
    int32 clamped = std::min(std::max(unclamped, min_val), max_val);
    output_data[i] = clamped;
 80ab686:	5570      	strb	r0, [r6, r5]
  const double scale = static_cast<double>(op_params.scale);
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
  static constexpr int32 min_val = std::numeric_limits<T>::min();
  static constexpr int32 max_val = std::numeric_limits<T>::max();

  for (int i = 0; i < flat_size; i++) {
 80ab688:	3501      	adds	r5, #1
 80ab68a:	e7e4      	b.n	80ab656 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x5a>
      break;
    case kTfLiteUInt8:
      reference_ops::AffineQuantize(
          op_params, GetTensorShape(input), GetTensorData<float>(input),
 80ab68c:	4621      	mov	r1, r4
 80ab68e:	a802      	add	r0, sp, #8
 80ab690:	f7f7 f93f 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80ab694:	b104      	cbz	r4, 80ab698 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x9c>
 80ab696:	6864      	ldr	r4, [r4, #4]
          GetTensorShape(output), GetTensorData<uint8_t>(output));
 80ab698:	4631      	mov	r1, r6
 80ab69a:	a807      	add	r0, sp, #28
 80ab69c:	f7f7 f939 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                           const RuntimeShape& input_shape,
                           const float* input_data,
                           const RuntimeShape& output_shape, T* output_data) {
  const int32 zero_point = op_params.zero_point;
  const double scale = static_cast<double>(op_params.scale);
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
 80ab6a0:	a907      	add	r1, sp, #28
 80ab6a2:	a802      	add	r0, sp, #8
 80ab6a4:	f7fd f8ac 	bl	80a8800 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_>
 80ab6a8:	4607      	mov	r7, r0
 80ab6aa:	6876      	ldr	r6, [r6, #4]
  static constexpr int32 min_val = std::numeric_limits<T>::min();
  static constexpr int32 max_val = std::numeric_limits<T>::max();

  for (int i = 0; i < flat_size; i++) {
 80ab6ac:	2500      	movs	r5, #0
 80ab6ae:	42af      	cmp	r7, r5
 80ab6b0:	dd16      	ble.n	80ab6e0 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0xe4>
 80ab6b2:	f854 0025 	ldr.w	r0, [r4, r5, lsl #2]
 80ab6b6:	f007 fb05 	bl	80b2cc4 <__aeabi_f2d>
 80ab6ba:	4642      	mov	r2, r8
 80ab6bc:	464b      	mov	r3, r9
 80ab6be:	f007 fc7f 	bl	80b2fc0 <__aeabi_ddiv>
 80ab6c2:	f005 fba1 	bl	80b0e08 <round>
    const float val = input_data[i];
    int32 unclamped = static_cast<int32>(TfLiteRound(val / scale)) + zero_point;
 80ab6c6:	f007 fdeb 	bl	80b32a0 <__aeabi_d2iz>
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
 80ab6ca:	eb10 000a 	adds.w	r0, r0, sl
 80ab6ce:	d403      	bmi.n	80ab6d8 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0xdc>
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
 80ab6d0:	28ff      	cmp	r0, #255	; 0xff
 80ab6d2:	bfa8      	it	ge
 80ab6d4:	20ff      	movge	r0, #255	; 0xff
 80ab6d6:	e000      	b.n	80ab6da <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0xde>
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
	return __b;
 80ab6d8:	2000      	movs	r0, #0
    int32 clamped = std::min(std::max(unclamped, min_val), max_val);
    output_data[i] = clamped;
 80ab6da:	5570      	strb	r0, [r6, r5]
  const double scale = static_cast<double>(op_params.scale);
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
  static constexpr int32 min_val = std::numeric_limits<T>::min();
  static constexpr int32 max_val = std::numeric_limits<T>::max();

  for (int i = 0; i < flat_size; i++) {
 80ab6dc:	3501      	adds	r5, #1
 80ab6de:	e7e6      	b.n	80ab6ae <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0xb2>
 80ab6e0:	a807      	add	r0, sp, #28
 80ab6e2:	f7f6 fe66 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
          op_params, GetTensorShape(input), GetTensorData<float>(input),
          GetTensorShape(output), GetTensorData<int8_t>(output));
      break;
    case kTfLiteUInt8:
      reference_ops::AffineQuantize(
          op_params, GetTensorShape(input), GetTensorData<float>(input),
 80ab6e6:	a802      	add	r0, sp, #8
 80ab6e8:	f7f6 fe63 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(context, "Output type %s (%d) not supported",
                           TfLiteTypeGetName(input->type), output->type);
      return kTfLiteError;
  }

  return kTfLiteOk;
 80ab6ec:	2000      	movs	r0, #0
      break;
    case kTfLiteUInt8:
      reference_ops::AffineQuantize(
          op_params, GetTensorShape(input), GetTensorData<float>(input),
          GetTensorShape(output), GetTensorData<uint8_t>(output));
      break;
 80ab6ee:	e00a      	b.n	80ab706 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x10a>
    default:
      context->ReportError(context, "Output type %s (%d) not supported",
 80ab6f0:	5cf8      	ldrb	r0, [r7, r3]
 80ab6f2:	f8db 4014 	ldr.w	r4, [fp, #20]
 80ab6f6:	f7f4 fd11 	bl	80a011c <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type), output->type);
 80ab6fa:	5d7b      	ldrb	r3, [r7, r5]
 80ab6fc:	4602      	mov	r2, r0
 80ab6fe:	4903      	ldr	r1, [pc, #12]	; (80ab70c <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x110>)
 80ab700:	4658      	mov	r0, fp
 80ab702:	47a0      	blx	r4
      return kTfLiteError;
 80ab704:	2001      	movs	r0, #1
  }

  return kTfLiteOk;
}
 80ab706:	b00d      	add	sp, #52	; 0x34
 80ab708:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80ab70c:	080b649a 	.word	0x080b649a

080ab710 <_ZN6tflite3ops5micro17Register_QUANTIZEEv>:
// quantized output, in int8 or uint8 format.
TfLiteRegistration* Register_QUANTIZE() {
  static TfLiteRegistration r = {quantize::Init, quantize::Free,
                                 quantize::Prepare, quantize::Eval};
  return &r;
}
 80ab710:	4800      	ldr	r0, [pc, #0]	; (80ab714 <_ZN6tflite3ops5micro17Register_QUANTIZEEv+0x4>)
 80ab712:	4770      	bx	lr
 80ab714:	20000408 	.word	0x20000408

080ab718 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode>:
  TF_LITE_ENSURE_EQ(context, input->type, output->type);
  TF_LITE_ENSURE_EQ(context, num_input_elements, num_output_elements);
  return kTfLiteOk;
}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 80ab718:	b530      	push	{r4, r5, lr}
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
 80ab71a:	680b      	ldr	r3, [r1, #0]
 80ab71c:	b085      	sub	sp, #20
  TF_LITE_ENSURE(context, NumInputs(node) == 1 || NumInputs(node) == 2);
 80ab71e:	681b      	ldr	r3, [r3, #0]
  TF_LITE_ENSURE_EQ(context, input->type, output->type);
  TF_LITE_ENSURE_EQ(context, num_input_elements, num_output_elements);
  return kTfLiteOk;
}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 80ab720:	4602      	mov	r2, r0
  TF_LITE_ENSURE(context, NumInputs(node) == 1 || NumInputs(node) == 2);
 80ab722:	3b01      	subs	r3, #1
 80ab724:	2b01      	cmp	r3, #1
 80ab726:	d813      	bhi.n	80ab750 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x38>
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
 80ab728:	684b      	ldr	r3, [r1, #4]
 80ab72a:	681b      	ldr	r3, [r3, #0]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
 80ab72c:	2b01      	cmp	r3, #1
 80ab72e:	d00d      	beq.n	80ab74c <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x34>
 80ab730:	9302      	str	r3, [sp, #8]
 80ab732:	4b0c      	ldr	r3, [pc, #48]	; (80ab764 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x4c>)
 80ab734:	2401      	movs	r4, #1
 80ab736:	9301      	str	r3, [sp, #4]
 80ab738:	4b0b      	ldr	r3, [pc, #44]	; (80ab768 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x50>)
 80ab73a:	9403      	str	r4, [sp, #12]
 80ab73c:	9300      	str	r3, [sp, #0]
 80ab73e:	6955      	ldr	r5, [r2, #20]
 80ab740:	2348      	movs	r3, #72	; 0x48
 80ab742:	4a0a      	ldr	r2, [pc, #40]	; (80ab76c <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x54>)
 80ab744:	490a      	ldr	r1, [pc, #40]	; (80ab770 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x58>)
 80ab746:	47a8      	blx	r5
 80ab748:	4620      	mov	r0, r4
 80ab74a:	e009      	b.n	80ab760 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x48>
  return kTfLiteOk;
 80ab74c:	2000      	movs	r0, #0
 80ab74e:	e007      	b.n	80ab760 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x48>
  TF_LITE_ENSURE_EQ(context, num_input_elements, num_output_elements);
  return kTfLiteOk;
}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  TF_LITE_ENSURE(context, NumInputs(node) == 1 || NumInputs(node) == 2);
 80ab750:	4b08      	ldr	r3, [pc, #32]	; (80ab774 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x5c>)
 80ab752:	4a06      	ldr	r2, [pc, #24]	; (80ab76c <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x54>)
 80ab754:	9300      	str	r3, [sp, #0]
 80ab756:	6944      	ldr	r4, [r0, #20]
 80ab758:	2347      	movs	r3, #71	; 0x47
 80ab75a:	4907      	ldr	r1, [pc, #28]	; (80ab778 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x60>)
 80ab75c:	47a0      	blx	r4
 80ab75e:	2001      	movs	r0, #1
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
  return kTfLiteOk;
}
 80ab760:	b005      	add	sp, #20
 80ab762:	bd30      	pop	{r4, r5, pc}
 80ab764:	080b75ad 	.word	0x080b75ad
 80ab768:	080b5c0a 	.word	0x080b5c0a
 80ab76c:	080b64bc 	.word	0x080b64bc
 80ab770:	080b5be0 	.word	0x080b5be0
 80ab774:	080b6565 	.word	0x080b6565
 80ab778:	080b5db0 	.word	0x080b5db0

080ab77c <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode>:

constexpr int kInputTensor = 0;
constexpr int kShapeTensor = 1;
constexpr int kOutputTensor = 0;

TfLiteStatus ReshapeOutput(TfLiteContext* context, TfLiteNode* node) {
 80ab77c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80ab780:	f8d1 c000 	ldr.w	ip, [r1]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ab784:	2338      	movs	r3, #56	; 0x38
 80ab786:	f8dc 7004 	ldr.w	r7, [ip, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80ab78a:	6849      	ldr	r1, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ab78c:	435f      	muls	r7, r3
 80ab78e:	6886      	ldr	r6, [r0, #8]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80ab790:	684d      	ldr	r5, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ab792:	19f2      	adds	r2, r6, r7
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80ab794:	435d      	muls	r5, r3
 80ab796:	6891      	ldr	r1, [r2, #8]
 80ab798:	b085      	sub	sp, #20
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
 80ab79a:	f8d1 b000 	ldr.w	fp, [r1]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80ab79e:	eb06 0a05 	add.w	sl, r6, r5
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
 80ab7a2:	2400      	movs	r4, #0
inline int NumIntermediates(const TfLiteNode* node) {
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
 80ab7a4:	2201      	movs	r2, #1
 80ab7a6:	2300      	movs	r3, #0
  for (int i = 0; i < dims->size; ++i) {
 80ab7a8:	45a3      	cmp	fp, r4
 80ab7aa:	dd0c      	ble.n	80ab7c6 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x4a>
    count *= dims->data[i];
 80ab7ac:	f851 ef04 	ldr.w	lr, [r1, #4]!
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
 80ab7b0:	3401      	adds	r4, #1
    count *= dims->data[i];
 80ab7b2:	ea4f 79ee 	mov.w	r9, lr, asr #31
 80ab7b6:	fb02 f809 	mul.w	r8, r2, r9
 80ab7ba:	fb0e 8803 	mla	r8, lr, r3, r8
 80ab7be:	fba2 230e 	umull	r2, r3, r2, lr
 80ab7c2:	4443      	add	r3, r8
 80ab7c4:	e7f0      	b.n	80ab7a8 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x2c>
  // of output elements in the same as the number of input elements.
  int num_input_elements = NumElements(input);
  TfLiteIntArray* output_shape = output->dims;

  if (NumInputs(node) == 1 &&  // Legacy scalar supported with params.
      output_shape->size == 1 && output_shape->data[0] == 0) {
 80ab7c6:	f8dc 3000 	ldr.w	r3, [ip]
  // Tensorflow's Reshape allows one of the shape components to have the
  // special -1 value, meaning it will be calculated automatically based on the
  // input. Here we calculate what that dimension should be so that the number
  // of output elements in the same as the number of input elements.
  int num_input_elements = NumElements(input);
  TfLiteIntArray* output_shape = output->dims;
 80ab7ca:	f8da 4008 	ldr.w	r4, [sl, #8]

  if (NumInputs(node) == 1 &&  // Legacy scalar supported with params.
      output_shape->size == 1 && output_shape->data[0] == 0) {
 80ab7ce:	2b01      	cmp	r3, #1
 80ab7d0:	d105      	bne.n	80ab7de <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x62>
  // input. Here we calculate what that dimension should be so that the number
  // of output elements in the same as the number of input elements.
  int num_input_elements = NumElements(input);
  TfLiteIntArray* output_shape = output->dims;

  if (NumInputs(node) == 1 &&  // Legacy scalar supported with params.
 80ab7d2:	6823      	ldr	r3, [r4, #0]
 80ab7d4:	2b01      	cmp	r3, #1
 80ab7d6:	d102      	bne.n	80ab7de <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x62>
      output_shape->size == 1 && output_shape->data[0] == 0) {
 80ab7d8:	6863      	ldr	r3, [r4, #4]
 80ab7da:	2b00      	cmp	r3, #0
 80ab7dc:	d04c      	beq.n	80ab878 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xfc>
 80ab7de:	46a0      	mov	r8, r4
    output_shape->size = 0;
  }

  int num_output_elements = 1;
  int stretch_dim = -1;
  for (int i = 0; i < output_shape->size; ++i) {
 80ab7e0:	f8d4 9000 	ldr.w	r9, [r4]
 80ab7e4:	f04f 31ff 	mov.w	r1, #4294967295	; 0xffffffff
 80ab7e8:	2301      	movs	r3, #1
 80ab7ea:	f04f 0e00 	mov.w	lr, #0
 80ab7ee:	45ce      	cmp	lr, r9
 80ab7f0:	da18      	bge.n	80ab824 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xa8>
    int value = output_shape->data[i];
 80ab7f2:	f858 cf04 	ldr.w	ip, [r8, #4]!
    if (value == -1) {
 80ab7f6:	f1bc 3fff 	cmp.w	ip, #4294967295	; 0xffffffff
 80ab7fa:	d10c      	bne.n	80ab816 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x9a>
      TF_LITE_ENSURE_EQ(context, stretch_dim, -1);
 80ab7fc:	f1b1 3fff 	cmp.w	r1, #4294967295	; 0xffffffff
 80ab800:	d00c      	beq.n	80ab81c <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xa0>
 80ab802:	4b20      	ldr	r3, [pc, #128]	; (80ab884 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x108>)
 80ab804:	f8cd c00c 	str.w	ip, [sp, #12]
 80ab808:	9301      	str	r3, [sp, #4]
 80ab80a:	4b1f      	ldr	r3, [pc, #124]	; (80ab888 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x10c>)
 80ab80c:	9102      	str	r1, [sp, #8]
 80ab80e:	9300      	str	r3, [sp, #0]
 80ab810:	6944      	ldr	r4, [r0, #20]
 80ab812:	2336      	movs	r3, #54	; 0x36
 80ab814:	e029      	b.n	80ab86a <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xee>
      stretch_dim = i;
    } else {
      num_output_elements *= value;
 80ab816:	fb0c f303 	mul.w	r3, ip, r3
 80ab81a:	e000      	b.n	80ab81e <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xa2>
 80ab81c:	4671      	mov	r1, lr
    output_shape->size = 0;
  }

  int num_output_elements = 1;
  int stretch_dim = -1;
  for (int i = 0; i < output_shape->size; ++i) {
 80ab81e:	f10e 0e01 	add.w	lr, lr, #1
 80ab822:	e7e4      	b.n	80ab7ee <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x72>
      stretch_dim = i;
    } else {
      num_output_elements *= value;
    }
  }
  if (stretch_dim != -1) {
 80ab824:	f1b1 3fff 	cmp.w	r1, #4294967295	; 0xffffffff
    output_shape->data[stretch_dim] = num_input_elements / num_output_elements;
 80ab828:	bf1e      	ittt	ne
 80ab82a:	fb92 fef3 	sdivne	lr, r2, r3
 80ab82e:	eb04 0181 	addne.w	r1, r4, r1, lsl #2
 80ab832:	f8c1 e004 	strne.w	lr, [r1, #4]
    num_output_elements *= output_shape->data[stretch_dim];
  }

  TF_LITE_ENSURE_EQ(context, input->type, output->type);
 80ab836:	5df1      	ldrb	r1, [r6, r7]
 80ab838:	5d74      	ldrb	r4, [r6, r5]
      num_output_elements *= value;
    }
  }
  if (stretch_dim != -1) {
    output_shape->data[stretch_dim] = num_input_elements / num_output_elements;
    num_output_elements *= output_shape->data[stretch_dim];
 80ab83a:	bf18      	it	ne
 80ab83c:	fb0e f303 	mulne.w	r3, lr, r3
  }

  TF_LITE_ENSURE_EQ(context, input->type, output->type);
 80ab840:	42a1      	cmp	r1, r4
 80ab842:	d008      	beq.n	80ab856 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xda>
 80ab844:	4b11      	ldr	r3, [pc, #68]	; (80ab88c <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x110>)
 80ab846:	9403      	str	r4, [sp, #12]
 80ab848:	9301      	str	r3, [sp, #4]
 80ab84a:	4b11      	ldr	r3, [pc, #68]	; (80ab890 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x114>)
 80ab84c:	9102      	str	r1, [sp, #8]
 80ab84e:	9300      	str	r3, [sp, #0]
 80ab850:	6944      	ldr	r4, [r0, #20]
 80ab852:	2341      	movs	r3, #65	; 0x41
 80ab854:	e009      	b.n	80ab86a <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xee>
  TF_LITE_ENSURE_EQ(context, num_input_elements, num_output_elements);
 80ab856:	429a      	cmp	r2, r3
 80ab858:	d00c      	beq.n	80ab874 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xf8>
 80ab85a:	9303      	str	r3, [sp, #12]
 80ab85c:	4b0d      	ldr	r3, [pc, #52]	; (80ab894 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x118>)
 80ab85e:	9202      	str	r2, [sp, #8]
 80ab860:	9301      	str	r3, [sp, #4]
 80ab862:	4b0d      	ldr	r3, [pc, #52]	; (80ab898 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x11c>)
 80ab864:	9300      	str	r3, [sp, #0]
 80ab866:	6944      	ldr	r4, [r0, #20]
 80ab868:	2342      	movs	r3, #66	; 0x42
 80ab86a:	4a0c      	ldr	r2, [pc, #48]	; (80ab89c <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x120>)
 80ab86c:	490c      	ldr	r1, [pc, #48]	; (80ab8a0 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x124>)
 80ab86e:	47a0      	blx	r4
 80ab870:	2001      	movs	r0, #1
 80ab872:	e003      	b.n	80ab87c <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x100>
  return kTfLiteOk;
 80ab874:	2000      	movs	r0, #0
 80ab876:	e001      	b.n	80ab87c <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x100>
  if (NumInputs(node) == 1 &&  // Legacy scalar supported with params.
      output_shape->size == 1 && output_shape->data[0] == 0) {
    // Legacy tflite models use a shape parameter of [0] to indicate scalars,
    // so adjust accordingly. TODO(b/111614235): Allow zero-sized buffers during
    // toco conversion.
    output_shape->size = 0;
 80ab878:	6023      	str	r3, [r4, #0]
 80ab87a:	e7b0      	b.n	80ab7de <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x62>
  }

  TF_LITE_ENSURE_EQ(context, input->type, output->type);
  TF_LITE_ENSURE_EQ(context, num_input_elements, num_output_elements);
  return kTfLiteOk;
}
 80ab87c:	b005      	add	sp, #20
 80ab87e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80ab882:	bf00      	nop
 80ab884:	080b6592 	.word	0x080b6592
 80ab888:	080b6595 	.word	0x080b6595
 80ab88c:	080b5c27 	.word	0x080b5c27
 80ab890:	080b5c1b 	.word	0x080b5c1b
 80ab894:	080b65a1 	.word	0x080b65a1
 80ab898:	080b65b5 	.word	0x080b65b5
 80ab89c:	080b64bc 	.word	0x080b64bc
 80ab8a0:	080b5be0 	.word	0x080b5be0

080ab8a4 <_ZN6tflite3ops5micro7reshape4EvalEP13TfLiteContextP10TfLiteNode>:
  TF_LITE_ENSURE(context, NumInputs(node) == 1 || NumInputs(node) == 2);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80ab8a4:	b570      	push	{r4, r5, r6, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ab8a6:	680a      	ldr	r2, [r1, #0]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80ab8a8:	684b      	ldr	r3, [r1, #4]
 80ab8aa:	6886      	ldr	r6, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ab8ac:	6854      	ldr	r4, [r2, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80ab8ae:	685d      	ldr	r5, [r3, #4]
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  if (ReshapeOutput(context, node) != kTfLiteOk) {
 80ab8b0:	f7ff ff64 	bl	80ab77c <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode>
 80ab8b4:	b970      	cbnz	r0, 80ab8d4 <_ZN6tflite3ops5micro7reshape4EvalEP13TfLiteContextP10TfLiteNode+0x30>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ab8b6:	2338      	movs	r3, #56	; 0x38
 80ab8b8:	fb03 6204 	mla	r2, r3, r4, r6
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80ab8bc:	fb03 6505 	mla	r5, r3, r5, r6
 80ab8c0:	4603      	mov	r3, r0
    return kTfLiteError;
  }

  for (int i = 0; i < input->bytes; ++i) {
 80ab8c2:	6991      	ldr	r1, [r2, #24]
 80ab8c4:	4299      	cmp	r1, r3
 80ab8c6:	d906      	bls.n	80ab8d6 <_ZN6tflite3ops5micro7reshape4EvalEP13TfLiteContextP10TfLiteNode+0x32>
    output->data.raw[i] = input->data.raw[i];
 80ab8c8:	6851      	ldr	r1, [r2, #4]
 80ab8ca:	5ccc      	ldrb	r4, [r1, r3]
 80ab8cc:	6869      	ldr	r1, [r5, #4]
 80ab8ce:	54cc      	strb	r4, [r1, r3]
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  if (ReshapeOutput(context, node) != kTfLiteOk) {
    return kTfLiteError;
  }

  for (int i = 0; i < input->bytes; ++i) {
 80ab8d0:	3301      	adds	r3, #1
 80ab8d2:	e7f6      	b.n	80ab8c2 <_ZN6tflite3ops5micro7reshape4EvalEP13TfLiteContextP10TfLiteNode+0x1e>

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  if (ReshapeOutput(context, node) != kTfLiteOk) {
    return kTfLiteError;
 80ab8d4:	2001      	movs	r0, #1

  for (int i = 0; i < input->bytes; ++i) {
    output->data.raw[i] = input->data.raw[i];
  }
  return kTfLiteOk;
}
 80ab8d6:	bd70      	pop	{r4, r5, r6, pc}

080ab8d8 <_ZN6tflite3ops5micro16Register_RESHAPEEv>:

TfLiteRegistration* Register_RESHAPE() {
  static TfLiteRegistration r = {nullptr, nullptr, reshape::Prepare,
                                 reshape::Eval};
  return &r;
}
 80ab8d8:	4800      	ldr	r0, [pc, #0]	; (80ab8dc <_ZN6tflite3ops5micro16Register_RESHAPEEv+0x4>)
 80ab8da:	4770      	bx	lr
 80ab8dc:	20000428 	.word	0x20000428

080ab8e0 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode>:
namespace round {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 80ab8e0:	b5f0      	push	{r4, r5, r6, r7, lr}
 80ab8e2:	680b      	ldr	r3, [r1, #0]
 80ab8e4:	b085      	sub	sp, #20
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
 80ab8e6:	681e      	ldr	r6, [r3, #0]
 80ab8e8:	4605      	mov	r5, r0
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
 80ab8ea:	2e01      	cmp	r6, #1
 80ab8ec:	d009      	beq.n	80ab902 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x22>
 80ab8ee:	4b3b      	ldr	r3, [pc, #236]	; (80ab9dc <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xfc>)
 80ab8f0:	2401      	movs	r4, #1
 80ab8f2:	9301      	str	r3, [sp, #4]
 80ab8f4:	4b3a      	ldr	r3, [pc, #232]	; (80ab9e0 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x100>)
 80ab8f6:	9403      	str	r4, [sp, #12]
 80ab8f8:	9300      	str	r3, [sp, #0]
 80ab8fa:	9602      	str	r6, [sp, #8]
 80ab8fc:	6945      	ldr	r5, [r0, #20]
 80ab8fe:	2321      	movs	r3, #33	; 0x21
 80ab900:	e01e      	b.n	80ab940 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
 80ab902:	f8d1 e004 	ldr.w	lr, [r1, #4]
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
 80ab906:	f8de 4000 	ldr.w	r4, [lr]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
 80ab90a:	2c01      	cmp	r4, #1
 80ab90c:	d008      	beq.n	80ab920 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x40>
 80ab90e:	4b33      	ldr	r3, [pc, #204]	; (80ab9dc <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xfc>)
 80ab910:	9603      	str	r6, [sp, #12]
 80ab912:	9301      	str	r3, [sp, #4]
 80ab914:	4b33      	ldr	r3, [pc, #204]	; (80ab9e4 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x104>)
 80ab916:	9402      	str	r4, [sp, #8]
 80ab918:	9300      	str	r3, [sp, #0]
 80ab91a:	6944      	ldr	r4, [r0, #20]
 80ab91c:	2322      	movs	r3, #34	; 0x22
 80ab91e:	e022      	b.n	80ab966 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x86>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ab920:	6859      	ldr	r1, [r3, #4]
 80ab922:	2338      	movs	r3, #56	; 0x38
 80ab924:	4359      	muls	r1, r3
 80ab926:	6882      	ldr	r2, [r0, #8]
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
 80ab928:	5c56      	ldrb	r6, [r2, r1]
 80ab92a:	1857      	adds	r7, r2, r1
 80ab92c:	2e01      	cmp	r6, #1
 80ab92e:	d00b      	beq.n	80ab948 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x68>
 80ab930:	4b2d      	ldr	r3, [pc, #180]	; (80ab9e8 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x108>)
 80ab932:	9403      	str	r4, [sp, #12]
 80ab934:	9301      	str	r3, [sp, #4]
 80ab936:	4b2d      	ldr	r3, [pc, #180]	; (80ab9ec <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x10c>)
 80ab938:	9602      	str	r6, [sp, #8]
 80ab93a:	9300      	str	r3, [sp, #0]
 80ab93c:	6945      	ldr	r5, [r0, #20]
 80ab93e:	2323      	movs	r3, #35	; 0x23
 80ab940:	4a2b      	ldr	r2, [pc, #172]	; (80ab9f0 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x110>)
 80ab942:	492c      	ldr	r1, [pc, #176]	; (80ab9f4 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x114>)
 80ab944:	47a8      	blx	r5
 80ab946:	e042      	b.n	80ab9ce <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xee>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80ab948:	f8de 1004 	ldr.w	r1, [lr, #4]
 80ab94c:	434b      	muls	r3, r1
  TF_LITE_ENSURE_EQ(context, output->type, input->type);
 80ab94e:	5cd4      	ldrb	r4, [r2, r3]
 80ab950:	18d1      	adds	r1, r2, r3
 80ab952:	2c01      	cmp	r4, #1
 80ab954:	d00a      	beq.n	80ab96c <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x8c>
 80ab956:	4b25      	ldr	r3, [pc, #148]	; (80ab9ec <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x10c>)
 80ab958:	9603      	str	r6, [sp, #12]
 80ab95a:	9301      	str	r3, [sp, #4]
 80ab95c:	4b26      	ldr	r3, [pc, #152]	; (80ab9f8 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x118>)
 80ab95e:	9402      	str	r4, [sp, #8]
 80ab960:	9300      	str	r3, [sp, #0]
 80ab962:	6944      	ldr	r4, [r0, #20]
 80ab964:	2324      	movs	r3, #36	; 0x24
 80ab966:	4a22      	ldr	r2, [pc, #136]	; (80ab9f0 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x110>)
 80ab968:	4922      	ldr	r1, [pc, #136]	; (80ab9f4 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x114>)
 80ab96a:	e02f      	b.n	80ab9cc <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xec>
  TF_LITE_ENSURE_EQ(context, output->bytes, input->bytes);
 80ab96c:	698b      	ldr	r3, [r1, #24]
 80ab96e:	69ba      	ldr	r2, [r7, #24]
 80ab970:	4293      	cmp	r3, r2
 80ab972:	d008      	beq.n	80ab986 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xa6>
 80ab974:	9302      	str	r3, [sp, #8]
 80ab976:	4b21      	ldr	r3, [pc, #132]	; (80ab9fc <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x11c>)
 80ab978:	9203      	str	r2, [sp, #12]
 80ab97a:	9301      	str	r3, [sp, #4]
 80ab97c:	4b20      	ldr	r3, [pc, #128]	; (80aba00 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x120>)
 80ab97e:	9300      	str	r3, [sp, #0]
 80ab980:	6945      	ldr	r5, [r0, #20]
 80ab982:	2325      	movs	r3, #37	; 0x25
 80ab984:	e7dc      	b.n	80ab940 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
  TF_LITE_ENSURE_EQ(context, output->dims->size, input->dims->size);
 80ab986:	688b      	ldr	r3, [r1, #8]
 80ab988:	68ba      	ldr	r2, [r7, #8]
 80ab98a:	681e      	ldr	r6, [r3, #0]
 80ab98c:	6811      	ldr	r1, [r2, #0]
 80ab98e:	428e      	cmp	r6, r1
 80ab990:	d008      	beq.n	80ab9a4 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xc4>
 80ab992:	4b1c      	ldr	r3, [pc, #112]	; (80aba04 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x124>)
 80ab994:	9103      	str	r1, [sp, #12]
 80ab996:	9301      	str	r3, [sp, #4]
 80ab998:	4b1b      	ldr	r3, [pc, #108]	; (80aba08 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x128>)
 80ab99a:	9602      	str	r6, [sp, #8]
 80ab99c:	9300      	str	r3, [sp, #0]
 80ab99e:	6945      	ldr	r5, [r0, #20]
 80ab9a0:	2326      	movs	r3, #38	; 0x26
 80ab9a2:	e7cd      	b.n	80ab940 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
 80ab9a4:	2100      	movs	r1, #0
  for (int i = 0; i < output->dims->size; ++i) {
 80ab9a6:	42b1      	cmp	r1, r6
 80ab9a8:	da15      	bge.n	80ab9d6 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xf6>
    TF_LITE_ENSURE_EQ(context, output->dims->data[i], input->dims->data[i]);
 80ab9aa:	f853 0f04 	ldr.w	r0, [r3, #4]!
 80ab9ae:	f852 4f04 	ldr.w	r4, [r2, #4]!
 80ab9b2:	42a0      	cmp	r0, r4
 80ab9b4:	d00d      	beq.n	80ab9d2 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xf2>
 80ab9b6:	9002      	str	r0, [sp, #8]
 80ab9b8:	4628      	mov	r0, r5
 80ab9ba:	4b14      	ldr	r3, [pc, #80]	; (80aba0c <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x12c>)
 80ab9bc:	9403      	str	r4, [sp, #12]
 80ab9be:	9301      	str	r3, [sp, #4]
 80ab9c0:	4b13      	ldr	r3, [pc, #76]	; (80aba10 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x130>)
 80ab9c2:	4a0b      	ldr	r2, [pc, #44]	; (80ab9f0 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x110>)
 80ab9c4:	9300      	str	r3, [sp, #0]
 80ab9c6:	696c      	ldr	r4, [r5, #20]
 80ab9c8:	490a      	ldr	r1, [pc, #40]	; (80ab9f4 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x114>)
 80ab9ca:	2328      	movs	r3, #40	; 0x28
 80ab9cc:	47a0      	blx	r4
 80ab9ce:	2001      	movs	r0, #1
 80ab9d0:	e002      	b.n	80ab9d8 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xf8>
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
  TF_LITE_ENSURE_EQ(context, output->type, input->type);
  TF_LITE_ENSURE_EQ(context, output->bytes, input->bytes);
  TF_LITE_ENSURE_EQ(context, output->dims->size, input->dims->size);
  for (int i = 0; i < output->dims->size; ++i) {
 80ab9d2:	3101      	adds	r1, #1
 80ab9d4:	e7e7      	b.n	80ab9a6 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xc6>
    TF_LITE_ENSURE_EQ(context, output->dims->data[i], input->dims->data[i]);
  }
  return kTfLiteOk;
 80ab9d6:	2000      	movs	r0, #0
}
 80ab9d8:	b005      	add	sp, #20
 80ab9da:	bdf0      	pop	{r4, r5, r6, r7, pc}
 80ab9dc:	080b75ad 	.word	0x080b75ad
 80ab9e0:	080b5bfa 	.word	0x080b5bfa
 80ab9e4:	080b5c0a 	.word	0x080b5c0a
 80ab9e8:	080b644f 	.word	0x080b644f
 80ab9ec:	080b5c1b 	.word	0x080b5c1b
 80ab9f0:	080b65c8 	.word	0x080b65c8
 80ab9f4:	080b5be0 	.word	0x080b5be0
 80ab9f8:	080b5c27 	.word	0x080b5c27
 80ab9fc:	080b5c34 	.word	0x080b5c34
 80aba00:	080b5c41 	.word	0x080b5c41
 80aba04:	080b5c4f 	.word	0x080b5c4f
 80aba08:	080b5c61 	.word	0x080b5c61
 80aba0c:	080b5c74 	.word	0x080b5c74
 80aba10:	080b5c89 	.word	0x080b5c89

080aba14 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf>:
    return floor_val = floor_val + 1.0f;
  }
}

inline void Round(const RuntimeShape& input_shape, const float* input_data,
                  const RuntimeShape& output_shape, float* output_data) {
 80aba14:	e92d 47f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, lr}
 80aba18:	461e      	mov	r6, r3
 80aba1a:	f8d0 8000 	ldr.w	r8, [r0]

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
 80aba1e:	6813      	ldr	r3, [r2, #0]
 80aba20:	4604      	mov	r4, r0
 80aba22:	4598      	cmp	r8, r3
 80aba24:	460f      	mov	r7, r1
 80aba26:	4691      	mov	r9, r2
 80aba28:	d101      	bne.n	80aba2e <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x1a>
 80aba2a:	2500      	movs	r5, #0
 80aba2c:	e00d      	b.n	80aba4a <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x36>
 80aba2e:	f004 faff 	bl	80b0030 <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
 80aba32:	4629      	mov	r1, r5
 80aba34:	4620      	mov	r0, r4
 80aba36:	f7f6 fcc7 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80aba3a:	4629      	mov	r1, r5
 80aba3c:	4682      	mov	sl, r0
 80aba3e:	4648      	mov	r0, r9
 80aba40:	f7f6 fcc2 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80aba44:	4582      	cmp	sl, r0
 80aba46:	d1f2      	bne.n	80aba2e <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x1a>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
 80aba48:	3501      	adds	r5, #1
 80aba4a:	45a8      	cmp	r8, r5
 80aba4c:	dcf1      	bgt.n	80aba32 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x1e>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80aba4e:	f1b8 0f04 	cmp.w	r8, #4
 80aba52:	bfcc      	ite	gt
 80aba54:	6864      	ldrgt	r4, [r4, #4]
 80aba56:	3404      	addle	r4, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80aba58:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
 80aba5a:	f04f 0901 	mov.w	r9, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80aba5e:	4598      	cmp	r8, r3
 80aba60:	dc01      	bgt.n	80aba66 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x52>
 80aba62:	2400      	movs	r4, #0
 80aba64:	e018      	b.n	80aba98 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x84>
      buffer_size *= dims_data[i];
 80aba66:	f854 2023 	ldr.w	r2, [r4, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
 80aba6a:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
 80aba6c:	fb02 f909 	mul.w	r9, r2, r9
 80aba70:	e7f5      	b.n	80aba5e <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x4a>
  for (int i = 0; i < flat_size; ++i) {
    // Note that this implementation matches that of tensorFlow tf.round
    // and corresponds to the bankers rounding method.
    // cfenv (for fesetround) is not yet supported universally on Android, so
    // using a work around.
    output_data[i] = RoundToNearest(input_data[i]);
 80aba72:	f857 8024 	ldr.w	r8, [r7, r4, lsl #2]
  using ::floor;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  floor(float __x)
  { return __builtin_floorf(__x); }
 80aba76:	4640      	mov	r0, r8
 80aba78:	f005 fa96 	bl	80b0fa8 <floorf>

namespace reference_ops {

inline float RoundToNearest(float value) {
  auto floor_val = std::floor(value);
  auto diff = value - floor_val;
 80aba7c:	4601      	mov	r1, r0
 80aba7e:	4605      	mov	r5, r0
 80aba80:	4640      	mov	r0, r8
 80aba82:	f007 fca9 	bl	80b33d8 <__aeabi_fsub>
  if ((diff < 0.5f) ||
 80aba86:	f04f 517c 	mov.w	r1, #1056964608	; 0x3f000000

namespace reference_ops {

inline float RoundToNearest(float value) {
  auto floor_val = std::floor(value);
  auto diff = value - floor_val;
 80aba8a:	4680      	mov	r8, r0
  if ((diff < 0.5f) ||
 80aba8c:	f007 ff4c 	bl	80b3928 <__aeabi_fcmplt>
 80aba90:	b128      	cbz	r0, 80aba9e <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x8a>
  for (int i = 0; i < flat_size; ++i) {
    // Note that this implementation matches that of tensorFlow tf.round
    // and corresponds to the bankers rounding method.
    // cfenv (for fesetround) is not yet supported universally on Android, so
    // using a work around.
    output_data[i] = RoundToNearest(input_data[i]);
 80aba92:	f846 5024 	str.w	r5, [r6, r4, lsl #2]
}

inline void Round(const RuntimeShape& input_shape, const float* input_data,
                  const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
  for (int i = 0; i < flat_size; ++i) {
 80aba96:	3401      	adds	r4, #1
 80aba98:	454c      	cmp	r4, r9
 80aba9a:	dbea      	blt.n	80aba72 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x5e>
 80aba9c:	e011      	b.n	80abac2 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0xae>
namespace reference_ops {

inline float RoundToNearest(float value) {
  auto floor_val = std::floor(value);
  auto diff = value - floor_val;
  if ((diff < 0.5f) ||
 80aba9e:	f04f 517c 	mov.w	r1, #1056964608	; 0x3f000000
 80abaa2:	4640      	mov	r0, r8
 80abaa4:	f007 ff36 	bl	80b3914 <__aeabi_fcmpeq>
 80abaa8:	b120      	cbz	r0, 80abab4 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0xa0>
      ((diff == 0.5f) && (static_cast<int>(floor_val) % 2 == 0))) {
 80abaaa:	4628      	mov	r0, r5
 80abaac:	f007 ff7a 	bl	80b39a4 <__aeabi_f2iz>
 80abab0:	07c3      	lsls	r3, r0, #31
 80abab2:	d5ee      	bpl.n	80aba92 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x7e>
    return floor_val;
  } else {
    return floor_val = floor_val + 1.0f;
 80abab4:	4628      	mov	r0, r5
 80abab6:	f04f 517e 	mov.w	r1, #1065353216	; 0x3f800000
 80ababa:	f007 fc8f 	bl	80b33dc <__addsf3>
 80ababe:	4605      	mov	r5, r0
 80abac0:	e7e7      	b.n	80aba92 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x7e>
 80abac2:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}

080abac6 <_ZN6tflite3ops5micro5round4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80abac6:	b530      	push	{r4, r5, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80abac8:	680a      	ldr	r2, [r1, #0]
 80abaca:	6883      	ldr	r3, [r0, #8]
 80abacc:	6854      	ldr	r4, [r2, #4]
 80abace:	2538      	movs	r5, #56	; 0x38
 80abad0:	fb05 3404 	mla	r4, r5, r4, r3
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80abad4:	684a      	ldr	r2, [r1, #4]
 80abad6:	b08b      	sub	sp, #44	; 0x2c
 80abad8:	6852      	ldr	r2, [r2, #4]
 80abada:	fb05 3502 	mla	r5, r5, r2, r3
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
}

inline RuntimeShape GetTensorShape(const TfLiteTensor* tensor) {
  if (tensor == nullptr) {
 80abade:	b90c      	cbnz	r4, 80abae4 <_ZN6tflite3ops5micro5round4EvalEP13TfLiteContextP10TfLiteNode+0x1e>
  // larger shapes are separately allocated.
  static constexpr int kMaxSmallSize = 4;

  RuntimeShape& operator=(RuntimeShape const&) = delete;

  RuntimeShape() : size_(0) {}
 80abae0:	9400      	str	r4, [sp, #0]
 80abae2:	e008      	b.n	80abaf6 <_ZN6tflite3ops5micro5round4EvalEP13TfLiteContextP10TfLiteNode+0x30>
    return RuntimeShape();
  }

  TfLiteIntArray* dims = tensor->dims;
  const int dims_size = dims->size;
 80abae4:	68a2      	ldr	r2, [r4, #8]
    for (int i = 0; i < shape_size; ++i) {
      SetDim(i, value);
    }
  }

  RuntimeShape(int dimensions_count, const int32* dims_data) : size_(0) {
 80abae6:	2300      	movs	r3, #0
 80abae8:	f852 1b04 	ldr.w	r1, [r2], #4
    ReplaceWith(dimensions_count, dims_data);
 80abaec:	4668      	mov	r0, sp
    for (int i = 0; i < shape_size; ++i) {
      SetDim(i, value);
    }
  }

  RuntimeShape(int dimensions_count, const int32* dims_data) : size_(0) {
 80abaee:	9300      	str	r3, [sp, #0]
    ReplaceWith(dimensions_count, dims_data);
 80abaf0:	f7f8 f8ec 	bl	80a3ccc <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80abaf4:	6864      	ldr	r4, [r4, #4]
}

inline RuntimeShape GetTensorShape(const TfLiteTensor* tensor) {
  if (tensor == nullptr) {
 80abaf6:	b915      	cbnz	r5, 80abafe <_ZN6tflite3ops5micro5round4EvalEP13TfLiteContextP10TfLiteNode+0x38>
  // larger shapes are separately allocated.
  static constexpr int kMaxSmallSize = 4;

  RuntimeShape& operator=(RuntimeShape const&) = delete;

  RuntimeShape() : size_(0) {}
 80abaf8:	9505      	str	r5, [sp, #20]

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80abafa:	462b      	mov	r3, r5
 80abafc:	e008      	b.n	80abb10 <_ZN6tflite3ops5micro5round4EvalEP13TfLiteContextP10TfLiteNode+0x4a>
  if (tensor == nullptr) {
    return RuntimeShape();
  }

  TfLiteIntArray* dims = tensor->dims;
  const int dims_size = dims->size;
 80abafe:	68aa      	ldr	r2, [r5, #8]
    for (int i = 0; i < shape_size; ++i) {
      SetDim(i, value);
    }
  }

  RuntimeShape(int dimensions_count, const int32* dims_data) : size_(0) {
 80abb00:	2300      	movs	r3, #0
 80abb02:	f852 1b04 	ldr.w	r1, [r2], #4
    ReplaceWith(dimensions_count, dims_data);
 80abb06:	a805      	add	r0, sp, #20
    for (int i = 0; i < shape_size; ++i) {
      SetDim(i, value);
    }
  }

  RuntimeShape(int dimensions_count, const int32* dims_data) : size_(0) {
 80abb08:	9305      	str	r3, [sp, #20]
    ReplaceWith(dimensions_count, dims_data);
 80abb0a:	f7f8 f8df 	bl	80a3ccc <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80abb0e:	686b      	ldr	r3, [r5, #4]
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  reference_ops::Round(GetTensorShape(input), GetTensorData<float>(input),
                       GetTensorShape(output), GetTensorData<float>(output));
 80abb10:	aa05      	add	r2, sp, #20
 80abb12:	4621      	mov	r1, r4
 80abb14:	4668      	mov	r0, sp
 80abb16:	f7ff ff7d 	bl	80aba14 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf>
 80abb1a:	a805      	add	r0, sp, #20
 80abb1c:	f7f6 fc49 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  reference_ops::Round(GetTensorShape(input), GetTensorData<float>(input),
 80abb20:	4668      	mov	r0, sp
 80abb22:	f7f6 fc46 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                       GetTensorShape(output), GetTensorData<float>(output));

  return kTfLiteOk;
}
 80abb26:	2000      	movs	r0, #0
 80abb28:	b00b      	add	sp, #44	; 0x2c
 80abb2a:	bd30      	pop	{r4, r5, pc}

080abb2c <_ZN6tflite3ops5micro14Register_ROUNDEv>:

TfLiteRegistration* Register_ROUND() {
  static TfLiteRegistration r = {/*init=*/nullptr,
                                 /*free=*/nullptr, round::Prepare, round::Eval};
  return &r;
}
 80abb2c:	4800      	ldr	r0, [pc, #0]	; (80abb30 <_ZN6tflite3ops5micro14Register_ROUNDEv+0x4>)
 80abb2e:	4770      	bx	lr
 80abb30:	20000448 	.word	0x20000448

080abb34 <_ZN6tflite3ops5micro11activations4InitEP13TfLiteContextPKcj>:

}  // namespace

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
 80abb34:	2000      	movs	r0, #0
 80abb36:	4770      	bx	lr

080abb38 <_ZN6tflite3ops5micro11activations4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
 80abb38:	4770      	bx	lr

080abb3a <_ZN6tflite3ops5micro11activations14SoftmaxPrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus SoftmaxPrepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
 80abb3a:	2000      	movs	r0, #0
 80abb3c:	4770      	bx	lr
	...

080abb40 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>:
// This function implements the same computation as the ARMv7 NEON VQRDMULH
// instruction.
template <>
inline std::int32_t SaturatingRoundingDoublingHighMul(std::int32_t a,
                                                      std::int32_t b) {
  bool overflow = a == b && a == std::numeric_limits<std::int32_t>::min();
 80abb40:	4288      	cmp	r0, r1

// This function implements the same computation as the ARMv7 NEON VQRDMULH
// instruction.
template <>
inline std::int32_t SaturatingRoundingDoublingHighMul(std::int32_t a,
                                                      std::int32_t b) {
 80abb42:	b510      	push	{r4, lr}
  bool overflow = a == b && a == std::numeric_limits<std::int32_t>::min();
 80abb44:	d104      	bne.n	80abb50 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_+0x10>
 80abb46:	f100 4300 	add.w	r3, r0, #2147483648	; 0x80000000
 80abb4a:	425c      	negs	r4, r3
 80abb4c:	415c      	adcs	r4, r3
 80abb4e:	e000      	b.n	80abb52 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_+0x12>
 80abb50:	2400      	movs	r4, #0
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
 80abb52:	fb80 2301 	smull	r2, r3, r0, r1
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
 80abb56:	2a00      	cmp	r2, #0
 80abb58:	f173 0100 	sbcs.w	r1, r3, #0
 80abb5c:	490b      	ldr	r1, [pc, #44]	; (80abb8c <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_+0x4c>)
 80abb5e:	bfa8      	it	ge
 80abb60:	f04f 4180 	movge.w	r1, #1073741824	; 0x40000000
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
 80abb64:	b97c      	cbnz	r4, 80abb86 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_+0x46>
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
 80abb66:	1852      	adds	r2, r2, r1
 80abb68:	eb43 73e1 	adc.w	r3, r3, r1, asr #31
 80abb6c:	2a00      	cmp	r2, #0
 80abb6e:	f173 0100 	sbcs.w	r1, r3, #0
 80abb72:	da04      	bge.n	80abb7e <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_+0x3e>
 80abb74:	f06f 4000 	mvn.w	r0, #2147483648	; 0x80000000
 80abb78:	2100      	movs	r1, #0
 80abb7a:	1812      	adds	r2, r2, r0
 80abb7c:	414b      	adcs	r3, r1
 80abb7e:	0fd0      	lsrs	r0, r2, #31
 80abb80:	ea40 0043 	orr.w	r0, r0, r3, lsl #1
 80abb84:	bd10      	pop	{r4, pc}
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
 80abb86:	f06f 4000 	mvn.w	r0, #2147483648	; 0x80000000
}
 80abb8a:	bd10      	pop	{r4, pc}
 80abb8c:	c0000001 	.word	0xc0000001

080abb90 <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_>:
  return flat_size;
}

// A combination of MatchingFlatSize() and FlatSizeSkipDim().
inline int MatchingFlatSizeSkipDim(const RuntimeShape& shape, int skip_dim,
                                   const RuntimeShape& check_shape_0) {
 80abb90:	e92d 43f8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, lr}
 80abb94:	4604      	mov	r4, r0
 80abb96:	460f      	mov	r7, r1
 80abb98:	4690      	mov	r8, r2
 80abb9a:	6806      	ldr	r6, [r0, #0]
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
 80abb9c:	2500      	movs	r5, #0
 80abb9e:	42b5      	cmp	r5, r6
 80abba0:	da10      	bge.n	80abbc4 <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x34>
    if (i != skip_dim) {
 80abba2:	42bd      	cmp	r5, r7
 80abba4:	d00c      	beq.n	80abbc0 <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x30>
      TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
 80abba6:	4629      	mov	r1, r5
 80abba8:	4620      	mov	r0, r4
 80abbaa:	f7f6 fc0d 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80abbae:	4629      	mov	r1, r5
 80abbb0:	4681      	mov	r9, r0
 80abbb2:	4640      	mov	r0, r8
 80abbb4:	f7f6 fc08 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80abbb8:	4581      	cmp	r9, r0
 80abbba:	d001      	beq.n	80abbc0 <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x30>
 80abbbc:	f004 fa38 	bl	80b0030 <abort>

// A combination of MatchingFlatSize() and FlatSizeSkipDim().
inline int MatchingFlatSizeSkipDim(const RuntimeShape& shape, int skip_dim,
                                   const RuntimeShape& check_shape_0) {
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
 80abbc0:	3501      	adds	r5, #1
 80abbc2:	e7ec      	b.n	80abb9e <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0xe>
// Data is required to be contiguous, and so many operators can use either the
// full array flat size or the flat size with one dimension skipped (commonly
// the depth).
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
 80abbc4:	2f00      	cmp	r7, #0
 80abbc6:	dbf9      	blt.n	80abbbc <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x2c>
 80abbc8:	42b7      	cmp	r7, r6
 80abbca:	daf7      	bge.n	80abbbc <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x2c>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
 80abbcc:	2e04      	cmp	r6, #4
 80abbce:	bfcc      	ite	gt
 80abbd0:	6864      	ldrgt	r4, [r4, #4]
 80abbd2:	3404      	addle	r4, #4
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
  const auto* dims_data = shape.DimsData();
  int flat_size = 1;
  for (int i = 0; i < dims_count; ++i) {
 80abbd4:	2300      	movs	r3, #0
// the depth).
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
  const auto* dims_data = shape.DimsData();
  int flat_size = 1;
 80abbd6:	2001      	movs	r0, #1
  for (int i = 0; i < dims_count; ++i) {
 80abbd8:	429e      	cmp	r6, r3
 80abbda:	dd07      	ble.n	80abbec <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x5c>
    flat_size *= (i == skip_dim) ? 1 : dims_data[i];
 80abbdc:	429f      	cmp	r7, r3
 80abbde:	bf14      	ite	ne
 80abbe0:	f854 2023 	ldrne.w	r2, [r4, r3, lsl #2]
 80abbe4:	2201      	moveq	r2, #1
 80abbe6:	4350      	muls	r0, r2
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
  const auto* dims_data = shape.DimsData();
  int flat_size = 1;
  for (int i = 0; i < dims_count; ++i) {
 80abbe8:	3301      	adds	r3, #1
 80abbea:	e7f5      	b.n	80abbd8 <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x48>
    if (i != skip_dim) {
      TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
    }
  }
  return FlatSizeSkipDim(shape, skip_dim);
}
 80abbec:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}

080abbf0 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf>:
namespace tflite {
namespace reference_ops {

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const float* input_data,
                    const RuntimeShape& output_shape, float* output_data) {
 80abbf0:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80abbf4:	460d      	mov	r5, r1
  const int trailing_dim = input_shape.DimensionsCount() - 1;
 80abbf6:	680e      	ldr	r6, [r1, #0]
namespace tflite {
namespace reference_ops {

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const float* input_data,
                    const RuntimeShape& output_shape, float* output_data) {
 80abbf8:	b087      	sub	sp, #28
  const int trailing_dim = input_shape.DimensionsCount() - 1;
 80abbfa:	3e01      	subs	r6, #1
namespace tflite {
namespace reference_ops {

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const float* input_data,
                    const RuntimeShape& output_shape, float* output_data) {
 80abbfc:	9002      	str	r0, [sp, #8]
  const int trailing_dim = input_shape.DimensionsCount() - 1;
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
 80abbfe:	4631      	mov	r1, r6
 80abc00:	4628      	mov	r0, r5
namespace tflite {
namespace reference_ops {

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const float* input_data,
                    const RuntimeShape& output_shape, float* output_data) {
 80abc02:	4614      	mov	r4, r2
  const int trailing_dim = input_shape.DimensionsCount() - 1;
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
 80abc04:	461a      	mov	r2, r3
namespace tflite {
namespace reference_ops {

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const float* input_data,
                    const RuntimeShape& output_shape, float* output_data) {
 80abc06:	461f      	mov	r7, r3
  const int trailing_dim = input_shape.DimensionsCount() - 1;
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
 80abc08:	f7ff ffc2 	bl	80abb90 <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_>
}

// Get common shape dim, DCHECKing that they all agree.
inline int MatchingDim(const RuntimeShape& shape1, int index1,
                       const RuntimeShape& shape2, int index2) {
  TFLITE_DCHECK_EQ(shape1.Dims(index1), shape2.Dims(index2));
 80abc0c:	4631      	mov	r1, r6
 80abc0e:	9003      	str	r0, [sp, #12]
 80abc10:	4628      	mov	r0, r5
 80abc12:	f7f6 fbd9 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80abc16:	4631      	mov	r1, r6
 80abc18:	4605      	mov	r5, r0
 80abc1a:	4638      	mov	r0, r7
 80abc1c:	f7f6 fbd4 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80abc20:	4285      	cmp	r5, r0
 80abc22:	d001      	beq.n	80abc28 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x38>
 80abc24:	f004 fa04 	bl	80b0030 <abort>
 80abc28:	00ab      	lsls	r3, r5, #2
 80abc2a:	9f10      	ldr	r7, [sp, #64]	; 0x40
 80abc2c:	9301      	str	r3, [sp, #4]
 80abc2e:	f04f 0a00 	mov.w	sl, #0
  const int depth =
      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);

  for (int i = 0; i < outer_size; ++i) {
 80abc32:	9b03      	ldr	r3, [sp, #12]
 80abc34:	459a      	cmp	sl, r3
 80abc36:	da5e      	bge.n	80abcf6 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x106>
    // Find max element value which we'll use to ensure numerical stability
    // taking advantage of the following equality:
    // exp(x[i])/sum(exp(x[i])) == exp(x[i]+C)/sum(exp(x[i]+C))
    float max = std::numeric_limits<float>::lowest();
 80abc38:	46a1      	mov	r9, r4
 80abc3a:	f46f 0300 	mvn.w	r3, #8388608	; 0x800000
 80abc3e:	9305      	str	r3, [sp, #20]
    for (int c = 0; c < depth; ++c) {
 80abc40:	f04f 0800 	mov.w	r8, #0
 80abc44:	45a8      	cmp	r8, r5
 80abc46:	da0d      	bge.n	80abc64 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x74>
      max = std::max(max, input_data[i * depth + c]);
 80abc48:	464e      	mov	r6, r9
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
 80abc4a:	9905      	ldr	r1, [sp, #20]
 80abc4c:	6830      	ldr	r0, [r6, #0]
 80abc4e:	f109 0904 	add.w	r9, r9, #4
 80abc52:	f007 fe87 	bl	80b3964 <__aeabi_fcmpgt>
 80abc56:	b900      	cbnz	r0, 80abc5a <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x6a>
	return __b;
      return __a;
 80abc58:	ae05      	add	r6, sp, #20
 80abc5a:	6833      	ldr	r3, [r6, #0]
  for (int i = 0; i < outer_size; ++i) {
    // Find max element value which we'll use to ensure numerical stability
    // taking advantage of the following equality:
    // exp(x[i])/sum(exp(x[i])) == exp(x[i]+C)/sum(exp(x[i]+C))
    float max = std::numeric_limits<float>::lowest();
    for (int c = 0; c < depth; ++c) {
 80abc5c:	f108 0801 	add.w	r8, r8, #1
      max = std::max(max, input_data[i * depth + c]);
 80abc60:	9305      	str	r3, [sp, #20]
  for (int i = 0; i < outer_size; ++i) {
    // Find max element value which we'll use to ensure numerical stability
    // taking advantage of the following equality:
    // exp(x[i])/sum(exp(x[i])) == exp(x[i]+C)/sum(exp(x[i]+C))
    float max = std::numeric_limits<float>::lowest();
    for (int c = 0; c < depth; ++c) {
 80abc62:	e7ef      	b.n	80abc44 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x54>
 80abc64:	2600      	movs	r6, #0
 80abc66:	f04f 0b00 	mov.w	fp, #0
      max = std::max(max, input_data[i * depth + c]);
    }

    // Compute sum.
    float sum = 0.f;
    for (int c = 0; c < depth; ++c) {
 80abc6a:	42ae      	cmp	r6, r5
 80abc6c:	da1b      	bge.n	80abca6 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0xb6>
      sum += std::exp((input_data[i * depth + c] - max) * params.beta);
 80abc6e:	9905      	ldr	r1, [sp, #20]
 80abc70:	f854 0026 	ldr.w	r0, [r4, r6, lsl #2]
 80abc74:	f007 fbb0 	bl	80b33d8 <__aeabi_fsub>
 80abc78:	f007 f824 	bl	80b2cc4 <__aeabi_f2d>
 80abc7c:	9b02      	ldr	r3, [sp, #8]
      max = std::max(max, input_data[i * depth + c]);
    }

    // Compute sum.
    float sum = 0.f;
    for (int c = 0; c < depth; ++c) {
 80abc7e:	3601      	adds	r6, #1
      sum += std::exp((input_data[i * depth + c] - max) * params.beta);
 80abc80:	e9d3 2300 	ldrd	r2, r3, [r3]
 80abc84:	f007 f872 	bl	80b2d6c <__aeabi_dmul>
 80abc88:	f005 fa7a 	bl	80b1180 <exp>
 80abc8c:	4680      	mov	r8, r0
 80abc8e:	4689      	mov	r9, r1
 80abc90:	4658      	mov	r0, fp
 80abc92:	f007 f817 	bl	80b2cc4 <__aeabi_f2d>
 80abc96:	4642      	mov	r2, r8
 80abc98:	464b      	mov	r3, r9
 80abc9a:	f006 feb5 	bl	80b2a08 <__adddf3>
 80abc9e:	f007 fb47 	bl	80b3330 <__aeabi_d2f>
 80abca2:	4683      	mov	fp, r0
      max = std::max(max, input_data[i * depth + c]);
    }

    // Compute sum.
    float sum = 0.f;
    for (int c = 0; c < depth; ++c) {
 80abca4:	e7e1      	b.n	80abc6a <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x7a>
 80abca6:	2600      	movs	r6, #0
      sum += std::exp((input_data[i * depth + c] - max) * params.beta);
    }

    // Compute result.
    for (int c = 0; c < depth; ++c) {
 80abca8:	42ae      	cmp	r6, r5
 80abcaa:	da1e      	bge.n	80abcea <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0xfa>
      output_data[i * depth + c] =
          std::exp((input_data[i * depth + c] - max) * params.beta) / sum;
 80abcac:	9905      	ldr	r1, [sp, #20]
 80abcae:	f854 0026 	ldr.w	r0, [r4, r6, lsl #2]
 80abcb2:	f007 fb91 	bl	80b33d8 <__aeabi_fsub>
 80abcb6:	f007 f805 	bl	80b2cc4 <__aeabi_f2d>
 80abcba:	9b02      	ldr	r3, [sp, #8]
 80abcbc:	e9d3 2300 	ldrd	r2, r3, [r3]
 80abcc0:	f007 f854 	bl	80b2d6c <__aeabi_dmul>
 80abcc4:	f005 fa5c 	bl	80b1180 <exp>
 80abcc8:	4680      	mov	r8, r0
 80abcca:	4658      	mov	r0, fp
 80abccc:	4689      	mov	r9, r1
 80abcce:	f006 fff9 	bl	80b2cc4 <__aeabi_f2d>
 80abcd2:	4602      	mov	r2, r0
 80abcd4:	460b      	mov	r3, r1
 80abcd6:	4640      	mov	r0, r8
 80abcd8:	4649      	mov	r1, r9
 80abcda:	f007 f971 	bl	80b2fc0 <__aeabi_ddiv>
 80abcde:	f007 fb27 	bl	80b3330 <__aeabi_d2f>
 80abce2:	f847 0026 	str.w	r0, [r7, r6, lsl #2]
    for (int c = 0; c < depth; ++c) {
      sum += std::exp((input_data[i * depth + c] - max) * params.beta);
    }

    // Compute result.
    for (int c = 0; c < depth; ++c) {
 80abce6:	3601      	adds	r6, #1
 80abce8:	e7de      	b.n	80abca8 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0xb8>
 80abcea:	9b01      	ldr	r3, [sp, #4]
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
  const int depth =
      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);

  for (int i = 0; i < outer_size; ++i) {
 80abcec:	f10a 0a01 	add.w	sl, sl, #1
 80abcf0:	441c      	add	r4, r3
 80abcf2:	441f      	add	r7, r3
 80abcf4:	e79d      	b.n	80abc32 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x42>
    for (int c = 0; c < depth; ++c) {
      output_data[i * depth + c] =
          std::exp((input_data[i * depth + c] - max) * params.beta) / sum;
    }
  }
}
 80abcf6:	b007      	add	sp, #28
 80abcf8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080abcfc <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf>:
  }
}

// Performs softmax along the input of size (input_size * batch_size).
inline void Softmax(const float* in, const int input_size, const int batch_size,
                    const float beta, float* out) {
 80abcfc:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80abd00:	4604      	mov	r4, r0
 80abd02:	460f      	mov	r7, r1
 80abd04:	469b      	mov	fp, r3
 80abd06:	b085      	sub	sp, #20
 80abd08:	9e0e      	ldr	r6, [sp, #56]	; 0x38
 80abd0a:	9200      	str	r2, [sp, #0]
    for (int i = 0; i < input_size; i++) {
      out[i] *= reciprocal_sum_exp;
    }

    // Advance in and out pointers for the next batch.
    in += input_size;
 80abd0c:	ea4f 0981 	mov.w	r9, r1, lsl #2
inline void Softmax(const float* in, const int input_size, const int batch_size,
                    const float beta, float* out) {
  //  TF_LITE_ASSERT(input_size > 0);

  // For each batch
  for (int b = 0; b < batch_size; b++) {
 80abd10:	f04f 0800 	mov.w	r8, #0
 80abd14:	9b00      	ldr	r3, [sp, #0]
 80abd16:	4598      	cmp	r8, r3
 80abd18:	da46      	bge.n	80abda8 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0xac>
    // Find the max coeff.
    float max_coeff = in[0];
 80abd1a:	f8d4 a000 	ldr.w	sl, [r4]
    for (int i = 1; i < input_size; i++) {
 80abd1e:	2501      	movs	r5, #1
 80abd20:	42bd      	cmp	r5, r7
 80abd22:	db02      	blt.n	80abd2a <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x2e>
 80abd24:	2500      	movs	r5, #0
 80abd26:	2200      	movs	r2, #0
 80abd28:	e00b      	b.n	80abd42 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x46>
      if (in[i] > max_coeff) max_coeff = in[i];
 80abd2a:	f854 2025 	ldr.w	r2, [r4, r5, lsl #2]
 80abd2e:	4650      	mov	r0, sl
 80abd30:	4611      	mov	r1, r2
 80abd32:	9201      	str	r2, [sp, #4]
 80abd34:	f007 fdf8 	bl	80b3928 <__aeabi_fcmplt>
 80abd38:	b108      	cbz	r0, 80abd3e <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x42>
 80abd3a:	9a01      	ldr	r2, [sp, #4]
 80abd3c:	4692      	mov	sl, r2

  // For each batch
  for (int b = 0; b < batch_size; b++) {
    // Find the max coeff.
    float max_coeff = in[0];
    for (int i = 1; i < input_size; i++) {
 80abd3e:	3501      	adds	r5, #1
 80abd40:	e7ee      	b.n	80abd20 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x24>
      if (in[i] > max_coeff) max_coeff = in[i];
    }

    // Compute the normalized sum of exps.
    float exp_sum = 0.0;
    for (int i = 0; i < input_size; i++) {
 80abd42:	42bd      	cmp	r5, r7
 80abd44:	da14      	bge.n	80abd70 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x74>
  using ::exp;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  exp(float __x)
  { return __builtin_expf(__x); }
 80abd46:	f854 0025 	ldr.w	r0, [r4, r5, lsl #2]
 80abd4a:	4651      	mov	r1, sl
 80abd4c:	9201      	str	r2, [sp, #4]
 80abd4e:	f007 fb43 	bl	80b33d8 <__aeabi_fsub>
 80abd52:	4659      	mov	r1, fp
 80abd54:	f007 fc4a 	bl	80b35ec <__aeabi_fmul>
 80abd58:	f005 fa8e 	bl	80b1278 <expf>
      out[i] = std::exp((in[i] - max_coeff) * beta);
      exp_sum += out[i];
 80abd5c:	9a01      	ldr	r2, [sp, #4]
    }

    // Compute the normalized sum of exps.
    float exp_sum = 0.0;
    for (int i = 0; i < input_size; i++) {
      out[i] = std::exp((in[i] - max_coeff) * beta);
 80abd5e:	f846 0025 	str.w	r0, [r6, r5, lsl #2]
 80abd62:	4601      	mov	r1, r0
      exp_sum += out[i];
 80abd64:	4610      	mov	r0, r2
 80abd66:	f007 fb39 	bl	80b33dc <__addsf3>
      if (in[i] > max_coeff) max_coeff = in[i];
    }

    // Compute the normalized sum of exps.
    float exp_sum = 0.0;
    for (int i = 0; i < input_size; i++) {
 80abd6a:	3501      	adds	r5, #1
      out[i] = std::exp((in[i] - max_coeff) * beta);
      exp_sum += out[i];
 80abd6c:	4602      	mov	r2, r0
      if (in[i] > max_coeff) max_coeff = in[i];
    }

    // Compute the normalized sum of exps.
    float exp_sum = 0.0;
    for (int i = 0; i < input_size; i++) {
 80abd6e:	e7e8      	b.n	80abd42 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x46>
      out[i] = std::exp((in[i] - max_coeff) * beta);
      exp_sum += out[i];
    }

    // Divide by the sum of exps.
    float reciprocal_sum_exp = 1.f / exp_sum;
 80abd70:	4611      	mov	r1, r2
 80abd72:	f04f 507e 	mov.w	r0, #1065353216	; 0x3f800000
 80abd76:	f007 fced 	bl	80b3754 <__aeabi_fdiv>
 80abd7a:	4602      	mov	r2, r0
 80abd7c:	1f33      	subs	r3, r6, #4
    for (int i = 0; i < input_size; i++) {
 80abd7e:	2500      	movs	r5, #0
 80abd80:	42bd      	cmp	r5, r7
 80abd82:	da0c      	bge.n	80abd9e <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0xa2>
      out[i] *= reciprocal_sum_exp;
 80abd84:	f853 0f04 	ldr.w	r0, [r3, #4]!
 80abd88:	4611      	mov	r1, r2
 80abd8a:	9202      	str	r2, [sp, #8]
 80abd8c:	9301      	str	r3, [sp, #4]
 80abd8e:	9303      	str	r3, [sp, #12]
 80abd90:	f007 fc2c 	bl	80b35ec <__aeabi_fmul>
 80abd94:	9b01      	ldr	r3, [sp, #4]
      exp_sum += out[i];
    }

    // Divide by the sum of exps.
    float reciprocal_sum_exp = 1.f / exp_sum;
    for (int i = 0; i < input_size; i++) {
 80abd96:	3501      	adds	r5, #1
      out[i] *= reciprocal_sum_exp;
 80abd98:	6018      	str	r0, [r3, #0]
      exp_sum += out[i];
    }

    // Divide by the sum of exps.
    float reciprocal_sum_exp = 1.f / exp_sum;
    for (int i = 0; i < input_size; i++) {
 80abd9a:	9a02      	ldr	r2, [sp, #8]
 80abd9c:	e7f0      	b.n	80abd80 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x84>
      out[i] *= reciprocal_sum_exp;
    }

    // Advance in and out pointers for the next batch.
    in += input_size;
 80abd9e:	444c      	add	r4, r9
    out += input_size;
 80abda0:	444e      	add	r6, r9
inline void Softmax(const float* in, const int input_size, const int batch_size,
                    const float beta, float* out) {
  //  TF_LITE_ASSERT(input_size > 0);

  // For each batch
  for (int b = 0; b < batch_size; b++) {
 80abda2:	f108 0801 	add.w	r8, r8, #1
 80abda6:	e7b5      	b.n	80abd14 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x18>

    // Advance in and out pointers for the next batch.
    in += input_size;
    out += input_size;
  }
}
 80abda8:	b005      	add	sp, #20
 80abdaa:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080abdae <_ZN6tflite3ops5micro11activations14Softmax1DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams>:

// Takes a 1D tensor and performs softmax along it.
void Softmax1DFloat(const TfLiteTensor* input, TfLiteTensor* output,
                    TfLiteSoftmaxParams* params) {
 80abdae:	b513      	push	{r0, r1, r4, lr}
  const int input_size = input->dims->data[0];
  tflite::reference_ops::Softmax(input->data.f, input_size, 1, params->beta,
                                 output->data.f);
 80abdb0:	684b      	ldr	r3, [r1, #4]
}

// Takes a 1D tensor and performs softmax along it.
void Softmax1DFloat(const TfLiteTensor* input, TfLiteTensor* output,
                    TfLiteSoftmaxParams* params) {
  const int input_size = input->dims->data[0];
 80abdb2:	6884      	ldr	r4, [r0, #8]
  tflite::reference_ops::Softmax(input->data.f, input_size, 1, params->beta,
                                 output->data.f);
 80abdb4:	9300      	str	r3, [sp, #0]
 80abdb6:	6813      	ldr	r3, [r2, #0]
 80abdb8:	6861      	ldr	r1, [r4, #4]
 80abdba:	2201      	movs	r2, #1
 80abdbc:	6840      	ldr	r0, [r0, #4]
 80abdbe:	f7ff ff9d 	bl	80abcfc <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf>
}
 80abdc2:	b002      	add	sp, #8
 80abdc4:	bd10      	pop	{r4, pc}

080abdc6 <_ZN6tflite3ops5micro11activations14Softmax2DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams>:

// Takes a 2D tensor and perform softmax along the last dimension.
void Softmax2DFloat(const TfLiteTensor* input, TfLiteTensor* output,
                    TfLiteSoftmaxParams* params) {
 80abdc6:	b513      	push	{r0, r1, r4, lr}
  const int batch_size = input->dims->data[0];
  const int input_size = input->dims->data[1];
  tflite::reference_ops::Softmax(input->data.f, input_size, batch_size,
                                 params->beta, output->data.f);
 80abdc8:	684b      	ldr	r3, [r1, #4]
}

// Takes a 2D tensor and perform softmax along the last dimension.
void Softmax2DFloat(const TfLiteTensor* input, TfLiteTensor* output,
                    TfLiteSoftmaxParams* params) {
  const int batch_size = input->dims->data[0];
 80abdca:	6884      	ldr	r4, [r0, #8]
  const int input_size = input->dims->data[1];
  tflite::reference_ops::Softmax(input->data.f, input_size, batch_size,
                                 params->beta, output->data.f);
 80abdcc:	9300      	str	r3, [sp, #0]
 80abdce:	6813      	ldr	r3, [r2, #0]
 80abdd0:	68a1      	ldr	r1, [r4, #8]
 80abdd2:	6862      	ldr	r2, [r4, #4]
 80abdd4:	6840      	ldr	r0, [r0, #4]
 80abdd6:	f7ff ff91 	bl	80abcfc <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf>
}
 80abdda:	b002      	add	sp, #8
 80abddc:	bd10      	pop	{r4, pc}

080abdde <_ZN6tflite3ops5micro11activations14Softmax4DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams>:
                                 GetTensorData<uint8_t>(output));
}

// Takes a 4D tensor and perform softmax along the forth dimension.
void Softmax4DFloat(const TfLiteTensor* input, TfLiteTensor* output,
                    TfLiteSoftmaxParams* params) {
 80abdde:	b530      	push	{r4, r5, lr}
 80abde0:	4604      	mov	r4, r0
 80abde2:	b097      	sub	sp, #92	; 0x5c
  SoftmaxParams op_params;
  op_params.beta = params->beta;
 80abde4:	6810      	ldr	r0, [r2, #0]
                                 GetTensorData<uint8_t>(output));
}

// Takes a 4D tensor and perform softmax along the forth dimension.
void Softmax4DFloat(const TfLiteTensor* input, TfLiteTensor* output,
                    TfLiteSoftmaxParams* params) {
 80abde6:	460d      	mov	r5, r1
  SoftmaxParams op_params;
  op_params.beta = params->beta;
 80abde8:	f006 ff6c 	bl	80b2cc4 <__aeabi_f2d>
 80abdec:	e9cd 010c 	strd	r0, r1, [sp, #48]	; 0x30
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
 80abdf0:	4621      	mov	r1, r4
 80abdf2:	a802      	add	r0, sp, #8
 80abdf4:	f7f6 fd8d 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80abdf8:	b104      	cbz	r4, 80abdfc <_ZN6tflite3ops5micro11activations14Softmax4DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams+0x1e>
 80abdfa:	6864      	ldr	r4, [r4, #4]
      GetTensorShape(output), GetTensorData<float>(output));
 80abdfc:	4629      	mov	r1, r5
 80abdfe:	a807      	add	r0, sp, #28
 80abe00:	f7f6 fd87 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80abe04:	b105      	cbz	r5, 80abe08 <_ZN6tflite3ops5micro11activations14Softmax4DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams+0x2a>
 80abe06:	686d      	ldr	r5, [r5, #4]
 80abe08:	ab07      	add	r3, sp, #28
 80abe0a:	4622      	mov	r2, r4
 80abe0c:	a902      	add	r1, sp, #8
 80abe0e:	a80c      	add	r0, sp, #48	; 0x30
 80abe10:	9500      	str	r5, [sp, #0]
 80abe12:	f7ff feed 	bl	80abbf0 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf>
 80abe16:	a807      	add	r0, sp, #28
 80abe18:	f7f6 facb 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
void Softmax4DFloat(const TfLiteTensor* input, TfLiteTensor* output,
                    TfLiteSoftmaxParams* params) {
  SoftmaxParams op_params;
  op_params.beta = params->beta;
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
 80abe1c:	a802      	add	r0, sp, #8
 80abe1e:	f7f6 fac8 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
      GetTensorShape(output), GetTensorData<float>(output));
}
 80abe22:	b017      	add	sp, #92	; 0x5c
 80abe24:	bd30      	pop	{r4, r5, pc}
	...

080abe28 <_ZN6tflite3ops5micro16Register_SOFTMAXEv>:
TfLiteRegistration* Register_SOFTMAX() {
  static TfLiteRegistration r = {activations::Init, activations::Free,
                                 activations::SoftmaxPrepare,
                                 activations::SoftmaxEval};
  return &r;
}
 80abe28:	4800      	ldr	r0, [pc, #0]	; (80abe2c <_ZN6tflite3ops5micro16Register_SOFTMAXEv+0x4>)
 80abe2a:	4770      	bx	lr
 80abe2c:	20000468 	.word	0x20000468

080abe30 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i>:
}

// Correctly-rounded-to-nearest division by a power-of-two.
// Also known as a rounding arithmetic right shift.
template <typename IntegerType>
inline IntegerType RoundingDivideByPOT(IntegerType x, int exponent) {
 80abe30:	b538      	push	{r3, r4, r5, lr}
  assert(exponent >= 0);
 80abe32:	1e0d      	subs	r5, r1, #0
}

// Correctly-rounded-to-nearest division by a power-of-two.
// Also known as a rounding arithmetic right shift.
template <typename IntegerType>
inline IntegerType RoundingDivideByPOT(IntegerType x, int exponent) {
 80abe34:	4604      	mov	r4, r0
  assert(exponent >= 0);
 80abe36:	da04      	bge.n	80abe42 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x12>
 80abe38:	4b0f      	ldr	r3, [pc, #60]	; (80abe78 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x48>)
 80abe3a:	4a10      	ldr	r2, [pc, #64]	; (80abe7c <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x4c>)
 80abe3c:	f44f 71b3 	mov.w	r1, #358	; 0x166
 80abe40:	e005      	b.n	80abe4e <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x1e>
  assert(exponent <= 31);
 80abe42:	2d1f      	cmp	r5, #31
 80abe44:	dd06      	ble.n	80abe54 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x24>
 80abe46:	f240 1167 	movw	r1, #359	; 0x167
 80abe4a:	4b0d      	ldr	r3, [pc, #52]	; (80abe80 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x50>)
 80abe4c:	4a0b      	ldr	r2, [pc, #44]	; (80abe7c <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x4c>)
 80abe4e:	480d      	ldr	r0, [pc, #52]	; (80abe84 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x54>)
 80abe50:	f004 f8fe 	bl	80b0050 <__assert_func>
  const IntegerType mask = Dup<IntegerType>((1ll << exponent) - 1);
 80abe54:	462a      	mov	r2, r5
 80abe56:	2001      	movs	r0, #1
 80abe58:	2100      	movs	r1, #0
 80abe5a:	f006 fdc3 	bl	80b29e4 <__aeabi_llsl>
 80abe5e:	3801      	subs	r0, #1
  const IntegerType one = Dup<IntegerType>(1);
  const IntegerType remainder = BitAnd(x, mask);
  const IntegerType threshold =
      Add(ShiftRight(mask, 1), BitAnd(MaskIfLessThan(x, zero), one));
  return Add(ShiftRight(x, exponent),
             BitAnd(MaskIfGreaterThan(remainder, threshold), one));
 80abe60:	1043      	asrs	r3, r0, #1
 80abe62:	ea00 0204 	and.w	r2, r0, r4
 80abe66:	eb03 73d4 	add.w	r3, r3, r4, lsr #31
 80abe6a:	fa44 f005 	asr.w	r0, r4, r5
}
 80abe6e:	429a      	cmp	r2, r3
 80abe70:	bfc8      	it	gt
 80abe72:	3001      	addgt	r0, #1
 80abe74:	bd38      	pop	{r3, r4, r5, pc}
 80abe76:	bf00      	nop
 80abe78:	080b596c 	.word	0x080b596c
 80abe7c:	080b67f8 	.word	0x080b67f8
 80abe80:	080b5a19 	.word	0x080b5a19
 80abe84:	080b597a 	.word	0x080b597a

080abe88 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>:
// A FixedPoint multiplication is just a
// SaturatingRoundingDoublingHighMul operation on the underlying
// raw integer values. The IntegerBits simply add up, as is obvious
// from the fact that the range is [-2^IntegerBits, 2^IntegerBits).
template <typename tRawType, int tIntegerBits_a, int tIntegerBits_b>
FixedPoint<tRawType, tIntegerBits_a + tIntegerBits_b> operator*(
 80abe88:	b508      	push	{r3, lr}
    FixedPoint<tRawType, tIntegerBits_a> a,
    FixedPoint<tRawType, tIntegerBits_b> b) {
  FixedPoint<tRawType, tIntegerBits_a + tIntegerBits_b> c;
  c.raw() = SaturatingRoundingDoublingHighMul(a.raw(), b.raw());
 80abe8a:	f7ff fe59 	bl	80abb40 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>
  return c;
}
 80abe8e:	bd08      	pop	{r3, pc}

080abe90 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>:
// fixed-point value, regardless of the actual Scalar type. This allows
// writing generic code that applies just as well to the 32-bit and 16-bit
// cases. In the 16-bit case, the raw integer value is internally
// rounding-shifted by 16 bits to the right.
template <typename FixedPointType>
inline typename FixedPointType::ScalarRawType RescaleConstantInitializer(
 80abe90:	b508      	push	{r3, lr}
    std::int32_t int32_value) {
  typedef typename FixedPointType::ScalarRawType ScalarRawType;
  static constexpr int ScalarTypeBits = 8 * sizeof(ScalarRawType);
  return static_cast<ScalarRawType>(
      RoundingDivideByPOT<std::int32_t>(int32_value, 32 - ScalarTypeBits));
 80abe92:	2100      	movs	r1, #0
 80abe94:	f7ff ffcc 	bl	80abe30 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i>
}
 80abe98:	bd08      	pop	{r3, pc}
	...

080abe9c <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_>:

// Implementation of logistic function.

// Returns 1 / (1 + x) for x in (0, 1).
template <typename tRawType>
FixedPoint<tRawType, 0> one_over_one_plus_x_for_x_in_0_1(
 80abe9c:	e92d 4ff8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, lr}

template <>
inline std::int32_t RoundingHalfSum(std::int32_t a, std::int32_t b) {
  std::int64_t a64 = a;
  std::int64_t b64 = b;
  std::int64_t sum = a64 + b64;
 80abea0:	f06f 4600 	mvn.w	r6, #2147483648	; 0x80000000
 80abea4:	1833      	adds	r3, r6, r0
 80abea6:	f04f 0700 	mov.w	r7, #0
 80abeaa:	eb47 74e0 	adc.w	r4, r7, r0, asr #31
 80abeae:	4618      	mov	r0, r3
  std::int64_t sign = sum >= 0 ? 1 : -1;
 80abeb0:	1c63      	adds	r3, r4, #1
 80abeb2:	bf03      	ittte	eq
 80abeb4:	f1b0 3fff 	cmpeq.w	r0, #4294967295	; 0xffffffff
 80abeb8:	4602      	moveq	r2, r0
 80abeba:	4623      	moveq	r3, r4
 80abebc:	2201      	movne	r2, #1
 80abebe:	bf18      	it	ne
 80abec0:	2300      	movne	r3, #0
  return static_cast<std::int32_t>((sum + sign) / 2);
 80abec2:	1886      	adds	r6, r0, r2
 80abec4:	eb44 0703 	adc.w	r7, r4, r3
 80abec8:	0ffb      	lsrs	r3, r7, #31
 80abeca:	18f6      	adds	r6, r6, r3
  F0 half_denominator = RoundingHalfSum(a, F0::One());
  // Newton-Raphson division
  // https://en.wikipedia.org/wiki/Division_algorithm#Newton.E2.80.93Raphson_division
  // Refer to that page for the logic behind the 48/17 and 32/17 constants.
  const F2 constant_48_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, 1515870810, 48.0 / 17.0);
 80abecc:	f04f 305a 	mov.w	r0, #1515870810	; 0x5a5a5a5a
inline std::int32_t RoundingHalfSum(std::int32_t a, std::int32_t b) {
  std::int64_t a64 = a;
  std::int64_t b64 = b;
  std::int64_t sum = a64 + b64;
  std::int64_t sign = sum >= 0 ? 1 : -1;
  return static_cast<std::int32_t>((sum + sign) / 2);
 80abed0:	f147 0700 	adc.w	r7, r7, #0
  F0 half_denominator = RoundingHalfSum(a, F0::One());
  // Newton-Raphson division
  // https://en.wikipedia.org/wiki/Division_algorithm#Newton.E2.80.93Raphson_division
  // Refer to that page for the logic behind the 48/17 and 32/17 constants.
  const F2 constant_48_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, 1515870810, 48.0 / 17.0);
 80abed4:	f7ff ffdc 	bl	80abe90 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
 80abed8:	4604      	mov	r4, r0
  const F2 constant_neg_32_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, -1010580540, -32.0 / 17.0);
 80abeda:	4840      	ldr	r0, [pc, #256]	; (80abfdc <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x140>)
inline std::int32_t RoundingHalfSum(std::int32_t a, std::int32_t b) {
  std::int64_t a64 = a;
  std::int64_t b64 = b;
  std::int64_t sum = a64 + b64;
  std::int64_t sign = sum >= 0 ? 1 : -1;
  return static_cast<std::int32_t>((sum + sign) / 2);
 80abedc:	107f      	asrs	r7, r7, #1
 80abede:	ea4f 0636 	mov.w	r6, r6, rrx
  // https://en.wikipedia.org/wiki/Division_algorithm#Newton.E2.80.93Raphson_division
  // Refer to that page for the logic behind the 48/17 and 32/17 constants.
  const F2 constant_48_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, 1515870810, 48.0 / 17.0);
  const F2 constant_neg_32_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, -1010580540, -32.0 / 17.0);
 80abee2:	f7ff ffd5 	bl	80abe90 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
template <typename tRawType, int tIntegerBits_a, int tIntegerBits_b>
FixedPoint<tRawType, tIntegerBits_a + tIntegerBits_b> operator*(
    FixedPoint<tRawType, tIntegerBits_a> a,
    FixedPoint<tRawType, tIntegerBits_b> b) {
  FixedPoint<tRawType, tIntegerBits_a + tIntegerBits_b> c;
  c.raw() = SaturatingRoundingDoublingHighMul(a.raw(), b.raw());
 80abee6:	4601      	mov	r1, r0
 80abee8:	4630      	mov	r0, r6
 80abeea:	f7ff fe29 	bl	80abb40 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
 80abeee:	f8df b0f0 	ldr.w	fp, [pc, #240]	; 80abfe0 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x144>
inline std::int32_t RoundingHalfSum(std::int32_t a, std::int32_t b) {
  std::int64_t a64 = a;
  std::int64_t b64 = b;
  std::int64_t sum = a64 + b64;
  std::int64_t sign = sum >= 0 ? 1 : -1;
  return static_cast<std::int32_t>((sum + sign) / 2);
 80abef2:	46b2      	mov	sl, r6
}

// Integer addition. Not saturating. Overflow is undefined behavior.
template <typename tIntegerType>
tIntegerType Add(tIntegerType a, tIntegerType b) {
  return a + b;
 80abef4:	4404      	add	r4, r0
 80abef6:	2503      	movs	r5, #3
  const auto min = std::numeric_limits<tIntegerType>::min();
  const auto max = std::numeric_limits<tIntegerType>::max();
  return wide_shifted < min
             ? min
             : wide_shifted > max ? max
                                  : static_cast<tIntegerType>(wide_shifted);
 80abef8:	f06f 4600 	mvn.w	r6, #2147483648	; 0x80000000
 80abefc:	2700      	movs	r7, #0
template <typename tRawType, int tIntegerBits_a, int tIntegerBits_b>
FixedPoint<tRawType, tIntegerBits_a + tIntegerBits_b> operator*(
    FixedPoint<tRawType, tIntegerBits_a> a,
    FixedPoint<tRawType, tIntegerBits_b> b) {
  FixedPoint<tRawType, tIntegerBits_a + tIntegerBits_b> c;
  c.raw() = SaturatingRoundingDoublingHighMul(a.raw(), b.raw());
 80abefe:	4621      	mov	r1, r4
 80abf00:	4650      	mov	r0, sl
 80abf02:	f7ff fe1d 	bl	80abb40 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>
 80abf06:	f1c0 5100 	rsb	r1, r0, #536870912	; 0x20000000
 80abf0a:	4620      	mov	r0, r4
 80abf0c:	f7ff fe18 	bl	80abb40 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
 80abf10:	f1b0 5f00 	cmp.w	r0, #536870912	; 0x20000000
 80abf14:	da07      	bge.n	80abf26 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x8a>
 80abf16:	4558      	cmp	r0, fp
 80abf18:	f04f 31ff 	mov.w	r1, #4294967295	; 0xffffffff
 80abf1c:	f04f 0e00 	mov.w	lr, #0
 80abf20:	bfa8      	it	ge
 80abf22:	2100      	movge	r1, #0
 80abf24:	e002      	b.n	80abf2c <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x90>
 80abf26:	f04f 3eff 	mov.w	lr, #4294967295	; 0xffffffff
 80abf2a:	2100      	movs	r1, #0
//
// tIntegerType may be int32 or any narrower signed type.
template <typename tIntegerType>
tIntegerType ShiftLeft(tIntegerType a, int offset) {
  const std::int64_t wide_a = static_cast<std::int64_t>(a);
  const std::int64_t wide_shifted = wide_a * (1 << offset);
 80abf2c:	17c3      	asrs	r3, r0, #31
 80abf2e:	ea4f 0983 	mov.w	r9, r3, lsl #2
 80abf32:	ea4f 0880 	mov.w	r8, r0, lsl #2
 80abf36:	ea49 7990 	orr.w	r9, r9, r0, lsr #30
  const auto min = std::numeric_limits<tIntegerType>::min();
  const auto max = std::numeric_limits<tIntegerType>::max();
  return wide_shifted < min
             ? min
             : wide_shifted > max ? max
                                  : static_cast<tIntegerType>(wide_shifted);
 80abf3a:	f1b8 4f00 	cmp.w	r8, #2147483648	; 0x80000000
 80abf3e:	f179 33ff 	sbcs.w	r3, r9, #4294967295	; 0xffffffff
 80abf42:	db07      	blt.n	80abf54 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0xb8>
 80abf44:	4546      	cmp	r6, r8
 80abf46:	eb77 0309 	sbcs.w	r3, r7, r9
 80abf4a:	bfac      	ite	ge
 80abf4c:	4643      	movge	r3, r8
 80abf4e:	f06f 4300 	mvnlt.w	r3, #2147483648	; 0x80000000
 80abf52:	e001      	b.n	80abf58 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0xbc>
 80abf54:	f04f 4300 	mov.w	r3, #2147483648	; 0x80000000
}

// Integer addition. Not saturating. Overflow is undefined behavior.
template <typename tIntegerType>
tIntegerType Add(tIntegerType a, tIntegerType b) {
  return a + b;
 80abf58:	ea6f 020e 	mvn.w	r2, lr
 80abf5c:	f02e 4e00 	bic.w	lr, lr, #2147483648	; 0x80000000
 80abf60:	4013      	ands	r3, r2
 80abf62:	ea83 0e0e 	eor.w	lr, r3, lr
 80abf66:	43cb      	mvns	r3, r1
 80abf68:	f001 4100 	and.w	r1, r1, #2147483648	; 0x80000000
 80abf6c:	ea0e 0e03 	and.w	lr, lr, r3
 80abf70:	ea8e 0101 	eor.w	r1, lr, r1
  const F2 constant_48_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, 1515870810, 48.0 / 17.0);
  const F2 constant_neg_32_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, -1010580540, -32.0 / 17.0);
  F2 x = constant_48_over_17 + half_denominator * constant_neg_32_over_17;
  for (int i = 0; i < 3; i++) {
 80abf74:	3d01      	subs	r5, #1
}

// Integer addition. Not saturating. Overflow is undefined behavior.
template <typename tIntegerType>
tIntegerType Add(tIntegerType a, tIntegerType b) {
  return a + b;
 80abf76:	440c      	add	r4, r1
  const F2 constant_48_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, 1515870810, 48.0 / 17.0);
  const F2 constant_neg_32_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, -1010580540, -32.0 / 17.0);
  F2 x = constant_48_over_17 + half_denominator * constant_neg_32_over_17;
  for (int i = 0; i < 3; i++) {
 80abf78:	d1c1      	bne.n	80abefe <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x62>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
 80abf7a:	f1b4 4f80 	cmp.w	r4, #1073741824	; 0x40000000
 80abf7e:	da07      	bge.n	80abf90 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0xf4>
 80abf80:	f1b4 4f40 	cmp.w	r4, #3221225472	; 0xc0000000
 80abf84:	f04f 30ff 	mov.w	r0, #4294967295	; 0xffffffff
 80abf88:	4629      	mov	r1, r5
 80abf8a:	bfc8      	it	gt
 80abf8c:	2000      	movgt	r0, #0
 80abf8e:	e002      	b.n	80abf96 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0xfa>
 80abf90:	4628      	mov	r0, r5
 80abf92:	f04f 31ff 	mov.w	r1, #4294967295	; 0xffffffff
//
// tIntegerType may be int32 or any narrower signed type.
template <typename tIntegerType>
tIntegerType ShiftLeft(tIntegerType a, int offset) {
  const std::int64_t wide_a = static_cast<std::int64_t>(a);
  const std::int64_t wide_shifted = wide_a * (1 << offset);
 80abf96:	1922      	adds	r2, r4, r4
 80abf98:	ea4f 73e4 	mov.w	r3, r4, asr #31
 80abf9c:	415b      	adcs	r3, r3
  const auto min = std::numeric_limits<tIntegerType>::min();
  const auto max = std::numeric_limits<tIntegerType>::max();
  return wide_shifted < min
             ? min
             : wide_shifted > max ? max
                                  : static_cast<tIntegerType>(wide_shifted);
 80abf9e:	f1b2 4f00 	cmp.w	r2, #2147483648	; 0x80000000
 80abfa2:	f173 34ff 	sbcs.w	r4, r3, #4294967295	; 0xffffffff
 80abfa6:	db0a      	blt.n	80abfbe <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x122>
 80abfa8:	f06f 4400 	mvn.w	r4, #2147483648	; 0x80000000
 80abfac:	4294      	cmp	r4, r2
 80abfae:	f04f 0500 	mov.w	r5, #0
 80abfb2:	eb75 0403 	sbcs.w	r4, r5, r3
 80abfb6:	bfb8      	it	lt
 80abfb8:	f06f 4200 	mvnlt.w	r2, #2147483648	; 0x80000000
 80abfbc:	e001      	b.n	80abfc2 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x126>
 80abfbe:	f04f 4200 	mov.w	r2, #2147483648	; 0x80000000
 80abfc2:	ea22 0201 	bic.w	r2, r2, r1
 80abfc6:	f021 4100 	bic.w	r1, r1, #2147483648	; 0x80000000
 80abfca:	404a      	eors	r2, r1
 80abfcc:	ea22 0200 	bic.w	r2, r2, r0
 80abfd0:	f000 4000 	and.w	r0, r0, #2147483648	; 0x80000000
    F2 one_minus_half_denominator_times_x =
        F2::One() - half_denominator_times_x;
    x = x + Rescale<2>(x * one_minus_half_denominator_times_x);
  }
  return Rescale<0>(ExactMulByPot<-1>(x));
}
 80abfd4:	4050      	eors	r0, r2
 80abfd6:	e8bd 8ff8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80abfda:	bf00      	nop
 80abfdc:	c3c3c3c4 	.word	0xc3c3c3c4
 80abfe0:	e0000001 	.word	0xe0000001

080abfe4 <_ZN8gemmlowp55exp_on_interval_between_negative_one_quarter_and_0_exclIlEENS_10FixedPointIT_Li0EEES3_>:

// Implementation of exponential function.

// Returns exp(x) for x in [-1/4, 0).
template <typename tRawType>
FixedPoint<tRawType, 0> exp_on_interval_between_negative_one_quarter_and_0_excl(
 80abfe4:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
 80abfe8:	4604      	mov	r4, r0
    FixedPoint<tRawType, 0> a) {
  typedef FixedPoint<tRawType, 0> F;
  const F constant_term =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F, 1895147668, std::exp(-1.0 / 8.0));
 80abfea:	4814      	ldr	r0, [pc, #80]	; (80ac03c <_ZN8gemmlowp55exp_on_interval_between_negative_one_quarter_and_0_exclIlEENS_10FixedPointIT_Li0EEES3_+0x58>)
 80abfec:	f7ff ff50 	bl	80abe90 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
 80abff0:	4606      	mov	r6, r0
  const F constant_1_over_3 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F, 715827883, 1.0 / 3.0);
 80abff2:	4813      	ldr	r0, [pc, #76]	; (80ac040 <_ZN8gemmlowp55exp_on_interval_between_negative_one_quarter_and_0_exclIlEENS_10FixedPointIT_Li0EEES3_+0x5c>)
 80abff4:	f7ff ff4c 	bl	80abe90 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
}

// Integer addition. Not saturating. Overflow is undefined behavior.
template <typename tIntegerType>
tIntegerType Add(tIntegerType a, tIntegerType b) {
  return a + b;
 80abff8:	f104 5480 	add.w	r4, r4, #268435456	; 0x10000000
    FixedPoint<tRawType, 0> a) {
  typedef FixedPoint<tRawType, 0> F;
  const F constant_term =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F, 1895147668, std::exp(-1.0 / 8.0));
  const F constant_1_over_3 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F, 715827883, 1.0 / 3.0);
 80abffc:	4680      	mov	r8, r0
  // We're evaluating a Taylor expansion around -1/8, so we do the change of
  // variable: x = a + 1/8.
  // In fixed-point with 0 integer bits, 1/8 is represented by 1 << 28.
  F x = a + F::template ConstantPOT<-3>();
  F x2 = x * x;
 80abffe:	4621      	mov	r1, r4
 80ac000:	4620      	mov	r0, r4
 80ac002:	f7ff ff41 	bl	80abe88 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
  F x3 = x2 * x;
 80ac006:	4621      	mov	r1, r4
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F, 715827883, 1.0 / 3.0);
  // We're evaluating a Taylor expansion around -1/8, so we do the change of
  // variable: x = a + 1/8.
  // In fixed-point with 0 integer bits, 1/8 is represented by 1 << 28.
  F x = a + F::template ConstantPOT<-3>();
  F x2 = x * x;
 80ac008:	4605      	mov	r5, r0
  F x3 = x2 * x;
 80ac00a:	f7ff ff3d 	bl	80abe88 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
  F x4 = x2 * x2;
 80ac00e:	4629      	mov	r1, r5
  // We're evaluating a Taylor expansion around -1/8, so we do the change of
  // variable: x = a + 1/8.
  // In fixed-point with 0 integer bits, 1/8 is represented by 1 << 28.
  F x = a + F::template ConstantPOT<-3>();
  F x2 = x * x;
  F x3 = x2 * x;
 80ac010:	4607      	mov	r7, r0
  F x4 = x2 * x2;
 80ac012:	4628      	mov	r0, r5
 80ac014:	f7ff ff38 	bl	80abe88 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
};

template <int Exponent, typename IntegerType>
struct ImplSaturatingRoundingMultiplyByPOT<Exponent, IntegerType, -1> {
  static IntegerType eval(IntegerType x) {
    return RoundingDivideByPOT<IntegerType>(x, -Exponent);
 80ac018:	2102      	movs	r1, #2
 80ac01a:	f7ff ff09 	bl	80abe30 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i>
  F x2 = x * x;
  F x3 = x2 * x;
  F x4 = x2 * x2;
  F x4_over_4 = SaturatingRoundingMultiplyByPOT<-2>(x4);
  F x4_over_24_plus_x3_over_6_plus_x2_over_2 =
      SaturatingRoundingMultiplyByPOT<-1>(
 80ac01e:	4641      	mov	r1, r8
 80ac020:	4438      	add	r0, r7
 80ac022:	f7ff ff31 	bl	80abe88 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
};

template <int Exponent, typename IntegerType>
struct ImplSaturatingRoundingMultiplyByPOT<Exponent, IntegerType, -1> {
  static IntegerType eval(IntegerType x) {
    return RoundingDivideByPOT<IntegerType>(x, -Exponent);
 80ac026:	2101      	movs	r1, #1
 80ac028:	4428      	add	r0, r5
 80ac02a:	f7ff ff01 	bl	80abe30 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i>
  F x4 = x2 * x2;
  F x4_over_4 = SaturatingRoundingMultiplyByPOT<-2>(x4);
  F x4_over_24_plus_x3_over_6_plus_x2_over_2 =
      SaturatingRoundingMultiplyByPOT<-1>(
          ((x4_over_4 + x3) * constant_1_over_3) + x2);
  return AddSaturatingIf16Bit(
 80ac02e:	1821      	adds	r1, r4, r0
 80ac030:	4630      	mov	r0, r6
 80ac032:	f7ff ff29 	bl	80abe88 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
      constant_term,
      constant_term * (x + x4_over_24_plus_x3_over_6_plus_x2_over_2));
}
 80ac036:	4430      	add	r0, r6
 80ac038:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80ac03c:	70f5a894 	.word	0x70f5a894
 80ac040:	2aaaaaab 	.word	0x2aaaaaab

080ac044 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE>:

// Returns exp(x) for x < 0.
template <typename tRawType, int tIntegerBits>
FixedPoint<tRawType, 0> exp_on_negative_values(
 80ac044:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
  return a * b;
}

template <typename tIntegerType>
tIntegerType Sub(tIntegerType a, tIntegerType b) {
  return a - b;
 80ac046:	f040 467f 	orr.w	r6, r0, #4278190080	; 0xff000000
      constant_term * (x + x4_over_24_plus_x3_over_6_plus_x2_over_2));
}

// Returns exp(x) for x < 0.
template <typename tRawType, int tIntegerBits>
FixedPoint<tRawType, 0> exp_on_negative_values(
 80ac04a:	4607      	mov	r7, r0
  static constexpr int kIntegerBits = InputF::kIntegerBits;
  const InputF kOneQuarter = InputF::template ConstantPOT<-2>();
  InputF mask = kOneQuarter - InputF::FromScalarRaw(1);
  InputF a_mod_quarter_minus_one_quarter = (a & mask) - kOneQuarter;
  ResultF result = exp_on_interval_between_negative_one_quarter_and_0_excl(
      Rescale<0>(a_mod_quarter_minus_one_quarter));
 80ac04c:	0170      	lsls	r0, r6, #5
 80ac04e:	f7ff ffc9 	bl	80abfe4 <_ZN8gemmlowp55exp_on_interval_between_negative_one_quarter_and_0_exclIlEENS_10FixedPointIT_Li0EEES3_>
 80ac052:	4605      	mov	r5, r0
    result = SelectUsingMask(                                               \
        MaskIfNonZero(BitAnd(remainder, Dup<tRawType>(1 << kShiftAmount))), \
        result * kMultiplier, result);                                      \
  }

  GEMMLOWP_EXP_BARREL_SHIFTER(-2, 1672461947);
 80ac054:	4833      	ldr	r0, [pc, #204]	; (80ac124 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE+0xe0>)
 80ac056:	f7ff ff1b 	bl	80abe90 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
 80ac05a:	4601      	mov	r1, r0
 80ac05c:	4628      	mov	r0, r5
 80ac05e:	f7ff ff13 	bl	80abe88 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
  return a * b;
}

template <typename tIntegerType>
tIntegerType Sub(tIntegerType a, tIntegerType b) {
  return a - b;
 80ac062:	1bf6      	subs	r6, r6, r7
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
 80ac064:	f346 6400 	sbfx	r4, r6, #24, #1
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
 80ac068:	4020      	ands	r0, r4
 80ac06a:	ea25 0504 	bic.w	r5, r5, r4
 80ac06e:	ea80 0405 	eor.w	r4, r0, r5
        MaskIfNonZero(BitAnd(remainder, Dup<tRawType>(1 << kShiftAmount))), \
        result * kMultiplier, result);                                      \
  }

  GEMMLOWP_EXP_BARREL_SHIFTER(-2, 1672461947);
  GEMMLOWP_EXP_BARREL_SHIFTER(-1, 1302514674);
 80ac072:	482d      	ldr	r0, [pc, #180]	; (80ac128 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE+0xe4>)
 80ac074:	f7ff ff0c 	bl	80abe90 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
 80ac078:	4601      	mov	r1, r0
 80ac07a:	4620      	mov	r0, r4
 80ac07c:	f7ff ff04 	bl	80abe88 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
 80ac080:	f346 6540 	sbfx	r5, r6, #25, #1
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
 80ac084:	4028      	ands	r0, r5
}

// Plain bit-wise AND
template <typename tIntegerType>
tIntegerType BitAnd(tIntegerType a, tIntegerType b) {
  return a & b;
 80ac086:	ea24 0405 	bic.w	r4, r4, r5
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
 80ac08a:	4044      	eors	r4, r0
        result * kMultiplier, result);                                      \
  }

  GEMMLOWP_EXP_BARREL_SHIFTER(-2, 1672461947);
  GEMMLOWP_EXP_BARREL_SHIFTER(-1, 1302514674);
  GEMMLOWP_EXP_BARREL_SHIFTER(+0, 790015084);
 80ac08c:	4827      	ldr	r0, [pc, #156]	; (80ac12c <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE+0xe8>)
 80ac08e:	f7ff feff 	bl	80abe90 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
 80ac092:	4601      	mov	r1, r0
 80ac094:	4620      	mov	r0, r4
 80ac096:	f7ff fef7 	bl	80abe88 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
 80ac09a:	f346 6580 	sbfx	r5, r6, #26, #1
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
 80ac09e:	4028      	ands	r0, r5
}

// Plain bit-wise AND
template <typename tIntegerType>
tIntegerType BitAnd(tIntegerType a, tIntegerType b) {
  return a & b;
 80ac0a0:	ea24 0405 	bic.w	r4, r4, r5
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
 80ac0a4:	4044      	eors	r4, r0
  }

  GEMMLOWP_EXP_BARREL_SHIFTER(-2, 1672461947);
  GEMMLOWP_EXP_BARREL_SHIFTER(-1, 1302514674);
  GEMMLOWP_EXP_BARREL_SHIFTER(+0, 790015084);
  GEMMLOWP_EXP_BARREL_SHIFTER(+1, 290630308);
 80ac0a6:	4822      	ldr	r0, [pc, #136]	; (80ac130 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE+0xec>)
 80ac0a8:	f7ff fef2 	bl	80abe90 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
 80ac0ac:	4601      	mov	r1, r0
 80ac0ae:	4620      	mov	r0, r4
 80ac0b0:	f7ff feea 	bl	80abe88 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
 80ac0b4:	f346 65c0 	sbfx	r5, r6, #27, #1
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
 80ac0b8:	4028      	ands	r0, r5
}

// Plain bit-wise AND
template <typename tIntegerType>
tIntegerType BitAnd(tIntegerType a, tIntegerType b) {
  return a & b;
 80ac0ba:	ea24 0405 	bic.w	r4, r4, r5
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
 80ac0be:	4044      	eors	r4, r0

  GEMMLOWP_EXP_BARREL_SHIFTER(-2, 1672461947);
  GEMMLOWP_EXP_BARREL_SHIFTER(-1, 1302514674);
  GEMMLOWP_EXP_BARREL_SHIFTER(+0, 790015084);
  GEMMLOWP_EXP_BARREL_SHIFTER(+1, 290630308);
  GEMMLOWP_EXP_BARREL_SHIFTER(+2, 39332535);
 80ac0c0:	481c      	ldr	r0, [pc, #112]	; (80ac134 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE+0xf0>)
 80ac0c2:	f7ff fee5 	bl	80abe90 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
 80ac0c6:	4601      	mov	r1, r0
 80ac0c8:	4620      	mov	r0, r4
 80ac0ca:	f7ff fedd 	bl	80abe88 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
 80ac0ce:	f346 7500 	sbfx	r5, r6, #28, #1
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
 80ac0d2:	4028      	ands	r0, r5
}

// Plain bit-wise AND
template <typename tIntegerType>
tIntegerType BitAnd(tIntegerType a, tIntegerType b) {
  return a & b;
 80ac0d4:	ea24 0405 	bic.w	r4, r4, r5
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
 80ac0d8:	4044      	eors	r4, r0
  GEMMLOWP_EXP_BARREL_SHIFTER(-2, 1672461947);
  GEMMLOWP_EXP_BARREL_SHIFTER(-1, 1302514674);
  GEMMLOWP_EXP_BARREL_SHIFTER(+0, 790015084);
  GEMMLOWP_EXP_BARREL_SHIFTER(+1, 290630308);
  GEMMLOWP_EXP_BARREL_SHIFTER(+2, 39332535);
  GEMMLOWP_EXP_BARREL_SHIFTER(+3, 720401);
 80ac0da:	4817      	ldr	r0, [pc, #92]	; (80ac138 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE+0xf4>)
 80ac0dc:	f7ff fed8 	bl	80abe90 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
 80ac0e0:	4601      	mov	r1, r0
 80ac0e2:	4620      	mov	r0, r4
 80ac0e4:	f7ff fed0 	bl	80abe88 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
 80ac0e8:	f346 7540 	sbfx	r5, r6, #29, #1
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
 80ac0ec:	4028      	ands	r0, r5
}

// Plain bit-wise AND
template <typename tIntegerType>
tIntegerType BitAnd(tIntegerType a, tIntegerType b) {
  return a & b;
 80ac0ee:	ea24 0405 	bic.w	r4, r4, r5
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
 80ac0f2:	4044      	eors	r4, r0
  GEMMLOWP_EXP_BARREL_SHIFTER(-1, 1302514674);
  GEMMLOWP_EXP_BARREL_SHIFTER(+0, 790015084);
  GEMMLOWP_EXP_BARREL_SHIFTER(+1, 290630308);
  GEMMLOWP_EXP_BARREL_SHIFTER(+2, 39332535);
  GEMMLOWP_EXP_BARREL_SHIFTER(+3, 720401);
  GEMMLOWP_EXP_BARREL_SHIFTER(+4, 242);
 80ac0f4:	20f2      	movs	r0, #242	; 0xf2
 80ac0f6:	f7ff fecb 	bl	80abe90 <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
 80ac0fa:	fab7 f787 	clz	r7, r7
  GEMMLOWP_EXP_BARREL_SHIFTER(-1, 1302514674);
  GEMMLOWP_EXP_BARREL_SHIFTER(+0, 790015084);
  GEMMLOWP_EXP_BARREL_SHIFTER(+1, 290630308);
  GEMMLOWP_EXP_BARREL_SHIFTER(+2, 39332535);
  GEMMLOWP_EXP_BARREL_SHIFTER(+3, 720401);
  GEMMLOWP_EXP_BARREL_SHIFTER(+4, 242);
 80ac0fe:	4601      	mov	r1, r0
 80ac100:	4620      	mov	r0, r4
 80ac102:	f7ff fec1 	bl	80abe88 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
 80ac106:	f346 7680 	sbfx	r6, r6, #30, #1
 80ac10a:	097f      	lsrs	r7, r7, #5
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
 80ac10c:	4030      	ands	r0, r6
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
 80ac10e:	427f      	negs	r7, r7
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
 80ac110:	ea24 0406 	bic.w	r4, r4, r6
 80ac114:	4044      	eors	r4, r0
        GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(InputF, -(1 << clampB), -32.0);
    result = SelectUsingMask(MaskIfLessThan(a, clamp), ResultF::Zero(), result);
  }

  result = SelectUsingMask(MaskIfZero(a), ResultF::One(), result);
  return result;
 80ac116:	43f8      	mvns	r0, r7
 80ac118:	4004      	ands	r4, r0
 80ac11a:	f027 4000 	bic.w	r0, r7, #2147483648	; 0x80000000
}
 80ac11e:	4060      	eors	r0, r4
 80ac120:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
 80ac122:	bf00      	nop
 80ac124:	63afbe7b 	.word	0x63afbe7b
 80ac128:	4da2cbf2 	.word	0x4da2cbf2
 80ac12c:	2f16ac6c 	.word	0x2f16ac6c
 80ac130:	1152aaa4 	.word	0x1152aaa4
 80ac134:	02582ab7 	.word	0x02582ab7
 80ac138:	000afe11 	.word	0x000afe11

080ac13c <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph>:
  }
}

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const uint8* input_data,
                    const RuntimeShape& output_shape, uint8* output_data) {
 80ac13c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80ac140:	460c      	mov	r4, r1
 80ac142:	461e      	mov	r6, r3
  const int32 input_beta_multiplier = params.input_multiplier;
 80ac144:	6883      	ldr	r3, [r0, #8]
  }
}

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const uint8* input_data,
                    const RuntimeShape& output_shape, uint8* output_data) {
 80ac146:	b08d      	sub	sp, #52	; 0x34
  const int32 input_beta_multiplier = params.input_multiplier;
 80ac148:	9301      	str	r3, [sp, #4]
  using FixedPointScaledDiff =
      gemmlowp::FixedPoint<int32, kScaledDiffIntegerBits>;
  using FixedPointAccum = gemmlowp::FixedPoint<int32, kAccumulationIntegerBits>;
  using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;

  const int trailing_dim = input_shape.DimensionsCount() - 1;
 80ac14a:	680d      	ldr	r5, [r1, #0]

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const uint8* input_data,
                    const RuntimeShape& output_shape, uint8* output_data) {
  const int32 input_beta_multiplier = params.input_multiplier;
  const int32 input_beta_left_shift = params.input_left_shift;
 80ac14c:	68c3      	ldr	r3, [r0, #12]
  using FixedPointScaledDiff =
      gemmlowp::FixedPoint<int32, kScaledDiffIntegerBits>;
  using FixedPointAccum = gemmlowp::FixedPoint<int32, kAccumulationIntegerBits>;
  using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;

  const int trailing_dim = input_shape.DimensionsCount() - 1;
 80ac14e:	3d01      	subs	r5, #1

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const uint8* input_data,
                    const RuntimeShape& output_shape, uint8* output_data) {
  const int32 input_beta_multiplier = params.input_multiplier;
  const int32 input_beta_left_shift = params.input_left_shift;
 80ac150:	9302      	str	r3, [sp, #8]
  const int diff_min = params.diff_min;
 80ac152:	6983      	ldr	r3, [r0, #24]
  using FixedPointAccum = gemmlowp::FixedPoint<int32, kAccumulationIntegerBits>;
  using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;

  const int trailing_dim = input_shape.DimensionsCount() - 1;
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
 80ac154:	4629      	mov	r1, r5
 80ac156:	4620      	mov	r0, r4
  }
}

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const uint8* input_data,
                    const RuntimeShape& output_shape, uint8* output_data) {
 80ac158:	4692      	mov	sl, r2
  using FixedPointAccum = gemmlowp::FixedPoint<int32, kAccumulationIntegerBits>;
  using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;

  const int trailing_dim = input_shape.DimensionsCount() - 1;
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
 80ac15a:	4632      	mov	r2, r6
inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const uint8* input_data,
                    const RuntimeShape& output_shape, uint8* output_data) {
  const int32 input_beta_multiplier = params.input_multiplier;
  const int32 input_beta_left_shift = params.input_left_shift;
  const int diff_min = params.diff_min;
 80ac15c:	9303      	str	r3, [sp, #12]
  using FixedPointAccum = gemmlowp::FixedPoint<int32, kAccumulationIntegerBits>;
  using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;

  const int trailing_dim = input_shape.DimensionsCount() - 1;
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
 80ac15e:	f7ff fd17 	bl	80abb90 <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_>
 80ac162:	4629      	mov	r1, r5
 80ac164:	9004      	str	r0, [sp, #16]
 80ac166:	4620      	mov	r0, r4
 80ac168:	f7f6 f92e 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80ac16c:	4629      	mov	r1, r5
 80ac16e:	4604      	mov	r4, r0
 80ac170:	4630      	mov	r0, r6
 80ac172:	f7f6 f929 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80ac176:	4284      	cmp	r4, r0
 80ac178:	d001      	beq.n	80ac17e <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x42>
 80ac17a:	f003 ff59 	bl	80b0030 <abort>
 80ac17e:	4656      	mov	r6, sl
 80ac180:	f8dd 9058 	ldr.w	r9, [sp, #88]	; 0x58
 80ac184:	f1ca 0500 	rsb	r5, sl, #0
 80ac188:	2700      	movs	r7, #0
  const int depth =
      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);

  for (int i = 0; i < outer_size; ++i) {
 80ac18a:	9b04      	ldr	r3, [sp, #16]
 80ac18c:	429f      	cmp	r7, r3
 80ac18e:	da7d      	bge.n	80ac28c <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x150>
    uint8 max_in_row = 0;
 80ac190:	f04f 0300 	mov.w	r3, #0
 80ac194:	4632      	mov	r2, r6
 80ac196:	f88d 3023 	strb.w	r3, [sp, #35]	; 0x23
    for (int c = 0; c < depth; ++c) {
 80ac19a:	1953      	adds	r3, r2, r5
 80ac19c:	42a3      	cmp	r3, r4
 80ac19e:	da0c      	bge.n	80ac1ba <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x7e>
      max_in_row = std::max(max_in_row, input_data[i * depth + c]);
 80ac1a0:	4613      	mov	r3, r2
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
 80ac1a2:	f89d 1023 	ldrb.w	r1, [sp, #35]	; 0x23
 80ac1a6:	7818      	ldrb	r0, [r3, #0]
 80ac1a8:	3201      	adds	r2, #1
 80ac1aa:	4288      	cmp	r0, r1
	return __b;
      return __a;
 80ac1ac:	bf98      	it	ls
 80ac1ae:	f10d 0323 	addls.w	r3, sp, #35	; 0x23
 80ac1b2:	781b      	ldrb	r3, [r3, #0]
 80ac1b4:	f88d 3023 	strb.w	r3, [sp, #35]	; 0x23
  const int depth =
      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);

  for (int i = 0; i < outer_size; ++i) {
    uint8 max_in_row = 0;
    for (int c = 0; c < depth; ++c) {
 80ac1b8:	e7ef      	b.n	80ac19a <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x5e>
 80ac1ba:	46b0      	mov	r8, r6
 80ac1bc:	f04f 0b00 	mov.w	fp, #0
      max_in_row = std::max(max_in_row, input_data[i * depth + c]);
    }

    FixedPointAccum sum_of_exps = FixedPointAccum::Zero();
    for (int c = 0; c < depth; ++c) {
 80ac1c0:	eb08 0305 	add.w	r3, r8, r5
 80ac1c4:	42a3      	cmp	r3, r4
 80ac1c6:	da13      	bge.n	80ac1f0 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xb4>
      int32 input_diff =
          static_cast<int32>(input_data[i * depth + c]) - max_in_row;
 80ac1c8:	f818 3b01 	ldrb.w	r3, [r8], #1
 80ac1cc:	f89d 0023 	ldrb.w	r0, [sp, #35]	; 0x23
 80ac1d0:	1a18      	subs	r0, r3, r0
      if (input_diff >= diff_min) {
 80ac1d2:	9b03      	ldr	r3, [sp, #12]
 80ac1d4:	4283      	cmp	r3, r0
 80ac1d6:	dcf3      	bgt.n	80ac1c0 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x84>

inline int32 MultiplyByQuantizedMultiplierGreaterThanOne(
    int32 x, int32 quantized_multiplier, int left_shift) {
  using gemmlowp::SaturatingRoundingDoublingHighMul;
  return SaturatingRoundingDoublingHighMul(x * (1 << left_shift),
                                           quantized_multiplier);
 80ac1d8:	9b02      	ldr	r3, [sp, #8]
 80ac1da:	9901      	ldr	r1, [sp, #4]
 80ac1dc:	4098      	lsls	r0, r3
 80ac1de:	f7ff fcaf 	bl	80abb40 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>
            MultiplyByQuantizedMultiplierGreaterThanOne(
                input_diff, input_beta_multiplier, input_beta_left_shift);
        const FixedPointScaledDiff scaled_diff_f8 =
            FixedPointScaledDiff::FromRaw(input_diff_rescaled);
        sum_of_exps = sum_of_exps + gemmlowp::Rescale<kAccumulationIntegerBits>(
                                        exp_on_negative_values(scaled_diff_f8));
 80ac1e2:	f7ff ff2f 	bl	80ac044 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE>
};

template <int Exponent, typename IntegerType>
struct ImplSaturatingRoundingMultiplyByPOT<Exponent, IntegerType, -1> {
  static IntegerType eval(IntegerType x) {
    return RoundingDivideByPOT<IntegerType>(x, -Exponent);
 80ac1e6:	210c      	movs	r1, #12
 80ac1e8:	f7ff fe22 	bl	80abe30 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i>
}

// Integer addition. Not saturating. Overflow is undefined behavior.
template <typename tIntegerType>
tIntegerType Add(tIntegerType a, tIntegerType b) {
  return a + b;
 80ac1ec:	4483      	add	fp, r0
    for (int c = 0; c < depth; ++c) {
      max_in_row = std::max(max_in_row, input_data[i * depth + c]);
    }

    FixedPointAccum sum_of_exps = FixedPointAccum::Zero();
    for (int c = 0; c < depth; ++c) {
 80ac1ee:	e7e7      	b.n	80ac1c0 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x84>
int CountLeadingZeros(T integer_input) {
  static_assert(std::is_unsigned<T>::value,
                "Only unsigned integer types handled.");
#if defined(__GNUC__)
  return integer_input ? __builtin_clz(integer_input)
                       : std::numeric_limits<T>::digits;
 80ac1f0:	f1bb 0f00 	cmp.w	fp, #0
 80ac1f4:	d002      	beq.n	80ac1fc <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xc0>
 80ac1f6:	fabb f88b 	clz	r8, fp
 80ac1fa:	e001      	b.n	80ac200 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xc4>
 80ac1fc:	f04f 0820 	mov.w	r8, #32
 80ac200:	fa0b f008 	lsl.w	r0, fp, r8
      static_cast<int32>((static_cast<uint32>(x) << headroom_plus_one) -
                         (static_cast<uint32>(1) << 31));

  gemmlowp::FixedPoint<int32, 0> shifted_scale =
      gemmlowp::one_over_one_plus_x_for_x_in_0_1(
          gemmlowp::FixedPoint<int32, 0>::FromRaw(shifted_sum_minus_one));
 80ac204:	f100 4000 	add.w	r0, r0, #2147483648	; 0x80000000
 80ac208:	f7ff fe48 	bl	80abe9c <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_>
      }
    }

    int num_bits_over_unit;
    FixedPoint0 shifted_scale = FixedPoint0::FromRaw(GetReciprocal(
        sum_of_exps.raw(), kAccumulationIntegerBits, &num_bits_over_unit));
 80ac20c:	4632      	mov	r2, r6
 80ac20e:	4683      	mov	fp, r0
 80ac210:	464b      	mov	r3, r9

    for (int c = 0; c < depth; ++c) {
 80ac212:	9916      	ldr	r1, [sp, #88]	; 0x58
                input_diff, input_beta_multiplier, input_beta_left_shift);
        const FixedPointScaledDiff scaled_diff_f8 =
            FixedPointScaledDiff::FromRaw(input_diff_rescaled);

        FixedPoint0 exp_in_0 = exp_on_negative_values(scaled_diff_f8);
        int32 unsat_output = gemmlowp::RoundingDivideByPOT(
 80ac214:	f1c8 0823 	rsb	r8, r8, #35	; 0x23

    int num_bits_over_unit;
    FixedPoint0 shifted_scale = FixedPoint0::FromRaw(GetReciprocal(
        sum_of_exps.raw(), kAccumulationIntegerBits, &num_bits_over_unit));

    for (int c = 0; c < depth; ++c) {
 80ac218:	1a69      	subs	r1, r5, r1
 80ac21a:	4451      	add	r1, sl
 80ac21c:	9105      	str	r1, [sp, #20]
 80ac21e:	9905      	ldr	r1, [sp, #20]
 80ac220:	4419      	add	r1, r3
 80ac222:	428c      	cmp	r4, r1
 80ac224:	dd2d      	ble.n	80ac282 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x146>
      int32 input_diff =
          static_cast<int32>(input_data[i * depth + c]) - max_in_row;
 80ac226:	f812 1b01 	ldrb.w	r1, [r2], #1
 80ac22a:	f89d 0023 	ldrb.w	r0, [sp, #35]	; 0x23
 80ac22e:	1a08      	subs	r0, r1, r0
      if (input_diff >= diff_min) {
 80ac230:	9903      	ldr	r1, [sp, #12]
 80ac232:	4281      	cmp	r1, r0
 80ac234:	dc20      	bgt.n	80ac278 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x13c>
 80ac236:	9306      	str	r3, [sp, #24]

inline int32 MultiplyByQuantizedMultiplierGreaterThanOne(
    int32 x, int32 quantized_multiplier, int left_shift) {
  using gemmlowp::SaturatingRoundingDoublingHighMul;
  return SaturatingRoundingDoublingHighMul(x * (1 << left_shift),
                                           quantized_multiplier);
 80ac238:	9b02      	ldr	r3, [sp, #8]
 80ac23a:	9901      	ldr	r1, [sp, #4]
 80ac23c:	4098      	lsls	r0, r3
 80ac23e:	9207      	str	r2, [sp, #28]
 80ac240:	f7ff fc7e 	bl	80abb40 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>
            MultiplyByQuantizedMultiplierGreaterThanOne(
                input_diff, input_beta_multiplier, input_beta_left_shift);
        const FixedPointScaledDiff scaled_diff_f8 =
            FixedPointScaledDiff::FromRaw(input_diff_rescaled);

        FixedPoint0 exp_in_0 = exp_on_negative_values(scaled_diff_f8);
 80ac244:	f7ff fefe 	bl	80ac044 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE>
        int32 unsat_output = gemmlowp::RoundingDivideByPOT(
            (shifted_scale * exp_in_0).raw(), num_bits_over_unit + 31 - 8);
 80ac248:	4601      	mov	r1, r0
 80ac24a:	4658      	mov	r0, fp
 80ac24c:	f7ff fe1c 	bl	80abe88 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
                input_diff, input_beta_multiplier, input_beta_left_shift);
        const FixedPointScaledDiff scaled_diff_f8 =
            FixedPointScaledDiff::FromRaw(input_diff_rescaled);

        FixedPoint0 exp_in_0 = exp_on_negative_values(scaled_diff_f8);
        int32 unsat_output = gemmlowp::RoundingDivideByPOT(
 80ac250:	4641      	mov	r1, r8
 80ac252:	f7ff fded 	bl	80abe30 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i>
            (shifted_scale * exp_in_0).raw(), num_bits_over_unit + 31 - 8);

        output_data[i * depth + c] = static_cast<uint8>(
            std::max(std::min(unsat_output, static_cast<int32>(255)),
 80ac256:	21ff      	movs	r1, #255	; 0xff
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
 80ac258:	4288      	cmp	r0, r1
 80ac25a:	910a      	str	r1, [sp, #40]	; 0x28
        const FixedPointScaledDiff scaled_diff_f8 =
            FixedPointScaledDiff::FromRaw(input_diff_rescaled);

        FixedPoint0 exp_in_0 = exp_on_negative_values(scaled_diff_f8);
        int32 unsat_output = gemmlowp::RoundingDivideByPOT(
            (shifted_scale * exp_in_0).raw(), num_bits_over_unit + 31 - 8);
 80ac25c:	9009      	str	r0, [sp, #36]	; 0x24
	return __b;
      return __a;
 80ac25e:	bfd4      	ite	le
 80ac260:	a909      	addle	r1, sp, #36	; 0x24
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
 80ac262:	a90a      	addgt	r1, sp, #40	; 0x28

        output_data[i * depth + c] = static_cast<uint8>(
            std::max(std::min(unsat_output, static_cast<int32>(255)),
                     static_cast<int32>(0)));
 80ac264:	2000      	movs	r0, #0
 80ac266:	900b      	str	r0, [sp, #44]	; 0x2c
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
 80ac268:	6808      	ldr	r0, [r1, #0]
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
 80ac26a:	9b06      	ldr	r3, [sp, #24]
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
 80ac26c:	2800      	cmp	r0, #0
	return __b;
 80ac26e:	bfb8      	it	lt
 80ac270:	a90b      	addlt	r1, sp, #44	; 0x2c
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
 80ac272:	9a07      	ldr	r2, [sp, #28]
 80ac274:	6809      	ldr	r1, [r1, #0]
 80ac276:	e001      	b.n	80ac27c <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x140>

      } else {
        output_data[i * depth + c] = 0;
 80ac278:	f04f 0100 	mov.w	r1, #0
 80ac27c:	7019      	strb	r1, [r3, #0]
 80ac27e:	3301      	adds	r3, #1

    int num_bits_over_unit;
    FixedPoint0 shifted_scale = FixedPoint0::FromRaw(GetReciprocal(
        sum_of_exps.raw(), kAccumulationIntegerBits, &num_bits_over_unit));

    for (int c = 0; c < depth; ++c) {
 80ac280:	e7cd      	b.n	80ac21e <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xe2>
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
  const int depth =
      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);

  for (int i = 0; i < outer_size; ++i) {
 80ac282:	3701      	adds	r7, #1
 80ac284:	44a1      	add	r9, r4
 80ac286:	4426      	add	r6, r4
 80ac288:	1b2d      	subs	r5, r5, r4
 80ac28a:	e77e      	b.n	80ac18a <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x4e>
      } else {
        output_data[i * depth + c] = 0;
      }
    }
  }
}
 80ac28c:	b00d      	add	sp, #52	; 0x34
 80ac28e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

080ac294 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode>:
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
      GetTensorShape(output), GetTensorData<uint8_t>(output));
}

TfLiteStatus SoftmaxEval(TfLiteContext* context, TfLiteNode* node) {
 80ac294:	e92d 47f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ac298:	680b      	ldr	r3, [r1, #0]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80ac29a:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ac29c:	685f      	ldr	r7, [r3, #4]
 80ac29e:	2338      	movs	r3, #56	; 0x38
 80ac2a0:	435f      	muls	r7, r3
 80ac2a2:	b09e      	sub	sp, #120	; 0x78
 80ac2a4:	f8d0 8008 	ldr.w	r8, [r0, #8]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80ac2a8:	6854      	ldr	r4, [r2, #4]
  auto* params = reinterpret_cast<TfLiteSoftmaxParams*>(node->builtin_data);

  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);

  OpData local_data_object;
 80ac2aa:	f10d 0a18 	add.w	sl, sp, #24
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
      GetTensorShape(output), GetTensorData<uint8_t>(output));
}

TfLiteStatus SoftmaxEval(TfLiteContext* context, TfLiteNode* node) {
  auto* params = reinterpret_cast<TfLiteSoftmaxParams*>(node->builtin_data);
 80ac2ae:	f8d1 9014 	ldr.w	r9, [r1, #20]
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
      GetTensorShape(output), GetTensorData<uint8_t>(output));
}

TfLiteStatus SoftmaxEval(TfLiteContext* context, TfLiteNode* node) {
 80ac2b2:	4605      	mov	r5, r0
  auto* params = reinterpret_cast<TfLiteSoftmaxParams*>(node->builtin_data);

  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);

  OpData local_data_object;
 80ac2b4:	2210      	movs	r2, #16
 80ac2b6:	2100      	movs	r1, #0
 80ac2b8:	4650      	mov	r0, sl
 80ac2ba:	fb03 8404 	mla	r4, r3, r4, r8
 80ac2be:	f007 fc0f 	bl	80b3ae0 <memset>
TfLiteStatus CalculateSoftmaxOpData(TfLiteContext* context,
                                    const TfLiteTensor* input,
                                    TfLiteTensor* output,
                                    const TfLiteSoftmaxParams* params,
                                    OpData* data) {
  if (input->type == kTfLiteUInt8) {
 80ac2c2:	f818 3007 	ldrb.w	r3, [r8, r7]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ac2c6:	eb08 0607 	add.w	r6, r8, r7
 80ac2ca:	2b03      	cmp	r3, #3
 80ac2cc:	d144      	bne.n	80ac358 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0xc4>
    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);
 80ac2ce:	6923      	ldr	r3, [r4, #16]
 80ac2d0:	b16b      	cbz	r3, 80ac2ee <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x5a>
 80ac2d2:	9302      	str	r3, [sp, #8]
 80ac2d4:	4b68      	ldr	r3, [pc, #416]	; (80ac478 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1e4>)
 80ac2d6:	2200      	movs	r2, #0
 80ac2d8:	9301      	str	r3, [sp, #4]
 80ac2da:	4b68      	ldr	r3, [pc, #416]	; (80ac47c <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1e8>)
 80ac2dc:	9203      	str	r2, [sp, #12]
 80ac2de:	9300      	str	r3, [sp, #0]
 80ac2e0:	696c      	ldr	r4, [r5, #20]
 80ac2e2:	232c      	movs	r3, #44	; 0x2c
 80ac2e4:	4a66      	ldr	r2, [pc, #408]	; (80ac480 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1ec>)
 80ac2e6:	4967      	ldr	r1, [pc, #412]	; (80ac484 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1f0>)
 80ac2e8:	4628      	mov	r0, r5
 80ac2ea:	47a0      	blx	r4
 80ac2ec:	e0c0      	b.n	80ac470 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1dc>
    TF_LITE_ENSURE(context, output->params.scale == 1.f / 256);
 80ac2ee:	f04f 516e 	mov.w	r1, #998244352	; 0x3b800000
 80ac2f2:	68e0      	ldr	r0, [r4, #12]
 80ac2f4:	f007 fb0e 	bl	80b3914 <__aeabi_fcmpeq>
 80ac2f8:	b940      	cbnz	r0, 80ac30c <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x78>
 80ac2fa:	4b63      	ldr	r3, [pc, #396]	; (80ac488 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1f4>)
 80ac2fc:	4a60      	ldr	r2, [pc, #384]	; (80ac480 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1ec>)
 80ac2fe:	9300      	str	r3, [sp, #0]
 80ac300:	696c      	ldr	r4, [r5, #20]
 80ac302:	232d      	movs	r3, #45	; 0x2d
 80ac304:	4961      	ldr	r1, [pc, #388]	; (80ac48c <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1f8>)
 80ac306:	4628      	mov	r0, r5
 80ac308:	47a0      	blx	r4
 80ac30a:	e0b1      	b.n	80ac470 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1dc>

    static const int kScaledDiffIntegerBits = 5;

    tflite::PreprocessSoftmaxScaling(
        params->beta, input->params.scale, kScaledDiffIntegerBits,
        &data->input_multiplier, &data->input_left_shift);
 80ac30c:	68f0      	ldr	r0, [r6, #12]
 80ac30e:	f006 fcd9 	bl	80b2cc4 <__aeabi_f2d>
 80ac312:	4602      	mov	r2, r0
 80ac314:	460b      	mov	r3, r1
 80ac316:	f8d9 0000 	ldr.w	r0, [r9]
 80ac31a:	e9cd 2304 	strd	r2, r3, [sp, #16]
 80ac31e:	f006 fcd1 	bl	80b2cc4 <__aeabi_f2d>
 80ac322:	e9dd 2304 	ldrd	r2, r3, [sp, #16]
 80ac326:	f10d 0e1c 	add.w	lr, sp, #28
 80ac32a:	f8cd a004 	str.w	sl, [sp, #4]
 80ac32e:	f04f 0a05 	mov.w	sl, #5
 80ac332:	f8cd e008 	str.w	lr, [sp, #8]
 80ac336:	f8cd a000 	str.w	sl, [sp]
 80ac33a:	f003 fc65 	bl	80afc08 <_ZN6tflite24PreprocessSoftmaxScalingEddiPlPi>
    data->diff_min = -1.0 * tflite::CalculateInputRadius(
 80ac33e:	221f      	movs	r2, #31
 80ac340:	9907      	ldr	r1, [sp, #28]
 80ac342:	4650      	mov	r0, sl
 80ac344:	f003 fc98 	bl	80afc78 <_ZN6tflite20CalculateInputRadiusEiii>
                                kScaledDiffIntegerBits, data->input_left_shift);
 80ac348:	f006 fcaa 	bl	80b2ca0 <__aeabi_i2d>
 80ac34c:	f101 4300 	add.w	r3, r1, #2147483648	; 0x80000000
 80ac350:	4619      	mov	r1, r3
 80ac352:	f006 ffa5 	bl	80b32a0 <__aeabi_d2iz>
 80ac356:	9009      	str	r0, [sp, #36]	; 0x24
  TF_LITE_ENSURE_STATUS(
      CalculateSoftmaxOpData(context, input, output, params, data));

  // TODO(ahentz): consider an implementation that works for many (all?)
  // dimensions.
  switch (input->type) {
 80ac358:	f818 7007 	ldrb.w	r7, [r8, r7]
 80ac35c:	2f01      	cmp	r7, #1
 80ac35e:	d11f      	bne.n	80ac3a0 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x10c>
#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/c_api_internal.h"

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
 80ac360:	68b3      	ldr	r3, [r6, #8]
 80ac362:	681a      	ldr	r2, [r3, #0]
    case kTfLiteFloat32: {
      if (NumDimensions(input) == 1) {
 80ac364:	2a01      	cmp	r2, #1
 80ac366:	d105      	bne.n	80ac374 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0xe0>
        Softmax1DFloat(input, output, params);
 80ac368:	464a      	mov	r2, r9
 80ac36a:	4621      	mov	r1, r4
 80ac36c:	4630      	mov	r0, r6
 80ac36e:	f7ff fd1e 	bl	80abdae <_ZN6tflite3ops5micro11activations14Softmax1DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams>
 80ac372:	e041      	b.n	80ac3f8 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x164>
        return kTfLiteOk;
      }
      if (NumDimensions(input) == 2) {
 80ac374:	2a02      	cmp	r2, #2
 80ac376:	d105      	bne.n	80ac384 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0xf0>
        Softmax2DFloat(input, output, params);
 80ac378:	464a      	mov	r2, r9
 80ac37a:	4621      	mov	r1, r4
 80ac37c:	4630      	mov	r0, r6
 80ac37e:	f7ff fd22 	bl	80abdc6 <_ZN6tflite3ops5micro11activations14Softmax2DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams>
 80ac382:	e039      	b.n	80ac3f8 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x164>
        return kTfLiteOk;
      }
      if (NumDimensions(input) == 4) {
 80ac384:	2a04      	cmp	r2, #4
 80ac386:	d105      	bne.n	80ac394 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x100>
        Softmax4DFloat(input, output, params);
 80ac388:	464a      	mov	r2, r9
 80ac38a:	4621      	mov	r1, r4
 80ac38c:	4630      	mov	r0, r6
 80ac38e:	f7ff fd26 	bl	80abdde <_ZN6tflite3ops5micro11activations14Softmax4DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams>
 80ac392:	e031      	b.n	80ac3f8 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x164>
        return kTfLiteOk;
      }
      context->ReportError(
          context, "Only 1D, 2D and 4D tensors supported currently, got %dD.",
          NumDimensions(input));
 80ac394:	4628      	mov	r0, r5
 80ac396:	696b      	ldr	r3, [r5, #20]
 80ac398:	493d      	ldr	r1, [pc, #244]	; (80ac490 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1fc>)
 80ac39a:	4798      	blx	r3
      return kTfLiteError;
 80ac39c:	4638      	mov	r0, r7
 80ac39e:	e068      	b.n	80ac472 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1de>
  TF_LITE_ENSURE_STATUS(
      CalculateSoftmaxOpData(context, input, output, params, data));

  // TODO(ahentz): consider an implementation that works for many (all?)
  // dimensions.
  switch (input->type) {
 80ac3a0:	2f03      	cmp	r7, #3
 80ac3a2:	d160      	bne.n	80ac466 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1d2>
 80ac3a4:	68b3      	ldr	r3, [r6, #8]
 80ac3a6:	681f      	ldr	r7, [r3, #0]
          context, "Only 1D, 2D and 4D tensors supported currently, got %dD.",
          NumDimensions(input));
      return kTfLiteError;
    }
    case kTfLiteUInt8: {
      if (NumDimensions(input) == 1) {
 80ac3a8:	2f01      	cmp	r7, #1
 80ac3aa:	d127      	bne.n	80ac3fc <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x168>
                        TfLiteSoftmaxParams* params, OpData* data) {
  // TODO(ahentz): this is arguably a dirty trick. Since the implementation
  // always traverses the last dimension of a 4D tensor, we will pretend our 1D
  // tensor is 4D in a special way. We will convert a (Y) shape into a (1,
  // 1, 1, Y) shape.
  const int input_size = input->dims->data[0];
 80ac3ac:	f8d3 8004 	ldr.w	r8, [r3, #4]
  const int32_t shape_data[4] = {1, 1, 1, input_size};
 80ac3b0:	ad0a      	add	r5, sp, #40	; 0x28
 80ac3b2:	2210      	movs	r2, #16
 80ac3b4:	2100      	movs	r1, #0
 80ac3b6:	4628      	mov	r0, r5
 80ac3b8:	f007 fb92 	bl	80b3ae0 <memset>
 80ac3bc:	970a      	str	r7, [sp, #40]	; 0x28
 80ac3be:	970b      	str	r7, [sp, #44]	; 0x2c
 80ac3c0:	970c      	str	r7, [sp, #48]	; 0x30
 80ac3c2:	f8cd 8034 	str.w	r8, [sp, #52]	; 0x34
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
#endif  // TF_LITE_STATIC_MEMORY
    }
    size_ = dimensions_count;
 80ac3c6:	2304      	movs	r3, #4
 80ac3c8:	930f      	str	r3, [sp, #60]	; 0x3c
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
    Resize(dimensions_count);
    int32* dst_dims = DimsData();
    std::memcpy(dst_dims, dims_data, dimensions_count * sizeof(int32));
 80ac3ca:	af10      	add	r7, sp, #64	; 0x40
 80ac3cc:	e895 000f 	ldmia.w	r5, {r0, r1, r2, r3}
 80ac3d0:	e887 000f 	stmia.w	r7, {r0, r1, r2, r3}
  const int batch_size = input->dims->data[0];
  const int input_size = input->dims->data[1];
  const int32_t shape_data[4] = {batch_size, 1, 1, input_size};
  RuntimeShape shape(4, shape_data);
  SoftmaxParams op_params;
  op_params.input_multiplier = data->input_multiplier;
 80ac3d4:	9b06      	ldr	r3, [sp, #24]
 80ac3d6:	9316      	str	r3, [sp, #88]	; 0x58
  op_params.input_left_shift = data->input_left_shift;
 80ac3d8:	9b07      	ldr	r3, [sp, #28]
 80ac3da:	9317      	str	r3, [sp, #92]	; 0x5c
  op_params.diff_min = data->diff_min;
 80ac3dc:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80ac3de:	931a      	str	r3, [sp, #104]	; 0x68
 80ac3e0:	b104      	cbz	r4, 80ac3e4 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x150>
 80ac3e2:	6864      	ldr	r4, [r4, #4]
  tflite::reference_ops::Softmax(op_params, shape,
                                 GetTensorData<uint8_t>(input), shape,
                                 GetTensorData<uint8_t>(output));
 80ac3e4:	9400      	str	r4, [sp, #0]
 80ac3e6:	ab0f      	add	r3, sp, #60	; 0x3c
 80ac3e8:	a814      	add	r0, sp, #80	; 0x50
 80ac3ea:	6872      	ldr	r2, [r6, #4]
 80ac3ec:	4619      	mov	r1, r3
 80ac3ee:	f7ff fea5 	bl	80ac13c <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph>
  // tensor is 4D in a special way. We will convert a (X, Y) shape into a (X,
  // 1, 1, Y) shape.
  const int batch_size = input->dims->data[0];
  const int input_size = input->dims->data[1];
  const int32_t shape_data[4] = {batch_size, 1, 1, input_size};
  RuntimeShape shape(4, shape_data);
 80ac3f2:	a80f      	add	r0, sp, #60	; 0x3c
  SoftmaxParams op_params;
  op_params.input_multiplier = data->input_multiplier;
  op_params.input_left_shift = data->input_left_shift;
  op_params.diff_min = data->diff_min;
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
 80ac3f4:	f7f5 ffdd 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
        Softmax2DQuantized(input, output, params, data);
        return kTfLiteOk;
      }
      if (NumDimensions(input) == 4) {
        Softmax4DQuantized(input, output, params, data);
        return kTfLiteOk;
 80ac3f8:	2000      	movs	r0, #0
 80ac3fa:	e03a      	b.n	80ac472 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1de>
    case kTfLiteUInt8: {
      if (NumDimensions(input) == 1) {
        Softmax1DQuantized(input, output, params, data);
        return kTfLiteOk;
      }
      if (NumDimensions(input) == 2) {
 80ac3fc:	2f02      	cmp	r7, #2
 80ac3fe:	d10f      	bne.n	80ac420 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x18c>
  // always traverses the last dimension of a 4D tensor, we will pretend our 2D
  // tensor is 4D in a special way. We will convert a (X, Y) shape into a (X,
  // 1, 1, Y) shape.
  const int batch_size = input->dims->data[0];
  const int input_size = input->dims->data[1];
  const int32_t shape_data[4] = {batch_size, 1, 1, input_size};
 80ac400:	ad0a      	add	r5, sp, #40	; 0x28
                        TfLiteSoftmaxParams* params, OpData* data) {
  // TODO(ahentz): this is arguably a dirty trick. Since the implementation
  // always traverses the last dimension of a 4D tensor, we will pretend our 2D
  // tensor is 4D in a special way. We will convert a (X, Y) shape into a (X,
  // 1, 1, Y) shape.
  const int batch_size = input->dims->data[0];
 80ac402:	f8d3 8004 	ldr.w	r8, [r3, #4]
  const int input_size = input->dims->data[1];
 80ac406:	689f      	ldr	r7, [r3, #8]
  const int32_t shape_data[4] = {batch_size, 1, 1, input_size};
 80ac408:	2210      	movs	r2, #16
 80ac40a:	2100      	movs	r1, #0
 80ac40c:	4628      	mov	r0, r5
 80ac40e:	f007 fb67 	bl	80b3ae0 <memset>
 80ac412:	2301      	movs	r3, #1
 80ac414:	930b      	str	r3, [sp, #44]	; 0x2c
 80ac416:	930c      	str	r3, [sp, #48]	; 0x30
 80ac418:	f8cd 8028 	str.w	r8, [sp, #40]	; 0x28
 80ac41c:	970d      	str	r7, [sp, #52]	; 0x34
 80ac41e:	e7d2      	b.n	80ac3c6 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x132>
      }
      if (NumDimensions(input) == 2) {
        Softmax2DQuantized(input, output, params, data);
        return kTfLiteOk;
      }
      if (NumDimensions(input) == 4) {
 80ac420:	2f04      	cmp	r7, #4
 80ac422:	d11c      	bne.n	80ac45e <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1ca>
}

void Softmax4DQuantized(const TfLiteTensor* input, TfLiteTensor* output,
                        TfLiteSoftmaxParams* params, OpData* data) {
  SoftmaxParams op_params;
  op_params.input_multiplier = data->input_multiplier;
 80ac424:	9b06      	ldr	r3, [sp, #24]
  op_params.input_left_shift = data->input_left_shift;
  op_params.diff_min = data->diff_min;
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
 80ac426:	4631      	mov	r1, r6
}

void Softmax4DQuantized(const TfLiteTensor* input, TfLiteTensor* output,
                        TfLiteSoftmaxParams* params, OpData* data) {
  SoftmaxParams op_params;
  op_params.input_multiplier = data->input_multiplier;
 80ac428:	9316      	str	r3, [sp, #88]	; 0x58
  op_params.input_left_shift = data->input_left_shift;
 80ac42a:	9b07      	ldr	r3, [sp, #28]
  op_params.diff_min = data->diff_min;
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
 80ac42c:	a80a      	add	r0, sp, #40	; 0x28

void Softmax4DQuantized(const TfLiteTensor* input, TfLiteTensor* output,
                        TfLiteSoftmaxParams* params, OpData* data) {
  SoftmaxParams op_params;
  op_params.input_multiplier = data->input_multiplier;
  op_params.input_left_shift = data->input_left_shift;
 80ac42e:	9317      	str	r3, [sp, #92]	; 0x5c
  op_params.diff_min = data->diff_min;
 80ac430:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80ac432:	931a      	str	r3, [sp, #104]	; 0x68
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
 80ac434:	f7f6 fa6d 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(output), GetTensorData<uint8_t>(output));
 80ac438:	4621      	mov	r1, r4
 80ac43a:	a80f      	add	r0, sp, #60	; 0x3c
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80ac43c:	6875      	ldr	r5, [r6, #4]
 80ac43e:	f7f6 fa68 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80ac442:	b104      	cbz	r4, 80ac446 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1b2>
 80ac444:	6864      	ldr	r4, [r4, #4]
 80ac446:	ab0f      	add	r3, sp, #60	; 0x3c
 80ac448:	462a      	mov	r2, r5
 80ac44a:	a90a      	add	r1, sp, #40	; 0x28
 80ac44c:	a814      	add	r0, sp, #80	; 0x50
 80ac44e:	9400      	str	r4, [sp, #0]
 80ac450:	f7ff fe74 	bl	80ac13c <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph>
 80ac454:	a80f      	add	r0, sp, #60	; 0x3c
 80ac456:	f7f5 ffac 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  SoftmaxParams op_params;
  op_params.input_multiplier = data->input_multiplier;
  op_params.input_left_shift = data->input_left_shift;
  op_params.diff_min = data->diff_min;
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
 80ac45a:	a80a      	add	r0, sp, #40	; 0x28
 80ac45c:	e7ca      	b.n	80ac3f4 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x160>
        Softmax4DQuantized(input, output, params, data);
        return kTfLiteOk;
      }
      context->ReportError(
          context, "Only 2D and 4D tensors supported currently, got %dD.",
          NumDimensions(input));
 80ac45e:	696b      	ldr	r3, [r5, #20]
 80ac460:	463a      	mov	r2, r7
 80ac462:	490c      	ldr	r1, [pc, #48]	; (80ac494 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x200>)
 80ac464:	e002      	b.n	80ac46c <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1d8>
      return kTfLiteError;
    }
    default:
      context->ReportError(
          context, "Only float32 and uint8_t supported currently, got %d.",
          input->type);
 80ac466:	463a      	mov	r2, r7
 80ac468:	696b      	ldr	r3, [r5, #20]
 80ac46a:	490b      	ldr	r1, [pc, #44]	; (80ac498 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x204>)
 80ac46c:	4628      	mov	r0, r5
 80ac46e:	4798      	blx	r3
  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);

  OpData local_data_object;
  OpData* data = &local_data_object;
  TF_LITE_ENSURE_STATUS(
 80ac470:	2001      	movs	r0, #1
      context->ReportError(
          context, "Only float32 and uint8_t supported currently, got %d.",
          input->type);
      return kTfLiteError;
  }
}
 80ac472:	b01e      	add	sp, #120	; 0x78
 80ac474:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}
 80ac478:	080b74a5 	.word	0x080b74a5
 80ac47c:	080b6718 	.word	0x080b6718
 80ac480:	080b666f 	.word	0x080b666f
 80ac484:	080b5be0 	.word	0x080b5be0
 80ac488:	080b6732 	.word	0x080b6732
 80ac48c:	080b5db0 	.word	0x080b5db0
 80ac490:	080b6754 	.word	0x080b6754
 80ac494:	080b678d 	.word	0x080b678d
 80ac498:	080b67c2 	.word	0x080b67c2

080ac49c <_ZN6tflite3ops5micro5split7PrepareEP13TfLiteContextP10TfLiteNode>:
namespace micro {
namespace split {

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
 80ac49c:	2000      	movs	r0, #0
 80ac49e:	4770      	bx	lr

080ac4a0 <_ZN6tflite3ops5micro14Register_SPLITEv>:
}  // namespace split

TfLiteRegistration* Register_SPLIT() {
  static TfLiteRegistration r = {nullptr, nullptr, split::Prepare, split::Eval};
  return &r;
}
 80ac4a0:	4800      	ldr	r0, [pc, #0]	; (80ac4a4 <_ZN6tflite3ops5micro14Register_SPLITEv+0x4>)
 80ac4a2:	4770      	bx	lr
 80ac4a4:	20000488 	.word	0x20000488

080ac4a8 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>:
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
 80ac4a8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80ac4ac:	684c      	ldr	r4, [r1, #4]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
 80ac4ae:	6896      	ldr	r6, [r2, #8]
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
 80ac4b0:	6865      	ldr	r5, [r4, #4]
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
 80ac4b2:	6827      	ldr	r7, [r4, #0]
 80ac4b4:	6884      	ldr	r4, [r0, #8]
 80ac4b6:	f04f 0e38 	mov.w	lr, #56	; 0x38

  const int split_dimensions = input_dims->size;
 80ac4ba:	f8d6 c000 	ldr.w	ip, [r6]
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
 80ac4be:	fb0e 4405 	mla	r4, lr, r5, r4

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
 80ac4c2:	2b00      	cmp	r3, #0
 80ac4c4:	bfb8      	it	lt
 80ac4c6:	4463      	addlt	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
 80ac4c8:	b085      	sub	sp, #20
  const TfLiteIntArray* output_dims = output0->dims;

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
 80ac4ca:	4563      	cmp	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
 80ac4cc:	9102      	str	r1, [sp, #8]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
 80ac4ce:	68a4      	ldr	r4, [r4, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
 80ac4d0:	db01      	blt.n	80ac4d6 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2e>
 80ac4d2:	f003 fdad 	bl	80b0030 <abort>
  TFLITE_DCHECK_EQ(output_dims->size, split_dimensions);
 80ac4d6:	6825      	ldr	r5, [r4, #0]
 80ac4d8:	45ac      	cmp	ip, r5
 80ac4da:	d1fa      	bne.n	80ac4d2 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
 80ac4dc:	009d      	lsls	r5, r3, #2
 80ac4de:	eb04 0e05 	add.w	lr, r4, r5

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
 80ac4e2:	f8de 4004 	ldr.w	r4, [lr, #4]
 80ac4e6:	4435      	add	r5, r6
 80ac4e8:	437c      	muls	r4, r7
 80ac4ea:	686d      	ldr	r5, [r5, #4]
 80ac4ec:	42ac      	cmp	r4, r5
 80ac4ee:	d1f0      	bne.n	80ac4d2 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
 80ac4f0:	46b2      	mov	sl, r6
 80ac4f2:	2401      	movs	r4, #1
 80ac4f4:	2500      	movs	r5, #0
 80ac4f6:	e9cd 4500 	strd	r4, r5, [sp]
 80ac4fa:	f04f 0800 	mov.w	r8, #0
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ac4fe:	4598      	cmp	r8, r3
 80ac500:	da14      	bge.n	80ac52c <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x84>
    outer_size *= input_dims->data[i];
 80ac502:	f85a 9f04 	ldr.w	r9, [sl, #4]!
 80ac506:	9900      	ldr	r1, [sp, #0]
 80ac508:	464c      	mov	r4, r9
 80ac50a:	17e5      	asrs	r5, r4, #31
 80ac50c:	fb01 f405 	mul.w	r4, r1, r5
 80ac510:	9901      	ldr	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ac512:	f108 0801 	add.w	r8, r8, #1
    outer_size *= input_dims->data[i];
 80ac516:	fb09 4b01 	mla	fp, r9, r1, r4
 80ac51a:	9900      	ldr	r1, [sp, #0]
 80ac51c:	fba1 4509 	umull	r4, r5, r1, r9
 80ac520:	e9cd 4500 	strd	r4, r5, [sp]
 80ac524:	9901      	ldr	r1, [sp, #4]
 80ac526:	4459      	add	r1, fp
 80ac528:	9101      	str	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ac52a:	e7e8      	b.n	80ac4fe <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x56>
 80ac52c:	3301      	adds	r3, #1
 80ac52e:	f10c 0c01 	add.w	ip, ip, #1
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
 80ac532:	2401      	movs	r4, #1
 80ac534:	2500      	movs	r5, #0
 80ac536:	3301      	adds	r3, #1
  for (int i = axis + 1; i < split_dimensions; ++i) {
 80ac538:	4563      	cmp	r3, ip
 80ac53a:	d00b      	beq.n	80ac554 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xac>
    base_inner_size *= input_dims->data[i];
 80ac53c:	f856 a023 	ldr.w	sl, [r6, r3, lsl #2]
 80ac540:	ea4f 79ea 	mov.w	r9, sl, asr #31
 80ac544:	fb04 f809 	mul.w	r8, r4, r9
 80ac548:	fb0a 8805 	mla	r8, sl, r5, r8
 80ac54c:	fba4 450a 	umull	r4, r5, r4, sl
 80ac550:	4445      	add	r5, r8
  for (int i = 0; i < axis; ++i) {
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
  for (int i = axis + 1; i < split_dimensions; ++i) {
 80ac552:	e7f0      	b.n	80ac536 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x8e>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80ac554:	f8d2 c004 	ldr.w	ip, [r2, #4]
 80ac558:	f04f 0800 	mov.w	r8, #0
 80ac55c:	f04f 0900 	mov.w	r9, #0
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
 80ac560:	e9dd 2300 	ldrd	r2, r3, [sp]
 80ac564:	4590      	cmp	r8, r2
 80ac566:	eb79 0303 	sbcs.w	r3, r9, r3
 80ac56a:	f8cd 800c 	str.w	r8, [sp, #12]
 80ac56e:	da29      	bge.n	80ac5c4 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x11c>
 80ac570:	2600      	movs	r6, #0
    for (int i = 0; i < output_count; ++i) {
 80ac572:	42be      	cmp	r6, r7
 80ac574:	da21      	bge.n	80ac5ba <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x112>
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
 80ac576:	9b02      	ldr	r3, [sp, #8]
 80ac578:	2138      	movs	r1, #56	; 0x38
 80ac57a:	685b      	ldr	r3, [r3, #4]
 80ac57c:	eb03 0386 	add.w	r3, r3, r6, lsl #2
 80ac580:	685a      	ldr	r2, [r3, #4]
 80ac582:	6883      	ldr	r3, [r0, #8]
 80ac584:	fb01 3302 	mla	r3, r1, r2, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80ac588:	b103      	cbz	r3, 80ac58c <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xe4>
 80ac58a:	685b      	ldr	r3, [r3, #4]
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
 80ac58c:	f8de 2004 	ldr.w	r2, [lr, #4]
      T* output_ptr = output_data + k * copy_size;
 80ac590:	9903      	ldr	r1, [sp, #12]
  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
 80ac592:	4362      	muls	r2, r4
      T* output_ptr = output_data + k * copy_size;
 80ac594:	fb02 fb01 	mul.w	fp, r2, r1
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
 80ac598:	f04f 0a00 	mov.w	sl, #0
 80ac59c:	eb03 038b 	add.w	r3, r3, fp, lsl #2
 80ac5a0:	4592      	cmp	sl, r2
 80ac5a2:	da06      	bge.n	80ac5b2 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x10a>
 80ac5a4:	f85c b02a 	ldr.w	fp, [ip, sl, lsl #2]
 80ac5a8:	f843 b02a 	str.w	fp, [r3, sl, lsl #2]
 80ac5ac:	f10a 0a01 	add.w	sl, sl, #1
 80ac5b0:	e7f6      	b.n	80ac5a0 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xf8>
      input_ptr += copy_size;
 80ac5b2:	eb0c 0c82 	add.w	ip, ip, r2, lsl #2
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
 80ac5b6:	3601      	adds	r6, #1
 80ac5b8:	e7db      	b.n	80ac572 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xca>
 80ac5ba:	f118 0801 	adds.w	r8, r8, #1
 80ac5be:	f149 0900 	adc.w	r9, r9, #0
  for (int i = axis + 1; i < split_dimensions; ++i) {
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
 80ac5c2:	e7cd      	b.n	80ac560 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xb8>
      input_ptr += copy_size;
    }
  }

  return kTfLiteOk;
}
 80ac5c4:	2000      	movs	r0, #0
 80ac5c6:	b005      	add	sp, #20
 80ac5c8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080ac5cc <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>:
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
 80ac5cc:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80ac5d0:	684c      	ldr	r4, [r1, #4]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
 80ac5d2:	6896      	ldr	r6, [r2, #8]
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
 80ac5d4:	6865      	ldr	r5, [r4, #4]
 80ac5d6:	6827      	ldr	r7, [r4, #0]
 80ac5d8:	6884      	ldr	r4, [r0, #8]
 80ac5da:	f04f 0e38 	mov.w	lr, #56	; 0x38

  const int split_dimensions = input_dims->size;
 80ac5de:	f8d6 c000 	ldr.w	ip, [r6]
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
 80ac5e2:	fb0e 4405 	mla	r4, lr, r5, r4

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
 80ac5e6:	2b00      	cmp	r3, #0
 80ac5e8:	bfb8      	it	lt
 80ac5ea:	4463      	addlt	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
 80ac5ec:	b085      	sub	sp, #20
  const TfLiteIntArray* output_dims = output0->dims;

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
 80ac5ee:	4563      	cmp	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
 80ac5f0:	9102      	str	r1, [sp, #8]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
 80ac5f2:	68a4      	ldr	r4, [r4, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
 80ac5f4:	db01      	blt.n	80ac5fa <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2e>
 80ac5f6:	f003 fd1b 	bl	80b0030 <abort>
  TFLITE_DCHECK_EQ(output_dims->size, split_dimensions);
 80ac5fa:	6825      	ldr	r5, [r4, #0]
 80ac5fc:	45ac      	cmp	ip, r5
 80ac5fe:	d1fa      	bne.n	80ac5f6 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
 80ac600:	009d      	lsls	r5, r3, #2
 80ac602:	eb04 0e05 	add.w	lr, r4, r5

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
 80ac606:	f8de 4004 	ldr.w	r4, [lr, #4]
 80ac60a:	4435      	add	r5, r6
 80ac60c:	437c      	muls	r4, r7
 80ac60e:	686d      	ldr	r5, [r5, #4]
 80ac610:	42ac      	cmp	r4, r5
 80ac612:	d1f0      	bne.n	80ac5f6 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
 80ac614:	46b2      	mov	sl, r6
 80ac616:	2401      	movs	r4, #1
 80ac618:	2500      	movs	r5, #0
 80ac61a:	e9cd 4500 	strd	r4, r5, [sp]
 80ac61e:	f04f 0800 	mov.w	r8, #0
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ac622:	4598      	cmp	r8, r3
 80ac624:	da14      	bge.n	80ac650 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x84>
    outer_size *= input_dims->data[i];
 80ac626:	f85a 9f04 	ldr.w	r9, [sl, #4]!
 80ac62a:	9900      	ldr	r1, [sp, #0]
 80ac62c:	464c      	mov	r4, r9
 80ac62e:	17e5      	asrs	r5, r4, #31
 80ac630:	fb01 f405 	mul.w	r4, r1, r5
 80ac634:	9901      	ldr	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ac636:	f108 0801 	add.w	r8, r8, #1
    outer_size *= input_dims->data[i];
 80ac63a:	fb09 4b01 	mla	fp, r9, r1, r4
 80ac63e:	9900      	ldr	r1, [sp, #0]
 80ac640:	fba1 4509 	umull	r4, r5, r1, r9
 80ac644:	e9cd 4500 	strd	r4, r5, [sp]
 80ac648:	9901      	ldr	r1, [sp, #4]
 80ac64a:	4459      	add	r1, fp
 80ac64c:	9101      	str	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ac64e:	e7e8      	b.n	80ac622 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x56>
 80ac650:	3301      	adds	r3, #1
 80ac652:	f10c 0c01 	add.w	ip, ip, #1
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
 80ac656:	2401      	movs	r4, #1
 80ac658:	2500      	movs	r5, #0
 80ac65a:	3301      	adds	r3, #1
  for (int i = axis + 1; i < split_dimensions; ++i) {
 80ac65c:	4563      	cmp	r3, ip
 80ac65e:	d00b      	beq.n	80ac678 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xac>
    base_inner_size *= input_dims->data[i];
 80ac660:	f856 a023 	ldr.w	sl, [r6, r3, lsl #2]
 80ac664:	ea4f 79ea 	mov.w	r9, sl, asr #31
 80ac668:	fb04 f809 	mul.w	r8, r4, r9
 80ac66c:	fb0a 8805 	mla	r8, sl, r5, r8
 80ac670:	fba4 450a 	umull	r4, r5, r4, sl
 80ac674:	4445      	add	r5, r8
  for (int i = 0; i < axis; ++i) {
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
  for (int i = axis + 1; i < split_dimensions; ++i) {
 80ac676:	e7f0      	b.n	80ac65a <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x8e>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80ac678:	6856      	ldr	r6, [r2, #4]
 80ac67a:	f04f 0800 	mov.w	r8, #0
 80ac67e:	f04f 0900 	mov.w	r9, #0
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
 80ac682:	e9dd 2300 	ldrd	r2, r3, [sp]
 80ac686:	4590      	cmp	r8, r2
 80ac688:	eb79 0303 	sbcs.w	r3, r9, r3
 80ac68c:	f8cd 800c 	str.w	r8, [sp, #12]
 80ac690:	da27      	bge.n	80ac6e2 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x116>
 80ac692:	f04f 0c00 	mov.w	ip, #0
    for (int i = 0; i < output_count; ++i) {
 80ac696:	45bc      	cmp	ip, r7
 80ac698:	da1e      	bge.n	80ac6d8 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x10c>
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
 80ac69a:	9b02      	ldr	r3, [sp, #8]
 80ac69c:	2138      	movs	r1, #56	; 0x38
 80ac69e:	685b      	ldr	r3, [r3, #4]
 80ac6a0:	eb03 038c 	add.w	r3, r3, ip, lsl #2
 80ac6a4:	685a      	ldr	r2, [r3, #4]
 80ac6a6:	6883      	ldr	r3, [r0, #8]
 80ac6a8:	fb01 3302 	mla	r3, r1, r2, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80ac6ac:	b103      	cbz	r3, 80ac6b0 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xe4>
 80ac6ae:	685b      	ldr	r3, [r3, #4]
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
      T* output_ptr = output_data + k * copy_size;
 80ac6b0:	46b2      	mov	sl, r6
  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
 80ac6b2:	f8de 2004 	ldr.w	r2, [lr, #4]
 80ac6b6:	9903      	ldr	r1, [sp, #12]
 80ac6b8:	4362      	muls	r2, r4
 80ac6ba:	fb02 3301 	mla	r3, r2, r1, r3
      T* output_ptr = output_data + k * copy_size;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
 80ac6be:	ebc6 0b0a 	rsb	fp, r6, sl
 80ac6c2:	4593      	cmp	fp, r2
 80ac6c4:	da04      	bge.n	80ac6d0 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x104>
 80ac6c6:	f81a bb01 	ldrb.w	fp, [sl], #1
 80ac6ca:	f803 bb01 	strb.w	fp, [r3], #1
 80ac6ce:	e7f6      	b.n	80ac6be <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xf2>
      input_ptr += copy_size;
 80ac6d0:	4416      	add	r6, r2
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
 80ac6d2:	f10c 0c01 	add.w	ip, ip, #1
 80ac6d6:	e7de      	b.n	80ac696 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xca>
 80ac6d8:	f118 0801 	adds.w	r8, r8, #1
 80ac6dc:	f149 0900 	adc.w	r9, r9, #0
  for (int i = axis + 1; i < split_dimensions; ++i) {
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
 80ac6e0:	e7cf      	b.n	80ac682 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xb6>
      input_ptr += copy_size;
    }
  }

  return kTfLiteOk;
}
 80ac6e2:	2000      	movs	r0, #0
 80ac6e4:	b005      	add	sp, #20
 80ac6e6:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080ac6ea <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>:
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
 80ac6ea:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80ac6ee:	684c      	ldr	r4, [r1, #4]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
 80ac6f0:	6896      	ldr	r6, [r2, #8]
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
 80ac6f2:	6865      	ldr	r5, [r4, #4]
 80ac6f4:	6827      	ldr	r7, [r4, #0]
 80ac6f6:	6884      	ldr	r4, [r0, #8]
 80ac6f8:	f04f 0e38 	mov.w	lr, #56	; 0x38

  const int split_dimensions = input_dims->size;
 80ac6fc:	f8d6 c000 	ldr.w	ip, [r6]
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
 80ac700:	fb0e 4405 	mla	r4, lr, r5, r4

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
 80ac704:	2b00      	cmp	r3, #0
 80ac706:	bfb8      	it	lt
 80ac708:	4463      	addlt	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
 80ac70a:	b085      	sub	sp, #20
  const TfLiteIntArray* output_dims = output0->dims;

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
 80ac70c:	4563      	cmp	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
 80ac70e:	9102      	str	r1, [sp, #8]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
 80ac710:	68a4      	ldr	r4, [r4, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
 80ac712:	db01      	blt.n	80ac718 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2e>
 80ac714:	f003 fc8c 	bl	80b0030 <abort>
  TFLITE_DCHECK_EQ(output_dims->size, split_dimensions);
 80ac718:	6825      	ldr	r5, [r4, #0]
 80ac71a:	45ac      	cmp	ip, r5
 80ac71c:	d1fa      	bne.n	80ac714 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
 80ac71e:	009d      	lsls	r5, r3, #2
 80ac720:	eb04 0e05 	add.w	lr, r4, r5

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
 80ac724:	f8de 4004 	ldr.w	r4, [lr, #4]
 80ac728:	4435      	add	r5, r6
 80ac72a:	437c      	muls	r4, r7
 80ac72c:	686d      	ldr	r5, [r5, #4]
 80ac72e:	42ac      	cmp	r4, r5
 80ac730:	d1f0      	bne.n	80ac714 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
 80ac732:	46b2      	mov	sl, r6
 80ac734:	2401      	movs	r4, #1
 80ac736:	2500      	movs	r5, #0
 80ac738:	e9cd 4500 	strd	r4, r5, [sp]
 80ac73c:	f04f 0800 	mov.w	r8, #0
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ac740:	4598      	cmp	r8, r3
 80ac742:	da14      	bge.n	80ac76e <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x84>
    outer_size *= input_dims->data[i];
 80ac744:	f85a 9f04 	ldr.w	r9, [sl, #4]!
 80ac748:	9900      	ldr	r1, [sp, #0]
 80ac74a:	464c      	mov	r4, r9
 80ac74c:	17e5      	asrs	r5, r4, #31
 80ac74e:	fb01 f405 	mul.w	r4, r1, r5
 80ac752:	9901      	ldr	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ac754:	f108 0801 	add.w	r8, r8, #1
    outer_size *= input_dims->data[i];
 80ac758:	fb09 4b01 	mla	fp, r9, r1, r4
 80ac75c:	9900      	ldr	r1, [sp, #0]
 80ac75e:	fba1 4509 	umull	r4, r5, r1, r9
 80ac762:	e9cd 4500 	strd	r4, r5, [sp]
 80ac766:	9901      	ldr	r1, [sp, #4]
 80ac768:	4459      	add	r1, fp
 80ac76a:	9101      	str	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ac76c:	e7e8      	b.n	80ac740 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x56>
 80ac76e:	3301      	adds	r3, #1
 80ac770:	f10c 0c01 	add.w	ip, ip, #1
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
 80ac774:	2401      	movs	r4, #1
 80ac776:	2500      	movs	r5, #0
 80ac778:	3301      	adds	r3, #1
  for (int i = axis + 1; i < split_dimensions; ++i) {
 80ac77a:	4563      	cmp	r3, ip
 80ac77c:	d00b      	beq.n	80ac796 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xac>
    base_inner_size *= input_dims->data[i];
 80ac77e:	f856 a023 	ldr.w	sl, [r6, r3, lsl #2]
 80ac782:	ea4f 79ea 	mov.w	r9, sl, asr #31
 80ac786:	fb04 f809 	mul.w	r8, r4, r9
 80ac78a:	fb0a 8805 	mla	r8, sl, r5, r8
 80ac78e:	fba4 450a 	umull	r4, r5, r4, sl
 80ac792:	4445      	add	r5, r8
  for (int i = 0; i < axis; ++i) {
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
  for (int i = axis + 1; i < split_dimensions; ++i) {
 80ac794:	e7f0      	b.n	80ac778 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x8e>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80ac796:	6856      	ldr	r6, [r2, #4]
 80ac798:	f04f 0800 	mov.w	r8, #0
 80ac79c:	f04f 0900 	mov.w	r9, #0
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
 80ac7a0:	e9dd 2300 	ldrd	r2, r3, [sp]
 80ac7a4:	4590      	cmp	r8, r2
 80ac7a6:	eb79 0303 	sbcs.w	r3, r9, r3
 80ac7aa:	f8cd 800c 	str.w	r8, [sp, #12]
 80ac7ae:	da27      	bge.n	80ac800 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x116>
 80ac7b0:	f04f 0c00 	mov.w	ip, #0
    for (int i = 0; i < output_count; ++i) {
 80ac7b4:	45bc      	cmp	ip, r7
 80ac7b6:	da1e      	bge.n	80ac7f6 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x10c>
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
 80ac7b8:	9b02      	ldr	r3, [sp, #8]
 80ac7ba:	2138      	movs	r1, #56	; 0x38
 80ac7bc:	685b      	ldr	r3, [r3, #4]
 80ac7be:	eb03 038c 	add.w	r3, r3, ip, lsl #2
 80ac7c2:	685a      	ldr	r2, [r3, #4]
 80ac7c4:	6883      	ldr	r3, [r0, #8]
 80ac7c6:	fb01 3302 	mla	r3, r1, r2, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80ac7ca:	b103      	cbz	r3, 80ac7ce <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xe4>
 80ac7cc:	685b      	ldr	r3, [r3, #4]
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
      T* output_ptr = output_data + k * copy_size;
 80ac7ce:	46b2      	mov	sl, r6
  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
 80ac7d0:	f8de 2004 	ldr.w	r2, [lr, #4]
 80ac7d4:	9903      	ldr	r1, [sp, #12]
 80ac7d6:	4362      	muls	r2, r4
 80ac7d8:	fb02 3301 	mla	r3, r2, r1, r3
      T* output_ptr = output_data + k * copy_size;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
 80ac7dc:	ebc6 0b0a 	rsb	fp, r6, sl
 80ac7e0:	4593      	cmp	fp, r2
 80ac7e2:	da04      	bge.n	80ac7ee <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x104>
 80ac7e4:	f91a bb01 	ldrsb.w	fp, [sl], #1
 80ac7e8:	f803 bb01 	strb.w	fp, [r3], #1
 80ac7ec:	e7f6      	b.n	80ac7dc <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xf2>
      input_ptr += copy_size;
 80ac7ee:	4416      	add	r6, r2
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
 80ac7f0:	f10c 0c01 	add.w	ip, ip, #1
 80ac7f4:	e7de      	b.n	80ac7b4 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xca>
 80ac7f6:	f118 0801 	adds.w	r8, r8, #1
 80ac7fa:	f149 0900 	adc.w	r9, r9, #0
  for (int i = axis + 1; i < split_dimensions; ++i) {
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
 80ac7fe:	e7cf      	b.n	80ac7a0 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xb6>
      input_ptr += copy_size;
    }
  }

  return kTfLiteOk;
}
 80ac800:	2000      	movs	r0, #0
 80ac802:	b005      	add	sp, #20
 80ac804:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080ac808 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>:
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
 80ac808:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80ac80c:	684c      	ldr	r4, [r1, #4]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
 80ac80e:	6896      	ldr	r6, [r2, #8]
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
 80ac810:	6865      	ldr	r5, [r4, #4]
 80ac812:	6827      	ldr	r7, [r4, #0]
 80ac814:	6884      	ldr	r4, [r0, #8]
 80ac816:	f04f 0e38 	mov.w	lr, #56	; 0x38

  const int split_dimensions = input_dims->size;
 80ac81a:	f8d6 c000 	ldr.w	ip, [r6]
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
 80ac81e:	fb0e 4405 	mla	r4, lr, r5, r4

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
 80ac822:	2b00      	cmp	r3, #0
 80ac824:	bfb8      	it	lt
 80ac826:	4463      	addlt	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
 80ac828:	b085      	sub	sp, #20
  const TfLiteIntArray* output_dims = output0->dims;

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
 80ac82a:	4563      	cmp	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
 80ac82c:	9102      	str	r1, [sp, #8]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
 80ac82e:	68a4      	ldr	r4, [r4, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
 80ac830:	db01      	blt.n	80ac836 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2e>
 80ac832:	f003 fbfd 	bl	80b0030 <abort>
  TFLITE_DCHECK_EQ(output_dims->size, split_dimensions);
 80ac836:	6825      	ldr	r5, [r4, #0]
 80ac838:	45ac      	cmp	ip, r5
 80ac83a:	d1fa      	bne.n	80ac832 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
 80ac83c:	009d      	lsls	r5, r3, #2
 80ac83e:	eb04 0e05 	add.w	lr, r4, r5

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
 80ac842:	f8de 4004 	ldr.w	r4, [lr, #4]
 80ac846:	4435      	add	r5, r6
 80ac848:	437c      	muls	r4, r7
 80ac84a:	686d      	ldr	r5, [r5, #4]
 80ac84c:	42ac      	cmp	r4, r5
 80ac84e:	d1f0      	bne.n	80ac832 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
 80ac850:	46b2      	mov	sl, r6
 80ac852:	2401      	movs	r4, #1
 80ac854:	2500      	movs	r5, #0
 80ac856:	e9cd 4500 	strd	r4, r5, [sp]
 80ac85a:	f04f 0800 	mov.w	r8, #0
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ac85e:	4598      	cmp	r8, r3
 80ac860:	da14      	bge.n	80ac88c <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x84>
    outer_size *= input_dims->data[i];
 80ac862:	f85a 9f04 	ldr.w	r9, [sl, #4]!
 80ac866:	9900      	ldr	r1, [sp, #0]
 80ac868:	464c      	mov	r4, r9
 80ac86a:	17e5      	asrs	r5, r4, #31
 80ac86c:	fb01 f405 	mul.w	r4, r1, r5
 80ac870:	9901      	ldr	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ac872:	f108 0801 	add.w	r8, r8, #1
    outer_size *= input_dims->data[i];
 80ac876:	fb09 4b01 	mla	fp, r9, r1, r4
 80ac87a:	9900      	ldr	r1, [sp, #0]
 80ac87c:	fba1 4509 	umull	r4, r5, r1, r9
 80ac880:	e9cd 4500 	strd	r4, r5, [sp]
 80ac884:	9901      	ldr	r1, [sp, #4]
 80ac886:	4459      	add	r1, fp
 80ac888:	9101      	str	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ac88a:	e7e8      	b.n	80ac85e <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x56>
 80ac88c:	3301      	adds	r3, #1
 80ac88e:	f10c 0c01 	add.w	ip, ip, #1
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
 80ac892:	2401      	movs	r4, #1
 80ac894:	2500      	movs	r5, #0
 80ac896:	3301      	adds	r3, #1
  for (int i = axis + 1; i < split_dimensions; ++i) {
 80ac898:	4563      	cmp	r3, ip
 80ac89a:	d00b      	beq.n	80ac8b4 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xac>
    base_inner_size *= input_dims->data[i];
 80ac89c:	f856 a023 	ldr.w	sl, [r6, r3, lsl #2]
 80ac8a0:	ea4f 79ea 	mov.w	r9, sl, asr #31
 80ac8a4:	fb04 f809 	mul.w	r8, r4, r9
 80ac8a8:	fb0a 8805 	mla	r8, sl, r5, r8
 80ac8ac:	fba4 450a 	umull	r4, r5, r4, sl
 80ac8b0:	4445      	add	r5, r8
  for (int i = 0; i < axis; ++i) {
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
  for (int i = axis + 1; i < split_dimensions; ++i) {
 80ac8b2:	e7f0      	b.n	80ac896 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x8e>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80ac8b4:	f8d2 c004 	ldr.w	ip, [r2, #4]
 80ac8b8:	f04f 0800 	mov.w	r8, #0
 80ac8bc:	f04f 0900 	mov.w	r9, #0
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
 80ac8c0:	e9dd 2300 	ldrd	r2, r3, [sp]
 80ac8c4:	4590      	cmp	r8, r2
 80ac8c6:	eb79 0303 	sbcs.w	r3, r9, r3
 80ac8ca:	f8cd 800c 	str.w	r8, [sp, #12]
 80ac8ce:	da29      	bge.n	80ac924 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x11c>
 80ac8d0:	2600      	movs	r6, #0
    for (int i = 0; i < output_count; ++i) {
 80ac8d2:	42be      	cmp	r6, r7
 80ac8d4:	da21      	bge.n	80ac91a <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x112>
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
 80ac8d6:	9b02      	ldr	r3, [sp, #8]
 80ac8d8:	2138      	movs	r1, #56	; 0x38
 80ac8da:	685b      	ldr	r3, [r3, #4]
 80ac8dc:	eb03 0386 	add.w	r3, r3, r6, lsl #2
 80ac8e0:	685a      	ldr	r2, [r3, #4]
 80ac8e2:	6883      	ldr	r3, [r0, #8]
 80ac8e4:	fb01 3302 	mla	r3, r1, r2, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80ac8e8:	b103      	cbz	r3, 80ac8ec <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xe4>
 80ac8ea:	685b      	ldr	r3, [r3, #4]
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
 80ac8ec:	f8de 2004 	ldr.w	r2, [lr, #4]
      T* output_ptr = output_data + k * copy_size;
 80ac8f0:	9903      	ldr	r1, [sp, #12]
  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
 80ac8f2:	4362      	muls	r2, r4
      T* output_ptr = output_data + k * copy_size;
 80ac8f4:	fb02 fb01 	mul.w	fp, r2, r1
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
 80ac8f8:	f04f 0a00 	mov.w	sl, #0
 80ac8fc:	eb03 034b 	add.w	r3, r3, fp, lsl #1
 80ac900:	4592      	cmp	sl, r2
 80ac902:	da06      	bge.n	80ac912 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x10a>
 80ac904:	f93c b01a 	ldrsh.w	fp, [ip, sl, lsl #1]
 80ac908:	f823 b01a 	strh.w	fp, [r3, sl, lsl #1]
 80ac90c:	f10a 0a01 	add.w	sl, sl, #1
 80ac910:	e7f6      	b.n	80ac900 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xf8>
      input_ptr += copy_size;
 80ac912:	eb0c 0c42 	add.w	ip, ip, r2, lsl #1
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
 80ac916:	3601      	adds	r6, #1
 80ac918:	e7db      	b.n	80ac8d2 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xca>
 80ac91a:	f118 0801 	adds.w	r8, r8, #1
 80ac91e:	f149 0900 	adc.w	r9, r9, #0
  for (int i = axis + 1; i < split_dimensions; ++i) {
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
 80ac922:	e7cd      	b.n	80ac8c0 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xb8>
      input_ptr += copy_size;
    }
  }

  return kTfLiteOk;
}
 80ac924:	2000      	movs	r0, #0
 80ac926:	b005      	add	sp, #20
 80ac928:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080ac92c <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>:
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
 80ac92c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80ac930:	684c      	ldr	r4, [r1, #4]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
 80ac932:	6896      	ldr	r6, [r2, #8]
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
 80ac934:	6865      	ldr	r5, [r4, #4]
 80ac936:	6827      	ldr	r7, [r4, #0]
 80ac938:	6884      	ldr	r4, [r0, #8]
 80ac93a:	f04f 0e38 	mov.w	lr, #56	; 0x38

  const int split_dimensions = input_dims->size;
 80ac93e:	f8d6 c000 	ldr.w	ip, [r6]
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
 80ac942:	fb0e 4405 	mla	r4, lr, r5, r4

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
 80ac946:	2b00      	cmp	r3, #0
 80ac948:	bfb8      	it	lt
 80ac94a:	4463      	addlt	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
 80ac94c:	b085      	sub	sp, #20
  const TfLiteIntArray* output_dims = output0->dims;

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
 80ac94e:	4563      	cmp	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
 80ac950:	9102      	str	r1, [sp, #8]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
 80ac952:	68a4      	ldr	r4, [r4, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
 80ac954:	db01      	blt.n	80ac95a <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2e>
 80ac956:	f003 fb6b 	bl	80b0030 <abort>
  TFLITE_DCHECK_EQ(output_dims->size, split_dimensions);
 80ac95a:	6825      	ldr	r5, [r4, #0]
 80ac95c:	45ac      	cmp	ip, r5
 80ac95e:	d1fa      	bne.n	80ac956 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
 80ac960:	009d      	lsls	r5, r3, #2
 80ac962:	eb04 0e05 	add.w	lr, r4, r5

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
 80ac966:	f8de 4004 	ldr.w	r4, [lr, #4]
 80ac96a:	4435      	add	r5, r6
 80ac96c:	437c      	muls	r4, r7
 80ac96e:	686d      	ldr	r5, [r5, #4]
 80ac970:	42ac      	cmp	r4, r5
 80ac972:	d1f0      	bne.n	80ac956 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
 80ac974:	46b2      	mov	sl, r6
 80ac976:	2401      	movs	r4, #1
 80ac978:	2500      	movs	r5, #0
 80ac97a:	e9cd 4500 	strd	r4, r5, [sp]
 80ac97e:	f04f 0800 	mov.w	r8, #0
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ac982:	4598      	cmp	r8, r3
 80ac984:	da14      	bge.n	80ac9b0 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x84>
    outer_size *= input_dims->data[i];
 80ac986:	f85a 9f04 	ldr.w	r9, [sl, #4]!
 80ac98a:	9900      	ldr	r1, [sp, #0]
 80ac98c:	464c      	mov	r4, r9
 80ac98e:	17e5      	asrs	r5, r4, #31
 80ac990:	fb01 f405 	mul.w	r4, r1, r5
 80ac994:	9901      	ldr	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ac996:	f108 0801 	add.w	r8, r8, #1
    outer_size *= input_dims->data[i];
 80ac99a:	fb09 4b01 	mla	fp, r9, r1, r4
 80ac99e:	9900      	ldr	r1, [sp, #0]
 80ac9a0:	fba1 4509 	umull	r4, r5, r1, r9
 80ac9a4:	e9cd 4500 	strd	r4, r5, [sp]
 80ac9a8:	9901      	ldr	r1, [sp, #4]
 80ac9aa:	4459      	add	r1, fp
 80ac9ac:	9101      	str	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ac9ae:	e7e8      	b.n	80ac982 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x56>
 80ac9b0:	3301      	adds	r3, #1
 80ac9b2:	f10c 0c01 	add.w	ip, ip, #1
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
 80ac9b6:	2401      	movs	r4, #1
 80ac9b8:	2500      	movs	r5, #0
 80ac9ba:	3301      	adds	r3, #1
  for (int i = axis + 1; i < split_dimensions; ++i) {
 80ac9bc:	4563      	cmp	r3, ip
 80ac9be:	d00b      	beq.n	80ac9d8 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xac>
    base_inner_size *= input_dims->data[i];
 80ac9c0:	f856 a023 	ldr.w	sl, [r6, r3, lsl #2]
 80ac9c4:	ea4f 79ea 	mov.w	r9, sl, asr #31
 80ac9c8:	fb04 f809 	mul.w	r8, r4, r9
 80ac9cc:	fb0a 8805 	mla	r8, sl, r5, r8
 80ac9d0:	fba4 450a 	umull	r4, r5, r4, sl
 80ac9d4:	4445      	add	r5, r8
  for (int i = 0; i < axis; ++i) {
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
  for (int i = axis + 1; i < split_dimensions; ++i) {
 80ac9d6:	e7f0      	b.n	80ac9ba <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x8e>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80ac9d8:	f8d2 c004 	ldr.w	ip, [r2, #4]
 80ac9dc:	f04f 0800 	mov.w	r8, #0
 80ac9e0:	f04f 0900 	mov.w	r9, #0
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
 80ac9e4:	e9dd 2300 	ldrd	r2, r3, [sp]
 80ac9e8:	4590      	cmp	r8, r2
 80ac9ea:	eb79 0303 	sbcs.w	r3, r9, r3
 80ac9ee:	f8cd 800c 	str.w	r8, [sp, #12]
 80ac9f2:	da29      	bge.n	80aca48 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x11c>
 80ac9f4:	2600      	movs	r6, #0
    for (int i = 0; i < output_count; ++i) {
 80ac9f6:	42be      	cmp	r6, r7
 80ac9f8:	da21      	bge.n	80aca3e <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x112>
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
 80ac9fa:	9b02      	ldr	r3, [sp, #8]
 80ac9fc:	2138      	movs	r1, #56	; 0x38
 80ac9fe:	685b      	ldr	r3, [r3, #4]
 80aca00:	eb03 0386 	add.w	r3, r3, r6, lsl #2
 80aca04:	685a      	ldr	r2, [r3, #4]
 80aca06:	6883      	ldr	r3, [r0, #8]
 80aca08:	fb01 3302 	mla	r3, r1, r2, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80aca0c:	b103      	cbz	r3, 80aca10 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xe4>
 80aca0e:	685b      	ldr	r3, [r3, #4]
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
 80aca10:	f8de 2004 	ldr.w	r2, [lr, #4]
      T* output_ptr = output_data + k * copy_size;
 80aca14:	9903      	ldr	r1, [sp, #12]
  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
 80aca16:	4362      	muls	r2, r4
      T* output_ptr = output_data + k * copy_size;
 80aca18:	fb02 fb01 	mul.w	fp, r2, r1
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
 80aca1c:	f04f 0a00 	mov.w	sl, #0
 80aca20:	eb03 038b 	add.w	r3, r3, fp, lsl #2
 80aca24:	4592      	cmp	sl, r2
 80aca26:	da06      	bge.n	80aca36 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x10a>
 80aca28:	f85c 102a 	ldr.w	r1, [ip, sl, lsl #2]
 80aca2c:	f843 102a 	str.w	r1, [r3, sl, lsl #2]
 80aca30:	f10a 0a01 	add.w	sl, sl, #1
 80aca34:	e7f6      	b.n	80aca24 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xf8>
      input_ptr += copy_size;
 80aca36:	eb0c 0c82 	add.w	ip, ip, r2, lsl #2
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
 80aca3a:	3601      	adds	r6, #1
 80aca3c:	e7db      	b.n	80ac9f6 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xca>
 80aca3e:	f118 0801 	adds.w	r8, r8, #1
 80aca42:	f149 0900 	adc.w	r9, r9, #0
  for (int i = axis + 1; i < split_dimensions; ++i) {
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
 80aca46:	e7cd      	b.n	80ac9e4 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xb8>
      input_ptr += copy_size;
    }
  }

  return kTfLiteOk;
}
 80aca48:	2000      	movs	r0, #0
 80aca4a:	b005      	add	sp, #20
 80aca4c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080aca50 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80aca50:	e92d 41f3 	stmdb	sp!, {r0, r1, r4, r5, r6, r7, r8, lr}
 80aca54:	680a      	ldr	r2, [r1, #0]
 80aca56:	f8d0 e008 	ldr.w	lr, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80aca5a:	6857      	ldr	r7, [r2, #4]
 80aca5c:	2338      	movs	r3, #56	; 0x38
 80aca5e:	fb03 e707 	mla	r7, r3, r7, lr
  const TfLiteTensor* input = GetInput(context, node, 1);

  // Dynamic output tensors are needed if axis tensor is not constant.
  // But Micro doesn't support dynamic memeory allocation, so we only support
  // constant axis tensor for now.
  TF_LITE_ENSURE_MSG(context, IsConstantTensor(axis),
 80aca62:	f897 8014 	ldrb.w	r8, [r7, #20]
  }

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80aca66:	4605      	mov	r5, r0
  const TfLiteTensor* input = GetInput(context, node, 1);

  // Dynamic output tensors are needed if axis tensor is not constant.
  // But Micro doesn't support dynamic memeory allocation, so we only support
  // constant axis tensor for now.
  TF_LITE_ENSURE_MSG(context, IsConstantTensor(axis),
 80aca68:	f1b8 0f01 	cmp.w	r8, #1
 80aca6c:	d003      	beq.n	80aca76 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0x26>
 80aca6e:	6943      	ldr	r3, [r0, #20]
 80aca70:	4926      	ldr	r1, [pc, #152]	; (80acb0c <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xbc>)
 80aca72:	4798      	blx	r3
 80aca74:	e046      	b.n	80acb04 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb4>
 80aca76:	6896      	ldr	r6, [r2, #8]
 80aca78:	435e      	muls	r6, r3
                     "Non constant axis tensor not supported");

  int axis_value = GetTensorData<int32_t>(axis)[0];
 80aca7a:	687b      	ldr	r3, [r7, #4]
 80aca7c:	eb0e 0206 	add.w	r2, lr, r6
 80aca80:	681b      	ldr	r3, [r3, #0]
 80aca82:	6897      	ldr	r7, [r2, #8]
  if (axis_value < 0) {
 80aca84:	2b00      	cmp	r3, #0
 80aca86:	da0a      	bge.n	80aca9e <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0x4e>
    axis_value += NumDimensions(input);
  }

  TF_LITE_ENSURE(context, axis_value >= 0);
 80aca88:	683c      	ldr	r4, [r7, #0]
 80aca8a:	191b      	adds	r3, r3, r4
 80aca8c:	d507      	bpl.n	80aca9e <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0x4e>
 80aca8e:	4b20      	ldr	r3, [pc, #128]	; (80acb10 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xc0>)
 80aca90:	4a20      	ldr	r2, [pc, #128]	; (80acb14 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xc4>)
 80aca92:	9300      	str	r3, [sp, #0]
 80aca94:	6945      	ldr	r5, [r0, #20]
 80aca96:	2357      	movs	r3, #87	; 0x57
 80aca98:	491f      	ldr	r1, [pc, #124]	; (80acb18 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xc8>)
 80aca9a:	47a8      	blx	r5
 80aca9c:	e032      	b.n	80acb04 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb4>
  TF_LITE_ENSURE(context, axis_value < NumDimensions(input));
 80aca9e:	6838      	ldr	r0, [r7, #0]
 80acaa0:	4283      	cmp	r3, r0
 80acaa2:	db08      	blt.n	80acab6 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0x66>
 80acaa4:	4b1d      	ldr	r3, [pc, #116]	; (80acb1c <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xcc>)
 80acaa6:	4a1b      	ldr	r2, [pc, #108]	; (80acb14 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xc4>)
 80acaa8:	9300      	str	r3, [sp, #0]
 80acaaa:	696c      	ldr	r4, [r5, #20]
 80acaac:	2358      	movs	r3, #88	; 0x58
 80acaae:	491a      	ldr	r1, [pc, #104]	; (80acb18 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xc8>)
 80acab0:	4628      	mov	r0, r5
 80acab2:	47a0      	blx	r4
 80acab4:	e026      	b.n	80acb04 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb4>

  switch (input->type) {
 80acab6:	f81e 0006 	ldrb.w	r0, [lr, r6]
 80acaba:	1e44      	subs	r4, r0, #1
 80acabc:	2c08      	cmp	r4, #8
 80acabe:	d81a      	bhi.n	80acaf6 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xa6>
 80acac0:	e8df f004 	tbb	[pc, r4]
 80acac4:	19091505 	.word	0x19091505
 80acac8:	19111919 	.word	0x19111919
 80acacc:	0d          	.byte	0x0d
 80acacd:	00          	.byte	0x00
    case kTfLiteFloat32: {
      return SplitImpl<float>(context, node, input, axis_value);
 80acace:	4628      	mov	r0, r5
 80acad0:	f7ff fcea 	bl	80ac4a8 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>
 80acad4:	e017      	b.n	80acb06 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb6>
    }
    case kTfLiteUInt8: {
      return SplitImpl<uint8_t>(context, node, input, axis_value);
 80acad6:	4628      	mov	r0, r5
 80acad8:	f7ff fd78 	bl	80ac5cc <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>
 80acadc:	e013      	b.n	80acb06 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb6>
    }
    case kTfLiteInt8: {
      return SplitImpl<int8_t>(context, node, input, axis_value);
 80acade:	4628      	mov	r0, r5
 80acae0:	f7ff fe03 	bl	80ac6ea <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>
 80acae4:	e00f      	b.n	80acb06 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb6>
    }
    case kTfLiteInt16: {
      return SplitImpl<int16_t>(context, node, input, axis_value);
 80acae6:	4628      	mov	r0, r5
 80acae8:	f7ff fe8e 	bl	80ac808 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>
 80acaec:	e00b      	b.n	80acb06 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb6>
    }
    case kTfLiteInt32: {
      return SplitImpl<int32_t>(context, node, input, axis_value);
 80acaee:	4628      	mov	r0, r5
 80acaf0:	f7ff ff1c 	bl	80ac92c <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>
 80acaf4:	e007      	b.n	80acb06 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb6>
    }
    default:
      context->ReportError(context, "Type %s currently not supported.",
 80acaf6:	696c      	ldr	r4, [r5, #20]
 80acaf8:	f7f3 fb10 	bl	80a011c <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type));
 80acafc:	4908      	ldr	r1, [pc, #32]	; (80acb20 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xd0>)
 80acafe:	4602      	mov	r2, r0
 80acb00:	4628      	mov	r0, r5
 80acb02:	47a0      	blx	r4
      return kTfLiteError;
 80acb04:	2001      	movs	r0, #1
  }
#undef TF_LITE_SPLIT

  return kTfLiteOk;
}
 80acb06:	b002      	add	sp, #8
 80acb08:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80acb0c:	080b6852 	.word	0x080b6852
 80acb10:	080b69c7 	.word	0x080b69c7
 80acb14:	080b6920 	.word	0x080b6920
 80acb18:	080b5db0 	.word	0x080b5db0
 80acb1c:	080b69d7 	.word	0x080b69d7
 80acb20:	080b69f9 	.word	0x080b69f9

080acb24 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi>:
}

inline void StridedSlicePadIndices(tflite::StridedSliceParams* p,
                                   int dim_count) {
  // Add indices and mask bits to fully include extra dimensions
  TFLITE_CHECK_LE(dim_count, 4);
 80acb24:	2904      	cmp	r1, #4
  if (v < lo) return lo;
  return v;
}

inline void StridedSlicePadIndices(tflite::StridedSliceParams* p,
                                   int dim_count) {
 80acb26:	b570      	push	{r4, r5, r6, lr}
  // Add indices and mask bits to fully include extra dimensions
  TFLITE_CHECK_LE(dim_count, 4);
 80acb28:	dd01      	ble.n	80acb2e <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0xa>
 80acb2a:	f003 fa81 	bl	80b0030 <abort>
  TFLITE_CHECK_GE(dim_count, p->start_indices_count);
 80acb2e:	f990 3000 	ldrsb.w	r3, [r0]
 80acb32:	428b      	cmp	r3, r1
 80acb34:	dcf9      	bgt.n	80acb2a <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x6>
  TFLITE_CHECK_EQ(p->start_indices_count, p->stop_indices_count);
 80acb36:	f990 200a 	ldrsb.w	r2, [r0, #10]
 80acb3a:	429a      	cmp	r2, r3
 80acb3c:	d1f5      	bne.n	80acb2a <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x6>
  TFLITE_CHECK_EQ(p->stop_indices_count, p->strides_count);
 80acb3e:	f990 3014 	ldrsb.w	r3, [r0, #20]
 80acb42:	4293      	cmp	r3, r2
 80acb44:	d1f1      	bne.n	80acb2a <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x6>

  const int pad_count = dim_count - p->start_indices_count;

  // Pad indices at start, so move arrays by pad_count.
  for (int i = p->start_indices_count - 1; i >= 0; --i) {
 80acb46:	1e5d      	subs	r5, r3, #1
 80acb48:	eb00 0443 	add.w	r4, r0, r3, lsl #1
 80acb4c:	eb00 0241 	add.w	r2, r0, r1, lsl #1
 80acb50:	1acb      	subs	r3, r1, r3
 80acb52:	2d00      	cmp	r5, #0
 80acb54:	db0c      	blt.n	80acb70 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x4c>
    p->strides[i + pad_count] = p->strides[i];
 80acb56:	f9b4 6014 	ldrsh.w	r6, [r4, #20]
  TFLITE_CHECK_EQ(p->stop_indices_count, p->strides_count);

  const int pad_count = dim_count - p->start_indices_count;

  // Pad indices at start, so move arrays by pad_count.
  for (int i = p->start_indices_count - 1; i >= 0; --i) {
 80acb5a:	3d01      	subs	r5, #1
    p->strides[i + pad_count] = p->strides[i];
 80acb5c:	8296      	strh	r6, [r2, #20]
    p->start_indices[i + pad_count] = p->start_indices[i];
 80acb5e:	f9b4 6000 	ldrsh.w	r6, [r4]
 80acb62:	3a02      	subs	r2, #2
 80acb64:	8056      	strh	r6, [r2, #2]
    p->stop_indices[i + pad_count] = p->stop_indices[i];
 80acb66:	f9b4 600a 	ldrsh.w	r6, [r4, #10]
 80acb6a:	3c02      	subs	r4, #2
 80acb6c:	8196      	strh	r6, [r2, #12]
  TFLITE_CHECK_EQ(p->stop_indices_count, p->strides_count);

  const int pad_count = dim_count - p->start_indices_count;

  // Pad indices at start, so move arrays by pad_count.
  for (int i = p->start_indices_count - 1; i >= 0; --i) {
 80acb6e:	e7f0      	b.n	80acb52 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x2e>
 80acb70:	2400      	movs	r4, #0
 80acb72:	4602      	mov	r2, r0
    p->strides[i + pad_count] = p->strides[i];
    p->start_indices[i + pad_count] = p->start_indices[i];
    p->stop_indices[i + pad_count] = p->stop_indices[i];
  }
  for (int i = 0; i < pad_count; ++i) {
    p->start_indices[i] = 0;
 80acb74:	4626      	mov	r6, r4
    p->stop_indices[i] = 1;
 80acb76:	2501      	movs	r5, #1
  for (int i = p->start_indices_count - 1; i >= 0; --i) {
    p->strides[i + pad_count] = p->strides[i];
    p->start_indices[i + pad_count] = p->start_indices[i];
    p->stop_indices[i + pad_count] = p->stop_indices[i];
  }
  for (int i = 0; i < pad_count; ++i) {
 80acb78:	429c      	cmp	r4, r3
 80acb7a:	f102 0202 	add.w	r2, r2, #2
 80acb7e:	da04      	bge.n	80acb8a <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x66>
    p->start_indices[i] = 0;
 80acb80:	8016      	strh	r6, [r2, #0]
    p->stop_indices[i] = 1;
 80acb82:	8155      	strh	r5, [r2, #10]
    p->strides[i] = 1;
 80acb84:	8295      	strh	r5, [r2, #20]
  for (int i = p->start_indices_count - 1; i >= 0; --i) {
    p->strides[i + pad_count] = p->strides[i];
    p->start_indices[i + pad_count] = p->start_indices[i];
    p->stop_indices[i + pad_count] = p->stop_indices[i];
  }
  for (int i = 0; i < pad_count; ++i) {
 80acb86:	3401      	adds	r4, #1
 80acb88:	e7f6      	b.n	80acb78 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x54>
    p->stop_indices[i] = 1;
    p->strides[i] = 1;
  }

  // Pad masks with 0s or 1s as required.
  p->shrink_axis_mask <<= pad_count;
 80acb8a:	f9b0 2026 	ldrsh.w	r2, [r0, #38]	; 0x26
  p->ellipsis_mask <<= pad_count;
  p->new_axis_mask <<= pad_count;
  p->begin_mask <<= pad_count;
  p->end_mask <<= pad_count;
  p->begin_mask |= (1 << pad_count) - 1;
 80acb8e:	2401      	movs	r4, #1
    p->stop_indices[i] = 1;
    p->strides[i] = 1;
  }

  // Pad masks with 0s or 1s as required.
  p->shrink_axis_mask <<= pad_count;
 80acb90:	409a      	lsls	r2, r3
 80acb92:	84c2      	strh	r2, [r0, #38]	; 0x26
  p->ellipsis_mask <<= pad_count;
 80acb94:	f9b0 2020 	ldrsh.w	r2, [r0, #32]
  p->new_axis_mask <<= pad_count;
  p->begin_mask <<= pad_count;
  p->end_mask <<= pad_count;
  p->begin_mask |= (1 << pad_count) - 1;
 80acb98:	409c      	lsls	r4, r3
    p->strides[i] = 1;
  }

  // Pad masks with 0s or 1s as required.
  p->shrink_axis_mask <<= pad_count;
  p->ellipsis_mask <<= pad_count;
 80acb9a:	409a      	lsls	r2, r3
 80acb9c:	8402      	strh	r2, [r0, #32]
  p->new_axis_mask <<= pad_count;
 80acb9e:	f9b0 2024 	ldrsh.w	r2, [r0, #36]	; 0x24
  p->begin_mask <<= pad_count;
  p->end_mask <<= pad_count;
  p->begin_mask |= (1 << pad_count) - 1;
 80acba2:	3c01      	subs	r4, #1
  }

  // Pad masks with 0s or 1s as required.
  p->shrink_axis_mask <<= pad_count;
  p->ellipsis_mask <<= pad_count;
  p->new_axis_mask <<= pad_count;
 80acba4:	409a      	lsls	r2, r3
 80acba6:	8482      	strh	r2, [r0, #36]	; 0x24
  p->begin_mask <<= pad_count;
  p->end_mask <<= pad_count;
 80acba8:	f9b0 2022 	ldrsh.w	r2, [r0, #34]	; 0x22
  p->begin_mask |= (1 << pad_count) - 1;
  p->end_mask |= (1 << pad_count) - 1;

  p->start_indices_count = dim_count;
 80acbac:	b249      	sxtb	r1, r1
  // Pad masks with 0s or 1s as required.
  p->shrink_axis_mask <<= pad_count;
  p->ellipsis_mask <<= pad_count;
  p->new_axis_mask <<= pad_count;
  p->begin_mask <<= pad_count;
  p->end_mask <<= pad_count;
 80acbae:	fa02 f503 	lsl.w	r5, r2, r3
  p->begin_mask |= (1 << pad_count) - 1;
 80acbb2:	b222      	sxth	r2, r4
 80acbb4:	f9b0 401e 	ldrsh.w	r4, [r0, #30]
  p->end_mask |= (1 << pad_count) - 1;

  p->start_indices_count = dim_count;
 80acbb8:	7001      	strb	r1, [r0, #0]
  p->shrink_axis_mask <<= pad_count;
  p->ellipsis_mask <<= pad_count;
  p->new_axis_mask <<= pad_count;
  p->begin_mask <<= pad_count;
  p->end_mask <<= pad_count;
  p->begin_mask |= (1 << pad_count) - 1;
 80acbba:	fa04 f303 	lsl.w	r3, r4, r3
 80acbbe:	4313      	orrs	r3, r2
  p->end_mask |= (1 << pad_count) - 1;
 80acbc0:	432a      	orrs	r2, r5
  p->shrink_axis_mask <<= pad_count;
  p->ellipsis_mask <<= pad_count;
  p->new_axis_mask <<= pad_count;
  p->begin_mask <<= pad_count;
  p->end_mask <<= pad_count;
  p->begin_mask |= (1 << pad_count) - 1;
 80acbc2:	83c3      	strh	r3, [r0, #30]
  p->end_mask |= (1 << pad_count) - 1;
 80acbc4:	8442      	strh	r2, [r0, #34]	; 0x22

  p->start_indices_count = dim_count;
  p->stop_indices_count = dim_count;
 80acbc6:	7281      	strb	r1, [r0, #10]
  p->strides_count = dim_count;
 80acbc8:	7501      	strb	r1, [r0, #20]
 80acbca:	bd70      	pop	{r4, r5, r6, pc}

080acbcc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>:

// Return the index for the first element along that axis. This index will be a
// positive integer between [0, axis_size - 1] that can be used to index
// directly into the data.
inline int StartForAxis(const tflite::StridedSliceParams& params,
                        const RuntimeShape& input_shape, int axis) {
 80acbcc:	4603      	mov	r3, r0
 80acbce:	b510      	push	{r4, lr}
 80acbd0:	4608      	mov	r0, r1
  const auto begin_mask = params.begin_mask;
  const auto* start_indices = params.start_indices;
  const auto* strides = params.strides;
  // Begin with the specified index.
  int start = start_indices[axis];
 80acbd2:	eb03 0142 	add.w	r1, r3, r2, lsl #1

  // begin_mask override
  if (begin_mask & 1 << axis) {
 80acbd6:	f9b3 301e 	ldrsh.w	r3, [r3, #30]
                        const RuntimeShape& input_shape, int axis) {
  const auto begin_mask = params.begin_mask;
  const auto* start_indices = params.start_indices;
  const auto* strides = params.strides;
  // Begin with the specified index.
  int start = start_indices[axis];
 80acbda:	f9b1 4002 	ldrsh.w	r4, [r1, #2]

  // begin_mask override
  if (begin_mask & 1 << axis) {
 80acbde:	4113      	asrs	r3, r2
 80acbe0:	07db      	lsls	r3, r3, #31
 80acbe2:	d507      	bpl.n	80acbf4 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi+0x28>
    if (strides[axis] > 0) {
 80acbe4:	f9b1 3016 	ldrsh.w	r3, [r1, #22]
 80acbe8:	2b00      	cmp	r3, #0
      // clamped below (Note: We could have set them to 0 and axis_size-1, but
      // use lowest() and max() to maintain symmetry with StopForAxis())
      start = std::numeric_limits<int>::lowest();
    } else {
      // Backward iteration - use the last element.
      start = std::numeric_limits<int>::max();
 80acbea:	bfcc      	ite	gt
 80acbec:	f04f 4400 	movgt.w	r4, #2147483648	; 0x80000000
 80acbf0:	f06f 4400 	mvnle.w	r4, #2147483648	; 0x80000000
    }
  }

  // Handle negative indices
  int axis_size = input_shape.Dims(axis);
 80acbf4:	4611      	mov	r1, r2
 80acbf6:	f7f5 fbe7 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  if (start < 0) {
 80acbfa:	2c00      	cmp	r4, #0
    start += axis_size;
 80acbfc:	bfb8      	it	lt
 80acbfe:	1824      	addlt	r4, r4, r0
namespace tflite {
namespace strided_slice {

// Use until std::clamp() is available from C++17.
inline int Clamp(const int v, const int lo, const int hi) {
  TFLITE_DCHECK(!(hi < lo));
 80acc00:	3801      	subs	r0, #1
 80acc02:	d501      	bpl.n	80acc08 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi+0x3c>
 80acc04:	f003 fa14 	bl	80b0030 <abort>
  if (hi < v) return hi;
 80acc08:	4284      	cmp	r4, r0
 80acc0a:	bfd8      	it	le
 80acc0c:	ea24 70e4 	bicle.w	r0, r4, r4, asr #31

  // Clamping
  start = Clamp(start, 0, axis_size - 1);

  return start;
}
 80acc10:	bd10      	pop	{r4, pc}

080acc12 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>:
// element. ie. So if you were iterating through all elements of a 1D array of
// size 4, this function would return 4 as the stop, because it is one past the
// "real" indices of 0, 1, 2 & 3.
inline int StopForAxis(const tflite::StridedSliceParams& params,
                       const RuntimeShape& input_shape, int axis,
                       int start_for_axis) {
 80acc12:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
 80acc14:	4615      	mov	r5, r2
 80acc16:	460f      	mov	r7, r1
  const auto* stop_indices = params.stop_indices;
  const auto* strides = params.strides;

  // Begin with the specified index
  const bool shrink_axis = shrink_axis_mask & (1 << axis);
  int stop = stop_indices[axis];
 80acc18:	eb00 0145 	add.w	r1, r0, r5, lsl #1
 80acc1c:	f9b1 400c 	ldrsh.w	r4, [r1, #12]

  // When shrinking an axis, the end position does not matter (and can be
  // incorrect when negative indexing is used, see Issue #19260). Always use
  // start_for_axis + 1 to generate a length 1 slice, since start_for_axis has
  // already been adjusted for negative indices.
  if (shrink_axis) {
 80acc20:	f9b0 1026 	ldrsh.w	r1, [r0, #38]	; 0x26
// size 4, this function would return 4 as the stop, because it is one past the
// "real" indices of 0, 1, 2 & 3.
inline int StopForAxis(const tflite::StridedSliceParams& params,
                       const RuntimeShape& input_shape, int axis,
                       int start_for_axis) {
  const auto end_mask = params.end_mask;
 80acc24:	f9b0 2022 	ldrsh.w	r2, [r0, #34]	; 0x22

  // When shrinking an axis, the end position does not matter (and can be
  // incorrect when negative indexing is used, see Issue #19260). Always use
  // start_for_axis + 1 to generate a length 1 slice, since start_for_axis has
  // already been adjusted for negative indices.
  if (shrink_axis) {
 80acc28:	4129      	asrs	r1, r5
 80acc2a:	07c9      	lsls	r1, r1, #31
    stop = start_for_axis + 1;
 80acc2c:	bf48      	it	mi
 80acc2e:	1c5c      	addmi	r4, r3, #1
  }

  // end_mask override
  if (end_mask & (1 << axis)) {
 80acc30:	fa42 f305 	asr.w	r3, r2, r5
 80acc34:	07da      	lsls	r2, r3, #31
                       const RuntimeShape& input_shape, int axis,
                       int start_for_axis) {
  const auto end_mask = params.end_mask;
  const auto shrink_axis_mask = params.shrink_axis_mask;
  const auto* stop_indices = params.stop_indices;
  const auto* strides = params.strides;
 80acc36:	f100 0616 	add.w	r6, r0, #22
  if (shrink_axis) {
    stop = start_for_axis + 1;
  }

  // end_mask override
  if (end_mask & (1 << axis)) {
 80acc3a:	d507      	bpl.n	80acc4c <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii+0x3a>
    if (strides[axis] > 0) {
 80acc3c:	f936 3015 	ldrsh.w	r3, [r6, r5, lsl #1]
      // Forward iteration - use the last element. These values will get
      // clamped below
      stop = std::numeric_limits<int>::max();
    } else {
      // Backward iteration - use the first element.
      stop = std::numeric_limits<int>::lowest();
 80acc40:	2b00      	cmp	r3, #0
 80acc42:	bfcc      	ite	gt
 80acc44:	f06f 4400 	mvngt.w	r4, #2147483648	; 0x80000000
 80acc48:	f04f 4400 	movle.w	r4, #2147483648	; 0x80000000
    }
  }

  // Handle negative indices
  const int axis_size = input_shape.Dims(axis);
 80acc4c:	4629      	mov	r1, r5
 80acc4e:	4638      	mov	r0, r7
 80acc50:	f7f5 fbba 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  }

  // Clamping
  // Because the end index points one past the last element, we need slightly
  // different clamping ranges depending on the direction.
  if (strides[axis] > 0) {
 80acc54:	f936 3015 	ldrsh.w	r3, [r6, r5, lsl #1]
    }
  }

  // Handle negative indices
  const int axis_size = input_shape.Dims(axis);
  if (stop < 0) {
 80acc58:	2c00      	cmp	r4, #0
    stop += axis_size;
 80acc5a:	bfb8      	it	lt
 80acc5c:	1824      	addlt	r4, r4, r0
  }

  // Clamping
  // Because the end index points one past the last element, we need slightly
  // different clamping ranges depending on the direction.
  if (strides[axis] > 0) {
 80acc5e:	2b00      	cmp	r3, #0
 80acc60:	dd08      	ble.n	80acc74 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii+0x62>
namespace tflite {
namespace strided_slice {

// Use until std::clamp() is available from C++17.
inline int Clamp(const int v, const int lo, const int hi) {
  TFLITE_DCHECK(!(hi < lo));
 80acc62:	2800      	cmp	r0, #0
 80acc64:	da01      	bge.n	80acc6a <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii+0x58>
 80acc66:	f003 f9e3 	bl	80b0030 <abort>
  if (hi < v) return hi;
 80acc6a:	4284      	cmp	r4, r0
 80acc6c:	dc09      	bgt.n	80acc82 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii+0x70>
 80acc6e:	ea24 70e4 	bic.w	r0, r4, r4, asr #31
 80acc72:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
  if (strides[axis] > 0) {
    // Forward iteration
    stop = Clamp(stop, 0, axis_size);
  } else {
    // Backward iteration
    stop = Clamp(stop, -1, axis_size - 1);
 80acc74:	3801      	subs	r0, #1
namespace tflite {
namespace strided_slice {

// Use until std::clamp() is available from C++17.
inline int Clamp(const int v, const int lo, const int hi) {
  TFLITE_DCHECK(!(hi < lo));
 80acc76:	1c43      	adds	r3, r0, #1
 80acc78:	dbf5      	blt.n	80acc66 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii+0x54>
  if (hi < v) return hi;
 80acc7a:	4284      	cmp	r4, r0
 80acc7c:	bfd8      	it	le
 80acc7e:	ea44 70e4 	orrle.w	r0, r4, r4, asr #31
    // Backward iteration
    stop = Clamp(stop, -1, axis_size - 1);
  }

  return stop;
}
 80acc82:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}

080acc84 <_ZN6tflite3ops5micro13strided_slice19StridedSliceContextC1EP13TfLiteContextP10TfLiteNode>:
constexpr int kEndTensor = 2;
constexpr int kStridesTensor = 3;
constexpr int kOutputTensor = 0;

struct StridedSliceContext {
  StridedSliceContext(TfLiteContext* context, TfLiteNode* node) {
 80acc84:	b5f0      	push	{r4, r5, r6, r7, lr}
    params = reinterpret_cast<TfLiteStridedSliceParams*>(node->builtin_data);
 80acc86:	6954      	ldr	r4, [r2, #20]
 80acc88:	6004      	str	r4, [r0, #0]
 80acc8a:	6814      	ldr	r4, [r2, #0]
 80acc8c:	688d      	ldr	r5, [r1, #8]
 80acc8e:	6866      	ldr	r6, [r4, #4]
 80acc90:	2438      	movs	r4, #56	; 0x38
 80acc92:	fb04 5506 	mla	r5, r4, r6, r5
    input = GetInput(context, node, kInputTensor);
 80acc96:	6045      	str	r5, [r0, #4]
 80acc98:	6816      	ldr	r6, [r2, #0]
    begin = GetInput(context, node, kBeginTensor);
 80acc9a:	688f      	ldr	r7, [r1, #8]
 80acc9c:	68b6      	ldr	r6, [r6, #8]
 80acc9e:	fb04 7606 	mla	r6, r4, r6, r7
 80acca2:	6086      	str	r6, [r0, #8]
 80acca4:	6816      	ldr	r6, [r2, #0]
    end = GetInput(context, node, kEndTensor);
 80acca6:	688f      	ldr	r7, [r1, #8]
 80acca8:	68f6      	ldr	r6, [r6, #12]
 80accaa:	fb04 7606 	mla	r6, r4, r6, r7
 80accae:	60c6      	str	r6, [r0, #12]
 80accb0:	6816      	ldr	r6, [r2, #0]
    strides = GetInput(context, node, kStridesTensor);
 80accb2:	688f      	ldr	r7, [r1, #8]
 80accb4:	6936      	ldr	r6, [r6, #16]
 80accb6:	fb04 7606 	mla	r6, r4, r6, r7
 80accba:	6106      	str	r6, [r0, #16]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80accbc:	6852      	ldr	r2, [r2, #4]
    output = GetOutput(context, node, kOutputTensor);
 80accbe:	6889      	ldr	r1, [r1, #8]
 80accc0:	6852      	ldr	r2, [r2, #4]
 80accc2:	fb04 1402 	mla	r4, r4, r2, r1
 80accc6:	6144      	str	r4, [r0, #20]
#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/c_api_internal.h"

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
 80accc8:	68aa      	ldr	r2, [r5, #8]
 80accca:	6812      	ldr	r2, [r2, #0]
    dims = NumDimensions(input);
 80acccc:	6182      	str	r2, [r0, #24]
  }
 80accce:	bdf0      	pop	{r4, r5, r6, r7, pc}

080accd0 <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE>:
tflite::StridedSliceParams BuildStridedSliceParams(
    StridedSliceContext* op_context) {
  tflite::StridedSliceParams op_params;
  op_params.start_indices_count = op_context->dims;
  op_params.stop_indices_count = op_context->dims;
  op_params.strides_count = op_context->dims;
 80accd0:	4603      	mov	r3, r0
// This Op only supports 1-4D cases and since we use the reference 4D
// implementation, the 1-3D tensors are mapped to 4D.
const int kMaxDim = 4;

tflite::StridedSliceParams BuildStridedSliceParams(
    StridedSliceContext* op_context) {
 80accd2:	b570      	push	{r4, r5, r6, lr}
  tflite::StridedSliceParams op_params;
  op_params.start_indices_count = op_context->dims;
 80accd4:	698e      	ldr	r6, [r1, #24]
  op_params.stop_indices_count = op_context->dims;
  op_params.strides_count = op_context->dims;

  for (int i = 0; i < op_context->dims; ++i) {
 80accd6:	2400      	movs	r4, #0
const int kMaxDim = 4;

tflite::StridedSliceParams BuildStridedSliceParams(
    StridedSliceContext* op_context) {
  tflite::StridedSliceParams op_params;
  op_params.start_indices_count = op_context->dims;
 80accd8:	b272      	sxtb	r2, r6
 80accda:	7002      	strb	r2, [r0, #0]
  op_params.stop_indices_count = op_context->dims;
 80accdc:	7282      	strb	r2, [r0, #10]
  op_params.strides_count = op_context->dims;
 80accde:	f803 2f14 	strb.w	r2, [r3, #20]!

  for (int i = 0; i < op_context->dims; ++i) {
 80acce2:	42b4      	cmp	r4, r6
 80acce4:	da15      	bge.n	80acd12 <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE+0x42>
    op_params.start_indices[i] = GetTensorData<int32_t>(op_context->begin)[i];
 80acce6:	688a      	ldr	r2, [r1, #8]
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80acce8:	b102      	cbz	r2, 80accec <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE+0x1c>
 80accea:	6852      	ldr	r2, [r2, #4]
 80accec:	f852 2024 	ldr.w	r2, [r2, r4, lsl #2]
 80accf0:	00a5      	lsls	r5, r4, #2
 80accf2:	f823 2c12 	strh.w	r2, [r3, #-18]
    op_params.stop_indices[i] = GetTensorData<int32_t>(op_context->end)[i];
 80accf6:	68ca      	ldr	r2, [r1, #12]
 80accf8:	b102      	cbz	r2, 80accfc <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE+0x2c>
 80accfa:	6852      	ldr	r2, [r2, #4]
 80accfc:	5952      	ldr	r2, [r2, r5]
 80accfe:	f823 2c08 	strh.w	r2, [r3, #-8]
    op_params.strides[i] = GetTensorData<int32_t>(op_context->strides)[i];
 80acd02:	690a      	ldr	r2, [r1, #16]
 80acd04:	b102      	cbz	r2, 80acd08 <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE+0x38>
 80acd06:	6852      	ldr	r2, [r2, #4]
 80acd08:	5952      	ldr	r2, [r2, r5]
  tflite::StridedSliceParams op_params;
  op_params.start_indices_count = op_context->dims;
  op_params.stop_indices_count = op_context->dims;
  op_params.strides_count = op_context->dims;

  for (int i = 0; i < op_context->dims; ++i) {
 80acd0a:	3401      	adds	r4, #1
    op_params.start_indices[i] = GetTensorData<int32_t>(op_context->begin)[i];
    op_params.stop_indices[i] = GetTensorData<int32_t>(op_context->end)[i];
    op_params.strides[i] = GetTensorData<int32_t>(op_context->strides)[i];
 80acd0c:	f823 2f02 	strh.w	r2, [r3, #2]!
  tflite::StridedSliceParams op_params;
  op_params.start_indices_count = op_context->dims;
  op_params.stop_indices_count = op_context->dims;
  op_params.strides_count = op_context->dims;

  for (int i = 0; i < op_context->dims; ++i) {
 80acd10:	e7e7      	b.n	80acce2 <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE+0x12>
    op_params.start_indices[i] = GetTensorData<int32_t>(op_context->begin)[i];
    op_params.stop_indices[i] = GetTensorData<int32_t>(op_context->end)[i];
    op_params.strides[i] = GetTensorData<int32_t>(op_context->strides)[i];
  }

  op_params.begin_mask = op_context->params->begin_mask;
 80acd12:	680b      	ldr	r3, [r1, #0]
 80acd14:	681a      	ldr	r2, [r3, #0]
  op_params.ellipsis_mask = 0;
  op_params.end_mask = op_context->params->end_mask;
 80acd16:	6859      	ldr	r1, [r3, #4]
  op_params.new_axis_mask = 0;
  op_params.shrink_axis_mask = op_context->params->shrink_axis_mask;
 80acd18:	691b      	ldr	r3, [r3, #16]
    op_params.start_indices[i] = GetTensorData<int32_t>(op_context->begin)[i];
    op_params.stop_indices[i] = GetTensorData<int32_t>(op_context->end)[i];
    op_params.strides[i] = GetTensorData<int32_t>(op_context->strides)[i];
  }

  op_params.begin_mask = op_context->params->begin_mask;
 80acd1a:	83c2      	strh	r2, [r0, #30]
  op_params.ellipsis_mask = 0;
 80acd1c:	2200      	movs	r2, #0
 80acd1e:	8402      	strh	r2, [r0, #32]
  op_params.end_mask = op_context->params->end_mask;
 80acd20:	8441      	strh	r1, [r0, #34]	; 0x22
  op_params.new_axis_mask = 0;
 80acd22:	8482      	strh	r2, [r0, #36]	; 0x24
  op_params.shrink_axis_mask = op_context->params->shrink_axis_mask;
 80acd24:	84c3      	strh	r3, [r0, #38]	; 0x26
  return op_params;
}
 80acd26:	bd70      	pop	{r4, r5, r6, pc}

080acd28 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE>:

// Processes the indexing tensors (begin, end and strides) to resize the
// output tensor. This function is callable from both Prepare() and Eval() as
// long as the caller ensures the indexing tensors are present.
TfLiteStatus CheckOutputSize(TfLiteContext* context,
                             StridedSliceContext* op_context) {
 80acd28:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80acd2c:	460f      	mov	r7, r1
  using ::tflite::strided_slice::StartForAxis;
  using ::tflite::strided_slice::StopForAxis;
  TfLiteIntArray* output_shape = op_context->output->dims;
  int shape_size = 0;
 80acd2e:	2600      	movs	r6, #0
  auto op_params = BuildStridedSliceParams(op_context);
  auto input_shape = GetTensorShape(op_context->input);
  for (int idx = 0; idx < op_context->dims; ++idx) {
 80acd30:	4634      	mov	r4, r6

// Processes the indexing tensors (begin, end and strides) to resize the
// output tensor. This function is callable from both Prepare() and Eval() as
// long as the caller ensures the indexing tensors are present.
TfLiteStatus CheckOutputSize(TfLiteContext* context,
                             StridedSliceContext* op_context) {
 80acd32:	b097      	sub	sp, #92	; 0x5c
  using ::tflite::strided_slice::StartForAxis;
  using ::tflite::strided_slice::StopForAxis;
  TfLiteIntArray* output_shape = op_context->output->dims;
 80acd34:	694b      	ldr	r3, [r1, #20]

// Processes the indexing tensors (begin, end and strides) to resize the
// output tensor. This function is callable from both Prepare() and Eval() as
// long as the caller ensures the indexing tensors are present.
TfLiteStatus CheckOutputSize(TfLiteContext* context,
                             StridedSliceContext* op_context) {
 80acd36:	4605      	mov	r5, r0
  using ::tflite::strided_slice::StartForAxis;
  using ::tflite::strided_slice::StopForAxis;
  TfLiteIntArray* output_shape = op_context->output->dims;
  int shape_size = 0;
  auto op_params = BuildStridedSliceParams(op_context);
 80acd38:	a80c      	add	r0, sp, #48	; 0x30
// long as the caller ensures the indexing tensors are present.
TfLiteStatus CheckOutputSize(TfLiteContext* context,
                             StridedSliceContext* op_context) {
  using ::tflite::strided_slice::StartForAxis;
  using ::tflite::strided_slice::StopForAxis;
  TfLiteIntArray* output_shape = op_context->output->dims;
 80acd3a:	f8d3 8008 	ldr.w	r8, [r3, #8]
  int shape_size = 0;
  auto op_params = BuildStridedSliceParams(op_context);
 80acd3e:	f7ff ffc7 	bl	80accd0 <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE>
  auto input_shape = GetTensorShape(op_context->input);
 80acd42:	6879      	ldr	r1, [r7, #4]
 80acd44:	a807      	add	r0, sp, #28
 80acd46:	f7f5 fde4 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
  for (int idx = 0; idx < op_context->dims; ++idx) {
 80acd4a:	f8d7 9018 	ldr.w	r9, [r7, #24]
 80acd4e:	454c      	cmp	r4, r9
 80acd50:	da4b      	bge.n	80acdea <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xc2>
    int32_t stride = GetTensorData<int32_t>(op_context->strides)[idx];
 80acd52:	693b      	ldr	r3, [r7, #16]
 80acd54:	b103      	cbz	r3, 80acd58 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0x30>
 80acd56:	685b      	ldr	r3, [r3, #4]
 80acd58:	f853 b024 	ldr.w	fp, [r3, r4, lsl #2]
    TF_LITE_ENSURE_MSG(context, stride != 0, "stride value has to be non-zero");
 80acd5c:	f1bb 0f00 	cmp.w	fp, #0
 80acd60:	d104      	bne.n	80acd6c <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0x44>
 80acd62:	696b      	ldr	r3, [r5, #20]
 80acd64:	492b      	ldr	r1, [pc, #172]	; (80ace14 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xec>)
 80acd66:	4628      	mov	r0, r5
 80acd68:	4798      	blx	r3
 80acd6a:	e039      	b.n	80acde0 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xb8>
    int32_t begin = StartForAxis(op_params, input_shape, idx);
 80acd6c:	4622      	mov	r2, r4
 80acd6e:	a907      	add	r1, sp, #28
 80acd70:	a80c      	add	r0, sp, #48	; 0x30
 80acd72:	f7ff ff2b 	bl	80acbcc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
    int32_t end = StopForAxis(op_params, input_shape, idx, begin);
 80acd76:	4622      	mov	r2, r4
  auto op_params = BuildStridedSliceParams(op_context);
  auto input_shape = GetTensorShape(op_context->input);
  for (int idx = 0; idx < op_context->dims; ++idx) {
    int32_t stride = GetTensorData<int32_t>(op_context->strides)[idx];
    TF_LITE_ENSURE_MSG(context, stride != 0, "stride value has to be non-zero");
    int32_t begin = StartForAxis(op_params, input_shape, idx);
 80acd78:	4682      	mov	sl, r0
    int32_t end = StopForAxis(op_params, input_shape, idx, begin);
 80acd7a:	a907      	add	r1, sp, #28
 80acd7c:	4603      	mov	r3, r0
 80acd7e:	a80c      	add	r0, sp, #48	; 0x30
 80acd80:	f7ff ff47 	bl	80acc12 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>

    // When shrinking an axis, the end position does not matter (and can be
    // incorrect when negative indexing is used, see Issue #19260). Always use
    // begin + 1 to generate a length 1 slice, since begin has
    // already been adjusted for negative indices by StartForAxis.
    const bool shrink_axis = op_context->params->shrink_axis_mask & (1 << idx);
 80acd84:	683b      	ldr	r3, [r7, #0]
 80acd86:	691b      	ldr	r3, [r3, #16]
 80acd88:	4123      	asrs	r3, r4
    if (shrink_axis) {
 80acd8a:	f013 0301 	ands.w	r3, r3, #1
      end = begin + 1;
 80acd8e:	bf18      	it	ne
 80acd90:	f10a 0001 	addne.w	r0, sl, #1
  using ::ceil;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  ceil(float __x)
  { return __builtin_ceilf(__x); }
 80acd94:	ebca 0000 	rsb	r0, sl, r0
 80acd98:	9305      	str	r3, [sp, #20]
 80acd9a:	f006 fbd3 	bl	80b3544 <__aeabi_i2f>
 80acd9e:	4682      	mov	sl, r0
 80acda0:	4658      	mov	r0, fp
 80acda2:	f006 fbcf 	bl	80b3544 <__aeabi_i2f>
 80acda6:	4601      	mov	r1, r0
 80acda8:	4650      	mov	r0, sl
 80acdaa:	f006 fcd3 	bl	80b3754 <__aeabi_fdiv>
 80acdae:	f004 f87b 	bl	80b0ea8 <ceilf>
    }

    // This is valid for both positive and negative strides
    int32_t dim_shape = std::ceil((end - begin) / static_cast<float>(stride));
    dim_shape = dim_shape < 0 ? 0 : dim_shape;
    if (!shrink_axis) {
 80acdb2:	9b05      	ldr	r3, [sp, #20]
 80acdb4:	b9bb      	cbnz	r3, 80acde6 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xbe>
    if (shrink_axis) {
      end = begin + 1;
    }

    // This is valid for both positive and negative strides
    int32_t dim_shape = std::ceil((end - begin) / static_cast<float>(stride));
 80acdb6:	f006 fdf5 	bl	80b39a4 <__aeabi_f2iz>
    dim_shape = dim_shape < 0 ? 0 : dim_shape;
    if (!shrink_axis) {
      TF_LITE_ENSURE_EQ(context, output_shape->data[shape_size], dim_shape);
 80acdba:	eb08 0386 	add.w	r3, r8, r6, lsl #2
 80acdbe:	685b      	ldr	r3, [r3, #4]
      end = begin + 1;
    }

    // This is valid for both positive and negative strides
    int32_t dim_shape = std::ceil((end - begin) / static_cast<float>(stride));
    dim_shape = dim_shape < 0 ? 0 : dim_shape;
 80acdc0:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
    if (!shrink_axis) {
      TF_LITE_ENSURE_EQ(context, output_shape->data[shape_size], dim_shape);
 80acdc4:	4298      	cmp	r0, r3
 80acdc6:	d00d      	beq.n	80acde4 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xbc>
 80acdc8:	9302      	str	r3, [sp, #8]
 80acdca:	4b13      	ldr	r3, [pc, #76]	; (80ace18 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xf0>)
 80acdcc:	696c      	ldr	r4, [r5, #20]
 80acdce:	9301      	str	r3, [sp, #4]
 80acdd0:	4b12      	ldr	r3, [pc, #72]	; (80ace1c <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xf4>)
 80acdd2:	9003      	str	r0, [sp, #12]
 80acdd4:	9300      	str	r3, [sp, #0]
 80acdd6:	2373      	movs	r3, #115	; 0x73
 80acdd8:	4a11      	ldr	r2, [pc, #68]	; (80ace20 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xf8>)
 80acdda:	4912      	ldr	r1, [pc, #72]	; (80ace24 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xfc>)
 80acddc:	4628      	mov	r0, r5
 80acdde:	47a0      	blx	r4
 80acde0:	2401      	movs	r4, #1
 80acde2:	e010      	b.n	80ace06 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xde>
      shape_size++;
 80acde4:	3601      	adds	r6, #1
  using ::tflite::strided_slice::StopForAxis;
  TfLiteIntArray* output_shape = op_context->output->dims;
  int shape_size = 0;
  auto op_params = BuildStridedSliceParams(op_context);
  auto input_shape = GetTensorShape(op_context->input);
  for (int idx = 0; idx < op_context->dims; ++idx) {
 80acde6:	3401      	adds	r4, #1
 80acde8:	e7b1      	b.n	80acd4e <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0x26>
    if (!shrink_axis) {
      TF_LITE_ENSURE_EQ(context, output_shape->data[shape_size], dim_shape);
      shape_size++;
    }
  }
  TF_LITE_ENSURE_EQ(context, output_shape->size, shape_size);
 80acdea:	f8d8 3000 	ldr.w	r3, [r8]
 80acdee:	42b3      	cmp	r3, r6
 80acdf0:	d008      	beq.n	80ace04 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xdc>
 80acdf2:	9302      	str	r3, [sp, #8]
 80acdf4:	4b0c      	ldr	r3, [pc, #48]	; (80ace28 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0x100>)
 80acdf6:	9603      	str	r6, [sp, #12]
 80acdf8:	9301      	str	r3, [sp, #4]
 80acdfa:	4b0c      	ldr	r3, [pc, #48]	; (80ace2c <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0x104>)
 80acdfc:	696c      	ldr	r4, [r5, #20]
 80acdfe:	9300      	str	r3, [sp, #0]
 80ace00:	2377      	movs	r3, #119	; 0x77
 80ace02:	e7e9      	b.n	80acdd8 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xb0>
  return kTfLiteOk;
 80ace04:	2400      	movs	r4, #0
  using ::tflite::strided_slice::StartForAxis;
  using ::tflite::strided_slice::StopForAxis;
  TfLiteIntArray* output_shape = op_context->output->dims;
  int shape_size = 0;
  auto op_params = BuildStridedSliceParams(op_context);
  auto input_shape = GetTensorShape(op_context->input);
 80ace06:	a807      	add	r0, sp, #28
 80ace08:	f7f5 fad3 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
      shape_size++;
    }
  }
  TF_LITE_ENSURE_EQ(context, output_shape->size, shape_size);
  return kTfLiteOk;
}
 80ace0c:	4620      	mov	r0, r4
 80ace0e:	b017      	add	sp, #92	; 0x5c
 80ace10:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80ace14:	080b6a1a 	.word	0x080b6a1a
 80ace18:	080b6b98 	.word	0x080b6b98
 80ace1c:	080b6ba2 	.word	0x080b6ba2
 80ace20:	080b6ae9 	.word	0x080b6ae9
 80ace24:	080b5be0 	.word	0x080b5be0
 80ace28:	080b6bc1 	.word	0x080b6bc1
 80ace2c:	080b6bcc 	.word	0x080b6bcc

080ace30 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 80ace30:	b570      	push	{r4, r5, r6, lr}
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
 80ace32:	680b      	ldr	r3, [r1, #0]
 80ace34:	b08c      	sub	sp, #48	; 0x30
 80ace36:	681b      	ldr	r3, [r3, #0]
 80ace38:	4605      	mov	r5, r0
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 4);
 80ace3a:	2b04      	cmp	r3, #4
  }
  TF_LITE_ENSURE_EQ(context, output_shape->size, shape_size);
  return kTfLiteOk;
}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 80ace3c:	460a      	mov	r2, r1
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 4);
 80ace3e:	d00d      	beq.n	80ace5c <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x2c>
 80ace40:	9302      	str	r3, [sp, #8]
 80ace42:	4b16      	ldr	r3, [pc, #88]	; (80ace9c <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x6c>)
 80ace44:	2204      	movs	r2, #4
 80ace46:	9301      	str	r3, [sp, #4]
 80ace48:	4b15      	ldr	r3, [pc, #84]	; (80acea0 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x70>)
 80ace4a:	9203      	str	r2, [sp, #12]
 80ace4c:	9300      	str	r3, [sp, #0]
 80ace4e:	6944      	ldr	r4, [r0, #20]
 80ace50:	237c      	movs	r3, #124	; 0x7c
 80ace52:	4a14      	ldr	r2, [pc, #80]	; (80acea4 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x74>)
 80ace54:	4914      	ldr	r1, [pc, #80]	; (80acea8 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x78>)
 80ace56:	47a0      	blx	r4
 80ace58:	2001      	movs	r0, #1
 80ace5a:	e01d      	b.n	80ace98 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x68>
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
 80ace5c:	684b      	ldr	r3, [r1, #4]
 80ace5e:	681c      	ldr	r4, [r3, #0]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
 80ace60:	2c01      	cmp	r4, #1
 80ace62:	d009      	beq.n	80ace78 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x48>
 80ace64:	4b11      	ldr	r3, [pc, #68]	; (80aceac <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x7c>)
 80ace66:	2601      	movs	r6, #1
 80ace68:	9301      	str	r3, [sp, #4]
 80ace6a:	4b11      	ldr	r3, [pc, #68]	; (80aceb0 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x80>)
 80ace6c:	9603      	str	r6, [sp, #12]
 80ace6e:	9300      	str	r3, [sp, #0]
 80ace70:	9402      	str	r4, [sp, #8]
 80ace72:	6944      	ldr	r4, [r0, #20]
 80ace74:	237d      	movs	r3, #125	; 0x7d
 80ace76:	e7ec      	b.n	80ace52 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x22>
  StridedSliceContext op_context(context, node);
 80ace78:	4601      	mov	r1, r0
 80ace7a:	a805      	add	r0, sp, #20
 80ace7c:	f7ff ff02 	bl	80acc84 <_ZN6tflite3ops5micro13strided_slice19StridedSliceContextC1EP13TfLiteContextP10TfLiteNode>
  TF_LITE_ENSURE_MSG(context, op_context.dims <= kMaxDim,
 80ace80:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80ace82:	2b04      	cmp	r3, #4
 80ace84:	dd04      	ble.n	80ace90 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
 80ace86:	696b      	ldr	r3, [r5, #20]
 80ace88:	490a      	ldr	r1, [pc, #40]	; (80aceb4 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x84>)
 80ace8a:	4628      	mov	r0, r5
 80ace8c:	4798      	blx	r3
 80ace8e:	e7e3      	b.n	80ace58 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x28>
                     "input dim should not exceed 4");
  return CheckOutputSize(context, &op_context);
 80ace90:	a905      	add	r1, sp, #20
 80ace92:	4628      	mov	r0, r5
 80ace94:	f7ff ff48 	bl	80acd28 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE>
}
 80ace98:	b00c      	add	sp, #48	; 0x30
 80ace9a:	bd70      	pop	{r4, r5, r6, pc}
 80ace9c:	080b6caa 	.word	0x080b6caa
 80acea0:	080b5bfa 	.word	0x080b5bfa
 80acea4:	080b6ae9 	.word	0x080b6ae9
 80acea8:	080b5be0 	.word	0x080b5be0
 80aceac:	080b75ad 	.word	0x080b75ad
 80aceb0:	080b5c0a 	.word	0x080b5c0a
 80aceb4:	080b6bdf 	.word	0x080b6bdf

080aceb8 <_ZN6tflite3ops5micro22Register_STRIDED_SLICEEv>:
TfLiteRegistration* Register_STRIDED_SLICE() {
  static TfLiteRegistration r = {
      nullptr, nullptr, strided_slice::Prepare,
      strided_slice::Eval<strided_slice::kReference>};
  return &r;
}
 80aceb8:	4800      	ldr	r0, [pc, #0]	; (80acebc <_ZN6tflite3ops5micro22Register_STRIDED_SLICEEv+0x4>)
 80aceba:	4770      	bx	lr
 80acebc:	200004a8 	.word	0x200004a8

080acec0 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_>:

namespace tflite {

namespace reference_ops {
template <typename T>
inline void StridedSlice(const tflite::StridedSliceParams& op_params,
 80acec0:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80acec4:	461e      	mov	r6, r3
 80acec6:	460f      	mov	r7, r1
                         const RuntimeShape& unextended_input_shape,
                         const T* input_data,
                         const RuntimeShape& unextended_output_shape,
                         T* output_data) {
  // Note that the output_shape is not used herein.
  tflite::StridedSliceParams params_copy = op_params;
 80acec8:	4603      	mov	r3, r0

namespace tflite {

namespace reference_ops {
template <typename T>
inline void StridedSlice(const tflite::StridedSliceParams& op_params,
 80aceca:	b0a1      	sub	sp, #132	; 0x84
 80acecc:	920a      	str	r2, [sp, #40]	; 0x28
                         const RuntimeShape& unextended_input_shape,
                         const T* input_data,
                         const RuntimeShape& unextended_output_shape,
                         T* output_data) {
  // Note that the output_shape is not used herein.
  tflite::StridedSliceParams params_copy = op_params;
 80acece:	ac16      	add	r4, sp, #88	; 0x58
 80aced0:	f100 0528 	add.w	r5, r0, #40	; 0x28
 80aced4:	4622      	mov	r2, r4
 80aced6:	6818      	ldr	r0, [r3, #0]
 80aced8:	6859      	ldr	r1, [r3, #4]
 80aceda:	3308      	adds	r3, #8
 80acedc:	c203      	stmia	r2!, {r0, r1}
 80acede:	42ab      	cmp	r3, r5
 80acee0:	4614      	mov	r4, r2
 80acee2:	d1f7      	bne.n	80aced4 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x14>

  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
 80acee4:	683b      	ldr	r3, [r7, #0]
 80acee6:	2b04      	cmp	r3, #4
 80acee8:	dd01      	ble.n	80aceee <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x2e>
 80aceea:	f003 f8a1 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80aceee:	6833      	ldr	r3, [r6, #0]
 80acef0:	2b04      	cmp	r3, #4
 80acef2:	dcfa      	bgt.n	80aceea <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x2a>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
 80acef4:	ad0c      	add	r5, sp, #48	; 0x30
 80acef6:	2301      	movs	r3, #1
 80acef8:	463a      	mov	r2, r7
 80acefa:	2104      	movs	r1, #4
 80acefc:	4628      	mov	r0, r5
 80acefe:	f7f5 fa9c 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80acf02:	2301      	movs	r3, #1
 80acf04:	4632      	mov	r2, r6
 80acf06:	2104      	movs	r1, #4
 80acf08:	a811      	add	r0, sp, #68	; 0x44
 80acf0a:	f7f5 fa96 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  // Reverse and pad to 4 dimensions because that is what the runtime code
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);
 80acf0e:	2104      	movs	r1, #4
 80acf10:	a816      	add	r0, sp, #88	; 0x58
 80acf12:	f7ff fe07 	bl	80acb24 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi>

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
 80acf16:	2200      	movs	r2, #0
 80acf18:	4629      	mov	r1, r5
 80acf1a:	a816      	add	r0, sp, #88	; 0x58
 80acf1c:	f7ff fe56 	bl	80acbcc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
 80acf20:	2200      	movs	r2, #0
 80acf22:	4603      	mov	r3, r0
 80acf24:	4629      	mov	r1, r5

  // Reverse and pad to 4 dimensions because that is what the runtime code
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
 80acf26:	4604      	mov	r4, r0
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
 80acf28:	a816      	add	r0, sp, #88	; 0x58
 80acf2a:	f7ff fe72 	bl	80acc12 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
 80acf2e:	2201      	movs	r2, #1
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
 80acf30:	9003      	str	r0, [sp, #12]
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
 80acf32:	4629      	mov	r1, r5
 80acf34:	a816      	add	r0, sp, #88	; 0x58
 80acf36:	f7ff fe49 	bl	80acbcc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
 80acf3a:	2201      	movs	r2, #1
 80acf3c:	4603      	mov	r3, r0
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
 80acf3e:	9004      	str	r0, [sp, #16]
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
 80acf40:	4629      	mov	r1, r5
 80acf42:	a816      	add	r0, sp, #88	; 0x58
 80acf44:	f7ff fe65 	bl	80acc12 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
 80acf48:	2202      	movs	r2, #2
  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
 80acf4a:	9005      	str	r0, [sp, #20]
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
 80acf4c:	4629      	mov	r1, r5
 80acf4e:	a816      	add	r0, sp, #88	; 0x58
 80acf50:	f7ff fe3c 	bl	80acbcc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
 80acf54:	2202      	movs	r2, #2
 80acf56:	4603      	mov	r3, r0
 80acf58:	4629      	mov	r1, r5
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
 80acf5a:	4680      	mov	r8, r0
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
 80acf5c:	a816      	add	r0, sp, #88	; 0x58
 80acf5e:	f7ff fe58 	bl	80acc12 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
 80acf62:	2203      	movs	r2, #3
 80acf64:	4629      	mov	r1, r5
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
 80acf66:	4681      	mov	r9, r0
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
 80acf68:	a816      	add	r0, sp, #88	; 0x58
 80acf6a:	f7ff fe2f 	bl	80acbcc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
 80acf6e:	2203      	movs	r2, #3
 80acf70:	4603      	mov	r3, r0
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
 80acf72:	4682      	mov	sl, r0
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
 80acf74:	4629      	mov	r1, r5
 80acf76:	a816      	add	r0, sp, #88	; 0x58
 80acf78:	f7ff fe4b 	bl	80acc12 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
 80acf7c:	4683      	mov	fp, r0

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
 80acf7e:	f9bd 306e 	ldrsh.w	r3, [sp, #110]	; 0x6e
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
 80acf82:	950b      	str	r5, [sp, #44]	; 0x2c
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
 80acf84:	9306      	str	r3, [sp, #24]
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
 80acf86:	f9bd 3070 	ldrsh.w	r3, [sp, #112]	; 0x70
 80acf8a:	9307      	str	r3, [sp, #28]
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
 80acf8c:	f9bd 3072 	ldrsh.w	r3, [sp, #114]	; 0x72
 80acf90:	9308      	str	r3, [sp, #32]
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
 80acf92:	f9bd 3074 	ldrsh.w	r3, [sp, #116]	; 0x74
 80acf96:	9309      	str	r3, [sp, #36]	; 0x24

inline bool LoopCondition(int index, int stop, int stride) {
  // True when we have reached the end of an axis and should loop.
  return stride > 0 ? index >= stop : index <= stop;
 80acf98:	9b06      	ldr	r3, [sp, #24]
 80acf9a:	2b00      	cmp	r3, #0
 80acf9c:	9b03      	ldr	r3, [sp, #12]
 80acf9e:	dd04      	ble.n	80acfaa <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xea>
 80acfa0:	429c      	cmp	r4, r3
 80acfa2:	bfb4      	ite	lt
 80acfa4:	2300      	movlt	r3, #0
 80acfa6:	2301      	movge	r3, #1
 80acfa8:	e003      	b.n	80acfb2 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xf2>
 80acfaa:	429c      	cmp	r4, r3
 80acfac:	bfcc      	ite	gt
 80acfae:	2300      	movgt	r3, #0
 80acfb0:	2301      	movle	r3, #1
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
 80acfb2:	2b00      	cmp	r3, #0
 80acfb4:	d145      	bne.n	80ad042 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x182>
 80acfb6:	9d04      	ldr	r5, [sp, #16]
 80acfb8:	9b07      	ldr	r3, [sp, #28]
 80acfba:	2b00      	cmp	r3, #0
 80acfbc:	9b05      	ldr	r3, [sp, #20]
 80acfbe:	dd04      	ble.n	80acfca <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x10a>
 80acfc0:	429d      	cmp	r5, r3
 80acfc2:	bfb4      	ite	lt
 80acfc4:	2300      	movlt	r3, #0
 80acfc6:	2301      	movge	r3, #1
 80acfc8:	e003      	b.n	80acfd2 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x112>
 80acfca:	429d      	cmp	r5, r3
 80acfcc:	bfcc      	ite	gt
 80acfce:	2300      	movgt	r3, #0
 80acfd0:	2301      	movle	r3, #1
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
 80acfd2:	2b00      	cmp	r3, #0
 80acfd4:	d132      	bne.n	80ad03c <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x17c>
 80acfd6:	4646      	mov	r6, r8
 80acfd8:	9b08      	ldr	r3, [sp, #32]
 80acfda:	2b00      	cmp	r3, #0
 80acfdc:	dd04      	ble.n	80acfe8 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x128>
 80acfde:	454e      	cmp	r6, r9
 80acfe0:	bfb4      	ite	lt
 80acfe2:	2300      	movlt	r3, #0
 80acfe4:	2301      	movge	r3, #1
 80acfe6:	e003      	b.n	80acff0 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x130>
 80acfe8:	454e      	cmp	r6, r9
 80acfea:	bfcc      	ite	gt
 80acfec:	2300      	movgt	r3, #0
 80acfee:	2301      	movle	r3, #1
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
 80acff0:	bb0b      	cbnz	r3, 80ad036 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x176>
 80acff2:	4657      	mov	r7, sl
 80acff4:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80acff6:	2b00      	cmp	r3, #0
 80acff8:	dd04      	ble.n	80ad004 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x144>
 80acffa:	455f      	cmp	r7, fp
 80acffc:	bfb4      	ite	lt
 80acffe:	2300      	movlt	r3, #0
 80ad000:	2301      	movge	r3, #1
 80ad002:	e003      	b.n	80ad00c <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x14c>
 80ad004:	455f      	cmp	r7, fp
 80ad006:	bfcc      	ite	gt
 80ad008:	2300      	movgt	r3, #0
 80ad00a:	2301      	movle	r3, #1
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
 80ad00c:	b983      	cbnz	r3, 80ad030 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x170>
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
 80ad00e:	9700      	str	r7, [sp, #0]
 80ad010:	4633      	mov	r3, r6
 80ad012:	462a      	mov	r2, r5
 80ad014:	4621      	mov	r1, r4
 80ad016:	980b      	ldr	r0, [sp, #44]	; 0x2c
 80ad018:	f7f5 fa3b 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80ad01c:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80ad01e:	9a2a      	ldr	r2, [sp, #168]	; 0xa8
 80ad020:	f853 3020 	ldr.w	r3, [r3, r0, lsl #2]
 80ad024:	f842 3b04 	str.w	r3, [r2], #4
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
 80ad028:	9b09      	ldr	r3, [sp, #36]	; 0x24
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
 80ad02a:	922a      	str	r2, [sp, #168]	; 0xa8
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
 80ad02c:	441f      	add	r7, r3
 80ad02e:	e7e1      	b.n	80acff4 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x134>
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
 80ad030:	9b08      	ldr	r3, [sp, #32]
 80ad032:	441e      	add	r6, r3
 80ad034:	e7d0      	b.n	80acfd8 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x118>

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
 80ad036:	9b07      	ldr	r3, [sp, #28]
 80ad038:	441d      	add	r5, r3
 80ad03a:	e7bd      	b.n	80acfb8 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xf8>
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
 80ad03c:	9b06      	ldr	r3, [sp, #24]
 80ad03e:	441c      	add	r4, r3
 80ad040:	e7aa      	b.n	80acf98 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xd8>
  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape input_shape =
      RuntimeShape::ExtendedShape(4, unextended_input_shape);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80ad042:	a811      	add	r0, sp, #68	; 0x44
 80ad044:	f7f5 f9b5 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  tflite::StridedSliceParams params_copy = op_params;

  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape input_shape =
      RuntimeShape::ExtendedShape(4, unextended_input_shape);
 80ad048:	a80c      	add	r0, sp, #48	; 0x30
 80ad04a:	f7f5 f9b2 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
        }
      }
    }
  }
}
 80ad04e:	b021      	add	sp, #132	; 0x84
 80ad050:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080ad054 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_>:

namespace tflite {

namespace reference_ops {
template <typename T>
inline void StridedSlice(const tflite::StridedSliceParams& op_params,
 80ad054:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80ad058:	461e      	mov	r6, r3
 80ad05a:	460f      	mov	r7, r1
                         const RuntimeShape& unextended_input_shape,
                         const T* input_data,
                         const RuntimeShape& unextended_output_shape,
                         T* output_data) {
  // Note that the output_shape is not used herein.
  tflite::StridedSliceParams params_copy = op_params;
 80ad05c:	4603      	mov	r3, r0

namespace tflite {

namespace reference_ops {
template <typename T>
inline void StridedSlice(const tflite::StridedSliceParams& op_params,
 80ad05e:	b0a1      	sub	sp, #132	; 0x84
 80ad060:	920a      	str	r2, [sp, #40]	; 0x28
                         const RuntimeShape& unextended_input_shape,
                         const T* input_data,
                         const RuntimeShape& unextended_output_shape,
                         T* output_data) {
  // Note that the output_shape is not used herein.
  tflite::StridedSliceParams params_copy = op_params;
 80ad062:	ac16      	add	r4, sp, #88	; 0x58
 80ad064:	f100 0528 	add.w	r5, r0, #40	; 0x28
 80ad068:	4622      	mov	r2, r4
 80ad06a:	6818      	ldr	r0, [r3, #0]
 80ad06c:	6859      	ldr	r1, [r3, #4]
 80ad06e:	3308      	adds	r3, #8
 80ad070:	c203      	stmia	r2!, {r0, r1}
 80ad072:	42ab      	cmp	r3, r5
 80ad074:	4614      	mov	r4, r2
 80ad076:	d1f7      	bne.n	80ad068 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x14>

  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
 80ad078:	683b      	ldr	r3, [r7, #0]
 80ad07a:	2b04      	cmp	r3, #4
 80ad07c:	dd01      	ble.n	80ad082 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x2e>
 80ad07e:	f002 ffd7 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80ad082:	6833      	ldr	r3, [r6, #0]
 80ad084:	2b04      	cmp	r3, #4
 80ad086:	dcfa      	bgt.n	80ad07e <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x2a>
 80ad088:	ad0c      	add	r5, sp, #48	; 0x30
 80ad08a:	2301      	movs	r3, #1
 80ad08c:	463a      	mov	r2, r7
 80ad08e:	2104      	movs	r1, #4
 80ad090:	4628      	mov	r0, r5
 80ad092:	f7f5 f9d2 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80ad096:	2301      	movs	r3, #1
 80ad098:	4632      	mov	r2, r6
 80ad09a:	2104      	movs	r1, #4
 80ad09c:	a811      	add	r0, sp, #68	; 0x44
 80ad09e:	f7f5 f9cc 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  // Reverse and pad to 4 dimensions because that is what the runtime code
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);
 80ad0a2:	2104      	movs	r1, #4
 80ad0a4:	a816      	add	r0, sp, #88	; 0x58
 80ad0a6:	f7ff fd3d 	bl	80acb24 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi>

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
 80ad0aa:	2200      	movs	r2, #0
 80ad0ac:	4629      	mov	r1, r5
 80ad0ae:	a816      	add	r0, sp, #88	; 0x58
 80ad0b0:	f7ff fd8c 	bl	80acbcc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
 80ad0b4:	2200      	movs	r2, #0
 80ad0b6:	4603      	mov	r3, r0
 80ad0b8:	4629      	mov	r1, r5

  // Reverse and pad to 4 dimensions because that is what the runtime code
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
 80ad0ba:	4604      	mov	r4, r0
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
 80ad0bc:	a816      	add	r0, sp, #88	; 0x58
 80ad0be:	f7ff fda8 	bl	80acc12 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
 80ad0c2:	2201      	movs	r2, #1
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
 80ad0c4:	9003      	str	r0, [sp, #12]
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
 80ad0c6:	4629      	mov	r1, r5
 80ad0c8:	a816      	add	r0, sp, #88	; 0x58
 80ad0ca:	f7ff fd7f 	bl	80acbcc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
 80ad0ce:	2201      	movs	r2, #1
 80ad0d0:	4603      	mov	r3, r0
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
 80ad0d2:	9004      	str	r0, [sp, #16]
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
 80ad0d4:	4629      	mov	r1, r5
 80ad0d6:	a816      	add	r0, sp, #88	; 0x58
 80ad0d8:	f7ff fd9b 	bl	80acc12 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
 80ad0dc:	2202      	movs	r2, #2
  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
 80ad0de:	9005      	str	r0, [sp, #20]
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
 80ad0e0:	4629      	mov	r1, r5
 80ad0e2:	a816      	add	r0, sp, #88	; 0x58
 80ad0e4:	f7ff fd72 	bl	80acbcc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
 80ad0e8:	2202      	movs	r2, #2
 80ad0ea:	4603      	mov	r3, r0
 80ad0ec:	4629      	mov	r1, r5
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
 80ad0ee:	4680      	mov	r8, r0
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
 80ad0f0:	a816      	add	r0, sp, #88	; 0x58
 80ad0f2:	f7ff fd8e 	bl	80acc12 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
 80ad0f6:	2203      	movs	r2, #3
 80ad0f8:	4629      	mov	r1, r5
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
 80ad0fa:	4681      	mov	r9, r0
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
 80ad0fc:	a816      	add	r0, sp, #88	; 0x58
 80ad0fe:	f7ff fd65 	bl	80acbcc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
 80ad102:	2203      	movs	r2, #3
 80ad104:	4603      	mov	r3, r0
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
 80ad106:	4682      	mov	sl, r0
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
 80ad108:	4629      	mov	r1, r5
 80ad10a:	a816      	add	r0, sp, #88	; 0x58
 80ad10c:	f7ff fd81 	bl	80acc12 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
 80ad110:	4683      	mov	fp, r0

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
 80ad112:	f9bd 306e 	ldrsh.w	r3, [sp, #110]	; 0x6e
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
 80ad116:	950b      	str	r5, [sp, #44]	; 0x2c
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
 80ad118:	9306      	str	r3, [sp, #24]
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
 80ad11a:	f9bd 3070 	ldrsh.w	r3, [sp, #112]	; 0x70
 80ad11e:	9307      	str	r3, [sp, #28]
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
 80ad120:	f9bd 3072 	ldrsh.w	r3, [sp, #114]	; 0x72
 80ad124:	9308      	str	r3, [sp, #32]
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
 80ad126:	f9bd 3074 	ldrsh.w	r3, [sp, #116]	; 0x74
 80ad12a:	9309      	str	r3, [sp, #36]	; 0x24
 80ad12c:	9b06      	ldr	r3, [sp, #24]
 80ad12e:	2b00      	cmp	r3, #0
 80ad130:	9b03      	ldr	r3, [sp, #12]
 80ad132:	dd04      	ble.n	80ad13e <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xea>
 80ad134:	429c      	cmp	r4, r3
 80ad136:	bfb4      	ite	lt
 80ad138:	2300      	movlt	r3, #0
 80ad13a:	2301      	movge	r3, #1
 80ad13c:	e003      	b.n	80ad146 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xf2>
 80ad13e:	429c      	cmp	r4, r3
 80ad140:	bfcc      	ite	gt
 80ad142:	2300      	movgt	r3, #0
 80ad144:	2301      	movle	r3, #1
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
 80ad146:	2b00      	cmp	r3, #0
 80ad148:	d144      	bne.n	80ad1d4 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x180>
 80ad14a:	9d04      	ldr	r5, [sp, #16]
 80ad14c:	9b07      	ldr	r3, [sp, #28]
 80ad14e:	2b00      	cmp	r3, #0
 80ad150:	9b05      	ldr	r3, [sp, #20]
 80ad152:	dd04      	ble.n	80ad15e <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x10a>
 80ad154:	429d      	cmp	r5, r3
 80ad156:	bfb4      	ite	lt
 80ad158:	2300      	movlt	r3, #0
 80ad15a:	2301      	movge	r3, #1
 80ad15c:	e003      	b.n	80ad166 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x112>
 80ad15e:	429d      	cmp	r5, r3
 80ad160:	bfcc      	ite	gt
 80ad162:	2300      	movgt	r3, #0
 80ad164:	2301      	movle	r3, #1
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
 80ad166:	2b00      	cmp	r3, #0
 80ad168:	d131      	bne.n	80ad1ce <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x17a>
 80ad16a:	4646      	mov	r6, r8
 80ad16c:	9b08      	ldr	r3, [sp, #32]
 80ad16e:	2b00      	cmp	r3, #0
 80ad170:	dd04      	ble.n	80ad17c <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x128>
 80ad172:	454e      	cmp	r6, r9
 80ad174:	bfb4      	ite	lt
 80ad176:	2300      	movlt	r3, #0
 80ad178:	2301      	movge	r3, #1
 80ad17a:	e003      	b.n	80ad184 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x130>
 80ad17c:	454e      	cmp	r6, r9
 80ad17e:	bfcc      	ite	gt
 80ad180:	2300      	movgt	r3, #0
 80ad182:	2301      	movle	r3, #1
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
 80ad184:	bb03      	cbnz	r3, 80ad1c8 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x174>
 80ad186:	4657      	mov	r7, sl
 80ad188:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80ad18a:	2b00      	cmp	r3, #0
 80ad18c:	dd04      	ble.n	80ad198 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x144>
 80ad18e:	455f      	cmp	r7, fp
 80ad190:	bfb4      	ite	lt
 80ad192:	2300      	movlt	r3, #0
 80ad194:	2301      	movge	r3, #1
 80ad196:	e003      	b.n	80ad1a0 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x14c>
 80ad198:	455f      	cmp	r7, fp
 80ad19a:	bfcc      	ite	gt
 80ad19c:	2300      	movgt	r3, #0
 80ad19e:	2301      	movle	r3, #1
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
 80ad1a0:	b97b      	cbnz	r3, 80ad1c2 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x16e>
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
 80ad1a2:	9700      	str	r7, [sp, #0]
 80ad1a4:	4633      	mov	r3, r6
 80ad1a6:	462a      	mov	r2, r5
 80ad1a8:	4621      	mov	r1, r4
 80ad1aa:	980b      	ldr	r0, [sp, #44]	; 0x2c
 80ad1ac:	f7f5 f971 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80ad1b0:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80ad1b2:	9a2a      	ldr	r2, [sp, #168]	; 0xa8
 80ad1b4:	5c1b      	ldrb	r3, [r3, r0]
 80ad1b6:	f802 3b01 	strb.w	r3, [r2], #1
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
 80ad1ba:	9b09      	ldr	r3, [sp, #36]	; 0x24
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
 80ad1bc:	922a      	str	r2, [sp, #168]	; 0xa8
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
 80ad1be:	441f      	add	r7, r3
 80ad1c0:	e7e2      	b.n	80ad188 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x134>
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
 80ad1c2:	9b08      	ldr	r3, [sp, #32]
 80ad1c4:	441e      	add	r6, r3
 80ad1c6:	e7d1      	b.n	80ad16c <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x118>

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
 80ad1c8:	9b07      	ldr	r3, [sp, #28]
 80ad1ca:	441d      	add	r5, r3
 80ad1cc:	e7be      	b.n	80ad14c <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xf8>
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
 80ad1ce:	9b06      	ldr	r3, [sp, #24]
 80ad1d0:	441c      	add	r4, r3
 80ad1d2:	e7ab      	b.n	80ad12c <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xd8>
  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape input_shape =
      RuntimeShape::ExtendedShape(4, unextended_input_shape);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80ad1d4:	a811      	add	r0, sp, #68	; 0x44
 80ad1d6:	f7f5 f8ec 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  tflite::StridedSliceParams params_copy = op_params;

  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape input_shape =
      RuntimeShape::ExtendedShape(4, unextended_input_shape);
 80ad1da:	a80c      	add	r0, sp, #48	; 0x30
 80ad1dc:	f7f5 f8e9 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
        }
      }
    }
  }
}
 80ad1e0:	b021      	add	sp, #132	; 0x84
 80ad1e2:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080ad1e6 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_>:

namespace tflite {

namespace reference_ops {
template <typename T>
inline void StridedSlice(const tflite::StridedSliceParams& op_params,
 80ad1e6:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80ad1ea:	461e      	mov	r6, r3
 80ad1ec:	460f      	mov	r7, r1
                         const RuntimeShape& unextended_input_shape,
                         const T* input_data,
                         const RuntimeShape& unextended_output_shape,
                         T* output_data) {
  // Note that the output_shape is not used herein.
  tflite::StridedSliceParams params_copy = op_params;
 80ad1ee:	4603      	mov	r3, r0

namespace tflite {

namespace reference_ops {
template <typename T>
inline void StridedSlice(const tflite::StridedSliceParams& op_params,
 80ad1f0:	b0a1      	sub	sp, #132	; 0x84
 80ad1f2:	920a      	str	r2, [sp, #40]	; 0x28
                         const RuntimeShape& unextended_input_shape,
                         const T* input_data,
                         const RuntimeShape& unextended_output_shape,
                         T* output_data) {
  // Note that the output_shape is not used herein.
  tflite::StridedSliceParams params_copy = op_params;
 80ad1f4:	ac16      	add	r4, sp, #88	; 0x58
 80ad1f6:	f100 0528 	add.w	r5, r0, #40	; 0x28
 80ad1fa:	4622      	mov	r2, r4
 80ad1fc:	6818      	ldr	r0, [r3, #0]
 80ad1fe:	6859      	ldr	r1, [r3, #4]
 80ad200:	3308      	adds	r3, #8
 80ad202:	c203      	stmia	r2!, {r0, r1}
 80ad204:	42ab      	cmp	r3, r5
 80ad206:	4614      	mov	r4, r2
 80ad208:	d1f7      	bne.n	80ad1fa <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x14>

  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
 80ad20a:	683b      	ldr	r3, [r7, #0]
 80ad20c:	2b04      	cmp	r3, #4
 80ad20e:	dd01      	ble.n	80ad214 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x2e>
 80ad210:	f002 ff0e 	bl	80b0030 <abort>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
 80ad214:	6833      	ldr	r3, [r6, #0]
 80ad216:	2b04      	cmp	r3, #4
 80ad218:	dcfa      	bgt.n	80ad210 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x2a>
 80ad21a:	ad0c      	add	r5, sp, #48	; 0x30
 80ad21c:	2301      	movs	r3, #1
 80ad21e:	463a      	mov	r2, r7
 80ad220:	2104      	movs	r1, #4
 80ad222:	4628      	mov	r0, r5
 80ad224:	f7f5 f909 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
 80ad228:	2301      	movs	r3, #1
 80ad22a:	4632      	mov	r2, r6
 80ad22c:	2104      	movs	r1, #4
 80ad22e:	a811      	add	r0, sp, #68	; 0x44
 80ad230:	f7f5 f903 	bl	80a243a <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  // Reverse and pad to 4 dimensions because that is what the runtime code
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);
 80ad234:	2104      	movs	r1, #4
 80ad236:	a816      	add	r0, sp, #88	; 0x58
 80ad238:	f7ff fc74 	bl	80acb24 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi>

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
 80ad23c:	2200      	movs	r2, #0
 80ad23e:	4629      	mov	r1, r5
 80ad240:	a816      	add	r0, sp, #88	; 0x58
 80ad242:	f7ff fcc3 	bl	80acbcc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
 80ad246:	2200      	movs	r2, #0
 80ad248:	4603      	mov	r3, r0
 80ad24a:	4629      	mov	r1, r5

  // Reverse and pad to 4 dimensions because that is what the runtime code
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
 80ad24c:	4604      	mov	r4, r0
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
 80ad24e:	a816      	add	r0, sp, #88	; 0x58
 80ad250:	f7ff fcdf 	bl	80acc12 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
 80ad254:	2201      	movs	r2, #1
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
 80ad256:	9003      	str	r0, [sp, #12]
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
 80ad258:	4629      	mov	r1, r5
 80ad25a:	a816      	add	r0, sp, #88	; 0x58
 80ad25c:	f7ff fcb6 	bl	80acbcc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
 80ad260:	2201      	movs	r2, #1
 80ad262:	4603      	mov	r3, r0
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
 80ad264:	9004      	str	r0, [sp, #16]
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
 80ad266:	4629      	mov	r1, r5
 80ad268:	a816      	add	r0, sp, #88	; 0x58
 80ad26a:	f7ff fcd2 	bl	80acc12 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
 80ad26e:	2202      	movs	r2, #2
  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
 80ad270:	9005      	str	r0, [sp, #20]
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
 80ad272:	4629      	mov	r1, r5
 80ad274:	a816      	add	r0, sp, #88	; 0x58
 80ad276:	f7ff fca9 	bl	80acbcc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
 80ad27a:	2202      	movs	r2, #2
 80ad27c:	4603      	mov	r3, r0
 80ad27e:	4629      	mov	r1, r5
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
 80ad280:	4680      	mov	r8, r0
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
 80ad282:	a816      	add	r0, sp, #88	; 0x58
 80ad284:	f7ff fcc5 	bl	80acc12 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
 80ad288:	2203      	movs	r2, #3
 80ad28a:	4629      	mov	r1, r5
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
 80ad28c:	4681      	mov	r9, r0
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
 80ad28e:	a816      	add	r0, sp, #88	; 0x58
 80ad290:	f7ff fc9c 	bl	80acbcc <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
 80ad294:	2203      	movs	r2, #3
 80ad296:	4603      	mov	r3, r0
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
 80ad298:	4682      	mov	sl, r0
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
 80ad29a:	4629      	mov	r1, r5
 80ad29c:	a816      	add	r0, sp, #88	; 0x58
 80ad29e:	f7ff fcb8 	bl	80acc12 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
 80ad2a2:	4683      	mov	fp, r0

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
 80ad2a4:	f9bd 306e 	ldrsh.w	r3, [sp, #110]	; 0x6e
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
 80ad2a8:	950b      	str	r5, [sp, #44]	; 0x2c
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
 80ad2aa:	9306      	str	r3, [sp, #24]
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
 80ad2ac:	f9bd 3070 	ldrsh.w	r3, [sp, #112]	; 0x70
 80ad2b0:	9307      	str	r3, [sp, #28]
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
 80ad2b2:	f9bd 3072 	ldrsh.w	r3, [sp, #114]	; 0x72
 80ad2b6:	9308      	str	r3, [sp, #32]
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
 80ad2b8:	f9bd 3074 	ldrsh.w	r3, [sp, #116]	; 0x74
 80ad2bc:	9309      	str	r3, [sp, #36]	; 0x24
 80ad2be:	9b06      	ldr	r3, [sp, #24]
 80ad2c0:	2b00      	cmp	r3, #0
 80ad2c2:	9b03      	ldr	r3, [sp, #12]
 80ad2c4:	dd04      	ble.n	80ad2d0 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xea>
 80ad2c6:	429c      	cmp	r4, r3
 80ad2c8:	bfb4      	ite	lt
 80ad2ca:	2300      	movlt	r3, #0
 80ad2cc:	2301      	movge	r3, #1
 80ad2ce:	e003      	b.n	80ad2d8 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xf2>
 80ad2d0:	429c      	cmp	r4, r3
 80ad2d2:	bfcc      	ite	gt
 80ad2d4:	2300      	movgt	r3, #0
 80ad2d6:	2301      	movle	r3, #1
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
 80ad2d8:	2b00      	cmp	r3, #0
 80ad2da:	d144      	bne.n	80ad366 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x180>
 80ad2dc:	9d04      	ldr	r5, [sp, #16]
 80ad2de:	9b07      	ldr	r3, [sp, #28]
 80ad2e0:	2b00      	cmp	r3, #0
 80ad2e2:	9b05      	ldr	r3, [sp, #20]
 80ad2e4:	dd04      	ble.n	80ad2f0 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x10a>
 80ad2e6:	429d      	cmp	r5, r3
 80ad2e8:	bfb4      	ite	lt
 80ad2ea:	2300      	movlt	r3, #0
 80ad2ec:	2301      	movge	r3, #1
 80ad2ee:	e003      	b.n	80ad2f8 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x112>
 80ad2f0:	429d      	cmp	r5, r3
 80ad2f2:	bfcc      	ite	gt
 80ad2f4:	2300      	movgt	r3, #0
 80ad2f6:	2301      	movle	r3, #1
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
 80ad2f8:	2b00      	cmp	r3, #0
 80ad2fa:	d131      	bne.n	80ad360 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x17a>
 80ad2fc:	4646      	mov	r6, r8
 80ad2fe:	9b08      	ldr	r3, [sp, #32]
 80ad300:	2b00      	cmp	r3, #0
 80ad302:	dd04      	ble.n	80ad30e <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x128>
 80ad304:	454e      	cmp	r6, r9
 80ad306:	bfb4      	ite	lt
 80ad308:	2300      	movlt	r3, #0
 80ad30a:	2301      	movge	r3, #1
 80ad30c:	e003      	b.n	80ad316 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x130>
 80ad30e:	454e      	cmp	r6, r9
 80ad310:	bfcc      	ite	gt
 80ad312:	2300      	movgt	r3, #0
 80ad314:	2301      	movle	r3, #1
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
 80ad316:	bb03      	cbnz	r3, 80ad35a <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x174>
 80ad318:	4657      	mov	r7, sl
 80ad31a:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80ad31c:	2b00      	cmp	r3, #0
 80ad31e:	dd04      	ble.n	80ad32a <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x144>
 80ad320:	455f      	cmp	r7, fp
 80ad322:	bfb4      	ite	lt
 80ad324:	2300      	movlt	r3, #0
 80ad326:	2301      	movge	r3, #1
 80ad328:	e003      	b.n	80ad332 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x14c>
 80ad32a:	455f      	cmp	r7, fp
 80ad32c:	bfcc      	ite	gt
 80ad32e:	2300      	movgt	r3, #0
 80ad330:	2301      	movle	r3, #1
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
 80ad332:	b97b      	cbnz	r3, 80ad354 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x16e>
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
 80ad334:	9700      	str	r7, [sp, #0]
 80ad336:	4633      	mov	r3, r6
 80ad338:	462a      	mov	r2, r5
 80ad33a:	4621      	mov	r1, r4
 80ad33c:	980b      	ldr	r0, [sp, #44]	; 0x2c
 80ad33e:	f7f5 f8a8 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80ad342:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80ad344:	9a2a      	ldr	r2, [sp, #168]	; 0xa8
 80ad346:	561b      	ldrsb	r3, [r3, r0]
 80ad348:	f802 3b01 	strb.w	r3, [r2], #1
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
 80ad34c:	9b09      	ldr	r3, [sp, #36]	; 0x24
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
 80ad34e:	922a      	str	r2, [sp, #168]	; 0xa8
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
 80ad350:	441f      	add	r7, r3
 80ad352:	e7e2      	b.n	80ad31a <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x134>
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
 80ad354:	9b08      	ldr	r3, [sp, #32]
 80ad356:	441e      	add	r6, r3
 80ad358:	e7d1      	b.n	80ad2fe <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x118>

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
 80ad35a:	9b07      	ldr	r3, [sp, #28]
 80ad35c:	441d      	add	r5, r3
 80ad35e:	e7be      	b.n	80ad2de <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xf8>
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
 80ad360:	9b06      	ldr	r3, [sp, #24]
 80ad362:	441c      	add	r4, r3
 80ad364:	e7ab      	b.n	80ad2be <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xd8>
  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape input_shape =
      RuntimeShape::ExtendedShape(4, unextended_input_shape);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
 80ad366:	a811      	add	r0, sp, #68	; 0x44
 80ad368:	f7f5 f823 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  tflite::StridedSliceParams params_copy = op_params;

  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape input_shape =
      RuntimeShape::ExtendedShape(4, unextended_input_shape);
 80ad36c:	a80c      	add	r0, sp, #48	; 0x30
 80ad36e:	f7f5 f820 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
        }
      }
    }
  }
}
 80ad372:	b021      	add	sp, #132	; 0x84
 80ad374:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080ad378 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode>:
                     "input dim should not exceed 4");
  return CheckOutputSize(context, &op_context);
}

template <KernelType kernel_type>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80ad378:	b510      	push	{r4, lr}
 80ad37a:	b09e      	sub	sp, #120	; 0x78
  StridedSliceContext op_context(context, node);
 80ad37c:	460a      	mov	r2, r1
                     "input dim should not exceed 4");
  return CheckOutputSize(context, &op_context);
}

template <KernelType kernel_type>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80ad37e:	4604      	mov	r4, r0
  StridedSliceContext op_context(context, node);
 80ad380:	4601      	mov	r1, r0
 80ad382:	a80d      	add	r0, sp, #52	; 0x34
 80ad384:	f7ff fc7e 	bl	80acc84 <_ZN6tflite3ops5micro13strided_slice19StridedSliceContextC1EP13TfLiteContextP10TfLiteNode>
  auto op_params = BuildStridedSliceParams(&op_context);
 80ad388:	a90d      	add	r1, sp, #52	; 0x34
 80ad38a:	a814      	add	r0, sp, #80	; 0x50
 80ad38c:	f7ff fca0 	bl	80accd0 <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE>
  kernel_type::StridedSlice(op_params, GetTensorShape(op_context.input), \
                            GetTensorData<data_type>(op_context.input),  \
                            GetTensorShape(op_context.output),           \
                            GetTensorData<data_type>(op_context.output))

  switch (op_context.input->type) {
 80ad390:	990e      	ldr	r1, [sp, #56]	; 0x38
 80ad392:	780a      	ldrb	r2, [r1, #0]
 80ad394:	2a03      	cmp	r2, #3
 80ad396:	d01a      	beq.n	80ad3ce <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x56>
 80ad398:	2a09      	cmp	r2, #9
 80ad39a:	d036      	beq.n	80ad40a <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x92>
 80ad39c:	2a01      	cmp	r2, #1
 80ad39e:	d14b      	bne.n	80ad438 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xc0>
    case kTfLiteFloat32:
      if (kernel_type == kReference) {
        TF_LITE_STRIDED_SLICE(reference_ops, float);
 80ad3a0:	a803      	add	r0, sp, #12
 80ad3a2:	f7f5 fab6 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80ad3a6:	9a0e      	ldr	r2, [sp, #56]	; 0x38
 80ad3a8:	b10a      	cbz	r2, 80ad3ae <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x36>
 80ad3aa:	6854      	ldr	r4, [r2, #4]
 80ad3ac:	e000      	b.n	80ad3b0 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x38>
 80ad3ae:	4614      	mov	r4, r2
 80ad3b0:	9912      	ldr	r1, [sp, #72]	; 0x48
 80ad3b2:	a808      	add	r0, sp, #32
 80ad3b4:	f7f5 faad 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80ad3b8:	9b12      	ldr	r3, [sp, #72]	; 0x48

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80ad3ba:	b103      	cbz	r3, 80ad3be <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x46>
 80ad3bc:	685b      	ldr	r3, [r3, #4]
 80ad3be:	9300      	str	r3, [sp, #0]
 80ad3c0:	4622      	mov	r2, r4
 80ad3c2:	ab08      	add	r3, sp, #32
 80ad3c4:	a903      	add	r1, sp, #12
 80ad3c6:	a814      	add	r0, sp, #80	; 0x50
 80ad3c8:	f7ff fd7a 	bl	80acec0 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_>
 80ad3cc:	e015      	b.n	80ad3fa <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x82>
      }
      break;
    case kTfLiteUInt8:
      if (kernel_type == kReference) {
        TF_LITE_STRIDED_SLICE(reference_ops, uint8_t);
 80ad3ce:	a803      	add	r0, sp, #12
 80ad3d0:	f7f5 fa9f 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80ad3d4:	9a0e      	ldr	r2, [sp, #56]	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80ad3d6:	b10a      	cbz	r2, 80ad3dc <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x64>
 80ad3d8:	6854      	ldr	r4, [r2, #4]
 80ad3da:	e000      	b.n	80ad3de <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x66>
 80ad3dc:	4614      	mov	r4, r2
 80ad3de:	9912      	ldr	r1, [sp, #72]	; 0x48
 80ad3e0:	a808      	add	r0, sp, #32
 80ad3e2:	f7f5 fa96 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80ad3e6:	9b12      	ldr	r3, [sp, #72]	; 0x48

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80ad3e8:	b103      	cbz	r3, 80ad3ec <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x74>
 80ad3ea:	685b      	ldr	r3, [r3, #4]
 80ad3ec:	9300      	str	r3, [sp, #0]
 80ad3ee:	4622      	mov	r2, r4
 80ad3f0:	ab08      	add	r3, sp, #32
 80ad3f2:	a903      	add	r1, sp, #12
 80ad3f4:	a814      	add	r0, sp, #80	; 0x50
 80ad3f6:	f7ff fe2d 	bl	80ad054 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_>
 80ad3fa:	a808      	add	r0, sp, #32
 80ad3fc:	f7f4 ffd9 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80ad400:	a803      	add	r0, sp, #12
 80ad402:	f7f4 ffd6 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
                           "by StridedSlice.",
                           op_context.input->type);
      return kTfLiteError;
  }
#undef TF_LITE_STRIDED_SLICE
  return kTfLiteOk;
 80ad406:	2000      	movs	r0, #0
      break;
    case kTfLiteUInt8:
      if (kernel_type == kReference) {
        TF_LITE_STRIDED_SLICE(reference_ops, uint8_t);
      }
      break;
 80ad408:	e01b      	b.n	80ad442 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xca>
    case kTfLiteInt8:
      if (kernel_type == kReference) {
        TF_LITE_STRIDED_SLICE(reference_ops, int8_t);
 80ad40a:	a803      	add	r0, sp, #12
 80ad40c:	f7f5 fa81 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80ad410:	9a0e      	ldr	r2, [sp, #56]	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80ad412:	b10a      	cbz	r2, 80ad418 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa0>
 80ad414:	6854      	ldr	r4, [r2, #4]
 80ad416:	e000      	b.n	80ad41a <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa2>
 80ad418:	4614      	mov	r4, r2
 80ad41a:	9912      	ldr	r1, [sp, #72]	; 0x48
 80ad41c:	a808      	add	r0, sp, #32
 80ad41e:	f7f5 fa78 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80ad422:	9b12      	ldr	r3, [sp, #72]	; 0x48

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80ad424:	b103      	cbz	r3, 80ad428 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xb0>
 80ad426:	685b      	ldr	r3, [r3, #4]
 80ad428:	9300      	str	r3, [sp, #0]
 80ad42a:	4622      	mov	r2, r4
 80ad42c:	ab08      	add	r3, sp, #32
 80ad42e:	a903      	add	r1, sp, #12
 80ad430:	a814      	add	r0, sp, #80	; 0x50
 80ad432:	f7ff fed8 	bl	80ad1e6 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_>
 80ad436:	e7e0      	b.n	80ad3fa <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x82>
      }
      break;
    default:
      context->ReportError(context,
 80ad438:	4620      	mov	r0, r4
 80ad43a:	6963      	ldr	r3, [r4, #20]
 80ad43c:	4902      	ldr	r1, [pc, #8]	; (80ad448 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xd0>)
 80ad43e:	4798      	blx	r3
                           "Type %d is currently not supported "
                           "by StridedSlice.",
                           op_context.input->type);
      return kTfLiteError;
 80ad440:	2001      	movs	r0, #1
  }
#undef TF_LITE_STRIDED_SLICE
  return kTfLiteOk;
}
 80ad442:	b01e      	add	sp, #120	; 0x78
 80ad444:	bd10      	pop	{r4, pc}
 80ad446:	bf00      	nop
 80ad448:	080b6cac 	.word	0x080b6cac

080ad44c <_ZN6tflite3ops5micro4svdf4InitEP13TfLiteContextPKcj>:
// Output tensor.
constexpr int kOutputTensor = 0;

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
 80ad44c:	2000      	movs	r0, #0
 80ad44e:	4770      	bx	lr

080ad450 <_ZN6tflite3ops5micro4svdf4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
 80ad450:	4770      	bx	lr

080ad452 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_>:

static inline void ApplyTimeWeightsBiasAndActivation(
    int batch_size, int memory_size, int num_filters, int num_units, int rank,
    const TfLiteTensor* weights_time, const TfLiteTensor* bias,
    TfLiteFusedActivation activation, TfLiteTensor* activation_state,
    TfLiteTensor* scratch, TfLiteTensor* output) {
 80ad452:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80ad456:	b08b      	sub	sp, #44	; 0x2c
 80ad458:	9301      	str	r3, [sp, #4]
 80ad45a:	f89d 305c 	ldrb.w	r3, [sp, #92]	; 0x5c
 80ad45e:	f04f 0800 	mov.w	r8, #0
 80ad462:	9307      	str	r3, [sp, #28]
 80ad464:	460b      	mov	r3, r1
 80ad466:	4353      	muls	r3, r2
 80ad468:	009b      	lsls	r3, r3, #2
 80ad46a:	9305      	str	r3, [sp, #20]
 80ad46c:	0093      	lsls	r3, r2, #2
 80ad46e:	9306      	str	r3, [sp, #24]
 80ad470:	ea21 73e1 	bic.w	r3, r1, r1, asr #31
 80ad474:	009b      	lsls	r3, r3, #2
 80ad476:	9304      	str	r3, [sp, #16]
 80ad478:	4605      	mov	r5, r0
 80ad47a:	46c3      	mov	fp, r8
  // Compute matmul(activation_state, weights_time).
  // The rightmost column is used to save temporary output (with the size of
  // num_filters). This is achieved by starting at
  // GetTensorData<float>(activation_state), and having the stride equal to
  // memory_size.
  for (int b = 0; b < batch_size; ++b) {
 80ad47c:	4643      	mov	r3, r8

static inline void ApplyTimeWeightsBiasAndActivation(
    int batch_size, int memory_size, int num_filters, int num_units, int rank,
    const TfLiteTensor* weights_time, const TfLiteTensor* bias,
    TfLiteFusedActivation activation, TfLiteTensor* activation_state,
    TfLiteTensor* scratch, TfLiteTensor* output) {
 80ad47e:	9c1a      	ldr	r4, [sp, #104]	; 0x68
 80ad480:	9102      	str	r1, [sp, #8]
 80ad482:	9203      	str	r2, [sp, #12]
  // Compute matmul(activation_state, weights_time).
  // The rightmost column is used to save temporary output (with the size of
  // num_filters). This is achieved by starting at
  // GetTensorData<float>(activation_state), and having the stride equal to
  // memory_size.
  for (int b = 0; b < batch_size; ++b) {
 80ad484:	42ab      	cmp	r3, r5
 80ad486:	da3d      	bge.n	80ad504 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xb2>
 80ad488:	9a19      	ldr	r2, [sp, #100]	; 0x64
 80ad48a:	b10a      	cbz	r2, 80ad490 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x3e>
 80ad48c:	6852      	ldr	r2, [r2, #4]
 80ad48e:	e000      	b.n	80ad492 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x40>
 80ad490:	9a19      	ldr	r2, [sp, #100]	; 0x64
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80ad492:	9915      	ldr	r1, [sp, #84]	; 0x54
 80ad494:	b111      	cbz	r1, 80ad49c <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x4a>
 80ad496:	f8d1 a004 	ldr.w	sl, [r1, #4]
 80ad49a:	e001      	b.n	80ad4a0 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x4e>
 80ad49c:	f8dd a054 	ldr.w	sl, [sp, #84]	; 0x54

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80ad4a0:	9918      	ldr	r1, [sp, #96]	; 0x60
 80ad4a2:	b109      	cbz	r1, 80ad4a8 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x56>
 80ad4a4:	684e      	ldr	r6, [r1, #4]
 80ad4a6:	e000      	b.n	80ad4aa <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x58>
 80ad4a8:	9e18      	ldr	r6, [sp, #96]	; 0x60
 80ad4aa:	f1a8 0704 	sub.w	r7, r8, #4
 80ad4ae:	4417      	add	r7, r2
    // Perform batched vector dot product:
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;
    const float* vector1_ptr = GetTensorData<float>(weights_time);
    const float* vector2_ptr =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
 80ad4b0:	445e      	add	r6, fp
    for (int i = 0; i < num_filters; ++i) {
 80ad4b2:	2200      	movs	r2, #0
 80ad4b4:	9903      	ldr	r1, [sp, #12]
 80ad4b6:	428a      	cmp	r2, r1
 80ad4b8:	da1e      	bge.n	80ad4f8 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xa6>
      *scratch_ptr_batch = 0.f;
 80ad4ba:	2100      	movs	r1, #0
 80ad4bc:	f847 1f04 	str.w	r1, [r7, #4]!
      for (int j = 0; j < memory_size; ++j) {
 80ad4c0:	f04f 0900 	mov.w	r9, #0
 80ad4c4:	9902      	ldr	r1, [sp, #8]
 80ad4c6:	4589      	cmp	r9, r1
 80ad4c8:	da11      	bge.n	80ad4ee <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x9c>
        *scratch_ptr_batch += *vector1_ptr++ * *vector2_ptr++;
 80ad4ca:	f856 1029 	ldr.w	r1, [r6, r9, lsl #2]
 80ad4ce:	f85a 0029 	ldr.w	r0, [sl, r9, lsl #2]
 80ad4d2:	9209      	str	r2, [sp, #36]	; 0x24
 80ad4d4:	9308      	str	r3, [sp, #32]
 80ad4d6:	f006 f889 	bl	80b35ec <__aeabi_fmul>
 80ad4da:	4601      	mov	r1, r0
 80ad4dc:	6838      	ldr	r0, [r7, #0]
 80ad4de:	f005 ff7d 	bl	80b33dc <__addsf3>
    const float* vector1_ptr = GetTensorData<float>(weights_time);
    const float* vector2_ptr =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int i = 0; i < num_filters; ++i) {
      *scratch_ptr_batch = 0.f;
      for (int j = 0; j < memory_size; ++j) {
 80ad4e2:	f109 0901 	add.w	r9, r9, #1
        *scratch_ptr_batch += *vector1_ptr++ * *vector2_ptr++;
 80ad4e6:	6038      	str	r0, [r7, #0]
    const float* vector1_ptr = GetTensorData<float>(weights_time);
    const float* vector2_ptr =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int i = 0; i < num_filters; ++i) {
      *scratch_ptr_batch = 0.f;
      for (int j = 0; j < memory_size; ++j) {
 80ad4e8:	9b08      	ldr	r3, [sp, #32]
 80ad4ea:	9a09      	ldr	r2, [sp, #36]	; 0x24
 80ad4ec:	e7ea      	b.n	80ad4c4 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x72>
 80ad4ee:	9904      	ldr	r1, [sp, #16]
    // Perform batched vector dot product:
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;
    const float* vector1_ptr = GetTensorData<float>(weights_time);
    const float* vector2_ptr =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int i = 0; i < num_filters; ++i) {
 80ad4f0:	3201      	adds	r2, #1
 80ad4f2:	448a      	add	sl, r1
 80ad4f4:	440e      	add	r6, r1
 80ad4f6:	e7dd      	b.n	80ad4b4 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x62>
 80ad4f8:	9a05      	ldr	r2, [sp, #20]
  // Compute matmul(activation_state, weights_time).
  // The rightmost column is used to save temporary output (with the size of
  // num_filters). This is achieved by starting at
  // GetTensorData<float>(activation_state), and having the stride equal to
  // memory_size.
  for (int b = 0; b < batch_size; ++b) {
 80ad4fa:	3301      	adds	r3, #1
 80ad4fc:	4493      	add	fp, r2
 80ad4fe:	9a06      	ldr	r2, [sp, #24]
 80ad500:	4490      	add	r8, r2
 80ad502:	e7bf      	b.n	80ad484 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x32>
      scratch_ptr_batch++;
    }
  }

  // Initialize output with bias if provided.
  if (bias) {
 80ad504:	9b16      	ldr	r3, [sp, #88]	; 0x58
 80ad506:	b313      	cbz	r3, 80ad54e <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xfc>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80ad508:	685e      	ldr	r6, [r3, #4]

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80ad50a:	b10c      	cbz	r4, 80ad510 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xbe>
 80ad50c:	6863      	ldr	r3, [r4, #4]
 80ad50e:	e000      	b.n	80ad512 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xc0>
 80ad510:	4623      	mov	r3, r4
 80ad512:	9a01      	ldr	r2, [sp, #4]
    // TODO(kreeger): doc me - VectorBatchVectorAssign
    const float* bias_data = GetTensorData<float>(bias);
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size; ++i) {
 80ad514:	2100      	movs	r1, #0
 80ad516:	0090      	lsls	r0, r2, #2
 80ad518:	42a9      	cmp	r1, r5
 80ad51a:	db0b      	blt.n	80ad534 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xe2>
 80ad51c:	9b01      	ldr	r3, [sp, #4]
        *output_ptr++ = *bias_ptr++;
      }
    }
  } else {
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size * num_units; ++i) {
 80ad51e:	2700      	movs	r7, #0
 80ad520:	ea4f 0b83 	mov.w	fp, r3, lsl #2
 80ad524:	9b14      	ldr	r3, [sp, #80]	; 0x50
 80ad526:	463a      	mov	r2, r7
 80ad528:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
 80ad52c:	009b      	lsls	r3, r3, #2
 80ad52e:	9304      	str	r3, [sp, #16]
 80ad530:	463b      	mov	r3, r7
 80ad532:	e01f      	b.n	80ad574 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x122>
    const float* bias_data = GetTensorData<float>(bias);
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size; ++i) {
      float* output_ptr = output_data + i * num_units;
      const float* bias_ptr = bias_data;
      for (int j = 0; j < num_units; ++j) {
 80ad534:	2200      	movs	r2, #0
 80ad536:	9f01      	ldr	r7, [sp, #4]
 80ad538:	42ba      	cmp	r2, r7
 80ad53a:	da05      	bge.n	80ad548 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xf6>
        *output_ptr++ = *bias_ptr++;
 80ad53c:	f856 7022 	ldr.w	r7, [r6, r2, lsl #2]
 80ad540:	f843 7022 	str.w	r7, [r3, r2, lsl #2]
    const float* bias_data = GetTensorData<float>(bias);
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size; ++i) {
      float* output_ptr = output_data + i * num_units;
      const float* bias_ptr = bias_data;
      for (int j = 0; j < num_units; ++j) {
 80ad544:	3201      	adds	r2, #1
 80ad546:	e7f6      	b.n	80ad536 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xe4>
  // Initialize output with bias if provided.
  if (bias) {
    // TODO(kreeger): doc me - VectorBatchVectorAssign
    const float* bias_data = GetTensorData<float>(bias);
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size; ++i) {
 80ad548:	3101      	adds	r1, #1
 80ad54a:	4403      	add	r3, r0
 80ad54c:	e7e4      	b.n	80ad518 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xc6>
 80ad54e:	b10c      	cbz	r4, 80ad554 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x102>
 80ad550:	6862      	ldr	r2, [r4, #4]
 80ad552:	e000      	b.n	80ad556 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x104>
 80ad554:	4622      	mov	r2, r4
        *output_ptr++ = *bias_ptr++;
      }
    }
  } else {
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size * num_units; ++i) {
 80ad556:	9b01      	ldr	r3, [sp, #4]
      *output_data++ = 0.0f;
 80ad558:	2100      	movs	r1, #0
        *output_ptr++ = *bias_ptr++;
      }
    }
  } else {
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size * num_units; ++i) {
 80ad55a:	fb03 f005 	mul.w	r0, r3, r5
 80ad55e:	2300      	movs	r3, #0
 80ad560:	4283      	cmp	r3, r0
 80ad562:	dadb      	bge.n	80ad51c <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xca>
      *output_data++ = 0.0f;
 80ad564:	f842 1023 	str.w	r1, [r2, r3, lsl #2]
        *output_ptr++ = *bias_ptr++;
      }
    }
  } else {
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size * num_units; ++i) {
 80ad568:	3301      	adds	r3, #1
 80ad56a:	e7f9      	b.n	80ad560 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x10e>
 80ad56c:	9906      	ldr	r1, [sp, #24]
      *output_data++ = 0.0f;
    }
  }

  // Reduction sum.
  for (int b = 0; b < batch_size; ++b) {
 80ad56e:	3201      	adds	r2, #1
 80ad570:	440b      	add	r3, r1
 80ad572:	445f      	add	r7, fp
 80ad574:	42aa      	cmp	r2, r5
 80ad576:	da29      	bge.n	80ad5cc <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x17a>
 80ad578:	b10c      	cbz	r4, 80ad57e <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x12c>
 80ad57a:	6861      	ldr	r1, [r4, #4]
 80ad57c:	e000      	b.n	80ad580 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x12e>
 80ad57e:	4621      	mov	r1, r4
 80ad580:	9819      	ldr	r0, [sp, #100]	; 0x64
 80ad582:	b108      	cbz	r0, 80ad588 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x136>
 80ad584:	6846      	ldr	r6, [r0, #4]
 80ad586:	e000      	b.n	80ad58a <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x138>
 80ad588:	9e19      	ldr	r6, [sp, #100]	; 0x64
    float* output_ptr_batch = GetTensorData<float>(output) + b * num_units;
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;
 80ad58a:	441e      	add	r6, r3

    // Reduction sum vector
    const float* input_vector_ptr = scratch_ptr_batch;
    for (int i = 0; i < num_units; ++i) {
 80ad58c:	f04f 0800 	mov.w	r8, #0
 80ad590:	eb01 0907 	add.w	r9, r1, r7
 80ad594:	9901      	ldr	r1, [sp, #4]
 80ad596:	4588      	cmp	r8, r1
 80ad598:	dae8      	bge.n	80ad56c <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x11a>
 80ad59a:	f04f 0a00 	mov.w	sl, #0
      for (int j = 0; j < rank; j++) {
 80ad59e:	9914      	ldr	r1, [sp, #80]	; 0x50
 80ad5a0:	458a      	cmp	sl, r1
 80ad5a2:	da0e      	bge.n	80ad5c2 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x170>
        output_ptr_batch[i] += *input_vector_ptr++;
 80ad5a4:	f856 102a 	ldr.w	r1, [r6, sl, lsl #2]
 80ad5a8:	f859 0028 	ldr.w	r0, [r9, r8, lsl #2]
 80ad5ac:	9309      	str	r3, [sp, #36]	; 0x24
 80ad5ae:	9208      	str	r2, [sp, #32]
 80ad5b0:	f005 ff14 	bl	80b33dc <__addsf3>
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;

    // Reduction sum vector
    const float* input_vector_ptr = scratch_ptr_batch;
    for (int i = 0; i < num_units; ++i) {
      for (int j = 0; j < rank; j++) {
 80ad5b4:	f10a 0a01 	add.w	sl, sl, #1
        output_ptr_batch[i] += *input_vector_ptr++;
 80ad5b8:	f849 0028 	str.w	r0, [r9, r8, lsl #2]
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;

    // Reduction sum vector
    const float* input_vector_ptr = scratch_ptr_batch;
    for (int i = 0; i < num_units; ++i) {
      for (int j = 0; j < rank; j++) {
 80ad5bc:	9a08      	ldr	r2, [sp, #32]
 80ad5be:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80ad5c0:	e7ed      	b.n	80ad59e <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x14c>
 80ad5c2:	9904      	ldr	r1, [sp, #16]
    float* output_ptr_batch = GetTensorData<float>(output) + b * num_units;
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;

    // Reduction sum vector
    const float* input_vector_ptr = scratch_ptr_batch;
    for (int i = 0; i < num_units; ++i) {
 80ad5c4:	f108 0801 	add.w	r8, r8, #1
 80ad5c8:	440e      	add	r6, r1
 80ad5ca:	e7e3      	b.n	80ad594 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x142>
 80ad5cc:	2700      	movs	r7, #0
 80ad5ce:	46b8      	mov	r8, r7
      }
    }
  }

  // Apply activation.
  for (int b = 0; b < batch_size; ++b) {
 80ad5d0:	45a8      	cmp	r8, r5
 80ad5d2:	da23      	bge.n	80ad61c <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1ca>
 80ad5d4:	b10c      	cbz	r4, 80ad5da <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x188>
 80ad5d6:	6866      	ldr	r6, [r4, #4]
 80ad5d8:	e000      	b.n	80ad5dc <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x18a>
 80ad5da:	4626      	mov	r6, r4
    float* output_ptr_batch = GetTensorData<float>(output) + b * num_units;
 80ad5dc:	443e      	add	r6, r7
    for (int i = 0; i < num_units; ++i) {
 80ad5de:	f04f 0900 	mov.w	r9, #0
 80ad5e2:	9b01      	ldr	r3, [sp, #4]
 80ad5e4:	4599      	cmp	r9, r3
 80ad5e6:	da15      	bge.n	80ad614 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1c2>
namespace ops {
namespace micro {

// Returns the floating point value for a fused activation:
inline float ActivationValFloat(TfLiteFusedActivation act, float a) {
  switch (act) {
 80ad5e8:	9b07      	ldr	r3, [sp, #28]
      *output_ptr_batch = ActivationValFloat(activation, *output_ptr_batch);
 80ad5ea:	f8d6 a000 	ldr.w	sl, [r6]
 80ad5ee:	b163      	cbz	r3, 80ad60a <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1b8>
 80ad5f0:	2b01      	cmp	r3, #1
 80ad5f2:	d107      	bne.n	80ad604 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1b2>
    case kTfLiteActNone:
      return a;
    case kTfLiteActRelu:
      return a < 0.f ? 0.f : a;
 80ad5f4:	2100      	movs	r1, #0
 80ad5f6:	4650      	mov	r0, sl
 80ad5f8:	f006 f996 	bl	80b3928 <__aeabi_fcmplt>
 80ad5fc:	b128      	cbz	r0, 80ad60a <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1b8>
 80ad5fe:	f04f 0a00 	mov.w	sl, #0
 80ad602:	e002      	b.n	80ad60a <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1b8>
    default:
      // TODO(kreeger): Implement more activations.
      exit(1);
 80ad604:	2001      	movs	r0, #1
 80ad606:	f006 fa3d 	bl	80b3a84 <exit>
 80ad60a:	f846 ab04 	str.w	sl, [r6], #4
  }

  // Apply activation.
  for (int b = 0; b < batch_size; ++b) {
    float* output_ptr_batch = GetTensorData<float>(output) + b * num_units;
    for (int i = 0; i < num_units; ++i) {
 80ad60e:	f109 0901 	add.w	r9, r9, #1
 80ad612:	e7e6      	b.n	80ad5e2 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x190>
      }
    }
  }

  // Apply activation.
  for (int b = 0; b < batch_size; ++b) {
 80ad614:	f108 0801 	add.w	r8, r8, #1
 80ad618:	445f      	add	r7, fp
 80ad61a:	e7d9      	b.n	80ad5d0 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x17e>
      float* batch_start = state_ptr_batch + 1;
      float* batch_end = state_ptr_batch + memory_size;
      while (batch_start != batch_end) {
        *batch_ptr++ = *batch_start++;
      }
      state_ptr_batch[memory_size - 1] = 0.0f;
 80ad61c:	2100      	movs	r1, #0
 80ad61e:	460c      	mov	r4, r1
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int f = 0; f < num_filters; ++f) {
      // Shift the vector left:
      float* batch_ptr = state_ptr_batch;
      float* batch_start = state_ptr_batch + 1;
      float* batch_end = state_ptr_batch + memory_size;
 80ad620:	9b02      	ldr	r3, [sp, #8]
      while (batch_start != batch_end) {
        *batch_ptr++ = *batch_start++;
      }
      state_ptr_batch[memory_size - 1] = 0.0f;
 80ad622:	2700      	movs	r7, #0
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int f = 0; f < num_filters; ++f) {
      // Shift the vector left:
      float* batch_ptr = state_ptr_batch;
      float* batch_start = state_ptr_batch + 1;
      float* batch_end = state_ptr_batch + memory_size;
 80ad624:	009a      	lsls	r2, r3, #2
    }
  }

  // Left shift the activation_state to make room for next cycle's activation.
  // TODO(alanchiao): explore collapsing this into a single loop.
  for (int b = 0; b < batch_size; ++b) {
 80ad626:	42ac      	cmp	r4, r5
 80ad628:	da1a      	bge.n	80ad660 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x20e>
 80ad62a:	9b18      	ldr	r3, [sp, #96]	; 0x60
 80ad62c:	b10b      	cbz	r3, 80ad632 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1e0>
 80ad62e:	685b      	ldr	r3, [r3, #4]
 80ad630:	e000      	b.n	80ad634 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1e2>
 80ad632:	9b18      	ldr	r3, [sp, #96]	; 0x60
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
 80ad634:	440b      	add	r3, r1
    for (int f = 0; f < num_filters; ++f) {
 80ad636:	2600      	movs	r6, #0
 80ad638:	9803      	ldr	r0, [sp, #12]
 80ad63a:	4286      	cmp	r6, r0
 80ad63c:	da0c      	bge.n	80ad658 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x206>
      // Shift the vector left:
      float* batch_ptr = state_ptr_batch;
      float* batch_start = state_ptr_batch + 1;
 80ad63e:	1d18      	adds	r0, r3, #4
      float* batch_end = state_ptr_batch + memory_size;
 80ad640:	4413      	add	r3, r2
      while (batch_start != batch_end) {
 80ad642:	4298      	cmp	r0, r3
 80ad644:	d004      	beq.n	80ad650 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1fe>
        *batch_ptr++ = *batch_start++;
 80ad646:	f850 eb04 	ldr.w	lr, [r0], #4
 80ad64a:	f840 ec08 	str.w	lr, [r0, #-8]
    for (int f = 0; f < num_filters; ++f) {
      // Shift the vector left:
      float* batch_ptr = state_ptr_batch;
      float* batch_start = state_ptr_batch + 1;
      float* batch_end = state_ptr_batch + memory_size;
      while (batch_start != batch_end) {
 80ad64e:	e7f8      	b.n	80ad642 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1f0>
        *batch_ptr++ = *batch_start++;
      }
      state_ptr_batch[memory_size - 1] = 0.0f;
 80ad650:	f843 7c04 	str.w	r7, [r3, #-4]
  // Left shift the activation_state to make room for next cycle's activation.
  // TODO(alanchiao): explore collapsing this into a single loop.
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int f = 0; f < num_filters; ++f) {
 80ad654:	3601      	adds	r6, #1
 80ad656:	e7ef      	b.n	80ad638 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1e6>
 80ad658:	9b05      	ldr	r3, [sp, #20]
    }
  }

  // Left shift the activation_state to make room for next cycle's activation.
  // TODO(alanchiao): explore collapsing this into a single loop.
  for (int b = 0; b < batch_size; ++b) {
 80ad65a:	3401      	adds	r4, #1
 80ad65c:	4419      	add	r1, r3
 80ad65e:	e7e2      	b.n	80ad626 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1d4>
      }
      state_ptr_batch[memory_size - 1] = 0.0f;
      state_ptr_batch += memory_size;
    }
  }
}
 80ad660:	b00b      	add	sp, #44	; 0x2c
 80ad662:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

080ad668 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode>:
  return nullptr;
}

void Free(TfLiteContext* context, void* buffer) {}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 80ad668:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  // [4] = Activation State (variable),
  //         {2, batch_size, memory_size * num_filters}
  // TODO(kreeger): Use input tensor as variable until scratch tensor allocation
  // has been implemented (cl/263032056)
  // TF_LITE_ENSURE_EQ(context, node->inputs->size, 5);
  TF_LITE_ENSURE_EQ(context, node->inputs->size, 6);
 80ad66c:	680d      	ldr	r5, [r1, #0]
  return nullptr;
}

void Free(TfLiteContext* context, void* buffer) {}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 80ad66e:	b087      	sub	sp, #28
  // [4] = Activation State (variable),
  //         {2, batch_size, memory_size * num_filters}
  // TODO(kreeger): Use input tensor as variable until scratch tensor allocation
  // has been implemented (cl/263032056)
  // TF_LITE_ENSURE_EQ(context, node->inputs->size, 5);
  TF_LITE_ENSURE_EQ(context, node->inputs->size, 6);
 80ad670:	682b      	ldr	r3, [r5, #0]
  return nullptr;
}

void Free(TfLiteContext* context, void* buffer) {}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 80ad672:	4607      	mov	r7, r0
  // [4] = Activation State (variable),
  //         {2, batch_size, memory_size * num_filters}
  // TODO(kreeger): Use input tensor as variable until scratch tensor allocation
  // has been implemented (cl/263032056)
  // TF_LITE_ENSURE_EQ(context, node->inputs->size, 5);
  TF_LITE_ENSURE_EQ(context, node->inputs->size, 6);
 80ad674:	2b06      	cmp	r3, #6
  return nullptr;
}

void Free(TfLiteContext* context, void* buffer) {}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 80ad676:	4689      	mov	r9, r1
  // [4] = Activation State (variable),
  //         {2, batch_size, memory_size * num_filters}
  // TODO(kreeger): Use input tensor as variable until scratch tensor allocation
  // has been implemented (cl/263032056)
  // TF_LITE_ENSURE_EQ(context, node->inputs->size, 5);
  TF_LITE_ENSURE_EQ(context, node->inputs->size, 6);
 80ad678:	d00e      	beq.n	80ad698 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x30>
 80ad67a:	9302      	str	r3, [sp, #8]
 80ad67c:	4b9f      	ldr	r3, [pc, #636]	; (80ad8fc <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x294>)
 80ad67e:	2206      	movs	r2, #6
 80ad680:	9301      	str	r3, [sp, #4]
 80ad682:	4b9f      	ldr	r3, [pc, #636]	; (80ad900 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x298>)
 80ad684:	9203      	str	r2, [sp, #12]
 80ad686:	9300      	str	r3, [sp, #0]
 80ad688:	6944      	ldr	r4, [r0, #20]
 80ad68a:	4a9e      	ldr	r2, [pc, #632]	; (80ad904 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x29c>)
 80ad68c:	499e      	ldr	r1, [pc, #632]	; (80ad908 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2a0>)
 80ad68e:	f44f 739f 	mov.w	r3, #318	; 0x13e
 80ad692:	47a0      	blx	r4
 80ad694:	2001      	movs	r0, #1
 80ad696:	e296      	b.n	80adbc6 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x55e>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ad698:	68a9      	ldr	r1, [r5, #8]
 80ad69a:	2238      	movs	r2, #56	; 0x38
 80ad69c:	4351      	muls	r1, r2
 80ad69e:	6883      	ldr	r3, [r0, #8]
 80ad6a0:	9105      	str	r1, [sp, #20]
 80ad6a2:	1858      	adds	r0, r3, r1
}

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
 80ad6a4:	6929      	ldr	r1, [r5, #16]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ad6a6:	686c      	ldr	r4, [r5, #4]

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
 80ad6a8:	f1b1 3fff 	cmp.w	r1, #4294967295	; 0xffffffff
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ad6ac:	fb02 f404 	mul.w	r4, r2, r4
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ad6b0:	bf18      	it	ne
 80ad6b2:	fb02 3e01 	mlane	lr, r2, r1, r3

  // Define input constants based on input tensor definition above:
  const int rank = params->rank;
  const int input_size = input->dims->data[1];
  const int batch_size = input->dims->data[0];
  const int num_filters = weights_feature->dims->data[0];
 80ad6b6:	6880      	ldr	r0, [r0, #8]
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);
  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];

  // Define input constants based on input tensor definition above:
  const int rank = params->rank;
 80ad6b8:	f8d9 2014 	ldr.w	r2, [r9, #20]
  const int input_size = input->dims->data[1];
  const int batch_size = input->dims->data[0];
  const int num_filters = weights_feature->dims->data[0];
 80ad6bc:	6841      	ldr	r1, [r0, #4]
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);
  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];

  // Define input constants based on input tensor definition above:
  const int rank = params->rank;
 80ad6be:	6812      	ldr	r2, [r2, #0]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ad6c0:	eb03 0604 	add.w	r6, r3, r4
  const int input_size = input->dims->data[1];
  const int batch_size = input->dims->data[0];
  const int num_filters = weights_feature->dims->data[0];
  TF_LITE_ENSURE_EQ(context, num_filters % rank, 0);
 80ad6c4:	fb91 fbf2 	sdiv	fp, r1, r2
 80ad6c8:	fb02 121b 	mls	r2, r2, fp, r1
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
  }
  return nullptr;
 80ad6cc:	bf08      	it	eq
 80ad6ce:	f04f 0e00 	moveq.w	lr, #0
 80ad6d2:	b152      	cbz	r2, 80ad6ea <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x82>
 80ad6d4:	2300      	movs	r3, #0
 80ad6d6:	9303      	str	r3, [sp, #12]
 80ad6d8:	4b8c      	ldr	r3, [pc, #560]	; (80ad90c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2a4>)
 80ad6da:	9202      	str	r2, [sp, #8]
 80ad6dc:	9301      	str	r3, [sp, #4]
 80ad6de:	4b8c      	ldr	r3, [pc, #560]	; (80ad910 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2a8>)
 80ad6e0:	9300      	str	r3, [sp, #0]
 80ad6e2:	697c      	ldr	r4, [r7, #20]
 80ad6e4:	f240 134d 	movw	r3, #333	; 0x14d
 80ad6e8:	e21d      	b.n	80adb26 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4be>
  const int num_units = num_filters / rank;
  const int memory_size = weights_time->dims->data[1];

  // Validate Input Tensor:
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
 80ad6ea:	5d1c      	ldrb	r4, [r3, r4]
 80ad6ec:	2c01      	cmp	r4, #1
 80ad6ee:	d00a      	beq.n	80ad706 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x9e>
 80ad6f0:	4b88      	ldr	r3, [pc, #544]	; (80ad914 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2ac>)
 80ad6f2:	2501      	movs	r5, #1
 80ad6f4:	9301      	str	r3, [sp, #4]
 80ad6f6:	4b88      	ldr	r3, [pc, #544]	; (80ad918 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b0>)
 80ad6f8:	9503      	str	r5, [sp, #12]
 80ad6fa:	9300      	str	r3, [sp, #0]
 80ad6fc:	9402      	str	r4, [sp, #8]
 80ad6fe:	697c      	ldr	r4, [r7, #20]
 80ad700:	f44f 73a9 	mov.w	r3, #338	; 0x152
 80ad704:	e20f      	b.n	80adb26 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4be>
  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];

  // Define input constants based on input tensor definition above:
  const int rank = params->rank;
  const int input_size = input->dims->data[1];
 80ad706:	68b6      	ldr	r6, [r6, #8]
#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/c_api_internal.h"

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
 80ad708:	6832      	ldr	r2, [r6, #0]
  const int num_units = num_filters / rank;
  const int memory_size = weights_time->dims->data[1];

  // Validate Input Tensor:
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 2);
 80ad70a:	2a02      	cmp	r2, #2
 80ad70c:	d00a      	beq.n	80ad724 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0xbc>
 80ad70e:	2302      	movs	r3, #2
 80ad710:	9303      	str	r3, [sp, #12]
 80ad712:	4b82      	ldr	r3, [pc, #520]	; (80ad91c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b4>)
 80ad714:	9202      	str	r2, [sp, #8]
 80ad716:	9301      	str	r3, [sp, #4]
 80ad718:	4b81      	ldr	r3, [pc, #516]	; (80ad920 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b8>)
 80ad71a:	9300      	str	r3, [sp, #0]
 80ad71c:	697d      	ldr	r5, [r7, #20]
 80ad71e:	f240 1353 	movw	r3, #339	; 0x153
 80ad722:	e22e      	b.n	80adb82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51a>
 80ad724:	f8d0 8000 	ldr.w	r8, [r0]

  // Validate Weights Feature Input Tensor:
  TF_LITE_ENSURE_EQ(context, NumDimensions(weights_feature), 2);
 80ad728:	f1b8 0f02 	cmp.w	r8, #2
 80ad72c:	d00a      	beq.n	80ad744 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0xdc>
 80ad72e:	4b7b      	ldr	r3, [pc, #492]	; (80ad91c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b4>)
 80ad730:	9203      	str	r2, [sp, #12]
 80ad732:	9301      	str	r3, [sp, #4]
 80ad734:	4b7b      	ldr	r3, [pc, #492]	; (80ad924 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2bc>)
 80ad736:	f8cd 8008 	str.w	r8, [sp, #8]
 80ad73a:	9300      	str	r3, [sp, #0]
 80ad73c:	697d      	ldr	r5, [r7, #20]
 80ad73e:	f44f 73ab 	mov.w	r3, #342	; 0x156
 80ad742:	e21e      	b.n	80adb82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51a>
  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];

  // Define input constants based on input tensor definition above:
  const int rank = params->rank;
  const int input_size = input->dims->data[1];
 80ad744:	f8d6 c008 	ldr.w	ip, [r6, #8]
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 2);

  // Validate Weights Feature Input Tensor:
  TF_LITE_ENSURE_EQ(context, NumDimensions(weights_feature), 2);
  TF_LITE_ENSURE_EQ(context, weights_feature->dims->data[1], input_size);
 80ad748:	6882      	ldr	r2, [r0, #8]
 80ad74a:	4594      	cmp	ip, r2
 80ad74c:	d00a      	beq.n	80ad764 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0xfc>
 80ad74e:	4b76      	ldr	r3, [pc, #472]	; (80ad928 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2c0>)
 80ad750:	f8cd c00c 	str.w	ip, [sp, #12]
 80ad754:	9301      	str	r3, [sp, #4]
 80ad756:	4b75      	ldr	r3, [pc, #468]	; (80ad92c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2c4>)
 80ad758:	9202      	str	r2, [sp, #8]
 80ad75a:	9300      	str	r3, [sp, #0]
 80ad75c:	697d      	ldr	r5, [r7, #20]
 80ad75e:	f240 1357 	movw	r3, #343	; 0x157
 80ad762:	e20e      	b.n	80adb82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51a>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80ad764:	68ea      	ldr	r2, [r5, #12]
 80ad766:	f04f 0c38 	mov.w	ip, #56	; 0x38
 80ad76a:	fb0c fc02 	mul.w	ip, ip, r2
 80ad76e:	eb03 020c 	add.w	r2, r3, ip
  const int input_size = input->dims->data[1];
  const int batch_size = input->dims->data[0];
  const int num_filters = weights_feature->dims->data[0];
  TF_LITE_ENSURE_EQ(context, num_filters % rank, 0);
  const int num_units = num_filters / rank;
  const int memory_size = weights_time->dims->data[1];
 80ad772:	6890      	ldr	r0, [r2, #8]
 80ad774:	9204      	str	r2, [sp, #16]
#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/c_api_internal.h"

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
 80ad776:	6802      	ldr	r2, [r0, #0]
  // Validate Weights Feature Input Tensor:
  TF_LITE_ENSURE_EQ(context, NumDimensions(weights_feature), 2);
  TF_LITE_ENSURE_EQ(context, weights_feature->dims->data[1], input_size);

  // Validate Weights Time Input Tensor:
  TF_LITE_ENSURE_EQ(context, NumDimensions(weights_time), 2);
 80ad778:	2a02      	cmp	r2, #2
 80ad77a:	d00a      	beq.n	80ad792 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x12a>
 80ad77c:	4b67      	ldr	r3, [pc, #412]	; (80ad91c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b4>)
 80ad77e:	f8cd 800c 	str.w	r8, [sp, #12]
 80ad782:	9301      	str	r3, [sp, #4]
 80ad784:	4b6a      	ldr	r3, [pc, #424]	; (80ad930 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2c8>)
 80ad786:	9202      	str	r2, [sp, #8]
 80ad788:	9300      	str	r3, [sp, #0]
 80ad78a:	697d      	ldr	r5, [r7, #20]
 80ad78c:	f44f 73ad 	mov.w	r3, #346	; 0x15a
 80ad790:	e1f7      	b.n	80adb82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51a>
  TF_LITE_ENSURE_EQ(context, weights_time->dims->data[0], num_filters);
 80ad792:	6842      	ldr	r2, [r0, #4]
 80ad794:	4291      	cmp	r1, r2
 80ad796:	d009      	beq.n	80ad7ac <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x144>
 80ad798:	4b66      	ldr	r3, [pc, #408]	; (80ad934 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2cc>)
 80ad79a:	9103      	str	r1, [sp, #12]
 80ad79c:	9301      	str	r3, [sp, #4]
 80ad79e:	4b66      	ldr	r3, [pc, #408]	; (80ad938 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2d0>)
 80ad7a0:	9202      	str	r2, [sp, #8]
 80ad7a2:	9300      	str	r3, [sp, #0]
 80ad7a4:	697d      	ldr	r5, [r7, #20]
 80ad7a6:	f240 135b 	movw	r3, #347	; 0x15b
 80ad7aa:	e1ea      	b.n	80adb82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51a>
  TF_LITE_ENSURE_EQ(context, weights_time->dims->data[1], memory_size);

  // Validate Optional Bias Input Tensor:
  if (bias) {
 80ad7ac:	f1be 0f00 	cmp.w	lr, #0
 80ad7b0:	d01e      	beq.n	80ad7f0 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x188>
    TF_LITE_ENSURE_EQ(context, bias->dims->data[0], num_units);
 80ad7b2:	f8de 2008 	ldr.w	r2, [lr, #8]
 80ad7b6:	6852      	ldr	r2, [r2, #4]
 80ad7b8:	4593      	cmp	fp, r2
 80ad7ba:	d00a      	beq.n	80ad7d2 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x16a>
 80ad7bc:	4b5f      	ldr	r3, [pc, #380]	; (80ad93c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2d4>)
 80ad7be:	f8cd b00c 	str.w	fp, [sp, #12]
 80ad7c2:	9301      	str	r3, [sp, #4]
 80ad7c4:	4b5e      	ldr	r3, [pc, #376]	; (80ad940 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2d8>)
 80ad7c6:	9202      	str	r2, [sp, #8]
 80ad7c8:	9300      	str	r3, [sp, #0]
 80ad7ca:	697d      	ldr	r5, [r7, #20]
 80ad7cc:	f44f 73b0 	mov.w	r3, #352	; 0x160
 80ad7d0:	e1d7      	b.n	80adb82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51a>
    TF_LITE_ENSURE_EQ(context, bias->type, kTfLiteFloat32);
 80ad7d2:	f89e 2000 	ldrb.w	r2, [lr]
 80ad7d6:	2a01      	cmp	r2, #1
 80ad7d8:	d00a      	beq.n	80ad7f0 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x188>
 80ad7da:	4b4e      	ldr	r3, [pc, #312]	; (80ad914 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2ac>)
 80ad7dc:	2401      	movs	r4, #1
 80ad7de:	9301      	str	r3, [sp, #4]
 80ad7e0:	4b58      	ldr	r3, [pc, #352]	; (80ad944 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2dc>)
 80ad7e2:	9403      	str	r4, [sp, #12]
 80ad7e4:	9300      	str	r3, [sp, #0]
 80ad7e6:	9202      	str	r2, [sp, #8]
 80ad7e8:	697d      	ldr	r5, [r7, #20]
 80ad7ea:	f240 1361 	movw	r3, #353	; 0x161
 80ad7ee:	e1c8      	b.n	80adb82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51a>
      GetInput(context, node, kWeightsFeatureTensor);
  const TfLiteTensor* weights_time =
      GetInput(context, node, kWeightsTimeTensor);
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);
  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];
 80ad7f0:	696a      	ldr	r2, [r5, #20]
 80ad7f2:	f04f 0a38 	mov.w	sl, #56	; 0x38
 80ad7f6:	fb0a f202 	mul.w	r2, sl, r2
    TF_LITE_ENSURE_EQ(context, bias->dims->data[0], num_units);
    TF_LITE_ENSURE_EQ(context, bias->type, kTfLiteFloat32);
  }

  // Validate Activation State Input Tensor:
  TF_LITE_ENSURE_EQ(context, activation_state->type, kTfLiteFloat32);
 80ad7fa:	5c9c      	ldrb	r4, [r3, r2]
      GetInput(context, node, kWeightsFeatureTensor);
  const TfLiteTensor* weights_time =
      GetInput(context, node, kWeightsTimeTensor);
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);
  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];
 80ad7fc:	eb03 0e02 	add.w	lr, r3, r2
    TF_LITE_ENSURE_EQ(context, bias->dims->data[0], num_units);
    TF_LITE_ENSURE_EQ(context, bias->type, kTfLiteFloat32);
  }

  // Validate Activation State Input Tensor:
  TF_LITE_ENSURE_EQ(context, activation_state->type, kTfLiteFloat32);
 80ad800:	2c01      	cmp	r4, #1
 80ad802:	d00a      	beq.n	80ad81a <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x1b2>
 80ad804:	4b43      	ldr	r3, [pc, #268]	; (80ad914 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2ac>)
 80ad806:	2501      	movs	r5, #1
 80ad808:	9301      	str	r3, [sp, #4]
 80ad80a:	4b4f      	ldr	r3, [pc, #316]	; (80ad948 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2e0>)
 80ad80c:	9503      	str	r5, [sp, #12]
 80ad80e:	9300      	str	r3, [sp, #0]
 80ad810:	9402      	str	r4, [sp, #8]
 80ad812:	697c      	ldr	r4, [r7, #20]
 80ad814:	f240 1365 	movw	r3, #357	; 0x165
 80ad818:	e185      	b.n	80adb26 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4be>
 80ad81a:	f8de e008 	ldr.w	lr, [lr, #8]
 80ad81e:	f8de 2000 	ldr.w	r2, [lr]
  TF_LITE_ENSURE_EQ(context, NumDimensions(activation_state), 2);
 80ad822:	2a02      	cmp	r2, #2
 80ad824:	d00a      	beq.n	80ad83c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x1d4>
 80ad826:	2302      	movs	r3, #2
 80ad828:	9303      	str	r3, [sp, #12]
 80ad82a:	4b3c      	ldr	r3, [pc, #240]	; (80ad91c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b4>)
 80ad82c:	9202      	str	r2, [sp, #8]
 80ad82e:	9301      	str	r3, [sp, #4]
 80ad830:	4b46      	ldr	r3, [pc, #280]	; (80ad94c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2e4>)
 80ad832:	9300      	str	r3, [sp, #0]
 80ad834:	697d      	ldr	r5, [r7, #20]
 80ad836:	f44f 73b3 	mov.w	r3, #358	; 0x166
 80ad83a:	e1a2      	b.n	80adb82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51a>
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];

  // Define input constants based on input tensor definition above:
  const int rank = params->rank;
  const int input_size = input->dims->data[1];
  const int batch_size = input->dims->data[0];
 80ad83c:	f8d6 8004 	ldr.w	r8, [r6, #4]
  }

  // Validate Activation State Input Tensor:
  TF_LITE_ENSURE_EQ(context, activation_state->type, kTfLiteFloat32);
  TF_LITE_ENSURE_EQ(context, NumDimensions(activation_state), 2);
  TF_LITE_ENSURE_EQ(context, activation_state->dims->data[0], batch_size);
 80ad840:	f8de 6004 	ldr.w	r6, [lr, #4]
 80ad844:	45b0      	cmp	r8, r6
 80ad846:	d00a      	beq.n	80ad85e <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x1f6>
 80ad848:	4b41      	ldr	r3, [pc, #260]	; (80ad950 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2e8>)
 80ad84a:	f8cd 800c 	str.w	r8, [sp, #12]
 80ad84e:	9301      	str	r3, [sp, #4]
 80ad850:	4b40      	ldr	r3, [pc, #256]	; (80ad954 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2ec>)
 80ad852:	9602      	str	r6, [sp, #8]
 80ad854:	9300      	str	r3, [sp, #0]
 80ad856:	697d      	ldr	r5, [r7, #20]
 80ad858:	f240 1367 	movw	r3, #359	; 0x167
 80ad85c:	e191      	b.n	80adb82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51a>
  const int input_size = input->dims->data[1];
  const int batch_size = input->dims->data[0];
  const int num_filters = weights_feature->dims->data[0];
  TF_LITE_ENSURE_EQ(context, num_filters % rank, 0);
  const int num_units = num_filters / rank;
  const int memory_size = weights_time->dims->data[1];
 80ad85e:	6880      	ldr	r0, [r0, #8]

  // Validate Activation State Input Tensor:
  TF_LITE_ENSURE_EQ(context, activation_state->type, kTfLiteFloat32);
  TF_LITE_ENSURE_EQ(context, NumDimensions(activation_state), 2);
  TF_LITE_ENSURE_EQ(context, activation_state->dims->data[0], batch_size);
  TF_LITE_ENSURE_EQ(context, activation_state->dims->data[1],
 80ad860:	f8de 6008 	ldr.w	r6, [lr, #8]
 80ad864:	fb00 fe01 	mul.w	lr, r0, r1
 80ad868:	4576      	cmp	r6, lr
 80ad86a:	d00a      	beq.n	80ad882 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x21a>
 80ad86c:	4b3a      	ldr	r3, [pc, #232]	; (80ad958 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2f0>)
 80ad86e:	f8cd e00c 	str.w	lr, [sp, #12]
 80ad872:	9301      	str	r3, [sp, #4]
 80ad874:	4b39      	ldr	r3, [pc, #228]	; (80ad95c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2f4>)
 80ad876:	9602      	str	r6, [sp, #8]
 80ad878:	9300      	str	r3, [sp, #0]
 80ad87a:	697d      	ldr	r5, [r7, #20]
 80ad87c:	f240 1369 	movw	r3, #361	; 0x169
 80ad880:	e17f      	b.n	80adb82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51a>
  //       ApplyTimeWeightsBiasAndActivation():
  //         float, {2, batch_size, num_filters}
  // TODO(kreeger): Use input tensor as variable until scratch tensor allocation
  // has been implemented (cl/263032056)
  // TfLiteTensor* scratch_tensor = GetTemporary(context, node, 0);
  TfLiteTensor* scratch_tensor = &context->tensors[node->inputs->data[5]];
 80ad882:	69ad      	ldr	r5, [r5, #24]
 80ad884:	fb0a fa05 	mul.w	sl, sl, r5

  TF_LITE_ENSURE_EQ(context, scratch_tensor->type, kTfLiteFloat32);
 80ad888:	f813 500a 	ldrb.w	r5, [r3, sl]
  //       ApplyTimeWeightsBiasAndActivation():
  //         float, {2, batch_size, num_filters}
  // TODO(kreeger): Use input tensor as variable until scratch tensor allocation
  // has been implemented (cl/263032056)
  // TfLiteTensor* scratch_tensor = GetTemporary(context, node, 0);
  TfLiteTensor* scratch_tensor = &context->tensors[node->inputs->data[5]];
 80ad88c:	eb03 060a 	add.w	r6, r3, sl

  TF_LITE_ENSURE_EQ(context, scratch_tensor->type, kTfLiteFloat32);
 80ad890:	2d01      	cmp	r5, #1
 80ad892:	d009      	beq.n	80ad8a8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x240>
 80ad894:	4b1f      	ldr	r3, [pc, #124]	; (80ad914 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2ac>)
 80ad896:	9403      	str	r4, [sp, #12]
 80ad898:	9301      	str	r3, [sp, #4]
 80ad89a:	4b31      	ldr	r3, [pc, #196]	; (80ad960 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2f8>)
 80ad89c:	9502      	str	r5, [sp, #8]
 80ad89e:	9300      	str	r3, [sp, #0]
 80ad8a0:	697d      	ldr	r5, [r7, #20]
 80ad8a2:	f44f 73ba 	mov.w	r3, #372	; 0x174
 80ad8a6:	e16c      	b.n	80adb82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51a>
 80ad8a8:	68b4      	ldr	r4, [r6, #8]
 80ad8aa:	6826      	ldr	r6, [r4, #0]
  TF_LITE_ENSURE_EQ(context, NumDimensions(scratch_tensor), 2);
 80ad8ac:	2e02      	cmp	r6, #2
 80ad8ae:	d009      	beq.n	80ad8c4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x25c>
 80ad8b0:	4b1a      	ldr	r3, [pc, #104]	; (80ad91c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b4>)
 80ad8b2:	9203      	str	r2, [sp, #12]
 80ad8b4:	9301      	str	r3, [sp, #4]
 80ad8b6:	4b2b      	ldr	r3, [pc, #172]	; (80ad964 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2fc>)
 80ad8b8:	9602      	str	r6, [sp, #8]
 80ad8ba:	9300      	str	r3, [sp, #0]
 80ad8bc:	697c      	ldr	r4, [r7, #20]
 80ad8be:	f240 1375 	movw	r3, #373	; 0x175
 80ad8c2:	e130      	b.n	80adb26 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4be>
  TF_LITE_ENSURE_EQ(context, scratch_tensor->dims->data[0], batch_size);
 80ad8c4:	6862      	ldr	r2, [r4, #4]
 80ad8c6:	4590      	cmp	r8, r2
 80ad8c8:	d00a      	beq.n	80ad8e0 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x278>
 80ad8ca:	4b21      	ldr	r3, [pc, #132]	; (80ad950 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2e8>)
 80ad8cc:	f8cd 800c 	str.w	r8, [sp, #12]
 80ad8d0:	9301      	str	r3, [sp, #4]
 80ad8d2:	4b25      	ldr	r3, [pc, #148]	; (80ad968 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x300>)
 80ad8d4:	9202      	str	r2, [sp, #8]
 80ad8d6:	9300      	str	r3, [sp, #0]
 80ad8d8:	697c      	ldr	r4, [r7, #20]
 80ad8da:	f44f 73bb 	mov.w	r3, #374	; 0x176
 80ad8de:	e122      	b.n	80adb26 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4be>
  TF_LITE_ENSURE_EQ(context, scratch_tensor->dims->data[1], num_filters);
 80ad8e0:	68a2      	ldr	r2, [r4, #8]
 80ad8e2:	4291      	cmp	r1, r2
 80ad8e4:	d044      	beq.n	80ad970 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x308>
 80ad8e6:	4b13      	ldr	r3, [pc, #76]	; (80ad934 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2cc>)
 80ad8e8:	9103      	str	r1, [sp, #12]
 80ad8ea:	9301      	str	r3, [sp, #4]
 80ad8ec:	4b1f      	ldr	r3, [pc, #124]	; (80ad96c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x304>)
 80ad8ee:	9202      	str	r2, [sp, #8]
 80ad8f0:	9300      	str	r3, [sp, #0]
 80ad8f2:	697c      	ldr	r4, [r7, #20]
 80ad8f4:	f240 1377 	movw	r3, #375	; 0x177
 80ad8f8:	e115      	b.n	80adb26 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4be>
 80ad8fa:	bf00      	nop
 80ad8fc:	080b3bd8 	.word	0x080b3bd8
 80ad900:	080b6d86 	.word	0x080b6d86
 80ad904:	080b6ce0 	.word	0x080b6ce0
 80ad908:	080b5be0 	.word	0x080b5be0
 80ad90c:	080b74a5 	.word	0x080b74a5
 80ad910:	080b6d99 	.word	0x080b6d99
 80ad914:	080b644f 	.word	0x080b644f
 80ad918:	080b5c1b 	.word	0x080b5c1b
 80ad91c:	080b6590 	.word	0x080b6590
 80ad920:	080b69e4 	.word	0x080b69e4
 80ad924:	080b6dac 	.word	0x080b6dac
 80ad928:	080b6dcb 	.word	0x080b6dcb
 80ad92c:	080b6dd6 	.word	0x080b6dd6
 80ad930:	080b6df5 	.word	0x080b6df5
 80ad934:	080b6eaa 	.word	0x080b6eaa
 80ad938:	080b70ef 	.word	0x080b70ef
 80ad93c:	080b6e11 	.word	0x080b6e11
 80ad940:	080b6e1b 	.word	0x080b6e1b
 80ad944:	080b6e2f 	.word	0x080b6e2f
 80ad948:	080b6e3a 	.word	0x080b6e3a
 80ad94c:	080b6e51 	.word	0x080b6e51
 80ad950:	080b6e71 	.word	0x080b6e71
 80ad954:	080b6e7c 	.word	0x080b6e7c
 80ad958:	080b6e9c 	.word	0x080b6e9c
 80ad95c:	080b6eb6 	.word	0x080b6eb6
 80ad960:	080b6ed6 	.word	0x080b6ed6
 80ad964:	080b6eeb 	.word	0x080b6eeb
 80ad968:	080b6f09 	.word	0x080b6f09
 80ad96c:	080b6f27 	.word	0x080b6f27
 80ad970:	9a05      	ldr	r2, [sp, #20]
 80ad972:	5c9c      	ldrb	r4, [r3, r2]
}

// Determines whether it is a hybrid op - one that has float inputs and
// quantized weights.
inline bool IsHybridOp(const TfLiteTensor* input, const TfLiteTensor* weight) {
  return ((weight->type == kTfLiteUInt8 || weight->type == kTfLiteInt8) &&
 80ad974:	2c03      	cmp	r4, #3
 80ad976:	d002      	beq.n	80ad97e <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x316>
 80ad978:	2c09      	cmp	r4, #9
 80ad97a:	f040 8109 	bne.w	80adb90 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x528>
  // TODO(kreeger): Handle full quant svdf b/139435798
  if (is_hybrid_op) {
    // Validate Input Tensor dtypes:
    TF_LITE_ENSURE(context, weights_feature->type == kTfLiteUInt8 ||
                                weights_feature->type == kTfLiteInt8);
    TF_LITE_ENSURE(context, weights_time->type == kTfLiteUInt8 ||
 80ad97e:	f813 200c 	ldrb.w	r2, [r3, ip]
 80ad982:	2a03      	cmp	r2, #3
 80ad984:	d007      	beq.n	80ad996 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x32e>
 80ad986:	2a09      	cmp	r2, #9
 80ad988:	d005      	beq.n	80ad996 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x32e>
 80ad98a:	4b90      	ldr	r3, [pc, #576]	; (80adbcc <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x564>)
 80ad98c:	9300      	str	r3, [sp, #0]
 80ad98e:	697c      	ldr	r4, [r7, #20]
 80ad990:	f240 1381 	movw	r3, #385	; 0x181
 80ad994:	e026      	b.n	80ad9e4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x37c>
    // Validate Scratch Tensors:
    // [0] = (shared - see above for usage)
    // [1] = Input Quantized, int8_t/uint8_t, {2, batch_size, input_size}
    // [2] = Scaling Factors, float, {1, batch_size}
    // [3] = Float Weights Time, float, {2, num_filters, memory_size}
    TF_LITE_ENSURE_EQ(context, node->temporaries->size, 4);
 80ad996:	f8d9 500c 	ldr.w	r5, [r9, #12]
 80ad99a:	682a      	ldr	r2, [r5, #0]
 80ad99c:	2a04      	cmp	r2, #4
 80ad99e:	d00a      	beq.n	80ad9b6 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x34e>
 80ad9a0:	2304      	movs	r3, #4
 80ad9a2:	9303      	str	r3, [sp, #12]
 80ad9a4:	4b8a      	ldr	r3, [pc, #552]	; (80adbd0 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x568>)
 80ad9a6:	9202      	str	r2, [sp, #8]
 80ad9a8:	9301      	str	r3, [sp, #4]
 80ad9aa:	4b8a      	ldr	r3, [pc, #552]	; (80adbd4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x56c>)
 80ad9ac:	9300      	str	r3, [sp, #0]
 80ad9ae:	697c      	ldr	r4, [r7, #20]
 80ad9b0:	f44f 73c4 	mov.w	r3, #392	; 0x188
 80ad9b4:	e0b7      	b.n	80adb26 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4be>
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
}
inline TfLiteTensor* GetTemporary(TfLiteContext* context, TfLiteNode* node,
                                  int index) {
  return &context->tensors[flatbuffers::EndianScalar(
      node->temporaries->data[index])];
 80ad9b6:	68ae      	ldr	r6, [r5, #8]
 80ad9b8:	2238      	movs	r2, #56	; 0x38
 80ad9ba:	4356      	muls	r6, r2
 80ad9bc:	68ec      	ldr	r4, [r5, #12]
 80ad9be:	692d      	ldr	r5, [r5, #16]
 80ad9c0:	4354      	muls	r4, r2
 80ad9c2:	436a      	muls	r2, r5
    TfLiteTensor* scratch_input_quantized = GetTemporary(context, node, 1);
    TfLiteTensor* scratch_scaling_factors = GetTemporary(context, node, 2);
    TfLiteTensor* scratch_float_weights_time = GetTemporary(context, node, 3);

    // Validate Input Quantized Scratch Tensor:
    TF_LITE_ENSURE(context, scratch_input_quantized->type == kTfLiteUInt8 ||
 80ad9c4:	5d9d      	ldrb	r5, [r3, r6]
 80ad9c6:	eb03 0a06 	add.w	sl, r3, r6
 80ad9ca:	2d03      	cmp	r5, #3
 80ad9cc:	eb03 0c04 	add.w	ip, r3, r4
 80ad9d0:	eb03 0e02 	add.w	lr, r3, r2
 80ad9d4:	d00b      	beq.n	80ad9ee <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x386>
 80ad9d6:	2d09      	cmp	r5, #9
 80ad9d8:	d009      	beq.n	80ad9ee <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x386>
 80ad9da:	4b7f      	ldr	r3, [pc, #508]	; (80adbd8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x570>)
 80ad9dc:	9300      	str	r3, [sp, #0]
 80ad9de:	f240 138f 	movw	r3, #399	; 0x18f
 80ad9e2:	697c      	ldr	r4, [r7, #20]
 80ad9e4:	4a7d      	ldr	r2, [pc, #500]	; (80adbdc <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x574>)
 80ad9e6:	497e      	ldr	r1, [pc, #504]	; (80adbe0 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x578>)
 80ad9e8:	4638      	mov	r0, r7
 80ad9ea:	47a0      	blx	r4
 80ad9ec:	e652      	b.n	80ad694 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2c>
                                scratch_input_quantized->type == kTfLiteInt8);
    TF_LITE_ENSURE_EQ(context, scratch_input_quantized->dims->data[0],
 80ad9ee:	f8da 5008 	ldr.w	r5, [sl, #8]
 80ad9f2:	686d      	ldr	r5, [r5, #4]
 80ad9f4:	45a8      	cmp	r8, r5
 80ad9f6:	d00a      	beq.n	80ada0e <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x3a6>
 80ad9f8:	4b7a      	ldr	r3, [pc, #488]	; (80adbe4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x57c>)
 80ad9fa:	f8cd 800c 	str.w	r8, [sp, #12]
 80ad9fe:	9301      	str	r3, [sp, #4]
 80ada00:	4b79      	ldr	r3, [pc, #484]	; (80adbe8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x580>)
 80ada02:	9502      	str	r5, [sp, #8]
 80ada04:	9300      	str	r3, [sp, #0]
 80ada06:	697c      	ldr	r4, [r7, #20]
 80ada08:	f240 1391 	movw	r3, #401	; 0x191
 80ada0c:	e08b      	b.n	80adb26 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4be>
                      batch_size);

    // Validate Scaling Factors Scratch Tensor:
    TF_LITE_ENSURE_EQ(context, scratch_scaling_factors->type, kTfLiteFloat32);
 80ada0e:	5d1e      	ldrb	r6, [r3, r4]
 80ada10:	2e01      	cmp	r6, #1
 80ada12:	d00a      	beq.n	80ada2a <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x3c2>
 80ada14:	4b75      	ldr	r3, [pc, #468]	; (80adbec <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x584>)
 80ada16:	2401      	movs	r4, #1
 80ada18:	9301      	str	r3, [sp, #4]
 80ada1a:	4b75      	ldr	r3, [pc, #468]	; (80adbf0 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x588>)
 80ada1c:	9403      	str	r4, [sp, #12]
 80ada1e:	9300      	str	r3, [sp, #0]
 80ada20:	9602      	str	r6, [sp, #8]
 80ada22:	697d      	ldr	r5, [r7, #20]
 80ada24:	f44f 73ca 	mov.w	r3, #404	; 0x194
 80ada28:	e0ab      	b.n	80adb82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51a>
 80ada2a:	f8dc 4008 	ldr.w	r4, [ip, #8]
#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/c_api_internal.h"

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
 80ada2e:	6825      	ldr	r5, [r4, #0]
    TF_LITE_ENSURE_EQ(context, NumDimensions(scratch_scaling_factors), 1);
 80ada30:	2d01      	cmp	r5, #1
 80ada32:	d009      	beq.n	80ada48 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x3e0>
 80ada34:	4b6f      	ldr	r3, [pc, #444]	; (80adbf4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x58c>)
 80ada36:	9603      	str	r6, [sp, #12]
 80ada38:	9301      	str	r3, [sp, #4]
 80ada3a:	4b6f      	ldr	r3, [pc, #444]	; (80adbf8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x590>)
 80ada3c:	9502      	str	r5, [sp, #8]
 80ada3e:	9300      	str	r3, [sp, #0]
 80ada40:	697c      	ldr	r4, [r7, #20]
 80ada42:	f240 1395 	movw	r3, #405	; 0x195
 80ada46:	e06e      	b.n	80adb26 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4be>
    TF_LITE_ENSURE_EQ(context, scratch_scaling_factors->dims->data[0],
 80ada48:	6864      	ldr	r4, [r4, #4]
 80ada4a:	45a0      	cmp	r8, r4
 80ada4c:	d00a      	beq.n	80ada64 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x3fc>
 80ada4e:	4b65      	ldr	r3, [pc, #404]	; (80adbe4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x57c>)
 80ada50:	f8cd 800c 	str.w	r8, [sp, #12]
 80ada54:	9301      	str	r3, [sp, #4]
 80ada56:	4b69      	ldr	r3, [pc, #420]	; (80adbfc <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x594>)
 80ada58:	9402      	str	r4, [sp, #8]
 80ada5a:	9300      	str	r3, [sp, #0]
 80ada5c:	697c      	ldr	r4, [r7, #20]
 80ada5e:	f240 1397 	movw	r3, #407	; 0x197
 80ada62:	e060      	b.n	80adb26 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4be>
                      batch_size);

    // Validate Float Weights Time Scratch Tensor:
    TF_LITE_ENSURE_EQ(context, scratch_float_weights_time->type,
 80ada64:	5c9c      	ldrb	r4, [r3, r2]
 80ada66:	2c01      	cmp	r4, #1
 80ada68:	d009      	beq.n	80ada7e <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x416>
 80ada6a:	4b60      	ldr	r3, [pc, #384]	; (80adbec <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x584>)
 80ada6c:	9503      	str	r5, [sp, #12]
 80ada6e:	9301      	str	r3, [sp, #4]
 80ada70:	4b63      	ldr	r3, [pc, #396]	; (80adc00 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x598>)
 80ada72:	9402      	str	r4, [sp, #8]
 80ada74:	9300      	str	r3, [sp, #0]
 80ada76:	697c      	ldr	r4, [r7, #20]
 80ada78:	f240 139b 	movw	r3, #411	; 0x19b
 80ada7c:	e053      	b.n	80adb26 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4be>
 80ada7e:	f8de 3008 	ldr.w	r3, [lr, #8]
 80ada82:	681a      	ldr	r2, [r3, #0]
                      kTfLiteFloat32);
    TF_LITE_ENSURE_EQ(context, NumDimensions(scratch_float_weights_time), 2);
 80ada84:	2a02      	cmp	r2, #2
 80ada86:	d00a      	beq.n	80ada9e <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x436>
 80ada88:	2302      	movs	r3, #2
 80ada8a:	9303      	str	r3, [sp, #12]
 80ada8c:	4b5d      	ldr	r3, [pc, #372]	; (80adc04 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x59c>)
 80ada8e:	9202      	str	r2, [sp, #8]
 80ada90:	9301      	str	r3, [sp, #4]
 80ada92:	4b5d      	ldr	r3, [pc, #372]	; (80adc08 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5a0>)
 80ada94:	9300      	str	r3, [sp, #0]
 80ada96:	697d      	ldr	r5, [r7, #20]
 80ada98:	f44f 73ce 	mov.w	r3, #412	; 0x19c
 80ada9c:	e071      	b.n	80adb82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51a>
    TF_LITE_ENSURE_EQ(context, scratch_float_weights_time->dims->data[0],
 80ada9e:	685a      	ldr	r2, [r3, #4]
 80adaa0:	4291      	cmp	r1, r2
 80adaa2:	d009      	beq.n	80adab8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x450>
 80adaa4:	4b59      	ldr	r3, [pc, #356]	; (80adc0c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5a4>)
 80adaa6:	9103      	str	r1, [sp, #12]
 80adaa8:	9301      	str	r3, [sp, #4]
 80adaaa:	4b59      	ldr	r3, [pc, #356]	; (80adc10 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5a8>)
 80adaac:	9202      	str	r2, [sp, #8]
 80adaae:	9300      	str	r3, [sp, #0]
 80adab0:	697d      	ldr	r5, [r7, #20]
 80adab2:	f44f 73cf 	mov.w	r3, #414	; 0x19e
 80adab6:	e064      	b.n	80adb82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51a>
                      num_filters);
    TF_LITE_ENSURE_EQ(context, scratch_float_weights_time->dims->data[1],
 80adab8:	689b      	ldr	r3, [r3, #8]
 80adaba:	4298      	cmp	r0, r3
 80adabc:	d009      	beq.n	80adad2 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x46a>
 80adabe:	9302      	str	r3, [sp, #8]
 80adac0:	4b54      	ldr	r3, [pc, #336]	; (80adc14 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5ac>)
 80adac2:	9003      	str	r0, [sp, #12]
 80adac4:	9301      	str	r3, [sp, #4]
 80adac6:	4b54      	ldr	r3, [pc, #336]	; (80adc18 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5b0>)
 80adac8:	9300      	str	r3, [sp, #0]
 80adaca:	697d      	ldr	r5, [r7, #20]
 80adacc:	f44f 73d0 	mov.w	r3, #416	; 0x1a0
 80adad0:	e057      	b.n	80adb82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51a>
      weights_time_ptr = GetTensorData<int8_t>(weights_time);
    }
    SymmetricDequantize(weights_time_ptr,
                        NumElements(scratch_float_weights_time),
                        weights_time->params.scale,
                        GetTensorData<float>(scratch_float_weights_time));
 80adad2:	9a04      	ldr	r2, [sp, #16]
 80adad4:	4341      	muls	r1, r0
 80adad6:	9804      	ldr	r0, [sp, #16]
 80adad8:	f8de 3004 	ldr.w	r3, [lr, #4]
 80adadc:	68d2      	ldr	r2, [r2, #12]
 80adade:	6840      	ldr	r0, [r0, #4]
 80adae0:	f7f4 fc20 	bl	80a2324 <_ZN6tflite19SymmetricDequantizeEPKaifPf>
    // TF_LITE_ENSURE_EQ(context, node->temporaries->size, 1);
  }

  // Validate Tensor Output:
  // [0] = float, {2, batch_size, num_units}
  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);
 80adae4:	f8d9 3004 	ldr.w	r3, [r9, #4]
 80adae8:	681d      	ldr	r5, [r3, #0]
 80adaea:	2d01      	cmp	r5, #1
 80adaec:	d00a      	beq.n	80adb04 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x49c>
 80adaee:	4b41      	ldr	r3, [pc, #260]	; (80adbf4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x58c>)
 80adaf0:	2401      	movs	r4, #1
 80adaf2:	9301      	str	r3, [sp, #4]
 80adaf4:	4b49      	ldr	r3, [pc, #292]	; (80adc1c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5b4>)
 80adaf6:	9403      	str	r4, [sp, #12]
 80adaf8:	9300      	str	r3, [sp, #0]
 80adafa:	9502      	str	r5, [sp, #8]
 80adafc:	697d      	ldr	r5, [r7, #20]
 80adafe:	f44f 73e0 	mov.w	r3, #448	; 0x1c0
 80adb02:	e03e      	b.n	80adb82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51a>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80adb04:	685a      	ldr	r2, [r3, #4]
 80adb06:	2338      	movs	r3, #56	; 0x38
 80adb08:	4353      	muls	r3, r2
 80adb0a:	68ba      	ldr	r2, [r7, #8]
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  TF_LITE_ENSURE_EQ(context, output->type, kTfLiteFloat32);
 80adb0c:	5cd4      	ldrb	r4, [r2, r3]
 80adb0e:	18d1      	adds	r1, r2, r3
 80adb10:	2c01      	cmp	r4, #1
 80adb12:	d00c      	beq.n	80adb2e <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c6>
 80adb14:	4b35      	ldr	r3, [pc, #212]	; (80adbec <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x584>)
 80adb16:	9503      	str	r5, [sp, #12]
 80adb18:	9301      	str	r3, [sp, #4]
 80adb1a:	4b41      	ldr	r3, [pc, #260]	; (80adc20 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5b8>)
 80adb1c:	9402      	str	r4, [sp, #8]
 80adb1e:	9300      	str	r3, [sp, #0]
 80adb20:	697c      	ldr	r4, [r7, #20]
 80adb22:	f44f 73e1 	mov.w	r3, #450	; 0x1c2
 80adb26:	4a2d      	ldr	r2, [pc, #180]	; (80adbdc <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x574>)
 80adb28:	493e      	ldr	r1, [pc, #248]	; (80adc24 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5bc>)
 80adb2a:	4638      	mov	r0, r7
 80adb2c:	e5b1      	b.n	80ad692 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2a>
 80adb2e:	688b      	ldr	r3, [r1, #8]
#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/c_api_internal.h"

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
 80adb30:	681a      	ldr	r2, [r3, #0]
  TF_LITE_ENSURE_EQ(context, NumDimensions(output), 2);
 80adb32:	2a02      	cmp	r2, #2
 80adb34:	d00a      	beq.n	80adb4c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4e4>
 80adb36:	2302      	movs	r3, #2
 80adb38:	9303      	str	r3, [sp, #12]
 80adb3a:	4b32      	ldr	r3, [pc, #200]	; (80adc04 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x59c>)
 80adb3c:	9202      	str	r2, [sp, #8]
 80adb3e:	9301      	str	r3, [sp, #4]
 80adb40:	4b39      	ldr	r3, [pc, #228]	; (80adc28 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5c0>)
 80adb42:	9300      	str	r3, [sp, #0]
 80adb44:	697d      	ldr	r5, [r7, #20]
 80adb46:	f240 13c3 	movw	r3, #451	; 0x1c3
 80adb4a:	e01a      	b.n	80adb82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51a>
  TF_LITE_ENSURE_EQ(context, output->dims->data[0], batch_size);
 80adb4c:	685a      	ldr	r2, [r3, #4]
 80adb4e:	4590      	cmp	r8, r2
 80adb50:	d00a      	beq.n	80adb68 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x500>
 80adb52:	4b24      	ldr	r3, [pc, #144]	; (80adbe4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x57c>)
 80adb54:	f8cd 800c 	str.w	r8, [sp, #12]
 80adb58:	9301      	str	r3, [sp, #4]
 80adb5a:	4b34      	ldr	r3, [pc, #208]	; (80adc2c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5c4>)
 80adb5c:	9202      	str	r2, [sp, #8]
 80adb5e:	9300      	str	r3, [sp, #0]
 80adb60:	697d      	ldr	r5, [r7, #20]
 80adb62:	f44f 73e2 	mov.w	r3, #452	; 0x1c4
 80adb66:	e00c      	b.n	80adb82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51a>
  TF_LITE_ENSURE_EQ(context, output->dims->data[1], num_units);
 80adb68:	689b      	ldr	r3, [r3, #8]
 80adb6a:	459b      	cmp	fp, r3
 80adb6c:	d00e      	beq.n	80adb8c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x524>
 80adb6e:	9302      	str	r3, [sp, #8]
 80adb70:	4b2f      	ldr	r3, [pc, #188]	; (80adc30 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5c8>)
 80adb72:	f8cd b00c 	str.w	fp, [sp, #12]
 80adb76:	9301      	str	r3, [sp, #4]
 80adb78:	4b2e      	ldr	r3, [pc, #184]	; (80adc34 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5cc>)
 80adb7a:	9300      	str	r3, [sp, #0]
 80adb7c:	f240 13c5 	movw	r3, #453	; 0x1c5
 80adb80:	697d      	ldr	r5, [r7, #20]
 80adb82:	4a16      	ldr	r2, [pc, #88]	; (80adbdc <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x574>)
 80adb84:	4927      	ldr	r1, [pc, #156]	; (80adc24 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5bc>)
 80adb86:	4638      	mov	r0, r7
 80adb88:	47a8      	blx	r5
 80adb8a:	e583      	b.n	80ad694 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2c>

  return kTfLiteOk;
 80adb8c:	2000      	movs	r0, #0
 80adb8e:	e01a      	b.n	80adbc6 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x55e>
                        NumElements(scratch_float_weights_time),
                        weights_time->params.scale,
                        GetTensorData<float>(scratch_float_weights_time));
  } else {
    // Validate Input Tensor dtypes:
    TF_LITE_ENSURE_EQ(context, weights_feature->type, kTfLiteFloat32);
 80adb90:	2c01      	cmp	r4, #1
 80adb92:	d00a      	beq.n	80adbaa <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x542>
 80adb94:	4b15      	ldr	r3, [pc, #84]	; (80adbec <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x584>)
 80adb96:	2501      	movs	r5, #1
 80adb98:	9301      	str	r3, [sp, #4]
 80adb9a:	4b27      	ldr	r3, [pc, #156]	; (80adc38 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5d0>)
 80adb9c:	9503      	str	r5, [sp, #12]
 80adb9e:	9300      	str	r3, [sp, #0]
 80adba0:	9402      	str	r4, [sp, #8]
 80adba2:	697c      	ldr	r4, [r7, #20]
 80adba4:	f44f 73da 	mov.w	r3, #436	; 0x1b4
 80adba8:	e7bd      	b.n	80adb26 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4be>
    TF_LITE_ENSURE_EQ(context, weights_time->type, kTfLiteFloat32);
 80adbaa:	f813 300c 	ldrb.w	r3, [r3, ip]
 80adbae:	2b01      	cmp	r3, #1
 80adbb0:	d098      	beq.n	80adae4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x47c>
 80adbb2:	9302      	str	r3, [sp, #8]
 80adbb4:	4b0d      	ldr	r3, [pc, #52]	; (80adbec <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x584>)
 80adbb6:	9403      	str	r4, [sp, #12]
 80adbb8:	9301      	str	r3, [sp, #4]
 80adbba:	4b20      	ldr	r3, [pc, #128]	; (80adc3c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5d4>)
 80adbbc:	9300      	str	r3, [sp, #0]
 80adbbe:	697d      	ldr	r5, [r7, #20]
 80adbc0:	f240 13b5 	movw	r3, #437	; 0x1b5
 80adbc4:	e7dd      	b.n	80adb82 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51a>
  TF_LITE_ENSURE_EQ(context, NumDimensions(output), 2);
  TF_LITE_ENSURE_EQ(context, output->dims->data[0], batch_size);
  TF_LITE_ENSURE_EQ(context, output->dims->data[1], num_units);

  return kTfLiteOk;
}
 80adbc6:	b007      	add	sp, #28
 80adbc8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80adbcc:	080b6f45 	.word	0x080b6f45
 80adbd0:	080b6caa 	.word	0x080b6caa
 80adbd4:	080b6f8d 	.word	0x080b6f8d
 80adbd8:	080b6fa5 	.word	0x080b6fa5
 80adbdc:	080b6ce0 	.word	0x080b6ce0
 80adbe0:	080b5db0 	.word	0x080b5db0
 80adbe4:	080b6e71 	.word	0x080b6e71
 80adbe8:	080b7003 	.word	0x080b7003
 80adbec:	080b644f 	.word	0x080b644f
 80adbf0:	080b702a 	.word	0x080b702a
 80adbf4:	080b75ad 	.word	0x080b75ad
 80adbf8:	080b7048 	.word	0x080b7048
 80adbfc:	080b706f 	.word	0x080b706f
 80adc00:	080b7096 	.word	0x080b7096
 80adc04:	080b6590 	.word	0x080b6590
 80adc08:	080b70b7 	.word	0x080b70b7
 80adc0c:	080b6eaa 	.word	0x080b6eaa
 80adc10:	080b70e1 	.word	0x080b70e1
 80adc14:	080b710b 	.word	0x080b710b
 80adc18:	080b7117 	.word	0x080b7117
 80adc1c:	080b5deb 	.word	0x080b5deb
 80adc20:	080b5c27 	.word	0x080b5c27
 80adc24:	080b5be0 	.word	0x080b5be0
 80adc28:	080b7157 	.word	0x080b7157
 80adc2c:	080b716d 	.word	0x080b716d
 80adc30:	080b6e11 	.word	0x080b6e11
 80adc34:	080b7183 	.word	0x080b7183
 80adc38:	080b7141 	.word	0x080b7141
 80adc3c:	080b70a4 	.word	0x080b70a4

080adc40 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80adc40:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80adc44:	680f      	ldr	r7, [r1, #0]
  auto* params = reinterpret_cast<TfLiteSVDFParams*>(node->builtin_data);
 80adc46:	694b      	ldr	r3, [r1, #20]
  TF_LITE_ENSURE_EQ(context, output->dims->data[1], num_units);

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80adc48:	b09b      	sub	sp, #108	; 0x6c
  auto* params = reinterpret_cast<TfLiteSVDFParams*>(node->builtin_data);
 80adc4a:	930a      	str	r3, [sp, #40]	; 0x28
}

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
 80adc4c:	693b      	ldr	r3, [r7, #16]
 80adc4e:	6886      	ldr	r6, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80adc50:	f04f 0e38 	mov.w	lr, #56	; 0x38

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
 80adc54:	f1b3 3fff 	cmp.w	r3, #4294967295	; 0xffffffff
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80adc58:	bf14      	ite	ne
 80adc5a:	fb0e 6303 	mlane	r3, lr, r3, r6
  }
  return nullptr;
 80adc5e:	2300      	moveq	r3, #0
 80adc60:	930e      	str	r3, [sp, #56]	; 0x38
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);

  // TODO(kreeger): Use input tensor as variable until scratch tensor allocation
  // has been implemented (cl/263032056)
  // TfLiteTensor* scratch = GetTemporary(context, node, /*index=*/0);
  TfLiteTensor* scratch = &context->tensors[node->inputs->data[5]];
 80adc62:	69bb      	ldr	r3, [r7, #24]
  TF_LITE_ENSURE_EQ(context, output->dims->data[1], num_units);

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80adc64:	4604      	mov	r4, r0
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);

  // TODO(kreeger): Use input tensor as variable until scratch tensor allocation
  // has been implemented (cl/263032056)
  // TfLiteTensor* scratch = GetTemporary(context, node, /*index=*/0);
  TfLiteTensor* scratch = &context->tensors[node->inputs->data[5]];
 80adc66:	fb0e 6303 	mla	r3, lr, r3, r6
 80adc6a:	9310      	str	r3, [sp, #64]	; 0x40

  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];
 80adc6c:	697b      	ldr	r3, [r7, #20]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80adc6e:	68b8      	ldr	r0, [r7, #8]
 80adc70:	fb0e 6b03 	mla	fp, lr, r3, r6
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80adc74:	684b      	ldr	r3, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80adc76:	fb0e f000 	mul.w	r0, lr, r0
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80adc7a:	685b      	ldr	r3, [r3, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80adc7c:	1832      	adds	r2, r6, r0
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80adc7e:	fb0e 6303 	mla	r3, lr, r3, r6
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (weights_feature->type) {
 80adc82:	5c30      	ldrb	r0, [r6, r0]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80adc84:	687d      	ldr	r5, [r7, #4]
 80adc86:	2803      	cmp	r0, #3
 80adc88:	fb0e 6505 	mla	r5, lr, r5, r6
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80adc8c:	930f      	str	r3, [sp, #60]	; 0x3c
 80adc8e:	f000 80b1 	beq.w	80addf4 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x1b4>
 80adc92:	2809      	cmp	r0, #9
 80adc94:	f000 80ae 	beq.w	80addf4 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x1b4>
 80adc98:	2801      	cmp	r0, #1
 80adc9a:	f040 818c 	bne.w	80adfb6 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x376>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80adc9e:	68fb      	ldr	r3, [r7, #12]
 80adca0:	2138      	movs	r1, #56	; 0x38
 80adca2:	fb01 6303 	mla	r3, r1, r3, r6
 80adca6:	930c      	str	r3, [sp, #48]	; 0x30
                          const TfLiteTensor* weights_time,
                          const TfLiteTensor* bias,
                          const TfLiteSVDFParams* params, TfLiteTensor* scratch,
                          TfLiteTensor* activation_state,
                          TfLiteTensor* output) {
  const int rank = params->rank;
 80adca8:	9b0a      	ldr	r3, [sp, #40]	; 0x28
  const int memory_size = weights_time->dims->data[1];

  // Clear the activation (activation_state's leftmost column).
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in leftmost column and make sure it passes.
  for (int b = 0; b < batch_size; ++b) {
 80adcaa:	f04f 0e00 	mov.w	lr, #0
                          const TfLiteTensor* weights_time,
                          const TfLiteTensor* bias,
                          const TfLiteSVDFParams* params, TfLiteTensor* scratch,
                          TfLiteTensor* activation_state,
                          TfLiteTensor* output) {
  const int rank = params->rank;
 80adcae:	681b      	ldr	r3, [r3, #0]
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0f;
 80adcb0:	f04f 0900 	mov.w	r9, #0
                          const TfLiteTensor* weights_time,
                          const TfLiteTensor* bias,
                          const TfLiteSVDFParams* params, TfLiteTensor* scratch,
                          TfLiteTensor* activation_state,
                          TfLiteTensor* output) {
  const int rank = params->rank;
 80adcb4:	930d      	str	r3, [sp, #52]	; 0x34
  const int batch_size = input->dims->data[0];
 80adcb6:	68ab      	ldr	r3, [r5, #8]
 80adcb8:	6859      	ldr	r1, [r3, #4]
  const int input_size = input->dims->data[1];
 80adcba:	689b      	ldr	r3, [r3, #8]
                          const TfLiteTensor* bias,
                          const TfLiteSVDFParams* params, TfLiteTensor* scratch,
                          TfLiteTensor* activation_state,
                          TfLiteTensor* output) {
  const int rank = params->rank;
  const int batch_size = input->dims->data[0];
 80adcbc:	9108      	str	r1, [sp, #32]
  const int input_size = input->dims->data[1];
 80adcbe:	9309      	str	r3, [sp, #36]	; 0x24
  const int num_filters = weights_feature->dims->data[0];
 80adcc0:	6893      	ldr	r3, [r2, #8]
 80adcc2:	685c      	ldr	r4, [r3, #4]
  const int num_units = num_filters / rank;
 80adcc4:	9b0d      	ldr	r3, [sp, #52]	; 0x34
 80adcc6:	fb94 f3f3 	sdiv	r3, r4, r3
 80adcca:	9312      	str	r3, [sp, #72]	; 0x48
  const int memory_size = weights_time->dims->data[1];
 80adccc:	9b0c      	ldr	r3, [sp, #48]	; 0x30
 80adcce:	689b      	ldr	r3, [r3, #8]
 80adcd0:	f8d3 a008 	ldr.w	sl, [r3, #8]
 80adcd4:	fb0a f704 	mul.w	r7, sl, r4
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0f;
 80adcd8:	f10a 4380 	add.w	r3, sl, #1073741824	; 0x40000000
 80adcdc:	3b01      	subs	r3, #1
 80adcde:	009b      	lsls	r3, r3, #2
 80adce0:	00bf      	lsls	r7, r7, #2
 80adce2:	eba3 068a 	sub.w	r6, r3, sl, lsl #2
 80adce6:	f103 0804 	add.w	r8, r3, #4
  const int memory_size = weights_time->dims->data[1];

  // Clear the activation (activation_state's leftmost column).
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in leftmost column and make sure it passes.
  for (int b = 0; b < batch_size; ++b) {
 80adcea:	9908      	ldr	r1, [sp, #32]
 80adcec:	4571      	cmp	r1, lr
 80adcee:	dd15      	ble.n	80add1c <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xdc>
 80adcf0:	f1bb 0f00 	cmp.w	fp, #0
 80adcf4:	d002      	beq.n	80adcfc <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xbc>
 80adcf6:	f8db 0004 	ldr.w	r0, [fp, #4]
 80adcfa:	e000      	b.n	80adcfe <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xbe>
 80adcfc:	4658      	mov	r0, fp
 80adcfe:	2100      	movs	r1, #0
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
 80add00:	468c      	mov	ip, r1
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0f;
 80add02:	4430      	add	r0, r6
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in leftmost column and make sure it passes.
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
 80add04:	4564      	cmp	r4, ip
 80add06:	4441      	add	r1, r8
 80add08:	dd04      	ble.n	80add14 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xd4>
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0f;
 80add0a:	f840 9001 	str.w	r9, [r0, r1]
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in leftmost column and make sure it passes.
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
 80add0e:	f10c 0c01 	add.w	ip, ip, #1
 80add12:	e7f7      	b.n	80add04 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xc4>
  const int memory_size = weights_time->dims->data[1];

  // Clear the activation (activation_state's leftmost column).
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in leftmost column and make sure it passes.
  for (int b = 0; b < batch_size; ++b) {
 80add14:	f10e 0e01 	add.w	lr, lr, #1
 80add18:	443e      	add	r6, r7
 80add1a:	e7e6      	b.n	80adcea <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xaa>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80add1c:	6852      	ldr	r2, [r2, #4]
 80add1e:	9213      	str	r2, [sp, #76]	; 0x4c
 80add20:	686a      	ldr	r2, [r5, #4]
 80add22:	9214      	str	r2, [sp, #80]	; 0x50

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80add24:	f1bb 0f00 	cmp.w	fp, #0
 80add28:	d002      	beq.n	80add30 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xf0>
 80add2a:	f8db 2004 	ldr.w	r2, [fp, #4]
 80add2e:	e000      	b.n	80add32 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xf2>
 80add30:	465a      	mov	r2, fp
      const float* vector_in_batch = vector + i * input_size;
      for (int k = 0; k < input_size; ++k) {
        dot_prod += *matrix_ptr++ * *vector_in_batch++;
      }
      *result_in_batch += dot_prod;
      result_in_batch += memory_size;
 80add32:	2600      	movs	r6, #0
  // Perform batched matrix vector multiply accumulate operation:
  const float* matrix = GetTensorData<float>(weights_feature);
  const float* vector = GetTensorData<float>(input);
  float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
  float* result_in_batch = result;
  for (int i = 0; i < batch_size; ++i) {
 80add34:	46b0      	mov	r8, r6
  // stride equal to memory_size.

  // Perform batched matrix vector multiply accumulate operation:
  const float* matrix = GetTensorData<float>(weights_feature);
  const float* vector = GetTensorData<float>(input);
  float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
 80add36:	441a      	add	r2, r3
      const float* vector_in_batch = vector + i * input_size;
      for (int k = 0; k < input_size; ++k) {
        dot_prod += *matrix_ptr++ * *vector_in_batch++;
      }
      *result_in_batch += dot_prod;
      result_in_batch += memory_size;
 80add38:	3304      	adds	r3, #4
 80add3a:	9311      	str	r3, [sp, #68]	; 0x44
 80add3c:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80add3e:	ea24 79e4 	bic.w	r9, r4, r4, asr #31
 80add42:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
 80add46:	009b      	lsls	r3, r3, #2
 80add48:	9317      	str	r3, [sp, #92]	; 0x5c
 80add4a:	9b11      	ldr	r3, [sp, #68]	; 0x44
  // stride equal to memory_size.

  // Perform batched matrix vector multiply accumulate operation:
  const float* matrix = GetTensorData<float>(weights_feature);
  const float* vector = GetTensorData<float>(input);
  float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
 80add4c:	920b      	str	r2, [sp, #44]	; 0x2c
 80add4e:	fb03 f309 	mul.w	r3, r3, r9
 80add52:	9316      	str	r3, [sp, #88]	; 0x58
  float* result_in_batch = result;
  for (int i = 0; i < batch_size; ++i) {
 80add54:	9b08      	ldr	r3, [sp, #32]
 80add56:	4543      	cmp	r3, r8
 80add58:	dd38      	ble.n	80addcc <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x18c>
    const float* matrix_ptr = matrix;
    for (int j = 0; j < num_filters; ++j) {
      float dot_prod = 0.0f;
      const float* vector_in_batch = vector + i * input_size;
 80add5a:	9b14      	ldr	r3, [sp, #80]	; 0x50
 80add5c:	9a13      	ldr	r2, [sp, #76]	; 0x4c
 80add5e:	eb03 0386 	add.w	r3, r3, r6, lsl #2
 80add62:	9f0b      	ldr	r7, [sp, #44]	; 0x2c
 80add64:	9315      	str	r3, [sp, #84]	; 0x54
 80add66:	f04f 0900 	mov.w	r9, #0
  const float* vector = GetTensorData<float>(input);
  float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
  float* result_in_batch = result;
  for (int i = 0; i < batch_size; ++i) {
    const float* matrix_ptr = matrix;
    for (int j = 0; j < num_filters; ++j) {
 80add6a:	454c      	cmp	r4, r9
 80add6c:	dd25      	ble.n	80addba <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x17a>
 80add6e:	2500      	movs	r5, #0
 80add70:	2300      	movs	r3, #0
      float dot_prod = 0.0f;
      const float* vector_in_batch = vector + i * input_size;
      for (int k = 0; k < input_size; ++k) {
 80add72:	9909      	ldr	r1, [sp, #36]	; 0x24
 80add74:	42a9      	cmp	r1, r5
 80add76:	dd11      	ble.n	80add9c <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x15c>
 80add78:	9319      	str	r3, [sp, #100]	; 0x64
        dot_prod += *matrix_ptr++ * *vector_in_batch++;
 80add7a:	9b15      	ldr	r3, [sp, #84]	; 0x54
 80add7c:	f852 0025 	ldr.w	r0, [r2, r5, lsl #2]
 80add80:	f853 1025 	ldr.w	r1, [r3, r5, lsl #2]
 80add84:	9218      	str	r2, [sp, #96]	; 0x60
 80add86:	f005 fc31 	bl	80b35ec <__aeabi_fmul>
 80add8a:	9b19      	ldr	r3, [sp, #100]	; 0x64
 80add8c:	4601      	mov	r1, r0
 80add8e:	4618      	mov	r0, r3
 80add90:	f005 fb24 	bl	80b33dc <__addsf3>
  for (int i = 0; i < batch_size; ++i) {
    const float* matrix_ptr = matrix;
    for (int j = 0; j < num_filters; ++j) {
      float dot_prod = 0.0f;
      const float* vector_in_batch = vector + i * input_size;
      for (int k = 0; k < input_size; ++k) {
 80add94:	3501      	adds	r5, #1
        dot_prod += *matrix_ptr++ * *vector_in_batch++;
 80add96:	4603      	mov	r3, r0
 80add98:	9a18      	ldr	r2, [sp, #96]	; 0x60
 80add9a:	e7ea      	b.n	80add72 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x132>
 80add9c:	9917      	ldr	r1, [sp, #92]	; 0x5c
      }
      *result_in_batch += dot_prod;
 80add9e:	6838      	ldr	r0, [r7, #0]
 80adda0:	440a      	add	r2, r1
 80adda2:	4619      	mov	r1, r3
 80adda4:	9218      	str	r2, [sp, #96]	; 0x60
 80adda6:	9219      	str	r2, [sp, #100]	; 0x64
 80adda8:	f005 fb18 	bl	80b33dc <__addsf3>
      result_in_batch += memory_size;
 80addac:	9b11      	ldr	r3, [sp, #68]	; 0x44
      float dot_prod = 0.0f;
      const float* vector_in_batch = vector + i * input_size;
      for (int k = 0; k < input_size; ++k) {
        dot_prod += *matrix_ptr++ * *vector_in_batch++;
      }
      *result_in_batch += dot_prod;
 80addae:	6038      	str	r0, [r7, #0]
  const float* vector = GetTensorData<float>(input);
  float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
  float* result_in_batch = result;
  for (int i = 0; i < batch_size; ++i) {
    const float* matrix_ptr = matrix;
    for (int j = 0; j < num_filters; ++j) {
 80addb0:	f109 0901 	add.w	r9, r9, #1
      const float* vector_in_batch = vector + i * input_size;
      for (int k = 0; k < input_size; ++k) {
        dot_prod += *matrix_ptr++ * *vector_in_batch++;
      }
      *result_in_batch += dot_prod;
      result_in_batch += memory_size;
 80addb4:	441f      	add	r7, r3
 80addb6:	9a18      	ldr	r2, [sp, #96]	; 0x60
 80addb8:	e7d7      	b.n	80add6a <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x12a>
 80addba:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80addbc:	9a16      	ldr	r2, [sp, #88]	; 0x58
  // Perform batched matrix vector multiply accumulate operation:
  const float* matrix = GetTensorData<float>(weights_feature);
  const float* vector = GetTensorData<float>(input);
  float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
  float* result_in_batch = result;
  for (int i = 0; i < batch_size; ++i) {
 80addbe:	f108 0801 	add.w	r8, r8, #1
 80addc2:	4413      	add	r3, r2
 80addc4:	930b      	str	r3, [sp, #44]	; 0x2c
 80addc6:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80addc8:	441e      	add	r6, r3
 80addca:	e7c3      	b.n	80add54 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x114>
    }
  }

  ApplyTimeWeightsBiasAndActivation(
      batch_size, memory_size, num_filters, num_units, rank, weights_time, bias,
      params->activation, activation_state, scratch, output);
 80addcc:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80addce:	f8cd b010 	str.w	fp, [sp, #16]
 80addd2:	9306      	str	r3, [sp, #24]
 80addd4:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80addd6:	4622      	mov	r2, r4
 80addd8:	9305      	str	r3, [sp, #20]
 80addda:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80adddc:	4651      	mov	r1, sl
 80addde:	791b      	ldrb	r3, [r3, #4]
 80adde0:	9808      	ldr	r0, [sp, #32]
 80adde2:	9303      	str	r3, [sp, #12]
 80adde4:	9b0e      	ldr	r3, [sp, #56]	; 0x38
 80adde6:	9302      	str	r3, [sp, #8]
 80adde8:	9b0c      	ldr	r3, [sp, #48]	; 0x30
 80addea:	9301      	str	r3, [sp, #4]
 80addec:	9b0d      	ldr	r3, [sp, #52]	; 0x34
 80addee:	9300      	str	r3, [sp, #0]
 80addf0:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80addf2:	e0dc      	b.n	80adfae <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x36e>
 80addf4:	68cf      	ldr	r7, [r1, #12]
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
}
inline TfLiteTensor* GetTemporary(TfLiteContext* context, TfLiteNode* node,
                                  int index) {
  return &context->tensors[flatbuffers::EndianScalar(
      node->temporaries->data[index])];
 80addf6:	2138      	movs	r1, #56	; 0x38
 80addf8:	68bb      	ldr	r3, [r7, #8]
 80addfa:	68fc      	ldr	r4, [r7, #12]
 80addfc:	693f      	ldr	r7, [r7, #16]
 80addfe:	fb01 6303 	mla	r3, r1, r3, r6
 80ade02:	fb01 6404 	mla	r4, r1, r4, r6
 80ade06:	fb01 6107 	mla	r1, r1, r7, r6
 80ade0a:	9112      	str	r1, [sp, #72]	; 0x48
    const TfLiteTensor* weights_feature, const TfLiteTensor* weights_time,
    const TfLiteTensor* bias, const TfLiteSVDFParams* params,
    TfLiteTensor* scratch, TfLiteTensor* scaling_factors,
    TfLiteTensor* input_quantized, TfLiteTensor* activation_state,
    TfLiteTensor* output) {
  const int rank = params->rank;
 80ade0c:	990a      	ldr	r1, [sp, #40]	; 0x28
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80ade0e:	f8d5 8004 	ldr.w	r8, [r5, #4]
 80ade12:	6809      	ldr	r1, [r1, #0]
 80ade14:	9111      	str	r1, [sp, #68]	; 0x44
  const int batch_size = input->dims->data[0];
 80ade16:	68a9      	ldr	r1, [r5, #8]
 80ade18:	684e      	ldr	r6, [r1, #4]
  const int input_size = input->dims->data[1];
 80ade1a:	6889      	ldr	r1, [r1, #8]
    const TfLiteTensor* bias, const TfLiteSVDFParams* params,
    TfLiteTensor* scratch, TfLiteTensor* scaling_factors,
    TfLiteTensor* input_quantized, TfLiteTensor* activation_state,
    TfLiteTensor* output) {
  const int rank = params->rank;
  const int batch_size = input->dims->data[0];
 80ade1c:	9609      	str	r6, [sp, #36]	; 0x24
  const int input_size = input->dims->data[1];
 80ade1e:	910b      	str	r1, [sp, #44]	; 0x2c
  const int num_filters = weights_feature->dims->data[0];
 80ade20:	6891      	ldr	r1, [r2, #8]
  const int num_units = num_filters / rank;
 80ade22:	9e11      	ldr	r6, [sp, #68]	; 0x44
    TfLiteTensor* input_quantized, TfLiteTensor* activation_state,
    TfLiteTensor* output) {
  const int rank = params->rank;
  const int batch_size = input->dims->data[0];
  const int input_size = input->dims->data[1];
  const int num_filters = weights_feature->dims->data[0];
 80ade24:	6849      	ldr	r1, [r1, #4]
 80ade26:	9108      	str	r1, [sp, #32]
  const int num_units = num_filters / rank;
 80ade28:	fb91 f1f6 	sdiv	r1, r1, r6
 80ade2c:	9113      	str	r1, [sp, #76]	; 0x4c
  const int memory_size = weights_time->dims->data[1];
 80ade2e:	9912      	ldr	r1, [sp, #72]	; 0x48
 80ade30:	6889      	ldr	r1, [r1, #8]
 80ade32:	6889      	ldr	r1, [r1, #8]
 80ade34:	910c      	str	r1, [sp, #48]	; 0x30
 80ade36:	6851      	ldr	r1, [r2, #4]
 80ade38:	9115      	str	r1, [sp, #84]	; 0x54

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80ade3a:	b113      	cbz	r3, 80ade42 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x202>
 80ade3c:	f8d3 a004 	ldr.w	sl, [r3, #4]
 80ade40:	e000      	b.n	80ade44 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x204>
 80ade42:	469a      	mov	sl, r3
 80ade44:	b114      	cbz	r4, 80ade4c <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x20c>
 80ade46:	6863      	ldr	r3, [r4, #4]
 80ade48:	930d      	str	r3, [sp, #52]	; 0x34
 80ade4a:	e000      	b.n	80ade4e <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x20e>
 80ade4c:	940d      	str	r4, [sp, #52]	; 0x34

  // Initialize the pointer to storage for scaling factors.
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors);

  // Initialize the weights scale.
  const float weights_feature_scale = weights_feature->params.scale;
 80ade4e:	68d3      	ldr	r3, [r2, #12]
 80ade50:	9a08      	ldr	r2, [sp, #32]
 80ade52:	9314      	str	r3, [sp, #80]	; 0x50
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0;
 80ade54:	9b0c      	ldr	r3, [sp, #48]	; 0x30

  // Clear the activation (activation_state's leftmost column).
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in the leftmost column and make sure it passes.
  // TODO(kreeger): Use a port of tensor_utils when ready (b/140272187).
  for (int b = 0; b < batch_size; ++b) {
 80ade56:	2600      	movs	r6, #0
 80ade58:	fb03 f002 	mul.w	r0, r3, r2
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0;
 80ade5c:	f103 4480 	add.w	r4, r3, #1073741824	; 0x40000000
 80ade60:	3c01      	subs	r4, #1
 80ade62:	00a4      	lsls	r4, r4, #2
 80ade64:	0080      	lsls	r0, r0, #2
 80ade66:	eba4 0183 	sub.w	r1, r4, r3, lsl #2
 80ade6a:	f104 0e04 	add.w	lr, r4, #4
 80ade6e:	f04f 0c00 	mov.w	ip, #0

  // Clear the activation (activation_state's leftmost column).
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in the leftmost column and make sure it passes.
  // TODO(kreeger): Use a port of tensor_utils when ready (b/140272187).
  for (int b = 0; b < batch_size; ++b) {
 80ade72:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80ade74:	42b3      	cmp	r3, r6
 80ade76:	dd15      	ble.n	80adea4 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x264>
 80ade78:	f1bb 0f00 	cmp.w	fp, #0
 80ade7c:	d002      	beq.n	80ade84 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x244>
 80ade7e:	f8db 2004 	ldr.w	r2, [fp, #4]
 80ade82:	e000      	b.n	80ade86 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x246>
 80ade84:	465a      	mov	r2, fp
 80ade86:	2300      	movs	r3, #0
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
 80ade88:	461f      	mov	r7, r3
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0;
 80ade8a:	eb02 0901 	add.w	r9, r2, r1
  // values in the leftmost column and make sure it passes.
  // TODO(kreeger): Use a port of tensor_utils when ready (b/140272187).
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
 80ade8e:	9a08      	ldr	r2, [sp, #32]
 80ade90:	4473      	add	r3, lr
 80ade92:	42ba      	cmp	r2, r7
 80ade94:	dd03      	ble.n	80ade9e <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x25e>
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0;
 80ade96:	f849 c003 	str.w	ip, [r9, r3]
  // values in the leftmost column and make sure it passes.
  // TODO(kreeger): Use a port of tensor_utils when ready (b/140272187).
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
 80ade9a:	3701      	adds	r7, #1
 80ade9c:	e7f7      	b.n	80ade8e <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x24e>

  // Clear the activation (activation_state's leftmost column).
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in the leftmost column and make sure it passes.
  // TODO(kreeger): Use a port of tensor_utils when ready (b/140272187).
  for (int b = 0; b < batch_size; ++b) {
 80ade9e:	3601      	adds	r6, #1
 80adea0:	4401      	add	r1, r0
 80adea2:	e7e6      	b.n	80ade72 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x232>
    }
  }

  // Determine if input pointer batch is a zero based vector:
  bool is_zero_vector = true;
  for (int i = 0; i < batch_size * input_size && is_zero_vector; ++i) {
 80adea4:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80adea6:	9a09      	ldr	r2, [sp, #36]	; 0x24
 80adea8:	2700      	movs	r7, #0
 80adeaa:	fb03 f902 	mul.w	r9, r3, r2
 80adeae:	2601      	movs	r6, #1
 80adeb0:	45b9      	cmp	r9, r7
 80adeb2:	dd0c      	ble.n	80adece <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x28e>
 80adeb4:	b16e      	cbz	r6, 80aded2 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x292>
    if (input_ptr_batch[i] != 0.0f) {
 80adeb6:	2100      	movs	r1, #0
 80adeb8:	f858 0027 	ldr.w	r0, [r8, r7, lsl #2]
 80adebc:	f04f 0601 	mov.w	r6, #1
 80adec0:	f005 fd28 	bl	80b3914 <__aeabi_fcmpeq>
 80adec4:	b900      	cbnz	r0, 80adec8 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x288>
 80adec6:	4606      	mov	r6, r0
 80adec8:	b2f6      	uxtb	r6, r6
    }
  }

  // Determine if input pointer batch is a zero based vector:
  bool is_zero_vector = true;
  for (int i = 0; i < batch_size * input_size && is_zero_vector; ++i) {
 80adeca:	3701      	adds	r7, #1
 80adecc:	e7f0      	b.n	80adeb0 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x270>
    if (input_ptr_batch[i] != 0.0f) {
      is_zero_vector = false;
    }
  }

  if (!is_zero_vector) {
 80adece:	2e00      	cmp	r6, #0
 80aded0:	d15a      	bne.n	80adf88 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x348>
    SignedSymmetricPerChannelQuantize(input_ptr_batch, input->dims, 0,
                                      quantized_input_ptr_batch,
                                      scaling_factors_ptr);
 80aded2:	9b0d      	ldr	r3, [sp, #52]	; 0x34
 80aded4:	2200      	movs	r2, #0
 80aded6:	9300      	str	r3, [sp, #0]
 80aded8:	68a9      	ldr	r1, [r5, #8]
 80adeda:	4653      	mov	r3, sl
 80adedc:	4640      	mov	r0, r8
 80adede:	f7f4 f99d 	bl	80a221c <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf>
 80adee2:	9b0d      	ldr	r3, [sp, #52]	; 0x34

    // Quantize input from float to int8.
    for (int b = 0; b < batch_size; ++b) {
 80adee4:	2500      	movs	r5, #0
 80adee6:	1f1e      	subs	r6, r3, #4
 80adee8:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80adeea:	42ab      	cmp	r3, r5
 80adeec:	dd07      	ble.n	80adefe <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x2be>
      scaling_factors_ptr[b] *= weights_feature_scale;
 80adeee:	f856 0f04 	ldr.w	r0, [r6, #4]!
 80adef2:	9914      	ldr	r1, [sp, #80]	; 0x50
 80adef4:	f005 fb7a 	bl	80b35ec <__aeabi_fmul>
    SignedSymmetricPerChannelQuantize(input_ptr_batch, input->dims, 0,
                                      quantized_input_ptr_batch,
                                      scaling_factors_ptr);

    // Quantize input from float to int8.
    for (int b = 0; b < batch_size; ++b) {
 80adef8:	3501      	adds	r5, #1
      scaling_factors_ptr[b] *= weights_feature_scale;
 80adefa:	6030      	str	r0, [r6, #0]
 80adefc:	e7f4      	b.n	80adee8 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x2a8>
 80adefe:	f1bb 0f00 	cmp.w	fp, #0
 80adf02:	d002      	beq.n	80adf0a <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x2ca>
 80adf04:	f8db 5004 	ldr.w	r5, [fp, #4]
 80adf08:	e000      	b.n	80adf0c <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x2cc>
 80adf0a:	465d      	mov	r5, fp
 80adf0c:	9b08      	ldr	r3, [sp, #32]
    // Compute conv1d(inputs, weights_feature).
    // The rightmost column of activation_state is used to save the current
    // cycle activation. This is achieved by starting at
    // GetTensorData<float>(activation_state)[memory_size - 1] and having the
    // stride equal to memory_size. (Matrix batch vector multiply accumulate)
    float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
 80adf0e:	4425      	add	r5, r4
 80adf10:	ea23 78e3 	bic.w	r8, r3, r3, asr #31
         ++i, quantized_input_ptr_batch += input_size) {
      const float batch_scaling_factor = scaling_factors_ptr[i];

      // Get the address of the first row:
      const int8_t* row_ptr = weights_feature_ptr;
      for (int j = 0; j < num_filters; ++j, result += memory_size) {
 80adf14:	3404      	adds	r4, #4
 80adf16:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80adf18:	fb04 f808 	mul.w	r8, r4, r8
 80adf1c:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
    // The rightmost column of activation_state is used to save the current
    // cycle activation. This is achieved by starting at
    // GetTensorData<float>(activation_state)[memory_size - 1] and having the
    // stride equal to memory_size. (Matrix batch vector multiply accumulate)
    float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
    for (int i = 0; i < batch_size;
 80adf20:	2600      	movs	r6, #0
 80adf22:	9316      	str	r3, [sp, #88]	; 0x58
 80adf24:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80adf26:	42b3      	cmp	r3, r6
 80adf28:	dd2e      	ble.n	80adf88 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x348>
         ++i, quantized_input_ptr_batch += input_size) {
      const float batch_scaling_factor = scaling_factors_ptr[i];
 80adf2a:	462f      	mov	r7, r5
 80adf2c:	9b0d      	ldr	r3, [sp, #52]	; 0x34

      // Get the address of the first row:
      const int8_t* row_ptr = weights_feature_ptr;
      for (int j = 0; j < num_filters; ++j, result += memory_size) {
 80adf2e:	f04f 0900 	mov.w	r9, #0
    // GetTensorData<float>(activation_state)[memory_size - 1] and having the
    // stride equal to memory_size. (Matrix batch vector multiply accumulate)
    float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
    for (int i = 0; i < batch_size;
         ++i, quantized_input_ptr_batch += input_size) {
      const float batch_scaling_factor = scaling_factors_ptr[i];
 80adf32:	f853 3026 	ldr.w	r3, [r3, r6, lsl #2]
 80adf36:	9314      	str	r3, [sp, #80]	; 0x50

      // Get the address of the first row:
      const int8_t* row_ptr = weights_feature_ptr;
 80adf38:	9b15      	ldr	r3, [sp, #84]	; 0x54
      for (int j = 0; j < num_filters; ++j, result += memory_size) {
 80adf3a:	9a08      	ldr	r2, [sp, #32]
 80adf3c:	454a      	cmp	r2, r9
 80adf3e:	dd1e      	ble.n	80adf7e <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x33e>
 80adf40:	2200      	movs	r2, #0
 80adf42:	4610      	mov	r0, r2
        // Initialize the dot product sum for the row to 0.
        int32_t dotprod = 0;
        for (int k = 0; k < input_size; ++k, ++row_ptr) {
 80adf44:	990b      	ldr	r1, [sp, #44]	; 0x2c
 80adf46:	4291      	cmp	r1, r2
 80adf48:	dd06      	ble.n	80adf58 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x318>
          dotprod += (*row_ptr) * (quantized_input_ptr_batch[k]);
 80adf4a:	5699      	ldrsb	r1, [r3, r2]
 80adf4c:	f91a e002 	ldrsb.w	lr, [sl, r2]
      // Get the address of the first row:
      const int8_t* row_ptr = weights_feature_ptr;
      for (int j = 0; j < num_filters; ++j, result += memory_size) {
        // Initialize the dot product sum for the row to 0.
        int32_t dotprod = 0;
        for (int k = 0; k < input_size; ++k, ++row_ptr) {
 80adf50:	3201      	adds	r2, #1
          dotprod += (*row_ptr) * (quantized_input_ptr_batch[k]);
 80adf52:	fb0e 0001 	mla	r0, lr, r1, r0
 80adf56:	e7f5      	b.n	80adf44 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x304>
 80adf58:	9a16      	ldr	r2, [sp, #88]	; 0x58
         ++i, quantized_input_ptr_batch += input_size) {
      const float batch_scaling_factor = scaling_factors_ptr[i];

      // Get the address of the first row:
      const int8_t* row_ptr = weights_feature_ptr;
      for (int j = 0; j < num_filters; ++j, result += memory_size) {
 80adf5a:	f109 0901 	add.w	r9, r9, #1
 80adf5e:	4413      	add	r3, r2
 80adf60:	9317      	str	r3, [sp, #92]	; 0x5c
 80adf62:	9318      	str	r3, [sp, #96]	; 0x60
        // Initialize the dot product sum for the row to 0.
        int32_t dotprod = 0;
        for (int k = 0; k < input_size; ++k, ++row_ptr) {
          dotprod += (*row_ptr) * (quantized_input_ptr_batch[k]);
        }
        *result += dotprod * batch_scaling_factor;
 80adf64:	f005 faee 	bl	80b3544 <__aeabi_i2f>
 80adf68:	9914      	ldr	r1, [sp, #80]	; 0x50
 80adf6a:	f005 fb3f 	bl	80b35ec <__aeabi_fmul>
 80adf6e:	4601      	mov	r1, r0
 80adf70:	6838      	ldr	r0, [r7, #0]
 80adf72:	f005 fa33 	bl	80b33dc <__addsf3>
 80adf76:	9b17      	ldr	r3, [sp, #92]	; 0x5c
 80adf78:	6038      	str	r0, [r7, #0]
         ++i, quantized_input_ptr_batch += input_size) {
      const float batch_scaling_factor = scaling_factors_ptr[i];

      // Get the address of the first row:
      const int8_t* row_ptr = weights_feature_ptr;
      for (int j = 0; j < num_filters; ++j, result += memory_size) {
 80adf7a:	4427      	add	r7, r4
 80adf7c:	e7dd      	b.n	80adf3a <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x2fa>
 80adf7e:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80adf80:	4445      	add	r5, r8
    // The rightmost column of activation_state is used to save the current
    // cycle activation. This is achieved by starting at
    // GetTensorData<float>(activation_state)[memory_size - 1] and having the
    // stride equal to memory_size. (Matrix batch vector multiply accumulate)
    float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
    for (int i = 0; i < batch_size;
 80adf82:	3601      	adds	r6, #1
 80adf84:	449a      	add	sl, r3
 80adf86:	e7cd      	b.n	80adf24 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x2e4>

  // TODO(alanchiao): can optimize hybrid case ~5% by unrolling loop in applying
  // time weights so that the inner loop multiplies eight elements at a time.
  ApplyTimeWeightsBiasAndActivation(
      batch_size, memory_size, num_filters, num_units, rank, weights_time, bias,
      params->activation, activation_state, scratch, output);
 80adf88:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80adf8a:	f8cd b010 	str.w	fp, [sp, #16]
 80adf8e:	9306      	str	r3, [sp, #24]
 80adf90:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80adf92:	9a08      	ldr	r2, [sp, #32]
 80adf94:	9305      	str	r3, [sp, #20]
 80adf96:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80adf98:	990c      	ldr	r1, [sp, #48]	; 0x30
 80adf9a:	791b      	ldrb	r3, [r3, #4]
 80adf9c:	9809      	ldr	r0, [sp, #36]	; 0x24
 80adf9e:	9303      	str	r3, [sp, #12]
 80adfa0:	9b0e      	ldr	r3, [sp, #56]	; 0x38
 80adfa2:	9302      	str	r3, [sp, #8]
 80adfa4:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80adfa6:	9301      	str	r3, [sp, #4]
 80adfa8:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80adfaa:	9300      	str	r3, [sp, #0]
 80adfac:	9b13      	ldr	r3, [sp, #76]	; 0x4c
 80adfae:	f7ff fa50 	bl	80ad452 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_>
      TfLiteTensor* scratch_float_weights_time = GetTemporary(context, node, 3);
      EvalHybridSVDF(context, node, input, weights_feature,
                     scratch_float_weights_time, bias, params, scratch,
                     scratch_scaling_factors, scratch_input_quantized,
                     activation_state, output);
      return kTfLiteOk;
 80adfb2:	2000      	movs	r0, #0
 80adfb4:	e007      	b.n	80adfc6 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x386>
      break;
    }

    default:
      // TODO(kreeger): Handle this case for full quant svdf b/139435798
      context->ReportError(context, "Type %s not currently supported.",
 80adfb6:	6965      	ldr	r5, [r4, #20]
 80adfb8:	f7f2 f8b0 	bl	80a011c <TfLiteTypeGetName>
                           TfLiteTypeGetName(weights_feature->type));
 80adfbc:	4903      	ldr	r1, [pc, #12]	; (80adfcc <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x38c>)
 80adfbe:	4602      	mov	r2, r0
 80adfc0:	4620      	mov	r0, r4
 80adfc2:	47a8      	blx	r5
      return kTfLiteError;
 80adfc4:	2001      	movs	r0, #1
  }
  return kTfLiteOk;
}
 80adfc6:	b01b      	add	sp, #108	; 0x6c
 80adfc8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80adfcc:	080b627a 	.word	0x080b627a

080adfd0 <_ZN6tflite3ops5micro13Register_SVDFEv>:

TfLiteRegistration* Register_SVDF() {
  static TfLiteRegistration r = {svdf::Init, svdf::Free, svdf::Prepare,
                                 svdf::Eval};
  return &r;
}
 80adfd0:	4800      	ldr	r0, [pc, #0]	; (80adfd4 <_ZN6tflite3ops5micro13Register_SVDFEv+0x4>)
 80adfd2:	4770      	bx	lr
 80adfd4:	200004c8 	.word	0x200004c8

080adfd8 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_17PrepareEP13TfLiteContextP10TfLiteNode>:

constexpr int kInputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
 80adfd8:	2000      	movs	r0, #0
 80adfda:	4770      	bx	lr

080adfdc <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode>:
  }

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80adfdc:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80adfe0:	680b      	ldr	r3, [r1, #0]
 80adfe2:	b085      	sub	sp, #20
 80adfe4:	6885      	ldr	r5, [r0, #8]
 80adfe6:	9001      	str	r0, [sp, #4]
 80adfe8:	6858      	ldr	r0, [r3, #4]
 80adfea:	2338      	movs	r3, #56	; 0x38
 80adfec:	4358      	muls	r0, r3
 80adfee:	182b      	adds	r3, r5, r0
  TfLiteUnpackParams* data =
      reinterpret_cast<TfLiteUnpackParams*>(node->builtin_data);

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);

  switch (input->type) {
 80adff0:	5c28      	ldrb	r0, [r5, r0]
  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteUnpackParams* data =
      reinterpret_cast<TfLiteUnpackParams*>(node->builtin_data);
 80adff2:	694a      	ldr	r2, [r1, #20]

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);

  switch (input->type) {
 80adff4:	1e46      	subs	r6, r0, #1
 80adff6:	2e08      	cmp	r6, #8
 80adff8:	f200 81c2 	bhi.w	80ae380 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3a4>
 80adffc:	e8df f016 	tbh	[pc, r6, lsl #1]
 80ae000:	00790009 	.word	0x00790009
 80ae004:	01c000e6 	.word	0x01c000e6
 80ae008:	01c001c0 	.word	0x01c001c0
 80ae00c:	01c001c0 	.word	0x01c001c0
 80ae010:	0153      	.short	0x0153
}

template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
 80ae012:	6848      	ldr	r0, [r1, #4]
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
 80ae014:	2638      	movs	r6, #56	; 0x38
 80ae016:	6840      	ldr	r0, [r0, #4]

template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
 80ae018:	f8d3 c008 	ldr.w	ip, [r3, #8]
  const TfLiteIntArray* output_dims = output0->dims;
 80ae01c:	fb06 5500 	mla	r5, r6, r0, r5

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);

  switch (input->type) {
    case kTfLiteFloat32: {
      return UnpackImpl<float>(context, node, input, data->num, data->axis);
 80ae020:	f8d2 8000 	ldr.w	r8, [r2]
 80ae024:	6852      	ldr	r2, [r2, #4]
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
 80ae026:	68ae      	ldr	r6, [r5, #8]
  const int dimensions = input_dims->size;
 80ae028:	f8dc 5000 	ldr.w	r5, [ip]

  if (axis < 0) {
 80ae02c:	2a00      	cmp	r2, #0
    axis += NumDimensions(input);
 80ae02e:	bfb8      	it	lt
 80ae030:	1952      	addlt	r2, r2, r5
  }

  TFLITE_DCHECK_LT(axis, dimensions);
 80ae032:	4295      	cmp	r5, r2
 80ae034:	dc01      	bgt.n	80ae03a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5e>
 80ae036:	f001 fffb 	bl	80b0030 <abort>
 80ae03a:	46e6      	mov	lr, ip
 80ae03c:	2000      	movs	r0, #0
 80ae03e:	2701      	movs	r7, #1

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ae040:	4282      	cmp	r2, r0
 80ae042:	dd05      	ble.n	80ae050 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x74>
    outer_size *= input_dims->data[i];
 80ae044:	f85e 9f04 	ldr.w	r9, [lr, #4]!
  }

  TFLITE_DCHECK_LT(axis, dimensions);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ae048:	3001      	adds	r0, #1
    outer_size *= input_dims->data[i];
 80ae04a:	fb09 f707 	mul.w	r7, r9, r7
 80ae04e:	e7f7      	b.n	80ae040 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x64>
 80ae050:	1c50      	adds	r0, r2, #1
 80ae052:	f105 0e01 	add.w	lr, r5, #1
  }
  int copy_size = 1;
 80ae056:	2201      	movs	r2, #1
 80ae058:	3001      	adds	r0, #1
  for (int i = axis + 1; i < dimensions; ++i) {
 80ae05a:	4570      	cmp	r0, lr
 80ae05c:	d003      	beq.n	80ae066 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x8a>
    copy_size *= input_dims->data[i];
 80ae05e:	f85c 5020 	ldr.w	r5, [ip, r0, lsl #2]
 80ae062:	436a      	muls	r2, r5
 80ae064:	e7f8      	b.n	80ae058 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x7c>
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
 80ae066:	f8d6 c000 	ldr.w	ip, [r6]
 80ae06a:	4630      	mov	r0, r6
 80ae06c:	2501      	movs	r5, #1
 80ae06e:	2600      	movs	r6, #0
 80ae070:	45b4      	cmp	ip, r6
 80ae072:	dd05      	ble.n	80ae080 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xa4>
    output_size *= output_dims->data[i];
 80ae074:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= input_dims->data[i];
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
 80ae078:	3601      	adds	r6, #1
    output_size *= output_dims->data[i];
 80ae07a:	fb0e f505 	mul.w	r5, lr, r5
 80ae07e:	e7f7      	b.n	80ae070 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x94>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);
 80ae080:	fb02 f007 	mul.w	r0, r2, r7
 80ae084:	4285      	cmp	r5, r0
 80ae086:	d1d6      	bne.n	80ae036 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80ae088:	2500      	movs	r5, #0

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
 80ae08a:	462e      	mov	r6, r5
 80ae08c:	685b      	ldr	r3, [r3, #4]
 80ae08e:	ea4f 0c82 	mov.w	ip, r2, lsl #2
 80ae092:	9302      	str	r3, [sp, #8]
 80ae094:	fb02 f308 	mul.w	r3, r2, r8
 80ae098:	009b      	lsls	r3, r3, #2
 80ae09a:	9303      	str	r3, [sp, #12]
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
 80ae09c:	f04f 0938 	mov.w	r9, #56	; 0x38
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
 80ae0a0:	45b0      	cmp	r8, r6
 80ae0a2:	dc01      	bgt.n	80ae0a8 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xcc>

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);

  switch (input->type) {
    case kTfLiteFloat32: {
      return UnpackImpl<float>(context, node, input, data->num, data->axis);
 80ae0a4:	2000      	movs	r0, #0
 80ae0a6:	e174      	b.n	80ae392 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3b6>
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
 80ae0a8:	684b      	ldr	r3, [r1, #4]
 80ae0aa:	eb03 0386 	add.w	r3, r3, r6, lsl #2
 80ae0ae:	6858      	ldr	r0, [r3, #4]
 80ae0b0:	9b01      	ldr	r3, [sp, #4]
 80ae0b2:	689b      	ldr	r3, [r3, #8]
 80ae0b4:	fb09 3300 	mla	r3, r9, r0, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80ae0b8:	b103      	cbz	r3, 80ae0bc <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xe0>
 80ae0ba:	685b      	ldr	r3, [r3, #4]
 80ae0bc:	f04f 0e00 	mov.w	lr, #0
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80ae0c0:	46f2      	mov	sl, lr
 80ae0c2:	4557      	cmp	r7, sl
 80ae0c4:	dd12      	ble.n	80ae0ec <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x110>
      T* output_ptr = output_data + copy_size * k;
      int loc = k * output_count * copy_size + i * copy_size;
      const T* input_ptr = input_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
 80ae0c6:	9c02      	ldr	r4, [sp, #8]
 80ae0c8:	eb05 0b0e 	add.w	fp, r5, lr
  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80ae0cc:	2000      	movs	r0, #0
      T* output_ptr = output_data + copy_size * k;
      int loc = k * output_count * copy_size + i * copy_size;
      const T* input_ptr = input_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
 80ae0ce:	44a3      	add	fp, r4
 80ae0d0:	4282      	cmp	r2, r0
 80ae0d2:	dd05      	ble.n	80ae0e0 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x104>
 80ae0d4:	f85b 4020 	ldr.w	r4, [fp, r0, lsl #2]
 80ae0d8:	f843 4020 	str.w	r4, [r3, r0, lsl #2]
 80ae0dc:	3001      	adds	r0, #1
 80ae0de:	e7f7      	b.n	80ae0d0 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xf4>
 80ae0e0:	9803      	ldr	r0, [sp, #12]
  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80ae0e2:	f10a 0a01 	add.w	sl, sl, #1
 80ae0e6:	4486      	add	lr, r0
 80ae0e8:	4463      	add	r3, ip
 80ae0ea:	e7ea      	b.n	80ae0c2 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xe6>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
 80ae0ec:	3601      	adds	r6, #1
 80ae0ee:	4465      	add	r5, ip
 80ae0f0:	e7d6      	b.n	80ae0a0 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc4>
}

template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
 80ae0f2:	6848      	ldr	r0, [r1, #4]
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
 80ae0f4:	2638      	movs	r6, #56	; 0x38
 80ae0f6:	6840      	ldr	r0, [r0, #4]

template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
 80ae0f8:	f8d3 c008 	ldr.w	ip, [r3, #8]
  const TfLiteIntArray* output_dims = output0->dims;
 80ae0fc:	fb06 5500 	mla	r5, r6, r0, r5
  switch (input->type) {
    case kTfLiteFloat32: {
      return UnpackImpl<float>(context, node, input, data->num, data->axis);
    }
    case kTfLiteInt32: {
      return UnpackImpl<int32_t>(context, node, input, data->num, data->axis);
 80ae100:	f8d2 8000 	ldr.w	r8, [r2]
 80ae104:	6852      	ldr	r2, [r2, #4]
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
 80ae106:	68ae      	ldr	r6, [r5, #8]
  const int dimensions = input_dims->size;
 80ae108:	f8dc 5000 	ldr.w	r5, [ip]

  if (axis < 0) {
 80ae10c:	2a00      	cmp	r2, #0
    axis += NumDimensions(input);
 80ae10e:	bfb8      	it	lt
 80ae110:	1952      	addlt	r2, r2, r5
  }

  TFLITE_DCHECK_LT(axis, dimensions);
 80ae112:	4295      	cmp	r5, r2
 80ae114:	dd8f      	ble.n	80ae036 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
 80ae116:	46e6      	mov	lr, ip
 80ae118:	2000      	movs	r0, #0
 80ae11a:	2701      	movs	r7, #1

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ae11c:	4282      	cmp	r2, r0
 80ae11e:	dd05      	ble.n	80ae12c <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x150>
    outer_size *= input_dims->data[i];
 80ae120:	f85e 9f04 	ldr.w	r9, [lr, #4]!
  }

  TFLITE_DCHECK_LT(axis, dimensions);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ae124:	3001      	adds	r0, #1
    outer_size *= input_dims->data[i];
 80ae126:	fb09 f707 	mul.w	r7, r9, r7
 80ae12a:	e7f7      	b.n	80ae11c <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x140>
 80ae12c:	1c50      	adds	r0, r2, #1
 80ae12e:	f105 0e01 	add.w	lr, r5, #1
  }
  int copy_size = 1;
 80ae132:	2201      	movs	r2, #1
 80ae134:	3001      	adds	r0, #1
  for (int i = axis + 1; i < dimensions; ++i) {
 80ae136:	4570      	cmp	r0, lr
 80ae138:	d003      	beq.n	80ae142 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x166>
    copy_size *= input_dims->data[i];
 80ae13a:	f85c 5020 	ldr.w	r5, [ip, r0, lsl #2]
 80ae13e:	436a      	muls	r2, r5
 80ae140:	e7f8      	b.n	80ae134 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x158>
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
 80ae142:	f8d6 c000 	ldr.w	ip, [r6]
 80ae146:	4630      	mov	r0, r6
 80ae148:	2501      	movs	r5, #1
 80ae14a:	2600      	movs	r6, #0
 80ae14c:	45b4      	cmp	ip, r6
 80ae14e:	dd05      	ble.n	80ae15c <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x180>
    output_size *= output_dims->data[i];
 80ae150:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= input_dims->data[i];
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
 80ae154:	3601      	adds	r6, #1
    output_size *= output_dims->data[i];
 80ae156:	fb0e f505 	mul.w	r5, lr, r5
 80ae15a:	e7f7      	b.n	80ae14c <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x170>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);
 80ae15c:	fb02 f007 	mul.w	r0, r2, r7
 80ae160:	4285      	cmp	r5, r0
 80ae162:	f47f af68 	bne.w	80ae036 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80ae166:	2500      	movs	r5, #0

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
 80ae168:	462e      	mov	r6, r5
 80ae16a:	685b      	ldr	r3, [r3, #4]
 80ae16c:	ea4f 0c82 	mov.w	ip, r2, lsl #2
 80ae170:	9302      	str	r3, [sp, #8]
 80ae172:	fb02 f308 	mul.w	r3, r2, r8
 80ae176:	009b      	lsls	r3, r3, #2
 80ae178:	9303      	str	r3, [sp, #12]
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
 80ae17a:	f04f 0938 	mov.w	r9, #56	; 0x38
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
 80ae17e:	45b0      	cmp	r8, r6
 80ae180:	dd90      	ble.n	80ae0a4 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc8>
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
 80ae182:	684b      	ldr	r3, [r1, #4]
 80ae184:	eb03 0386 	add.w	r3, r3, r6, lsl #2
 80ae188:	6858      	ldr	r0, [r3, #4]
 80ae18a:	9b01      	ldr	r3, [sp, #4]
 80ae18c:	689b      	ldr	r3, [r3, #8]
 80ae18e:	fb09 3300 	mla	r3, r9, r0, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80ae192:	b103      	cbz	r3, 80ae196 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1ba>
 80ae194:	685b      	ldr	r3, [r3, #4]
 80ae196:	f04f 0e00 	mov.w	lr, #0
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80ae19a:	46f2      	mov	sl, lr
 80ae19c:	4557      	cmp	r7, sl
 80ae19e:	dd12      	ble.n	80ae1c6 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1ea>
      T* output_ptr = output_data + copy_size * k;
      int loc = k * output_count * copy_size + i * copy_size;
      const T* input_ptr = input_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
 80ae1a0:	9c02      	ldr	r4, [sp, #8]
 80ae1a2:	eb05 0b0e 	add.w	fp, r5, lr
  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80ae1a6:	2000      	movs	r0, #0
      T* output_ptr = output_data + copy_size * k;
      int loc = k * output_count * copy_size + i * copy_size;
      const T* input_ptr = input_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
 80ae1a8:	44a3      	add	fp, r4
 80ae1aa:	4282      	cmp	r2, r0
 80ae1ac:	dd05      	ble.n	80ae1ba <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1de>
 80ae1ae:	f85b 4020 	ldr.w	r4, [fp, r0, lsl #2]
 80ae1b2:	f843 4020 	str.w	r4, [r3, r0, lsl #2]
 80ae1b6:	3001      	adds	r0, #1
 80ae1b8:	e7f7      	b.n	80ae1aa <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1ce>
 80ae1ba:	9803      	ldr	r0, [sp, #12]
  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80ae1bc:	f10a 0a01 	add.w	sl, sl, #1
 80ae1c0:	4486      	add	lr, r0
 80ae1c2:	4463      	add	r3, ip
 80ae1c4:	e7ea      	b.n	80ae19c <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1c0>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
 80ae1c6:	3601      	adds	r6, #1
 80ae1c8:	4465      	add	r5, ip
 80ae1ca:	e7d8      	b.n	80ae17e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1a2>
    }
    case kTfLiteInt32: {
      return UnpackImpl<int32_t>(context, node, input, data->num, data->axis);
    }
    case kTfLiteUInt8: {
      return UnpackImpl<uint8_t>(context, node, input, data->num, data->axis);
 80ae1cc:	6810      	ldr	r0, [r2, #0]
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
 80ae1ce:	2638      	movs	r6, #56	; 0x38
    }
    case kTfLiteInt32: {
      return UnpackImpl<int32_t>(context, node, input, data->num, data->axis);
    }
    case kTfLiteUInt8: {
      return UnpackImpl<uint8_t>(context, node, input, data->num, data->axis);
 80ae1d0:	9002      	str	r0, [sp, #8]
}

template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
 80ae1d2:	6848      	ldr	r0, [r1, #4]
  const TfLiteIntArray* input_dims = input->dims;
 80ae1d4:	f8d3 c008 	ldr.w	ip, [r3, #8]
  const TfLiteIntArray* output_dims = output0->dims;
 80ae1d8:	6840      	ldr	r0, [r0, #4]
    }
    case kTfLiteInt32: {
      return UnpackImpl<int32_t>(context, node, input, data->num, data->axis);
    }
    case kTfLiteUInt8: {
      return UnpackImpl<uint8_t>(context, node, input, data->num, data->axis);
 80ae1da:	6852      	ldr	r2, [r2, #4]
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
 80ae1dc:	fb06 5500 	mla	r5, r6, r0, r5
 80ae1e0:	68af      	ldr	r7, [r5, #8]
  const int dimensions = input_dims->size;
 80ae1e2:	f8dc 5000 	ldr.w	r5, [ip]

  if (axis < 0) {
 80ae1e6:	2a00      	cmp	r2, #0
    axis += NumDimensions(input);
 80ae1e8:	bfb8      	it	lt
 80ae1ea:	1952      	addlt	r2, r2, r5
  }

  TFLITE_DCHECK_LT(axis, dimensions);
 80ae1ec:	4295      	cmp	r5, r2
 80ae1ee:	f77f af22 	ble.w	80ae036 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
 80ae1f2:	46e6      	mov	lr, ip
 80ae1f4:	2000      	movs	r0, #0
 80ae1f6:	2601      	movs	r6, #1

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ae1f8:	4282      	cmp	r2, r0
 80ae1fa:	dd05      	ble.n	80ae208 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x22c>
    outer_size *= input_dims->data[i];
 80ae1fc:	f85e 8f04 	ldr.w	r8, [lr, #4]!
  }

  TFLITE_DCHECK_LT(axis, dimensions);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ae200:	3001      	adds	r0, #1
    outer_size *= input_dims->data[i];
 80ae202:	fb08 f606 	mul.w	r6, r8, r6
 80ae206:	e7f7      	b.n	80ae1f8 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x21c>
 80ae208:	1c50      	adds	r0, r2, #1
 80ae20a:	f105 0e01 	add.w	lr, r5, #1
  }
  int copy_size = 1;
 80ae20e:	2201      	movs	r2, #1
 80ae210:	3001      	adds	r0, #1
  for (int i = axis + 1; i < dimensions; ++i) {
 80ae212:	4570      	cmp	r0, lr
 80ae214:	d003      	beq.n	80ae21e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x242>
    copy_size *= input_dims->data[i];
 80ae216:	f85c 5020 	ldr.w	r5, [ip, r0, lsl #2]
 80ae21a:	436a      	muls	r2, r5
 80ae21c:	e7f8      	b.n	80ae210 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x234>
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
 80ae21e:	f8d7 c000 	ldr.w	ip, [r7]
 80ae222:	4638      	mov	r0, r7
 80ae224:	2501      	movs	r5, #1
 80ae226:	2700      	movs	r7, #0
 80ae228:	45bc      	cmp	ip, r7
 80ae22a:	dd05      	ble.n	80ae238 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x25c>
    output_size *= output_dims->data[i];
 80ae22c:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= input_dims->data[i];
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
 80ae230:	3701      	adds	r7, #1
    output_size *= output_dims->data[i];
 80ae232:	fb0e f505 	mul.w	r5, lr, r5
 80ae236:	e7f7      	b.n	80ae228 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x24c>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);
 80ae238:	fb02 f006 	mul.w	r0, r2, r6
 80ae23c:	4285      	cmp	r5, r0
 80ae23e:	f47f aefa 	bne.w	80ae036 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
 80ae242:	6858      	ldr	r0, [r3, #4]
 80ae244:	9b02      	ldr	r3, [sp, #8]
 80ae246:	4247      	negs	r7, r0
 80ae248:	fb02 f903 	mul.w	r9, r2, r3

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
 80ae24c:	2500      	movs	r5, #0
 80ae24e:	9b02      	ldr	r3, [sp, #8]
 80ae250:	42ab      	cmp	r3, r5
 80ae252:	f77f af27 	ble.w	80ae0a4 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc8>
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
 80ae256:	684b      	ldr	r3, [r1, #4]
 80ae258:	9c01      	ldr	r4, [sp, #4]
 80ae25a:	eb03 0385 	add.w	r3, r3, r5, lsl #2
 80ae25e:	685b      	ldr	r3, [r3, #4]
 80ae260:	68a4      	ldr	r4, [r4, #8]
 80ae262:	f04f 0e38 	mov.w	lr, #56	; 0x38
 80ae266:	fb0e 4303 	mla	r3, lr, r3, r4
 80ae26a:	b103      	cbz	r3, 80ae26e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x292>
 80ae26c:	685b      	ldr	r3, [r3, #4]
 80ae26e:	46be      	mov	lr, r7
 80ae270:	4684      	mov	ip, r0
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80ae272:	f04f 0800 	mov.w	r8, #0
 80ae276:	4546      	cmp	r6, r8
 80ae278:	dd11      	ble.n	80ae29e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2c2>
 80ae27a:	461c      	mov	r4, r3
 80ae27c:	46e2      	mov	sl, ip
      T* output_ptr = output_data + copy_size * k;
      int loc = k * output_count * copy_size + i * copy_size;
      const T* input_ptr = input_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
 80ae27e:	eb0e 0b0a 	add.w	fp, lr, sl
 80ae282:	455a      	cmp	r2, fp
 80ae284:	dd04      	ble.n	80ae290 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2b4>
 80ae286:	f81a bb01 	ldrb.w	fp, [sl], #1
 80ae28a:	f804 bb01 	strb.w	fp, [r4], #1
 80ae28e:	e7f6      	b.n	80ae27e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2a2>
  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80ae290:	f108 0801 	add.w	r8, r8, #1
 80ae294:	44cc      	add	ip, r9
 80ae296:	4413      	add	r3, r2
 80ae298:	ebc9 0e0e 	rsb	lr, r9, lr
 80ae29c:	e7eb      	b.n	80ae276 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x29a>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
 80ae29e:	3501      	adds	r5, #1
 80ae2a0:	4410      	add	r0, r2
 80ae2a2:	1abf      	subs	r7, r7, r2
 80ae2a4:	e7d3      	b.n	80ae24e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x272>
    }
    case kTfLiteUInt8: {
      return UnpackImpl<uint8_t>(context, node, input, data->num, data->axis);
    }
    case kTfLiteInt8: {
      return UnpackImpl<int8_t>(context, node, input, data->num, data->axis);
 80ae2a6:	6810      	ldr	r0, [r2, #0]
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
 80ae2a8:	2638      	movs	r6, #56	; 0x38
    }
    case kTfLiteUInt8: {
      return UnpackImpl<uint8_t>(context, node, input, data->num, data->axis);
    }
    case kTfLiteInt8: {
      return UnpackImpl<int8_t>(context, node, input, data->num, data->axis);
 80ae2aa:	9002      	str	r0, [sp, #8]
}

template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
 80ae2ac:	6848      	ldr	r0, [r1, #4]
  const TfLiteIntArray* input_dims = input->dims;
 80ae2ae:	f8d3 c008 	ldr.w	ip, [r3, #8]
  const TfLiteIntArray* output_dims = output0->dims;
 80ae2b2:	6840      	ldr	r0, [r0, #4]
    }
    case kTfLiteUInt8: {
      return UnpackImpl<uint8_t>(context, node, input, data->num, data->axis);
    }
    case kTfLiteInt8: {
      return UnpackImpl<int8_t>(context, node, input, data->num, data->axis);
 80ae2b4:	6852      	ldr	r2, [r2, #4]
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
 80ae2b6:	fb06 5500 	mla	r5, r6, r0, r5
 80ae2ba:	68af      	ldr	r7, [r5, #8]
  const int dimensions = input_dims->size;
 80ae2bc:	f8dc 5000 	ldr.w	r5, [ip]

  if (axis < 0) {
 80ae2c0:	2a00      	cmp	r2, #0
    axis += NumDimensions(input);
 80ae2c2:	bfb8      	it	lt
 80ae2c4:	1952      	addlt	r2, r2, r5
  }

  TFLITE_DCHECK_LT(axis, dimensions);
 80ae2c6:	4295      	cmp	r5, r2
 80ae2c8:	f77f aeb5 	ble.w	80ae036 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
 80ae2cc:	46e6      	mov	lr, ip
 80ae2ce:	2000      	movs	r0, #0
 80ae2d0:	2601      	movs	r6, #1

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ae2d2:	4282      	cmp	r2, r0
 80ae2d4:	dd05      	ble.n	80ae2e2 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x306>
    outer_size *= input_dims->data[i];
 80ae2d6:	f85e 8f04 	ldr.w	r8, [lr, #4]!
  }

  TFLITE_DCHECK_LT(axis, dimensions);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
 80ae2da:	3001      	adds	r0, #1
    outer_size *= input_dims->data[i];
 80ae2dc:	fb08 f606 	mul.w	r6, r8, r6
 80ae2e0:	e7f7      	b.n	80ae2d2 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2f6>
 80ae2e2:	1c50      	adds	r0, r2, #1
 80ae2e4:	f105 0e01 	add.w	lr, r5, #1
  }
  int copy_size = 1;
 80ae2e8:	2201      	movs	r2, #1
 80ae2ea:	3001      	adds	r0, #1
  for (int i = axis + 1; i < dimensions; ++i) {
 80ae2ec:	4570      	cmp	r0, lr
 80ae2ee:	d003      	beq.n	80ae2f8 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x31c>
    copy_size *= input_dims->data[i];
 80ae2f0:	f85c 5020 	ldr.w	r5, [ip, r0, lsl #2]
 80ae2f4:	436a      	muls	r2, r5
 80ae2f6:	e7f8      	b.n	80ae2ea <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x30e>
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
 80ae2f8:	f8d7 c000 	ldr.w	ip, [r7]
 80ae2fc:	4638      	mov	r0, r7
 80ae2fe:	2501      	movs	r5, #1
 80ae300:	2700      	movs	r7, #0
 80ae302:	45bc      	cmp	ip, r7
 80ae304:	dd05      	ble.n	80ae312 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x336>
    output_size *= output_dims->data[i];
 80ae306:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= input_dims->data[i];
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
 80ae30a:	3701      	adds	r7, #1
    output_size *= output_dims->data[i];
 80ae30c:	fb0e f505 	mul.w	r5, lr, r5
 80ae310:	e7f7      	b.n	80ae302 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x326>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);
 80ae312:	fb02 f006 	mul.w	r0, r2, r6
 80ae316:	4285      	cmp	r5, r0
 80ae318:	f47f ae8d 	bne.w	80ae036 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
 80ae31c:	6858      	ldr	r0, [r3, #4]
 80ae31e:	9b02      	ldr	r3, [sp, #8]
 80ae320:	4247      	negs	r7, r0
 80ae322:	fb02 f903 	mul.w	r9, r2, r3

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
 80ae326:	2500      	movs	r5, #0
 80ae328:	9b02      	ldr	r3, [sp, #8]
 80ae32a:	42ab      	cmp	r3, r5
 80ae32c:	f77f aeba 	ble.w	80ae0a4 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc8>
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
 80ae330:	684b      	ldr	r3, [r1, #4]
 80ae332:	9c01      	ldr	r4, [sp, #4]
 80ae334:	eb03 0385 	add.w	r3, r3, r5, lsl #2
 80ae338:	685b      	ldr	r3, [r3, #4]
 80ae33a:	68a4      	ldr	r4, [r4, #8]
 80ae33c:	f04f 0e38 	mov.w	lr, #56	; 0x38
 80ae340:	fb0e 4303 	mla	r3, lr, r3, r4
 80ae344:	b103      	cbz	r3, 80ae348 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x36c>
 80ae346:	685b      	ldr	r3, [r3, #4]
 80ae348:	46be      	mov	lr, r7
 80ae34a:	4684      	mov	ip, r0
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80ae34c:	f04f 0800 	mov.w	r8, #0
 80ae350:	4546      	cmp	r6, r8
 80ae352:	dd11      	ble.n	80ae378 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x39c>
 80ae354:	461c      	mov	r4, r3
 80ae356:	46e2      	mov	sl, ip
      T* output_ptr = output_data + copy_size * k;
      int loc = k * output_count * copy_size + i * copy_size;
      const T* input_ptr = input_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
 80ae358:	eb0a 0b0e 	add.w	fp, sl, lr
 80ae35c:	455a      	cmp	r2, fp
 80ae35e:	dd04      	ble.n	80ae36a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x38e>
 80ae360:	f91a bb01 	ldrsb.w	fp, [sl], #1
 80ae364:	f804 bb01 	strb.w	fp, [r4], #1
 80ae368:	e7f6      	b.n	80ae358 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x37c>
  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
 80ae36a:	f108 0801 	add.w	r8, r8, #1
 80ae36e:	44cc      	add	ip, r9
 80ae370:	4413      	add	r3, r2
 80ae372:	ebc9 0e0e 	rsb	lr, r9, lr
 80ae376:	e7eb      	b.n	80ae350 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x374>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
 80ae378:	3501      	adds	r5, #1
 80ae37a:	4410      	add	r0, r2
 80ae37c:	1abf      	subs	r7, r7, r2
 80ae37e:	e7d3      	b.n	80ae328 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x34c>
    }
    case kTfLiteInt8: {
      return UnpackImpl<int8_t>(context, node, input, data->num, data->axis);
    }
    default: {
      context->ReportError(context, "Type '%s' is not supported by unpack.",
 80ae380:	9b01      	ldr	r3, [sp, #4]
 80ae382:	695d      	ldr	r5, [r3, #20]
 80ae384:	f7f1 feca 	bl	80a011c <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type));
 80ae388:	4903      	ldr	r1, [pc, #12]	; (80ae398 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3bc>)
 80ae38a:	4602      	mov	r2, r0
 80ae38c:	9801      	ldr	r0, [sp, #4]
 80ae38e:	47a8      	blx	r5
      return kTfLiteError;
 80ae390:	2001      	movs	r0, #1
    }
  }

  return kTfLiteOk;
}
 80ae392:	b005      	add	sp, #20
 80ae394:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80ae398:	080b7199 	.word	0x080b7199

080ae39c <_ZN6tflite3ops5micro15Register_UNPACKEv>:

TfLiteRegistration* Register_UNPACK() {
  static TfLiteRegistration r = {nullptr, nullptr, unpack::Prepare,
                                 unpack::Eval};
  return &r;
}
 80ae39c:	4800      	ldr	r0, [pc, #0]	; (80ae3a0 <_ZN6tflite3ops5micro15Register_UNPACKEv+0x4>)
 80ae39e:	4770      	bx	lr
 80ae3a0:	200004e8 	.word	0x200004e8

080ae3a4 <_ZN6tflite3ops5micro14depthwise_conv4InitEP13TfLiteContextPKcj>:

}  // namespace

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
 80ae3a4:	2000      	movs	r0, #0
 80ae3a6:	4770      	bx	lr

080ae3a8 <_ZN6tflite3ops5micro14depthwise_conv4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
 80ae3a8:	4770      	bx	lr

080ae3aa <_ZN6tflite3ops5micro14depthwise_conv7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
 80ae3aa:	2000      	movs	r0, #0
 80ae3ac:	4770      	bx	lr

080ae3ae <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa>:
    const DepthwiseParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
 80ae3ae:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80ae3b2:	469b      	mov	fp, r3
  // Get parameters.
  // TODO(b/141565753): Re-introduce ScopedProfilingLabel on Micro.
  const int stride_width = params.stride_width;
 80ae3b4:	f9b0 300a 	ldrsh.w	r3, [r0, #10]
    const DepthwiseParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
 80ae3b8:	b0a5      	sub	sp, #148	; 0x94
  // Get parameters.
  // TODO(b/141565753): Re-introduce ScopedProfilingLabel on Micro.
  const int stride_width = params.stride_width;
 80ae3ba:	930f      	str	r3, [sp, #60]	; 0x3c
  const int stride_height = params.stride_height;
 80ae3bc:	f9b0 300c 	ldrsh.w	r3, [r0, #12]
    const DepthwiseParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
 80ae3c0:	9122      	str	r1, [sp, #136]	; 0x88
  // Get parameters.
  // TODO(b/141565753): Re-introduce ScopedProfilingLabel on Micro.
  const int stride_width = params.stride_width;
  const int stride_height = params.stride_height;
 80ae3c2:	9310      	str	r3, [sp, #64]	; 0x40
  const int dilation_width_factor = params.dilation_width_factor;
 80ae3c4:	f9b0 300e 	ldrsh.w	r3, [r0, #14]
    const DepthwiseParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
 80ae3c8:	9223      	str	r2, [sp, #140]	; 0x8c
  // Get parameters.
  // TODO(b/141565753): Re-introduce ScopedProfilingLabel on Micro.
  const int stride_width = params.stride_width;
  const int stride_height = params.stride_height;
  const int dilation_width_factor = params.dilation_width_factor;
 80ae3ca:	9311      	str	r3, [sp, #68]	; 0x44
  const int dilation_height_factor = params.dilation_height_factor;
 80ae3cc:	f9b0 3010 	ldrsh.w	r3, [r0, #16]
    const DepthwiseParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
 80ae3d0:	f8dd a0cc 	ldr.w	sl, [sp, #204]	; 0xcc
  // Get parameters.
  // TODO(b/141565753): Re-introduce ScopedProfilingLabel on Micro.
  const int stride_width = params.stride_width;
  const int stride_height = params.stride_height;
  const int dilation_width_factor = params.dilation_width_factor;
  const int dilation_height_factor = params.dilation_height_factor;
 80ae3d4:	9312      	str	r3, [sp, #72]	; 0x48
  const int pad_width = params.padding_values.width;
 80ae3d6:	f9b0 3002 	ldrsh.w	r3, [r0, #2]
 80ae3da:	9313      	str	r3, [sp, #76]	; 0x4c
  const int pad_height = params.padding_values.height;
 80ae3dc:	f9b0 3004 	ldrsh.w	r3, [r0, #4]
 80ae3e0:	9314      	str	r3, [sp, #80]	; 0x50
  const int depth_multiplier = params.depth_multiplier;
 80ae3e2:	f9b0 3012 	ldrsh.w	r3, [r0, #18]
 80ae3e6:	9308      	str	r3, [sp, #32]
  const int32 input_offset = params.input_offset;
 80ae3e8:	6943      	ldr	r3, [r0, #20]
 80ae3ea:	9315      	str	r3, [sp, #84]	; 0x54
  const int32 output_offset = params.output_offset;
 80ae3ec:	69c3      	ldr	r3, [r0, #28]
 80ae3ee:	9316      	str	r3, [sp, #88]	; 0x58
  const int32 output_activation_min = params.quantized_activation_min;
 80ae3f0:	6a83      	ldr	r3, [r0, #40]	; 0x28
 80ae3f2:	930b      	str	r3, [sp, #44]	; 0x2c
  const int32 output_activation_max = params.quantized_activation_max;
 80ae3f4:	6ac3      	ldr	r3, [r0, #44]	; 0x2c
 80ae3f6:	930c      	str	r3, [sp, #48]	; 0x30

  // Check dimensions of the tensors.
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
 80ae3f8:	f8db 3000 	ldr.w	r3, [fp]
 80ae3fc:	2b04      	cmp	r3, #4
 80ae3fe:	d001      	beq.n	80ae404 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x56>
 80ae400:	f001 fe16 	bl	80b0030 <abort>
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
 80ae404:	9b2f      	ldr	r3, [sp, #188]	; 0xbc
 80ae406:	681b      	ldr	r3, [r3, #0]
 80ae408:	2b04      	cmp	r3, #4
 80ae40a:	d1f9      	bne.n	80ae400 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x52>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
 80ae40c:	f8da 3000 	ldr.w	r3, [sl]
 80ae410:	2b04      	cmp	r3, #4
 80ae412:	d1f5      	bne.n	80ae400 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x52>

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
 80ae414:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80ae416:	9a0c      	ldr	r2, [sp, #48]	; 0x30
 80ae418:	4293      	cmp	r3, r2
 80ae41a:	dcf1      	bgt.n	80ae400 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x52>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80ae41c:	2300      	movs	r3, #0
 80ae41e:	4619      	mov	r1, r3
 80ae420:	4652      	mov	r2, sl
 80ae422:	4658      	mov	r0, fp
 80ae424:	f7f9 fb2b 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
 80ae428:	2303      	movs	r3, #3
 80ae42a:	4619      	mov	r1, r3
 80ae42c:	4652      	mov	r2, sl
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80ae42e:	9017      	str	r0, [sp, #92]	; 0x5c
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
 80ae430:	982f      	ldr	r0, [sp, #188]	; 0xbc
 80ae432:	f7f9 fb24 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
 80ae436:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
 80ae438:	4604      	mov	r4, r0
  const int input_height = input_shape.Dims(1);
 80ae43a:	4658      	mov	r0, fp
 80ae43c:	f7f3 ffc4 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
 80ae440:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
 80ae442:	9018      	str	r0, [sp, #96]	; 0x60
  const int input_width = input_shape.Dims(2);
 80ae444:	4658      	mov	r0, fp
 80ae446:	f7f3 ffbf 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_depth = input_shape.Dims(3);
 80ae44a:	2103      	movs	r1, #3

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
 80ae44c:	9019      	str	r0, [sp, #100]	; 0x64
  const int input_depth = input_shape.Dims(3);
 80ae44e:	4658      	mov	r0, fp
 80ae450:	f7f3 ffba 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_height = filter_shape.Dims(1);
 80ae454:	2101      	movs	r1, #1
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
 80ae456:	900d      	str	r0, [sp, #52]	; 0x34
  const int filter_height = filter_shape.Dims(1);
 80ae458:	982f      	ldr	r0, [sp, #188]	; 0xbc
 80ae45a:	f7f3 ffb5 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_width = filter_shape.Dims(2);
 80ae45e:	2102      	movs	r1, #2
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
 80ae460:	901a      	str	r0, [sp, #104]	; 0x68
  const int filter_width = filter_shape.Dims(2);
 80ae462:	982f      	ldr	r0, [sp, #188]	; 0xbc
 80ae464:	f7f3 ffb0 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
 80ae468:	2101      	movs	r1, #1
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
 80ae46a:	901b      	str	r0, [sp, #108]	; 0x6c
  const int output_height = output_shape.Dims(1);
 80ae46c:	4650      	mov	r0, sl
 80ae46e:	f7f3 ffab 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
 80ae472:	2102      	movs	r1, #2
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
 80ae474:	901c      	str	r0, [sp, #112]	; 0x70
  const int output_width = output_shape.Dims(2);
 80ae476:	4650      	mov	r0, sl
 80ae478:	f7f3 ffa6 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
 80ae47c:	9b0d      	ldr	r3, [sp, #52]	; 0x34
 80ae47e:	9a08      	ldr	r2, [sp, #32]
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
 80ae480:	901d      	str	r0, [sp, #116]	; 0x74
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
 80ae482:	4353      	muls	r3, r2
 80ae484:	429c      	cmp	r4, r3
 80ae486:	d1bb      	bne.n	80ae400 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x52>
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
 80ae488:	9831      	ldr	r0, [sp, #196]	; 0xc4
 80ae48a:	f7f9 fae8 	bl	80a7a5e <_ZNK6tflite12RuntimeShape8FlatSizeEv>
 80ae48e:	4284      	cmp	r4, r0
 80ae490:	d1b6      	bne.n	80ae400 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x52>
 80ae492:	f04f 0900 	mov.w	r9, #0

  for (int batch = 0; batch < batches; ++batch) {
 80ae496:	9b17      	ldr	r3, [sp, #92]	; 0x5c
 80ae498:	4599      	cmp	r9, r3
 80ae49a:	f280 80ab 	bge.w	80ae5f4 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x246>
 80ae49e:	9b14      	ldr	r3, [sp, #80]	; 0x50
 80ae4a0:	425b      	negs	r3, r3
 80ae4a2:	9309      	str	r3, [sp, #36]	; 0x24
 80ae4a4:	2300      	movs	r3, #0
 80ae4a6:	9303      	str	r3, [sp, #12]
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80ae4a8:	9b03      	ldr	r3, [sp, #12]
 80ae4aa:	9a1c      	ldr	r2, [sp, #112]	; 0x70
 80ae4ac:	4293      	cmp	r3, r2
 80ae4ae:	f280 8083 	bge.w	80ae5b8 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x20a>
 80ae4b2:	9b13      	ldr	r3, [sp, #76]	; 0x4c
 80ae4b4:	425b      	negs	r3, r3
 80ae4b6:	930a      	str	r3, [sp, #40]	; 0x28
 80ae4b8:	2300      	movs	r3, #0
 80ae4ba:	9304      	str	r3, [sp, #16]
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80ae4bc:	9b04      	ldr	r3, [sp, #16]
 80ae4be:	9a1d      	ldr	r2, [sp, #116]	; 0x74
 80ae4c0:	4293      	cmp	r3, r2
 80ae4c2:	da71      	bge.n	80ae5a8 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1fa>
 80ae4c4:	f04f 0800 	mov.w	r8, #0
 80ae4c8:	f8cd 8014 	str.w	r8, [sp, #20]
        for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
 80ae4cc:	9b05      	ldr	r3, [sp, #20]
 80ae4ce:	9a0d      	ldr	r2, [sp, #52]	; 0x34
 80ae4d0:	4293      	cmp	r3, r2
 80ae4d2:	da61      	bge.n	80ae598 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1ea>
 80ae4d4:	9a32      	ldr	r2, [sp, #200]	; 0xc8
 80ae4d6:	ea4f 0388 	mov.w	r3, r8, lsl #2
 80ae4da:	441a      	add	r2, r3
 80ae4dc:	9221      	str	r2, [sp, #132]	; 0x84
 80ae4de:	9a22      	ldr	r2, [sp, #136]	; 0x88
 80ae4e0:	2400      	movs	r4, #0
 80ae4e2:	441a      	add	r2, r3
 80ae4e4:	9220      	str	r2, [sp, #128]	; 0x80
 80ae4e6:	9a23      	ldr	r2, [sp, #140]	; 0x8c
 80ae4e8:	18d3      	adds	r3, r2, r3
 80ae4ea:	931f      	str	r3, [sp, #124]	; 0x7c
          for (int m = 0; m < depth_multiplier; ++m) {
 80ae4ec:	9b08      	ldr	r3, [sp, #32]
 80ae4ee:	429c      	cmp	r4, r3
 80ae4f0:	da4c      	bge.n	80ae58c <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1de>
 80ae4f2:	eb08 0304 	add.w	r3, r8, r4
            const int output_channel = m + in_channel * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            int32 acc = 0;
 80ae4f6:	2500      	movs	r5, #0

  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
          for (int m = 0; m < depth_multiplier; ++m) {
 80ae4f8:	9e09      	ldr	r6, [sp, #36]	; 0x24
 80ae4fa:	930e      	str	r3, [sp, #56]	; 0x38
            const int output_channel = m + in_channel * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            int32 acc = 0;
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
 80ae4fc:	9506      	str	r5, [sp, #24]
 80ae4fe:	9b06      	ldr	r3, [sp, #24]
 80ae500:	9a1a      	ldr	r2, [sp, #104]	; 0x68
 80ae502:	4293      	cmp	r3, r2
 80ae504:	da1c      	bge.n	80ae540 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x192>
 80ae506:	2300      	movs	r3, #0
 80ae508:	9f0a      	ldr	r7, [sp, #40]	; 0x28
 80ae50a:	9307      	str	r3, [sp, #28]
              for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
 80ae50c:	9b07      	ldr	r3, [sp, #28]
 80ae50e:	9a1b      	ldr	r2, [sp, #108]	; 0x6c
 80ae510:	4293      	cmp	r3, r2
 80ae512:	da0f      	bge.n	80ae534 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x186>
                const int in_x = in_x_origin + dilation_width_factor * filter_x;
                const int in_y =
                    in_y_origin + dilation_height_factor * filter_y;
                // Zero padding by omitting the areas outside the image.
                const bool is_point_inside_image =
                    (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
 80ae514:	2f00      	cmp	r7, #0
 80ae516:	db07      	blt.n	80ae528 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x17a>
 80ae518:	9b19      	ldr	r3, [sp, #100]	; 0x64
 80ae51a:	42bb      	cmp	r3, r7
 80ae51c:	dd04      	ble.n	80ae528 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x17a>
 80ae51e:	2e00      	cmp	r6, #0
 80ae520:	db02      	blt.n	80ae528 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x17a>
 80ae522:	9b18      	ldr	r3, [sp, #96]	; 0x60
 80ae524:	42b3      	cmp	r3, r6
 80ae526:	dc4a      	bgt.n	80ae5be <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x210>
            const int output_channel = m + in_channel * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            int32 acc = 0;
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
              for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
 80ae528:	9b07      	ldr	r3, [sp, #28]
 80ae52a:	3301      	adds	r3, #1
 80ae52c:	9307      	str	r3, [sp, #28]
 80ae52e:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80ae530:	441f      	add	r7, r3
 80ae532:	e7eb      	b.n	80ae50c <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x15e>
          for (int m = 0; m < depth_multiplier; ++m) {
            const int output_channel = m + in_channel * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            int32 acc = 0;
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
 80ae534:	9b06      	ldr	r3, [sp, #24]
 80ae536:	3301      	adds	r3, #1
 80ae538:	9306      	str	r3, [sp, #24]
 80ae53a:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80ae53c:	441e      	add	r6, r3
 80ae53e:	e7de      	b.n	80ae4fe <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x150>
                  // accumulator depth is smaller than 2^16.
                  acc += filter_val * (input_val + input_offset);
                }
              }
            }
            if (bias_data) {
 80ae540:	9b32      	ldr	r3, [sp, #200]	; 0xc8
 80ae542:	b11b      	cbz	r3, 80ae54c <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x19e>
              acc += bias_data[output_channel];
 80ae544:	9b21      	ldr	r3, [sp, #132]	; 0x84
 80ae546:	f853 3024 	ldr.w	r3, [r3, r4, lsl #2]
 80ae54a:	441d      	add	r5, r3
            }
            acc = MultiplyByQuantizedMultiplier(
 80ae54c:	9b1f      	ldr	r3, [sp, #124]	; 0x7c
 80ae54e:	4628      	mov	r0, r5
 80ae550:	f853 2024 	ldr.w	r2, [r3, r4, lsl #2]
 80ae554:	9b20      	ldr	r3, [sp, #128]	; 0x80
 80ae556:	f853 1024 	ldr.w	r1, [r3, r4, lsl #2]
 80ae55a:	f7f9 fa9f 	bl	80a7a9c <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
                acc, output_multiplier[output_channel],
                output_shift[output_channel]);
            acc += output_offset;
 80ae55e:	9b16      	ldr	r3, [sp, #88]	; 0x58
            acc = std::max(acc, output_activation_min);
            acc = std::min(acc, output_activation_max);
            output_data[Offset(output_shape, batch, out_y, out_x,
 80ae560:	9a03      	ldr	r2, [sp, #12]
              acc += bias_data[output_channel];
            }
            acc = MultiplyByQuantizedMultiplier(
                acc, output_multiplier[output_channel],
                output_shift[output_channel]);
            acc += output_offset;
 80ae562:	4418      	add	r0, r3
 80ae564:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
            acc = std::max(acc, output_activation_min);
            acc = std::min(acc, output_activation_max);
            output_data[Offset(output_shape, batch, out_y, out_x,
 80ae566:	4649      	mov	r1, r9
 80ae568:	4283      	cmp	r3, r0
 80ae56a:	bfb8      	it	lt
 80ae56c:	4603      	movlt	r3, r0
 80ae56e:	461d      	mov	r5, r3
 80ae570:	9b0c      	ldr	r3, [sp, #48]	; 0x30
 80ae572:	4650      	mov	r0, sl
 80ae574:	429d      	cmp	r5, r3
 80ae576:	bfa8      	it	ge
 80ae578:	461d      	movge	r5, r3
 80ae57a:	9b0e      	ldr	r3, [sp, #56]	; 0x38

  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
          for (int m = 0; m < depth_multiplier; ++m) {
 80ae57c:	3401      	adds	r4, #1
                acc, output_multiplier[output_channel],
                output_shift[output_channel]);
            acc += output_offset;
            acc = std::max(acc, output_activation_min);
            acc = std::min(acc, output_activation_max);
            output_data[Offset(output_shape, batch, out_y, out_x,
 80ae57e:	9300      	str	r3, [sp, #0]
 80ae580:	9b04      	ldr	r3, [sp, #16]
 80ae582:	f7f3 ff86 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                               output_channel)] = static_cast<int8_t>(acc);
 80ae586:	9b34      	ldr	r3, [sp, #208]	; 0xd0
 80ae588:	541d      	strb	r5, [r3, r0]

  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
          for (int m = 0; m < depth_multiplier; ++m) {
 80ae58a:	e7af      	b.n	80ae4ec <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x13e>
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
 80ae58c:	9b05      	ldr	r3, [sp, #20]
 80ae58e:	3301      	adds	r3, #1
 80ae590:	9305      	str	r3, [sp, #20]
 80ae592:	9b08      	ldr	r3, [sp, #32]
 80ae594:	4498      	add	r8, r3
 80ae596:	e799      	b.n	80ae4cc <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x11e>
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80ae598:	9b04      	ldr	r3, [sp, #16]
 80ae59a:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
 80ae59c:	3301      	adds	r3, #1
 80ae59e:	9304      	str	r3, [sp, #16]
 80ae5a0:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80ae5a2:	4413      	add	r3, r2
 80ae5a4:	930a      	str	r3, [sp, #40]	; 0x28
 80ae5a6:	e789      	b.n	80ae4bc <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x10e>
  const int output_width = output_shape.Dims(2);
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80ae5a8:	9b03      	ldr	r3, [sp, #12]
 80ae5aa:	9a10      	ldr	r2, [sp, #64]	; 0x40
 80ae5ac:	3301      	adds	r3, #1
 80ae5ae:	9303      	str	r3, [sp, #12]
 80ae5b0:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80ae5b2:	4413      	add	r3, r2
 80ae5b4:	9309      	str	r3, [sp, #36]	; 0x24
 80ae5b6:	e777      	b.n	80ae4a8 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0xfa>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int batch = 0; batch < batches; ++batch) {
 80ae5b8:	f109 0901 	add.w	r9, r9, #1
 80ae5bc:	e76b      	b.n	80ae496 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0xe8>
                // Zero padding by omitting the areas outside the image.
                const bool is_point_inside_image =
                    (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height);
                if (is_point_inside_image) {
                  int32 input_val = input_data[Offset(input_shape, batch, in_y,
 80ae5be:	9b05      	ldr	r3, [sp, #20]
 80ae5c0:	4632      	mov	r2, r6
 80ae5c2:	9300      	str	r3, [sp, #0]
 80ae5c4:	4649      	mov	r1, r9
 80ae5c6:	463b      	mov	r3, r7
 80ae5c8:	4658      	mov	r0, fp
 80ae5ca:	f7f3 ff62 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                                                      in_x, in_channel)];
                  int32 filter_val = filter_data[Offset(
 80ae5ce:	9b0e      	ldr	r3, [sp, #56]	; 0x38
                // Zero padding by omitting the areas outside the image.
                const bool is_point_inside_image =
                    (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height);
                if (is_point_inside_image) {
                  int32 input_val = input_data[Offset(input_shape, batch, in_y,
 80ae5d0:	901e      	str	r0, [sp, #120]	; 0x78
                                                      in_x, in_channel)];
                  int32 filter_val = filter_data[Offset(
 80ae5d2:	9300      	str	r3, [sp, #0]
 80ae5d4:	9a06      	ldr	r2, [sp, #24]
 80ae5d6:	9b07      	ldr	r3, [sp, #28]
 80ae5d8:	2100      	movs	r1, #0
 80ae5da:	982f      	ldr	r0, [sp, #188]	; 0xbc
 80ae5dc:	f7f3 ff59 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                  // long as the filter size (filter_y * filter_x * in_channel)
                  // does not exceed 2^16, which is the case in all the models
                  // we have seen so far.
                  // TODO(jianlijianli): Add a check to make sure the
                  // accumulator depth is smaller than 2^16.
                  acc += filter_val * (input_val + input_offset);
 80ae5e0:	9a1e      	ldr	r2, [sp, #120]	; 0x78
 80ae5e2:	9b2e      	ldr	r3, [sp, #184]	; 0xb8
 80ae5e4:	569b      	ldrsb	r3, [r3, r2]
 80ae5e6:	9a15      	ldr	r2, [sp, #84]	; 0x54
 80ae5e8:	4413      	add	r3, r2
 80ae5ea:	9a30      	ldr	r2, [sp, #192]	; 0xc0
 80ae5ec:	5612      	ldrsb	r2, [r2, r0]
 80ae5ee:	fb02 5503 	mla	r5, r2, r3, r5
 80ae5f2:	e799      	b.n	80ae528 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x17a>
          }
        }
      }
    }
  }
}
 80ae5f4:	b025      	add	sp, #148	; 0x94
 80ae5f6:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080ae5fa <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf>:
inline void DepthwiseConv(
    const DepthwiseParams& params, const RuntimeShape& input_shape,
    const float* input_data, const RuntimeShape& filter_shape,
    const float* filter_data, const RuntimeShape& bias_shape,
    const float* bias_data, const RuntimeShape& output_shape,
    float* output_data) {
 80ae5fa:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80ae5fe:	4698      	mov	r8, r3
  const int stride_width = params.stride_width;
 80ae600:	f9b0 300a 	ldrsh.w	r3, [r0, #10]
inline void DepthwiseConv(
    const DepthwiseParams& params, const RuntimeShape& input_shape,
    const float* input_data, const RuntimeShape& filter_shape,
    const float* filter_data, const RuntimeShape& bias_shape,
    const float* bias_data, const RuntimeShape& output_shape,
    float* output_data) {
 80ae604:	b0a1      	sub	sp, #132	; 0x84
  const int stride_width = params.stride_width;
 80ae606:	9310      	str	r3, [sp, #64]	; 0x40
  const int stride_height = params.stride_height;
 80ae608:	f9b0 300c 	ldrsh.w	r3, [r0, #12]
inline void DepthwiseConv(
    const DepthwiseParams& params, const RuntimeShape& input_shape,
    const float* input_data, const RuntimeShape& filter_shape,
    const float* filter_data, const RuntimeShape& bias_shape,
    const float* bias_data, const RuntimeShape& output_shape,
    float* output_data) {
 80ae60c:	468a      	mov	sl, r1
  const int stride_width = params.stride_width;
  const int stride_height = params.stride_height;
 80ae60e:	9311      	str	r3, [sp, #68]	; 0x44
  const int dilation_width_factor = params.dilation_width_factor;
 80ae610:	f9b0 300e 	ldrsh.w	r3, [r0, #14]
inline void DepthwiseConv(
    const DepthwiseParams& params, const RuntimeShape& input_shape,
    const float* input_data, const RuntimeShape& filter_shape,
    const float* filter_data, const RuntimeShape& bias_shape,
    const float* bias_data, const RuntimeShape& output_shape,
    float* output_data) {
 80ae614:	921f      	str	r2, [sp, #124]	; 0x7c
  const int stride_width = params.stride_width;
  const int stride_height = params.stride_height;
  const int dilation_width_factor = params.dilation_width_factor;
 80ae616:	9312      	str	r3, [sp, #72]	; 0x48
  const int dilation_height_factor = params.dilation_height_factor;
 80ae618:	f9b0 3010 	ldrsh.w	r3, [r0, #16]
inline void DepthwiseConv(
    const DepthwiseParams& params, const RuntimeShape& input_shape,
    const float* input_data, const RuntimeShape& filter_shape,
    const float* filter_data, const RuntimeShape& bias_shape,
    const float* bias_data, const RuntimeShape& output_shape,
    float* output_data) {
 80ae61c:	f8dd 90b4 	ldr.w	r9, [sp, #180]	; 0xb4
  const int stride_width = params.stride_width;
  const int stride_height = params.stride_height;
  const int dilation_width_factor = params.dilation_width_factor;
  const int dilation_height_factor = params.dilation_height_factor;
 80ae620:	9313      	str	r3, [sp, #76]	; 0x4c
  const int pad_width = params.padding_values.width;
 80ae622:	f9b0 3002 	ldrsh.w	r3, [r0, #2]
 80ae626:	9314      	str	r3, [sp, #80]	; 0x50
  const int pad_height = params.padding_values.height;
 80ae628:	f9b0 3004 	ldrsh.w	r3, [r0, #4]
 80ae62c:	9315      	str	r3, [sp, #84]	; 0x54
  const int depth_multiplier = params.depth_multiplier;
 80ae62e:	f9b0 3012 	ldrsh.w	r3, [r0, #18]
 80ae632:	9309      	str	r3, [sp, #36]	; 0x24
  const float output_activation_min = params.float_activation_min;
 80ae634:	6b03      	ldr	r3, [r0, #48]	; 0x30
 80ae636:	930c      	str	r3, [sp, #48]	; 0x30
  const float output_activation_max = params.float_activation_max;
 80ae638:	6b43      	ldr	r3, [r0, #52]	; 0x34
 80ae63a:	930d      	str	r3, [sp, #52]	; 0x34
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
 80ae63c:	680b      	ldr	r3, [r1, #0]
 80ae63e:	2b04      	cmp	r3, #4
 80ae640:	d001      	beq.n	80ae646 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x4c>
 80ae642:	f001 fcf5 	bl	80b0030 <abort>
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
 80ae646:	f8d8 3000 	ldr.w	r3, [r8]
 80ae64a:	2b04      	cmp	r3, #4
 80ae64c:	d1f9      	bne.n	80ae642 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x48>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
 80ae64e:	f8d9 3000 	ldr.w	r3, [r9]
 80ae652:	2b04      	cmp	r3, #4
 80ae654:	d1f5      	bne.n	80ae642 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x48>

  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80ae656:	2300      	movs	r3, #0
 80ae658:	4619      	mov	r1, r3
 80ae65a:	464a      	mov	r2, r9
 80ae65c:	4650      	mov	r0, sl
 80ae65e:	f7f9 fa0e 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
 80ae662:	2303      	movs	r3, #3
 80ae664:	4619      	mov	r1, r3
 80ae666:	464a      	mov	r2, r9
  const float output_activation_max = params.float_activation_max;
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80ae668:	9016      	str	r0, [sp, #88]	; 0x58
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
 80ae66a:	4640      	mov	r0, r8
 80ae66c:	f7f9 fa07 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
 80ae670:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
 80ae672:	4604      	mov	r4, r0
  const int input_height = input_shape.Dims(1);
 80ae674:	4650      	mov	r0, sl
 80ae676:	f7f3 fea7 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
 80ae67a:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
 80ae67c:	9017      	str	r0, [sp, #92]	; 0x5c
  const int input_width = input_shape.Dims(2);
 80ae67e:	4650      	mov	r0, sl
 80ae680:	f7f3 fea2 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_depth = input_shape.Dims(3);
 80ae684:	2103      	movs	r1, #3
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
 80ae686:	9018      	str	r0, [sp, #96]	; 0x60
  const int input_depth = input_shape.Dims(3);
 80ae688:	4650      	mov	r0, sl
 80ae68a:	f7f3 fe9d 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_height = filter_shape.Dims(1);
 80ae68e:	2101      	movs	r1, #1

  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
 80ae690:	900e      	str	r0, [sp, #56]	; 0x38
  const int filter_height = filter_shape.Dims(1);
 80ae692:	4640      	mov	r0, r8
 80ae694:	f7f3 fe98 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_width = filter_shape.Dims(2);
 80ae698:	2102      	movs	r1, #2
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
 80ae69a:	9019      	str	r0, [sp, #100]	; 0x64
  const int filter_width = filter_shape.Dims(2);
 80ae69c:	4640      	mov	r0, r8
 80ae69e:	f7f3 fe93 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
 80ae6a2:	2101      	movs	r1, #1
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
 80ae6a4:	901a      	str	r0, [sp, #104]	; 0x68
  const int output_height = output_shape.Dims(1);
 80ae6a6:	4648      	mov	r0, r9
 80ae6a8:	f7f3 fe8e 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
 80ae6ac:	2102      	movs	r1, #2
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
 80ae6ae:	901b      	str	r0, [sp, #108]	; 0x6c
  const int output_width = output_shape.Dims(2);
 80ae6b0:	4648      	mov	r0, r9
 80ae6b2:	f7f3 fe89 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
 80ae6b6:	9b0e      	ldr	r3, [sp, #56]	; 0x38
 80ae6b8:	9a09      	ldr	r2, [sp, #36]	; 0x24
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
 80ae6ba:	901c      	str	r0, [sp, #112]	; 0x70
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
 80ae6bc:	4353      	muls	r3, r2
 80ae6be:	429c      	cmp	r4, r3
 80ae6c0:	d1bf      	bne.n	80ae642 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x48>
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
 80ae6c2:	982b      	ldr	r0, [sp, #172]	; 0xac
 80ae6c4:	f7f9 f9cb 	bl	80a7a5e <_ZNK6tflite12RuntimeShape8FlatSizeEv>
 80ae6c8:	4284      	cmp	r4, r0
 80ae6ca:	d1ba      	bne.n	80ae642 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x48>
 80ae6cc:	f04f 0b00 	mov.w	fp, #0

  for (int b = 0; b < batches; ++b) {
 80ae6d0:	9b16      	ldr	r3, [sp, #88]	; 0x58
 80ae6d2:	459b      	cmp	fp, r3
 80ae6d4:	f280 80a8 	bge.w	80ae828 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x22e>
 80ae6d8:	9b15      	ldr	r3, [sp, #84]	; 0x54
 80ae6da:	425b      	negs	r3, r3
 80ae6dc:	930b      	str	r3, [sp, #44]	; 0x2c
 80ae6de:	2300      	movs	r3, #0
 80ae6e0:	9303      	str	r3, [sp, #12]
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80ae6e2:	9b03      	ldr	r3, [sp, #12]
 80ae6e4:	9a1b      	ldr	r2, [sp, #108]	; 0x6c
 80ae6e6:	4293      	cmp	r3, r2
 80ae6e8:	f280 809b 	bge.w	80ae822 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x228>
 80ae6ec:	9b14      	ldr	r3, [sp, #80]	; 0x50
 80ae6ee:	425b      	negs	r3, r3
 80ae6f0:	930a      	str	r3, [sp, #40]	; 0x28
 80ae6f2:	2300      	movs	r3, #0
 80ae6f4:	9304      	str	r3, [sp, #16]
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80ae6f6:	9b04      	ldr	r3, [sp, #16]
 80ae6f8:	9a1c      	ldr	r2, [sp, #112]	; 0x70
 80ae6fa:	4293      	cmp	r3, r2
 80ae6fc:	f280 8089 	bge.w	80ae812 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x218>
 80ae700:	2400      	movs	r4, #0
 80ae702:	9405      	str	r4, [sp, #20]
        for (int ic = 0; ic < input_depth; ++ic) {
 80ae704:	9b05      	ldr	r3, [sp, #20]
 80ae706:	9a0e      	ldr	r2, [sp, #56]	; 0x38
 80ae708:	4293      	cmp	r3, r2
 80ae70a:	da7a      	bge.n	80ae802 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x208>
 80ae70c:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
 80ae70e:	2500      	movs	r5, #0
 80ae710:	eb03 0384 	add.w	r3, r3, r4, lsl #2
 80ae714:	931e      	str	r3, [sp, #120]	; 0x78
          for (int m = 0; m < depth_multiplier; m++) {
 80ae716:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80ae718:	429d      	cmp	r5, r3
 80ae71a:	da6c      	bge.n	80ae7f6 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x1fc>
 80ae71c:	1963      	adds	r3, r4, r5
 80ae71e:	930f      	str	r3, [sp, #60]	; 0x3c
            const int oc = m + ic * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            float total = 0.f;
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
 80ae720:	2300      	movs	r3, #0
 80ae722:	9306      	str	r3, [sp, #24]

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int ic = 0; ic < input_depth; ++ic) {
          for (int m = 0; m < depth_multiplier; m++) {
 80ae724:	9e0b      	ldr	r6, [sp, #44]	; 0x2c
            const int oc = m + ic * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            float total = 0.f;
 80ae726:	2300      	movs	r3, #0
 80ae728:	9308      	str	r3, [sp, #32]
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
 80ae72a:	9b06      	ldr	r3, [sp, #24]
 80ae72c:	9a19      	ldr	r2, [sp, #100]	; 0x64
 80ae72e:	4293      	cmp	r3, r2
 80ae730:	da3b      	bge.n	80ae7aa <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x1b0>
 80ae732:	2300      	movs	r3, #0
 80ae734:	9f0a      	ldr	r7, [sp, #40]	; 0x28
 80ae736:	9307      	str	r3, [sp, #28]
              for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
 80ae738:	9b07      	ldr	r3, [sp, #28]
 80ae73a:	9a1a      	ldr	r2, [sp, #104]	; 0x68
 80ae73c:	4293      	cmp	r3, r2
 80ae73e:	da2e      	bge.n	80ae79e <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x1a4>
                const int in_x = in_x_origin + dilation_width_factor * filter_x;
                const int in_y =
                    in_y_origin + dilation_height_factor * filter_y;
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
 80ae740:	2f00      	cmp	r7, #0
 80ae742:	db26      	blt.n	80ae792 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x198>
 80ae744:	9b18      	ldr	r3, [sp, #96]	; 0x60
 80ae746:	42bb      	cmp	r3, r7
 80ae748:	dd23      	ble.n	80ae792 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x198>
 80ae74a:	2e00      	cmp	r6, #0
 80ae74c:	db21      	blt.n	80ae792 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x198>
 80ae74e:	9b17      	ldr	r3, [sp, #92]	; 0x5c
 80ae750:	42b3      	cmp	r3, r6
 80ae752:	dd1e      	ble.n	80ae792 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x198>
                    (in_y < input_height)) {
                  float input_value =
                      input_data[Offset(input_shape, b, in_y, in_x, ic)];
 80ae754:	9b05      	ldr	r3, [sp, #20]
 80ae756:	4632      	mov	r2, r6
 80ae758:	9300      	str	r3, [sp, #0]
 80ae75a:	4659      	mov	r1, fp
 80ae75c:	463b      	mov	r3, r7
 80ae75e:	4650      	mov	r0, sl
 80ae760:	f7f3 fe97 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                  float filter_value = filter_data[Offset(
 80ae764:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height)) {
                  float input_value =
                      input_data[Offset(input_shape, b, in_y, in_x, ic)];
 80ae766:	901d      	str	r0, [sp, #116]	; 0x74
                  float filter_value = filter_data[Offset(
 80ae768:	9300      	str	r3, [sp, #0]
 80ae76a:	9a06      	ldr	r2, [sp, #24]
 80ae76c:	9b07      	ldr	r3, [sp, #28]
 80ae76e:	2100      	movs	r1, #0
 80ae770:	4640      	mov	r0, r8
 80ae772:	f7f3 fe8e 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                      filter_shape, 0, filter_y, filter_x, oc)];
                  total += (input_value * filter_value);
 80ae776:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
 80ae778:	9a1d      	ldr	r2, [sp, #116]	; 0x74
 80ae77a:	f853 1020 	ldr.w	r1, [r3, r0, lsl #2]
 80ae77e:	9b1f      	ldr	r3, [sp, #124]	; 0x7c
 80ae780:	f853 0022 	ldr.w	r0, [r3, r2, lsl #2]
 80ae784:	f004 ff32 	bl	80b35ec <__aeabi_fmul>
 80ae788:	4601      	mov	r1, r0
 80ae78a:	9808      	ldr	r0, [sp, #32]
 80ae78c:	f004 fe26 	bl	80b33dc <__addsf3>
 80ae790:	9008      	str	r0, [sp, #32]
            const int oc = m + ic * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            float total = 0.f;
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
              for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
 80ae792:	9b07      	ldr	r3, [sp, #28]
 80ae794:	3301      	adds	r3, #1
 80ae796:	9307      	str	r3, [sp, #28]
 80ae798:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80ae79a:	441f      	add	r7, r3
 80ae79c:	e7cc      	b.n	80ae738 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x13e>
          for (int m = 0; m < depth_multiplier; m++) {
            const int oc = m + ic * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            float total = 0.f;
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
 80ae79e:	9b06      	ldr	r3, [sp, #24]
 80ae7a0:	3301      	adds	r3, #1
 80ae7a2:	9306      	str	r3, [sp, #24]
 80ae7a4:	9b13      	ldr	r3, [sp, #76]	; 0x4c
 80ae7a6:	441e      	add	r6, r3
 80ae7a8:	e7bf      	b.n	80ae72a <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x130>
                  total += (input_value * filter_value);
                }
              }
            }
            float bias_value = 0.0f;
            if (bias_data) {
 80ae7aa:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
 80ae7ac:	b11b      	cbz	r3, 80ae7b6 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x1bc>
              bias_value = bias_data[oc];
 80ae7ae:	9b1e      	ldr	r3, [sp, #120]	; 0x78
 80ae7b0:	f853 6025 	ldr.w	r6, [r3, r5, lsl #2]
 80ae7b4:	e000      	b.n	80ae7b8 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x1be>
                      filter_shape, 0, filter_y, filter_x, oc)];
                  total += (input_value * filter_value);
                }
              }
            }
            float bias_value = 0.0f;
 80ae7b6:	2600      	movs	r6, #0
            if (bias_data) {
              bias_value = bias_data[oc];
            }
            output_data[Offset(output_shape, b, out_y, out_x, oc)] =
 80ae7b8:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80ae7ba:	9a03      	ldr	r2, [sp, #12]
 80ae7bc:	9300      	str	r3, [sp, #0]
 80ae7be:	4659      	mov	r1, fp
 80ae7c0:	9b04      	ldr	r3, [sp, #16]
 80ae7c2:	4648      	mov	r0, r9
 80ae7c4:	f7f3 fe65 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                ActivationFunctionWithMinMax(total + bias_value,
 80ae7c8:	4631      	mov	r1, r6
            }
            float bias_value = 0.0f;
            if (bias_data) {
              bias_value = bias_data[oc];
            }
            output_data[Offset(output_shape, b, out_y, out_x, oc)] =
 80ae7ca:	4607      	mov	r7, r0
                ActivationFunctionWithMinMax(total + bias_value,
 80ae7cc:	9808      	ldr	r0, [sp, #32]
 80ae7ce:	f004 fe05 	bl	80b33dc <__addsf3>
 80ae7d2:	4606      	mov	r6, r0
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
 80ae7d4:	4601      	mov	r1, r0
 80ae7d6:	980c      	ldr	r0, [sp, #48]	; 0x30
 80ae7d8:	f005 f8c4 	bl	80b3964 <__aeabi_fcmpgt>
 80ae7dc:	b100      	cbz	r0, 80ae7e0 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x1e6>
	return __b;
 80ae7de:	9e0c      	ldr	r6, [sp, #48]	; 0x30
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
 80ae7e0:	4631      	mov	r1, r6
 80ae7e2:	980d      	ldr	r0, [sp, #52]	; 0x34
 80ae7e4:	f005 f8a0 	bl	80b3928 <__aeabi_fcmplt>
 80ae7e8:	b100      	cbz	r0, 80ae7ec <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x1f2>
	return __b;
 80ae7ea:	9e0d      	ldr	r6, [sp, #52]	; 0x34
                                             output_activation_min,
                                             output_activation_max);
 80ae7ec:	9b2e      	ldr	r3, [sp, #184]	; 0xb8

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int ic = 0; ic < input_depth; ++ic) {
          for (int m = 0; m < depth_multiplier; m++) {
 80ae7ee:	3501      	adds	r5, #1
              bias_value = bias_data[oc];
            }
            output_data[Offset(output_shape, b, out_y, out_x, oc)] =
                ActivationFunctionWithMinMax(total + bias_value,
                                             output_activation_min,
                                             output_activation_max);
 80ae7f0:	f843 6027 	str.w	r6, [r3, r7, lsl #2]

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int ic = 0; ic < input_depth; ++ic) {
          for (int m = 0; m < depth_multiplier; m++) {
 80ae7f4:	e78f      	b.n	80ae716 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x11c>
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int ic = 0; ic < input_depth; ++ic) {
 80ae7f6:	9b05      	ldr	r3, [sp, #20]
 80ae7f8:	3301      	adds	r3, #1
 80ae7fa:	9305      	str	r3, [sp, #20]
 80ae7fc:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80ae7fe:	441c      	add	r4, r3
 80ae800:	e780      	b.n	80ae704 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x10a>
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80ae802:	9b04      	ldr	r3, [sp, #16]
 80ae804:	9a10      	ldr	r2, [sp, #64]	; 0x40
 80ae806:	3301      	adds	r3, #1
 80ae808:	9304      	str	r3, [sp, #16]
 80ae80a:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80ae80c:	4413      	add	r3, r2
 80ae80e:	930a      	str	r3, [sp, #40]	; 0x28
 80ae810:	e771      	b.n	80ae6f6 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0xfc>
  const int output_width = output_shape.Dims(2);
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80ae812:	9b03      	ldr	r3, [sp, #12]
 80ae814:	9a11      	ldr	r2, [sp, #68]	; 0x44
 80ae816:	3301      	adds	r3, #1
 80ae818:	9303      	str	r3, [sp, #12]
 80ae81a:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80ae81c:	4413      	add	r3, r2
 80ae81e:	930b      	str	r3, [sp, #44]	; 0x2c
 80ae820:	e75f      	b.n	80ae6e2 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0xe8>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int b = 0; b < batches; ++b) {
 80ae822:	f10b 0b01 	add.w	fp, fp, #1
 80ae826:	e753      	b.n	80ae6d0 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0xd6>
          }
        }
      }
    }
  }
}
 80ae828:	b021      	add	sp, #132	; 0x84
 80ae82a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

080ae830 <_ZN6tflite3ops5micro26Register_DEPTHWISE_CONV_2DEv>:

TfLiteRegistration* Register_DEPTHWISE_CONV_2D() {
  static TfLiteRegistration r = {depthwise_conv::Init, depthwise_conv::Free,
                                 depthwise_conv::Prepare, depthwise_conv::Eval};
  return &r;
}
 80ae830:	4800      	ldr	r0, [pc, #0]	; (80ae834 <_ZN6tflite3ops5micro26Register_DEPTHWISE_CONV_2DEv+0x4>)
 80ae832:	4770      	bx	lr
 80ae834:	20000508 	.word	0x20000508

080ae838 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph>:
         right_shift;
}

template <DepthwiseConvOutputRounding output_rounding>
struct DepthwiseConvBasicKernel {
  static inline void Run(const DepthwiseParams& params,
 80ae838:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80ae83c:	469a      	mov	sl, r3
                         const uint8* input_data,
                         const RuntimeShape& filter_shape,
                         const uint8* filter_data,
                         const RuntimeShape& bias_shape, const int32* bias_data,
                         const RuntimeShape& output_shape, uint8* output_data) {
    const int stride_width = params.stride_width;
 80ae83e:	f9b0 300a 	ldrsh.w	r3, [r0, #10]
         right_shift;
}

template <DepthwiseConvOutputRounding output_rounding>
struct DepthwiseConvBasicKernel {
  static inline void Run(const DepthwiseParams& params,
 80ae842:	b0a5      	sub	sp, #148	; 0x94
                         const uint8* input_data,
                         const RuntimeShape& filter_shape,
                         const uint8* filter_data,
                         const RuntimeShape& bias_shape, const int32* bias_data,
                         const RuntimeShape& output_shape, uint8* output_data) {
    const int stride_width = params.stride_width;
 80ae844:	930f      	str	r3, [sp, #60]	; 0x3c
    const int stride_height = params.stride_height;
 80ae846:	f9b0 300c 	ldrsh.w	r3, [r0, #12]
         right_shift;
}

template <DepthwiseConvOutputRounding output_rounding>
struct DepthwiseConvBasicKernel {
  static inline void Run(const DepthwiseParams& params,
 80ae84a:	4689      	mov	r9, r1
                         const RuntimeShape& filter_shape,
                         const uint8* filter_data,
                         const RuntimeShape& bias_shape, const int32* bias_data,
                         const RuntimeShape& output_shape, uint8* output_data) {
    const int stride_width = params.stride_width;
    const int stride_height = params.stride_height;
 80ae84c:	9310      	str	r3, [sp, #64]	; 0x40
    const int dilation_width_factor = params.dilation_width_factor;
 80ae84e:	f9b0 300e 	ldrsh.w	r3, [r0, #14]
         right_shift;
}

template <DepthwiseConvOutputRounding output_rounding>
struct DepthwiseConvBasicKernel {
  static inline void Run(const DepthwiseParams& params,
 80ae852:	9223      	str	r2, [sp, #140]	; 0x8c
                         const uint8* filter_data,
                         const RuntimeShape& bias_shape, const int32* bias_data,
                         const RuntimeShape& output_shape, uint8* output_data) {
    const int stride_width = params.stride_width;
    const int stride_height = params.stride_height;
    const int dilation_width_factor = params.dilation_width_factor;
 80ae854:	9311      	str	r3, [sp, #68]	; 0x44
    const int dilation_height_factor = params.dilation_height_factor;
 80ae856:	f9b0 3010 	ldrsh.w	r3, [r0, #16]
 80ae85a:	9312      	str	r3, [sp, #72]	; 0x48
    const int pad_width = params.padding_values.width;
 80ae85c:	f9b0 3002 	ldrsh.w	r3, [r0, #2]
 80ae860:	9313      	str	r3, [sp, #76]	; 0x4c
    const int pad_height = params.padding_values.height;
 80ae862:	f9b0 3004 	ldrsh.w	r3, [r0, #4]
 80ae866:	9314      	str	r3, [sp, #80]	; 0x50
    const int depth_multiplier = params.depth_multiplier;
 80ae868:	f9b0 3012 	ldrsh.w	r3, [r0, #18]
 80ae86c:	9308      	str	r3, [sp, #32]
    const int32 output_activation_min = params.quantized_activation_min;
 80ae86e:	6a83      	ldr	r3, [r0, #40]	; 0x28
 80ae870:	930b      	str	r3, [sp, #44]	; 0x2c
    const int32 output_activation_max = params.quantized_activation_max;
 80ae872:	6ac3      	ldr	r3, [r0, #44]	; 0x2c
 80ae874:	930c      	str	r3, [sp, #48]	; 0x30
    const int32 input_offset = params.input_offset;
 80ae876:	6943      	ldr	r3, [r0, #20]
 80ae878:	9315      	str	r3, [sp, #84]	; 0x54
    const int32 filter_offset = params.weights_offset;
 80ae87a:	6983      	ldr	r3, [r0, #24]
 80ae87c:	9316      	str	r3, [sp, #88]	; 0x58
    const int32 output_offset = params.output_offset;
 80ae87e:	69c3      	ldr	r3, [r0, #28]
 80ae880:	9317      	str	r3, [sp, #92]	; 0x5c
    const int32 output_multiplier = params.output_multiplier;
 80ae882:	6a03      	ldr	r3, [r0, #32]
 80ae884:	9318      	str	r3, [sp, #96]	; 0x60
    const int output_shift = params.output_shift;
 80ae886:	6a43      	ldr	r3, [r0, #36]	; 0x24
 80ae888:	9319      	str	r3, [sp, #100]	; 0x64
    TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
 80ae88a:	680b      	ldr	r3, [r1, #0]
 80ae88c:	2b04      	cmp	r3, #4
 80ae88e:	d001      	beq.n	80ae894 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x5c>
 80ae890:	f001 fbce 	bl	80b0030 <abort>
    TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
 80ae894:	f8da 3000 	ldr.w	r3, [sl]
 80ae898:	2b04      	cmp	r3, #4
 80ae89a:	d1f9      	bne.n	80ae890 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x58>
    TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
 80ae89c:	9b31      	ldr	r3, [sp, #196]	; 0xc4
 80ae89e:	681b      	ldr	r3, [r3, #0]
 80ae8a0:	2b04      	cmp	r3, #4
 80ae8a2:	d1f5      	bne.n	80ae890 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x58>

    TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
 80ae8a4:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80ae8a6:	9a0c      	ldr	r2, [sp, #48]	; 0x30
 80ae8a8:	4293      	cmp	r3, r2
 80ae8aa:	dcf1      	bgt.n	80ae890 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x58>
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80ae8ac:	2300      	movs	r3, #0
 80ae8ae:	4619      	mov	r1, r3
 80ae8b0:	9a31      	ldr	r2, [sp, #196]	; 0xc4
 80ae8b2:	4648      	mov	r0, r9
 80ae8b4:	f7f9 f8e3 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
 80ae8b8:	2303      	movs	r3, #3
 80ae8ba:	4619      	mov	r1, r3
 80ae8bc:	9a31      	ldr	r2, [sp, #196]	; 0xc4
    TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
    TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
    TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

    TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80ae8be:	901a      	str	r0, [sp, #104]	; 0x68
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
 80ae8c0:	4650      	mov	r0, sl
 80ae8c2:	f7f9 f8dc 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
    const int input_height = input_shape.Dims(1);
 80ae8c6:	2101      	movs	r1, #1
    TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
    TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

    TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
 80ae8c8:	4604      	mov	r4, r0
    const int input_height = input_shape.Dims(1);
 80ae8ca:	4648      	mov	r0, r9
 80ae8cc:	f7f3 fd7c 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
    const int input_width = input_shape.Dims(2);
 80ae8d0:	2102      	movs	r1, #2
    TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

    TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
    const int input_height = input_shape.Dims(1);
 80ae8d2:	901b      	str	r0, [sp, #108]	; 0x6c
    const int input_width = input_shape.Dims(2);
 80ae8d4:	4648      	mov	r0, r9
 80ae8d6:	f7f3 fd77 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
    const int input_depth = input_shape.Dims(3);
 80ae8da:	2103      	movs	r1, #3

    TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
    const int input_height = input_shape.Dims(1);
    const int input_width = input_shape.Dims(2);
 80ae8dc:	901c      	str	r0, [sp, #112]	; 0x70
    const int input_depth = input_shape.Dims(3);
 80ae8de:	4648      	mov	r0, r9
 80ae8e0:	f7f3 fd72 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
    const int filter_height = filter_shape.Dims(1);
 80ae8e4:	2101      	movs	r1, #1
    TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
    const int input_height = input_shape.Dims(1);
    const int input_width = input_shape.Dims(2);
    const int input_depth = input_shape.Dims(3);
 80ae8e6:	900d      	str	r0, [sp, #52]	; 0x34
    const int filter_height = filter_shape.Dims(1);
 80ae8e8:	4650      	mov	r0, sl
 80ae8ea:	f7f3 fd6d 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
    const int filter_width = filter_shape.Dims(2);
 80ae8ee:	2102      	movs	r1, #2
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
    const int input_height = input_shape.Dims(1);
    const int input_width = input_shape.Dims(2);
    const int input_depth = input_shape.Dims(3);
    const int filter_height = filter_shape.Dims(1);
 80ae8f0:	901d      	str	r0, [sp, #116]	; 0x74
    const int filter_width = filter_shape.Dims(2);
 80ae8f2:	4650      	mov	r0, sl
 80ae8f4:	f7f3 fd68 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
    const int output_height = output_shape.Dims(1);
 80ae8f8:	2101      	movs	r1, #1
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
    const int input_height = input_shape.Dims(1);
    const int input_width = input_shape.Dims(2);
    const int input_depth = input_shape.Dims(3);
    const int filter_height = filter_shape.Dims(1);
    const int filter_width = filter_shape.Dims(2);
 80ae8fa:	901e      	str	r0, [sp, #120]	; 0x78
    const int output_height = output_shape.Dims(1);
 80ae8fc:	9831      	ldr	r0, [sp, #196]	; 0xc4
 80ae8fe:	f7f3 fd63 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
    const int output_width = output_shape.Dims(2);
 80ae902:	2102      	movs	r1, #2
    const int input_height = input_shape.Dims(1);
    const int input_width = input_shape.Dims(2);
    const int input_depth = input_shape.Dims(3);
    const int filter_height = filter_shape.Dims(1);
    const int filter_width = filter_shape.Dims(2);
    const int output_height = output_shape.Dims(1);
 80ae904:	901f      	str	r0, [sp, #124]	; 0x7c
    const int output_width = output_shape.Dims(2);
 80ae906:	9831      	ldr	r0, [sp, #196]	; 0xc4
 80ae908:	f7f3 fd5e 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
    TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
 80ae90c:	9b0d      	ldr	r3, [sp, #52]	; 0x34
 80ae90e:	9a08      	ldr	r2, [sp, #32]
    const int input_width = input_shape.Dims(2);
    const int input_depth = input_shape.Dims(3);
    const int filter_height = filter_shape.Dims(1);
    const int filter_width = filter_shape.Dims(2);
    const int output_height = output_shape.Dims(1);
    const int output_width = output_shape.Dims(2);
 80ae910:	9020      	str	r0, [sp, #128]	; 0x80
    TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
 80ae912:	4353      	muls	r3, r2
 80ae914:	429c      	cmp	r4, r3
 80ae916:	d1bb      	bne.n	80ae890 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x58>
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
 80ae918:	982f      	ldr	r0, [sp, #188]	; 0xbc
 80ae91a:	f7f9 f8a0 	bl	80a7a5e <_ZNK6tflite12RuntimeShape8FlatSizeEv>
 80ae91e:	4284      	cmp	r4, r0
 80ae920:	d1b6      	bne.n	80ae890 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x58>
 80ae922:	f04f 0b00 	mov.w	fp, #0

    for (int b = 0; b < batches; ++b) {
 80ae926:	9b1a      	ldr	r3, [sp, #104]	; 0x68
 80ae928:	459b      	cmp	fp, r3
 80ae92a:	f280 80a1 	bge.w	80aea70 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x238>
 80ae92e:	9b14      	ldr	r3, [sp, #80]	; 0x50
 80ae930:	425b      	negs	r3, r3
 80ae932:	9309      	str	r3, [sp, #36]	; 0x24
 80ae934:	2300      	movs	r3, #0
 80ae936:	9303      	str	r3, [sp, #12]
      for (int out_y = 0; out_y < output_height; ++out_y) {
 80ae938:	9b03      	ldr	r3, [sp, #12]
 80ae93a:	9a1f      	ldr	r2, [sp, #124]	; 0x7c
 80ae93c:	4293      	cmp	r3, r2
 80ae93e:	f280 8094 	bge.w	80aea6a <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x232>
 80ae942:	9b13      	ldr	r3, [sp, #76]	; 0x4c
 80ae944:	425b      	negs	r3, r3
 80ae946:	930a      	str	r3, [sp, #40]	; 0x28
 80ae948:	2300      	movs	r3, #0
 80ae94a:	9304      	str	r3, [sp, #16]
        for (int out_x = 0; out_x < output_width; ++out_x) {
 80ae94c:	9b04      	ldr	r3, [sp, #16]
 80ae94e:	9a20      	ldr	r2, [sp, #128]	; 0x80
 80ae950:	4293      	cmp	r3, r2
 80ae952:	f280 8082 	bge.w	80aea5a <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x222>
 80ae956:	2500      	movs	r5, #0
 80ae958:	9505      	str	r5, [sp, #20]
          for (int ic = 0; ic < input_depth; ++ic) {
 80ae95a:	9b05      	ldr	r3, [sp, #20]
 80ae95c:	9a0d      	ldr	r2, [sp, #52]	; 0x34
 80ae95e:	4293      	cmp	r3, r2
 80ae960:	da73      	bge.n	80aea4a <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x212>
 80ae962:	9b30      	ldr	r3, [sp, #192]	; 0xc0
 80ae964:	2600      	movs	r6, #0
 80ae966:	eb03 0385 	add.w	r3, r3, r5, lsl #2
 80ae96a:	9322      	str	r3, [sp, #136]	; 0x88
            for (int m = 0; m < depth_multiplier; m++) {
 80ae96c:	9b08      	ldr	r3, [sp, #32]
 80ae96e:	429e      	cmp	r6, r3
 80ae970:	da65      	bge.n	80aea3e <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x206>
 80ae972:	1973      	adds	r3, r6, r5
              const int oc = m + ic * depth_multiplier;
              const int in_x_origin = (out_x * stride_width) - pad_width;
              const int in_y_origin = (out_y * stride_height) - pad_height;
              int32 acc = 0;
 80ae974:	2400      	movs	r4, #0

    for (int b = 0; b < batches; ++b) {
      for (int out_y = 0; out_y < output_height; ++out_y) {
        for (int out_x = 0; out_x < output_width; ++out_x) {
          for (int ic = 0; ic < input_depth; ++ic) {
            for (int m = 0; m < depth_multiplier; m++) {
 80ae976:	9f09      	ldr	r7, [sp, #36]	; 0x24
 80ae978:	930e      	str	r3, [sp, #56]	; 0x38
              const int oc = m + ic * depth_multiplier;
              const int in_x_origin = (out_x * stride_width) - pad_width;
              const int in_y_origin = (out_y * stride_height) - pad_height;
              int32 acc = 0;
              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
 80ae97a:	9406      	str	r4, [sp, #24]
 80ae97c:	9b06      	ldr	r3, [sp, #24]
 80ae97e:	9a1d      	ldr	r2, [sp, #116]	; 0x74
 80ae980:	4293      	cmp	r3, r2
 80ae982:	da3a      	bge.n	80ae9fa <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1c2>
 80ae984:	2300      	movs	r3, #0
 80ae986:	f8dd 8028 	ldr.w	r8, [sp, #40]	; 0x28
 80ae98a:	9307      	str	r3, [sp, #28]
                for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
 80ae98c:	9b07      	ldr	r3, [sp, #28]
 80ae98e:	9a1e      	ldr	r2, [sp, #120]	; 0x78
 80ae990:	4293      	cmp	r3, r2
 80ae992:	da2c      	bge.n	80ae9ee <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1b6>
                      in_x_origin + dilation_width_factor * filter_x;
                  const int in_y =
                      in_y_origin + dilation_height_factor * filter_y;
                  // If the location is outside the bounds of the input image,
                  // use zero as a default value.
                  if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
 80ae994:	f1b8 0f00 	cmp.w	r8, #0
 80ae998:	db23      	blt.n	80ae9e2 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1aa>
 80ae99a:	9b1c      	ldr	r3, [sp, #112]	; 0x70
 80ae99c:	4543      	cmp	r3, r8
 80ae99e:	dd20      	ble.n	80ae9e2 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1aa>
 80ae9a0:	2f00      	cmp	r7, #0
 80ae9a2:	db1e      	blt.n	80ae9e2 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1aa>
 80ae9a4:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
 80ae9a6:	42bb      	cmp	r3, r7
 80ae9a8:	dd1b      	ble.n	80ae9e2 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1aa>
                      (in_y < input_height)) {
                    int32 input_val =
                        input_data[Offset(input_shape, b, in_y, in_x, ic)];
 80ae9aa:	9b05      	ldr	r3, [sp, #20]
 80ae9ac:	463a      	mov	r2, r7
 80ae9ae:	9300      	str	r3, [sp, #0]
 80ae9b0:	4659      	mov	r1, fp
 80ae9b2:	4643      	mov	r3, r8
 80ae9b4:	4648      	mov	r0, r9
 80ae9b6:	f7f3 fd6c 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                    int32 filter_val = filter_data[Offset(
 80ae9ba:	9b0e      	ldr	r3, [sp, #56]	; 0x38
                  // If the location is outside the bounds of the input image,
                  // use zero as a default value.
                  if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                      (in_y < input_height)) {
                    int32 input_val =
                        input_data[Offset(input_shape, b, in_y, in_x, ic)];
 80ae9bc:	9021      	str	r0, [sp, #132]	; 0x84
                    int32 filter_val = filter_data[Offset(
 80ae9be:	9300      	str	r3, [sp, #0]
 80ae9c0:	9a06      	ldr	r2, [sp, #24]
 80ae9c2:	9b07      	ldr	r3, [sp, #28]
 80ae9c4:	2100      	movs	r1, #0
 80ae9c6:	4650      	mov	r0, sl
 80ae9c8:	f7f3 fd63 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                        filter_shape, 0, filter_y, filter_x, oc)];
                    acc += (filter_val + filter_offset) *
 80ae9cc:	9b2e      	ldr	r3, [sp, #184]	; 0xb8
 80ae9ce:	9a16      	ldr	r2, [sp, #88]	; 0x58
 80ae9d0:	5c1b      	ldrb	r3, [r3, r0]
 80ae9d2:	9921      	ldr	r1, [sp, #132]	; 0x84
 80ae9d4:	4413      	add	r3, r2
 80ae9d6:	9a23      	ldr	r2, [sp, #140]	; 0x8c
 80ae9d8:	5c52      	ldrb	r2, [r2, r1]
 80ae9da:	9915      	ldr	r1, [sp, #84]	; 0x54
 80ae9dc:	440a      	add	r2, r1
 80ae9de:	fb02 4403 	mla	r4, r2, r3, r4
              const int oc = m + ic * depth_multiplier;
              const int in_x_origin = (out_x * stride_width) - pad_width;
              const int in_y_origin = (out_y * stride_height) - pad_height;
              int32 acc = 0;
              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
                for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
 80ae9e2:	9b07      	ldr	r3, [sp, #28]
 80ae9e4:	3301      	adds	r3, #1
 80ae9e6:	9307      	str	r3, [sp, #28]
 80ae9e8:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80ae9ea:	4498      	add	r8, r3
 80ae9ec:	e7ce      	b.n	80ae98c <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x154>
            for (int m = 0; m < depth_multiplier; m++) {
              const int oc = m + ic * depth_multiplier;
              const int in_x_origin = (out_x * stride_width) - pad_width;
              const int in_y_origin = (out_y * stride_height) - pad_height;
              int32 acc = 0;
              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
 80ae9ee:	9b06      	ldr	r3, [sp, #24]
 80ae9f0:	3301      	adds	r3, #1
 80ae9f2:	9306      	str	r3, [sp, #24]
 80ae9f4:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80ae9f6:	441f      	add	r7, r3
 80ae9f8:	e7c0      	b.n	80ae97c <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x144>
                    acc += (filter_val + filter_offset) *
                           (input_val + input_offset);
                  }
                }
              }
              if (bias_data) {
 80ae9fa:	9b30      	ldr	r3, [sp, #192]	; 0xc0
 80ae9fc:	b11b      	cbz	r3, 80aea06 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1ce>
                acc += bias_data[oc];
 80ae9fe:	9b22      	ldr	r3, [sp, #136]	; 0x88
 80aea00:	f853 3026 	ldr.w	r3, [r3, r6, lsl #2]
 80aea04:	441c      	add	r4, r3
}

template <>
inline int32 DepthwiseConvRound<DepthwiseConvOutputRounding::kAwayFromZero>(
    int32 x, int32 quantized_multiplier, int shift) {
  return MultiplyByQuantizedMultiplier(x, quantized_multiplier, shift);
 80aea06:	9a19      	ldr	r2, [sp, #100]	; 0x64
 80aea08:	9918      	ldr	r1, [sp, #96]	; 0x60
 80aea0a:	4620      	mov	r0, r4
 80aea0c:	f7f9 f846 	bl	80a7a9c <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
              if (bias_data) {
                acc += bias_data[oc];
              }
              acc = DepthwiseConvRound<output_rounding>(acc, output_multiplier,
                                                        output_shift);
              acc += output_offset;
 80aea10:	9b17      	ldr	r3, [sp, #92]	; 0x5c
              acc = std::max(acc, output_activation_min);
              acc = std::min(acc, output_activation_max);
              output_data[Offset(output_shape, b, out_y, out_x, oc)] =
 80aea12:	9a03      	ldr	r2, [sp, #12]
              if (bias_data) {
                acc += bias_data[oc];
              }
              acc = DepthwiseConvRound<output_rounding>(acc, output_multiplier,
                                                        output_shift);
              acc += output_offset;
 80aea14:	4418      	add	r0, r3
 80aea16:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
              acc = std::max(acc, output_activation_min);
              acc = std::min(acc, output_activation_max);
              output_data[Offset(output_shape, b, out_y, out_x, oc)] =
 80aea18:	4659      	mov	r1, fp
 80aea1a:	4283      	cmp	r3, r0
 80aea1c:	bfb8      	it	lt
 80aea1e:	4603      	movlt	r3, r0
 80aea20:	461c      	mov	r4, r3
 80aea22:	9b0c      	ldr	r3, [sp, #48]	; 0x30
 80aea24:	9831      	ldr	r0, [sp, #196]	; 0xc4
 80aea26:	429c      	cmp	r4, r3
 80aea28:	bfa8      	it	ge
 80aea2a:	461c      	movge	r4, r3
 80aea2c:	9b0e      	ldr	r3, [sp, #56]	; 0x38

    for (int b = 0; b < batches; ++b) {
      for (int out_y = 0; out_y < output_height; ++out_y) {
        for (int out_x = 0; out_x < output_width; ++out_x) {
          for (int ic = 0; ic < input_depth; ++ic) {
            for (int m = 0; m < depth_multiplier; m++) {
 80aea2e:	3601      	adds	r6, #1
              acc = DepthwiseConvRound<output_rounding>(acc, output_multiplier,
                                                        output_shift);
              acc += output_offset;
              acc = std::max(acc, output_activation_min);
              acc = std::min(acc, output_activation_max);
              output_data[Offset(output_shape, b, out_y, out_x, oc)] =
 80aea30:	9300      	str	r3, [sp, #0]
 80aea32:	9b04      	ldr	r3, [sp, #16]
 80aea34:	f7f3 fd2d 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80aea38:	9b32      	ldr	r3, [sp, #200]	; 0xc8
 80aea3a:	541c      	strb	r4, [r3, r0]

    for (int b = 0; b < batches; ++b) {
      for (int out_y = 0; out_y < output_height; ++out_y) {
        for (int out_x = 0; out_x < output_width; ++out_x) {
          for (int ic = 0; ic < input_depth; ++ic) {
            for (int m = 0; m < depth_multiplier; m++) {
 80aea3c:	e796      	b.n	80ae96c <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x134>
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

    for (int b = 0; b < batches; ++b) {
      for (int out_y = 0; out_y < output_height; ++out_y) {
        for (int out_x = 0; out_x < output_width; ++out_x) {
          for (int ic = 0; ic < input_depth; ++ic) {
 80aea3e:	9b05      	ldr	r3, [sp, #20]
 80aea40:	3301      	adds	r3, #1
 80aea42:	9305      	str	r3, [sp, #20]
 80aea44:	9b08      	ldr	r3, [sp, #32]
 80aea46:	441d      	add	r5, r3
 80aea48:	e787      	b.n	80ae95a <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x122>
    TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

    for (int b = 0; b < batches; ++b) {
      for (int out_y = 0; out_y < output_height; ++out_y) {
        for (int out_x = 0; out_x < output_width; ++out_x) {
 80aea4a:	9b04      	ldr	r3, [sp, #16]
 80aea4c:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
 80aea4e:	3301      	adds	r3, #1
 80aea50:	9304      	str	r3, [sp, #16]
 80aea52:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80aea54:	4413      	add	r3, r2
 80aea56:	930a      	str	r3, [sp, #40]	; 0x28
 80aea58:	e778      	b.n	80ae94c <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x114>
    const int output_width = output_shape.Dims(2);
    TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

    for (int b = 0; b < batches; ++b) {
      for (int out_y = 0; out_y < output_height; ++out_y) {
 80aea5a:	9b03      	ldr	r3, [sp, #12]
 80aea5c:	9a10      	ldr	r2, [sp, #64]	; 0x40
 80aea5e:	3301      	adds	r3, #1
 80aea60:	9303      	str	r3, [sp, #12]
 80aea62:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80aea64:	4413      	add	r3, r2
 80aea66:	9309      	str	r3, [sp, #36]	; 0x24
 80aea68:	e766      	b.n	80ae938 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x100>
    const int output_height = output_shape.Dims(1);
    const int output_width = output_shape.Dims(2);
    TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

    for (int b = 0; b < batches; ++b) {
 80aea6a:	f10b 0b01 	add.w	fp, fp, #1
 80aea6e:	e75a      	b.n	80ae926 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0xee>
            }
          }
        }
      }
    }
  }
 80aea70:	b025      	add	sp, #148	; 0x94
 80aea72:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

080aea78 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode>:
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
        GetTensorShape(output), GetTensorData<uint8_t>(output));
  }
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80aea78:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80aea7c:	684b      	ldr	r3, [r1, #4]
 80aea7e:	f5ad 7d69 	sub.w	sp, sp, #932	; 0x3a4
 80aea82:	f8d0 a008 	ldr.w	sl, [r0, #8]
 80aea86:	930c      	str	r3, [sp, #48]	; 0x30
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
 80aea88:	685b      	ldr	r3, [r3, #4]
 80aea8a:	2238      	movs	r2, #56	; 0x38
 80aea8c:	fb02 a303 	mla	r3, r2, r3, sl
 80aea90:	f8d1 b000 	ldr.w	fp, [r1]
 80aea94:	9309      	str	r3, [sp, #36]	; 0x24
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80aea96:	f8db 3004 	ldr.w	r3, [fp, #4]
 80aea9a:	f8db 7008 	ldr.w	r7, [fp, #8]
 80aea9e:	4353      	muls	r3, r2
 80aeaa0:	930b      	str	r3, [sp, #44]	; 0x2c
 80aeaa2:	eb0a 0903 	add.w	r9, sl, r3
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
 80aeaa6:	f8db 3000 	ldr.w	r3, [fp]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80aeaaa:	fb02 a707 	mla	r7, r2, r7, sl

  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
  const TfLiteTensor* bias =
      (NumInputs(node) == 3) ? GetInput(context, node, kBiasTensor) : nullptr;
 80aeaae:	2b03      	cmp	r3, #3
 80aeab0:	bf08      	it	eq
 80aeab2:	f8db 400c 	ldreq.w	r4, [fp, #12]
  }
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  auto* params =
      reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);
 80aeab6:	694d      	ldr	r5, [r1, #20]
 80aeab8:	bf08      	it	eq
 80aeaba:	fb02 a404 	mlaeq	r4, r2, r4, sl
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
  const TfLiteTensor* bias =
      (NumInputs(node) == 3) ? GetInput(context, node, kBiasTensor) : nullptr;

  const TfLiteType data_type = input->type;
 80aeabe:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
        GetTensorShape(output), GetTensorData<uint8_t>(output));
  }
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80aeac0:	910a      	str	r1, [sp, #40]	; 0x28
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
  const TfLiteTensor* bias =
      (NumInputs(node) == 3) ? GetInput(context, node, kBiasTensor) : nullptr;

  const TfLiteType data_type = input->type;
 80aeac2:	f81a 2002 	ldrb.w	r2, [sl, r2]

  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
  const TfLiteTensor* bias =
      (NumInputs(node) == 3) ? GetInput(context, node, kBiasTensor) : nullptr;
 80aeac6:	bf18      	it	ne
 80aeac8:	2400      	movne	r4, #0

  const TfLiteType data_type = input->type;
 80aeaca:	920d      	str	r2, [sp, #52]	; 0x34
 80aeacc:	f8d9 2008 	ldr.w	r2, [r9, #8]
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
        GetTensorShape(output), GetTensorData<uint8_t>(output));
  }
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 80aead0:	4606      	mov	r6, r0

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
inline int SizeOfDimension(const TfLiteTensor* t, int dim) {
  return t->dims->data[dim];
 80aead2:	68d1      	ldr	r1, [r2, #12]
 80aead4:	6892      	ldr	r2, [r2, #8]
 80aead6:	9114      	str	r1, [sp, #80]	; 0x50
 80aead8:	9213      	str	r2, [sp, #76]	; 0x4c
 80aeada:	68ba      	ldr	r2, [r7, #8]
 80aeadc:	68d1      	ldr	r1, [r2, #12]
 80aeade:	6892      	ldr	r2, [r2, #8]
 80aeae0:	9112      	str	r1, [sp, #72]	; 0x48
 80aeae2:	9211      	str	r2, [sp, #68]	; 0x44
  const TfLiteType data_type = input->type;
  int width = SizeOfDimension(input, 2);
  int height = SizeOfDimension(input, 1);
  int filter_width = SizeOfDimension(filter, 2);
  int filter_height = SizeOfDimension(filter, 1);
  int out_width = ComputeOutSize(params->padding, width, filter_width,
 80aeae4:	782a      	ldrb	r2, [r5, #0]
 80aeae6:	920e      	str	r2, [sp, #56]	; 0x38
 80aeae8:	686a      	ldr	r2, [r5, #4]
 80aeaea:	920f      	str	r2, [sp, #60]	; 0x3c
                                 params->stride_width);
  int out_height = ComputeOutSize(params->padding, height, filter_height,
 80aeaec:	68aa      	ldr	r2, [r5, #8]
 80aeaee:	9210      	str	r2, [sp, #64]	; 0x40
                                  params->stride_height);
  OpData data;
  if (input->type != kTfLiteFloat32) {
 80aeaf0:	9a0d      	ldr	r2, [sp, #52]	; 0x34
 80aeaf2:	2a01      	cmp	r2, #1
 80aeaf4:	d02b      	beq.n	80aeb4e <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0xd6>
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
 80aeaf6:	f897 8030 	ldrb.w	r8, [r7, #48]	; 0x30
 80aeafa:	f1b8 0f01 	cmp.w	r8, #1
 80aeafe:	d010      	beq.n	80aeb22 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0xaa>
 80aeb00:	4baa      	ldr	r3, [pc, #680]	; (80aedac <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x334>)
 80aeb02:	2401      	movs	r4, #1
 80aeb04:	9301      	str	r3, [sp, #4]
 80aeb06:	4baa      	ldr	r3, [pc, #680]	; (80aedb0 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x338>)
 80aeb08:	9403      	str	r4, [sp, #12]
 80aeb0a:	9300      	str	r3, [sp, #0]
 80aeb0c:	f8cd 8008 	str.w	r8, [sp, #8]
 80aeb10:	6945      	ldr	r5, [r0, #20]
 80aeb12:	f240 13cd 	movw	r3, #461	; 0x1cd
 80aeb16:	4aa7      	ldr	r2, [pc, #668]	; (80aedb4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x33c>)
 80aeb18:	49a7      	ldr	r1, [pc, #668]	; (80aedb8 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x340>)
 80aeb1a:	47a8      	blx	r5
 80aeb1c:	4620      	mov	r0, r4
 80aeb1e:	f000 bc82 	b.w	80af426 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x9ae>
                      kTfLiteAffineQuantization);

    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
 80aeb22:	6b7a      	ldr	r2, [r7, #52]	; 0x34
    TF_LITE_ENSURE(context, affine_quantization);
 80aeb24:	b92a      	cbnz	r2, 80aeb32 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0xba>
 80aeb26:	4ba5      	ldr	r3, [pc, #660]	; (80aedbc <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x344>)
 80aeb28:	9300      	str	r3, [sp, #0]
 80aeb2a:	6944      	ldr	r4, [r0, #20]
 80aeb2c:	f44f 73e9 	mov.w	r3, #466	; 0x1d2
 80aeb30:	e006      	b.n	80aeb40 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0xc8>
    TF_LITE_ENSURE(context, affine_quantization->scale);
 80aeb32:	6812      	ldr	r2, [r2, #0]
 80aeb34:	b95a      	cbnz	r2, 80aeb4e <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0xd6>
 80aeb36:	4ba2      	ldr	r3, [pc, #648]	; (80aedc0 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x348>)
 80aeb38:	9300      	str	r3, [sp, #0]
 80aeb3a:	f240 13d3 	movw	r3, #467	; 0x1d3
 80aeb3e:	6944      	ldr	r4, [r0, #20]
 80aeb40:	4630      	mov	r0, r6
 80aeb42:	4a9c      	ldr	r2, [pc, #624]	; (80aedb4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x33c>)
 80aeb44:	499f      	ldr	r1, [pc, #636]	; (80aedc4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x34c>)
 80aeb46:	47a0      	blx	r4
 80aeb48:	4640      	mov	r0, r8
 80aeb4a:	f000 bc6c 	b.w	80af426 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x9ae>
                             int height, int filter_width, int filter_height,
                             int out_width, int out_height,
                             const TfLiteType data_type, OpData* data) {
  bool has_bias = node->inputs->size == 3;
  // Check number of inputs/outputs
  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
 80aeb4e:	3b02      	subs	r3, #2
 80aeb50:	2b01      	cmp	r3, #1
 80aeb52:	d908      	bls.n	80aeb66 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0xee>
 80aeb54:	4b9c      	ldr	r3, [pc, #624]	; (80aedc8 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x350>)
 80aeb56:	4a97      	ldr	r2, [pc, #604]	; (80aedb4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x33c>)
 80aeb58:	9300      	str	r3, [sp, #0]
 80aeb5a:	6974      	ldr	r4, [r6, #20]
 80aeb5c:	2343      	movs	r3, #67	; 0x43
 80aeb5e:	4999      	ldr	r1, [pc, #612]	; (80aedc4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x34c>)
 80aeb60:	4630      	mov	r0, r6
 80aeb62:	47a0      	blx	r4
 80aeb64:	e2ad      	b.n	80af0c2 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x64a>
  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);
 80aeb66:	9b0c      	ldr	r3, [sp, #48]	; 0x30
 80aeb68:	681b      	ldr	r3, [r3, #0]
 80aeb6a:	2b01      	cmp	r3, #1
 80aeb6c:	d00d      	beq.n	80aeb8a <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x112>
 80aeb6e:	9302      	str	r3, [sp, #8]
 80aeb70:	4b96      	ldr	r3, [pc, #600]	; (80aedcc <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x354>)
 80aeb72:	2201      	movs	r2, #1
 80aeb74:	9301      	str	r3, [sp, #4]
 80aeb76:	4b96      	ldr	r3, [pc, #600]	; (80aedd0 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x358>)
 80aeb78:	9203      	str	r2, [sp, #12]
 80aeb7a:	9300      	str	r3, [sp, #0]
 80aeb7c:	6974      	ldr	r4, [r6, #20]
 80aeb7e:	2344      	movs	r3, #68	; 0x44
 80aeb80:	4a8c      	ldr	r2, [pc, #560]	; (80aedb4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x33c>)
 80aeb82:	498d      	ldr	r1, [pc, #564]	; (80aedb8 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x340>)
 80aeb84:	4630      	mov	r0, r6
 80aeb86:	47a0      	blx	r4
 80aeb88:	e29b      	b.n	80af0c2 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x64a>
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
 80aeb8a:	696b      	ldr	r3, [r5, #20]
 80aeb8c:	f8d5 8018 	ldr.w	r8, [r5, #24]
 80aeb90:	9315      	str	r3, [sp, #84]	; 0x54

inline TfLitePaddingValues ComputePaddingHeightWidth(
    int stride_height, int stride_width, int dilation_rate_height,
    int dilation_rate_width, int in_height, int in_width, int filter_height,
    int filter_width, TfLitePadding padding, int* out_height, int* out_width) {
  *out_width = ComputeOutSize(padding, in_width, filter_width, stride_width,
 80aeb92:	9300      	str	r3, [sp, #0]
 80aeb94:	9a12      	ldr	r2, [sp, #72]	; 0x48
 80aeb96:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80aeb98:	9914      	ldr	r1, [sp, #80]	; 0x50
 80aeb9a:	980e      	ldr	r0, [sp, #56]	; 0x38
 80aeb9c:	f7f9 fae5 	bl	80a816a <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>
                              dilation_rate_width);
  *out_height = ComputeOutSize(padding, in_height, filter_height, stride_height,
 80aeba0:	f8cd 8000 	str.w	r8, [sp]

inline TfLitePaddingValues ComputePaddingHeightWidth(
    int stride_height, int stride_width, int dilation_rate_height,
    int dilation_rate_width, int in_height, int in_width, int filter_height,
    int filter_width, TfLitePadding padding, int* out_height, int* out_width) {
  *out_width = ComputeOutSize(padding, in_width, filter_width, stride_width,
 80aeba4:	9016      	str	r0, [sp, #88]	; 0x58
                              dilation_rate_width);
  *out_height = ComputeOutSize(padding, in_height, filter_height, stride_height,
 80aeba6:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80aeba8:	9a11      	ldr	r2, [sp, #68]	; 0x44
 80aebaa:	9913      	ldr	r1, [sp, #76]	; 0x4c
 80aebac:	980e      	ldr	r0, [sp, #56]	; 0x38
 80aebae:	f7f9 fadc 	bl	80a816a <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>
inline int ComputePaddingWithOffset(int stride, int dilation_rate, int in_size,
                                    int filter_size, int out_size,
                                    int* offset) {
  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;
  int total_padding =
      ((out_size - 1) * stride + effective_filter_size - in_size);
 80aebb2:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80aebb4:	3801      	subs	r0, #1
 80aebb6:	3b01      	subs	r3, #1
 80aebb8:	fb08 f803 	mul.w	r8, r8, r3
 80aebbc:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80aebbe:	f108 0801 	add.w	r8, r8, #1
 80aebc2:	fb03 8000 	mla	r0, r3, r0, r8
 80aebc6:	9b13      	ldr	r3, [sp, #76]	; 0x4c
 80aebc8:	9a15      	ldr	r2, [sp, #84]	; 0x54
 80aebca:	1ac0      	subs	r0, r0, r3
 80aebcc:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80aebce:	990f      	ldr	r1, [sp, #60]	; 0x3c
 80aebd0:	3b01      	subs	r3, #1
 80aebd2:	4353      	muls	r3, r2
 80aebd4:	9a16      	ldr	r2, [sp, #88]	; 0x58
 80aebd6:	3301      	adds	r3, #1
 80aebd8:	3a01      	subs	r2, #1
 80aebda:	fb01 3302 	mla	r3, r1, r2, r3
 80aebde:	9a14      	ldr	r2, [sp, #80]	; 0x50
  total_padding = total_padding > 0 ? total_padding : 0;
 80aebe0:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
inline int ComputePaddingWithOffset(int stride, int dilation_rate, int in_size,
                                    int filter_size, int out_size,
                                    int* offset) {
  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;
  int total_padding =
      ((out_size - 1) * stride + effective_filter_size - in_size);
 80aebe4:	1a9b      	subs	r3, r3, r2
  total_padding = total_padding > 0 ? total_padding : 0;
 80aebe6:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
 80aebea:	105a      	asrs	r2, r3, #1
 80aebec:	f003 0301 	and.w	r3, r3, #1
 80aebf0:	9362      	str	r3, [sp, #392]	; 0x188

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training.
  if (data_type != kTfLiteFloat32) {
 80aebf2:	9b0d      	ldr	r3, [sp, #52]	; 0x34
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
 80aebf4:	9260      	str	r2, [sp, #384]	; 0x180

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training.
  if (data_type != kTfLiteFloat32) {
 80aebf6:	2b01      	cmp	r3, #1
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
 80aebf8:	ea4f 0260 	mov.w	r2, r0, asr #1
 80aebfc:	f000 0001 	and.w	r0, r0, #1
 80aec00:	9261      	str	r2, [sp, #388]	; 0x184
 80aec02:	9063      	str	r0, [sp, #396]	; 0x18c

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training.
  if (data_type != kTfLiteFloat32) {
 80aec04:	d02d      	beq.n	80aec62 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x1ea>
}

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
 80aec06:	f8db 000c 	ldr.w	r0, [fp, #12]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80aec0a:	f8db 1004 	ldr.w	r1, [fp, #4]
 80aec0e:	f8db 2008 	ldr.w	r2, [fp, #8]
 80aec12:	2338      	movs	r3, #56	; 0x38

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
 80aec14:	f1b0 3fff 	cmp.w	r0, #4294967295	; 0xffffffff
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80aec18:	fb03 a101 	mla	r1, r3, r1, sl
 80aec1c:	fb03 a202 	mla	r2, r3, r2, sl
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
 80aec20:	bf18      	it	ne
 80aec22:	fb03 a300 	mlane	r3, r3, r0, sl
    const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
    const TfLiteTensor* bias =
        GetOptionalInputTensor(context, node, kBiasTensor);
    TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
 80aec26:	a8a6      	add	r0, sp, #664	; 0x298
 80aec28:	9007      	str	r0, [sp, #28]
 80aec2a:	a866      	add	r0, sp, #408	; 0x198
 80aec2c:	9006      	str	r0, [sp, #24]
 80aec2e:	a8e7      	add	r0, sp, #924	; 0x39c
 80aec30:	9005      	str	r0, [sp, #20]
 80aec32:	a8e6      	add	r0, sp, #920	; 0x398
 80aec34:	9004      	str	r0, [sp, #16]
 80aec36:	a865      	add	r0, sp, #404	; 0x194
 80aec38:	9003      	str	r0, [sp, #12]
 80aec3a:	a864      	add	r0, sp, #400	; 0x190
 80aec3c:	9002      	str	r0, [sp, #8]
 80aec3e:	f105 0010 	add.w	r0, r5, #16
 80aec42:	9001      	str	r0, [sp, #4]
 80aec44:	980c      	ldr	r0, [sp, #48]	; 0x30
 80aec46:	f04f 0e38 	mov.w	lr, #56	; 0x38
 80aec4a:	6840      	ldr	r0, [r0, #4]
  }
  return nullptr;
 80aec4c:	bf08      	it	eq
 80aec4e:	2300      	moveq	r3, #0
 80aec50:	fb0e a000 	mla	r0, lr, r0, sl
 80aec54:	9000      	str	r0, [sp, #0]
 80aec56:	4630      	mov	r0, r6
 80aec58:	f000 fe64 	bl	80af924 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_>
 80aec5c:	2800      	cmp	r0, #0
 80aec5e:	f040 8230 	bne.w	80af0c2 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x64a>
                                        filter_width, filter_height, out_width,
                                        out_height, data_type, &data));

  // TODO(aselle): Consider whether float conv and quantized conv should be
  // separate ops to avoid dispatch overhead here.
  switch (input->type) {  // Already know in/out types are same.
 80aec62:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80aec64:	f81a 8003 	ldrb.w	r8, [sl, r3]
 80aec68:	f1b8 0f03 	cmp.w	r8, #3
 80aec6c:	f040 80ba 	bne.w	80aede4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x36c>

void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteDepthwiseConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
 80aec70:	f8d9 3010 	ldr.w	r3, [r9, #16]
  const int32_t output_offset = output->params.zero_point;

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
 80aec74:	9960      	ldr	r1, [sp, #384]	; 0x180

void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteDepthwiseConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
 80aec76:	f1c3 0a00 	rsb	sl, r3, #0
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;
 80aec7a:	9b09      	ldr	r3, [sp, #36]	; 0x24
void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteDepthwiseConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
 80aec7c:	693a      	ldr	r2, [r7, #16]
  const int32_t output_offset = output->params.zero_point;
 80aec7e:	6918      	ldr	r0, [r3, #16]

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
 80aec80:	f8ad 114a 	strh.w	r1, [sp, #330]	; 0x14a
  op_params.padding_values.height = data->padding.height;
 80aec84:	9961      	ldr	r1, [sp, #388]	; 0x184
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
 80aec86:	2301      	movs	r3, #1
 80aec88:	f88d 3148 	strb.w	r3, [sp, #328]	; 0x148
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
 80aec8c:	f8ad 114c 	strh.w	r1, [sp, #332]	; 0x14c
  op_params.stride_width = params->stride_width;
 80aec90:	6869      	ldr	r1, [r5, #4]
void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteDepthwiseConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
 80aec92:	4252      	negs	r2, r2
  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
 80aec94:	f8ad 1152 	strh.w	r1, [sp, #338]	; 0x152
  op_params.stride_height = params->stride_height;
 80aec98:	68a9      	ldr	r1, [r5, #8]
  op_params.dilation_width_factor = 1;
 80aec9a:	f8ad 3156 	strh.w	r3, [sp, #342]	; 0x156
  op_params.dilation_height_factor = 1;
 80aec9e:	f8ad 3158 	strh.w	r3, [sp, #344]	; 0x158
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
 80aeca2:	f8ad 1154 	strh.w	r1, [sp, #340]	; 0x154
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
  op_params.depth_multiplier = params->depth_multiplier;
 80aeca6:	68e9      	ldr	r1, [r5, #12]
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
 80aeca8:	9258      	str	r2, [sp, #352]	; 0x160
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
  op_params.depth_multiplier = params->depth_multiplier;
 80aecaa:	f8ad 115a 	strh.w	r1, [sp, #346]	; 0x15a
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
 80aecae:	9a64      	ldr	r2, [sp, #400]	; 0x190
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.quantized_activation_min = data->output_activation_min;
 80aecb0:	99e6      	ldr	r1, [sp, #920]	; 0x398
  op_params.quantized_activation_max = data->output_activation_max;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
 80aecb2:	925a      	str	r2, [sp, #360]	; 0x168
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.quantized_activation_min = data->output_activation_min;
 80aecb4:	915c      	str	r1, [sp, #368]	; 0x170
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;
 80aecb6:	9a65      	ldr	r2, [sp, #404]	; 0x194
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
 80aecb8:	99e7      	ldr	r1, [sp, #924]	; 0x39c
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;
 80aecba:	4252      	negs	r2, r2
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
 80aecbc:	915d      	str	r1, [sp, #372]	; 0x174
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
 80aecbe:	9059      	str	r0, [sp, #356]	; 0x164
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;

  // Figure out if we can use the optimized path for this set of parameters.
  const int filter_width = GetTensorShape(filter).Dims(2);
 80aecc0:	4639      	mov	r1, r7
 80aecc2:	a84d      	add	r0, sp, #308	; 0x134
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;
 80aecc4:	925b      	str	r2, [sp, #364]	; 0x16c
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
 80aecc6:	930b      	str	r3, [sp, #44]	; 0x2c
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  op_params.input_offset = input_offset;
 80aecc8:	f8cd a15c 	str.w	sl, [sp, #348]	; 0x15c
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;

  // Figure out if we can use the optimized path for this set of parameters.
  const int filter_width = GetTensorShape(filter).Dims(2);
 80aeccc:	f7f3 fe21 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80aecd0:	2102      	movs	r1, #2
 80aecd2:	a84d      	add	r0, sp, #308	; 0x134
 80aecd4:	f7f3 fb78 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80aecd8:	4605      	mov	r5, r0
 80aecda:	a84d      	add	r0, sp, #308	; 0x134
 80aecdc:	f7f3 fb69 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  const int input_depth = GetTensorShape(input).Dims(3);
 80aece0:	4649      	mov	r1, r9
 80aece2:	a84d      	add	r0, sp, #308	; 0x134
 80aece4:	f7f3 fe15 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80aece8:	4641      	mov	r1, r8
 80aecea:	a84d      	add	r0, sp, #308	; 0x134
 80aecec:	f7f3 fb6c 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80aecf0:	4683      	mov	fp, r0
 80aecf2:	a84d      	add	r0, sp, #308	; 0x134
 80aecf4:	f7f3 fb5d 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  const int output_depth = GetTensorShape(filter).Dims(3);
 80aecf8:	4639      	mov	r1, r7
 80aecfa:	a84d      	add	r0, sp, #308	; 0x134
 80aecfc:	f7f3 fe09 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80aed00:	4641      	mov	r1, r8
 80aed02:	a84d      	add	r0, sp, #308	; 0x134
 80aed04:	f7f3 fb60 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
 80aed08:	4680      	mov	r8, r0
 80aed0a:	a84d      	add	r0, sp, #308	; 0x134
 80aed0c:	f7f3 fb51 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  const int filter_height = GetTensorShape(filter).Dims(1);
 80aed10:	4639      	mov	r1, r7
 80aed12:	a84d      	add	r0, sp, #308	; 0x134
 80aed14:	f7f3 fdfd 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80aed18:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80aed1a:	a84d      	add	r0, sp, #308	; 0x134
 80aed1c:	4619      	mov	r1, r3
 80aed1e:	f7f3 fb53 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int needed_size =
      output_depth * filter_width * filter_height * input_depth;
 80aed22:	fb08 f805 	mul.w	r8, r8, r5

  // Figure out if we can use the optimized path for this set of parameters.
  const int filter_width = GetTensorShape(filter).Dims(2);
  const int input_depth = GetTensorShape(input).Dims(3);
  const int output_depth = GetTensorShape(filter).Dims(3);
  const int filter_height = GetTensorShape(filter).Dims(1);
 80aed26:	900b      	str	r0, [sp, #44]	; 0x2c
 80aed28:	a84d      	add	r0, sp, #308	; 0x134
 80aed2a:	f7f3 fb42 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  const int needed_size =
      output_depth * filter_width * filter_height * input_depth;
 80aed2e:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
  bool use_optimized_path = false;
  if ((filter_width == 8) && (input_offset == 0) && (input_depth == 1) &&
 80aed30:	2d08      	cmp	r5, #8
  const int filter_width = GetTensorShape(filter).Dims(2);
  const int input_depth = GetTensorShape(input).Dims(3);
  const int output_depth = GetTensorShape(filter).Dims(3);
  const int filter_height = GetTensorShape(filter).Dims(1);
  const int needed_size =
      output_depth * filter_width * filter_height * input_depth;
 80aed32:	fb03 f308 	mul.w	r3, r3, r8
 80aed36:	ad3e      	add	r5, sp, #248	; 0xf8
 80aed38:	fb0b f303 	mul.w	r3, fp, r3
 80aed3c:	f50d 7886 	add.w	r8, sp, #268	; 0x10c
  bool use_optimized_path = false;
  if ((filter_width == 8) && (input_offset == 0) && (input_depth == 1) &&
 80aed40:	f040 8344 	bne.w	80af3cc <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x954>
 80aed44:	f1ba 0f00 	cmp.w	sl, #0
 80aed48:	f040 8340 	bne.w	80af3cc <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x954>
 80aed4c:	f1bb 0f01 	cmp.w	fp, #1
 80aed50:	f040 833c 	bne.w	80af3cc <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x954>
 80aed54:	f5b3 6f80 	cmp.w	r3, #1024	; 0x400
 80aed58:	f300 8338 	bgt.w	80af3cc <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x954>
    // with an allocation mechanism available through the context API.
    // Use the address of the node as a proxy for its identity, since we need
    // to ensure the weight values are consistent between calls, and there's
    // no easy way to do that quickly other than relying on the identity of
    // the owning node.
    static TfLiteNode* initialized_node_address = node;
 80aed5c:	f8df b078 	ldr.w	fp, [pc, #120]	; 80aedd8 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x360>
 80aed60:	f8df a078 	ldr.w	sl, [pc, #120]	; 80aeddc <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x364>
 80aed64:	f8db 3000 	ldr.w	r3, [fp]
 80aed68:	f013 0f01 	tst.w	r3, #1
 80aed6c:	d109      	bne.n	80aed82 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x30a>
 80aed6e:	4658      	mov	r0, fp
 80aed70:	f7f1 f9a6 	bl	80a00c0 <__cxa_guard_acquire>
 80aed74:	b128      	cbz	r0, 80aed82 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x30a>
 80aed76:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80aed78:	4658      	mov	r0, fp
 80aed7a:	f8ca 3000 	str.w	r3, [sl]
 80aed7e:	f7f1 f9a4 	bl	80a00ca <__cxa_guard_release>
    if (initialized_node_address == node) {
 80aed82:	f8da 3000 	ldr.w	r3, [sl]
 80aed86:	9a0a      	ldr	r2, [sp, #40]	; 0x28
 80aed88:	429a      	cmp	r2, r3
 80aed8a:	f000 808a 	beq.w	80aeea2 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x42a>
      use_optimized_path = true;
    } else {
      static bool has_warned = false;
      if (!has_warned) {
 80aed8e:	f8df a050 	ldr.w	sl, [pc, #80]	; 80aede0 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x368>
 80aed92:	f89a 3000 	ldrb.w	r3, [sl]
 80aed96:	2b00      	cmp	r3, #0
 80aed98:	f040 8318 	bne.w	80af3cc <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x954>
        context->ReportError(
            context,
            "Multiple depthwise conv ops match optimization parameters, but "
            "only the first will use the fast path, because there's only one "
            "RAM cache available");
 80aed9c:	6973      	ldr	r3, [r6, #20]
 80aed9e:	490d      	ldr	r1, [pc, #52]	; (80aedd4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x35c>)
 80aeda0:	4630      	mov	r0, r6
 80aeda2:	4798      	blx	r3
        has_warned = true;
 80aeda4:	2301      	movs	r3, #1
 80aeda6:	f88a 3000 	strb.w	r3, [sl]
 80aedaa:	e30f      	b.n	80af3cc <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x954>
 80aedac:	080b5dff 	.word	0x080b5dff
 80aedb0:	080b5e19 	.word	0x080b5e19
 80aedb4:	080b71bf 	.word	0x080b71bf
 80aedb8:	080b5be0 	.word	0x080b5be0
 80aedbc:	080b5e33 	.word	0x080b5e33
 80aedc0:	080b5e47 	.word	0x080b5e47
 80aedc4:	080b5db0 	.word	0x080b5db0
 80aedc8:	080b5dc7 	.word	0x080b5dc7
 80aedcc:	080b75ad 	.word	0x080b75ad
 80aedd0:	080b5deb 	.word	0x080b5deb
 80aedd4:	080b7282 	.word	0x080b7282
 80aedd8:	20002658 	.word	0x20002658
 80aeddc:	20002650 	.word	0x20002650
 80aede0:	20002654 	.word	0x20002654
                                        filter_width, filter_height, out_width,
                                        out_height, data_type, &data));

  // TODO(aselle): Consider whether float conv and quantized conv should be
  // separate ops to avoid dispatch overhead here.
  switch (input->type) {  // Already know in/out types are same.
 80aede4:	f1b8 0f09 	cmp.w	r8, #9
 80aede8:	f040 8101 	bne.w	80aefee <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x576>
                             TfLiteDepthwiseConvParams* params, OpData* data,
                             const TfLiteTensor* input,
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output) {
  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
 80aedec:	2301      	movs	r3, #1
 80aedee:	f88d 3148 	strb.w	r3, [sp, #328]	; 0x148
  op_params.padding_values.width = data->padding.width;
 80aedf2:	9b60      	ldr	r3, [sp, #384]	; 0x180
  op_params.quantized_activation_min = std::numeric_limits<int8_t>::min();
  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
 80aedf4:	4649      	mov	r1, r9
                             const TfLiteTensor* input,
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output) {
  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
 80aedf6:	f8ad 314a 	strh.w	r3, [sp, #330]	; 0x14a
  op_params.padding_values.height = data->padding.height;
 80aedfa:	9b61      	ldr	r3, [sp, #388]	; 0x184
  op_params.quantized_activation_min = std::numeric_limits<int8_t>::min();
  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
 80aedfc:	a83e      	add	r0, sp, #248	; 0xf8
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output) {
  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
 80aedfe:	f8ad 314c 	strh.w	r3, [sp, #332]	; 0x14c
  op_params.stride_width = params->stride_width;
 80aee02:	686b      	ldr	r3, [r5, #4]
  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
 80aee04:	ae43      	add	r6, sp, #268	; 0x10c
                             const TfLiteTensor* bias, TfLiteTensor* output) {
  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
 80aee06:	f8ad 3152 	strh.w	r3, [sp, #338]	; 0x152
  op_params.stride_height = params->stride_height;
 80aee0a:	68ab      	ldr	r3, [r5, #8]
 80aee0c:	f8ad 3154 	strh.w	r3, [sp, #340]	; 0x154
  op_params.dilation_width_factor = params->dilation_width_factor;
 80aee10:	696b      	ldr	r3, [r5, #20]
 80aee12:	f8ad 3156 	strh.w	r3, [sp, #342]	; 0x156
  op_params.dilation_height_factor = params->dilation_height_factor;
 80aee16:	69ab      	ldr	r3, [r5, #24]
 80aee18:	f8ad 3158 	strh.w	r3, [sp, #344]	; 0x158
  op_params.depth_multiplier = params->depth_multiplier;
 80aee1c:	68eb      	ldr	r3, [r5, #12]
 80aee1e:	f8ad 315a 	strh.w	r3, [sp, #346]	; 0x15a
  op_params.input_offset = -input->params.zero_point;
 80aee22:	f8d9 3010 	ldr.w	r3, [r9, #16]
 80aee26:	425b      	negs	r3, r3
 80aee28:	9357      	str	r3, [sp, #348]	; 0x15c
  op_params.weights_offset = 0;
 80aee2a:	2300      	movs	r3, #0
 80aee2c:	9358      	str	r3, [sp, #352]	; 0x160
  op_params.output_offset = output->params.zero_point;
 80aee2e:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80aee30:	691b      	ldr	r3, [r3, #16]
 80aee32:	9359      	str	r3, [sp, #356]	; 0x164
  // TODO(b/130439627): Use calculated value for clamping.
  op_params.quantized_activation_min = std::numeric_limits<int8_t>::min();
 80aee34:	f06f 037f 	mvn.w	r3, #127	; 0x7f
 80aee38:	935c      	str	r3, [sp, #368]	; 0x170
  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();
 80aee3a:	237f      	movs	r3, #127	; 0x7f
 80aee3c:	935d      	str	r3, [sp, #372]	; 0x174

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
 80aee3e:	f7f3 fd68 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorData<int8>(input), GetTensorShape(filter),
 80aee42:	4639      	mov	r1, r7
 80aee44:	4630      	mov	r0, r6
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80aee46:	f8d9 9004 	ldr.w	r9, [r9, #4]
 80aee4a:	f7f3 fd62 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80aee4e:	f8d7 a004 	ldr.w	sl, [r7, #4]
      GetTensorData<int8>(filter), GetTensorShape(bias),
 80aee52:	af48      	add	r7, sp, #288	; 0x120
 80aee54:	4621      	mov	r1, r4
 80aee56:	4638      	mov	r0, r7
 80aee58:	f7f3 fd5b 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80aee5c:	b104      	cbz	r4, 80aee60 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x3e8>
 80aee5e:	6864      	ldr	r4, [r4, #4]
      GetTensorData<int32>(bias), GetTensorShape(output),
 80aee60:	ad4d      	add	r5, sp, #308	; 0x134
 80aee62:	9909      	ldr	r1, [sp, #36]	; 0x24
 80aee64:	4628      	mov	r0, r5
 80aee66:	f7f3 fd54 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorData<int8>(output));
 80aee6a:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80aee6c:	aaa6      	add	r2, sp, #664	; 0x298
 80aee6e:	685b      	ldr	r3, [r3, #4]
 80aee70:	a966      	add	r1, sp, #408	; 0x198
 80aee72:	9306      	str	r3, [sp, #24]
 80aee74:	a852      	add	r0, sp, #328	; 0x148
 80aee76:	ab3e      	add	r3, sp, #248	; 0xf8
 80aee78:	9505      	str	r5, [sp, #20]
 80aee7a:	9404      	str	r4, [sp, #16]
 80aee7c:	9703      	str	r7, [sp, #12]
 80aee7e:	f8cd a008 	str.w	sl, [sp, #8]
 80aee82:	9601      	str	r6, [sp, #4]
 80aee84:	f8cd 9000 	str.w	r9, [sp]
 80aee88:	f7ff fa91 	bl	80ae3ae <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa>
  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
      GetTensorData<int8>(filter), GetTensorShape(bias),
      GetTensorData<int32>(bias), GetTensorShape(output),
 80aee8c:	4628      	mov	r0, r5
 80aee8e:	f7f3 fa90 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
      GetTensorData<int8>(filter), GetTensorShape(bias),
 80aee92:	4638      	mov	r0, r7
 80aee94:	f7f3 fa8d 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
 80aee98:	4630      	mov	r0, r6
 80aee9a:	f7f3 fa8a 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.quantized_activation_min = std::numeric_limits<int8_t>::min();
  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
 80aee9e:	a83e      	add	r0, sp, #248	; 0xf8
 80aeea0:	e0a1      	b.n	80aefe6 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x56e>
      }
    }
  }
  if (use_optimized_path) {
    DepthwiseConvOptimizedForFilterWidthEight(
        context, op_params, GetTensorShape(input),
 80aeea2:	4649      	mov	r1, r9
 80aeea4:	a848      	add	r0, sp, #288	; 0x120
 80aeea6:	f7f3 fd34 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80aeeaa:	f8d9 3004 	ldr.w	r3, [r9, #4]
        GetTensorData<uint8_t>(input), GetTensorShape(filter),
 80aeeae:	4639      	mov	r1, r7
 80aeeb0:	4640      	mov	r0, r8
 80aeeb2:	931a      	str	r3, [sp, #104]	; 0x68
 80aeeb4:	f7f3 fd2d 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80aeeb8:	687b      	ldr	r3, [r7, #4]
        GetTensorData<uint8_t>(filter), GetTensorShape(bias),
 80aeeba:	4621      	mov	r1, r4
 80aeebc:	4628      	mov	r0, r5
 80aeebe:	931b      	str	r3, [sp, #108]	; 0x6c
 80aeec0:	f7f3 fd27 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80aeec4:	2c00      	cmp	r4, #0
 80aeec6:	f000 827b 	beq.w	80af3c0 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x948>
 80aeeca:	6863      	ldr	r3, [r4, #4]
 80aeecc:	9316      	str	r3, [sp, #88]	; 0x58
        GetTensorData<int32_t>(bias), GetTensorShape(output),
 80aeece:	9909      	ldr	r1, [sp, #36]	; 0x24
 80aeed0:	a839      	add	r0, sp, #228	; 0xe4
 80aeed2:	f7f3 fd1e 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80aeed6:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80aeed8:	685b      	ldr	r3, [r3, #4]
 80aeeda:	9326      	str	r3, [sp, #152]	; 0x98
    TfLiteContext* context, const DepthwiseParams& params,
    const RuntimeShape& input_shape, const uint8* input_data,
    const RuntimeShape& filter_shape, const uint8* filter_data,
    const RuntimeShape& bias_shape, const int32* bias_data,
    const RuntimeShape& output_shape, uint8* output_data) {
  const int stride_width = params.stride_width;
 80aeedc:	f9bd 3152 	ldrsh.w	r3, [sp, #338]	; 0x152
 80aeee0:	9319      	str	r3, [sp, #100]	; 0x64
  const int stride_height = params.stride_height;
 80aeee2:	f9bd 3154 	ldrsh.w	r3, [sp, #340]	; 0x154
 80aeee6:	931c      	str	r3, [sp, #112]	; 0x70
  const int pad_width = params.padding_values.width;
 80aeee8:	f9bd 314a 	ldrsh.w	r3, [sp, #330]	; 0x14a
 80aeeec:	931d      	str	r3, [sp, #116]	; 0x74
  const int pad_height = params.padding_values.height;
 80aeeee:	f9bd 314c 	ldrsh.w	r3, [sp, #332]	; 0x14c
 80aeef2:	931e      	str	r3, [sp, #120]	; 0x78
  const int depth_multiplier = params.depth_multiplier;
 80aeef4:	f9bd 315a 	ldrsh.w	r3, [sp, #346]	; 0x15a
 80aeef8:	9317      	str	r3, [sp, #92]	; 0x5c
  const int32 output_activation_min = params.quantized_activation_min;
 80aeefa:	9b5c      	ldr	r3, [sp, #368]	; 0x170
 80aeefc:	931f      	str	r3, [sp, #124]	; 0x7c
  const int32 output_activation_max = params.quantized_activation_max;
 80aeefe:	9b5d      	ldr	r3, [sp, #372]	; 0x174
 80aef00:	9320      	str	r3, [sp, #128]	; 0x80
  const int32 input_offset = params.input_offset;
 80aef02:	9b57      	ldr	r3, [sp, #348]	; 0x15c
 80aef04:	9327      	str	r3, [sp, #156]	; 0x9c
  const int32 filter_offset = params.weights_offset;
 80aef06:	9b58      	ldr	r3, [sp, #352]	; 0x160
 80aef08:	9321      	str	r3, [sp, #132]	; 0x84
  const int32 output_offset = params.output_offset;
 80aef0a:	9b59      	ldr	r3, [sp, #356]	; 0x164
 80aef0c:	9328      	str	r3, [sp, #160]	; 0xa0
  const int32 output_multiplier = params.output_multiplier;
 80aef0e:	9b5a      	ldr	r3, [sp, #360]	; 0x168
 80aef10:	9329      	str	r3, [sp, #164]	; 0xa4
  const int output_shift = params.output_shift;
 80aef12:	9b5b      	ldr	r3, [sp, #364]	; 0x16c
 80aef14:	932a      	str	r3, [sp, #168]	; 0xa8
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
 80aef16:	9b48      	ldr	r3, [sp, #288]	; 0x120
 80aef18:	2b04      	cmp	r3, #4
 80aef1a:	f040 80d4 	bne.w	80af0c6 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x64e>
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
 80aef1e:	9b43      	ldr	r3, [sp, #268]	; 0x10c
 80aef20:	2b04      	cmp	r3, #4
 80aef22:	f040 80d0 	bne.w	80af0c6 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x64e>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
 80aef26:	9c39      	ldr	r4, [sp, #228]	; 0xe4
 80aef28:	2c04      	cmp	r4, #4
 80aef2a:	f040 80cc 	bne.w	80af0c6 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x64e>

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
 80aef2e:	9b1f      	ldr	r3, [sp, #124]	; 0x7c
 80aef30:	9a20      	ldr	r2, [sp, #128]	; 0x80
 80aef32:	4293      	cmp	r3, r2
 80aef34:	f300 80c7 	bgt.w	80af0c6 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x64e>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80aef38:	2300      	movs	r3, #0
 80aef3a:	4619      	mov	r1, r3
 80aef3c:	aa39      	add	r2, sp, #228	; 0xe4
 80aef3e:	a848      	add	r0, sp, #288	; 0x120
 80aef40:	f7f8 fd9d 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
 80aef44:	2303      	movs	r3, #3
 80aef46:	4619      	mov	r1, r3
 80aef48:	aa39      	add	r2, sp, #228	; 0xe4
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
 80aef4a:	902b      	str	r0, [sp, #172]	; 0xac
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
 80aef4c:	a843      	add	r0, sp, #268	; 0x10c
 80aef4e:	f7f8 fd96 	bl	80a7a7e <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
 80aef52:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
 80aef54:	4682      	mov	sl, r0
  const int input_height = input_shape.Dims(1);
 80aef56:	a848      	add	r0, sp, #288	; 0x120
 80aef58:	f7f3 fa36 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
 80aef5c:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
 80aef5e:	9018      	str	r0, [sp, #96]	; 0x60
  const int input_width = input_shape.Dims(2);
 80aef60:	a848      	add	r0, sp, #288	; 0x120
 80aef62:	f7f3 fa31 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_depth = input_shape.Dims(3);
 80aef66:	2103      	movs	r1, #3

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
 80aef68:	900e      	str	r0, [sp, #56]	; 0x38
  const int input_depth = input_shape.Dims(3);
 80aef6a:	a848      	add	r0, sp, #288	; 0x120
 80aef6c:	f7f3 fa2c 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_height = filter_shape.Dims(1);
 80aef70:	2101      	movs	r1, #1
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
 80aef72:	900a      	str	r0, [sp, #40]	; 0x28
  const int filter_height = filter_shape.Dims(1);
 80aef74:	a843      	add	r0, sp, #268	; 0x10c
 80aef76:	f7f3 fa27 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_width = filter_shape.Dims(2);
 80aef7a:	2102      	movs	r1, #2
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
 80aef7c:	9009      	str	r0, [sp, #36]	; 0x24
  const int filter_width = filter_shape.Dims(2);
 80aef7e:	a843      	add	r0, sp, #268	; 0x10c
 80aef80:	f7f3 fa22 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
 80aef84:	2101      	movs	r1, #1
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
 80aef86:	4683      	mov	fp, r0
  const int output_height = output_shape.Dims(1);
 80aef88:	a839      	add	r0, sp, #228	; 0xe4
 80aef8a:	f7f3 fa1d 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
 80aef8e:	2102      	movs	r1, #2
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
 80aef90:	902c      	str	r0, [sp, #176]	; 0xb0
  const int output_width = output_shape.Dims(2);
 80aef92:	a839      	add	r0, sp, #228	; 0xe4
 80aef94:	f7f3 fa18 	bl	80a23c8 <_ZNK6tflite12RuntimeShape4DimsEi>
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
 80aef98:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80aef9a:	9a17      	ldr	r2, [sp, #92]	; 0x5c
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
 80aef9c:	902d      	str	r0, [sp, #180]	; 0xb4
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
 80aef9e:	4353      	muls	r3, r2
 80aefa0:	459a      	cmp	sl, r3
 80aefa2:	f040 8090 	bne.w	80af0c6 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x64e>
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
 80aefa6:	a83e      	add	r0, sp, #248	; 0xf8
 80aefa8:	f7f8 fd59 	bl	80a7a5e <_ZNK6tflite12RuntimeShape8FlatSizeEv>
 80aefac:	4582      	cmp	sl, r0
 80aefae:	f040 808a 	bne.w	80af0c6 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x64e>

  static int16_t reshaped_filter_data[kReshapedFilterDataSize];
  const int needed_size =
      output_depth * filter_width * filter_height * input_depth;
 80aefb2:	fb0b f20a 	mul.w	r2, fp, sl
 80aefb6:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80aefb8:	435a      	muls	r2, r3
 80aefba:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80aefbc:	435a      	muls	r2, r3
  if (needed_size > kReshapedFilterDataSize) {
 80aefbe:	f5b2 6f80 	cmp.w	r2, #1024	; 0x400
 80aefc2:	f340 8082 	ble.w	80af0ca <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x652>
    context->ReportError(
        context,
        "Size too large for reshaped weight buffer (%d needed, %d available)",
        needed_size, kReshapedFilterDataSize);
 80aefc6:	6974      	ldr	r4, [r6, #20]
 80aefc8:	f44f 6380 	mov.w	r3, #1024	; 0x400
 80aefcc:	495b      	ldr	r1, [pc, #364]	; (80af13c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6c4>)
 80aefce:	4630      	mov	r0, r6
 80aefd0:	47a0      	blx	r4
  if (use_optimized_path) {
    DepthwiseConvOptimizedForFilterWidthEight(
        context, op_params, GetTensorShape(input),
        GetTensorData<uint8_t>(input), GetTensorShape(filter),
        GetTensorData<uint8_t>(filter), GetTensorShape(bias),
        GetTensorData<int32_t>(bias), GetTensorShape(output),
 80aefd2:	a839      	add	r0, sp, #228	; 0xe4
 80aefd4:	f7f3 f9ed 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  }
  if (use_optimized_path) {
    DepthwiseConvOptimizedForFilterWidthEight(
        context, op_params, GetTensorShape(input),
        GetTensorData<uint8_t>(input), GetTensorShape(filter),
        GetTensorData<uint8_t>(filter), GetTensorShape(bias),
 80aefd8:	a83e      	add	r0, sp, #248	; 0xf8
 80aefda:	f7f3 f9ea 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
    }
  }
  if (use_optimized_path) {
    DepthwiseConvOptimizedForFilterWidthEight(
        context, op_params, GetTensorShape(input),
        GetTensorData<uint8_t>(input), GetTensorShape(filter),
 80aefde:	a843      	add	r0, sp, #268	; 0x10c
 80aefe0:	f7f3 f9e7 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
      }
    }
  }
  if (use_optimized_path) {
    DepthwiseConvOptimizedForFilterWidthEight(
        context, op_params, GetTensorShape(input),
 80aefe4:	a848      	add	r0, sp, #288	; 0x120
        GetTensorData<uint8_t>(filter), GetTensorShape(bias),
        GetTensorData<int32_t>(bias), GetTensorShape(output),
        GetTensorData<uint8_t>(output));
  } else {
    tflite::reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
 80aefe6:	f7f3 f9e4 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
    default:
      context->ReportError(context, "Type %s (%d) not supported.",
                           TfLiteTypeGetName(input->type), input->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
 80aefea:	2000      	movs	r0, #0
 80aefec:	e21b      	b.n	80af426 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x9ae>
                                        filter_width, filter_height, out_width,
                                        out_height, data_type, &data));

  // TODO(aselle): Consider whether float conv and quantized conv should be
  // separate ops to avoid dispatch overhead here.
  switch (input->type) {  // Already know in/out types are same.
 80aefee:	f1b8 0f01 	cmp.w	r8, #1
 80aeff2:	d15b      	bne.n	80af0ac <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x634>
void EvalFloat(TfLiteContext* context, TfLiteNode* node,
               TfLiteDepthwiseConvParams* params, OpData* data,
               const TfLiteTensor* input, const TfLiteTensor* filter,
               const TfLiteTensor* bias, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
 80aeff4:	7c2b      	ldrb	r3, [r5, #16]
// Calculates the useful range of an activation layer given its activation
// tensor.a
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
 80aeff6:	2b01      	cmp	r3, #1
 80aeff8:	d007      	beq.n	80af00a <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x592>
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
 80aeffa:	2b03      	cmp	r3, #3
 80aeffc:	d007      	beq.n	80af00e <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x596>
    *activation_min = 0;
    *activation_max = 6;
  } else if (activation == kTfLiteActRelu1) {
 80aeffe:	2b02      	cmp	r3, #2
 80af000:	d008      	beq.n	80af014 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x59c>
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
    *activation_max = std::numeric_limits<T>::max();
 80af002:	4b4f      	ldr	r3, [pc, #316]	; (80af140 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6c8>)
    *activation_max = 6;
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
 80af004:	f46f 0200 	mvn.w	r2, #8388608	; 0x800000
 80af008:	e007      	b.n	80af01a <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x5a2>
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
 80af00a:	4b4d      	ldr	r3, [pc, #308]	; (80af140 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6c8>)
 80af00c:	e000      	b.n	80af010 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x598>
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
    *activation_max = 6;
 80af00e:	4b4d      	ldr	r3, [pc, #308]	; (80af144 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6cc>)
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
 80af010:	2200      	movs	r2, #0
 80af012:	e002      	b.n	80af01a <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x5a2>
    *activation_max = 6;
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
 80af014:	4a4c      	ldr	r2, [pc, #304]	; (80af148 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6d0>)
    *activation_max = 1;
 80af016:	f04f 537e 	mov.w	r3, #1065353216	; 0x3f800000
                           &output_activation_max);

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
 80af01a:	9860      	ldr	r0, [sp, #384]	; 0x180
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
 80af01c:	2101      	movs	r1, #1
  op_params.padding_values.width = data->padding.width;
 80af01e:	f8ad 014a 	strh.w	r0, [sp, #330]	; 0x14a
  op_params.padding_values.height = data->padding.height;
 80af022:	9861      	ldr	r0, [sp, #388]	; 0x184
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
 80af024:	f88d 1148 	strb.w	r1, [sp, #328]	; 0x148
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
 80af028:	f8ad 014c 	strh.w	r0, [sp, #332]	; 0x14c
  op_params.stride_width = params->stride_width;
 80af02c:	6868      	ldr	r0, [r5, #4]
 80af02e:	f8ad 0152 	strh.w	r0, [sp, #338]	; 0x152
  op_params.stride_height = params->stride_height;
 80af032:	68a8      	ldr	r0, [r5, #8]
  op_params.dilation_width_factor = 1;
 80af034:	f8ad 1156 	strh.w	r1, [sp, #342]	; 0x156
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
 80af038:	f8ad 0154 	strh.w	r0, [sp, #340]	; 0x154
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
 80af03c:	f8ad 1158 	strh.w	r1, [sp, #344]	; 0x158
  op_params.depth_multiplier = params->depth_multiplier;
 80af040:	68e9      	ldr	r1, [r5, #12]
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  tflite::reference_ops::DepthwiseConv(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
 80af042:	a83e      	add	r0, sp, #248	; 0xf8
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
  op_params.depth_multiplier = params->depth_multiplier;
 80af044:	f8ad 115a 	strh.w	r1, [sp, #346]	; 0x15a
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  tflite::reference_ops::DepthwiseConv(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
 80af048:	4649      	mov	r1, r9
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.float_activation_min = output_activation_min;
 80af04a:	925e      	str	r2, [sp, #376]	; 0x178
  op_params.float_activation_max = output_activation_max;
 80af04c:	935f      	str	r3, [sp, #380]	; 0x17c

  tflite::reference_ops::DepthwiseConv(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
      GetTensorShape(filter), GetTensorData<float>(filter),
      GetTensorShape(bias), GetTensorData<float>(bias), GetTensorShape(output),
 80af04e:	ad48      	add	r5, sp, #288	; 0x120
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  tflite::reference_ops::DepthwiseConv(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
 80af050:	f7f3 fc5f 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(filter), GetTensorData<float>(filter),
 80af054:	4639      	mov	r1, r7
 80af056:	a843      	add	r0, sp, #268	; 0x10c
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80af058:	f8d9 8004 	ldr.w	r8, [r9, #4]
 80af05c:	f7f3 fc59 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(bias), GetTensorData<float>(bias), GetTensorShape(output),
 80af060:	4621      	mov	r1, r4
 80af062:	4628      	mov	r0, r5
 80af064:	f8d7 9004 	ldr.w	r9, [r7, #4]
 80af068:	f7f3 fc53 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80af06c:	b104      	cbz	r4, 80af070 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x5f8>
 80af06e:	6864      	ldr	r4, [r4, #4]
 80af070:	af4d      	add	r7, sp, #308	; 0x134
 80af072:	9909      	ldr	r1, [sp, #36]	; 0x24
 80af074:	4638      	mov	r0, r7
 80af076:	f7f3 fc4c 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
 80af07a:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80af07c:	b10b      	cbz	r3, 80af082 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x60a>
 80af07e:	685e      	ldr	r6, [r3, #4]
 80af080:	e000      	b.n	80af084 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x60c>
 80af082:	9e09      	ldr	r6, [sp, #36]	; 0x24
      GetTensorData<float>(output));
 80af084:	ab43      	add	r3, sp, #268	; 0x10c
 80af086:	4642      	mov	r2, r8
 80af088:	a93e      	add	r1, sp, #248	; 0xf8
 80af08a:	a852      	add	r0, sp, #328	; 0x148
 80af08c:	9604      	str	r6, [sp, #16]
 80af08e:	9703      	str	r7, [sp, #12]
 80af090:	9402      	str	r4, [sp, #8]
 80af092:	9501      	str	r5, [sp, #4]
 80af094:	f8cd 9000 	str.w	r9, [sp]
 80af098:	f7ff faaf 	bl	80ae5fa <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf>
  op_params.float_activation_max = output_activation_max;

  tflite::reference_ops::DepthwiseConv(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
      GetTensorShape(filter), GetTensorData<float>(filter),
      GetTensorShape(bias), GetTensorData<float>(bias), GetTensorShape(output),
 80af09c:	4638      	mov	r0, r7
 80af09e:	f7f3 f988 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80af0a2:	4628      	mov	r0, r5
 80af0a4:	f7f3 f985 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  tflite::reference_ops::DepthwiseConv(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
      GetTensorShape(filter), GetTensorData<float>(filter),
 80af0a8:	a843      	add	r0, sp, #268	; 0x10c
 80af0aa:	e6f6      	b.n	80aee9a <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x422>
      break;
    case kTfLiteUInt8:
      EvalQuantized(context, node, params, &data, input, filter, bias, output);
      break;
    default:
      context->ReportError(context, "Type %s (%d) not supported.",
 80af0ac:	4640      	mov	r0, r8
 80af0ae:	6974      	ldr	r4, [r6, #20]
 80af0b0:	f7f1 f834 	bl	80a011c <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type), input->type);
 80af0b4:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80af0b6:	4602      	mov	r2, r0
 80af0b8:	f81a 3003 	ldrb.w	r3, [sl, r3]
 80af0bc:	4923      	ldr	r1, [pc, #140]	; (80af14c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6d4>)
 80af0be:	4630      	mov	r0, r6
 80af0c0:	47a0      	blx	r4
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
  }

  TF_LITE_ENSURE_STATUS(CalculateOpData(context, node, params, width, height,
 80af0c2:	2001      	movs	r0, #1
 80af0c4:	e1af      	b.n	80af426 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x9ae>
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
 80af0c6:	f000 ffb3 	bl	80b0030 <abort>
    return;
  }

  RuntimeShape reshaped_filter_shape;
  reshaped_filter_shape.BuildFrom(
      {1, output_depth, filter_height, filter_width});
 80af0ca:	2301      	movs	r3, #1
 80af0cc:	9335      	str	r3, [sp, #212]	; 0xd4
    const int dimensions_count =
        std::distance(src_iterable.begin(), src_iterable.end());
    Resize(dimensions_count);
    int32* data = DimsData();
    for (auto it : src_iterable) {
      *data = it;
 80af0ce:	934e      	str	r3, [sp, #312]	; 0x138

  // If this is the first time through, repack the weights into a cached buffer
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
 80af0d0:	4b1f      	ldr	r3, [pc, #124]	; (80af150 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6d8>)
    return;
  }

  RuntimeShape reshaped_filter_shape;
  reshaped_filter_shape.BuildFrom(
      {1, output_depth, filter_height, filter_width});
 80af0d2:	9a09      	ldr	r2, [sp, #36]	; 0x24
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
#endif  // TF_LITE_STATIC_MEMORY
    }
    size_ = dimensions_count;
 80af0d4:	944d      	str	r4, [sp, #308]	; 0x134

  // If this is the first time through, repack the weights into a cached buffer
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
 80af0d6:	781c      	ldrb	r4, [r3, #0]
    return;
  }

  RuntimeShape reshaped_filter_shape;
  reshaped_filter_shape.BuildFrom(
      {1, output_depth, filter_height, filter_width});
 80af0d8:	f8cd a0d8 	str.w	sl, [sp, #216]	; 0xd8
 80af0dc:	9237      	str	r2, [sp, #220]	; 0xdc
 80af0de:	f8cd b0e0 	str.w	fp, [sp, #224]	; 0xe0
    const int dimensions_count =
        std::distance(src_iterable.begin(), src_iterable.end());
    Resize(dimensions_count);
    int32* data = DimsData();
    for (auto it : src_iterable) {
      *data = it;
 80af0e2:	f8cd a13c 	str.w	sl, [sp, #316]	; 0x13c
 80af0e6:	9250      	str	r2, [sp, #320]	; 0x140
 80af0e8:	f8cd b144 	str.w	fp, [sp, #324]	; 0x144

  // If this is the first time through, repack the weights into a cached buffer
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
 80af0ec:	2c00      	cmp	r4, #0
 80af0ee:	d136      	bne.n	80af15e <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6e6>
              filter_data + Offset(filter_shape, 0, filter_y, filter_x, oc);
          int16_t* reshaped_filter =
              reshaped_filter_data +
              Offset(reshaped_filter_shape, 0, oc, filter_y, filter_x);
          *reshaped_filter =
              static_cast<int16_t>(*current_filter) + filter_offset;
 80af0f0:	4f18      	ldr	r7, [pc, #96]	; (80af154 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6dc>)

  // If this is the first time through, repack the weights into a cached buffer
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
    for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
 80af0f2:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80af0f4:	42a3      	cmp	r3, r4
 80af0f6:	dd2f      	ble.n	80af158 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6e0>
 80af0f8:	2500      	movs	r5, #0
      for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
 80af0fa:	45ab      	cmp	fp, r5
 80af0fc:	dd1c      	ble.n	80af138 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6c0>
 80af0fe:	2600      	movs	r6, #0
        for (int oc = 0; oc < output_depth; ++oc) {
 80af100:	45b2      	cmp	sl, r6
 80af102:	dd17      	ble.n	80af134 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6bc>
          const uint8* current_filter =
              filter_data + Offset(filter_shape, 0, filter_y, filter_x, oc);
 80af104:	9600      	str	r6, [sp, #0]
 80af106:	462b      	mov	r3, r5
 80af108:	4622      	mov	r2, r4
 80af10a:	2100      	movs	r1, #0
 80af10c:	a843      	add	r0, sp, #268	; 0x10c
 80af10e:	f7f3 f9c0 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80af112:	4680      	mov	r8, r0
          int16_t* reshaped_filter =
              reshaped_filter_data +
              Offset(reshaped_filter_shape, 0, oc, filter_y, filter_x);
 80af114:	4632      	mov	r2, r6
 80af116:	4623      	mov	r3, r4
 80af118:	9500      	str	r5, [sp, #0]
 80af11a:	2100      	movs	r1, #0
 80af11c:	a84d      	add	r0, sp, #308	; 0x134
 80af11e:	f7f3 f9b8 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          *reshaped_filter =
              static_cast<int16_t>(*current_filter) + filter_offset;
 80af122:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
 80af124:	9a21      	ldr	r2, [sp, #132]	; 0x84
 80af126:	f813 3008 	ldrb.w	r3, [r3, r8]
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
    for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
      for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
        for (int oc = 0; oc < output_depth; ++oc) {
 80af12a:	3601      	adds	r6, #1
              filter_data + Offset(filter_shape, 0, filter_y, filter_x, oc);
          int16_t* reshaped_filter =
              reshaped_filter_data +
              Offset(reshaped_filter_shape, 0, oc, filter_y, filter_x);
          *reshaped_filter =
              static_cast<int16_t>(*current_filter) + filter_offset;
 80af12c:	4413      	add	r3, r2
 80af12e:	f827 3010 	strh.w	r3, [r7, r0, lsl #1]
 80af132:	e7e5      	b.n	80af100 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x688>
  // If this is the first time through, repack the weights into a cached buffer
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
    for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
      for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
 80af134:	3501      	adds	r5, #1
 80af136:	e7e0      	b.n	80af0fa <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x682>

  // If this is the first time through, repack the weights into a cached buffer
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
    for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
 80af138:	3401      	adds	r4, #1
 80af13a:	e7da      	b.n	80af0f2 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x67a>
 80af13c:	080b7315 	.word	0x080b7315
 80af140:	7f7fffff 	.word	0x7f7fffff
 80af144:	40c00000 	.word	0x40c00000
 80af148:	bf800000 	.word	0xbf800000
 80af14c:	080b5e62 	.word	0x080b5e62
 80af150:	20001e4e 	.word	0x20001e4e
 80af154:	20001e50 	.word	0x20001e50
          *reshaped_filter =
              static_cast<int16_t>(*current_filter) + filter_offset;
        }
      }
    }
    is_reshaped_filter_initialized = true;
 80af158:	4b9a      	ldr	r3, [pc, #616]	; (80af3c4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x94c>)
 80af15a:	2201      	movs	r2, #1
 80af15c:	701a      	strb	r2, [r3, #0]
  // If this is the first time through, repack the weights into a cached buffer
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
    for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
      for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
 80af15e:	2300      	movs	r3, #0
 80af160:	930b      	str	r3, [sp, #44]	; 0x2c
                    Offset(filter_shape, 0, filter_y, filter_x_start, oc);
                for (int filter_x = filter_x_start; filter_x < filter_x_end;
                     ++filter_x) {
                  int32 input_val = *current_input;
                  current_input += input_depth;
                  int32 filter_val = *current_filter;
 80af162:	f1ca 0300 	rsb	r3, sl, #0
 80af166:	9333      	str	r3, [sp, #204]	; 0xcc
      }
    }
    is_reshaped_filter_initialized = true;
  }

  for (int b = 0; b < batches; ++b) {
 80af168:	9b2b      	ldr	r3, [sp, #172]	; 0xac
 80af16a:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
 80af16c:	4293      	cmp	r3, r2
 80af16e:	f340 8123 	ble.w	80af3b8 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x940>
 80af172:	9b1e      	ldr	r3, [sp, #120]	; 0x78
 80af174:	9a18      	ldr	r2, [sp, #96]	; 0x60
 80af176:	4413      	add	r3, r2
 80af178:	9313      	str	r3, [sp, #76]	; 0x4c
 80af17a:	9b1e      	ldr	r3, [sp, #120]	; 0x78
 80af17c:	425b      	negs	r3, r3
 80af17e:	930c      	str	r3, [sp, #48]	; 0x30
 80af180:	2300      	movs	r3, #0
 80af182:	930f      	str	r3, [sp, #60]	; 0x3c
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80af184:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
 80af186:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
 80af188:	4293      	cmp	r3, r2
 80af18a:	f340 8111 	ble.w	80af3b0 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x938>
 80af18e:	9a0c      	ldr	r2, [sp, #48]	; 0x30
 80af190:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80af192:	9818      	ldr	r0, [sp, #96]	; 0x60
 80af194:	4413      	add	r3, r2
 80af196:	9909      	ldr	r1, [sp, #36]	; 0x24
 80af198:	9a13      	ldr	r2, [sp, #76]	; 0x4c
 80af19a:	4298      	cmp	r0, r3
 80af19c:	bfc8      	it	gt
 80af19e:	460a      	movgt	r2, r1
 80af1a0:	9b1d      	ldr	r3, [sp, #116]	; 0x74
 80af1a2:	922e      	str	r2, [sp, #184]	; 0xb8
 80af1a4:	ebc3 030b 	rsb	r3, r3, fp
 80af1a8:	9a0e      	ldr	r2, [sp, #56]	; 0x38
 80af1aa:	930d      	str	r3, [sp, #52]	; 0x34
 80af1ac:	9b1d      	ldr	r3, [sp, #116]	; 0x74
 80af1ae:	4413      	add	r3, r2
 80af1b0:	9314      	str	r3, [sp, #80]	; 0x50
 80af1b2:	9b0c      	ldr	r3, [sp, #48]	; 0x30
 80af1b4:	9a0c      	ldr	r2, [sp, #48]	; 0x30
 80af1b6:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
 80af1ba:	9331      	str	r3, [sp, #196]	; 0xc4
 80af1bc:	9b13      	ldr	r3, [sp, #76]	; 0x4c
 80af1be:	2a00      	cmp	r2, #0
 80af1c0:	eba3 0300 	sub.w	r3, r3, r0
 80af1c4:	bfa8      	it	ge
 80af1c6:	2300      	movge	r3, #0
 80af1c8:	9324      	str	r3, [sp, #144]	; 0x90
 80af1ca:	2300      	movs	r3, #0
 80af1cc:	9310      	str	r3, [sp, #64]	; 0x40
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80af1ce:	9b2d      	ldr	r3, [sp, #180]	; 0xb4
 80af1d0:	9a10      	ldr	r2, [sp, #64]	; 0x40
 80af1d2:	4293      	cmp	r3, r2
 80af1d4:	f340 80e1 	ble.w	80af39a <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x922>
 80af1d8:	9b0d      	ldr	r3, [sp, #52]	; 0x34
 80af1da:	9a0e      	ldr	r2, [sp, #56]	; 0x38
 80af1dc:	ebcb 0303 	rsb	r3, fp, r3
 80af1e0:	9325      	str	r3, [sp, #148]	; 0x94
 80af1e2:	9b14      	ldr	r3, [sp, #80]	; 0x50
 80af1e4:	990d      	ldr	r1, [sp, #52]	; 0x34
 80af1e6:	1a9b      	subs	r3, r3, r2
 80af1e8:	9330      	str	r3, [sp, #192]	; 0xc0
 80af1ea:	9b14      	ldr	r3, [sp, #80]	; 0x50
 80af1ec:	f04f 0800 	mov.w	r8, #0
 80af1f0:	428a      	cmp	r2, r1
 80af1f2:	bfc8      	it	gt
 80af1f4:	465b      	movgt	r3, fp
 80af1f6:	f8cd 8044 	str.w	r8, [sp, #68]	; 0x44
 80af1fa:	932f      	str	r3, [sp, #188]	; 0xbc
        for (int ic = 0; ic < input_depth; ++ic) {
 80af1fc:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80af1fe:	9a11      	ldr	r2, [sp, #68]	; 0x44
 80af200:	4293      	cmp	r3, r2
 80af202:	f340 80bf 	ble.w	80af384 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x90c>
 80af206:	9b16      	ldr	r3, [sp, #88]	; 0x58
 80af208:	f04f 0900 	mov.w	r9, #0
 80af20c:	eb03 0388 	add.w	r3, r3, r8, lsl #2
 80af210:	9332      	str	r3, [sp, #200]	; 0xc8
          for (int m = 0; m < depth_multiplier; m++) {
 80af212:	9b17      	ldr	r3, [sp, #92]	; 0x5c
 80af214:	454b      	cmp	r3, r9
 80af216:	f340 80af 	ble.w	80af378 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x900>
 80af21a:	eb09 0308 	add.w	r3, r9, r8
 80af21e:	9315      	str	r3, [sp, #84]	; 0x54
            }
            int in_y = in_y_start;
            int in_x_start = in_x_origin;
            int filter_x_start = 0;
            bool is_out_of_x_bounds = false;
            if (in_x_origin < 0) {
 80af220:	9b25      	ldr	r3, [sp, #148]	; 0x94
              is_out_of_x_bounds = true;
            }
            int filter_x_end = filter_width;
            if ((in_x_origin + filter_width) >= input_width) {
              filter_x_end -= (in_x_origin + filter_width) - input_width;
              is_out_of_x_bounds = true;
 80af222:	9a0d      	ldr	r2, [sp, #52]	; 0x34
            }
            int in_y = in_y_start;
            int in_x_start = in_x_origin;
            int filter_x_start = 0;
            bool is_out_of_x_bounds = false;
            if (in_x_origin < 0) {
 80af224:	2b00      	cmp	r3, #0
              in_x_start = 0;
              filter_x_start = 0 - in_x_origin;
 80af226:	bfb7      	itett	lt
 80af228:	9b30      	ldrlt	r3, [sp, #192]	; 0xc0
            }
            int in_y = in_y_start;
            int in_x_start = in_x_origin;
            int filter_x_start = 0;
            bool is_out_of_x_bounds = false;
            if (in_x_origin < 0) {
 80af22a:	9322      	strge	r3, [sp, #136]	; 0x88
              in_x_start = 0;
              filter_x_start = 0 - in_x_origin;
 80af22c:	9312      	strlt	r3, [sp, #72]	; 0x48
            int in_y = in_y_start;
            int in_x_start = in_x_origin;
            int filter_x_start = 0;
            bool is_out_of_x_bounds = false;
            if (in_x_origin < 0) {
              in_x_start = 0;
 80af22e:	2300      	movlt	r3, #0
 80af230:	bfb8      	it	lt
 80af232:	9322      	strlt	r3, [sp, #136]	; 0x88
              is_out_of_x_bounds = true;
            }
            int filter_x_end = filter_width;
            if ((in_x_origin + filter_width) >= input_width) {
              filter_x_end -= (in_x_origin + filter_width) - input_width;
              is_out_of_x_bounds = true;
 80af234:	9b0e      	ldr	r3, [sp, #56]	; 0x38
              filter_y_end -= (in_y_origin + filter_height) - input_height;
            }
            int in_y = in_y_start;
            int in_x_start = in_x_origin;
            int filter_x_start = 0;
            bool is_out_of_x_bounds = false;
 80af236:	bfaa      	itet	ge
 80af238:	2600      	movge	r6, #0
            if (in_x_origin < 0) {
              in_x_start = 0;
              filter_x_start = 0 - in_x_origin;
              is_out_of_x_bounds = true;
 80af23a:	2601      	movlt	r6, #1
            if ((in_y_origin + filter_height) >= input_height) {
              filter_y_end -= (in_y_origin + filter_height) - input_height;
            }
            int in_y = in_y_start;
            int in_x_start = in_x_origin;
            int filter_x_start = 0;
 80af23c:	9612      	strge	r6, [sp, #72]	; 0x48
              is_out_of_x_bounds = true;
            }
            int filter_x_end = filter_width;
            if ((in_x_origin + filter_width) >= input_width) {
              filter_x_end -= (in_x_origin + filter_width) - input_width;
              is_out_of_x_bounds = true;
 80af23e:	4293      	cmp	r3, r2
 80af240:	bfd8      	it	le
 80af242:	2601      	movle	r6, #1
            }
            for (int filter_y = filter_y_start; filter_y < filter_y_end;
 80af244:	9d24      	ldr	r5, [sp, #144]	; 0x90
        for (int ic = 0; ic < input_depth; ++ic) {
          for (int m = 0; m < depth_multiplier; m++) {
            const int oc = m + ic * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            int32 acc = 0;
 80af246:	2400      	movs	r4, #0
 80af248:	9b31      	ldr	r3, [sp, #196]	; 0xc4
 80af24a:	9a24      	ldr	r2, [sp, #144]	; 0x90
 80af24c:	1a9a      	subs	r2, r3, r2
            int filter_x_end = filter_width;
            if ((in_x_origin + filter_width) >= input_width) {
              filter_x_end -= (in_x_origin + filter_width) - input_width;
              is_out_of_x_bounds = true;
            }
            for (int filter_y = filter_y_start; filter_y < filter_y_end;
 80af24e:	9b2e      	ldr	r3, [sp, #184]	; 0xb8
 80af250:	442a      	add	r2, r5
 80af252:	42ab      	cmp	r3, r5
 80af254:	dd6d      	ble.n	80af332 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x8ba>
                 ++filter_y, ++in_y) {
              const uint8* current_input =
                  input_data + Offset(input_shape, b, in_y, in_x_start, ic);
 80af256:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80af258:	990b      	ldr	r1, [sp, #44]	; 0x2c
 80af25a:	9300      	str	r3, [sp, #0]
 80af25c:	a848      	add	r0, sp, #288	; 0x120
 80af25e:	9b22      	ldr	r3, [sp, #136]	; 0x88
 80af260:	f7f3 f917 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80af264:	9b1a      	ldr	r3, [sp, #104]	; 0x68
              if ((filter_width == 8) && !is_out_of_x_bounds) {
 80af266:	f1bb 0f08 	cmp.w	fp, #8
              is_out_of_x_bounds = true;
            }
            for (int filter_y = filter_y_start; filter_y < filter_y_end;
                 ++filter_y, ++in_y) {
              const uint8* current_input =
                  input_data + Offset(input_shape, b, in_y, in_x_start, ic);
 80af26a:	9023      	str	r0, [sp, #140]	; 0x8c
 80af26c:	eb03 0700 	add.w	r7, r3, r0
              if ((filter_width == 8) && !is_out_of_x_bounds) {
 80af270:	d13c      	bne.n	80af2ec <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x874>
 80af272:	2e00      	cmp	r6, #0
 80af274:	d13a      	bne.n	80af2ec <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x874>
                int16* current_filter =
                    reshaped_filter_data + Offset(reshaped_filter_shape, 0, oc,
 80af276:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80af278:	9a15      	ldr	r2, [sp, #84]	; 0x54
 80af27a:	9300      	str	r3, [sp, #0]
 80af27c:	4631      	mov	r1, r6
 80af27e:	462b      	mov	r3, r5
 80af280:	a84d      	add	r0, sp, #308	; 0x134
 80af282:	f7f3 f906 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                                                  filter_y, filter_x_start);
 80af286:	4b50      	ldr	r3, [pc, #320]	; (80af3c8 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x950>)
                const uint32_t input_vals0 =
                    *reinterpret_cast<const uint32_t*>(current_input);
 80af288:	9a23      	ldr	r2, [sp, #140]	; 0x8c
              const uint8* current_input =
                  input_data + Offset(input_shape, b, in_y, in_x_start, ic);
              if ((filter_width == 8) && !is_out_of_x_bounds) {
                int16* current_filter =
                    reshaped_filter_data + Offset(reshaped_filter_shape, 0, oc,
                                                  filter_y, filter_x_start);
 80af28a:	eb03 0040 	add.w	r0, r3, r0, lsl #1
                const uint32_t input_vals0 =
                    *reinterpret_cast<const uint32_t*>(current_input);
 80af28e:	9b1a      	ldr	r3, [sp, #104]	; 0x68
                current_input += 4;
                const int32_t filter_vals0 =
                    *reinterpret_cast<const int32_t*>(current_filter);
 80af290:	f8d0 c000 	ldr.w	ip, [r0]
              if ((filter_width == 8) && !is_out_of_x_bounds) {
                int16* current_filter =
                    reshaped_filter_data + Offset(reshaped_filter_shape, 0, oc,
                                                  filter_y, filter_x_start);
                const uint32_t input_vals0 =
                    *reinterpret_cast<const uint32_t*>(current_input);
 80af294:	5899      	ldr	r1, [r3, r2]
                const int32_t filter_vals0 =
                    *reinterpret_cast<const int32_t*>(current_filter);
                current_filter += 2;
                const uint8 input_val0 = input_vals0 & 0xff;
                const int16 filter_val0 = filter_vals0 & 0xffff;
                acc += filter_val0 * input_val0;
 80af296:	fa0f f28c 	sxth.w	r2, ip
 80af29a:	b2cb      	uxtb	r3, r1
 80af29c:	fb02 4203 	mla	r2, r2, r3, r4
                const uint8 input_val1 = (input_vals0 >> 8) & 0xff;
                const int16 filter_val1 = (filter_vals0 >> 16) & 0xffff;
                acc += filter_val1 * input_val1;
 80af2a0:	ea4f 4e2c 	mov.w	lr, ip, asr #16
 80af2a4:	f3c1 2307 	ubfx	r3, r1, #8, #8
 80af2a8:	fb0e 2e03 	mla	lr, lr, r3, r2

                const int32_t filter_vals1 =
                    *reinterpret_cast<const int32_t*>(current_filter);
 80af2ac:	6843      	ldr	r3, [r0, #4]
                current_filter += 2;
                const uint8 input_val2 = (input_vals0 >> 16) & 0xff;
                const int16 filter_val2 = filter_vals1 & 0xffff;
                acc += filter_val2 * input_val2;
 80af2ae:	f3c1 4c07 	ubfx	ip, r1, #16, #8
 80af2b2:	b21a      	sxth	r2, r3
 80af2b4:	fb02 e20c 	mla	r2, r2, ip, lr
                const uint8 input_val3 = (input_vals0 >> 24) & 0xff;
                const int16 filter_val3 = (filter_vals1 >> 16) & 0xffff;
                acc += filter_val3 * input_val3;
 80af2b8:	141b      	asrs	r3, r3, #16
 80af2ba:	0e09      	lsrs	r1, r1, #24
 80af2bc:	fb01 2203 	mla	r2, r1, r3, r2

                const uint32_t input_vals1 =
                    *reinterpret_cast<const uint32_t*>(current_input);
 80af2c0:	687b      	ldr	r3, [r7, #4]
                const int32_t filter_vals2 =
                    *reinterpret_cast<const int32_t*>(current_filter);
 80af2c2:	6881      	ldr	r1, [r0, #8]
                current_filter += 2;
                const uint8 input_val4 = input_vals1 & 0xff;
                const int16 filter_val4 = filter_vals2 & 0xffff;
                acc += filter_val4 * input_val4;
 80af2c4:	b2dc      	uxtb	r4, r3
 80af2c6:	b20f      	sxth	r7, r1
 80af2c8:	fb07 2204 	mla	r2, r7, r4, r2
                const uint8 input_val5 = (input_vals1 >> 8) & 0xff;
                const int16 filter_val5 = (filter_vals2 >> 16) & 0xffff;
                acc += filter_val5 * input_val5;
 80af2cc:	1409      	asrs	r1, r1, #16
 80af2ce:	f3c3 2707 	ubfx	r7, r3, #8, #8
 80af2d2:	fb01 2207 	mla	r2, r1, r7, r2

                const int32_t filter_vals3 =
                    *reinterpret_cast<const int32_t*>(current_filter);
 80af2d6:	68c1      	ldr	r1, [r0, #12]
                const uint8 input_val6 = (input_vals1 >> 16) & 0xff;
                const int16 filter_val6 = filter_vals3 & 0xffff;
                acc += filter_val6 * input_val6;
 80af2d8:	f3c3 4007 	ubfx	r0, r3, #16, #8
 80af2dc:	b20c      	sxth	r4, r1
 80af2de:	fb04 2400 	mla	r4, r4, r0, r2
                const uint8 input_val7 = (input_vals1 >> 24) & 0xff;
                const int16 filter_val7 = (filter_vals3 >> 16) & 0xffff;
                acc += filter_val7 * input_val7;
 80af2e2:	1409      	asrs	r1, r1, #16
 80af2e4:	0e1b      	lsrs	r3, r3, #24
 80af2e6:	fb03 4401 	mla	r4, r3, r1, r4
 80af2ea:	e020      	b.n	80af32e <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x8b6>
              } else {
                const uint8* current_filter =
                    filter_data +
                    Offset(filter_shape, 0, filter_y, filter_x_start, oc);
 80af2ec:	9b15      	ldr	r3, [sp, #84]	; 0x54
 80af2ee:	462a      	mov	r2, r5
 80af2f0:	9300      	str	r3, [sp, #0]
 80af2f2:	2100      	movs	r1, #0
 80af2f4:	9b12      	ldr	r3, [sp, #72]	; 0x48
 80af2f6:	a843      	add	r0, sp, #268	; 0x10c
 80af2f8:	f7f3 f8cb 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
 80af2fc:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
                for (int filter_x = filter_x_start; filter_x < filter_x_end;
                     ++filter_x) {
                  int32 input_val = *current_input;
 80af2fe:	9a0a      	ldr	r2, [sp, #40]	; 0x28
 80af300:	4418      	add	r0, r3
                acc += filter_val7 * input_val7;
              } else {
                const uint8* current_filter =
                    filter_data +
                    Offset(filter_shape, 0, filter_y, filter_x_start, oc);
                for (int filter_x = filter_x_start; filter_x < filter_x_end;
 80af302:	9b12      	ldr	r3, [sp, #72]	; 0x48
                     ++filter_x) {
                  int32 input_val = *current_input;
 80af304:	f1c2 0e00 	rsb	lr, r2, #0
 80af308:	9a0a      	ldr	r2, [sp, #40]	; 0x28
 80af30a:	4450      	add	r0, sl
 80af30c:	4417      	add	r7, r2
                acc += filter_val7 * input_val7;
              } else {
                const uint8* current_filter =
                    filter_data +
                    Offset(filter_shape, 0, filter_y, filter_x_start, oc);
                for (int filter_x = filter_x_start; filter_x < filter_x_end;
 80af30e:	9a2f      	ldr	r2, [sp, #188]	; 0xbc
 80af310:	429a      	cmp	r2, r3
 80af312:	dd0c      	ble.n	80af32e <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x8b6>
                  int32 input_val = *current_input;
                  current_input += input_depth;
                  int32 filter_val = *current_filter;
                  current_filter += output_depth;
                  acc +=
                      (filter_val + filter_offset) * (input_val + input_offset);
 80af314:	9a33      	ldr	r2, [sp, #204]	; 0xcc
 80af316:	9921      	ldr	r1, [sp, #132]	; 0x84
 80af318:	5c82      	ldrb	r2, [r0, r2]
                acc += filter_val7 * input_val7;
              } else {
                const uint8* current_filter =
                    filter_data +
                    Offset(filter_shape, 0, filter_y, filter_x_start, oc);
                for (int filter_x = filter_x_start; filter_x < filter_x_end;
 80af31a:	3301      	adds	r3, #1
                  int32 input_val = *current_input;
                  current_input += input_depth;
                  int32 filter_val = *current_filter;
                  current_filter += output_depth;
                  acc +=
                      (filter_val + filter_offset) * (input_val + input_offset);
 80af31c:	eb02 0c01 	add.w	ip, r2, r1
 80af320:	f817 100e 	ldrb.w	r1, [r7, lr]
 80af324:	9a27      	ldr	r2, [sp, #156]	; 0x9c
 80af326:	4411      	add	r1, r2
 80af328:	fb01 440c 	mla	r4, r1, ip, r4
 80af32c:	e7ec      	b.n	80af308 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x890>
            int filter_x_end = filter_width;
            if ((in_x_origin + filter_width) >= input_width) {
              filter_x_end -= (in_x_origin + filter_width) - input_width;
              is_out_of_x_bounds = true;
            }
            for (int filter_y = filter_y_start; filter_y < filter_y_end;
 80af32e:	3501      	adds	r5, #1
 80af330:	e78a      	b.n	80af248 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x7d0>
                  acc +=
                      (filter_val + filter_offset) * (input_val + input_offset);
                }
              }
            }
            if (bias_data) {
 80af332:	9b16      	ldr	r3, [sp, #88]	; 0x58
 80af334:	b11b      	cbz	r3, 80af33e <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x8c6>
              acc += bias_data[oc];
 80af336:	9b32      	ldr	r3, [sp, #200]	; 0xc8
 80af338:	f853 3029 	ldr.w	r3, [r3, r9, lsl #2]
 80af33c:	441c      	add	r4, r3
}

template <>
inline int32 DepthwiseConvRound<DepthwiseConvOutputRounding::kAwayFromZero>(
    int32 x, int32 quantized_multiplier, int shift) {
  return MultiplyByQuantizedMultiplier(x, quantized_multiplier, shift);
 80af33e:	9a2a      	ldr	r2, [sp, #168]	; 0xa8
 80af340:	9929      	ldr	r1, [sp, #164]	; 0xa4
 80af342:	4620      	mov	r0, r4
 80af344:	f7f8 fbaa 	bl	80a7a9c <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
            }
            acc = reference_ops::depthwise_conv::DepthwiseConvRound<
                DepthwiseConvOutputRounding::kAwayFromZero>(
                acc, output_multiplier, output_shift);
            acc += output_offset;
 80af348:	9b28      	ldr	r3, [sp, #160]	; 0xa0
            acc = std::max(acc, output_activation_min);
            acc = std::min(acc, output_activation_max);
            output_data[Offset(output_shape, b, out_y, out_x, oc)] =
 80af34a:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
              acc += bias_data[oc];
            }
            acc = reference_ops::depthwise_conv::DepthwiseConvRound<
                DepthwiseConvOutputRounding::kAwayFromZero>(
                acc, output_multiplier, output_shift);
            acc += output_offset;
 80af34c:	4418      	add	r0, r3
 80af34e:	9b1f      	ldr	r3, [sp, #124]	; 0x7c
            acc = std::max(acc, output_activation_min);
            acc = std::min(acc, output_activation_max);
            output_data[Offset(output_shape, b, out_y, out_x, oc)] =
 80af350:	990b      	ldr	r1, [sp, #44]	; 0x2c
 80af352:	4283      	cmp	r3, r0
 80af354:	bfb8      	it	lt
 80af356:	4603      	movlt	r3, r0
 80af358:	461c      	mov	r4, r3
 80af35a:	9b20      	ldr	r3, [sp, #128]	; 0x80
 80af35c:	a839      	add	r0, sp, #228	; 0xe4
 80af35e:	429c      	cmp	r4, r3
 80af360:	bfa8      	it	ge
 80af362:	461c      	movge	r4, r3
 80af364:	9b15      	ldr	r3, [sp, #84]	; 0x54

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int ic = 0; ic < input_depth; ++ic) {
          for (int m = 0; m < depth_multiplier; m++) {
 80af366:	f109 0901 	add.w	r9, r9, #1
                DepthwiseConvOutputRounding::kAwayFromZero>(
                acc, output_multiplier, output_shift);
            acc += output_offset;
            acc = std::max(acc, output_activation_min);
            acc = std::min(acc, output_activation_max);
            output_data[Offset(output_shape, b, out_y, out_x, oc)] =
 80af36a:	9300      	str	r3, [sp, #0]
 80af36c:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80af36e:	f7f3 f890 	bl	80a2492 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                static_cast<uint8>(acc);
 80af372:	9b26      	ldr	r3, [sp, #152]	; 0x98
 80af374:	541c      	strb	r4, [r3, r0]
 80af376:	e74c      	b.n	80af212 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x79a>
  }

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int ic = 0; ic < input_depth; ++ic) {
 80af378:	9b11      	ldr	r3, [sp, #68]	; 0x44
 80af37a:	3301      	adds	r3, #1
 80af37c:	9311      	str	r3, [sp, #68]	; 0x44
 80af37e:	9b17      	ldr	r3, [sp, #92]	; 0x5c
 80af380:	4498      	add	r8, r3
 80af382:	e73b      	b.n	80af1fc <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x784>
    is_reshaped_filter_initialized = true;
  }

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
 80af384:	9b10      	ldr	r3, [sp, #64]	; 0x40
 80af386:	9a19      	ldr	r2, [sp, #100]	; 0x64
 80af388:	3301      	adds	r3, #1
 80af38a:	9310      	str	r3, [sp, #64]	; 0x40
 80af38c:	9b0d      	ldr	r3, [sp, #52]	; 0x34
 80af38e:	4413      	add	r3, r2
 80af390:	930d      	str	r3, [sp, #52]	; 0x34
 80af392:	9b14      	ldr	r3, [sp, #80]	; 0x50
 80af394:	1a9b      	subs	r3, r3, r2
 80af396:	9314      	str	r3, [sp, #80]	; 0x50
 80af398:	e719      	b.n	80af1ce <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x756>
    }
    is_reshaped_filter_initialized = true;
  }

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
 80af39a:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
 80af39c:	9a1c      	ldr	r2, [sp, #112]	; 0x70
 80af39e:	3301      	adds	r3, #1
 80af3a0:	930f      	str	r3, [sp, #60]	; 0x3c
 80af3a2:	9b13      	ldr	r3, [sp, #76]	; 0x4c
 80af3a4:	1a9b      	subs	r3, r3, r2
 80af3a6:	9313      	str	r3, [sp, #76]	; 0x4c
 80af3a8:	9b0c      	ldr	r3, [sp, #48]	; 0x30
 80af3aa:	4413      	add	r3, r2
 80af3ac:	930c      	str	r3, [sp, #48]	; 0x30
 80af3ae:	e6e9      	b.n	80af184 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x70c>
      }
    }
    is_reshaped_filter_initialized = true;
  }

  for (int b = 0; b < batches; ++b) {
 80af3b0:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
 80af3b2:	3301      	adds	r3, #1
 80af3b4:	930b      	str	r3, [sp, #44]	; 0x2c
 80af3b6:	e6d7      	b.n	80af168 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6f0>
        "Size too large for reshaped weight buffer (%d needed, %d available)",
        needed_size, kReshapedFilterDataSize);
    return;
  }

  RuntimeShape reshaped_filter_shape;
 80af3b8:	a84d      	add	r0, sp, #308	; 0x134
 80af3ba:	f7f2 fffa 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
 80af3be:	e608      	b.n	80aefd2 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x55a>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
 80af3c0:	9416      	str	r4, [sp, #88]	; 0x58
 80af3c2:	e584      	b.n	80aeece <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x456>
 80af3c4:	20001e4e 	.word	0x20001e4e
 80af3c8:	20001e50 	.word	0x20001e50
        GetTensorData<uint8_t>(filter), GetTensorShape(bias),
        GetTensorData<int32_t>(bias), GetTensorShape(output),
        GetTensorData<uint8_t>(output));
  } else {
    tflite::reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
 80af3cc:	4649      	mov	r1, r9
 80af3ce:	a84d      	add	r0, sp, #308	; 0x134
 80af3d0:	f7f3 fa9f 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
        GetTensorShape(filter), GetTensorData<uint8_t>(filter),
 80af3d4:	4639      	mov	r1, r7
 80af3d6:	a848      	add	r0, sp, #288	; 0x120
 80af3d8:	f8d9 9004 	ldr.w	r9, [r9, #4]
 80af3dc:	f7f3 fa99 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
 80af3e0:	4621      	mov	r1, r4
 80af3e2:	4640      	mov	r0, r8
 80af3e4:	687f      	ldr	r7, [r7, #4]
 80af3e6:	f7f3 fa94 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
 80af3ea:	b104      	cbz	r4, 80af3ee <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x976>
 80af3ec:	6864      	ldr	r4, [r4, #4]
        GetTensorShape(output), GetTensorData<uint8_t>(output));
 80af3ee:	9909      	ldr	r1, [sp, #36]	; 0x24
 80af3f0:	4628      	mov	r0, r5
 80af3f2:	f7f3 fa8e 	bl	80a2912 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
  return depthwise_conv::DepthwiseConvBasicKernel<
      DepthwiseConvOutputRounding::kAwayFromZero>::Run(params, input_shape,
                                                       input_data, filter_shape,
                                                       filter_data, bias_shape,
                                                       bias_data, output_shape,
                                                       output_data);
 80af3f6:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80af3f8:	464a      	mov	r2, r9
 80af3fa:	685b      	ldr	r3, [r3, #4]
 80af3fc:	a94d      	add	r1, sp, #308	; 0x134
 80af3fe:	9304      	str	r3, [sp, #16]
 80af400:	a852      	add	r0, sp, #328	; 0x148
 80af402:	ab48      	add	r3, sp, #288	; 0x120
 80af404:	9503      	str	r5, [sp, #12]
 80af406:	9402      	str	r4, [sp, #8]
 80af408:	e88d 0180 	stmia.w	sp, {r7, r8}
 80af40c:	f7ff fa14 	bl	80ae838 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph>
 80af410:	4628      	mov	r0, r5
 80af412:	f7f2 ffce 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
        GetTensorData<uint8_t>(output));
  } else {
    tflite::reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
        GetTensorShape(filter), GetTensorData<uint8_t>(filter),
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
 80af416:	4640      	mov	r0, r8
 80af418:	f7f2 ffcb 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
        GetTensorData<int32_t>(bias), GetTensorShape(output),
        GetTensorData<uint8_t>(output));
  } else {
    tflite::reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
        GetTensorShape(filter), GetTensorData<uint8_t>(filter),
 80af41c:	a848      	add	r0, sp, #288	; 0x120
 80af41e:	f7f2 ffc8 	bl	80a23b2 <_ZN6tflite12RuntimeShapeD1Ev>
        GetTensorData<uint8_t>(filter), GetTensorShape(bias),
        GetTensorData<int32_t>(bias), GetTensorShape(output),
        GetTensorData<uint8_t>(output));
  } else {
    tflite::reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
 80af422:	a84d      	add	r0, sp, #308	; 0x134
 80af424:	e5df      	b.n	80aefe6 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x56e>
      context->ReportError(context, "Type %s (%d) not supported.",
                           TfLiteTypeGetName(input->type), input->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
 80af426:	f50d 7d69 	add.w	sp, sp, #932	; 0x3a4
 80af42a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80af42e:	bf00      	nop

080af430 <_ZN6tflite19GreedyMemoryPlannerD1Ev>:
  buffer_offsets_ = reinterpret_cast<int*>(next_free);
}

GreedyMemoryPlanner::~GreedyMemoryPlanner() {
  // We don't own the scratch buffer, so don't deallocate anything.
}
 80af430:	4770      	bx	lr

080af432 <_ZN6tflite19GreedyMemoryPlanner14GetBufferCountEv>:
    line[kLineWidth] = 0;
    error_reporter->Report("%s", line);
  }
}

int GreedyMemoryPlanner::GetBufferCount() { return buffer_count_; }
 80af432:	6880      	ldr	r0, [r0, #8]
 80af434:	4770      	bx	lr

080af436 <_ZN6tflite19GreedyMemoryPlannerD0Ev>:
  next_free += sizeof(ListEntry) * max_buffer_count_;

  buffer_offsets_ = reinterpret_cast<int*>(next_free);
}

GreedyMemoryPlanner::~GreedyMemoryPlanner() {
 80af436:	b510      	push	{r4, lr}
 80af438:	4604      	mov	r4, r0
  // We don't own the scratch buffer, so don't deallocate anything.
}
 80af43a:	2128      	movs	r1, #40	; 0x28
 80af43c:	f001 fadd 	bl	80b09fa <_ZdlPvj>
 80af440:	4620      	mov	r0, r4
 80af442:	bd10      	pop	{r4, pc}

080af444 <_ZN6tflite19GreedyMemoryPlanner9AddBufferEPNS_13ErrorReporterEiii>:

TfLiteStatus GreedyMemoryPlanner::AddBuffer(
    tflite::ErrorReporter* error_reporter, int size, int first_time_used,
    int last_time_used) {
 80af444:	b570      	push	{r4, r5, r6, lr}
 80af446:	4616      	mov	r6, r2
  if (buffer_count_ >= max_buffer_count_) {
 80af448:	6884      	ldr	r4, [r0, #8]
 80af44a:	6842      	ldr	r2, [r0, #4]
  // We don't own the scratch buffer, so don't deallocate anything.
}

TfLiteStatus GreedyMemoryPlanner::AddBuffer(
    tflite::ErrorReporter* error_reporter, int size, int first_time_used,
    int last_time_used) {
 80af44c:	460d      	mov	r5, r1
  if (buffer_count_ >= max_buffer_count_) {
 80af44e:	4294      	cmp	r4, r2
 80af450:	db05      	blt.n	80af45e <_ZN6tflite19GreedyMemoryPlanner9AddBufferEPNS_13ErrorReporterEiii+0x1a>
    error_reporter->Report("Too many buffers (max is %d)", max_buffer_count_);
 80af452:	490b      	ldr	r1, [pc, #44]	; (80af480 <_ZN6tflite19GreedyMemoryPlanner9AddBufferEPNS_13ErrorReporterEiii+0x3c>)
 80af454:	4628      	mov	r0, r5
 80af456:	f7f0 ffa9 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return kTfLiteError;
 80af45a:	2001      	movs	r0, #1
 80af45c:	bd70      	pop	{r4, r5, r6, pc}
  }
  BufferRequirements* current = &requirements_[buffer_count_];
 80af45e:	210c      	movs	r1, #12
 80af460:	4361      	muls	r1, r4
 80af462:	68c5      	ldr	r5, [r0, #12]
 80af464:	186c      	adds	r4, r5, r1
  current->size = size;
 80af466:	506e      	str	r6, [r5, r1]
  current->first_time_used = first_time_used;
 80af468:	6063      	str	r3, [r4, #4]
  current->last_time_used = last_time_used;
 80af46a:	9b04      	ldr	r3, [sp, #16]
 80af46c:	60a3      	str	r3, [r4, #8]
  ++buffer_count_;
 80af46e:	6883      	ldr	r3, [r0, #8]
 80af470:	3301      	adds	r3, #1
 80af472:	6083      	str	r3, [r0, #8]
  need_to_calculate_offsets_ = true;
 80af474:	2301      	movs	r3, #1
 80af476:	f880 3024 	strb.w	r3, [r0, #36]	; 0x24
  return kTfLiteOk;
 80af47a:	2000      	movs	r0, #0
}
 80af47c:	bd70      	pop	{r4, r5, r6, pc}
 80af47e:	bf00      	nop
 80af480:	080b7359 	.word	0x080b7359

080af484 <_ZN6tflite18ReverseSortInPlaceEPiS0_i>:
namespace tflite {

// Simple stable in-place sort function. Not time-efficient for large arrays.
// Would normally be in an anonymous namespace to keep it private, but we want
// to be able to test it externally.
void ReverseSortInPlace(int* values, int* ids, int size) {
 80af484:	b5f0      	push	{r4, r5, r6, r7, lr}
 80af486:	4696      	mov	lr, r2
 80af488:	4604      	mov	r4, r0
 80af48a:	460b      	mov	r3, r1
  bool any_swapped;
  do {
    any_swapped = false;
    for (int i = 1; i < size; ++i) {
 80af48c:	2501      	movs	r5, #1
// Would normally be in an anonymous namespace to keep it private, but we want
// to be able to test it externally.
void ReverseSortInPlace(int* values, int* ids, int size) {
  bool any_swapped;
  do {
    any_swapped = false;
 80af48e:	2600      	movs	r6, #0
    for (int i = 1; i < size; ++i) {
 80af490:	4575      	cmp	r5, lr
 80af492:	da0f      	bge.n	80af4b4 <_ZN6tflite18ReverseSortInPlaceEPiS0_i+0x30>
      if (values[i - 1] < values[i]) {
 80af494:	6827      	ldr	r7, [r4, #0]
 80af496:	f854 2f04 	ldr.w	r2, [r4, #4]!
 80af49a:	4297      	cmp	r7, r2
 80af49c:	da07      	bge.n	80af4ae <_ZN6tflite18ReverseSortInPlaceEPiS0_i+0x2a>
        const int value_temp = values[i - 1];
        values[i - 1] = values[i];
 80af49e:	f844 2c04 	str.w	r2, [r4, #-4]
        values[i] = value_temp;
 80af4a2:	6027      	str	r7, [r4, #0]
        const int id_temp = ids[i - 1];
        ids[i - 1] = ids[i];
 80af4a4:	e893 0044 	ldmia.w	r3, {r2, r6}
 80af4a8:	601e      	str	r6, [r3, #0]
        ids[i] = id_temp;
 80af4aa:	605a      	str	r2, [r3, #4]
        any_swapped = true;
 80af4ac:	2601      	movs	r6, #1
// to be able to test it externally.
void ReverseSortInPlace(int* values, int* ids, int size) {
  bool any_swapped;
  do {
    any_swapped = false;
    for (int i = 1; i < size; ++i) {
 80af4ae:	3501      	adds	r5, #1
 80af4b0:	3304      	adds	r3, #4
 80af4b2:	e7ed      	b.n	80af490 <_ZN6tflite18ReverseSortInPlaceEPiS0_i+0xc>
// Simple stable in-place sort function. Not time-efficient for large arrays.
// Would normally be in an anonymous namespace to keep it private, but we want
// to be able to test it externally.
void ReverseSortInPlace(int* values, int* ids, int size) {
  bool any_swapped;
  do {
 80af4b4:	2e00      	cmp	r6, #0
 80af4b6:	d1e7      	bne.n	80af488 <_ZN6tflite18ReverseSortInPlaceEPiS0_i+0x4>
        ids[i] = id_temp;
        any_swapped = true;
      }
    }
  } while (any_swapped);
}
 80af4b8:	bdf0      	pop	{r4, r5, r6, r7, pc}
	...

080af4bc <_ZN6tflite19GreedyMemoryPlannerC1EPhi>:

GreedyMemoryPlanner::GreedyMemoryPlanner(unsigned char* scratch_buffer,
                                         int scratch_buffer_size)
    : buffer_count_(0), need_to_calculate_offsets_(true) {
 80af4bc:	4b0c      	ldr	r3, [pc, #48]	; (80af4f0 <_ZN6tflite19GreedyMemoryPlannerC1EPhi+0x34>)
      }
    }
  } while (any_swapped);
}

GreedyMemoryPlanner::GreedyMemoryPlanner(unsigned char* scratch_buffer,
 80af4be:	b510      	push	{r4, lr}
                                         int scratch_buffer_size)
    : buffer_count_(0), need_to_calculate_offsets_(true) {
 80af4c0:	6003      	str	r3, [r0, #0]
 80af4c2:	2300      	movs	r3, #0
 80af4c4:	6083      	str	r3, [r0, #8]
 80af4c6:	2301      	movs	r3, #1
 80af4c8:	f880 3024 	strb.w	r3, [r0, #36]	; 0x24
                              sizeof(int) +  // buffer_sizes_sorted_by_size_
                              sizeof(int) +  // buffer_ids_sorted_by_size_
                              sizeof(ListEntry) +  // buffers_sorted_by_offset_
                              sizeof(int);         // buffer_offsets_;
  // Allocate the arrays we need within the scratch buffer arena.
  max_buffer_count_ = scratch_buffer_size / per_buffer_size;
 80af4cc:	2324      	movs	r3, #36	; 0x24
 80af4ce:	fb92 f2f3 	sdiv	r2, r2, r3

  unsigned char* next_free = scratch_buffer;
  requirements_ = reinterpret_cast<BufferRequirements*>(next_free);
  next_free += sizeof(BufferRequirements) * max_buffer_count_;
 80af4d2:	230c      	movs	r3, #12
 80af4d4:	4353      	muls	r3, r2
                              sizeof(int) +  // buffer_sizes_sorted_by_size_
                              sizeof(int) +  // buffer_ids_sorted_by_size_
                              sizeof(ListEntry) +  // buffers_sorted_by_offset_
                              sizeof(int);         // buffer_offsets_;
  // Allocate the arrays we need within the scratch buffer arena.
  max_buffer_count_ = scratch_buffer_size / per_buffer_size;
 80af4d6:	6042      	str	r2, [r0, #4]

  unsigned char* next_free = scratch_buffer;
  requirements_ = reinterpret_cast<BufferRequirements*>(next_free);
 80af4d8:	60c1      	str	r1, [r0, #12]
  next_free += sizeof(BufferRequirements) * max_buffer_count_;

  buffer_sizes_sorted_by_size_ = reinterpret_cast<int*>(next_free);
  next_free += sizeof(int) * max_buffer_count_;
 80af4da:	0092      	lsls	r2, r2, #2
  // Allocate the arrays we need within the scratch buffer arena.
  max_buffer_count_ = scratch_buffer_size / per_buffer_size;

  unsigned char* next_free = scratch_buffer;
  requirements_ = reinterpret_cast<BufferRequirements*>(next_free);
  next_free += sizeof(BufferRequirements) * max_buffer_count_;
 80af4dc:	4419      	add	r1, r3

  buffer_sizes_sorted_by_size_ = reinterpret_cast<int*>(next_free);
 80af4de:	6101      	str	r1, [r0, #16]
  next_free += sizeof(int) * max_buffer_count_;
 80af4e0:	4411      	add	r1, r2

  buffer_ids_sorted_by_size_ = reinterpret_cast<int*>(next_free);
  next_free += sizeof(int) * max_buffer_count_;
 80af4e2:	440a      	add	r2, r1

  buffers_sorted_by_offset_ = reinterpret_cast<ListEntry*>(next_free);
 80af4e4:	6182      	str	r2, [r0, #24]
  next_free += sizeof(ListEntry) * max_buffer_count_;

  buffer_offsets_ = reinterpret_cast<int*>(next_free);
 80af4e6:	441a      	add	r2, r3
  next_free += sizeof(BufferRequirements) * max_buffer_count_;

  buffer_sizes_sorted_by_size_ = reinterpret_cast<int*>(next_free);
  next_free += sizeof(int) * max_buffer_count_;

  buffer_ids_sorted_by_size_ = reinterpret_cast<int*>(next_free);
 80af4e8:	6141      	str	r1, [r0, #20]
  next_free += sizeof(int) * max_buffer_count_;

  buffers_sorted_by_offset_ = reinterpret_cast<ListEntry*>(next_free);
  next_free += sizeof(ListEntry) * max_buffer_count_;

  buffer_offsets_ = reinterpret_cast<int*>(next_free);
 80af4ea:	6202      	str	r2, [r0, #32]
}
 80af4ec:	bd10      	pop	{r4, pc}
 80af4ee:	bf00      	nop
 80af4f0:	080b73dc 	.word	0x080b73dc

080af4f4 <_ZNK6tflite19GreedyMemoryPlanner22DoesEntryOverlapInTimeEPKNS0_9ListEntryEii>:
  return kTfLiteOk;
}

bool GreedyMemoryPlanner::DoesEntryOverlapInTime(
    const GreedyMemoryPlanner::ListEntry* entry, const int first_time_used,
    const int last_time_used) const {
 80af4f4:	b510      	push	{r4, lr}
  const BufferRequirements* entry_requirements =
      &requirements_[entry->requirements_index];
 80af4f6:	684c      	ldr	r4, [r1, #4]
 80af4f8:	68c1      	ldr	r1, [r0, #12]
 80af4fa:	200c      	movs	r0, #12
 80af4fc:	fb00 1104 	mla	r1, r0, r4, r1
  if (entry_requirements->first_time_used > last_time_used) {
 80af500:	6848      	ldr	r0, [r1, #4]
 80af502:	4298      	cmp	r0, r3
 80af504:	dc05      	bgt.n	80af512 <_ZNK6tflite19GreedyMemoryPlanner22DoesEntryOverlapInTimeEPKNS0_9ListEntryEii+0x1e>
    return false;
  }
  if (first_time_used > entry_requirements->last_time_used) {
 80af506:	6888      	ldr	r0, [r1, #8]
 80af508:	4290      	cmp	r0, r2
 80af50a:	bfb4      	ite	lt
 80af50c:	2000      	movlt	r0, #0
 80af50e:	2001      	movge	r0, #1
 80af510:	bd10      	pop	{r4, pc}
    const GreedyMemoryPlanner::ListEntry* entry, const int first_time_used,
    const int last_time_used) const {
  const BufferRequirements* entry_requirements =
      &requirements_[entry->requirements_index];
  if (entry_requirements->first_time_used > last_time_used) {
    return false;
 80af512:	2000      	movs	r0, #0
  }
  if (first_time_used > entry_requirements->last_time_used) {
    return false;
  }
  return true;
}
 80af514:	bd10      	pop	{r4, pc}

080af516 <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii>:

GreedyMemoryPlanner::ListEntry*
GreedyMemoryPlanner::NextSimultaneouslyActiveBuffer(
    const GreedyMemoryPlanner::ListEntry* start, const int first_time_used,
    const int last_time_used) {
 80af516:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
 80af51a:	4605      	mov	r5, r0
 80af51c:	4616      	mov	r6, r2
 80af51e:	461f      	mov	r7, r3
  ListEntry* result = nullptr;
  ListEntry* candidate_next_entry;
  if (start == nullptr) {
 80af520:	b919      	cbnz	r1, 80af52a <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0x14>
    candidate_next_entry = &buffers_sorted_by_offset_[0];
 80af522:	6984      	ldr	r4, [r0, #24]
    }
    if (candidate_next_entry->next_entry_index == -1) {
      break;
    }
    candidate_next_entry =
        &buffers_sorted_by_offset_[candidate_next_entry->next_entry_index];
 80af524:	f04f 080c 	mov.w	r8, #12
 80af528:	e00d      	b.n	80af546 <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0x30>
  ListEntry* result = nullptr;
  ListEntry* candidate_next_entry;
  if (start == nullptr) {
    candidate_next_entry = &buffers_sorted_by_offset_[0];
  } else {
    if (start->next_entry_index == -1) {
 80af52a:	688b      	ldr	r3, [r1, #8]
 80af52c:	1c59      	adds	r1, r3, #1
 80af52e:	d013      	beq.n	80af558 <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0x42>
      return nullptr;
    }
    candidate_next_entry = &buffers_sorted_by_offset_[start->next_entry_index];
 80af530:	6982      	ldr	r2, [r0, #24]
 80af532:	240c      	movs	r4, #12
 80af534:	fb04 2403 	mla	r4, r4, r3, r2
 80af538:	e7f4      	b.n	80af524 <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0xe>
    if (DoesEntryOverlapInTime(candidate_next_entry, first_time_used,
                               last_time_used)) {
      result = candidate_next_entry;
      break;
    }
    if (candidate_next_entry->next_entry_index == -1) {
 80af53a:	68a3      	ldr	r3, [r4, #8]
 80af53c:	1c5a      	adds	r2, r3, #1
 80af53e:	d00f      	beq.n	80af560 <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0x4a>
      break;
    }
    candidate_next_entry =
        &buffers_sorted_by_offset_[candidate_next_entry->next_entry_index];
 80af540:	69ac      	ldr	r4, [r5, #24]
 80af542:	fb08 4403 	mla	r4, r8, r3, r4
      return nullptr;
    }
    candidate_next_entry = &buffers_sorted_by_offset_[start->next_entry_index];
  }
  do {
    if (DoesEntryOverlapInTime(candidate_next_entry, first_time_used,
 80af546:	463b      	mov	r3, r7
 80af548:	4632      	mov	r2, r6
 80af54a:	4621      	mov	r1, r4
 80af54c:	4628      	mov	r0, r5
 80af54e:	f7ff ffd1 	bl	80af4f4 <_ZNK6tflite19GreedyMemoryPlanner22DoesEntryOverlapInTimeEPKNS0_9ListEntryEii>
 80af552:	2800      	cmp	r0, #0
 80af554:	d0f1      	beq.n	80af53a <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0x24>
 80af556:	e002      	b.n	80af55e <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0x48>
  ListEntry* candidate_next_entry;
  if (start == nullptr) {
    candidate_next_entry = &buffers_sorted_by_offset_[0];
  } else {
    if (start->next_entry_index == -1) {
      return nullptr;
 80af558:	2000      	movs	r0, #0
 80af55a:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80af55e:	4620      	mov	r0, r4
    }
    candidate_next_entry =
        &buffers_sorted_by_offset_[candidate_next_entry->next_entry_index];
  } while (true);
  return result;
}
 80af560:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}

080af564 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv>:

void GreedyMemoryPlanner::CalculateOffsetsIfNeeded() {
 80af564:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  if (!need_to_calculate_offsets_ || (buffer_count_ == 0)) {
 80af568:	f890 3024 	ldrb.w	r3, [r0, #36]	; 0x24
        &buffers_sorted_by_offset_[candidate_next_entry->next_entry_index];
  } while (true);
  return result;
}

void GreedyMemoryPlanner::CalculateOffsetsIfNeeded() {
 80af56c:	b085      	sub	sp, #20
 80af56e:	4604      	mov	r4, r0
  if (!need_to_calculate_offsets_ || (buffer_count_ == 0)) {
 80af570:	2b00      	cmp	r3, #0
 80af572:	f000 8089 	beq.w	80af688 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x124>
 80af576:	6883      	ldr	r3, [r0, #8]
 80af578:	2b00      	cmp	r3, #0
 80af57a:	f000 8085 	beq.w	80af688 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x124>
    return;
  }
  need_to_calculate_offsets_ = false;
 80af57e:	2300      	movs	r3, #0
 80af580:	f880 3024 	strb.w	r3, [r0, #36]	; 0x24
  // This helps find a more compact layout. Intuitively, you can think
  // about putting the large buffers in place first, and then the
  // smaller buffers can fit in the gaps, rather than fragmenting the
  // gaps with small buffers at the beginning.
  for (int i = 0; i < buffer_count_; ++i) {
    buffer_sizes_sorted_by_size_[i] = requirements_[i].size;
 80af584:	250c      	movs	r5, #12
    buffer_ids_sorted_by_size_[i] = i;
    buffer_offsets_[i] = -1;
 80af586:	f04f 31ff 	mov.w	r1, #4294967295	; 0xffffffff
  // Start off by ordering the buffers in descending order of size.
  // This helps find a more compact layout. Intuitively, you can think
  // about putting the large buffers in place first, and then the
  // smaller buffers can fit in the gaps, rather than fragmenting the
  // gaps with small buffers at the beginning.
  for (int i = 0; i < buffer_count_; ++i) {
 80af58a:	68a2      	ldr	r2, [r4, #8]
 80af58c:	429a      	cmp	r2, r3
 80af58e:	dd0e      	ble.n	80af5ae <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x4a>
    buffer_sizes_sorted_by_size_[i] = requirements_[i].size;
 80af590:	fb05 f203 	mul.w	r2, r5, r3
 80af594:	68e0      	ldr	r0, [r4, #12]
 80af596:	5880      	ldr	r0, [r0, r2]
 80af598:	6922      	ldr	r2, [r4, #16]
 80af59a:	f842 0023 	str.w	r0, [r2, r3, lsl #2]
    buffer_ids_sorted_by_size_[i] = i;
 80af59e:	6962      	ldr	r2, [r4, #20]
 80af5a0:	f842 3023 	str.w	r3, [r2, r3, lsl #2]
    buffer_offsets_[i] = -1;
 80af5a4:	6a22      	ldr	r2, [r4, #32]
 80af5a6:	f842 1023 	str.w	r1, [r2, r3, lsl #2]
  // Start off by ordering the buffers in descending order of size.
  // This helps find a more compact layout. Intuitively, you can think
  // about putting the large buffers in place first, and then the
  // smaller buffers can fit in the gaps, rather than fragmenting the
  // gaps with small buffers at the beginning.
  for (int i = 0; i < buffer_count_; ++i) {
 80af5aa:	3301      	adds	r3, #1
 80af5ac:	e7ed      	b.n	80af58a <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x26>
    buffer_offsets_[i] = -1;
  }
  // This sorting algorithm is naive, and may end up taking a very long time
  // with hundreds of buffers.
  ReverseSortInPlace(buffer_sizes_sorted_by_size_, buffer_ids_sorted_by_size_,
                     buffer_count_);
 80af5ae:	6961      	ldr	r1, [r4, #20]
 80af5b0:	6920      	ldr	r0, [r4, #16]
 80af5b2:	f7ff ff67 	bl	80af484 <_ZN6tflite18ReverseSortInPlaceEPiS0_i>

  // Put the largest buffer at offset zero to start the process.
  ListEntry* first_entry = &buffers_sorted_by_offset_[0];
 80af5b6:	f8d4 8018 	ldr.w	r8, [r4, #24]
  first_entry->offset = 0;
 80af5ba:	2300      	movs	r3, #0
 80af5bc:	f8c8 3000 	str.w	r3, [r8]
  first_entry->requirements_index = buffer_ids_sorted_by_size_[0];
 80af5c0:	6962      	ldr	r2, [r4, #20]
  first_entry->next_entry_index = -1;
  next_free_entry_ = 1;
 80af5c2:	2501      	movs	r5, #1
                     buffer_count_);

  // Put the largest buffer at offset zero to start the process.
  ListEntry* first_entry = &buffers_sorted_by_offset_[0];
  first_entry->offset = 0;
  first_entry->requirements_index = buffer_ids_sorted_by_size_[0];
 80af5c4:	6812      	ldr	r2, [r2, #0]
  // Work through the rest of the buffers to find a good gap to place each one.
  for (int i = 1; i < buffer_count_; ++i) {
    // The id is the order the buffer was originally added by the client.
    const int buffer_id = buffer_ids_sorted_by_size_[i];
    // Look at what size and time range the buffer needs to be active.
    BufferRequirements* wanted_requirements = &requirements_[buffer_id];
 80af5c6:	f04f 090c 	mov.w	r9, #12
                     buffer_count_);

  // Put the largest buffer at offset zero to start the process.
  ListEntry* first_entry = &buffers_sorted_by_offset_[0];
  first_entry->offset = 0;
  first_entry->requirements_index = buffer_ids_sorted_by_size_[0];
 80af5ca:	f8c8 2004 	str.w	r2, [r8, #4]
  first_entry->next_entry_index = -1;
 80af5ce:	f04f 32ff 	mov.w	r2, #4294967295	; 0xffffffff
 80af5d2:	f8c8 2008 	str.w	r2, [r8, #8]
  next_free_entry_ = 1;
  buffer_offsets_[buffer_ids_sorted_by_size_[0]] = 0;
 80af5d6:	6962      	ldr	r2, [r4, #20]
  // Put the largest buffer at offset zero to start the process.
  ListEntry* first_entry = &buffers_sorted_by_offset_[0];
  first_entry->offset = 0;
  first_entry->requirements_index = buffer_ids_sorted_by_size_[0];
  first_entry->next_entry_index = -1;
  next_free_entry_ = 1;
 80af5d8:	61e5      	str	r5, [r4, #28]
  buffer_offsets_[buffer_ids_sorted_by_size_[0]] = 0;
 80af5da:	6811      	ldr	r1, [r2, #0]
 80af5dc:	6a22      	ldr	r2, [r4, #32]
 80af5de:	f842 3021 	str.w	r3, [r2, r1, lsl #2]

  // Work through the rest of the buffers to find a good gap to place each one.
  for (int i = 1; i < buffer_count_; ++i) {
 80af5e2:	68a3      	ldr	r3, [r4, #8]
 80af5e4:	42ab      	cmp	r3, r5
 80af5e6:	dd4f      	ble.n	80af688 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x124>
    // buffers are stored in the order of their starting position in the arena
    // so that it's easy to find the next buffer in memory, and so the gap.
    // The candidate_entry variable holds the buffer that we're considering
    // placing the current buffer after.
    ListEntry* prior_entry = nullptr;
    int candidate_offset = 0;
 80af5e8:	2600      	movs	r6, #0
    // Find the first buffer that's active in our time range. All placed
    // buffers are stored in the order of their starting position in the arena
    // so that it's easy to find the next buffer in memory, and so the gap.
    // The candidate_entry variable holds the buffer that we're considering
    // placing the current buffer after.
    ListEntry* prior_entry = nullptr;
 80af5ea:	4637      	mov	r7, r6
  buffer_offsets_[buffer_ids_sorted_by_size_[0]] = 0;

  // Work through the rest of the buffers to find a good gap to place each one.
  for (int i = 1; i < buffer_count_; ++i) {
    // The id is the order the buffer was originally added by the client.
    const int buffer_id = buffer_ids_sorted_by_size_[i];
 80af5ec:	6963      	ldr	r3, [r4, #20]
    // Look at what size and time range the buffer needs to be active.
    BufferRequirements* wanted_requirements = &requirements_[buffer_id];
 80af5ee:	f8d4 b00c 	ldr.w	fp, [r4, #12]
  buffer_offsets_[buffer_ids_sorted_by_size_[0]] = 0;

  // Work through the rest of the buffers to find a good gap to place each one.
  for (int i = 1; i < buffer_count_; ++i) {
    // The id is the order the buffer was originally added by the client.
    const int buffer_id = buffer_ids_sorted_by_size_[i];
 80af5f2:	f853 a025 	ldr.w	sl, [r3, r5, lsl #2]
    // Look at what size and time range the buffer needs to be active.
    BufferRequirements* wanted_requirements = &requirements_[buffer_id];
 80af5f6:	fb09 f20a 	mul.w	r2, r9, sl
 80af5fa:	eb0b 0302 	add.w	r3, fp, r2
    const int wanted_size = wanted_requirements->size;
 80af5fe:	f85b 2002 	ldr.w	r2, [fp, r2]
 80af602:	9201      	str	r2, [sp, #4]
    const int wanted_first_time_used = wanted_requirements->first_time_used;
 80af604:	685a      	ldr	r2, [r3, #4]
    const int wanted_last_time_used = wanted_requirements->last_time_used;
 80af606:	689b      	ldr	r3, [r3, #8]
    // The id is the order the buffer was originally added by the client.
    const int buffer_id = buffer_ids_sorted_by_size_[i];
    // Look at what size and time range the buffer needs to be active.
    BufferRequirements* wanted_requirements = &requirements_[buffer_id];
    const int wanted_size = wanted_requirements->size;
    const int wanted_first_time_used = wanted_requirements->first_time_used;
 80af608:	9202      	str	r2, [sp, #8]
    const int wanted_last_time_used = wanted_requirements->last_time_used;
 80af60a:	9303      	str	r3, [sp, #12]
    int candidate_offset = 0;
    // Loop through the offset-ordered list of buffers, looking for gaps.
    while (true) {
      // Find out what the next active buffer is.
      ListEntry* next_entry = NextSimultaneouslyActiveBuffer(
          prior_entry, wanted_first_time_used, wanted_last_time_used);
 80af60c:	9b03      	ldr	r3, [sp, #12]
 80af60e:	9a02      	ldr	r2, [sp, #8]
 80af610:	4639      	mov	r1, r7
 80af612:	4620      	mov	r0, r4
 80af614:	f7ff ff7f 	bl	80af516 <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii>

      if (prior_entry) {
 80af618:	b14f      	cbz	r7, 80af62e <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0xca>
        BufferRequirements* candidate_requirements =
            &requirements_[prior_entry->requirements_index];
        const int prior_entry_offset =
            prior_entry->offset + candidate_requirements->size;
 80af61a:	687b      	ldr	r3, [r7, #4]
 80af61c:	fb09 f303 	mul.w	r3, r9, r3
 80af620:	f85b 2003 	ldr.w	r2, [fp, r3]
 80af624:	683b      	ldr	r3, [r7, #0]
 80af626:	4413      	add	r3, r2
 80af628:	429e      	cmp	r6, r3
 80af62a:	bfb8      	it	lt
 80af62c:	461e      	movlt	r6, r3
        if (prior_entry_offset > candidate_offset) {
          candidate_offset = prior_entry_offset;
        }
      }
      if (next_entry == nullptr) {
 80af62e:	b978      	cbnz	r0, 80af650 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0xec>
    }
    // At this point, we've either found a gap (possibly at the end of the
    // list) and want to place the buffer there, or there are no other active
    // buffers in this time range and so we can put it at offset zero.
    // Record the buffer's offset in our plan.
    buffer_offsets_[buffer_id] = candidate_offset;
 80af630:	6a23      	ldr	r3, [r4, #32]
 80af632:	f843 602a 	str.w	r6, [r3, sl, lsl #2]
    // Add the newly-placed buffer to our offset-ordered list, so that
    // subsequent passes can fit in their buffers around it.
    ListEntry* new_entry = &buffers_sorted_by_offset_[next_free_entry_];
 80af636:	69e3      	ldr	r3, [r4, #28]
 80af638:	69a2      	ldr	r2, [r4, #24]
 80af63a:	fb09 f303 	mul.w	r3, r9, r3
 80af63e:	18d7      	adds	r7, r2, r3
    new_entry->offset = candidate_offset;
 80af640:	50d6      	str	r6, [r2, r3]
    new_entry->requirements_index = buffer_id;
 80af642:	f8c7 a004 	str.w	sl, [r7, #4]
    const int new_entry_index = next_free_entry_;
 80af646:	69e0      	ldr	r0, [r4, #28]
    ++next_free_entry_;
 80af648:	1c43      	adds	r3, r0, #1
 80af64a:	61e3      	str	r3, [r4, #28]
 80af64c:	4643      	mov	r3, r8
 80af64e:	e011      	b.n	80af674 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x110>
        // here.
        break;
      }
      // Find out how much space there is between us and the next buffer.
      const int gap = next_entry->offset - candidate_offset;
      if (gap >= wanted_size) {
 80af650:	6803      	ldr	r3, [r0, #0]
 80af652:	9a01      	ldr	r2, [sp, #4]
 80af654:	1b9b      	subs	r3, r3, r6
 80af656:	429a      	cmp	r2, r3
 80af658:	4607      	mov	r7, r0
 80af65a:	dcd7      	bgt.n	80af60c <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0xa8>
 80af65c:	e7e8      	b.n	80af630 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0xcc>
        // We're at the end of the list, so just add the new entry here.
        current_entry->next_entry_index = new_entry_index;
        new_entry->next_entry_index = -1;
        break;
      }
      ListEntry* next_entry = &buffers_sorted_by_offset_[next_entry_index];
 80af65e:	fb09 f102 	mul.w	r1, r9, r2
 80af662:	f8d4 e018 	ldr.w	lr, [r4, #24]
 80af666:	eb0e 0c01 	add.w	ip, lr, r1
      if (next_entry->offset > candidate_offset) {
 80af66a:	f85e 1001 	ldr.w	r1, [lr, r1]
 80af66e:	428e      	cmp	r6, r1
 80af670:	db06      	blt.n	80af680 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x11c>
 80af672:	4663      	mov	r3, ip
    ++next_free_entry_;
    ListEntry* current_entry = first_entry;
    // Make sure that we insert the buffer at the correct place in the ordered
    // list.
    while (true) {
      const int next_entry_index = current_entry->next_entry_index;
 80af674:	689a      	ldr	r2, [r3, #8]
      if (next_entry_index == -1) {
 80af676:	1c51      	adds	r1, r2, #1
 80af678:	d1f1      	bne.n	80af65e <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0xfa>
        // We're at the end of the list, so just add the new entry here.
        current_entry->next_entry_index = new_entry_index;
 80af67a:	6098      	str	r0, [r3, #8]
        new_entry->next_entry_index = -1;
 80af67c:	60ba      	str	r2, [r7, #8]
 80af67e:	e001      	b.n	80af684 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x120>
      }
      ListEntry* next_entry = &buffers_sorted_by_offset_[next_entry_index];
      if (next_entry->offset > candidate_offset) {
        // We're at the right spot to do an insertion and retain the sorting
        // order, so place the new entry here.
        new_entry->next_entry_index = current_entry->next_entry_index;
 80af680:	60ba      	str	r2, [r7, #8]
        current_entry->next_entry_index = new_entry_index;
 80af682:	6098      	str	r0, [r3, #8]
  first_entry->next_entry_index = -1;
  next_free_entry_ = 1;
  buffer_offsets_[buffer_ids_sorted_by_size_[0]] = 0;

  // Work through the rest of the buffers to find a good gap to place each one.
  for (int i = 1; i < buffer_count_; ++i) {
 80af684:	3501      	adds	r5, #1
 80af686:	e7ac      	b.n	80af5e2 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x7e>
        break;
      }
      current_entry = next_entry;
    }
  }
}
 80af688:	b005      	add	sp, #20
 80af68a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

080af68e <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv>:

int GreedyMemoryPlanner::GetMaximumMemorySize() {
 80af68e:	b570      	push	{r4, r5, r6, lr}
 80af690:	4604      	mov	r4, r0
  CalculateOffsetsIfNeeded();
 80af692:	f7ff ff67 	bl	80af564 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv>
  if (buffer_count_ == 0) {
 80af696:	68a0      	ldr	r0, [r4, #8]
 80af698:	b198      	cbz	r0, 80af6c2 <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv+0x34>
    return 0;
  }
  ListEntry* entry = &buffers_sorted_by_offset_[0];
 80af69a:	69a1      	ldr	r1, [r4, #24]
  int max_size = 0;
 80af69c:	2000      	movs	r0, #0
int GreedyMemoryPlanner::GetMaximumMemorySize() {
  CalculateOffsetsIfNeeded();
  if (buffer_count_ == 0) {
    return 0;
  }
  ListEntry* entry = &buffers_sorted_by_offset_[0];
 80af69e:	460b      	mov	r3, r1
  int max_size = 0;
  while (entry) {
    BufferRequirements* requirements =
        &requirements_[entry->requirements_index];
    const int current_size = entry->offset + requirements->size;
 80af6a0:	250c      	movs	r5, #12
  if (buffer_count_ == 0) {
    return 0;
  }
  ListEntry* entry = &buffers_sorted_by_offset_[0];
  int max_size = 0;
  while (entry) {
 80af6a2:	b173      	cbz	r3, 80af6c2 <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv+0x34>
    BufferRequirements* requirements =
        &requirements_[entry->requirements_index];
    const int current_size = entry->offset + requirements->size;
 80af6a4:	685a      	ldr	r2, [r3, #4]
 80af6a6:	68e6      	ldr	r6, [r4, #12]
 80af6a8:	436a      	muls	r2, r5
 80af6aa:	58b6      	ldr	r6, [r6, r2]
 80af6ac:	681a      	ldr	r2, [r3, #0]
    if (current_size > max_size) {
      max_size = current_size;
    }
    if (entry->next_entry_index == -1) {
 80af6ae:	689b      	ldr	r3, [r3, #8]
  ListEntry* entry = &buffers_sorted_by_offset_[0];
  int max_size = 0;
  while (entry) {
    BufferRequirements* requirements =
        &requirements_[entry->requirements_index];
    const int current_size = entry->offset + requirements->size;
 80af6b0:	4432      	add	r2, r6
 80af6b2:	4290      	cmp	r0, r2
 80af6b4:	bfb8      	it	lt
 80af6b6:	4610      	movlt	r0, r2
    if (current_size > max_size) {
      max_size = current_size;
    }
    if (entry->next_entry_index == -1) {
 80af6b8:	1c5a      	adds	r2, r3, #1
 80af6ba:	d002      	beq.n	80af6c2 <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv+0x34>
      break;
    }
    entry = &buffers_sorted_by_offset_[entry->next_entry_index];
 80af6bc:	fb05 1303 	mla	r3, r5, r3, r1
 80af6c0:	e7ef      	b.n	80af6a2 <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv+0x14>
  }
  return max_size;
}
 80af6c2:	bd70      	pop	{r4, r5, r6, pc}

080af6c4 <_ZN6tflite19GreedyMemoryPlanner18GetOffsetForBufferEPNS_13ErrorReporterEiPi>:
}

int GreedyMemoryPlanner::GetBufferCount() { return buffer_count_; }

TfLiteStatus GreedyMemoryPlanner::GetOffsetForBuffer(
    tflite::ErrorReporter* error_reporter, int buffer_index, int* offset) {
 80af6c4:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
 80af6c6:	4614      	mov	r4, r2
 80af6c8:	4605      	mov	r5, r0
 80af6ca:	460f      	mov	r7, r1
 80af6cc:	461e      	mov	r6, r3
  CalculateOffsetsIfNeeded();
 80af6ce:	f7ff ff49 	bl	80af564 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv>
  if ((buffer_index < 0) || (buffer_index >= buffer_count_)) {
 80af6d2:	2c00      	cmp	r4, #0
 80af6d4:	db02      	blt.n	80af6dc <_ZN6tflite19GreedyMemoryPlanner18GetOffsetForBufferEPNS_13ErrorReporterEiPi+0x18>
 80af6d6:	68ab      	ldr	r3, [r5, #8]
 80af6d8:	429c      	cmp	r4, r3
 80af6da:	db07      	blt.n	80af6ec <_ZN6tflite19GreedyMemoryPlanner18GetOffsetForBufferEPNS_13ErrorReporterEiPi+0x28>
    error_reporter->Report("buffer index %d is outside range 0 to %d",
                           buffer_index, buffer_count_);
 80af6dc:	68ab      	ldr	r3, [r5, #8]
 80af6de:	4622      	mov	r2, r4
 80af6e0:	4905      	ldr	r1, [pc, #20]	; (80af6f8 <_ZN6tflite19GreedyMemoryPlanner18GetOffsetForBufferEPNS_13ErrorReporterEiPi+0x34>)
 80af6e2:	4638      	mov	r0, r7
 80af6e4:	f7f0 fe62 	bl	80a03ac <_ZN6tflite13ErrorReporter6ReportEPKcz>
 80af6e8:	2001      	movs	r0, #1
 80af6ea:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
    return kTfLiteError;
  }
  *offset = buffer_offsets_[buffer_index];
 80af6ec:	6a2b      	ldr	r3, [r5, #32]
  return kTfLiteOk;
 80af6ee:	2000      	movs	r0, #0
  if ((buffer_index < 0) || (buffer_index >= buffer_count_)) {
    error_reporter->Report("buffer index %d is outside range 0 to %d",
                           buffer_index, buffer_count_);
    return kTfLiteError;
  }
  *offset = buffer_offsets_[buffer_index];
 80af6f0:	f853 3024 	ldr.w	r3, [r3, r4, lsl #2]
 80af6f4:	6033      	str	r3, [r6, #0]
  return kTfLiteOk;
}
 80af6f6:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
 80af6f8:	080b7376 	.word	0x080b7376

080af6fc <_ZZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_ENKUlfE_clEf>:
                                           TfLiteTensor* output,
                                           int32_t* act_min, int32_t* act_max) {
  const auto scale = output->params.scale;
  const auto zero_point = output->params.zero_point;

  auto quantize = [scale, zero_point](float f) {
 80af6fc:	b510      	push	{r4, lr}
 80af6fe:	4604      	mov	r4, r0
 80af700:	4608      	mov	r0, r1
    { return __builtin_rint(__x); }

#ifndef __CORRECT_ISO_CPP11_MATH_H_PROTO
  constexpr float
  round(float __x)
  { return __builtin_roundf(__x); }
 80af702:	6821      	ldr	r1, [r4, #0]
 80af704:	f004 f826 	bl	80b3754 <__aeabi_fdiv>
 80af708:	f001 fcd6 	bl	80b10b8 <roundf>
    return zero_point + static_cast<int32_t>(TfLiteRound(f / scale));
  };
 80af70c:	f004 f94a 	bl	80b39a4 <__aeabi_f2iz>
 80af710:	6863      	ldr	r3, [r4, #4]
 80af712:	4418      	add	r0, r3
 80af714:	bd10      	pop	{r4, pc}
	...

080af718 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_>:

namespace {
void CalculateActivationRangeQuantizedImpl(TfLiteFusedActivation activation,
                                           int32_t qmin, int32_t qmax,
                                           TfLiteTensor* output,
                                           int32_t* act_min, int32_t* act_max) {
 80af718:	b5f7      	push	{r0, r1, r2, r4, r5, r6, r7, lr}
 80af71a:	4615      	mov	r5, r2
  const auto scale = output->params.scale;
  const auto zero_point = output->params.zero_point;
 80af71c:	691a      	ldr	r2, [r3, #16]

  auto quantize = [scale, zero_point](float f) {
    return zero_point + static_cast<int32_t>(TfLiteRound(f / scale));
  };
 80af71e:	68db      	ldr	r3, [r3, #12]

  if (activation == kTfLiteActRelu) {
 80af720:	2801      	cmp	r0, #1

namespace {
void CalculateActivationRangeQuantizedImpl(TfLiteFusedActivation activation,
                                           int32_t qmin, int32_t qmax,
                                           TfLiteTensor* output,
                                           int32_t* act_min, int32_t* act_max) {
 80af722:	460f      	mov	r7, r1
 80af724:	9e08      	ldr	r6, [sp, #32]
 80af726:	9c09      	ldr	r4, [sp, #36]	; 0x24
  const auto scale = output->params.scale;
  const auto zero_point = output->params.zero_point;

  auto quantize = [scale, zero_point](float f) {
    return zero_point + static_cast<int32_t>(TfLiteRound(f / scale));
  };
 80af728:	9300      	str	r3, [sp, #0]
 80af72a:	9201      	str	r2, [sp, #4]

  if (activation == kTfLiteActRelu) {
 80af72c:	d108      	bne.n	80af740 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x28>
    *act_min = std::max(qmin, quantize(0.0));
 80af72e:	2100      	movs	r1, #0
 80af730:	4668      	mov	r0, sp
 80af732:	f7ff ffe3 	bl	80af6fc <_ZZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_ENKUlfE_clEf>
 80af736:	42b8      	cmp	r0, r7
 80af738:	bfac      	ite	ge
 80af73a:	6030      	strge	r0, [r6, #0]
 80af73c:	6037      	strlt	r7, [r6, #0]
 80af73e:	e020      	b.n	80af782 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x6a>
    *act_max = qmax;
  } else if (activation == kTfLiteActRelu6) {
 80af740:	2803      	cmp	r0, #3
 80af742:	d109      	bne.n	80af758 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x40>
    *act_min = std::max(qmin, quantize(0.0));
 80af744:	2100      	movs	r1, #0
 80af746:	4668      	mov	r0, sp
 80af748:	f7ff ffd8 	bl	80af6fc <_ZZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_ENKUlfE_clEf>
    *act_max = std::min(qmax, quantize(6.0));
 80af74c:	490e      	ldr	r1, [pc, #56]	; (80af788 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x70>)

  if (activation == kTfLiteActRelu) {
    *act_min = std::max(qmin, quantize(0.0));
    *act_max = qmax;
  } else if (activation == kTfLiteActRelu6) {
    *act_min = std::max(qmin, quantize(0.0));
 80af74e:	42b8      	cmp	r0, r7
 80af750:	bfac      	ite	ge
 80af752:	6030      	strge	r0, [r6, #0]
 80af754:	6037      	strlt	r7, [r6, #0]
 80af756:	e00b      	b.n	80af770 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x58>
    *act_max = std::min(qmax, quantize(6.0));
  } else if (activation == kTfLiteActRelu1) {
 80af758:	2802      	cmp	r0, #2
 80af75a:	d111      	bne.n	80af780 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x68>
    *act_min = std::max(qmin, quantize(-1.0));
 80af75c:	490b      	ldr	r1, [pc, #44]	; (80af78c <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x74>)
 80af75e:	4668      	mov	r0, sp
 80af760:	f7ff ffcc 	bl	80af6fc <_ZZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_ENKUlfE_clEf>
    *act_max = std::min(qmax, quantize(1.0));
 80af764:	f04f 517e 	mov.w	r1, #1065353216	; 0x3f800000
    *act_max = qmax;
  } else if (activation == kTfLiteActRelu6) {
    *act_min = std::max(qmin, quantize(0.0));
    *act_max = std::min(qmax, quantize(6.0));
  } else if (activation == kTfLiteActRelu1) {
    *act_min = std::max(qmin, quantize(-1.0));
 80af768:	42b8      	cmp	r0, r7
 80af76a:	bfac      	ite	ge
 80af76c:	6030      	strge	r0, [r6, #0]
 80af76e:	6037      	strlt	r7, [r6, #0]
    *act_max = std::min(qmax, quantize(1.0));
 80af770:	4668      	mov	r0, sp
 80af772:	f7ff ffc3 	bl	80af6fc <_ZZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_ENKUlfE_clEf>
 80af776:	4285      	cmp	r5, r0
 80af778:	bfd4      	ite	le
 80af77a:	6025      	strle	r5, [r4, #0]
 80af77c:	6020      	strgt	r0, [r4, #0]
 80af77e:	e001      	b.n	80af784 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x6c>
  } else {
    *act_min = qmin;
 80af780:	6031      	str	r1, [r6, #0]
    *act_max = qmax;
 80af782:	6025      	str	r5, [r4, #0]
  }
}
 80af784:	b003      	add	sp, #12
 80af786:	bdf0      	pop	{r4, r5, r6, r7, pc}
 80af788:	40c00000 	.word	0x40c00000
 80af78c:	bf800000 	.word	0xbf800000

080af790 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd>:

TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              TfLiteTensor* output,
                                              double* multiplier) {
 80af790:	b5f7      	push	{r0, r1, r2, r4, r5, r6, r7, lr}
 80af792:	4604      	mov	r4, r0
 80af794:	4608      	mov	r0, r1
  const double input_product_scale = input->params.scale * filter->params.scale;
 80af796:	68d1      	ldr	r1, [r2, #12]
 80af798:	68c0      	ldr	r0, [r0, #12]

TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              TfLiteTensor* output,
                                              double* multiplier) {
 80af79a:	461d      	mov	r5, r3
  const double input_product_scale = input->params.scale * filter->params.scale;
 80af79c:	f003 ff26 	bl	80b35ec <__aeabi_fmul>
 80af7a0:	f003 fa90 	bl	80b2cc4 <__aeabi_f2d>
  TF_LITE_ENSURE(context, input_product_scale >= 0);
 80af7a4:	2200      	movs	r2, #0
 80af7a6:	2300      	movs	r3, #0
TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              TfLiteTensor* output,
                                              double* multiplier) {
  const double input_product_scale = input->params.scale * filter->params.scale;
 80af7a8:	4606      	mov	r6, r0
 80af7aa:	460f      	mov	r7, r1
  TF_LITE_ENSURE(context, input_product_scale >= 0);
 80af7ac:	f003 fd64 	bl	80b3278 <__aeabi_dcmpge>
 80af7b0:	b948      	cbnz	r0, 80af7c6 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd+0x36>
 80af7b2:	4b0c      	ldr	r3, [pc, #48]	; (80af7e4 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd+0x54>)
 80af7b4:	4620      	mov	r0, r4
 80af7b6:	9300      	str	r3, [sp, #0]
 80af7b8:	6965      	ldr	r5, [r4, #20]
 80af7ba:	2376      	movs	r3, #118	; 0x76
 80af7bc:	4a0a      	ldr	r2, [pc, #40]	; (80af7e8 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd+0x58>)
 80af7be:	490b      	ldr	r1, [pc, #44]	; (80af7ec <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd+0x5c>)
 80af7c0:	47a8      	blx	r5
 80af7c2:	2001      	movs	r0, #1
 80af7c4:	e00c      	b.n	80af7e0 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd+0x50>
  *multiplier = input_product_scale / output->params.scale;
 80af7c6:	68e8      	ldr	r0, [r5, #12]
 80af7c8:	f003 fa7c 	bl	80b2cc4 <__aeabi_f2d>
 80af7cc:	460b      	mov	r3, r1
 80af7ce:	4602      	mov	r2, r0
 80af7d0:	4639      	mov	r1, r7
 80af7d2:	4630      	mov	r0, r6
 80af7d4:	f003 fbf4 	bl	80b2fc0 <__aeabi_ddiv>
 80af7d8:	9b08      	ldr	r3, [sp, #32]
 80af7da:	e9c3 0100 	strd	r0, r1, [r3]

  return kTfLiteOk;
 80af7de:	2000      	movs	r0, #0
}
 80af7e0:	b003      	add	sp, #12
 80af7e2:	bdf0      	pop	{r4, r5, r6, r7, pc}
 80af7e4:	080b748e 	.word	0x080b748e
 80af7e8:	080b73f4 	.word	0x080b73f4
 80af7ec:	080b5db0 	.word	0x080b5db0

080af7f0 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd>:
TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              const TfLiteTensor* bias,
                                              TfLiteTensor* output,
                                              double* multiplier) {
 80af7f0:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80af7f4:	460d      	mov	r5, r1
 80af7f6:	b087      	sub	sp, #28
 80af7f8:	4699      	mov	r9, r3
 80af7fa:	9b11      	ldr	r3, [sp, #68]	; 0x44
  const double input_product_scale = input->params.scale * filter->params.scale;
 80af7fc:	68d1      	ldr	r1, [r2, #12]
TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              const TfLiteTensor* bias,
                                              TfLiteTensor* output,
                                              double* multiplier) {
 80af7fe:	4604      	mov	r4, r0
  const double input_product_scale = input->params.scale * filter->params.scale;
 80af800:	68e8      	ldr	r0, [r5, #12]
TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              const TfLiteTensor* bias,
                                              TfLiteTensor* output,
                                              double* multiplier) {
 80af802:	4616      	mov	r6, r2
 80af804:	9303      	str	r3, [sp, #12]
  const double input_product_scale = input->params.scale * filter->params.scale;
 80af806:	f003 fef1 	bl	80b35ec <__aeabi_fmul>
 80af80a:	f003 fa5b 	bl	80b2cc4 <__aeabi_f2d>
TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              const TfLiteTensor* bias,
                                              TfLiteTensor* output,
                                              double* multiplier) {
 80af80e:	9f10      	ldr	r7, [sp, #64]	; 0x40
  const double input_product_scale = input->params.scale * filter->params.scale;
 80af810:	4682      	mov	sl, r0
 80af812:	468b      	mov	fp, r1
  // TODO(ahentz): The following conditions must be guaranteed by the training
  // pipeline.
  if (bias) {
 80af814:	f1b9 0f00 	cmp.w	r9, #0
 80af818:	d024      	beq.n	80af864 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0x74>
    const double bias_scale = bias->params.scale;
 80af81a:	f8d9 000c 	ldr.w	r0, [r9, #12]
 80af81e:	f003 fa51 	bl	80b2cc4 <__aeabi_f2d>
_GLIBCXX_BEGIN_NAMESPACE_VERSION

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR double
  abs(double __x)
  { return __builtin_fabs(__x); }
 80af822:	4602      	mov	r2, r0
 80af824:	460b      	mov	r3, r1
 80af826:	4680      	mov	r8, r0
 80af828:	4689      	mov	r9, r1
 80af82a:	4650      	mov	r0, sl
 80af82c:	4659      	mov	r1, fp
 80af82e:	f003 f8e9 	bl	80b2a04 <__aeabi_dsub>
 80af832:	f021 4300 	bic.w	r3, r1, #2147483648	; 0x80000000
 80af836:	9005      	str	r0, [sp, #20]
 80af838:	9304      	str	r3, [sp, #16]
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
 80af83a:	4642      	mov	r2, r8
 80af83c:	464b      	mov	r3, r9
 80af83e:	4650      	mov	r0, sl
 80af840:	4659      	mov	r1, fp
 80af842:	f003 fd23 	bl	80b328c <__aeabi_dcmpgt>
 80af846:	b108      	cbz	r0, 80af84c <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0x5c>
	return __b;
 80af848:	46c2      	mov	sl, r8
 80af84a:	46cb      	mov	fp, r9
    TF_LITE_ENSURE(context,
 80af84c:	a312      	add	r3, pc, #72	; (adr r3, 80af898 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0xa8>)
 80af84e:	e9d3 2300 	ldrd	r2, r3, [r3]
 80af852:	4650      	mov	r0, sl
 80af854:	4659      	mov	r1, fp
 80af856:	f003 fa89 	bl	80b2d6c <__aeabi_dmul>
 80af85a:	9a05      	ldr	r2, [sp, #20]
 80af85c:	9b04      	ldr	r3, [sp, #16]
 80af85e:	f003 fd0b 	bl	80b3278 <__aeabi_dcmpge>
 80af862:	b150      	cbz	r0, 80af87a <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0x8a>
                   std::abs(input_product_scale - bias_scale) <=
                       1e-6 * std::min(input_product_scale, bias_scale));
  }
  return GetQuantizedConvolutionMultipler(context, input, filter, output,
                                          multiplier);
 80af864:	9b03      	ldr	r3, [sp, #12]
 80af866:	4632      	mov	r2, r6
 80af868:	9310      	str	r3, [sp, #64]	; 0x40
 80af86a:	4629      	mov	r1, r5
 80af86c:	463b      	mov	r3, r7
 80af86e:	4620      	mov	r0, r4
}
 80af870:	b007      	add	sp, #28
 80af872:	e8bd 4ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
    TF_LITE_ENSURE(context,
                   std::abs(input_product_scale - bias_scale) <=
                       1e-6 * std::min(input_product_scale, bias_scale));
  }
  return GetQuantizedConvolutionMultipler(context, input, filter, output,
                                          multiplier);
 80af876:	f7ff bf8b 	b.w	80af790 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd>
  const double input_product_scale = input->params.scale * filter->params.scale;
  // TODO(ahentz): The following conditions must be guaranteed by the training
  // pipeline.
  if (bias) {
    const double bias_scale = bias->params.scale;
    TF_LITE_ENSURE(context,
 80af87a:	4b09      	ldr	r3, [pc, #36]	; (80af8a0 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0xb0>)
 80af87c:	4620      	mov	r0, r4
 80af87e:	9300      	str	r3, [sp, #0]
 80af880:	6965      	ldr	r5, [r4, #20]
 80af882:	236a      	movs	r3, #106	; 0x6a
 80af884:	4a07      	ldr	r2, [pc, #28]	; (80af8a4 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0xb4>)
 80af886:	4908      	ldr	r1, [pc, #32]	; (80af8a8 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0xb8>)
 80af888:	47a8      	blx	r5
                   std::abs(input_product_scale - bias_scale) <=
                       1e-6 * std::min(input_product_scale, bias_scale));
  }
  return GetQuantizedConvolutionMultipler(context, input, filter, output,
                                          multiplier);
}
 80af88a:	2001      	movs	r0, #1
 80af88c:	b007      	add	sp, #28
 80af88e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80af892:	bf00      	nop
 80af894:	f3af 8000 	nop.w
 80af898:	a0b5ed8d 	.word	0xa0b5ed8d
 80af89c:	3eb0c6f7 	.word	0x3eb0c6f7
 80af8a0:	080b74a7 	.word	0x080b74a7
 80af8a4:	080b73f4 	.word	0x080b73f4
 80af8a8:	080b5db0 	.word	0x080b5db0

080af8ac <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_>:

TfLiteStatus CalculateActivationRangeQuantized(TfLiteContext* context,
                                               TfLiteFusedActivation activation,
                                               TfLiteTensor* output,
                                               int32_t* act_min,
                                               int32_t* act_max) {
 80af8ac:	b573      	push	{r0, r1, r4, r5, r6, lr}
 80af8ae:	460d      	mov	r5, r1
  int32_t qmin = 0;
  int32_t qmax = 0;
  if (output->type == kTfLiteUInt8) {
 80af8b0:	7811      	ldrb	r1, [r2, #0]

TfLiteStatus CalculateActivationRangeQuantized(TfLiteContext* context,
                                               TfLiteFusedActivation activation,
                                               TfLiteTensor* output,
                                               int32_t* act_min,
                                               int32_t* act_max) {
 80af8b2:	4614      	mov	r4, r2
  int32_t qmin = 0;
  int32_t qmax = 0;
  if (output->type == kTfLiteUInt8) {
 80af8b4:	2903      	cmp	r1, #3
 80af8b6:	d00c      	beq.n	80af8d2 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x26>
    qmin = std::numeric_limits<uint8_t>::min();
    qmax = std::numeric_limits<uint8_t>::max();
  } else if (output->type == kTfLiteInt8) {
 80af8b8:	2909      	cmp	r1, #9
 80af8ba:	d00d      	beq.n	80af8d8 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x2c>
    qmin = std::numeric_limits<int8_t>::min();
    qmax = std::numeric_limits<int8_t>::max();
  } else if (output->type == kTfLiteInt16) {
 80af8bc:	2907      	cmp	r1, #7
 80af8be:	d00f      	beq.n	80af8e0 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x34>
    qmin = std::numeric_limits<int16_t>::min();
    qmax = std::numeric_limits<int16_t>::max();
  } else {
    TF_LITE_ENSURE(context, false);
 80af8c0:	4b0e      	ldr	r3, [pc, #56]	; (80af8fc <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x50>)
 80af8c2:	4a0f      	ldr	r2, [pc, #60]	; (80af900 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x54>)
 80af8c4:	9300      	str	r3, [sp, #0]
 80af8c6:	6944      	ldr	r4, [r0, #20]
 80af8c8:	23a9      	movs	r3, #169	; 0xa9
 80af8ca:	490e      	ldr	r1, [pc, #56]	; (80af904 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x58>)
 80af8cc:	47a0      	blx	r4
 80af8ce:	2001      	movs	r0, #1
 80af8d0:	e011      	b.n	80af8f6 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x4a>
                                               int32_t* act_max) {
  int32_t qmin = 0;
  int32_t qmax = 0;
  if (output->type == kTfLiteUInt8) {
    qmin = std::numeric_limits<uint8_t>::min();
    qmax = std::numeric_limits<uint8_t>::max();
 80af8d2:	22ff      	movs	r2, #255	; 0xff
                                               int32_t* act_min,
                                               int32_t* act_max) {
  int32_t qmin = 0;
  int32_t qmax = 0;
  if (output->type == kTfLiteUInt8) {
    qmin = std::numeric_limits<uint8_t>::min();
 80af8d4:	2100      	movs	r1, #0
 80af8d6:	e006      	b.n	80af8e6 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x3a>
    qmax = std::numeric_limits<uint8_t>::max();
  } else if (output->type == kTfLiteInt8) {
    qmin = std::numeric_limits<int8_t>::min();
    qmax = std::numeric_limits<int8_t>::max();
 80af8d8:	227f      	movs	r2, #127	; 0x7f
  int32_t qmax = 0;
  if (output->type == kTfLiteUInt8) {
    qmin = std::numeric_limits<uint8_t>::min();
    qmax = std::numeric_limits<uint8_t>::max();
  } else if (output->type == kTfLiteInt8) {
    qmin = std::numeric_limits<int8_t>::min();
 80af8da:	f06f 017f 	mvn.w	r1, #127	; 0x7f
 80af8de:	e002      	b.n	80af8e6 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x3a>
    qmax = std::numeric_limits<int8_t>::max();
  } else if (output->type == kTfLiteInt16) {
    qmin = std::numeric_limits<int16_t>::min();
    qmax = std::numeric_limits<int16_t>::max();
 80af8e0:	f647 72ff 	movw	r2, #32767	; 0x7fff
    qmax = std::numeric_limits<uint8_t>::max();
  } else if (output->type == kTfLiteInt8) {
    qmin = std::numeric_limits<int8_t>::min();
    qmax = std::numeric_limits<int8_t>::max();
  } else if (output->type == kTfLiteInt16) {
    qmin = std::numeric_limits<int16_t>::min();
 80af8e4:	4908      	ldr	r1, [pc, #32]	; (80af908 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x5c>)
  } else {
    TF_LITE_ENSURE(context, false);
  }

  CalculateActivationRangeQuantizedImpl(activation, qmin, qmax, output, act_min,
                                        act_max);
 80af8e6:	9806      	ldr	r0, [sp, #24]
 80af8e8:	9300      	str	r3, [sp, #0]
 80af8ea:	9001      	str	r0, [sp, #4]
 80af8ec:	4623      	mov	r3, r4
 80af8ee:	4628      	mov	r0, r5
 80af8f0:	f7ff ff12 	bl	80af718 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_>
  return kTfLiteOk;
 80af8f4:	2000      	movs	r0, #0
}
 80af8f6:	b002      	add	sp, #8
 80af8f8:	bd70      	pop	{r4, r5, r6, pc}
 80af8fa:	bf00      	nop
 80af8fc:	080b7506 	.word	0x080b7506
 80af900:	080b73f4 	.word	0x080b73f4
 80af904:	080b5db0 	.word	0x080b5db0
 80af908:	ffff8000 	.word	0xffff8000

080af90c <_ZN6tflite29CalculateActivationRangeUint8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>:

void CalculateActivationRangeUint8(TfLiteFusedActivation activation,
                                   TfLiteTensor* output, int32_t* act_min,
                                   int32_t* act_max) {
 80af90c:	b507      	push	{r0, r1, r2, lr}
  const int32_t qmin = std::numeric_limits<uint8_t>::min();
  const int32_t qmax = std::numeric_limits<uint8_t>::max();

  CalculateActivationRangeQuantizedImpl(activation, qmin, qmax, output, act_min,
                                        act_max);
 80af90e:	e88d 000c 	stmia.w	sp, {r2, r3}
 80af912:	460b      	mov	r3, r1
 80af914:	22ff      	movs	r2, #255	; 0xff
 80af916:	2100      	movs	r1, #0
 80af918:	f7ff fefe 	bl	80af718 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_>
}
 80af91c:	b003      	add	sp, #12
 80af91e:	f85d fb04 	ldr.w	pc, [sp], #4
	...

080af924 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_>:
TfLiteStatus PopulateConvolutionQuantizationParams(
    TfLiteContext* context, const TfLiteTensor* input,
    const TfLiteTensor* filter, const TfLiteTensor* bias, TfLiteTensor* output,
    const TfLiteFusedActivation& activation, int32_t* multiplier, int* shift,
    int32_t* output_activation_min, int32_t* output_activation_max,
    int32_t* per_channel_multiplier, int* per_channel_shift) {
 80af924:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  TF_LITE_ENSURE_EQ(context, input->quantization.type,
 80af928:	f891 8030 	ldrb.w	r8, [r1, #48]	; 0x30
TfLiteStatus PopulateConvolutionQuantizationParams(
    TfLiteContext* context, const TfLiteTensor* input,
    const TfLiteTensor* filter, const TfLiteTensor* bias, TfLiteTensor* output,
    const TfLiteFusedActivation& activation, int32_t* multiplier, int* shift,
    int32_t* output_activation_min, int32_t* output_activation_max,
    int32_t* per_channel_multiplier, int* per_channel_shift) {
 80af92c:	b08d      	sub	sp, #52	; 0x34
  TF_LITE_ENSURE_EQ(context, input->quantization.type,
 80af92e:	f1b8 0f01 	cmp.w	r8, #1
TfLiteStatus PopulateConvolutionQuantizationParams(
    TfLiteContext* context, const TfLiteTensor* input,
    const TfLiteTensor* filter, const TfLiteTensor* bias, TfLiteTensor* output,
    const TfLiteFusedActivation& activation, int32_t* multiplier, int* shift,
    int32_t* output_activation_min, int32_t* output_activation_max,
    int32_t* per_channel_multiplier, int* per_channel_shift) {
 80af932:	4604      	mov	r4, r0
 80af934:	460e      	mov	r6, r1
 80af936:	4617      	mov	r7, r2
 80af938:	469b      	mov	fp, r3
  TF_LITE_ENSURE_EQ(context, input->quantization.type,
 80af93a:	d00a      	beq.n	80af952 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x2e>
 80af93c:	4b60      	ldr	r3, [pc, #384]	; (80afac0 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x19c>)
 80af93e:	2501      	movs	r5, #1
 80af940:	9301      	str	r3, [sp, #4]
 80af942:	4b60      	ldr	r3, [pc, #384]	; (80afac4 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1a0>)
 80af944:	9503      	str	r5, [sp, #12]
 80af946:	9300      	str	r3, [sp, #0]
 80af948:	f8cd 8008 	str.w	r8, [sp, #8]
 80af94c:	6944      	ldr	r4, [r0, #20]
 80af94e:	2321      	movs	r3, #33	; 0x21
 80af950:	e033      	b.n	80af9ba <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x96>
                    kTfLiteAffineQuantization);
  TF_LITE_ENSURE_EQ(context, filter->quantization.type,
 80af952:	f892 5030 	ldrb.w	r5, [r2, #48]	; 0x30
 80af956:	2d01      	cmp	r5, #1
 80af958:	d00d      	beq.n	80af976 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x52>
 80af95a:	4b59      	ldr	r3, [pc, #356]	; (80afac0 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x19c>)
 80af95c:	9502      	str	r5, [sp, #8]
 80af95e:	9301      	str	r3, [sp, #4]
 80af960:	4b59      	ldr	r3, [pc, #356]	; (80afac8 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1a4>)
 80af962:	f8cd 800c 	str.w	r8, [sp, #12]
 80af966:	9300      	str	r3, [sp, #0]
 80af968:	6944      	ldr	r4, [r0, #20]
 80af96a:	2323      	movs	r3, #35	; 0x23
 80af96c:	4a57      	ldr	r2, [pc, #348]	; (80afacc <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1a8>)
 80af96e:	4958      	ldr	r1, [pc, #352]	; (80afad0 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1ac>)
 80af970:	47a0      	blx	r4
 80af972:	4645      	mov	r5, r8
 80af974:	e0a0      	b.n	80afab8 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x194>
  // TF_LITE_ENSURE_EQ(context, bias->quantization.type,
  // kTfLiteAffineQuantization);

  // Check data type.
  const auto* affine_quantization =
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);
 80af976:	6b51      	ldr	r1, [r2, #52]	; 0x34
  TF_LITE_ENSURE(context, affine_quantization);
 80af978:	b921      	cbnz	r1, 80af984 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x60>
 80af97a:	4b56      	ldr	r3, [pc, #344]	; (80afad4 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1b0>)
 80af97c:	9300      	str	r3, [sp, #0]
 80af97e:	6944      	ldr	r4, [r0, #20]
 80af980:	232d      	movs	r3, #45	; 0x2d
 80af982:	e005      	b.n	80af990 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x6c>
  TF_LITE_ENSURE(context, affine_quantization->scale);
 80af984:	680b      	ldr	r3, [r1, #0]
 80af986:	b93b      	cbnz	r3, 80af998 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x74>
 80af988:	4b53      	ldr	r3, [pc, #332]	; (80afad8 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1b4>)
 80af98a:	9300      	str	r3, [sp, #0]
 80af98c:	6944      	ldr	r4, [r0, #20]
 80af98e:	232e      	movs	r3, #46	; 0x2e
 80af990:	4a4e      	ldr	r2, [pc, #312]	; (80afacc <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1a8>)
 80af992:	4952      	ldr	r1, [pc, #328]	; (80afadc <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1b8>)
 80af994:	47a0      	blx	r4
 80af996:	e08f      	b.n	80afab8 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x194>
  const bool is_per_channel = affine_quantization->scale->size > 1;
 80af998:	f8d3 a000 	ldr.w	sl, [r3]
  if (is_per_channel) {
 80af99c:	f1ba 0f01 	cmp.w	sl, #1
 80af9a0:	dd2f      	ble.n	80afa02 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0xde>
    //  Currently only Int8 is supported for per channel quantization.
    TF_LITE_ENSURE_EQ(context, input->type, kTfLiteInt8);
 80af9a2:	7832      	ldrb	r2, [r6, #0]
 80af9a4:	2a09      	cmp	r2, #9
 80af9a6:	d00c      	beq.n	80af9c2 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x9e>
 80af9a8:	2309      	movs	r3, #9
 80af9aa:	9303      	str	r3, [sp, #12]
 80af9ac:	4b4c      	ldr	r3, [pc, #304]	; (80afae0 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1bc>)
 80af9ae:	9202      	str	r2, [sp, #8]
 80af9b0:	9301      	str	r3, [sp, #4]
 80af9b2:	4b4c      	ldr	r3, [pc, #304]	; (80afae4 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1c0>)
 80af9b4:	9300      	str	r3, [sp, #0]
 80af9b6:	6944      	ldr	r4, [r0, #20]
 80af9b8:	2332      	movs	r3, #50	; 0x32
 80af9ba:	4a44      	ldr	r2, [pc, #272]	; (80afacc <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1a8>)
 80af9bc:	4944      	ldr	r1, [pc, #272]	; (80afad0 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1ac>)
 80af9be:	47a0      	blx	r4
 80af9c0:	e07a      	b.n	80afab8 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x194>
    TF_LITE_ENSURE_EQ(context, filter->type, kTfLiteInt8);
 80af9c2:	f897 e000 	ldrb.w	lr, [r7]
 80af9c6:	f1be 0f09 	cmp.w	lr, #9
 80af9ca:	d009      	beq.n	80af9e0 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0xbc>
 80af9cc:	4b44      	ldr	r3, [pc, #272]	; (80afae0 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1bc>)
 80af9ce:	9203      	str	r2, [sp, #12]
 80af9d0:	9301      	str	r3, [sp, #4]
 80af9d2:	4b45      	ldr	r3, [pc, #276]	; (80afae8 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1c4>)
 80af9d4:	f8cd e008 	str.w	lr, [sp, #8]
 80af9d8:	9300      	str	r3, [sp, #0]
 80af9da:	6944      	ldr	r4, [r0, #20]
 80af9dc:	2333      	movs	r3, #51	; 0x33
 80af9de:	e7ec      	b.n	80af9ba <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x96>
    TF_LITE_ENSURE_EQ(
 80af9e0:	68ba      	ldr	r2, [r7, #8]
 80af9e2:	6889      	ldr	r1, [r1, #8]
 80af9e4:	eb02 0281 	add.w	r2, r2, r1, lsl #2
 80af9e8:	6852      	ldr	r2, [r2, #4]
 80af9ea:	4592      	cmp	sl, r2
 80af9ec:	d009      	beq.n	80afa02 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0xde>
 80af9ee:	4b3f      	ldr	r3, [pc, #252]	; (80afaec <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1c8>)
 80af9f0:	9203      	str	r2, [sp, #12]
 80af9f2:	9301      	str	r3, [sp, #4]
 80af9f4:	4b3e      	ldr	r3, [pc, #248]	; (80afaf0 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1cc>)
 80af9f6:	f8cd a008 	str.w	sl, [sp, #8]
 80af9fa:	9300      	str	r3, [sp, #0]
 80af9fc:	6944      	ldr	r4, [r0, #20]
 80af9fe:	2336      	movs	r3, #54	; 0x36
 80afa00:	e7db      	b.n	80af9ba <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x96>
        filter->dims->data[affine_quantization->quantized_dimension]);
  }

  // Populate multiplier and shift using affine quantization.
  const int num_channels = affine_quantization->scale->size;
  const float input_scale = input->params.scale;
 80afa02:	68f2      	ldr	r2, [r6, #12]
  const float output_scale = output->params.scale;
  const float* filter_scales = affine_quantization->scale->data;
 80afa04:	3304      	adds	r3, #4
        filter->dims->data[affine_quantization->quantized_dimension]);
  }

  // Populate multiplier and shift using affine quantization.
  const int num_channels = affine_quantization->scale->size;
  const float input_scale = input->params.scale;
 80afa06:	9205      	str	r2, [sp, #20]
  const float output_scale = output->params.scale;
 80afa08:	9a16      	ldr	r2, [sp, #88]	; 0x58
  const float* filter_scales = affine_quantization->scale->data;
 80afa0a:	9307      	str	r3, [sp, #28]
  }

  // Populate multiplier and shift using affine quantization.
  const int num_channels = affine_quantization->scale->size;
  const float input_scale = input->params.scale;
  const float output_scale = output->params.scale;
 80afa0c:	68d2      	ldr	r2, [r2, #12]
  const float* filter_scales = affine_quantization->scale->data;
  for (int i = 0; i < num_channels; ++i) {
 80afa0e:	2500      	movs	r5, #0
  }

  // Populate multiplier and shift using affine quantization.
  const int num_channels = affine_quantization->scale->size;
  const float input_scale = input->params.scale;
  const float output_scale = output->params.scale;
 80afa10:	9206      	str	r2, [sp, #24]
  const float* filter_scales = affine_quantization->scale->data;
  for (int i = 0; i < num_channels; ++i) {
 80afa12:	4555      	cmp	r5, sl
 80afa14:	da28      	bge.n	80afa68 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x144>
    const double effective_output_scale = static_cast<double>(input_scale) *
                                          filter_scale /
                                          static_cast<double>(output_scale);
    int32_t significand;
    int shift;
    QuantizeMultiplier(effective_output_scale, &significand, &shift);
 80afa16:	9b07      	ldr	r3, [sp, #28]
 80afa18:	f853 0025 	ldr.w	r0, [r3, r5, lsl #2]
 80afa1c:	f003 f952 	bl	80b2cc4 <__aeabi_f2d>
 80afa20:	4680      	mov	r8, r0
 80afa22:	9805      	ldr	r0, [sp, #20]
 80afa24:	4689      	mov	r9, r1
 80afa26:	f003 f94d 	bl	80b2cc4 <__aeabi_f2d>
 80afa2a:	4602      	mov	r2, r0
 80afa2c:	460b      	mov	r3, r1
 80afa2e:	4640      	mov	r0, r8
 80afa30:	4649      	mov	r1, r9
 80afa32:	f003 f99b 	bl	80b2d6c <__aeabi_dmul>
 80afa36:	4680      	mov	r8, r0
 80afa38:	9806      	ldr	r0, [sp, #24]
 80afa3a:	4689      	mov	r9, r1
 80afa3c:	f003 f942 	bl	80b2cc4 <__aeabi_f2d>
 80afa40:	4602      	mov	r2, r0
 80afa42:	460b      	mov	r3, r1
 80afa44:	4640      	mov	r0, r8
 80afa46:	4649      	mov	r1, r9
 80afa48:	f003 faba 	bl	80b2fc0 <__aeabi_ddiv>
 80afa4c:	ab0a      	add	r3, sp, #40	; 0x28
 80afa4e:	aa09      	add	r2, sp, #36	; 0x24
 80afa50:	f000 f866 	bl	80afb20 <_ZN6tflite18QuantizeMultiplierEdPlPi>
    per_channel_multiplier[i] = significand;
 80afa54:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80afa56:	9a1c      	ldr	r2, [sp, #112]	; 0x70
 80afa58:	f842 3025 	str.w	r3, [r2, r5, lsl #2]
    per_channel_shift[i] = shift;
 80afa5c:	9b0a      	ldr	r3, [sp, #40]	; 0x28
 80afa5e:	9a1d      	ldr	r2, [sp, #116]	; 0x74
 80afa60:	f842 3025 	str.w	r3, [r2, r5, lsl #2]
  // Populate multiplier and shift using affine quantization.
  const int num_channels = affine_quantization->scale->size;
  const float input_scale = input->params.scale;
  const float output_scale = output->params.scale;
  const float* filter_scales = affine_quantization->scale->data;
  for (int i = 0; i < num_channels; ++i) {
 80afa64:	3501      	adds	r5, #1
 80afa66:	e7d4      	b.n	80afa12 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0xee>
  }

  // Populate scalar quantization parameters.
  // This check on legacy quantization parameters is kept only for backward
  // compatibility.
  if (input->type == kTfLiteUInt8) {
 80afa68:	7833      	ldrb	r3, [r6, #0]
 80afa6a:	2b03      	cmp	r3, #3
 80afa6c:	d123      	bne.n	80afab6 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x192>
    // Check bias scale == input scale * filter scale.
    double real_multiplier = 0.0;
 80afa6e:	ab0c      	add	r3, sp, #48	; 0x30
 80afa70:	2000      	movs	r0, #0
 80afa72:	2100      	movs	r1, #0
 80afa74:	e963 0102 	strd	r0, r1, [r3, #-8]!
    TF_LITE_ENSURE_STATUS(GetQuantizedConvolutionMultipler(
 80afa78:	9301      	str	r3, [sp, #4]
 80afa7a:	9b16      	ldr	r3, [sp, #88]	; 0x58
 80afa7c:	463a      	mov	r2, r7
 80afa7e:	9300      	str	r3, [sp, #0]
 80afa80:	4631      	mov	r1, r6
 80afa82:	465b      	mov	r3, fp
 80afa84:	4620      	mov	r0, r4
 80afa86:	f7ff feb3 	bl	80af7f0 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd>
 80afa8a:	4605      	mov	r5, r0
 80afa8c:	b108      	cbz	r0, 80afa92 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x16e>
 80afa8e:	2501      	movs	r5, #1
 80afa90:	e012      	b.n	80afab8 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x194>
        context, input, filter, bias, output, &real_multiplier));
    int exponent;

    // Populate quantization parameteters with multiplier and shift.
    QuantizeMultiplier(real_multiplier, multiplier, &exponent);
 80afa92:	e9dd 010a 	ldrd	r0, r1, [sp, #40]	; 0x28
 80afa96:	ab09      	add	r3, sp, #36	; 0x24
 80afa98:	9a18      	ldr	r2, [sp, #96]	; 0x60
 80afa9a:	f000 f841 	bl	80afb20 <_ZN6tflite18QuantizeMultiplierEdPlPi>
    *shift = -exponent;
 80afa9e:	9b09      	ldr	r3, [sp, #36]	; 0x24
 80afaa0:	9a19      	ldr	r2, [sp, #100]	; 0x64
 80afaa2:	425b      	negs	r3, r3
    CalculateActivationRangeUint8(activation, output, output_activation_min,
                                  output_activation_max);
 80afaa4:	9817      	ldr	r0, [sp, #92]	; 0x5c
        context, input, filter, bias, output, &real_multiplier));
    int exponent;

    // Populate quantization parameteters with multiplier and shift.
    QuantizeMultiplier(real_multiplier, multiplier, &exponent);
    *shift = -exponent;
 80afaa6:	6013      	str	r3, [r2, #0]
    CalculateActivationRangeUint8(activation, output, output_activation_min,
                                  output_activation_max);
 80afaa8:	9916      	ldr	r1, [sp, #88]	; 0x58
 80afaaa:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
 80afaac:	9a1a      	ldr	r2, [sp, #104]	; 0x68
 80afaae:	7800      	ldrb	r0, [r0, #0]
 80afab0:	f7ff ff2c 	bl	80af90c <_ZN6tflite29CalculateActivationRangeUint8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>
 80afab4:	e000      	b.n	80afab8 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x194>
  }
  return kTfLiteOk;
 80afab6:	2500      	movs	r5, #0
}
 80afab8:	4628      	mov	r0, r5
 80afaba:	b00d      	add	sp, #52	; 0x34
 80afabc:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80afac0:	080b5dff 	.word	0x080b5dff
 80afac4:	080b750c 	.word	0x080b750c
 80afac8:	080b5e19 	.word	0x080b5e19
 80afacc:	080b73f4 	.word	0x080b73f4
 80afad0:	080b5be0 	.word	0x080b5be0
 80afad4:	080b5e33 	.word	0x080b5e33
 80afad8:	080b5e47 	.word	0x080b5e47
 80afadc:	080b5db0 	.word	0x080b5db0
 80afae0:	080b6ff7 	.word	0x080b6ff7
 80afae4:	080b5c1b 	.word	0x080b5c1b
 80afae8:	080b7525 	.word	0x080b7525
 80afaec:	080b7532 	.word	0x080b7532
 80afaf0:	080b756f 	.word	0x080b756f

080afaf4 <_ZN6tflite28CalculateActivationRangeInt8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>:
                                        act_max);
}

void CalculateActivationRangeInt8(TfLiteFusedActivation activation,
                                  TfLiteTensor* output, int32_t* act_min,
                                  int32_t* act_max) {
 80afaf4:	b507      	push	{r0, r1, r2, lr}
  const int32_t qmin = std::numeric_limits<int8_t>::min();
  const int32_t qmax = std::numeric_limits<int8_t>::max();

  CalculateActivationRangeQuantizedImpl(activation, qmin, qmax, output, act_min,
                                        act_max);
 80afaf6:	e88d 000c 	stmia.w	sp, {r2, r3}
 80afafa:	460b      	mov	r3, r1
 80afafc:	227f      	movs	r2, #127	; 0x7f
 80afafe:	f06f 017f 	mvn.w	r1, #127	; 0x7f
 80afb02:	f7ff fe09 	bl	80af718 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_>
}
 80afb06:	b003      	add	sp, #12
 80afb08:	f85d fb04 	ldr.w	pc, [sp], #4

080afb0c <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>:

bool HaveSameShapes(const TfLiteTensor* input1, const TfLiteTensor* input2) {
 80afb0c:	b508      	push	{r3, lr}
  return TfLiteIntArrayEqual(input1->dims, input2->dims);
 80afb0e:	6889      	ldr	r1, [r1, #8]
 80afb10:	6880      	ldr	r0, [r0, #8]
 80afb12:	f7f0 faf5 	bl	80a0100 <TfLiteIntArrayEqual>
}
 80afb16:	3000      	adds	r0, #0
 80afb18:	bf18      	it	ne
 80afb1a:	2001      	movne	r0, #1
 80afb1c:	bd08      	pop	{r3, pc}
	...

080afb20 <_ZN6tflite18QuantizeMultiplierEdPlPi>:
constexpr uint32_t kFractionRoundingMask = 0x003fffff;
constexpr uint32_t kFractionRoundingThreshold = 0x00200000;
}  // namespace

void QuantizeMultiplier(double double_multiplier, int32_t* quantized_multiplier,
                        int* shift) {
 80afb20:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
 80afb22:	4615      	mov	r5, r2
 80afb24:	461c      	mov	r4, r3
  if (double_multiplier == 0.) {
 80afb26:	2200      	movs	r2, #0
 80afb28:	2300      	movs	r3, #0
constexpr uint32_t kFractionRoundingMask = 0x003fffff;
constexpr uint32_t kFractionRoundingThreshold = 0x00200000;
}  // namespace

void QuantizeMultiplier(double double_multiplier, int32_t* quantized_multiplier,
                        int* shift) {
 80afb2a:	4606      	mov	r6, r0
 80afb2c:	460f      	mov	r7, r1
  if (double_multiplier == 0.) {
 80afb2e:	f003 fb85 	bl	80b323c <__aeabi_dcmpeq>
 80afb32:	b118      	cbz	r0, 80afb3c <_ZN6tflite18QuantizeMultiplierEdPlPi+0x1c>
    *quantized_multiplier = 0;
 80afb34:	2300      	movs	r3, #0
 80afb36:	602b      	str	r3, [r5, #0]
    *shift = 0;
 80afb38:	6023      	str	r3, [r4, #0]
 80afb3a:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
  // example on microcontrollers) then use an alternative implementation
  // that only requires integer and bitwise operations. To enable this, you
  // need to set the define during the build process for your platform.
  int64_t q_fixed = IntegerFrExp(double_multiplier, shift);
#else   // TFLITE_EMULATE_FLOAT
  const double q = std::frexp(double_multiplier, shift);
 80afb3c:	4622      	mov	r2, r4
 80afb3e:	4630      	mov	r0, r6
 80afb40:	4639      	mov	r1, r7
 80afb42:	f001 f92d 	bl	80b0da0 <frexp>
 80afb46:	2200      	movs	r2, #0
 80afb48:	4b10      	ldr	r3, [pc, #64]	; (80afb8c <_ZN6tflite18QuantizeMultiplierEdPlPi+0x6c>)
 80afb4a:	f003 f90f 	bl	80b2d6c <__aeabi_dmul>
 80afb4e:	f001 f95b 	bl	80b0e08 <round>
  auto q_fixed = static_cast<int64_t>(TfLiteRound(q * (1ll << 31)));
 80afb52:	f003 ff4d 	bl	80b39f0 <__aeabi_d2lz>
#endif  // TFLITE_EMULATE_FLOAT
  TFLITE_CHECK(q_fixed <= (1ll << 31));
 80afb56:	f04f 4200 	mov.w	r2, #2147483648	; 0x80000000
 80afb5a:	2300      	movs	r3, #0
 80afb5c:	4282      	cmp	r2, r0
 80afb5e:	418b      	sbcs	r3, r1
 80afb60:	da01      	bge.n	80afb66 <_ZN6tflite18QuantizeMultiplierEdPlPi+0x46>
 80afb62:	f000 fa65 	bl	80b0030 <abort>
  if (q_fixed == (1ll << 31)) {
 80afb66:	2900      	cmp	r1, #0
 80afb68:	bf01      	itttt	eq
 80afb6a:	f1b0 4f00 	cmpeq.w	r0, #2147483648	; 0x80000000
    q_fixed /= 2;
    ++*shift;
 80afb6e:	6823      	ldreq	r3, [r4, #0]
  const double q = std::frexp(double_multiplier, shift);
  auto q_fixed = static_cast<int64_t>(TfLiteRound(q * (1ll << 31)));
#endif  // TFLITE_EMULATE_FLOAT
  TFLITE_CHECK(q_fixed <= (1ll << 31));
  if (q_fixed == (1ll << 31)) {
    q_fixed /= 2;
 80afb70:	f04f 4080 	moveq.w	r0, #1073741824	; 0x40000000
    ++*shift;
 80afb74:	3301      	addeq	r3, #1
 80afb76:	bf08      	it	eq
 80afb78:	6023      	streq	r3, [r4, #0]
  // that we're effectively flushing tiny double_multiplier's to zero.
  // We could conceivably handle values in the range (roughly) [32, 63]
  // as 'denormals' i.e. (shift==0, q_fixed < 2^30). In that point of view
  // the present handling is just doing 'flush denormals to zero'. We could
  // reconsider and actually generate nonzero denormals if a need arises.
  if (*shift < -31) {
 80afb7a:	6823      	ldr	r3, [r4, #0]
 80afb7c:	331f      	adds	r3, #31
    *shift = 0;
 80afb7e:	bfbe      	ittt	lt
 80afb80:	2300      	movlt	r3, #0
    q_fixed = 0;
 80afb82:	2000      	movlt	r0, #0
  // We could conceivably handle values in the range (roughly) [32, 63]
  // as 'denormals' i.e. (shift==0, q_fixed < 2^30). In that point of view
  // the present handling is just doing 'flush denormals to zero'. We could
  // reconsider and actually generate nonzero denormals if a need arises.
  if (*shift < -31) {
    *shift = 0;
 80afb84:	6023      	strlt	r3, [r4, #0]
    q_fixed = 0;
  }
  *quantized_multiplier = static_cast<int32_t>(q_fixed);
 80afb86:	6028      	str	r0, [r5, #0]
 80afb88:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
 80afb8a:	bf00      	nop
 80afb8c:	41e00000 	.word	0x41e00000

080afb90 <_ZN6tflite32QuantizeMultiplierGreaterThanOneEdPlPi>:
}

void QuantizeMultiplierGreaterThanOne(double double_multiplier,
                                      int32_t* quantized_multiplier,
                                      int* left_shift) {
 80afb90:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
 80afb92:	4615      	mov	r5, r2
 80afb94:	461c      	mov	r4, r3
  TFLITE_CHECK_GT(double_multiplier, 1.);
 80afb96:	2200      	movs	r2, #0
 80afb98:	4b08      	ldr	r3, [pc, #32]	; (80afbbc <_ZN6tflite32QuantizeMultiplierGreaterThanOneEdPlPi+0x2c>)
  *quantized_multiplier = static_cast<int32_t>(q_fixed);
}

void QuantizeMultiplierGreaterThanOne(double double_multiplier,
                                      int32_t* quantized_multiplier,
                                      int* left_shift) {
 80afb9a:	4606      	mov	r6, r0
 80afb9c:	460f      	mov	r7, r1
  TFLITE_CHECK_GT(double_multiplier, 1.);
 80afb9e:	f003 fb75 	bl	80b328c <__aeabi_dcmpgt>
 80afba2:	b908      	cbnz	r0, 80afba8 <_ZN6tflite32QuantizeMultiplierGreaterThanOneEdPlPi+0x18>
 80afba4:	f000 fa44 	bl	80b0030 <abort>
  QuantizeMultiplier(double_multiplier, quantized_multiplier, left_shift);
 80afba8:	4623      	mov	r3, r4
 80afbaa:	462a      	mov	r2, r5
 80afbac:	4630      	mov	r0, r6
 80afbae:	4639      	mov	r1, r7
 80afbb0:	f7ff ffb6 	bl	80afb20 <_ZN6tflite18QuantizeMultiplierEdPlPi>
  TFLITE_CHECK_GE(*left_shift, 0);
 80afbb4:	6823      	ldr	r3, [r4, #0]
 80afbb6:	2b00      	cmp	r3, #0
 80afbb8:	dbf4      	blt.n	80afba4 <_ZN6tflite32QuantizeMultiplierGreaterThanOneEdPlPi+0x14>
}
 80afbba:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
 80afbbc:	3ff00000 	.word	0x3ff00000

080afbc0 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>:

void QuantizeMultiplierSmallerThanOneExp(double double_multiplier,
                                         int32_t* quantized_multiplier,
                                         int* left_shift) {
 80afbc0:	b5f7      	push	{r0, r1, r2, r4, r5, r6, r7, lr}
 80afbc2:	4617      	mov	r7, r2
 80afbc4:	461e      	mov	r6, r3
  TFLITE_CHECK_LT(double_multiplier, 1.);
 80afbc6:	2200      	movs	r2, #0
 80afbc8:	4b0d      	ldr	r3, [pc, #52]	; (80afc00 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi+0x40>)
  TFLITE_CHECK_GE(*left_shift, 0);
}

void QuantizeMultiplierSmallerThanOneExp(double double_multiplier,
                                         int32_t* quantized_multiplier,
                                         int* left_shift) {
 80afbca:	4604      	mov	r4, r0
 80afbcc:	460d      	mov	r5, r1
  TFLITE_CHECK_LT(double_multiplier, 1.);
 80afbce:	f003 fb3f 	bl	80b3250 <__aeabi_dcmplt>
 80afbd2:	b908      	cbnz	r0, 80afbd8 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi+0x18>
 80afbd4:	f000 fa2c 	bl	80b0030 <abort>
  TFLITE_CHECK_GT(double_multiplier, 0.);
 80afbd8:	2200      	movs	r2, #0
 80afbda:	2300      	movs	r3, #0
 80afbdc:	4620      	mov	r0, r4
 80afbde:	4629      	mov	r1, r5
 80afbe0:	f003 fb54 	bl	80b328c <__aeabi_dcmpgt>
 80afbe4:	2800      	cmp	r0, #0
 80afbe6:	d0f5      	beq.n	80afbd4 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi+0x14>
  int shift;
  QuantizeMultiplier(double_multiplier, quantized_multiplier, &shift);
 80afbe8:	ab01      	add	r3, sp, #4
 80afbea:	463a      	mov	r2, r7
 80afbec:	4620      	mov	r0, r4
 80afbee:	4629      	mov	r1, r5
 80afbf0:	f7ff ff96 	bl	80afb20 <_ZN6tflite18QuantizeMultiplierEdPlPi>
  TFLITE_CHECK_LE(shift, 0);
 80afbf4:	9b01      	ldr	r3, [sp, #4]
 80afbf6:	2b00      	cmp	r3, #0
 80afbf8:	dcec      	bgt.n	80afbd4 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi+0x14>
  *left_shift = shift;
 80afbfa:	6033      	str	r3, [r6, #0]
}
 80afbfc:	b003      	add	sp, #12
 80afbfe:	bdf0      	pop	{r4, r5, r6, r7, pc}
 80afc00:	3ff00000 	.word	0x3ff00000
 80afc04:	00000000 	.word	0x00000000

080afc08 <_ZN6tflite24PreprocessSoftmaxScalingEddiPlPi>:
  }
}

void PreprocessSoftmaxScaling(double beta, double input_scale,
                              int input_integer_bits,
                              int32_t* quantized_multiplier, int* left_shift) {
 80afc08:	e92d 4ff1 	stmdb	sp!, {r0, r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80afc0c:	4606      	mov	r6, r0
  if (IntegerDoubleCompare(input_beta_real_multiplier, (1ll << 31) - 1.0) > 0) {
    input_beta_real_multiplier = (1ll << 31) - 1.0;
  }
#else   // TFLITE_EMULATE_FLOAT
  const double input_beta_real_multiplier = std::min(
      beta * input_scale * (1 << (31 - input_integer_bits)), (1ll << 31) - 1.0);
 80afc0e:	a80a      	add	r0, sp, #40	; 0x28
  }
}

void PreprocessSoftmaxScaling(double beta, double input_scale,
                              int input_integer_bits,
                              int32_t* quantized_multiplier, int* left_shift) {
 80afc10:	4699      	mov	r9, r3
  if (IntegerDoubleCompare(input_beta_real_multiplier, (1ll << 31) - 1.0) > 0) {
    input_beta_real_multiplier = (1ll << 31) - 1.0;
  }
#else   // TFLITE_EMULATE_FLOAT
  const double input_beta_real_multiplier = std::min(
      beta * input_scale * (1 << (31 - input_integer_bits)), (1ll << 31) - 1.0);
 80afc12:	e890 0c01 	ldmia.w	r0, {r0, sl, fp}
 80afc16:	2301      	movs	r3, #1
 80afc18:	f1c0 001f 	rsb	r0, r0, #31
 80afc1c:	fa03 f000 	lsl.w	r0, r3, r0
  }
}

void PreprocessSoftmaxScaling(double beta, double input_scale,
                              int input_integer_bits,
                              int32_t* quantized_multiplier, int* left_shift) {
 80afc20:	460f      	mov	r7, r1
 80afc22:	4690      	mov	r8, r2
  if (IntegerDoubleCompare(input_beta_real_multiplier, (1ll << 31) - 1.0) > 0) {
    input_beta_real_multiplier = (1ll << 31) - 1.0;
  }
#else   // TFLITE_EMULATE_FLOAT
  const double input_beta_real_multiplier = std::min(
      beta * input_scale * (1 << (31 - input_integer_bits)), (1ll << 31) - 1.0);
 80afc24:	f003 f83c 	bl	80b2ca0 <__aeabi_i2d>
 80afc28:	4642      	mov	r2, r8
 80afc2a:	4604      	mov	r4, r0
 80afc2c:	460d      	mov	r5, r1
 80afc2e:	464b      	mov	r3, r9
 80afc30:	4630      	mov	r0, r6
 80afc32:	4639      	mov	r1, r7
 80afc34:	f003 f89a 	bl	80b2d6c <__aeabi_dmul>
 80afc38:	4602      	mov	r2, r0
 80afc3a:	460b      	mov	r3, r1
 80afc3c:	4620      	mov	r0, r4
 80afc3e:	4629      	mov	r1, r5
 80afc40:	f003 f894 	bl	80b2d6c <__aeabi_dmul>
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
 80afc44:	a30a      	add	r3, pc, #40	; (adr r3, 80afc70 <_ZN6tflite24PreprocessSoftmaxScalingEddiPlPi+0x68>)
 80afc46:	e9d3 2300 	ldrd	r2, r3, [r3]
 80afc4a:	4604      	mov	r4, r0
 80afc4c:	460d      	mov	r5, r1
 80afc4e:	f003 fb1d 	bl	80b328c <__aeabi_dcmpgt>
 80afc52:	b110      	cbz	r0, 80afc5a <_ZN6tflite24PreprocessSoftmaxScalingEddiPlPi+0x52>
	return __b;
 80afc54:	a506      	add	r5, pc, #24	; (adr r5, 80afc70 <_ZN6tflite24PreprocessSoftmaxScalingEddiPlPi+0x68>)
 80afc56:	e9d5 4500 	ldrd	r4, r5, [r5]
#endif  // TFLITE_EMULATE_FLOAT

  QuantizeMultiplierGreaterThanOne(input_beta_real_multiplier,
                                   quantized_multiplier, left_shift);
 80afc5a:	465b      	mov	r3, fp
 80afc5c:	4652      	mov	r2, sl
 80afc5e:	4620      	mov	r0, r4
 80afc60:	4629      	mov	r1, r5
}
 80afc62:	b001      	add	sp, #4
 80afc64:	e8bd 4ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  const double input_beta_real_multiplier = std::min(
      beta * input_scale * (1 << (31 - input_integer_bits)), (1ll << 31) - 1.0);
#endif  // TFLITE_EMULATE_FLOAT

  QuantizeMultiplierGreaterThanOne(input_beta_real_multiplier,
                                   quantized_multiplier, left_shift);
 80afc68:	f7ff bf92 	b.w	80afb90 <_ZN6tflite32QuantizeMultiplierGreaterThanOneEdPlPi>
 80afc6c:	f3af 8000 	nop.w
 80afc70:	ffc00000 	.word	0xffc00000
 80afc74:	41dfffff 	.word	0x41dfffff

080afc78 <_ZN6tflite20CalculateInputRadiusEiii>:
                                              reverse_scaling_divisor,
                                              reverse_scaling_left_shift);
}

int CalculateInputRadius(int input_integer_bits, int input_left_shift,
                         int total_signed_bits) {
 80afc78:	e92d 4370 	stmdb	sp!, {r4, r5, r6, r8, r9, lr}
 80afc7c:	4604      	mov	r4, r0
      (1ll << (total_signed_bits - input_integer_bits)) /
      (1ll << input_left_shift);
  // Tighten bound using floor.  Suppose that we could use the exact value.
  // After scaling the difference, the result would be at the maximum.  Thus we
  // must ensure that our value has lower magnitude.
  return static_cast<int>(std::floor(max_input_rescaled));
 80afc7e:	2001      	movs	r0, #1
 80afc80:	40a0      	lsls	r0, r4
 80afc82:	3801      	subs	r0, #1
                                              reverse_scaling_divisor,
                                              reverse_scaling_left_shift);
}

int CalculateInputRadius(int input_integer_bits, int input_left_shift,
                         int total_signed_bits) {
 80afc84:	460e      	mov	r6, r1
 80afc86:	4615      	mov	r5, r2
      (1ll << (total_signed_bits - input_integer_bits)) /
      (1ll << input_left_shift);
  // Tighten bound using floor.  Suppose that we could use the exact value.
  // After scaling the difference, the result would be at the maximum.  Thus we
  // must ensure that our value has lower magnitude.
  return static_cast<int>(std::floor(max_input_rescaled));
 80afc88:	f003 f80a 	bl	80b2ca0 <__aeabi_i2d>
 80afc8c:	1b2a      	subs	r2, r5, r4
 80afc8e:	4680      	mov	r8, r0
 80afc90:	4689      	mov	r9, r1
 80afc92:	2001      	movs	r0, #1
 80afc94:	2100      	movs	r1, #0
 80afc96:	f002 fea5 	bl	80b29e4 <__aeabi_llsl>
 80afc9a:	f003 f839 	bl	80b2d10 <__aeabi_l2d>
 80afc9e:	460b      	mov	r3, r1
 80afca0:	4602      	mov	r2, r0
 80afca2:	4649      	mov	r1, r9
 80afca4:	4640      	mov	r0, r8
 80afca6:	f003 f861 	bl	80b2d6c <__aeabi_dmul>
 80afcaa:	4632      	mov	r2, r6
 80afcac:	4604      	mov	r4, r0
 80afcae:	460d      	mov	r5, r1
 80afcb0:	2001      	movs	r0, #1
 80afcb2:	2100      	movs	r1, #0
 80afcb4:	f002 fe96 	bl	80b29e4 <__aeabi_llsl>
 80afcb8:	f003 f82a 	bl	80b2d10 <__aeabi_l2d>
 80afcbc:	4602      	mov	r2, r0
 80afcbe:	460b      	mov	r3, r1
 80afcc0:	4620      	mov	r0, r4
 80afcc2:	4629      	mov	r1, r5
 80afcc4:	f003 f97c 	bl	80b2fc0 <__aeabi_ddiv>
 80afcc8:	f000 ffe2 	bl	80b0c90 <floor>
 80afccc:	f003 fae8 	bl	80b32a0 <__aeabi_d2iz>
#endif  // TFLITE_EMULATE_FLOAT
}
 80afcd0:	e8bd 8370 	ldmia.w	sp!, {r4, r5, r6, r8, r9, pc}

080afcd4 <os_mutex_create>:
DYNALIB_FN(8, hal_concurrent, os_timer_create, int(os_timer_t*, unsigned, void(*)(os_timer_t), void*, bool, void*))
DYNALIB_FN(9, hal_concurrent, os_timer_destroy, int(os_timer_t, void*))
DYNALIB_FN(10, hal_concurrent, os_timer_get_id, int(os_timer_t, void**))
DYNALIB_FN(11, hal_concurrent, os_timer_change, int(os_timer_t, os_timer_change_t, bool, unsigned, unsigned, void*))

DYNALIB_FN(12, hal_concurrent, os_mutex_create, int(os_mutex_t*))
 80afcd4:	b508      	push	{r3, lr}
 80afcd6:	4b02      	ldr	r3, [pc, #8]	; (80afce0 <os_mutex_create+0xc>)
 80afcd8:	681b      	ldr	r3, [r3, #0]
 80afcda:	6b1b      	ldr	r3, [r3, #48]	; 0x30
 80afcdc:	9301      	str	r3, [sp, #4]
 80afcde:	bd08      	pop	{r3, pc}
 80afce0:	080601d0 	.word	0x080601d0

080afce4 <HAL_RNG_GetRandomNumber>:

DYNALIB_BEGIN(hal)

#if PLATFORM_ID > 3
DYNALIB_FN(0, hal, HAL_RNG_Configuration, void(void))
DYNALIB_FN(1, hal, HAL_RNG_GetRandomNumber, uint32_t(void))
 80afce4:	b508      	push	{r3, lr}
 80afce6:	4b02      	ldr	r3, [pc, #8]	; (80afcf0 <HAL_RNG_GetRandomNumber+0xc>)
 80afce8:	681b      	ldr	r3, [r3, #0]
 80afcea:	685b      	ldr	r3, [r3, #4]
 80afcec:	9301      	str	r3, [sp, #4]
 80afcee:	bd08      	pop	{r3, pc}
 80afcf0:	0806019c 	.word	0x0806019c

080afcf4 <HAL_Delay_Microseconds>:
#else
#define BASE_IDX 0
#endif

DYNALIB_FN(BASE_IDX + 0, hal, HAL_Delay_Milliseconds, void(uint32_t))
DYNALIB_FN(BASE_IDX + 1, hal, HAL_Delay_Microseconds, void(uint32_t))
 80afcf4:	b508      	push	{r3, lr}
 80afcf6:	4b02      	ldr	r3, [pc, #8]	; (80afd00 <HAL_Delay_Microseconds+0xc>)
 80afcf8:	681b      	ldr	r3, [r3, #0]
 80afcfa:	68db      	ldr	r3, [r3, #12]
 80afcfc:	9301      	str	r3, [sp, #4]
 80afcfe:	bd08      	pop	{r3, pc}
 80afd00:	0806019c 	.word	0x0806019c

080afd04 <HAL_Timer_Get_Milli_Seconds>:
DYNALIB_FN(BASE_IDX + 2, hal, HAL_Timer_Get_Micro_Seconds, system_tick_t(void))
DYNALIB_FN(BASE_IDX + 3, hal, HAL_Timer_Get_Milli_Seconds, system_tick_t(void))
 80afd04:	b508      	push	{r3, lr}
 80afd06:	4b02      	ldr	r3, [pc, #8]	; (80afd10 <HAL_Timer_Get_Milli_Seconds+0xc>)
 80afd08:	681b      	ldr	r3, [r3, #0]
 80afd0a:	695b      	ldr	r3, [r3, #20]
 80afd0c:	9301      	str	r3, [sp, #4]
 80afd0e:	bd08      	pop	{r3, pc}
 80afd10:	0806019c 	.word	0x0806019c

080afd14 <HAL_Pin_Map>:
// New HAL functions must be added to the end of this list.
// GNINRAW

DYNALIB_BEGIN(hal_gpio)

DYNALIB_FN(0, hal_gpio, HAL_Pin_Map, Hal_Pin_Info*(void))
 80afd14:	b508      	push	{r3, lr}
 80afd16:	4b02      	ldr	r3, [pc, #8]	; (80afd20 <HAL_Pin_Map+0xc>)
 80afd18:	681b      	ldr	r3, [r3, #0]
 80afd1a:	681b      	ldr	r3, [r3, #0]
 80afd1c:	9301      	str	r3, [sp, #4]
 80afd1e:	bd08      	pop	{r3, pc}
 80afd20:	080601b0 	.word	0x080601b0

080afd24 <HAL_Validate_Pin_Function>:
DYNALIB_FN(1, hal_gpio, HAL_Validate_Pin_Function, PinFunction(pin_t, PinFunction))
 80afd24:	b508      	push	{r3, lr}
 80afd26:	4b02      	ldr	r3, [pc, #8]	; (80afd30 <HAL_Validate_Pin_Function+0xc>)
 80afd28:	681b      	ldr	r3, [r3, #0]
 80afd2a:	685b      	ldr	r3, [r3, #4]
 80afd2c:	9301      	str	r3, [sp, #4]
 80afd2e:	bd08      	pop	{r3, pc}
 80afd30:	080601b0 	.word	0x080601b0

080afd34 <HAL_Pin_Mode>:
DYNALIB_FN(2, hal_gpio, HAL_Pin_Mode, void(pin_t, PinMode))
 80afd34:	b508      	push	{r3, lr}
 80afd36:	4b02      	ldr	r3, [pc, #8]	; (80afd40 <HAL_Pin_Mode+0xc>)
 80afd38:	681b      	ldr	r3, [r3, #0]
 80afd3a:	689b      	ldr	r3, [r3, #8]
 80afd3c:	9301      	str	r3, [sp, #4]
 80afd3e:	bd08      	pop	{r3, pc}
 80afd40:	080601b0 	.word	0x080601b0

080afd44 <HAL_Get_Pin_Mode>:
DYNALIB_FN(3, hal_gpio, HAL_Get_Pin_Mode, PinMode(pin_t))
 80afd44:	b508      	push	{r3, lr}
 80afd46:	4b02      	ldr	r3, [pc, #8]	; (80afd50 <HAL_Get_Pin_Mode+0xc>)
 80afd48:	681b      	ldr	r3, [r3, #0]
 80afd4a:	68db      	ldr	r3, [r3, #12]
 80afd4c:	9301      	str	r3, [sp, #4]
 80afd4e:	bd08      	pop	{r3, pc}
 80afd50:	080601b0 	.word	0x080601b0

080afd54 <HAL_DAC_Write>:
DYNALIB_FN(6, hal_gpio, HAL_Interrupts_Attach, int(uint16_t, HAL_InterruptHandler, void*, InterruptMode, HAL_InterruptExtraConfiguration*))
DYNALIB_FN(7, hal_gpio, HAL_Interrupts_Detach, int(uint16_t))
DYNALIB_FN(8, hal_gpio, HAL_Interrupts_Enable_All, void(void))
DYNALIB_FN(9, hal_gpio, HAL_Interrupts_Disable_All, void(void))

DYNALIB_FN(10, hal_gpio, HAL_DAC_Write, void(pin_t, uint16_t))
 80afd54:	b508      	push	{r3, lr}
 80afd56:	4b02      	ldr	r3, [pc, #8]	; (80afd60 <HAL_DAC_Write+0xc>)
 80afd58:	681b      	ldr	r3, [r3, #0]
 80afd5a:	6a9b      	ldr	r3, [r3, #40]	; 0x28
 80afd5c:	9301      	str	r3, [sp, #4]
 80afd5e:	bd08      	pop	{r3, pc}
 80afd60:	080601b0 	.word	0x080601b0

080afd64 <HAL_PWM_Write_Ext>:
DYNALIB_FN(25, hal_gpio, HAL_DAC_Get_Resolution, uint8_t(pin_t))
DYNALIB_FN(26, hal_gpio, HAL_DAC_Set_Resolution, void(pin_t, uint8_t))
DYNALIB_FN(27, hal_gpio, HAL_DAC_Enable_Buffer, void(pin_t pin, uint8_t state))
DYNALIB_FN(28, hal_gpio, HAL_PWM_Get_Resolution, uint8_t(uint16_t))
DYNALIB_FN(29, hal_gpio, HAL_PWM_Set_Resolution, void(uint16_t, uint8_t))
DYNALIB_FN(30, hal_gpio, HAL_PWM_Write_Ext, void(uint16_t, uint32_t))
 80afd64:	b508      	push	{r3, lr}
 80afd66:	4b02      	ldr	r3, [pc, #8]	; (80afd70 <HAL_PWM_Write_Ext+0xc>)
 80afd68:	681b      	ldr	r3, [r3, #0]
 80afd6a:	6f9b      	ldr	r3, [r3, #120]	; 0x78
 80afd6c:	9301      	str	r3, [sp, #4]
 80afd6e:	bd08      	pop	{r3, pc}
 80afd70:	080601b0 	.word	0x080601b0

080afd74 <HAL_I2C_Write_Data>:
DYNALIB_FN(BASE_IDX + 3, hal_i2c, HAL_I2C_Begin, void(HAL_I2C_Interface, I2C_Mode, uint8_t, void*))
DYNALIB_FN(BASE_IDX + 4, hal_i2c, HAL_I2C_End, void(HAL_I2C_Interface, void*))
DYNALIB_FN(BASE_IDX + 5, hal_i2c, HAL_I2C_Request_Data, uint32_t(HAL_I2C_Interface, uint8_t, uint8_t, uint8_t, void*))
DYNALIB_FN(BASE_IDX + 6, hal_i2c, HAL_I2C_Begin_Transmission, void(HAL_I2C_Interface, uint8_t, void*))
DYNALIB_FN(BASE_IDX + 7, hal_i2c, HAL_I2C_End_Transmission, uint8_t(HAL_I2C_Interface, uint8_t, void*))
DYNALIB_FN(BASE_IDX + 8, hal_i2c, HAL_I2C_Write_Data, uint32_t(HAL_I2C_Interface, uint8_t, void*))
 80afd74:	b508      	push	{r3, lr}
 80afd76:	4b02      	ldr	r3, [pc, #8]	; (80afd80 <HAL_I2C_Write_Data+0xc>)
 80afd78:	681b      	ldr	r3, [r3, #0]
 80afd7a:	6e1b      	ldr	r3, [r3, #96]	; 0x60
 80afd7c:	9301      	str	r3, [sp, #4]
 80afd7e:	bd08      	pop	{r3, pc}
 80afd80:	080601ac 	.word	0x080601ac

080afd84 <HAL_I2C_Available_Data>:
DYNALIB_FN(BASE_IDX + 9, hal_i2c, HAL_I2C_Available_Data, int32_t(HAL_I2C_Interface, void*))
 80afd84:	b508      	push	{r3, lr}
 80afd86:	4b02      	ldr	r3, [pc, #8]	; (80afd90 <HAL_I2C_Available_Data+0xc>)
 80afd88:	681b      	ldr	r3, [r3, #0]
 80afd8a:	6e5b      	ldr	r3, [r3, #100]	; 0x64
 80afd8c:	9301      	str	r3, [sp, #4]
 80afd8e:	bd08      	pop	{r3, pc}
 80afd90:	080601ac 	.word	0x080601ac

080afd94 <HAL_I2C_Read_Data>:
DYNALIB_FN(BASE_IDX + 10, hal_i2c, HAL_I2C_Read_Data, int32_t(HAL_I2C_Interface, void*))
 80afd94:	b508      	push	{r3, lr}
 80afd96:	4b02      	ldr	r3, [pc, #8]	; (80afda0 <HAL_I2C_Read_Data+0xc>)
 80afd98:	681b      	ldr	r3, [r3, #0]
 80afd9a:	6e9b      	ldr	r3, [r3, #104]	; 0x68
 80afd9c:	9301      	str	r3, [sp, #4]
 80afd9e:	bd08      	pop	{r3, pc}
 80afda0:	080601ac 	.word	0x080601ac

080afda4 <HAL_I2C_Peek_Data>:
DYNALIB_FN(BASE_IDX + 11, hal_i2c, HAL_I2C_Peek_Data, int32_t(HAL_I2C_Interface, void*))
 80afda4:	b508      	push	{r3, lr}
 80afda6:	4b02      	ldr	r3, [pc, #8]	; (80afdb0 <HAL_I2C_Peek_Data+0xc>)
 80afda8:	681b      	ldr	r3, [r3, #0]
 80afdaa:	6edb      	ldr	r3, [r3, #108]	; 0x6c
 80afdac:	9301      	str	r3, [sp, #4]
 80afdae:	bd08      	pop	{r3, pc}
 80afdb0:	080601ac 	.word	0x080601ac

080afdb4 <HAL_I2C_Flush_Data>:
DYNALIB_FN(BASE_IDX + 12, hal_i2c, HAL_I2C_Flush_Data, void(HAL_I2C_Interface, void*))
 80afdb4:	b508      	push	{r3, lr}
 80afdb6:	4b02      	ldr	r3, [pc, #8]	; (80afdc0 <HAL_I2C_Flush_Data+0xc>)
 80afdb8:	681b      	ldr	r3, [r3, #0]
 80afdba:	6f1b      	ldr	r3, [r3, #112]	; 0x70
 80afdbc:	9301      	str	r3, [sp, #4]
 80afdbe:	bd08      	pop	{r3, pc}
 80afdc0:	080601ac 	.word	0x080601ac

080afdc4 <HAL_I2C_Is_Enabled>:
DYNALIB_FN(BASE_IDX + 13, hal_i2c, HAL_I2C_Is_Enabled, bool(HAL_I2C_Interface, void*))
 80afdc4:	b508      	push	{r3, lr}
 80afdc6:	4b02      	ldr	r3, [pc, #8]	; (80afdd0 <HAL_I2C_Is_Enabled+0xc>)
 80afdc8:	681b      	ldr	r3, [r3, #0]
 80afdca:	6f5b      	ldr	r3, [r3, #116]	; 0x74
 80afdcc:	9301      	str	r3, [sp, #4]
 80afdce:	bd08      	pop	{r3, pc}
 80afdd0:	080601ac 	.word	0x080601ac

080afdd4 <HAL_I2C_Init>:
DYNALIB_FN(BASE_IDX + 14, hal_i2c, HAL_I2C_Set_Callback_On_Receive, void(HAL_I2C_Interface, void(*)(int), void*))
DYNALIB_FN(BASE_IDX + 15, hal_i2c, HAL_I2C_Set_Callback_On_Request, void(HAL_I2C_Interface, void(*)(void), void*))
DYNALIB_FN(BASE_IDX + 16, hal_i2c, HAL_I2C_Init, void(HAL_I2C_Interface, void*))
 80afdd4:	b508      	push	{r3, lr}
 80afdd6:	4b03      	ldr	r3, [pc, #12]	; (80afde4 <HAL_I2C_Init+0x10>)
 80afdd8:	681b      	ldr	r3, [r3, #0]
 80afdda:	f8d3 3080 	ldr.w	r3, [r3, #128]	; 0x80
 80afdde:	9301      	str	r3, [sp, #4]
 80afde0:	bd08      	pop	{r3, pc}
 80afde2:	0000      	.short	0x0000
 80afde4:	080601ac 	.word	0x080601ac

080afde8 <HAL_SPI_Init>:
DYNALIB_FN(2, hal_spi, HAL_SPI_Set_Bit_Order, void(HAL_SPI_Interface, uint8_t))
DYNALIB_FN(3, hal_spi, HAL_SPI_Set_Data_Mode, void(HAL_SPI_Interface, uint8_t))
DYNALIB_FN(4, hal_spi, HAL_SPI_Set_Clock_Divider, void(HAL_SPI_Interface, uint8_t))
DYNALIB_FN(5, hal_spi, HAL_SPI_Send_Receive_Data, uint16_t(HAL_SPI_Interface, uint16_t))
DYNALIB_FN(6, hal_spi, HAL_SPI_Is_Enabled_Old, bool(void))
DYNALIB_FN(7, hal_spi, HAL_SPI_Init, void(HAL_SPI_Interface))
 80afde8:	b508      	push	{r3, lr}
 80afdea:	4b02      	ldr	r3, [pc, #8]	; (80afdf4 <HAL_SPI_Init+0xc>)
 80afdec:	681b      	ldr	r3, [r3, #0]
 80afdee:	69db      	ldr	r3, [r3, #28]
 80afdf0:	9301      	str	r3, [sp, #4]
 80afdf2:	bd08      	pop	{r3, pc}
 80afdf4:	080601b4 	.word	0x080601b4

080afdf8 <HAL_SPI_Is_Enabled>:
DYNALIB_FN(8, hal_spi, HAL_SPI_Is_Enabled, bool(HAL_SPI_Interface))
 80afdf8:	b508      	push	{r3, lr}
 80afdfa:	4b02      	ldr	r3, [pc, #8]	; (80afe04 <HAL_SPI_Is_Enabled+0xc>)
 80afdfc:	681b      	ldr	r3, [r3, #0]
 80afdfe:	6a1b      	ldr	r3, [r3, #32]
 80afe00:	9301      	str	r3, [sp, #4]
 80afe02:	bd08      	pop	{r3, pc}
 80afe04:	080601b4 	.word	0x080601b4

080afe08 <HAL_USART_Init>:
#define BASE_IDX 6 // Base index for all subsequent functions
#else
#define BASE_IDX 0
#endif

DYNALIB_FN(BASE_IDX + 0, hal_usart, HAL_USART_Init, void(HAL_USART_Serial, Ring_Buffer*, Ring_Buffer*))
 80afe08:	b508      	push	{r3, lr}
 80afe0a:	4b02      	ldr	r3, [pc, #8]	; (80afe14 <HAL_USART_Init+0xc>)
 80afe0c:	681b      	ldr	r3, [r3, #0]
 80afe0e:	699b      	ldr	r3, [r3, #24]
 80afe10:	9301      	str	r3, [sp, #4]
 80afe12:	bd08      	pop	{r3, pc}
 80afe14:	080601c4 	.word	0x080601c4

080afe18 <HAL_USART_Write_Data>:
DYNALIB_FN(BASE_IDX + 1, hal_usart, HAL_USART_Begin, void(HAL_USART_Serial, uint32_t))
DYNALIB_FN(BASE_IDX + 2, hal_usart, HAL_USART_End, void(HAL_USART_Serial))
DYNALIB_FN(BASE_IDX + 3, hal_usart, HAL_USART_Write_Data, uint32_t(HAL_USART_Serial, uint8_t))
 80afe18:	b508      	push	{r3, lr}
 80afe1a:	4b02      	ldr	r3, [pc, #8]	; (80afe24 <HAL_USART_Write_Data+0xc>)
 80afe1c:	681b      	ldr	r3, [r3, #0]
 80afe1e:	6a5b      	ldr	r3, [r3, #36]	; 0x24
 80afe20:	9301      	str	r3, [sp, #4]
 80afe22:	bd08      	pop	{r3, pc}
 80afe24:	080601c4 	.word	0x080601c4

080afe28 <HAL_USART_Available_Data>:
DYNALIB_FN(BASE_IDX + 4, hal_usart, HAL_USART_Available_Data, int32_t(HAL_USART_Serial))
 80afe28:	b508      	push	{r3, lr}
 80afe2a:	4b02      	ldr	r3, [pc, #8]	; (80afe34 <HAL_USART_Available_Data+0xc>)
 80afe2c:	681b      	ldr	r3, [r3, #0]
 80afe2e:	6a9b      	ldr	r3, [r3, #40]	; 0x28
 80afe30:	9301      	str	r3, [sp, #4]
 80afe32:	bd08      	pop	{r3, pc}
 80afe34:	080601c4 	.word	0x080601c4

080afe38 <HAL_USART_Read_Data>:
DYNALIB_FN(BASE_IDX + 5, hal_usart, HAL_USART_Read_Data, int32_t(HAL_USART_Serial))
 80afe38:	b508      	push	{r3, lr}
 80afe3a:	4b02      	ldr	r3, [pc, #8]	; (80afe44 <HAL_USART_Read_Data+0xc>)
 80afe3c:	681b      	ldr	r3, [r3, #0]
 80afe3e:	6adb      	ldr	r3, [r3, #44]	; 0x2c
 80afe40:	9301      	str	r3, [sp, #4]
 80afe42:	bd08      	pop	{r3, pc}
 80afe44:	080601c4 	.word	0x080601c4

080afe48 <HAL_USART_Peek_Data>:
DYNALIB_FN(BASE_IDX + 6, hal_usart, HAL_USART_Peek_Data, int32_t(HAL_USART_Serial))
 80afe48:	b508      	push	{r3, lr}
 80afe4a:	4b02      	ldr	r3, [pc, #8]	; (80afe54 <HAL_USART_Peek_Data+0xc>)
 80afe4c:	681b      	ldr	r3, [r3, #0]
 80afe4e:	6b1b      	ldr	r3, [r3, #48]	; 0x30
 80afe50:	9301      	str	r3, [sp, #4]
 80afe52:	bd08      	pop	{r3, pc}
 80afe54:	080601c4 	.word	0x080601c4

080afe58 <HAL_USART_Flush_Data>:
DYNALIB_FN(BASE_IDX + 7, hal_usart, HAL_USART_Flush_Data, void(HAL_USART_Serial))
 80afe58:	b508      	push	{r3, lr}
 80afe5a:	4b02      	ldr	r3, [pc, #8]	; (80afe64 <HAL_USART_Flush_Data+0xc>)
 80afe5c:	681b      	ldr	r3, [r3, #0]
 80afe5e:	6b5b      	ldr	r3, [r3, #52]	; 0x34
 80afe60:	9301      	str	r3, [sp, #4]
 80afe62:	bd08      	pop	{r3, pc}
 80afe64:	080601c4 	.word	0x080601c4

080afe68 <HAL_USART_Is_Enabled>:
DYNALIB_FN(BASE_IDX + 8, hal_usart, HAL_USART_Is_Enabled, bool(HAL_USART_Serial))
 80afe68:	b508      	push	{r3, lr}
 80afe6a:	4b02      	ldr	r3, [pc, #8]	; (80afe74 <HAL_USART_Is_Enabled+0xc>)
 80afe6c:	681b      	ldr	r3, [r3, #0]
 80afe6e:	6b9b      	ldr	r3, [r3, #56]	; 0x38
 80afe70:	9301      	str	r3, [sp, #4]
 80afe72:	bd08      	pop	{r3, pc}
 80afe74:	080601c4 	.word	0x080601c4

080afe78 <HAL_USART_Available_Data_For_Write>:
DYNALIB_FN(BASE_IDX + 9, hal_usart, HAL_USART_Half_Duplex, void(HAL_USART_Serial, bool))
DYNALIB_FN(BASE_IDX + 10, hal_usart, HAL_USART_Available_Data_For_Write, int32_t(HAL_USART_Serial))
 80afe78:	b508      	push	{r3, lr}
 80afe7a:	4b02      	ldr	r3, [pc, #8]	; (80afe84 <HAL_USART_Available_Data_For_Write+0xc>)
 80afe7c:	681b      	ldr	r3, [r3, #0]
 80afe7e:	6c1b      	ldr	r3, [r3, #64]	; 0x40
 80afe80:	9301      	str	r3, [sp, #4]
 80afe82:	bd08      	pop	{r3, pc}
 80afe84:	080601c4 	.word	0x080601c4

080afe88 <HAL_USB_USART_Init>:
#endif

DYNALIB_BEGIN(hal_usb)

#ifdef USB_CDC_ENABLE
DYNALIB_FN(0, hal_usb, HAL_USB_USART_Init, void(HAL_USB_USART_Serial, const HAL_USB_USART_Config*))
 80afe88:	b508      	push	{r3, lr}
 80afe8a:	4b02      	ldr	r3, [pc, #8]	; (80afe94 <HAL_USB_USART_Init+0xc>)
 80afe8c:	681b      	ldr	r3, [r3, #0]
 80afe8e:	681b      	ldr	r3, [r3, #0]
 80afe90:	9301      	str	r3, [sp, #4]
 80afe92:	bd08      	pop	{r3, pc}
 80afe94:	080601d8 	.word	0x080601d8

080afe98 <HAL_USB_USART_Begin>:
DYNALIB_FN(1, hal_usb, HAL_USB_USART_Begin, void(HAL_USB_USART_Serial, uint32_t, void *))
 80afe98:	b508      	push	{r3, lr}
 80afe9a:	4b02      	ldr	r3, [pc, #8]	; (80afea4 <HAL_USB_USART_Begin+0xc>)
 80afe9c:	681b      	ldr	r3, [r3, #0]
 80afe9e:	685b      	ldr	r3, [r3, #4]
 80afea0:	9301      	str	r3, [sp, #4]
 80afea2:	bd08      	pop	{r3, pc}
 80afea4:	080601d8 	.word	0x080601d8

080afea8 <HAL_USB_USART_Available_Data>:
DYNALIB_FN(2, hal_usb, HAL_USB_USART_End, void(HAL_USB_USART_Serial))
DYNALIB_FN(3, hal_usb, HAL_USB_USART_Baud_Rate, unsigned int(HAL_USB_USART_Serial))
DYNALIB_FN(4, hal_usb, HAL_USB_USART_Available_Data, int32_t(HAL_USB_USART_Serial))
 80afea8:	b508      	push	{r3, lr}
 80afeaa:	4b02      	ldr	r3, [pc, #8]	; (80afeb4 <HAL_USB_USART_Available_Data+0xc>)
 80afeac:	681b      	ldr	r3, [r3, #0]
 80afeae:	691b      	ldr	r3, [r3, #16]
 80afeb0:	9301      	str	r3, [sp, #4]
 80afeb2:	bd08      	pop	{r3, pc}
 80afeb4:	080601d8 	.word	0x080601d8

080afeb8 <HAL_USB_USART_Available_Data_For_Write>:
DYNALIB_FN(5, hal_usb, HAL_USB_USART_Available_Data_For_Write, int32_t(HAL_USB_USART_Serial))
 80afeb8:	b508      	push	{r3, lr}
 80afeba:	4b02      	ldr	r3, [pc, #8]	; (80afec4 <HAL_USB_USART_Available_Data_For_Write+0xc>)
 80afebc:	681b      	ldr	r3, [r3, #0]
 80afebe:	695b      	ldr	r3, [r3, #20]
 80afec0:	9301      	str	r3, [sp, #4]
 80afec2:	bd08      	pop	{r3, pc}
 80afec4:	080601d8 	.word	0x080601d8

080afec8 <HAL_USB_USART_Receive_Data>:
DYNALIB_FN(6, hal_usb, HAL_USB_USART_Receive_Data, int32_t(HAL_USB_USART_Serial, uint8_t))
 80afec8:	b508      	push	{r3, lr}
 80afeca:	4b02      	ldr	r3, [pc, #8]	; (80afed4 <HAL_USB_USART_Receive_Data+0xc>)
 80afecc:	681b      	ldr	r3, [r3, #0]
 80afece:	699b      	ldr	r3, [r3, #24]
 80afed0:	9301      	str	r3, [sp, #4]
 80afed2:	bd08      	pop	{r3, pc}
 80afed4:	080601d8 	.word	0x080601d8

080afed8 <HAL_USB_USART_Send_Data>:
DYNALIB_FN(7, hal_usb, HAL_USB_USART_Send_Data, int32_t(HAL_USB_USART_Serial, uint8_t))
 80afed8:	b508      	push	{r3, lr}
 80afeda:	4b02      	ldr	r3, [pc, #8]	; (80afee4 <HAL_USB_USART_Send_Data+0xc>)
 80afedc:	681b      	ldr	r3, [r3, #0]
 80afede:	69db      	ldr	r3, [r3, #28]
 80afee0:	9301      	str	r3, [sp, #4]
 80afee2:	bd08      	pop	{r3, pc}
 80afee4:	080601d8 	.word	0x080601d8

080afee8 <HAL_USB_USART_Flush_Data>:
DYNALIB_FN(8, hal_usb, HAL_USB_USART_Flush_Data, void(HAL_USB_USART_Serial))
 80afee8:	b508      	push	{r3, lr}
 80afeea:	4b02      	ldr	r3, [pc, #8]	; (80afef4 <HAL_USB_USART_Flush_Data+0xc>)
 80afeec:	681b      	ldr	r3, [r3, #0]
 80afeee:	6a1b      	ldr	r3, [r3, #32]
 80afef0:	9301      	str	r3, [sp, #4]
 80afef2:	bd08      	pop	{r3, pc}
 80afef4:	080601d8 	.word	0x080601d8

080afef8 <inet_gethostbyname>:
DYNALIB_FN(14, hal_wlan, wlan_set_error_count, void(uint32_t))
DYNALIB_FN(15, hal_wlan, wlan_fetch_ipconfig, int(WLanConfig*))
DYNALIB_FN(16, hal_wlan, wlan_setup, void(void))

DYNALIB_FN(17, hal_wlan, HAL_NET_SetNetWatchDog, uint32_t(uint32_t))
DYNALIB_FN(18, hal_wlan, inet_gethostbyname, int(const char*, uint16_t, HAL_IPAddress*, network_interface_t, void*))
 80afef8:	b508      	push	{r3, lr}
 80afefa:	4b02      	ldr	r3, [pc, #8]	; (80aff04 <inet_gethostbyname+0xc>)
 80afefc:	681b      	ldr	r3, [r3, #0]
 80afefe:	6c9b      	ldr	r3, [r3, #72]	; 0x48
 80aff00:	9301      	str	r3, [sp, #4]
 80aff02:	bd08      	pop	{r3, pc}
 80aff04:	080601c0 	.word	0x080601c0

080aff08 <panic_>:
DYNALIB_FN(9, services, LED_Toggle, void(Led_TypeDef))
DYNALIB_FN(10, services, LED_Fade, void(Led_TypeDef))
DYNALIB_FN(11, services, Get_LED_Brightness, uint8_t(void))

DYNALIB_FN(12, services, set_logger_output, void(debug_output_fn, LoggerOutputLevel)) // Deprecated
DYNALIB_FN(13, services, panic_, void(ePanicCode, void*, void(*)(uint32_t)))
 80aff08:	b508      	push	{r3, lr}
 80aff0a:	4b02      	ldr	r3, [pc, #8]	; (80aff14 <panic_+0xc>)
 80aff0c:	681b      	ldr	r3, [r3, #0]
 80aff0e:	6b5b      	ldr	r3, [r3, #52]	; 0x34
 80aff10:	9301      	str	r3, [sp, #4]
 80aff12:	bd08      	pop	{r3, pc}
 80aff14:	080201a8 	.word	0x080201a8

080aff18 <set_system_mode>:
#endif

DYNALIB_BEGIN(system)

DYNALIB_FN(0, system, system_mode, System_Mode_TypeDef(void))
DYNALIB_FN(1, system, set_system_mode, void(System_Mode_TypeDef))
 80aff18:	b508      	push	{r3, lr}
 80aff1a:	4b02      	ldr	r3, [pc, #8]	; (80aff24 <set_system_mode+0xc>)
 80aff1c:	681b      	ldr	r3, [r3, #0]
 80aff1e:	685b      	ldr	r3, [r3, #4]
 80aff20:	9301      	str	r3, [sp, #4]
 80aff22:	bd08      	pop	{r3, pc}
 80aff24:	080601a4 	.word	0x080601a4

080aff28 <system_thread_set_state>:
DYNALIB_FN(6, system, system_sleep, int(Spark_Sleep_TypeDef, long, uint32_t, void*))
DYNALIB_FN(7, system, system_sleep_pin, int(uint16_t, uint16_t, long, uint32_t, void*))
DYNALIB_FN(8, system, system_subscribe_event, int(system_event_t, system_event_handler_t*, void*))
DYNALIB_FN(9, system, system_unsubscribe_event, void(system_event_t, system_event_handler_t*, void*))
DYNALIB_FN(10, system, system_button_pushed_duration, uint16_t(uint8_t, void*))
DYNALIB_FN(11, system, system_thread_set_state, void(spark::feature::State, void*))
 80aff28:	b508      	push	{r3, lr}
 80aff2a:	4b02      	ldr	r3, [pc, #8]	; (80aff34 <system_thread_set_state+0xc>)
 80aff2c:	681b      	ldr	r3, [r3, #0]
 80aff2e:	6adb      	ldr	r3, [r3, #44]	; 0x2c
 80aff30:	9301      	str	r3, [sp, #4]
 80aff32:	bd08      	pop	{r3, pc}
 80aff34:	080601a4 	.word	0x080601a4

080aff38 <system_ctrl_set_app_request_handler>:
DYNALIB_FN(BASE_IDX + 6, system, led_pattern_period, uint16_t(int, int, void*))
DYNALIB_FN(BASE_IDX + 7, system, system_set_tester_handlers, int(system_tester_handlers_t*, void*))
DYNALIB_FN(BASE_IDX + 8, system, system_format_diag_data, int(const uint16_t*, size_t, unsigned, appender_fn, void*, void*))

// Control requests
DYNALIB_FN(BASE_IDX + 9, system, system_ctrl_set_app_request_handler, int(ctrl_request_handler_fn, void*))
 80aff38:	b508      	push	{r3, lr}
 80aff3a:	4b03      	ldr	r3, [pc, #12]	; (80aff48 <system_ctrl_set_app_request_handler+0x10>)
 80aff3c:	681b      	ldr	r3, [r3, #0]
 80aff3e:	f8d3 3090 	ldr.w	r3, [r3, #144]	; 0x90
 80aff42:	9301      	str	r3, [sp, #4]
 80aff44:	bd08      	pop	{r3, pc}
 80aff46:	0000      	.short	0x0000
 80aff48:	080601a4 	.word	0x080601a4

080aff4c <system_ctrl_set_result>:
DYNALIB_FN(BASE_IDX + 10, system, system_ctrl_alloc_reply_data, int(ctrl_request*, size_t, void*))
DYNALIB_FN(BASE_IDX + 11, system, system_ctrl_free_request_data, void(ctrl_request*, void*))
DYNALIB_FN(BASE_IDX + 12, system, system_ctrl_set_result, void(ctrl_request*, int, ctrl_completion_handler_fn, void*, void*))
 80aff4c:	b508      	push	{r3, lr}
 80aff4e:	4b03      	ldr	r3, [pc, #12]	; (80aff5c <system_ctrl_set_result+0x10>)
 80aff50:	681b      	ldr	r3, [r3, #0]
 80aff52:	f8d3 309c 	ldr.w	r3, [r3, #156]	; 0x9c
 80aff56:	9301      	str	r3, [sp, #4]
 80aff58:	bd08      	pop	{r3, pc}
 80aff5a:	0000      	.short	0x0000
 80aff5c:	080601a4 	.word	0x080601a4

080aff60 <spark_set_random_seed_from_cloud_handler>:
DYNALIB_FN(10, system_cloud, spark_unsubscribe, void(void*))
DYNALIB_FN(11, system_cloud, spark_sync_time, bool(void*))
DYNALIB_FN(12, system_cloud, spark_sync_time_pending, bool(void*))
DYNALIB_FN(13, system_cloud, spark_sync_time_last, system_tick_t(time_t*, void*))
DYNALIB_FN(14, system_cloud, spark_set_connection_property, int(unsigned, unsigned, particle::protocol::connection_properties_t*, void*))
DYNALIB_FN(15, system_cloud, spark_set_random_seed_from_cloud_handler, int(void (*handler)(unsigned int), void*))
 80aff60:	b508      	push	{r3, lr}
 80aff62:	4b02      	ldr	r3, [pc, #8]	; (80aff6c <spark_set_random_seed_from_cloud_handler+0xc>)
 80aff64:	681b      	ldr	r3, [r3, #0]
 80aff66:	6bdb      	ldr	r3, [r3, #60]	; 0x3c
 80aff68:	9301      	str	r3, [sp, #4]
 80aff6a:	bd08      	pop	{r3, pc}
 80aff6c:	080601cc 	.word	0x080601cc

080aff70 <network_connect>:
#endif

DYNALIB_BEGIN(system_net)

DYNALIB_FN(0, system_net, network_config, const void*(network_handle_t, uint32_t, void*))
DYNALIB_FN(1, system_net, network_connect, void(network_handle_t, uint32_t, uint32_t, void*))
 80aff70:	b508      	push	{r3, lr}
 80aff72:	4b02      	ldr	r3, [pc, #8]	; (80aff7c <network_connect+0xc>)
 80aff74:	681b      	ldr	r3, [r3, #0]
 80aff76:	685b      	ldr	r3, [r3, #4]
 80aff78:	9301      	str	r3, [sp, #4]
 80aff7a:	bd08      	pop	{r3, pc}
 80aff7c:	080601c8 	.word	0x080601c8

080aff80 <network_connecting>:
DYNALIB_FN(2, system_net, network_connecting, bool(network_handle_t, uint32_t, void*))
 80aff80:	b508      	push	{r3, lr}
 80aff82:	4b02      	ldr	r3, [pc, #8]	; (80aff8c <network_connecting+0xc>)
 80aff84:	681b      	ldr	r3, [r3, #0]
 80aff86:	689b      	ldr	r3, [r3, #8]
 80aff88:	9301      	str	r3, [sp, #4]
 80aff8a:	bd08      	pop	{r3, pc}
 80aff8c:	080601c8 	.word	0x080601c8

080aff90 <network_disconnect>:
DYNALIB_FN(3, system_net, network_disconnect, void(network_handle_t, uint32_t, void*))
 80aff90:	b508      	push	{r3, lr}
 80aff92:	4b02      	ldr	r3, [pc, #8]	; (80aff9c <network_disconnect+0xc>)
 80aff94:	681b      	ldr	r3, [r3, #0]
 80aff96:	68db      	ldr	r3, [r3, #12]
 80aff98:	9301      	str	r3, [sp, #4]
 80aff9a:	bd08      	pop	{r3, pc}
 80aff9c:	080601c8 	.word	0x080601c8

080affa0 <network_ready>:
DYNALIB_FN(4, system_net, network_ready, bool(network_handle_t, uint32_t, void*))
 80affa0:	b508      	push	{r3, lr}
 80affa2:	4b02      	ldr	r3, [pc, #8]	; (80affac <network_ready+0xc>)
 80affa4:	681b      	ldr	r3, [r3, #0]
 80affa6:	691b      	ldr	r3, [r3, #16]
 80affa8:	9301      	str	r3, [sp, #4]
 80affaa:	bd08      	pop	{r3, pc}
 80affac:	080601c8 	.word	0x080601c8

080affb0 <network_on>:
DYNALIB_FN(5, system_net, network_on, void(network_handle_t, uint32_t, uint32_t, void*))
 80affb0:	b508      	push	{r3, lr}
 80affb2:	4b02      	ldr	r3, [pc, #8]	; (80affbc <network_on+0xc>)
 80affb4:	681b      	ldr	r3, [r3, #0]
 80affb6:	695b      	ldr	r3, [r3, #20]
 80affb8:	9301      	str	r3, [sp, #4]
 80affba:	bd08      	pop	{r3, pc}
 80affbc:	080601c8 	.word	0x080601c8

080affc0 <network_off>:
DYNALIB_FN(6, system_net, network_off, void(network_handle_t, uint32_t, uint32_t, void*))
 80affc0:	b508      	push	{r3, lr}
 80affc2:	4b02      	ldr	r3, [pc, #8]	; (80affcc <network_off+0xc>)
 80affc4:	681b      	ldr	r3, [r3, #0]
 80affc6:	699b      	ldr	r3, [r3, #24]
 80affc8:	9301      	str	r3, [sp, #4]
 80affca:	bd08      	pop	{r3, pc}
 80affcc:	080601c8 	.word	0x080601c8

080affd0 <network_listen>:
DYNALIB_FN(7, system_net, network_listen, void(network_handle_t, uint32_t, void*))
 80affd0:	b508      	push	{r3, lr}
 80affd2:	4b02      	ldr	r3, [pc, #8]	; (80affdc <network_listen+0xc>)
 80affd4:	681b      	ldr	r3, [r3, #0]
 80affd6:	69db      	ldr	r3, [r3, #28]
 80affd8:	9301      	str	r3, [sp, #4]
 80affda:	bd08      	pop	{r3, pc}
 80affdc:	080601c8 	.word	0x080601c8

080affe0 <network_listening>:
DYNALIB_FN(8, system_net, network_listening, bool(network_handle_t, uint32_t, void*))
 80affe0:	b508      	push	{r3, lr}
 80affe2:	4b02      	ldr	r3, [pc, #8]	; (80affec <network_listening+0xc>)
 80affe4:	681b      	ldr	r3, [r3, #0]
 80affe6:	6a1b      	ldr	r3, [r3, #32]
 80affe8:	9301      	str	r3, [sp, #4]
 80affea:	bd08      	pop	{r3, pc}
 80affec:	080601c8 	.word	0x080601c8

080afff0 <network_set_listen_timeout>:
DYNALIB_FN(9, system_net, network_has_credentials, bool(network_handle_t, uint32_t, void*))
DYNALIB_FN(10, system_net, network_set_credentials, int(network_handle_t, uint32_t, NetworkCredentials*, void*))
DYNALIB_FN(11, system_net, network_clear_credentials, bool(network_handle_t, uint32_t, NetworkCredentials*, void*))
DYNALIB_FN(12, system_net, network_set_listen_timeout, void(network_handle_t, uint16_t, void*))
 80afff0:	b508      	push	{r3, lr}
 80afff2:	4b02      	ldr	r3, [pc, #8]	; (80afffc <network_set_listen_timeout+0xc>)
 80afff4:	681b      	ldr	r3, [r3, #0]
 80afff6:	6b1b      	ldr	r3, [r3, #48]	; 0x30
 80afff8:	9301      	str	r3, [sp, #4]
 80afffa:	bd08      	pop	{r3, pc}
 80afffc:	080601c8 	.word	0x080601c8

080b0000 <network_get_listen_timeout>:
DYNALIB_FN(13, system_net, network_get_listen_timeout, uint16_t(network_handle_t, uint32_t, void*))
 80b0000:	b508      	push	{r3, lr}
 80b0002:	4b02      	ldr	r3, [pc, #8]	; (80b000c <network_get_listen_timeout+0xc>)
 80b0004:	681b      	ldr	r3, [r3, #0]
 80b0006:	6b5b      	ldr	r3, [r3, #52]	; 0x34
 80b0008:	9301      	str	r3, [sp, #4]
 80b000a:	bd08      	pop	{r3, pc}
 80b000c:	080601c8 	.word	0x080601c8

080b0010 <malloc>:
#include <assert.h>
#endif

DYNALIB_BEGIN(rt)

DYNALIB_FN(0, rt, malloc, void*(size_t))
 80b0010:	b508      	push	{r3, lr}
 80b0012:	4b02      	ldr	r3, [pc, #8]	; (80b001c <malloc+0xc>)
 80b0014:	681b      	ldr	r3, [r3, #0]
 80b0016:	681b      	ldr	r3, [r3, #0]
 80b0018:	9301      	str	r3, [sp, #4]
 80b001a:	bd08      	pop	{r3, pc}
 80b001c:	080601a0 	.word	0x080601a0

080b0020 <free>:
DYNALIB_FN(1, rt, free, void(void*))
 80b0020:	b508      	push	{r3, lr}
 80b0022:	4b02      	ldr	r3, [pc, #8]	; (80b002c <free+0xc>)
 80b0024:	681b      	ldr	r3, [r3, #0]
 80b0026:	685b      	ldr	r3, [r3, #4]
 80b0028:	9301      	str	r3, [sp, #4]
 80b002a:	bd08      	pop	{r3, pc}
 80b002c:	080601a0 	.word	0x080601a0

080b0030 <abort>:
DYNALIB_FN(6, rt, siscanf, int(const char*, const char*, ...))
DYNALIB_FN(7, rt, snprintf, int(char*, size_t, const char*, ...))
DYNALIB_FN(8, rt, sniprintf, int(char*, size_t, const char*, ...))
DYNALIB_FN(9, rt, vsnprintf, int(char*, size_t, const char*, va_list))
DYNALIB_FN(10, rt, vsniprintf, int(char*, size_t, const char*, va_list))
DYNALIB_FN(11, rt, abort, void(void))
 80b0030:	b508      	push	{r3, lr}
 80b0032:	4b02      	ldr	r3, [pc, #8]	; (80b003c <abort+0xc>)
 80b0034:	681b      	ldr	r3, [r3, #0]
 80b0036:	6adb      	ldr	r3, [r3, #44]	; 0x2c
 80b0038:	9301      	str	r3, [sp, #4]
 80b003a:	bd08      	pop	{r3, pc}
 80b003c:	080601a0 	.word	0x080601a0

080b0040 <__errno>:
DYNALIB_FN(12, rt, _malloc_r, void*(struct _reent*, size_t))
DYNALIB_FN(13, rt, _free_r, void(struct _reent*, void*))
DYNALIB_FN(14, rt, _realloc_r, void*(struct _reent*, void*, size_t))
DYNALIB_FN(15, rt, __errno, int*())
 80b0040:	b508      	push	{r3, lr}
 80b0042:	4b02      	ldr	r3, [pc, #8]	; (80b004c <__errno+0xc>)
 80b0044:	681b      	ldr	r3, [r3, #0]
 80b0046:	6bdb      	ldr	r3, [r3, #60]	; 0x3c
 80b0048:	9301      	str	r3, [sp, #4]
 80b004a:	bd08      	pop	{r3, pc}
 80b004c:	080601a0 	.word	0x080601a0

080b0050 <__assert_func>:
// on Gen 2 platforms without breaking inter-module dependencies.
// RT is currently being imported into system-part1 from system-part2,
// which is the reverse direction.

#if defined(DYNALIB_IMPORT) && !defined(RT_DYNALIB_NO_DEPENDENCY_BREAKING_IMPORTS)
DYNALIB_FN(16, rt, __assert_func, void(const char*, int, const char*, const char*))
 80b0050:	b508      	push	{r3, lr}
 80b0052:	4b02      	ldr	r3, [pc, #8]	; (80b005c <__assert_func+0xc>)
 80b0054:	681b      	ldr	r3, [r3, #0]
 80b0056:	6c1b      	ldr	r3, [r3, #64]	; 0x40
 80b0058:	9301      	str	r3, [sp, #4]
 80b005a:	bd08      	pop	{r3, pc}
 80b005c:	080601a0 	.word	0x080601a0

080b0060 <_ZNSt14_Function_baseD1Ev>:
	}
      };

    _Function_base() : _M_manager(nullptr) { }

    ~_Function_base()
 80b0060:	b510      	push	{r4, lr}
    {
      if (_M_manager)
 80b0062:	6883      	ldr	r3, [r0, #8]
	}
      };

    _Function_base() : _M_manager(nullptr) { }

    ~_Function_base()
 80b0064:	4604      	mov	r4, r0
    {
      if (_M_manager)
 80b0066:	b113      	cbz	r3, 80b006e <_ZNSt14_Function_baseD1Ev+0xe>
	_M_manager(_M_functor, _M_functor, __destroy_functor);
 80b0068:	2203      	movs	r2, #3
 80b006a:	4601      	mov	r1, r0
 80b006c:	4798      	blx	r3
    }
 80b006e:	4620      	mov	r0, r4
 80b0070:	bd10      	pop	{r4, pc}

080b0072 <_ZN7TwoWireD1Ev>:
private:
  HAL_I2C_Interface _i2c;

public:
  TwoWire(HAL_I2C_Interface i2c);
  virtual ~TwoWire() {};
 80b0072:	4770      	bx	lr

080b0074 <_ZN7TwoWire5writeEPKhj>:

// must be called in:
// slave tx event callback
// or after beginTransmission(address)
size_t TwoWire::write(const uint8_t *data, size_t quantity)
{
 80b0074:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
 80b0076:	4606      	mov	r6, r0
 80b0078:	4615      	mov	r5, r2
 80b007a:	460c      	mov	r4, r1
 80b007c:	188f      	adds	r7, r1, r2
  // in master/slave transmitter mode
  for(size_t i = 0; i < quantity; ++i)
 80b007e:	42bc      	cmp	r4, r7
 80b0080:	d006      	beq.n	80b0090 <_ZN7TwoWire5writeEPKhj+0x1c>
  {
    write(data[i]);
 80b0082:	6833      	ldr	r3, [r6, #0]
 80b0084:	f814 1b01 	ldrb.w	r1, [r4], #1
 80b0088:	689b      	ldr	r3, [r3, #8]
 80b008a:	4630      	mov	r0, r6
 80b008c:	4798      	blx	r3
// slave tx event callback
// or after beginTransmission(address)
size_t TwoWire::write(const uint8_t *data, size_t quantity)
{
  // in master/slave transmitter mode
  for(size_t i = 0; i < quantity; ++i)
 80b008e:	e7f6      	b.n	80b007e <_ZN7TwoWire5writeEPKhj+0xa>
  {
    write(data[i]);
  }

  return quantity;
}
 80b0090:	4628      	mov	r0, r5
 80b0092:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}

080b0094 <_ZN7TwoWire5writeEh>:
// must be called in:
// slave tx event callback
// or after beginTransmission(address)
size_t TwoWire::write(uint8_t data)
{
  return HAL_I2C_Write_Data(_i2c, data, NULL);
 80b0094:	2200      	movs	r2, #0
 80b0096:	7c00      	ldrb	r0, [r0, #16]
 80b0098:	f7ff be6c 	b.w	80afd74 <HAL_I2C_Write_Data>

080b009c <_ZN7TwoWire9availableEv>:
// must be called in:
// slave rx event callback
// or after requestFrom(address, numBytes)
int TwoWire::available(void)
{
  return HAL_I2C_Available_Data(_i2c, NULL);
 80b009c:	2100      	movs	r1, #0
 80b009e:	7c00      	ldrb	r0, [r0, #16]
 80b00a0:	f7ff be70 	b.w	80afd84 <HAL_I2C_Available_Data>

080b00a4 <_ZN7TwoWire4readEv>:
// must be called in:
// slave rx event callback
// or after requestFrom(address, numBytes)
int TwoWire::read(void)
{
  return HAL_I2C_Read_Data(_i2c, NULL);
 80b00a4:	2100      	movs	r1, #0
 80b00a6:	7c00      	ldrb	r0, [r0, #16]
 80b00a8:	f7ff be74 	b.w	80afd94 <HAL_I2C_Read_Data>

080b00ac <_ZN7TwoWire4peekEv>:
// must be called in:
// slave rx event callback
// or after requestFrom(address, numBytes)
int TwoWire::peek(void)
{
  return HAL_I2C_Peek_Data(_i2c, NULL);
 80b00ac:	2100      	movs	r1, #0
 80b00ae:	7c00      	ldrb	r0, [r0, #16]
 80b00b0:	f7ff be78 	b.w	80afda4 <HAL_I2C_Peek_Data>

080b00b4 <_ZN7TwoWire5flushEv>:
}

void TwoWire::flush(void)
{
  HAL_I2C_Flush_Data(_i2c, NULL);
 80b00b4:	2100      	movs	r1, #0
 80b00b6:	7c00      	ldrb	r0, [r0, #16]
 80b00b8:	f7ff be7c 	b.w	80afdb4 <HAL_I2C_Flush_Data>

080b00bc <_ZN7TwoWireD0Ev>:
 80b00bc:	b510      	push	{r4, lr}
 80b00be:	4604      	mov	r4, r0
 80b00c0:	2114      	movs	r1, #20
 80b00c2:	f000 fc9a 	bl	80b09fa <_ZdlPvj>
 80b00c6:	4620      	mov	r0, r4
 80b00c8:	bd10      	pop	{r4, pc}
	...

080b00cc <_ZN7TwoWireC1E17HAL_I2C_Interface>:
#include "i2c_hal.h"
#include "spark_wiring_thread.h"

// Constructors ////////////////////////////////////////////////////////////////

TwoWire::TwoWire(HAL_I2C_Interface i2c)
 80b00cc:	b510      	push	{r4, lr}
 80b00ce:	4604      	mov	r4, r0
 80b00d0:	4608      	mov	r0, r1
    virtual int available() = 0;
    virtual int read() = 0;
    virtual int peek() = 0;
    virtual void flush() = 0;

    Stream() {_timeout=1000;}
 80b00d2:	f44f 737a 	mov.w	r3, #1000	; 0x3e8
 80b00d6:	60a3      	str	r3, [r4, #8]
 80b00d8:	4b04      	ldr	r3, [pc, #16]	; (80b00ec <_ZN7TwoWireC1E17HAL_I2C_Interface+0x20>)
  protected:
    void setWriteError(int err = 1) { write_error = err; }
    size_t printf_impl(bool newline, const char* format, ...);

  public:
    Print() : write_error(0) {}
 80b00da:	2100      	movs	r1, #0
{
  _i2c = i2c;
 80b00dc:	7420      	strb	r0, [r4, #16]
 80b00de:	6061      	str	r1, [r4, #4]
#include "i2c_hal.h"
#include "spark_wiring_thread.h"

// Constructors ////////////////////////////////////////////////////////////////

TwoWire::TwoWire(HAL_I2C_Interface i2c)
 80b00e0:	6023      	str	r3, [r4, #0]
{
  _i2c = i2c;
  HAL_I2C_Init(_i2c, NULL);
 80b00e2:	f7ff fe77 	bl	80afdd4 <HAL_I2C_Init>

}
 80b00e6:	4620      	mov	r0, r4
 80b00e8:	bd10      	pop	{r4, pc}
 80b00ea:	bf00      	nop
 80b00ec:	080b75b8 	.word	0x080b75b8

080b00f0 <_ZN7TwoWire9isEnabledEv>:
  HAL_I2C_Set_Callback_On_Request(_i2c, function, NULL);
}

bool TwoWire::isEnabled()
{
  return HAL_I2C_Is_Enabled(_i2c, NULL);
 80b00f0:	2100      	movs	r1, #0
 80b00f2:	7c00      	ldrb	r0, [r0, #16]
 80b00f4:	f7ff be66 	b.w	80afdc4 <HAL_I2C_Is_Enabled>

080b00f8 <_ZN9IPAddressD1Ev>:
    IPAddress(uint8_t first_octet, uint8_t second_octet, uint8_t third_octet, uint8_t fourth_octet);
    IPAddress(uint32_t address);
    IPAddress(const uint8_t* address);
    IPAddress(const HAL_IPAddress& address);

    virtual ~IPAddress() {}
 80b00f8:	4770      	bx	lr

080b00fa <_ZN9IPAddressD0Ev>:
 80b00fa:	b510      	push	{r4, lr}
 80b00fc:	4604      	mov	r4, r0
 80b00fe:	2118      	movs	r1, #24
 80b0100:	f000 fc7b 	bl	80b09fa <_ZdlPvj>
 80b0104:	4620      	mov	r0, r4
 80b0106:	bd10      	pop	{r4, pc}

080b0108 <_ZNK9IPAddress7printToER5Print>:
#endif // Wiring_IPv6
	return address.ipv4==that.address.ipv4;
}

size_t IPAddress::printTo(Print& p) const
{
 80b0108:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
 80b010a:	460f      	mov	r7, r1
 80b010c:	f100 0608 	add.w	r6, r0, #8
 80b0110:	1d05      	adds	r5, r0, #4
 80b0112:	2400      	movs	r4, #0
    size_t n = 0;
    for (int i = 0; i < 4; i++)
    {
        if (n)
            n += p.print('.');
        n += p.print((*this)[i], DEC);
 80b0114:	f816 1d01 	ldrb.w	r1, [r6, #-1]!
 80b0118:	220a      	movs	r2, #10
 80b011a:	4638      	mov	r0, r7
 80b011c:	f000 f915 	bl	80b034a <_ZN5Print5printEhi>
#else
#pragma message "HAL_USE_INET_HAL_POSIX is required for IPv6 support in IPAddress::printTo()"
#endif // HAL_USE_INET_HAL_POSIX
#endif // Wiring_IPv6
    size_t n = 0;
    for (int i = 0; i < 4; i++)
 80b0120:	42ae      	cmp	r6, r5
    {
        if (n)
            n += p.print('.');
        n += p.print((*this)[i], DEC);
 80b0122:	4404      	add	r4, r0
#else
#pragma message "HAL_USE_INET_HAL_POSIX is required for IPv6 support in IPAddress::printTo()"
#endif // HAL_USE_INET_HAL_POSIX
#endif // Wiring_IPv6
    size_t n = 0;
    for (int i = 0; i < 4; i++)
 80b0124:	d007      	beq.n	80b0136 <_ZNK9IPAddress7printToER5Print+0x2e>
    {
        if (n)
 80b0126:	2c00      	cmp	r4, #0
 80b0128:	d0f4      	beq.n	80b0114 <_ZNK9IPAddress7printToER5Print+0xc>
            n += p.print('.');
 80b012a:	212e      	movs	r1, #46	; 0x2e
 80b012c:	4638      	mov	r0, r7
 80b012e:	f000 f8df 	bl	80b02f0 <_ZN5Print5printEc>
 80b0132:	4404      	add	r4, r0
 80b0134:	e7ee      	b.n	80b0114 <_ZNK9IPAddress7printToER5Print+0xc>
        n += p.print((*this)[i], DEC);
    }
    return n;
}
 80b0136:	4620      	mov	r0, r4
 80b0138:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
	...

080b013c <_ZN9IPAddressC1Ev>:

#if HAL_USE_INET_HAL_POSIX
#include <arpa/inet.h>
#endif // HAL_USE_INET_HAL_POSIX

IPAddress::IPAddress()
 80b013c:	b510      	push	{r4, lr}
 80b013e:	4604      	mov	r4, r0
 80b0140:	4b04      	ldr	r3, [pc, #16]	; (80b0154 <_ZN9IPAddressC1Ev+0x18>)
        return address;
    }

    virtual size_t printTo(Print& p) const;

    void clear() { memset(&address, 0, sizeof (address)); }
 80b0142:	2211      	movs	r2, #17
 80b0144:	f840 3b04 	str.w	r3, [r0], #4
 80b0148:	2100      	movs	r1, #0
 80b014a:	f003 fcc9 	bl	80b3ae0 <memset>
{
    clear();
}
 80b014e:	4620      	mov	r0, r4
 80b0150:	bd10      	pop	{r4, pc}
 80b0152:	bf00      	nop
 80b0154:	080b75e0 	.word	0x080b75e0

080b0158 <_ZN9IPAddressC1ERK16_HAL_IPAddress_t>:

IPAddress::IPAddress(const HAL_IPAddress& address)
 80b0158:	4603      	mov	r3, r0
 80b015a:	4a07      	ldr	r2, [pc, #28]	; (80b0178 <_ZN9IPAddressC1ERK16_HAL_IPAddress_t+0x20>)
 80b015c:	b510      	push	{r4, lr}
 80b015e:	f843 2b04 	str.w	r2, [r3], #4
{
    memcpy(&this->address, &address, sizeof(address));
 80b0162:	f101 0210 	add.w	r2, r1, #16
 80b0166:	f851 4b04 	ldr.w	r4, [r1], #4
 80b016a:	4291      	cmp	r1, r2
 80b016c:	f843 4b04 	str.w	r4, [r3], #4
 80b0170:	d1f9      	bne.n	80b0166 <_ZN9IPAddressC1ERK16_HAL_IPAddress_t+0xe>
 80b0172:	780a      	ldrb	r2, [r1, #0]
 80b0174:	701a      	strb	r2, [r3, #0]
}
 80b0176:	bd10      	pop	{r4, pc}
 80b0178:	080b75e0 	.word	0x080b75e0

080b017c <_ZN9IPAddressC1Em>:
IPAddress::IPAddress(uint8_t first_octet, uint8_t second_octet, uint8_t third_octet, uint8_t fourth_octet)
{
    set_ipv4(first_octet, second_octet, third_octet, fourth_octet);
}

IPAddress::IPAddress(uint32_t address)
 80b017c:	4a02      	ldr	r2, [pc, #8]	; (80b0188 <_ZN9IPAddressC1Em+0xc>)
    return *this;
}

IPAddress& IPAddress::operator=(uint32_t ipv4)
{
    address.ipv4 = ipv4;
 80b017e:	6041      	str	r1, [r0, #4]
IPAddress::IPAddress(uint8_t first_octet, uint8_t second_octet, uint8_t third_octet, uint8_t fourth_octet)
{
    set_ipv4(first_octet, second_octet, third_octet, fourth_octet);
}

IPAddress::IPAddress(uint32_t address)
 80b0180:	6002      	str	r2, [r0, #0]
        return &address;
    }

    inline void setVersion(uint8_t version) {
#if HAL_IPv6
        address.v = version;
 80b0182:	2204      	movs	r2, #4
 80b0184:	7502      	strb	r2, [r0, #20]
{
    *this = address;
}
 80b0186:	4770      	bx	lr
 80b0188:	080b75e0 	.word	0x080b75e0

080b018c <_ZN9IPAddress8set_ipv4Ehhhh>:
    return address.ipv4!=0;
#endif
}

void IPAddress::set_ipv4(uint8_t b0, uint8_t b1, uint8_t b2, uint8_t b3)
{
 80b018c:	b510      	push	{r4, lr}
    address.ipv4 = b0<<24 | b1 << 16 | b2 << 8 | b3;
 80b018e:	f89d 4008 	ldrb.w	r4, [sp, #8]
 80b0192:	ea44 2303 	orr.w	r3, r4, r3, lsl #8
 80b0196:	ea43 4202 	orr.w	r2, r3, r2, lsl #16
 80b019a:	ea42 6101 	orr.w	r1, r2, r1, lsl #24
 80b019e:	2304      	movs	r3, #4
 80b01a0:	6041      	str	r1, [r0, #4]
 80b01a2:	7503      	strb	r3, [r0, #20]
 80b01a4:	bd10      	pop	{r4, pc}
	...

080b01a8 <_ZN9IPAddressC1Ehhhh>:
{
    memcpy(&this->address, &address, sizeof(address));
}


IPAddress::IPAddress(uint8_t first_octet, uint8_t second_octet, uint8_t third_octet, uint8_t fourth_octet)
 80b01a8:	b537      	push	{r0, r1, r2, r4, r5, lr}
 80b01aa:	4d04      	ldr	r5, [pc, #16]	; (80b01bc <_ZN9IPAddressC1Ehhhh+0x14>)
 80b01ac:	6005      	str	r5, [r0, #0]
{
    set_ipv4(first_octet, second_octet, third_octet, fourth_octet);
 80b01ae:	f89d 5018 	ldrb.w	r5, [sp, #24]
 80b01b2:	9500      	str	r5, [sp, #0]
 80b01b4:	f7ff ffea 	bl	80b018c <_ZN9IPAddress8set_ipv4Ehhhh>
}
 80b01b8:	b003      	add	sp, #12
 80b01ba:	bd30      	pop	{r4, r5, pc}
 80b01bc:	080b75e0 	.word	0x080b75e0

080b01c0 <_GLOBAL__sub_I__ZN5spark3LogE>:
    // This handler doesn't support direct logging
}

// spark::Logger
inline spark::Logger::Logger(const char *name) :
        name_(name) {
 80b01c0:	4a01      	ldr	r2, [pc, #4]	; (80b01c8 <_GLOBAL__sub_I__ZN5spark3LogE+0x8>)
 80b01c2:	4b02      	ldr	r3, [pc, #8]	; (80b01cc <_GLOBAL__sub_I__ZN5spark3LogE+0xc>)
 80b01c4:	601a      	str	r2, [r3, #0]
 80b01c6:	4770      	bx	lr
 80b01c8:	080b765d 	.word	0x080b765d
 80b01cc:	2000265c 	.word	0x2000265c

080b01d0 <_ZN5spark12NetworkClass7connectEj>:
        return Network;
    }
}

void NetworkClass::connect(unsigned flags) {
    network_connect(*this, flags, 0, nullptr);
 80b01d0:	2300      	movs	r3, #0
 80b01d2:	461a      	mov	r2, r3
 80b01d4:	6840      	ldr	r0, [r0, #4]
 80b01d6:	f7ff becb 	b.w	80aff70 <network_connect>

080b01da <_ZN5spark12NetworkClass10disconnectEv>:
}

void NetworkClass::disconnect() {
    network_disconnect(*this, NETWORK_DISCONNECT_REASON_USER, nullptr);
 80b01da:	2200      	movs	r2, #0
 80b01dc:	2102      	movs	r1, #2
 80b01de:	6840      	ldr	r0, [r0, #4]
 80b01e0:	f7ff bed6 	b.w	80aff90 <network_disconnect>

080b01e4 <_ZN5spark12NetworkClass10connectingEv>:
}

bool NetworkClass::connecting() {
    return network_connecting(*this, 0, nullptr);
 80b01e4:	2200      	movs	r2, #0
 80b01e6:	4611      	mov	r1, r2
 80b01e8:	6840      	ldr	r0, [r0, #4]
 80b01ea:	f7ff bec9 	b.w	80aff80 <network_connecting>

080b01ee <_ZN5spark12NetworkClass5readyEv>:
}

bool NetworkClass::ready() {
    return network_ready(*this, 0, nullptr);
 80b01ee:	2200      	movs	r2, #0
 80b01f0:	4611      	mov	r1, r2
 80b01f2:	6840      	ldr	r0, [r0, #4]
 80b01f4:	f7ff bed4 	b.w	80affa0 <network_ready>

080b01f8 <_ZN5spark12NetworkClass2onEv>:
}

void NetworkClass::on() {
    network_on(*this, 0, 0, nullptr);
 80b01f8:	2300      	movs	r3, #0
 80b01fa:	461a      	mov	r2, r3
 80b01fc:	4619      	mov	r1, r3
 80b01fe:	6840      	ldr	r0, [r0, #4]
 80b0200:	f7ff bed6 	b.w	80affb0 <network_on>

080b0204 <_ZN5spark12NetworkClass3offEv>:
}

void NetworkClass::off() {
    network_off(*this, 0, 0, nullptr);
 80b0204:	2300      	movs	r3, #0
 80b0206:	461a      	mov	r2, r3
 80b0208:	4619      	mov	r1, r3
 80b020a:	6840      	ldr	r0, [r0, #4]
 80b020c:	f7ff bed8 	b.w	80affc0 <network_off>

080b0210 <_ZN5spark12NetworkClass6listenEb>:
}

void NetworkClass::listen(bool begin) {
    network_listen(*this, begin ? 0 : 1, nullptr);
 80b0210:	2200      	movs	r2, #0
 80b0212:	f081 0101 	eor.w	r1, r1, #1
 80b0216:	6840      	ldr	r0, [r0, #4]
 80b0218:	f7ff beda 	b.w	80affd0 <network_listen>

080b021c <_ZN5spark12NetworkClass16setListenTimeoutEt>:
}

void NetworkClass::setListenTimeout(uint16_t timeout) {
    network_set_listen_timeout(*this, timeout, nullptr);
 80b021c:	2200      	movs	r2, #0
 80b021e:	6840      	ldr	r0, [r0, #4]
 80b0220:	f7ff bee6 	b.w	80afff0 <network_set_listen_timeout>

080b0224 <_ZN5spark12NetworkClass16getListenTimeoutEv>:
}

uint16_t NetworkClass::getListenTimeout() {
    return network_get_listen_timeout(*this, 0, nullptr);
 80b0224:	2200      	movs	r2, #0
 80b0226:	4611      	mov	r1, r2
 80b0228:	6840      	ldr	r0, [r0, #4]
 80b022a:	f7ff bee9 	b.w	80b0000 <network_get_listen_timeout>

080b022e <_ZN5spark12NetworkClass9listeningEv>:
}

bool NetworkClass::listening() {
    return network_listening(*this, 0, nullptr);
 80b022e:	2200      	movs	r2, #0
 80b0230:	4611      	mov	r1, r2
 80b0232:	6840      	ldr	r0, [r0, #4]
 80b0234:	f7ff bed4 	b.w	80affe0 <network_listening>

080b0238 <_ZN5spark12NetworkClass7resolveEPKc>:
}

IPAddress NetworkClass::resolve(const char* name) {
 80b0238:	b570      	push	{r4, r5, r6, lr}
 80b023a:	4616      	mov	r6, r2
 80b023c:	b08e      	sub	sp, #56	; 0x38
 80b023e:	4604      	mov	r4, r0
    IPAddress addr;
 80b0240:	a808      	add	r0, sp, #32
 80b0242:	f7ff ff7b 	bl	80b013c <_ZN9IPAddressC1Ev>
    }

#if !HAL_USE_INET_HAL_POSIX
    IPAddress resolve(const char* name)
    {
        HAL_IPAddress ip = {};
 80b0246:	2211      	movs	r2, #17
 80b0248:	2100      	movs	r1, #0
 80b024a:	a803      	add	r0, sp, #12
 80b024c:	f003 fc48 	bl	80b3ae0 <memset>
        return (inet_gethostbyname(name, strlen(name), &ip, *this, NULL) != 0) ?
 80b0250:	4630      	mov	r0, r6
 80b0252:	f003 fc7f 	bl	80b3b54 <strlen>
 80b0256:	2500      	movs	r5, #0
 80b0258:	4b0a      	ldr	r3, [pc, #40]	; (80b0284 <_ZN5spark12NetworkClass7resolveEPKc+0x4c>)
 80b025a:	9500      	str	r5, [sp, #0]
 80b025c:	b281      	uxth	r1, r0
 80b025e:	685b      	ldr	r3, [r3, #4]
 80b0260:	aa03      	add	r2, sp, #12
 80b0262:	4630      	mov	r0, r6
 80b0264:	f7ff fe48 	bl	80afef8 <inet_gethostbyname>
                IPAddress(uint32_t(0)) : IPAddress(ip);
 80b0268:	b120      	cbz	r0, 80b0274 <_ZN5spark12NetworkClass7resolveEPKc+0x3c>
 80b026a:	4629      	mov	r1, r5
 80b026c:	4620      	mov	r0, r4
 80b026e:	f7ff ff85 	bl	80b017c <_ZN9IPAddressC1Em>
 80b0272:	e003      	b.n	80b027c <_ZN5spark12NetworkClass7resolveEPKc+0x44>
 80b0274:	a903      	add	r1, sp, #12
 80b0276:	4620      	mov	r0, r4
 80b0278:	f7ff ff6e 	bl	80b0158 <_ZN9IPAddressC1ERK16_HAL_IPAddress_t>
    return Cellular.resolve(name);
#endif // Wiring_Cellular

#endif // HAL_USE_INET_HAL_POSIX
    return addr;
}
 80b027c:	4620      	mov	r0, r4
 80b027e:	b00e      	add	sp, #56	; 0x38
 80b0280:	bd70      	pop	{r4, r5, r6, pc}
 80b0282:	bf00      	nop
 80b0284:	20002ae0 	.word	0x20002ae0

080b0288 <_GLOBAL__sub_I__ZN5spark7NetworkE>:
    static NetworkClass& from(network_interface_t nif);

    virtual IPAddress resolve(const char* name);

    explicit NetworkClass(network_interface_t iface)
            : iface_(iface) {
 80b0288:	4b02      	ldr	r3, [pc, #8]	; (80b0294 <_GLOBAL__sub_I__ZN5spark7NetworkE+0xc>)
 80b028a:	4a03      	ldr	r2, [pc, #12]	; (80b0298 <_GLOBAL__sub_I__ZN5spark7NetworkE+0x10>)
 80b028c:	601a      	str	r2, [r3, #0]
 80b028e:	2200      	movs	r2, #0
 80b0290:	605a      	str	r2, [r3, #4]
 80b0292:	4770      	bx	lr
 80b0294:	20002660 	.word	0x20002660
 80b0298:	080b76c8 	.word	0x080b76c8

080b029c <_ZN5Print5writeEPKhj>:

// Public Methods //////////////////////////////////////////////////////////////

/* default implementation: may be overridden */
size_t Print::write(const uint8_t *buffer, size_t size)
{
 80b029c:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
 80b029e:	4606      	mov	r6, r0
 80b02a0:	460d      	mov	r5, r1
 80b02a2:	188f      	adds	r7, r1, r2
  size_t n = 0;
 80b02a4:	2400      	movs	r4, #0
  while (size--) {
 80b02a6:	42bd      	cmp	r5, r7
 80b02a8:	d00c      	beq.n	80b02c4 <_ZN5Print5writeEPKhj+0x28>
     int chunk = write(*buffer++);
 80b02aa:	6833      	ldr	r3, [r6, #0]
 80b02ac:	f815 1b01 	ldrb.w	r1, [r5], #1
 80b02b0:	689b      	ldr	r3, [r3, #8]
 80b02b2:	4630      	mov	r0, r6
 80b02b4:	4798      	blx	r3
     if (chunk>=0)
 80b02b6:	2800      	cmp	r0, #0
 80b02b8:	db01      	blt.n	80b02be <_ZN5Print5writeEPKhj+0x22>
         n += chunk;
 80b02ba:	4404      	add	r4, r0

/* default implementation: may be overridden */
size_t Print::write(const uint8_t *buffer, size_t size)
{
  size_t n = 0;
  while (size--) {
 80b02bc:	e7f3      	b.n	80b02a6 <_ZN5Print5writeEPKhj+0xa>
     int chunk = write(*buffer++);
 80b02be:	2c00      	cmp	r4, #0
 80b02c0:	bf08      	it	eq
 80b02c2:	4604      	moveq	r4, r0
             n = chunk;
         break;
     }
  }
  return n;
}
 80b02c4:	4620      	mov	r0, r4
 80b02c6:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}

080b02c8 <_ZN5Print5writeEPKc>:

    int getWriteError() { return write_error; }
    void clearWriteError() { setWriteError(0); }

    virtual size_t write(uint8_t) = 0;
    size_t write(const char *str) {
 80b02c8:	b570      	push	{r4, r5, r6, lr}
 80b02ca:	4605      	mov	r5, r0
      if (str == NULL) return 0;
 80b02cc:	460c      	mov	r4, r1
      return write((const uint8_t *)str, strlen(str));
    }
 80b02ce:	4608      	mov	r0, r1
    int getWriteError() { return write_error; }
    void clearWriteError() { setWriteError(0); }

    virtual size_t write(uint8_t) = 0;
    size_t write(const char *str) {
      if (str == NULL) return 0;
 80b02d0:	b149      	cbz	r1, 80b02e6 <_ZN5Print5writeEPKc+0x1e>
      return write((const uint8_t *)str, strlen(str));
 80b02d2:	f003 fc3f 	bl	80b3b54 <strlen>
 80b02d6:	682b      	ldr	r3, [r5, #0]
 80b02d8:	4602      	mov	r2, r0
 80b02da:	4621      	mov	r1, r4
 80b02dc:	4628      	mov	r0, r5
    }
 80b02de:	e8bd 4070 	ldmia.w	sp!, {r4, r5, r6, lr}
    void clearWriteError() { setWriteError(0); }

    virtual size_t write(uint8_t) = 0;
    size_t write(const char *str) {
      if (str == NULL) return 0;
      return write((const uint8_t *)str, strlen(str));
 80b02e2:	68db      	ldr	r3, [r3, #12]
 80b02e4:	4718      	bx	r3
    }
 80b02e6:	bd70      	pop	{r4, r5, r6, pc}

080b02e8 <_ZN5Print5printEPKc>:
 80b02e8:	b508      	push	{r3, lr}
 80b02ea:	f7ff ffed 	bl	80b02c8 <_ZN5Print5writeEPKc>
 80b02ee:	bd08      	pop	{r3, pc}

080b02f0 <_ZN5Print5printEc>:
  return write(str);
}

size_t Print::print(char c)
{
  return write(c);
 80b02f0:	6803      	ldr	r3, [r0, #0]
 80b02f2:	689b      	ldr	r3, [r3, #8]
 80b02f4:	4718      	bx	r3

080b02f6 <_ZN5Print11printNumberEmh>:
  char *str = &buf[sizeof(buf) - 1];

  *str = '\0';

  // prevent crash if called with base == 1
  if (base < 2) base = 10;
 80b02f6:	2a01      	cmp	r2, #1
 80b02f8:	bf98      	it	ls
 80b02fa:	220a      	movls	r2, #10
  return println(reinterpret_cast<const char*>(str));
}

// Private Methods /////////////////////////////////////////////////////////////

size_t Print::printNumber(unsigned long n, uint8_t base) {
 80b02fc:	b530      	push	{r4, r5, lr}
 80b02fe:	460b      	mov	r3, r1
 80b0300:	b08b      	sub	sp, #44	; 0x2c
  char buf[8 * sizeof(long) + 1]; // Assumes 8-bit chars plus zero byte.
  char *str = &buf[sizeof(buf) - 1];

  *str = '\0';
 80b0302:	2100      	movs	r1, #0
 80b0304:	f88d 1024 	strb.w	r1, [sp, #36]	; 0x24
 80b0308:	f10d 0423 	add.w	r4, sp, #35	; 0x23
  // prevent crash if called with base == 1
  if (base < 2) base = 10;

  do {
    unsigned long m = n;
    n /= base;
 80b030c:	fbb3 f5f2 	udiv	r5, r3, r2
    char c = m - base * n;
 80b0310:	fb05 3312 	mls	r3, r5, r2, r3
 80b0314:	b2db      	uxtb	r3, r3
    *--str = c < 10 ? c + '0' : c + 'A' - 10;
 80b0316:	2b09      	cmp	r3, #9
 80b0318:	bf94      	ite	ls
 80b031a:	3330      	addls	r3, #48	; 0x30
 80b031c:	3337      	addhi	r3, #55	; 0x37
 80b031e:	b2db      	uxtb	r3, r3
 80b0320:	4621      	mov	r1, r4
 80b0322:	f804 3901 	strb.w	r3, [r4], #-1
 80b0326:	462b      	mov	r3, r5
  *str = '\0';

  // prevent crash if called with base == 1
  if (base < 2) base = 10;

  do {
 80b0328:	2d00      	cmp	r5, #0
 80b032a:	d1ef      	bne.n	80b030c <_ZN5Print11printNumberEmh+0x16>
    n /= base;
    char c = m - base * n;
    *--str = c < 10 ? c + '0' : c + 'A' - 10;
  } while(n);

  return write(str);
 80b032c:	f7ff ffcc 	bl	80b02c8 <_ZN5Print5writeEPKc>
}
 80b0330:	b00b      	add	sp, #44	; 0x2c
 80b0332:	bd30      	pop	{r4, r5, pc}

080b0334 <_ZN5Print5printEmi>:
    return printNumber(n, base);
  }
}

size_t Print::print(unsigned long n, int base)
{
 80b0334:	b410      	push	{r4}
  if (base == 0) return write(n);
 80b0336:	b922      	cbnz	r2, 80b0342 <_ZN5Print5printEmi+0xe>
 80b0338:	6803      	ldr	r3, [r0, #0]
 80b033a:	b2c9      	uxtb	r1, r1
 80b033c:	689b      	ldr	r3, [r3, #8]
  else return printNumber(n, base);
}
 80b033e:	bc10      	pop	{r4}
  }
}

size_t Print::print(unsigned long n, int base)
{
  if (base == 0) return write(n);
 80b0340:	4718      	bx	r3
  else return printNumber(n, base);
 80b0342:	b2d2      	uxtb	r2, r2
}
 80b0344:	bc10      	pop	{r4}
}

size_t Print::print(unsigned long n, int base)
{
  if (base == 0) return write(n);
  else return printNumber(n, base);
 80b0346:	f7ff bfd6 	b.w	80b02f6 <_ZN5Print11printNumberEmh>

080b034a <_ZN5Print5printEhi>:
  return write(c);
}

size_t Print::print(unsigned char b, int base)
{
  return print((unsigned long) b, base);
 80b034a:	f7ff bff3 	b.w	80b0334 <_ZN5Print5printEmi>

080b034e <_ZN8RGBClassD1Ev>:
#include "rgbled.h"

typedef void (raw_rgb_change_handler_t)(uint8_t, uint8_t, uint8_t);
typedef std::function<raw_rgb_change_handler_t> wiring_rgb_change_handler_t;

class RGBClass {
 80b034e:	b510      	push	{r4, lr}
 80b0350:	4604      	mov	r4, r0
   *  @ingroup functors
   *
   *  Polymorphic function wrapper.
   */
  template<typename _Res, typename... _ArgTypes>
    class function<_Res(_ArgTypes...)>
 80b0352:	f7ff fe85 	bl	80b0060 <_ZNSt14_Function_baseD1Ev>
 80b0356:	4620      	mov	r0, r4
 80b0358:	bd10      	pop	{r4, pc}
	...

080b035c <_GLOBAL__sub_I_RGB>:
	{
	  _Base::_M_init_functor(__functor, std::__addressof(__f.get()));
	}
      };

    _Function_base() : _M_manager(nullptr) { }
 80b035c:	4803      	ldr	r0, [pc, #12]	; (80b036c <_GLOBAL__sub_I_RGB+0x10>)
 80b035e:	2300      	movs	r3, #0
 80b0360:	6083      	str	r3, [r0, #8]
#include "spark_wiring_rgb.h"
#include "spark_wiring_interrupts.h"

#include "core_hal.h"

RGBClass RGB;
 80b0362:	4a03      	ldr	r2, [pc, #12]	; (80b0370 <_GLOBAL__sub_I_RGB+0x14>)
 80b0364:	4903      	ldr	r1, [pc, #12]	; (80b0374 <_GLOBAL__sub_I_RGB+0x18>)
 80b0366:	f000 bb43 	b.w	80b09f0 <__aeabi_atexit>
 80b036a:	bf00      	nop
 80b036c:	20002668 	.word	0x20002668
 80b0370:	20000594 	.word	0x20000594
 80b0374:	080b034f 	.word	0x080b034f

080b0378 <_ZN8SPIClassD1Ev>:
  Mutex mutex_;
#endif

public:
  SPIClass(HAL_SPI_Interface spi);
  virtual ~SPIClass() {};
 80b0378:	4770      	bx	lr

080b037a <_ZN8SPIClassD0Ev>:
 80b037a:	b510      	push	{r4, lr}
 80b037c:	4604      	mov	r4, r0
 80b037e:	2110      	movs	r1, #16
 80b0380:	f000 fb3b 	bl	80b09fa <_ZdlPvj>
 80b0384:	4620      	mov	r0, r4
 80b0386:	bd10      	pop	{r4, pc}

080b0388 <_ZN8SPIClassC1E17HAL_SPI_Interface>:
  if (!info->enabled || info->default_settings)
    return particle::__SPISettings();
  return particle::__SPISettings(info->clock, info->bit_order, info->data_mode);
}

SPIClass::SPIClass(HAL_SPI_Interface spi)
 80b0388:	b570      	push	{r4, r5, r6, lr}
 80b038a:	4604      	mov	r4, r0
 80b038c:	460e      	mov	r6, r1
 80b038e:	4b07      	ldr	r3, [pc, #28]	; (80b03ac <_ZN8SPIClassC1E17HAL_SPI_Interface+0x24>)
    Mutex(os_mutex_t handle) : handle_(handle) {}

    /**
     * Creates a new mutex.
     */
    Mutex() : handle_(nullptr)
 80b0390:	2500      	movs	r5, #0
 80b0392:	6003      	str	r3, [r0, #0]
 80b0394:	f840 5f0c 	str.w	r5, [r0, #12]!
    {
        os_mutex_create(&handle_);
 80b0398:	f7ff fc9c 	bl	80afcd4 <os_mutex_create>
{
  _spi = spi;
  HAL_SPI_Init(_spi);
 80b039c:	4630      	mov	r0, r6
  return particle::__SPISettings(info->clock, info->bit_order, info->data_mode);
}

SPIClass::SPIClass(HAL_SPI_Interface spi)
{
  _spi = spi;
 80b039e:	7126      	strb	r6, [r4, #4]
  HAL_SPI_Init(_spi);
 80b03a0:	f7ff fd22 	bl	80afde8 <HAL_SPI_Init>
  dividerReference = SPI_CLK_SYSTEM;     // 0 indicates the system clock
 80b03a4:	60a5      	str	r5, [r4, #8]
}
 80b03a6:	4620      	mov	r0, r4
 80b03a8:	bd70      	pop	{r4, r5, r6, pc}
 80b03aa:	bf00      	nop
 80b03ac:	080b76fc 	.word	0x080b76fc

080b03b0 <_ZN8SPIClass9isEnabledEv>:
  //To Do
}

bool SPIClass::isEnabled()
{
  return HAL_SPI_Is_Enabled(_spi);
 80b03b0:	7900      	ldrb	r0, [r0, #4]
 80b03b2:	f7ff bd21 	b.w	80afdf8 <HAL_SPI_Is_Enabled>
	...

080b03b8 <_GLOBAL__sub_I_System>:
    WAKEUP_REASON_RTC = 2,
    WAKEUP_REASON_PIN_OR_RTC = 3
};

struct SleepResult {
    SleepResult() {}
 80b03b8:	f64f 72ff 	movw	r2, #65535	; 0xffff
 80b03bc:	4b03      	ldr	r3, [pc, #12]	; (80b03cc <_GLOBAL__sub_I_System+0x14>)
 80b03be:	2000      	movs	r0, #0
 80b03c0:	7018      	strb	r0, [r3, #0]
 80b03c2:	8058      	strh	r0, [r3, #2]
 80b03c4:	809a      	strh	r2, [r3, #4]

class SystemClass {
public:

    SystemClass(System_Mode_TypeDef mode = DEFAULT) {
        set_system_mode(mode);
 80b03c6:	f7ff bda7 	b.w	80aff18 <set_system_mode>
 80b03ca:	bf00      	nop
 80b03cc:	20002678 	.word	0x20002678

080b03d0 <_GLOBAL__sub_I_TIME_FORMAT_DEFAULT>:
            calendar_time_cache = Convert_UnixTime_To_CalendarTime(unix_time);
            unix_time_cache = unix_time;
    }
}

const char* TimeClass::format_spec = TIME_FORMAT_DEFAULT;
 80b03d0:	4b02      	ldr	r3, [pc, #8]	; (80b03dc <_GLOBAL__sub_I_TIME_FORMAT_DEFAULT+0xc>)
 80b03d2:	681a      	ldr	r2, [r3, #0]
 80b03d4:	4b02      	ldr	r3, [pc, #8]	; (80b03e0 <_GLOBAL__sub_I_TIME_FORMAT_DEFAULT+0x10>)
 80b03d6:	601a      	str	r2, [r3, #0]
 80b03d8:	4770      	bx	lr
 80b03da:	bf00      	nop
 80b03dc:	20000528 	.word	0x20000528
 80b03e0:	20002680 	.word	0x20002680

080b03e4 <_ZN11USARTSerialD1Ev>:
private:
  HAL_USART_Serial _serial;
  bool _blocking;
public:
  USARTSerial(HAL_USART_Serial serial, Ring_Buffer *rx_buffer, Ring_Buffer *tx_buffer);
  virtual ~USARTSerial() {};
 80b03e4:	4770      	bx	lr

080b03e6 <_ZN11USARTSerial14blockOnOverrunEb>:
    HAL_USART_Half_Duplex(_serial, Enable);
}

void USARTSerial::blockOnOverrun(bool block)
{
  _blocking = block;
 80b03e6:	7441      	strb	r1, [r0, #17]
 80b03e8:	4770      	bx	lr

080b03ea <_ZN11USARTSerial17availableForWriteEv>:
}


int USARTSerial::availableForWrite(void)
{
 80b03ea:	b508      	push	{r3, lr}
  return std::max(0, (int)HAL_USART_Available_Data_For_Write(_serial));
 80b03ec:	7c00      	ldrb	r0, [r0, #16]
 80b03ee:	f7ff fd43 	bl	80afe78 <HAL_USART_Available_Data_For_Write>
}
 80b03f2:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
 80b03f6:	bd08      	pop	{r3, pc}

080b03f8 <_ZN11USARTSerial9availableEv>:

int USARTSerial::available(void)
{
 80b03f8:	b508      	push	{r3, lr}
  return std::max(0, (int)HAL_USART_Available_Data(_serial));
 80b03fa:	7c00      	ldrb	r0, [r0, #16]
 80b03fc:	f7ff fd14 	bl	80afe28 <HAL_USART_Available_Data>
}
 80b0400:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
 80b0404:	bd08      	pop	{r3, pc}

080b0406 <_ZN11USARTSerial4peekEv>:

int USARTSerial::peek(void)
{
 80b0406:	b508      	push	{r3, lr}
  return std::max(-1, (int)HAL_USART_Peek_Data(_serial));
 80b0408:	7c00      	ldrb	r0, [r0, #16]
 80b040a:	f7ff fd1d 	bl	80afe48 <HAL_USART_Peek_Data>
}
 80b040e:	ea30 0020 	bics.w	r0, r0, r0, asr #32
 80b0412:	bf28      	it	cs
 80b0414:	f04f 30ff 	movcs.w	r0, #4294967295	; 0xffffffff
 80b0418:	bd08      	pop	{r3, pc}

080b041a <_ZN11USARTSerial4readEv>:

int USARTSerial::read(void)
{
 80b041a:	b508      	push	{r3, lr}
  return std::max(-1, (int)HAL_USART_Read_Data(_serial));
 80b041c:	7c00      	ldrb	r0, [r0, #16]
 80b041e:	f7ff fd0b 	bl	80afe38 <HAL_USART_Read_Data>
}
 80b0422:	ea30 0020 	bics.w	r0, r0, r0, asr #32
 80b0426:	bf28      	it	cs
 80b0428:	f04f 30ff 	movcs.w	r0, #4294967295	; 0xffffffff
 80b042c:	bd08      	pop	{r3, pc}

080b042e <_ZN11USARTSerial5flushEv>:

void USARTSerial::flush()
{
  HAL_USART_Flush_Data(_serial);
 80b042e:	7c00      	ldrb	r0, [r0, #16]
 80b0430:	f7ff bd12 	b.w	80afe58 <HAL_USART_Flush_Data>

080b0434 <_ZN11USARTSerialD0Ev>:
 80b0434:	b510      	push	{r4, lr}
 80b0436:	4604      	mov	r4, r0
 80b0438:	2114      	movs	r1, #20
 80b043a:	f000 fade 	bl	80b09fa <_ZdlPvj>
 80b043e:	4620      	mov	r0, r4
 80b0440:	bd10      	pop	{r4, pc}

080b0442 <_ZN11USARTSerial5writeEh>:
}

size_t USARTSerial::write(uint8_t c)
{
 80b0442:	b570      	push	{r4, r5, r6, lr}
  // attempt a write if blocking, or for non-blocking if there is room.
  if (_blocking || HAL_USART_Available_Data_For_Write(_serial) > 0) {
 80b0444:	7c45      	ldrb	r5, [r0, #17]
{
  HAL_USART_Flush_Data(_serial);
}

size_t USARTSerial::write(uint8_t c)
{
 80b0446:	4604      	mov	r4, r0
 80b0448:	460e      	mov	r6, r1
  // attempt a write if blocking, or for non-blocking if there is room.
  if (_blocking || HAL_USART_Available_Data_For_Write(_serial) > 0) {
 80b044a:	b925      	cbnz	r5, 80b0456 <_ZN11USARTSerial5writeEh+0x14>
 80b044c:	7c00      	ldrb	r0, [r0, #16]
 80b044e:	f7ff fd13 	bl	80afe78 <HAL_USART_Available_Data_For_Write>
 80b0452:	2800      	cmp	r0, #0
 80b0454:	dd05      	ble.n	80b0462 <_ZN11USARTSerial5writeEh+0x20>
    // the HAL always blocks.
	  return HAL_USART_Write_Data(_serial, c);
 80b0456:	4631      	mov	r1, r6
 80b0458:	7c20      	ldrb	r0, [r4, #16]
  }
  return 0;
}
 80b045a:	e8bd 4070 	ldmia.w	sp!, {r4, r5, r6, lr}
size_t USARTSerial::write(uint8_t c)
{
  // attempt a write if blocking, or for non-blocking if there is room.
  if (_blocking || HAL_USART_Available_Data_For_Write(_serial) > 0) {
    // the HAL always blocks.
	  return HAL_USART_Write_Data(_serial, c);
 80b045e:	f7ff bcdb 	b.w	80afe18 <HAL_USART_Write_Data>
  }
  return 0;
}
 80b0462:	4628      	mov	r0, r5
 80b0464:	bd70      	pop	{r4, r5, r6, pc}
	...

080b0468 <_ZN11USARTSerialC1E16HAL_USART_SerialP11Ring_BufferS2_>:
#include "spark_wiring_constants.h"
#include "module_info.h"

// Constructors ////////////////////////////////////////////////////////////////

USARTSerial::USARTSerial(HAL_USART_Serial serial, Ring_Buffer *rx_buffer, Ring_Buffer *tx_buffer)
 80b0468:	b510      	push	{r4, lr}
 80b046a:	4604      	mov	r4, r0
 80b046c:	4608      	mov	r0, r1
 80b046e:	4611      	mov	r1, r2
  protected:
    void setWriteError(int err = 1) { write_error = err; }
    size_t printf_impl(bool newline, const char* format, ...);

  public:
    Print() : write_error(0) {}
 80b0470:	2200      	movs	r2, #0
 80b0472:	6062      	str	r2, [r4, #4]
 80b0474:	f44f 727a 	mov.w	r2, #1000	; 0x3e8
 80b0478:	60a2      	str	r2, [r4, #8]
 80b047a:	4a05      	ldr	r2, [pc, #20]	; (80b0490 <_ZN11USARTSerialC1E16HAL_USART_SerialP11Ring_BufferS2_+0x28>)
{
  _serial = serial;
 80b047c:	7420      	strb	r0, [r4, #16]
#include "spark_wiring_constants.h"
#include "module_info.h"

// Constructors ////////////////////////////////////////////////////////////////

USARTSerial::USARTSerial(HAL_USART_Serial serial, Ring_Buffer *rx_buffer, Ring_Buffer *tx_buffer)
 80b047e:	6022      	str	r2, [r4, #0]
{
  _serial = serial;
  // Default is blocking mode
  _blocking = true;
 80b0480:	2201      	movs	r2, #1
 80b0482:	7462      	strb	r2, [r4, #17]
  HAL_USART_Init(serial, rx_buffer, tx_buffer);
 80b0484:	461a      	mov	r2, r3
 80b0486:	f7ff fcbf 	bl	80afe08 <HAL_USART_Init>
}
 80b048a:	4620      	mov	r0, r4
 80b048c:	bd10      	pop	{r4, pc}
 80b048e:	bf00      	nop
 80b0490:	080b7738 	.word	0x080b7738

080b0494 <_ZN11USARTSerial9isEnabledEv>:
USARTSerial::operator bool() {
  return true;
}

bool USARTSerial::isEnabled() {
  return HAL_USART_Is_Enabled(_serial);
 80b0494:	7c00      	ldrb	r0, [r0, #16]
 80b0496:	f7ff bce7 	b.w	80afe68 <HAL_USART_Is_Enabled>
	...

080b049c <_Z22__fetch_global_Serial1v>:
static Ring_Buffer* serial1_rx_buffer = NULL;
static Ring_Buffer* serial1_tx_buffer = NULL;
#endif

USARTSerial& __fetch_global_Serial1()
{
 80b049c:	b538      	push	{r3, r4, r5, lr}
#if ((MODULE_FUNCTION == MOD_FUNC_USER_PART) || (MODULE_FUNCTION == MOD_FUNC_MONO_FIRMWARE))
	static USARTSerial serial1(HAL_USART_SERIAL1, &serial1_rx_buffer, &serial1_tx_buffer);
 80b049e:	4d0c      	ldr	r5, [pc, #48]	; (80b04d0 <_Z22__fetch_global_Serial1v+0x34>)
 80b04a0:	6829      	ldr	r1, [r5, #0]
 80b04a2:	f011 0401 	ands.w	r4, r1, #1
 80b04a6:	d111      	bne.n	80b04cc <_Z22__fetch_global_Serial1v+0x30>
 80b04a8:	4628      	mov	r0, r5
 80b04aa:	f7ef fe09 	bl	80a00c0 <__cxa_guard_acquire>
 80b04ae:	b168      	cbz	r0, 80b04cc <_Z22__fetch_global_Serial1v+0x30>
 80b04b0:	4a08      	ldr	r2, [pc, #32]	; (80b04d4 <_Z22__fetch_global_Serial1v+0x38>)
 80b04b2:	4621      	mov	r1, r4
 80b04b4:	4b08      	ldr	r3, [pc, #32]	; (80b04d8 <_Z22__fetch_global_Serial1v+0x3c>)
 80b04b6:	4809      	ldr	r0, [pc, #36]	; (80b04dc <_Z22__fetch_global_Serial1v+0x40>)
 80b04b8:	f7ff ffd6 	bl	80b0468 <_ZN11USARTSerialC1E16HAL_USART_SerialP11Ring_BufferS2_>
 80b04bc:	4628      	mov	r0, r5
 80b04be:	f7ef fe04 	bl	80a00ca <__cxa_guard_release>
 80b04c2:	4a07      	ldr	r2, [pc, #28]	; (80b04e0 <_Z22__fetch_global_Serial1v+0x44>)
 80b04c4:	4907      	ldr	r1, [pc, #28]	; (80b04e4 <_Z22__fetch_global_Serial1v+0x48>)
 80b04c6:	4805      	ldr	r0, [pc, #20]	; (80b04dc <_Z22__fetch_global_Serial1v+0x40>)
 80b04c8:	f000 fa92 	bl	80b09f0 <__aeabi_atexit>
    serial1_tx_buffer = new Ring_Buffer();
  }
  static USARTSerial serial1(HAL_USART_SERIAL1, serial1_rx_buffer, serial1_tx_buffer);
#endif
	return serial1;
}
 80b04cc:	4803      	ldr	r0, [pc, #12]	; (80b04dc <_Z22__fetch_global_Serial1v+0x40>)
 80b04ce:	bd38      	pop	{r3, r4, r5, pc}
 80b04d0:	2000271c 	.word	0x2000271c
 80b04d4:	20002720 	.word	0x20002720
 80b04d8:	20002698 	.word	0x20002698
 80b04dc:	20002684 	.word	0x20002684
 80b04e0:	20000594 	.word	0x20000594
 80b04e4:	080b03e5 	.word	0x080b03e5

080b04e8 <_ZN9USBSerial14blockOnOverrunEb>:
  HAL_USB_USART_Flush_Data(_serial);
}

void USBSerial::blockOnOverrun(bool block)
{
  _blocking = block;
 80b04e8:	7441      	strb	r1, [r0, #17]
 80b04ea:	4770      	bx	lr

080b04ec <_ZN9USBSerialD1Ev>:
#include "usb_hal.h"
#include "system_task.h"
#include "spark_wiring_startup.h"
#include "concurrent_hal.h"

class USBSerial : public Stream
 80b04ec:	4770      	bx	lr

080b04ee <_ZN9USBSerial4readEv>:
}


// Read data from buffer
int USBSerial::read()
{
 80b04ee:	b508      	push	{r3, lr}
	return std::max(-1, (int)HAL_USB_USART_Receive_Data(_serial, false));
 80b04f0:	2100      	movs	r1, #0
 80b04f2:	7c00      	ldrb	r0, [r0, #16]
 80b04f4:	f7ff fce8 	bl	80afec8 <HAL_USB_USART_Receive_Data>
}
 80b04f8:	ea30 0020 	bics.w	r0, r0, r0, asr #32
 80b04fc:	bf28      	it	cs
 80b04fe:	f04f 30ff 	movcs.w	r0, #4294967295	; 0xffffffff
 80b0502:	bd08      	pop	{r3, pc}

080b0504 <_ZN9USBSerial4peekEv>:
{
  _blocking = block;
}

int USBSerial::peek()
{
 80b0504:	b508      	push	{r3, lr}
	return std::max(-1, (int)HAL_USB_USART_Receive_Data(_serial, true));
 80b0506:	2101      	movs	r1, #1
 80b0508:	7c00      	ldrb	r0, [r0, #16]
 80b050a:	f7ff fcdd 	bl	80afec8 <HAL_USB_USART_Receive_Data>
}
 80b050e:	ea30 0020 	bics.w	r0, r0, r0, asr #32
 80b0512:	bf28      	it	cs
 80b0514:	f04f 30ff 	movcs.w	r0, #4294967295	; 0xffffffff
 80b0518:	bd08      	pop	{r3, pc}

080b051a <_ZN9USBSerial17availableForWriteEv>:
{
	return std::max(-1, (int)HAL_USB_USART_Receive_Data(_serial, false));
}

int USBSerial::availableForWrite()
{
 80b051a:	b508      	push	{r3, lr}
  return std::max(0, (int)HAL_USB_USART_Available_Data_For_Write(_serial));
 80b051c:	7c00      	ldrb	r0, [r0, #16]
 80b051e:	f7ff fccb 	bl	80afeb8 <HAL_USB_USART_Available_Data_For_Write>
}
 80b0522:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
 80b0526:	bd08      	pop	{r3, pc}

080b0528 <_ZN9USBSerial9availableEv>:

int USBSerial::available()
{
 80b0528:	b508      	push	{r3, lr}
	return std::max(0, (int)HAL_USB_USART_Available_Data(_serial));
 80b052a:	7c00      	ldrb	r0, [r0, #16]
 80b052c:	f7ff fcbc 	bl	80afea8 <HAL_USB_USART_Available_Data>
}
 80b0530:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
 80b0534:	bd08      	pop	{r3, pc}

080b0536 <_ZN9USBSerial5flushEv>:
  return 0;
}

void USBSerial::flush()
{
  HAL_USB_USART_Flush_Data(_serial);
 80b0536:	7c00      	ldrb	r0, [r0, #16]
 80b0538:	f7ff bcd6 	b.w	80afee8 <HAL_USB_USART_Flush_Data>

080b053c <_ZN9USBSerialD0Ev>:
 80b053c:	b510      	push	{r4, lr}
 80b053e:	4604      	mov	r4, r0
 80b0540:	2114      	movs	r1, #20
 80b0542:	f000 fa5a 	bl	80b09fa <_ZdlPvj>
 80b0546:	4620      	mov	r0, r4
 80b0548:	bd10      	pop	{r4, pc}

080b054a <_ZN9USBSerial5writeEh>:
{
	return std::max(0, (int)HAL_USB_USART_Available_Data(_serial));
}

size_t USBSerial::write(uint8_t byte)
{
 80b054a:	b538      	push	{r3, r4, r5, lr}
 80b054c:	4604      	mov	r4, r0
  if (HAL_USB_USART_Available_Data_For_Write(_serial) > 0 || _blocking) {
 80b054e:	7c00      	ldrb	r0, [r0, #16]
{
	return std::max(0, (int)HAL_USB_USART_Available_Data(_serial));
}

size_t USBSerial::write(uint8_t byte)
{
 80b0550:	460d      	mov	r5, r1
  if (HAL_USB_USART_Available_Data_For_Write(_serial) > 0 || _blocking) {
 80b0552:	f7ff fcb1 	bl	80afeb8 <HAL_USB_USART_Available_Data_For_Write>
 80b0556:	2800      	cmp	r0, #0
 80b0558:	dc01      	bgt.n	80b055e <_ZN9USBSerial5writeEh+0x14>
 80b055a:	7c60      	ldrb	r0, [r4, #17]
 80b055c:	b128      	cbz	r0, 80b056a <_ZN9USBSerial5writeEh+0x20>
    return std::max(0, (int)HAL_USB_USART_Send_Data(_serial, byte));
 80b055e:	4629      	mov	r1, r5
 80b0560:	7c20      	ldrb	r0, [r4, #16]
 80b0562:	f7ff fcb9 	bl	80afed8 <HAL_USB_USART_Send_Data>
 80b0566:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
  }
  return 0;
}
 80b056a:	bd38      	pop	{r3, r4, r5, pc}

080b056c <_ZN9USBSerialC1E20HAL_USB_USART_SerialRK20HAL_USB_USART_Config>:

  HAL_USB_USART_Config conf = acquireSerialBuffer();
  HAL_USB_USART_Init(_serial, &conf);
}

USBSerial::USBSerial(HAL_USB_USART_Serial serial, const HAL_USB_USART_Config& conf)
 80b056c:	b510      	push	{r4, lr}
 80b056e:	4604      	mov	r4, r0
 80b0570:	2300      	movs	r3, #0
 80b0572:	6063      	str	r3, [r4, #4]
 80b0574:	f44f 737a 	mov.w	r3, #1000	; 0x3e8
 80b0578:	60a3      	str	r3, [r4, #8]
 80b057a:	4b05      	ldr	r3, [pc, #20]	; (80b0590 <_ZN9USBSerialC1E20HAL_USB_USART_SerialRK20HAL_USB_USART_Config+0x24>)
 80b057c:	4608      	mov	r0, r1
 80b057e:	6023      	str	r3, [r4, #0]
{
  _serial = serial;
  _blocking = true;
 80b0580:	2301      	movs	r3, #1
  HAL_USB_USART_Init(_serial, &conf);
}

USBSerial::USBSerial(HAL_USB_USART_Serial serial, const HAL_USB_USART_Config& conf)
{
  _serial = serial;
 80b0582:	7421      	strb	r1, [r4, #16]
  _blocking = true;
 80b0584:	7463      	strb	r3, [r4, #17]

  HAL_USB_USART_Init(_serial, &conf);
 80b0586:	4611      	mov	r1, r2
 80b0588:	f7ff fc7e 	bl	80afe88 <HAL_USB_USART_Init>
}
 80b058c:	4620      	mov	r0, r4
 80b058e:	bd10      	pop	{r4, pc}
 80b0590:	080b7768 	.word	0x080b7768

080b0594 <_ZN9USBSerial5beginEl>:
// Public methods
//

void USBSerial::begin(long speed)
{
    HAL_USB_USART_Begin(_serial, speed, NULL);
 80b0594:	2200      	movs	r2, #0
 80b0596:	7c00      	ldrb	r0, [r0, #16]
 80b0598:	f7ff bc7e 	b.w	80afe98 <HAL_USB_USART_Begin>

080b059c <_Z19acquireSerialBufferv>:

// Preinstantiate Objects //////////////////////////////////////////////////////
#ifdef SPARK_USB_SERIAL

HAL_USB_USART_Config __attribute__((weak)) acquireSerialBuffer()
{
 80b059c:	b510      	push	{r4, lr}
 80b059e:	4604      	mov	r4, r0
  HAL_USB_USART_Config conf = {0};
 80b05a0:	2214      	movs	r2, #20
 80b05a2:	2100      	movs	r1, #0
 80b05a4:	f003 fa9c 	bl	80b3ae0 <memset>

#if defined(USB_SERIAL_USERSPACE_BUFFERS) && ((MODULE_FUNCTION == MOD_FUNC_USER_PART) || (MODULE_FUNCTION == MOD_FUNC_MONO_FIRMWARE))
  static uint8_t serial_rx_buffer[USB_RX_BUFFER_SIZE];
  static uint8_t serial_tx_buffer[USB_TX_BUFFER_SIZE];

  conf.rx_buffer = serial_rx_buffer;
 80b05a8:	4b05      	ldr	r3, [pc, #20]	; (80b05c0 <_Z19acquireSerialBufferv+0x24>)
  conf.rx_buffer_size = USB_RX_BUFFER_SIZE;
  conf.tx_buffer_size = USB_TX_BUFFER_SIZE;
#endif

  return conf;
}
 80b05aa:	4620      	mov	r0, r4

#if defined(USB_SERIAL_USERSPACE_BUFFERS) && ((MODULE_FUNCTION == MOD_FUNC_USER_PART) || (MODULE_FUNCTION == MOD_FUNC_MONO_FIRMWARE))
  static uint8_t serial_rx_buffer[USB_RX_BUFFER_SIZE];
  static uint8_t serial_tx_buffer[USB_TX_BUFFER_SIZE];

  conf.rx_buffer = serial_rx_buffer;
 80b05ac:	6063      	str	r3, [r4, #4]
  conf.tx_buffer = serial_tx_buffer;
 80b05ae:	4b05      	ldr	r3, [pc, #20]	; (80b05c4 <_Z19acquireSerialBufferv+0x28>)
 80b05b0:	60e3      	str	r3, [r4, #12]
  conf.rx_buffer_size = USB_RX_BUFFER_SIZE;
 80b05b2:	f240 1301 	movw	r3, #257	; 0x101
 80b05b6:	8123      	strh	r3, [r4, #8]
  conf.tx_buffer_size = USB_TX_BUFFER_SIZE;
 80b05b8:	2381      	movs	r3, #129	; 0x81
 80b05ba:	8223      	strh	r3, [r4, #16]
#endif

  return conf;
}
 80b05bc:	bd10      	pop	{r4, pc}
 80b05be:	bf00      	nop
 80b05c0:	20002829 	.word	0x20002829
 80b05c4:	200027a8 	.word	0x200027a8

080b05c8 <_Z16_fetch_usbserialv>:

USBSerial& _fetch_usbserial()
{
 80b05c8:	b530      	push	{r4, r5, lr}
  HAL_USB_USART_Config conf = acquireSerialBuffer();
	static USBSerial _usbserial(HAL_USB_USART_SERIAL, conf);
 80b05ca:	4d0e      	ldr	r5, [pc, #56]	; (80b0604 <_Z16_fetch_usbserialv+0x3c>)

  return conf;
}

USBSerial& _fetch_usbserial()
{
 80b05cc:	b087      	sub	sp, #28
  HAL_USB_USART_Config conf = acquireSerialBuffer();
 80b05ce:	a801      	add	r0, sp, #4
 80b05d0:	f7ff ffe4 	bl	80b059c <_Z19acquireSerialBufferv>
	static USBSerial _usbserial(HAL_USB_USART_SERIAL, conf);
 80b05d4:	6829      	ldr	r1, [r5, #0]
 80b05d6:	f011 0401 	ands.w	r4, r1, #1
 80b05da:	d110      	bne.n	80b05fe <_Z16_fetch_usbserialv+0x36>
 80b05dc:	4628      	mov	r0, r5
 80b05de:	f7ef fd6f 	bl	80a00c0 <__cxa_guard_acquire>
 80b05e2:	b160      	cbz	r0, 80b05fe <_Z16_fetch_usbserialv+0x36>
 80b05e4:	aa01      	add	r2, sp, #4
 80b05e6:	4621      	mov	r1, r4
 80b05e8:	4807      	ldr	r0, [pc, #28]	; (80b0608 <_Z16_fetch_usbserialv+0x40>)
 80b05ea:	f7ff ffbf 	bl	80b056c <_ZN9USBSerialC1E20HAL_USB_USART_SerialRK20HAL_USB_USART_Config>
 80b05ee:	4628      	mov	r0, r5
 80b05f0:	f7ef fd6b 	bl	80a00ca <__cxa_guard_release>
 80b05f4:	4a05      	ldr	r2, [pc, #20]	; (80b060c <_Z16_fetch_usbserialv+0x44>)
 80b05f6:	4906      	ldr	r1, [pc, #24]	; (80b0610 <_Z16_fetch_usbserialv+0x48>)
 80b05f8:	4803      	ldr	r0, [pc, #12]	; (80b0608 <_Z16_fetch_usbserialv+0x40>)
 80b05fa:	f000 f9f9 	bl	80b09f0 <__aeabi_atexit>
	return _usbserial;
}
 80b05fe:	4802      	ldr	r0, [pc, #8]	; (80b0608 <_Z16_fetch_usbserialv+0x40>)
 80b0600:	b007      	add	sp, #28
 80b0602:	bd30      	pop	{r4, r5, pc}
 80b0604:	200027a4 	.word	0x200027a4
 80b0608:	2000292c 	.word	0x2000292c
 80b060c:	20000594 	.word	0x20000594
 80b0610:	080b04ed 	.word	0x080b04ed

080b0614 <_Z23acquireUSBSerial1Bufferv>:

#if Wiring_USBSerial1

HAL_USB_USART_Config __attribute__((weak)) acquireUSBSerial1Buffer()
{
 80b0614:	b510      	push	{r4, lr}
 80b0616:	4604      	mov	r4, r0
  HAL_USB_USART_Config conf = {0};
 80b0618:	2214      	movs	r2, #20
 80b061a:	2100      	movs	r1, #0
 80b061c:	f003 fa60 	bl	80b3ae0 <memset>

#if defined(USB_SERIAL_USERSPACE_BUFFERS) && ((MODULE_FUNCTION == MOD_FUNC_USER_PART) || (MODULE_FUNCTION == MOD_FUNC_MONO_FIRMWARE))
  static uint8_t usbserial1_rx_buffer[USB_RX_BUFFER_SIZE];
  static uint8_t usbserial1_tx_buffer[USB_TX_BUFFER_SIZE];

  conf.rx_buffer = usbserial1_rx_buffer;
 80b0620:	4b05      	ldr	r3, [pc, #20]	; (80b0638 <_Z23acquireUSBSerial1Bufferv+0x24>)
  conf.rx_buffer_size = USB_RX_BUFFER_SIZE;
  conf.tx_buffer_size = USB_TX_BUFFER_SIZE;
#endif

  return conf;
}
 80b0622:	4620      	mov	r0, r4

#if defined(USB_SERIAL_USERSPACE_BUFFERS) && ((MODULE_FUNCTION == MOD_FUNC_USER_PART) || (MODULE_FUNCTION == MOD_FUNC_MONO_FIRMWARE))
  static uint8_t usbserial1_rx_buffer[USB_RX_BUFFER_SIZE];
  static uint8_t usbserial1_tx_buffer[USB_TX_BUFFER_SIZE];

  conf.rx_buffer = usbserial1_rx_buffer;
 80b0624:	6063      	str	r3, [r4, #4]
  conf.tx_buffer = usbserial1_tx_buffer;
 80b0626:	4b05      	ldr	r3, [pc, #20]	; (80b063c <_Z23acquireUSBSerial1Bufferv+0x28>)
 80b0628:	60e3      	str	r3, [r4, #12]
  conf.rx_buffer_size = USB_RX_BUFFER_SIZE;
 80b062a:	f240 1301 	movw	r3, #257	; 0x101
 80b062e:	8123      	strh	r3, [r4, #8]
  conf.tx_buffer_size = USB_TX_BUFFER_SIZE;
 80b0630:	2381      	movs	r3, #129	; 0x81
 80b0632:	8223      	strh	r3, [r4, #16]
#endif

  return conf;
}
 80b0634:	bd10      	pop	{r4, pc}
 80b0636:	bf00      	nop
 80b0638:	200029dc 	.word	0x200029dc
 80b063c:	20002944 	.word	0x20002944

080b0640 <_Z17_fetch_usbserial1v>:

USBSerial& _fetch_usbserial1()
{
 80b0640:	b510      	push	{r4, lr}
  HAL_USB_USART_Config conf = acquireUSBSerial1Buffer();
  static USBSerial _usbserial1(HAL_USB_USART_SERIAL1, conf);
 80b0642:	4c0e      	ldr	r4, [pc, #56]	; (80b067c <_Z17_fetch_usbserial1v+0x3c>)

  return conf;
}

USBSerial& _fetch_usbserial1()
{
 80b0644:	b086      	sub	sp, #24
  HAL_USB_USART_Config conf = acquireUSBSerial1Buffer();
 80b0646:	a801      	add	r0, sp, #4
 80b0648:	f7ff ffe4 	bl	80b0614 <_Z23acquireUSBSerial1Bufferv>
  static USBSerial _usbserial1(HAL_USB_USART_SERIAL1, conf);
 80b064c:	6823      	ldr	r3, [r4, #0]
 80b064e:	07db      	lsls	r3, r3, #31
 80b0650:	d410      	bmi.n	80b0674 <_Z17_fetch_usbserial1v+0x34>
 80b0652:	4620      	mov	r0, r4
 80b0654:	f7ef fd34 	bl	80a00c0 <__cxa_guard_acquire>
 80b0658:	b160      	cbz	r0, 80b0674 <_Z17_fetch_usbserial1v+0x34>
 80b065a:	aa01      	add	r2, sp, #4
 80b065c:	2101      	movs	r1, #1
 80b065e:	4808      	ldr	r0, [pc, #32]	; (80b0680 <_Z17_fetch_usbserial1v+0x40>)
 80b0660:	f7ff ff84 	bl	80b056c <_ZN9USBSerialC1E20HAL_USB_USART_SerialRK20HAL_USB_USART_Config>
 80b0664:	4620      	mov	r0, r4
 80b0666:	f7ef fd30 	bl	80a00ca <__cxa_guard_release>
 80b066a:	4a06      	ldr	r2, [pc, #24]	; (80b0684 <_Z17_fetch_usbserial1v+0x44>)
 80b066c:	4906      	ldr	r1, [pc, #24]	; (80b0688 <_Z17_fetch_usbserial1v+0x48>)
 80b066e:	4804      	ldr	r0, [pc, #16]	; (80b0680 <_Z17_fetch_usbserial1v+0x40>)
 80b0670:	f000 f9be 	bl	80b09f0 <__aeabi_atexit>
  return _usbserial1;
}
 80b0674:	4802      	ldr	r0, [pc, #8]	; (80b0680 <_Z17_fetch_usbserial1v+0x40>)
 80b0676:	b006      	add	sp, #24
 80b0678:	bd10      	pop	{r4, pc}
 80b067a:	bf00      	nop
 80b067c:	20002940 	.word	0x20002940
 80b0680:	200029c8 	.word	0x200029c8
 80b0684:	20000594 	.word	0x20000594
 80b0688:	080b04ed 	.word	0x080b04ed

080b068c <_ZN5spark9WiFiClass5readyEv>:
    bool connecting(void) {
        return network_connecting(*this, 0, NULL);
    }

    bool ready(void) {
        return network_ready(*this, 0, NULL);
 80b068c:	2200      	movs	r2, #0
 80b068e:	4611      	mov	r1, r2
 80b0690:	6840      	ldr	r0, [r0, #4]
 80b0692:	f7ff bc85 	b.w	80affa0 <network_ready>

080b0696 <_ZN5spark9WiFiClass7resolveEPKc>:
    WLanSelectAntenna_TypeDef getAntenna() {
        return wlan_get_antenna(nullptr);
    }

#if !HAL_USE_INET_HAL_POSIX
    IPAddress resolve(const char* name)
 80b0696:	b5f0      	push	{r4, r5, r6, r7, lr}
 80b0698:	4616      	mov	r6, r2
 80b069a:	b089      	sub	sp, #36	; 0x24
    {
        HAL_IPAddress ip = {};
 80b069c:	2211      	movs	r2, #17
    WLanSelectAntenna_TypeDef getAntenna() {
        return wlan_get_antenna(nullptr);
    }

#if !HAL_USE_INET_HAL_POSIX
    IPAddress resolve(const char* name)
 80b069e:	460f      	mov	r7, r1
 80b06a0:	4604      	mov	r4, r0
    {
        HAL_IPAddress ip = {};
 80b06a2:	2100      	movs	r1, #0
 80b06a4:	a803      	add	r0, sp, #12
 80b06a6:	f003 fa1b 	bl	80b3ae0 <memset>
        return (inet_gethostbyname(name, strlen(name), &ip, *this, NULL) != 0) ?
 80b06aa:	4630      	mov	r0, r6
 80b06ac:	f003 fa52 	bl	80b3b54 <strlen>
 80b06b0:	2500      	movs	r5, #0
 80b06b2:	9500      	str	r5, [sp, #0]
 80b06b4:	b281      	uxth	r1, r0
 80b06b6:	687b      	ldr	r3, [r7, #4]
 80b06b8:	aa03      	add	r2, sp, #12
 80b06ba:	4630      	mov	r0, r6
 80b06bc:	f7ff fc1c 	bl	80afef8 <inet_gethostbyname>
                IPAddress(uint32_t(0)) : IPAddress(ip);
 80b06c0:	b120      	cbz	r0, 80b06cc <_ZN5spark9WiFiClass7resolveEPKc+0x36>
 80b06c2:	4629      	mov	r1, r5
 80b06c4:	4620      	mov	r0, r4
 80b06c6:	f7ff fd59 	bl	80b017c <_ZN9IPAddressC1Em>
 80b06ca:	e003      	b.n	80b06d4 <_ZN5spark9WiFiClass7resolveEPKc+0x3e>
 80b06cc:	a903      	add	r1, sp, #12
 80b06ce:	4620      	mov	r0, r4
 80b06d0:	f7ff fd42 	bl	80b0158 <_ZN9IPAddressC1ERK16_HAL_IPAddress_t>
    }
 80b06d4:	4620      	mov	r0, r4
 80b06d6:	b009      	add	sp, #36	; 0x24
 80b06d8:	bdf0      	pop	{r4, r5, r6, r7, pc}

080b06da <_ZN5spark9WiFiClass9listeningEv>:
    uint16_t getListenTimeout(void) {
        return network_get_listen_timeout(*this, 0, NULL);
    }

    bool listening(void) {
        return network_listening(*this, 0, NULL);
 80b06da:	2200      	movs	r2, #0
 80b06dc:	4611      	mov	r1, r2
 80b06de:	6840      	ldr	r0, [r0, #4]
 80b06e0:	f7ff bc7e 	b.w	80affe0 <network_listening>

080b06e4 <_ZN5spark9WiFiClass16getListenTimeoutEv>:
    void setListenTimeout(uint16_t timeout) {
        network_set_listen_timeout(*this, timeout, NULL);
    }

    uint16_t getListenTimeout(void) {
        return network_get_listen_timeout(*this, 0, NULL);
 80b06e4:	2200      	movs	r2, #0
 80b06e6:	4611      	mov	r1, r2
 80b06e8:	6840      	ldr	r0, [r0, #4]
 80b06ea:	f7ff bc89 	b.w	80b0000 <network_get_listen_timeout>

080b06ee <_ZN5spark9WiFiClass16setListenTimeoutEt>:
    void listen(bool begin=true) {
        network_listen(*this, begin ? 0 : 1, NULL);
    }

    void setListenTimeout(uint16_t timeout) {
        network_set_listen_timeout(*this, timeout, NULL);
 80b06ee:	2200      	movs	r2, #0
 80b06f0:	6840      	ldr	r0, [r0, #4]
 80b06f2:	f7ff bc7d 	b.w	80afff0 <network_set_listen_timeout>

080b06f6 <_ZN5spark9WiFiClass6listenEb>:
    void off(void) {
        network_off(*this, 0, 0, NULL);
    }

    void listen(bool begin=true) {
        network_listen(*this, begin ? 0 : 1, NULL);
 80b06f6:	2200      	movs	r2, #0
 80b06f8:	f081 0101 	eor.w	r1, r1, #1
 80b06fc:	6840      	ldr	r0, [r0, #4]
 80b06fe:	f7ff bc67 	b.w	80affd0 <network_listen>

080b0702 <_ZN5spark9WiFiClass3offEv>:
    void on(void) {
        network_on(*this, 0, 0, NULL);
    }

    void off(void) {
        network_off(*this, 0, 0, NULL);
 80b0702:	2300      	movs	r3, #0
 80b0704:	461a      	mov	r2, r3
 80b0706:	4619      	mov	r1, r3
 80b0708:	6840      	ldr	r0, [r0, #4]
 80b070a:	f7ff bc59 	b.w	80affc0 <network_off>

080b070e <_ZN5spark9WiFiClass2onEv>:
    bool ready(void) {
        return network_ready(*this, 0, NULL);
    }

    void on(void) {
        network_on(*this, 0, 0, NULL);
 80b070e:	2300      	movs	r3, #0
 80b0710:	461a      	mov	r2, r3
 80b0712:	4619      	mov	r1, r3
 80b0714:	6840      	ldr	r0, [r0, #4]
 80b0716:	f7ff bc4b 	b.w	80affb0 <network_on>

080b071a <_ZN5spark9WiFiClass10connectingEv>:
    void disconnect(void) {
        network_disconnect(*this, NETWORK_DISCONNECT_REASON_USER, NULL);
    }

    bool connecting(void) {
        return network_connecting(*this, 0, NULL);
 80b071a:	2200      	movs	r2, #0
 80b071c:	4611      	mov	r1, r2
 80b071e:	6840      	ldr	r0, [r0, #4]
 80b0720:	f7ff bc2e 	b.w	80aff80 <network_connecting>

080b0724 <_ZN5spark9WiFiClass10disconnectEv>:
    void connect(unsigned flags=0) {
        network_connect(*this, flags, 0, NULL);
    }

    void disconnect(void) {
        network_disconnect(*this, NETWORK_DISCONNECT_REASON_USER, NULL);
 80b0724:	2200      	movs	r2, #0
 80b0726:	2102      	movs	r1, #2
 80b0728:	6840      	ldr	r0, [r0, #4]
 80b072a:	f7ff bc31 	b.w	80aff90 <network_disconnect>

080b072e <_ZN5spark9WiFiClass7connectEj>:
    uint32_t ping(IPAddress remoteIP, uint8_t nTries) {
        return inet_ping(&remoteIP.raw(), *this, nTries, NULL);
    }

    void connect(unsigned flags=0) {
        network_connect(*this, flags, 0, NULL);
 80b072e:	2300      	movs	r3, #0
 80b0730:	461a      	mov	r2, r3
 80b0732:	6840      	ldr	r0, [r0, #4]
 80b0734:	f7ff bc1c 	b.w	80aff70 <network_connect>

080b0738 <_GLOBAL__sub_I__ZN10WiFiSignalC2ERK21wlan_connected_info_t>:
 80b0738:	4b02      	ldr	r3, [pc, #8]	; (80b0744 <_GLOBAL__sub_I__ZN10WiFiSignalC2ERK21wlan_connected_info_t+0xc>)
 80b073a:	2204      	movs	r2, #4
 80b073c:	605a      	str	r2, [r3, #4]
        wlan_set_ipaddress_source(source, true, NULL);
    }

public:
    WiFiClass() :
            NetworkClass(NETWORK_INTERFACE_WIFI_STA) {
 80b073e:	4a02      	ldr	r2, [pc, #8]	; (80b0748 <_GLOBAL__sub_I__ZN10WiFiSignalC2ERK21wlan_connected_info_t+0x10>)
 80b0740:	601a      	str	r2, [r3, #0]
 80b0742:	4770      	bx	lr
 80b0744:	20002ae0 	.word	0x20002ae0
 80b0748:	080b7798 	.word	0x080b7798

080b074c <serialEventRun>:

/**
 * Provides background processing of serial data.
 */
void serialEventRun()
{
 80b074c:	b508      	push	{r3, lr}
    if (serialEvent && Serial.available()>0)
 80b074e:	4b14      	ldr	r3, [pc, #80]	; (80b07a0 <serialEventRun+0x54>)
 80b0750:	b133      	cbz	r3, 80b0760 <serialEventRun+0x14>
 80b0752:	f7ff ff39 	bl	80b05c8 <_Z16_fetch_usbserialv>
 80b0756:	6803      	ldr	r3, [r0, #0]
 80b0758:	691b      	ldr	r3, [r3, #16]
 80b075a:	4798      	blx	r3
 80b075c:	2800      	cmp	r0, #0
 80b075e:	dc16      	bgt.n	80b078e <serialEventRun+0x42>
        serialEvent();

    if (serialEvent1 && Serial1.available()>0)
 80b0760:	4b10      	ldr	r3, [pc, #64]	; (80b07a4 <serialEventRun+0x58>)
 80b0762:	b133      	cbz	r3, 80b0772 <serialEventRun+0x26>
 80b0764:	f7ff fe9a 	bl	80b049c <_Z22__fetch_global_Serial1v>
 80b0768:	6803      	ldr	r3, [r0, #0]
 80b076a:	691b      	ldr	r3, [r3, #16]
 80b076c:	4798      	blx	r3
 80b076e:	2800      	cmp	r0, #0
 80b0770:	dc10      	bgt.n	80b0794 <serialEventRun+0x48>
        serialEvent1();

#if Wiring_Serial2
    if (serialEventRun2) serialEventRun2();
 80b0772:	4b0d      	ldr	r3, [pc, #52]	; (80b07a8 <serialEventRun+0x5c>)
 80b0774:	b10b      	cbz	r3, 80b077a <serialEventRun+0x2e>
 80b0776:	f3af 8000 	nop.w
#if Wiring_Serial5
    if (serialEventRun5) serialEventRun5();
#endif

#if Wiring_USBSerial1
    if (usbSerialEvent1 && USBSerial1.available()>0)
 80b077a:	4b0c      	ldr	r3, [pc, #48]	; (80b07ac <serialEventRun+0x60>)
 80b077c:	b17b      	cbz	r3, 80b079e <serialEventRun+0x52>
 80b077e:	f7ff ff5f 	bl	80b0640 <_Z17_fetch_usbserial1v>
 80b0782:	6803      	ldr	r3, [r0, #0]
 80b0784:	691b      	ldr	r3, [r3, #16]
 80b0786:	4798      	blx	r3
 80b0788:	2800      	cmp	r0, #0
 80b078a:	dc06      	bgt.n	80b079a <serialEventRun+0x4e>
 80b078c:	bd08      	pop	{r3, pc}
 * Provides background processing of serial data.
 */
void serialEventRun()
{
    if (serialEvent && Serial.available()>0)
        serialEvent();
 80b078e:	f3af 8000 	nop.w
 80b0792:	e7e5      	b.n	80b0760 <serialEventRun+0x14>

    if (serialEvent1 && Serial1.available()>0)
        serialEvent1();
 80b0794:	f3af 8000 	nop.w
 80b0798:	e7eb      	b.n	80b0772 <serialEventRun+0x26>
    if (serialEventRun5) serialEventRun5();
#endif

#if Wiring_USBSerial1
    if (usbSerialEvent1 && USBSerial1.available()>0)
        usbSerialEvent1();
 80b079a:	f3af 8000 	nop.w
 80b079e:	bd08      	pop	{r3, pc}
	...

080b07b0 <_post_loop>:
#if Wiring_Serial5
void serialEvent5() __attribute__((weak));
#endif

void _post_loop()
{
 80b07b0:	b508      	push	{r3, lr}
	serialEventRun();
 80b07b2:	f7ff ffcb 	bl	80b074c <serialEventRun>
		return !timeout_fn;
	}

	static inline system_tick_t current_time()
	{
		return HAL_Timer_Get_Milli_Seconds();
 80b07b6:	f7ff faa5 	bl	80afd04 <HAL_Timer_Get_Milli_Seconds>
	/**
	 * Lifesign that the application is still working normally.
	 */
	static void checkin()
	{
		last_checkin = current_time();
 80b07ba:	4b01      	ldr	r3, [pc, #4]	; (80b07c0 <_post_loop+0x10>)
 80b07bc:	6018      	str	r0, [r3, #0]
 80b07be:	bd08      	pop	{r3, pc}
 80b07c0:	20002af0 	.word	0x20002af0

080b07c4 <_Z33system_initialize_user_backup_ramv>:
 * the dynamically linked application module.
 */
void system_initialize_user_backup_ram()
{
    size_t len = &link_global_retained_end-&link_global_retained_start;
    memcpy(&link_global_retained_start, &link_global_retained_initial_values, len);
 80b07c4:	4802      	ldr	r0, [pc, #8]	; (80b07d0 <_Z33system_initialize_user_backup_ramv+0xc>)
 80b07c6:	4a03      	ldr	r2, [pc, #12]	; (80b07d4 <_Z33system_initialize_user_backup_ramv+0x10>)
 80b07c8:	4903      	ldr	r1, [pc, #12]	; (80b07d8 <_Z33system_initialize_user_backup_ramv+0x14>)
 80b07ca:	1a12      	subs	r2, r2, r0
 80b07cc:	f003 b97d 	b.w	80b3aca <memcpy>
 80b07d0:	40024000 	.word	0x40024000
 80b07d4:	40024004 	.word	0x40024004
 80b07d8:	080b7ca0 	.word	0x080b7ca0

080b07dc <_Z27ctrl_request_custom_handlerP12ctrl_request>:
bool __backup_ram_was_valid() { return false; }

#endif

// Default handler for CTRL_REQUEST_APP_CUSTOM requests
void __attribute((weak)) ctrl_request_custom_handler(ctrl_request* req) {
 80b07dc:	b507      	push	{r0, r1, r2, lr}
    system_ctrl_set_result(req, SYSTEM_ERROR_NOT_SUPPORTED, nullptr, nullptr, nullptr);
 80b07de:	2300      	movs	r3, #0
 80b07e0:	9300      	str	r3, [sp, #0]
 80b07e2:	461a      	mov	r2, r3
 80b07e4:	f06f 0177 	mvn.w	r1, #119	; 0x77
 80b07e8:	f7ff fbb0 	bl	80aff4c <system_ctrl_set_result>
}
 80b07ec:	b003      	add	sp, #12
 80b07ee:	f85d fb04 	ldr.w	pc, [sp], #4
	...

080b07f4 <_ZL20ctrl_request_handlerP12ctrl_request>:
// Callback invoked to process a logging configuration request
void(*log_process_ctrl_request_callback)(ctrl_request* req) = nullptr;
#endif

// Application handler for control requests
static void ctrl_request_handler(ctrl_request* req) {
 80b07f4:	b507      	push	{r0, r1, r2, lr}
    switch (req->type) {
 80b07f6:	8843      	ldrh	r3, [r0, #2]
 80b07f8:	2b0a      	cmp	r3, #10
 80b07fa:	d008      	beq.n	80b080e <_ZL20ctrl_request_handlerP12ctrl_request+0x1a>
 80b07fc:	2b50      	cmp	r3, #80	; 0x50
 80b07fe:	d109      	bne.n	80b0814 <_ZL20ctrl_request_handlerP12ctrl_request+0x20>
#if Wiring_LogConfig
    case CTRL_REQUEST_LOG_CONFIG: {
        if (log_process_ctrl_request_callback) {
 80b0800:	4b09      	ldr	r3, [pc, #36]	; (80b0828 <_ZL20ctrl_request_handlerP12ctrl_request+0x34>)
 80b0802:	681b      	ldr	r3, [r3, #0]
 80b0804:	b13b      	cbz	r3, 80b0816 <_ZL20ctrl_request_handlerP12ctrl_request+0x22>
    }
    default:
        system_ctrl_set_result(req, SYSTEM_ERROR_NOT_SUPPORTED, nullptr, nullptr, nullptr);
        break;
    }
}
 80b0806:	b003      	add	sp, #12
 80b0808:	f85d eb04 	ldr.w	lr, [sp], #4
static void ctrl_request_handler(ctrl_request* req) {
    switch (req->type) {
#if Wiring_LogConfig
    case CTRL_REQUEST_LOG_CONFIG: {
        if (log_process_ctrl_request_callback) {
            log_process_ctrl_request_callback(req);
 80b080c:	4718      	bx	r3
        }
        break;
    }
#endif
    case CTRL_REQUEST_APP_CUSTOM: {
        ctrl_request_custom_handler(req);
 80b080e:	f7ff ffe5 	bl	80b07dc <_Z27ctrl_request_custom_handlerP12ctrl_request>
        break;
 80b0812:	e006      	b.n	80b0822 <_ZL20ctrl_request_handlerP12ctrl_request+0x2e>
    }
    default:
        system_ctrl_set_result(req, SYSTEM_ERROR_NOT_SUPPORTED, nullptr, nullptr, nullptr);
 80b0814:	2300      	movs	r3, #0
 80b0816:	9300      	str	r3, [sp, #0]
 80b0818:	461a      	mov	r2, r3
 80b081a:	f06f 0177 	mvn.w	r1, #119	; 0x77
 80b081e:	f7ff fb95 	bl	80aff4c <system_ctrl_set_result>
        break;
    }
}
 80b0822:	b003      	add	sp, #12
 80b0824:	f85d fb04 	ldr.w	pc, [sp], #4
 80b0828:	20002ae8 	.word	0x20002ae8

080b082c <module_user_init_hook>:

void module_user_init_hook()
{
 80b082c:	b538      	push	{r3, r4, r5, lr}
#if PLATFORM_BACKUP_RAM
    backup_ram_was_valid_ =  __backup_sram_signature==signature;
 80b082e:	4c10      	ldr	r4, [pc, #64]	; (80b0870 <module_user_init_hook+0x44>)
 80b0830:	4d10      	ldr	r5, [pc, #64]	; (80b0874 <module_user_init_hook+0x48>)
 80b0832:	6823      	ldr	r3, [r4, #0]
 80b0834:	42ab      	cmp	r3, r5
 80b0836:	4b10      	ldr	r3, [pc, #64]	; (80b0878 <module_user_init_hook+0x4c>)
 80b0838:	bf0c      	ite	eq
 80b083a:	2201      	moveq	r2, #1
 80b083c:	2200      	movne	r2, #0
 80b083e:	701a      	strb	r2, [r3, #0]
    if (!backup_ram_was_valid_) {
 80b0840:	d002      	beq.n	80b0848 <module_user_init_hook+0x1c>
        system_initialize_user_backup_ram();
 80b0842:	f7ff ffbf 	bl	80b07c4 <_Z33system_initialize_user_backup_ramv>
        __backup_sram_signature = signature;
 80b0846:	6025      	str	r5, [r4, #0]
    }
#endif

#if HAL_PLATFORM_RNG
    // Initialize the default stdlib PRNG using hardware RNG as a seed
    const uint32_t seed = HAL_RNG_GetRandomNumber();
 80b0848:	f7ff fa4c 	bl	80afce4 <HAL_RNG_GetRandomNumber>
 80b084c:	4604      	mov	r4, r0
    srand(seed);
 80b084e:	f003 f94f 	bl	80b3af0 <srand>

    // If the user defines random_seed_from_cloud, call it with a seed value
    // generated by a hardware RNG as well.
    if (random_seed_from_cloud) {
 80b0852:	4b0a      	ldr	r3, [pc, #40]	; (80b087c <module_user_init_hook+0x50>)
 80b0854:	b113      	cbz	r3, 80b085c <module_user_init_hook+0x30>
        random_seed_from_cloud(seed);
 80b0856:	4620      	mov	r0, r4
 80b0858:	f3af 8000 	nop.w
    }
#endif
    // Register the random_seed_from_cloud handler
    spark_set_random_seed_from_cloud_handler(&random_seed_from_cloud, nullptr);
 80b085c:	2100      	movs	r1, #0
 80b085e:	4807      	ldr	r0, [pc, #28]	; (80b087c <module_user_init_hook+0x50>)
 80b0860:	f7ff fb7e 	bl	80aff60 <spark_set_random_seed_from_cloud_handler>

    // Register application handler for the control requests
    system_ctrl_set_app_request_handler(ctrl_request_handler, nullptr);
}
 80b0864:	e8bd 4038 	ldmia.w	sp!, {r3, r4, r5, lr}
#endif
    // Register the random_seed_from_cloud handler
    spark_set_random_seed_from_cloud_handler(&random_seed_from_cloud, nullptr);

    // Register application handler for the control requests
    system_ctrl_set_app_request_handler(ctrl_request_handler, nullptr);
 80b0868:	2100      	movs	r1, #0
 80b086a:	4805      	ldr	r0, [pc, #20]	; (80b0880 <module_user_init_hook+0x54>)
 80b086c:	f7ff bb64 	b.w	80aff38 <system_ctrl_set_app_request_handler>
 80b0870:	40024000 	.word	0x40024000
 80b0874:	9a271c1e 	.word	0x9a271c1e
 80b0878:	20002aec 	.word	0x20002aec
 80b087c:	00000000 	.word	0x00000000
 80b0880:	080b07f5 	.word	0x080b07f5

080b0884 <pinAvailable>:

/*
 * @brief Perform safety check on desired pin to see if it's already
 * being used.  Return 0 if used, otherwise return 1 if available.
 */
bool pinAvailable(uint16_t pin) {
 80b0884:	b510      	push	{r4, lr}
 80b0886:	4604      	mov	r4, r0

  // SPI safety check
#ifndef SPARK_WIRING_NO_SPI
  if(SPI.isEnabled() == true && (pin == SCK || pin == MOSI || pin == MISO))
 80b0888:	480f      	ldr	r0, [pc, #60]	; (80b08c8 <pinAvailable+0x44>)
 80b088a:	f7ff fd91 	bl	80b03b0 <_ZN8SPIClass9isEnabledEv>
 80b088e:	b128      	cbz	r0, 80b089c <pinAvailable+0x18>
 80b0890:	f1a4 030d 	sub.w	r3, r4, #13
 80b0894:	2b02      	cmp	r3, #2
 80b0896:	d801      	bhi.n	80b089c <pinAvailable+0x18>
  {
    return 0; // 'pin' is used
 80b0898:	2000      	movs	r0, #0
 80b089a:	bd10      	pop	{r4, pc}
  }
#endif
  // I2C safety check
#ifndef SPARK_WIRING_NO_I2C
  if(Wire.isEnabled() == true && (pin == SCL || pin == SDA))
 80b089c:	f000 f84e 	bl	80b093c <_Z19__fetch_global_Wirev>
 80b08a0:	f7ff fc26 	bl	80b00f0 <_ZN7TwoWire9isEnabledEv>
 80b08a4:	b108      	cbz	r0, 80b08aa <pinAvailable+0x26>
 80b08a6:	2c01      	cmp	r4, #1
 80b08a8:	d9f6      	bls.n	80b0898 <pinAvailable+0x14>
    return 0; // 'pin' is used
  }
#endif
#ifndef SPARK_WIRING_NO_USART_SERIAL
  // Serial1 safety check
  if(Serial1.isEnabled() == true && (pin == RX || pin == TX))
 80b08aa:	f7ff fdf7 	bl	80b049c <_Z22__fetch_global_Serial1v>
 80b08ae:	f7ff fdf1 	bl	80b0494 <_ZN11USARTSerial9isEnabledEv>
 80b08b2:	b118      	cbz	r0, 80b08bc <pinAvailable+0x38>
 80b08b4:	f1a4 0312 	sub.w	r3, r4, #18
 80b08b8:	2b01      	cmp	r3, #1
 80b08ba:	d9ed      	bls.n	80b0898 <pinAvailable+0x14>
  {
    return 0; // 'pin' is used
  }
#endif

  if (pin >= TOTAL_PINS)
 80b08bc:	2c17      	cmp	r4, #23
 80b08be:	bf8c      	ite	hi
 80b08c0:	2000      	movhi	r0, #0
 80b08c2:	2001      	movls	r0, #1
    return 0;
  else
    return 1; // 'pin' is available
}
 80b08c4:	bd10      	pop	{r4, pc}
 80b08c6:	bf00      	nop
 80b08c8:	20002b24 	.word	0x20002b24

080b08cc <pinMode>:
 * or INPUT_PULLDOWN
 */
void pinMode(uint16_t pin, PinMode setMode)
{

  if(pin >= TOTAL_PINS || setMode == PIN_MODE_NONE )
 80b08cc:	2817      	cmp	r0, #23
/*
 * @brief Set the mode of the pin to OUTPUT, INPUT, INPUT_PULLUP,
 * or INPUT_PULLDOWN
 */
void pinMode(uint16_t pin, PinMode setMode)
{
 80b08ce:	b538      	push	{r3, r4, r5, lr}
 80b08d0:	4604      	mov	r4, r0
 80b08d2:	460d      	mov	r5, r1

  if(pin >= TOTAL_PINS || setMode == PIN_MODE_NONE )
 80b08d4:	d80a      	bhi.n	80b08ec <pinMode+0x20>
 80b08d6:	29ff      	cmp	r1, #255	; 0xff
 80b08d8:	d008      	beq.n	80b08ec <pinMode+0x20>
  {
    return;
  }

  // Safety check
  if( !pinAvailable(pin) ) {
 80b08da:	f7ff ffd3 	bl	80b0884 <pinAvailable>
 80b08de:	b128      	cbz	r0, 80b08ec <pinMode+0x20>
    return;
  }

  HAL_Pin_Mode(pin, setMode);
 80b08e0:	4629      	mov	r1, r5
 80b08e2:	4620      	mov	r0, r4
}
 80b08e4:	e8bd 4038 	ldmia.w	sp!, {r3, r4, r5, lr}
  // Safety check
  if( !pinAvailable(pin) ) {
    return;
  }

  HAL_Pin_Mode(pin, setMode);
 80b08e8:	f7ff ba24 	b.w	80afd34 <HAL_Pin_Mode>
 80b08ec:	bd38      	pop	{r3, r4, r5, pc}

080b08ee <_Z11analogWritetm>:
/*
 * @brief Should take an integer 0-255 and create a 500Hz PWM signal with a duty cycle from 0-100%.
 * On Photon, DAC1 and DAC2 act as true analog outputs(values: 0 to 4095) using onchip DAC peripheral
 */
void analogWrite(pin_t pin, uint32_t value)
{
 80b08ee:	b538      	push	{r3, r4, r5, lr}
 80b08f0:	4604      	mov	r4, r0
 80b08f2:	460d      	mov	r5, r1
    // Safety check
    if (!pinAvailable(pin))
 80b08f4:	f7ff ffc6 	bl	80b0884 <pinAvailable>
 80b08f8:	b1f0      	cbz	r0, 80b0938 <_Z11analogWritetm+0x4a>
    {
        return;
    }

    if (HAL_Validate_Pin_Function(pin, PF_DAC) == PF_DAC)
 80b08fa:	2104      	movs	r1, #4
 80b08fc:	4620      	mov	r0, r4
 80b08fe:	f7ff fa11 	bl	80afd24 <HAL_Validate_Pin_Function>
 80b0902:	2804      	cmp	r0, #4
 80b0904:	d105      	bne.n	80b0912 <_Z11analogWritetm+0x24>
    {
        HAL_DAC_Write(pin, value);
 80b0906:	b2a9      	uxth	r1, r5
 80b0908:	4620      	mov	r0, r4
            return;
        }

        HAL_PWM_Write_Ext(pin, value);
    }
}
 80b090a:	e8bd 4038 	ldmia.w	sp!, {r3, r4, r5, lr}
        return;
    }

    if (HAL_Validate_Pin_Function(pin, PF_DAC) == PF_DAC)
    {
        HAL_DAC_Write(pin, value);
 80b090e:	f7ff ba21 	b.w	80afd54 <HAL_DAC_Write>
    }
    else if (HAL_Validate_Pin_Function(pin, PF_TIMER) == PF_TIMER)
 80b0912:	2102      	movs	r1, #2
 80b0914:	4620      	mov	r0, r4
 80b0916:	f7ff fa05 	bl	80afd24 <HAL_Validate_Pin_Function>
 80b091a:	2802      	cmp	r0, #2
 80b091c:	d10c      	bne.n	80b0938 <_Z11analogWritetm+0x4a>
    {
        PinMode mode = HAL_Get_Pin_Mode(pin);
 80b091e:	4620      	mov	r0, r4
 80b0920:	f7ff fa10 	bl	80afd44 <HAL_Get_Pin_Mode>

        if (mode != OUTPUT && mode != AF_OUTPUT_PUSHPULL)
 80b0924:	2801      	cmp	r0, #1
 80b0926:	d001      	beq.n	80b092c <_Z11analogWritetm+0x3e>
 80b0928:	2804      	cmp	r0, #4
 80b092a:	d105      	bne.n	80b0938 <_Z11analogWritetm+0x4a>
        {
            return;
        }

        HAL_PWM_Write_Ext(pin, value);
 80b092c:	4629      	mov	r1, r5
 80b092e:	4620      	mov	r0, r4
    }
}
 80b0930:	e8bd 4038 	ldmia.w	sp!, {r3, r4, r5, lr}
        if (mode != OUTPUT && mode != AF_OUTPUT_PUSHPULL)
        {
            return;
        }

        HAL_PWM_Write_Ext(pin, value);
 80b0934:	f7ff ba16 	b.w	80afd64 <HAL_PWM_Write_Ext>
 80b0938:	bd38      	pop	{r3, r4, r5, pc}
	...

080b093c <_Z19__fetch_global_Wirev>:
#include "i2c_hal.h"

#ifndef SPARK_WIRING_NO_I2C

TwoWire& __fetch_global_Wire()
{
 80b093c:	b538      	push	{r3, r4, r5, lr}
	static TwoWire wire(HAL_I2C_INTERFACE1);
 80b093e:	4d0b      	ldr	r5, [pc, #44]	; (80b096c <_Z19__fetch_global_Wirev+0x30>)
 80b0940:	6829      	ldr	r1, [r5, #0]
 80b0942:	f011 0401 	ands.w	r4, r1, #1
 80b0946:	d10f      	bne.n	80b0968 <_Z19__fetch_global_Wirev+0x2c>
 80b0948:	4628      	mov	r0, r5
 80b094a:	f7ef fbb9 	bl	80a00c0 <__cxa_guard_acquire>
 80b094e:	b158      	cbz	r0, 80b0968 <_Z19__fetch_global_Wirev+0x2c>
 80b0950:	4621      	mov	r1, r4
 80b0952:	4807      	ldr	r0, [pc, #28]	; (80b0970 <_Z19__fetch_global_Wirev+0x34>)
 80b0954:	f7ff fbba 	bl	80b00cc <_ZN7TwoWireC1E17HAL_I2C_Interface>
 80b0958:	4628      	mov	r0, r5
 80b095a:	f7ef fbb6 	bl	80a00ca <__cxa_guard_release>
 80b095e:	4a05      	ldr	r2, [pc, #20]	; (80b0974 <_Z19__fetch_global_Wirev+0x38>)
 80b0960:	4905      	ldr	r1, [pc, #20]	; (80b0978 <_Z19__fetch_global_Wirev+0x3c>)
 80b0962:	4803      	ldr	r0, [pc, #12]	; (80b0970 <_Z19__fetch_global_Wirev+0x34>)
 80b0964:	f000 f844 	bl	80b09f0 <__aeabi_atexit>
	return wire;
}
 80b0968:	4801      	ldr	r0, [pc, #4]	; (80b0970 <_Z19__fetch_global_Wirev+0x34>)
 80b096a:	bd38      	pop	{r3, r4, r5, pc}
 80b096c:	20002af4 	.word	0x20002af4
 80b0970:	20002af8 	.word	0x20002af8
 80b0974:	20000594 	.word	0x20000594
 80b0978:	080b0073 	.word	0x080b0073

080b097c <_GLOBAL__sub_I_INADDR_NONE>:
#include "spark_wiring_ipaddress.h"

#if !HAL_USE_SOCKET_HAL_POSIX
const IPAddress INADDR_NONE(0, 0, 0, 0);
 80b097c:	b513      	push	{r0, r1, r4, lr}
 80b097e:	4c08      	ldr	r4, [pc, #32]	; (80b09a0 <_GLOBAL__sub_I_INADDR_NONE+0x24>)
 80b0980:	2300      	movs	r3, #0
 80b0982:	461a      	mov	r2, r3
 80b0984:	4619      	mov	r1, r3
 80b0986:	9300      	str	r3, [sp, #0]
 80b0988:	4620      	mov	r0, r4
 80b098a:	f7ff fc0d 	bl	80b01a8 <_ZN9IPAddressC1Ehhhh>
 80b098e:	4620      	mov	r0, r4
 80b0990:	4a04      	ldr	r2, [pc, #16]	; (80b09a4 <_GLOBAL__sub_I_INADDR_NONE+0x28>)
 80b0992:	4905      	ldr	r1, [pc, #20]	; (80b09a8 <_GLOBAL__sub_I_INADDR_NONE+0x2c>)
 80b0994:	b002      	add	sp, #8
 80b0996:	e8bd 4010 	ldmia.w	sp!, {r4, lr}
 80b099a:	f000 b829 	b.w	80b09f0 <__aeabi_atexit>
 80b099e:	bf00      	nop
 80b09a0:	20002b0c 	.word	0x20002b0c
 80b09a4:	20000594 	.word	0x20000594
 80b09a8:	080b00f9 	.word	0x080b00f9

080b09ac <_GLOBAL__sub_I_SPI>:
#ifndef SPARK_WIRING_NO_SPI

SPIClass SPI(HAL_SPI_INTERFACE1);

#if Wiring_SPI1
SPIClass SPI1(HAL_SPI_INTERFACE2);
 80b09ac:	b570      	push	{r4, r5, r6, lr}
#include "core_hal.h"
#include "spark_macros.h"

#ifndef SPARK_WIRING_NO_SPI

SPIClass SPI(HAL_SPI_INTERFACE1);
 80b09ae:	4c0c      	ldr	r4, [pc, #48]	; (80b09e0 <_GLOBAL__sub_I_SPI+0x34>)
 80b09b0:	4e0c      	ldr	r6, [pc, #48]	; (80b09e4 <_GLOBAL__sub_I_SPI+0x38>)
 80b09b2:	4d0d      	ldr	r5, [pc, #52]	; (80b09e8 <_GLOBAL__sub_I_SPI+0x3c>)
 80b09b4:	2100      	movs	r1, #0
 80b09b6:	4620      	mov	r0, r4
 80b09b8:	f7ff fce6 	bl	80b0388 <_ZN8SPIClassC1E17HAL_SPI_Interface>
 80b09bc:	4620      	mov	r0, r4

#if Wiring_SPI1
SPIClass SPI1(HAL_SPI_INTERFACE2);
 80b09be:	4c0b      	ldr	r4, [pc, #44]	; (80b09ec <_GLOBAL__sub_I_SPI+0x40>)
#include "core_hal.h"
#include "spark_macros.h"

#ifndef SPARK_WIRING_NO_SPI

SPIClass SPI(HAL_SPI_INTERFACE1);
 80b09c0:	4632      	mov	r2, r6
 80b09c2:	4629      	mov	r1, r5
 80b09c4:	f000 f814 	bl	80b09f0 <__aeabi_atexit>

#if Wiring_SPI1
SPIClass SPI1(HAL_SPI_INTERFACE2);
 80b09c8:	2101      	movs	r1, #1
 80b09ca:	4620      	mov	r0, r4
 80b09cc:	f7ff fcdc 	bl	80b0388 <_ZN8SPIClassC1E17HAL_SPI_Interface>
 80b09d0:	4632      	mov	r2, r6
 80b09d2:	4629      	mov	r1, r5
 80b09d4:	4620      	mov	r0, r4
 80b09d6:	e8bd 4070 	ldmia.w	sp!, {r4, r5, r6, lr}
 80b09da:	f000 b809 	b.w	80b09f0 <__aeabi_atexit>
 80b09de:	bf00      	nop
 80b09e0:	20002b24 	.word	0x20002b24
 80b09e4:	20000594 	.word	0x20000594
 80b09e8:	080b0379 	.word	0x080b0379
 80b09ec:	20002b34 	.word	0x20002b34

080b09f0 <__aeabi_atexit>:
 80b09f0:	460b      	mov	r3, r1
 80b09f2:	4601      	mov	r1, r0
 80b09f4:	4618      	mov	r0, r3
 80b09f6:	f003 b837 	b.w	80b3a68 <__cxa_atexit>

080b09fa <_ZdlPvj>:
 80b09fa:	f7ef bb52 	b.w	80a00a2 <_ZdlPv>
	...

080b0a00 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj>:
 80b0a00:	4b24      	ldr	r3, [pc, #144]	; (80b0a94 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0x94>)
 80b0a02:	681a      	ldr	r2, [r3, #0]
 80b0a04:	07d0      	lsls	r0, r2, #31
 80b0a06:	bf5c      	itt	pl
 80b0a08:	2201      	movpl	r2, #1
 80b0a0a:	601a      	strpl	r2, [r3, #0]
 80b0a0c:	4b22      	ldr	r3, [pc, #136]	; (80b0a98 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0x98>)
 80b0a0e:	681a      	ldr	r2, [r3, #0]
 80b0a10:	07d1      	lsls	r1, r2, #31
 80b0a12:	bf5c      	itt	pl
 80b0a14:	2201      	movpl	r2, #1
 80b0a16:	601a      	strpl	r2, [r3, #0]
 80b0a18:	4b20      	ldr	r3, [pc, #128]	; (80b0a9c <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0x9c>)
 80b0a1a:	681a      	ldr	r2, [r3, #0]
 80b0a1c:	07d2      	lsls	r2, r2, #31
 80b0a1e:	bf5c      	itt	pl
 80b0a20:	2201      	movpl	r2, #1
 80b0a22:	601a      	strpl	r2, [r3, #0]
 80b0a24:	4b1e      	ldr	r3, [pc, #120]	; (80b0aa0 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xa0>)
 80b0a26:	681a      	ldr	r2, [r3, #0]
 80b0a28:	07d0      	lsls	r0, r2, #31
 80b0a2a:	bf5c      	itt	pl
 80b0a2c:	2201      	movpl	r2, #1
 80b0a2e:	601a      	strpl	r2, [r3, #0]
 80b0a30:	4b1c      	ldr	r3, [pc, #112]	; (80b0aa4 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xa4>)
 80b0a32:	681a      	ldr	r2, [r3, #0]
 80b0a34:	07d1      	lsls	r1, r2, #31
 80b0a36:	bf5c      	itt	pl
 80b0a38:	2201      	movpl	r2, #1
 80b0a3a:	601a      	strpl	r2, [r3, #0]
 80b0a3c:	4b1a      	ldr	r3, [pc, #104]	; (80b0aa8 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xa8>)
 80b0a3e:	681a      	ldr	r2, [r3, #0]
 80b0a40:	07d2      	lsls	r2, r2, #31
 80b0a42:	bf5c      	itt	pl
 80b0a44:	2201      	movpl	r2, #1
 80b0a46:	601a      	strpl	r2, [r3, #0]
 80b0a48:	4b18      	ldr	r3, [pc, #96]	; (80b0aac <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xac>)
 80b0a4a:	681a      	ldr	r2, [r3, #0]
 80b0a4c:	07d0      	lsls	r0, r2, #31
 80b0a4e:	bf5c      	itt	pl
 80b0a50:	2201      	movpl	r2, #1
 80b0a52:	601a      	strpl	r2, [r3, #0]
 80b0a54:	4b16      	ldr	r3, [pc, #88]	; (80b0ab0 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xb0>)
 80b0a56:	681a      	ldr	r2, [r3, #0]
 80b0a58:	07d1      	lsls	r1, r2, #31
 80b0a5a:	bf5c      	itt	pl
 80b0a5c:	2201      	movpl	r2, #1
 80b0a5e:	601a      	strpl	r2, [r3, #0]
 80b0a60:	4b14      	ldr	r3, [pc, #80]	; (80b0ab4 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xb4>)
 80b0a62:	681a      	ldr	r2, [r3, #0]
 80b0a64:	07d2      	lsls	r2, r2, #31
 80b0a66:	bf5c      	itt	pl
 80b0a68:	2201      	movpl	r2, #1
 80b0a6a:	601a      	strpl	r2, [r3, #0]
 80b0a6c:	4b12      	ldr	r3, [pc, #72]	; (80b0ab8 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xb8>)
 80b0a6e:	681a      	ldr	r2, [r3, #0]
 80b0a70:	07d0      	lsls	r0, r2, #31
 80b0a72:	bf5c      	itt	pl
 80b0a74:	2201      	movpl	r2, #1
 80b0a76:	601a      	strpl	r2, [r3, #0]
 80b0a78:	4b10      	ldr	r3, [pc, #64]	; (80b0abc <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xbc>)
 80b0a7a:	681a      	ldr	r2, [r3, #0]
 80b0a7c:	07d1      	lsls	r1, r2, #31
 80b0a7e:	bf5c      	itt	pl
 80b0a80:	2201      	movpl	r2, #1
 80b0a82:	601a      	strpl	r2, [r3, #0]
 80b0a84:	4b0e      	ldr	r3, [pc, #56]	; (80b0ac0 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xc0>)
 80b0a86:	681a      	ldr	r2, [r3, #0]
 80b0a88:	07d2      	lsls	r2, r2, #31
 80b0a8a:	bf5c      	itt	pl
 80b0a8c:	2201      	movpl	r2, #1
 80b0a8e:	601a      	strpl	r2, [r3, #0]
 80b0a90:	4770      	bx	lr
 80b0a92:	bf00      	nop
 80b0a94:	20002b70 	.word	0x20002b70
 80b0a98:	20002b6c 	.word	0x20002b6c
 80b0a9c:	20002b68 	.word	0x20002b68
 80b0aa0:	20002b64 	.word	0x20002b64
 80b0aa4:	20002b60 	.word	0x20002b60
 80b0aa8:	20002b5c 	.word	0x20002b5c
 80b0aac:	20002b58 	.word	0x20002b58
 80b0ab0:	20002b54 	.word	0x20002b54
 80b0ab4:	20002b50 	.word	0x20002b50
 80b0ab8:	20002b4c 	.word	0x20002b4c
 80b0abc:	20002b48 	.word	0x20002b48
 80b0ac0:	20002b44 	.word	0x20002b44

080b0ac4 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj>:
 80b0ac4:	4b18      	ldr	r3, [pc, #96]	; (80b0b28 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x64>)
 80b0ac6:	681a      	ldr	r2, [r3, #0]
 80b0ac8:	07d1      	lsls	r1, r2, #31
 80b0aca:	bf5c      	itt	pl
 80b0acc:	2201      	movpl	r2, #1
 80b0ace:	601a      	strpl	r2, [r3, #0]
 80b0ad0:	4b16      	ldr	r3, [pc, #88]	; (80b0b2c <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x68>)
 80b0ad2:	681a      	ldr	r2, [r3, #0]
 80b0ad4:	07d2      	lsls	r2, r2, #31
 80b0ad6:	bf5c      	itt	pl
 80b0ad8:	2201      	movpl	r2, #1
 80b0ada:	601a      	strpl	r2, [r3, #0]
 80b0adc:	4b14      	ldr	r3, [pc, #80]	; (80b0b30 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x6c>)
 80b0ade:	681a      	ldr	r2, [r3, #0]
 80b0ae0:	07d0      	lsls	r0, r2, #31
 80b0ae2:	bf5c      	itt	pl
 80b0ae4:	2201      	movpl	r2, #1
 80b0ae6:	601a      	strpl	r2, [r3, #0]
 80b0ae8:	4b12      	ldr	r3, [pc, #72]	; (80b0b34 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x70>)
 80b0aea:	681a      	ldr	r2, [r3, #0]
 80b0aec:	07d1      	lsls	r1, r2, #31
 80b0aee:	bf5c      	itt	pl
 80b0af0:	2201      	movpl	r2, #1
 80b0af2:	601a      	strpl	r2, [r3, #0]
 80b0af4:	4b10      	ldr	r3, [pc, #64]	; (80b0b38 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x74>)
 80b0af6:	681a      	ldr	r2, [r3, #0]
 80b0af8:	07d2      	lsls	r2, r2, #31
 80b0afa:	bf5c      	itt	pl
 80b0afc:	2201      	movpl	r2, #1
 80b0afe:	601a      	strpl	r2, [r3, #0]
 80b0b00:	4b0e      	ldr	r3, [pc, #56]	; (80b0b3c <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x78>)
 80b0b02:	681a      	ldr	r2, [r3, #0]
 80b0b04:	07d0      	lsls	r0, r2, #31
 80b0b06:	bf5c      	itt	pl
 80b0b08:	2201      	movpl	r2, #1
 80b0b0a:	601a      	strpl	r2, [r3, #0]
 80b0b0c:	4b0c      	ldr	r3, [pc, #48]	; (80b0b40 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x7c>)
 80b0b0e:	681a      	ldr	r2, [r3, #0]
 80b0b10:	07d1      	lsls	r1, r2, #31
 80b0b12:	bf5c      	itt	pl
 80b0b14:	2201      	movpl	r2, #1
 80b0b16:	601a      	strpl	r2, [r3, #0]
 80b0b18:	4b0a      	ldr	r3, [pc, #40]	; (80b0b44 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x80>)
 80b0b1a:	681a      	ldr	r2, [r3, #0]
 80b0b1c:	07d2      	lsls	r2, r2, #31
 80b0b1e:	bf5c      	itt	pl
 80b0b20:	2201      	movpl	r2, #1
 80b0b22:	601a      	strpl	r2, [r3, #0]
 80b0b24:	4770      	bx	lr
 80b0b26:	bf00      	nop
 80b0b28:	20002b90 	.word	0x20002b90
 80b0b2c:	20002b8c 	.word	0x20002b8c
 80b0b30:	20002b88 	.word	0x20002b88
 80b0b34:	20002b84 	.word	0x20002b84
 80b0b38:	20002b80 	.word	0x20002b80
 80b0b3c:	20002b7c 	.word	0x20002b7c
 80b0b40:	20002b78 	.word	0x20002b78
 80b0b44:	20002b74 	.word	0x20002b74

080b0b48 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj>:
 80b0b48:	4b18      	ldr	r3, [pc, #96]	; (80b0bac <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x64>)
 80b0b4a:	681a      	ldr	r2, [r3, #0]
 80b0b4c:	07d1      	lsls	r1, r2, #31
 80b0b4e:	bf5c      	itt	pl
 80b0b50:	2201      	movpl	r2, #1
 80b0b52:	601a      	strpl	r2, [r3, #0]
 80b0b54:	4b16      	ldr	r3, [pc, #88]	; (80b0bb0 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x68>)
 80b0b56:	681a      	ldr	r2, [r3, #0]
 80b0b58:	07d2      	lsls	r2, r2, #31
 80b0b5a:	bf5c      	itt	pl
 80b0b5c:	2201      	movpl	r2, #1
 80b0b5e:	601a      	strpl	r2, [r3, #0]
 80b0b60:	4b14      	ldr	r3, [pc, #80]	; (80b0bb4 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x6c>)
 80b0b62:	681a      	ldr	r2, [r3, #0]
 80b0b64:	07d0      	lsls	r0, r2, #31
 80b0b66:	bf5c      	itt	pl
 80b0b68:	2201      	movpl	r2, #1
 80b0b6a:	601a      	strpl	r2, [r3, #0]
 80b0b6c:	4b12      	ldr	r3, [pc, #72]	; (80b0bb8 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x70>)
 80b0b6e:	681a      	ldr	r2, [r3, #0]
 80b0b70:	07d1      	lsls	r1, r2, #31
 80b0b72:	bf5c      	itt	pl
 80b0b74:	2201      	movpl	r2, #1
 80b0b76:	601a      	strpl	r2, [r3, #0]
 80b0b78:	4b10      	ldr	r3, [pc, #64]	; (80b0bbc <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x74>)
 80b0b7a:	681a      	ldr	r2, [r3, #0]
 80b0b7c:	07d2      	lsls	r2, r2, #31
 80b0b7e:	bf5c      	itt	pl
 80b0b80:	2201      	movpl	r2, #1
 80b0b82:	601a      	strpl	r2, [r3, #0]
 80b0b84:	4b0e      	ldr	r3, [pc, #56]	; (80b0bc0 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x78>)
 80b0b86:	681a      	ldr	r2, [r3, #0]
 80b0b88:	07d0      	lsls	r0, r2, #31
 80b0b8a:	bf5c      	itt	pl
 80b0b8c:	2201      	movpl	r2, #1
 80b0b8e:	601a      	strpl	r2, [r3, #0]
 80b0b90:	4b0c      	ldr	r3, [pc, #48]	; (80b0bc4 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x7c>)
 80b0b92:	681a      	ldr	r2, [r3, #0]
 80b0b94:	07d1      	lsls	r1, r2, #31
 80b0b96:	bf5c      	itt	pl
 80b0b98:	2201      	movpl	r2, #1
 80b0b9a:	601a      	strpl	r2, [r3, #0]
 80b0b9c:	4b0a      	ldr	r3, [pc, #40]	; (80b0bc8 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x80>)
 80b0b9e:	681a      	ldr	r2, [r3, #0]
 80b0ba0:	07d2      	lsls	r2, r2, #31
 80b0ba2:	bf5c      	itt	pl
 80b0ba4:	2201      	movpl	r2, #1
 80b0ba6:	601a      	strpl	r2, [r3, #0]
 80b0ba8:	4770      	bx	lr
 80b0baa:	bf00      	nop
 80b0bac:	20002bb0 	.word	0x20002bb0
 80b0bb0:	20002bac 	.word	0x20002bac
 80b0bb4:	20002ba8 	.word	0x20002ba8
 80b0bb8:	20002ba4 	.word	0x20002ba4
 80b0bbc:	20002ba0 	.word	0x20002ba0
 80b0bc0:	20002b9c 	.word	0x20002b9c
 80b0bc4:	20002b98 	.word	0x20002b98
 80b0bc8:	20002b94 	.word	0x20002b94

080b0bcc <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj>:
 80b0bcc:	4b24      	ldr	r3, [pc, #144]	; (80b0c60 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0x94>)
 80b0bce:	681a      	ldr	r2, [r3, #0]
 80b0bd0:	07d0      	lsls	r0, r2, #31
 80b0bd2:	bf5c      	itt	pl
 80b0bd4:	2201      	movpl	r2, #1
 80b0bd6:	601a      	strpl	r2, [r3, #0]
 80b0bd8:	4b22      	ldr	r3, [pc, #136]	; (80b0c64 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0x98>)
 80b0bda:	681a      	ldr	r2, [r3, #0]
 80b0bdc:	07d1      	lsls	r1, r2, #31
 80b0bde:	bf5c      	itt	pl
 80b0be0:	2201      	movpl	r2, #1
 80b0be2:	601a      	strpl	r2, [r3, #0]
 80b0be4:	4b20      	ldr	r3, [pc, #128]	; (80b0c68 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0x9c>)
 80b0be6:	681a      	ldr	r2, [r3, #0]
 80b0be8:	07d2      	lsls	r2, r2, #31
 80b0bea:	bf5c      	itt	pl
 80b0bec:	2201      	movpl	r2, #1
 80b0bee:	601a      	strpl	r2, [r3, #0]
 80b0bf0:	4b1e      	ldr	r3, [pc, #120]	; (80b0c6c <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xa0>)
 80b0bf2:	681a      	ldr	r2, [r3, #0]
 80b0bf4:	07d0      	lsls	r0, r2, #31
 80b0bf6:	bf5c      	itt	pl
 80b0bf8:	2201      	movpl	r2, #1
 80b0bfa:	601a      	strpl	r2, [r3, #0]
 80b0bfc:	4b1c      	ldr	r3, [pc, #112]	; (80b0c70 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xa4>)
 80b0bfe:	681a      	ldr	r2, [r3, #0]
 80b0c00:	07d1      	lsls	r1, r2, #31
 80b0c02:	bf5c      	itt	pl
 80b0c04:	2201      	movpl	r2, #1
 80b0c06:	601a      	strpl	r2, [r3, #0]
 80b0c08:	4b1a      	ldr	r3, [pc, #104]	; (80b0c74 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xa8>)
 80b0c0a:	681a      	ldr	r2, [r3, #0]
 80b0c0c:	07d2      	lsls	r2, r2, #31
 80b0c0e:	bf5c      	itt	pl
 80b0c10:	2201      	movpl	r2, #1
 80b0c12:	601a      	strpl	r2, [r3, #0]
 80b0c14:	4b18      	ldr	r3, [pc, #96]	; (80b0c78 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xac>)
 80b0c16:	681a      	ldr	r2, [r3, #0]
 80b0c18:	07d0      	lsls	r0, r2, #31
 80b0c1a:	bf5c      	itt	pl
 80b0c1c:	2201      	movpl	r2, #1
 80b0c1e:	601a      	strpl	r2, [r3, #0]
 80b0c20:	4b16      	ldr	r3, [pc, #88]	; (80b0c7c <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xb0>)
 80b0c22:	681a      	ldr	r2, [r3, #0]
 80b0c24:	07d1      	lsls	r1, r2, #31
 80b0c26:	bf5c      	itt	pl
 80b0c28:	2201      	movpl	r2, #1
 80b0c2a:	601a      	strpl	r2, [r3, #0]
 80b0c2c:	4b14      	ldr	r3, [pc, #80]	; (80b0c80 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xb4>)
 80b0c2e:	681a      	ldr	r2, [r3, #0]
 80b0c30:	07d2      	lsls	r2, r2, #31
 80b0c32:	bf5c      	itt	pl
 80b0c34:	2201      	movpl	r2, #1
 80b0c36:	601a      	strpl	r2, [r3, #0]
 80b0c38:	4b12      	ldr	r3, [pc, #72]	; (80b0c84 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xb8>)
 80b0c3a:	681a      	ldr	r2, [r3, #0]
 80b0c3c:	07d0      	lsls	r0, r2, #31
 80b0c3e:	bf5c      	itt	pl
 80b0c40:	2201      	movpl	r2, #1
 80b0c42:	601a      	strpl	r2, [r3, #0]
 80b0c44:	4b10      	ldr	r3, [pc, #64]	; (80b0c88 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xbc>)
 80b0c46:	681a      	ldr	r2, [r3, #0]
 80b0c48:	07d1      	lsls	r1, r2, #31
 80b0c4a:	bf5c      	itt	pl
 80b0c4c:	2201      	movpl	r2, #1
 80b0c4e:	601a      	strpl	r2, [r3, #0]
 80b0c50:	4b0e      	ldr	r3, [pc, #56]	; (80b0c8c <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xc0>)
 80b0c52:	681a      	ldr	r2, [r3, #0]
 80b0c54:	07d2      	lsls	r2, r2, #31
 80b0c56:	bf5c      	itt	pl
 80b0c58:	2201      	movpl	r2, #1
 80b0c5a:	601a      	strpl	r2, [r3, #0]
 80b0c5c:	4770      	bx	lr
 80b0c5e:	bf00      	nop
 80b0c60:	20002be0 	.word	0x20002be0
 80b0c64:	20002bdc 	.word	0x20002bdc
 80b0c68:	20002bd8 	.word	0x20002bd8
 80b0c6c:	20002bd4 	.word	0x20002bd4
 80b0c70:	20002bd0 	.word	0x20002bd0
 80b0c74:	20002bcc 	.word	0x20002bcc
 80b0c78:	20002bc8 	.word	0x20002bc8
 80b0c7c:	20002bc4 	.word	0x20002bc4
 80b0c80:	20002bc0 	.word	0x20002bc0
 80b0c84:	20002bbc 	.word	0x20002bbc
 80b0c88:	20002bb8 	.word	0x20002bb8
 80b0c8c:	20002bb4 	.word	0x20002bb4

080b0c90 <floor>:
 80b0c90:	f3c1 520a 	ubfx	r2, r1, #20, #11
 80b0c94:	e92d 43f8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, lr}
 80b0c98:	f2a2 36ff 	subw	r6, r2, #1023	; 0x3ff
 80b0c9c:	2e13      	cmp	r6, #19
 80b0c9e:	460b      	mov	r3, r1
 80b0ca0:	4604      	mov	r4, r0
 80b0ca2:	460d      	mov	r5, r1
 80b0ca4:	4688      	mov	r8, r1
 80b0ca6:	4607      	mov	r7, r0
 80b0ca8:	dc1c      	bgt.n	80b0ce4 <floor+0x54>
 80b0caa:	2e00      	cmp	r6, #0
 80b0cac:	db3f      	blt.n	80b0d2e <floor+0x9e>
 80b0cae:	4a3a      	ldr	r2, [pc, #232]	; (80b0d98 <floor+0x108>)
 80b0cb0:	4686      	mov	lr, r0
 80b0cb2:	fa42 f906 	asr.w	r9, r2, r6
 80b0cb6:	ea01 0209 	and.w	r2, r1, r9
 80b0cba:	4302      	orrs	r2, r0
 80b0cbc:	d017      	beq.n	80b0cee <floor+0x5e>
 80b0cbe:	a334      	add	r3, pc, #208	; (adr r3, 80b0d90 <floor+0x100>)
 80b0cc0:	e9d3 2300 	ldrd	r2, r3, [r3]
 80b0cc4:	f001 fea0 	bl	80b2a08 <__adddf3>
 80b0cc8:	2200      	movs	r2, #0
 80b0cca:	2300      	movs	r3, #0
 80b0ccc:	f002 fade 	bl	80b328c <__aeabi_dcmpgt>
 80b0cd0:	b120      	cbz	r0, 80b0cdc <floor+0x4c>
 80b0cd2:	2d00      	cmp	r5, #0
 80b0cd4:	db40      	blt.n	80b0d58 <floor+0xc8>
 80b0cd6:	ea28 0509 	bic.w	r5, r8, r9
 80b0cda:	2700      	movs	r7, #0
 80b0cdc:	4638      	mov	r0, r7
 80b0cde:	4629      	mov	r1, r5
 80b0ce0:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
 80b0ce4:	2e33      	cmp	r6, #51	; 0x33
 80b0ce6:	dd06      	ble.n	80b0cf6 <floor+0x66>
 80b0ce8:	f5b6 6f80 	cmp.w	r6, #1024	; 0x400
 80b0cec:	d02f      	beq.n	80b0d4e <floor+0xbe>
 80b0cee:	4620      	mov	r0, r4
 80b0cf0:	4619      	mov	r1, r3
 80b0cf2:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
 80b0cf6:	f2a2 4213 	subw	r2, r2, #1043	; 0x413
 80b0cfa:	f04f 39ff 	mov.w	r9, #4294967295	; 0xffffffff
 80b0cfe:	fa29 f902 	lsr.w	r9, r9, r2
 80b0d02:	ea10 0f09 	tst.w	r0, r9
 80b0d06:	d0f2      	beq.n	80b0cee <floor+0x5e>
 80b0d08:	a321      	add	r3, pc, #132	; (adr r3, 80b0d90 <floor+0x100>)
 80b0d0a:	e9d3 2300 	ldrd	r2, r3, [r3]
 80b0d0e:	f001 fe7b 	bl	80b2a08 <__adddf3>
 80b0d12:	2200      	movs	r2, #0
 80b0d14:	2300      	movs	r3, #0
 80b0d16:	f002 fab9 	bl	80b328c <__aeabi_dcmpgt>
 80b0d1a:	2800      	cmp	r0, #0
 80b0d1c:	d0de      	beq.n	80b0cdc <floor+0x4c>
 80b0d1e:	2d00      	cmp	r5, #0
 80b0d20:	db20      	blt.n	80b0d64 <floor+0xd4>
 80b0d22:	4645      	mov	r5, r8
 80b0d24:	ea27 0709 	bic.w	r7, r7, r9
 80b0d28:	4638      	mov	r0, r7
 80b0d2a:	4629      	mov	r1, r5
 80b0d2c:	e7d8      	b.n	80b0ce0 <floor+0x50>
 80b0d2e:	a318      	add	r3, pc, #96	; (adr r3, 80b0d90 <floor+0x100>)
 80b0d30:	e9d3 2300 	ldrd	r2, r3, [r3]
 80b0d34:	f001 fe68 	bl	80b2a08 <__adddf3>
 80b0d38:	2200      	movs	r2, #0
 80b0d3a:	2300      	movs	r3, #0
 80b0d3c:	f002 faa6 	bl	80b328c <__aeabi_dcmpgt>
 80b0d40:	2800      	cmp	r0, #0
 80b0d42:	d0cb      	beq.n	80b0cdc <floor+0x4c>
 80b0d44:	2d00      	cmp	r5, #0
 80b0d46:	db18      	blt.n	80b0d7a <floor+0xea>
 80b0d48:	2700      	movs	r7, #0
 80b0d4a:	463d      	mov	r5, r7
 80b0d4c:	e7c6      	b.n	80b0cdc <floor+0x4c>
 80b0d4e:	4602      	mov	r2, r0
 80b0d50:	460b      	mov	r3, r1
 80b0d52:	f001 fe59 	bl	80b2a08 <__adddf3>
 80b0d56:	e7cc      	b.n	80b0cf2 <floor+0x62>
 80b0d58:	f44f 1380 	mov.w	r3, #1048576	; 0x100000
 80b0d5c:	fa43 f606 	asr.w	r6, r3, r6
 80b0d60:	44b0      	add	r8, r6
 80b0d62:	e7b8      	b.n	80b0cd6 <floor+0x46>
 80b0d64:	2e14      	cmp	r6, #20
 80b0d66:	d010      	beq.n	80b0d8a <floor+0xfa>
 80b0d68:	2301      	movs	r3, #1
 80b0d6a:	f1c6 0634 	rsb	r6, r6, #52	; 0x34
 80b0d6e:	fa03 f606 	lsl.w	r6, r3, r6
 80b0d72:	1937      	adds	r7, r6, r4
 80b0d74:	bf28      	it	cs
 80b0d76:	4498      	addcs	r8, r3
 80b0d78:	e7d3      	b.n	80b0d22 <floor+0x92>
 80b0d7a:	f025 4200 	bic.w	r2, r5, #2147483648	; 0x80000000
 80b0d7e:	4b07      	ldr	r3, [pc, #28]	; (80b0d9c <floor+0x10c>)
 80b0d80:	4322      	orrs	r2, r4
 80b0d82:	bf18      	it	ne
 80b0d84:	461d      	movne	r5, r3
 80b0d86:	2700      	movs	r7, #0
 80b0d88:	e7a8      	b.n	80b0cdc <floor+0x4c>
 80b0d8a:	f105 0801 	add.w	r8, r5, #1
 80b0d8e:	e7c8      	b.n	80b0d22 <floor+0x92>
 80b0d90:	8800759c 	.word	0x8800759c
 80b0d94:	7e37e43c 	.word	0x7e37e43c
 80b0d98:	000fffff 	.word	0x000fffff
 80b0d9c:	bff00000 	.word	0xbff00000

080b0da0 <frexp>:
 80b0da0:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
 80b0da2:	4616      	mov	r6, r2
 80b0da4:	f8df e05c 	ldr.w	lr, [pc, #92]	; 80b0e04 <frexp+0x64>
 80b0da8:	f021 4300 	bic.w	r3, r1, #2147483648	; 0x80000000
 80b0dac:	2200      	movs	r2, #0
 80b0dae:	4573      	cmp	r3, lr
 80b0db0:	4604      	mov	r4, r0
 80b0db2:	460d      	mov	r5, r1
 80b0db4:	6032      	str	r2, [r6, #0]
 80b0db6:	dc20      	bgt.n	80b0dfa <frexp+0x5a>
 80b0db8:	ea50 0e03 	orrs.w	lr, r0, r3
 80b0dbc:	d01d      	beq.n	80b0dfa <frexp+0x5a>
 80b0dbe:	f5b3 1f80 	cmp.w	r3, #1048576	; 0x100000
 80b0dc2:	460f      	mov	r7, r1
 80b0dc4:	da09      	bge.n	80b0dda <frexp+0x3a>
 80b0dc6:	2200      	movs	r2, #0
 80b0dc8:	4b0d      	ldr	r3, [pc, #52]	; (80b0e00 <frexp+0x60>)
 80b0dca:	f001 ffcf 	bl	80b2d6c <__aeabi_dmul>
 80b0dce:	4604      	mov	r4, r0
 80b0dd0:	460f      	mov	r7, r1
 80b0dd2:	f06f 0235 	mvn.w	r2, #53	; 0x35
 80b0dd6:	f021 4300 	bic.w	r3, r1, #2147483648	; 0x80000000
 80b0dda:	f027 47ff 	bic.w	r7, r7, #2139095040	; 0x7f800000
 80b0dde:	f427 07e0 	bic.w	r7, r7, #7340032	; 0x700000
 80b0de2:	151b      	asrs	r3, r3, #20
 80b0de4:	f047 557f 	orr.w	r5, r7, #1069547520	; 0x3fc00000
 80b0de8:	f2a3 33fe 	subw	r3, r3, #1022	; 0x3fe
 80b0dec:	f445 1500 	orr.w	r5, r5, #2097152	; 0x200000
 80b0df0:	4413      	add	r3, r2
 80b0df2:	4620      	mov	r0, r4
 80b0df4:	4629      	mov	r1, r5
 80b0df6:	6033      	str	r3, [r6, #0]
 80b0df8:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
 80b0dfa:	4620      	mov	r0, r4
 80b0dfc:	4629      	mov	r1, r5
 80b0dfe:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
 80b0e00:	43500000 	.word	0x43500000
 80b0e04:	7fefffff 	.word	0x7fefffff

080b0e08 <round>:
 80b0e08:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
 80b0e0a:	f3c1 560a 	ubfx	r6, r1, #20, #11
 80b0e0e:	f2a6 35ff 	subw	r5, r6, #1023	; 0x3ff
 80b0e12:	2d13      	cmp	r5, #19
 80b0e14:	460a      	mov	r2, r1
 80b0e16:	460b      	mov	r3, r1
 80b0e18:	4604      	mov	r4, r0
 80b0e1a:	4686      	mov	lr, r0
 80b0e1c:	dc11      	bgt.n	80b0e42 <round+0x3a>
 80b0e1e:	2d00      	cmp	r5, #0
 80b0e20:	db2e      	blt.n	80b0e80 <round+0x78>
 80b0e22:	460f      	mov	r7, r1
 80b0e24:	491f      	ldr	r1, [pc, #124]	; (80b0ea4 <round+0x9c>)
 80b0e26:	4129      	asrs	r1, r5
 80b0e28:	420a      	tst	r2, r1
 80b0e2a:	d025      	beq.n	80b0e78 <round+0x70>
 80b0e2c:	f44f 2300 	mov.w	r3, #524288	; 0x80000
 80b0e30:	412b      	asrs	r3, r5
 80b0e32:	443b      	add	r3, r7
 80b0e34:	ea23 0301 	bic.w	r3, r3, r1
 80b0e38:	2600      	movs	r6, #0
 80b0e3a:	4619      	mov	r1, r3
 80b0e3c:	4634      	mov	r4, r6
 80b0e3e:	4620      	mov	r0, r4
 80b0e40:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
 80b0e42:	2d33      	cmp	r5, #51	; 0x33
 80b0e44:	dd05      	ble.n	80b0e52 <round+0x4a>
 80b0e46:	f5b5 6f80 	cmp.w	r5, #1024	; 0x400
 80b0e4a:	d01f      	beq.n	80b0e8c <round+0x84>
 80b0e4c:	4611      	mov	r1, r2
 80b0e4e:	4620      	mov	r0, r4
 80b0e50:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
 80b0e52:	f2a6 4613 	subw	r6, r6, #1043	; 0x413
 80b0e56:	f04f 31ff 	mov.w	r1, #4294967295	; 0xffffffff
 80b0e5a:	fa21 f606 	lsr.w	r6, r1, r6
 80b0e5e:	4230      	tst	r0, r6
 80b0e60:	d0f4      	beq.n	80b0e4c <round+0x44>
 80b0e62:	2201      	movs	r2, #1
 80b0e64:	f1c5 0533 	rsb	r5, r5, #51	; 0x33
 80b0e68:	fa02 f505 	lsl.w	r5, r2, r5
 80b0e6c:	182d      	adds	r5, r5, r0
 80b0e6e:	bf28      	it	cs
 80b0e70:	189b      	addcs	r3, r3, r2
 80b0e72:	ea25 0606 	bic.w	r6, r5, r6
 80b0e76:	e7e0      	b.n	80b0e3a <round+0x32>
 80b0e78:	2800      	cmp	r0, #0
 80b0e7a:	d1d7      	bne.n	80b0e2c <round+0x24>
 80b0e7c:	4611      	mov	r1, r2
 80b0e7e:	e7e6      	b.n	80b0e4e <round+0x46>
 80b0e80:	3501      	adds	r5, #1
 80b0e82:	f001 4300 	and.w	r3, r1, #2147483648	; 0x80000000
 80b0e86:	d007      	beq.n	80b0e98 <round+0x90>
 80b0e88:	2600      	movs	r6, #0
 80b0e8a:	e7d6      	b.n	80b0e3a <round+0x32>
 80b0e8c:	4602      	mov	r2, r0
 80b0e8e:	460b      	mov	r3, r1
 80b0e90:	f001 fdba 	bl	80b2a08 <__adddf3>
 80b0e94:	4604      	mov	r4, r0
 80b0e96:	e7d2      	b.n	80b0e3e <round+0x36>
 80b0e98:	f043 537f 	orr.w	r3, r3, #1069547520	; 0x3fc00000
 80b0e9c:	f443 1340 	orr.w	r3, r3, #3145728	; 0x300000
 80b0ea0:	2600      	movs	r6, #0
 80b0ea2:	e7ca      	b.n	80b0e3a <round+0x32>
 80b0ea4:	000fffff 	.word	0x000fffff

080b0ea8 <ceilf>:
 80b0ea8:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
 80b0eac:	f020 4700 	bic.w	r7, r0, #2147483648	; 0x80000000
 80b0eb0:	0dfc      	lsrs	r4, r7, #23
 80b0eb2:	3c7f      	subs	r4, #127	; 0x7f
 80b0eb4:	2c16      	cmp	r4, #22
 80b0eb6:	4605      	mov	r5, r0
 80b0eb8:	dc18      	bgt.n	80b0eec <ceilf+0x44>
 80b0eba:	2c00      	cmp	r4, #0
 80b0ebc:	4680      	mov	r8, r0
 80b0ebe:	db1d      	blt.n	80b0efc <ceilf+0x54>
 80b0ec0:	4f19      	ldr	r7, [pc, #100]	; (80b0f28 <ceilf+0x80>)
 80b0ec2:	4127      	asrs	r7, r4
 80b0ec4:	4238      	tst	r0, r7
 80b0ec6:	d023      	beq.n	80b0f10 <ceilf+0x68>
 80b0ec8:	4918      	ldr	r1, [pc, #96]	; (80b0f2c <ceilf+0x84>)
 80b0eca:	f002 fa87 	bl	80b33dc <__addsf3>
 80b0ece:	2100      	movs	r1, #0
 80b0ed0:	f002 fd48 	bl	80b3964 <__aeabi_fcmpgt>
 80b0ed4:	b1e0      	cbz	r0, 80b0f10 <ceilf+0x68>
 80b0ed6:	2d00      	cmp	r5, #0
 80b0ed8:	dd04      	ble.n	80b0ee4 <ceilf+0x3c>
 80b0eda:	f44f 0300 	mov.w	r3, #8388608	; 0x800000
 80b0ede:	fa43 f404 	asr.w	r4, r3, r4
 80b0ee2:	44a0      	add	r8, r4
 80b0ee4:	ea28 0007 	bic.w	r0, r8, r7
 80b0ee8:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80b0eec:	f1b7 4fff 	cmp.w	r7, #2139095040	; 0x7f800000
 80b0ef0:	d30e      	bcc.n	80b0f10 <ceilf+0x68>
 80b0ef2:	4601      	mov	r1, r0
 80b0ef4:	f002 fa72 	bl	80b33dc <__addsf3>
 80b0ef8:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80b0efc:	490b      	ldr	r1, [pc, #44]	; (80b0f2c <ceilf+0x84>)
 80b0efe:	f002 fa6d 	bl	80b33dc <__addsf3>
 80b0f02:	2100      	movs	r1, #0
 80b0f04:	f002 fd2e 	bl	80b3964 <__aeabi_fcmpgt>
 80b0f08:	b110      	cbz	r0, 80b0f10 <ceilf+0x68>
 80b0f0a:	2d00      	cmp	r5, #0
 80b0f0c:	db07      	blt.n	80b0f1e <ceilf+0x76>
 80b0f0e:	b917      	cbnz	r7, 80b0f16 <ceilf+0x6e>
 80b0f10:	4628      	mov	r0, r5
 80b0f12:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80b0f16:	f04f 507e 	mov.w	r0, #1065353216	; 0x3f800000
 80b0f1a:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80b0f1e:	f04f 4000 	mov.w	r0, #2147483648	; 0x80000000
 80b0f22:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80b0f26:	bf00      	nop
 80b0f28:	007fffff 	.word	0x007fffff
 80b0f2c:	7149f2ca 	.word	0x7149f2ca

080b0f30 <cosf>:
 80b0f30:	b500      	push	{lr}
 80b0f32:	4a1c      	ldr	r2, [pc, #112]	; (80b0fa4 <cosf+0x74>)
 80b0f34:	f020 4300 	bic.w	r3, r0, #2147483648	; 0x80000000
 80b0f38:	4293      	cmp	r3, r2
 80b0f3a:	b083      	sub	sp, #12
 80b0f3c:	dd18      	ble.n	80b0f70 <cosf+0x40>
 80b0f3e:	f1b3 4fff 	cmp.w	r3, #2139095040	; 0x7f800000
 80b0f42:	db05      	blt.n	80b0f50 <cosf+0x20>
 80b0f44:	4601      	mov	r1, r0
 80b0f46:	f002 fa47 	bl	80b33d8 <__aeabi_fsub>
 80b0f4a:	b003      	add	sp, #12
 80b0f4c:	f85d fb04 	ldr.w	pc, [sp], #4
 80b0f50:	4669      	mov	r1, sp
 80b0f52:	f000 fe7b 	bl	80b1c4c <__ieee754_rem_pio2f>
 80b0f56:	f000 0203 	and.w	r2, r0, #3
 80b0f5a:	2a01      	cmp	r2, #1
 80b0f5c:	d015      	beq.n	80b0f8a <cosf+0x5a>
 80b0f5e:	2a02      	cmp	r2, #2
 80b0f60:	d00c      	beq.n	80b0f7c <cosf+0x4c>
 80b0f62:	b1ca      	cbz	r2, 80b0f98 <cosf+0x68>
 80b0f64:	2201      	movs	r2, #1
 80b0f66:	9901      	ldr	r1, [sp, #4]
 80b0f68:	9800      	ldr	r0, [sp, #0]
 80b0f6a:	f001 fc4d 	bl	80b2808 <__kernel_sinf>
 80b0f6e:	e7ec      	b.n	80b0f4a <cosf+0x1a>
 80b0f70:	2100      	movs	r1, #0
 80b0f72:	f001 f831 	bl	80b1fd8 <__kernel_cosf>
 80b0f76:	b003      	add	sp, #12
 80b0f78:	f85d fb04 	ldr.w	pc, [sp], #4
 80b0f7c:	9901      	ldr	r1, [sp, #4]
 80b0f7e:	9800      	ldr	r0, [sp, #0]
 80b0f80:	f001 f82a 	bl	80b1fd8 <__kernel_cosf>
 80b0f84:	f100 4000 	add.w	r0, r0, #2147483648	; 0x80000000
 80b0f88:	e7df      	b.n	80b0f4a <cosf+0x1a>
 80b0f8a:	9901      	ldr	r1, [sp, #4]
 80b0f8c:	9800      	ldr	r0, [sp, #0]
 80b0f8e:	f001 fc3b 	bl	80b2808 <__kernel_sinf>
 80b0f92:	f100 4000 	add.w	r0, r0, #2147483648	; 0x80000000
 80b0f96:	e7d8      	b.n	80b0f4a <cosf+0x1a>
 80b0f98:	9901      	ldr	r1, [sp, #4]
 80b0f9a:	9800      	ldr	r0, [sp, #0]
 80b0f9c:	f001 f81c 	bl	80b1fd8 <__kernel_cosf>
 80b0fa0:	e7d3      	b.n	80b0f4a <cosf+0x1a>
 80b0fa2:	bf00      	nop
 80b0fa4:	3f490fd8 	.word	0x3f490fd8

080b0fa8 <floorf>:
 80b0fa8:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
 80b0fac:	f020 4700 	bic.w	r7, r0, #2147483648	; 0x80000000
 80b0fb0:	0dfc      	lsrs	r4, r7, #23
 80b0fb2:	3c7f      	subs	r4, #127	; 0x7f
 80b0fb4:	2c16      	cmp	r4, #22
 80b0fb6:	4605      	mov	r5, r0
 80b0fb8:	dc13      	bgt.n	80b0fe2 <floorf+0x3a>
 80b0fba:	2c00      	cmp	r4, #0
 80b0fbc:	4680      	mov	r8, r0
 80b0fbe:	db1b      	blt.n	80b0ff8 <floorf+0x50>
 80b0fc0:	4f19      	ldr	r7, [pc, #100]	; (80b1028 <floorf+0x80>)
 80b0fc2:	4127      	asrs	r7, r4
 80b0fc4:	4238      	tst	r0, r7
 80b0fc6:	d014      	beq.n	80b0ff2 <floorf+0x4a>
 80b0fc8:	4918      	ldr	r1, [pc, #96]	; (80b102c <floorf+0x84>)
 80b0fca:	f002 fa07 	bl	80b33dc <__addsf3>
 80b0fce:	2100      	movs	r1, #0
 80b0fd0:	f002 fcc8 	bl	80b3964 <__aeabi_fcmpgt>
 80b0fd4:	b168      	cbz	r0, 80b0ff2 <floorf+0x4a>
 80b0fd6:	2d00      	cmp	r5, #0
 80b0fd8:	db1b      	blt.n	80b1012 <floorf+0x6a>
 80b0fda:	ea28 0007 	bic.w	r0, r8, r7
 80b0fde:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80b0fe2:	f1b7 4fff 	cmp.w	r7, #2139095040	; 0x7f800000
 80b0fe6:	d304      	bcc.n	80b0ff2 <floorf+0x4a>
 80b0fe8:	4601      	mov	r1, r0
 80b0fea:	f002 f9f7 	bl	80b33dc <__addsf3>
 80b0fee:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80b0ff2:	4628      	mov	r0, r5
 80b0ff4:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80b0ff8:	490c      	ldr	r1, [pc, #48]	; (80b102c <floorf+0x84>)
 80b0ffa:	f002 f9ef 	bl	80b33dc <__addsf3>
 80b0ffe:	2100      	movs	r1, #0
 80b1000:	f002 fcb0 	bl	80b3964 <__aeabi_fcmpgt>
 80b1004:	2800      	cmp	r0, #0
 80b1006:	d0f4      	beq.n	80b0ff2 <floorf+0x4a>
 80b1008:	2d00      	cmp	r5, #0
 80b100a:	db08      	blt.n	80b101e <floorf+0x76>
 80b100c:	2000      	movs	r0, #0
 80b100e:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80b1012:	f44f 0300 	mov.w	r3, #8388608	; 0x800000
 80b1016:	fa43 f404 	asr.w	r4, r3, r4
 80b101a:	44a0      	add	r8, r4
 80b101c:	e7dd      	b.n	80b0fda <floorf+0x32>
 80b101e:	2f00      	cmp	r7, #0
 80b1020:	d0e7      	beq.n	80b0ff2 <floorf+0x4a>
 80b1022:	4803      	ldr	r0, [pc, #12]	; (80b1030 <floorf+0x88>)
 80b1024:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80b1028:	007fffff 	.word	0x007fffff
 80b102c:	7149f2ca 	.word	0x7149f2ca
 80b1030:	bf800000 	.word	0xbf800000

080b1034 <fmaxf>:
 80b1034:	b538      	push	{r3, r4, r5, lr}
 80b1036:	460c      	mov	r4, r1
 80b1038:	4605      	mov	r5, r0
 80b103a:	f000 f823 	bl	80b1084 <__fpclassifyf>
 80b103e:	b908      	cbnz	r0, 80b1044 <fmaxf+0x10>
 80b1040:	4620      	mov	r0, r4
 80b1042:	bd38      	pop	{r3, r4, r5, pc}
 80b1044:	4620      	mov	r0, r4
 80b1046:	f000 f81d 	bl	80b1084 <__fpclassifyf>
 80b104a:	b128      	cbz	r0, 80b1058 <fmaxf+0x24>
 80b104c:	4621      	mov	r1, r4
 80b104e:	4628      	mov	r0, r5
 80b1050:	f002 fc88 	bl	80b3964 <__aeabi_fcmpgt>
 80b1054:	2800      	cmp	r0, #0
 80b1056:	d0f3      	beq.n	80b1040 <fmaxf+0xc>
 80b1058:	4628      	mov	r0, r5
 80b105a:	bd38      	pop	{r3, r4, r5, pc}

080b105c <fminf>:
 80b105c:	b538      	push	{r3, r4, r5, lr}
 80b105e:	460c      	mov	r4, r1
 80b1060:	4605      	mov	r5, r0
 80b1062:	f000 f80f 	bl	80b1084 <__fpclassifyf>
 80b1066:	b908      	cbnz	r0, 80b106c <fminf+0x10>
 80b1068:	4620      	mov	r0, r4
 80b106a:	bd38      	pop	{r3, r4, r5, pc}
 80b106c:	4620      	mov	r0, r4
 80b106e:	f000 f809 	bl	80b1084 <__fpclassifyf>
 80b1072:	b128      	cbz	r0, 80b1080 <fminf+0x24>
 80b1074:	4621      	mov	r1, r4
 80b1076:	4628      	mov	r0, r5
 80b1078:	f002 fc56 	bl	80b3928 <__aeabi_fcmplt>
 80b107c:	2800      	cmp	r0, #0
 80b107e:	d0f3      	beq.n	80b1068 <fminf+0xc>
 80b1080:	4628      	mov	r0, r5
 80b1082:	bd38      	pop	{r3, r4, r5, pc}

080b1084 <__fpclassifyf>:
 80b1084:	f030 4000 	bics.w	r0, r0, #2147483648	; 0x80000000
 80b1088:	d101      	bne.n	80b108e <__fpclassifyf+0xa>
 80b108a:	2002      	movs	r0, #2
 80b108c:	4770      	bx	lr
 80b108e:	f5a0 0300 	sub.w	r3, r0, #8388608	; 0x800000
 80b1092:	f1b3 4ffe 	cmp.w	r3, #2130706432	; 0x7f000000
 80b1096:	d201      	bcs.n	80b109c <__fpclassifyf+0x18>
 80b1098:	2004      	movs	r0, #4
 80b109a:	4770      	bx	lr
 80b109c:	4b05      	ldr	r3, [pc, #20]	; (80b10b4 <__fpclassifyf+0x30>)
 80b109e:	1e42      	subs	r2, r0, #1
 80b10a0:	429a      	cmp	r2, r3
 80b10a2:	d801      	bhi.n	80b10a8 <__fpclassifyf+0x24>
 80b10a4:	2003      	movs	r0, #3
 80b10a6:	4770      	bx	lr
 80b10a8:	f1a0 40ff 	sub.w	r0, r0, #2139095040	; 0x7f800000
 80b10ac:	fab0 f080 	clz	r0, r0
 80b10b0:	0940      	lsrs	r0, r0, #5
 80b10b2:	4770      	bx	lr
 80b10b4:	007ffffe 	.word	0x007ffffe

080b10b8 <roundf>:
 80b10b8:	b508      	push	{r3, lr}
 80b10ba:	f3c0 53c7 	ubfx	r3, r0, #23, #8
 80b10be:	3b7f      	subs	r3, #127	; 0x7f
 80b10c0:	2b16      	cmp	r3, #22
 80b10c2:	4601      	mov	r1, r0
 80b10c4:	dc0e      	bgt.n	80b10e4 <roundf+0x2c>
 80b10c6:	2b00      	cmp	r3, #0
 80b10c8:	4602      	mov	r2, r0
 80b10ca:	db10      	blt.n	80b10ee <roundf+0x36>
 80b10cc:	480b      	ldr	r0, [pc, #44]	; (80b10fc <roundf+0x44>)
 80b10ce:	4118      	asrs	r0, r3
 80b10d0:	4201      	tst	r1, r0
 80b10d2:	d005      	beq.n	80b10e0 <roundf+0x28>
 80b10d4:	f44f 0180 	mov.w	r1, #4194304	; 0x400000
 80b10d8:	4119      	asrs	r1, r3
 80b10da:	4411      	add	r1, r2
 80b10dc:	ea21 0100 	bic.w	r1, r1, r0
 80b10e0:	4608      	mov	r0, r1
 80b10e2:	bd08      	pop	{r3, pc}
 80b10e4:	2b80      	cmp	r3, #128	; 0x80
 80b10e6:	d1fb      	bne.n	80b10e0 <roundf+0x28>
 80b10e8:	f002 f978 	bl	80b33dc <__addsf3>
 80b10ec:	bd08      	pop	{r3, pc}
 80b10ee:	3301      	adds	r3, #1
 80b10f0:	f000 4100 	and.w	r1, r0, #2147483648	; 0x80000000
 80b10f4:	d1f4      	bne.n	80b10e0 <roundf+0x28>
 80b10f6:	f041 517e 	orr.w	r1, r1, #1065353216	; 0x3f800000
 80b10fa:	e7f1      	b.n	80b10e0 <roundf+0x28>
 80b10fc:	007fffff 	.word	0x007fffff

080b1100 <sinf>:
 80b1100:	b500      	push	{lr}
 80b1102:	4a1d      	ldr	r2, [pc, #116]	; (80b1178 <sinf+0x78>)
 80b1104:	f020 4300 	bic.w	r3, r0, #2147483648	; 0x80000000
 80b1108:	4293      	cmp	r3, r2
 80b110a:	b083      	sub	sp, #12
 80b110c:	dd19      	ble.n	80b1142 <sinf+0x42>
 80b110e:	f1b3 4fff 	cmp.w	r3, #2139095040	; 0x7f800000
 80b1112:	db05      	blt.n	80b1120 <sinf+0x20>
 80b1114:	4601      	mov	r1, r0
 80b1116:	f002 f95f 	bl	80b33d8 <__aeabi_fsub>
 80b111a:	b003      	add	sp, #12
 80b111c:	f85d fb04 	ldr.w	pc, [sp], #4
 80b1120:	4669      	mov	r1, sp
 80b1122:	f000 fd93 	bl	80b1c4c <__ieee754_rem_pio2f>
 80b1126:	f000 0003 	and.w	r0, r0, #3
 80b112a:	2801      	cmp	r0, #1
 80b112c:	d018      	beq.n	80b1160 <sinf+0x60>
 80b112e:	2802      	cmp	r0, #2
 80b1130:	d00e      	beq.n	80b1150 <sinf+0x50>
 80b1132:	b1d0      	cbz	r0, 80b116a <sinf+0x6a>
 80b1134:	9901      	ldr	r1, [sp, #4]
 80b1136:	9800      	ldr	r0, [sp, #0]
 80b1138:	f000 ff4e 	bl	80b1fd8 <__kernel_cosf>
 80b113c:	f100 4000 	add.w	r0, r0, #2147483648	; 0x80000000
 80b1140:	e7eb      	b.n	80b111a <sinf+0x1a>
 80b1142:	2200      	movs	r2, #0
 80b1144:	2100      	movs	r1, #0
 80b1146:	f001 fb5f 	bl	80b2808 <__kernel_sinf>
 80b114a:	b003      	add	sp, #12
 80b114c:	f85d fb04 	ldr.w	pc, [sp], #4
 80b1150:	2201      	movs	r2, #1
 80b1152:	9901      	ldr	r1, [sp, #4]
 80b1154:	9800      	ldr	r0, [sp, #0]
 80b1156:	f001 fb57 	bl	80b2808 <__kernel_sinf>
 80b115a:	f100 4000 	add.w	r0, r0, #2147483648	; 0x80000000
 80b115e:	e7dc      	b.n	80b111a <sinf+0x1a>
 80b1160:	9901      	ldr	r1, [sp, #4]
 80b1162:	9800      	ldr	r0, [sp, #0]
 80b1164:	f000 ff38 	bl	80b1fd8 <__kernel_cosf>
 80b1168:	e7d7      	b.n	80b111a <sinf+0x1a>
 80b116a:	2201      	movs	r2, #1
 80b116c:	9901      	ldr	r1, [sp, #4]
 80b116e:	9800      	ldr	r0, [sp, #0]
 80b1170:	f001 fb4a 	bl	80b2808 <__kernel_sinf>
 80b1174:	e7d1      	b.n	80b111a <sinf+0x1a>
 80b1176:	bf00      	nop
 80b1178:	3f490fd8 	.word	0x3f490fd8
 80b117c:	00000000 	.word	0x00000000

080b1180 <exp>:
 80b1180:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
 80b1184:	b08b      	sub	sp, #44	; 0x2c
 80b1186:	4606      	mov	r6, r0
 80b1188:	460f      	mov	r7, r1
 80b118a:	f000 f9a9 	bl	80b14e0 <__ieee754_exp>
 80b118e:	f8df 80e4 	ldr.w	r8, [pc, #228]	; 80b1274 <exp+0xf4>
 80b1192:	4604      	mov	r4, r0
 80b1194:	f998 3000 	ldrsb.w	r3, [r8]
 80b1198:	460d      	mov	r5, r1
 80b119a:	3301      	adds	r3, #1
 80b119c:	d037      	beq.n	80b120e <exp+0x8e>
 80b119e:	4630      	mov	r0, r6
 80b11a0:	4639      	mov	r1, r7
 80b11a2:	f001 fba7 	bl	80b28f4 <finite>
 80b11a6:	2800      	cmp	r0, #0
 80b11a8:	d031      	beq.n	80b120e <exp+0x8e>
 80b11aa:	a32b      	add	r3, pc, #172	; (adr r3, 80b1258 <exp+0xd8>)
 80b11ac:	e9d3 2300 	ldrd	r2, r3, [r3]
 80b11b0:	4630      	mov	r0, r6
 80b11b2:	4639      	mov	r1, r7
 80b11b4:	f002 f86a 	bl	80b328c <__aeabi_dcmpgt>
 80b11b8:	4681      	mov	r9, r0
 80b11ba:	bb68      	cbnz	r0, 80b1218 <exp+0x98>
 80b11bc:	a328      	add	r3, pc, #160	; (adr r3, 80b1260 <exp+0xe0>)
 80b11be:	e9d3 2300 	ldrd	r2, r3, [r3]
 80b11c2:	4630      	mov	r0, r6
 80b11c4:	4639      	mov	r1, r7
 80b11c6:	f002 f843 	bl	80b3250 <__aeabi_dcmplt>
 80b11ca:	b300      	cbz	r0, 80b120e <exp+0x8e>
 80b11cc:	f998 3000 	ldrsb.w	r3, [r8]
 80b11d0:	4c25      	ldr	r4, [pc, #148]	; (80b1268 <exp+0xe8>)
 80b11d2:	2000      	movs	r0, #0
 80b11d4:	2100      	movs	r1, #0
 80b11d6:	2204      	movs	r2, #4
 80b11d8:	2b02      	cmp	r3, #2
 80b11da:	f8cd 9020 	str.w	r9, [sp, #32]
 80b11de:	e9cd 6704 	strd	r6, r7, [sp, #16]
 80b11e2:	e9cd 6702 	strd	r6, r7, [sp, #8]
 80b11e6:	e9cd 0106 	strd	r0, r1, [sp, #24]
 80b11ea:	e88d 0014 	stmia.w	sp, {r2, r4}
 80b11ee:	d02c      	beq.n	80b124a <exp+0xca>
 80b11f0:	4668      	mov	r0, sp
 80b11f2:	f001 fb85 	bl	80b2900 <matherr>
 80b11f6:	b340      	cbz	r0, 80b124a <exp+0xca>
 80b11f8:	9b08      	ldr	r3, [sp, #32]
 80b11fa:	b11b      	cbz	r3, 80b1204 <exp+0x84>
 80b11fc:	f7fe ff20 	bl	80b0040 <__errno>
 80b1200:	9b08      	ldr	r3, [sp, #32]
 80b1202:	6003      	str	r3, [r0, #0]
 80b1204:	e9dd 0106 	ldrd	r0, r1, [sp, #24]
 80b1208:	b00b      	add	sp, #44	; 0x2c
 80b120a:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}
 80b120e:	4620      	mov	r0, r4
 80b1210:	4629      	mov	r1, r5
 80b1212:	b00b      	add	sp, #44	; 0x2c
 80b1214:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}
 80b1218:	4813      	ldr	r0, [pc, #76]	; (80b1268 <exp+0xe8>)
 80b121a:	f998 3000 	ldrsb.w	r3, [r8]
 80b121e:	2103      	movs	r1, #3
 80b1220:	2200      	movs	r2, #0
 80b1222:	e9cd 6704 	strd	r6, r7, [sp, #16]
 80b1226:	e9cd 6702 	strd	r6, r7, [sp, #8]
 80b122a:	9001      	str	r0, [sp, #4]
 80b122c:	9100      	str	r1, [sp, #0]
 80b122e:	9208      	str	r2, [sp, #32]
 80b1230:	b92b      	cbnz	r3, 80b123e <exp+0xbe>
 80b1232:	4b0e      	ldr	r3, [pc, #56]	; (80b126c <exp+0xec>)
 80b1234:	f04f 4260 	mov.w	r2, #3758096384	; 0xe0000000
 80b1238:	e9cd 2306 	strd	r2, r3, [sp, #24]
 80b123c:	e7d8      	b.n	80b11f0 <exp+0x70>
 80b123e:	490c      	ldr	r1, [pc, #48]	; (80b1270 <exp+0xf0>)
 80b1240:	2000      	movs	r0, #0
 80b1242:	2b02      	cmp	r3, #2
 80b1244:	e9cd 0106 	strd	r0, r1, [sp, #24]
 80b1248:	d1d2      	bne.n	80b11f0 <exp+0x70>
 80b124a:	f7fe fef9 	bl	80b0040 <__errno>
 80b124e:	2322      	movs	r3, #34	; 0x22
 80b1250:	6003      	str	r3, [r0, #0]
 80b1252:	e7d1      	b.n	80b11f8 <exp+0x78>
 80b1254:	f3af 8000 	nop.w
 80b1258:	fefa39ef 	.word	0xfefa39ef
 80b125c:	40862e42 	.word	0x40862e42
 80b1260:	d52d3051 	.word	0xd52d3051
 80b1264:	c0874910 	.word	0xc0874910
 80b1268:	080b77c4 	.word	0x080b77c4
 80b126c:	47efffff 	.word	0x47efffff
 80b1270:	7ff00000 	.word	0x7ff00000
 80b1274:	2000052c 	.word	0x2000052c

080b1278 <expf>:
 80b1278:	b5f0      	push	{r4, r5, r6, r7, lr}
 80b127a:	b08b      	sub	sp, #44	; 0x2c
 80b127c:	4605      	mov	r5, r0
 80b127e:	f000 faa5 	bl	80b17cc <__ieee754_expf>
 80b1282:	4e30      	ldr	r6, [pc, #192]	; (80b1344 <expf+0xcc>)
 80b1284:	4604      	mov	r4, r0
 80b1286:	f996 3000 	ldrsb.w	r3, [r6]
 80b128a:	3301      	adds	r3, #1
 80b128c:	d035      	beq.n	80b12fa <expf+0x82>
 80b128e:	4628      	mov	r0, r5
 80b1290:	f001 fb42 	bl	80b2918 <finitef>
 80b1294:	2800      	cmp	r0, #0
 80b1296:	d030      	beq.n	80b12fa <expf+0x82>
 80b1298:	492b      	ldr	r1, [pc, #172]	; (80b1348 <expf+0xd0>)
 80b129a:	4628      	mov	r0, r5
 80b129c:	f002 fb62 	bl	80b3964 <__aeabi_fcmpgt>
 80b12a0:	4607      	mov	r7, r0
 80b12a2:	2800      	cmp	r0, #0
 80b12a4:	d12c      	bne.n	80b1300 <expf+0x88>
 80b12a6:	4929      	ldr	r1, [pc, #164]	; (80b134c <expf+0xd4>)
 80b12a8:	4628      	mov	r0, r5
 80b12aa:	f002 fb3d 	bl	80b3928 <__aeabi_fcmplt>
 80b12ae:	b320      	cbz	r0, 80b12fa <expf+0x82>
 80b12b0:	4a27      	ldr	r2, [pc, #156]	; (80b1350 <expf+0xd8>)
 80b12b2:	2304      	movs	r3, #4
 80b12b4:	4628      	mov	r0, r5
 80b12b6:	9300      	str	r3, [sp, #0]
 80b12b8:	9708      	str	r7, [sp, #32]
 80b12ba:	9201      	str	r2, [sp, #4]
 80b12bc:	f001 fd02 	bl	80b2cc4 <__aeabi_f2d>
 80b12c0:	f996 3000 	ldrsb.w	r3, [r6]
 80b12c4:	2400      	movs	r4, #0
 80b12c6:	2500      	movs	r5, #0
 80b12c8:	2b02      	cmp	r3, #2
 80b12ca:	e9cd 0104 	strd	r0, r1, [sp, #16]
 80b12ce:	e9cd 0102 	strd	r0, r1, [sp, #8]
 80b12d2:	e9cd 4506 	strd	r4, r5, [sp, #24]
 80b12d6:	d02f      	beq.n	80b1338 <expf+0xc0>
 80b12d8:	4668      	mov	r0, sp
 80b12da:	f001 fb11 	bl	80b2900 <matherr>
 80b12de:	2800      	cmp	r0, #0
 80b12e0:	d02a      	beq.n	80b1338 <expf+0xc0>
 80b12e2:	9b08      	ldr	r3, [sp, #32]
 80b12e4:	b11b      	cbz	r3, 80b12ee <expf+0x76>
 80b12e6:	f7fe feab 	bl	80b0040 <__errno>
 80b12ea:	9b08      	ldr	r3, [sp, #32]
 80b12ec:	6003      	str	r3, [r0, #0]
 80b12ee:	e9dd 0106 	ldrd	r0, r1, [sp, #24]
 80b12f2:	f002 f81d 	bl	80b3330 <__aeabi_d2f>
 80b12f6:	b00b      	add	sp, #44	; 0x2c
 80b12f8:	bdf0      	pop	{r4, r5, r6, r7, pc}
 80b12fa:	4620      	mov	r0, r4
 80b12fc:	b00b      	add	sp, #44	; 0x2c
 80b12fe:	bdf0      	pop	{r4, r5, r6, r7, pc}
 80b1300:	4913      	ldr	r1, [pc, #76]	; (80b1350 <expf+0xd8>)
 80b1302:	2300      	movs	r3, #0
 80b1304:	2203      	movs	r2, #3
 80b1306:	4628      	mov	r0, r5
 80b1308:	9308      	str	r3, [sp, #32]
 80b130a:	9101      	str	r1, [sp, #4]
 80b130c:	9200      	str	r2, [sp, #0]
 80b130e:	f001 fcd9 	bl	80b2cc4 <__aeabi_f2d>
 80b1312:	f996 3000 	ldrsb.w	r3, [r6]
 80b1316:	e9cd 0104 	strd	r0, r1, [sp, #16]
 80b131a:	e9cd 0102 	strd	r0, r1, [sp, #8]
 80b131e:	b92b      	cbnz	r3, 80b132c <expf+0xb4>
 80b1320:	4b0c      	ldr	r3, [pc, #48]	; (80b1354 <expf+0xdc>)
 80b1322:	f04f 4260 	mov.w	r2, #3758096384	; 0xe0000000
 80b1326:	e9cd 2306 	strd	r2, r3, [sp, #24]
 80b132a:	e7d5      	b.n	80b12d8 <expf+0x60>
 80b132c:	490a      	ldr	r1, [pc, #40]	; (80b1358 <expf+0xe0>)
 80b132e:	2000      	movs	r0, #0
 80b1330:	2b02      	cmp	r3, #2
 80b1332:	e9cd 0106 	strd	r0, r1, [sp, #24]
 80b1336:	d1cf      	bne.n	80b12d8 <expf+0x60>
 80b1338:	f7fe fe82 	bl	80b0040 <__errno>
 80b133c:	2322      	movs	r3, #34	; 0x22
 80b133e:	6003      	str	r3, [r0, #0]
 80b1340:	e7cf      	b.n	80b12e2 <expf+0x6a>
 80b1342:	bf00      	nop
 80b1344:	2000052c 	.word	0x2000052c
 80b1348:	42b17180 	.word	0x42b17180
 80b134c:	c2cff1b5 	.word	0xc2cff1b5
 80b1350:	080b77c8 	.word	0x080b77c8
 80b1354:	47efffff 	.word	0x47efffff
 80b1358:	7ff00000 	.word	0x7ff00000

080b135c <logf>:
 80b135c:	b570      	push	{r4, r5, r6, lr}
 80b135e:	b08a      	sub	sp, #40	; 0x28
 80b1360:	4604      	mov	r4, r0
 80b1362:	f000 fb31 	bl	80b19c8 <__ieee754_logf>
 80b1366:	4b32      	ldr	r3, [pc, #200]	; (80b1430 <logf+0xd4>)
 80b1368:	4605      	mov	r5, r0
 80b136a:	f993 6000 	ldrsb.w	r6, [r3]
 80b136e:	1c73      	adds	r3, r6, #1
 80b1370:	d00a      	beq.n	80b1388 <logf+0x2c>
 80b1372:	4621      	mov	r1, r4
 80b1374:	4620      	mov	r0, r4
 80b1376:	f002 faff 	bl	80b3978 <__aeabi_fcmpun>
 80b137a:	b928      	cbnz	r0, 80b1388 <logf+0x2c>
 80b137c:	2100      	movs	r1, #0
 80b137e:	4620      	mov	r0, r4
 80b1380:	f002 faf0 	bl	80b3964 <__aeabi_fcmpgt>
 80b1384:	4603      	mov	r3, r0
 80b1386:	b110      	cbz	r0, 80b138e <logf+0x32>
 80b1388:	4628      	mov	r0, r5
 80b138a:	b00a      	add	sp, #40	; 0x28
 80b138c:	bd70      	pop	{r4, r5, r6, pc}
 80b138e:	4a29      	ldr	r2, [pc, #164]	; (80b1434 <logf+0xd8>)
 80b1390:	4620      	mov	r0, r4
 80b1392:	9308      	str	r3, [sp, #32]
 80b1394:	9201      	str	r2, [sp, #4]
 80b1396:	f001 fc95 	bl	80b2cc4 <__aeabi_f2d>
 80b139a:	e9cd 0104 	strd	r0, r1, [sp, #16]
 80b139e:	e9cd 0102 	strd	r0, r1, [sp, #8]
 80b13a2:	b9c6      	cbnz	r6, 80b13d6 <logf+0x7a>
 80b13a4:	4b24      	ldr	r3, [pc, #144]	; (80b1438 <logf+0xdc>)
 80b13a6:	f04f 4260 	mov.w	r2, #3758096384	; 0xe0000000
 80b13aa:	4620      	mov	r0, r4
 80b13ac:	2100      	movs	r1, #0
 80b13ae:	e9cd 2306 	strd	r2, r3, [sp, #24]
 80b13b2:	f002 faaf 	bl	80b3914 <__aeabi_fcmpeq>
 80b13b6:	2800      	cmp	r0, #0
 80b13b8:	d032      	beq.n	80b1420 <logf+0xc4>
 80b13ba:	2302      	movs	r3, #2
 80b13bc:	9300      	str	r3, [sp, #0]
 80b13be:	4668      	mov	r0, sp
 80b13c0:	f001 fa9e 	bl	80b2900 <matherr>
 80b13c4:	b1a0      	cbz	r0, 80b13f0 <logf+0x94>
 80b13c6:	9b08      	ldr	r3, [sp, #32]
 80b13c8:	b9bb      	cbnz	r3, 80b13fa <logf+0x9e>
 80b13ca:	e9dd 0106 	ldrd	r0, r1, [sp, #24]
 80b13ce:	f001 ffaf 	bl	80b3330 <__aeabi_d2f>
 80b13d2:	b00a      	add	sp, #40	; 0x28
 80b13d4:	bd70      	pop	{r4, r5, r6, pc}
 80b13d6:	4b19      	ldr	r3, [pc, #100]	; (80b143c <logf+0xe0>)
 80b13d8:	2200      	movs	r2, #0
 80b13da:	4620      	mov	r0, r4
 80b13dc:	2100      	movs	r1, #0
 80b13de:	e9cd 2306 	strd	r2, r3, [sp, #24]
 80b13e2:	f002 fa97 	bl	80b3914 <__aeabi_fcmpeq>
 80b13e6:	b168      	cbz	r0, 80b1404 <logf+0xa8>
 80b13e8:	2302      	movs	r3, #2
 80b13ea:	429e      	cmp	r6, r3
 80b13ec:	9300      	str	r3, [sp, #0]
 80b13ee:	d1e6      	bne.n	80b13be <logf+0x62>
 80b13f0:	f7fe fe26 	bl	80b0040 <__errno>
 80b13f4:	2322      	movs	r3, #34	; 0x22
 80b13f6:	6003      	str	r3, [r0, #0]
 80b13f8:	e7e5      	b.n	80b13c6 <logf+0x6a>
 80b13fa:	f7fe fe21 	bl	80b0040 <__errno>
 80b13fe:	9b08      	ldr	r3, [sp, #32]
 80b1400:	6003      	str	r3, [r0, #0]
 80b1402:	e7e2      	b.n	80b13ca <logf+0x6e>
 80b1404:	2301      	movs	r3, #1
 80b1406:	2e02      	cmp	r6, #2
 80b1408:	9300      	str	r3, [sp, #0]
 80b140a:	d10b      	bne.n	80b1424 <logf+0xc8>
 80b140c:	f7fe fe18 	bl	80b0040 <__errno>
 80b1410:	2321      	movs	r3, #33	; 0x21
 80b1412:	6003      	str	r3, [r0, #0]
 80b1414:	480a      	ldr	r0, [pc, #40]	; (80b1440 <logf+0xe4>)
 80b1416:	f001 fa75 	bl	80b2904 <nan>
 80b141a:	e9cd 0106 	strd	r0, r1, [sp, #24]
 80b141e:	e7d2      	b.n	80b13c6 <logf+0x6a>
 80b1420:	2301      	movs	r3, #1
 80b1422:	9300      	str	r3, [sp, #0]
 80b1424:	4668      	mov	r0, sp
 80b1426:	f001 fa6b 	bl	80b2900 <matherr>
 80b142a:	2800      	cmp	r0, #0
 80b142c:	d1f2      	bne.n	80b1414 <logf+0xb8>
 80b142e:	e7ed      	b.n	80b140c <logf+0xb0>
 80b1430:	2000052c 	.word	0x2000052c
 80b1434:	080b77d0 	.word	0x080b77d0
 80b1438:	c7efffff 	.word	0xc7efffff
 80b143c:	fff00000 	.word	0xfff00000
 80b1440:	080b77d4 	.word	0x080b77d4

080b1444 <sqrtf>:
 80b1444:	b5f0      	push	{r4, r5, r6, r7, lr}
 80b1446:	b08b      	sub	sp, #44	; 0x2c
 80b1448:	4605      	mov	r5, r0
 80b144a:	f000 fd71 	bl	80b1f30 <__ieee754_sqrtf>
 80b144e:	4b22      	ldr	r3, [pc, #136]	; (80b14d8 <sqrtf+0x94>)
 80b1450:	4604      	mov	r4, r0
 80b1452:	f993 6000 	ldrsb.w	r6, [r3]
 80b1456:	1c73      	adds	r3, r6, #1
 80b1458:	d00a      	beq.n	80b1470 <sqrtf+0x2c>
 80b145a:	4629      	mov	r1, r5
 80b145c:	4628      	mov	r0, r5
 80b145e:	f002 fa8b 	bl	80b3978 <__aeabi_fcmpun>
 80b1462:	4607      	mov	r7, r0
 80b1464:	b920      	cbnz	r0, 80b1470 <sqrtf+0x2c>
 80b1466:	2100      	movs	r1, #0
 80b1468:	4628      	mov	r0, r5
 80b146a:	f002 fa5d 	bl	80b3928 <__aeabi_fcmplt>
 80b146e:	b910      	cbnz	r0, 80b1476 <sqrtf+0x32>
 80b1470:	4620      	mov	r0, r4
 80b1472:	b00b      	add	sp, #44	; 0x2c
 80b1474:	bdf0      	pop	{r4, r5, r6, r7, pc}
 80b1476:	4a19      	ldr	r2, [pc, #100]	; (80b14dc <sqrtf+0x98>)
 80b1478:	2301      	movs	r3, #1
 80b147a:	4628      	mov	r0, r5
 80b147c:	9201      	str	r2, [sp, #4]
 80b147e:	9300      	str	r3, [sp, #0]
 80b1480:	9708      	str	r7, [sp, #32]
 80b1482:	f001 fc1f 	bl	80b2cc4 <__aeabi_f2d>
 80b1486:	2200      	movs	r2, #0
 80b1488:	e9cd 0104 	strd	r0, r1, [sp, #16]
 80b148c:	e9cd 0102 	strd	r0, r1, [sp, #8]
 80b1490:	2300      	movs	r3, #0
 80b1492:	b19e      	cbz	r6, 80b14bc <sqrtf+0x78>
 80b1494:	4610      	mov	r0, r2
 80b1496:	4619      	mov	r1, r3
 80b1498:	f001 fd92 	bl	80b2fc0 <__aeabi_ddiv>
 80b149c:	2e02      	cmp	r6, #2
 80b149e:	e9cd 0106 	strd	r0, r1, [sp, #24]
 80b14a2:	d10d      	bne.n	80b14c0 <sqrtf+0x7c>
 80b14a4:	f7fe fdcc 	bl	80b0040 <__errno>
 80b14a8:	2321      	movs	r3, #33	; 0x21
 80b14aa:	6003      	str	r3, [r0, #0]
 80b14ac:	9b08      	ldr	r3, [sp, #32]
 80b14ae:	b96b      	cbnz	r3, 80b14cc <sqrtf+0x88>
 80b14b0:	e9dd 0106 	ldrd	r0, r1, [sp, #24]
 80b14b4:	f001 ff3c 	bl	80b3330 <__aeabi_d2f>
 80b14b8:	b00b      	add	sp, #44	; 0x2c
 80b14ba:	bdf0      	pop	{r4, r5, r6, r7, pc}
 80b14bc:	e9cd 2306 	strd	r2, r3, [sp, #24]
 80b14c0:	4668      	mov	r0, sp
 80b14c2:	f001 fa1d 	bl	80b2900 <matherr>
 80b14c6:	2800      	cmp	r0, #0
 80b14c8:	d1f0      	bne.n	80b14ac <sqrtf+0x68>
 80b14ca:	e7eb      	b.n	80b14a4 <sqrtf+0x60>
 80b14cc:	f7fe fdb8 	bl	80b0040 <__errno>
 80b14d0:	9b08      	ldr	r3, [sp, #32]
 80b14d2:	6003      	str	r3, [r0, #0]
 80b14d4:	e7ec      	b.n	80b14b0 <sqrtf+0x6c>
 80b14d6:	bf00      	nop
 80b14d8:	2000052c 	.word	0x2000052c
 80b14dc:	080b77d8 	.word	0x080b77d8

080b14e0 <__ieee754_exp>:
 80b14e0:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80b14e4:	4fb0      	ldr	r7, [pc, #704]	; (80b17a8 <__ieee754_exp+0x2c8>)
 80b14e6:	f021 4200 	bic.w	r2, r1, #2147483648	; 0x80000000
 80b14ea:	42ba      	cmp	r2, r7
 80b14ec:	b083      	sub	sp, #12
 80b14ee:	460c      	mov	r4, r1
 80b14f0:	ea4f 76d1 	mov.w	r6, r1, lsr #31
 80b14f4:	4605      	mov	r5, r0
 80b14f6:	d90d      	bls.n	80b1514 <__ieee754_exp+0x34>
 80b14f8:	4fac      	ldr	r7, [pc, #688]	; (80b17ac <__ieee754_exp+0x2cc>)
 80b14fa:	42ba      	cmp	r2, r7
 80b14fc:	d925      	bls.n	80b154a <__ieee754_exp+0x6a>
 80b14fe:	f3c1 0313 	ubfx	r3, r1, #0, #20
 80b1502:	4303      	orrs	r3, r0
 80b1504:	f040 80ef 	bne.w	80b16e6 <__ieee754_exp+0x206>
 80b1508:	b10e      	cbz	r6, 80b150e <__ieee754_exp+0x2e>
 80b150a:	2000      	movs	r0, #0
 80b150c:	4601      	mov	r1, r0
 80b150e:	b003      	add	sp, #12
 80b1510:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80b1514:	4ba6      	ldr	r3, [pc, #664]	; (80b17b0 <__ieee754_exp+0x2d0>)
 80b1516:	429a      	cmp	r2, r3
 80b1518:	f200 80cc 	bhi.w	80b16b4 <__ieee754_exp+0x1d4>
 80b151c:	4ba5      	ldr	r3, [pc, #660]	; (80b17b4 <__ieee754_exp+0x2d4>)
 80b151e:	429a      	cmp	r2, r3
 80b1520:	f200 80de 	bhi.w	80b16e0 <__ieee754_exp+0x200>
 80b1524:	a38a      	add	r3, pc, #552	; (adr r3, 80b1750 <__ieee754_exp+0x270>)
 80b1526:	e9d3 2300 	ldrd	r2, r3, [r3]
 80b152a:	f001 fa6d 	bl	80b2a08 <__adddf3>
 80b152e:	2200      	movs	r2, #0
 80b1530:	4ba1      	ldr	r3, [pc, #644]	; (80b17b8 <__ieee754_exp+0x2d8>)
 80b1532:	f001 feab 	bl	80b328c <__aeabi_dcmpgt>
 80b1536:	2800      	cmp	r0, #0
 80b1538:	f000 8105 	beq.w	80b1746 <__ieee754_exp+0x266>
 80b153c:	4628      	mov	r0, r5
 80b153e:	4621      	mov	r1, r4
 80b1540:	2200      	movs	r2, #0
 80b1542:	4b9d      	ldr	r3, [pc, #628]	; (80b17b8 <__ieee754_exp+0x2d8>)
 80b1544:	f001 fa60 	bl	80b2a08 <__adddf3>
 80b1548:	e7e1      	b.n	80b150e <__ieee754_exp+0x2e>
 80b154a:	a383      	add	r3, pc, #524	; (adr r3, 80b1758 <__ieee754_exp+0x278>)
 80b154c:	e9d3 2300 	ldrd	r2, r3, [r3]
 80b1550:	f001 fe9c 	bl	80b328c <__aeabi_dcmpgt>
 80b1554:	2800      	cmp	r0, #0
 80b1556:	f040 80cb 	bne.w	80b16f0 <__ieee754_exp+0x210>
 80b155a:	a381      	add	r3, pc, #516	; (adr r3, 80b1760 <__ieee754_exp+0x280>)
 80b155c:	e9d3 2300 	ldrd	r2, r3, [r3]
 80b1560:	4628      	mov	r0, r5
 80b1562:	4621      	mov	r1, r4
 80b1564:	f001 fe74 	bl	80b3250 <__aeabi_dcmplt>
 80b1568:	2800      	cmp	r0, #0
 80b156a:	d1ce      	bne.n	80b150a <__ieee754_exp+0x2a>
 80b156c:	a37e      	add	r3, pc, #504	; (adr r3, 80b1768 <__ieee754_exp+0x288>)
 80b156e:	e9d3 2300 	ldrd	r2, r3, [r3]
 80b1572:	4f92      	ldr	r7, [pc, #584]	; (80b17bc <__ieee754_exp+0x2dc>)
 80b1574:	4628      	mov	r0, r5
 80b1576:	eb07 08c6 	add.w	r8, r7, r6, lsl #3
 80b157a:	4621      	mov	r1, r4
 80b157c:	f001 fbf6 	bl	80b2d6c <__aeabi_dmul>
 80b1580:	e9d8 2300 	ldrd	r2, r3, [r8]
 80b1584:	f001 fa40 	bl	80b2a08 <__adddf3>
 80b1588:	f001 fe8a 	bl	80b32a0 <__aeabi_d2iz>
 80b158c:	4680      	mov	r8, r0
 80b158e:	f001 fb87 	bl	80b2ca0 <__aeabi_i2d>
 80b1592:	a377      	add	r3, pc, #476	; (adr r3, 80b1770 <__ieee754_exp+0x290>)
 80b1594:	e9d3 2300 	ldrd	r2, r3, [r3]
 80b1598:	4606      	mov	r6, r0
 80b159a:	460f      	mov	r7, r1
 80b159c:	f001 fbe6 	bl	80b2d6c <__aeabi_dmul>
 80b15a0:	4602      	mov	r2, r0
 80b15a2:	460b      	mov	r3, r1
 80b15a4:	4628      	mov	r0, r5
 80b15a6:	4621      	mov	r1, r4
 80b15a8:	f001 fa2c 	bl	80b2a04 <__aeabi_dsub>
 80b15ac:	a372      	add	r3, pc, #456	; (adr r3, 80b1778 <__ieee754_exp+0x298>)
 80b15ae:	e9d3 2300 	ldrd	r2, r3, [r3]
 80b15b2:	e9cd 0100 	strd	r0, r1, [sp]
 80b15b6:	4630      	mov	r0, r6
 80b15b8:	4639      	mov	r1, r7
 80b15ba:	f001 fbd7 	bl	80b2d6c <__aeabi_dmul>
 80b15be:	4682      	mov	sl, r0
 80b15c0:	468b      	mov	fp, r1
 80b15c2:	e9dd 0100 	ldrd	r0, r1, [sp]
 80b15c6:	4652      	mov	r2, sl
 80b15c8:	465b      	mov	r3, fp
 80b15ca:	f001 fa1b 	bl	80b2a04 <__aeabi_dsub>
 80b15ce:	4605      	mov	r5, r0
 80b15d0:	460c      	mov	r4, r1
 80b15d2:	462a      	mov	r2, r5
 80b15d4:	4623      	mov	r3, r4
 80b15d6:	4628      	mov	r0, r5
 80b15d8:	4621      	mov	r1, r4
 80b15da:	f001 fbc7 	bl	80b2d6c <__aeabi_dmul>
 80b15de:	a368      	add	r3, pc, #416	; (adr r3, 80b1780 <__ieee754_exp+0x2a0>)
 80b15e0:	e9d3 2300 	ldrd	r2, r3, [r3]
 80b15e4:	4606      	mov	r6, r0
 80b15e6:	460f      	mov	r7, r1
 80b15e8:	f001 fbc0 	bl	80b2d6c <__aeabi_dmul>
 80b15ec:	a366      	add	r3, pc, #408	; (adr r3, 80b1788 <__ieee754_exp+0x2a8>)
 80b15ee:	e9d3 2300 	ldrd	r2, r3, [r3]
 80b15f2:	f001 fa07 	bl	80b2a04 <__aeabi_dsub>
 80b15f6:	4632      	mov	r2, r6
 80b15f8:	463b      	mov	r3, r7
 80b15fa:	f001 fbb7 	bl	80b2d6c <__aeabi_dmul>
 80b15fe:	a364      	add	r3, pc, #400	; (adr r3, 80b1790 <__ieee754_exp+0x2b0>)
 80b1600:	e9d3 2300 	ldrd	r2, r3, [r3]
 80b1604:	f001 fa00 	bl	80b2a08 <__adddf3>
 80b1608:	4632      	mov	r2, r6
 80b160a:	463b      	mov	r3, r7
 80b160c:	f001 fbae 	bl	80b2d6c <__aeabi_dmul>
 80b1610:	a361      	add	r3, pc, #388	; (adr r3, 80b1798 <__ieee754_exp+0x2b8>)
 80b1612:	e9d3 2300 	ldrd	r2, r3, [r3]
 80b1616:	f001 f9f5 	bl	80b2a04 <__aeabi_dsub>
 80b161a:	4632      	mov	r2, r6
 80b161c:	463b      	mov	r3, r7
 80b161e:	f001 fba5 	bl	80b2d6c <__aeabi_dmul>
 80b1622:	a35f      	add	r3, pc, #380	; (adr r3, 80b17a0 <__ieee754_exp+0x2c0>)
 80b1624:	e9d3 2300 	ldrd	r2, r3, [r3]
 80b1628:	f001 f9ee 	bl	80b2a08 <__adddf3>
 80b162c:	4632      	mov	r2, r6
 80b162e:	463b      	mov	r3, r7
 80b1630:	f001 fb9c 	bl	80b2d6c <__aeabi_dmul>
 80b1634:	4602      	mov	r2, r0
 80b1636:	460b      	mov	r3, r1
 80b1638:	4628      	mov	r0, r5
 80b163a:	4621      	mov	r1, r4
 80b163c:	f001 f9e2 	bl	80b2a04 <__aeabi_dsub>
 80b1640:	4606      	mov	r6, r0
 80b1642:	460f      	mov	r7, r1
 80b1644:	4628      	mov	r0, r5
 80b1646:	4621      	mov	r1, r4
 80b1648:	4632      	mov	r2, r6
 80b164a:	463b      	mov	r3, r7
 80b164c:	f1b8 0f00 	cmp.w	r8, #0
 80b1650:	d056      	beq.n	80b1700 <__ieee754_exp+0x220>
 80b1652:	f001 fb8b 	bl	80b2d6c <__aeabi_dmul>
 80b1656:	4632      	mov	r2, r6
 80b1658:	4604      	mov	r4, r0
 80b165a:	460d      	mov	r5, r1
 80b165c:	463b      	mov	r3, r7
 80b165e:	2000      	movs	r0, #0
 80b1660:	f04f 4180 	mov.w	r1, #1073741824	; 0x40000000
 80b1664:	f001 f9ce 	bl	80b2a04 <__aeabi_dsub>
 80b1668:	4602      	mov	r2, r0
 80b166a:	460b      	mov	r3, r1
 80b166c:	4620      	mov	r0, r4
 80b166e:	4629      	mov	r1, r5
 80b1670:	f001 fca6 	bl	80b2fc0 <__aeabi_ddiv>
 80b1674:	4602      	mov	r2, r0
 80b1676:	460b      	mov	r3, r1
 80b1678:	4650      	mov	r0, sl
 80b167a:	4659      	mov	r1, fp
 80b167c:	f001 f9c2 	bl	80b2a04 <__aeabi_dsub>
 80b1680:	e9dd 2300 	ldrd	r2, r3, [sp]
 80b1684:	f001 f9be 	bl	80b2a04 <__aeabi_dsub>
 80b1688:	460b      	mov	r3, r1
 80b168a:	4602      	mov	r2, r0
 80b168c:	494a      	ldr	r1, [pc, #296]	; (80b17b8 <__ieee754_exp+0x2d8>)
 80b168e:	2000      	movs	r0, #0
 80b1690:	f001 f9b8 	bl	80b2a04 <__aeabi_dsub>
 80b1694:	f46f 737f 	mvn.w	r3, #1020	; 0x3fc
 80b1698:	4598      	cmp	r8, r3
 80b169a:	da4f      	bge.n	80b173c <__ieee754_exp+0x25c>
 80b169c:	f508 787a 	add.w	r8, r8, #1000	; 0x3e8
 80b16a0:	eb01 5108 	add.w	r1, r1, r8, lsl #20
 80b16a4:	2200      	movs	r2, #0
 80b16a6:	f04f 73b8 	mov.w	r3, #24117248	; 0x1700000
 80b16aa:	f001 fb5f 	bl	80b2d6c <__aeabi_dmul>
 80b16ae:	b003      	add	sp, #12
 80b16b0:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80b16b4:	4b42      	ldr	r3, [pc, #264]	; (80b17c0 <__ieee754_exp+0x2e0>)
 80b16b6:	429a      	cmp	r2, r3
 80b16b8:	f63f af58 	bhi.w	80b156c <__ieee754_exp+0x8c>
 80b16bc:	4b41      	ldr	r3, [pc, #260]	; (80b17c4 <__ieee754_exp+0x2e4>)
 80b16be:	00f4      	lsls	r4, r6, #3
 80b16c0:	4423      	add	r3, r4
 80b16c2:	e9d3 2300 	ldrd	r2, r3, [r3]
 80b16c6:	f001 f99d 	bl	80b2a04 <__aeabi_dsub>
 80b16ca:	4b3f      	ldr	r3, [pc, #252]	; (80b17c8 <__ieee754_exp+0x2e8>)
 80b16cc:	f1c6 0801 	rsb	r8, r6, #1
 80b16d0:	441c      	add	r4, r3
 80b16d2:	e9cd 0100 	strd	r0, r1, [sp]
 80b16d6:	e9d4 ab00 	ldrd	sl, fp, [r4]
 80b16da:	ebc6 0808 	rsb	r8, r6, r8
 80b16de:	e770      	b.n	80b15c2 <__ieee754_exp+0xe2>
 80b16e0:	f04f 0800 	mov.w	r8, #0
 80b16e4:	e775      	b.n	80b15d2 <__ieee754_exp+0xf2>
 80b16e6:	4602      	mov	r2, r0
 80b16e8:	460b      	mov	r3, r1
 80b16ea:	f001 f98d 	bl	80b2a08 <__adddf3>
 80b16ee:	e70e      	b.n	80b150e <__ieee754_exp+0x2e>
 80b16f0:	a317      	add	r3, pc, #92	; (adr r3, 80b1750 <__ieee754_exp+0x270>)
 80b16f2:	e9d3 2300 	ldrd	r2, r3, [r3]
 80b16f6:	4610      	mov	r0, r2
 80b16f8:	4619      	mov	r1, r3
 80b16fa:	f001 fb37 	bl	80b2d6c <__aeabi_dmul>
 80b16fe:	e706      	b.n	80b150e <__ieee754_exp+0x2e>
 80b1700:	f001 fb34 	bl	80b2d6c <__aeabi_dmul>
 80b1704:	2200      	movs	r2, #0
 80b1706:	4680      	mov	r8, r0
 80b1708:	4689      	mov	r9, r1
 80b170a:	f04f 4380 	mov.w	r3, #1073741824	; 0x40000000
 80b170e:	4630      	mov	r0, r6
 80b1710:	4639      	mov	r1, r7
 80b1712:	f001 f977 	bl	80b2a04 <__aeabi_dsub>
 80b1716:	4602      	mov	r2, r0
 80b1718:	460b      	mov	r3, r1
 80b171a:	4640      	mov	r0, r8
 80b171c:	4649      	mov	r1, r9
 80b171e:	f001 fc4f 	bl	80b2fc0 <__aeabi_ddiv>
 80b1722:	462a      	mov	r2, r5
 80b1724:	4623      	mov	r3, r4
 80b1726:	f001 f96d 	bl	80b2a04 <__aeabi_dsub>
 80b172a:	4602      	mov	r2, r0
 80b172c:	460b      	mov	r3, r1
 80b172e:	2000      	movs	r0, #0
 80b1730:	4921      	ldr	r1, [pc, #132]	; (80b17b8 <__ieee754_exp+0x2d8>)
 80b1732:	f001 f967 	bl	80b2a04 <__aeabi_dsub>
 80b1736:	b003      	add	sp, #12
 80b1738:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80b173c:	eb01 5108 	add.w	r1, r1, r8, lsl #20
 80b1740:	b003      	add	sp, #12
 80b1742:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80b1746:	4680      	mov	r8, r0
 80b1748:	e743      	b.n	80b15d2 <__ieee754_exp+0xf2>
 80b174a:	bf00      	nop
 80b174c:	f3af 8000 	nop.w
 80b1750:	8800759c 	.word	0x8800759c
 80b1754:	7e37e43c 	.word	0x7e37e43c
 80b1758:	fefa39ef 	.word	0xfefa39ef
 80b175c:	40862e42 	.word	0x40862e42
 80b1760:	d52d3051 	.word	0xd52d3051
 80b1764:	c0874910 	.word	0xc0874910
 80b1768:	652b82fe 	.word	0x652b82fe
 80b176c:	3ff71547 	.word	0x3ff71547
 80b1770:	fee00000 	.word	0xfee00000
 80b1774:	3fe62e42 	.word	0x3fe62e42
 80b1778:	35793c76 	.word	0x35793c76
 80b177c:	3dea39ef 	.word	0x3dea39ef
 80b1780:	72bea4d0 	.word	0x72bea4d0
 80b1784:	3e663769 	.word	0x3e663769
 80b1788:	c5d26bf1 	.word	0xc5d26bf1
 80b178c:	3ebbbd41 	.word	0x3ebbbd41
 80b1790:	af25de2c 	.word	0xaf25de2c
 80b1794:	3f11566a 	.word	0x3f11566a
 80b1798:	16bebd93 	.word	0x16bebd93
 80b179c:	3f66c16c 	.word	0x3f66c16c
 80b17a0:	5555553e 	.word	0x5555553e
 80b17a4:	3fc55555 	.word	0x3fc55555
 80b17a8:	40862e41 	.word	0x40862e41
 80b17ac:	7fefffff 	.word	0x7fefffff
 80b17b0:	3fd62e42 	.word	0x3fd62e42
 80b17b4:	3e2fffff 	.word	0x3e2fffff
 80b17b8:	3ff00000 	.word	0x3ff00000
 80b17bc:	080b77e0 	.word	0x080b77e0
 80b17c0:	3ff0a2b1 	.word	0x3ff0a2b1
 80b17c4:	080b7800 	.word	0x080b7800
 80b17c8:	080b77f0 	.word	0x080b77f0

080b17cc <__ieee754_expf>:
 80b17cc:	f020 4200 	bic.w	r2, r0, #2147483648	; 0x80000000
 80b17d0:	f1b2 4fff 	cmp.w	r2, #2139095040	; 0x7f800000
 80b17d4:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
 80b17d8:	4604      	mov	r4, r0
 80b17da:	d86f      	bhi.n	80b18bc <__ieee754_expf+0xf0>
 80b17dc:	ea4f 75d0 	mov.w	r5, r0, lsr #31
 80b17e0:	d071      	beq.n	80b18c6 <__ieee754_expf+0xfa>
 80b17e2:	4969      	ldr	r1, [pc, #420]	; (80b1988 <__ieee754_expf+0x1bc>)
 80b17e4:	4288      	cmp	r0, r1
 80b17e6:	f300 808c 	bgt.w	80b1902 <__ieee754_expf+0x136>
 80b17ea:	2800      	cmp	r0, #0
 80b17ec:	f2c0 8082 	blt.w	80b18f4 <__ieee754_expf+0x128>
 80b17f0:	4b66      	ldr	r3, [pc, #408]	; (80b198c <__ieee754_expf+0x1c0>)
 80b17f2:	429a      	cmp	r2, r3
 80b17f4:	d96a      	bls.n	80b18cc <__ieee754_expf+0x100>
 80b17f6:	4b66      	ldr	r3, [pc, #408]	; (80b1990 <__ieee754_expf+0x1c4>)
 80b17f8:	429a      	cmp	r2, r3
 80b17fa:	f200 80a2 	bhi.w	80b1942 <__ieee754_expf+0x176>
 80b17fe:	4b65      	ldr	r3, [pc, #404]	; (80b1994 <__ieee754_expf+0x1c8>)
 80b1800:	4620      	mov	r0, r4
 80b1802:	f853 1025 	ldr.w	r1, [r3, r5, lsl #2]
 80b1806:	f001 fde7 	bl	80b33d8 <__aeabi_fsub>
 80b180a:	4b63      	ldr	r3, [pc, #396]	; (80b1998 <__ieee754_expf+0x1cc>)
 80b180c:	4607      	mov	r7, r0
 80b180e:	f1c5 0001 	rsb	r0, r5, #1
 80b1812:	f853 8025 	ldr.w	r8, [r3, r5, lsl #2]
 80b1816:	1b45      	subs	r5, r0, r5
 80b1818:	4641      	mov	r1, r8
 80b181a:	4638      	mov	r0, r7
 80b181c:	f001 fddc 	bl	80b33d8 <__aeabi_fsub>
 80b1820:	4604      	mov	r4, r0
 80b1822:	4621      	mov	r1, r4
 80b1824:	4620      	mov	r0, r4
 80b1826:	f001 fee1 	bl	80b35ec <__aeabi_fmul>
 80b182a:	4606      	mov	r6, r0
 80b182c:	495b      	ldr	r1, [pc, #364]	; (80b199c <__ieee754_expf+0x1d0>)
 80b182e:	f001 fedd 	bl	80b35ec <__aeabi_fmul>
 80b1832:	495b      	ldr	r1, [pc, #364]	; (80b19a0 <__ieee754_expf+0x1d4>)
 80b1834:	f001 fdd0 	bl	80b33d8 <__aeabi_fsub>
 80b1838:	4631      	mov	r1, r6
 80b183a:	f001 fed7 	bl	80b35ec <__aeabi_fmul>
 80b183e:	4959      	ldr	r1, [pc, #356]	; (80b19a4 <__ieee754_expf+0x1d8>)
 80b1840:	f001 fdcc 	bl	80b33dc <__addsf3>
 80b1844:	4631      	mov	r1, r6
 80b1846:	f001 fed1 	bl	80b35ec <__aeabi_fmul>
 80b184a:	4957      	ldr	r1, [pc, #348]	; (80b19a8 <__ieee754_expf+0x1dc>)
 80b184c:	f001 fdc4 	bl	80b33d8 <__aeabi_fsub>
 80b1850:	4631      	mov	r1, r6
 80b1852:	f001 fecb 	bl	80b35ec <__aeabi_fmul>
 80b1856:	4955      	ldr	r1, [pc, #340]	; (80b19ac <__ieee754_expf+0x1e0>)
 80b1858:	f001 fdc0 	bl	80b33dc <__addsf3>
 80b185c:	4631      	mov	r1, r6
 80b185e:	f001 fec5 	bl	80b35ec <__aeabi_fmul>
 80b1862:	4601      	mov	r1, r0
 80b1864:	4620      	mov	r0, r4
 80b1866:	f001 fdb7 	bl	80b33d8 <__aeabi_fsub>
 80b186a:	4606      	mov	r6, r0
 80b186c:	2d00      	cmp	r5, #0
 80b186e:	d04e      	beq.n	80b190e <__ieee754_expf+0x142>
 80b1870:	4620      	mov	r0, r4
 80b1872:	4631      	mov	r1, r6
 80b1874:	f001 feba 	bl	80b35ec <__aeabi_fmul>
 80b1878:	4631      	mov	r1, r6
 80b187a:	4604      	mov	r4, r0
 80b187c:	f04f 4080 	mov.w	r0, #1073741824	; 0x40000000
 80b1880:	f001 fdaa 	bl	80b33d8 <__aeabi_fsub>
 80b1884:	4601      	mov	r1, r0
 80b1886:	4620      	mov	r0, r4
 80b1888:	f001 ff64 	bl	80b3754 <__aeabi_fdiv>
 80b188c:	4601      	mov	r1, r0
 80b188e:	4640      	mov	r0, r8
 80b1890:	f001 fda2 	bl	80b33d8 <__aeabi_fsub>
 80b1894:	4639      	mov	r1, r7
 80b1896:	f001 fd9f 	bl	80b33d8 <__aeabi_fsub>
 80b189a:	4601      	mov	r1, r0
 80b189c:	f04f 507e 	mov.w	r0, #1065353216	; 0x3f800000
 80b18a0:	f001 fd9a 	bl	80b33d8 <__aeabi_fsub>
 80b18a4:	f115 0f7d 	cmn.w	r5, #125	; 0x7d
 80b18a8:	da6a      	bge.n	80b1980 <__ieee754_expf+0x1b4>
 80b18aa:	3564      	adds	r5, #100	; 0x64
 80b18ac:	eb00 50c5 	add.w	r0, r0, r5, lsl #23
 80b18b0:	f04f 6158 	mov.w	r1, #226492416	; 0xd800000
 80b18b4:	f001 fe9a 	bl	80b35ec <__aeabi_fmul>
 80b18b8:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80b18bc:	4601      	mov	r1, r0
 80b18be:	f001 fd8d 	bl	80b33dc <__addsf3>
 80b18c2:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80b18c6:	b9cd      	cbnz	r5, 80b18fc <__ieee754_expf+0x130>
 80b18c8:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80b18cc:	f1b2 5f46 	cmp.w	r2, #830472192	; 0x31800000
 80b18d0:	d235      	bcs.n	80b193e <__ieee754_expf+0x172>
 80b18d2:	4937      	ldr	r1, [pc, #220]	; (80b19b0 <__ieee754_expf+0x1e4>)
 80b18d4:	4620      	mov	r0, r4
 80b18d6:	f001 fd81 	bl	80b33dc <__addsf3>
 80b18da:	f04f 517e 	mov.w	r1, #1065353216	; 0x3f800000
 80b18de:	f002 f841 	bl	80b3964 <__aeabi_fcmpgt>
 80b18e2:	2800      	cmp	r0, #0
 80b18e4:	d04a      	beq.n	80b197c <__ieee754_expf+0x1b0>
 80b18e6:	4620      	mov	r0, r4
 80b18e8:	f04f 517e 	mov.w	r1, #1065353216	; 0x3f800000
 80b18ec:	f001 fd76 	bl	80b33dc <__addsf3>
 80b18f0:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80b18f4:	4b2f      	ldr	r3, [pc, #188]	; (80b19b4 <__ieee754_expf+0x1e8>)
 80b18f6:	429a      	cmp	r2, r3
 80b18f8:	f67f af7a 	bls.w	80b17f0 <__ieee754_expf+0x24>
 80b18fc:	2000      	movs	r0, #0
 80b18fe:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80b1902:	492b      	ldr	r1, [pc, #172]	; (80b19b0 <__ieee754_expf+0x1e4>)
 80b1904:	4608      	mov	r0, r1
 80b1906:	f001 fe71 	bl	80b35ec <__aeabi_fmul>
 80b190a:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80b190e:	4601      	mov	r1, r0
 80b1910:	4620      	mov	r0, r4
 80b1912:	f001 fe6b 	bl	80b35ec <__aeabi_fmul>
 80b1916:	f04f 4180 	mov.w	r1, #1073741824	; 0x40000000
 80b191a:	4605      	mov	r5, r0
 80b191c:	4630      	mov	r0, r6
 80b191e:	f001 fd5b 	bl	80b33d8 <__aeabi_fsub>
 80b1922:	4601      	mov	r1, r0
 80b1924:	4628      	mov	r0, r5
 80b1926:	f001 ff15 	bl	80b3754 <__aeabi_fdiv>
 80b192a:	4621      	mov	r1, r4
 80b192c:	f001 fd54 	bl	80b33d8 <__aeabi_fsub>
 80b1930:	4601      	mov	r1, r0
 80b1932:	f04f 507e 	mov.w	r0, #1065353216	; 0x3f800000
 80b1936:	f001 fd4f 	bl	80b33d8 <__aeabi_fsub>
 80b193a:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80b193e:	2500      	movs	r5, #0
 80b1940:	e76f      	b.n	80b1822 <__ieee754_expf+0x56>
 80b1942:	491d      	ldr	r1, [pc, #116]	; (80b19b8 <__ieee754_expf+0x1ec>)
 80b1944:	4620      	mov	r0, r4
 80b1946:	f001 fe51 	bl	80b35ec <__aeabi_fmul>
 80b194a:	4b1c      	ldr	r3, [pc, #112]	; (80b19bc <__ieee754_expf+0x1f0>)
 80b194c:	f853 1025 	ldr.w	r1, [r3, r5, lsl #2]
 80b1950:	f001 fd44 	bl	80b33dc <__addsf3>
 80b1954:	f002 f826 	bl	80b39a4 <__aeabi_f2iz>
 80b1958:	4605      	mov	r5, r0
 80b195a:	f001 fdf3 	bl	80b3544 <__aeabi_i2f>
 80b195e:	4918      	ldr	r1, [pc, #96]	; (80b19c0 <__ieee754_expf+0x1f4>)
 80b1960:	4606      	mov	r6, r0
 80b1962:	f001 fe43 	bl	80b35ec <__aeabi_fmul>
 80b1966:	4601      	mov	r1, r0
 80b1968:	4620      	mov	r0, r4
 80b196a:	f001 fd35 	bl	80b33d8 <__aeabi_fsub>
 80b196e:	4915      	ldr	r1, [pc, #84]	; (80b19c4 <__ieee754_expf+0x1f8>)
 80b1970:	4607      	mov	r7, r0
 80b1972:	4630      	mov	r0, r6
 80b1974:	f001 fe3a 	bl	80b35ec <__aeabi_fmul>
 80b1978:	4680      	mov	r8, r0
 80b197a:	e74d      	b.n	80b1818 <__ieee754_expf+0x4c>
 80b197c:	4605      	mov	r5, r0
 80b197e:	e750      	b.n	80b1822 <__ieee754_expf+0x56>
 80b1980:	eb00 50c5 	add.w	r0, r0, r5, lsl #23
 80b1984:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
 80b1988:	42b17217 	.word	0x42b17217
 80b198c:	3eb17218 	.word	0x3eb17218
 80b1990:	3f851591 	.word	0x3f851591
 80b1994:	080b7820 	.word	0x080b7820
 80b1998:	080b7818 	.word	0x080b7818
 80b199c:	3331bb4c 	.word	0x3331bb4c
 80b19a0:	35ddea0e 	.word	0x35ddea0e
 80b19a4:	388ab355 	.word	0x388ab355
 80b19a8:	3b360b61 	.word	0x3b360b61
 80b19ac:	3e2aaaab 	.word	0x3e2aaaab
 80b19b0:	7149f2ca 	.word	0x7149f2ca
 80b19b4:	42cff1b5 	.word	0x42cff1b5
 80b19b8:	3fb8aa3b 	.word	0x3fb8aa3b
 80b19bc:	080b7810 	.word	0x080b7810
 80b19c0:	3f317180 	.word	0x3f317180
 80b19c4:	3717f7d1 	.word	0x3717f7d1

080b19c8 <__ieee754_logf>:
 80b19c8:	f020 4200 	bic.w	r2, r0, #2147483648	; 0x80000000
 80b19cc:	e92d 47f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, lr}
 80b19d0:	b36a      	cbz	r2, 80b1a2e <__ieee754_logf+0x66>
 80b19d2:	2800      	cmp	r0, #0
 80b19d4:	4601      	mov	r1, r0
 80b19d6:	4603      	mov	r3, r0
 80b19d8:	db4a      	blt.n	80b1a70 <__ieee754_logf+0xa8>
 80b19da:	f1b0 4fff 	cmp.w	r0, #2139095040	; 0x7f800000
 80b19de:	da3b      	bge.n	80b1a58 <__ieee754_logf+0x90>
 80b19e0:	f5b0 0f00 	cmp.w	r0, #8388608	; 0x800000
 80b19e4:	db3c      	blt.n	80b1a60 <__ieee754_logf+0x98>
 80b19e6:	2400      	movs	r4, #0
 80b19e8:	4a8c      	ldr	r2, [pc, #560]	; (80b1c1c <__ieee754_logf+0x254>)
 80b19ea:	f3c3 0516 	ubfx	r5, r3, #0, #23
 80b19ee:	442a      	add	r2, r5
 80b19f0:	f402 0200 	and.w	r2, r2, #8388608	; 0x800000
 80b19f4:	15db      	asrs	r3, r3, #23
 80b19f6:	f082 507e 	eor.w	r0, r2, #1065353216	; 0x3f800000
 80b19fa:	3b7f      	subs	r3, #127	; 0x7f
 80b19fc:	4423      	add	r3, r4
 80b19fe:	4328      	orrs	r0, r5
 80b1a00:	f04f 517e 	mov.w	r1, #1065353216	; 0x3f800000
 80b1a04:	eb03 54d2 	add.w	r4, r3, r2, lsr #23
 80b1a08:	f001 fce6 	bl	80b33d8 <__aeabi_fsub>
 80b1a0c:	f105 030f 	add.w	r3, r5, #15
 80b1a10:	f3c3 0316 	ubfx	r3, r3, #0, #23
 80b1a14:	2b0f      	cmp	r3, #15
 80b1a16:	4606      	mov	r6, r0
 80b1a18:	dc31      	bgt.n	80b1a7e <__ieee754_logf+0xb6>
 80b1a1a:	2100      	movs	r1, #0
 80b1a1c:	f001 ff7a 	bl	80b3914 <__aeabi_fcmpeq>
 80b1a20:	2800      	cmp	r0, #0
 80b1a22:	f000 8092 	beq.w	80b1b4a <__ieee754_logf+0x182>
 80b1a26:	b94c      	cbnz	r4, 80b1a3c <__ieee754_logf+0x74>
 80b1a28:	2000      	movs	r0, #0
 80b1a2a:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}
 80b1a2e:	2100      	movs	r1, #0
 80b1a30:	f04f 404c 	mov.w	r0, #3422552064	; 0xcc000000
 80b1a34:	f001 fe8e 	bl	80b3754 <__aeabi_fdiv>
 80b1a38:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}
 80b1a3c:	4620      	mov	r0, r4
 80b1a3e:	f001 fd81 	bl	80b3544 <__aeabi_i2f>
 80b1a42:	4977      	ldr	r1, [pc, #476]	; (80b1c20 <__ieee754_logf+0x258>)
 80b1a44:	4605      	mov	r5, r0
 80b1a46:	f001 fdd1 	bl	80b35ec <__aeabi_fmul>
 80b1a4a:	4976      	ldr	r1, [pc, #472]	; (80b1c24 <__ieee754_logf+0x25c>)
 80b1a4c:	4604      	mov	r4, r0
 80b1a4e:	4628      	mov	r0, r5
 80b1a50:	f001 fdcc 	bl	80b35ec <__aeabi_fmul>
 80b1a54:	4601      	mov	r1, r0
 80b1a56:	4620      	mov	r0, r4
 80b1a58:	f001 fcc0 	bl	80b33dc <__addsf3>
 80b1a5c:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}
 80b1a60:	f04f 4198 	mov.w	r1, #1275068416	; 0x4c000000
 80b1a64:	f001 fdc2 	bl	80b35ec <__aeabi_fmul>
 80b1a68:	f06f 0418 	mvn.w	r4, #24
 80b1a6c:	4603      	mov	r3, r0
 80b1a6e:	e7bb      	b.n	80b19e8 <__ieee754_logf+0x20>
 80b1a70:	f001 fcb2 	bl	80b33d8 <__aeabi_fsub>
 80b1a74:	2100      	movs	r1, #0
 80b1a76:	f001 fe6d 	bl	80b3754 <__aeabi_fdiv>
 80b1a7a:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}
 80b1a7e:	f04f 4180 	mov.w	r1, #1073741824	; 0x40000000
 80b1a82:	f001 fcab 	bl	80b33dc <__addsf3>
 80b1a86:	4601      	mov	r1, r0
 80b1a88:	4630      	mov	r0, r6
 80b1a8a:	f001 fe63 	bl	80b3754 <__aeabi_fdiv>
 80b1a8e:	4680      	mov	r8, r0
 80b1a90:	4620      	mov	r0, r4
 80b1a92:	f001 fd57 	bl	80b3544 <__aeabi_i2f>
 80b1a96:	4641      	mov	r1, r8
 80b1a98:	4681      	mov	r9, r0
 80b1a9a:	4640      	mov	r0, r8
 80b1a9c:	f001 fda6 	bl	80b35ec <__aeabi_fmul>
 80b1aa0:	4601      	mov	r1, r0
 80b1aa2:	4682      	mov	sl, r0
 80b1aa4:	f001 fda2 	bl	80b35ec <__aeabi_fmul>
 80b1aa8:	4607      	mov	r7, r0
 80b1aaa:	495f      	ldr	r1, [pc, #380]	; (80b1c28 <__ieee754_logf+0x260>)
 80b1aac:	f001 fd9e 	bl	80b35ec <__aeabi_fmul>
 80b1ab0:	495e      	ldr	r1, [pc, #376]	; (80b1c2c <__ieee754_logf+0x264>)
 80b1ab2:	f001 fc93 	bl	80b33dc <__addsf3>
 80b1ab6:	4639      	mov	r1, r7
 80b1ab8:	f001 fd98 	bl	80b35ec <__aeabi_fmul>
 80b1abc:	495c      	ldr	r1, [pc, #368]	; (80b1c30 <__ieee754_logf+0x268>)
 80b1abe:	f001 fc8d 	bl	80b33dc <__addsf3>
 80b1ac2:	4639      	mov	r1, r7
 80b1ac4:	f001 fd92 	bl	80b35ec <__aeabi_fmul>
 80b1ac8:	495a      	ldr	r1, [pc, #360]	; (80b1c34 <__ieee754_logf+0x26c>)
 80b1aca:	f001 fc87 	bl	80b33dc <__addsf3>
 80b1ace:	4651      	mov	r1, sl
 80b1ad0:	f001 fd8c 	bl	80b35ec <__aeabi_fmul>
 80b1ad4:	4958      	ldr	r1, [pc, #352]	; (80b1c38 <__ieee754_logf+0x270>)
 80b1ad6:	4682      	mov	sl, r0
 80b1ad8:	4638      	mov	r0, r7
 80b1ada:	f001 fd87 	bl	80b35ec <__aeabi_fmul>
 80b1ade:	4957      	ldr	r1, [pc, #348]	; (80b1c3c <__ieee754_logf+0x274>)
 80b1ae0:	f001 fc7c 	bl	80b33dc <__addsf3>
 80b1ae4:	4639      	mov	r1, r7
 80b1ae6:	f001 fd81 	bl	80b35ec <__aeabi_fmul>
 80b1aea:	4955      	ldr	r1, [pc, #340]	; (80b1c40 <__ieee754_logf+0x278>)
 80b1aec:	f001 fc76 	bl	80b33dc <__addsf3>
 80b1af0:	4639      	mov	r1, r7
 80b1af2:	f001 fd7b 	bl	80b35ec <__aeabi_fmul>
 80b1af6:	4601      	mov	r1, r0
 80b1af8:	4650      	mov	r0, sl
 80b1afa:	f001 fc6f 	bl	80b33dc <__addsf3>
 80b1afe:	4f51      	ldr	r7, [pc, #324]	; (80b1c44 <__ieee754_logf+0x27c>)
 80b1b00:	f5c5 1357 	rsb	r3, r5, #3522560	; 0x35c000
 80b1b04:	442f      	add	r7, r5
 80b1b06:	f503 7322 	add.w	r3, r3, #648	; 0x288
 80b1b0a:	431f      	orrs	r7, r3
 80b1b0c:	2f00      	cmp	r7, #0
 80b1b0e:	4682      	mov	sl, r0
 80b1b10:	dd65      	ble.n	80b1bde <__ieee754_logf+0x216>
 80b1b12:	f04f 517c 	mov.w	r1, #1056964608	; 0x3f000000
 80b1b16:	4630      	mov	r0, r6
 80b1b18:	f001 fd68 	bl	80b35ec <__aeabi_fmul>
 80b1b1c:	4631      	mov	r1, r6
 80b1b1e:	f001 fd65 	bl	80b35ec <__aeabi_fmul>
 80b1b22:	4605      	mov	r5, r0
 80b1b24:	2c00      	cmp	r4, #0
 80b1b26:	d132      	bne.n	80b1b8e <__ieee754_logf+0x1c6>
 80b1b28:	4629      	mov	r1, r5
 80b1b2a:	4650      	mov	r0, sl
 80b1b2c:	f001 fc56 	bl	80b33dc <__addsf3>
 80b1b30:	4641      	mov	r1, r8
 80b1b32:	f001 fd5b 	bl	80b35ec <__aeabi_fmul>
 80b1b36:	4601      	mov	r1, r0
 80b1b38:	4628      	mov	r0, r5
 80b1b3a:	f001 fc4d 	bl	80b33d8 <__aeabi_fsub>
 80b1b3e:	4601      	mov	r1, r0
 80b1b40:	4630      	mov	r0, r6
 80b1b42:	f001 fc49 	bl	80b33d8 <__aeabi_fsub>
 80b1b46:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}
 80b1b4a:	493f      	ldr	r1, [pc, #252]	; (80b1c48 <__ieee754_logf+0x280>)
 80b1b4c:	4630      	mov	r0, r6
 80b1b4e:	f001 fd4d 	bl	80b35ec <__aeabi_fmul>
 80b1b52:	4601      	mov	r1, r0
 80b1b54:	f04f 507c 	mov.w	r0, #1056964608	; 0x3f000000
 80b1b58:	f001 fc3e 	bl	80b33d8 <__aeabi_fsub>
 80b1b5c:	4631      	mov	r1, r6
 80b1b5e:	4605      	mov	r5, r0
 80b1b60:	4630      	mov	r0, r6
 80b1b62:	f001 fd43 	bl	80b35ec <__aeabi_fmul>
 80b1b66:	4601      	mov	r1, r0
 80b1b68:	4628      	mov	r0, r5
 80b1b6a:	f001 fd3f 	bl	80b35ec <__aeabi_fmul>
 80b1b6e:	4605      	mov	r5, r0
 80b1b70:	2c00      	cmp	r4, #0
 80b1b72:	d02e      	beq.n	80b1bd2 <__ieee754_logf+0x20a>
 80b1b74:	4620      	mov	r0, r4
 80b1b76:	f001 fce5 	bl	80b3544 <__aeabi_i2f>
 80b1b7a:	4929      	ldr	r1, [pc, #164]	; (80b1c20 <__ieee754_logf+0x258>)
 80b1b7c:	4607      	mov	r7, r0
 80b1b7e:	f001 fd35 	bl	80b35ec <__aeabi_fmul>
 80b1b82:	4604      	mov	r4, r0
 80b1b84:	4638      	mov	r0, r7
 80b1b86:	4927      	ldr	r1, [pc, #156]	; (80b1c24 <__ieee754_logf+0x25c>)
 80b1b88:	f001 fd30 	bl	80b35ec <__aeabi_fmul>
 80b1b8c:	e014      	b.n	80b1bb8 <__ieee754_logf+0x1f0>
 80b1b8e:	4924      	ldr	r1, [pc, #144]	; (80b1c20 <__ieee754_logf+0x258>)
 80b1b90:	4648      	mov	r0, r9
 80b1b92:	f001 fd2b 	bl	80b35ec <__aeabi_fmul>
 80b1b96:	4629      	mov	r1, r5
 80b1b98:	4604      	mov	r4, r0
 80b1b9a:	4650      	mov	r0, sl
 80b1b9c:	f001 fc1e 	bl	80b33dc <__addsf3>
 80b1ba0:	4641      	mov	r1, r8
 80b1ba2:	f001 fd23 	bl	80b35ec <__aeabi_fmul>
 80b1ba6:	491f      	ldr	r1, [pc, #124]	; (80b1c24 <__ieee754_logf+0x25c>)
 80b1ba8:	4607      	mov	r7, r0
 80b1baa:	4648      	mov	r0, r9
 80b1bac:	f001 fd1e 	bl	80b35ec <__aeabi_fmul>
 80b1bb0:	4601      	mov	r1, r0
 80b1bb2:	4638      	mov	r0, r7
 80b1bb4:	f001 fc12 	bl	80b33dc <__addsf3>
 80b1bb8:	4601      	mov	r1, r0
 80b1bba:	4628      	mov	r0, r5
 80b1bbc:	f001 fc0c 	bl	80b33d8 <__aeabi_fsub>
 80b1bc0:	4631      	mov	r1, r6
 80b1bc2:	f001 fc09 	bl	80b33d8 <__aeabi_fsub>
 80b1bc6:	4601      	mov	r1, r0
 80b1bc8:	4620      	mov	r0, r4
 80b1bca:	f001 fc05 	bl	80b33d8 <__aeabi_fsub>
 80b1bce:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}
 80b1bd2:	4601      	mov	r1, r0
 80b1bd4:	4630      	mov	r0, r6
 80b1bd6:	f001 fbff 	bl	80b33d8 <__aeabi_fsub>
 80b1bda:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}
 80b1bde:	b17c      	cbz	r4, 80b1c00 <__ieee754_logf+0x238>
 80b1be0:	490f      	ldr	r1, [pc, #60]	; (80b1c20 <__ieee754_logf+0x258>)
 80b1be2:	4648      	mov	r0, r9
 80b1be4:	f001 fd02 	bl	80b35ec <__aeabi_fmul>
 80b1be8:	4651      	mov	r1, sl
 80b1bea:	4604      	mov	r4, r0
 80b1bec:	4630      	mov	r0, r6
 80b1bee:	f001 fbf3 	bl	80b33d8 <__aeabi_fsub>
 80b1bf2:	4641      	mov	r1, r8
 80b1bf4:	f001 fcfa 	bl	80b35ec <__aeabi_fmul>
 80b1bf8:	490a      	ldr	r1, [pc, #40]	; (80b1c24 <__ieee754_logf+0x25c>)
 80b1bfa:	4605      	mov	r5, r0
 80b1bfc:	4648      	mov	r0, r9
 80b1bfe:	e7c3      	b.n	80b1b88 <__ieee754_logf+0x1c0>
 80b1c00:	4601      	mov	r1, r0
 80b1c02:	4630      	mov	r0, r6
 80b1c04:	f001 fbe8 	bl	80b33d8 <__aeabi_fsub>
 80b1c08:	4641      	mov	r1, r8
 80b1c0a:	f001 fcef 	bl	80b35ec <__aeabi_fmul>
 80b1c0e:	4601      	mov	r1, r0
 80b1c10:	4630      	mov	r0, r6
 80b1c12:	f001 fbe1 	bl	80b33d8 <__aeabi_fsub>
 80b1c16:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}
 80b1c1a:	bf00      	nop
 80b1c1c:	004afb20 	.word	0x004afb20
 80b1c20:	3f317180 	.word	0x3f317180
 80b1c24:	3717f7d1 	.word	0x3717f7d1
 80b1c28:	3e178897 	.word	0x3e178897
 80b1c2c:	3e3a3325 	.word	0x3e3a3325
 80b1c30:	3e924925 	.word	0x3e924925
 80b1c34:	3f2aaaab 	.word	0x3f2aaaab
 80b1c38:	3e1cd04f 	.word	0x3e1cd04f
 80b1c3c:	3e638e29 	.word	0x3e638e29
 80b1c40:	3ecccccd 	.word	0x3ecccccd
 80b1c44:	ffcf5c30 	.word	0xffcf5c30
 80b1c48:	3eaaaaab 	.word	0x3eaaaaab

080b1c4c <__ieee754_rem_pio2f>:
 80b1c4c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80b1c50:	4aaa      	ldr	r2, [pc, #680]	; (80b1efc <__ieee754_rem_pio2f+0x2b0>)
 80b1c52:	f020 4400 	bic.w	r4, r0, #2147483648	; 0x80000000
 80b1c56:	4294      	cmp	r4, r2
 80b1c58:	b089      	sub	sp, #36	; 0x24
 80b1c5a:	dd67      	ble.n	80b1d2c <__ieee754_rem_pio2f+0xe0>
 80b1c5c:	4aa8      	ldr	r2, [pc, #672]	; (80b1f00 <__ieee754_rem_pio2f+0x2b4>)
 80b1c5e:	460d      	mov	r5, r1
 80b1c60:	4294      	cmp	r4, r2
 80b1c62:	4607      	mov	r7, r0
 80b1c64:	4601      	mov	r1, r0
 80b1c66:	dc1c      	bgt.n	80b1ca2 <__ieee754_rem_pio2f+0x56>
 80b1c68:	2800      	cmp	r0, #0
 80b1c6a:	49a6      	ldr	r1, [pc, #664]	; (80b1f04 <__ieee754_rem_pio2f+0x2b8>)
 80b1c6c:	f340 80ea 	ble.w	80b1e44 <__ieee754_rem_pio2f+0x1f8>
 80b1c70:	f001 fbb2 	bl	80b33d8 <__aeabi_fsub>
 80b1c74:	4ba4      	ldr	r3, [pc, #656]	; (80b1f08 <__ieee754_rem_pio2f+0x2bc>)
 80b1c76:	f024 040f 	bic.w	r4, r4, #15
 80b1c7a:	429c      	cmp	r4, r3
 80b1c7c:	4606      	mov	r6, r0
 80b1c7e:	d063      	beq.n	80b1d48 <__ieee754_rem_pio2f+0xfc>
 80b1c80:	49a2      	ldr	r1, [pc, #648]	; (80b1f0c <__ieee754_rem_pio2f+0x2c0>)
 80b1c82:	f001 fba9 	bl	80b33d8 <__aeabi_fsub>
 80b1c86:	4601      	mov	r1, r0
 80b1c88:	4630      	mov	r0, r6
 80b1c8a:	6029      	str	r1, [r5, #0]
 80b1c8c:	f001 fba4 	bl	80b33d8 <__aeabi_fsub>
 80b1c90:	499e      	ldr	r1, [pc, #632]	; (80b1f0c <__ieee754_rem_pio2f+0x2c0>)
 80b1c92:	f001 fba1 	bl	80b33d8 <__aeabi_fsub>
 80b1c96:	2301      	movs	r3, #1
 80b1c98:	6068      	str	r0, [r5, #4]
 80b1c9a:	4618      	mov	r0, r3
 80b1c9c:	b009      	add	sp, #36	; 0x24
 80b1c9e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80b1ca2:	4a9b      	ldr	r2, [pc, #620]	; (80b1f10 <__ieee754_rem_pio2f+0x2c4>)
 80b1ca4:	4294      	cmp	r4, r2
 80b1ca6:	dd61      	ble.n	80b1d6c <__ieee754_rem_pio2f+0x120>
 80b1ca8:	f1b4 4fff 	cmp.w	r4, #2139095040	; 0x7f800000
 80b1cac:	da46      	bge.n	80b1d3c <__ieee754_rem_pio2f+0xf0>
 80b1cae:	15e6      	asrs	r6, r4, #23
 80b1cb0:	3e86      	subs	r6, #134	; 0x86
 80b1cb2:	eba4 54c6 	sub.w	r4, r4, r6, lsl #23
 80b1cb6:	4620      	mov	r0, r4
 80b1cb8:	f001 fe74 	bl	80b39a4 <__aeabi_f2iz>
 80b1cbc:	f001 fc42 	bl	80b3544 <__aeabi_i2f>
 80b1cc0:	4603      	mov	r3, r0
 80b1cc2:	4620      	mov	r0, r4
 80b1cc4:	4619      	mov	r1, r3
 80b1cc6:	9305      	str	r3, [sp, #20]
 80b1cc8:	f001 fb86 	bl	80b33d8 <__aeabi_fsub>
 80b1ccc:	f04f 4187 	mov.w	r1, #1132462080	; 0x43800000
 80b1cd0:	f001 fc8c 	bl	80b35ec <__aeabi_fmul>
 80b1cd4:	4680      	mov	r8, r0
 80b1cd6:	f001 fe65 	bl	80b39a4 <__aeabi_f2iz>
 80b1cda:	f001 fc33 	bl	80b3544 <__aeabi_i2f>
 80b1cde:	4604      	mov	r4, r0
 80b1ce0:	4640      	mov	r0, r8
 80b1ce2:	4621      	mov	r1, r4
 80b1ce4:	9406      	str	r4, [sp, #24]
 80b1ce6:	f001 fb77 	bl	80b33d8 <__aeabi_fsub>
 80b1cea:	f04f 4187 	mov.w	r1, #1132462080	; 0x43800000
 80b1cee:	f001 fc7d 	bl	80b35ec <__aeabi_fmul>
 80b1cf2:	2100      	movs	r1, #0
 80b1cf4:	9007      	str	r0, [sp, #28]
 80b1cf6:	f001 fe0d 	bl	80b3914 <__aeabi_fcmpeq>
 80b1cfa:	2800      	cmp	r0, #0
 80b1cfc:	f000 80c3 	beq.w	80b1e86 <__ieee754_rem_pio2f+0x23a>
 80b1d00:	4620      	mov	r0, r4
 80b1d02:	2100      	movs	r1, #0
 80b1d04:	f001 fe06 	bl	80b3914 <__aeabi_fcmpeq>
 80b1d08:	2800      	cmp	r0, #0
 80b1d0a:	bf14      	ite	ne
 80b1d0c:	2301      	movne	r3, #1
 80b1d0e:	2302      	moveq	r3, #2
 80b1d10:	4880      	ldr	r0, [pc, #512]	; (80b1f14 <__ieee754_rem_pio2f+0x2c8>)
 80b1d12:	2102      	movs	r1, #2
 80b1d14:	9001      	str	r0, [sp, #4]
 80b1d16:	9100      	str	r1, [sp, #0]
 80b1d18:	4632      	mov	r2, r6
 80b1d1a:	4629      	mov	r1, r5
 80b1d1c:	a805      	add	r0, sp, #20
 80b1d1e:	f000 fa1f 	bl	80b2160 <__kernel_rem_pio2f>
 80b1d22:	2f00      	cmp	r7, #0
 80b1d24:	f2c0 80a5 	blt.w	80b1e72 <__ieee754_rem_pio2f+0x226>
 80b1d28:	4603      	mov	r3, r0
 80b1d2a:	e003      	b.n	80b1d34 <__ieee754_rem_pio2f+0xe8>
 80b1d2c:	2200      	movs	r2, #0
 80b1d2e:	6008      	str	r0, [r1, #0]
 80b1d30:	604a      	str	r2, [r1, #4]
 80b1d32:	2300      	movs	r3, #0
 80b1d34:	4618      	mov	r0, r3
 80b1d36:	b009      	add	sp, #36	; 0x24
 80b1d38:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80b1d3c:	f001 fb4c 	bl	80b33d8 <__aeabi_fsub>
 80b1d40:	2300      	movs	r3, #0
 80b1d42:	6068      	str	r0, [r5, #4]
 80b1d44:	6028      	str	r0, [r5, #0]
 80b1d46:	e7f5      	b.n	80b1d34 <__ieee754_rem_pio2f+0xe8>
 80b1d48:	4973      	ldr	r1, [pc, #460]	; (80b1f18 <__ieee754_rem_pio2f+0x2cc>)
 80b1d4a:	f001 fb45 	bl	80b33d8 <__aeabi_fsub>
 80b1d4e:	4973      	ldr	r1, [pc, #460]	; (80b1f1c <__ieee754_rem_pio2f+0x2d0>)
 80b1d50:	4604      	mov	r4, r0
 80b1d52:	f001 fb41 	bl	80b33d8 <__aeabi_fsub>
 80b1d56:	4601      	mov	r1, r0
 80b1d58:	4620      	mov	r0, r4
 80b1d5a:	6029      	str	r1, [r5, #0]
 80b1d5c:	f001 fb3c 	bl	80b33d8 <__aeabi_fsub>
 80b1d60:	496e      	ldr	r1, [pc, #440]	; (80b1f1c <__ieee754_rem_pio2f+0x2d0>)
 80b1d62:	f001 fb39 	bl	80b33d8 <__aeabi_fsub>
 80b1d66:	2301      	movs	r3, #1
 80b1d68:	6068      	str	r0, [r5, #4]
 80b1d6a:	e7e3      	b.n	80b1d34 <__ieee754_rem_pio2f+0xe8>
 80b1d6c:	f000 fdd0 	bl	80b2910 <fabsf>
 80b1d70:	496b      	ldr	r1, [pc, #428]	; (80b1f20 <__ieee754_rem_pio2f+0x2d4>)
 80b1d72:	4680      	mov	r8, r0
 80b1d74:	f001 fc3a 	bl	80b35ec <__aeabi_fmul>
 80b1d78:	f04f 517c 	mov.w	r1, #1056964608	; 0x3f000000
 80b1d7c:	f001 fb2e 	bl	80b33dc <__addsf3>
 80b1d80:	f001 fe10 	bl	80b39a4 <__aeabi_f2iz>
 80b1d84:	4606      	mov	r6, r0
 80b1d86:	f001 fbdd 	bl	80b3544 <__aeabi_i2f>
 80b1d8a:	495e      	ldr	r1, [pc, #376]	; (80b1f04 <__ieee754_rem_pio2f+0x2b8>)
 80b1d8c:	4683      	mov	fp, r0
 80b1d8e:	f001 fc2d 	bl	80b35ec <__aeabi_fmul>
 80b1d92:	4601      	mov	r1, r0
 80b1d94:	4640      	mov	r0, r8
 80b1d96:	f001 fb1f 	bl	80b33d8 <__aeabi_fsub>
 80b1d9a:	495c      	ldr	r1, [pc, #368]	; (80b1f0c <__ieee754_rem_pio2f+0x2c0>)
 80b1d9c:	4681      	mov	r9, r0
 80b1d9e:	4658      	mov	r0, fp
 80b1da0:	f001 fc24 	bl	80b35ec <__aeabi_fmul>
 80b1da4:	2e1f      	cmp	r6, #31
 80b1da6:	4682      	mov	sl, r0
 80b1da8:	4601      	mov	r1, r0
 80b1daa:	4648      	mov	r0, r9
 80b1dac:	dc18      	bgt.n	80b1de0 <__ieee754_rem_pio2f+0x194>
 80b1dae:	4b5d      	ldr	r3, [pc, #372]	; (80b1f24 <__ieee754_rem_pio2f+0x2d8>)
 80b1db0:	1e72      	subs	r2, r6, #1
 80b1db2:	f853 3022 	ldr.w	r3, [r3, r2, lsl #2]
 80b1db6:	f024 02ff 	bic.w	r2, r4, #255	; 0xff
 80b1dba:	429a      	cmp	r2, r3
 80b1dbc:	d010      	beq.n	80b1de0 <__ieee754_rem_pio2f+0x194>
 80b1dbe:	f001 fb0b 	bl	80b33d8 <__aeabi_fsub>
 80b1dc2:	4680      	mov	r8, r0
 80b1dc4:	f8c5 8000 	str.w	r8, [r5]
 80b1dc8:	4641      	mov	r1, r8
 80b1dca:	4648      	mov	r0, r9
 80b1dcc:	f001 fb04 	bl	80b33d8 <__aeabi_fsub>
 80b1dd0:	4651      	mov	r1, sl
 80b1dd2:	f001 fb01 	bl	80b33d8 <__aeabi_fsub>
 80b1dd6:	2f00      	cmp	r7, #0
 80b1dd8:	6068      	str	r0, [r5, #4]
 80b1dda:	db56      	blt.n	80b1e8a <__ieee754_rem_pio2f+0x23e>
 80b1ddc:	4633      	mov	r3, r6
 80b1dde:	e7a9      	b.n	80b1d34 <__ieee754_rem_pio2f+0xe8>
 80b1de0:	f001 fafa 	bl	80b33d8 <__aeabi_fsub>
 80b1de4:	15e2      	asrs	r2, r4, #23
 80b1de6:	f3c0 53c7 	ubfx	r3, r0, #23, #8
 80b1dea:	1ad3      	subs	r3, r2, r3
 80b1dec:	2b08      	cmp	r3, #8
 80b1dee:	4680      	mov	r8, r0
 80b1df0:	dde8      	ble.n	80b1dc4 <__ieee754_rem_pio2f+0x178>
 80b1df2:	4949      	ldr	r1, [pc, #292]	; (80b1f18 <__ieee754_rem_pio2f+0x2cc>)
 80b1df4:	4658      	mov	r0, fp
 80b1df6:	9203      	str	r2, [sp, #12]
 80b1df8:	f001 fbf8 	bl	80b35ec <__aeabi_fmul>
 80b1dfc:	4680      	mov	r8, r0
 80b1dfe:	4601      	mov	r1, r0
 80b1e00:	4648      	mov	r0, r9
 80b1e02:	f001 fae9 	bl	80b33d8 <__aeabi_fsub>
 80b1e06:	4601      	mov	r1, r0
 80b1e08:	4604      	mov	r4, r0
 80b1e0a:	4648      	mov	r0, r9
 80b1e0c:	f001 fae4 	bl	80b33d8 <__aeabi_fsub>
 80b1e10:	4641      	mov	r1, r8
 80b1e12:	f001 fae1 	bl	80b33d8 <__aeabi_fsub>
 80b1e16:	4680      	mov	r8, r0
 80b1e18:	4940      	ldr	r1, [pc, #256]	; (80b1f1c <__ieee754_rem_pio2f+0x2d0>)
 80b1e1a:	4658      	mov	r0, fp
 80b1e1c:	f001 fbe6 	bl	80b35ec <__aeabi_fmul>
 80b1e20:	4641      	mov	r1, r8
 80b1e22:	f001 fad9 	bl	80b33d8 <__aeabi_fsub>
 80b1e26:	4601      	mov	r1, r0
 80b1e28:	4682      	mov	sl, r0
 80b1e2a:	4620      	mov	r0, r4
 80b1e2c:	f001 fad4 	bl	80b33d8 <__aeabi_fsub>
 80b1e30:	9a03      	ldr	r2, [sp, #12]
 80b1e32:	f3c0 53c7 	ubfx	r3, r0, #23, #8
 80b1e36:	1ad2      	subs	r2, r2, r3
 80b1e38:	2a19      	cmp	r2, #25
 80b1e3a:	4680      	mov	r8, r0
 80b1e3c:	dc41      	bgt.n	80b1ec2 <__ieee754_rem_pio2f+0x276>
 80b1e3e:	6028      	str	r0, [r5, #0]
 80b1e40:	46a1      	mov	r9, r4
 80b1e42:	e7c1      	b.n	80b1dc8 <__ieee754_rem_pio2f+0x17c>
 80b1e44:	f001 faca 	bl	80b33dc <__addsf3>
 80b1e48:	4b2f      	ldr	r3, [pc, #188]	; (80b1f08 <__ieee754_rem_pio2f+0x2bc>)
 80b1e4a:	f024 040f 	bic.w	r4, r4, #15
 80b1e4e:	429c      	cmp	r4, r3
 80b1e50:	4606      	mov	r6, r0
 80b1e52:	d023      	beq.n	80b1e9c <__ieee754_rem_pio2f+0x250>
 80b1e54:	492d      	ldr	r1, [pc, #180]	; (80b1f0c <__ieee754_rem_pio2f+0x2c0>)
 80b1e56:	f001 fac1 	bl	80b33dc <__addsf3>
 80b1e5a:	4601      	mov	r1, r0
 80b1e5c:	4630      	mov	r0, r6
 80b1e5e:	6029      	str	r1, [r5, #0]
 80b1e60:	f001 faba 	bl	80b33d8 <__aeabi_fsub>
 80b1e64:	4929      	ldr	r1, [pc, #164]	; (80b1f0c <__ieee754_rem_pio2f+0x2c0>)
 80b1e66:	f001 fab9 	bl	80b33dc <__addsf3>
 80b1e6a:	f04f 33ff 	mov.w	r3, #4294967295	; 0xffffffff
 80b1e6e:	6068      	str	r0, [r5, #4]
 80b1e70:	e760      	b.n	80b1d34 <__ieee754_rem_pio2f+0xe8>
 80b1e72:	e895 000c 	ldmia.w	r5, {r2, r3}
 80b1e76:	f102 4100 	add.w	r1, r2, #2147483648	; 0x80000000
 80b1e7a:	f103 4200 	add.w	r2, r3, #2147483648	; 0x80000000
 80b1e7e:	6029      	str	r1, [r5, #0]
 80b1e80:	4243      	negs	r3, r0
 80b1e82:	606a      	str	r2, [r5, #4]
 80b1e84:	e756      	b.n	80b1d34 <__ieee754_rem_pio2f+0xe8>
 80b1e86:	2303      	movs	r3, #3
 80b1e88:	e742      	b.n	80b1d10 <__ieee754_rem_pio2f+0xc4>
 80b1e8a:	f108 4800 	add.w	r8, r8, #2147483648	; 0x80000000
 80b1e8e:	f100 4000 	add.w	r0, r0, #2147483648	; 0x80000000
 80b1e92:	f8c5 8000 	str.w	r8, [r5]
 80b1e96:	6068      	str	r0, [r5, #4]
 80b1e98:	4273      	negs	r3, r6
 80b1e9a:	e74b      	b.n	80b1d34 <__ieee754_rem_pio2f+0xe8>
 80b1e9c:	491e      	ldr	r1, [pc, #120]	; (80b1f18 <__ieee754_rem_pio2f+0x2cc>)
 80b1e9e:	f001 fa9d 	bl	80b33dc <__addsf3>
 80b1ea2:	491e      	ldr	r1, [pc, #120]	; (80b1f1c <__ieee754_rem_pio2f+0x2d0>)
 80b1ea4:	4604      	mov	r4, r0
 80b1ea6:	f001 fa99 	bl	80b33dc <__addsf3>
 80b1eaa:	4601      	mov	r1, r0
 80b1eac:	4620      	mov	r0, r4
 80b1eae:	6029      	str	r1, [r5, #0]
 80b1eb0:	f001 fa92 	bl	80b33d8 <__aeabi_fsub>
 80b1eb4:	4919      	ldr	r1, [pc, #100]	; (80b1f1c <__ieee754_rem_pio2f+0x2d0>)
 80b1eb6:	f001 fa91 	bl	80b33dc <__addsf3>
 80b1eba:	f04f 33ff 	mov.w	r3, #4294967295	; 0xffffffff
 80b1ebe:	6068      	str	r0, [r5, #4]
 80b1ec0:	e738      	b.n	80b1d34 <__ieee754_rem_pio2f+0xe8>
 80b1ec2:	4919      	ldr	r1, [pc, #100]	; (80b1f28 <__ieee754_rem_pio2f+0x2dc>)
 80b1ec4:	4658      	mov	r0, fp
 80b1ec6:	f001 fb91 	bl	80b35ec <__aeabi_fmul>
 80b1eca:	4601      	mov	r1, r0
 80b1ecc:	4680      	mov	r8, r0
 80b1ece:	4620      	mov	r0, r4
 80b1ed0:	f001 fa82 	bl	80b33d8 <__aeabi_fsub>
 80b1ed4:	4601      	mov	r1, r0
 80b1ed6:	4681      	mov	r9, r0
 80b1ed8:	4620      	mov	r0, r4
 80b1eda:	f001 fa7d 	bl	80b33d8 <__aeabi_fsub>
 80b1ede:	4641      	mov	r1, r8
 80b1ee0:	f001 fa7a 	bl	80b33d8 <__aeabi_fsub>
 80b1ee4:	4604      	mov	r4, r0
 80b1ee6:	4911      	ldr	r1, [pc, #68]	; (80b1f2c <__ieee754_rem_pio2f+0x2e0>)
 80b1ee8:	4658      	mov	r0, fp
 80b1eea:	f001 fb7f 	bl	80b35ec <__aeabi_fmul>
 80b1eee:	4621      	mov	r1, r4
 80b1ef0:	f001 fa72 	bl	80b33d8 <__aeabi_fsub>
 80b1ef4:	4682      	mov	sl, r0
 80b1ef6:	4601      	mov	r1, r0
 80b1ef8:	4648      	mov	r0, r9
 80b1efa:	e760      	b.n	80b1dbe <__ieee754_rem_pio2f+0x172>
 80b1efc:	3f490fd8 	.word	0x3f490fd8
 80b1f00:	4016cbe3 	.word	0x4016cbe3
 80b1f04:	3fc90f80 	.word	0x3fc90f80
 80b1f08:	3fc90fd0 	.word	0x3fc90fd0
 80b1f0c:	37354443 	.word	0x37354443
 80b1f10:	43490f80 	.word	0x43490f80
 80b1f14:	080b78a8 	.word	0x080b78a8
 80b1f18:	37354400 	.word	0x37354400
 80b1f1c:	2e85a308 	.word	0x2e85a308
 80b1f20:	3f22f984 	.word	0x3f22f984
 80b1f24:	080b7828 	.word	0x080b7828
 80b1f28:	2e85a300 	.word	0x2e85a300
 80b1f2c:	248d3132 	.word	0x248d3132

080b1f30 <__ieee754_sqrtf>:
 80b1f30:	f020 4200 	bic.w	r2, r0, #2147483648	; 0x80000000
 80b1f34:	f1b2 4fff 	cmp.w	r2, #2139095040	; 0x7f800000
 80b1f38:	b570      	push	{r4, r5, r6, lr}
 80b1f3a:	4604      	mov	r4, r0
 80b1f3c:	d22e      	bcs.n	80b1f9c <__ieee754_sqrtf+0x6c>
 80b1f3e:	b362      	cbz	r2, 80b1f9a <__ieee754_sqrtf+0x6a>
 80b1f40:	2800      	cmp	r0, #0
 80b1f42:	4603      	mov	r3, r0
 80b1f44:	db3d      	blt.n	80b1fc2 <__ieee754_sqrtf+0x92>
 80b1f46:	f5b2 0f00 	cmp.w	r2, #8388608	; 0x800000
 80b1f4a:	ea4f 50e0 	mov.w	r0, r0, asr #23
 80b1f4e:	d32c      	bcc.n	80b1faa <__ieee754_sqrtf+0x7a>
 80b1f50:	2600      	movs	r6, #0
 80b1f52:	4631      	mov	r1, r6
 80b1f54:	387f      	subs	r0, #127	; 0x7f
 80b1f56:	f3c3 0316 	ubfx	r3, r3, #0, #23
 80b1f5a:	07c2      	lsls	r2, r0, #31
 80b1f5c:	f443 0300 	orr.w	r3, r3, #8388608	; 0x800000
 80b1f60:	bf48      	it	mi
 80b1f62:	005b      	lslmi	r3, r3, #1
 80b1f64:	1040      	asrs	r0, r0, #1
 80b1f66:	005b      	lsls	r3, r3, #1
 80b1f68:	2419      	movs	r4, #25
 80b1f6a:	f04f 7280 	mov.w	r2, #16777216	; 0x1000000
 80b1f6e:	188d      	adds	r5, r1, r2
 80b1f70:	429d      	cmp	r5, r3
 80b1f72:	dc02      	bgt.n	80b1f7a <__ieee754_sqrtf+0x4a>
 80b1f74:	1b5b      	subs	r3, r3, r5
 80b1f76:	18a9      	adds	r1, r5, r2
 80b1f78:	4416      	add	r6, r2
 80b1f7a:	3c01      	subs	r4, #1
 80b1f7c:	ea4f 0343 	mov.w	r3, r3, lsl #1
 80b1f80:	ea4f 0252 	mov.w	r2, r2, lsr #1
 80b1f84:	d1f3      	bne.n	80b1f6e <__ieee754_sqrtf+0x3e>
 80b1f86:	b113      	cbz	r3, 80b1f8e <__ieee754_sqrtf+0x5e>
 80b1f88:	f006 0301 	and.w	r3, r6, #1
 80b1f8c:	441e      	add	r6, r3
 80b1f8e:	1076      	asrs	r6, r6, #1
 80b1f90:	f106 567c 	add.w	r6, r6, #1056964608	; 0x3f000000
 80b1f94:	eb06 50c0 	add.w	r0, r6, r0, lsl #23
 80b1f98:	bd70      	pop	{r4, r5, r6, pc}
 80b1f9a:	bd70      	pop	{r4, r5, r6, pc}
 80b1f9c:	4601      	mov	r1, r0
 80b1f9e:	f001 fb25 	bl	80b35ec <__aeabi_fmul>
 80b1fa2:	4621      	mov	r1, r4
 80b1fa4:	f001 fa1a 	bl	80b33dc <__addsf3>
 80b1fa8:	bd70      	pop	{r4, r5, r6, pc}
 80b1faa:	f414 0200 	ands.w	r2, r4, #8388608	; 0x800000
 80b1fae:	d001      	beq.n	80b1fb4 <__ieee754_sqrtf+0x84>
 80b1fb0:	e00e      	b.n	80b1fd0 <__ieee754_sqrtf+0xa0>
 80b1fb2:	460a      	mov	r2, r1
 80b1fb4:	005b      	lsls	r3, r3, #1
 80b1fb6:	021c      	lsls	r4, r3, #8
 80b1fb8:	f102 0101 	add.w	r1, r2, #1
 80b1fbc:	d5f9      	bpl.n	80b1fb2 <__ieee754_sqrtf+0x82>
 80b1fbe:	1a80      	subs	r0, r0, r2
 80b1fc0:	e7c6      	b.n	80b1f50 <__ieee754_sqrtf+0x20>
 80b1fc2:	4601      	mov	r1, r0
 80b1fc4:	f001 fa08 	bl	80b33d8 <__aeabi_fsub>
 80b1fc8:	4601      	mov	r1, r0
 80b1fca:	f001 fbc3 	bl	80b3754 <__aeabi_fdiv>
 80b1fce:	bd70      	pop	{r4, r5, r6, pc}
 80b1fd0:	f04f 32ff 	mov.w	r2, #4294967295	; 0xffffffff
 80b1fd4:	e7f3      	b.n	80b1fbe <__ieee754_sqrtf+0x8e>
 80b1fd6:	bf00      	nop

080b1fd8 <__kernel_cosf>:
 80b1fd8:	e92d 43f8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, lr}
 80b1fdc:	f020 4400 	bic.w	r4, r0, #2147483648	; 0x80000000
 80b1fe0:	f1b4 5f48 	cmp.w	r4, #838860800	; 0x32000000
 80b1fe4:	4606      	mov	r6, r0
 80b1fe6:	460f      	mov	r7, r1
 80b1fe8:	da49      	bge.n	80b207e <__kernel_cosf+0xa6>
 80b1fea:	f001 fcdb 	bl	80b39a4 <__aeabi_f2iz>
 80b1fee:	2800      	cmp	r0, #0
 80b1ff0:	f000 809d 	beq.w	80b212e <__kernel_cosf+0x156>
 80b1ff4:	4631      	mov	r1, r6
 80b1ff6:	4630      	mov	r0, r6
 80b1ff8:	f001 faf8 	bl	80b35ec <__aeabi_fmul>
 80b1ffc:	4605      	mov	r5, r0
 80b1ffe:	494e      	ldr	r1, [pc, #312]	; (80b2138 <__kernel_cosf+0x160>)
 80b2000:	f001 faf4 	bl	80b35ec <__aeabi_fmul>
 80b2004:	494d      	ldr	r1, [pc, #308]	; (80b213c <__kernel_cosf+0x164>)
 80b2006:	f001 f9e9 	bl	80b33dc <__addsf3>
 80b200a:	4629      	mov	r1, r5
 80b200c:	f001 faee 	bl	80b35ec <__aeabi_fmul>
 80b2010:	494b      	ldr	r1, [pc, #300]	; (80b2140 <__kernel_cosf+0x168>)
 80b2012:	f001 f9e1 	bl	80b33d8 <__aeabi_fsub>
 80b2016:	4629      	mov	r1, r5
 80b2018:	f001 fae8 	bl	80b35ec <__aeabi_fmul>
 80b201c:	4949      	ldr	r1, [pc, #292]	; (80b2144 <__kernel_cosf+0x16c>)
 80b201e:	f001 f9dd 	bl	80b33dc <__addsf3>
 80b2022:	4629      	mov	r1, r5
 80b2024:	f001 fae2 	bl	80b35ec <__aeabi_fmul>
 80b2028:	4947      	ldr	r1, [pc, #284]	; (80b2148 <__kernel_cosf+0x170>)
 80b202a:	f001 f9d5 	bl	80b33d8 <__aeabi_fsub>
 80b202e:	4629      	mov	r1, r5
 80b2030:	f001 fadc 	bl	80b35ec <__aeabi_fmul>
 80b2034:	4945      	ldr	r1, [pc, #276]	; (80b214c <__kernel_cosf+0x174>)
 80b2036:	f001 f9d1 	bl	80b33dc <__addsf3>
 80b203a:	4629      	mov	r1, r5
 80b203c:	f001 fad6 	bl	80b35ec <__aeabi_fmul>
 80b2040:	4680      	mov	r8, r0
 80b2042:	f04f 517c 	mov.w	r1, #1056964608	; 0x3f000000
 80b2046:	4628      	mov	r0, r5
 80b2048:	f001 fad0 	bl	80b35ec <__aeabi_fmul>
 80b204c:	4641      	mov	r1, r8
 80b204e:	4604      	mov	r4, r0
 80b2050:	4628      	mov	r0, r5
 80b2052:	f001 facb 	bl	80b35ec <__aeabi_fmul>
 80b2056:	4639      	mov	r1, r7
 80b2058:	4605      	mov	r5, r0
 80b205a:	4630      	mov	r0, r6
 80b205c:	f001 fac6 	bl	80b35ec <__aeabi_fmul>
 80b2060:	4601      	mov	r1, r0
 80b2062:	4628      	mov	r0, r5
 80b2064:	f001 f9b8 	bl	80b33d8 <__aeabi_fsub>
 80b2068:	4601      	mov	r1, r0
 80b206a:	4620      	mov	r0, r4
 80b206c:	f001 f9b4 	bl	80b33d8 <__aeabi_fsub>
 80b2070:	4601      	mov	r1, r0
 80b2072:	f04f 507e 	mov.w	r0, #1065353216	; 0x3f800000
 80b2076:	f001 f9af 	bl	80b33d8 <__aeabi_fsub>
 80b207a:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
 80b207e:	4601      	mov	r1, r0
 80b2080:	f001 fab4 	bl	80b35ec <__aeabi_fmul>
 80b2084:	4605      	mov	r5, r0
 80b2086:	492c      	ldr	r1, [pc, #176]	; (80b2138 <__kernel_cosf+0x160>)
 80b2088:	f001 fab0 	bl	80b35ec <__aeabi_fmul>
 80b208c:	492b      	ldr	r1, [pc, #172]	; (80b213c <__kernel_cosf+0x164>)
 80b208e:	f001 f9a5 	bl	80b33dc <__addsf3>
 80b2092:	4629      	mov	r1, r5
 80b2094:	f001 faaa 	bl	80b35ec <__aeabi_fmul>
 80b2098:	4929      	ldr	r1, [pc, #164]	; (80b2140 <__kernel_cosf+0x168>)
 80b209a:	f001 f99d 	bl	80b33d8 <__aeabi_fsub>
 80b209e:	4629      	mov	r1, r5
 80b20a0:	f001 faa4 	bl	80b35ec <__aeabi_fmul>
 80b20a4:	4927      	ldr	r1, [pc, #156]	; (80b2144 <__kernel_cosf+0x16c>)
 80b20a6:	f001 f999 	bl	80b33dc <__addsf3>
 80b20aa:	4629      	mov	r1, r5
 80b20ac:	f001 fa9e 	bl	80b35ec <__aeabi_fmul>
 80b20b0:	4925      	ldr	r1, [pc, #148]	; (80b2148 <__kernel_cosf+0x170>)
 80b20b2:	f001 f991 	bl	80b33d8 <__aeabi_fsub>
 80b20b6:	4629      	mov	r1, r5
 80b20b8:	f001 fa98 	bl	80b35ec <__aeabi_fmul>
 80b20bc:	4923      	ldr	r1, [pc, #140]	; (80b214c <__kernel_cosf+0x174>)
 80b20be:	f001 f98d 	bl	80b33dc <__addsf3>
 80b20c2:	4629      	mov	r1, r5
 80b20c4:	f001 fa92 	bl	80b35ec <__aeabi_fmul>
 80b20c8:	4b21      	ldr	r3, [pc, #132]	; (80b2150 <__kernel_cosf+0x178>)
 80b20ca:	4680      	mov	r8, r0
 80b20cc:	429c      	cmp	r4, r3
 80b20ce:	ddb8      	ble.n	80b2042 <__kernel_cosf+0x6a>
 80b20d0:	4b20      	ldr	r3, [pc, #128]	; (80b2154 <__kernel_cosf+0x17c>)
 80b20d2:	429c      	cmp	r4, r3
 80b20d4:	dc27      	bgt.n	80b2126 <__kernel_cosf+0x14e>
 80b20d6:	f104 447f 	add.w	r4, r4, #4278190080	; 0xff000000
 80b20da:	4621      	mov	r1, r4
 80b20dc:	f04f 507e 	mov.w	r0, #1065353216	; 0x3f800000
 80b20e0:	f001 f97a 	bl	80b33d8 <__aeabi_fsub>
 80b20e4:	4681      	mov	r9, r0
 80b20e6:	f04f 517c 	mov.w	r1, #1056964608	; 0x3f000000
 80b20ea:	4628      	mov	r0, r5
 80b20ec:	f001 fa7e 	bl	80b35ec <__aeabi_fmul>
 80b20f0:	4621      	mov	r1, r4
 80b20f2:	f001 f971 	bl	80b33d8 <__aeabi_fsub>
 80b20f6:	4641      	mov	r1, r8
 80b20f8:	4604      	mov	r4, r0
 80b20fa:	4628      	mov	r0, r5
 80b20fc:	f001 fa76 	bl	80b35ec <__aeabi_fmul>
 80b2100:	4639      	mov	r1, r7
 80b2102:	4605      	mov	r5, r0
 80b2104:	4630      	mov	r0, r6
 80b2106:	f001 fa71 	bl	80b35ec <__aeabi_fmul>
 80b210a:	4601      	mov	r1, r0
 80b210c:	4628      	mov	r0, r5
 80b210e:	f001 f963 	bl	80b33d8 <__aeabi_fsub>
 80b2112:	4601      	mov	r1, r0
 80b2114:	4620      	mov	r0, r4
 80b2116:	f001 f95f 	bl	80b33d8 <__aeabi_fsub>
 80b211a:	4601      	mov	r1, r0
 80b211c:	4648      	mov	r0, r9
 80b211e:	f001 f95b 	bl	80b33d8 <__aeabi_fsub>
 80b2122:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
 80b2126:	f8df 9034 	ldr.w	r9, [pc, #52]	; 80b215c <__kernel_cosf+0x184>
 80b212a:	4c0b      	ldr	r4, [pc, #44]	; (80b2158 <__kernel_cosf+0x180>)
 80b212c:	e7db      	b.n	80b20e6 <__kernel_cosf+0x10e>
 80b212e:	f04f 507e 	mov.w	r0, #1065353216	; 0x3f800000
 80b2132:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
 80b2136:	bf00      	nop
 80b2138:	ad47d74e 	.word	0xad47d74e
 80b213c:	310f74f6 	.word	0x310f74f6
 80b2140:	3493f27c 	.word	0x3493f27c
 80b2144:	37d00d01 	.word	0x37d00d01
 80b2148:	3ab60b61 	.word	0x3ab60b61
 80b214c:	3d2aaaab 	.word	0x3d2aaaab
 80b2150:	3e999999 	.word	0x3e999999
 80b2154:	3f480000 	.word	0x3f480000
 80b2158:	3e900000 	.word	0x3e900000
 80b215c:	3f380000 	.word	0x3f380000

080b2160 <__kernel_rem_pio2f>:
 80b2160:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
 80b2164:	461c      	mov	r4, r3
 80b2166:	b0df      	sub	sp, #380	; 0x17c
 80b2168:	9108      	str	r1, [sp, #32]
 80b216a:	1e61      	subs	r1, r4, #1
 80b216c:	9101      	str	r1, [sp, #4]
 80b216e:	930a      	str	r3, [sp, #40]	; 0x28
 80b2170:	9968      	ldr	r1, [sp, #416]	; 0x1a0
 80b2172:	4bbd      	ldr	r3, [pc, #756]	; (80b2468 <__kernel_rem_pio2f+0x308>)
 80b2174:	9002      	str	r0, [sp, #8]
 80b2176:	f853 3021 	ldr.w	r3, [r3, r1, lsl #2]
 80b217a:	9307      	str	r3, [sp, #28]
 80b217c:	1ed3      	subs	r3, r2, #3
 80b217e:	bf48      	it	mi
 80b2180:	1d13      	addmi	r3, r2, #4
 80b2182:	10db      	asrs	r3, r3, #3
 80b2184:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
 80b2188:	4619      	mov	r1, r3
 80b218a:	930b      	str	r3, [sp, #44]	; 0x2c
 80b218c:	3301      	adds	r3, #1
 80b218e:	eba2 03c3 	sub.w	r3, r2, r3, lsl #3
 80b2192:	9306      	str	r3, [sp, #24]
 80b2194:	9a01      	ldr	r2, [sp, #4]
 80b2196:	9b07      	ldr	r3, [sp, #28]
 80b2198:	1a8d      	subs	r5, r1, r2
 80b219a:	189c      	adds	r4, r3, r2
 80b219c:	d415      	bmi.n	80b21ca <__kernel_rem_pio2f+0x6a>
 80b219e:	442c      	add	r4, r5
 80b21a0:	3401      	adds	r4, #1
 80b21a2:	ae21      	add	r6, sp, #132	; 0x84
 80b21a4:	9f69      	ldr	r7, [sp, #420]	; 0x1a4
 80b21a6:	e008      	b.n	80b21ba <__kernel_rem_pio2f+0x5a>
 80b21a8:	f857 0025 	ldr.w	r0, [r7, r5, lsl #2]
 80b21ac:	f001 f9ca 	bl	80b3544 <__aeabi_i2f>
 80b21b0:	3501      	adds	r5, #1
 80b21b2:	42a5      	cmp	r5, r4
 80b21b4:	f846 0f04 	str.w	r0, [r6, #4]!
 80b21b8:	d007      	beq.n	80b21ca <__kernel_rem_pio2f+0x6a>
 80b21ba:	2d00      	cmp	r5, #0
 80b21bc:	daf4      	bge.n	80b21a8 <__kernel_rem_pio2f+0x48>
 80b21be:	3501      	adds	r5, #1
 80b21c0:	2000      	movs	r0, #0
 80b21c2:	42a5      	cmp	r5, r4
 80b21c4:	f846 0f04 	str.w	r0, [r6, #4]!
 80b21c8:	d1f7      	bne.n	80b21ba <__kernel_rem_pio2f+0x5a>
 80b21ca:	9b07      	ldr	r3, [sp, #28]
 80b21cc:	2b00      	cmp	r3, #0
 80b21ce:	db2d      	blt.n	80b222c <__kernel_rem_pio2f+0xcc>
 80b21d0:	9b07      	ldr	r3, [sp, #28]
 80b21d2:	9a0a      	ldr	r2, [sp, #40]	; 0x28
 80b21d4:	4619      	mov	r1, r3
 80b21d6:	9b01      	ldr	r3, [sp, #4]
 80b21d8:	eb01 0802 	add.w	r8, r1, r2
 80b21dc:	ebc2 7b82 	rsb	fp, r2, r2, lsl #30
 80b21e0:	aa22      	add	r2, sp, #136	; 0x88
 80b21e2:	eb02 0683 	add.w	r6, r2, r3, lsl #2
 80b21e6:	eb02 0888 	add.w	r8, r2, r8, lsl #2
 80b21ea:	ea4f 0b8b 	mov.w	fp, fp, lsl #2
 80b21ee:	af49      	add	r7, sp, #292	; 0x124
 80b21f0:	9b01      	ldr	r3, [sp, #4]
 80b21f2:	2b00      	cmp	r3, #0
 80b21f4:	f2c0 819d 	blt.w	80b2532 <__kernel_rem_pio2f+0x3d2>
 80b21f8:	4634      	mov	r4, r6
 80b21fa:	9b02      	ldr	r3, [sp, #8]
 80b21fc:	eb06 050b 	add.w	r5, r6, fp
 80b2200:	f1a3 0a04 	sub.w	sl, r3, #4
 80b2204:	f04f 0900 	mov.w	r9, #0
 80b2208:	f85a 1f04 	ldr.w	r1, [sl, #4]!
 80b220c:	f854 0904 	ldr.w	r0, [r4], #-4
 80b2210:	f001 f9ec 	bl	80b35ec <__aeabi_fmul>
 80b2214:	4601      	mov	r1, r0
 80b2216:	4648      	mov	r0, r9
 80b2218:	f001 f8e0 	bl	80b33dc <__addsf3>
 80b221c:	42ac      	cmp	r4, r5
 80b221e:	4681      	mov	r9, r0
 80b2220:	d1f2      	bne.n	80b2208 <__kernel_rem_pio2f+0xa8>
 80b2222:	3604      	adds	r6, #4
 80b2224:	4546      	cmp	r6, r8
 80b2226:	f847 9f04 	str.w	r9, [r7, #4]!
 80b222a:	d1e1      	bne.n	80b21f0 <__kernel_rem_pio2f+0x90>
 80b222c:	9b07      	ldr	r3, [sp, #28]
 80b222e:	a90e      	add	r1, sp, #56	; 0x38
 80b2230:	461e      	mov	r6, r3
 80b2232:	f103 4380 	add.w	r3, r3, #1073741824	; 0x40000000
 80b2236:	3b02      	subs	r3, #2
 80b2238:	009b      	lsls	r3, r3, #2
 80b223a:	1d1a      	adds	r2, r3, #4
 80b223c:	440a      	add	r2, r1
 80b223e:	440b      	add	r3, r1
 80b2240:	920d      	str	r2, [sp, #52]	; 0x34
 80b2242:	930c      	str	r3, [sp, #48]	; 0x30
 80b2244:	ab5e      	add	r3, sp, #376	; 0x178
 80b2246:	eb03 0386 	add.w	r3, r3, r6, lsl #2
 80b224a:	2e00      	cmp	r6, #0
 80b224c:	f853 8c50 	ldr.w	r8, [r3, #-80]
 80b2250:	dd23      	ble.n	80b229a <__kernel_rem_pio2f+0x13a>
 80b2252:	ab5e      	add	r3, sp, #376	; 0x178
 80b2254:	eb03 0486 	add.w	r4, r3, r6, lsl #2
 80b2258:	3c54      	subs	r4, #84	; 0x54
 80b225a:	ad0d      	add	r5, sp, #52	; 0x34
 80b225c:	af49      	add	r7, sp, #292	; 0x124
 80b225e:	f04f 516e 	mov.w	r1, #998244352	; 0x3b800000
 80b2262:	4640      	mov	r0, r8
 80b2264:	f001 f9c2 	bl	80b35ec <__aeabi_fmul>
 80b2268:	f001 fb9c 	bl	80b39a4 <__aeabi_f2iz>
 80b226c:	f001 f96a 	bl	80b3544 <__aeabi_i2f>
 80b2270:	f04f 4187 	mov.w	r1, #1132462080	; 0x43800000
 80b2274:	4681      	mov	r9, r0
 80b2276:	f001 f9b9 	bl	80b35ec <__aeabi_fmul>
 80b227a:	4601      	mov	r1, r0
 80b227c:	4640      	mov	r0, r8
 80b227e:	f001 f8ab 	bl	80b33d8 <__aeabi_fsub>
 80b2282:	f001 fb8f 	bl	80b39a4 <__aeabi_f2iz>
 80b2286:	f854 1904 	ldr.w	r1, [r4], #-4
 80b228a:	f845 0f04 	str.w	r0, [r5, #4]!
 80b228e:	4648      	mov	r0, r9
 80b2290:	f001 f8a4 	bl	80b33dc <__addsf3>
 80b2294:	42bc      	cmp	r4, r7
 80b2296:	4680      	mov	r8, r0
 80b2298:	d1e1      	bne.n	80b225e <__kernel_rem_pio2f+0xfe>
 80b229a:	4640      	mov	r0, r8
 80b229c:	f8dd 8018 	ldr.w	r8, [sp, #24]
 80b22a0:	4641      	mov	r1, r8
 80b22a2:	f000 fb41 	bl	80b2928 <scalbnf>
 80b22a6:	f04f 5178 	mov.w	r1, #1040187392	; 0x3e000000
 80b22aa:	4604      	mov	r4, r0
 80b22ac:	f001 f99e 	bl	80b35ec <__aeabi_fmul>
 80b22b0:	f7fe fe7a 	bl	80b0fa8 <floorf>
 80b22b4:	f04f 4182 	mov.w	r1, #1090519040	; 0x41000000
 80b22b8:	f001 f998 	bl	80b35ec <__aeabi_fmul>
 80b22bc:	4601      	mov	r1, r0
 80b22be:	4620      	mov	r0, r4
 80b22c0:	f001 f88a 	bl	80b33d8 <__aeabi_fsub>
 80b22c4:	4604      	mov	r4, r0
 80b22c6:	f001 fb6d 	bl	80b39a4 <__aeabi_f2iz>
 80b22ca:	4607      	mov	r7, r0
 80b22cc:	f001 f93a 	bl	80b3544 <__aeabi_i2f>
 80b22d0:	4601      	mov	r1, r0
 80b22d2:	4620      	mov	r0, r4
 80b22d4:	4644      	mov	r4, r8
 80b22d6:	f001 f87f 	bl	80b33d8 <__aeabi_fsub>
 80b22da:	2c00      	cmp	r4, #0
 80b22dc:	4605      	mov	r5, r0
 80b22de:	f340 810f 	ble.w	80b2500 <__kernel_rem_pio2f+0x3a0>
 80b22e2:	1e70      	subs	r0, r6, #1
 80b22e4:	ab0e      	add	r3, sp, #56	; 0x38
 80b22e6:	f853 3020 	ldr.w	r3, [r3, r0, lsl #2]
 80b22ea:	f1c8 0208 	rsb	r2, r8, #8
 80b22ee:	fa43 f102 	asr.w	r1, r3, r2
 80b22f2:	fa01 f202 	lsl.w	r2, r1, r2
 80b22f6:	1a9b      	subs	r3, r3, r2
 80b22f8:	f1c8 0a07 	rsb	sl, r8, #7
 80b22fc:	aa0e      	add	r2, sp, #56	; 0x38
 80b22fe:	f842 3020 	str.w	r3, [r2, r0, lsl #2]
 80b2302:	440f      	add	r7, r1
 80b2304:	fa43 fa0a 	asr.w	sl, r3, sl
 80b2308:	f1ba 0f00 	cmp.w	sl, #0
 80b230c:	dd30      	ble.n	80b2370 <__kernel_rem_pio2f+0x210>
 80b230e:	2e00      	cmp	r6, #0
 80b2310:	f107 0701 	add.w	r7, r7, #1
 80b2314:	f340 824e 	ble.w	80b27b4 <__kernel_rem_pio2f+0x654>
 80b2318:	2200      	movs	r2, #0
 80b231a:	4614      	mov	r4, r2
 80b231c:	a90d      	add	r1, sp, #52	; 0x34
 80b231e:	e007      	b.n	80b2330 <__kernel_rem_pio2f+0x1d0>
 80b2320:	f5c3 7080 	rsb	r0, r3, #256	; 0x100
 80b2324:	b10b      	cbz	r3, 80b232a <__kernel_rem_pio2f+0x1ca>
 80b2326:	6008      	str	r0, [r1, #0]
 80b2328:	2401      	movs	r4, #1
 80b232a:	3201      	adds	r2, #1
 80b232c:	4296      	cmp	r6, r2
 80b232e:	dd0b      	ble.n	80b2348 <__kernel_rem_pio2f+0x1e8>
 80b2330:	f851 3f04 	ldr.w	r3, [r1, #4]!
 80b2334:	2c00      	cmp	r4, #0
 80b2336:	d0f3      	beq.n	80b2320 <__kernel_rem_pio2f+0x1c0>
 80b2338:	3201      	adds	r2, #1
 80b233a:	f1c3 03ff 	rsb	r3, r3, #255	; 0xff
 80b233e:	4296      	cmp	r6, r2
 80b2340:	600b      	str	r3, [r1, #0]
 80b2342:	f04f 0401 	mov.w	r4, #1
 80b2346:	dcf3      	bgt.n	80b2330 <__kernel_rem_pio2f+0x1d0>
 80b2348:	9b06      	ldr	r3, [sp, #24]
 80b234a:	2b00      	cmp	r3, #0
 80b234c:	dd0d      	ble.n	80b236a <__kernel_rem_pio2f+0x20a>
 80b234e:	2b01      	cmp	r3, #1
 80b2350:	f000 80de 	beq.w	80b2510 <__kernel_rem_pio2f+0x3b0>
 80b2354:	2b02      	cmp	r3, #2
 80b2356:	d108      	bne.n	80b236a <__kernel_rem_pio2f+0x20a>
 80b2358:	1e72      	subs	r2, r6, #1
 80b235a:	ab0e      	add	r3, sp, #56	; 0x38
 80b235c:	f853 3022 	ldr.w	r3, [r3, r2, lsl #2]
 80b2360:	a90e      	add	r1, sp, #56	; 0x38
 80b2362:	f003 033f 	and.w	r3, r3, #63	; 0x3f
 80b2366:	f841 3022 	str.w	r3, [r1, r2, lsl #2]
 80b236a:	f1ba 0f02 	cmp.w	sl, #2
 80b236e:	d07d      	beq.n	80b246c <__kernel_rem_pio2f+0x30c>
 80b2370:	2100      	movs	r1, #0
 80b2372:	4628      	mov	r0, r5
 80b2374:	f001 face 	bl	80b3914 <__aeabi_fcmpeq>
 80b2378:	2800      	cmp	r0, #0
 80b237a:	f000 8091 	beq.w	80b24a0 <__kernel_rem_pio2f+0x340>
 80b237e:	9b07      	ldr	r3, [sp, #28]
 80b2380:	1e74      	subs	r4, r6, #1
 80b2382:	42a3      	cmp	r3, r4
 80b2384:	dc10      	bgt.n	80b23a8 <__kernel_rem_pio2f+0x248>
 80b2386:	f106 4380 	add.w	r3, r6, #1073741824	; 0x40000000
 80b238a:	aa0e      	add	r2, sp, #56	; 0x38
 80b238c:	3b01      	subs	r3, #1
 80b238e:	980d      	ldr	r0, [sp, #52]	; 0x34
 80b2390:	eb02 0383 	add.w	r3, r2, r3, lsl #2
 80b2394:	2200      	movs	r2, #0
 80b2396:	f853 1904 	ldr.w	r1, [r3], #-4
 80b239a:	4283      	cmp	r3, r0
 80b239c:	ea42 0201 	orr.w	r2, r2, r1
 80b23a0:	d1f9      	bne.n	80b2396 <__kernel_rem_pio2f+0x236>
 80b23a2:	2a00      	cmp	r2, #0
 80b23a4:	f040 80df 	bne.w	80b2566 <__kernel_rem_pio2f+0x406>
 80b23a8:	9b07      	ldr	r3, [sp, #28]
 80b23aa:	aa0e      	add	r2, sp, #56	; 0x38
 80b23ac:	3b01      	subs	r3, #1
 80b23ae:	f852 3023 	ldr.w	r3, [r2, r3, lsl #2]
 80b23b2:	2b00      	cmp	r3, #0
 80b23b4:	f040 81fc 	bne.w	80b27b0 <__kernel_rem_pio2f+0x650>
 80b23b8:	9b0c      	ldr	r3, [sp, #48]	; 0x30
 80b23ba:	2701      	movs	r7, #1
 80b23bc:	f853 2904 	ldr.w	r2, [r3], #-4
 80b23c0:	3701      	adds	r7, #1
 80b23c2:	2a00      	cmp	r2, #0
 80b23c4:	d0fa      	beq.n	80b23bc <__kernel_rem_pio2f+0x25c>
 80b23c6:	19f2      	adds	r2, r6, r7
 80b23c8:	1c73      	adds	r3, r6, #1
 80b23ca:	4293      	cmp	r3, r2
 80b23cc:	9209      	str	r2, [sp, #36]	; 0x24
 80b23ce:	dc40      	bgt.n	80b2452 <__kernel_rem_pio2f+0x2f2>
 80b23d0:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
 80b23d2:	00b9      	lsls	r1, r7, #2
 80b23d4:	eb03 0802 	add.w	r8, r3, r2
 80b23d8:	9a0a      	ldr	r2, [sp, #40]	; 0x28
 80b23da:	f108 4880 	add.w	r8, r8, #1073741824	; 0x40000000
 80b23de:	9103      	str	r1, [sp, #12]
 80b23e0:	9969      	ldr	r1, [sp, #420]	; 0x1a4
 80b23e2:	4416      	add	r6, r2
 80b23e4:	f108 38ff 	add.w	r8, r8, #4294967295	; 0xffffffff
 80b23e8:	00b2      	lsls	r2, r6, #2
 80b23ea:	eb01 0888 	add.w	r8, r1, r8, lsl #2
 80b23ee:	a922      	add	r1, sp, #136	; 0x88
 80b23f0:	009b      	lsls	r3, r3, #2
 80b23f2:	9204      	str	r2, [sp, #16]
 80b23f4:	eb01 0a02 	add.w	sl, r1, r2
 80b23f8:	aa4a      	add	r2, sp, #296	; 0x128
 80b23fa:	eb02 0903 	add.w	r9, r2, r3
 80b23fe:	3b04      	subs	r3, #4
 80b2400:	9305      	str	r3, [sp, #20]
 80b2402:	2600      	movs	r6, #0
 80b2404:	f858 0f04 	ldr.w	r0, [r8, #4]!
 80b2408:	f001 f89c 	bl	80b3544 <__aeabi_i2f>
 80b240c:	9b01      	ldr	r3, [sp, #4]
 80b240e:	f84a 0b04 	str.w	r0, [sl], #4
 80b2412:	2b00      	cmp	r3, #0
 80b2414:	db1f      	blt.n	80b2456 <__kernel_rem_pio2f+0x2f6>
 80b2416:	9b04      	ldr	r3, [sp, #16]
 80b2418:	2700      	movs	r7, #0
 80b241a:	18f4      	adds	r4, r6, r3
 80b241c:	9b05      	ldr	r3, [sp, #20]
 80b241e:	18f5      	adds	r5, r6, r3
 80b2420:	ab22      	add	r3, sp, #136	; 0x88
 80b2422:	441c      	add	r4, r3
 80b2424:	441d      	add	r5, r3
 80b2426:	9b02      	ldr	r3, [sp, #8]
 80b2428:	f1a3 0b04 	sub.w	fp, r3, #4
 80b242c:	f85b 1f04 	ldr.w	r1, [fp, #4]!
 80b2430:	f854 0904 	ldr.w	r0, [r4], #-4
 80b2434:	f001 f8da 	bl	80b35ec <__aeabi_fmul>
 80b2438:	4601      	mov	r1, r0
 80b243a:	4638      	mov	r0, r7
 80b243c:	f000 ffce 	bl	80b33dc <__addsf3>
 80b2440:	42ac      	cmp	r4, r5
 80b2442:	4607      	mov	r7, r0
 80b2444:	d1f2      	bne.n	80b242c <__kernel_rem_pio2f+0x2cc>
 80b2446:	9b03      	ldr	r3, [sp, #12]
 80b2448:	3604      	adds	r6, #4
 80b244a:	429e      	cmp	r6, r3
 80b244c:	f849 7b04 	str.w	r7, [r9], #4
 80b2450:	d1d8      	bne.n	80b2404 <__kernel_rem_pio2f+0x2a4>
 80b2452:	9e09      	ldr	r6, [sp, #36]	; 0x24
 80b2454:	e6f6      	b.n	80b2244 <__kernel_rem_pio2f+0xe4>
 80b2456:	9b03      	ldr	r3, [sp, #12]
 80b2458:	3604      	adds	r6, #4
 80b245a:	2700      	movs	r7, #0
 80b245c:	429e      	cmp	r6, r3
 80b245e:	f849 7b04 	str.w	r7, [r9], #4
 80b2462:	d1cf      	bne.n	80b2404 <__kernel_rem_pio2f+0x2a4>
 80b2464:	e7f5      	b.n	80b2452 <__kernel_rem_pio2f+0x2f2>
 80b2466:	bf00      	nop
 80b2468:	080b7bc0 	.word	0x080b7bc0
 80b246c:	4629      	mov	r1, r5
 80b246e:	f04f 507e 	mov.w	r0, #1065353216	; 0x3f800000
 80b2472:	f000 ffb1 	bl	80b33d8 <__aeabi_fsub>
 80b2476:	4605      	mov	r5, r0
 80b2478:	2c00      	cmp	r4, #0
 80b247a:	f43f af79 	beq.w	80b2370 <__kernel_rem_pio2f+0x210>
 80b247e:	9906      	ldr	r1, [sp, #24]
 80b2480:	f04f 507e 	mov.w	r0, #1065353216	; 0x3f800000
 80b2484:	f000 fa50 	bl	80b2928 <scalbnf>
 80b2488:	4601      	mov	r1, r0
 80b248a:	4628      	mov	r0, r5
 80b248c:	f000 ffa4 	bl	80b33d8 <__aeabi_fsub>
 80b2490:	4605      	mov	r5, r0
 80b2492:	2100      	movs	r1, #0
 80b2494:	4628      	mov	r0, r5
 80b2496:	f001 fa3d 	bl	80b3914 <__aeabi_fcmpeq>
 80b249a:	2800      	cmp	r0, #0
 80b249c:	f47f af6f 	bne.w	80b237e <__kernel_rem_pio2f+0x21e>
 80b24a0:	9702      	str	r7, [sp, #8]
 80b24a2:	9f06      	ldr	r7, [sp, #24]
 80b24a4:	4628      	mov	r0, r5
 80b24a6:	4279      	negs	r1, r7
 80b24a8:	f000 fa3e 	bl	80b2928 <scalbnf>
 80b24ac:	f04f 4187 	mov.w	r1, #1132462080	; 0x43800000
 80b24b0:	4604      	mov	r4, r0
 80b24b2:	f001 fa4d 	bl	80b3950 <__aeabi_fcmpge>
 80b24b6:	2800      	cmp	r0, #0
 80b24b8:	f000 8181 	beq.w	80b27be <__kernel_rem_pio2f+0x65e>
 80b24bc:	f04f 516e 	mov.w	r1, #998244352	; 0x3b800000
 80b24c0:	4620      	mov	r0, r4
 80b24c2:	f001 f893 	bl	80b35ec <__aeabi_fmul>
 80b24c6:	f001 fa6d 	bl	80b39a4 <__aeabi_f2iz>
 80b24ca:	f001 f83b 	bl	80b3544 <__aeabi_i2f>
 80b24ce:	f04f 4187 	mov.w	r1, #1132462080	; 0x43800000
 80b24d2:	4605      	mov	r5, r0
 80b24d4:	f001 f88a 	bl	80b35ec <__aeabi_fmul>
 80b24d8:	4601      	mov	r1, r0
 80b24da:	4620      	mov	r0, r4
 80b24dc:	f000 ff7c 	bl	80b33d8 <__aeabi_fsub>
 80b24e0:	f001 fa60 	bl	80b39a4 <__aeabi_f2iz>
 80b24e4:	ab0e      	add	r3, sp, #56	; 0x38
 80b24e6:	f843 0026 	str.w	r0, [r3, r6, lsl #2]
 80b24ea:	4628      	mov	r0, r5
 80b24ec:	f001 fa5a 	bl	80b39a4 <__aeabi_f2iz>
 80b24f0:	463b      	mov	r3, r7
 80b24f2:	3308      	adds	r3, #8
 80b24f4:	1c74      	adds	r4, r6, #1
 80b24f6:	9306      	str	r3, [sp, #24]
 80b24f8:	ab0e      	add	r3, sp, #56	; 0x38
 80b24fa:	f843 0024 	str.w	r0, [r3, r4, lsl #2]
 80b24fe:	e047      	b.n	80b2590 <__kernel_rem_pio2f+0x430>
 80b2500:	d110      	bne.n	80b2524 <__kernel_rem_pio2f+0x3c4>
 80b2502:	1e73      	subs	r3, r6, #1
 80b2504:	aa0e      	add	r2, sp, #56	; 0x38
 80b2506:	f852 3023 	ldr.w	r3, [r2, r3, lsl #2]
 80b250a:	ea4f 2a23 	mov.w	sl, r3, asr #8
 80b250e:	e6fb      	b.n	80b2308 <__kernel_rem_pio2f+0x1a8>
 80b2510:	1e72      	subs	r2, r6, #1
 80b2512:	ab0e      	add	r3, sp, #56	; 0x38
 80b2514:	f853 3022 	ldr.w	r3, [r3, r2, lsl #2]
 80b2518:	a90e      	add	r1, sp, #56	; 0x38
 80b251a:	f003 037f 	and.w	r3, r3, #127	; 0x7f
 80b251e:	f841 3022 	str.w	r3, [r1, r2, lsl #2]
 80b2522:	e722      	b.n	80b236a <__kernel_rem_pio2f+0x20a>
 80b2524:	f04f 517c 	mov.w	r1, #1056964608	; 0x3f000000
 80b2528:	f001 fa12 	bl	80b3950 <__aeabi_fcmpge>
 80b252c:	b950      	cbnz	r0, 80b2544 <__kernel_rem_pio2f+0x3e4>
 80b252e:	4682      	mov	sl, r0
 80b2530:	e71e      	b.n	80b2370 <__kernel_rem_pio2f+0x210>
 80b2532:	3604      	adds	r6, #4
 80b2534:	f04f 0900 	mov.w	r9, #0
 80b2538:	4546      	cmp	r6, r8
 80b253a:	f847 9f04 	str.w	r9, [r7, #4]!
 80b253e:	f47f ae57 	bne.w	80b21f0 <__kernel_rem_pio2f+0x90>
 80b2542:	e673      	b.n	80b222c <__kernel_rem_pio2f+0xcc>
 80b2544:	2e00      	cmp	r6, #0
 80b2546:	f107 0701 	add.w	r7, r7, #1
 80b254a:	bfc8      	it	gt
 80b254c:	f04f 0a02 	movgt.w	sl, #2
 80b2550:	f73f aee2 	bgt.w	80b2318 <__kernel_rem_pio2f+0x1b8>
 80b2554:	4629      	mov	r1, r5
 80b2556:	f04f 507e 	mov.w	r0, #1065353216	; 0x3f800000
 80b255a:	f000 ff3d 	bl	80b33d8 <__aeabi_fsub>
 80b255e:	f04f 0a02 	mov.w	sl, #2
 80b2562:	4605      	mov	r5, r0
 80b2564:	e704      	b.n	80b2370 <__kernel_rem_pio2f+0x210>
 80b2566:	9a06      	ldr	r2, [sp, #24]
 80b2568:	ab0e      	add	r3, sp, #56	; 0x38
 80b256a:	f853 3024 	ldr.w	r3, [r3, r4, lsl #2]
 80b256e:	3a08      	subs	r2, #8
 80b2570:	9702      	str	r7, [sp, #8]
 80b2572:	9206      	str	r2, [sp, #24]
 80b2574:	b963      	cbnz	r3, 80b2590 <__kernel_rem_pio2f+0x430>
 80b2576:	f104 4380 	add.w	r3, r4, #1073741824	; 0x40000000
 80b257a:	3b01      	subs	r3, #1
 80b257c:	a90e      	add	r1, sp, #56	; 0x38
 80b257e:	eb01 0383 	add.w	r3, r1, r3, lsl #2
 80b2582:	f853 1904 	ldr.w	r1, [r3], #-4
 80b2586:	3c01      	subs	r4, #1
 80b2588:	3a08      	subs	r2, #8
 80b258a:	2900      	cmp	r1, #0
 80b258c:	d0f9      	beq.n	80b2582 <__kernel_rem_pio2f+0x422>
 80b258e:	9206      	str	r2, [sp, #24]
 80b2590:	9906      	ldr	r1, [sp, #24]
 80b2592:	f04f 507e 	mov.w	r0, #1065353216	; 0x3f800000
 80b2596:	f000 f9c7 	bl	80b2928 <scalbnf>
 80b259a:	2c00      	cmp	r4, #0
 80b259c:	4605      	mov	r5, r0
 80b259e:	f2c0 8118 	blt.w	80b27d2 <__kernel_rem_pio2f+0x672>
 80b25a2:	00a3      	lsls	r3, r4, #2
 80b25a4:	aa4a      	add	r2, sp, #296	; 0x128
 80b25a6:	1d1f      	adds	r7, r3, #4
 80b25a8:	eb02 0903 	add.w	r9, r2, r3
 80b25ac:	9301      	str	r3, [sp, #4]
 80b25ae:	ab0e      	add	r3, sp, #56	; 0x38
 80b25b0:	441f      	add	r7, r3
 80b25b2:	f109 0604 	add.w	r6, r9, #4
 80b25b6:	f857 0d04 	ldr.w	r0, [r7, #-4]!
 80b25ba:	f000 ffc3 	bl	80b3544 <__aeabi_i2f>
 80b25be:	4629      	mov	r1, r5
 80b25c0:	f001 f814 	bl	80b35ec <__aeabi_fmul>
 80b25c4:	f04f 516e 	mov.w	r1, #998244352	; 0x3b800000
 80b25c8:	f846 0d04 	str.w	r0, [r6, #-4]!
 80b25cc:	4628      	mov	r0, r5
 80b25ce:	f001 f80d 	bl	80b35ec <__aeabi_fmul>
 80b25d2:	ab0e      	add	r3, sp, #56	; 0x38
 80b25d4:	429f      	cmp	r7, r3
 80b25d6:	4605      	mov	r5, r0
 80b25d8:	d1ed      	bne.n	80b25b6 <__kernel_rem_pio2f+0x456>
 80b25da:	46a3      	mov	fp, r4
 80b25dc:	f8dd 801c 	ldr.w	r8, [sp, #28]
 80b25e0:	f1a9 0904 	sub.w	r9, r9, #4
 80b25e4:	2700      	movs	r7, #0
 80b25e6:	f8cd a00c 	str.w	sl, [sp, #12]
 80b25ea:	f1b8 0f00 	cmp.w	r8, #0
 80b25ee:	bfb8      	it	lt
 80b25f0:	2600      	movlt	r6, #0
 80b25f2:	db15      	blt.n	80b2620 <__kernel_rem_pio2f+0x4c0>
 80b25f4:	4c82      	ldr	r4, [pc, #520]	; (80b2800 <__kernel_rem_pio2f+0x6a0>)
 80b25f6:	46ca      	mov	sl, r9
 80b25f8:	4882      	ldr	r0, [pc, #520]	; (80b2804 <__kernel_rem_pio2f+0x6a4>)
 80b25fa:	2600      	movs	r6, #0
 80b25fc:	2500      	movs	r5, #0
 80b25fe:	e003      	b.n	80b2608 <__kernel_rem_pio2f+0x4a8>
 80b2600:	42af      	cmp	r7, r5
 80b2602:	db0d      	blt.n	80b2620 <__kernel_rem_pio2f+0x4c0>
 80b2604:	f854 0f04 	ldr.w	r0, [r4, #4]!
 80b2608:	f85a 1f04 	ldr.w	r1, [sl, #4]!
 80b260c:	f000 ffee 	bl	80b35ec <__aeabi_fmul>
 80b2610:	4601      	mov	r1, r0
 80b2612:	4630      	mov	r0, r6
 80b2614:	f000 fee2 	bl	80b33dc <__addsf3>
 80b2618:	3501      	adds	r5, #1
 80b261a:	45a8      	cmp	r8, r5
 80b261c:	4606      	mov	r6, r0
 80b261e:	daef      	bge.n	80b2600 <__kernel_rem_pio2f+0x4a0>
 80b2620:	ab5e      	add	r3, sp, #376	; 0x178
 80b2622:	eb03 0387 	add.w	r3, r3, r7, lsl #2
 80b2626:	f1a9 0904 	sub.w	r9, r9, #4
 80b262a:	f843 6ca0 	str.w	r6, [r3, #-160]
 80b262e:	ab48      	add	r3, sp, #288	; 0x120
 80b2630:	4599      	cmp	r9, r3
 80b2632:	f107 0701 	add.w	r7, r7, #1
 80b2636:	d1d8      	bne.n	80b25ea <__kernel_rem_pio2f+0x48a>
 80b2638:	9b68      	ldr	r3, [sp, #416]	; 0x1a0
 80b263a:	465c      	mov	r4, fp
 80b263c:	f8dd a00c 	ldr.w	sl, [sp, #12]
 80b2640:	2b03      	cmp	r3, #3
 80b2642:	d85a      	bhi.n	80b26fa <__kernel_rem_pio2f+0x59a>
 80b2644:	e8df f003 	tbb	[pc, r3]
 80b2648:	025f5f8d 	.word	0x025f5f8d
 80b264c:	2c00      	cmp	r4, #0
 80b264e:	f340 80be 	ble.w	80b27ce <__kernel_rem_pio2f+0x66e>
 80b2652:	9a01      	ldr	r2, [sp, #4]
 80b2654:	a95e      	add	r1, sp, #376	; 0x178
 80b2656:	188b      	adds	r3, r1, r2
 80b2658:	ae36      	add	r6, sp, #216	; 0xd8
 80b265a:	f853 7ca0 	ldr.w	r7, [r3, #-160]
 80b265e:	18b5      	adds	r5, r6, r2
 80b2660:	f855 9c04 	ldr.w	r9, [r5, #-4]
 80b2664:	4639      	mov	r1, r7
 80b2666:	4648      	mov	r0, r9
 80b2668:	f000 feb8 	bl	80b33dc <__addsf3>
 80b266c:	4680      	mov	r8, r0
 80b266e:	4601      	mov	r1, r0
 80b2670:	4648      	mov	r0, r9
 80b2672:	f000 feb1 	bl	80b33d8 <__aeabi_fsub>
 80b2676:	4639      	mov	r1, r7
 80b2678:	f000 feb0 	bl	80b33dc <__addsf3>
 80b267c:	6028      	str	r0, [r5, #0]
 80b267e:	f845 8d04 	str.w	r8, [r5, #-4]!
 80b2682:	42ae      	cmp	r6, r5
 80b2684:	4647      	mov	r7, r8
 80b2686:	d1eb      	bne.n	80b2660 <__kernel_rem_pio2f+0x500>
 80b2688:	2c01      	cmp	r4, #1
 80b268a:	f340 80a0 	ble.w	80b27ce <__kernel_rem_pio2f+0x66e>
 80b268e:	9b01      	ldr	r3, [sp, #4]
 80b2690:	aa5e      	add	r2, sp, #376	; 0x178
 80b2692:	18f4      	adds	r4, r6, r3
 80b2694:	4625      	mov	r5, r4
 80b2696:	eb02 0803 	add.w	r8, r2, r3
 80b269a:	f858 6ca0 	ldr.w	r6, [r8, #-160]
 80b269e:	f10d 09dc 	add.w	r9, sp, #220	; 0xdc
 80b26a2:	f855 8c04 	ldr.w	r8, [r5, #-4]
 80b26a6:	4630      	mov	r0, r6
 80b26a8:	4641      	mov	r1, r8
 80b26aa:	f000 fe97 	bl	80b33dc <__addsf3>
 80b26ae:	4607      	mov	r7, r0
 80b26b0:	4601      	mov	r1, r0
 80b26b2:	4640      	mov	r0, r8
 80b26b4:	f000 fe90 	bl	80b33d8 <__aeabi_fsub>
 80b26b8:	4631      	mov	r1, r6
 80b26ba:	f000 fe8f 	bl	80b33dc <__addsf3>
 80b26be:	6028      	str	r0, [r5, #0]
 80b26c0:	f845 7d04 	str.w	r7, [r5, #-4]!
 80b26c4:	45a9      	cmp	r9, r5
 80b26c6:	463e      	mov	r6, r7
 80b26c8:	d1eb      	bne.n	80b26a2 <__kernel_rem_pio2f+0x542>
 80b26ca:	2000      	movs	r0, #0
 80b26cc:	3404      	adds	r4, #4
 80b26ce:	ad38      	add	r5, sp, #224	; 0xe0
 80b26d0:	f854 1d04 	ldr.w	r1, [r4, #-4]!
 80b26d4:	f000 fe82 	bl	80b33dc <__addsf3>
 80b26d8:	42a5      	cmp	r5, r4
 80b26da:	d1f9      	bne.n	80b26d0 <__kernel_rem_pio2f+0x570>
 80b26dc:	f1ba 0f00 	cmp.w	sl, #0
 80b26e0:	d06a      	beq.n	80b27b8 <__kernel_rem_pio2f+0x658>
 80b26e2:	9a36      	ldr	r2, [sp, #216]	; 0xd8
 80b26e4:	9b37      	ldr	r3, [sp, #220]	; 0xdc
 80b26e6:	f100 4000 	add.w	r0, r0, #2147483648	; 0x80000000
 80b26ea:	f102 4200 	add.w	r2, r2, #2147483648	; 0x80000000
 80b26ee:	f103 4300 	add.w	r3, r3, #2147483648	; 0x80000000
 80b26f2:	9c08      	ldr	r4, [sp, #32]
 80b26f4:	60a0      	str	r0, [r4, #8]
 80b26f6:	6022      	str	r2, [r4, #0]
 80b26f8:	6063      	str	r3, [r4, #4]
 80b26fa:	9b02      	ldr	r3, [sp, #8]
 80b26fc:	f003 0007 	and.w	r0, r3, #7
 80b2700:	b05f      	add	sp, #380	; 0x17c
 80b2702:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80b2706:	9d01      	ldr	r5, [sp, #4]
 80b2708:	ae36      	add	r6, sp, #216	; 0xd8
 80b270a:	3504      	adds	r5, #4
 80b270c:	4435      	add	r5, r6
 80b270e:	2000      	movs	r0, #0
 80b2710:	f855 1d04 	ldr.w	r1, [r5, #-4]!
 80b2714:	f000 fe62 	bl	80b33dc <__addsf3>
 80b2718:	42b5      	cmp	r5, r6
 80b271a:	d1f9      	bne.n	80b2710 <__kernel_rem_pio2f+0x5b0>
 80b271c:	f1ba 0f00 	cmp.w	sl, #0
 80b2720:	d039      	beq.n	80b2796 <__kernel_rem_pio2f+0x636>
 80b2722:	9a08      	ldr	r2, [sp, #32]
 80b2724:	f100 4300 	add.w	r3, r0, #2147483648	; 0x80000000
 80b2728:	4601      	mov	r1, r0
 80b272a:	6013      	str	r3, [r2, #0]
 80b272c:	9836      	ldr	r0, [sp, #216]	; 0xd8
 80b272e:	f000 fe53 	bl	80b33d8 <__aeabi_fsub>
 80b2732:	2c00      	cmp	r4, #0
 80b2734:	dd0b      	ble.n	80b274e <__kernel_rem_pio2f+0x5ee>
 80b2736:	ae36      	add	r6, sp, #216	; 0xd8
 80b2738:	2501      	movs	r5, #1
 80b273a:	3501      	adds	r5, #1
 80b273c:	f856 1f04 	ldr.w	r1, [r6, #4]!
 80b2740:	f000 fe4c 	bl	80b33dc <__addsf3>
 80b2744:	42ac      	cmp	r4, r5
 80b2746:	daf8      	bge.n	80b273a <__kernel_rem_pio2f+0x5da>
 80b2748:	f1ba 0f00 	cmp.w	sl, #0
 80b274c:	d050      	beq.n	80b27f0 <__kernel_rem_pio2f+0x690>
 80b274e:	9a08      	ldr	r2, [sp, #32]
 80b2750:	f100 4300 	add.w	r3, r0, #2147483648	; 0x80000000
 80b2754:	6053      	str	r3, [r2, #4]
 80b2756:	9b02      	ldr	r3, [sp, #8]
 80b2758:	f003 0007 	and.w	r0, r3, #7
 80b275c:	b05f      	add	sp, #380	; 0x17c
 80b275e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80b2762:	9b01      	ldr	r3, [sp, #4]
 80b2764:	aa5e      	add	r2, sp, #376	; 0x178
 80b2766:	4413      	add	r3, r2
 80b2768:	2000      	movs	r0, #0
 80b276a:	f1a3 059c 	sub.w	r5, r3, #156	; 0x9c
 80b276e:	3c01      	subs	r4, #1
 80b2770:	f855 1d04 	ldr.w	r1, [r5, #-4]!
 80b2774:	f000 fe32 	bl	80b33dc <__addsf3>
 80b2778:	1c63      	adds	r3, r4, #1
 80b277a:	d1f8      	bne.n	80b276e <__kernel_rem_pio2f+0x60e>
 80b277c:	f1ba 0f00 	cmp.w	sl, #0
 80b2780:	d001      	beq.n	80b2786 <__kernel_rem_pio2f+0x626>
 80b2782:	f100 4000 	add.w	r0, r0, #2147483648	; 0x80000000
 80b2786:	9b08      	ldr	r3, [sp, #32]
 80b2788:	6018      	str	r0, [r3, #0]
 80b278a:	9b02      	ldr	r3, [sp, #8]
 80b278c:	f003 0007 	and.w	r0, r3, #7
 80b2790:	b05f      	add	sp, #380	; 0x17c
 80b2792:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
 80b2796:	9b08      	ldr	r3, [sp, #32]
 80b2798:	4601      	mov	r1, r0
 80b279a:	6018      	str	r0, [r3, #0]
 80b279c:	9836      	ldr	r0, [sp, #216]	; 0xd8
 80b279e:	f000 fe1b 	bl	80b33d8 <__aeabi_fsub>
 80b27a2:	2c00      	cmp	r4, #0
 80b27a4:	bfd8      	it	le
 80b27a6:	4603      	movle	r3, r0
 80b27a8:	dcc5      	bgt.n	80b2736 <__kernel_rem_pio2f+0x5d6>
 80b27aa:	9a08      	ldr	r2, [sp, #32]
 80b27ac:	6053      	str	r3, [r2, #4]
 80b27ae:	e7d2      	b.n	80b2756 <__kernel_rem_pio2f+0x5f6>
 80b27b0:	2701      	movs	r7, #1
 80b27b2:	e608      	b.n	80b23c6 <__kernel_rem_pio2f+0x266>
 80b27b4:	2400      	movs	r4, #0
 80b27b6:	e5c7      	b.n	80b2348 <__kernel_rem_pio2f+0x1e8>
 80b27b8:	9a36      	ldr	r2, [sp, #216]	; 0xd8
 80b27ba:	9b37      	ldr	r3, [sp, #220]	; 0xdc
 80b27bc:	e799      	b.n	80b26f2 <__kernel_rem_pio2f+0x592>
 80b27be:	4620      	mov	r0, r4
 80b27c0:	f001 f8f0 	bl	80b39a4 <__aeabi_f2iz>
 80b27c4:	ab0e      	add	r3, sp, #56	; 0x38
 80b27c6:	4634      	mov	r4, r6
 80b27c8:	f843 0026 	str.w	r0, [r3, r6, lsl #2]
 80b27cc:	e6e0      	b.n	80b2590 <__kernel_rem_pio2f+0x430>
 80b27ce:	2000      	movs	r0, #0
 80b27d0:	e784      	b.n	80b26dc <__kernel_rem_pio2f+0x57c>
 80b27d2:	9b68      	ldr	r3, [sp, #416]	; 0x1a0
 80b27d4:	2b03      	cmp	r3, #3
 80b27d6:	d890      	bhi.n	80b26fa <__kernel_rem_pio2f+0x59a>
 80b27d8:	a201      	add	r2, pc, #4	; (adr r2, 80b27e0 <__kernel_rem_pio2f+0x680>)
 80b27da:	f852 f023 	ldr.w	pc, [r2, r3, lsl #2]
 80b27de:	bf00      	nop
 80b27e0:	080b27fd 	.word	0x080b27fd
 80b27e4:	080b27f9 	.word	0x080b27f9
 80b27e8:	080b27f9 	.word	0x080b27f9
 80b27ec:	080b27cf 	.word	0x080b27cf
 80b27f0:	4603      	mov	r3, r0
 80b27f2:	9a08      	ldr	r2, [sp, #32]
 80b27f4:	6053      	str	r3, [r2, #4]
 80b27f6:	e7ae      	b.n	80b2756 <__kernel_rem_pio2f+0x5f6>
 80b27f8:	2000      	movs	r0, #0
 80b27fa:	e78f      	b.n	80b271c <__kernel_rem_pio2f+0x5bc>
 80b27fc:	2000      	movs	r0, #0
 80b27fe:	e7bd      	b.n	80b277c <__kernel_rem_pio2f+0x61c>
 80b2800:	080b7bcc 	.word	0x080b7bcc
 80b2804:	3fc90000 	.word	0x3fc90000

080b2808 <__kernel_sinf>:
 80b2808:	e92d 43f8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, lr}
 80b280c:	f020 4300 	bic.w	r3, r0, #2147483648	; 0x80000000
 80b2810:	f1b3 5f48 	cmp.w	r3, #838860800	; 0x32000000
 80b2814:	4604      	mov	r4, r0
 80b2816:	460f      	mov	r7, r1
 80b2818:	4690      	mov	r8, r2
 80b281a:	da03      	bge.n	80b2824 <__kernel_sinf+0x1c>
 80b281c:	f001 f8c2 	bl	80b39a4 <__aeabi_f2iz>
 80b2820:	2800      	cmp	r0, #0
 80b2822:	d058      	beq.n	80b28d6 <__kernel_sinf+0xce>
 80b2824:	4621      	mov	r1, r4
 80b2826:	4620      	mov	r0, r4
 80b2828:	f000 fee0 	bl	80b35ec <__aeabi_fmul>
 80b282c:	4605      	mov	r5, r0
 80b282e:	4601      	mov	r1, r0
 80b2830:	4620      	mov	r0, r4
 80b2832:	f000 fedb 	bl	80b35ec <__aeabi_fmul>
 80b2836:	4929      	ldr	r1, [pc, #164]	; (80b28dc <__kernel_sinf+0xd4>)
 80b2838:	4606      	mov	r6, r0
 80b283a:	4628      	mov	r0, r5
 80b283c:	f000 fed6 	bl	80b35ec <__aeabi_fmul>
 80b2840:	4927      	ldr	r1, [pc, #156]	; (80b28e0 <__kernel_sinf+0xd8>)
 80b2842:	f000 fdc9 	bl	80b33d8 <__aeabi_fsub>
 80b2846:	4629      	mov	r1, r5
 80b2848:	f000 fed0 	bl	80b35ec <__aeabi_fmul>
 80b284c:	4925      	ldr	r1, [pc, #148]	; (80b28e4 <__kernel_sinf+0xdc>)
 80b284e:	f000 fdc5 	bl	80b33dc <__addsf3>
 80b2852:	4629      	mov	r1, r5
 80b2854:	f000 feca 	bl	80b35ec <__aeabi_fmul>
 80b2858:	4923      	ldr	r1, [pc, #140]	; (80b28e8 <__kernel_sinf+0xe0>)
 80b285a:	f000 fdbd 	bl	80b33d8 <__aeabi_fsub>
 80b285e:	4629      	mov	r1, r5
 80b2860:	f000 fec4 	bl	80b35ec <__aeabi_fmul>
 80b2864:	4921      	ldr	r1, [pc, #132]	; (80b28ec <__kernel_sinf+0xe4>)
 80b2866:	f000 fdb9 	bl	80b33dc <__addsf3>
 80b286a:	4681      	mov	r9, r0
 80b286c:	f1b8 0f00 	cmp.w	r8, #0
 80b2870:	d022      	beq.n	80b28b8 <__kernel_sinf+0xb0>
 80b2872:	f04f 517c 	mov.w	r1, #1056964608	; 0x3f000000
 80b2876:	4638      	mov	r0, r7
 80b2878:	f000 feb8 	bl	80b35ec <__aeabi_fmul>
 80b287c:	4649      	mov	r1, r9
 80b287e:	4680      	mov	r8, r0
 80b2880:	4630      	mov	r0, r6
 80b2882:	f000 feb3 	bl	80b35ec <__aeabi_fmul>
 80b2886:	4601      	mov	r1, r0
 80b2888:	4640      	mov	r0, r8
 80b288a:	f000 fda5 	bl	80b33d8 <__aeabi_fsub>
 80b288e:	4629      	mov	r1, r5
 80b2890:	f000 feac 	bl	80b35ec <__aeabi_fmul>
 80b2894:	4639      	mov	r1, r7
 80b2896:	f000 fd9f 	bl	80b33d8 <__aeabi_fsub>
 80b289a:	4915      	ldr	r1, [pc, #84]	; (80b28f0 <__kernel_sinf+0xe8>)
 80b289c:	4605      	mov	r5, r0
 80b289e:	4630      	mov	r0, r6
 80b28a0:	f000 fea4 	bl	80b35ec <__aeabi_fmul>
 80b28a4:	4601      	mov	r1, r0
 80b28a6:	4628      	mov	r0, r5
 80b28a8:	f000 fd98 	bl	80b33dc <__addsf3>
 80b28ac:	4601      	mov	r1, r0
 80b28ae:	4620      	mov	r0, r4
 80b28b0:	f000 fd92 	bl	80b33d8 <__aeabi_fsub>
 80b28b4:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
 80b28b8:	4601      	mov	r1, r0
 80b28ba:	4628      	mov	r0, r5
 80b28bc:	f000 fe96 	bl	80b35ec <__aeabi_fmul>
 80b28c0:	490b      	ldr	r1, [pc, #44]	; (80b28f0 <__kernel_sinf+0xe8>)
 80b28c2:	f000 fd89 	bl	80b33d8 <__aeabi_fsub>
 80b28c6:	4631      	mov	r1, r6
 80b28c8:	f000 fe90 	bl	80b35ec <__aeabi_fmul>
 80b28cc:	4621      	mov	r1, r4
 80b28ce:	f000 fd85 	bl	80b33dc <__addsf3>
 80b28d2:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
 80b28d6:	4620      	mov	r0, r4
 80b28d8:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
 80b28dc:	2f2ec9d3 	.word	0x2f2ec9d3
 80b28e0:	32d72f34 	.word	0x32d72f34
 80b28e4:	3638ef1b 	.word	0x3638ef1b
 80b28e8:	39500d01 	.word	0x39500d01
 80b28ec:	3c088889 	.word	0x3c088889
 80b28f0:	3e2aaaab 	.word	0x3e2aaaab

080b28f4 <finite>:
 80b28f4:	f041 4100 	orr.w	r1, r1, #2147483648	; 0x80000000
 80b28f8:	f501 1080 	add.w	r0, r1, #1048576	; 0x100000
 80b28fc:	0fc0      	lsrs	r0, r0, #31
 80b28fe:	4770      	bx	lr

080b2900 <matherr>:
 80b2900:	2000      	movs	r0, #0
 80b2902:	4770      	bx	lr

080b2904 <nan>:
 80b2904:	2000      	movs	r0, #0
 80b2906:	4901      	ldr	r1, [pc, #4]	; (80b290c <nan+0x8>)
 80b2908:	4770      	bx	lr
 80b290a:	bf00      	nop
 80b290c:	7ff80000 	.word	0x7ff80000

080b2910 <fabsf>:
 80b2910:	f020 4000 	bic.w	r0, r0, #2147483648	; 0x80000000
 80b2914:	4770      	bx	lr
 80b2916:	bf00      	nop

080b2918 <finitef>:
 80b2918:	f020 4000 	bic.w	r0, r0, #2147483648	; 0x80000000
 80b291c:	f1b0 4fff 	cmp.w	r0, #2139095040	; 0x7f800000
 80b2920:	bfac      	ite	ge
 80b2922:	2000      	movge	r0, #0
 80b2924:	2001      	movlt	r0, #1
 80b2926:	4770      	bx	lr

080b2928 <scalbnf>:
 80b2928:	f030 4200 	bics.w	r2, r0, #2147483648	; 0x80000000
 80b292c:	b538      	push	{r3, r4, r5, lr}
 80b292e:	4603      	mov	r3, r0
 80b2930:	d016      	beq.n	80b2960 <scalbnf+0x38>
 80b2932:	f1b2 4fff 	cmp.w	r2, #2139095040	; 0x7f800000
 80b2936:	d20f      	bcs.n	80b2958 <scalbnf+0x30>
 80b2938:	f5b2 0f00 	cmp.w	r2, #8388608	; 0x800000
 80b293c:	460d      	mov	r5, r1
 80b293e:	d310      	bcc.n	80b2962 <scalbnf+0x3a>
 80b2940:	4604      	mov	r4, r0
 80b2942:	0dd2      	lsrs	r2, r2, #23
 80b2944:	442a      	add	r2, r5
 80b2946:	2afe      	cmp	r2, #254	; 0xfe
 80b2948:	dc2e      	bgt.n	80b29a8 <scalbnf+0x80>
 80b294a:	2a00      	cmp	r2, #0
 80b294c:	dd1d      	ble.n	80b298a <scalbnf+0x62>
 80b294e:	f024 44ff 	bic.w	r4, r4, #2139095040	; 0x7f800000
 80b2952:	ea44 50c2 	orr.w	r0, r4, r2, lsl #23
 80b2956:	bd38      	pop	{r3, r4, r5, pc}
 80b2958:	4601      	mov	r1, r0
 80b295a:	f000 fd3f 	bl	80b33dc <__addsf3>
 80b295e:	bd38      	pop	{r3, r4, r5, pc}
 80b2960:	bd38      	pop	{r3, r4, r5, pc}
 80b2962:	f04f 4198 	mov.w	r1, #1275068416	; 0x4c000000
 80b2966:	f000 fe41 	bl	80b35ec <__aeabi_fmul>
 80b296a:	4a18      	ldr	r2, [pc, #96]	; (80b29cc <scalbnf+0xa4>)
 80b296c:	4603      	mov	r3, r0
 80b296e:	4295      	cmp	r5, r2
 80b2970:	db07      	blt.n	80b2982 <scalbnf+0x5a>
 80b2972:	f3c0 52c7 	ubfx	r2, r0, #23, #8
 80b2976:	4604      	mov	r4, r0
 80b2978:	3a19      	subs	r2, #25
 80b297a:	e7e3      	b.n	80b2944 <scalbnf+0x1c>
 80b297c:	4814      	ldr	r0, [pc, #80]	; (80b29d0 <scalbnf+0xa8>)
 80b297e:	f000 f82b 	bl	80b29d8 <copysignf>
 80b2982:	4913      	ldr	r1, [pc, #76]	; (80b29d0 <scalbnf+0xa8>)
 80b2984:	f000 fe32 	bl	80b35ec <__aeabi_fmul>
 80b2988:	bd38      	pop	{r3, r4, r5, pc}
 80b298a:	f112 0f16 	cmn.w	r2, #22
 80b298e:	da13      	bge.n	80b29b8 <scalbnf+0x90>
 80b2990:	f24c 3250 	movw	r2, #50000	; 0xc350
 80b2994:	4295      	cmp	r5, r2
 80b2996:	4619      	mov	r1, r3
 80b2998:	ddf0      	ble.n	80b297c <scalbnf+0x54>
 80b299a:	480e      	ldr	r0, [pc, #56]	; (80b29d4 <scalbnf+0xac>)
 80b299c:	f000 f81c 	bl	80b29d8 <copysignf>
 80b29a0:	490c      	ldr	r1, [pc, #48]	; (80b29d4 <scalbnf+0xac>)
 80b29a2:	f000 fe23 	bl	80b35ec <__aeabi_fmul>
 80b29a6:	bd38      	pop	{r3, r4, r5, pc}
 80b29a8:	4619      	mov	r1, r3
 80b29aa:	480a      	ldr	r0, [pc, #40]	; (80b29d4 <scalbnf+0xac>)
 80b29ac:	f000 f814 	bl	80b29d8 <copysignf>
 80b29b0:	4908      	ldr	r1, [pc, #32]	; (80b29d4 <scalbnf+0xac>)
 80b29b2:	f000 fe1b 	bl	80b35ec <__aeabi_fmul>
 80b29b6:	bd38      	pop	{r3, r4, r5, pc}
 80b29b8:	3219      	adds	r2, #25
 80b29ba:	f024 40ff 	bic.w	r0, r4, #2139095040	; 0x7f800000
 80b29be:	ea40 50c2 	orr.w	r0, r0, r2, lsl #23
 80b29c2:	f04f 514c 	mov.w	r1, #855638016	; 0x33000000
 80b29c6:	f000 fe11 	bl	80b35ec <__aeabi_fmul>
 80b29ca:	bd38      	pop	{r3, r4, r5, pc}
 80b29cc:	ffff3cb0 	.word	0xffff3cb0
 80b29d0:	0da24260 	.word	0x0da24260
 80b29d4:	7149f2ca 	.word	0x7149f2ca

080b29d8 <copysignf>:
 80b29d8:	f001 4100 	and.w	r1, r1, #2147483648	; 0x80000000
 80b29dc:	f020 4000 	bic.w	r0, r0, #2147483648	; 0x80000000
 80b29e0:	4308      	orrs	r0, r1
 80b29e2:	4770      	bx	lr

080b29e4 <__aeabi_llsl>:
 80b29e4:	4091      	lsls	r1, r2
 80b29e6:	1c03      	adds	r3, r0, #0
 80b29e8:	4090      	lsls	r0, r2
 80b29ea:	469c      	mov	ip, r3
 80b29ec:	3a20      	subs	r2, #32
 80b29ee:	4093      	lsls	r3, r2
 80b29f0:	4319      	orrs	r1, r3
 80b29f2:	4252      	negs	r2, r2
 80b29f4:	4663      	mov	r3, ip
 80b29f6:	40d3      	lsrs	r3, r2
 80b29f8:	4319      	orrs	r1, r3
 80b29fa:	4770      	bx	lr

080b29fc <__aeabi_drsub>:
 80b29fc:	f081 4100 	eor.w	r1, r1, #2147483648	; 0x80000000
 80b2a00:	e002      	b.n	80b2a08 <__adddf3>
 80b2a02:	bf00      	nop

080b2a04 <__aeabi_dsub>:
 80b2a04:	f083 4300 	eor.w	r3, r3, #2147483648	; 0x80000000

080b2a08 <__adddf3>:
 80b2a08:	b530      	push	{r4, r5, lr}
 80b2a0a:	ea4f 0441 	mov.w	r4, r1, lsl #1
 80b2a0e:	ea4f 0543 	mov.w	r5, r3, lsl #1
 80b2a12:	ea94 0f05 	teq	r4, r5
 80b2a16:	bf08      	it	eq
 80b2a18:	ea90 0f02 	teqeq	r0, r2
 80b2a1c:	bf1f      	itttt	ne
 80b2a1e:	ea54 0c00 	orrsne.w	ip, r4, r0
 80b2a22:	ea55 0c02 	orrsne.w	ip, r5, r2
 80b2a26:	ea7f 5c64 	mvnsne.w	ip, r4, asr #21
 80b2a2a:	ea7f 5c65 	mvnsne.w	ip, r5, asr #21
 80b2a2e:	f000 80e2 	beq.w	80b2bf6 <__adddf3+0x1ee>
 80b2a32:	ea4f 5454 	mov.w	r4, r4, lsr #21
 80b2a36:	ebd4 5555 	rsbs	r5, r4, r5, lsr #21
 80b2a3a:	bfb8      	it	lt
 80b2a3c:	426d      	neglt	r5, r5
 80b2a3e:	dd0c      	ble.n	80b2a5a <__adddf3+0x52>
 80b2a40:	442c      	add	r4, r5
 80b2a42:	ea80 0202 	eor.w	r2, r0, r2
 80b2a46:	ea81 0303 	eor.w	r3, r1, r3
 80b2a4a:	ea82 0000 	eor.w	r0, r2, r0
 80b2a4e:	ea83 0101 	eor.w	r1, r3, r1
 80b2a52:	ea80 0202 	eor.w	r2, r0, r2
 80b2a56:	ea81 0303 	eor.w	r3, r1, r3
 80b2a5a:	2d36      	cmp	r5, #54	; 0x36
 80b2a5c:	bf88      	it	hi
 80b2a5e:	bd30      	pophi	{r4, r5, pc}
 80b2a60:	f011 4f00 	tst.w	r1, #2147483648	; 0x80000000
 80b2a64:	ea4f 3101 	mov.w	r1, r1, lsl #12
 80b2a68:	f44f 1c80 	mov.w	ip, #1048576	; 0x100000
 80b2a6c:	ea4c 3111 	orr.w	r1, ip, r1, lsr #12
 80b2a70:	d002      	beq.n	80b2a78 <__adddf3+0x70>
 80b2a72:	4240      	negs	r0, r0
 80b2a74:	eb61 0141 	sbc.w	r1, r1, r1, lsl #1
 80b2a78:	f013 4f00 	tst.w	r3, #2147483648	; 0x80000000
 80b2a7c:	ea4f 3303 	mov.w	r3, r3, lsl #12
 80b2a80:	ea4c 3313 	orr.w	r3, ip, r3, lsr #12
 80b2a84:	d002      	beq.n	80b2a8c <__adddf3+0x84>
 80b2a86:	4252      	negs	r2, r2
 80b2a88:	eb63 0343 	sbc.w	r3, r3, r3, lsl #1
 80b2a8c:	ea94 0f05 	teq	r4, r5
 80b2a90:	f000 80a7 	beq.w	80b2be2 <__adddf3+0x1da>
 80b2a94:	f1a4 0401 	sub.w	r4, r4, #1
 80b2a98:	f1d5 0e20 	rsbs	lr, r5, #32
 80b2a9c:	db0d      	blt.n	80b2aba <__adddf3+0xb2>
 80b2a9e:	fa02 fc0e 	lsl.w	ip, r2, lr
 80b2aa2:	fa22 f205 	lsr.w	r2, r2, r5
 80b2aa6:	1880      	adds	r0, r0, r2
 80b2aa8:	f141 0100 	adc.w	r1, r1, #0
 80b2aac:	fa03 f20e 	lsl.w	r2, r3, lr
 80b2ab0:	1880      	adds	r0, r0, r2
 80b2ab2:	fa43 f305 	asr.w	r3, r3, r5
 80b2ab6:	4159      	adcs	r1, r3
 80b2ab8:	e00e      	b.n	80b2ad8 <__adddf3+0xd0>
 80b2aba:	f1a5 0520 	sub.w	r5, r5, #32
 80b2abe:	f10e 0e20 	add.w	lr, lr, #32
 80b2ac2:	2a01      	cmp	r2, #1
 80b2ac4:	fa03 fc0e 	lsl.w	ip, r3, lr
 80b2ac8:	bf28      	it	cs
 80b2aca:	f04c 0c02 	orrcs.w	ip, ip, #2
 80b2ace:	fa43 f305 	asr.w	r3, r3, r5
 80b2ad2:	18c0      	adds	r0, r0, r3
 80b2ad4:	eb51 71e3 	adcs.w	r1, r1, r3, asr #31
 80b2ad8:	f001 4500 	and.w	r5, r1, #2147483648	; 0x80000000
 80b2adc:	d507      	bpl.n	80b2aee <__adddf3+0xe6>
 80b2ade:	f04f 0e00 	mov.w	lr, #0
 80b2ae2:	f1dc 0c00 	rsbs	ip, ip, #0
 80b2ae6:	eb7e 0000 	sbcs.w	r0, lr, r0
 80b2aea:	eb6e 0101 	sbc.w	r1, lr, r1
 80b2aee:	f5b1 1f80 	cmp.w	r1, #1048576	; 0x100000
 80b2af2:	d31b      	bcc.n	80b2b2c <__adddf3+0x124>
 80b2af4:	f5b1 1f00 	cmp.w	r1, #2097152	; 0x200000
 80b2af8:	d30c      	bcc.n	80b2b14 <__adddf3+0x10c>
 80b2afa:	0849      	lsrs	r1, r1, #1
 80b2afc:	ea5f 0030 	movs.w	r0, r0, rrx
 80b2b00:	ea4f 0c3c 	mov.w	ip, ip, rrx
 80b2b04:	f104 0401 	add.w	r4, r4, #1
 80b2b08:	ea4f 5244 	mov.w	r2, r4, lsl #21
 80b2b0c:	f512 0f80 	cmn.w	r2, #4194304	; 0x400000
 80b2b10:	f080 809a 	bcs.w	80b2c48 <__adddf3+0x240>
 80b2b14:	f1bc 4f00 	cmp.w	ip, #2147483648	; 0x80000000
 80b2b18:	bf08      	it	eq
 80b2b1a:	ea5f 0c50 	movseq.w	ip, r0, lsr #1
 80b2b1e:	f150 0000 	adcs.w	r0, r0, #0
 80b2b22:	eb41 5104 	adc.w	r1, r1, r4, lsl #20
 80b2b26:	ea41 0105 	orr.w	r1, r1, r5
 80b2b2a:	bd30      	pop	{r4, r5, pc}
 80b2b2c:	ea5f 0c4c 	movs.w	ip, ip, lsl #1
 80b2b30:	4140      	adcs	r0, r0
 80b2b32:	eb41 0101 	adc.w	r1, r1, r1
 80b2b36:	f411 1f80 	tst.w	r1, #1048576	; 0x100000
 80b2b3a:	f1a4 0401 	sub.w	r4, r4, #1
 80b2b3e:	d1e9      	bne.n	80b2b14 <__adddf3+0x10c>
 80b2b40:	f091 0f00 	teq	r1, #0
 80b2b44:	bf04      	itt	eq
 80b2b46:	4601      	moveq	r1, r0
 80b2b48:	2000      	moveq	r0, #0
 80b2b4a:	fab1 f381 	clz	r3, r1
 80b2b4e:	bf08      	it	eq
 80b2b50:	3320      	addeq	r3, #32
 80b2b52:	f1a3 030b 	sub.w	r3, r3, #11
 80b2b56:	f1b3 0220 	subs.w	r2, r3, #32
 80b2b5a:	da0c      	bge.n	80b2b76 <__adddf3+0x16e>
 80b2b5c:	320c      	adds	r2, #12
 80b2b5e:	dd08      	ble.n	80b2b72 <__adddf3+0x16a>
 80b2b60:	f102 0c14 	add.w	ip, r2, #20
 80b2b64:	f1c2 020c 	rsb	r2, r2, #12
 80b2b68:	fa01 f00c 	lsl.w	r0, r1, ip
 80b2b6c:	fa21 f102 	lsr.w	r1, r1, r2
 80b2b70:	e00c      	b.n	80b2b8c <__adddf3+0x184>
 80b2b72:	f102 0214 	add.w	r2, r2, #20
 80b2b76:	bfd8      	it	le
 80b2b78:	f1c2 0c20 	rsble	ip, r2, #32
 80b2b7c:	fa01 f102 	lsl.w	r1, r1, r2
 80b2b80:	fa20 fc0c 	lsr.w	ip, r0, ip
 80b2b84:	bfdc      	itt	le
 80b2b86:	ea41 010c 	orrle.w	r1, r1, ip
 80b2b8a:	4090      	lslle	r0, r2
 80b2b8c:	1ae4      	subs	r4, r4, r3
 80b2b8e:	bfa2      	ittt	ge
 80b2b90:	eb01 5104 	addge.w	r1, r1, r4, lsl #20
 80b2b94:	4329      	orrge	r1, r5
 80b2b96:	bd30      	popge	{r4, r5, pc}
 80b2b98:	ea6f 0404 	mvn.w	r4, r4
 80b2b9c:	3c1f      	subs	r4, #31
 80b2b9e:	da1c      	bge.n	80b2bda <__adddf3+0x1d2>
 80b2ba0:	340c      	adds	r4, #12
 80b2ba2:	dc0e      	bgt.n	80b2bc2 <__adddf3+0x1ba>
 80b2ba4:	f104 0414 	add.w	r4, r4, #20
 80b2ba8:	f1c4 0220 	rsb	r2, r4, #32
 80b2bac:	fa20 f004 	lsr.w	r0, r0, r4
 80b2bb0:	fa01 f302 	lsl.w	r3, r1, r2
 80b2bb4:	ea40 0003 	orr.w	r0, r0, r3
 80b2bb8:	fa21 f304 	lsr.w	r3, r1, r4
 80b2bbc:	ea45 0103 	orr.w	r1, r5, r3
 80b2bc0:	bd30      	pop	{r4, r5, pc}
 80b2bc2:	f1c4 040c 	rsb	r4, r4, #12
 80b2bc6:	f1c4 0220 	rsb	r2, r4, #32
 80b2bca:	fa20 f002 	lsr.w	r0, r0, r2
 80b2bce:	fa01 f304 	lsl.w	r3, r1, r4
 80b2bd2:	ea40 0003 	orr.w	r0, r0, r3
 80b2bd6:	4629      	mov	r1, r5
 80b2bd8:	bd30      	pop	{r4, r5, pc}
 80b2bda:	fa21 f004 	lsr.w	r0, r1, r4
 80b2bde:	4629      	mov	r1, r5
 80b2be0:	bd30      	pop	{r4, r5, pc}
 80b2be2:	f094 0f00 	teq	r4, #0
 80b2be6:	f483 1380 	eor.w	r3, r3, #1048576	; 0x100000
 80b2bea:	bf06      	itte	eq
 80b2bec:	f481 1180 	eoreq.w	r1, r1, #1048576	; 0x100000
 80b2bf0:	3401      	addeq	r4, #1
 80b2bf2:	3d01      	subne	r5, #1
 80b2bf4:	e74e      	b.n	80b2a94 <__adddf3+0x8c>
 80b2bf6:	ea7f 5c64 	mvns.w	ip, r4, asr #21
 80b2bfa:	bf18      	it	ne
 80b2bfc:	ea7f 5c65 	mvnsne.w	ip, r5, asr #21
 80b2c00:	d029      	beq.n	80b2c56 <__adddf3+0x24e>
 80b2c02:	ea94 0f05 	teq	r4, r5
 80b2c06:	bf08      	it	eq
 80b2c08:	ea90 0f02 	teqeq	r0, r2
 80b2c0c:	d005      	beq.n	80b2c1a <__adddf3+0x212>
 80b2c0e:	ea54 0c00 	orrs.w	ip, r4, r0
 80b2c12:	bf04      	itt	eq
 80b2c14:	4619      	moveq	r1, r3
 80b2c16:	4610      	moveq	r0, r2
 80b2c18:	bd30      	pop	{r4, r5, pc}
 80b2c1a:	ea91 0f03 	teq	r1, r3
 80b2c1e:	bf1e      	ittt	ne
 80b2c20:	2100      	movne	r1, #0
 80b2c22:	2000      	movne	r0, #0
 80b2c24:	bd30      	popne	{r4, r5, pc}
 80b2c26:	ea5f 5c54 	movs.w	ip, r4, lsr #21
 80b2c2a:	d105      	bne.n	80b2c38 <__adddf3+0x230>
 80b2c2c:	0040      	lsls	r0, r0, #1
 80b2c2e:	4149      	adcs	r1, r1
 80b2c30:	bf28      	it	cs
 80b2c32:	f041 4100 	orrcs.w	r1, r1, #2147483648	; 0x80000000
 80b2c36:	bd30      	pop	{r4, r5, pc}
 80b2c38:	f514 0480 	adds.w	r4, r4, #4194304	; 0x400000
 80b2c3c:	bf3c      	itt	cc
 80b2c3e:	f501 1180 	addcc.w	r1, r1, #1048576	; 0x100000
 80b2c42:	bd30      	popcc	{r4, r5, pc}
 80b2c44:	f001 4500 	and.w	r5, r1, #2147483648	; 0x80000000
 80b2c48:	f045 41fe 	orr.w	r1, r5, #2130706432	; 0x7f000000
 80b2c4c:	f441 0170 	orr.w	r1, r1, #15728640	; 0xf00000
 80b2c50:	f04f 0000 	mov.w	r0, #0
 80b2c54:	bd30      	pop	{r4, r5, pc}
 80b2c56:	ea7f 5c64 	mvns.w	ip, r4, asr #21
 80b2c5a:	bf1a      	itte	ne
 80b2c5c:	4619      	movne	r1, r3
 80b2c5e:	4610      	movne	r0, r2
 80b2c60:	ea7f 5c65 	mvnseq.w	ip, r5, asr #21
 80b2c64:	bf1c      	itt	ne
 80b2c66:	460b      	movne	r3, r1
 80b2c68:	4602      	movne	r2, r0
 80b2c6a:	ea50 3401 	orrs.w	r4, r0, r1, lsl #12
 80b2c6e:	bf06      	itte	eq
 80b2c70:	ea52 3503 	orrseq.w	r5, r2, r3, lsl #12
 80b2c74:	ea91 0f03 	teqeq	r1, r3
 80b2c78:	f441 2100 	orrne.w	r1, r1, #524288	; 0x80000
 80b2c7c:	bd30      	pop	{r4, r5, pc}
 80b2c7e:	bf00      	nop

080b2c80 <__aeabi_ui2d>:
 80b2c80:	f090 0f00 	teq	r0, #0
 80b2c84:	bf04      	itt	eq
 80b2c86:	2100      	moveq	r1, #0
 80b2c88:	4770      	bxeq	lr
 80b2c8a:	b530      	push	{r4, r5, lr}
 80b2c8c:	f44f 6480 	mov.w	r4, #1024	; 0x400
 80b2c90:	f104 0432 	add.w	r4, r4, #50	; 0x32
 80b2c94:	f04f 0500 	mov.w	r5, #0
 80b2c98:	f04f 0100 	mov.w	r1, #0
 80b2c9c:	e750      	b.n	80b2b40 <__adddf3+0x138>
 80b2c9e:	bf00      	nop

080b2ca0 <__aeabi_i2d>:
 80b2ca0:	f090 0f00 	teq	r0, #0
 80b2ca4:	bf04      	itt	eq
 80b2ca6:	2100      	moveq	r1, #0
 80b2ca8:	4770      	bxeq	lr
 80b2caa:	b530      	push	{r4, r5, lr}
 80b2cac:	f44f 6480 	mov.w	r4, #1024	; 0x400
 80b2cb0:	f104 0432 	add.w	r4, r4, #50	; 0x32
 80b2cb4:	f010 4500 	ands.w	r5, r0, #2147483648	; 0x80000000
 80b2cb8:	bf48      	it	mi
 80b2cba:	4240      	negmi	r0, r0
 80b2cbc:	f04f 0100 	mov.w	r1, #0
 80b2cc0:	e73e      	b.n	80b2b40 <__adddf3+0x138>
 80b2cc2:	bf00      	nop

080b2cc4 <__aeabi_f2d>:
 80b2cc4:	0042      	lsls	r2, r0, #1
 80b2cc6:	ea4f 01e2 	mov.w	r1, r2, asr #3
 80b2cca:	ea4f 0131 	mov.w	r1, r1, rrx
 80b2cce:	ea4f 7002 	mov.w	r0, r2, lsl #28
 80b2cd2:	bf1f      	itttt	ne
 80b2cd4:	f012 437f 	andsne.w	r3, r2, #4278190080	; 0xff000000
 80b2cd8:	f093 4f7f 	teqne	r3, #4278190080	; 0xff000000
 80b2cdc:	f081 5160 	eorne.w	r1, r1, #939524096	; 0x38000000
 80b2ce0:	4770      	bxne	lr
 80b2ce2:	f092 0f00 	teq	r2, #0
 80b2ce6:	bf14      	ite	ne
 80b2ce8:	f093 4f7f 	teqne	r3, #4278190080	; 0xff000000
 80b2cec:	4770      	bxeq	lr
 80b2cee:	b530      	push	{r4, r5, lr}
 80b2cf0:	f44f 7460 	mov.w	r4, #896	; 0x380
 80b2cf4:	f001 4500 	and.w	r5, r1, #2147483648	; 0x80000000
 80b2cf8:	f021 4100 	bic.w	r1, r1, #2147483648	; 0x80000000
 80b2cfc:	e720      	b.n	80b2b40 <__adddf3+0x138>
 80b2cfe:	bf00      	nop

080b2d00 <__aeabi_ul2d>:
 80b2d00:	ea50 0201 	orrs.w	r2, r0, r1
 80b2d04:	bf08      	it	eq
 80b2d06:	4770      	bxeq	lr
 80b2d08:	b530      	push	{r4, r5, lr}
 80b2d0a:	f04f 0500 	mov.w	r5, #0
 80b2d0e:	e00a      	b.n	80b2d26 <__aeabi_l2d+0x16>

080b2d10 <__aeabi_l2d>:
 80b2d10:	ea50 0201 	orrs.w	r2, r0, r1
 80b2d14:	bf08      	it	eq
 80b2d16:	4770      	bxeq	lr
 80b2d18:	b530      	push	{r4, r5, lr}
 80b2d1a:	f011 4500 	ands.w	r5, r1, #2147483648	; 0x80000000
 80b2d1e:	d502      	bpl.n	80b2d26 <__aeabi_l2d+0x16>
 80b2d20:	4240      	negs	r0, r0
 80b2d22:	eb61 0141 	sbc.w	r1, r1, r1, lsl #1
 80b2d26:	f44f 6480 	mov.w	r4, #1024	; 0x400
 80b2d2a:	f104 0432 	add.w	r4, r4, #50	; 0x32
 80b2d2e:	ea5f 5c91 	movs.w	ip, r1, lsr #22
 80b2d32:	f43f aedc 	beq.w	80b2aee <__adddf3+0xe6>
 80b2d36:	f04f 0203 	mov.w	r2, #3
 80b2d3a:	ea5f 0cdc 	movs.w	ip, ip, lsr #3
 80b2d3e:	bf18      	it	ne
 80b2d40:	3203      	addne	r2, #3
 80b2d42:	ea5f 0cdc 	movs.w	ip, ip, lsr #3
 80b2d46:	bf18      	it	ne
 80b2d48:	3203      	addne	r2, #3
 80b2d4a:	eb02 02dc 	add.w	r2, r2, ip, lsr #3
 80b2d4e:	f1c2 0320 	rsb	r3, r2, #32
 80b2d52:	fa00 fc03 	lsl.w	ip, r0, r3
 80b2d56:	fa20 f002 	lsr.w	r0, r0, r2
 80b2d5a:	fa01 fe03 	lsl.w	lr, r1, r3
 80b2d5e:	ea40 000e 	orr.w	r0, r0, lr
 80b2d62:	fa21 f102 	lsr.w	r1, r1, r2
 80b2d66:	4414      	add	r4, r2
 80b2d68:	e6c1      	b.n	80b2aee <__adddf3+0xe6>
 80b2d6a:	bf00      	nop

080b2d6c <__aeabi_dmul>:
 80b2d6c:	b570      	push	{r4, r5, r6, lr}
 80b2d6e:	f04f 0cff 	mov.w	ip, #255	; 0xff
 80b2d72:	f44c 6ce0 	orr.w	ip, ip, #1792	; 0x700
 80b2d76:	ea1c 5411 	ands.w	r4, ip, r1, lsr #20
 80b2d7a:	bf1d      	ittte	ne
 80b2d7c:	ea1c 5513 	andsne.w	r5, ip, r3, lsr #20
 80b2d80:	ea94 0f0c 	teqne	r4, ip
 80b2d84:	ea95 0f0c 	teqne	r5, ip
 80b2d88:	f000 f8de 	bleq	80b2f48 <__aeabi_dmul+0x1dc>
 80b2d8c:	442c      	add	r4, r5
 80b2d8e:	ea81 0603 	eor.w	r6, r1, r3
 80b2d92:	ea21 514c 	bic.w	r1, r1, ip, lsl #21
 80b2d96:	ea23 534c 	bic.w	r3, r3, ip, lsl #21
 80b2d9a:	ea50 3501 	orrs.w	r5, r0, r1, lsl #12
 80b2d9e:	bf18      	it	ne
 80b2da0:	ea52 3503 	orrsne.w	r5, r2, r3, lsl #12
 80b2da4:	f441 1180 	orr.w	r1, r1, #1048576	; 0x100000
 80b2da8:	f443 1380 	orr.w	r3, r3, #1048576	; 0x100000
 80b2dac:	d038      	beq.n	80b2e20 <__aeabi_dmul+0xb4>
 80b2dae:	fba0 ce02 	umull	ip, lr, r0, r2
 80b2db2:	f04f 0500 	mov.w	r5, #0
 80b2db6:	fbe1 e502 	umlal	lr, r5, r1, r2
 80b2dba:	f006 4200 	and.w	r2, r6, #2147483648	; 0x80000000
 80b2dbe:	fbe0 e503 	umlal	lr, r5, r0, r3
 80b2dc2:	f04f 0600 	mov.w	r6, #0
 80b2dc6:	fbe1 5603 	umlal	r5, r6, r1, r3
 80b2dca:	f09c 0f00 	teq	ip, #0
 80b2dce:	bf18      	it	ne
 80b2dd0:	f04e 0e01 	orrne.w	lr, lr, #1
 80b2dd4:	f1a4 04ff 	sub.w	r4, r4, #255	; 0xff
 80b2dd8:	f5b6 7f00 	cmp.w	r6, #512	; 0x200
 80b2ddc:	f564 7440 	sbc.w	r4, r4, #768	; 0x300
 80b2de0:	d204      	bcs.n	80b2dec <__aeabi_dmul+0x80>
 80b2de2:	ea5f 0e4e 	movs.w	lr, lr, lsl #1
 80b2de6:	416d      	adcs	r5, r5
 80b2de8:	eb46 0606 	adc.w	r6, r6, r6
 80b2dec:	ea42 21c6 	orr.w	r1, r2, r6, lsl #11
 80b2df0:	ea41 5155 	orr.w	r1, r1, r5, lsr #21
 80b2df4:	ea4f 20c5 	mov.w	r0, r5, lsl #11
 80b2df8:	ea40 505e 	orr.w	r0, r0, lr, lsr #21
 80b2dfc:	ea4f 2ece 	mov.w	lr, lr, lsl #11
 80b2e00:	f1b4 0cfd 	subs.w	ip, r4, #253	; 0xfd
 80b2e04:	bf88      	it	hi
 80b2e06:	f5bc 6fe0 	cmphi.w	ip, #1792	; 0x700
 80b2e0a:	d81e      	bhi.n	80b2e4a <__aeabi_dmul+0xde>
 80b2e0c:	f1be 4f00 	cmp.w	lr, #2147483648	; 0x80000000
 80b2e10:	bf08      	it	eq
 80b2e12:	ea5f 0e50 	movseq.w	lr, r0, lsr #1
 80b2e16:	f150 0000 	adcs.w	r0, r0, #0
 80b2e1a:	eb41 5104 	adc.w	r1, r1, r4, lsl #20
 80b2e1e:	bd70      	pop	{r4, r5, r6, pc}
 80b2e20:	f006 4600 	and.w	r6, r6, #2147483648	; 0x80000000
 80b2e24:	ea46 0101 	orr.w	r1, r6, r1
 80b2e28:	ea40 0002 	orr.w	r0, r0, r2
 80b2e2c:	ea81 0103 	eor.w	r1, r1, r3
 80b2e30:	ebb4 045c 	subs.w	r4, r4, ip, lsr #1
 80b2e34:	bfc2      	ittt	gt
 80b2e36:	ebd4 050c 	rsbsgt	r5, r4, ip
 80b2e3a:	ea41 5104 	orrgt.w	r1, r1, r4, lsl #20
 80b2e3e:	bd70      	popgt	{r4, r5, r6, pc}
 80b2e40:	f441 1180 	orr.w	r1, r1, #1048576	; 0x100000
 80b2e44:	f04f 0e00 	mov.w	lr, #0
 80b2e48:	3c01      	subs	r4, #1
 80b2e4a:	f300 80ab 	bgt.w	80b2fa4 <__aeabi_dmul+0x238>
 80b2e4e:	f114 0f36 	cmn.w	r4, #54	; 0x36
 80b2e52:	bfde      	ittt	le
 80b2e54:	2000      	movle	r0, #0
 80b2e56:	f001 4100 	andle.w	r1, r1, #2147483648	; 0x80000000
 80b2e5a:	bd70      	pople	{r4, r5, r6, pc}
 80b2e5c:	f1c4 0400 	rsb	r4, r4, #0
 80b2e60:	3c20      	subs	r4, #32
 80b2e62:	da35      	bge.n	80b2ed0 <__aeabi_dmul+0x164>
 80b2e64:	340c      	adds	r4, #12
 80b2e66:	dc1b      	bgt.n	80b2ea0 <__aeabi_dmul+0x134>
 80b2e68:	f104 0414 	add.w	r4, r4, #20
 80b2e6c:	f1c4 0520 	rsb	r5, r4, #32
 80b2e70:	fa00 f305 	lsl.w	r3, r0, r5
 80b2e74:	fa20 f004 	lsr.w	r0, r0, r4
 80b2e78:	fa01 f205 	lsl.w	r2, r1, r5
 80b2e7c:	ea40 0002 	orr.w	r0, r0, r2
 80b2e80:	f001 4200 	and.w	r2, r1, #2147483648	; 0x80000000
 80b2e84:	f021 4100 	bic.w	r1, r1, #2147483648	; 0x80000000
 80b2e88:	eb10 70d3 	adds.w	r0, r0, r3, lsr #31
 80b2e8c:	fa21 f604 	lsr.w	r6, r1, r4
 80b2e90:	eb42 0106 	adc.w	r1, r2, r6
 80b2e94:	ea5e 0e43 	orrs.w	lr, lr, r3, lsl #1
 80b2e98:	bf08      	it	eq
 80b2e9a:	ea20 70d3 	biceq.w	r0, r0, r3, lsr #31
 80b2e9e:	bd70      	pop	{r4, r5, r6, pc}
 80b2ea0:	f1c4 040c 	rsb	r4, r4, #12
 80b2ea4:	f1c4 0520 	rsb	r5, r4, #32
 80b2ea8:	fa00 f304 	lsl.w	r3, r0, r4
 80b2eac:	fa20 f005 	lsr.w	r0, r0, r5
 80b2eb0:	fa01 f204 	lsl.w	r2, r1, r4
 80b2eb4:	ea40 0002 	orr.w	r0, r0, r2
 80b2eb8:	f001 4100 	and.w	r1, r1, #2147483648	; 0x80000000
 80b2ebc:	eb10 70d3 	adds.w	r0, r0, r3, lsr #31
 80b2ec0:	f141 0100 	adc.w	r1, r1, #0
 80b2ec4:	ea5e 0e43 	orrs.w	lr, lr, r3, lsl #1
 80b2ec8:	bf08      	it	eq
 80b2eca:	ea20 70d3 	biceq.w	r0, r0, r3, lsr #31
 80b2ece:	bd70      	pop	{r4, r5, r6, pc}
 80b2ed0:	f1c4 0520 	rsb	r5, r4, #32
 80b2ed4:	fa00 f205 	lsl.w	r2, r0, r5
 80b2ed8:	ea4e 0e02 	orr.w	lr, lr, r2
 80b2edc:	fa20 f304 	lsr.w	r3, r0, r4
 80b2ee0:	fa01 f205 	lsl.w	r2, r1, r5
 80b2ee4:	ea43 0302 	orr.w	r3, r3, r2
 80b2ee8:	fa21 f004 	lsr.w	r0, r1, r4
 80b2eec:	f001 4100 	and.w	r1, r1, #2147483648	; 0x80000000
 80b2ef0:	fa21 f204 	lsr.w	r2, r1, r4
 80b2ef4:	ea20 0002 	bic.w	r0, r0, r2
 80b2ef8:	eb00 70d3 	add.w	r0, r0, r3, lsr #31
 80b2efc:	ea5e 0e43 	orrs.w	lr, lr, r3, lsl #1
 80b2f00:	bf08      	it	eq
 80b2f02:	ea20 70d3 	biceq.w	r0, r0, r3, lsr #31
 80b2f06:	bd70      	pop	{r4, r5, r6, pc}
 80b2f08:	f094 0f00 	teq	r4, #0
 80b2f0c:	d10f      	bne.n	80b2f2e <__aeabi_dmul+0x1c2>
 80b2f0e:	f001 4600 	and.w	r6, r1, #2147483648	; 0x80000000
 80b2f12:	0040      	lsls	r0, r0, #1
 80b2f14:	eb41 0101 	adc.w	r1, r1, r1
 80b2f18:	f411 1f80 	tst.w	r1, #1048576	; 0x100000
 80b2f1c:	bf08      	it	eq
 80b2f1e:	3c01      	subeq	r4, #1
 80b2f20:	d0f7      	beq.n	80b2f12 <__aeabi_dmul+0x1a6>
 80b2f22:	ea41 0106 	orr.w	r1, r1, r6
 80b2f26:	f095 0f00 	teq	r5, #0
 80b2f2a:	bf18      	it	ne
 80b2f2c:	4770      	bxne	lr
 80b2f2e:	f003 4600 	and.w	r6, r3, #2147483648	; 0x80000000
 80b2f32:	0052      	lsls	r2, r2, #1
 80b2f34:	eb43 0303 	adc.w	r3, r3, r3
 80b2f38:	f413 1f80 	tst.w	r3, #1048576	; 0x100000
 80b2f3c:	bf08      	it	eq
 80b2f3e:	3d01      	subeq	r5, #1
 80b2f40:	d0f7      	beq.n	80b2f32 <__aeabi_dmul+0x1c6>
 80b2f42:	ea43 0306 	orr.w	r3, r3, r6
 80b2f46:	4770      	bx	lr
 80b2f48:	ea94 0f0c 	teq	r4, ip
 80b2f4c:	ea0c 5513 	and.w	r5, ip, r3, lsr #20
 80b2f50:	bf18      	it	ne
 80b2f52:	ea95 0f0c 	teqne	r5, ip
 80b2f56:	d00c      	beq.n	80b2f72 <__aeabi_dmul+0x206>
 80b2f58:	ea50 0641 	orrs.w	r6, r0, r1, lsl #1
 80b2f5c:	bf18      	it	ne
 80b2f5e:	ea52 0643 	orrsne.w	r6, r2, r3, lsl #1
 80b2f62:	d1d1      	bne.n	80b2f08 <__aeabi_dmul+0x19c>
 80b2f64:	ea81 0103 	eor.w	r1, r1, r3
 80b2f68:	f001 4100 	and.w	r1, r1, #2147483648	; 0x80000000
 80b2f6c:	f04f 0000 	mov.w	r0, #0
 80b2f70:	bd70      	pop	{r4, r5, r6, pc}
 80b2f72:	ea50 0641 	orrs.w	r6, r0, r1, lsl #1
 80b2f76:	bf06      	itte	eq
 80b2f78:	4610      	moveq	r0, r2
 80b2f7a:	4619      	moveq	r1, r3
 80b2f7c:	ea52 0643 	orrsne.w	r6, r2, r3, lsl #1
 80b2f80:	d019      	beq.n	80b2fb6 <__aeabi_dmul+0x24a>
 80b2f82:	ea94 0f0c 	teq	r4, ip
 80b2f86:	d102      	bne.n	80b2f8e <__aeabi_dmul+0x222>
 80b2f88:	ea50 3601 	orrs.w	r6, r0, r1, lsl #12
 80b2f8c:	d113      	bne.n	80b2fb6 <__aeabi_dmul+0x24a>
 80b2f8e:	ea95 0f0c 	teq	r5, ip
 80b2f92:	d105      	bne.n	80b2fa0 <__aeabi_dmul+0x234>
 80b2f94:	ea52 3603 	orrs.w	r6, r2, r3, lsl #12
 80b2f98:	bf1c      	itt	ne
 80b2f9a:	4610      	movne	r0, r2
 80b2f9c:	4619      	movne	r1, r3
 80b2f9e:	d10a      	bne.n	80b2fb6 <__aeabi_dmul+0x24a>
 80b2fa0:	ea81 0103 	eor.w	r1, r1, r3
 80b2fa4:	f001 4100 	and.w	r1, r1, #2147483648	; 0x80000000
 80b2fa8:	f041 41fe 	orr.w	r1, r1, #2130706432	; 0x7f000000
 80b2fac:	f441 0170 	orr.w	r1, r1, #15728640	; 0xf00000
 80b2fb0:	f04f 0000 	mov.w	r0, #0
 80b2fb4:	bd70      	pop	{r4, r5, r6, pc}
 80b2fb6:	f041 41fe 	orr.w	r1, r1, #2130706432	; 0x7f000000
 80b2fba:	f441 0178 	orr.w	r1, r1, #16252928	; 0xf80000
 80b2fbe:	bd70      	pop	{r4, r5, r6, pc}

080b2fc0 <__aeabi_ddiv>:
 80b2fc0:	b570      	push	{r4, r5, r6, lr}
 80b2fc2:	f04f 0cff 	mov.w	ip, #255	; 0xff
 80b2fc6:	f44c 6ce0 	orr.w	ip, ip, #1792	; 0x700
 80b2fca:	ea1c 5411 	ands.w	r4, ip, r1, lsr #20
 80b2fce:	bf1d      	ittte	ne
 80b2fd0:	ea1c 5513 	andsne.w	r5, ip, r3, lsr #20
 80b2fd4:	ea94 0f0c 	teqne	r4, ip
 80b2fd8:	ea95 0f0c 	teqne	r5, ip
 80b2fdc:	f000 f8a7 	bleq	80b312e <__aeabi_ddiv+0x16e>
 80b2fe0:	eba4 0405 	sub.w	r4, r4, r5
 80b2fe4:	ea81 0e03 	eor.w	lr, r1, r3
 80b2fe8:	ea52 3503 	orrs.w	r5, r2, r3, lsl #12
 80b2fec:	ea4f 3101 	mov.w	r1, r1, lsl #12
 80b2ff0:	f000 8088 	beq.w	80b3104 <__aeabi_ddiv+0x144>
 80b2ff4:	ea4f 3303 	mov.w	r3, r3, lsl #12
 80b2ff8:	f04f 5580 	mov.w	r5, #268435456	; 0x10000000
 80b2ffc:	ea45 1313 	orr.w	r3, r5, r3, lsr #4
 80b3000:	ea43 6312 	orr.w	r3, r3, r2, lsr #24
 80b3004:	ea4f 2202 	mov.w	r2, r2, lsl #8
 80b3008:	ea45 1511 	orr.w	r5, r5, r1, lsr #4
 80b300c:	ea45 6510 	orr.w	r5, r5, r0, lsr #24
 80b3010:	ea4f 2600 	mov.w	r6, r0, lsl #8
 80b3014:	f00e 4100 	and.w	r1, lr, #2147483648	; 0x80000000
 80b3018:	429d      	cmp	r5, r3
 80b301a:	bf08      	it	eq
 80b301c:	4296      	cmpeq	r6, r2
 80b301e:	f144 04fd 	adc.w	r4, r4, #253	; 0xfd
 80b3022:	f504 7440 	add.w	r4, r4, #768	; 0x300
 80b3026:	d202      	bcs.n	80b302e <__aeabi_ddiv+0x6e>
 80b3028:	085b      	lsrs	r3, r3, #1
 80b302a:	ea4f 0232 	mov.w	r2, r2, rrx
 80b302e:	1ab6      	subs	r6, r6, r2
 80b3030:	eb65 0503 	sbc.w	r5, r5, r3
 80b3034:	085b      	lsrs	r3, r3, #1
 80b3036:	ea4f 0232 	mov.w	r2, r2, rrx
 80b303a:	f44f 1080 	mov.w	r0, #1048576	; 0x100000
 80b303e:	f44f 2c00 	mov.w	ip, #524288	; 0x80000
 80b3042:	ebb6 0e02 	subs.w	lr, r6, r2
 80b3046:	eb75 0e03 	sbcs.w	lr, r5, r3
 80b304a:	bf22      	ittt	cs
 80b304c:	1ab6      	subcs	r6, r6, r2
 80b304e:	4675      	movcs	r5, lr
 80b3050:	ea40 000c 	orrcs.w	r0, r0, ip
 80b3054:	085b      	lsrs	r3, r3, #1
 80b3056:	ea4f 0232 	mov.w	r2, r2, rrx
 80b305a:	ebb6 0e02 	subs.w	lr, r6, r2
 80b305e:	eb75 0e03 	sbcs.w	lr, r5, r3
 80b3062:	bf22      	ittt	cs
 80b3064:	1ab6      	subcs	r6, r6, r2
 80b3066:	4675      	movcs	r5, lr
 80b3068:	ea40 005c 	orrcs.w	r0, r0, ip, lsr #1
 80b306c:	085b      	lsrs	r3, r3, #1
 80b306e:	ea4f 0232 	mov.w	r2, r2, rrx
 80b3072:	ebb6 0e02 	subs.w	lr, r6, r2
 80b3076:	eb75 0e03 	sbcs.w	lr, r5, r3
 80b307a:	bf22      	ittt	cs
 80b307c:	1ab6      	subcs	r6, r6, r2
 80b307e:	4675      	movcs	r5, lr
 80b3080:	ea40 009c 	orrcs.w	r0, r0, ip, lsr #2
 80b3084:	085b      	lsrs	r3, r3, #1
 80b3086:	ea4f 0232 	mov.w	r2, r2, rrx
 80b308a:	ebb6 0e02 	subs.w	lr, r6, r2
 80b308e:	eb75 0e03 	sbcs.w	lr, r5, r3
 80b3092:	bf22      	ittt	cs
 80b3094:	1ab6      	subcs	r6, r6, r2
 80b3096:	4675      	movcs	r5, lr
 80b3098:	ea40 00dc 	orrcs.w	r0, r0, ip, lsr #3
 80b309c:	ea55 0e06 	orrs.w	lr, r5, r6
 80b30a0:	d018      	beq.n	80b30d4 <__aeabi_ddiv+0x114>
 80b30a2:	ea4f 1505 	mov.w	r5, r5, lsl #4
 80b30a6:	ea45 7516 	orr.w	r5, r5, r6, lsr #28
 80b30aa:	ea4f 1606 	mov.w	r6, r6, lsl #4
 80b30ae:	ea4f 03c3 	mov.w	r3, r3, lsl #3
 80b30b2:	ea43 7352 	orr.w	r3, r3, r2, lsr #29
 80b30b6:	ea4f 02c2 	mov.w	r2, r2, lsl #3
 80b30ba:	ea5f 1c1c 	movs.w	ip, ip, lsr #4
 80b30be:	d1c0      	bne.n	80b3042 <__aeabi_ddiv+0x82>
 80b30c0:	f411 1f80 	tst.w	r1, #1048576	; 0x100000
 80b30c4:	d10b      	bne.n	80b30de <__aeabi_ddiv+0x11e>
 80b30c6:	ea41 0100 	orr.w	r1, r1, r0
 80b30ca:	f04f 0000 	mov.w	r0, #0
 80b30ce:	f04f 4c00 	mov.w	ip, #2147483648	; 0x80000000
 80b30d2:	e7b6      	b.n	80b3042 <__aeabi_ddiv+0x82>
 80b30d4:	f411 1f80 	tst.w	r1, #1048576	; 0x100000
 80b30d8:	bf04      	itt	eq
 80b30da:	4301      	orreq	r1, r0
 80b30dc:	2000      	moveq	r0, #0
 80b30de:	f1b4 0cfd 	subs.w	ip, r4, #253	; 0xfd
 80b30e2:	bf88      	it	hi
 80b30e4:	f5bc 6fe0 	cmphi.w	ip, #1792	; 0x700
 80b30e8:	f63f aeaf 	bhi.w	80b2e4a <__aeabi_dmul+0xde>
 80b30ec:	ebb5 0c03 	subs.w	ip, r5, r3
 80b30f0:	bf04      	itt	eq
 80b30f2:	ebb6 0c02 	subseq.w	ip, r6, r2
 80b30f6:	ea5f 0c50 	movseq.w	ip, r0, lsr #1
 80b30fa:	f150 0000 	adcs.w	r0, r0, #0
 80b30fe:	eb41 5104 	adc.w	r1, r1, r4, lsl #20
 80b3102:	bd70      	pop	{r4, r5, r6, pc}
 80b3104:	f00e 4e00 	and.w	lr, lr, #2147483648	; 0x80000000
 80b3108:	ea4e 3111 	orr.w	r1, lr, r1, lsr #12
 80b310c:	eb14 045c 	adds.w	r4, r4, ip, lsr #1
 80b3110:	bfc2      	ittt	gt
 80b3112:	ebd4 050c 	rsbsgt	r5, r4, ip
 80b3116:	ea41 5104 	orrgt.w	r1, r1, r4, lsl #20
 80b311a:	bd70      	popgt	{r4, r5, r6, pc}
 80b311c:	f441 1180 	orr.w	r1, r1, #1048576	; 0x100000
 80b3120:	f04f 0e00 	mov.w	lr, #0
 80b3124:	3c01      	subs	r4, #1
 80b3126:	e690      	b.n	80b2e4a <__aeabi_dmul+0xde>
 80b3128:	ea45 0e06 	orr.w	lr, r5, r6
 80b312c:	e68d      	b.n	80b2e4a <__aeabi_dmul+0xde>
 80b312e:	ea0c 5513 	and.w	r5, ip, r3, lsr #20
 80b3132:	ea94 0f0c 	teq	r4, ip
 80b3136:	bf08      	it	eq
 80b3138:	ea95 0f0c 	teqeq	r5, ip
 80b313c:	f43f af3b 	beq.w	80b2fb6 <__aeabi_dmul+0x24a>
 80b3140:	ea94 0f0c 	teq	r4, ip
 80b3144:	d10a      	bne.n	80b315c <__aeabi_ddiv+0x19c>
 80b3146:	ea50 3401 	orrs.w	r4, r0, r1, lsl #12
 80b314a:	f47f af34 	bne.w	80b2fb6 <__aeabi_dmul+0x24a>
 80b314e:	ea95 0f0c 	teq	r5, ip
 80b3152:	f47f af25 	bne.w	80b2fa0 <__aeabi_dmul+0x234>
 80b3156:	4610      	mov	r0, r2
 80b3158:	4619      	mov	r1, r3
 80b315a:	e72c      	b.n	80b2fb6 <__aeabi_dmul+0x24a>
 80b315c:	ea95 0f0c 	teq	r5, ip
 80b3160:	d106      	bne.n	80b3170 <__aeabi_ddiv+0x1b0>
 80b3162:	ea52 3503 	orrs.w	r5, r2, r3, lsl #12
 80b3166:	f43f aefd 	beq.w	80b2f64 <__aeabi_dmul+0x1f8>
 80b316a:	4610      	mov	r0, r2
 80b316c:	4619      	mov	r1, r3
 80b316e:	e722      	b.n	80b2fb6 <__aeabi_dmul+0x24a>
 80b3170:	ea50 0641 	orrs.w	r6, r0, r1, lsl #1
 80b3174:	bf18      	it	ne
 80b3176:	ea52 0643 	orrsne.w	r6, r2, r3, lsl #1
 80b317a:	f47f aec5 	bne.w	80b2f08 <__aeabi_dmul+0x19c>
 80b317e:	ea50 0441 	orrs.w	r4, r0, r1, lsl #1
 80b3182:	f47f af0d 	bne.w	80b2fa0 <__aeabi_dmul+0x234>
 80b3186:	ea52 0543 	orrs.w	r5, r2, r3, lsl #1
 80b318a:	f47f aeeb 	bne.w	80b2f64 <__aeabi_dmul+0x1f8>
 80b318e:	e712      	b.n	80b2fb6 <__aeabi_dmul+0x24a>

080b3190 <__gedf2>:
 80b3190:	f04f 3cff 	mov.w	ip, #4294967295	; 0xffffffff
 80b3194:	e006      	b.n	80b31a4 <__cmpdf2+0x4>
 80b3196:	bf00      	nop

080b3198 <__ledf2>:
 80b3198:	f04f 0c01 	mov.w	ip, #1
 80b319c:	e002      	b.n	80b31a4 <__cmpdf2+0x4>
 80b319e:	bf00      	nop

080b31a0 <__cmpdf2>:
 80b31a0:	f04f 0c01 	mov.w	ip, #1
 80b31a4:	f84d cd04 	str.w	ip, [sp, #-4]!
 80b31a8:	ea4f 0c41 	mov.w	ip, r1, lsl #1
 80b31ac:	ea7f 5c6c 	mvns.w	ip, ip, asr #21
 80b31b0:	ea4f 0c43 	mov.w	ip, r3, lsl #1
 80b31b4:	bf18      	it	ne
 80b31b6:	ea7f 5c6c 	mvnsne.w	ip, ip, asr #21
 80b31ba:	d01b      	beq.n	80b31f4 <__cmpdf2+0x54>
 80b31bc:	b001      	add	sp, #4
 80b31be:	ea50 0c41 	orrs.w	ip, r0, r1, lsl #1
 80b31c2:	bf0c      	ite	eq
 80b31c4:	ea52 0c43 	orrseq.w	ip, r2, r3, lsl #1
 80b31c8:	ea91 0f03 	teqne	r1, r3
 80b31cc:	bf02      	ittt	eq
 80b31ce:	ea90 0f02 	teqeq	r0, r2
 80b31d2:	2000      	moveq	r0, #0
 80b31d4:	4770      	bxeq	lr
 80b31d6:	f110 0f00 	cmn.w	r0, #0
 80b31da:	ea91 0f03 	teq	r1, r3
 80b31de:	bf58      	it	pl
 80b31e0:	4299      	cmppl	r1, r3
 80b31e2:	bf08      	it	eq
 80b31e4:	4290      	cmpeq	r0, r2
 80b31e6:	bf2c      	ite	cs
 80b31e8:	17d8      	asrcs	r0, r3, #31
 80b31ea:	ea6f 70e3 	mvncc.w	r0, r3, asr #31
 80b31ee:	f040 0001 	orr.w	r0, r0, #1
 80b31f2:	4770      	bx	lr
 80b31f4:	ea4f 0c41 	mov.w	ip, r1, lsl #1
 80b31f8:	ea7f 5c6c 	mvns.w	ip, ip, asr #21
 80b31fc:	d102      	bne.n	80b3204 <__cmpdf2+0x64>
 80b31fe:	ea50 3c01 	orrs.w	ip, r0, r1, lsl #12
 80b3202:	d107      	bne.n	80b3214 <__cmpdf2+0x74>
 80b3204:	ea4f 0c43 	mov.w	ip, r3, lsl #1
 80b3208:	ea7f 5c6c 	mvns.w	ip, ip, asr #21
 80b320c:	d1d6      	bne.n	80b31bc <__cmpdf2+0x1c>
 80b320e:	ea52 3c03 	orrs.w	ip, r2, r3, lsl #12
 80b3212:	d0d3      	beq.n	80b31bc <__cmpdf2+0x1c>
 80b3214:	f85d 0b04 	ldr.w	r0, [sp], #4
 80b3218:	4770      	bx	lr
 80b321a:	bf00      	nop

080b321c <__aeabi_cdrcmple>:
 80b321c:	4684      	mov	ip, r0
 80b321e:	4610      	mov	r0, r2
 80b3220:	4662      	mov	r2, ip
 80b3222:	468c      	mov	ip, r1
 80b3224:	4619      	mov	r1, r3
 80b3226:	4663      	mov	r3, ip
 80b3228:	e000      	b.n	80b322c <__aeabi_cdcmpeq>
 80b322a:	bf00      	nop

080b322c <__aeabi_cdcmpeq>:
 80b322c:	b501      	push	{r0, lr}
 80b322e:	f7ff ffb7 	bl	80b31a0 <__cmpdf2>
 80b3232:	2800      	cmp	r0, #0
 80b3234:	bf48      	it	mi
 80b3236:	f110 0f00 	cmnmi.w	r0, #0
 80b323a:	bd01      	pop	{r0, pc}

080b323c <__aeabi_dcmpeq>:
 80b323c:	f84d ed08 	str.w	lr, [sp, #-8]!
 80b3240:	f7ff fff4 	bl	80b322c <__aeabi_cdcmpeq>
 80b3244:	bf0c      	ite	eq
 80b3246:	2001      	moveq	r0, #1
 80b3248:	2000      	movne	r0, #0
 80b324a:	f85d fb08 	ldr.w	pc, [sp], #8
 80b324e:	bf00      	nop

080b3250 <__aeabi_dcmplt>:
 80b3250:	f84d ed08 	str.w	lr, [sp, #-8]!
 80b3254:	f7ff ffea 	bl	80b322c <__aeabi_cdcmpeq>
 80b3258:	bf34      	ite	cc
 80b325a:	2001      	movcc	r0, #1
 80b325c:	2000      	movcs	r0, #0
 80b325e:	f85d fb08 	ldr.w	pc, [sp], #8
 80b3262:	bf00      	nop

080b3264 <__aeabi_dcmple>:
 80b3264:	f84d ed08 	str.w	lr, [sp, #-8]!
 80b3268:	f7ff ffe0 	bl	80b322c <__aeabi_cdcmpeq>
 80b326c:	bf94      	ite	ls
 80b326e:	2001      	movls	r0, #1
 80b3270:	2000      	movhi	r0, #0
 80b3272:	f85d fb08 	ldr.w	pc, [sp], #8
 80b3276:	bf00      	nop

080b3278 <__aeabi_dcmpge>:
 80b3278:	f84d ed08 	str.w	lr, [sp, #-8]!
 80b327c:	f7ff ffce 	bl	80b321c <__aeabi_cdrcmple>
 80b3280:	bf94      	ite	ls
 80b3282:	2001      	movls	r0, #1
 80b3284:	2000      	movhi	r0, #0
 80b3286:	f85d fb08 	ldr.w	pc, [sp], #8
 80b328a:	bf00      	nop

080b328c <__aeabi_dcmpgt>:
 80b328c:	f84d ed08 	str.w	lr, [sp, #-8]!
 80b3290:	f7ff ffc4 	bl	80b321c <__aeabi_cdrcmple>
 80b3294:	bf34      	ite	cc
 80b3296:	2001      	movcc	r0, #1
 80b3298:	2000      	movcs	r0, #0
 80b329a:	f85d fb08 	ldr.w	pc, [sp], #8
 80b329e:	bf00      	nop

080b32a0 <__aeabi_d2iz>:
 80b32a0:	ea4f 0241 	mov.w	r2, r1, lsl #1
 80b32a4:	f512 1200 	adds.w	r2, r2, #2097152	; 0x200000
 80b32a8:	d215      	bcs.n	80b32d6 <__aeabi_d2iz+0x36>
 80b32aa:	d511      	bpl.n	80b32d0 <__aeabi_d2iz+0x30>
 80b32ac:	f46f 7378 	mvn.w	r3, #992	; 0x3e0
 80b32b0:	ebb3 5262 	subs.w	r2, r3, r2, asr #21
 80b32b4:	d912      	bls.n	80b32dc <__aeabi_d2iz+0x3c>
 80b32b6:	ea4f 23c1 	mov.w	r3, r1, lsl #11
 80b32ba:	f043 4300 	orr.w	r3, r3, #2147483648	; 0x80000000
 80b32be:	ea43 5350 	orr.w	r3, r3, r0, lsr #21
 80b32c2:	f011 4f00 	tst.w	r1, #2147483648	; 0x80000000
 80b32c6:	fa23 f002 	lsr.w	r0, r3, r2
 80b32ca:	bf18      	it	ne
 80b32cc:	4240      	negne	r0, r0
 80b32ce:	4770      	bx	lr
 80b32d0:	f04f 0000 	mov.w	r0, #0
 80b32d4:	4770      	bx	lr
 80b32d6:	ea50 3001 	orrs.w	r0, r0, r1, lsl #12
 80b32da:	d105      	bne.n	80b32e8 <__aeabi_d2iz+0x48>
 80b32dc:	f011 4000 	ands.w	r0, r1, #2147483648	; 0x80000000
 80b32e0:	bf08      	it	eq
 80b32e2:	f06f 4000 	mvneq.w	r0, #2147483648	; 0x80000000
 80b32e6:	4770      	bx	lr
 80b32e8:	f04f 0000 	mov.w	r0, #0
 80b32ec:	4770      	bx	lr
 80b32ee:	bf00      	nop

080b32f0 <__aeabi_d2uiz>:
 80b32f0:	004a      	lsls	r2, r1, #1
 80b32f2:	d211      	bcs.n	80b3318 <__aeabi_d2uiz+0x28>
 80b32f4:	f512 1200 	adds.w	r2, r2, #2097152	; 0x200000
 80b32f8:	d211      	bcs.n	80b331e <__aeabi_d2uiz+0x2e>
 80b32fa:	d50d      	bpl.n	80b3318 <__aeabi_d2uiz+0x28>
 80b32fc:	f46f 7378 	mvn.w	r3, #992	; 0x3e0
 80b3300:	ebb3 5262 	subs.w	r2, r3, r2, asr #21
 80b3304:	d40e      	bmi.n	80b3324 <__aeabi_d2uiz+0x34>
 80b3306:	ea4f 23c1 	mov.w	r3, r1, lsl #11
 80b330a:	f043 4300 	orr.w	r3, r3, #2147483648	; 0x80000000
 80b330e:	ea43 5350 	orr.w	r3, r3, r0, lsr #21
 80b3312:	fa23 f002 	lsr.w	r0, r3, r2
 80b3316:	4770      	bx	lr
 80b3318:	f04f 0000 	mov.w	r0, #0
 80b331c:	4770      	bx	lr
 80b331e:	ea50 3001 	orrs.w	r0, r0, r1, lsl #12
 80b3322:	d102      	bne.n	80b332a <__aeabi_d2uiz+0x3a>
 80b3324:	f04f 30ff 	mov.w	r0, #4294967295	; 0xffffffff
 80b3328:	4770      	bx	lr
 80b332a:	f04f 0000 	mov.w	r0, #0
 80b332e:	4770      	bx	lr

080b3330 <__aeabi_d2f>:
 80b3330:	ea4f 0241 	mov.w	r2, r1, lsl #1
 80b3334:	f1b2 43e0 	subs.w	r3, r2, #1879048192	; 0x70000000
 80b3338:	bf24      	itt	cs
 80b333a:	f5b3 1c00 	subscs.w	ip, r3, #2097152	; 0x200000
 80b333e:	f1dc 5cfe 	rsbscs	ip, ip, #532676608	; 0x1fc00000
 80b3342:	d90d      	bls.n	80b3360 <__aeabi_d2f+0x30>
 80b3344:	f001 4c00 	and.w	ip, r1, #2147483648	; 0x80000000
 80b3348:	ea4f 02c0 	mov.w	r2, r0, lsl #3
 80b334c:	ea4c 7050 	orr.w	r0, ip, r0, lsr #29
 80b3350:	f1b2 4f00 	cmp.w	r2, #2147483648	; 0x80000000
 80b3354:	eb40 0083 	adc.w	r0, r0, r3, lsl #2
 80b3358:	bf08      	it	eq
 80b335a:	f020 0001 	biceq.w	r0, r0, #1
 80b335e:	4770      	bx	lr
 80b3360:	f011 4f80 	tst.w	r1, #1073741824	; 0x40000000
 80b3364:	d121      	bne.n	80b33aa <__aeabi_d2f+0x7a>
 80b3366:	f113 7238 	adds.w	r2, r3, #48234496	; 0x2e00000
 80b336a:	bfbc      	itt	lt
 80b336c:	f001 4000 	andlt.w	r0, r1, #2147483648	; 0x80000000
 80b3370:	4770      	bxlt	lr
 80b3372:	f441 1180 	orr.w	r1, r1, #1048576	; 0x100000
 80b3376:	ea4f 5252 	mov.w	r2, r2, lsr #21
 80b337a:	f1c2 0218 	rsb	r2, r2, #24
 80b337e:	f1c2 0c20 	rsb	ip, r2, #32
 80b3382:	fa10 f30c 	lsls.w	r3, r0, ip
 80b3386:	fa20 f002 	lsr.w	r0, r0, r2
 80b338a:	bf18      	it	ne
 80b338c:	f040 0001 	orrne.w	r0, r0, #1
 80b3390:	ea4f 23c1 	mov.w	r3, r1, lsl #11
 80b3394:	ea4f 23d3 	mov.w	r3, r3, lsr #11
 80b3398:	fa03 fc0c 	lsl.w	ip, r3, ip
 80b339c:	ea40 000c 	orr.w	r0, r0, ip
 80b33a0:	fa23 f302 	lsr.w	r3, r3, r2
 80b33a4:	ea4f 0343 	mov.w	r3, r3, lsl #1
 80b33a8:	e7cc      	b.n	80b3344 <__aeabi_d2f+0x14>
 80b33aa:	ea7f 5362 	mvns.w	r3, r2, asr #21
 80b33ae:	d107      	bne.n	80b33c0 <__aeabi_d2f+0x90>
 80b33b0:	ea50 3301 	orrs.w	r3, r0, r1, lsl #12
 80b33b4:	bf1e      	ittt	ne
 80b33b6:	f04f 40fe 	movne.w	r0, #2130706432	; 0x7f000000
 80b33ba:	f440 0040 	orrne.w	r0, r0, #12582912	; 0xc00000
 80b33be:	4770      	bxne	lr
 80b33c0:	f001 4000 	and.w	r0, r1, #2147483648	; 0x80000000
 80b33c4:	f040 40fe 	orr.w	r0, r0, #2130706432	; 0x7f000000
 80b33c8:	f440 0000 	orr.w	r0, r0, #8388608	; 0x800000
 80b33cc:	4770      	bx	lr
 80b33ce:	bf00      	nop

080b33d0 <__aeabi_frsub>:
 80b33d0:	f080 4000 	eor.w	r0, r0, #2147483648	; 0x80000000
 80b33d4:	e002      	b.n	80b33dc <__addsf3>
 80b33d6:	bf00      	nop

080b33d8 <__aeabi_fsub>:
 80b33d8:	f081 4100 	eor.w	r1, r1, #2147483648	; 0x80000000

080b33dc <__addsf3>:
 80b33dc:	0042      	lsls	r2, r0, #1
 80b33de:	bf1f      	itttt	ne
 80b33e0:	ea5f 0341 	movsne.w	r3, r1, lsl #1
 80b33e4:	ea92 0f03 	teqne	r2, r3
 80b33e8:	ea7f 6c22 	mvnsne.w	ip, r2, asr #24
 80b33ec:	ea7f 6c23 	mvnsne.w	ip, r3, asr #24
 80b33f0:	d06a      	beq.n	80b34c8 <__addsf3+0xec>
 80b33f2:	ea4f 6212 	mov.w	r2, r2, lsr #24
 80b33f6:	ebd2 6313 	rsbs	r3, r2, r3, lsr #24
 80b33fa:	bfc1      	itttt	gt
 80b33fc:	18d2      	addgt	r2, r2, r3
 80b33fe:	4041      	eorgt	r1, r0
 80b3400:	4048      	eorgt	r0, r1
 80b3402:	4041      	eorgt	r1, r0
 80b3404:	bfb8      	it	lt
 80b3406:	425b      	neglt	r3, r3
 80b3408:	2b19      	cmp	r3, #25
 80b340a:	bf88      	it	hi
 80b340c:	4770      	bxhi	lr
 80b340e:	f010 4f00 	tst.w	r0, #2147483648	; 0x80000000
 80b3412:	f440 0000 	orr.w	r0, r0, #8388608	; 0x800000
 80b3416:	f020 407f 	bic.w	r0, r0, #4278190080	; 0xff000000
 80b341a:	bf18      	it	ne
 80b341c:	4240      	negne	r0, r0
 80b341e:	f011 4f00 	tst.w	r1, #2147483648	; 0x80000000
 80b3422:	f441 0100 	orr.w	r1, r1, #8388608	; 0x800000
 80b3426:	f021 417f 	bic.w	r1, r1, #4278190080	; 0xff000000
 80b342a:	bf18      	it	ne
 80b342c:	4249      	negne	r1, r1
 80b342e:	ea92 0f03 	teq	r2, r3
 80b3432:	d03f      	beq.n	80b34b4 <__addsf3+0xd8>
 80b3434:	f1a2 0201 	sub.w	r2, r2, #1
 80b3438:	fa41 fc03 	asr.w	ip, r1, r3
 80b343c:	eb10 000c 	adds.w	r0, r0, ip
 80b3440:	f1c3 0320 	rsb	r3, r3, #32
 80b3444:	fa01 f103 	lsl.w	r1, r1, r3
 80b3448:	f000 4300 	and.w	r3, r0, #2147483648	; 0x80000000
 80b344c:	d502      	bpl.n	80b3454 <__addsf3+0x78>
 80b344e:	4249      	negs	r1, r1
 80b3450:	eb60 0040 	sbc.w	r0, r0, r0, lsl #1
 80b3454:	f5b0 0f00 	cmp.w	r0, #8388608	; 0x800000
 80b3458:	d313      	bcc.n	80b3482 <__addsf3+0xa6>
 80b345a:	f1b0 7f80 	cmp.w	r0, #16777216	; 0x1000000
 80b345e:	d306      	bcc.n	80b346e <__addsf3+0x92>
 80b3460:	0840      	lsrs	r0, r0, #1
 80b3462:	ea4f 0131 	mov.w	r1, r1, rrx
 80b3466:	f102 0201 	add.w	r2, r2, #1
 80b346a:	2afe      	cmp	r2, #254	; 0xfe
 80b346c:	d251      	bcs.n	80b3512 <__addsf3+0x136>
 80b346e:	f1b1 4f00 	cmp.w	r1, #2147483648	; 0x80000000
 80b3472:	eb40 50c2 	adc.w	r0, r0, r2, lsl #23
 80b3476:	bf08      	it	eq
 80b3478:	f020 0001 	biceq.w	r0, r0, #1
 80b347c:	ea40 0003 	orr.w	r0, r0, r3
 80b3480:	4770      	bx	lr
 80b3482:	0049      	lsls	r1, r1, #1
 80b3484:	eb40 0000 	adc.w	r0, r0, r0
 80b3488:	f410 0f00 	tst.w	r0, #8388608	; 0x800000
 80b348c:	f1a2 0201 	sub.w	r2, r2, #1
 80b3490:	d1ed      	bne.n	80b346e <__addsf3+0x92>
 80b3492:	fab0 fc80 	clz	ip, r0
 80b3496:	f1ac 0c08 	sub.w	ip, ip, #8
 80b349a:	ebb2 020c 	subs.w	r2, r2, ip
 80b349e:	fa00 f00c 	lsl.w	r0, r0, ip
 80b34a2:	bfaa      	itet	ge
 80b34a4:	eb00 50c2 	addge.w	r0, r0, r2, lsl #23
 80b34a8:	4252      	neglt	r2, r2
 80b34aa:	4318      	orrge	r0, r3
 80b34ac:	bfbc      	itt	lt
 80b34ae:	40d0      	lsrlt	r0, r2
 80b34b0:	4318      	orrlt	r0, r3
 80b34b2:	4770      	bx	lr
 80b34b4:	f092 0f00 	teq	r2, #0
 80b34b8:	f481 0100 	eor.w	r1, r1, #8388608	; 0x800000
 80b34bc:	bf06      	itte	eq
 80b34be:	f480 0000 	eoreq.w	r0, r0, #8388608	; 0x800000
 80b34c2:	3201      	addeq	r2, #1
 80b34c4:	3b01      	subne	r3, #1
 80b34c6:	e7b5      	b.n	80b3434 <__addsf3+0x58>
 80b34c8:	ea4f 0341 	mov.w	r3, r1, lsl #1
 80b34cc:	ea7f 6c22 	mvns.w	ip, r2, asr #24
 80b34d0:	bf18      	it	ne
 80b34d2:	ea7f 6c23 	mvnsne.w	ip, r3, asr #24
 80b34d6:	d021      	beq.n	80b351c <__addsf3+0x140>
 80b34d8:	ea92 0f03 	teq	r2, r3
 80b34dc:	d004      	beq.n	80b34e8 <__addsf3+0x10c>
 80b34de:	f092 0f00 	teq	r2, #0
 80b34e2:	bf08      	it	eq
 80b34e4:	4608      	moveq	r0, r1
 80b34e6:	4770      	bx	lr
 80b34e8:	ea90 0f01 	teq	r0, r1
 80b34ec:	bf1c      	itt	ne
 80b34ee:	2000      	movne	r0, #0
 80b34f0:	4770      	bxne	lr
 80b34f2:	f012 4f7f 	tst.w	r2, #4278190080	; 0xff000000
 80b34f6:	d104      	bne.n	80b3502 <__addsf3+0x126>
 80b34f8:	0040      	lsls	r0, r0, #1
 80b34fa:	bf28      	it	cs
 80b34fc:	f040 4000 	orrcs.w	r0, r0, #2147483648	; 0x80000000
 80b3500:	4770      	bx	lr
 80b3502:	f112 7200 	adds.w	r2, r2, #33554432	; 0x2000000
 80b3506:	bf3c      	itt	cc
 80b3508:	f500 0000 	addcc.w	r0, r0, #8388608	; 0x800000
 80b350c:	4770      	bxcc	lr
 80b350e:	f000 4300 	and.w	r3, r0, #2147483648	; 0x80000000
 80b3512:	f043 40fe 	orr.w	r0, r3, #2130706432	; 0x7f000000
 80b3516:	f440 0000 	orr.w	r0, r0, #8388608	; 0x800000
 80b351a:	4770      	bx	lr
 80b351c:	ea7f 6222 	mvns.w	r2, r2, asr #24
 80b3520:	bf16      	itet	ne
 80b3522:	4608      	movne	r0, r1
 80b3524:	ea7f 6323 	mvnseq.w	r3, r3, asr #24
 80b3528:	4601      	movne	r1, r0
 80b352a:	0242      	lsls	r2, r0, #9
 80b352c:	bf06      	itte	eq
 80b352e:	ea5f 2341 	movseq.w	r3, r1, lsl #9
 80b3532:	ea90 0f01 	teqeq	r0, r1
 80b3536:	f440 0080 	orrne.w	r0, r0, #4194304	; 0x400000
 80b353a:	4770      	bx	lr

080b353c <__aeabi_ui2f>:
 80b353c:	f04f 0300 	mov.w	r3, #0
 80b3540:	e004      	b.n	80b354c <__aeabi_i2f+0x8>
 80b3542:	bf00      	nop

080b3544 <__aeabi_i2f>:
 80b3544:	f010 4300 	ands.w	r3, r0, #2147483648	; 0x80000000
 80b3548:	bf48      	it	mi
 80b354a:	4240      	negmi	r0, r0
 80b354c:	ea5f 0c00 	movs.w	ip, r0
 80b3550:	bf08      	it	eq
 80b3552:	4770      	bxeq	lr
 80b3554:	f043 4396 	orr.w	r3, r3, #1258291200	; 0x4b000000
 80b3558:	4601      	mov	r1, r0
 80b355a:	f04f 0000 	mov.w	r0, #0
 80b355e:	e01c      	b.n	80b359a <__aeabi_l2f+0x2a>

080b3560 <__aeabi_ul2f>:
 80b3560:	ea50 0201 	orrs.w	r2, r0, r1
 80b3564:	bf08      	it	eq
 80b3566:	4770      	bxeq	lr
 80b3568:	f04f 0300 	mov.w	r3, #0
 80b356c:	e00a      	b.n	80b3584 <__aeabi_l2f+0x14>
 80b356e:	bf00      	nop

080b3570 <__aeabi_l2f>:
 80b3570:	ea50 0201 	orrs.w	r2, r0, r1
 80b3574:	bf08      	it	eq
 80b3576:	4770      	bxeq	lr
 80b3578:	f011 4300 	ands.w	r3, r1, #2147483648	; 0x80000000
 80b357c:	d502      	bpl.n	80b3584 <__aeabi_l2f+0x14>
 80b357e:	4240      	negs	r0, r0
 80b3580:	eb61 0141 	sbc.w	r1, r1, r1, lsl #1
 80b3584:	ea5f 0c01 	movs.w	ip, r1
 80b3588:	bf02      	ittt	eq
 80b358a:	4684      	moveq	ip, r0
 80b358c:	4601      	moveq	r1, r0
 80b358e:	2000      	moveq	r0, #0
 80b3590:	f043 43b6 	orr.w	r3, r3, #1526726656	; 0x5b000000
 80b3594:	bf08      	it	eq
 80b3596:	f1a3 5380 	subeq.w	r3, r3, #268435456	; 0x10000000
 80b359a:	f5a3 0300 	sub.w	r3, r3, #8388608	; 0x800000
 80b359e:	fabc f28c 	clz	r2, ip
 80b35a2:	3a08      	subs	r2, #8
 80b35a4:	eba3 53c2 	sub.w	r3, r3, r2, lsl #23
 80b35a8:	db10      	blt.n	80b35cc <__aeabi_l2f+0x5c>
 80b35aa:	fa01 fc02 	lsl.w	ip, r1, r2
 80b35ae:	4463      	add	r3, ip
 80b35b0:	fa00 fc02 	lsl.w	ip, r0, r2
 80b35b4:	f1c2 0220 	rsb	r2, r2, #32
 80b35b8:	f1bc 4f00 	cmp.w	ip, #2147483648	; 0x80000000
 80b35bc:	fa20 f202 	lsr.w	r2, r0, r2
 80b35c0:	eb43 0002 	adc.w	r0, r3, r2
 80b35c4:	bf08      	it	eq
 80b35c6:	f020 0001 	biceq.w	r0, r0, #1
 80b35ca:	4770      	bx	lr
 80b35cc:	f102 0220 	add.w	r2, r2, #32
 80b35d0:	fa01 fc02 	lsl.w	ip, r1, r2
 80b35d4:	f1c2 0220 	rsb	r2, r2, #32
 80b35d8:	ea50 004c 	orrs.w	r0, r0, ip, lsl #1
 80b35dc:	fa21 f202 	lsr.w	r2, r1, r2
 80b35e0:	eb43 0002 	adc.w	r0, r3, r2
 80b35e4:	bf08      	it	eq
 80b35e6:	ea20 70dc 	biceq.w	r0, r0, ip, lsr #31
 80b35ea:	4770      	bx	lr

080b35ec <__aeabi_fmul>:
 80b35ec:	f04f 0cff 	mov.w	ip, #255	; 0xff
 80b35f0:	ea1c 52d0 	ands.w	r2, ip, r0, lsr #23
 80b35f4:	bf1e      	ittt	ne
 80b35f6:	ea1c 53d1 	andsne.w	r3, ip, r1, lsr #23
 80b35fa:	ea92 0f0c 	teqne	r2, ip
 80b35fe:	ea93 0f0c 	teqne	r3, ip
 80b3602:	d06f      	beq.n	80b36e4 <__aeabi_fmul+0xf8>
 80b3604:	441a      	add	r2, r3
 80b3606:	ea80 0c01 	eor.w	ip, r0, r1
 80b360a:	0240      	lsls	r0, r0, #9
 80b360c:	bf18      	it	ne
 80b360e:	ea5f 2141 	movsne.w	r1, r1, lsl #9
 80b3612:	d01e      	beq.n	80b3652 <__aeabi_fmul+0x66>
 80b3614:	f04f 6300 	mov.w	r3, #134217728	; 0x8000000
 80b3618:	ea43 1050 	orr.w	r0, r3, r0, lsr #5
 80b361c:	ea43 1151 	orr.w	r1, r3, r1, lsr #5
 80b3620:	fba0 3101 	umull	r3, r1, r0, r1
 80b3624:	f00c 4000 	and.w	r0, ip, #2147483648	; 0x80000000
 80b3628:	f5b1 0f00 	cmp.w	r1, #8388608	; 0x800000
 80b362c:	bf3e      	ittt	cc
 80b362e:	0049      	lslcc	r1, r1, #1
 80b3630:	ea41 71d3 	orrcc.w	r1, r1, r3, lsr #31
 80b3634:	005b      	lslcc	r3, r3, #1
 80b3636:	ea40 0001 	orr.w	r0, r0, r1
 80b363a:	f162 027f 	sbc.w	r2, r2, #127	; 0x7f
 80b363e:	2afd      	cmp	r2, #253	; 0xfd
 80b3640:	d81d      	bhi.n	80b367e <__aeabi_fmul+0x92>
 80b3642:	f1b3 4f00 	cmp.w	r3, #2147483648	; 0x80000000
 80b3646:	eb40 50c2 	adc.w	r0, r0, r2, lsl #23
 80b364a:	bf08      	it	eq
 80b364c:	f020 0001 	biceq.w	r0, r0, #1
 80b3650:	4770      	bx	lr
 80b3652:	f090 0f00 	teq	r0, #0
 80b3656:	f00c 4c00 	and.w	ip, ip, #2147483648	; 0x80000000
 80b365a:	bf08      	it	eq
 80b365c:	0249      	lsleq	r1, r1, #9
 80b365e:	ea4c 2050 	orr.w	r0, ip, r0, lsr #9
 80b3662:	ea40 2051 	orr.w	r0, r0, r1, lsr #9
 80b3666:	3a7f      	subs	r2, #127	; 0x7f
 80b3668:	bfc2      	ittt	gt
 80b366a:	f1d2 03ff 	rsbsgt	r3, r2, #255	; 0xff
 80b366e:	ea40 50c2 	orrgt.w	r0, r0, r2, lsl #23
 80b3672:	4770      	bxgt	lr
 80b3674:	f440 0000 	orr.w	r0, r0, #8388608	; 0x800000
 80b3678:	f04f 0300 	mov.w	r3, #0
 80b367c:	3a01      	subs	r2, #1
 80b367e:	dc5d      	bgt.n	80b373c <__aeabi_fmul+0x150>
 80b3680:	f112 0f19 	cmn.w	r2, #25
 80b3684:	bfdc      	itt	le
 80b3686:	f000 4000 	andle.w	r0, r0, #2147483648	; 0x80000000
 80b368a:	4770      	bxle	lr
 80b368c:	f1c2 0200 	rsb	r2, r2, #0
 80b3690:	0041      	lsls	r1, r0, #1
 80b3692:	fa21 f102 	lsr.w	r1, r1, r2
 80b3696:	f1c2 0220 	rsb	r2, r2, #32
 80b369a:	fa00 fc02 	lsl.w	ip, r0, r2
 80b369e:	ea5f 0031 	movs.w	r0, r1, rrx
 80b36a2:	f140 0000 	adc.w	r0, r0, #0
 80b36a6:	ea53 034c 	orrs.w	r3, r3, ip, lsl #1
 80b36aa:	bf08      	it	eq
 80b36ac:	ea20 70dc 	biceq.w	r0, r0, ip, lsr #31
 80b36b0:	4770      	bx	lr
 80b36b2:	f092 0f00 	teq	r2, #0
 80b36b6:	f000 4c00 	and.w	ip, r0, #2147483648	; 0x80000000
 80b36ba:	bf02      	ittt	eq
 80b36bc:	0040      	lsleq	r0, r0, #1
 80b36be:	f410 0f00 	tsteq.w	r0, #8388608	; 0x800000
 80b36c2:	3a01      	subeq	r2, #1
 80b36c4:	d0f9      	beq.n	80b36ba <__aeabi_fmul+0xce>
 80b36c6:	ea40 000c 	orr.w	r0, r0, ip
 80b36ca:	f093 0f00 	teq	r3, #0
 80b36ce:	f001 4c00 	and.w	ip, r1, #2147483648	; 0x80000000
 80b36d2:	bf02      	ittt	eq
 80b36d4:	0049      	lsleq	r1, r1, #1
 80b36d6:	f411 0f00 	tsteq.w	r1, #8388608	; 0x800000
 80b36da:	3b01      	subeq	r3, #1
 80b36dc:	d0f9      	beq.n	80b36d2 <__aeabi_fmul+0xe6>
 80b36de:	ea41 010c 	orr.w	r1, r1, ip
 80b36e2:	e78f      	b.n	80b3604 <__aeabi_fmul+0x18>
 80b36e4:	ea0c 53d1 	and.w	r3, ip, r1, lsr #23
 80b36e8:	ea92 0f0c 	teq	r2, ip
 80b36ec:	bf18      	it	ne
 80b36ee:	ea93 0f0c 	teqne	r3, ip
 80b36f2:	d00a      	beq.n	80b370a <__aeabi_fmul+0x11e>
 80b36f4:	f030 4c00 	bics.w	ip, r0, #2147483648	; 0x80000000
 80b36f8:	bf18      	it	ne
 80b36fa:	f031 4c00 	bicsne.w	ip, r1, #2147483648	; 0x80000000
 80b36fe:	d1d8      	bne.n	80b36b2 <__aeabi_fmul+0xc6>
 80b3700:	ea80 0001 	eor.w	r0, r0, r1
 80b3704:	f000 4000 	and.w	r0, r0, #2147483648	; 0x80000000
 80b3708:	4770      	bx	lr
 80b370a:	f090 0f00 	teq	r0, #0
 80b370e:	bf17      	itett	ne
 80b3710:	f090 4f00 	teqne	r0, #2147483648	; 0x80000000
 80b3714:	4608      	moveq	r0, r1
 80b3716:	f091 0f00 	teqne	r1, #0
 80b371a:	f091 4f00 	teqne	r1, #2147483648	; 0x80000000
 80b371e:	d014      	beq.n	80b374a <__aeabi_fmul+0x15e>
 80b3720:	ea92 0f0c 	teq	r2, ip
 80b3724:	d101      	bne.n	80b372a <__aeabi_fmul+0x13e>
 80b3726:	0242      	lsls	r2, r0, #9
 80b3728:	d10f      	bne.n	80b374a <__aeabi_fmul+0x15e>
 80b372a:	ea93 0f0c 	teq	r3, ip
 80b372e:	d103      	bne.n	80b3738 <__aeabi_fmul+0x14c>
 80b3730:	024b      	lsls	r3, r1, #9
 80b3732:	bf18      	it	ne
 80b3734:	4608      	movne	r0, r1
 80b3736:	d108      	bne.n	80b374a <__aeabi_fmul+0x15e>
 80b3738:	ea80 0001 	eor.w	r0, r0, r1
 80b373c:	f000 4000 	and.w	r0, r0, #2147483648	; 0x80000000
 80b3740:	f040 40fe 	orr.w	r0, r0, #2130706432	; 0x7f000000
 80b3744:	f440 0000 	orr.w	r0, r0, #8388608	; 0x800000
 80b3748:	4770      	bx	lr
 80b374a:	f040 40fe 	orr.w	r0, r0, #2130706432	; 0x7f000000
 80b374e:	f440 0040 	orr.w	r0, r0, #12582912	; 0xc00000
 80b3752:	4770      	bx	lr

080b3754 <__aeabi_fdiv>:
 80b3754:	f04f 0cff 	mov.w	ip, #255	; 0xff
 80b3758:	ea1c 52d0 	ands.w	r2, ip, r0, lsr #23
 80b375c:	bf1e      	ittt	ne
 80b375e:	ea1c 53d1 	andsne.w	r3, ip, r1, lsr #23
 80b3762:	ea92 0f0c 	teqne	r2, ip
 80b3766:	ea93 0f0c 	teqne	r3, ip
 80b376a:	d069      	beq.n	80b3840 <__aeabi_fdiv+0xec>
 80b376c:	eba2 0203 	sub.w	r2, r2, r3
 80b3770:	ea80 0c01 	eor.w	ip, r0, r1
 80b3774:	0249      	lsls	r1, r1, #9
 80b3776:	ea4f 2040 	mov.w	r0, r0, lsl #9
 80b377a:	d037      	beq.n	80b37ec <__aeabi_fdiv+0x98>
 80b377c:	f04f 5380 	mov.w	r3, #268435456	; 0x10000000
 80b3780:	ea43 1111 	orr.w	r1, r3, r1, lsr #4
 80b3784:	ea43 1310 	orr.w	r3, r3, r0, lsr #4
 80b3788:	f00c 4000 	and.w	r0, ip, #2147483648	; 0x80000000
 80b378c:	428b      	cmp	r3, r1
 80b378e:	bf38      	it	cc
 80b3790:	005b      	lslcc	r3, r3, #1
 80b3792:	f142 027d 	adc.w	r2, r2, #125	; 0x7d
 80b3796:	f44f 0c00 	mov.w	ip, #8388608	; 0x800000
 80b379a:	428b      	cmp	r3, r1
 80b379c:	bf24      	itt	cs
 80b379e:	1a5b      	subcs	r3, r3, r1
 80b37a0:	ea40 000c 	orrcs.w	r0, r0, ip
 80b37a4:	ebb3 0f51 	cmp.w	r3, r1, lsr #1
 80b37a8:	bf24      	itt	cs
 80b37aa:	eba3 0351 	subcs.w	r3, r3, r1, lsr #1
 80b37ae:	ea40 005c 	orrcs.w	r0, r0, ip, lsr #1
 80b37b2:	ebb3 0f91 	cmp.w	r3, r1, lsr #2
 80b37b6:	bf24      	itt	cs
 80b37b8:	eba3 0391 	subcs.w	r3, r3, r1, lsr #2
 80b37bc:	ea40 009c 	orrcs.w	r0, r0, ip, lsr #2
 80b37c0:	ebb3 0fd1 	cmp.w	r3, r1, lsr #3
 80b37c4:	bf24      	itt	cs
 80b37c6:	eba3 03d1 	subcs.w	r3, r3, r1, lsr #3
 80b37ca:	ea40 00dc 	orrcs.w	r0, r0, ip, lsr #3
 80b37ce:	011b      	lsls	r3, r3, #4
 80b37d0:	bf18      	it	ne
 80b37d2:	ea5f 1c1c 	movsne.w	ip, ip, lsr #4
 80b37d6:	d1e0      	bne.n	80b379a <__aeabi_fdiv+0x46>
 80b37d8:	2afd      	cmp	r2, #253	; 0xfd
 80b37da:	f63f af50 	bhi.w	80b367e <__aeabi_fmul+0x92>
 80b37de:	428b      	cmp	r3, r1
 80b37e0:	eb40 50c2 	adc.w	r0, r0, r2, lsl #23
 80b37e4:	bf08      	it	eq
 80b37e6:	f020 0001 	biceq.w	r0, r0, #1
 80b37ea:	4770      	bx	lr
 80b37ec:	f00c 4c00 	and.w	ip, ip, #2147483648	; 0x80000000
 80b37f0:	ea4c 2050 	orr.w	r0, ip, r0, lsr #9
 80b37f4:	327f      	adds	r2, #127	; 0x7f
 80b37f6:	bfc2      	ittt	gt
 80b37f8:	f1d2 03ff 	rsbsgt	r3, r2, #255	; 0xff
 80b37fc:	ea40 50c2 	orrgt.w	r0, r0, r2, lsl #23
 80b3800:	4770      	bxgt	lr
 80b3802:	f440 0000 	orr.w	r0, r0, #8388608	; 0x800000
 80b3806:	f04f 0300 	mov.w	r3, #0
 80b380a:	3a01      	subs	r2, #1
 80b380c:	e737      	b.n	80b367e <__aeabi_fmul+0x92>
 80b380e:	f092 0f00 	teq	r2, #0
 80b3812:	f000 4c00 	and.w	ip, r0, #2147483648	; 0x80000000
 80b3816:	bf02      	ittt	eq
 80b3818:	0040      	lsleq	r0, r0, #1
 80b381a:	f410 0f00 	tsteq.w	r0, #8388608	; 0x800000
 80b381e:	3a01      	subeq	r2, #1
 80b3820:	d0f9      	beq.n	80b3816 <__aeabi_fdiv+0xc2>
 80b3822:	ea40 000c 	orr.w	r0, r0, ip
 80b3826:	f093 0f00 	teq	r3, #0
 80b382a:	f001 4c00 	and.w	ip, r1, #2147483648	; 0x80000000
 80b382e:	bf02      	ittt	eq
 80b3830:	0049      	lsleq	r1, r1, #1
 80b3832:	f411 0f00 	tsteq.w	r1, #8388608	; 0x800000
 80b3836:	3b01      	subeq	r3, #1
 80b3838:	d0f9      	beq.n	80b382e <__aeabi_fdiv+0xda>
 80b383a:	ea41 010c 	orr.w	r1, r1, ip
 80b383e:	e795      	b.n	80b376c <__aeabi_fdiv+0x18>
 80b3840:	ea0c 53d1 	and.w	r3, ip, r1, lsr #23
 80b3844:	ea92 0f0c 	teq	r2, ip
 80b3848:	d108      	bne.n	80b385c <__aeabi_fdiv+0x108>
 80b384a:	0242      	lsls	r2, r0, #9
 80b384c:	f47f af7d 	bne.w	80b374a <__aeabi_fmul+0x15e>
 80b3850:	ea93 0f0c 	teq	r3, ip
 80b3854:	f47f af70 	bne.w	80b3738 <__aeabi_fmul+0x14c>
 80b3858:	4608      	mov	r0, r1
 80b385a:	e776      	b.n	80b374a <__aeabi_fmul+0x15e>
 80b385c:	ea93 0f0c 	teq	r3, ip
 80b3860:	d104      	bne.n	80b386c <__aeabi_fdiv+0x118>
 80b3862:	024b      	lsls	r3, r1, #9
 80b3864:	f43f af4c 	beq.w	80b3700 <__aeabi_fmul+0x114>
 80b3868:	4608      	mov	r0, r1
 80b386a:	e76e      	b.n	80b374a <__aeabi_fmul+0x15e>
 80b386c:	f030 4c00 	bics.w	ip, r0, #2147483648	; 0x80000000
 80b3870:	bf18      	it	ne
 80b3872:	f031 4c00 	bicsne.w	ip, r1, #2147483648	; 0x80000000
 80b3876:	d1ca      	bne.n	80b380e <__aeabi_fdiv+0xba>
 80b3878:	f030 4200 	bics.w	r2, r0, #2147483648	; 0x80000000
 80b387c:	f47f af5c 	bne.w	80b3738 <__aeabi_fmul+0x14c>
 80b3880:	f031 4300 	bics.w	r3, r1, #2147483648	; 0x80000000
 80b3884:	f47f af3c 	bne.w	80b3700 <__aeabi_fmul+0x114>
 80b3888:	e75f      	b.n	80b374a <__aeabi_fmul+0x15e>
 80b388a:	bf00      	nop

080b388c <__gesf2>:
 80b388c:	f04f 3cff 	mov.w	ip, #4294967295	; 0xffffffff
 80b3890:	e006      	b.n	80b38a0 <__cmpsf2+0x4>
 80b3892:	bf00      	nop

080b3894 <__lesf2>:
 80b3894:	f04f 0c01 	mov.w	ip, #1
 80b3898:	e002      	b.n	80b38a0 <__cmpsf2+0x4>
 80b389a:	bf00      	nop

080b389c <__cmpsf2>:
 80b389c:	f04f 0c01 	mov.w	ip, #1
 80b38a0:	f84d cd04 	str.w	ip, [sp, #-4]!
 80b38a4:	ea4f 0240 	mov.w	r2, r0, lsl #1
 80b38a8:	ea4f 0341 	mov.w	r3, r1, lsl #1
 80b38ac:	ea7f 6c22 	mvns.w	ip, r2, asr #24
 80b38b0:	bf18      	it	ne
 80b38b2:	ea7f 6c23 	mvnsne.w	ip, r3, asr #24
 80b38b6:	d011      	beq.n	80b38dc <__cmpsf2+0x40>
 80b38b8:	b001      	add	sp, #4
 80b38ba:	ea52 0c53 	orrs.w	ip, r2, r3, lsr #1
 80b38be:	bf18      	it	ne
 80b38c0:	ea90 0f01 	teqne	r0, r1
 80b38c4:	bf58      	it	pl
 80b38c6:	ebb2 0003 	subspl.w	r0, r2, r3
 80b38ca:	bf88      	it	hi
 80b38cc:	17c8      	asrhi	r0, r1, #31
 80b38ce:	bf38      	it	cc
 80b38d0:	ea6f 70e1 	mvncc.w	r0, r1, asr #31
 80b38d4:	bf18      	it	ne
 80b38d6:	f040 0001 	orrne.w	r0, r0, #1
 80b38da:	4770      	bx	lr
 80b38dc:	ea7f 6c22 	mvns.w	ip, r2, asr #24
 80b38e0:	d102      	bne.n	80b38e8 <__cmpsf2+0x4c>
 80b38e2:	ea5f 2c40 	movs.w	ip, r0, lsl #9
 80b38e6:	d105      	bne.n	80b38f4 <__cmpsf2+0x58>
 80b38e8:	ea7f 6c23 	mvns.w	ip, r3, asr #24
 80b38ec:	d1e4      	bne.n	80b38b8 <__cmpsf2+0x1c>
 80b38ee:	ea5f 2c41 	movs.w	ip, r1, lsl #9
 80b38f2:	d0e1      	beq.n	80b38b8 <__cmpsf2+0x1c>
 80b38f4:	f85d 0b04 	ldr.w	r0, [sp], #4
 80b38f8:	4770      	bx	lr
 80b38fa:	bf00      	nop

080b38fc <__aeabi_cfrcmple>:
 80b38fc:	4684      	mov	ip, r0
 80b38fe:	4608      	mov	r0, r1
 80b3900:	4661      	mov	r1, ip
 80b3902:	e7ff      	b.n	80b3904 <__aeabi_cfcmpeq>

080b3904 <__aeabi_cfcmpeq>:
 80b3904:	b50f      	push	{r0, r1, r2, r3, lr}
 80b3906:	f7ff ffc9 	bl	80b389c <__cmpsf2>
 80b390a:	2800      	cmp	r0, #0
 80b390c:	bf48      	it	mi
 80b390e:	f110 0f00 	cmnmi.w	r0, #0
 80b3912:	bd0f      	pop	{r0, r1, r2, r3, pc}

080b3914 <__aeabi_fcmpeq>:
 80b3914:	f84d ed08 	str.w	lr, [sp, #-8]!
 80b3918:	f7ff fff4 	bl	80b3904 <__aeabi_cfcmpeq>
 80b391c:	bf0c      	ite	eq
 80b391e:	2001      	moveq	r0, #1
 80b3920:	2000      	movne	r0, #0
 80b3922:	f85d fb08 	ldr.w	pc, [sp], #8
 80b3926:	bf00      	nop

080b3928 <__aeabi_fcmplt>:
 80b3928:	f84d ed08 	str.w	lr, [sp, #-8]!
 80b392c:	f7ff ffea 	bl	80b3904 <__aeabi_cfcmpeq>
 80b3930:	bf34      	ite	cc
 80b3932:	2001      	movcc	r0, #1
 80b3934:	2000      	movcs	r0, #0
 80b3936:	f85d fb08 	ldr.w	pc, [sp], #8
 80b393a:	bf00      	nop

080b393c <__aeabi_fcmple>:
 80b393c:	f84d ed08 	str.w	lr, [sp, #-8]!
 80b3940:	f7ff ffe0 	bl	80b3904 <__aeabi_cfcmpeq>
 80b3944:	bf94      	ite	ls
 80b3946:	2001      	movls	r0, #1
 80b3948:	2000      	movhi	r0, #0
 80b394a:	f85d fb08 	ldr.w	pc, [sp], #8
 80b394e:	bf00      	nop

080b3950 <__aeabi_fcmpge>:
 80b3950:	f84d ed08 	str.w	lr, [sp, #-8]!
 80b3954:	f7ff ffd2 	bl	80b38fc <__aeabi_cfrcmple>
 80b3958:	bf94      	ite	ls
 80b395a:	2001      	movls	r0, #1
 80b395c:	2000      	movhi	r0, #0
 80b395e:	f85d fb08 	ldr.w	pc, [sp], #8
 80b3962:	bf00      	nop

080b3964 <__aeabi_fcmpgt>:
 80b3964:	f84d ed08 	str.w	lr, [sp, #-8]!
 80b3968:	f7ff ffc8 	bl	80b38fc <__aeabi_cfrcmple>
 80b396c:	bf34      	ite	cc
 80b396e:	2001      	movcc	r0, #1
 80b3970:	2000      	movcs	r0, #0
 80b3972:	f85d fb08 	ldr.w	pc, [sp], #8
 80b3976:	bf00      	nop

080b3978 <__aeabi_fcmpun>:
 80b3978:	ea4f 0240 	mov.w	r2, r0, lsl #1
 80b397c:	ea4f 0341 	mov.w	r3, r1, lsl #1
 80b3980:	ea7f 6c22 	mvns.w	ip, r2, asr #24
 80b3984:	d102      	bne.n	80b398c <__aeabi_fcmpun+0x14>
 80b3986:	ea5f 2c40 	movs.w	ip, r0, lsl #9
 80b398a:	d108      	bne.n	80b399e <__aeabi_fcmpun+0x26>
 80b398c:	ea7f 6c23 	mvns.w	ip, r3, asr #24
 80b3990:	d102      	bne.n	80b3998 <__aeabi_fcmpun+0x20>
 80b3992:	ea5f 2c41 	movs.w	ip, r1, lsl #9
 80b3996:	d102      	bne.n	80b399e <__aeabi_fcmpun+0x26>
 80b3998:	f04f 0000 	mov.w	r0, #0
 80b399c:	4770      	bx	lr
 80b399e:	f04f 0001 	mov.w	r0, #1
 80b39a2:	4770      	bx	lr

080b39a4 <__aeabi_f2iz>:
 80b39a4:	ea4f 0240 	mov.w	r2, r0, lsl #1
 80b39a8:	f1b2 4ffe 	cmp.w	r2, #2130706432	; 0x7f000000
 80b39ac:	d30f      	bcc.n	80b39ce <__aeabi_f2iz+0x2a>
 80b39ae:	f04f 039e 	mov.w	r3, #158	; 0x9e
 80b39b2:	ebb3 6212 	subs.w	r2, r3, r2, lsr #24
 80b39b6:	d90d      	bls.n	80b39d4 <__aeabi_f2iz+0x30>
 80b39b8:	ea4f 2300 	mov.w	r3, r0, lsl #8
 80b39bc:	f043 4300 	orr.w	r3, r3, #2147483648	; 0x80000000
 80b39c0:	f010 4f00 	tst.w	r0, #2147483648	; 0x80000000
 80b39c4:	fa23 f002 	lsr.w	r0, r3, r2
 80b39c8:	bf18      	it	ne
 80b39ca:	4240      	negne	r0, r0
 80b39cc:	4770      	bx	lr
 80b39ce:	f04f 0000 	mov.w	r0, #0
 80b39d2:	4770      	bx	lr
 80b39d4:	f112 0f61 	cmn.w	r2, #97	; 0x61
 80b39d8:	d101      	bne.n	80b39de <__aeabi_f2iz+0x3a>
 80b39da:	0242      	lsls	r2, r0, #9
 80b39dc:	d105      	bne.n	80b39ea <__aeabi_f2iz+0x46>
 80b39de:	f010 4000 	ands.w	r0, r0, #2147483648	; 0x80000000
 80b39e2:	bf08      	it	eq
 80b39e4:	f06f 4000 	mvneq.w	r0, #2147483648	; 0x80000000
 80b39e8:	4770      	bx	lr
 80b39ea:	f04f 0000 	mov.w	r0, #0
 80b39ee:	4770      	bx	lr

080b39f0 <__aeabi_d2lz>:
 80b39f0:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
 80b39f2:	2200      	movs	r2, #0
 80b39f4:	2300      	movs	r3, #0
 80b39f6:	4607      	mov	r7, r0
 80b39f8:	460e      	mov	r6, r1
 80b39fa:	f7ff fc29 	bl	80b3250 <__aeabi_dcmplt>
 80b39fe:	b928      	cbnz	r0, 80b3a0c <__aeabi_d2lz+0x1c>
 80b3a00:	4638      	mov	r0, r7
 80b3a02:	4631      	mov	r1, r6
 80b3a04:	e8bd 40f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, lr}
 80b3a08:	f000 b80a 	b.w	80b3a20 <__aeabi_d2ulz>
 80b3a0c:	4638      	mov	r0, r7
 80b3a0e:	f106 4100 	add.w	r1, r6, #2147483648	; 0x80000000
 80b3a12:	f000 f805 	bl	80b3a20 <__aeabi_d2ulz>
 80b3a16:	4240      	negs	r0, r0
 80b3a18:	eb61 0141 	sbc.w	r1, r1, r1, lsl #1
 80b3a1c:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
 80b3a1e:	bf00      	nop

080b3a20 <__aeabi_d2ulz>:
 80b3a20:	b5d0      	push	{r4, r6, r7, lr}
 80b3a22:	2200      	movs	r2, #0
 80b3a24:	4b0e      	ldr	r3, [pc, #56]	; (80b3a60 <__aeabi_d2ulz+0x40>)
 80b3a26:	4606      	mov	r6, r0
 80b3a28:	460f      	mov	r7, r1
 80b3a2a:	f7ff f99f 	bl	80b2d6c <__aeabi_dmul>
 80b3a2e:	f7ff fc5f 	bl	80b32f0 <__aeabi_d2uiz>
 80b3a32:	4604      	mov	r4, r0
 80b3a34:	f7ff f924 	bl	80b2c80 <__aeabi_ui2d>
 80b3a38:	2200      	movs	r2, #0
 80b3a3a:	4b0a      	ldr	r3, [pc, #40]	; (80b3a64 <__aeabi_d2ulz+0x44>)
 80b3a3c:	f7ff f996 	bl	80b2d6c <__aeabi_dmul>
 80b3a40:	4602      	mov	r2, r0
 80b3a42:	460b      	mov	r3, r1
 80b3a44:	4630      	mov	r0, r6
 80b3a46:	4639      	mov	r1, r7
 80b3a48:	f7fe ffdc 	bl	80b2a04 <__aeabi_dsub>
 80b3a4c:	f7ff fc50 	bl	80b32f0 <__aeabi_d2uiz>
 80b3a50:	4623      	mov	r3, r4
 80b3a52:	2200      	movs	r2, #0
 80b3a54:	ea42 0200 	orr.w	r2, r2, r0
 80b3a58:	4610      	mov	r0, r2
 80b3a5a:	4619      	mov	r1, r3
 80b3a5c:	bdd0      	pop	{r4, r6, r7, pc}
 80b3a5e:	bf00      	nop
 80b3a60:	3df00000 	.word	0x3df00000
 80b3a64:	41f00000 	.word	0x41f00000

080b3a68 <__cxa_atexit>:
 80b3a68:	b510      	push	{r4, lr}
 80b3a6a:	4c05      	ldr	r4, [pc, #20]	; (80b3a80 <__cxa_atexit+0x18>)
 80b3a6c:	4613      	mov	r3, r2
 80b3a6e:	b12c      	cbz	r4, 80b3a7c <__cxa_atexit+0x14>
 80b3a70:	460a      	mov	r2, r1
 80b3a72:	4601      	mov	r1, r0
 80b3a74:	2002      	movs	r0, #2
 80b3a76:	f3af 8000 	nop.w
 80b3a7a:	bd10      	pop	{r4, pc}
 80b3a7c:	4620      	mov	r0, r4
 80b3a7e:	bd10      	pop	{r4, pc}
 80b3a80:	00000000 	.word	0x00000000

080b3a84 <exit>:
 80b3a84:	b508      	push	{r3, lr}
 80b3a86:	4b07      	ldr	r3, [pc, #28]	; (80b3aa4 <exit+0x20>)
 80b3a88:	4604      	mov	r4, r0
 80b3a8a:	b113      	cbz	r3, 80b3a92 <exit+0xe>
 80b3a8c:	2100      	movs	r1, #0
 80b3a8e:	f3af 8000 	nop.w
 80b3a92:	4b05      	ldr	r3, [pc, #20]	; (80b3aa8 <exit+0x24>)
 80b3a94:	6818      	ldr	r0, [r3, #0]
 80b3a96:	6a83      	ldr	r3, [r0, #40]	; 0x28
 80b3a98:	b103      	cbz	r3, 80b3a9c <exit+0x18>
 80b3a9a:	4798      	blx	r3
 80b3a9c:	4620      	mov	r0, r4
 80b3a9e:	f7ec fb05 	bl	80a00ac <_exit>
 80b3aa2:	bf00      	nop
 80b3aa4:	00000000 	.word	0x00000000
 80b3aa8:	080b7c58 	.word	0x080b7c58

080b3aac <memcmp>:
 80b3aac:	b510      	push	{r4, lr}
 80b3aae:	3901      	subs	r1, #1
 80b3ab0:	4402      	add	r2, r0
 80b3ab2:	4290      	cmp	r0, r2
 80b3ab4:	d007      	beq.n	80b3ac6 <memcmp+0x1a>
 80b3ab6:	f810 3b01 	ldrb.w	r3, [r0], #1
 80b3aba:	f811 4f01 	ldrb.w	r4, [r1, #1]!
 80b3abe:	42a3      	cmp	r3, r4
 80b3ac0:	d0f7      	beq.n	80b3ab2 <memcmp+0x6>
 80b3ac2:	1b18      	subs	r0, r3, r4
 80b3ac4:	bd10      	pop	{r4, pc}
 80b3ac6:	2000      	movs	r0, #0
 80b3ac8:	bd10      	pop	{r4, pc}

080b3aca <memcpy>:
 80b3aca:	b510      	push	{r4, lr}
 80b3acc:	1e43      	subs	r3, r0, #1
 80b3ace:	440a      	add	r2, r1
 80b3ad0:	4291      	cmp	r1, r2
 80b3ad2:	d004      	beq.n	80b3ade <memcpy+0x14>
 80b3ad4:	f811 4b01 	ldrb.w	r4, [r1], #1
 80b3ad8:	f803 4f01 	strb.w	r4, [r3, #1]!
 80b3adc:	e7f8      	b.n	80b3ad0 <memcpy+0x6>
 80b3ade:	bd10      	pop	{r4, pc}

080b3ae0 <memset>:
 80b3ae0:	4603      	mov	r3, r0
 80b3ae2:	4402      	add	r2, r0
 80b3ae4:	4293      	cmp	r3, r2
 80b3ae6:	d002      	beq.n	80b3aee <memset+0xe>
 80b3ae8:	f803 1b01 	strb.w	r1, [r3], #1
 80b3aec:	e7fa      	b.n	80b3ae4 <memset+0x4>
 80b3aee:	4770      	bx	lr

080b3af0 <srand>:
 80b3af0:	b538      	push	{r3, r4, r5, lr}
 80b3af2:	4b12      	ldr	r3, [pc, #72]	; (80b3b3c <srand+0x4c>)
 80b3af4:	4605      	mov	r5, r0
 80b3af6:	681c      	ldr	r4, [r3, #0]
 80b3af8:	6ba3      	ldr	r3, [r4, #56]	; 0x38
 80b3afa:	b9d3      	cbnz	r3, 80b3b32 <srand+0x42>
 80b3afc:	2018      	movs	r0, #24
 80b3afe:	f7fc fa87 	bl	80b0010 <malloc>
 80b3b02:	f243 330e 	movw	r3, #13070	; 0x330e
 80b3b06:	63a0      	str	r0, [r4, #56]	; 0x38
 80b3b08:	8003      	strh	r3, [r0, #0]
 80b3b0a:	f64a 33cd 	movw	r3, #43981	; 0xabcd
 80b3b0e:	8043      	strh	r3, [r0, #2]
 80b3b10:	f241 2334 	movw	r3, #4660	; 0x1234
 80b3b14:	8083      	strh	r3, [r0, #4]
 80b3b16:	f24e 636d 	movw	r3, #58989	; 0xe66d
 80b3b1a:	80c3      	strh	r3, [r0, #6]
 80b3b1c:	f64d 63ec 	movw	r3, #57068	; 0xdeec
 80b3b20:	8103      	strh	r3, [r0, #8]
 80b3b22:	2305      	movs	r3, #5
 80b3b24:	8143      	strh	r3, [r0, #10]
 80b3b26:	230b      	movs	r3, #11
 80b3b28:	8183      	strh	r3, [r0, #12]
 80b3b2a:	2201      	movs	r2, #1
 80b3b2c:	2300      	movs	r3, #0
 80b3b2e:	e9c0 2304 	strd	r2, r3, [r0, #16]
 80b3b32:	6ba3      	ldr	r3, [r4, #56]	; 0x38
 80b3b34:	2200      	movs	r2, #0
 80b3b36:	611d      	str	r5, [r3, #16]
 80b3b38:	615a      	str	r2, [r3, #20]
 80b3b3a:	bd38      	pop	{r3, r4, r5, pc}
 80b3b3c:	20000590 	.word	0x20000590

080b3b40 <strcmp>:
 80b3b40:	f810 2b01 	ldrb.w	r2, [r0], #1
 80b3b44:	f811 3b01 	ldrb.w	r3, [r1], #1
 80b3b48:	2a01      	cmp	r2, #1
 80b3b4a:	bf28      	it	cs
 80b3b4c:	429a      	cmpcs	r2, r3
 80b3b4e:	d0f7      	beq.n	80b3b40 <strcmp>
 80b3b50:	1ad0      	subs	r0, r2, r3
 80b3b52:	4770      	bx	lr

080b3b54 <strlen>:
 80b3b54:	4603      	mov	r3, r0
 80b3b56:	f813 2b01 	ldrb.w	r2, [r3], #1
 80b3b5a:	2a00      	cmp	r2, #0
 80b3b5c:	d1fb      	bne.n	80b3b56 <strlen+0x2>
 80b3b5e:	1a18      	subs	r0, r3, r0
 80b3b60:	3801      	subs	r0, #1
 80b3b62:	4770      	bx	lr

080b3b64 <dynalib_user>:
 80b3b64:	0021 080a 0061 080a 008d 080a 0091 080a     !...a...........
 80b3b74:	0000 0000 7325 203a 656c 676e 6874 253d     ....%s: length=%
 80b3b84:	2064 005b 6e55 6e6b 776f 206e 7974 6570     d [.Unknown type
 80b3b94:	4e00 544f 5059 0045 4c46 414f 3354 0032     .NOTYPE.FLOAT32.
 80b3ba4:	4e49 3354 0032 4955 544e 0038 4e49 3654     INT32.UINT8.INT6
 80b3bb4:	0034 5453 4952 474e 4200 4f4f 004c 4e49     4.STRING.BOOL.IN
 80b3bc4:	3154 0036 4f43 504d 454c 3658 0034 4c46     T16.COMPLEX64.FL
 80b3bd4:	414f 3154 0036 0000                         OAT16...

080b3bdc <CSWTCH.19>:
 80b3bdc:	3b95 080b 3b9c 080b 3ba4 080b 3baa 080b     .;...;...;...;..
 80b3bec:	3bb0 080b 3bb6 080b 3bbd 080b 3bc2 080b     .;...;...;...;..
 80b3bfc:	3bc8 080b 3bab 080b 3bd2 080b 6f4d 6564     .;...;...;..Mode
 80b3c0c:	206c 7270 766f 6469 6465 6920 2073 6373     l provided is sc
 80b3c1c:	6568 616d 7620 7265 6973 6e6f 2520 2064     hema version %d 
 80b3c2c:	6f6e 2074 7165 6175 206c 6f74 7320 7075     not equal to sup
 80b3c3c:	6f70 7472 6465 7620 7265 6973 6e6f 2520     ported version %
 80b3c4c:	2e64 4100 6c6c 636f 7461 5465 6e65 6f73     d..AllocateTenso
 80b3c5c:	7372 2928 6620 6961 656c 0064 6e49 6f76     rs() failed.Invo
 80b3c6c:	656b 6620 6961 656c 2064 6e6f 7820 765f     ke failed on x_v
 80b3c7c:	6c61 203a 6625 000a 6c46 7461 7542 6666     al: %f..FlatBuff
 80b3c8c:	7265 2073 2e31 3131 302e 0000               ers 1.11.0..

080b3c98 <kInferencesPerCycle>:
 80b3c98:	03e8 0000                                   ....

080b3c9c <g_sine_model_data>:
 80b3c9c:	0018 0000 4654 334c 0000 000e 0018 0004     ....TFL3........
 80b3cac:	0008 000c 0010 0014 000e 0000 0003 0000     ................
 80b3cbc:	0a10 0000 05b8 0000 05a0 0000 0004 0000     ................
 80b3ccc:	000b 0000 0590 0000 057c 0000 0524 0000     ........|...$...
 80b3cdc:	04d4 0000 00c4 0000 0074 0000 0024 0000     ........t...$...
 80b3cec:	001c 0000 0014 0000 000c 0000 0004 0000     ................
 80b3cfc:	f654 ffff f658 ffff f65c ffff f660 ffff     T...X...\...`...
 80b3d0c:	fac2 ffff 0004 0000 0040 0000 197c 3ea7     ........@...|..>
 80b3d1c:	8199 3eb9 8b56 3e9f d888 bf12 1074 3e56     ...>V..>....t.V>
 80b3d2c:	c6fe bedf 10f2 be5a e2f0 be0a 5a10 be98     ......Z......Z..
 80b3d3c:	36b9 3dce 7f8f 3e87 b12c bdfd a6e6 be8a     .6.=...>,.......
 80b3d4c:	3ea5 3eda 3450 bded 9190 be69 fb0e ffff     .>.>P4....i.....
 80b3d5c:	0004 0000 0040 0000 4167 bf48 cd24 bea0     ....@...gAH.$...
 80b3d6c:	92b7 bf0c 0000 0000 fe98 3f3c 0000 0000     ..........<?....
	...
 80b3d88:	174a be9a cb41 beb6 0000 0000 0000 0000     J...A...........
 80b3d98:	d613 3e1e 0000 0000 0000 0000 fb5a ffff     ...>........Z...
 80b3da8:	0004 0000 0400 0000 984b bddd 6b40 becb     ........K...@k..
 80b3db8:	0c36 3cd4 44bd 3eb5 7095 3ee3 ace7 3e86     6..<.D.>.p.>...>
 80b3dc8:	c400 3d4e a67e 3e1d 87bd 3ebb b8b4 bf09     ..N=~..>...>....
 80b3dd8:	1fa1 bef8 908d 3edd fade be6f 75b2 3de4     .......>..o..u.=
 80b3de8:	fe6e 3e36 1820 bec2 c739 befb a4fe be30     n.6> ...9.....0.
 80b3df8:	91f7 bede abde 3e24 bbfb 3ece 23eb be80     ......$>...>.#..
 80b3e08:	587b be73 2e9a 3e03 4210 bca9 1210 bd64     {Xs....>.B....d.
 80b3e18:	8de3 3d0c 489e be97 5134 bed4 3b02 3e0d     ...=.H..4Q...;.>
 80b3e28:	6762 be89 df74 3da2 25f3 beb3 34ef 3d7b     bg..t..=.%...4{=
 80b3e38:	7061 3de3 76ba bec0 e97d 3ea7 abc3 bed0     ap.=.v..}..>....
 80b3e48:	7ccf bedb 2770 be9a f598 bd3c 4bff 3e4b     .|..p'....<..KK>
 80b3e58:	a07e bdf8 6ed4 3d86 4a00 3a07 244c be61     ~....n.=.J.:L$a.
 80b3e68:	6854 bdf7 3f02 be77 7923 3eb3 831c bdad     Th...?w.#y.>....
 80b3e78:	92c8 3e8d f3a8 bd15 4de6 3d6c e7ac be98     ...>.....Ml=....
 80b3e88:	ec81 3ebd 55e2 3e73 77c1 3ec7 1b6e 3d5e     ...>.Us>.w.>n.^=
 80b3e98:	7827 3f02 21d4 3d90 dc52 3e1f dabf 3e88     'x.?.!.=R..>...>
 80b3ea8:	7980 bde3 6f40 be10 4320 bd2e 76f0 bdc5     .y..@o.. C...v..
 80b3eb8:	a0cc be04 69f0 bed7 feb1 be64 4120 be84     .....i....d. A..
 80b3ec8:	c3b2 be26 f4d8 be09 4464 3dd1 e1d5 bec8     ..&.....dD.=....
 80b3ed8:	bc35 be3f 94c0 3d82 2bdc bdb1 db02 bebf     5.?....=.+......
 80b3ee8:	7fa5 3e8a b421 3ea2 86cd bf56 3b9c bc76     ...>!..>..V..;v.
 80b3ef8:	6d85 bf60 0086 be3c 23c1 3e7e cd96 3e3f     .m`...<..#~>..?>
 80b3f08:	9186 3e2d ef55 3e87 977e be03 cd2a 3e01     ..->U..>~...*..>
 80b3f18:	c932 be8e 7772 be3b a1e0 bebc b78d 3ea7     2...rw;........>
 80b3f28:	051c be95 1ff7 3ebb 3ec9 3ed6 4280 bde9     .......>.>.>.B..
 80b3f38:	0c27 bed2 325c be34 cb14 bdca 3add be67     '...\24......:g.
 80b3f48:	bb1c be8d ac91 be5c 4052 be6f 71d7 3e94     ......\.R@o..q.>
 80b3f58:	7118 be09 299b bed9 667d bed2 d698 beb2     .q...)..}f......
 80b3f68:	c900 3a84 dabc bdc2 c21d bf1b ddd4 3e92     ...:...........>
 80b3f78:	8707 be6c c240 be3b e2bd 3e9c b50a bea0     ..l.@.;....>....
 80b3f88:	d5e2 be9c bb3e 3e7c b417 3ecf 8ed5 bec8     ....>.|>...>....
 80b3f98:	f97c 3e5c fc80 3d0d d5c5 3e8b 17f5 3ea2     |.\>...=...>...>
 80b3fa8:	60c7 be89 95ec 3d87 c27a bf5d 9477 3e98     .`.....=z.].w..>
 80b3fb8:	3977 bc07 2942 3e00 d0af 3ea9 2331 bec4     w9..B).>...>1#..
 80b3fc8:	3695 be5b dcc7 be83 6b1e 3e47 245b 3e99     .6[......kG>[$.>
 80b3fd8:	2799 3e54 20c8 bddd 865a 3e2f f080 be69     .'T>. ..Z./>..i.
 80b3fe8:	fc44 bd84 a082 be2a e687 3e2a 34d8 3dae     D.....*...*>.4.=
 80b3ff8:	bd50 3eb5 8cc4 be88 bce3 3ea5 daa9 3e9e     P..>.......>...>
 80b4008:	b83e be23 9080 3d15 3f97 3ec3 5cca 3e9d     >.#....=.?.>.\.>
 80b4018:	e821 3ee1 49c0 bc01 0b00 bd88 f73f 3cca     !..>.I......?..<
 80b4028:	5afb 3eb1 d260 3c0d 23ce bf78 4f8f beb9     .Z.>`..<.#x..O..
 80b4038:	6a69 bf34 5e4b 3ea9 8c64 3ed9 7752 3e36     ij4.K^.>d..>Rw6>
 80b4048:	afeb 3ebe be40 3c36 6508 bd3b e055 bd66     ...>@.6<.e;.U.f.
 80b4058:	e8d2 be9b e386 be09 3d93 3edd 660f 3f18     .........=.>.f.?
 80b4068:	0518 bd33 15de bed7 cfaa be49 a5a2 3e64     ..3.......I...d>
 80b4078:	9ce6 be42 4254 3dcc bda0 be9d 69c2 3e48     ..B.TB.=.....iH>
 80b4088:	8b5b bea2 13c0 3d87 fd36 3e69 8605 be40     [......=6.i>..@.
 80b4098:	7a1e bece 1346 bea7 5268 be86 9e04 bd86     .z..F...hR......
 80b40a8:	548c 3dc1 3be0 3cad 6742 bd85 97ea 3e42     .T.=.;.<Bg....B>
 80b40b8:	136e bf3b 5b56 3e16 abaa 3edf 41c8 3d36     n.;.V[.>...>.A6=
 80b40c8:	2d24 be47 a577 3eae c2c0 3c5b acac 3e4e     $-G.w..>..[<..N>
 80b40d8:	ec99 be13 abf2 3e73 a1aa be48 d3e8 be01     ......s>..H.....
 80b40e8:	b760 bdc7 7264 3dd3 d383 3e99 760c be34     `...dr.=...>.v4.
 80b40f8:	da42 3e0d 47fb 3e9a dc8b be92 7f56 3e6b     B..>.G.>....V.k>
 80b4108:	d404 bd88 9e11 3e80 893c 3dff 3eb3 3e88     .......><..=.>.>
 80b4118:	f0f7 3e88 fb28 bec9 3e53 3ecf 75ac bedc     ...>(...S>.>.u..
 80b4128:	cadd 3ed7 5801 3ea7 b829 bf13 8176 bc12     ...>.X.>)...v...
 80b4138:	8b28 bf16 ec0e 3e0e 0a40 bddb ec98 bdbf     (......>@.......
 80b4148:	5532 be0c f9fb 3ec9 4a83 be6d 5976 bee2     2U.....>.Jm.vY..
 80b4158:	7d54 bb9f e89d 3e95 d35c 3dd0 8a19 3eb0     T}.....>\..=...>
 80b4168:	6fde be2e 16d0 3d83 7d9c bf11 cc2b 3c25     .o.....=.}..+.%<
 80b4178:	a52a be27 1422 bec7 7a5e 3eac 414e be94     *.'."...^z.>NA..
 80b4188:	685a 3e7b fd86 3e4e 56a2 be6a feca be81     Zh{>..N>.Vj.....
 80b4198:	c343 bdb1 b8c5 3ea7 2355 3ecd 2eaf 3e76     C......>U#.>..v>
 80b41a8:	a869 be90 ba0d 3eb9 ff66 ffff 0004 0000     i......>f.......
 80b41b8:	0040 0000 d653 3de2 b666 3ecc e703 3ef6     @...S..=f..>...>
 80b41c8:	28e0 bf10 0000 0000 3d3e 3eb0 0000 0000     .(......>=.>....
 80b41d8:	f062 3e77 9da6 3ea4 4b3a bef3 9e71 3ea7     b.w>...>:K..q..>
 80b41e8:	0000 0000 3934 3ea2 0000 0000 9ccc 3e4a     ....49.>......J>
 80b41f8:	40ab 3ea3 ffb2 ffff 0004 0000 0040 0000     .@.>........@...
 80b4208:	71b3 3f67 7a9a bf95 48e1 bee8 728a 3e96     .qg?.z...H...r.>
 80b4218:	d200 bbd3 c51a 3fd7 7eac bec8 a790 be95     .......?.~......
 80b4228:	d73b bedc a841 3f16 5b50 3fcb b952 beed     ;...A..?P[.?R...
 80b4238:	a72e bec6 0faf bf14 dab3 3f59 ec02 bed7     ..........Y?....
 80b4248:	0000 0006 0008 0004 0006 0000 0004 0000     ................
 80b4258:	0004 0000 1166 bf1f fbb8 ffff 000f 0000     ....f...........
 80b4268:	4f54 4f43 4320 6e6f 6576 7472 6465 002e     TOCO Converted..
 80b4278:	0001 0000 0010 0000 000c 0014 0004 0008     ................
 80b4288:	000c 0010 000c 0000 00f0 0000 00e4 0000     ................
 80b4298:	00d8 0000 0004 0000 0003 0000 0090 0000     ................
 80b42a8:	0048 0000 0004 0000 ffce ffff 0000 0800     H...............
 80b42b8:	0018 0000 000c 0000 0004 0000 fc1c ffff     ................
 80b42c8:	0001 0000 0000 0000 0003 0000 0007 0000     ................
 80b42d8:	0008 0000 0009 0000 0000 000e 0014 0000     ................
 80b42e8:	0008 000c 0007 0010 000e 0000 0000 0800     ................
 80b42f8:	001c 0000 0010 0000 0004 0000 ffba ffff     ................
 80b4308:	0000 0100 0001 0000 0007 0000 0003 0000     ................
 80b4318:	0004 0000 0005 0000 0006 0000 0000 000e     ................
 80b4328:	0016 0000 0008 000c 0007 0010 000e 0000     ................
 80b4338:	0000 0800 0024 0000 0018 0000 000c 0000     ....$...........
 80b4348:	0000 0006 0008 0007 0006 0000 0000 0100     ................
 80b4358:	0001 0000 0004 0000 0003 0000 0001 0000     ................
 80b4368:	0002 0000 0003 0000 0001 0000 0000 0000     ................
 80b4378:	0001 0000 0001 0000 000a 0000 0310 0000     ................
 80b4388:	02a4 0000 0240 0000 01f4 0000 01ac 0000     ....@...........
 80b4398:	0148 0000 00fc 0000 00b4 0000 0050 0000     H...........P...
 80b43a8:	0004 0000 fd26 ffff 003c 0000 0001 0000     ....&...<.......
 80b43b8:	000c 0000 0004 0000 fd18 ffff 0020 0000     ............ ...
 80b43c8:	6573 7571 6e65 6974 6c61 315f 642f 6e65     sequential_1/den
 80b43d8:	6573 345f 4d2f 7461 754d 5f6c 6962 7361     se_4/MatMul_bias
 80b43e8:	0000 0000 0001 0000 0001 0000 fd6e ffff     ............n...
 80b43f8:	0050 0000 0002 0000 000c 0000 0004 0000     P...............
 80b4408:	fd60 ffff 0034 0000 6573 7571 6e65 6974     `...4...sequenti
 80b4418:	6c61 315f 642f 6e65 6573 345f 4d2f 7461     al_1/dense_4/Mat
 80b4428:	754d 2f6c 6552 6461 6156 6972 6261 656c     Mul/ReadVariable
 80b4438:	704f 742f 6172 736e 6f70 6573 0000 0000     Op/transpose....
 80b4448:	0002 0000 0001 0000 0010 0000 fdce ffff     ................
 80b4458:	0034 0000 0008 0000 000c 0000 0004 0000     4...............
 80b4468:	fdc0 ffff 0019 0000 6573 7571 6e65 6974     ........sequenti
 80b4478:	6c61 315f 642f 6e65 6573 335f 522f 6c65     al_1/dense_3/Rel
 80b4488:	0075 0000 0002 0000 0001 0000 0010 0000     u...............
 80b4498:	fe12 ffff 003c 0000 0003 0000 000c 0000     ....<...........
 80b44a8:	0004 0000 fe04 ffff 0020 0000 6573 7571     ........ ...sequ
 80b44b8:	6e65 6974 6c61 315f 642f 6e65 6573 335f     ential_1/dense_3
 80b44c8:	4d2f 7461 754d 5f6c 6962 7361 0000 0000     /MatMul_bias....
 80b44d8:	0001 0000 0010 0000 fe5a ffff 0050 0000     ........Z...P...
 80b44e8:	0004 0000 000c 0000 0004 0000 fe4c ffff     ............L...
 80b44f8:	0034 0000 6573 7571 6e65 6974 6c61 315f     4...sequential_1
 80b4508:	642f 6e65 6573 335f 4d2f 7461 754d 2f6c     /dense_3/MatMul/
 80b4518:	6552 6461 6156 6972 6261 656c 704f 742f     ReadVariableOp/t
 80b4528:	6172 736e 6f70 6573 0000 0000 0002 0000     ranspose........
 80b4538:	0010 0000 0010 0000 feba ffff 0034 0000     ............4...
 80b4548:	000a 0000 000c 0000 0004 0000 feac ffff     ................
 80b4558:	0019 0000 6573 7571 6e65 6974 6c61 315f     ....sequential_1
 80b4568:	642f 6e65 6573 325f 522f 6c65 0075 0000     /dense_2/Relu...
 80b4578:	0002 0000 0001 0000 0010 0000 fefe ffff     ................
 80b4588:	003c 0000 0005 0000 000c 0000 0004 0000     <...............
 80b4598:	fef0 ffff 0020 0000 6573 7571 6e65 6974     .... ...sequenti
 80b45a8:	6c61 315f 642f 6e65 6573 325f 4d2f 7461     al_1/dense_2/Mat
 80b45b8:	754d 5f6c 6962 7361 0000 0000 0001 0000     Mul_bias........
 80b45c8:	0010 0000 ff46 ffff 0050 0000 0006 0000     ....F...P.......
 80b45d8:	000c 0000 0004 0000 ff38 ffff 0034 0000     ........8...4...
 80b45e8:	6573 7571 6e65 6974 6c61 315f 642f 6e65     sequential_1/den
 80b45f8:	6573 325f 4d2f 7461 754d 2f6c 6552 6461     se_2/MatMul/Read
 80b4608:	6156 6972 6261 656c 704f 742f 6172 736e     VariableOp/trans
 80b4618:	6f70 6573 0000 0000 0002 0000 0010 0000     pose............
 80b4628:	0001 0000 ffa6 ffff 0048 0000 0009 0000     ........H.......
 80b4638:	002c 0000 000c 0000 0008 000c 0004 0008     ,...............
 80b4648:	0008 0000 0010 0000 0004 0000 0001 0000     ................
 80b4658:	0000 437f 0001 0000 0000 0000 000d 0000     ...C............
 80b4668:	6564 736e 5f65 5f32 6e69 7570 0074 0000     dense_2_input...
 80b4678:	0002 0000 0001 0000 0001 0000 0000 000e     ................
 80b4688:	0014 0004 0000 0008 000c 0010 000e 0000     ................
 80b4698:	0028 0000 0007 0000 0010 0000 0008 0000     (...............
 80b46a8:	0004 0004 0004 0000 0008 0000 6449 6e65     ............Iden
 80b46b8:	6974 7974 0000 0000 0002 0000 0001 0000     tity............
 80b46c8:	0001 0000 0001 0000 0010 0000 0000 000a     ................
 80b46d8:	000c 0007 0000 0008 000a 0000 0000 0900     ................
 80b46e8:	0003 0000                                   ....

080b46ec <_ZZNK11flatbuffers6VectorIlE3GetEmE19__PRETTY_FUNCTION__>:
 80b46ec:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
 80b46fc:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
 80b470c:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
 80b471c:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
 80b472c:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
 80b473c:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
 80b474c:	7469 2068 2054 203d 6f6c 676e 6920 746e     ith T = long int
 80b475c:	203b 6c66 7461 7562 6666 7265 3a73 563a     ; flatbuffers::V
 80b476c:	6365 6f74 3c72 3e54 3a3a 6572 7574 6e72     ector<T>::return
 80b477c:	745f 7079 2065 203d 6f6c 676e 6920 746e     _type = long int
 80b478c:	203b 6c66 7461 7562 6666 7265 3a73 753a     ; flatbuffers::u
 80b479c:	666f 7366 7465 745f 3d20 6c20 6e6f 2067     offset_t = long 
 80b47ac:	6e75 6973 6e67 6465 6920 746e 005d 6e49     unsigned int].In
 80b47bc:	7570 2074 7261 6172 2079 6f6e 2074 7270     put array not pr
 80b47cc:	766f 6469 6465 6620 726f 6f20 6570 6172     ovided for opera
 80b47dc:	6974 6e6f 2720 7325 2e27 000a 6f46 6e75     tion '%s'...Foun
 80b47ec:	2064 6f74 206f 616d 796e 6420 6d69 6e65     d too many dimen
 80b47fc:	6973 6e6f 2073 6e69 7420 6568 6920 706e     sions in the inp
 80b480c:	7475 6120 7272 7961 6f20 2066 706f 7265     ut array of oper
 80b481c:	7461 6f69 206e 2527 2773 0a2e 6900 3c20     ation '%s'...i <
 80b482c:	7320 7a69 2865 0029 552f 6573 7372 622f      size()./Users/b
 80b483c:	6173 7274 6d6f 442f 7665 6c65 706f 656d     satrom/Developme
 80b484c:	746e 702f 7261 6974 6c63 2f65 696c 7262     nt/particle/libr
 80b485c:	7261 6569 2f73 6150 7472 6369 656c 545f     aries/Particle_T
 80b486c:	6e65 6f73 4672 6f6c 4c77 7469 5f65 7845     ensorFlowLite_Ex
 80b487c:	6d61 6c70 7365 682f 6c65 6f6c 775f 726f     amples/hello_wor
 80b488c:	646c 6c2f 6269 542f 6e65 6f73 4672 6f6c     ld/lib/TensorFlo
 80b489c:	4c77 7469 2f65 7273 2f63 6874 7269 5f64     wLite/src/third_
 80b48ac:	6170 7472 2f79 6c66 7461 7562 6666 7265     party/flatbuffer
 80b48bc:	2f73 6e69 6c63 6475 2f65 6c66 7461 7562     s/include/flatbu
 80b48cc:	6666 7265 2f73 6c66 7461 7562 6666 7265     ffers/flatbuffer
 80b48dc:	2e73 0068 6e55 7573 7070 726f 6574 2064     s.h.Unsupported 
 80b48ec:	6164 6174 7420 7079 2065 6425 6920 206e     data type %d in 
 80b48fc:	6574 736e 726f 000a 6e55 6168 646e 656c     tensor..Unhandle
 80b490c:	2064 7566 6c6c 2d79 6f63 6e6e 6365 6574     d fully-connecte
 80b491c:	2064 6577 6769 7468 2073 6f66 6d72 7461     d weights format
 80b492c:	002e 6e55 6168 646e 656c 2064 534c 4d54     ..Unhandled LSTM
 80b493c:	6b20 7265 656e 206c 7974 6570 203a 6425      kernel type: %d
 80b494c:	4e00 206f 6176 696c 2064 534c 4d54 6220     .No valid LSTM b
 80b495c:	6975 746c 6e69 6f20 7470 6f69 736e 6520     uiltin options e
 80b496c:	6978 7473 7200 7365 6168 6570 7300 7571     xist.reshape.squ
 80b497c:	6565 657a 4400 4c45 4745 5441 2045 706f     eeze.DELEGATE op
 80b498c:	7320 6f68 6c75 6e64 7427 6520 6978 7473      shouldn't exist
 80b499c:	6920 206e 6f6d 6564 2e6c 0100                     in model..

080b49a7 <CSWTCH.73>:
 80b49a7:	0201 0403 5805                                   .....

080b49ac <_ZZN6tflite24EnumNamesBuiltinOperatorEvE5names>:
 80b49ac:	4c58 080b 4c5c 080b 4c6c 080b 4c84 080b     XL..\L..lL...L..
 80b49bc:	4c7a 080b 4c8c 080b 4c9b 080b 4ca6 080b     zL...L...L...L..
 80b49cc:	4cb7 080b 4cbd 080b 4ccd 080b 4cde 080b     .L...L...L...L..
 80b49dc:	4cef 080b 4cfa 080b 4d17 080b 4d20 080b     .L...L...M.. M..
 80b49ec:	4ea9 080b 4d2f 080b 4d3b 080b 4eb4 080b     .N../M..;M...N..
 80b49fc:	4d3f 080b 4d4c 080b 4d52 080b 4d5a 080b     ?M..LM..RM..ZM..
 80b4a0c:	4e67 080b 4e81 080b 4d6a 080b 4d79 080b     gN...N..jM..yM..
 80b4a1c:	4d7e 080b 4d83 080b 4d95 080b 4d9f 080b     ~M...M...M...M..
 80b4a2c:	4da4 080b 4dab 080b 5038 080b 4dc3 080b     .M...M..8P...M..
 80b4a3c:	4ddf 080b 4de6 080b 4df8 080b 4e0a 080b     .M...M...M...N..
 80b4a4c:	4e14 080b 4e19 080b 4fc5 080b 4e1d 080b     .N...N...O...N..
 80b4a5c:	4e25 080b 4e42 080b 4e50 080b 4e6b 080b     %N..BN..PN..kN..
 80b4a6c:	4e6f 080b 4e77 080b 4e7d 080b 4e89 080b     oN..wN..}N...N..
 80b4a7c:	4e92 080b 4eae 080b 4eb3 080b 4eb9 080b     .N...N...N...N..
 80b4a8c:	4ec1 080b 4ec9 080b 4ed1 080b 4ed6 080b     .N...N...N...N..
 80b4a9c:	4eda 080b 4ee0 080b 4ee8 080b 4ef6 080b     .N...N...N...N..
 80b4aac:	4f01 080b 4e4a 080b 4f08 080b 4f0c 080b     .O..JN...O...O..
 80b4abc:	4f1b 080b 4f2b 080b 4f30 080b 4ef0 080b     .O..+O..0O...N..
 80b4acc:	4f3c 080b 4f46 080b 4f4a 080b 4f4f 080b     <O..FO..JO..OO..
 80b4adc:	4f4e 080b 4d54 080b 4f54 080b 4f58 080b     NO..TM..TO..XO..
 80b4aec:	4f60 080b 4f6b 080b 4f77 080b 4faf 080b     `O..kO..wO...O..
 80b4afc:	4f82 080b 4f8d 080b 4f95 080b 4fa1 080b     .O...O...O...O..
 80b4b0c:	4fad 080b 4fb4 080b 4fbf 080b 4fc9 080b     .O...O...O...O..
 80b4b1c:	4fd4 080b 4fdb 080b 4fe6 080b 4feb 080b     .O...O...O...O..
 80b4b2c:	4ff5 080b 4ffb 080b 5013 080b 501e 080b     .O...O...P...P..
 80b4b3c:	5031 080b 503c 080b 5040 080b 5048 080b     1P..<P..@P..HP..
 80b4b4c:	504f 080b 5054 080b 505f 080b 5065 080b     OP..TP.._P..eP..
 80b4b5c:	506f 080b 5073 080b 5079 080b 4eb5 080b     oP..sP..yP...N..
 80b4b6c:	507e 080b 508f 080b 4c9d 080b 509b 080b     ~P...P...L...P..
 80b4b7c:	50ab 080b 50b1 080b 50bc 080b 50bf 080b     .P...P...P...P..
 80b4b8c:	50c5 080b 50dc 080b 0000 0000 704f 6220     .P...P......Op b
 80b4b9c:	6975 746c 6e69 635f 646f 2065 756f 2074     uiltin_code out 
 80b4bac:	666f 7220 6e61 6567 203a 6425 202e 7241     of range: %d. Ar
 80b4bbc:	2065 6f79 2075 7375 6e69 2067 6c6f 2064     e you using old 
 80b4bcc:	4654 694c 6574 6220 6e69 7261 2079 6977     TFLite binary wi
 80b4bdc:	6874 6e20 7765 7265 6d20 646f 6c65 003f     th newer model?.
 80b4bec:	6944 6e64 7427 6620 6e69 2064 706f 6620     Didn't find op f
 80b4bfc:	726f 6220 6975 746c 6e69 6f20 6370 646f     or builtin opcod
 80b4c0c:	2065 2527 2773 7620 7265 6973 6e6f 2720     e '%s' version '
 80b4c1c:	6425 0a27 4f00 6570 6172 6f74 2072 6977     %d'..Operator wi
 80b4c2c:	6874 4320 5355 4f54 204d 7562 6c69 6974     th CUSTOM builti
 80b4c3c:	5f6e 6f63 6564 6820 7361 6e20 206f 7563     n_code has no cu
 80b4c4c:	7473 6d6f 635f 646f 2e65 000a 4441 0044     stom_code...ADD.
 80b4c5c:	5641 5245 4741 5f45 4f50 4c4f 325f 0044     AVERAGE_POOL_2D.
 80b4c6c:	4f43 434e 5441 4e45 5441 4f49 004e 4544     CONCATENATION.DE
 80b4c7c:	5450 5748 5349 5f45 4f43 564e 325f 0044     PTHWISE_CONV_2D.
 80b4c8c:	4544 5450 5f48 4f54 535f 4150 4543 4400     DEPTH_TO_SPACE.D
 80b4c9c:	5145 4155 544e 5a49 0045 4d45 4542 4444     EQUANTIZE.EMBEDD
 80b4cac:	4e49 5f47 4f4c 4b4f 5055 4600 4f4c 524f     ING_LOOKUP.FLOOR
 80b4cbc:	4600 4c55 594c 435f 4e4f 454e 5443 4445     .FULLY_CONNECTED
 80b4ccc:	4800 5341 5448 4241 454c 4c5f 4f4f 554b     .HASHTABLE_LOOKU
 80b4cdc:	0050 324c 4e5f 524f 414d 494c 415a 4954     P.L2_NORMALIZATI
 80b4cec:	4e4f 4c00 5f32 4f50 4c4f 325f 0044 4f4c     ON.L2_POOL_2D.LO
 80b4cfc:	4143 5f4c 4552 5053 4e4f 4553 4e5f 524f     CAL_RESPONSE_NOR
 80b4d0c:	414d 494c 415a 4954 4e4f 4c00 474f 5349     MALIZATION.LOGIS
 80b4d1c:	4954 0043 534c 5f48 5250 4a4f 4345 4954     TIC.LSH_PROJECTI
 80b4d2c:	4e4f 4d00 5841 505f 4f4f 5f4c 4432 4d00     ON.MAX_POOL_2D.M
 80b4d3c:	4c55 5200 4c45 5f55 314e 545f 5f4f 0031     UL.RELU_N1_TO_1.
 80b4d4c:	4552 554c 0036 4552 4853 5041 0045 4552     RELU6.RESHAPE.RE
 80b4d5c:	4953 455a 425f 4c49 4e49 4145 0052 5053     SIZE_BILINEAR.SP
 80b4d6c:	4341 5f45 4f54 445f 5045 4854 5300 4456     ACE_TO_DEPTH.SVD
 80b4d7c:	0046 4154 484e 4300 4e4f 4143 5f54 4d45     F.TANH.CONCAT_EM
 80b4d8c:	4542 4444 4e49 5347 5300 494b 5f50 5247     BEDDINGS.SKIP_GR
 80b4d9c:	4d41 4300 4c41 004c 5543 5453 4d4f 4500     AM.CALL.CUSTOM.E
 80b4dac:	424d 4445 4944 474e 4c5f 4f4f 554b 5f50     MBEDDING_LOOKUP_
 80b4dbc:	5053 5241 4553 5500 494e 4944 4552 5443     SPARSE.UNIDIRECT
 80b4dcc:	4f49 414e 5f4c 4553 5551 4e45 4543 525f     IONAL_SEQUENCE_R
 80b4ddc:	4e4e 4700 5441 4548 0052 4142 4354 5f48     NN.GATHER.BATCH_
 80b4dec:	4f54 535f 4150 4543 4e5f 0044 5053 4341     TO_SPACE_ND.SPAC
 80b4dfc:	5f45 4f54 425f 5441 4843 4e5f 0044 5254     E_TO_BATCH_ND.TR
 80b4e0c:	4e41 5053 534f 0045 454d 4e41 5300 4255     ANSPOSE.MEAN.SUB
 80b4e1c:	5300 5551 4545 455a 5500 494e 4944 4552     .SQUEEZE.UNIDIRE
 80b4e2c:	5443 4f49 414e 5f4c 4553 5551 4e45 4543     CTIONAL_SEQUENCE
 80b4e3c:	4c5f 5453 004d 5453 4952 4544 5f44 4c53     _LSTM.STRIDED_SL
 80b4e4c:	4349 0045 4942 4944 4552 5443 4f49 414e     ICE.BIDIRECTIONA
 80b4e5c:	5f4c 4553 5551 4e45 4543 525f 4e4e 4500     L_SEQUENCE_RNN.E
 80b4e6c:	5058 5400 504f 5f4b 3256 5300 4c50 5449     XP.TOPK_V2.SPLIT
 80b4e7c:	4c00 474f 535f 464f 4d54 5841 4400 4c45     .LOG_SOFTMAX.DEL
 80b4e8c:	4745 5441 0045 4942 4944 4552 5443 4f49     EGATE.BIDIRECTIO
 80b4e9c:	414e 5f4c 4553 5551 4e45 4543 4c5f 5453     NAL_SEQUENCE_LST
 80b4eac:	004d 4143 5453 5000 4552 554c 4d00 5841     M.CAST.PRELU.MAX
 80b4ebc:	4d49 4d55 4100 4752 4d5f 5841 4d00 4e49     IMUM.ARG_MAX.MIN
 80b4ecc:	4d49 4d55 4c00 5345 0053 454e 0047 4150     IMUM.LESS.NEG.PA
 80b4edc:	5644 0032 5247 4145 4554 0052 5247 4145     DV2.GREATER.GREA
 80b4eec:	4554 5f52 5145 4155 004c 454c 5353 455f     TER_EQUAL.LESS_E
 80b4efc:	5551 4c41 5300 4c45 4345 0054 4953 004e     QUAL.SELECT.SIN.
 80b4f0c:	5254 4e41 5053 534f 5f45 4f43 564e 5300     TRANSPOSE_CONV.S
 80b4f1c:	4150 5352 5f45 4f54 445f 4e45 4553 5400     PARSE_TO_DENSE.T
 80b4f2c:	4c49 0045 5845 4150 444e 445f 4d49 0053     ILE.EXPAND_DIMS.
 80b4f3c:	4f4e 5f54 5145 4155 004c 4f4c 0047 5553     NOT_EQUAL.LOG.SU
 80b4f4c:	004d 5352 5251 0054 4f50 0057 5241 5f47     M.RSQRT.POW.ARG_
 80b4f5c:	494d 004e 4146 454b 515f 4155 544e 5200     MIN.FAKE_QUANT.R
 80b4f6c:	4445 4355 5f45 5250 444f 5200 4445 4355     EDUCE_PROD.REDUC
 80b4f7c:	5f45 414d 0058 4f4c 4947 4143 5f4c 524f     E_MAX.LOGICAL_OR
 80b4f8c:	4f00 454e 485f 544f 4c00 474f 4349 4c41     .ONE_HOT.LOGICAL
 80b4f9c:	415f 444e 4c00 474f 4349 4c41 4e5f 544f     _AND.LOGICAL_NOT
 80b4fac:	5500 504e 4341 004b 4552 5544 4543 4d5f     .UNPACK.REDUCE_M
 80b4fbc:	4e49 4600 4f4c 524f 445f 5649 5200 4445     IN.FLOOR_DIV.RED
 80b4fcc:	4355 5f45 4e41 0059 5153 4155 4552 5a00     UCE_ANY.SQUARE.Z
 80b4fdc:	5245 534f 4c5f 4b49 0045 4946 4c4c 4600     EROS_LIKE.FILL.F
 80b4fec:	4f4c 524f 4d5f 444f 5200 4e41 4547 5200     LOOR_MOD.RANGE.R
 80b4ffc:	5345 5a49 5f45 454e 5241 5345 5f54 454e     ESIZE_NEAREST_NE
 80b500c:	4749 4248 524f 4c00 4145 594b 525f 4c45     IGHBOR.LEAKY_REL
 80b501c:	0055 5153 4155 4552 5f44 4944 4646 5245     U.SQUARED_DIFFER
 80b502c:	4e45 4543 4d00 5249 4f52 5f52 4150 0044     ENCE.MIRROR_PAD.
 80b503c:	4241 0053 5053 494c 5f54 0056 4e55 5149     ABS.SPLIT_V.UNIQ
 80b504c:	4555 4300 4945 004c 4552 4556 5352 5f45     UE.CEIL.REVERSE_
 80b505c:	3256 4100 4444 4e5f 4700 5441 4548 5f52     V2.ADD_N.GATHER_
 80b506c:	444e 4300 534f 5700 4548 4552 5200 4e41     ND.COS.WHERE.RAN
 80b507c:	004b 4552 4556 5352 5f45 4553 5551 4e45     K.REVERSE_SEQUEN
 80b508c:	4543 4d00 5441 4952 5f58 4944 4741 4d00     CE.MATRIX_DIAG.M
 80b509c:	5441 4952 5f58 4553 5f54 4944 4741 5200     ATRIX_SET_DIAG.R
 80b50ac:	554f 444e 4800 5241 5f44 5753 5349 0048     OUND.HARD_SWISH.
 80b50bc:	4649 5700 4948 454c 4e00 4e4f 4d5f 5841     IF.WHILE.NON_MAX
 80b50cc:	535f 5055 5250 5345 4953 4e4f 565f 0034     _SUPPRESSION_V4.
 80b50dc:	4f4e 5f4e 414d 5f58 5553 5050 4552 5353     NON_MAX_SUPPRESS
 80b50ec:	4f49 5f4e 3556 0300 0804 0d0b 110e 1312     ION_V5..........
 80b50fc:	1514 1716 6e49 0066 614e 004e 322a 005e     ....Inf.NaN.*2^.
 80b510c:	7954 6570 2520 2073 2528 2964 6e20 746f     Type %s (%d) not
 80b511c:	6920 2073 6f6e 2074 7573 7070 726f 6574      is not supporte
 80b512c:	0064                                        d.

080b512e <_ZZNK11flatbuffers6VectorIlE3GetEmE19__PRETTY_FUNCTION__>:
 80b512e:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
 80b513e:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
 80b514e:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
 80b515e:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
 80b516e:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
 80b517e:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
 80b518e:	7469 2068 2054 203d 6f6c 676e 6920 746e     ith T = long int
 80b519e:	203b 6c66 7461 7562 6666 7265 3a73 563a     ; flatbuffers::V
 80b51ae:	6365 6f74 3c72 3e54 3a3a 6572 7574 6e72     ector<T>::return
 80b51be:	745f 7079 2065 203d 6f6c 676e 6920 746e     _type = long int
 80b51ce:	203b 6c66 7461 7562 6666 7265 3a73 753a     ; flatbuffers::u
 80b51de:	666f 7366 7465 745f 3d20 6c20 6e6f 2067     offset_t = long 
 80b51ee:	6e75 6973 6e67 6465 6920 746e 005d          unsigned int].

080b51fc <_ZZNK11flatbuffers6VectorINS_6OffsetIN6tflite8OperatorEEEE3GetEmE19__PRETTY_FUNCTION__>:
 80b51fc:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
 80b520c:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
 80b521c:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
 80b522c:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
 80b523c:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
 80b524c:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
 80b525c:	7469 2068 2054 203d 6c66 7461 7562 6666     ith T = flatbuff
 80b526c:	7265 3a73 4f3a 6666 6573 3c74 6674 696c     ers::Offset<tfli
 80b527c:	6574 3a3a 704f 7265 7461 726f 3b3e 6620     te::Operator>; f
 80b528c:	616c 6274 6675 6566 7372 3a3a 6556 7463     latbuffers::Vect
 80b529c:	726f 543c 3a3e 723a 7465 7275 5f6e 7974     or<T>::return_ty
 80b52ac:	6570 3d20 6320 6e6f 7473 7420 6c66 7469     pe = const tflit
 80b52bc:	3a65 4f3a 6570 6172 6f74 2a72 203b 6c66     e::Operator*; fl
 80b52cc:	7461 7562 6666 7265 3a73 753a 666f 7366     atbuffers::uoffs
 80b52dc:	7465 745f 3d20 6c20 6e6f 2067 6e75 6973     et_t = long unsi
 80b52ec:	6e67 6465 6920 746e 005d                    gned int].

080b52f6 <_ZZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEmE19__PRETTY_FUNCTION__>:
 80b52f6:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
 80b5306:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
 80b5316:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
 80b5326:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
 80b5336:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
 80b5346:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
 80b5356:	7469 2068 2054 203d 6c66 7461 7562 6666     ith T = flatbuff
 80b5366:	7265 3a73 4f3a 6666 6573 3c74 6674 696c     ers::Offset<tfli
 80b5376:	6574 3a3a 6554 736e 726f 3b3e 6620 616c     te::Tensor>; fla
 80b5386:	6274 6675 6566 7372 3a3a 6556 7463 726f     tbuffers::Vector
 80b5396:	543c 3a3e 723a 7465 7275 5f6e 7974 6570     <T>::return_type
 80b53a6:	3d20 6320 6e6f 7473 7420 6c66 7469 3a65      = const tflite:
 80b53b6:	543a 6e65 6f73 2a72 203b 6c66 7461 7562     :Tensor*; flatbu
 80b53c6:	6666 7265 3a73 753a 666f 7366 7465 745f     ffers::uoffset_t
 80b53d6:	3d20 6c20 6e6f 2067 6e75 6973 6e67 6465      = long unsigned
 80b53e6:	6920 746e 005d 6e4f 796c 3120 7320 6275      int].Only 1 sub
 80b53f6:	7267 7061 2068 7369 6320 7275 6572 746e     graph is current
 80b5406:	796c 7320 7075 6f70 7472 6465 0a2e 3c00     ly supported...<
 80b5416:	6f4e 6e20 6d61 3e65 4900 766e 6c61 6469     No name>.Invalid
 80b5426:	7020 6572 612d 6c6c 636f 7461 6465 6920      pre-allocated i
 80b5436:	706e 7475 2520 2064 7270 766f 6469 6465     nput %d provided
 80b5446:	002e 7241 6e65 2061 6973 657a 6920 2073     ..Arena size is 
 80b5456:	6f74 206f 6d73 6c61 206c 6f66 2072 6361     too small for ac
 80b5466:	6974 6176 6974 6e6f 6220 6675 6566 7372     tivation buffers
 80b5476:	202e 654e 6465 6465 2520 2064 7562 2074     . Needed %d but 
 80b5486:	6e6f 796c 2520 2064 6177 2073 7661 6961     only %d was avai
 80b5496:	616c 6c62 2e65 5600 7261 6169 6c62 2065     lable..Variable 
 80b54a6:	7369 6e20 746f 6120 6c6c 636f 7461 6465     is not allocated
 80b54b6:	4c00 676f 6369 6520 7272 726f 6920 206e     .Logic error in 
 80b54c6:	656d 6f6d 7972 7020 616c 6e6e 7265 202c     memory planner, 
 80b54d6:	6574 736e 726f 2520 2064 6168 2073 6e61     tensor %d has an
 80b54e6:	6920 766e 6c61 6469 6c20 6669 7465 6d69      invalid lifetim
 80b54f6:	0065                                        e.

080b54f8 <_ZZNK11flatbuffers6VectorIfE3GetEmE19__PRETTY_FUNCTION__>:
 80b54f8:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
 80b5508:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
 80b5518:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
 80b5528:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
 80b5538:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
 80b5548:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
 80b5558:	7469 2068 2054 203d 6c66 616f 3b74 6620     ith T = float; f
 80b5568:	616c 6274 6675 6566 7372 3a3a 6556 7463     latbuffers::Vect
 80b5578:	726f 543c 3a3e 723a 7465 7275 5f6e 7974     or<T>::return_ty
 80b5588:	6570 3d20 6620 6f6c 7461 203b 6c66 7461     pe = float; flat
 80b5598:	7562 6666 7265 3a73 753a 666f 7366 7465     buffers::uoffset
 80b55a8:	745f 3d20 6c20 6e6f 2067 6e75 6973 6e67     _t = long unsign
 80b55b8:	6465 6920 746e 005d                         ed int].

080b55c0 <_ZZNK11flatbuffers6VectorINS_6OffsetIN6tflite6BufferEEEE3GetEmE19__PRETTY_FUNCTION__>:
 80b55c0:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
 80b55d0:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
 80b55e0:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
 80b55f0:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
 80b5600:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
 80b5610:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
 80b5620:	7469 2068 2054 203d 6c66 7461 7562 6666     ith T = flatbuff
 80b5630:	7265 3a73 4f3a 6666 6573 3c74 6674 696c     ers::Offset<tfli
 80b5640:	6574 3a3a 7542 6666 7265 3b3e 6620 616c     te::Buffer>; fla
 80b5650:	6274 6675 6566 7372 3a3a 6556 7463 726f     tbuffers::Vector
 80b5660:	543c 3a3e 723a 7465 7275 5f6e 7974 6570     <T>::return_type
 80b5670:	3d20 6320 6e6f 7473 7420 6c66 7469 3a65      = const tflite:
 80b5680:	423a 6675 6566 2a72 203b 6c66 7461 7562     :Buffer*; flatbu
 80b5690:	6666 7265 3a73 753a 666f 7366 7465 745f     ffers::uoffset_t
 80b56a0:	3d20 6c20 6e6f 2067 6e75 6973 6e67 6465      = long unsigned
 80b56b0:	6920 746e 005d                               int].

080b56b6 <_ZZNK11flatbuffers6VectorIxE3GetEmE19__PRETTY_FUNCTION__>:
 80b56b6:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
 80b56c6:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
 80b56d6:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
 80b56e6:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
 80b56f6:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
 80b5706:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
 80b5716:	7469 2068 2054 203d 6f6c 676e 6c20 6e6f     ith T = long lon
 80b5726:	2067 6e69 3b74 6620 616c 6274 6675 6566     g int; flatbuffe
 80b5736:	7372 3a3a 6556 7463 726f 543c 3a3e 723a     rs::Vector<T>::r
 80b5746:	7465 7275 5f6e 7974 6570 3d20 6c20 6e6f     eturn_type = lon
 80b5756:	2067 6f6c 676e 6920 746e 203b 6c66 7461     g long int; flat
 80b5766:	7562 6666 7265 3a73 753a 666f 7366 7465     buffers::uoffset
 80b5776:	745f 3d20 6c20 6e6f 2067 6e75 6973 6e67     _t = long unsign
 80b5786:	6465 6920 746e 005d 0000                    ed int]...

080b5790 <_ZTVN6tflite18MicroErrorReporterE>:
	...
 80b5798:	0135 080a 0149 080a 1d15 080a 0a0d 5400     5...I..........T
 80b57a8:	6e65 6f73 2072 6e69 6564 2078 6425 6f20     ensor index %d o
 80b57b8:	7475 6f20 2066 6172 676e 2065 6c28 6e65     ut of range (len
 80b57c8:	7467 2068 7369 2520 2964 4f00 7475 7570     gth is %d).Outpu
 80b57d8:	2074 6e69 6564 2078 6425 6f20 7475 6f20     t index %d out o
 80b57e8:	2066 6172 676e 2065 6c28 6e65 7467 2068     f range (length 
 80b57f8:	7369 2520 2964 4900 706e 7475 6920 646e     is %d).Input ind
 80b5808:	7865 2520 2064 756f 2074 666f 7220 6e61     ex %d out of ran
 80b5818:	6567 2820 656c 676e 6874 6920 2073 6425     ge (length is %d
 80b5828:	0029 6e49 6f76 656b 2928 6320 6c61 656c     ).Invoke() calle
 80b5838:	2064 6661 6574 2072 6e69 7469 6169 696c     d after initiali
 80b5848:	617a 6974 6e6f 6620 6961 656c 0a64 4d00     zation failed..M
 80b5858:	7369 6973 676e 7220 6765 7369 7274 7461     issing registrat
 80b5868:	6f69 206e 6f66 2072 706f 6f63 6564 695f     ion for opcode_i
 80b5878:	646e 7865 2520 0a64 5300 696b 7070 6e69     ndex %d..Skippin
 80b5888:	2067 706f 6620 726f 6f20 6370 646f 5f65     g op for opcode_
 80b5898:	6e69 6564 2078 6425 000a 6e55 7573 7070     index %d..Unsupp
 80b58a8:	726f 6574 2064 6562 6168 6976 726f 203a     orted behavior: 
 80b58b8:	6f66 6e75 2064 7562 6c69 6974 206e 706f     found builtin op
 80b58c8:	7265 7461 726f 2520 2073 6977 6874 6320     erator %s with c
 80b58d8:	7375 6f74 206d 706f 6974 6e6f 2e73 000a     ustom options...
 80b58e8:	6f4e 6564 2520 2073 6e28 6d75 6562 2072     Node %s (number 
 80b58f8:	6425 2029 6166 6c69 6465 7420 206f 7270     %d) failed to pr
 80b5908:	7065 7261 2065 6977 6874 7320 6174 7574     epare with statu
 80b5918:	2073 6425 4e00 646f 2065 7325 2820 756e     s %d.Node %s (nu
 80b5928:	626d 7265 2520 2964 6620 6961 656c 2064     mber %d) failed 
 80b5938:	6f74 6920 766e 6b6f 2065 6977 6874 7320     to invoke with s
 80b5948:	6174 7574 2073 6425 0000 0000               tatus %d....

080b5954 <_ZTVN6tflite12_GLOBAL__N_118StackDataAllocatorE>:
	...
 80b595c:	1dc9 080a 1dd3 080a 1dd5 080a 1df7 080a     ................
 80b596c:	7865 6f70 656e 746e 3e20 203d 0030 552f     exponent >= 0./U
 80b597c:	6573 7372 622f 6173 7274 6d6f 442f 7665     sers/bsatrom/Dev
 80b598c:	6c65 706f 656d 746e 702f 7261 6974 6c63     elopment/particl
 80b599c:	2f65 696c 7262 7261 6569 2f73 6150 7472     e/libraries/Part
 80b59ac:	6369 656c 545f 6e65 6f73 4672 6f6c 4c77     icle_TensorFlowL
 80b59bc:	7469 5f65 7845 6d61 6c70 7365 682f 6c65     ite_Examples/hel
 80b59cc:	6f6c 775f 726f 646c 6c2f 6269 542f 6e65     lo_world/lib/Ten
 80b59dc:	6f73 4672 6f6c 4c77 7469 2f65 7273 2f63     sorFlowLite/src/
 80b59ec:	6874 7269 5f64 6170 7472 2f79 6567 6d6d     third_party/gemm
 80b59fc:	6f6c 7077 662f 7869 6465 6f70 6e69 2f74     lowp/fixedpoint/
 80b5a0c:	6966 6578 7064 696f 746e 682e 6500 7078     fixedpoint.h.exp
 80b5a1c:	6e6f 6e65 2074 3d3c 3320 0031 6e49 7570     onent <= 31.Inpu
 80b5a2c:	7374 6120 646e 6f20 7475 7570 7374 6e20     ts and outputs n
 80b5a3c:	746f 6120 6c6c 6620 6f6c 7461 757c 6e69     ot all float|uin
 80b5a4c:	3874 697c 746e 2038 7974 6570 2e73 4900          t8|int8 types..

080b5a5b <_ZZN8gemmlowp19RoundingDivideByPOTIlEET_S1_iE19__PRETTY_FUNCTION__>:
 80b5a5b:	6e49 6574 6567 5472 7079 2065 6567 6d6d     IntegerType gemm
 80b5a6b:	6f6c 7077 3a3a 6f52 6e75 6964 676e 6944     lowp::RoundingDi
 80b5a7b:	6976 6564 7942 4f50 2854 6e49 6574 6567     videByPOT(Intege
 80b5a8b:	5472 7079 2c65 6920 746e 2029 775b 7469     rType, int) [wit
 80b5a9b:	2068 6e49 6574 6567 5472 7079 2065 203d     h IntegerType = 
 80b5aab:	6f6c 676e 6920 746e 005d 0000 0000               long int]....

080b5ab8 <_ZTVN6tflite3ops5micro14AllOpsResolverE>:
	...
 80b5ac0:	2145 080a 2173 080a 0137 080a 0139 080a     E!..s!..7...9...
 80b5ad0:	6e4f 796c 6620 6f6c 7461 3233 202c 6975     Only float32, ui
 80b5ae0:	746e 2038 6e61 2064 6e69 3874 6120 6572     nt8 and int8 are
 80b5af0:	7320 7075 6f70 7472 6465 6320 7275 6572      supported curre
 80b5b00:	746e 796c 202c 6f67 2074 7325 002e 6e4f     ntly, got %s..On
 80b5b10:	796c 6920 746e 3233 6120 6572 7320 7075     ly int32 are sup
 80b5b20:	6f70 7472 6465 6320 7275 6572 746e 796c     ported currently
 80b5b30:	202c 6f67 2074 7325 002e 552f 6573 7372     , got %s../Users
 80b5b40:	622f 6173 7274 6d6f 442f 7665 6c65 706f     /bsatrom/Develop
 80b5b50:	656d 746e 702f 7261 6974 6c63 2f65 696c     ment/particle/li
 80b5b60:	7262 7261 6569 2f73 6150 7472 6369 656c     braries/Particle
 80b5b70:	545f 6e65 6f73 4672 6f6c 4c77 7469 5f65     _TensorFlowLite_
 80b5b80:	7845 6d61 6c70 7365 682f 6c65 6f6c 775f     Examples/hello_w
 80b5b90:	726f 646c 6c2f 6269 542f 6e65 6f73 4672     orld/lib/TensorF
 80b5ba0:	6f6c 4c77 7469 2f65 7273 2f63 6574 736e     lowLite/src/tens
 80b5bb0:	726f 6c66 776f 6c2f 7469 2f65 7865 6570     orflow/lite/expe
 80b5bc0:	6972 656d 746e 6c61 6d2f 6369 6f72 6b2f     rimental/micro/k
 80b5bd0:	7265 656e 736c 632f 6965 2e6c 7063 0070     ernels/ceil.cpp.
 80b5be0:	7325 253a 2064 7325 2120 203d 7325 2820     %s:%d %s != %s (
 80b5bf0:	6425 2120 203d 6425 0029 754e 496d 706e     %d != %d).NumInp
 80b5c00:	7475 2873 6f6e 6564 0029 754e 4f6d 7475     uts(node).NumOut
 80b5c10:	7570 7374 6e28 646f 2965 6900 706e 7475     puts(node).input
 80b5c20:	3e2d 7974 6570 6f00 7475 7570 2d74 743e     ->type.output->t
 80b5c30:	7079 0065 6e69 7570 2d74 623e 7479 7365     ype.input->bytes
 80b5c40:	6f00 7475 7570 2d74 623e 7479 7365 6900     .output->bytes.i
 80b5c50:	706e 7475 3e2d 6964 736d 3e2d 6973 657a     nput->dims->size
 80b5c60:	6f00 7475 7570 2d74 643e 6d69 2d73 733e     .output->dims->s
 80b5c70:	7a69 0065 6e69 7570 2d74 643e 6d69 2d73     ize.input->dims-
 80b5c80:	643e 7461 5b61 5d69 6f00 7475 7570 2d74     >data[i].output-
 80b5c90:	643e 6d69 2d73 643e 7461 5b61 5d69 4400     >dims->data[i].D
 80b5ca0:	656f 2073 6f6e 2074 7573 7070 726f 2074     oes not support 
 80b5cb0:	7974 6570 2520 2c64 7220 7165 6975 6572     type %d, require
 80b5cc0:	2073 6f62 6c6f 667c 6f6c 7461 697c 746e     s bool|float|int
 80b5cd0:	757c 6e69 3874 4400 656f 2073 6f6e 2074     |uint8.Does not 
 80b5ce0:	7573 7070 726f 2074 7974 6570 2520 2c64     support type %d,
 80b5cf0:	7220 7165 6975 6572 2073 6c66 616f 7c74      requires float|
 80b5d00:	6e69 7c74 6975 746e 0038 552f 6573 7372     int|uint8./Users
 80b5d10:	622f 6173 7274 6d6f 442f 7665 6c65 706f     /bsatrom/Develop
 80b5d20:	656d 746e 702f 7261 6974 6c63 2f65 696c     ment/particle/li
 80b5d30:	7262 7261 6569 2f73 6150 7472 6369 656c     braries/Particle
 80b5d40:	545f 6e65 6f73 4672 6f6c 4c77 7469 5f65     _TensorFlowLite_
 80b5d50:	7845 6d61 6c70 7365 682f 6c65 6f6c 775f     Examples/hello_w
 80b5d60:	726f 646c 6c2f 6269 542f 6e65 6f73 4672     orld/lib/TensorF
 80b5d70:	6f6c 4c77 7469 2f65 7273 2f63 6574 736e     lowLite/src/tens
 80b5d80:	726f 6c66 776f 6c2f 7469 2f65 7865 6570     orflow/lite/expe
 80b5d90:	6972 656d 746e 6c61 6d2f 6369 6f72 6b2f     rimental/micro/k
 80b5da0:	7265 656e 736c 632f 6e6f 2e76 7063 0070     ernels/conv.cpp.
 80b5db0:	7325 253a 2064 7325 7720 7361 6e20 746f     %s:%d %s was not
 80b5dc0:	7420 7572 2e65 6800 7361 625f 6169 2073      true..has_bias 
 80b5dd0:	7c7c 6e20 646f 2d65 693e 706e 7475 2d73     || node->inputs-
 80b5de0:	733e 7a69 2065 3d3d 3220 6e00 646f 2d65     >size == 2.node-
 80b5df0:	6f3e 7475 7570 7374 3e2d 6973 657a 6b00     >outputs->size.k
 80b5e00:	6654 694c 6574 6641 6966 656e 7551 6e61     TfLiteAffineQuan
 80b5e10:	6974 617a 6974 6e6f 6600 6c69 6574 2d72     tization.filter-
 80b5e20:	713e 6175 746e 7a69 7461 6f69 2e6e 7974     >quantization.ty
 80b5e30:	6570 6100 6666 6e69 5f65 7571 6e61 6974     pe.affine_quanti
 80b5e40:	617a 6974 6e6f 6100 6666 6e69 5f65 7571     zation.affine_qu
 80b5e50:	6e61 6974 617a 6974 6e6f 3e2d 6373 6c61     antization->scal
 80b5e60:	0065 7954 6570 2520 2073 2528 2964 6e20     e.Type %s (%d) n
 80b5e70:	746f 7320 7075 6f70 7472 6465 002e          ot supported..

080b5e7e <_ZZN8gemmlowp19RoundingDivideByPOTIlEET_S1_iE19__PRETTY_FUNCTION__>:
 80b5e7e:	6e49 6574 6567 5472 7079 2065 6567 6d6d     IntegerType gemm
 80b5e8e:	6f6c 7077 3a3a 6f52 6e75 6964 676e 6944     lowp::RoundingDi
 80b5e9e:	6976 6564 7942 4f50 2854 6e49 6574 6567     videByPOT(Intege
 80b5eae:	5472 7079 2c65 6920 746e 2029 775b 7469     rType, int) [wit
 80b5ebe:	2068 6e49 6574 6567 5472 7079 2065 203d     h IntegerType = 
 80b5ece:	6f6c 676e 6920 746e 005d 552f 6573 7372     long int]./Users
 80b5ede:	622f 6173 7274 6d6f 442f 7665 6c65 706f     /bsatrom/Develop
 80b5eee:	656d 746e 702f 7261 6974 6c63 2f65 696c     ment/particle/li
 80b5efe:	7262 7261 6569 2f73 6150 7472 6369 656c     braries/Particle
 80b5f0e:	545f 6e65 6f73 4672 6f6c 4c77 7469 5f65     _TensorFlowLite_
 80b5f1e:	7845 6d61 6c70 7365 682f 6c65 6f6c 775f     Examples/hello_w
 80b5f2e:	726f 646c 6c2f 6269 542f 6e65 6f73 4672     orld/lib/TensorF
 80b5f3e:	6f6c 4c77 7469 2f65 7273 2f63 6574 736e     lowLite/src/tens
 80b5f4e:	726f 6c66 776f 6c2f 7469 2f65 7865 6570     orflow/lite/expe
 80b5f5e:	6972 656d 746e 6c61 6d2f 6369 6f72 6b2f     rimental/micro/k
 80b5f6e:	7265 656e 736c 642f 7165 6175 746e 7a69     ernels/dequantiz
 80b5f7e:	2e65 7063 0070 6e69 7570 2d74 743e 7079     e.cpp.input->typ
 80b5f8e:	2065 3d3d 6b20 6654 694c 6574 4955 746e     e == kTfLiteUInt
 80b5f9e:	2038 7c7c 6920 706e 7475 3e2d 7974 6570     8 || input->type
 80b5fae:	3d20 203d 546b 4c66 7469 4965 746e 0038      == kTfLiteInt8.
 80b5fbe:	756f 7074 7475 3e2d 7974 6570 3d20 203d     output->type == 
 80b5fce:	546b 4c66 7469 4665 6f6c 7461 3233 2f00     kTfLiteFloat32./
 80b5fde:	7355 7265 2f73 7362 7461 6f72 2f6d 6544     Users/bsatrom/De
 80b5fee:	6576 6f6c 6d70 6e65 2f74 6170 7472 6369     velopment/partic
 80b5ffe:	656c 6c2f 6269 6172 6972 7365 502f 7261     le/libraries/Par
 80b600e:	6974 6c63 5f65 6554 736e 726f 6c46 776f     ticle_TensorFlow
 80b601e:	694c 6574 455f 6178 706d 656c 2f73 6568     Lite_Examples/he
 80b602e:	6c6c 5f6f 6f77 6c72 2f64 696c 2f62 6554     llo_world/lib/Te
 80b603e:	736e 726f 6c46 776f 694c 6574 732f 6372     nsorFlowLite/src
 80b604e:	742f 6e65 6f73 6672 6f6c 2f77 696c 6574     /tensorflow/lite
 80b605e:	652f 7078 7265 6d69 6e65 6174 2f6c 696d     /experimental/mi
 80b606e:	7263 2f6f 656b 6e72 6c65 2f73 6c65 6d65     cro/kernels/elem
 80b607e:	6e65 7774 7369 2e65 7063 0070 6e49 7570     entwise.cpp.Inpu
 80b608e:	2074 6164 6174 7420 7079 2065 7325 2820     t data type %s (
 80b609e:	6425 2029 7369 6e20 746f 7320 7075 6f70     %d) is not suppo
 80b60ae:	7472 6465 002e 7865 6570 7463 6465 745f     rted..expected_t
 80b60be:	7079 0065 552f 6573 7372 622f 6173 7274     ype./Users/bsatr
 80b60ce:	6d6f 442f 7665 6c65 706f 656d 746e 702f     om/Development/p
 80b60de:	7261 6974 6c63 2f65 696c 7262 7261 6569     article/librarie
 80b60ee:	2f73 6150 7472 6369 656c 545f 6e65 6f73     s/Particle_Tenso
 80b60fe:	4672 6f6c 4c77 7469 5f65 7845 6d61 6c70     rFlowLite_Exampl
 80b610e:	7365 682f 6c65 6f6c 775f 726f 646c 6c2f     es/hello_world/l
 80b611e:	6269 542f 6e65 6f73 4672 6f6c 4c77 7469     ib/TensorFlowLit
 80b612e:	2f65 7273 2f63 6574 736e 726f 6c66 776f     e/src/tensorflow
 80b613e:	6c2f 7469 2f65 7865 6570 6972 656d 746e     /lite/experiment
 80b614e:	6c61 6d2f 6369 6f72 6b2f 7265 656e 736c     al/micro/kernels
 80b615e:	662f 6f6c 726f 632e 7070 5100 6175 746e     /floor.cpp.Quant
 80b616e:	7a69 6465 4620 6c75 796c 6f43 6e6e 6365     ized FullyConnec
 80b617e:	6574 2064 7865 6570 7463 2073 756f 7074     ted expects outp
 80b618e:	7475 6420 7461 2061 7974 6570 7520 6e69     ut data type uin
 80b619e:	3874 6f20 2072 6e69 3174 0036 7954 6570     t8 or int16.Type
 80b61ae:	2520 2064 6f6e 2074 7563 7272 6e65 6c74      %d not currentl
 80b61be:	2079 7573 7070 726f 6574 2e64 4f00 6c6e     y supported..Onl
 80b61ce:	2079 6c66 616f 3374 2032 7369 7320 7075     y float32 is sup
 80b61de:	6f70 7472 6465 6320 7275 6572 746e 796c     ported currently
 80b61ee:	202c 6f67 2074 7325 5400 7079 2065 7325     , got %s.Type %s
 80b61fe:	2820 6425 2029 7369 6e20 746f 7320 7075      (%d) is not sup
 80b620e:	6f70 7472 6465 6220 2079 614d 6978 756d     ported by Maximu
 80b621e:	2f6d 694d 696e 756d 2e6d 4e00 6765 6f20     m/Minimum..Neg o
 80b622e:	6c6e 2079 7563 7272 6e65 6c74 2079 7573     nly currently su
 80b623e:	7070 726f 7374 6620 6f6c 7461 3233 202c     pports float32, 
 80b624e:	6f67 2074 6425 002e 7954 6570 2720 7325     got %d..Type '%s
 80b625e:	2027 7369 6e20 746f 7320 7075 6f70 7472     ' is not support
 80b626e:	6465 6220 2079 6170 6b63 002e 7954 6570     ed by pack..Type
 80b627e:	2520 2073 6f6e 2074 7563 7272 6e65 6c74      %s not currentl
 80b628e:	2079 7573 7070 726f 6574 2e64 4900 706e     y supported..Inp
 80b629e:	7475 7420 7079 2065 7325 6920 2073 6f6e     ut type %s is no
 80b62ae:	2074 7563 7272 6e65 6c74 2079 7573 7070     t currently supp
 80b62be:	726f 6574 0064 6e4f 796c 6620 6f6c 7461     orted.Only float
 80b62ce:	3233 6120 646e 7520 6e69 3874 6120 6572     32 and uint8 are
 80b62de:	7320 7075 6f70 7472 6465 6320 7275 6572      supported curre
 80b62ee:	746e 796c 202c 6f67 2074 6425 002e          ntly, got %d..

080b62fc <_ZZN8gemmlowp19RoundingDivideByPOTIlEET_S1_iE19__PRETTY_FUNCTION__>:
 80b62fc:	6e49 6574 6567 5472 7079 2065 6567 6d6d     IntegerType gemm
 80b630c:	6f6c 7077 3a3a 6f52 6e75 6964 676e 6944     lowp::RoundingDi
 80b631c:	6976 6564 7942 4f50 2854 6e49 6574 6567     videByPOT(Intege
 80b632c:	5472 7079 2c65 6920 746e 2029 775b 7469     rType, int) [wit
 80b633c:	2068 6e49 6574 6567 5472 7079 2065 203d     h IntegerType = 
 80b634c:	6f6c 676e 6920 746e 005d 552f 6573 7372     long int]./Users
 80b635c:	622f 6173 7274 6d6f 442f 7665 6c65 706f     /bsatrom/Develop
 80b636c:	656d 746e 702f 7261 6974 6c63 2f65 696c     ment/particle/li
 80b637c:	7262 7261 6569 2f73 6150 7472 6369 656c     braries/Particle
 80b638c:	545f 6e65 6f73 4672 6f6c 4c77 7469 5f65     _TensorFlowLite_
 80b639c:	7845 6d61 6c70 7365 682f 6c65 6f6c 775f     Examples/hello_w
 80b63ac:	726f 646c 6c2f 6269 542f 6e65 6f73 4672     orld/lib/TensorF
 80b63bc:	6f6c 4c77 7469 2f65 7273 2f63 6574 736e     lowLite/src/tens
 80b63cc:	726f 6c66 776f 6c2f 7469 2f65 7865 6570     orflow/lite/expe
 80b63dc:	6972 656d 746e 6c61 6d2f 6369 6f72 6b2f     rimental/micro/k
 80b63ec:	7265 656e 736c 712f 6175 746e 7a69 2e65     ernels/quantize.
 80b63fc:	7063 0070 756f 7074 7475 3e2d 7571 6e61     cpp.output->quan
 80b640c:	6974 617a 6974 6e6f 742e 7079 0065 6661     tization.type.af
 80b641c:	6966 656e 715f 6175 746e 7a69 7461 6f69     fine_quantizatio
 80b642c:	2d6e 733e 6163 656c 3e2d 6973 657a 3d20     n->scale->size =
 80b643c:	203d 0031 6e69 7570 2d74 743e 7079 2065     = 1.input->type 
 80b644c:	3d3d 6b20 6654 694c 6574 6c46 616f 3374     == kTfLiteFloat3
 80b645c:	0032 756f 7074 7475 3e2d 7974 6570 3d20     2.output->type =
 80b646c:	203d 546b 4c66 7469 5565 6e49 3874 7c20     = kTfLiteUInt8 |
 80b647c:	207c 756f 7074 7475 3e2d 7974 6570 3d20     | output->type =
 80b648c:	203d 546b 4c66 7469 4965 746e 0038 754f     = kTfLiteInt8.Ou
 80b649c:	7074 7475 7420 7079 2065 7325 2820 6425     tput type %s (%d
 80b64ac:	2029 6f6e 2074 7573 7070 726f 6574 0064     ) not supported.
 80b64bc:	552f 6573 7372 622f 6173 7274 6d6f 442f     /Users/bsatrom/D
 80b64cc:	7665 6c65 706f 656d 746e 702f 7261 6974     evelopment/parti
 80b64dc:	6c63 2f65 696c 7262 7261 6569 2f73 6150     cle/libraries/Pa
 80b64ec:	7472 6369 656c 545f 6e65 6f73 4672 6f6c     rticle_TensorFlo
 80b64fc:	4c77 7469 5f65 7845 6d61 6c70 7365 682f     wLite_Examples/h
 80b650c:	6c65 6f6c 775f 726f 646c 6c2f 6269 542f     ello_world/lib/T
 80b651c:	6e65 6f73 4672 6f6c 4c77 7469 2f65 7273     ensorFlowLite/sr
 80b652c:	2f63 6574 736e 726f 6c66 776f 6c2f 7469     c/tensorflow/lit
 80b653c:	2f65 7865 6570 6972 656d 746e 6c61 6d2f     e/experimental/m
 80b654c:	6369 6f72 6b2f 7265 656e 736c 722f 7365     icro/kernels/res
 80b655c:	6168 6570 632e 7070 4e00 6d75 6e49 7570     hape.cpp.NumInpu
 80b656c:	7374 6e28 646f 2965 3d20 203d 2031 7c7c     ts(node) == 1 ||
 80b657c:	4e20 6d75 6e49 7570 7374 6e28 646f 2965      NumInputs(node)
 80b658c:	3d20 203d 0032 312d 7300 7274 7465 6863      == 2.-1.stretch
 80b659c:	645f 6d69 6e00 6d75 6f5f 7475 7570 5f74     _dim.num_output_
 80b65ac:	6c65 6d65 6e65 7374 6e00 6d75 695f 706e     elements.num_inp
 80b65bc:	7475 655f 656c 656d 746e 0073 552f 6573     ut_elements./Use
 80b65cc:	7372 622f 6173 7274 6d6f 442f 7665 6c65     rs/bsatrom/Devel
 80b65dc:	706f 656d 746e 702f 7261 6974 6c63 2f65     opment/particle/
 80b65ec:	696c 7262 7261 6569 2f73 6150 7472 6369     libraries/Partic
 80b65fc:	656c 545f 6e65 6f73 4672 6f6c 4c77 7469     le_TensorFlowLit
 80b660c:	5f65 7845 6d61 6c70 7365 682f 6c65 6f6c     e_Examples/hello
 80b661c:	775f 726f 646c 6c2f 6269 542f 6e65 6f73     _world/lib/Tenso
 80b662c:	4672 6f6c 4c77 7469 2f65 7273 2f63 6574     rFlowLite/src/te
 80b663c:	736e 726f 6c66 776f 6c2f 7469 2f65 7865     nsorflow/lite/ex
 80b664c:	6570 6972 656d 746e 6c61 6d2f 6369 6f72     perimental/micro
 80b665c:	6b2f 7265 656e 736c 722f 756f 646e 632e     /kernels/round.c
 80b666c:	7070 2f00 7355 7265 2f73 7362 7461 6f72     pp./Users/bsatro
 80b667c:	2f6d 6544 6576 6f6c 6d70 6e65 2f74 6170     m/Development/pa
 80b668c:	7472 6369 656c 6c2f 6269 6172 6972 7365     rticle/libraries
 80b669c:	502f 7261 6974 6c63 5f65 6554 736e 726f     /Particle_Tensor
 80b66ac:	6c46 776f 694c 6574 455f 6178 706d 656c     FlowLite_Example
 80b66bc:	2f73 6568 6c6c 5f6f 6f77 6c72 2f64 696c     s/hello_world/li
 80b66cc:	2f62 6554 736e 726f 6c46 776f 694c 6574     b/TensorFlowLite
 80b66dc:	732f 6372 742f 6e65 6f73 6672 6f6c 2f77     /src/tensorflow/
 80b66ec:	696c 6574 652f 7078 7265 6d69 6e65 6174     lite/experimenta
 80b66fc:	2f6c 696d 7263 2f6f 656b 6e72 6c65 2f73     l/micro/kernels/
 80b670c:	6f73 7466 616d 2e78 7063 0070 756f 7074     softmax.cpp.outp
 80b671c:	7475 3e2d 6170 6172 736d 7a2e 7265 5f6f     ut->params.zero_
 80b672c:	6f70 6e69 0074 756f 7074 7475 3e2d 6170     point.output->pa
 80b673c:	6172 736d 732e 6163 656c 3d20 203d 2e31     rams.scale == 1.
 80b674c:	2066 202f 3532 0036 6e4f 796c 3120 2c44     f / 256.Only 1D,
 80b675c:	3220 2044 6e61 2064 4434 7420 6e65 6f73      2D and 4D tenso
 80b676c:	7372 7320 7075 6f70 7472 6465 6320 7275     rs supported cur
 80b677c:	6572 746e 796c 202c 6f67 2074 6425 2e44     rently, got %dD.
 80b678c:	4f00 6c6e 2079 4432 6120 646e 3420 2044     .Only 2D and 4D 
 80b679c:	6574 736e 726f 2073 7573 7070 726f 6574     tensors supporte
 80b67ac:	2064 7563 7272 6e65 6c74 2c79 6720 746f     d currently, got
 80b67bc:	2520 4464 002e 6e4f 796c 6620 6f6c 7461      %dD..Only float
 80b67cc:	3233 6120 646e 7520 6e69 3874 745f 7320     32 and uint8_t s
 80b67dc:	7075 6f70 7472 6465 6320 7275 6572 746e     upported current
 80b67ec:	796c 202c 6f67 2074 6425 002e               ly, got %d..

080b67f8 <_ZZN8gemmlowp19RoundingDivideByPOTIlEET_S1_iE19__PRETTY_FUNCTION__>:
 80b67f8:	6e49 6574 6567 5472 7079 2065 6567 6d6d     IntegerType gemm
 80b6808:	6f6c 7077 3a3a 6f52 6e75 6964 676e 6944     lowp::RoundingDi
 80b6818:	6976 6564 7942 4f50 2854 6e49 6574 6567     videByPOT(Intege
 80b6828:	5472 7079 2c65 6920 746e 2029 775b 7469     rType, int) [wit
 80b6838:	2068 6e49 6574 6567 5472 7079 2065 203d     h IntegerType = 
 80b6848:	6f6c 676e 6920 746e 005d 552f 6573 7372     long int]./Users
 80b6858:	622f 6173 7274 6d6f 442f 7665 6c65 706f     /bsatrom/Develop
 80b6868:	656d 746e 702f 7261 6974 6c63 2f65 696c     ment/particle/li
 80b6878:	7262 7261 6569 2f73 6150 7472 6369 656c     braries/Particle
 80b6888:	545f 6e65 6f73 4672 6f6c 4c77 7469 5f65     _TensorFlowLite_
 80b6898:	7845 6d61 6c70 7365 682f 6c65 6f6c 775f     Examples/hello_w
 80b68a8:	726f 646c 6c2f 6269 542f 6e65 6f73 4672     orld/lib/TensorF
 80b68b8:	6f6c 4c77 7469 2f65 7273 2f63 6574 736e     lowLite/src/tens
 80b68c8:	726f 6c66 776f 6c2f 7469 2f65 7865 6570     orflow/lite/expe
 80b68d8:	6972 656d 746e 6c61 6d2f 6369 6f72 6b2f     rimental/micro/k
 80b68e8:	7265 656e 736c 732f 6c70 7469 632e 7070     ernels/split.cpp
 80b68f8:	4e20 6e6f 6320 6e6f 7473 6e61 2074 7861      Non constant ax
 80b6908:	7369 7420 6e65 6f73 2072 6f6e 2074 7573     is tensor not su
 80b6918:	7070 726f 6574 0064 552f 6573 7372 622f     pported./Users/b
 80b6928:	6173 7274 6d6f 442f 7665 6c65 706f 656d     satrom/Developme
 80b6938:	746e 702f 7261 6974 6c63 2f65 696c 7262     nt/particle/libr
 80b6948:	7261 6569 2f73 6150 7472 6369 656c 545f     aries/Particle_T
 80b6958:	6e65 6f73 4672 6f6c 4c77 7469 5f65 7845     ensorFlowLite_Ex
 80b6968:	6d61 6c70 7365 682f 6c65 6f6c 775f 726f     amples/hello_wor
 80b6978:	646c 6c2f 6269 542f 6e65 6f73 4672 6f6c     ld/lib/TensorFlo
 80b6988:	4c77 7469 2f65 7273 2f63 6574 736e 726f     wLite/src/tensor
 80b6998:	6c66 776f 6c2f 7469 2f65 7865 6570 6972     flow/lite/experi
 80b69a8:	656d 746e 6c61 6d2f 6369 6f72 6b2f 7265     mental/micro/ker
 80b69b8:	656e 736c 732f 6c70 7469 632e 7070 6100     nels/split.cpp.a
 80b69c8:	6978 5f73 6176 756c 2065 3d3e 3020 6100     xis_value >= 0.a
 80b69d8:	6978 5f73 6176 756c 2065 203c 754e 446d     xis_value < NumD
 80b69e8:	6d69 6e65 6973 6e6f 2873 6e69 7570 2974     imensions(input)
 80b69f8:	5400 7079 2065 7325 6320 7275 6572 746e     .Type %s current
 80b6a08:	796c 6e20 746f 7320 7075 6f70 7472 6465     ly not supported
 80b6a18:	002e 552f 6573 7372 622f 6173 7274 6d6f     ../Users/bsatrom
 80b6a28:	442f 7665 6c65 706f 656d 746e 702f 7261     /Development/par
 80b6a38:	6974 6c63 2f65 696c 7262 7261 6569 2f73     ticle/libraries/
 80b6a48:	6150 7472 6369 656c 545f 6e65 6f73 4672     Particle_TensorF
 80b6a58:	6f6c 4c77 7469 5f65 7845 6d61 6c70 7365     lowLite_Examples
 80b6a68:	682f 6c65 6f6c 775f 726f 646c 6c2f 6269     /hello_world/lib
 80b6a78:	542f 6e65 6f73 4672 6f6c 4c77 7469 2f65     /TensorFlowLite/
 80b6a88:	7273 2f63 6574 736e 726f 6c66 776f 6c2f     src/tensorflow/l
 80b6a98:	7469 2f65 7865 6570 6972 656d 746e 6c61     ite/experimental
 80b6aa8:	6d2f 6369 6f72 6b2f 7265 656e 736c 732f     /micro/kernels/s
 80b6ab8:	7274 6469 6465 735f 696c 6563 632e 7070     trided_slice.cpp
 80b6ac8:	7320 7274 6469 2065 6176 756c 2065 6168      stride value ha
 80b6ad8:	2073 6f74 6220 2065 6f6e 2d6e 657a 6f72     s to be non-zero
 80b6ae8:	2f00 7355 7265 2f73 7362 7461 6f72 2f6d     ./Users/bsatrom/
 80b6af8:	6544 6576 6f6c 6d70 6e65 2f74 6170 7472     Development/part
 80b6b08:	6369 656c 6c2f 6269 6172 6972 7365 502f     icle/libraries/P
 80b6b18:	7261 6974 6c63 5f65 6554 736e 726f 6c46     article_TensorFl
 80b6b28:	776f 694c 6574 455f 6178 706d 656c 2f73     owLite_Examples/
 80b6b38:	6568 6c6c 5f6f 6f77 6c72 2f64 696c 2f62     hello_world/lib/
 80b6b48:	6554 736e 726f 6c46 776f 694c 6574 732f     TensorFlowLite/s
 80b6b58:	6372 742f 6e65 6f73 6672 6f6c 2f77 696c     rc/tensorflow/li
 80b6b68:	6574 652f 7078 7265 6d69 6e65 6174 2f6c     te/experimental/
 80b6b78:	696d 7263 2f6f 656b 6e72 6c65 2f73 7473     micro/kernels/st
 80b6b88:	6972 6564 5f64 6c73 6369 2e65 7063 0070     rided_slice.cpp.
 80b6b98:	6964 5f6d 6873 7061 0065 756f 7074 7475     dim_shape.output
 80b6ba8:	735f 6168 6570 3e2d 6164 6174 735b 6168     _shape->data[sha
 80b6bb8:	6570 735f 7a69 5d65 7300 6168 6570 735f     pe_size].shape_s
 80b6bc8:	7a69 0065 756f 7074 7475 735f 6168 6570     ize.output_shape
 80b6bd8:	3e2d 6973 657a 2f00 7355 7265 2f73 7362     ->size./Users/bs
 80b6be8:	7461 6f72 2f6d 6544 6576 6f6c 6d70 6e65     atrom/Developmen
 80b6bf8:	2f74 6170 7472 6369 656c 6c2f 6269 6172     t/particle/libra
 80b6c08:	6972 7365 502f 7261 6974 6c63 5f65 6554     ries/Particle_Te
 80b6c18:	736e 726f 6c46 776f 694c 6574 455f 6178     nsorFlowLite_Exa
 80b6c28:	706d 656c 2f73 6568 6c6c 5f6f 6f77 6c72     mples/hello_worl
 80b6c38:	2f64 696c 2f62 6554 736e 726f 6c46 776f     d/lib/TensorFlow
 80b6c48:	694c 6574 732f 6372 742f 6e65 6f73 6672     Lite/src/tensorf
 80b6c58:	6f6c 2f77 696c 6574 652f 7078 7265 6d69     low/lite/experim
 80b6c68:	6e65 6174 2f6c 696d 7263 2f6f 656b 6e72     ental/micro/kern
 80b6c78:	6c65 2f73 7473 6972 6564 5f64 6c73 6369     els/strided_slic
 80b6c88:	2e65 7063 2070 6e69 7570 2074 6964 206d     e.cpp input dim 
 80b6c98:	6873 756f 646c 6e20 746f 6520 6378 6565     should not excee
 80b6ca8:	2064 0034 7954 6570 2520 2064 7369 6320     d 4.Type %d is c
 80b6cb8:	7275 6572 746e 796c 6e20 746f 7320 7075     urrently not sup
 80b6cc8:	6f70 7472 6465 6220 2079 7453 6972 6564     ported by Stride
 80b6cd8:	5364 696c 6563 002e 552f 6573 7372 622f     dSlice../Users/b
 80b6ce8:	6173 7274 6d6f 442f 7665 6c65 706f 656d     satrom/Developme
 80b6cf8:	746e 702f 7261 6974 6c63 2f65 696c 7262     nt/particle/libr
 80b6d08:	7261 6569 2f73 6150 7472 6369 656c 545f     aries/Particle_T
 80b6d18:	6e65 6f73 4672 6f6c 4c77 7469 5f65 7845     ensorFlowLite_Ex
 80b6d28:	6d61 6c70 7365 682f 6c65 6f6c 775f 726f     amples/hello_wor
 80b6d38:	646c 6c2f 6269 542f 6e65 6f73 4672 6f6c     ld/lib/TensorFlo
 80b6d48:	4c77 7469 2f65 7273 2f63 6574 736e 726f     wLite/src/tensor
 80b6d58:	6c66 776f 6c2f 7469 2f65 7865 6570 6972     flow/lite/experi
 80b6d68:	656d 746e 6c61 6d2f 6369 6f72 6b2f 7265     mental/micro/ker
 80b6d78:	656e 736c 732f 6476 2e66 7063 0070 6f6e     nels/svdf.cpp.no
 80b6d88:	6564 3e2d 6e69 7570 7374 3e2d 6973 657a     de->inputs->size
 80b6d98:	6e00 6d75 665f 6c69 6574 7372 2520 7220     .num_filters % r
 80b6da8:	6e61 006b 754e 446d 6d69 6e65 6973 6e6f     ank.NumDimension
 80b6db8:	2873 6577 6769 7468 5f73 6566 7461 7275     s(weights_featur
 80b6dc8:	2965 6900 706e 7475 735f 7a69 0065 6577     e).input_size.we
 80b6dd8:	6769 7468 5f73 6566 7461 7275 2d65 643e     ights_feature->d
 80b6de8:	6d69 2d73 643e 7461 5b61 5d31 4e00 6d75     ims->data[1].Num
 80b6df8:	6944 656d 736e 6f69 736e 7728 6965 6867     Dimensions(weigh
 80b6e08:	7374 745f 6d69 2965 6e00 6d75 755f 696e     ts_time).num_uni
 80b6e18:	7374 6200 6169 2d73 643e 6d69 2d73 643e     ts.bias->dims->d
 80b6e28:	7461 5b61 5d30 6200 6169 2d73 743e 7079     ata[0].bias->typ
 80b6e38:	0065 6361 6974 6176 6974 6e6f 735f 6174     e.activation_sta
 80b6e48:	6574 3e2d 7974 6570 4e00 6d75 6944 656d     te->type.NumDime
 80b6e58:	736e 6f69 736e 6128 7463 7669 7461 6f69     nsions(activatio
 80b6e68:	5f6e 7473 7461 2965 6200 7461 6863 735f     n_state).batch_s
 80b6e78:	7a69 0065 6361 6974 6176 6974 6e6f 735f     ize.activation_s
 80b6e88:	6174 6574 3e2d 6964 736d 3e2d 6164 6174     tate->dims->data
 80b6e98:	305b 005d 656d 6f6d 7972 735f 7a69 2065     [0].memory_size 
 80b6ea8:	202a 756e 5f6d 6966 746c 7265 0073 6361     * num_filters.ac
 80b6eb8:	6974 6176 6974 6e6f 735f 6174 6574 3e2d     tivation_state->
 80b6ec8:	6964 736d 3e2d 6164 6174 315b 005d 6373     dims->data[1].sc
 80b6ed8:	6172 6374 5f68 6574 736e 726f 3e2d 7974     ratch_tensor->ty
 80b6ee8:	6570 4e00 6d75 6944 656d 736e 6f69 736e     pe.NumDimensions
 80b6ef8:	7328 7263 7461 6863 745f 6e65 6f73 2972     (scratch_tensor)
 80b6f08:	7300 7263 7461 6863 745f 6e65 6f73 2d72     .scratch_tensor-
 80b6f18:	643e 6d69 2d73 643e 7461 5b61 5d30 7300     >dims->data[0].s
 80b6f28:	7263 7461 6863 745f 6e65 6f73 2d72 643e     cratch_tensor->d
 80b6f38:	6d69 2d73 643e 7461 5b61 5d31 7700 6965     ims->data[1].wei
 80b6f48:	6867 7374 745f 6d69 2d65 743e 7079 2065     ghts_time->type 
 80b6f58:	3d3d 6b20 6654 694c 6574 4955 746e 2038     == kTfLiteUInt8 
 80b6f68:	7c7c 7720 6965 6867 7374 745f 6d69 2d65     || weights_time-
 80b6f78:	743e 7079 2065 3d3d 6b20 6654 694c 6574     >type == kTfLite
 80b6f88:	6e49 3874 6e00 646f 2d65 743e 6d65 6f70     Int8.node->tempo
 80b6f98:	6172 6972 7365 3e2d 6973 657a 7300 7263     raries->size.scr
 80b6fa8:	7461 6863 695f 706e 7475 715f 6175 746e     atch_input_quant
 80b6fb8:	7a69 6465 3e2d 7974 6570 3d20 203d 546b     ized->type == kT
 80b6fc8:	4c66 7469 5565 6e49 3874 7c20 207c 6373     fLiteUInt8 || sc
 80b6fd8:	6172 6374 5f68 6e69 7570 5f74 7571 6e61     ratch_input_quan
 80b6fe8:	6974 657a 2d64 743e 7079 2065 3d3d 6b20     tized->type == k
 80b6ff8:	6654 694c 6574 6e49 3874 7300 7263 7461     TfLiteInt8.scrat
 80b7008:	6863 695f 706e 7475 715f 6175 746e 7a69     ch_input_quantiz
 80b7018:	6465 3e2d 6964 736d 3e2d 6164 6174 305b     ed->dims->data[0
 80b7028:	005d 6373 6172 6374 5f68 6373 6c61 6e69     ].scratch_scalin
 80b7038:	5f67 6166 7463 726f 2d73 743e 7079 0065     g_factors->type.
 80b7048:	754e 446d 6d69 6e65 6973 6e6f 2873 6373     NumDimensions(sc
 80b7058:	6172 6374 5f68 6373 6c61 6e69 5f67 6166     ratch_scaling_fa
 80b7068:	7463 726f 2973 7300 7263 7461 6863 735f     ctors).scratch_s
 80b7078:	6163 696c 676e 665f 6361 6f74 7372 3e2d     caling_factors->
 80b7088:	6964 736d 3e2d 6164 6174 305b 005d 6373     dims->data[0].sc
 80b7098:	6172 6374 5f68 6c66 616f 5f74 6577 6769     ratch_float_weig
 80b70a8:	7468 5f73 6974 656d 3e2d 7974 6570 4e00     hts_time->type.N
 80b70b8:	6d75 6944 656d 736e 6f69 736e 7328 7263     umDimensions(scr
 80b70c8:	7461 6863 665f 6f6c 7461 775f 6965 6867     atch_float_weigh
 80b70d8:	7374 745f 6d69 2965 7300 7263 7461 6863     ts_time).scratch
 80b70e8:	665f 6f6c 7461 775f 6965 6867 7374 745f     _float_weights_t
 80b70f8:	6d69 2d65 643e 6d69 2d73 643e 7461 5b61     ime->dims->data[
 80b7108:	5d30 6d00 6d65 726f 5f79 6973 657a 7300     0].memory_size.s
 80b7118:	7263 7461 6863 665f 6f6c 7461 775f 6965     cratch_float_wei
 80b7128:	6867 7374 745f 6d69 2d65 643e 6d69 2d73     ghts_time->dims-
 80b7138:	643e 7461 5b61 5d31 7700 6965 6867 7374     >data[1].weights
 80b7148:	665f 6165 7574 6572 3e2d 7974 6570 4e00     _feature->type.N
 80b7158:	6d75 6944 656d 736e 6f69 736e 6f28 7475     umDimensions(out
 80b7168:	7570 2974 6f00 7475 7570 2d74 643e 6d69     put).output->dim
 80b7178:	2d73 643e 7461 5b61 5d30 6f00 7475 7570     s->data[0].outpu
 80b7188:	2d74 643e 6d69 2d73 643e 7461 5b61 5d31     t->dims->data[1]
 80b7198:	5400 7079 2065 2527 2773 6920 2073 6f6e     .Type '%s' is no
 80b71a8:	2074 7573 7070 726f 6574 2064 7962 7520     t supported by u
 80b71b8:	706e 6361 2e6b 2f00 7355 7265 2f73 7362     npack../Users/bs
 80b71c8:	7461 6f72 2f6d 6544 6576 6f6c 6d70 6e65     atrom/Developmen
 80b71d8:	2f74 6170 7472 6369 656c 6c2f 6269 6172     t/particle/libra
 80b71e8:	6972 7365 502f 7261 6974 6c63 5f65 6554     ries/Particle_Te
 80b71f8:	736e 726f 6c46 776f 694c 6574 455f 6178     nsorFlowLite_Exa
 80b7208:	706d 656c 2f73 6568 6c6c 5f6f 6f77 6c72     mples/hello_worl
 80b7218:	2f64 696c 2f62 6554 736e 726f 6c46 776f     d/lib/TensorFlow
 80b7228:	694c 6574 732f 6372 742f 6e65 6f73 6672     Lite/src/tensorf
 80b7238:	6f6c 2f77 696c 6574 652f 7078 7265 6d69     low/lite/experim
 80b7248:	6e65 6174 2f6c 696d 7263 2f6f 656b 6e72     ental/micro/kern
 80b7258:	6c65 2f73 6f70 7472 6261 656c 6f5f 7470     els/portable_opt
 80b7268:	6d69 7a69 6465 642f 7065 6874 6977 6573     imized/depthwise
 80b7278:	635f 6e6f 2e76 7063 0070 754d 746c 7069     _conv.cpp.Multip
 80b7288:	656c 6420 7065 6874 6977 6573 6320 6e6f     le depthwise con
 80b7298:	2076 706f 2073 616d 6374 2068 706f 6974     v ops match opti
 80b72a8:	696d 617a 6974 6e6f 7020 7261 6d61 7465     mization paramet
 80b72b8:	7265 2c73 6220 7475 6f20 6c6e 2079 6874     ers, but only th
 80b72c8:	2065 6966 7372 2074 6977 6c6c 7520 6573     e first will use
 80b72d8:	7420 6568 6620 7361 2074 6170 6874 202c      the fast path, 
 80b72e8:	6562 6163 7375 2065 6874 7265 2765 2073     because there's 
 80b72f8:	6e6f 796c 6f20 656e 5220 4d41 6320 6361     only one RAM cac
 80b7308:	6568 6120 6176 6c69 6261 656c 5300 7a69     he available.Siz
 80b7318:	2065 6f74 206f 616c 6772 2065 6f66 2072     e too large for 
 80b7328:	6572 6873 7061 6465 7720 6965 6867 2074     reshaped weight 
 80b7338:	7562 6666 7265 2820 6425 6e20 6565 6564     buffer (%d neede
 80b7348:	2c64 2520 2064 7661 6961 616c 6c62 2965     d, %d available)
 80b7358:	5400 6f6f 6d20 6e61 2079 7562 6666 7265     .Too many buffer
 80b7368:	2073 6d28 7861 6920 2073 6425 0029 7562     s (max is %d).bu
 80b7378:	6666 7265 6920 646e 7865 2520 2064 7369     ffer index %d is
 80b7388:	6f20 7475 6973 6564 7220 6e61 6567 3020      outside range 0
 80b7398:	7420 206f 6425 4f00 6576 6c72 7061 203a      to %d.Overlap: 
 80b73a8:	6425 2820 6425 3e3d 6425 202c 6425 3e2d     %d (%d=>%d, %d->
 80b73b8:	6425 2029 7376 2520 2064 2528 3d64 253e     %d) vs %d (%d=>%
 80b73c8:	2c64 2520 2d64 253e 2964 0000               d, %d->%d)..

080b73d4 <_ZTVN6tflite19GreedyMemoryPlannerE>:
	...
 80b73dc:	f431 080a f437 080a f445 080a f68f 080a     1...7...E.......
 80b73ec:	f433 080a f6c5 080a 552f 6573 7372 622f     3......./Users/b
 80b73fc:	6173 7274 6d6f 442f 7665 6c65 706f 656d     satrom/Developme
 80b740c:	746e 702f 7261 6974 6c63 2f65 696c 7262     nt/particle/libr
 80b741c:	7261 6569 2f73 6150 7472 6369 656c 545f     aries/Particle_T
 80b742c:	6e65 6f73 4672 6f6c 4c77 7469 5f65 7845     ensorFlowLite_Ex
 80b743c:	6d61 6c70 7365 682f 6c65 6f6c 775f 726f     amples/hello_wor
 80b744c:	646c 6c2f 6269 542f 6e65 6f73 4672 6f6c     ld/lib/TensorFlo
 80b745c:	4c77 7469 2f65 7273 2f63 6574 736e 726f     wLite/src/tensor
 80b746c:	6c66 776f 6c2f 7469 2f65 656b 6e72 6c65     flow/lite/kernel
 80b747c:	2f73 656b 6e72 6c65 755f 6974 2e6c 7063     s/kernel_util.cp
 80b748c:	0070 6e69 7570 5f74 7270 646f 6375 5f74     p.input_product_
 80b749c:	6373 6c61 2065 3d3e 3020 7300 6474 3a3a     scale >= 0.std::
 80b74ac:	6261 2873 6e69 7570 5f74 7270 646f 6375     abs(input_produc
 80b74bc:	5f74 6373 6c61 2065 202d 6962 7361 735f     t_scale - bias_s
 80b74cc:	6163 656c 2029 3d3c 3120 2d65 2036 202a     cale) <= 1e-6 * 
 80b74dc:	7473 3a64 6d3a 6e69 6928 706e 7475 705f     std::min(input_p
 80b74ec:	6f72 7564 7463 735f 6163 656c 202c 6962     roduct_scale, bi
 80b74fc:	7361 735f 6163 656c 0029 6166 736c 0065     as_scale).false.
 80b750c:	6e69 7570 2d74 713e 6175 746e 7a69 7461     input->quantizat
 80b751c:	6f69 2e6e 7974 6570 6600 6c69 6574 2d72     ion.type.filter-
 80b752c:	743e 7079 0065 6966 746c 7265 3e2d 6964     >type.filter->di
 80b753c:	736d 3e2d 6164 6174 615b 6666 6e69 5f65     ms->data[affine_
 80b754c:	7571 6e61 6974 617a 6974 6e6f 3e2d 7571     quantization->qu
 80b755c:	6e61 6974 657a 5f64 6964 656d 736e 6f69     antized_dimensio
 80b756c:	5d6e 6100 6666 6e69 5f65 7571 6e61 6974     n].affine_quanti
 80b757c:	617a 6974 6e6f 3e2d 6373 6c61 2d65 733e     zation->scale->s
 80b758c:	7a69 0065 3164 3d20 203d 3264 7c20 207c     ize.d1 == d2 || 
 80b759c:	3164 3d20 203d 2031 7c7c 6420 2032 3d3d     d1 == 1 || d2 ==
 80b75ac:	3120 0000                                    1..

080b75b0 <_ZTV7TwoWire>:
	...
 80b75b8:	0073 080b 00bd 080b 0095 080b 0075 080b     s...........u...
 80b75c8:	009d 080b 00a5 080b 00ad 080b 00b5 080b     ................

080b75d8 <_ZTV9IPAddress>:
	...
 80b75e0:	0109 080b 00f9 080b 00fb 080b 6162 6475     ............baud
 80b75f0:	5300 7265 6169 006c 5355 5342 7265 6169     .Serial.USBSeria
 80b7600:	316c 7000 7261 6d61 6300 646d 6900 0064     l1.param.cmd.id.
 80b7610:	6e68 0064 7473 6d72 6600 6c69 0074 766c     hnd.strm.filt.lv
 80b7620:	006c 6461 4864 6e61 6c64 7265 7200 6d65     l.addHandler.rem
 80b7630:	766f 4865 6e61 6c64 7265 6500 756e 486d     oveHandler.enumH
 80b7640:	6e61 6c64 7265 0073 534a 4e4f 7453 6572     andlers.JSONStre
 80b7650:	6d61 6f4c 4867 6e61 6c64 7265 6100 7070     amLogHandler.app
 80b7660:	2500 3130 7530 0020 205d 2c00 0020 2928     .%010u .] ., .()
 80b7670:	203a 6300 646f 2065 203d 2500 0069 6564     : .code = .%i.de
 80b7680:	6174 6c69 2073 203d 6c00 006e 6e66 6300     tails = .ln.fn.c
 80b7690:	646f 0065 6564 6174 6c69 6e00 6e6f 0065     ode.detail.none.
 80b76a0:	7274 6361 0065 6e69 6f66 7700 7261 006e     trace.info.warn.
 80b76b0:	7265 6f72 0072 6170 696e 0063 6c61 006c     error.panic.all.

080b76c0 <_ZTVN5spark12NetworkClassE>:
	...
 80b76c8:	01d1 080b 01db 080b 01e5 080b 01ef 080b     ................
 80b76d8:	01f9 080b 0205 080b 0211 080b 021d 080b     ................
 80b76e8:	0225 080b 022f 080b 0239 080b               %.../...9...

080b76f4 <_ZTV8SPIClass>:
	...
 80b76fc:	0379 080b 037b 080b 005a 2b25 3330 3a64     y...{...Z.%+03d:
 80b770c:	3025 7532 2500 2d59 6d25 252d 5464 4825     %02u.%Y-%m-%dT%H
 80b771c:	253a 3a4d 5325 7a25 6100 6373 6974 656d     :%M:%S%z.asctime
 80b772c:	0000 0000                                   ....

080b7730 <_ZTV11USARTSerial>:
	...
 80b7738:	03e5 080b 0435 080b 0443 080b 029d 080b     ....5...C.......
 80b7748:	03f9 080b 041b 080b 0407 080b 042f 080b     ............/...
 80b7758:	03e7 080b 03eb 080b                         ........

080b7760 <_ZTV9USBSerial>:
	...
 80b7768:	04ed 080b 053d 080b 054b 080b 029d 080b     ....=...K.......
 80b7778:	0529 080b 04ef 080b 0505 080b 0537 080b     )...........7...
 80b7788:	051b 080b 04e9 080b                         ........

080b7790 <_ZTVN5spark9WiFiClassE>:
	...
 80b7798:	072f 080b 0725 080b 071b 080b 068d 080b     /...%...........
 80b77a8:	070f 080b 0703 080b 06f7 080b 06ef 080b     ................
 80b77b8:	06e5 080b 06db 080b 0697 080b 7865 0070     ............exp.
 80b77c8:	7865 6670 0000 0000 6f6c 6667 0000 0000     expf....logf....
 80b77d8:	7173 7472 0066 0000                         sqrtf...

080b77e0 <halF>:
 80b77e0:	0000 0000 0000 3fe0 0000 0000 0000 bfe0     .......?........

080b77f0 <ln2LO>:
 80b77f0:	3c76 3579 39ef 3dea 3c76 3579 39ef bdea     v<y5.9.=v<y5.9..

080b7800 <ln2HI>:
 80b7800:	0000 fee0 2e42 3fe6 0000 fee0 2e42 bfe6     ....B..?....B...

080b7810 <halF>:
 80b7810:	0000 3f00 0000 bf00                         ...?....

080b7818 <ln2LO>:
 80b7818:	f7d1 3717 f7d1 b717                         ...7....

080b7820 <ln2HI>:
 80b7820:	7180 3f31 7180 bf31                         .q1?.q1.

080b7828 <npio2_hw>:
 80b7828:	0f00 3fc9 0f00 4049 cb00 4096 0f00 40c9     ...?..I@...@...@
 80b7838:	5300 40fb cb00 4116 ed00 412f 0f00 4149     .S.@...A../A..IA
 80b7848:	3100 4162 5300 417b 3a00 418a cb00 4196     .1bA.S{A.:.A...A
 80b7858:	5c00 41a3 ed00 41af 7e00 41bc 0f00 41c9     .\.A...A.~.A...A
 80b7868:	a000 41d5 3100 41e2 c200 41ee 5300 41fb     ...A.1.A...A.S.A
 80b7878:	f200 4203 3a00 420a 8300 4210 cb00 4216     ...B.:.B...B...B
 80b7888:	1400 421d 5c00 4223 a500 4229 ed00 422f     ...B.\#B..)B../B
 80b7898:	3600 4236 7e00 423c c700 4242 0f00 4249     .66B.~<B..BB..IB

080b78a8 <two_over_pi>:
 80b78a8:	00a2 0000 00f9 0000 0083 0000 006e 0000     ............n...
 80b78b8:	004e 0000 0044 0000 0015 0000 0029 0000     N...D.......)...
 80b78c8:	00fc 0000 0027 0000 0057 0000 00d1 0000     ....'...W.......
 80b78d8:	00f5 0000 0034 0000 00dd 0000 00c0 0000     ....4...........
 80b78e8:	00db 0000 0062 0000 0095 0000 0099 0000     ....b...........
 80b78f8:	003c 0000 0043 0000 0090 0000 0041 0000     <...C.......A...
 80b7908:	00fe 0000 0051 0000 0063 0000 00ab 0000     ....Q...c.......
 80b7918:	00de 0000 00bb 0000 00c5 0000 0061 0000     ............a...
 80b7928:	00b7 0000 0024 0000 006e 0000 003a 0000     ....$...n...:...
 80b7938:	0042 0000 004d 0000 00d2 0000 00e0 0000     B...M...........
 80b7948:	0006 0000 0049 0000 002e 0000 00ea 0000     ....I...........
 80b7958:	0009 0000 00d1 0000 0092 0000 001c 0000     ................
 80b7968:	00fe 0000 001d 0000 00eb 0000 001c 0000     ................
 80b7978:	00b1 0000 0029 0000 00a7 0000 003e 0000     ....).......>...
 80b7988:	00e8 0000 0082 0000 0035 0000 00f5 0000     ........5.......
 80b7998:	002e 0000 00bb 0000 0044 0000 0084 0000     ........D.......
 80b79a8:	00e9 0000 009c 0000 0070 0000 0026 0000     ........p...&...
 80b79b8:	00b4 0000 005f 0000 007e 0000 0041 0000     ...._...~...A...
 80b79c8:	0039 0000 0091 0000 00d6 0000 0039 0000     9...........9...
 80b79d8:	0083 0000 0053 0000 0039 0000 00f4 0000     ....S...9.......
 80b79e8:	009c 0000 0084 0000 005f 0000 008b 0000     ........_.......
 80b79f8:	00bd 0000 00f9 0000 0028 0000 003b 0000     ........(...;...
 80b7a08:	001f 0000 00f8 0000 0097 0000 00ff 0000     ................
 80b7a18:	00de 0000 0005 0000 0098 0000 000f 0000     ................
 80b7a28:	00ef 0000 002f 0000 0011 0000 008b 0000     ..../...........
 80b7a38:	005a 0000 000a 0000 006d 0000 001f 0000     Z.......m.......
 80b7a48:	006d 0000 0036 0000 007e 0000 00cf 0000     m...6...~.......
 80b7a58:	0027 0000 00cb 0000 0009 0000 00b7 0000     '...............
 80b7a68:	004f 0000 0046 0000 003f 0000 0066 0000     O...F...?...f...
 80b7a78:	009e 0000 005f 0000 00ea 0000 002d 0000     ...._.......-...
 80b7a88:	0075 0000 0027 0000 00ba 0000 00c7 0000     u...'...........
 80b7a98:	00eb 0000 00e5 0000 00f1 0000 007b 0000     ............{...
 80b7aa8:	003d 0000 0007 0000 0039 0000 00f7 0000     =.......9.......
 80b7ab8:	008a 0000 0052 0000 0092 0000 00ea 0000     ....R...........
 80b7ac8:	006b 0000 00fb 0000 005f 0000 00b1 0000     k......._.......
 80b7ad8:	001f 0000 008d 0000 005d 0000 0008 0000     ........].......
 80b7ae8:	0056 0000 0003 0000 0030 0000 0046 0000     V.......0...F...
 80b7af8:	00fc 0000 007b 0000 006b 0000 00ab 0000     ....{...k.......
 80b7b08:	00f0 0000 00cf 0000 00bc 0000 0020 0000     ............ ...
 80b7b18:	009a 0000 00f4 0000 0036 0000 001d 0000     ........6.......
 80b7b28:	00a9 0000 00e3 0000 0091 0000 0061 0000     ............a...
 80b7b38:	005e 0000 00e6 0000 001b 0000 0008 0000     ^...............
 80b7b48:	0065 0000 0099 0000 0085 0000 005f 0000     e..........._...
 80b7b58:	0014 0000 00a0 0000 0068 0000 0040 0000     ........h...@...
 80b7b68:	008d 0000 00ff 0000 00d8 0000 0080 0000     ................
 80b7b78:	004d 0000 0073 0000 0027 0000 0031 0000     M...s...'...1...
 80b7b88:	0006 0000 0006 0000 0015 0000 0056 0000     ............V...
 80b7b98:	00ca 0000 0073 0000 00a8 0000 00c9 0000     ....s...........
 80b7ba8:	0060 0000 00e2 0000 007b 0000 00c0 0000     `.......{.......
 80b7bb8:	008c 0000 006b 0000                         ....k...

080b7bc0 <init_jk>:
 80b7bc0:	0004 0000 0007 0000 0009 0000               ............

080b7bcc <PIo2>:
 80b7bcc:	0000 3fc9 0000 39f0 0000 37da 0000 33a2     ...?...9...7...3
 80b7bdc:	0000 2e84 0000 2b50 0000 27c2 0000 22d0     ......P+...'..."
 80b7bec:	0000 1fc4 0000 1bc6 0000 1744               ..........D.

080b7bf8 <__sf_fake_stdin>:
	...

080b7c18 <__sf_fake_stdout>:
	...

080b7c38 <__sf_fake_stderr>:
	...

080b7c58 <_global_impure_ptr>:
 80b7c58:	0530 2000                                   0.. 

080b7c5c <link_const_variable_data_end>:
 80b7c5c:	080a0325 	.word	0x080a0325
 80b7c60:	080a03a9 	.word	0x080a03a9
 80b7c64:	080a23a5 	.word	0x080a23a5
 80b7c68:	080b01c1 	.word	0x080b01c1
 80b7c6c:	080b0289 	.word	0x080b0289
 80b7c70:	080b035d 	.word	0x080b035d
 80b7c74:	080b03b9 	.word	0x080b03b9
 80b7c78:	080b03d1 	.word	0x080b03d1
 80b7c7c:	080b0739 	.word	0x080b0739
 80b7c80:	080b097d 	.word	0x080b097d
 80b7c84:	080b09ad 	.word	0x080b09ad
 80b7c88:	080b0a01 	.word	0x080b0a01
 80b7c8c:	080b0ac5 	.word	0x080b0ac5
 80b7c90:	080b0b49 	.word	0x080b0b49
 80b7c94:	080b0bcd 	.word	0x080b0bcd

080b7c98 <link_constructors_end>:
	...
