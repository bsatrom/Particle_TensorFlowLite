
/Users/bsatrom/Development/particle/libraries/Particle_TensorFlowLite_Examples/hello_world/target/1.4.2/xenon/hello_world.elf:     file format elf32-littlearm

Sections:
Idx Name          Size      VMA       LMA       File off  Algn
  0 .module_info  00000018  000d4000  000d4000  00004000  2**2
                  CONTENTS, ALLOC, LOAD, READONLY, DATA
  1 .dynalib      00000004  000d4018  000d4018  00004018  2**2
                  CONTENTS, ALLOC, LOAD, READONLY, DATA
  2 .text         000179a0  000d4020  000d4020  00004020  2**3
                  CONTENTS, ALLOC, LOAD, READONLY, CODE
  3 .backup       00000000  2003f400  2003f400  0002bf80  2**0
                  CONTENTS
  4 .data         00000594  2003be74  000eb9c0  0001be74  2**2
                  CONTENTS, ALLOC, LOAD, DATA
  5 .bss          000023e8  2003c408  2003c408  0002c408  2**2
                  ALLOC
  6 .module_info_suffix 00000028  000ebf54  000ebf54  0002bf54  2**0
                  CONTENTS, ALLOC, LOAD, READONLY, DATA
  7 .module_info_crc 00000004  000ebf7c  000ebf7c  0002bf7c  2**0
                  CONTENTS, ALLOC, LOAD, READONLY, DATA
  8 .debug_info   00290002  00000000  00000000  0002bf80  2**0
                  CONTENTS, READONLY, DEBUGGING
  9 .debug_abbrev 0002618d  00000000  00000000  002bbf82  2**0
                  CONTENTS, READONLY, DEBUGGING
 10 .debug_loc    00053770  00000000  00000000  002e210f  2**0
                  CONTENTS, READONLY, DEBUGGING
 11 .debug_aranges 00003648  00000000  00000000  0033587f  2**0
                  CONTENTS, READONLY, DEBUGGING
 12 .debug_ranges 000096c8  00000000  00000000  00338ec7  2**0
                  CONTENTS, READONLY, DEBUGGING
 13 .debug_macro  00049f9f  00000000  00000000  0034258f  2**0
                  CONTENTS, READONLY, DEBUGGING
 14 .debug_line   0005c405  00000000  00000000  0038c52e  2**0
                  CONTENTS, READONLY, DEBUGGING
 15 .debug_str    0018db2f  00000000  00000000  003e8933  2**0
                  CONTENTS, READONLY, DEBUGGING
 16 .debug_frame  00010950  00000000  00000000  00576464  2**2
                  CONTENTS, READONLY, DEBUGGING

Disassembly of section .text:

000d4020 <module_user_pre_init>:
/**
 * Initializes this user module. Returns the start of the heap.
 */
void* module_user_pre_init() {

    if ( (&link_global_data_start!=&link_global_data_initial_values) && (link_global_data_size != 0))
   d4020:	4809      	ldr	r0, [pc, #36]	; (d4048 <module_user_pre_init+0x28>)
   d4022:	490a      	ldr	r1, [pc, #40]	; (d404c <module_user_pre_init+0x2c>)
   d4024:	4288      	cmp	r0, r1
extern constructor_ptr_t link_constructors_end;

/**
 * Initializes this user module. Returns the start of the heap.
 */
void* module_user_pre_init() {
   d4026:	b508      	push	{r3, lr}

    if ( (&link_global_data_start!=&link_global_data_initial_values) && (link_global_data_size != 0))
   d4028:	d005      	beq.n	d4036 <module_user_pre_init+0x16>
   d402a:	4a09      	ldr	r2, [pc, #36]	; (d4050 <module_user_pre_init+0x30>)
   d402c:	4282      	cmp	r2, r0
   d402e:	d002      	beq.n	d4036 <module_user_pre_init+0x16>
    {
        memcpy(&link_global_data_start, &link_global_data_initial_values, link_global_data_size);
   d4030:	1a12      	subs	r2, r2, r0
   d4032:	f013 fbbe 	bl	e77b2 <memcpy>
    }

    memset(&link_bss_location, 0, link_bss_size );
   d4036:	4807      	ldr	r0, [pc, #28]	; (d4054 <module_user_pre_init+0x34>)
   d4038:	4a07      	ldr	r2, [pc, #28]	; (d4058 <module_user_pre_init+0x38>)
   d403a:	2100      	movs	r1, #0
   d403c:	1a12      	subs	r2, r2, r0
   d403e:	f013 fbc3 	bl	e77c8 <memset>
    return &link_global_data_start;
}
   d4042:	4801      	ldr	r0, [pc, #4]	; (d4048 <module_user_pre_init+0x28>)
   d4044:	bd08      	pop	{r3, pc}
   d4046:	bf00      	nop
   d4048:	2003be74 	.word	0x2003be74
   d404c:	000eb9c0 	.word	0x000eb9c0
   d4050:	2003c408 	.word	0x2003c408
   d4054:	2003c408 	.word	0x2003c408
   d4058:	2003e7f0 	.word	0x2003e7f0

000d405c <module_user_init>:
extern constructor_ptr_t link_constructors_location[];
extern constructor_ptr_t link_constructors_end;
#define link_constructors_size   ((unsigned long)&link_constructors_end  -  (unsigned long)&link_constructors_location )

void module_user_init()
{
   d405c:	b570      	push	{r4, r5, r6, lr}
    module_user_init_hook();
   d405e:	f010 fe11 	bl	e4c84 <module_user_init_hook>
   d4062:	4c07      	ldr	r4, [pc, #28]	; (d4080 <module_user_init+0x24>)
   d4064:	4b07      	ldr	r3, [pc, #28]	; (d4084 <module_user_init+0x28>)
   d4066:	1ae4      	subs	r4, r4, r3
   d4068:	08a4      	lsrs	r4, r4, #2

    // invoke constructors
    int ctor_num;
    for (ctor_num=0; ctor_num < link_constructors_size/sizeof(constructor_ptr_t); ctor_num++ )
   d406a:	2500      	movs	r5, #0
   d406c:	461e      	mov	r6, r3
   d406e:	42a5      	cmp	r5, r4
   d4070:	d004      	beq.n	d407c <module_user_init+0x20>
    {
        link_constructors_location[ctor_num]();
   d4072:	f856 3025 	ldr.w	r3, [r6, r5, lsl #2]
   d4076:	4798      	blx	r3
{
    module_user_init_hook();

    // invoke constructors
    int ctor_num;
    for (ctor_num=0; ctor_num < link_constructors_size/sizeof(constructor_ptr_t); ctor_num++ )
   d4078:	3501      	adds	r5, #1
   d407a:	e7f8      	b.n	d406e <module_user_init+0x12>
    {
        link_constructors_location[ctor_num]();
    }
}
   d407c:	bd70      	pop	{r4, r5, r6, pc}
   d407e:	bf00      	nop
   d4080:	000eb9bc 	.word	0x000eb9bc
   d4084:	000eb97c 	.word	0x000eb97c

000d4088 <module_user_setup>:

/**
 * Export these functions with a fuller name so they don't clash with the setup/loop wrappers in the system module.
 */
void module_user_setup() {
    setup();
   d4088:	f000 b866 	b.w	d4158 <setup>

000d408c <module_user_loop>:
}

void module_user_loop() {
   d408c:	b508      	push	{r3, lr}
    loop();
   d408e:	f000 f8ff 	bl	d4290 <loop>
    _post_loop();
}
   d4092:	e8bd 4008 	ldmia.w	sp!, {r3, lr}
    setup();
}

void module_user_loop() {
    loop();
    _post_loop();
   d4096:	f010 bdc3 	b.w	e4c20 <_post_loop>

000d409a <_Znwj>:
   d409a:	f010 b947 	b.w	e432c <malloc>

000d409e <_Znaj>:
	return malloc(size);
}

void *operator new[](size_t size)
{
	return malloc(size);
   d409e:	f010 b945 	b.w	e432c <malloc>

000d40a2 <_ZdlPv>:
   d40a2:	f010 b94b 	b.w	e433c <free>

000d40a6 <_ZdaPv>:
	free(p);
}

void operator delete[](void *p)
{
	free(p);
   d40a6:	f010 b949 	b.w	e433c <free>
	...

000d40ac <_exit>:
int _getpid(void)
{
	return 1;
}

void _exit(int status) {
   d40ac:	b508      	push	{r3, lr}
    PANIC(Exit,"Exit Called");
   d40ae:	4a03      	ldr	r2, [pc, #12]	; (d40bc <_exit+0x10>)
   d40b0:	2100      	movs	r1, #0
   d40b2:	2007      	movs	r0, #7
   d40b4:	f010 f8b6 	bl	e4224 <panic_>
   d40b8:	e7fe      	b.n	d40b8 <_exit+0xc>
   d40ba:	bf00      	nop
   d40bc:	000e3ff5 	.word	0x000e3ff5

000d40c0 <__cxa_guard_acquire>:

/* Provide default implemenation for __cxa_guard_acquire() and
 * __cxa_guard_release(). Note: these must be revisited if a multitasking
 * OS is ported to this platform. */
__extension__ typedef int __guard __attribute__((mode (__DI__)));
int __cxa_guard_acquire(__guard *g) {return !*(char *)(g);};
   d40c0:	7800      	ldrb	r0, [r0, #0]
   d40c2:	fab0 f080 	clz	r0, r0
   d40c6:	0940      	lsrs	r0, r0, #5
   d40c8:	4770      	bx	lr

000d40ca <__cxa_guard_release>:
void __cxa_guard_release (__guard *g) {*(char *)g = 1;};
   d40ca:	2301      	movs	r3, #1
   d40cc:	7003      	strb	r3, [r0, #0]
   d40ce:	4770      	bx	lr

000d40d0 <TfLiteIntArrayEqualsArray>:
  if (a == NULL || b == NULL) return 0;
  return TfLiteIntArrayEqualsArray(a, b->size, b->data);
}

int TfLiteIntArrayEqualsArray(const TfLiteIntArray* a, int b_size,
                              const int b_data[]) {
   d40d0:	b530      	push	{r4, r5, lr}
  if (a == NULL) return (b_size == 0);
   d40d2:	b918      	cbnz	r0, d40dc <TfLiteIntArrayEqualsArray+0xc>
   d40d4:	fab1 f081 	clz	r0, r1
   d40d8:	0940      	lsrs	r0, r0, #5
   d40da:	bd30      	pop	{r4, r5, pc}
  if (a->size != b_size) return 0;
   d40dc:	6803      	ldr	r3, [r0, #0]
   d40de:	4299      	cmp	r1, r3
   d40e0:	d10c      	bne.n	d40fc <TfLiteIntArrayEqualsArray+0x2c>
   d40e2:	2300      	movs	r3, #0
  int i = 0;
  for (; i < a->size; i++)
   d40e4:	428b      	cmp	r3, r1
   d40e6:	da07      	bge.n	d40f8 <TfLiteIntArrayEqualsArray+0x28>
    if (a->data[i] != b_data[i]) return 0;
   d40e8:	f850 5f04 	ldr.w	r5, [r0, #4]!
   d40ec:	f852 4023 	ldr.w	r4, [r2, r3, lsl #2]
   d40f0:	42a5      	cmp	r5, r4
   d40f2:	d103      	bne.n	d40fc <TfLiteIntArrayEqualsArray+0x2c>
int TfLiteIntArrayEqualsArray(const TfLiteIntArray* a, int b_size,
                              const int b_data[]) {
  if (a == NULL) return (b_size == 0);
  if (a->size != b_size) return 0;
  int i = 0;
  for (; i < a->size; i++)
   d40f4:	3301      	adds	r3, #1
   d40f6:	e7f5      	b.n	d40e4 <TfLiteIntArrayEqualsArray+0x14>
    if (a->data[i] != b_data[i]) return 0;
  return 1;
   d40f8:	2001      	movs	r0, #1
   d40fa:	bd30      	pop	{r4, r5, pc}
}

int TfLiteIntArrayEqualsArray(const TfLiteIntArray* a, int b_size,
                              const int b_data[]) {
  if (a == NULL) return (b_size == 0);
  if (a->size != b_size) return 0;
   d40fc:	2000      	movs	r0, #0
  int i = 0;
  for (; i < a->size; i++)
    if (a->data[i] != b_data[i]) return 0;
  return 1;
}
   d40fe:	bd30      	pop	{r4, r5, pc}

000d4100 <TfLiteIntArrayEqual>:
  static TfLiteIntArray dummy;
  return sizeof(dummy) + sizeof(dummy.data[0]) * size;
}

int TfLiteIntArrayEqual(const TfLiteIntArray* a, const TfLiteIntArray* b) {
  if (a == b) return 1;
   d4100:	4288      	cmp	r0, r1
   d4102:	d005      	beq.n	d4110 <TfLiteIntArrayEqual+0x10>
  if (a == NULL || b == NULL) return 0;
   d4104:	b130      	cbz	r0, d4114 <TfLiteIntArrayEqual+0x14>
   d4106:	b131      	cbz	r1, d4116 <TfLiteIntArrayEqual+0x16>
  return TfLiteIntArrayEqualsArray(a, b->size, b->data);
   d4108:	1d0a      	adds	r2, r1, #4
   d410a:	6809      	ldr	r1, [r1, #0]
   d410c:	f7ff bfe0 	b.w	d40d0 <TfLiteIntArrayEqualsArray>
  static TfLiteIntArray dummy;
  return sizeof(dummy) + sizeof(dummy.data[0]) * size;
}

int TfLiteIntArrayEqual(const TfLiteIntArray* a, const TfLiteIntArray* b) {
  if (a == b) return 1;
   d4110:	2001      	movs	r0, #1
   d4112:	4770      	bx	lr
   d4114:	4770      	bx	lr
  if (a == NULL || b == NULL) return 0;
   d4116:	4608      	mov	r0, r1
  return TfLiteIntArrayEqualsArray(a, b->size, b->data);
}
   d4118:	4770      	bx	lr
	...

000d411c <TfLiteTypeGetName>:
  }
  tensor->bytes = num_bytes;
}
#endif  // TF_LITE_STATIC_MEMORY

const char* TfLiteTypeGetName(TfLiteType type) {
   d411c:	280a      	cmp	r0, #10
   d411e:	bf9a      	itte	ls
   d4120:	4b02      	ldrls	r3, [pc, #8]	; (d412c <TfLiteTypeGetName+0x10>)
   d4122:	f853 0020 	ldrls.w	r0, [r3, r0, lsl #2]
   d4126:	4802      	ldrhi	r0, [pc, #8]	; (d4130 <TfLiteTypeGetName+0x14>)
      return "STRING";
    case kTfLiteFloat16:
      return "FLOAT16";
  }
  return "Unknown type";
}
   d4128:	4770      	bx	lr
   d412a:	bf00      	nop
   d412c:	000e78c4 	.word	0x000e78c4
   d4130:	000e7870 	.word	0x000e7870

000d4134 <_ZN6tflite18MicroErrorReporterD1Ev>:

namespace tflite {

class MicroErrorReporter : public ErrorReporter {
 public:
  ~MicroErrorReporter() {}
   d4134:	4770      	bx	lr

000d4136 <_ZN6tflite3ops5micro14AllOpsResolverD1Ev>:

namespace tflite {
namespace ops {
namespace micro {

class AllOpsResolver : public MicroMutableOpResolver {
   d4136:	4770      	bx	lr

000d4138 <_ZN6tflite3ops5micro14AllOpsResolverD0Ev>:
   d4138:	b510      	push	{r4, lr}
   d413a:	f241 0108 	movw	r1, #4104	; 0x1008
   d413e:	4604      	mov	r4, r0
   d4140:	f010 fe5d 	bl	e4dfe <_ZdlPvj>
   d4144:	4620      	mov	r0, r4
   d4146:	bd10      	pop	{r4, pc}

000d4148 <_ZN6tflite18MicroErrorReporterD0Ev>:
   d4148:	b510      	push	{r4, lr}
   d414a:	2104      	movs	r1, #4
   d414c:	4604      	mov	r4, r0
   d414e:	f010 fe56 	bl	e4dfe <_ZdlPvj>
   d4152:	4620      	mov	r0, r4
   d4154:	bd10      	pop	{r4, pc}
	...

000d4158 <setup>:
uint8_t tensor_arena[kTensorArenaSize];
} // namespace

// The name of this function is important for Arduino compatibility.
void setup()
{
   d4158:	b573      	push	{r0, r1, r4, r5, r6, lr}
  // Set up logging. Google style is to avoid globals or statics because of
  // lifetime uncertainty, but since this has a trivial destructor it's okay.
  // NOLINTNEXTLINE(runtime-global-variables)
  static tflite::MicroErrorReporter micro_error_reporter;
   d415a:	4c3a      	ldr	r4, [pc, #232]	; (d4244 <setup+0xec>)
   d415c:	6823      	ldr	r3, [r4, #0]
   d415e:	07d9      	lsls	r1, r3, #31
   d4160:	d40b      	bmi.n	d417a <setup+0x22>
   d4162:	4620      	mov	r0, r4
   d4164:	f7ff ffac 	bl	d40c0 <__cxa_guard_acquire>
   d4168:	b138      	cbz	r0, d417a <setup+0x22>
   d416a:	4620      	mov	r0, r4
   d416c:	f7ff ffad 	bl	d40ca <__cxa_guard_release>
   d4170:	4a35      	ldr	r2, [pc, #212]	; (d4248 <setup+0xf0>)
   d4172:	4936      	ldr	r1, [pc, #216]	; (d424c <setup+0xf4>)
   d4174:	4836      	ldr	r0, [pc, #216]	; (d4250 <setup+0xf8>)
   d4176:	f010 fe3d 	bl	e4df4 <__aeabi_atexit>
  error_reporter = &micro_error_reporter;
   d417a:	4c36      	ldr	r4, [pc, #216]	; (d4254 <setup+0xfc>)
   d417c:	4b34      	ldr	r3, [pc, #208]	; (d4250 <setup+0xf8>)
   d417e:	6023      	str	r3, [r4, #0]
// Helpers to get a typed pointer to the root object contained in the buffer.
template<typename T> T *GetMutableRoot(void *buf) {
  EndianCheck();
  return reinterpret_cast<T *>(
      reinterpret_cast<uint8_t *>(buf) +
      EndianScalar(*reinterpret_cast<uoffset_t *>(buf)));
   d4180:	4b35      	ldr	r3, [pc, #212]	; (d4258 <setup+0x100>)

  // Map the model into a usable data structure. This doesn't involve any
  // copying or parsing, it's a very lightweight operation.
  model = tflite::GetModel(g_sine_model_data);
   d4182:	4a36      	ldr	r2, [pc, #216]	; (d425c <setup+0x104>)
   d4184:	6818      	ldr	r0, [r3, #0]
   d4186:	18c1      	adds	r1, r0, r3
// "tables" use an offset table (possibly shared) that allows fields to be
// omitted and added at will, but uses an extra indirection to read.
class Table {
 public:
  const uint8_t *GetVTable() const {
    return data_ - ReadScalar<soffset_t>(data_);
   d4188:	58c3      	ldr	r3, [r0, r3]
   d418a:	6011      	str	r1, [r2, #0]
   d418c:	1acb      	subs	r3, r1, r3
   d418e:	4616      	mov	r6, r2
    auto vtable = GetVTable();
    // The first element is the size of the vtable (fields + type id + itself).
    auto vtsize = ReadScalar<voffset_t>(vtable);
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
   d4190:	8818      	ldrh	r0, [r3, #0]
   d4192:	2804      	cmp	r0, #4
   d4194:	d905      	bls.n	d41a2 <setup+0x4a>

template<typename T>
// UBSAN: C++ aliasing type rules, see std::bit_cast<> for details.
__supress_ubsan__("alignment")
T ReadScalar(const void *p) {
  return EndianScalar(*reinterpret_cast<const T *>(p));
   d4196:	889a      	ldrh	r2, [r3, #4]
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d4198:	b122      	cbz	r2, d41a4 <setup+0x4c>
   d419a:	588a      	ldr	r2, [r1, r2]
  if (model->version() != TFLITE_SCHEMA_VERSION)
   d419c:	2a03      	cmp	r2, #3
   d419e:	d009      	beq.n	d41b4 <setup+0x5c>
   d41a0:	e000      	b.n	d41a4 <setup+0x4c>
   d41a2:	2200      	movs	r2, #0
  {
    error_reporter->Report(
        "Model provided is schema version %d not equal "
        "to supported version %d.",
        model->version(), TFLITE_SCHEMA_VERSION);
   d41a4:	492e      	ldr	r1, [pc, #184]	; (d4260 <setup+0x108>)
   d41a6:	482a      	ldr	r0, [pc, #168]	; (d4250 <setup+0xf8>)
   d41a8:	2303      	movs	r3, #3
  input = interpreter->input(0);
  output = interpreter->output(0);

  // Keep track of how many inferences we have performed.
  inference_count = 0;
}
   d41aa:	b002      	add	sp, #8
   d41ac:	e8bd 4070 	ldmia.w	sp!, {r4, r5, r6, lr}
  if (model->version() != TFLITE_SCHEMA_VERSION)
  {
    error_reporter->Report(
        "Model provided is schema version %d not equal "
        "to supported version %d.",
        model->version(), TFLITE_SCHEMA_VERSION);
   d41b0:	f000 b910 	b.w	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return;
  }

  // This pulls in all the operation implementations we need.
  // NOLINTNEXTLINE(runtime-global-variables)
  static tflite::ops::micro::AllOpsResolver resolver;
   d41b4:	4d2b      	ldr	r5, [pc, #172]	; (d4264 <setup+0x10c>)
   d41b6:	682b      	ldr	r3, [r5, #0]
   d41b8:	07da      	lsls	r2, r3, #31
   d41ba:	d40e      	bmi.n	d41da <setup+0x82>
   d41bc:	4628      	mov	r0, r5
   d41be:	f7ff ff7f 	bl	d40c0 <__cxa_guard_acquire>
   d41c2:	b150      	cbz	r0, d41da <setup+0x82>
   d41c4:	4828      	ldr	r0, [pc, #160]	; (d4268 <setup+0x110>)
   d41c6:	f003 f82d 	bl	d7224 <_ZN6tflite3ops5micro14AllOpsResolverC1Ev>
   d41ca:	4628      	mov	r0, r5
   d41cc:	f7ff ff7d 	bl	d40ca <__cxa_guard_release>
   d41d0:	4a1d      	ldr	r2, [pc, #116]	; (d4248 <setup+0xf0>)
   d41d2:	4926      	ldr	r1, [pc, #152]	; (d426c <setup+0x114>)
   d41d4:	4824      	ldr	r0, [pc, #144]	; (d4268 <setup+0x110>)
   d41d6:	f010 fe0d 	bl	e4df4 <__aeabi_atexit>

  // Build an interpreter to run the model with.
  static tflite::MicroInterpreter static_interpreter(
      model, resolver, tensor_arena, kTensorArenaSize, error_reporter);
   d41da:	4d25      	ldr	r5, [pc, #148]	; (d4270 <setup+0x118>)
   d41dc:	682b      	ldr	r3, [r5, #0]
   d41de:	07db      	lsls	r3, r3, #31
   d41e0:	d411      	bmi.n	d4206 <setup+0xae>
   d41e2:	4628      	mov	r0, r5
   d41e4:	f7ff ff6c 	bl	d40c0 <__cxa_guard_acquire>
   d41e8:	b168      	cbz	r0, d4206 <setup+0xae>
   d41ea:	6823      	ldr	r3, [r4, #0]
   d41ec:	9301      	str	r3, [sp, #4]
   d41ee:	f44f 6300 	mov.w	r3, #2048	; 0x800
   d41f2:	9300      	str	r3, [sp, #0]
   d41f4:	4a1c      	ldr	r2, [pc, #112]	; (d4268 <setup+0x110>)
   d41f6:	4b1f      	ldr	r3, [pc, #124]	; (d4274 <setup+0x11c>)
   d41f8:	6831      	ldr	r1, [r6, #0]
   d41fa:	481f      	ldr	r0, [pc, #124]	; (d4278 <setup+0x120>)
   d41fc:	f001 fe5c 	bl	d5eb8 <_ZN6tflite16MicroInterpreterC1EPKNS_5ModelERKNS_10OpResolverEPhjPNS_13ErrorReporterE>
   d4200:	4628      	mov	r0, r5
   d4202:	f7ff ff62 	bl	d40ca <__cxa_guard_release>
  interpreter = &static_interpreter;
   d4206:	4e1d      	ldr	r6, [pc, #116]	; (d427c <setup+0x124>)
   d4208:	481b      	ldr	r0, [pc, #108]	; (d4278 <setup+0x120>)
   d420a:	6030      	str	r0, [r6, #0]

  // Allocate memory from the tensor_arena for the model's tensors.
  TfLiteStatus allocate_status = interpreter->AllocateTensors();
   d420c:	f001 fe3a 	bl	d5e84 <_ZN6tflite16MicroInterpreter15AllocateTensorsEv>
  if (allocate_status != kTfLiteOk)
   d4210:	4605      	mov	r5, r0
   d4212:	b130      	cbz	r0, d4222 <setup+0xca>
  {
    error_reporter->Report("AllocateTensors() failed");
   d4214:	491a      	ldr	r1, [pc, #104]	; (d4280 <setup+0x128>)
   d4216:	6820      	ldr	r0, [r4, #0]
  input = interpreter->input(0);
  output = interpreter->output(0);

  // Keep track of how many inferences we have performed.
  inference_count = 0;
}
   d4218:	b002      	add	sp, #8
   d421a:	e8bd 4070 	ldmia.w	sp!, {r4, r5, r6, lr}

  // Allocate memory from the tensor_arena for the model's tensors.
  TfLiteStatus allocate_status = interpreter->AllocateTensors();
  if (allocate_status != kTfLiteOk)
  {
    error_reporter->Report("AllocateTensors() failed");
   d421e:	f000 b8d9 	b.w	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return;
  }

  // Obtain pointers to the model's input and output tensors.
  input = interpreter->input(0);
   d4222:	4601      	mov	r1, r0
   d4224:	6830      	ldr	r0, [r6, #0]
   d4226:	f001 feab 	bl	d5f80 <_ZN6tflite16MicroInterpreter5inputEj>
   d422a:	4b16      	ldr	r3, [pc, #88]	; (d4284 <setup+0x12c>)
  output = interpreter->output(0);
   d422c:	4629      	mov	r1, r5
    error_reporter->Report("AllocateTensors() failed");
    return;
  }

  // Obtain pointers to the model's input and output tensors.
  input = interpreter->input(0);
   d422e:	6018      	str	r0, [r3, #0]
  output = interpreter->output(0);
   d4230:	6830      	ldr	r0, [r6, #0]
   d4232:	f001 fe89 	bl	d5f48 <_ZN6tflite16MicroInterpreter6outputEj>
   d4236:	4b14      	ldr	r3, [pc, #80]	; (d4288 <setup+0x130>)
   d4238:	6018      	str	r0, [r3, #0]

  // Keep track of how many inferences we have performed.
  inference_count = 0;
   d423a:	4b14      	ldr	r3, [pc, #80]	; (d428c <setup+0x134>)
   d423c:	601d      	str	r5, [r3, #0]
}
   d423e:	b002      	add	sp, #8
   d4240:	bd70      	pop	{r4, r5, r6, pc}
   d4242:	bf00      	nop
   d4244:	2003dcb0 	.word	0x2003dcb0
   d4248:	2003c408 	.word	0x2003c408
   d424c:	000d4135 	.word	0x000d4135
   d4250:	2003be74 	.word	0x2003be74
   d4254:	2003cc20 	.word	0x2003cc20
   d4258:	000e7984 	.word	0x000e7984
   d425c:	2003cc1c 	.word	0x2003cc1c
   d4260:	000e78f0 	.word	0x000e78f0
   d4264:	2003c410 	.word	0x2003c410
   d4268:	2003cc28 	.word	0x2003cc28
   d426c:	000d4137 	.word	0x000d4137
   d4270:	2003dcb8 	.word	0x2003dcb8
   d4274:	2003c414 	.word	0x2003c414
   d4278:	2003dc30 	.word	0x2003dc30
   d427c:	2003cc24 	.word	0x2003cc24
   d4280:	000e7937 	.word	0x000e7937
   d4284:	2003c40c 	.word	0x2003c40c
   d4288:	2003dcb4 	.word	0x2003dcb4
   d428c:	2003dcbc 	.word	0x2003dcbc

000d4290 <loop>:

// The name of this function is important for Arduino compatibility.
void loop()
{
   d4290:	b530      	push	{r4, r5, lr}
  // Calculate an x value to feed into the model. We compare the current
  // inference_count to the number of inferences per cycle to determine
  // our position within the range of possible x values the model was
  // trained on, and use this to calculate a value.
  float position = static_cast<float>(inference_count) /
   d4292:	4b22      	ldr	r3, [pc, #136]	; (d431c <loop+0x8c>)
   d4294:	4c22      	ldr	r4, [pc, #136]	; (d4320 <loop+0x90>)
   d4296:	681b      	ldr	r3, [r3, #0]
                   static_cast<float>(kInferencesPerCycle);
  float x_val = position * kXrange;
   d4298:	edd4 7a00 	vldr	s15, [r4]
   d429c:	4d21      	ldr	r5, [pc, #132]	; (d4324 <loop+0x94>)
   d429e:	eeb8 7ae7 	vcvt.f32.s32	s14, s15
   d42a2:	ee07 3a90 	vmov	s15, r3
   d42a6:	eef8 6ae7 	vcvt.f32.s32	s13, s15
  inference_count = 0;
}

// The name of this function is important for Arduino compatibility.
void loop()
{
   d42aa:	ed2d 8b02 	vpush	{d8}
  // inference_count to the number of inferences per cycle to determine
  // our position within the range of possible x values the model was
  // trained on, and use this to calculate a value.
  float position = static_cast<float>(inference_count) /
                   static_cast<float>(kInferencesPerCycle);
  float x_val = position * kXrange;
   d42ae:	eec7 7a26 	vdiv.f32	s15, s14, s13
  inference_count = 0;
}

// The name of this function is important for Arduino compatibility.
void loop()
{
   d42b2:	b083      	sub	sp, #12
  // Calculate an x value to feed into the model. We compare the current
  // inference_count to the number of inferences per cycle to determine
  // our position within the range of possible x values the model was
  // trained on, and use this to calculate a value.
  float position = static_cast<float>(inference_count) /
   d42b4:	9301      	str	r3, [sp, #4]
                   static_cast<float>(kInferencesPerCycle);
  float x_val = position * kXrange;

  // Place our calculated x value in the model's input tensor
  input->data.f[0] = x_val;
   d42b6:	4b1c      	ldr	r3, [pc, #112]	; (d4328 <loop+0x98>)
   d42b8:	681b      	ldr	r3, [r3, #0]
   d42ba:	685b      	ldr	r3, [r3, #4]
  // inference_count to the number of inferences per cycle to determine
  // our position within the range of possible x values the model was
  // trained on, and use this to calculate a value.
  float position = static_cast<float>(inference_count) /
                   static_cast<float>(kInferencesPerCycle);
  float x_val = position * kXrange;
   d42bc:	ed9f 8a1b 	vldr	s16, [pc, #108]	; d432c <loop+0x9c>
   d42c0:	ee27 8a88 	vmul.f32	s16, s15, s16

  // Place our calculated x value in the model's input tensor
  input->data.f[0] = x_val;
   d42c4:	ed83 8a00 	vstr	s16, [r3]

  // Run inference, and report any error
  TfLiteStatus invoke_status = interpreter->Invoke();
   d42c8:	4b19      	ldr	r3, [pc, #100]	; (d4330 <loop+0xa0>)
   d42ca:	6818      	ldr	r0, [r3, #0]
   d42cc:	f001 fe74 	bl	d5fb8 <_ZN6tflite16MicroInterpreter6InvokeEv>
  if (invoke_status != kTfLiteOk)
   d42d0:	b170      	cbz	r0, d42f0 <loop+0x60>
  {
    error_reporter->Report("Invoke failed on x_val: %f\n",
                           static_cast<double>(x_val));
   d42d2:	ee18 0a10 	vmov	r0, s16
   d42d6:	f012 fe79 	bl	e6fcc <__aeabi_f2d>
   d42da:	4602      	mov	r2, r0
   d42dc:	460b      	mov	r3, r1
   d42de:	6828      	ldr	r0, [r5, #0]
   d42e0:	4914      	ldr	r1, [pc, #80]	; (d4334 <loop+0xa4>)
  // Increment the inference_counter, and reset it if we have reached
  // the total number per cycle
  inference_count += 1;
  if (inference_count >= kInferencesPerCycle)
    inference_count = 0;
}
   d42e2:	b003      	add	sp, #12
   d42e4:	ecbd 8b02 	vpop	{d8}
   d42e8:	e8bd 4030 	ldmia.w	sp!, {r4, r5, lr}
  // Run inference, and report any error
  TfLiteStatus invoke_status = interpreter->Invoke();
  if (invoke_status != kTfLiteOk)
  {
    error_reporter->Report("Invoke failed on x_val: %f\n",
                           static_cast<double>(x_val));
   d42ec:	f000 b872 	b.w	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return;
  }

  // Read the predicted y value from the model's output tensor
  float y_val = output->data.f[0];
   d42f0:	4b11      	ldr	r3, [pc, #68]	; (d4338 <loop+0xa8>)

  // Output the results. A custom HandleOutput function can be implemented
  // for each supported hardware target.
  HandleOutput(error_reporter, x_val, y_val);
   d42f2:	6828      	ldr	r0, [r5, #0]
                           static_cast<double>(x_val));
    return;
  }

  // Read the predicted y value from the model's output tensor
  float y_val = output->data.f[0];
   d42f4:	681b      	ldr	r3, [r3, #0]
   d42f6:	685b      	ldr	r3, [r3, #4]

  // Output the results. A custom HandleOutput function can be implemented
  // for each supported hardware target.
  HandleOutput(error_reporter, x_val, y_val);
   d42f8:	eeb0 0a48 	vmov.f32	s0, s16
   d42fc:	edd3 0a00 	vldr	s1, [r3]
   d4300:	f000 f832 	bl	d4368 <_Z12HandleOutputPN6tflite13ErrorReporterEff>

  // Increment the inference_counter, and reset it if we have reached
  // the total number per cycle
  inference_count += 1;
   d4304:	6823      	ldr	r3, [r4, #0]
   d4306:	9a01      	ldr	r2, [sp, #4]
   d4308:	3301      	adds	r3, #1
   d430a:	429a      	cmp	r2, r3
   d430c:	bfd8      	it	le
   d430e:	2300      	movle	r3, #0
   d4310:	6023      	str	r3, [r4, #0]
  if (inference_count >= kInferencesPerCycle)
    inference_count = 0;
}
   d4312:	b003      	add	sp, #12
   d4314:	ecbd 8b02 	vpop	{d8}
   d4318:	bd30      	pop	{r4, r5, pc}
   d431a:	bf00      	nop
   d431c:	000e7980 	.word	0x000e7980
   d4320:	2003dcbc 	.word	0x2003dcbc
   d4324:	2003cc20 	.word	0x2003cc20
   d4328:	2003c40c 	.word	0x2003c40c
   d432c:	40c90fdb 	.word	0x40c90fdb
   d4330:	2003cc24 	.word	0x2003cc24
   d4334:	000e7950 	.word	0x000e7950
   d4338:	2003dcb4 	.word	0x2003dcb4

000d433c <_GLOBAL__sub_I_SystemMode>:
   d433c:	b510      	push	{r4, lr}

inline void pinSetFast(pin_t _pin) __attribute__((always_inline));
inline void pinResetFast(pin_t _pin) __attribute__((always_inline));
inline int32_t pinReadFast(pin_t _pin) __attribute__((always_inline));

static Hal_Pin_Info* PIN_MAP = HAL_Pin_Map();
   d433e:	f00f fe69 	bl	e4014 <HAL_Pin_Map>
    WAKEUP_REASON_RTC = 2,
    WAKEUP_REASON_PIN_OR_RTC = 3
};

struct SleepResult {
    SleepResult() {}
   d4342:	4b08      	ldr	r3, [pc, #32]	; (d4364 <_GLOBAL__sub_I_SystemMode+0x28>)
   d4344:	2400      	movs	r4, #0
   d4346:	f64f 72ff 	movw	r2, #65535	; 0xffff
   d434a:	701c      	strb	r4, [r3, #0]
   d434c:	805c      	strh	r4, [r3, #2]

class SystemClass {
public:

    SystemClass(System_Mode_TypeDef mode = DEFAULT) {
        set_system_mode(mode);
   d434e:	2003      	movs	r0, #3
    WAKEUP_REASON_RTC = 2,
    WAKEUP_REASON_PIN_OR_RTC = 3
};

struct SleepResult {
    SleepResult() {}
   d4350:	809a      	strh	r2, [r3, #4]

class SystemClass {
public:

    SystemClass(System_Mode_TypeDef mode = DEFAULT) {
        set_system_mode(mode);
   d4352:	f00f ff6f 	bl	e4234 <set_system_mode>

#include <TensorFlowLite.h>
#include <Particle.h>

SYSTEM_MODE(MANUAL);
SYSTEM_THREAD(ENABLED);
   d4356:	4621      	mov	r1, r4
   d4358:	2001      	movs	r0, #1
  // Increment the inference_counter, and reset it if we have reached
  // the total number per cycle
  inference_count += 1;
  if (inference_count >= kInferencesPerCycle)
    inference_count = 0;
}
   d435a:	e8bd 4010 	ldmia.w	sp!, {r4, lr}

#include <TensorFlowLite.h>
#include <Particle.h>

SYSTEM_MODE(MANUAL);
SYSTEM_THREAD(ENABLED);
   d435e:	f00f bf71 	b.w	e4244 <system_thread_set_state>
   d4362:	bf00      	nop
   d4364:	2003cc14 	.word	0x2003cc14

000d4368 <_Z12HandleOutputPN6tflite13ErrorReporterEff>:
bool initialized = false;

// Animates a dot across the screen to represent the current x and y values
void HandleOutput(tflite::ErrorReporter *error_reporter, float x_value,
                  float y_value)
{
   d4368:	b570      	push	{r4, r5, r6, lr}
  // Do this only once
  if (!initialized)
   d436a:	4d15      	ldr	r5, [pc, #84]	; (d43c0 <_Z12HandleOutputPN6tflite13ErrorReporterEff+0x58>)
   d436c:	4c15      	ldr	r4, [pc, #84]	; (d43c4 <_Z12HandleOutputPN6tflite13ErrorReporterEff+0x5c>)
   d436e:	782b      	ldrb	r3, [r5, #0]
bool initialized = false;

// Animates a dot across the screen to represent the current x and y values
void HandleOutput(tflite::ErrorReporter *error_reporter, float x_value,
                  float y_value)
{
   d4370:	ed2d 8b02 	vpush	{d8}
   d4374:	4606      	mov	r6, r0
   d4376:	b082      	sub	sp, #8
   d4378:	eeb0 8a60 	vmov.f32	s16, s1
  // Do this only once
  if (!initialized)
   d437c:	b92b      	cbnz	r3, d438a <_Z12HandleOutputPN6tflite13ErrorReporterEff+0x22>
  {
    // Set the LED pin to output
    pinMode(led, OUTPUT);
   d437e:	2101      	movs	r1, #1
   d4380:	8820      	ldrh	r0, [r4, #0]
   d4382:	f010 fcbd 	bl	e4d00 <pinMode>
    initialized = true;
   d4386:	2301      	movs	r3, #1
   d4388:	702b      	strb	r3, [r5, #0]
  }

  // Calculate the brightness of the LED such that y=-1 is fully off
  // and y=1 is fully on. The LED's brightness can range from 0-255.
  int brightness = (int)(127.5f * (y_value + 1));
   d438a:	eef7 0a00 	vmov.f32	s1, #112	; 0x3f800000  1.0
   d438e:	ee78 0a20 	vadd.f32	s1, s16, s1
   d4392:	eddf 7a0d 	vldr	s15, [pc, #52]	; d43c8 <_Z12HandleOutputPN6tflite13ErrorReporterEff+0x60>

  // Set the brightness of the LED. If the specified pin does not support PWM,
  // this will result in the LED being on when y > 127, off otherwise.
  analogWrite(led, brightness);
   d4396:	8820      	ldrh	r0, [r4, #0]
    initialized = true;
  }

  // Calculate the brightness of the LED such that y=-1 is fully off
  // and y=1 is fully on. The LED's brightness can range from 0-255.
  int brightness = (int)(127.5f * (y_value + 1));
   d4398:	ee60 0aa7 	vmul.f32	s1, s1, s15
   d439c:	eefd 7ae0 	vcvt.s32.f32	s15, s1

  // Set the brightness of the LED. If the specified pin does not support PWM,
  // this will result in the LED being on when y > 127, off otherwise.
  analogWrite(led, brightness);
   d43a0:	ee17 1a90 	vmov	r1, s15
    initialized = true;
  }

  // Calculate the brightness of the LED such that y=-1 is fully off
  // and y=1 is fully on. The LED's brightness can range from 0-255.
  int brightness = (int)(127.5f * (y_value + 1));
   d43a4:	edcd 7a01 	vstr	s15, [sp, #4]

  // Set the brightness of the LED. If the specified pin does not support PWM,
  // this will result in the LED being on when y > 127, off otherwise.
  analogWrite(led, brightness);
   d43a8:	f010 fcbb 	bl	e4d22 <_Z11analogWritetm>

  // Log the current brightness value
  error_reporter->Report("%d\n", brightness);
   d43ac:	9a01      	ldr	r2, [sp, #4]
   d43ae:	4907      	ldr	r1, [pc, #28]	; (d43cc <_Z12HandleOutputPN6tflite13ErrorReporterEff+0x64>)
   d43b0:	4630      	mov	r0, r6
}
   d43b2:	b002      	add	sp, #8
   d43b4:	ecbd 8b02 	vpop	{d8}
   d43b8:	e8bd 4070 	ldmia.w	sp!, {r4, r5, r6, lr}
  // Set the brightness of the LED. If the specified pin does not support PWM,
  // this will result in the LED being on when y > 127, off otherwise.
  analogWrite(led, brightness);

  // Log the current brightness value
  error_reporter->Report("%d\n", brightness);
   d43bc:	f000 b80a 	b.w	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
   d43c0:	2003dcc0 	.word	0x2003dcc0
   d43c4:	2003be78 	.word	0x2003be78
   d43c8:	42ff0000 	.word	0x42ff0000
   d43cc:	000e9565 	.word	0x000e9565

000d43d0 <_GLOBAL__sub_I_led>:
   d43d0:	f00f be20 	b.w	e4014 <HAL_Pin_Map>

000d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>:
#include "tensorflow/lite/core/api/error_reporter.h"
#include <cstdarg>

namespace tflite {

int ErrorReporter::Report(const char* format, ...) {
   d43d4:	b40e      	push	{r1, r2, r3}
   d43d6:	b503      	push	{r0, r1, lr}
   d43d8:	aa03      	add	r2, sp, #12
  va_list args;
  va_start(args, format);
  int code = Report(format, args);
   d43da:	6803      	ldr	r3, [r0, #0]
#include "tensorflow/lite/core/api/error_reporter.h"
#include <cstdarg>

namespace tflite {

int ErrorReporter::Report(const char* format, ...) {
   d43dc:	f852 1b04 	ldr.w	r1, [r2], #4
  va_list args;
  va_start(args, format);
   d43e0:	9201      	str	r2, [sp, #4]
  int code = Report(format, args);
   d43e2:	689b      	ldr	r3, [r3, #8]
   d43e4:	4798      	blx	r3
  va_end(args);
  return code;
}
   d43e6:	b002      	add	sp, #8
   d43e8:	f85d eb04 	ldr.w	lr, [sp], #4
   d43ec:	b003      	add	sp, #12
   d43ee:	4770      	bx	lr

000d43f0 <_ZN6tflite12_GLOBAL__N_124SafeBuiltinDataAllocator18BuiltinDataDeleterclEPv>:
  class BuiltinDataDeleter {
   public:
    explicit BuiltinDataDeleter(BuiltinDataAllocator* allocator)
        : allocator_(allocator) {}

    void operator()(void* data) { allocator_->Deallocate(data); }
   d43f0:	6800      	ldr	r0, [r0, #0]
   d43f2:	6803      	ldr	r3, [r0, #0]
   d43f4:	685b      	ldr	r3, [r3, #4]
   d43f6:	4718      	bx	r3

000d43f8 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_7PaddingEE_clESA_.isra.0>:
// If it returns kTfLiteError, `builtin_data` will be `nullptr`.
TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,
                         ErrorReporter* error_reporter,
                         BuiltinDataAllocator* allocator, void** builtin_data) {
  auto parse_padding = [](Padding padding) {
    switch (padding) {
   d43f8:	b120      	cbz	r0, d4404 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_7PaddingEE_clESA_.isra.0+0xc>
   d43fa:	2801      	cmp	r0, #1
      case Padding_SAME:
        return kTfLitePaddingSame;
      case Padding_VALID:
        return kTfLitePaddingValid;
    }
    return kTfLitePaddingUnknown;
   d43fc:	bf0c      	ite	eq
   d43fe:	2002      	moveq	r0, #2
   d4400:	2000      	movne	r0, #0
   d4402:	4770      	bx	lr
                         ErrorReporter* error_reporter,
                         BuiltinDataAllocator* allocator, void** builtin_data) {
  auto parse_padding = [](Padding padding) {
    switch (padding) {
      case Padding_SAME:
        return kTfLitePaddingSame;
   d4404:	2001      	movs	r0, #1
      case Padding_VALID:
        return kTfLitePaddingValid;
    }
    return kTfLitePaddingUnknown;
  };
   d4406:	4770      	bx	lr

000d4408 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>:
  auto parse_activation = [](ActivationFunctionType activation) {
   d4408:	3801      	subs	r0, #1
   d440a:	b2c0      	uxtb	r0, r0
   d440c:	2804      	cmp	r0, #4
   d440e:	bf9a      	itte	ls
   d4410:	4b01      	ldrls	r3, [pc, #4]	; (d4418 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1+0x10>)
   d4412:	5c18      	ldrbls	r0, [r3, r0]
   d4414:	2000      	movhi	r0, #0
        return kTfLiteActTanh;
      case ActivationFunctionType_SIGN_BIT:
        return kTfLiteActSignBit;
    }
    return kTfLiteActNone;
  };
   d4416:	4770      	bx	lr
   d4418:	000e868f 	.word	0x000e868f

000d441c <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4>:
};

// Copies the contents from the flatbuffer int vector `flatbuffer` into the
// int array `buffer`. `flat_vector` and `buffer` represent the same
// configuration operation for a given operation.
TfLiteStatus FlatBufferIntVectorToArray(
   d441c:	b538      	push	{r3, r4, r5, lr}
   d441e:	4615      	mov	r5, r2
   d4420:	461a      	mov	r2, r3
    int max_size_of_buffer, const flatbuffers::Vector<int32_t>* flat_vector,
    int* buffer, ErrorReporter* error_reporter, const char* op_name) {
  if (!flat_vector) {
   d4422:	b908      	cbnz	r0, d4428 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0xc>
    error_reporter->Report("Input array not provided for operation '%s'.\n",
                           op_name);
   d4424:	490f      	ldr	r1, [pc, #60]	; (d4464 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x48>)
   d4426:	e003      	b.n	d4430 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x14>
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d4428:	6804      	ldr	r4, [r0, #0]
    return kTfLiteError;
  } else {
    int num_dimensions = flat_vector->size();
    if (num_dimensions > max_size_of_buffer / sizeof(int)) {
   d442a:	2c08      	cmp	r4, #8
   d442c:	d905      	bls.n	d443a <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x1e>
      error_reporter->Report(
          "Found too many dimensions in the input array of operation '%s'.\n",
          op_name);
   d442e:	490e      	ldr	r1, [pc, #56]	; (d4468 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x4c>)
   d4430:	4628      	mov	r0, r5
   d4432:	f7ff ffcf 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
      return kTfLiteError;
   d4436:	2001      	movs	r0, #1
   d4438:	bd38      	pop	{r3, r4, r5, pc}
   d443a:	4602      	mov	r2, r0
    error_reporter->Report("Input array not provided for operation '%s'.\n",
                           op_name);
    return kTfLiteError;
  } else {
    int num_dimensions = flat_vector->size();
    if (num_dimensions > max_size_of_buffer / sizeof(int)) {
   d443c:	2300      	movs	r3, #0
      error_reporter->Report(
          "Found too many dimensions in the input array of operation '%s'.\n",
          op_name);
      return kTfLiteError;
    } else {
      for (int i = 0; i < num_dimensions; ++i) {
   d443e:	429c      	cmp	r4, r3
   d4440:	d00e      	beq.n	d4460 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x44>

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
   d4442:	6805      	ldr	r5, [r0, #0]
   d4444:	42ab      	cmp	r3, r5
   d4446:	d305      	bcc.n	d4454 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x38>
   d4448:	4b08      	ldr	r3, [pc, #32]	; (d446c <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x50>)
   d444a:	4a09      	ldr	r2, [pc, #36]	; (d4470 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x54>)
   d444c:	4809      	ldr	r0, [pc, #36]	; (d4474 <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x58>)
   d444e:	21ed      	movs	r1, #237	; 0xed
   d4450:	f00f ff8c 	bl	e436c <__assert_func>
template<typename T> struct IndirectHelper {
  typedef T return_type;
  typedef T mutable_return_type;
  static const size_t element_stride = sizeof(T);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    return EndianScalar((reinterpret_cast<const T *>(p))[i]);
   d4454:	f852 5f04 	ldr.w	r5, [r2, #4]!
        buffer[i] = flat_vector->Get(i);
   d4458:	f841 5023 	str.w	r5, [r1, r3, lsl #2]
      error_reporter->Report(
          "Found too many dimensions in the input array of operation '%s'.\n",
          op_name);
      return kTfLiteError;
    } else {
      for (int i = 0; i < num_dimensions; ++i) {
   d445c:	3301      	adds	r3, #1
   d445e:	e7ee      	b.n	d443e <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4+0x22>
        buffer[i] = flat_vector->Get(i);
      }
    }
  }
  return kTfLiteOk;
   d4460:	2000      	movs	r0, #0
}
   d4462:	bd38      	pop	{r3, r4, r5, pc}
   d4464:	000e84a2 	.word	0x000e84a2
   d4468:	000e84d0 	.word	0x000e84d0
   d446c:	000e8511 	.word	0x000e8511
   d4470:	000e83d4 	.word	0x000e83d4
   d4474:	000e851c 	.word	0x000e851c

000d4478 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>:
// "tables" use an offset table (possibly shared) that allows fields to be
// omitted and added at will, but uses an extra indirection to read.
class Table {
 public:
  const uint8_t *GetVTable() const {
    return data_ - ReadScalar<soffset_t>(data_);
   d4478:	6803      	ldr	r3, [r0, #0]
   d447a:	1ac0      	subs	r0, r0, r3
    auto vtable = GetVTable();
    // The first element is the size of the vtable (fields + type id + itself).
    auto vtsize = ReadScalar<voffset_t>(vtable);
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
   d447c:	8803      	ldrh	r3, [r0, #0]
   d447e:	428b      	cmp	r3, r1
   d4480:	bf8c      	ite	hi
   d4482:	5a40      	ldrhhi	r0, [r0, r1]
   d4484:	2000      	movls	r0, #0
  }
   d4486:	4770      	bx	lr

000d4488 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>:

}  // namespace

TfLiteStatus ConvertTensorType(TensorType tensor_type, TfLiteType* type,
                               ErrorReporter* error_reporter) {
   d4488:	b508      	push	{r3, lr}
   d448a:	4603      	mov	r3, r0
   d448c:	4610      	mov	r0, r2
  *type = kTfLiteNoType;
  switch (tensor_type) {
   d448e:	2b09      	cmp	r3, #9
   d4490:	d806      	bhi.n	d44a0 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x18>
   d4492:	e8df f003 	tbb	[pc, r3]
   d4496:	0907      	.short	0x0907
   d4498:	15130f0d 	.word	0x15130f0d
   d449c:	11190b17 	.word	0x11190b17

}  // namespace

TfLiteStatus ConvertTensorType(TensorType tensor_type, TfLiteType* type,
                               ErrorReporter* error_reporter) {
  *type = kTfLiteNoType;
   d44a0:	2200      	movs	r2, #0
   d44a2:	e012      	b.n	d44ca <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
  switch (tensor_type) {
    case TensorType_FLOAT32:
      *type = kTfLiteFloat32;
   d44a4:	2201      	movs	r2, #1
   d44a6:	e010      	b.n	d44ca <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_FLOAT16:
      *type = kTfLiteFloat16;
   d44a8:	220a      	movs	r2, #10
   d44aa:	e00e      	b.n	d44ca <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_INT16:
      *type = kTfLiteInt16;
   d44ac:	2207      	movs	r2, #7
   d44ae:	e00c      	b.n	d44ca <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_INT32:
      *type = kTfLiteInt32;
   d44b0:	2202      	movs	r2, #2
   d44b2:	e00a      	b.n	d44ca <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_UINT8:
      *type = kTfLiteUInt8;
   d44b4:	2203      	movs	r2, #3
   d44b6:	e008      	b.n	d44ca <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_INT8:
      *type = kTfLiteInt8;
   d44b8:	2209      	movs	r2, #9
   d44ba:	e006      	b.n	d44ca <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_INT64:
      *type = kTfLiteInt64;
   d44bc:	2204      	movs	r2, #4
   d44be:	e004      	b.n	d44ca <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_STRING:
      *type = kTfLiteString;
   d44c0:	2205      	movs	r2, #5
   d44c2:	e002      	b.n	d44ca <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_BOOL:
      *type = kTfLiteBool;
   d44c4:	2206      	movs	r2, #6
   d44c6:	e000      	b.n	d44ca <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x42>
      break;
    case TensorType_COMPLEX64:
      *type = kTfLiteComplex64;
   d44c8:	2208      	movs	r2, #8
   d44ca:	700a      	strb	r2, [r1, #0]
      break;
  }
  if (*type == kTfLiteNoType) {
   d44cc:	780a      	ldrb	r2, [r1, #0]
   d44ce:	b92a      	cbnz	r2, d44dc <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x54>
    error_reporter->Report("Unsupported data type %d in tensor\n", tensor_type);
   d44d0:	461a      	mov	r2, r3
   d44d2:	4903      	ldr	r1, [pc, #12]	; (d44e0 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE+0x58>)
   d44d4:	f7ff ff7e 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return kTfLiteError;
   d44d8:	2001      	movs	r0, #1
   d44da:	bd08      	pop	{r3, pc}
  }
  return kTfLiteOk;
   d44dc:	2000      	movs	r0, #0
}
   d44de:	bd08      	pop	{r3, pc}
   d44e0:	000e85c8 	.word	0x000e85c8

000d44e4 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>:

  template<typename T> T GetField(voffset_t field, T defaultval) const {
   d44e4:	b538      	push	{r3, r4, r5, lr}
   d44e6:	4605      	mov	r5, r0
   d44e8:	4614      	mov	r4, r2
    auto field_offset = GetOptionalFieldOffset(field);
   d44ea:	f7ff ffc5 	bl	d4478 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d44ee:	b108      	cbz	r0, d44f4 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_+0x10>
   d44f0:	5c28      	ldrb	r0, [r5, r0]
   d44f2:	bd38      	pop	{r3, r4, r5, pc}
   d44f4:	4620      	mov	r0, r4
  }
   d44f6:	bd38      	pop	{r3, r4, r5, pc}

000d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>:
    return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_INPUTS);
  }
  const flatbuffers::Vector<int32_t> *outputs() const {
    return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_OUTPUTS);
  }
  BuiltinOptions builtin_options_type() const {
   d44f8:	b508      	push	{r3, lr}
    return static_cast<BuiltinOptions>(GetField<uint8_t>(VT_BUILTIN_OPTIONS_TYPE, 0));
   d44fa:	2200      	movs	r2, #0
   d44fc:	210a      	movs	r1, #10
   d44fe:	f7ff fff1 	bl	d44e4 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
  }
   d4502:	bd08      	pop	{r3, pc}

000d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>:
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
   d4504:	b538      	push	{r3, r4, r5, lr}
   d4506:	4605      	mov	r5, r0
   d4508:	4614      	mov	r4, r2
    auto field_offset = GetOptionalFieldOffset(field);
   d450a:	f7ff ffb5 	bl	d4478 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d450e:	b108      	cbz	r0, d4514 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_+0x10>
   d4510:	5828      	ldr	r0, [r5, r0]
   d4512:	bd38      	pop	{r3, r4, r5, pc}
   d4514:	4620      	mov	r0, r4
  }
   d4516:	bd38      	pop	{r3, r4, r5, pc}

000d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>:
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
   d4518:	b538      	push	{r3, r4, r5, lr}
   d451a:	4605      	mov	r5, r0
   d451c:	4614      	mov	r4, r2
    auto field_offset = GetOptionalFieldOffset(field);
   d451e:	f7ff ffab 	bl	d4478 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d4522:	b108      	cbz	r0, d4528 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_+0x10>
   d4524:	5628      	ldrsb	r0, [r5, r0]
   d4526:	bd38      	pop	{r3, r4, r5, pc}
   d4528:	4620      	mov	r0, r4
  }
   d452a:	bd38      	pop	{r3, r4, r5, pc}

000d452c <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>:
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
   d452c:	b510      	push	{r4, lr}
   d452e:	ed2d 8b02 	vpush	{d8}
   d4532:	4604      	mov	r4, r0
   d4534:	eeb0 8a40 	vmov.f32	s16, s0
    auto field_offset = GetOptionalFieldOffset(field);
   d4538:	f7ff ff9e 	bl	d4478 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d453c:	b118      	cbz	r0, d4546 <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_+0x1a>
   d453e:	4420      	add	r0, r4
   d4540:	ed90 0a00 	vldr	s0, [r0]
   d4544:	e001      	b.n	d454a <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_+0x1e>
   d4546:	eeb0 0a48 	vmov.f32	s0, s16
  }
   d454a:	ecbd 8b02 	vpop	{d8}
   d454e:	bd10      	pop	{r4, pc}

000d4550 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorIlEEEET_t>:

  template<typename P> P GetPointer(voffset_t field) {
   d4550:	b510      	push	{r4, lr}
   d4552:	4604      	mov	r4, r0
    auto field_offset = GetOptionalFieldOffset(field);
   d4554:	f7ff ff90 	bl	d4478 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    auto p = data_ + field_offset;
   d4558:	1822      	adds	r2, r4, r0
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
   d455a:	b108      	cbz	r0, d4560 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorIlEEEET_t+0x10>
   d455c:	5823      	ldr	r3, [r4, r0]
   d455e:	18d0      	adds	r0, r2, r3
  }
   d4560:	bd10      	pop	{r4, pc}

000d4562 <_ZNK6tflite8Operator15builtin_optionsEv>:
  const void *builtin_options() const {
   d4562:	b508      	push	{r3, lr}
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d4564:	210c      	movs	r1, #12
   d4566:	f7ff fff3 	bl	d4550 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorIlEEEET_t>
    return GetPointer<const void *>(VT_BUILTIN_OPTIONS);
  }
   d456a:	bd08      	pop	{r3, pc}

000d456c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI23TfLiteSequenceRNNParamsEEPT_v>:
  // extension part will take ownership so destructors  will not be run during
  // deallocation.
  template <typename T>
  T* AllocatePOD() {
    static_assert(std::is_pod<T>::value, "Builtin data structure must be POD.");
    return static_cast<T*>(this->Allocate(sizeof(T)));
   d456c:	6803      	ldr	r3, [r0, #0]
   d456e:	2102      	movs	r1, #2
   d4570:	681b      	ldr	r3, [r3, #0]
   d4572:	4718      	bx	r3

000d4574 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI26TfLiteFullyConnectedParamsEEPT_v>:
   d4574:	6803      	ldr	r3, [r0, #0]
   d4576:	2103      	movs	r1, #3
   d4578:	681b      	ldr	r3, [r3, #0]
   d457a:	4718      	bx	r3

000d457c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI19TfLiteSqueezeParamsEEPT_v>:
   d457c:	6803      	ldr	r3, [r0, #0]
   d457e:	2124      	movs	r1, #36	; 0x24
   d4580:	681b      	ldr	r3, [r3, #0]
   d4582:	4718      	bx	r3

000d4584 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI25TfLiteTransposeConvParamsEEPT_v>:
   d4584:	6803      	ldr	r3, [r0, #0]
   d4586:	210c      	movs	r1, #12
   d4588:	681b      	ldr	r3, [r3, #0]
   d458a:	4718      	bx	r3

000d458c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteFakeQuantParamsEEPT_v>:
   d458c:	6803      	ldr	r3, [r0, #0]
   d458e:	2110      	movs	r1, #16
   d4590:	681b      	ldr	r3, [r3, #0]
   d4592:	4718      	bx	r3

000d4594 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>:
   d4594:	6803      	ldr	r3, [r0, #0]
   d4596:	2104      	movs	r1, #4
   d4598:	681b      	ldr	r3, [r3, #0]
   d459a:	4718      	bx	r3

000d459c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>:
   d459c:	6803      	ldr	r3, [r0, #0]
   d459e:	2101      	movs	r1, #1
   d45a0:	681b      	ldr	r3, [r3, #0]
   d45a2:	4718      	bx	r3

000d45a4 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>:
   d45a4:	6803      	ldr	r3, [r0, #0]
   d45a6:	2108      	movs	r1, #8
   d45a8:	681b      	ldr	r3, [r3, #0]
   d45aa:	4718      	bx	r3

000d45ac <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv>:
// If it returns kTfLiteOk, it passes the data out with `builtin_data`, which
// need to be released by calling `free`.`
// If it returns kTfLiteError, `builtin_data` will be `nullptr`.
TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,
                         ErrorReporter* error_reporter,
                         BuiltinDataAllocator* allocator, void** builtin_data) {
   d45ac:	e92d 41f3 	stmdb	sp!, {r0, r1, r4, r5, r6, r7, r8, lr}
   d45b0:	9e08      	ldr	r6, [sp, #32]
   d45b2:	461d      	mov	r5, r3
        return kTfLiteCombinerTypeSum;
    }
  };

  SafeBuiltinDataAllocator safe_allocator(allocator);
  *builtin_data = nullptr;
   d45b4:	2300      	movs	r3, #0
// If it returns kTfLiteOk, it passes the data out with `builtin_data`, which
// need to be released by calling `free`.`
// If it returns kTfLiteError, `builtin_data` will be `nullptr`.
TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,
                         ErrorReporter* error_reporter,
                         BuiltinDataAllocator* allocator, void** builtin_data) {
   d45b6:	4604      	mov	r4, r0
   d45b8:	4617      	mov	r7, r2
        return kTfLiteCombinerTypeSum;
    }
  };

  SafeBuiltinDataAllocator safe_allocator(allocator);
  *builtin_data = nullptr;
   d45ba:	6033      	str	r3, [r6, #0]
   d45bc:	4698      	mov	r8, r3
  switch (op_type) {
   d45be:	2977      	cmp	r1, #119	; 0x77
   d45c0:	f200 870b 	bhi.w	d53da <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2e>
   d45c4:	e8df f011 	tbh	[pc, r1, lsl #1]
   d45c8:	010402be 	.word	0x010402be
   d45cc:	00780283 	.word	0x00780283
   d45d0:	048f0142 	.word	0x048f0142
   d45d4:	07090709 	.word	0x07090709
   d45d8:	02300709 	.word	0x02300709
   d45dc:	030b0709 	.word	0x030b0709
   d45e0:	03240104 	.word	0x03240104
   d45e4:	00e60709 	.word	0x00e60709
   d45e8:	01040353 	.word	0x01040353
   d45ec:	070902a4 	.word	0x070902a4
   d45f0:	07090709 	.word	0x07090709
   d45f4:	0400043b 	.word	0x0400043b
   d45f8:	026a01f9 	.word	0x026a01f9
   d45fc:	01860479 	.word	0x01860479
   d4600:	07090709 	.word	0x07090709
   d4604:	07090453 	.word	0x07090453
   d4608:	02130709 	.word	0x02130709
   d460c:	01a80709 	.word	0x01a80709
   d4610:	070904a5 	.word	0x070904a5
   d4614:	07090709 	.word	0x07090709
   d4618:	02f204bd 	.word	0x02f204bd
   d461c:	050202d8 	.word	0x050202d8
   d4620:	05280391 	.word	0x05280391
   d4624:	070901cc 	.word	0x070901cc
   d4628:	04d60709 	.word	0x04d60709
   d462c:	06070709 	.word	0x06070709
   d4630:	00b703c4 	.word	0x00b703c4
   d4634:	07090709 	.word	0x07090709
   d4638:	07090559 	.word	0x07090559
   d463c:	07090709 	.word	0x07090709
   d4640:	07090709 	.word	0x07090709
   d4644:	07090709 	.word	0x07090709
   d4648:	07090709 	.word	0x07090709
   d464c:	058d0709 	.word	0x058d0709
   d4650:	070905b3 	.word	0x070905b3
   d4654:	07090709 	.word	0x07090709
   d4658:	07090709 	.word	0x07090709
   d465c:	070904bd 	.word	0x070904bd
   d4660:	05cc0709 	.word	0x05cc0709
   d4664:	05730709 	.word	0x05730709
   d4668:	04bd060d 	.word	0x04bd060d
   d466c:	05ea04bd 	.word	0x05ea04bd
   d4670:	063d0709 	.word	0x063d0709
   d4674:	07090709 	.word	0x07090709
   d4678:	04bd0653 	.word	0x04bd0653
   d467c:	04bd0709 	.word	0x04bd0709
   d4680:	07090709 	.word	0x07090709
   d4684:	07090709 	.word	0x07090709
   d4688:	04220709 	.word	0x04220709
   d468c:	07090670 	.word	0x07090670
   d4690:	07090688 	.word	0x07090688
   d4694:	06a104ec 	.word	0x06a104ec
   d4698:	07090709 	.word	0x07090709
   d469c:	07090709 	.word	0x07090709
   d46a0:	07090709 	.word	0x07090709
   d46a4:	07090709 	.word	0x07090709
   d46a8:	070906ba 	.word	0x070906ba
   d46ac:	07090709 	.word	0x07090709
   d46b0:	07090709 	.word	0x07090709
   d46b4:	06ef06d5 	.word	0x06ef06d5
   d46b8:	682b      	ldr	r3, [r5, #0]
   d46ba:	2118      	movs	r1, #24
   d46bc:	681b      	ldr	r3, [r3, #0]
   d46be:	4628      	mov	r0, r5
   d46c0:	4798      	blx	r3
   d46c2:	4605      	mov	r5, r0
  template<typename T> const T *builtin_options_as() const;
  const Conv2DOptions *builtin_options_as_Conv2DOptions() const {
    return builtin_options_type() == BuiltinOptions_Conv2DOptions ? static_cast<const Conv2DOptions *>(builtin_options()) : nullptr;
   d46c4:	4620      	mov	r0, r4
   d46c6:	f7ff ff17 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d46ca:	2801      	cmp	r0, #1
   d46cc:	4607      	mov	r7, r0
   d46ce:	f040 8683 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d46d2:	4620      	mov	r0, r4
   d46d4:	f7ff ff45 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
    case BuiltinOperator_CONV_2D: {
      auto params = safe_allocator.Allocate<TfLiteConvParams>();
      if (auto* conv_params = op->builtin_options_as_Conv2DOptions()) {
   d46d8:	4604      	mov	r4, r0
   d46da:	2800      	cmp	r0, #0
   d46dc:	f000 867c 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_FUSED_ACTIVATION_FUNCTION = 10,
    VT_DILATION_W_FACTOR = 12,
    VT_DILATION_H_FACTOR = 14
  };
  Padding padding() const {
    return static_cast<Padding>(GetField<int8_t>(VT_PADDING, 0));
   d46e0:	2200      	movs	r2, #0
   d46e2:	2104      	movs	r1, #4
   d46e4:	f7ff ff18 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->padding = parse_padding(conv_params->padding());
   d46e8:	b2c0      	uxtb	r0, r0
   d46ea:	f7ff fe85 	bl	d43f8 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_7PaddingEE_clESA_.isra.0>
  }
  int32_t stride_w() const {
    return GetField<int32_t>(VT_STRIDE_W, 0);
   d46ee:	2200      	movs	r2, #0
   d46f0:	7028      	strb	r0, [r5, #0]
   d46f2:	2106      	movs	r1, #6
   d46f4:	4620      	mov	r0, r4
   d46f6:	f7ff ff05 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t stride_h() const {
    return GetField<int32_t>(VT_STRIDE_H, 0);
   d46fa:	2200      	movs	r2, #0
        params->stride_width = conv_params->stride_w();
   d46fc:	6068      	str	r0, [r5, #4]
   d46fe:	2108      	movs	r1, #8
   d4700:	4620      	mov	r0, r4
   d4702:	f7ff feff 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4706:	2200      	movs	r2, #0
   d4708:	210a      	movs	r1, #10
        params->stride_height = conv_params->stride_h();
   d470a:	60a8      	str	r0, [r5, #8]
   d470c:	4620      	mov	r0, r4
   d470e:	f7ff ff03 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(conv_params->fused_activation_function());
   d4712:	b2c0      	uxtb	r0, r0
   d4714:	f7ff fe78 	bl	d4408 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  }
  int32_t dilation_w_factor() const {
    return GetField<int32_t>(VT_DILATION_W_FACTOR, 1);
   d4718:	463a      	mov	r2, r7
   d471a:	7528      	strb	r0, [r5, #20]
   d471c:	210c      	movs	r1, #12
   d471e:	4620      	mov	r0, r4
   d4720:	f7ff fef0 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t dilation_h_factor() const {
    return GetField<int32_t>(VT_DILATION_H_FACTOR, 1);
   d4724:	463a      	mov	r2, r7

        params->dilation_width_factor = conv_params->dilation_w_factor();
   d4726:	60e8      	str	r0, [r5, #12]
   d4728:	210e      	movs	r1, #14
   d472a:	4620      	mov	r0, r4
   d472c:	f7ff feea 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->dilation_height_factor = conv_params->dilation_h_factor();
   d4730:	6128      	str	r0, [r5, #16]
   d4732:	f000 be51 	b.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4736:	4628      	mov	r0, r5
   d4738:	f7ff ff18 	bl	d456c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI23TfLiteSequenceRNNParamsEEPT_v>
   d473c:	4680      	mov	r8, r0
  }
  const LogSoftmaxOptions *builtin_options_as_LogSoftmaxOptions() const {
    return builtin_options_type() == BuiltinOptions_LogSoftmaxOptions ? static_cast<const LogSoftmaxOptions *>(builtin_options()) : nullptr;
  }
  const CastOptions *builtin_options_as_CastOptions() const {
    return builtin_options_type() == BuiltinOptions_CastOptions ? static_cast<const CastOptions *>(builtin_options()) : nullptr;
   d473e:	4620      	mov	r0, r4
      constexpr _Head_base(const _Head_base&) = default;
      constexpr _Head_base(_Head_base&&) = default;

      template<typename _UHead>
        constexpr _Head_base(_UHead&& __h)
	: _M_head_impl(std::forward<_UHead>(__h)) { }
   d4740:	e88d 0120 	stmia.w	sp, {r5, r8}
   d4744:	f7ff fed8 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4748:	2825      	cmp	r0, #37	; 0x25
   d474a:	f040 8462 	bne.w	d5012 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
   d474e:	4620      	mov	r0, r4
   d4750:	f7ff ff07 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_CAST: {
      auto params = safe_allocator.Allocate<TfLiteCastParams>();
      if (const auto* schema_params = op->builtin_options_as_CastOptions()) {
   d4754:	4605      	mov	r5, r0
   d4756:	2800      	cmp	r0, #0
   d4758:	f000 845b 	beq.w	d5012 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_IN_DATA_TYPE = 4,
    VT_OUT_DATA_TYPE = 6
  };
  TensorType in_data_type() const {
    return static_cast<TensorType>(GetField<int8_t>(VT_IN_DATA_TYPE, 0));
   d475c:	2200      	movs	r2, #0
   d475e:	2104      	movs	r1, #4
   d4760:	f7ff feda 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        auto in_status =
            ConvertTensorType(schema_params->in_data_type(),
                              &params->in_data_type, error_reporter);
   d4764:	463a      	mov	r2, r7
   d4766:	4641      	mov	r1, r8
   d4768:	b2c0      	uxtb	r0, r0
   d476a:	f7ff fe8d 	bl	d4488 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
  }
  TensorType out_data_type() const {
    return static_cast<TensorType>(GetField<int8_t>(VT_OUT_DATA_TYPE, 0));
   d476e:	2200      	movs	r2, #0
   d4770:	4604      	mov	r4, r0
   d4772:	2106      	movs	r1, #6
   d4774:	4628      	mov	r0, r5
   d4776:	f7ff fecf 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        auto out_status =
            ConvertTensorType(schema_params->out_data_type(),
                              &params->out_data_type, error_reporter);
   d477a:	9901      	ldr	r1, [sp, #4]
   d477c:	463a      	mov	r2, r7
   d477e:	3101      	adds	r1, #1
   d4780:	b2c0      	uxtb	r0, r0
   d4782:	f7ff fe81 	bl	d4488 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
        if (in_status != kTfLiteOk || out_status != kTfLiteOk) {
   d4786:	2c00      	cmp	r4, #0
   d4788:	f040 8186 	bne.w	d4a98 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x4ec>
   d478c:	2800      	cmp	r0, #0
   d478e:	f000 8440 	beq.w	d5012 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
   d4792:	e181      	b.n	d4a98 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x4ec>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4794:	4628      	mov	r0, r5
   d4796:	f7ff ff01 	bl	d459c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d479a:	4605      	mov	r5, r0
  }
  const ConcatEmbeddingsOptions *builtin_options_as_ConcatEmbeddingsOptions() const {
    return builtin_options_type() == BuiltinOptions_ConcatEmbeddingsOptions ? static_cast<const ConcatEmbeddingsOptions *>(builtin_options()) : nullptr;
  }
  const LSHProjectionOptions *builtin_options_as_LSHProjectionOptions() const {
    return builtin_options_type() == BuiltinOptions_LSHProjectionOptions ? static_cast<const LSHProjectionOptions *>(builtin_options()) : nullptr;
   d479c:	4620      	mov	r0, r4
   d479e:	f7ff feab 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d47a2:	2804      	cmp	r0, #4
   d47a4:	4607      	mov	r7, r0
   d47a6:	f040 8617 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d47aa:	4620      	mov	r0, r4
   d47ac:	f7ff fed9 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_LSH_PROJECTION: {
      auto params = safe_allocator.Allocate<TfLiteLSHProjectionParams>();
      if (const auto* lshParams =
   d47b0:	2800      	cmp	r0, #0
   d47b2:	f000 8611 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef LSHProjectionOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TYPE = 4
  };
  LSHProjectionType type() const {
    return static_cast<LSHProjectionType>(GetField<int8_t>(VT_TYPE, 0));
   d47b6:	2200      	movs	r2, #0
   d47b8:	4639      	mov	r1, r7
   d47ba:	f7ff fead 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
      default:
        return kTfLiteLshProjectionUnknown;
    }
  };
  auto parseCombinerType = [](CombinerType type) {
    switch (type) {
   d47be:	2801      	cmp	r0, #1
   d47c0:	d003      	beq.n	d47ca <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x21e>
        return kTfLiteCombinerTypeMean;
      case CombinerType_SQRTN:
        return kTfLiteCombinerTypeSqrtn;
      case CombinerType_SUM:
      default:
        return kTfLiteCombinerTypeSum;
   d47c2:	2802      	cmp	r0, #2
   d47c4:	bf0c      	ite	eq
   d47c6:	2002      	moveq	r0, #2
   d47c8:	2000      	movne	r0, #0
    }
    case BuiltinOperator_LSH_PROJECTION: {
      auto params = safe_allocator.Allocate<TfLiteLSHProjectionParams>();
      if (const auto* lshParams =
              op->builtin_options_as_LSHProjectionOptions()) {
        params->type = parseLSHProjectionType(lshParams->type());
   d47ca:	7028      	strb	r0, [r5, #0]
   d47cc:	f000 be04 	b.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d47d0:	682b      	ldr	r3, [r5, #0]
   d47d2:	2128      	movs	r1, #40	; 0x28
   d47d4:	681b      	ldr	r3, [r3, #0]
   d47d6:	4628      	mov	r0, r5
   d47d8:	4798      	blx	r3
   d47da:	4605      	mov	r5, r0
  }
  const LSHProjectionOptions *builtin_options_as_LSHProjectionOptions() const {
    return builtin_options_type() == BuiltinOptions_LSHProjectionOptions ? static_cast<const LSHProjectionOptions *>(builtin_options()) : nullptr;
  }
  const Pool2DOptions *builtin_options_as_Pool2DOptions() const {
    return builtin_options_type() == BuiltinOptions_Pool2DOptions ? static_cast<const Pool2DOptions *>(builtin_options()) : nullptr;
   d47dc:	4620      	mov	r0, r4
   d47de:	f7ff fe8b 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d47e2:	2805      	cmp	r0, #5
   d47e4:	f040 85f8 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d47e8:	4620      	mov	r0, r4
   d47ea:	f7ff feba 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
    }
    case BuiltinOperator_AVERAGE_POOL_2D:
    case BuiltinOperator_MAX_POOL_2D:
    case BuiltinOperator_L2_POOL_2D: {
      auto params = safe_allocator.Allocate<TfLitePoolParams>();
      if (const auto* pool_params = op->builtin_options_as_Pool2DOptions()) {
   d47ee:	4604      	mov	r4, r0
   d47f0:	2800      	cmp	r0, #0
   d47f2:	f000 85f1 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_FILTER_WIDTH = 10,
    VT_FILTER_HEIGHT = 12,
    VT_FUSED_ACTIVATION_FUNCTION = 14
  };
  Padding padding() const {
    return static_cast<Padding>(GetField<int8_t>(VT_PADDING, 0));
   d47f6:	2200      	movs	r2, #0
   d47f8:	2104      	movs	r1, #4
   d47fa:	f7ff fe8d 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->padding = parse_padding(pool_params->padding());
   d47fe:	b2c0      	uxtb	r0, r0
   d4800:	f7ff fdfa 	bl	d43f8 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_7PaddingEE_clESA_.isra.0>
  }
  int32_t stride_w() const {
    return GetField<int32_t>(VT_STRIDE_W, 0);
   d4804:	2200      	movs	r2, #0
   d4806:	7028      	strb	r0, [r5, #0]
   d4808:	2106      	movs	r1, #6
   d480a:	4620      	mov	r0, r4
   d480c:	f7ff fe7a 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t stride_h() const {
    return GetField<int32_t>(VT_STRIDE_H, 0);
   d4810:	2200      	movs	r2, #0
        params->stride_width = pool_params->stride_w();
   d4812:	6068      	str	r0, [r5, #4]
   d4814:	2108      	movs	r1, #8
   d4816:	4620      	mov	r0, r4
   d4818:	f7ff fe74 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t filter_width() const {
    return GetField<int32_t>(VT_FILTER_WIDTH, 0);
   d481c:	2200      	movs	r2, #0
        params->stride_height = pool_params->stride_h();
   d481e:	60a8      	str	r0, [r5, #8]
   d4820:	210a      	movs	r1, #10
   d4822:	4620      	mov	r0, r4
   d4824:	f7ff fe6e 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t filter_height() const {
    return GetField<int32_t>(VT_FILTER_HEIGHT, 0);
   d4828:	2200      	movs	r2, #0
        params->filter_width = pool_params->filter_width();
   d482a:	60e8      	str	r0, [r5, #12]
   d482c:	210c      	movs	r1, #12
   d482e:	4620      	mov	r0, r4
   d4830:	f7ff fe68 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4834:	2200      	movs	r2, #0
        params->filter_height = pool_params->filter_height();
   d4836:	6128      	str	r0, [r5, #16]
   d4838:	210e      	movs	r1, #14
   d483a:	4620      	mov	r0, r4
   d483c:	f7ff fe6c 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(pool_params->fused_activation_function());
   d4840:	b2c0      	uxtb	r0, r0
   d4842:	f7ff fde1 	bl	d4408 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
   d4846:	7528      	strb	r0, [r5, #20]
   d4848:	f000 bdc6 	b.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d484c:	682b      	ldr	r3, [r5, #0]
   d484e:	211c      	movs	r1, #28
   d4850:	681b      	ldr	r3, [r3, #0]
   d4852:	4628      	mov	r0, r5
   d4854:	4798      	blx	r3
   d4856:	4605      	mov	r5, r0
  template<typename T> const T *builtin_options_as() const;
  const Conv2DOptions *builtin_options_as_Conv2DOptions() const {
    return builtin_options_type() == BuiltinOptions_Conv2DOptions ? static_cast<const Conv2DOptions *>(builtin_options()) : nullptr;
  }
  const DepthwiseConv2DOptions *builtin_options_as_DepthwiseConv2DOptions() const {
    return builtin_options_type() == BuiltinOptions_DepthwiseConv2DOptions ? static_cast<const DepthwiseConv2DOptions *>(builtin_options()) : nullptr;
   d4858:	4620      	mov	r0, r4
   d485a:	f7ff fe4d 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d485e:	2802      	cmp	r0, #2
   d4860:	f040 85ba 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4864:	4620      	mov	r0, r4
   d4866:	f7ff fe7c 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_DEPTHWISE_CONV_2D: {
      auto params = safe_allocator.Allocate<TfLiteDepthwiseConvParams>();
      if (const auto* conv_params =
   d486a:	4604      	mov	r4, r0
   d486c:	2800      	cmp	r0, #0
   d486e:	f000 85b3 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_FUSED_ACTIVATION_FUNCTION = 12,
    VT_DILATION_W_FACTOR = 14,
    VT_DILATION_H_FACTOR = 16
  };
  Padding padding() const {
    return static_cast<Padding>(GetField<int8_t>(VT_PADDING, 0));
   d4872:	2200      	movs	r2, #0
   d4874:	2104      	movs	r1, #4
   d4876:	f7ff fe4f 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_DepthwiseConv2DOptions()) {
        params->padding = parse_padding(conv_params->padding());
   d487a:	b2c0      	uxtb	r0, r0
   d487c:	f7ff fdbc 	bl	d43f8 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_7PaddingEE_clESA_.isra.0>
  }
  int32_t stride_w() const {
    return GetField<int32_t>(VT_STRIDE_W, 0);
   d4880:	2200      	movs	r2, #0
   d4882:	7028      	strb	r0, [r5, #0]
   d4884:	2106      	movs	r1, #6
   d4886:	4620      	mov	r0, r4
   d4888:	f7ff fe3c 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t stride_h() const {
    return GetField<int32_t>(VT_STRIDE_H, 0);
   d488c:	2200      	movs	r2, #0
        params->stride_width = conv_params->stride_w();
   d488e:	6068      	str	r0, [r5, #4]
   d4890:	2108      	movs	r1, #8
   d4892:	4620      	mov	r0, r4
   d4894:	f7ff fe36 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t depth_multiplier() const {
    return GetField<int32_t>(VT_DEPTH_MULTIPLIER, 0);
   d4898:	2200      	movs	r2, #0
        params->stride_height = conv_params->stride_h();
   d489a:	60a8      	str	r0, [r5, #8]
   d489c:	210a      	movs	r1, #10
   d489e:	4620      	mov	r0, r4
   d48a0:	f7ff fe30 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d48a4:	2200      	movs	r2, #0
   d48a6:	210c      	movs	r1, #12
        params->depth_multiplier = conv_params->depth_multiplier();
   d48a8:	60e8      	str	r0, [r5, #12]
   d48aa:	4620      	mov	r0, r4
   d48ac:	f7ff fe34 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(conv_params->fused_activation_function());
   d48b0:	b2c0      	uxtb	r0, r0
   d48b2:	f7ff fda9 	bl	d4408 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  }
  int32_t dilation_w_factor() const {
    return GetField<int32_t>(VT_DILATION_W_FACTOR, 1);
   d48b6:	2201      	movs	r2, #1
   d48b8:	7428      	strb	r0, [r5, #16]
   d48ba:	210e      	movs	r1, #14
   d48bc:	4620      	mov	r0, r4
   d48be:	f7ff fe21 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t dilation_h_factor() const {
    return GetField<int32_t>(VT_DILATION_H_FACTOR, 1);
   d48c2:	2201      	movs	r2, #1

        params->dilation_width_factor = conv_params->dilation_w_factor();
   d48c4:	6168      	str	r0, [r5, #20]
   d48c6:	2110      	movs	r1, #16
   d48c8:	4620      	mov	r0, r4
   d48ca:	f7ff fe1b 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->dilation_height_factor = conv_params->dilation_h_factor();
   d48ce:	61a8      	str	r0, [r5, #24]
   d48d0:	f000 bd82 	b.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d48d4:	4628      	mov	r0, r5
   d48d6:	f7ff fe65 	bl	d45a4 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
   d48da:	4605      	mov	r5, r0
  }
  const Pool2DOptions *builtin_options_as_Pool2DOptions() const {
    return builtin_options_type() == BuiltinOptions_Pool2DOptions ? static_cast<const Pool2DOptions *>(builtin_options()) : nullptr;
  }
  const SVDFOptions *builtin_options_as_SVDFOptions() const {
    return builtin_options_type() == BuiltinOptions_SVDFOptions ? static_cast<const SVDFOptions *>(builtin_options()) : nullptr;
   d48dc:	4620      	mov	r0, r4
   d48de:	f7ff fe0b 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d48e2:	2806      	cmp	r0, #6
   d48e4:	4607      	mov	r7, r0
   d48e6:	f040 8577 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d48ea:	4620      	mov	r0, r4
   d48ec:	f7ff fe39 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SVDF: {
      auto params = safe_allocator.Allocate<TfLiteSVDFParams>();
      if (const auto* svdf_params = op->builtin_options_as_SVDFOptions()) {
   d48f0:	4604      	mov	r4, r0
   d48f2:	2800      	cmp	r0, #0
   d48f4:	f000 8570 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_RANK = 4,
    VT_FUSED_ACTIVATION_FUNCTION = 6
  };
  int32_t rank() const {
    return GetField<int32_t>(VT_RANK, 0);
   d48f8:	2200      	movs	r2, #0
   d48fa:	2104      	movs	r1, #4
   d48fc:	f7ff fe02 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4900:	2200      	movs	r2, #0
        params->rank = svdf_params->rank();
   d4902:	6028      	str	r0, [r5, #0]
   d4904:	4639      	mov	r1, r7
   d4906:	4620      	mov	r0, r4
   d4908:	f7ff fe06 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(svdf_params->fused_activation_function());
   d490c:	b2c0      	uxtb	r0, r0
   d490e:	f7ff fd7b 	bl	d4408 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
   d4912:	7128      	strb	r0, [r5, #4]
   d4914:	f000 bd60 	b.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4918:	4628      	mov	r0, r5
   d491a:	f7ff fe27 	bl	d456c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI23TfLiteSequenceRNNParamsEEPT_v>
   d491e:	4605      	mov	r5, r0
  }
  const SqueezeOptions *builtin_options_as_SqueezeOptions() const {
    return builtin_options_type() == BuiltinOptions_SqueezeOptions ? static_cast<const SqueezeOptions *>(builtin_options()) : nullptr;
  }
  const SequenceRNNOptions *builtin_options_as_SequenceRNNOptions() const {
    return builtin_options_type() == BuiltinOptions_SequenceRNNOptions ? static_cast<const SequenceRNNOptions *>(builtin_options()) : nullptr;
   d4920:	4620      	mov	r0, r4
   d4922:	f7ff fde9 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4926:	281f      	cmp	r0, #31
   d4928:	f040 8556 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d492c:	4620      	mov	r0, r4
   d492e:	f7ff fe18 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_UNIDIRECTIONAL_SEQUENCE_RNN: {
      auto params = safe_allocator.Allocate<TfLiteSequenceRNNParams>();
      if (const auto* sequence_rnn_params =
   d4932:	4604      	mov	r4, r0
   d4934:	2800      	cmp	r0, #0
   d4936:	f000 854f 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  };
  bool time_major() const {
    return GetField<uint8_t>(VT_TIME_MAJOR, 0) != 0;
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d493a:	2200      	movs	r2, #0
   d493c:	2106      	movs	r1, #6
   d493e:	f7ff fdeb 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_SequenceRNNOptions()) {
        params->activation =
            parse_activation(sequence_rnn_params->fused_activation_function());
   d4942:	b2c0      	uxtb	r0, r0
   d4944:	f7ff fd60 	bl	d4408 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TIME_MAJOR = 4,
    VT_FUSED_ACTIVATION_FUNCTION = 6
  };
  bool time_major() const {
    return GetField<uint8_t>(VT_TIME_MAJOR, 0) != 0;
   d4948:	2200      	movs	r2, #0
   d494a:	7068      	strb	r0, [r5, #1]
   d494c:	2104      	movs	r1, #4
   d494e:	4620      	mov	r0, r4
   d4950:	f7ff fdc8 	bl	d44e4 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->time_major = sequence_rnn_params->time_major();
   d4954:	3000      	adds	r0, #0
   d4956:	bf18      	it	ne
   d4958:	2001      	movne	r0, #1
   d495a:	7028      	strb	r0, [r5, #0]
   d495c:	f000 bd3c 	b.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4960:	4628      	mov	r0, r5
   d4962:	f7ff fe07 	bl	d4574 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI26TfLiteFullyConnectedParamsEEPT_v>
   d4966:	4605      	mov	r5, r0
  }
  const BidirectionalSequenceLSTMOptions *builtin_options_as_BidirectionalSequenceLSTMOptions() const {
    return builtin_options_type() == BuiltinOptions_BidirectionalSequenceLSTMOptions ? static_cast<const BidirectionalSequenceLSTMOptions *>(builtin_options()) : nullptr;
  }
  const BidirectionalSequenceRNNOptions *builtin_options_as_BidirectionalSequenceRNNOptions() const {
    return builtin_options_type() == BuiltinOptions_BidirectionalSequenceRNNOptions ? static_cast<const BidirectionalSequenceRNNOptions *>(builtin_options()) : nullptr;
   d4968:	4620      	mov	r0, r4
   d496a:	f7ff fdc5 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d496e:	2846      	cmp	r0, #70	; 0x46
   d4970:	f040 8532 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4974:	4620      	mov	r0, r4
   d4976:	f7ff fdf4 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_BIDIRECTIONAL_SEQUENCE_RNN: {
      auto params =
          safe_allocator.Allocate<TfLiteBidirectionalSequenceRNNParams>();
      if (const auto* bidi_sequence_rnn_params =
   d497a:	4604      	mov	r4, r0
   d497c:	2800      	cmp	r0, #0
   d497e:	f000 852b 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  };
  bool time_major() const {
    return GetField<uint8_t>(VT_TIME_MAJOR, 0) != 0;
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4982:	2200      	movs	r2, #0
   d4984:	2106      	movs	r1, #6
   d4986:	f7ff fdc7 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_BidirectionalSequenceRNNOptions()) {
        params->activation = parse_activation(
   d498a:	b2c0      	uxtb	r0, r0
   d498c:	f7ff fd3c 	bl	d4408 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
    VT_TIME_MAJOR = 4,
    VT_FUSED_ACTIVATION_FUNCTION = 6,
    VT_MERGE_OUTPUTS = 8
  };
  bool time_major() const {
    return GetField<uint8_t>(VT_TIME_MAJOR, 0) != 0;
   d4990:	2200      	movs	r2, #0
            bidi_sequence_rnn_params->fused_activation_function());
   d4992:	7068      	strb	r0, [r5, #1]
   d4994:	2104      	movs	r1, #4
   d4996:	4620      	mov	r0, r4
   d4998:	f7ff fda4 	bl	d44e4 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->time_major = bidi_sequence_rnn_params->time_major();
   d499c:	3000      	adds	r0, #0
   d499e:	bf18      	it	ne
   d49a0:	2001      	movne	r0, #1
   d49a2:	7028      	strb	r0, [r5, #0]
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
  }
  bool merge_outputs() const {
    return GetField<uint8_t>(VT_MERGE_OUTPUTS, 0) != 0;
   d49a4:	2200      	movs	r2, #0
   d49a6:	2108      	movs	r1, #8
   d49a8:	4620      	mov	r0, r4
   d49aa:	f7ff fd9b 	bl	d44e4 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->merge_outputs = bidi_sequence_rnn_params->merge_outputs();
   d49ae:	3000      	adds	r0, #0
   d49b0:	bf18      	it	ne
   d49b2:	2001      	movne	r0, #1
   d49b4:	70a8      	strb	r0, [r5, #2]
   d49b6:	f000 bd0f 	b.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d49ba:	4628      	mov	r0, r5
   d49bc:	f7ff fdee 	bl	d459c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d49c0:	4605      	mov	r5, r0
  }
  const SVDFOptions *builtin_options_as_SVDFOptions() const {
    return builtin_options_type() == BuiltinOptions_SVDFOptions ? static_cast<const SVDFOptions *>(builtin_options()) : nullptr;
  }
  const RNNOptions *builtin_options_as_RNNOptions() const {
    return builtin_options_type() == BuiltinOptions_RNNOptions ? static_cast<const RNNOptions *>(builtin_options()) : nullptr;
   d49c2:	4620      	mov	r0, r4
   d49c4:	f7ff fd98 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d49c8:	2807      	cmp	r0, #7
   d49ca:	f040 8505 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d49ce:	4620      	mov	r0, r4
   d49d0:	f7ff fdc7 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_RNN: {
      auto params = safe_allocator.Allocate<TfLiteRNNParams>();
      if (const auto* rnn_params = op->builtin_options_as_RNNOptions()) {
   d49d4:	2800      	cmp	r0, #0
   d49d6:	f000 84ff 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef RNNOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSED_ACTIVATION_FUNCTION = 4
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d49da:	2200      	movs	r2, #0
   d49dc:	2104      	movs	r1, #4
   d49de:	f7ff fd9b 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(rnn_params->fused_activation_function());
   d49e2:	b2c0      	uxtb	r0, r0
   d49e4:	f7ff fd10 	bl	d4408 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
   d49e8:	7028      	strb	r0, [r5, #0]
   d49ea:	f000 bcf5 	b.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d49ee:	4628      	mov	r0, r5
   d49f0:	f7ff fdd4 	bl	d459c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d49f4:	4605      	mov	r5, r0
  }
  const SpaceToDepthOptions *builtin_options_as_SpaceToDepthOptions() const {
    return builtin_options_type() == BuiltinOptions_SpaceToDepthOptions ? static_cast<const SpaceToDepthOptions *>(builtin_options()) : nullptr;
  }
  const EmbeddingLookupSparseOptions *builtin_options_as_EmbeddingLookupSparseOptions() const {
    return builtin_options_type() == BuiltinOptions_EmbeddingLookupSparseOptions ? static_cast<const EmbeddingLookupSparseOptions *>(builtin_options()) : nullptr;
   d49f6:	4620      	mov	r0, r4
   d49f8:	f7ff fd7e 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d49fc:	2814      	cmp	r0, #20
   d49fe:	f040 84eb 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4a02:	4620      	mov	r0, r4
   d4a04:	f7ff fdad 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_EMBEDDING_LOOKUP_SPARSE: {
      auto params =
          safe_allocator.Allocate<TfLiteEmbeddingLookupSparseParams>();
      if (const auto* embedding_params =
   d4a08:	2800      	cmp	r0, #0
   d4a0a:	f000 84e5 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef EmbeddingLookupSparseOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_COMBINER = 4
  };
  CombinerType combiner() const {
    return static_cast<CombinerType>(GetField<int8_t>(VT_COMBINER, 0));
   d4a0e:	2200      	movs	r2, #0
   d4a10:	2104      	movs	r1, #4
   d4a12:	f7ff fd81 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
      default:
        return kTfLiteLshProjectionUnknown;
    }
  };
  auto parseCombinerType = [](CombinerType type) {
    switch (type) {
   d4a16:	2801      	cmp	r0, #1
   d4a18:	d003      	beq.n	d4a22 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x476>
        return kTfLiteCombinerTypeMean;
      case CombinerType_SQRTN:
        return kTfLiteCombinerTypeSqrtn;
      case CombinerType_SUM:
      default:
        return kTfLiteCombinerTypeSum;
   d4a1a:	2802      	cmp	r0, #2
   d4a1c:	bf0c      	ite	eq
   d4a1e:	2002      	moveq	r0, #2
   d4a20:	2000      	movne	r0, #0
    case BuiltinOperator_EMBEDDING_LOOKUP_SPARSE: {
      auto params =
          safe_allocator.Allocate<TfLiteEmbeddingLookupSparseParams>();
      if (const auto* embedding_params =
              op->builtin_options_as_EmbeddingLookupSparseOptions()) {
        params->combiner = parseCombinerType(embedding_params->combiner());
   d4a22:	7028      	strb	r0, [r5, #0]
   d4a24:	f000 bcd8 	b.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4a28:	4628      	mov	r0, r5
   d4a2a:	f7ff fda3 	bl	d4574 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI26TfLiteFullyConnectedParamsEEPT_v>
   d4a2e:	4680      	mov	r8, r0
  }
  const RNNOptions *builtin_options_as_RNNOptions() const {
    return builtin_options_type() == BuiltinOptions_RNNOptions ? static_cast<const RNNOptions *>(builtin_options()) : nullptr;
  }
  const FullyConnectedOptions *builtin_options_as_FullyConnectedOptions() const {
    return builtin_options_type() == BuiltinOptions_FullyConnectedOptions ? static_cast<const FullyConnectedOptions *>(builtin_options()) : nullptr;
   d4a30:	4620      	mov	r0, r4
   d4a32:	e88d 0120 	stmia.w	sp, {r5, r8}
   d4a36:	f7ff fd5f 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4a3a:	2808      	cmp	r0, #8
   d4a3c:	4605      	mov	r5, r0
   d4a3e:	f040 82e8 	bne.w	d5012 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
   d4a42:	4620      	mov	r0, r4
   d4a44:	f7ff fd8d 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_FULLY_CONNECTED: {
      auto params = safe_allocator.Allocate<TfLiteFullyConnectedParams>();
      if (const auto* fully_connected_params =
   d4a48:	4604      	mov	r4, r0
   d4a4a:	2800      	cmp	r0, #0
   d4a4c:	f000 82e1 	beq.w	d5012 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
    VT_FUSED_ACTIVATION_FUNCTION = 4,
    VT_WEIGHTS_FORMAT = 6,
    VT_KEEP_NUM_DIMS = 8
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4a50:	2200      	movs	r2, #0
   d4a52:	2104      	movs	r1, #4
   d4a54:	f7ff fd60 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_FullyConnectedOptions()) {
        params->activation = parse_activation(
   d4a58:	b2c0      	uxtb	r0, r0
   d4a5a:	f7ff fcd5 	bl	d4408 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  }
  FullyConnectedOptionsWeightsFormat weights_format() const {
    return static_cast<FullyConnectedOptionsWeightsFormat>(GetField<int8_t>(VT_WEIGHTS_FORMAT, 0));
  }
  bool keep_num_dims() const {
    return GetField<uint8_t>(VT_KEEP_NUM_DIMS, 0) != 0;
   d4a5e:	2200      	movs	r2, #0
            fully_connected_params->fused_activation_function());
   d4a60:	f888 0000 	strb.w	r0, [r8]
   d4a64:	4629      	mov	r1, r5
   d4a66:	4620      	mov	r0, r4
   d4a68:	f7ff fd3c 	bl	d44e4 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
      }

      /// Return the stored pointer.
      pointer
      get() const noexcept
      { return std::get<0>(_M_t); }
   d4a6c:	f8dd 8004 	ldr.w	r8, [sp, #4]
        params->keep_num_dims = fully_connected_params->keep_num_dims();
   d4a70:	3000      	adds	r0, #0
   d4a72:	bf18      	it	ne
   d4a74:	2001      	movne	r0, #1
   d4a76:	f888 0002 	strb.w	r0, [r8, #2]
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
  }
  FullyConnectedOptionsWeightsFormat weights_format() const {
    return static_cast<FullyConnectedOptionsWeightsFormat>(GetField<int8_t>(VT_WEIGHTS_FORMAT, 0));
   d4a7a:	2200      	movs	r2, #0
   d4a7c:	2106      	movs	r1, #6
   d4a7e:	4620      	mov	r0, r4
   d4a80:	f7ff fd4a 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        switch (fully_connected_params->weights_format()) {
   d4a84:	b108      	cbz	r0, d4a8a <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x4de>
   d4a86:	2801      	cmp	r0, #1
   d4a88:	d102      	bne.n	d4a90 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x4e4>
   d4a8a:	9b01      	ldr	r3, [sp, #4]
          case FullyConnectedOptionsWeightsFormat_DEFAULT:
            params->weights_format = kTfLiteFullyConnectedWeightsFormatDefault;
            break;
          case FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8:
            params->weights_format =
                kTfLiteFullyConnectedWeightsFormatShuffled4x16Int8;
   d4a8c:	7058      	strb	r0, [r3, #1]
            break;
   d4a8e:	e2c0      	b.n	d5012 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
          default:
            error_reporter->Report("Unhandled fully-connected weights format.");
   d4a90:	49da      	ldr	r1, [pc, #872]	; (d4dfc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x850>)
   d4a92:	4638      	mov	r0, r7
   d4a94:	f7ff fc9e 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>

      /// Destructor, invokes the deleter if the stored pointer is not null.
      ~unique_ptr() noexcept
      {
	auto& __ptr = std::get<0>(_M_t);
	if (__ptr != nullptr)
   d4a98:	9901      	ldr	r1, [sp, #4]
   d4a9a:	e2b1      	b.n	d5000 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa54>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4a9c:	4628      	mov	r0, r5
   d4a9e:	f7ff fd79 	bl	d4594 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
   d4aa2:	4605      	mov	r5, r0
  }
  const FullyConnectedOptions *builtin_options_as_FullyConnectedOptions() const {
    return builtin_options_type() == BuiltinOptions_FullyConnectedOptions ? static_cast<const FullyConnectedOptions *>(builtin_options()) : nullptr;
  }
  const SoftmaxOptions *builtin_options_as_SoftmaxOptions() const {
    return builtin_options_type() == BuiltinOptions_SoftmaxOptions ? static_cast<const SoftmaxOptions *>(builtin_options()) : nullptr;
   d4aa4:	4620      	mov	r0, r4
   d4aa6:	f7ff fd27 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4aaa:	2809      	cmp	r0, #9
   d4aac:	f040 8494 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4ab0:	4620      	mov	r0, r4
   d4ab2:	f7ff fd56 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
    case BuiltinOperator_HASHTABLE_LOOKUP:
      // no-op.
      break;
    case BuiltinOperator_SOFTMAX: {
      auto params = safe_allocator.Allocate<TfLiteSoftmaxParams>();
      if (const auto* softmax_params =
   d4ab6:	2800      	cmp	r0, #0
   d4ab8:	f000 848e 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef SoftmaxOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BETA = 4
  };
  float beta() const {
    return GetField<float>(VT_BETA, 0.0f);
   d4abc:	ed9f 0ad0 	vldr	s0, [pc, #832]	; d4e00 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
   d4ac0:	2104      	movs	r1, #4
   d4ac2:	f7ff fd33 	bl	d452c <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
              op->builtin_options_as_SoftmaxOptions()) {
        params->beta = softmax_params->beta();
   d4ac6:	ed85 0a00 	vstr	s0, [r5]
   d4aca:	f000 bc85 	b.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4ace:	4628      	mov	r0, r5
   d4ad0:	f7ff fd68 	bl	d45a4 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
   d4ad4:	4605      	mov	r5, r0
  }
  const SoftmaxOptions *builtin_options_as_SoftmaxOptions() const {
    return builtin_options_type() == BuiltinOptions_SoftmaxOptions ? static_cast<const SoftmaxOptions *>(builtin_options()) : nullptr;
  }
  const ConcatenationOptions *builtin_options_as_ConcatenationOptions() const {
    return builtin_options_type() == BuiltinOptions_ConcatenationOptions ? static_cast<const ConcatenationOptions *>(builtin_options()) : nullptr;
   d4ad6:	4620      	mov	r0, r4
   d4ad8:	f7ff fd0e 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4adc:	280a      	cmp	r0, #10
   d4ade:	f040 847b 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4ae2:	4620      	mov	r0, r4
   d4ae4:	f7ff fd3d 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_CONCATENATION: {
      auto params = safe_allocator.Allocate<TfLiteConcatenationParams>();
      if (const auto* concatenation_params =
   d4ae8:	4604      	mov	r4, r0
   d4aea:	2800      	cmp	r0, #0
   d4aec:	f000 8474 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  };
  int32_t axis() const {
    return GetField<int32_t>(VT_AXIS, 0);
  }
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4af0:	2200      	movs	r2, #0
   d4af2:	2106      	movs	r1, #6
   d4af4:	f7ff fd10 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_ConcatenationOptions()) {
        params->activation =
            parse_activation(concatenation_params->fused_activation_function());
   d4af8:	b2c0      	uxtb	r0, r0
   d4afa:	f7ff fc85 	bl	d4408 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXIS = 4,
    VT_FUSED_ACTIVATION_FUNCTION = 6
  };
  int32_t axis() const {
    return GetField<int32_t>(VT_AXIS, 0);
   d4afe:	2200      	movs	r2, #0
   d4b00:	7128      	strb	r0, [r5, #4]
   d4b02:	2104      	movs	r1, #4
   d4b04:	4620      	mov	r0, r4
   d4b06:	f7ff fcfd 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->axis = concatenation_params->axis();
   d4b0a:	6028      	str	r0, [r5, #0]
   d4b0c:	f000 bc64 	b.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4b10:	4628      	mov	r0, r5
   d4b12:	f7ff fd43 	bl	d459c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d4b16:	4605      	mov	r5, r0
  }
  const EmbeddingLookupSparseOptions *builtin_options_as_EmbeddingLookupSparseOptions() const {
    return builtin_options_type() == BuiltinOptions_EmbeddingLookupSparseOptions ? static_cast<const EmbeddingLookupSparseOptions *>(builtin_options()) : nullptr;
  }
  const MulOptions *builtin_options_as_MulOptions() const {
    return builtin_options_type() == BuiltinOptions_MulOptions ? static_cast<const MulOptions *>(builtin_options()) : nullptr;
   d4b18:	4620      	mov	r0, r4
   d4b1a:	f7ff fced 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4b1e:	2815      	cmp	r0, #21
   d4b20:	f040 845a 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4b24:	4620      	mov	r0, r4
   d4b26:	f7ff fd1c 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_MUL: {
      auto params = safe_allocator.Allocate<TfLiteMulParams>();
      if (const auto* schema_params = op->builtin_options_as_MulOptions()) {
   d4b2a:	2800      	cmp	r0, #0
   d4b2c:	f000 8454 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef MulOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSED_ACTIVATION_FUNCTION = 4
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4b30:	2200      	movs	r2, #0
   d4b32:	2104      	movs	r1, #4
   d4b34:	f7ff fcf0 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(schema_params->fused_activation_function());
   d4b38:	b2c0      	uxtb	r0, r0
   d4b3a:	f7ff fc65 	bl	d4408 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
   d4b3e:	7028      	strb	r0, [r5, #0]
   d4b40:	f000 bc4a 	b.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4b44:	4628      	mov	r0, r5
   d4b46:	f7ff fd29 	bl	d459c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d4b4a:	4605      	mov	r5, r0
  }
  const ConcatenationOptions *builtin_options_as_ConcatenationOptions() const {
    return builtin_options_type() == BuiltinOptions_ConcatenationOptions ? static_cast<const ConcatenationOptions *>(builtin_options()) : nullptr;
  }
  const AddOptions *builtin_options_as_AddOptions() const {
    return builtin_options_type() == BuiltinOptions_AddOptions ? static_cast<const AddOptions *>(builtin_options()) : nullptr;
   d4b4c:	4620      	mov	r0, r4
   d4b4e:	f7ff fcd3 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4b52:	280b      	cmp	r0, #11
   d4b54:	f040 8440 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4b58:	4620      	mov	r0, r4
   d4b5a:	f7ff fd02 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_ADD: {
      auto params = safe_allocator.Allocate<TfLiteAddParams>();
      if (const auto* schema_params = op->builtin_options_as_AddOptions()) {
   d4b5e:	2800      	cmp	r0, #0
   d4b60:	f000 843a 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef AddOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSED_ACTIVATION_FUNCTION = 4
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4b64:	2200      	movs	r2, #0
   d4b66:	2104      	movs	r1, #4
   d4b68:	f7ff fcd6 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(schema_params->fused_activation_function());
   d4b6c:	b2c0      	uxtb	r0, r0
   d4b6e:	f7ff fc4b 	bl	d4408 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
   d4b72:	7028      	strb	r0, [r5, #0]
   d4b74:	f000 bc30 	b.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4b78:	4628      	mov	r0, r5
   d4b7a:	f7ff fd0f 	bl	d459c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d4b7e:	4605      	mov	r5, r0
  }
  const SubOptions *builtin_options_as_SubOptions() const {
    return builtin_options_type() == BuiltinOptions_SubOptions ? static_cast<const SubOptions *>(builtin_options()) : nullptr;
  }
  const DivOptions *builtin_options_as_DivOptions() const {
    return builtin_options_type() == BuiltinOptions_DivOptions ? static_cast<const DivOptions *>(builtin_options()) : nullptr;
   d4b80:	4620      	mov	r0, r4
   d4b82:	f7ff fcb9 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4b86:	281d      	cmp	r0, #29
   d4b88:	f040 8426 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4b8c:	4620      	mov	r0, r4
   d4b8e:	f7ff fce8 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_DIV: {
      auto params = safe_allocator.Allocate<TfLiteDivParams>();
      if (const auto* schema_params = op->builtin_options_as_DivOptions()) {
   d4b92:	2800      	cmp	r0, #0
   d4b94:	f000 8420 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef DivOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSED_ACTIVATION_FUNCTION = 4
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4b98:	2200      	movs	r2, #0
   d4b9a:	2104      	movs	r1, #4
   d4b9c:	f7ff fcbc 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(schema_params->fused_activation_function());
   d4ba0:	b2c0      	uxtb	r0, r0
   d4ba2:	f7ff fc31 	bl	d4408 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
   d4ba6:	7028      	strb	r0, [r5, #0]
   d4ba8:	f000 bc16 	b.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4bac:	4628      	mov	r0, r5
   d4bae:	f7ff fcf5 	bl	d459c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d4bb2:	4605      	mov	r5, r0
  }
  const ReducerOptions *builtin_options_as_ReducerOptions() const {
    return builtin_options_type() == BuiltinOptions_ReducerOptions ? static_cast<const ReducerOptions *>(builtin_options()) : nullptr;
  }
  const SubOptions *builtin_options_as_SubOptions() const {
    return builtin_options_type() == BuiltinOptions_SubOptions ? static_cast<const SubOptions *>(builtin_options()) : nullptr;
   d4bb4:	4620      	mov	r0, r4
   d4bb6:	f7ff fc9f 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4bba:	281c      	cmp	r0, #28
   d4bbc:	f040 840c 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4bc0:	4620      	mov	r0, r4
   d4bc2:	f7ff fcce 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SUB: {
      auto params = safe_allocator.Allocate<TfLiteSubParams>();
      if (const auto* schema_params = op->builtin_options_as_SubOptions()) {
   d4bc6:	2800      	cmp	r0, #0
   d4bc8:	f000 8406 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef SubOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSED_ACTIVATION_FUNCTION = 4
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4bcc:	2200      	movs	r2, #0
   d4bce:	2104      	movs	r1, #4
   d4bd0:	f7ff fca2 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(schema_params->fused_activation_function());
   d4bd4:	b2c0      	uxtb	r0, r0
   d4bd6:	f7ff fc17 	bl	d4408 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
   d4bda:	7028      	strb	r0, [r5, #0]
   d4bdc:	e3fc      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4bde:	4628      	mov	r0, r5
   d4be0:	f7ff fcdc 	bl	d459c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d4be4:	4605      	mov	r5, r0
  }
  const AddOptions *builtin_options_as_AddOptions() const {
    return builtin_options_type() == BuiltinOptions_AddOptions ? static_cast<const AddOptions *>(builtin_options()) : nullptr;
  }
  const L2NormOptions *builtin_options_as_L2NormOptions() const {
    return builtin_options_type() == BuiltinOptions_L2NormOptions ? static_cast<const L2NormOptions *>(builtin_options()) : nullptr;
   d4be6:	4620      	mov	r0, r4
   d4be8:	f7ff fc86 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4bec:	280c      	cmp	r0, #12
   d4bee:	f040 83f3 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4bf2:	4620      	mov	r0, r4
   d4bf4:	f7ff fcb5 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_L2_NORMALIZATION: {
      auto params = safe_allocator.Allocate<TfLiteL2NormParams>();
      if (const auto* schema_params = op->builtin_options_as_L2NormOptions()) {
   d4bf8:	2800      	cmp	r0, #0
   d4bfa:	f000 83ed 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef L2NormOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSED_ACTIVATION_FUNCTION = 4
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4bfe:	2200      	movs	r2, #0
   d4c00:	2104      	movs	r1, #4
   d4c02:	f7ff fc89 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(schema_params->fused_activation_function());
   d4c06:	b2c0      	uxtb	r0, r0
   d4c08:	f7ff fbfe 	bl	d4408 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
   d4c0c:	7028      	strb	r0, [r5, #0]
   d4c0e:	e3e3      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4c10:	4628      	mov	r0, r5
   d4c12:	f7ff fcbb 	bl	d458c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteFakeQuantParamsEEPT_v>
   d4c16:	4605      	mov	r5, r0
  }
  const L2NormOptions *builtin_options_as_L2NormOptions() const {
    return builtin_options_type() == BuiltinOptions_L2NormOptions ? static_cast<const L2NormOptions *>(builtin_options()) : nullptr;
  }
  const LocalResponseNormalizationOptions *builtin_options_as_LocalResponseNormalizationOptions() const {
    return builtin_options_type() == BuiltinOptions_LocalResponseNormalizationOptions ? static_cast<const LocalResponseNormalizationOptions *>(builtin_options()) : nullptr;
   d4c18:	4620      	mov	r0, r4
   d4c1a:	f7ff fc6d 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4c1e:	280d      	cmp	r0, #13
   d4c20:	f040 83da 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4c24:	4620      	mov	r0, r4
   d4c26:	f7ff fc9c 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_LOCAL_RESPONSE_NORMALIZATION: {
      auto params = safe_allocator.Allocate<TfLiteLocalResponseNormParams>();
      if (const auto* schema_params =
   d4c2a:	4604      	mov	r4, r0
   d4c2c:	2800      	cmp	r0, #0
   d4c2e:	f000 83d3 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_BIAS = 6,
    VT_ALPHA = 8,
    VT_BETA = 10
  };
  int32_t radius() const {
    return GetField<int32_t>(VT_RADIUS, 0);
   d4c32:	2200      	movs	r2, #0
   d4c34:	2104      	movs	r1, #4
   d4c36:	f7ff fc65 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  float bias() const {
    return GetField<float>(VT_BIAS, 0.0f);
   d4c3a:	2106      	movs	r1, #6
              op->builtin_options_as_LocalResponseNormalizationOptions()) {
        params->radius = schema_params->radius();
   d4c3c:	6028      	str	r0, [r5, #0]
   d4c3e:	ed9f 0a70 	vldr	s0, [pc, #448]	; d4e00 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
   d4c42:	4620      	mov	r0, r4
   d4c44:	f7ff fc72 	bl	d452c <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  float alpha() const {
    return GetField<float>(VT_ALPHA, 0.0f);
   d4c48:	2108      	movs	r1, #8
        params->bias = schema_params->bias();
   d4c4a:	ed85 0a01 	vstr	s0, [r5, #4]
   d4c4e:	4620      	mov	r0, r4
   d4c50:	ed9f 0a6b 	vldr	s0, [pc, #428]	; d4e00 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
   d4c54:	f7ff fc6a 	bl	d452c <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  float beta() const {
    return GetField<float>(VT_BETA, 0.0f);
   d4c58:	210a      	movs	r1, #10
        params->alpha = schema_params->alpha();
   d4c5a:	ed85 0a02 	vstr	s0, [r5, #8]
   d4c5e:	4620      	mov	r0, r4
   d4c60:	ed9f 0a67 	vldr	s0, [pc, #412]	; d4e00 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
   d4c64:	f7ff fc62 	bl	d452c <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
        params->beta = schema_params->beta();
   d4c68:	ed85 0a03 	vstr	s0, [r5, #12]
   d4c6c:	e3b4      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4c6e:	4628      	mov	r0, r5
   d4c70:	f7ff fc8c 	bl	d458c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteFakeQuantParamsEEPT_v>
   d4c74:	4680      	mov	r8, r0
  }
  const LocalResponseNormalizationOptions *builtin_options_as_LocalResponseNormalizationOptions() const {
    return builtin_options_type() == BuiltinOptions_LocalResponseNormalizationOptions ? static_cast<const LocalResponseNormalizationOptions *>(builtin_options()) : nullptr;
  }
  const LSTMOptions *builtin_options_as_LSTMOptions() const {
    return builtin_options_type() == BuiltinOptions_LSTMOptions ? static_cast<const LSTMOptions *>(builtin_options()) : nullptr;
   d4c76:	4620      	mov	r0, r4
   d4c78:	e88d 0120 	stmia.w	sp, {r5, r8}
   d4c7c:	f7ff fc3c 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4c80:	280e      	cmp	r0, #14
   d4c82:	d130      	bne.n	d4ce6 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x73a>
   d4c84:	4620      	mov	r0, r4
   d4c86:	f7ff fc6c 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_LSTM: {
      auto params = safe_allocator.Allocate<TfLiteLSTMParams>();
      if (const auto* lstm_params = op->builtin_options_as_LSTMOptions()) {
   d4c8a:	4605      	mov	r5, r0
   d4c8c:	b358      	cbz	r0, d4ce6 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x73a>
    VT_CELL_CLIP = 6,
    VT_PROJ_CLIP = 8,
    VT_KERNEL_TYPE = 10
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4c8e:	2200      	movs	r2, #0
   d4c90:	2104      	movs	r1, #4
   d4c92:	f7ff fc41 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->activation =
            parse_activation(lstm_params->fused_activation_function());
   d4c96:	b2c0      	uxtb	r0, r0
   d4c98:	f7ff fbb6 	bl	d4408 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  }
  float cell_clip() const {
    return GetField<float>(VT_CELL_CLIP, 0.0f);
   d4c9c:	2106      	movs	r1, #6
   d4c9e:	f888 0000 	strb.w	r0, [r8]
   d4ca2:	ed9f 0a57 	vldr	s0, [pc, #348]	; d4e00 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
      }

      /// Return the stored pointer.
      pointer
      get() const noexcept
      { return std::get<0>(_M_t); }
   d4ca6:	9c01      	ldr	r4, [sp, #4]
   d4ca8:	4628      	mov	r0, r5
   d4caa:	f7ff fc3f 	bl	d452c <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  float proj_clip() const {
    return GetField<float>(VT_PROJ_CLIP, 0.0f);
   d4cae:	2108      	movs	r1, #8
        params->cell_clip = lstm_params->cell_clip();
   d4cb0:	ed84 0a01 	vstr	s0, [r4, #4]
   d4cb4:	4628      	mov	r0, r5
   d4cb6:	ed9f 0a52 	vldr	s0, [pc, #328]	; d4e00 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
   d4cba:	9c01      	ldr	r4, [sp, #4]
   d4cbc:	f7ff fc36 	bl	d452c <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  LSTMKernelType kernel_type() const {
    return static_cast<LSTMKernelType>(GetField<int8_t>(VT_KERNEL_TYPE, 0));
   d4cc0:	2200      	movs	r2, #0
        params->proj_clip = lstm_params->proj_clip();
   d4cc2:	ed84 0a02 	vstr	s0, [r4, #8]
   d4cc6:	210a      	movs	r1, #10
   d4cc8:	4628      	mov	r0, r5
   d4cca:	f7ff fc25 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        switch (lstm_params->kernel_type()) {
   d4cce:	b108      	cbz	r0, d4cd4 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x728>
   d4cd0:	2801      	cmp	r0, #1
   d4cd2:	d102      	bne.n	d4cda <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x72e>
          case LSTMKernelType_FULL:
            params->kernel_type = kTfLiteLSTMFullKernel;
            break;
          case LSTMKernelType_BASIC:
            params->kernel_type = kTfLiteLSTMBasicKernel;
   d4cd4:	7320      	strb	r0, [r4, #12]
        }
      } else {
        error_reporter->Report("No valid LSTM builtin options exist");
        return kTfLiteError;
      }
      *builtin_data = reinterpret_cast<void*>(params.release());
   d4cd6:	6034      	str	r4, [r6, #0]
   d4cd8:	e37f      	b.n	d53da <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2e>
          case LSTMKernelType_BASIC:
            params->kernel_type = kTfLiteLSTMBasicKernel;
            break;
          default:
            error_reporter->Report("Unhandled LSTM kernel type: %d",
                                   lstm_params->kernel_type());
   d4cda:	b2c2      	uxtb	r2, r0
   d4cdc:	4949      	ldr	r1, [pc, #292]	; (d4e04 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x858>)
   d4cde:	4638      	mov	r0, r7
   d4ce0:	f7ff fb78 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
            return kTfLiteError;
   d4ce4:	e6d8      	b.n	d4a98 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x4ec>
        }
      } else {
        error_reporter->Report("No valid LSTM builtin options exist");
   d4ce6:	4948      	ldr	r1, [pc, #288]	; (d4e08 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x85c>)
   d4ce8:	e6d3      	b.n	d4a92 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x4e6>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4cea:	4628      	mov	r0, r5
   d4cec:	f7ff fc4e 	bl	d458c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteFakeQuantParamsEEPT_v>
   d4cf0:	4605      	mov	r5, r0
  }
  const BidirectionalSequenceRNNOptions *builtin_options_as_BidirectionalSequenceRNNOptions() const {
    return builtin_options_type() == BuiltinOptions_BidirectionalSequenceRNNOptions ? static_cast<const BidirectionalSequenceRNNOptions *>(builtin_options()) : nullptr;
  }
  const UnidirectionalSequenceLSTMOptions *builtin_options_as_UnidirectionalSequenceLSTMOptions() const {
    return builtin_options_type() == BuiltinOptions_UnidirectionalSequenceLSTMOptions ? static_cast<const UnidirectionalSequenceLSTMOptions *>(builtin_options()) : nullptr;
   d4cf2:	4620      	mov	r0, r4
   d4cf4:	f7ff fc00 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4cf8:	2847      	cmp	r0, #71	; 0x47
   d4cfa:	f040 836d 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4cfe:	4620      	mov	r0, r4
   d4d00:	f7ff fc2f 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_UNIDIRECTIONAL_SEQUENCE_LSTM: {
      auto params =
          safe_allocator.Allocate<TfLiteUnidirectionalSequenceLSTMParams>();
      if (const auto* seq_lstm_params =
   d4d04:	4604      	mov	r4, r0
   d4d06:	2800      	cmp	r0, #0
   d4d08:	f000 8366 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_CELL_CLIP = 6,
    VT_PROJ_CLIP = 8,
    VT_TIME_MAJOR = 10
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4d0c:	2200      	movs	r2, #0
   d4d0e:	2104      	movs	r1, #4
   d4d10:	f7ff fc02 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_UnidirectionalSequenceLSTMOptions()) {
        params->activation =
            parse_activation(seq_lstm_params->fused_activation_function());
   d4d14:	b2c0      	uxtb	r0, r0
   d4d16:	f7ff fb77 	bl	d4408 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  }
  float cell_clip() const {
    return GetField<float>(VT_CELL_CLIP, 0.0f);
   d4d1a:	2106      	movs	r1, #6
   d4d1c:	7028      	strb	r0, [r5, #0]
   d4d1e:	ed9f 0a38 	vldr	s0, [pc, #224]	; d4e00 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
   d4d22:	4620      	mov	r0, r4
   d4d24:	f7ff fc02 	bl	d452c <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  float proj_clip() const {
    return GetField<float>(VT_PROJ_CLIP, 0.0f);
   d4d28:	2108      	movs	r1, #8
        params->cell_clip = seq_lstm_params->cell_clip();
   d4d2a:	ed85 0a01 	vstr	s0, [r5, #4]
   d4d2e:	4620      	mov	r0, r4
   d4d30:	ed9f 0a33 	vldr	s0, [pc, #204]	; d4e00 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
   d4d34:	f7ff fbfa 	bl	d452c <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  bool time_major() const {
    return GetField<uint8_t>(VT_TIME_MAJOR, 0) != 0;
   d4d38:	2200      	movs	r2, #0
        params->proj_clip = seq_lstm_params->proj_clip();
   d4d3a:	ed85 0a02 	vstr	s0, [r5, #8]
   d4d3e:	210a      	movs	r1, #10
   d4d40:	4620      	mov	r0, r4
   d4d42:	f7ff fbcf 	bl	d44e4 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->time_major = seq_lstm_params->time_major();
   d4d46:	3000      	adds	r0, #0
   d4d48:	bf18      	it	ne
   d4d4a:	2001      	movne	r0, #1
   d4d4c:	7328      	strb	r0, [r5, #12]
   d4d4e:	e343      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4d50:	4628      	mov	r0, r5
   d4d52:	f7ff fc1b 	bl	d458c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteFakeQuantParamsEEPT_v>
   d4d56:	4605      	mov	r5, r0
  }
  const FillOptions *builtin_options_as_FillOptions() const {
    return builtin_options_type() == BuiltinOptions_FillOptions ? static_cast<const FillOptions *>(builtin_options()) : nullptr;
  }
  const BidirectionalSequenceLSTMOptions *builtin_options_as_BidirectionalSequenceLSTMOptions() const {
    return builtin_options_type() == BuiltinOptions_BidirectionalSequenceLSTMOptions ? static_cast<const BidirectionalSequenceLSTMOptions *>(builtin_options()) : nullptr;
   d4d58:	4620      	mov	r0, r4
   d4d5a:	f7ff fbcd 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4d5e:	2845      	cmp	r0, #69	; 0x45
   d4d60:	f040 833a 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4d64:	4620      	mov	r0, r4
   d4d66:	f7ff fbfc 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_BIDIRECTIONAL_SEQUENCE_LSTM: {
      auto params =
          safe_allocator.Allocate<TfLiteBidirectionalSequenceLSTMParams>();
      if (const auto* bidi_lstm_params =
   d4d6a:	4604      	mov	r4, r0
   d4d6c:	2800      	cmp	r0, #0
   d4d6e:	f000 8333 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_PROJ_CLIP = 8,
    VT_MERGE_OUTPUTS = 10,
    VT_TIME_MAJOR = 12
  };
  ActivationFunctionType fused_activation_function() const {
    return static_cast<ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   d4d72:	2200      	movs	r2, #0
   d4d74:	2104      	movs	r1, #4
   d4d76:	f7ff fbcf 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_BidirectionalSequenceLSTMOptions()) {
        params->activation =
            parse_activation(bidi_lstm_params->fused_activation_function());
   d4d7a:	b2c0      	uxtb	r0, r0
   d4d7c:	f7ff fb44 	bl	d4408 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_22ActivationFunctionTypeEE0_clESA_.isra.1>
  }
  float cell_clip() const {
    return GetField<float>(VT_CELL_CLIP, 0.0f);
   d4d80:	2106      	movs	r1, #6
   d4d82:	7028      	strb	r0, [r5, #0]
   d4d84:	ed9f 0a1e 	vldr	s0, [pc, #120]	; d4e00 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
   d4d88:	4620      	mov	r0, r4
   d4d8a:	f7ff fbcf 	bl	d452c <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  float proj_clip() const {
    return GetField<float>(VT_PROJ_CLIP, 0.0f);
   d4d8e:	2108      	movs	r1, #8
        params->cell_clip = bidi_lstm_params->cell_clip();
   d4d90:	ed85 0a01 	vstr	s0, [r5, #4]
   d4d94:	4620      	mov	r0, r4
   d4d96:	ed9f 0a1a 	vldr	s0, [pc, #104]	; d4e00 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0x854>
   d4d9a:	f7ff fbc7 	bl	d452c <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  bool merge_outputs() const {
    return GetField<uint8_t>(VT_MERGE_OUTPUTS, 0) != 0;
   d4d9e:	2200      	movs	r2, #0
        params->proj_clip = bidi_lstm_params->proj_clip();
   d4da0:	ed85 0a02 	vstr	s0, [r5, #8]
   d4da4:	210a      	movs	r1, #10
   d4da6:	4620      	mov	r0, r4
   d4da8:	f7ff fb9c 	bl	d44e4 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->merge_outputs = bidi_lstm_params->merge_outputs();
   d4dac:	3000      	adds	r0, #0
   d4dae:	bf18      	it	ne
   d4db0:	2001      	movne	r0, #1
   d4db2:	7328      	strb	r0, [r5, #12]
  }
  bool time_major() const {
    return GetField<uint8_t>(VT_TIME_MAJOR, 1) != 0;
   d4db4:	2201      	movs	r2, #1
   d4db6:	210c      	movs	r1, #12
   d4db8:	4620      	mov	r0, r4
   d4dba:	f7ff fb93 	bl	d44e4 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->time_major = bidi_lstm_params->time_major();
   d4dbe:	3000      	adds	r0, #0
   d4dc0:	bf18      	it	ne
   d4dc2:	2001      	movne	r0, #1
   d4dc4:	7368      	strb	r0, [r5, #13]
   d4dc6:	e307      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4dc8:	4628      	mov	r0, r5
   d4dca:	f7ff fbe7 	bl	d459c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d4dce:	4605      	mov	r5, r0
  }
  const LSTMOptions *builtin_options_as_LSTMOptions() const {
    return builtin_options_type() == BuiltinOptions_LSTMOptions ? static_cast<const LSTMOptions *>(builtin_options()) : nullptr;
  }
  const ResizeBilinearOptions *builtin_options_as_ResizeBilinearOptions() const {
    return builtin_options_type() == BuiltinOptions_ResizeBilinearOptions ? static_cast<const ResizeBilinearOptions *>(builtin_options()) : nullptr;
   d4dd0:	4620      	mov	r0, r4
   d4dd2:	f7ff fb91 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4dd6:	280f      	cmp	r0, #15
   d4dd8:	f040 82fe 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4ddc:	4620      	mov	r0, r4
   d4dde:	f7ff fbc0 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_RESIZE_BILINEAR: {
      auto params = safe_allocator.Allocate<TfLiteResizeBilinearParams>();
      if (const auto* schema_params =
   d4de2:	2800      	cmp	r0, #0
   d4de4:	f000 82f8 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef ResizeBilinearOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ALIGN_CORNERS = 8
  };
  bool align_corners() const {
    return GetField<uint8_t>(VT_ALIGN_CORNERS, 0) != 0;
   d4de8:	2200      	movs	r2, #0
   d4dea:	2108      	movs	r1, #8
   d4dec:	f7ff fb7a 	bl	d44e4 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
              op->builtin_options_as_ResizeBilinearOptions()) {
        params->align_corners = schema_params->align_corners();
   d4df0:	3000      	adds	r0, #0
   d4df2:	bf18      	it	ne
   d4df4:	2001      	movne	r0, #1
   d4df6:	7028      	strb	r0, [r5, #0]
   d4df8:	e2ee      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4dfa:	bf00      	nop
   d4dfc:	000e85ec 	.word	0x000e85ec
   d4e00:	00000000 	.word	0x00000000
   d4e04:	000e8616 	.word	0x000e8616
   d4e08:	000e8635 	.word	0x000e8635
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4e0c:	4628      	mov	r0, r5
   d4e0e:	f7ff fbc5 	bl	d459c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d4e12:	4605      	mov	r5, r0
  }
  const RangeOptions *builtin_options_as_RangeOptions() const {
    return builtin_options_type() == BuiltinOptions_RangeOptions ? static_cast<const RangeOptions *>(builtin_options()) : nullptr;
  }
  const ResizeNearestNeighborOptions *builtin_options_as_ResizeNearestNeighborOptions() const {
    return builtin_options_type() == BuiltinOptions_ResizeNearestNeighborOptions ? static_cast<const ResizeNearestNeighborOptions *>(builtin_options()) : nullptr;
   d4e14:	4620      	mov	r0, r4
   d4e16:	f7ff fb6f 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4e1a:	284a      	cmp	r0, #74	; 0x4a
   d4e1c:	f040 82dc 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4e20:	4620      	mov	r0, r4
   d4e22:	f7ff fb9e 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      // required to minimize function size. TODO(b/118447267): Simplify
      // ParseOpData function and reduce its length.
      [&]() {
        auto params =
            safe_allocator.Allocate<TfLiteResizeNearestNeighborParams>();
        if (const auto* schema_params =
   d4e26:	2800      	cmp	r0, #0
   d4e28:	f000 82d6 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef ResizeNearestNeighborOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ALIGN_CORNERS = 4
  };
  bool align_corners() const {
    return GetField<uint8_t>(VT_ALIGN_CORNERS, 0) != 0;
   d4e2c:	2200      	movs	r2, #0
   d4e2e:	2104      	movs	r1, #4
   d4e30:	f7ff fb58 	bl	d44e4 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
                op->builtin_options_as_ResizeNearestNeighborOptions()) {
          params->align_corners = schema_params->align_corners();
   d4e34:	3000      	adds	r0, #0
   d4e36:	bf18      	it	ne
   d4e38:	2001      	movne	r0, #1
   d4e3a:	7028      	strb	r0, [r5, #0]
   d4e3c:	e2cc      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4e3e:	4628      	mov	r0, r5
   d4e40:	f7ff fb9c 	bl	d457c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI19TfLiteSqueezeParamsEEPT_v>
   d4e44:	4680      	mov	r8, r0
  }
  const CallOptions *builtin_options_as_CallOptions() const {
    return builtin_options_type() == BuiltinOptions_CallOptions ? static_cast<const CallOptions *>(builtin_options()) : nullptr;
  }
  const ReshapeOptions *builtin_options_as_ReshapeOptions() const {
    return builtin_options_type() == BuiltinOptions_ReshapeOptions ? static_cast<const ReshapeOptions *>(builtin_options()) : nullptr;
   d4e46:	4620      	mov	r0, r4
   d4e48:	e88d 0120 	stmia.w	sp, {r5, r8}
   d4e4c:	f7ff fb54 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4e50:	2811      	cmp	r0, #17
   d4e52:	f040 80de 	bne.w	d5012 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
   d4e56:	4620      	mov	r0, r4
   d4e58:	f7ff fb83 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      }();
      break;
    }
    case BuiltinOperator_RESHAPE: {
      auto params = safe_allocator.Allocate<TfLiteReshapeParams>();
      if (const auto* schema_params = op->builtin_options_as_ReshapeOptions()) {
   d4e5c:	2800      	cmp	r0, #0
   d4e5e:	f000 80d8 	beq.w	d5012 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
   d4e62:	2104      	movs	r1, #4
   d4e64:	f7ff fb74 	bl	d4550 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorIlEEEET_t>
        auto* new_shape = schema_params->new_shape();
        TF_LITE_ENSURE_STATUS(FlatBufferIntVectorToArray(
   d4e68:	4bca      	ldr	r3, [pc, #808]	; (d5194 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xbe8>)
   d4e6a:	4604      	mov	r4, r0
   d4e6c:	e0c2      	b.n	d4ff4 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa48>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4e6e:	4628      	mov	r0, r5
   d4e70:	f7ff fb88 	bl	d4584 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI25TfLiteTransposeConvParamsEEPT_v>
   d4e74:	4605      	mov	r5, r0
  }
  const SkipGramOptions *builtin_options_as_SkipGramOptions() const {
    return builtin_options_type() == BuiltinOptions_SkipGramOptions ? static_cast<const SkipGramOptions *>(builtin_options()) : nullptr;
   d4e76:	4620      	mov	r0, r4
   d4e78:	f7ff fb3e 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4e7c:	2812      	cmp	r0, #18
   d4e7e:	f040 82ab 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4e82:	4620      	mov	r0, r4
   d4e84:	f7ff fb6d 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SKIP_GRAM: {
      auto params = safe_allocator.Allocate<TfLiteSkipGramParams>();
      if (const auto* skip_gram_params =
   d4e88:	4604      	mov	r4, r0
   d4e8a:	2800      	cmp	r0, #0
   d4e8c:	f000 82a4 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_NGRAM_SIZE = 4,
    VT_MAX_SKIP_SIZE = 6,
    VT_INCLUDE_ALL_NGRAMS = 8
  };
  int32_t ngram_size() const {
    return GetField<int32_t>(VT_NGRAM_SIZE, 0);
   d4e90:	2200      	movs	r2, #0
   d4e92:	2104      	movs	r1, #4
   d4e94:	f7ff fb36 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t max_skip_size() const {
    return GetField<int32_t>(VT_MAX_SKIP_SIZE, 0);
   d4e98:	2200      	movs	r2, #0
              op->builtin_options_as_SkipGramOptions()) {
        params->ngram_size = skip_gram_params->ngram_size();
   d4e9a:	6028      	str	r0, [r5, #0]
   d4e9c:	2106      	movs	r1, #6
   d4e9e:	4620      	mov	r0, r4
   d4ea0:	f7ff fb30 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  bool include_all_ngrams() const {
    return GetField<uint8_t>(VT_INCLUDE_ALL_NGRAMS, 0) != 0;
   d4ea4:	2200      	movs	r2, #0
        params->max_skip_size = skip_gram_params->max_skip_size();
   d4ea6:	6068      	str	r0, [r5, #4]
   d4ea8:	2108      	movs	r1, #8
   d4eaa:	4620      	mov	r0, r4
   d4eac:	f7ff fb1a 	bl	d44e4 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->include_all_ngrams = skip_gram_params->include_all_ngrams();
   d4eb0:	3000      	adds	r0, #0
   d4eb2:	bf18      	it	ne
   d4eb4:	2001      	movne	r0, #1
   d4eb6:	7228      	strb	r0, [r5, #8]
   d4eb8:	e28e      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4eba:	4628      	mov	r0, r5
   d4ebc:	f7ff fb6a 	bl	d4594 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
   d4ec0:	4605      	mov	r5, r0
  }
  const SkipGramOptions *builtin_options_as_SkipGramOptions() const {
    return builtin_options_type() == BuiltinOptions_SkipGramOptions ? static_cast<const SkipGramOptions *>(builtin_options()) : nullptr;
  }
  const SpaceToDepthOptions *builtin_options_as_SpaceToDepthOptions() const {
    return builtin_options_type() == BuiltinOptions_SpaceToDepthOptions ? static_cast<const SpaceToDepthOptions *>(builtin_options()) : nullptr;
   d4ec2:	4620      	mov	r0, r4
   d4ec4:	f7ff fb18 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4ec8:	2813      	cmp	r0, #19
   d4eca:	f040 8285 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4ece:	4620      	mov	r0, r4
   d4ed0:	f7ff fb47 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SPACE_TO_DEPTH: {
      auto params = safe_allocator.Allocate<TfLiteSpaceToDepthParams>();
      if (const auto* schema_params =
   d4ed4:	2800      	cmp	r0, #0
   d4ed6:	f000 827f 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef SpaceToDepthOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BLOCK_SIZE = 4
  };
  int32_t block_size() const {
    return GetField<int32_t>(VT_BLOCK_SIZE, 0);
   d4eda:	2200      	movs	r2, #0
   d4edc:	2104      	movs	r1, #4
   d4ede:	f7ff fb11 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
              op->builtin_options_as_SpaceToDepthOptions()) {
        params->block_size = schema_params->block_size();
   d4ee2:	6028      	str	r0, [r5, #0]
   d4ee4:	e278      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4ee6:	4628      	mov	r0, r5
   d4ee8:	f7ff fb54 	bl	d4594 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
   d4eec:	4605      	mov	r5, r0
  }
  const WhileOptions *builtin_options_as_WhileOptions() const {
    return builtin_options_type() == BuiltinOptions_WhileOptions ? static_cast<const WhileOptions *>(builtin_options()) : nullptr;
  }
  const DepthToSpaceOptions *builtin_options_as_DepthToSpaceOptions() const {
    return builtin_options_type() == BuiltinOptions_DepthToSpaceOptions ? static_cast<const DepthToSpaceOptions *>(builtin_options()) : nullptr;
   d4eee:	4620      	mov	r0, r4
   d4ef0:	f7ff fb02 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4ef4:	285e      	cmp	r0, #94	; 0x5e
   d4ef6:	f040 826f 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4efa:	4620      	mov	r0, r4
   d4efc:	f7ff fb31 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_DEPTH_TO_SPACE: {
      auto params = safe_allocator.Allocate<TfLiteDepthToSpaceParams>();
      if (const auto* schema_params =
   d4f00:	2800      	cmp	r0, #0
   d4f02:	f000 8269 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef DepthToSpaceOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BLOCK_SIZE = 4
  };
  int32_t block_size() const {
    return GetField<int32_t>(VT_BLOCK_SIZE, 0);
   d4f06:	2200      	movs	r2, #0
   d4f08:	2104      	movs	r1, #4
   d4f0a:	f7ff fafb 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
              op->builtin_options_as_DepthToSpaceOptions()) {
        params->block_size = schema_params->block_size();
   d4f0e:	6028      	str	r0, [r5, #0]
   d4f10:	e262      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4f12:	4628      	mov	r0, r5
   d4f14:	f7ff fb3e 	bl	d4594 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_GATHER: {
      auto params = safe_allocator.Allocate<TfLiteGatherParams>();
      params->axis = 0;
   d4f18:	f8c0 8000 	str.w	r8, [r0]
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4f1c:	4605      	mov	r5, r0
  }
  const PadOptions *builtin_options_as_PadOptions() const {
    return builtin_options_type() == BuiltinOptions_PadOptions ? static_cast<const PadOptions *>(builtin_options()) : nullptr;
  }
  const GatherOptions *builtin_options_as_GatherOptions() const {
    return builtin_options_type() == BuiltinOptions_GatherOptions ? static_cast<const GatherOptions *>(builtin_options()) : nullptr;
   d4f1e:	4620      	mov	r0, r4
   d4f20:	f7ff faea 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4f24:	2817      	cmp	r0, #23
   d4f26:	f040 8257 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4f2a:	4620      	mov	r0, r4
   d4f2c:	f7ff fb19 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_GATHER: {
      auto params = safe_allocator.Allocate<TfLiteGatherParams>();
      params->axis = 0;
      if (const auto* gather_params = op->builtin_options_as_GatherOptions()) {
   d4f30:	2800      	cmp	r0, #0
   d4f32:	f000 8251 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef GatherOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXIS = 4
  };
  int32_t axis() const {
    return GetField<int32_t>(VT_AXIS, 0);
   d4f36:	2200      	movs	r2, #0
   d4f38:	2104      	movs	r1, #4
   d4f3a:	f7ff fae3 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->axis = gather_params->axis();
   d4f3e:	6028      	str	r0, [r5, #0]
   d4f40:	e24a      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4f42:	4628      	mov	r0, r5
   d4f44:	f7ff fb2a 	bl	d459c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d4f48:	4605      	mov	r5, r0
  }
  const TransposeOptions *builtin_options_as_TransposeOptions() const {
    return builtin_options_type() == BuiltinOptions_TransposeOptions ? static_cast<const TransposeOptions *>(builtin_options()) : nullptr;
  }
  const ReducerOptions *builtin_options_as_ReducerOptions() const {
    return builtin_options_type() == BuiltinOptions_ReducerOptions ? static_cast<const ReducerOptions *>(builtin_options()) : nullptr;
   d4f4a:	4620      	mov	r0, r4
   d4f4c:	f7ff fad4 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4f50:	281b      	cmp	r0, #27
   d4f52:	f040 8241 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4f56:	4620      	mov	r0, r4
   d4f58:	f7ff fb03 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
    case BuiltinOperator_REDUCE_MIN:
    case BuiltinOperator_REDUCE_PROD:
    case BuiltinOperator_REDUCE_ANY:
    case BuiltinOperator_SUM: {
      auto params = safe_allocator.Allocate<TfLiteReducerParams>();
      if (const auto* schema_params = op->builtin_options_as_ReducerOptions()) {
   d4f5c:	2800      	cmp	r0, #0
   d4f5e:	f000 823b 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef ReducerOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_KEEP_DIMS = 4
  };
  bool keep_dims() const {
    return GetField<uint8_t>(VT_KEEP_DIMS, 0) != 0;
   d4f62:	2200      	movs	r2, #0
   d4f64:	2104      	movs	r1, #4
   d4f66:	f7ff fabd 	bl	d44e4 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->keep_dims = schema_params->keep_dims();
   d4f6a:	3000      	adds	r0, #0
   d4f6c:	bf18      	it	ne
   d4f6e:	2001      	movne	r0, #1
   d4f70:	7028      	strb	r0, [r5, #0]
   d4f72:	e231      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4f74:	4628      	mov	r0, r5
   d4f76:	f7ff fb0d 	bl	d4594 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
   d4f7a:	4605      	mov	r5, r0
  }
  const TopKV2Options *builtin_options_as_TopKV2Options() const {
    return builtin_options_type() == BuiltinOptions_TopKV2Options ? static_cast<const TopKV2Options *>(builtin_options()) : nullptr;
  }
  const SplitOptions *builtin_options_as_SplitOptions() const {
    return builtin_options_type() == BuiltinOptions_SplitOptions ? static_cast<const SplitOptions *>(builtin_options()) : nullptr;
   d4f7c:	4620      	mov	r0, r4
   d4f7e:	f7ff fabb 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4f82:	2823      	cmp	r0, #35	; 0x23
   d4f84:	f040 8228 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4f88:	4620      	mov	r0, r4
   d4f8a:	f7ff faea 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SPLIT: {
      auto params = safe_allocator.Allocate<TfLiteSplitParams>();
      if (const auto* schema_params = op->builtin_options_as_SplitOptions()) {
   d4f8e:	2800      	cmp	r0, #0
   d4f90:	f000 8222 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef SplitOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_NUM_SPLITS = 4
  };
  int32_t num_splits() const {
    return GetField<int32_t>(VT_NUM_SPLITS, 0);
   d4f94:	2200      	movs	r2, #0
   d4f96:	2104      	movs	r1, #4
   d4f98:	f7ff fab4 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->num_splits = schema_params->num_splits();
   d4f9c:	6028      	str	r0, [r5, #0]
   d4f9e:	e21b      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4fa0:	4628      	mov	r0, r5
   d4fa2:	f7ff faf7 	bl	d4594 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
   d4fa6:	4605      	mov	r5, r0
  }
  const AbsOptions *builtin_options_as_AbsOptions() const {
    return builtin_options_type() == BuiltinOptions_AbsOptions ? static_cast<const AbsOptions *>(builtin_options()) : nullptr;
  }
  const SplitVOptions *builtin_options_as_SplitVOptions() const {
    return builtin_options_type() == BuiltinOptions_SplitVOptions ? static_cast<const SplitVOptions *>(builtin_options()) : nullptr;
   d4fa8:	4620      	mov	r0, r4
   d4faa:	f7ff faa5 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4fae:	284f      	cmp	r0, #79	; 0x4f
   d4fb0:	f040 8212 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d4fb4:	4620      	mov	r0, r4
   d4fb6:	f7ff fad4 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SPLIT_V: {
      auto params = safe_allocator.Allocate<TfLiteSplitParams>();
      if (const auto* schema_params = op->builtin_options_as_SplitVOptions()) {
   d4fba:	2800      	cmp	r0, #0
   d4fbc:	f000 820c 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef SplitVOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_NUM_SPLITS = 4
  };
  int32_t num_splits() const {
    return GetField<int32_t>(VT_NUM_SPLITS, 0);
   d4fc0:	2200      	movs	r2, #0
   d4fc2:	2104      	movs	r1, #4
   d4fc4:	f7ff fa9e 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->num_splits = schema_params->num_splits();
   d4fc8:	6028      	str	r0, [r5, #0]
   d4fca:	e205      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d4fcc:	4628      	mov	r0, r5
   d4fce:	f7ff fad5 	bl	d457c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI19TfLiteSqueezeParamsEEPT_v>
   d4fd2:	4680      	mov	r8, r0
  }
  const DivOptions *builtin_options_as_DivOptions() const {
    return builtin_options_type() == BuiltinOptions_DivOptions ? static_cast<const DivOptions *>(builtin_options()) : nullptr;
  }
  const SqueezeOptions *builtin_options_as_SqueezeOptions() const {
    return builtin_options_type() == BuiltinOptions_SqueezeOptions ? static_cast<const SqueezeOptions *>(builtin_options()) : nullptr;
   d4fd4:	4620      	mov	r0, r4
   d4fd6:	e88d 0120 	stmia.w	sp, {r5, r8}
   d4fda:	f7ff fa8d 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d4fde:	281e      	cmp	r0, #30
   d4fe0:	d117      	bne.n	d5012 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
   d4fe2:	4620      	mov	r0, r4
   d4fe4:	f7ff fabd 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SQUEEZE: {
      auto params = safe_allocator.Allocate<TfLiteSqueezeParams>();
      if (const auto* schema_params = op->builtin_options_as_SqueezeOptions()) {
   d4fe8:	b198      	cbz	r0, d5012 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa66>
   d4fea:	2104      	movs	r1, #4
   d4fec:	f7ff fab0 	bl	d4550 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorIlEEEET_t>
        const auto& squeeze_dims = schema_params->squeeze_dims();
        TF_LITE_ENSURE_STATUS(FlatBufferIntVectorToArray(
   d4ff0:	4b69      	ldr	r3, [pc, #420]	; (d5198 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xbec>)
   d4ff2:	4604      	mov	r4, r0
   d4ff4:	4641      	mov	r1, r8
   d4ff6:	463a      	mov	r2, r7
   d4ff8:	f7ff fa10 	bl	d441c <_ZN6tflite12_GLOBAL__N_126FlatBufferIntVectorToArrayEiPKN11flatbuffers6VectorIlEEPiPNS_13ErrorReporterEPKc.constprop.4>
   d4ffc:	9901      	ldr	r1, [sp, #4]
   d4ffe:	b130      	cbz	r0, d500e <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xa62>

      /// Destructor, invokes the deleter if the stored pointer is not null.
      ~unique_ptr() noexcept
      {
	auto& __ptr = std::get<0>(_M_t);
	if (__ptr != nullptr)
   d5000:	2900      	cmp	r1, #0
   d5002:	f000 80ec 	beq.w	d51de <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xc32>
	  get_deleter()(__ptr);
   d5006:	4668      	mov	r0, sp
   d5008:	f7ff f9f2 	bl	d43f0 <_ZN6tflite12_GLOBAL__N_124SafeBuiltinDataAllocator18BuiltinDataDeleterclEPv>
   d500c:	e0e7      	b.n	d51de <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xc32>
            sizeof(params->squeeze_dims), squeeze_dims, params->squeeze_dims,
            error_reporter, "squeeze"));
        params->num_squeeze_dims = squeeze_dims->size();
   d500e:	6823      	ldr	r3, [r4, #0]
   d5010:	620b      	str	r3, [r1, #32]
      }
      *builtin_data = reinterpret_cast<void*>(params.release());
   d5012:	9b01      	ldr	r3, [sp, #4]
   d5014:	6033      	str	r3, [r6, #0]
   d5016:	e1e0      	b.n	d53da <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2e>
   d5018:	682b      	ldr	r3, [r5, #0]
   d501a:	2114      	movs	r1, #20
   d501c:	681b      	ldr	r3, [r3, #0]
   d501e:	4628      	mov	r0, r5
   d5020:	4798      	blx	r3
   d5022:	4605      	mov	r5, r0
  }
  const SequenceRNNOptions *builtin_options_as_SequenceRNNOptions() const {
    return builtin_options_type() == BuiltinOptions_SequenceRNNOptions ? static_cast<const SequenceRNNOptions *>(builtin_options()) : nullptr;
  }
  const StridedSliceOptions *builtin_options_as_StridedSliceOptions() const {
    return builtin_options_type() == BuiltinOptions_StridedSliceOptions ? static_cast<const StridedSliceOptions *>(builtin_options()) : nullptr;
   d5024:	4620      	mov	r0, r4
   d5026:	f7ff fa67 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d502a:	2820      	cmp	r0, #32
   d502c:	f040 81d4 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d5030:	4620      	mov	r0, r4
   d5032:	f7ff fa96 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_STRIDED_SLICE: {
      auto params = safe_allocator.Allocate<TfLiteStridedSliceParams>();
      if (const auto* schema_params =
   d5036:	4604      	mov	r4, r0
   d5038:	2800      	cmp	r0, #0
   d503a:	f000 81cd 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_ELLIPSIS_MASK = 8,
    VT_NEW_AXIS_MASK = 10,
    VT_SHRINK_AXIS_MASK = 12
  };
  int32_t begin_mask() const {
    return GetField<int32_t>(VT_BEGIN_MASK, 0);
   d503e:	2200      	movs	r2, #0
   d5040:	2104      	movs	r1, #4
   d5042:	f7ff fa5f 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t end_mask() const {
    return GetField<int32_t>(VT_END_MASK, 0);
   d5046:	2200      	movs	r2, #0
              op->builtin_options_as_StridedSliceOptions()) {
        params->begin_mask = schema_params->begin_mask();
   d5048:	6028      	str	r0, [r5, #0]
   d504a:	2106      	movs	r1, #6
   d504c:	4620      	mov	r0, r4
   d504e:	f7ff fa59 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t ellipsis_mask() const {
    return GetField<int32_t>(VT_ELLIPSIS_MASK, 0);
   d5052:	2200      	movs	r2, #0
        params->end_mask = schema_params->end_mask();
   d5054:	6068      	str	r0, [r5, #4]
   d5056:	2108      	movs	r1, #8
   d5058:	4620      	mov	r0, r4
   d505a:	f7ff fa53 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t new_axis_mask() const {
    return GetField<int32_t>(VT_NEW_AXIS_MASK, 0);
   d505e:	2200      	movs	r2, #0
        params->ellipsis_mask = schema_params->ellipsis_mask();
   d5060:	60a8      	str	r0, [r5, #8]
   d5062:	210a      	movs	r1, #10
   d5064:	4620      	mov	r0, r4
   d5066:	f7ff fa4d 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t shrink_axis_mask() const {
    return GetField<int32_t>(VT_SHRINK_AXIS_MASK, 0);
   d506a:	2200      	movs	r2, #0
        params->new_axis_mask = schema_params->new_axis_mask();
   d506c:	60e8      	str	r0, [r5, #12]
   d506e:	210c      	movs	r1, #12
   d5070:	4620      	mov	r0, r4
   d5072:	f7ff fa47 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->shrink_axis_mask = schema_params->shrink_axis_mask();
   d5076:	6128      	str	r0, [r5, #16]
   d5078:	e1ae      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d507a:	4628      	mov	r0, r5
   d507c:	f7ff fa8e 	bl	d459c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d5080:	4605      	mov	r5, r0
  }
  const MaximumMinimumOptions *builtin_options_as_MaximumMinimumOptions() const {
    return builtin_options_type() == BuiltinOptions_MaximumMinimumOptions ? static_cast<const MaximumMinimumOptions *>(builtin_options()) : nullptr;
  }
  const ArgMaxOptions *builtin_options_as_ArgMaxOptions() const {
    return builtin_options_type() == BuiltinOptions_ArgMaxOptions ? static_cast<const ArgMaxOptions *>(builtin_options()) : nullptr;
   d5082:	4620      	mov	r0, r4
   d5084:	f7ff fa38 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d5088:	2828      	cmp	r0, #40	; 0x28
   d508a:	f040 81a5 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d508e:	4620      	mov	r0, r4
   d5090:	f7ff fa67 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_ARG_MAX: {
      auto params = safe_allocator.Allocate<TfLiteArgMaxParams>();
      if (const auto* schema_params = op->builtin_options_as_ArgMaxOptions()) {
   d5094:	2800      	cmp	r0, #0
   d5096:	f000 819f 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef ArgMaxOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OUTPUT_TYPE = 4
  };
  TensorType output_type() const {
    return static_cast<TensorType>(GetField<int8_t>(VT_OUTPUT_TYPE, 0));
   d509a:	2200      	movs	r2, #0
   d509c:	2104      	movs	r1, #4
   d509e:	f7ff fa3b 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        ConvertTensorType(schema_params->output_type(), &params->output_type,
                          error_reporter);
   d50a2:	463a      	mov	r2, r7
   d50a4:	4629      	mov	r1, r5
   d50a6:	b2c0      	uxtb	r0, r0
   d50a8:	f7ff f9ee 	bl	d4488 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
   d50ac:	e194      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d50ae:	4628      	mov	r0, r5
   d50b0:	f7ff fa74 	bl	d459c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d50b4:	4605      	mov	r5, r0
  }
  const PowOptions *builtin_options_as_PowOptions() const {
    return builtin_options_type() == BuiltinOptions_PowOptions ? static_cast<const PowOptions *>(builtin_options()) : nullptr;
  }
  const ArgMinOptions *builtin_options_as_ArgMinOptions() const {
    return builtin_options_type() == BuiltinOptions_ArgMinOptions ? static_cast<const ArgMinOptions *>(builtin_options()) : nullptr;
   d50b6:	4620      	mov	r0, r4
   d50b8:	f7ff fa1e 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d50bc:	2839      	cmp	r0, #57	; 0x39
   d50be:	f040 818b 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d50c2:	4620      	mov	r0, r4
   d50c4:	f7ff fa4d 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_ARG_MIN: {
      auto params = safe_allocator.Allocate<TfLiteArgMinParams>();
      if (const auto* schema_params = op->builtin_options_as_ArgMinOptions()) {
   d50c8:	2800      	cmp	r0, #0
   d50ca:	f000 8185 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef ArgMinOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OUTPUT_TYPE = 4
  };
  TensorType output_type() const {
    return static_cast<TensorType>(GetField<int8_t>(VT_OUTPUT_TYPE, 0));
   d50ce:	2200      	movs	r2, #0
   d50d0:	2104      	movs	r1, #4
   d50d2:	f7ff fa21 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        ConvertTensorType(schema_params->output_type(), &params->output_type,
                          error_reporter);
   d50d6:	463a      	mov	r2, r7
   d50d8:	4629      	mov	r1, r5
   d50da:	b2c0      	uxtb	r0, r0
   d50dc:	f7ff f9d4 	bl	d4488 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
   d50e0:	e17a      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d50e2:	4628      	mov	r0, r5
   d50e4:	f7ff fa4e 	bl	d4584 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI25TfLiteTransposeConvParamsEEPT_v>
   d50e8:	4605      	mov	r5, r0
  }
  const SliceOptions *builtin_options_as_SliceOptions() const {
    return builtin_options_type() == BuiltinOptions_SliceOptions ? static_cast<const SliceOptions *>(builtin_options()) : nullptr;
  }
  const TransposeConvOptions *builtin_options_as_TransposeConvOptions() const {
    return builtin_options_type() == BuiltinOptions_TransposeConvOptions ? static_cast<const TransposeConvOptions *>(builtin_options()) : nullptr;
   d50ea:	4620      	mov	r0, r4
   d50ec:	f7ff fa04 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d50f0:	2831      	cmp	r0, #49	; 0x31
   d50f2:	f040 8171 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d50f6:	4620      	mov	r0, r4
   d50f8:	f7ff fa33 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_TRANSPOSE_CONV: {
      auto params = safe_allocator.Allocate<TfLiteTransposeConvParams>();
      if (const auto* transpose_conv_params =
   d50fc:	4604      	mov	r4, r0
   d50fe:	2800      	cmp	r0, #0
   d5100:	f000 816a 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_PADDING = 4,
    VT_STRIDE_W = 6,
    VT_STRIDE_H = 8
  };
  Padding padding() const {
    return static_cast<Padding>(GetField<int8_t>(VT_PADDING, 0));
   d5104:	2200      	movs	r2, #0
   d5106:	2104      	movs	r1, #4
   d5108:	f7ff fa06 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
              op->builtin_options_as_TransposeConvOptions()) {
        params->padding = parse_padding(transpose_conv_params->padding());
   d510c:	b2c0      	uxtb	r0, r0
   d510e:	f7ff f973 	bl	d43f8 <_ZZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPvENKUlNS_7PaddingEE_clESA_.isra.0>
  }
  int32_t stride_w() const {
    return GetField<int32_t>(VT_STRIDE_W, 0);
   d5112:	2200      	movs	r2, #0
   d5114:	7028      	strb	r0, [r5, #0]
   d5116:	2106      	movs	r1, #6
   d5118:	4620      	mov	r0, r4
   d511a:	f7ff f9f3 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t stride_h() const {
    return GetField<int32_t>(VT_STRIDE_H, 0);
   d511e:	2200      	movs	r2, #0
        params->stride_width = transpose_conv_params->stride_w();
   d5120:	6068      	str	r0, [r5, #4]
   d5122:	2108      	movs	r1, #8
   d5124:	4620      	mov	r0, r4
   d5126:	f7ff f9ed 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->stride_height = transpose_conv_params->stride_h();
   d512a:	60a8      	str	r0, [r5, #8]
   d512c:	e154      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d512e:	4628      	mov	r0, r5
   d5130:	f7ff fa34 	bl	d459c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d5134:	4605      	mov	r5, r0
  }
  const TransposeConvOptions *builtin_options_as_TransposeConvOptions() const {
    return builtin_options_type() == BuiltinOptions_TransposeConvOptions ? static_cast<const TransposeConvOptions *>(builtin_options()) : nullptr;
  }
  const SparseToDenseOptions *builtin_options_as_SparseToDenseOptions() const {
    return builtin_options_type() == BuiltinOptions_SparseToDenseOptions ? static_cast<const SparseToDenseOptions *>(builtin_options()) : nullptr;
   d5136:	4620      	mov	r0, r4
   d5138:	f7ff f9de 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d513c:	2832      	cmp	r0, #50	; 0x32
   d513e:	f040 814b 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d5142:	4620      	mov	r0, r4
   d5144:	f7ff fa0d 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SPARSE_TO_DENSE: {
      auto params = safe_allocator.Allocate<TfLiteSparseToDenseParams>();
      if (const auto* sparse_to_dense_params =
   d5148:	2800      	cmp	r0, #0
   d514a:	f000 8145 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef SparseToDenseOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_VALIDATE_INDICES = 4
  };
  bool validate_indices() const {
    return GetField<uint8_t>(VT_VALIDATE_INDICES, 0) != 0;
   d514e:	2200      	movs	r2, #0
   d5150:	2104      	movs	r1, #4
   d5152:	f7ff f9c7 	bl	d44e4 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
              op->builtin_options_as_SparseToDenseOptions()) {
        params->validate_indices = sparse_to_dense_params->validate_indices();
   d5156:	3000      	adds	r0, #0
   d5158:	bf18      	it	ne
   d515a:	2001      	movne	r0, #1
   d515c:	7028      	strb	r0, [r5, #0]
   d515e:	e13b      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d5160:	4628      	mov	r0, r5
   d5162:	f7ff fa1b 	bl	d459c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d5166:	4605      	mov	r5, r0
  }
  const NotEqualOptions *builtin_options_as_NotEqualOptions() const {
    return builtin_options_type() == BuiltinOptions_NotEqualOptions ? static_cast<const NotEqualOptions *>(builtin_options()) : nullptr;
  }
  const ShapeOptions *builtin_options_as_ShapeOptions() const {
    return builtin_options_type() == BuiltinOptions_ShapeOptions ? static_cast<const ShapeOptions *>(builtin_options()) : nullptr;
   d5168:	4620      	mov	r0, r4
   d516a:	f7ff f9c5 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d516e:	2837      	cmp	r0, #55	; 0x37
   d5170:	f040 8132 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d5174:	4620      	mov	r0, r4
   d5176:	f7ff f9f4 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_SHAPE: {
      auto params = safe_allocator.Allocate<TfLiteShapeParams>();
      if (const auto* schema_params = op->builtin_options_as_ShapeOptions()) {
   d517a:	2800      	cmp	r0, #0
   d517c:	f000 812c 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef ShapeOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OUT_TYPE = 4
  };
  TensorType out_type() const {
    return static_cast<TensorType>(GetField<int8_t>(VT_OUT_TYPE, 0));
   d5180:	2200      	movs	r2, #0
   d5182:	2104      	movs	r1, #4
   d5184:	f7ff f9c8 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        ConvertTensorType(schema_params->out_type(), &params->out_type,
                          error_reporter);
   d5188:	463a      	mov	r2, r7
   d518a:	4629      	mov	r1, r5
   d518c:	b2c0      	uxtb	r0, r0
   d518e:	f7ff f97b 	bl	d4488 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
   d5192:	e121      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d5194:	000e8659 	.word	0x000e8659
   d5198:	000e8661 	.word	0x000e8661
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d519c:	4628      	mov	r0, r5
   d519e:	f7ff fa01 	bl	d45a4 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
   d51a2:	4605      	mov	r5, r0
  }
  const FakeQuantOptions *builtin_options_as_FakeQuantOptions() const {
    return builtin_options_type() == BuiltinOptions_FakeQuantOptions ? static_cast<const FakeQuantOptions *>(builtin_options()) : nullptr;
  }
  const PackOptions *builtin_options_as_PackOptions() const {
    return builtin_options_type() == BuiltinOptions_PackOptions ? static_cast<const PackOptions *>(builtin_options()) : nullptr;
   d51a4:	4620      	mov	r0, r4
   d51a6:	f7ff f9a7 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d51aa:	283b      	cmp	r0, #59	; 0x3b
   d51ac:	f040 8114 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d51b0:	4620      	mov	r0, r4
   d51b2:	f7ff f9d6 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = static_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_PACK: {
      auto params = safe_allocator.Allocate<TfLitePackParams>();
      if (const auto* pack_params = op->builtin_options_as_PackOptions()) {
   d51b6:	4604      	mov	r4, r0
   d51b8:	2800      	cmp	r0, #0
   d51ba:	f000 810d 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_VALUES_COUNT = 4,
    VT_AXIS = 6
  };
  int32_t values_count() const {
    return GetField<int32_t>(VT_VALUES_COUNT, 0);
   d51be:	2200      	movs	r2, #0
   d51c0:	2104      	movs	r1, #4
   d51c2:	f7ff f99f 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t axis() const {
    return GetField<int32_t>(VT_AXIS, 0);
   d51c6:	2200      	movs	r2, #0
        params->values_count = pack_params->values_count();
   d51c8:	6028      	str	r0, [r5, #0]
   d51ca:	2106      	movs	r1, #6
   d51cc:	4620      	mov	r0, r4
   d51ce:	f7ff f999 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->axis = pack_params->axis();
   d51d2:	6068      	str	r0, [r5, #4]
   d51d4:	e100      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_DELEGATE: {
      // TODO(ycling): Revisit when supporting saving delegated models.
      error_reporter->Report("DELEGATE op shouldn't exist in model.");
   d51d6:	4983      	ldr	r1, [pc, #524]	; (d53e4 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe38>)
   d51d8:	4610      	mov	r0, r2
   d51da:	f7ff f8fb 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
      return kTfLiteError;
   d51de:	2001      	movs	r0, #1
   d51e0:	e0fc      	b.n	d53dc <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe30>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d51e2:	4628      	mov	r0, r5
   d51e4:	f7ff f9d2 	bl	d458c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteFakeQuantParamsEEPT_v>
   d51e8:	4605      	mov	r5, r0
  }
  const ArgMinOptions *builtin_options_as_ArgMinOptions() const {
    return builtin_options_type() == BuiltinOptions_ArgMinOptions ? static_cast<const ArgMinOptions *>(builtin_options()) : nullptr;
  }
  const FakeQuantOptions *builtin_options_as_FakeQuantOptions() const {
    return builtin_options_type() == BuiltinOptions_FakeQuantOptions ? static_cast<const FakeQuantOptions *>(builtin_options()) : nullptr;
   d51ea:	4620      	mov	r0, r4
   d51ec:	f7ff f984 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d51f0:	283a      	cmp	r0, #58	; 0x3a
   d51f2:	f040 80f1 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d51f6:	4620      	mov	r0, r4
   d51f8:	f7ff f9b3 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      error_reporter->Report("DELEGATE op shouldn't exist in model.");
      return kTfLiteError;
    }
    case BuiltinOperator_FAKE_QUANT: {
      auto params = safe_allocator.Allocate<TfLiteFakeQuantParams>();
      if (const auto* schema_params =
   d51fc:	4604      	mov	r4, r0
   d51fe:	2800      	cmp	r0, #0
   d5200:	f000 80ea 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
    VT_MAX = 6,
    VT_NUM_BITS = 8,
    VT_NARROW_RANGE = 10
  };
  float min() const {
    return GetField<float>(VT_MIN, 0.0f);
   d5204:	2104      	movs	r1, #4
   d5206:	ed9f 0a78 	vldr	s0, [pc, #480]	; d53e8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe3c>
   d520a:	f7ff f98f 	bl	d452c <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  float max() const {
    return GetField<float>(VT_MAX, 0.0f);
   d520e:	2106      	movs	r1, #6
              op->builtin_options_as_FakeQuantOptions()) {
        params->min = schema_params->min();
   d5210:	ed85 0a00 	vstr	s0, [r5]
   d5214:	4620      	mov	r0, r4
   d5216:	ed9f 0a74 	vldr	s0, [pc, #464]	; d53e8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe3c>
   d521a:	f7ff f987 	bl	d452c <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
  }
  int32_t num_bits() const {
    return GetField<int32_t>(VT_NUM_BITS, 0);
   d521e:	2200      	movs	r2, #0
        params->max = schema_params->max();
   d5220:	ed85 0a01 	vstr	s0, [r5, #4]
   d5224:	2108      	movs	r1, #8
   d5226:	4620      	mov	r0, r4
   d5228:	f7ff f96c 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  bool narrow_range() const {
    return GetField<uint8_t>(VT_NARROW_RANGE, 0) != 0;
   d522c:	2200      	movs	r2, #0
        params->num_bits = schema_params->num_bits();
   d522e:	60a8      	str	r0, [r5, #8]
   d5230:	210a      	movs	r1, #10
   d5232:	4620      	mov	r0, r4
   d5234:	f7ff f956 	bl	d44e4 <_ZNK11flatbuffers5Table8GetFieldIhEET_tS2_>
        params->narrow_range = schema_params->narrow_range();
   d5238:	3000      	adds	r0, #0
   d523a:	bf18      	it	ne
   d523c:	2001      	movne	r0, #1
   d523e:	7328      	strb	r0, [r5, #12]
   d5240:	e0ca      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d5242:	4628      	mov	r0, r5
   d5244:	f7ff f9a6 	bl	d4594 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
   d5248:	4605      	mov	r5, r0
  }
  const LogicalOrOptions *builtin_options_as_LogicalOrOptions() const {
    return builtin_options_type() == BuiltinOptions_LogicalOrOptions ? static_cast<const LogicalOrOptions *>(builtin_options()) : nullptr;
  }
  const OneHotOptions *builtin_options_as_OneHotOptions() const {
    return builtin_options_type() == BuiltinOptions_OneHotOptions ? static_cast<const OneHotOptions *>(builtin_options()) : nullptr;
   d524a:	4620      	mov	r0, r4
   d524c:	f7ff f954 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d5250:	283d      	cmp	r0, #61	; 0x3d
   d5252:	f040 80c1 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d5256:	4620      	mov	r0, r4
   d5258:	f7ff f983 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = static_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_ONE_HOT: {
      auto params = safe_allocator.Allocate<TfLiteOneHotParams>();
      if (const auto* schema_params = op->builtin_options_as_OneHotOptions()) {
   d525c:	2800      	cmp	r0, #0
   d525e:	f000 80bb 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef OneHotOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXIS = 4
  };
  int32_t axis() const {
    return GetField<int32_t>(VT_AXIS, 0);
   d5262:	2200      	movs	r2, #0
   d5264:	2104      	movs	r1, #4
   d5266:	f7ff f94d 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->axis = schema_params->axis();
   d526a:	6028      	str	r0, [r5, #0]
   d526c:	e0b4      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d526e:	4628      	mov	r0, r5
   d5270:	f7ff f998 	bl	d45a4 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
   d5274:	4605      	mov	r5, r0
  }
  const LogicalNotOptions *builtin_options_as_LogicalNotOptions() const {
    return builtin_options_type() == BuiltinOptions_LogicalNotOptions ? static_cast<const LogicalNotOptions *>(builtin_options()) : nullptr;
  }
  const UnpackOptions *builtin_options_as_UnpackOptions() const {
    return builtin_options_type() == BuiltinOptions_UnpackOptions ? static_cast<const UnpackOptions *>(builtin_options()) : nullptr;
   d5276:	4620      	mov	r0, r4
   d5278:	f7ff f93e 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d527c:	2840      	cmp	r0, #64	; 0x40
   d527e:	f040 80ab 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d5282:	4620      	mov	r0, r4
   d5284:	f7ff f96d 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = static_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_UNPACK: {
      auto params = safe_allocator.Allocate<TfLiteUnpackParams>();
      if (const auto* unpack_params = op->builtin_options_as_UnpackOptions()) {
   d5288:	4604      	mov	r4, r0
   d528a:	2800      	cmp	r0, #0
   d528c:	f000 80a4 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_NUM = 4,
    VT_AXIS = 6
  };
  int32_t num() const {
    return GetField<int32_t>(VT_NUM, 0);
   d5290:	2200      	movs	r2, #0
   d5292:	2104      	movs	r1, #4
   d5294:	f7ff f936 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t axis() const {
    return GetField<int32_t>(VT_AXIS, 0);
   d5298:	2200      	movs	r2, #0
        params->num = unpack_params->num();
   d529a:	6028      	str	r0, [r5, #0]
   d529c:	2106      	movs	r1, #6
   d529e:	4620      	mov	r0, r4
   d52a0:	f7ff f930 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->axis = unpack_params->axis();
   d52a4:	6068      	str	r0, [r5, #4]
   d52a6:	e097      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d52a8:	4628      	mov	r0, r5
   d52aa:	f7ff f973 	bl	d4594 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI21TfLiteLeakyReluParamsEEPT_v>
   d52ae:	4605      	mov	r5, r0
  }
  const ResizeNearestNeighborOptions *builtin_options_as_ResizeNearestNeighborOptions() const {
    return builtin_options_type() == BuiltinOptions_ResizeNearestNeighborOptions ? static_cast<const ResizeNearestNeighborOptions *>(builtin_options()) : nullptr;
  }
  const LeakyReluOptions *builtin_options_as_LeakyReluOptions() const {
    return builtin_options_type() == BuiltinOptions_LeakyReluOptions ? static_cast<const LeakyReluOptions *>(builtin_options()) : nullptr;
   d52b0:	4620      	mov	r0, r4
   d52b2:	f7ff f921 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d52b6:	284b      	cmp	r0, #75	; 0x4b
   d52b8:	f040 808e 	bne.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d52bc:	4620      	mov	r0, r4
   d52be:	f7ff f950 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_LEAKY_RELU: {
      auto params = safe_allocator.Allocate<TfLiteLeakyReluParams>();
      if (const auto* leaky_relu_params =
   d52c2:	2800      	cmp	r0, #0
   d52c4:	f000 8088 	beq.w	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef LeakyReluOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ALPHA = 4
  };
  float alpha() const {
    return GetField<float>(VT_ALPHA, 0.0f);
   d52c8:	ed9f 0a47 	vldr	s0, [pc, #284]	; d53e8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe3c>
   d52cc:	2104      	movs	r1, #4
   d52ce:	f7ff f92d 	bl	d452c <_ZNK11flatbuffers5Table8GetFieldIfEET_tS2_>
              op->builtin_options_as_LeakyReluOptions()) {
        params->alpha = leaky_relu_params->alpha();
   d52d2:	ed85 0a00 	vstr	s0, [r5]
   d52d6:	e07f      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d52d8:	4628      	mov	r0, r5
   d52da:	f7ff f95f 	bl	d459c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d52de:	4605      	mov	r5, r0
  }
  const SquaredDifferenceOptions *builtin_options_as_SquaredDifferenceOptions() const {
    return builtin_options_type() == BuiltinOptions_SquaredDifferenceOptions ? static_cast<const SquaredDifferenceOptions *>(builtin_options()) : nullptr;
  }
  const MirrorPadOptions *builtin_options_as_MirrorPadOptions() const {
    return builtin_options_type() == BuiltinOptions_MirrorPadOptions ? static_cast<const MirrorPadOptions *>(builtin_options()) : nullptr;
   d52e0:	4620      	mov	r0, r4
   d52e2:	f7ff f909 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d52e6:	284d      	cmp	r0, #77	; 0x4d
   d52e8:	d176      	bne.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d52ea:	4620      	mov	r0, r4
   d52ec:	f7ff f939 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_MIRROR_PAD: {
      auto params = safe_allocator.Allocate<TfLiteMirrorPaddingParams>();
      const auto* mirror_pad_params = op->builtin_options_as_MirrorPadOptions();
      if (mirror_pad_params != nullptr) {
   d52f0:	2800      	cmp	r0, #0
   d52f2:	d071      	beq.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef MirrorPadOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_MODE = 4
  };
  MirrorPadMode mode() const {
    return static_cast<MirrorPadMode>(GetField<int8_t>(VT_MODE, 0));
   d52f4:	2200      	movs	r2, #0
   d52f6:	2104      	movs	r1, #4
   d52f8:	f7ff f90e 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->mode =
            mirror_pad_params->mode() == tflite::MirrorPadMode_REFLECT
                ? TfLiteMirrorPaddingMode::kTfLiteMirrorPaddingReflect
                : TfLiteMirrorPaddingMode::kTfLiteMirrorPaddingSymmetric;
   d52fc:	b2c0      	uxtb	r0, r0
   d52fe:	2800      	cmp	r0, #0
   d5300:	bf0c      	ite	eq
   d5302:	2301      	moveq	r3, #1
   d5304:	2302      	movne	r3, #2
   d5306:	702b      	strb	r3, [r5, #0]
   d5308:	e066      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d530a:	4628      	mov	r0, r5
   d530c:	f7ff f946 	bl	d459c <_ZN6tflite20BuiltinDataAllocator11AllocatePODI18TfLiteUniqueParamsEEPT_v>
   d5310:	4605      	mov	r5, r0
  }
  const SplitVOptions *builtin_options_as_SplitVOptions() const {
    return builtin_options_type() == BuiltinOptions_SplitVOptions ? static_cast<const SplitVOptions *>(builtin_options()) : nullptr;
  }
  const UniqueOptions *builtin_options_as_UniqueOptions() const {
    return builtin_options_type() == BuiltinOptions_UniqueOptions ? static_cast<const UniqueOptions *>(builtin_options()) : nullptr;
   d5312:	4620      	mov	r0, r4
   d5314:	f7ff f8f0 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d5318:	2850      	cmp	r0, #80	; 0x50
   d531a:	d15d      	bne.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d531c:	4620      	mov	r0, r4
   d531e:	f7ff f920 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      break;
    }
    case BuiltinOperator_UNIQUE: {
      auto params = safe_allocator.Allocate<TfLiteUniqueParams>();
      const auto* unique_params = op->builtin_options_as_UniqueOptions();
      if (unique_params != nullptr) {
   d5322:	2800      	cmp	r0, #0
   d5324:	d058      	beq.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  typedef UniqueOptionsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_IDX_OUT_TYPE = 4
  };
  TensorType idx_out_type() const {
    return static_cast<TensorType>(GetField<int8_t>(VT_IDX_OUT_TYPE, 2));
   d5326:	2202      	movs	r2, #2
   d5328:	2104      	movs	r1, #4
   d532a:	f7ff f8f5 	bl	d4518 <_ZNK11flatbuffers5Table8GetFieldIaEET_tS2_>
        params->index_out_type =
            unique_params->idx_out_type() == tflite::TensorType_INT64
                ? TfLiteType::kTfLiteInt64
                : TfLiteType::kTfLiteInt32;
   d532e:	b2c0      	uxtb	r0, r0
   d5330:	2804      	cmp	r0, #4
   d5332:	bf0c      	ite	eq
   d5334:	2304      	moveq	r3, #4
   d5336:	2302      	movne	r3, #2
   d5338:	702b      	strb	r3, [r5, #0]
   d533a:	e04d      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      : allocator_(allocator) {}

  template <typename T>
  BuiltinDataPtr<T> Allocate() {
    return BuiltinDataPtr<T>(allocator_->AllocatePOD<T>(),
                             BuiltinDataDeleter(allocator_));
   d533c:	4628      	mov	r0, r5
   d533e:	f7ff f931 	bl	d45a4 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
   d5342:	4605      	mov	r5, r0
  }
  const RankOptions *builtin_options_as_RankOptions() const {
    return builtin_options_type() == BuiltinOptions_RankOptions ? static_cast<const RankOptions *>(builtin_options()) : nullptr;
  }
  const ReverseSequenceOptions *builtin_options_as_ReverseSequenceOptions() const {
    return builtin_options_type() == BuiltinOptions_ReverseSequenceOptions ? static_cast<const ReverseSequenceOptions *>(builtin_options()) : nullptr;
   d5344:	4620      	mov	r0, r4
   d5346:	f7ff f8d7 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d534a:	2857      	cmp	r0, #87	; 0x57
   d534c:	d144      	bne.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d534e:	4620      	mov	r0, r4
   d5350:	f7ff f907 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_REVERSE_SEQUENCE: {
      auto params = safe_allocator.Allocate<TfLiteReverseSequenceParams>();
      if (const auto* reverse_seq_params =
   d5354:	4604      	mov	r4, r0
   d5356:	2800      	cmp	r0, #0
   d5358:	d03e      	beq.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SEQ_DIM = 4,
    VT_BATCH_DIM = 6
  };
  int32_t seq_dim() const {
    return GetField<int32_t>(VT_SEQ_DIM, 0);
   d535a:	2200      	movs	r2, #0
   d535c:	2104      	movs	r1, #4
   d535e:	f7ff f8d1 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t batch_dim() const {
    return GetField<int32_t>(VT_BATCH_DIM, 0);
   d5362:	2200      	movs	r2, #0
              op->builtin_options_as_ReverseSequenceOptions()) {
        params->seq_dim = reverse_seq_params->seq_dim();
   d5364:	6028      	str	r0, [r5, #0]
   d5366:	2106      	movs	r1, #6
   d5368:	4620      	mov	r0, r4
   d536a:	f7ff f8cb 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->batch_dim = reverse_seq_params->batch_dim();
   d536e:	6068      	str	r0, [r5, #4]
   d5370:	e032      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      }
      *builtin_data = reinterpret_cast<void*>(params.release());
      break;
    }
    case BuiltinOperator_IF: {
      TfLiteIfParams* params = allocator->AllocatePOD<TfLiteIfParams>();
   d5372:	4628      	mov	r0, r5
   d5374:	f7ff f916 	bl	d45a4 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
   d5378:	4605      	mov	r5, r0
  }
  const HardSwishOptions *builtin_options_as_HardSwishOptions() const {
    return builtin_options_type() == BuiltinOptions_HardSwishOptions ? static_cast<const HardSwishOptions *>(builtin_options()) : nullptr;
  }
  const IfOptions *builtin_options_as_IfOptions() const {
    return builtin_options_type() == BuiltinOptions_IfOptions ? static_cast<const IfOptions *>(builtin_options()) : nullptr;
   d537a:	4620      	mov	r0, r4
   d537c:	f7ff f8bc 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d5380:	285c      	cmp	r0, #92	; 0x5c
   d5382:	d129      	bne.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d5384:	4620      	mov	r0, r4
   d5386:	f7ff f8ec 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      if (const auto* if_params = op->builtin_options_as_IfOptions()) {
   d538a:	4604      	mov	r4, r0
   d538c:	b320      	cbz	r0, d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_THEN_SUBGRAPH_INDEX = 4,
    VT_ELSE_SUBGRAPH_INDEX = 6
  };
  int32_t then_subgraph_index() const {
    return GetField<int32_t>(VT_THEN_SUBGRAPH_INDEX, 0);
   d538e:	2200      	movs	r2, #0
   d5390:	2104      	movs	r1, #4
   d5392:	f7ff f8b7 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t else_subgraph_index() const {
    return GetField<int32_t>(VT_ELSE_SUBGRAPH_INDEX, 0);
   d5396:	2200      	movs	r2, #0
        params->then_subgraph_index = if_params->then_subgraph_index();
   d5398:	6028      	str	r0, [r5, #0]
   d539a:	2106      	movs	r1, #6
   d539c:	4620      	mov	r0, r4
   d539e:	f7ff f8b1 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->else_subgraph_index = if_params->else_subgraph_index();
   d53a2:	6068      	str	r0, [r5, #4]
   d53a4:	e018      	b.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
      }
      *builtin_data = reinterpret_cast<void*>(params);
      break;
    }
    case BuiltinOperator_WHILE: {
      TfLiteWhileParams* params = allocator->AllocatePOD<TfLiteWhileParams>();
   d53a6:	4628      	mov	r0, r5
   d53a8:	f7ff f8fc 	bl	d45a4 <_ZN6tflite20BuiltinDataAllocator11AllocatePODI27TfLiteReverseSequenceParamsEEPT_v>
   d53ac:	4605      	mov	r5, r0
  }
  const IfOptions *builtin_options_as_IfOptions() const {
    return builtin_options_type() == BuiltinOptions_IfOptions ? static_cast<const IfOptions *>(builtin_options()) : nullptr;
  }
  const WhileOptions *builtin_options_as_WhileOptions() const {
    return builtin_options_type() == BuiltinOptions_WhileOptions ? static_cast<const WhileOptions *>(builtin_options()) : nullptr;
   d53ae:	4620      	mov	r0, r4
   d53b0:	f7ff f8a2 	bl	d44f8 <_ZNK6tflite8Operator20builtin_options_typeEv>
   d53b4:	285d      	cmp	r0, #93	; 0x5d
   d53b6:	d10f      	bne.n	d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
   d53b8:	4620      	mov	r0, r4
   d53ba:	f7ff f8d2 	bl	d4562 <_ZNK6tflite8Operator15builtin_optionsEv>
      if (const auto* while_params = op->builtin_options_as_WhileOptions()) {
   d53be:	4604      	mov	r4, r0
   d53c0:	b150      	cbz	r0, d53d8 <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv+0xe2c>
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_COND_SUBGRAPH_INDEX = 4,
    VT_BODY_SUBGRAPH_INDEX = 6
  };
  int32_t cond_subgraph_index() const {
    return GetField<int32_t>(VT_COND_SUBGRAPH_INDEX, 0);
   d53c2:	2200      	movs	r2, #0
   d53c4:	2104      	movs	r1, #4
   d53c6:	f7ff f89d 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
  }
  int32_t body_subgraph_index() const {
    return GetField<int32_t>(VT_BODY_SUBGRAPH_INDEX, 0);
   d53ca:	2200      	movs	r2, #0
        params->cond_subgraph_index = while_params->cond_subgraph_index();
   d53cc:	6028      	str	r0, [r5, #0]
   d53ce:	2106      	movs	r1, #6
   d53d0:	4620      	mov	r0, r4
   d53d2:	f7ff f897 	bl	d4504 <_ZNK11flatbuffers5Table8GetFieldIlEET_tS2_>
        params->body_subgraph_index = while_params->body_subgraph_index();
   d53d6:	6068      	str	r0, [r5, #4]
      }
      *builtin_data = reinterpret_cast<void*>(params);
   d53d8:	6035      	str	r5, [r6, #0]
    case BuiltinOperator_QUANTIZE:
    case BuiltinOperator_NON_MAX_SUPPRESSION_V4:
    case BuiltinOperator_NON_MAX_SUPPRESSION_V5:
      break;
  }
  return kTfLiteOk;
   d53da:	2000      	movs	r0, #0
}  // NOLINT[readability/fn_size]
   d53dc:	b002      	add	sp, #8
   d53de:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   d53e2:	bf00      	nop
   d53e4:	000e8669 	.word	0x000e8669
   d53e8:	00000000 	.word	0x00000000

000d53ec <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration>:

namespace tflite {

TfLiteStatus GetRegistrationFromOpCode(
    const OperatorCode* opcode, const OpResolver& op_resolver,
    ErrorReporter* error_reporter, const TfLiteRegistration** registration) {
   d53ec:	e92d 43f8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, lr}
   d53f0:	461f      	mov	r7, r3
  TfLiteStatus status = kTfLiteOk;
  *registration = nullptr;
   d53f2:	2300      	movs	r3, #0

namespace tflite {

TfLiteStatus GetRegistrationFromOpCode(
    const OperatorCode* opcode, const OpResolver& op_resolver,
    ErrorReporter* error_reporter, const TfLiteRegistration** registration) {
   d53f4:	460e      	mov	r6, r1
  TfLiteStatus status = kTfLiteOk;
  *registration = nullptr;
   d53f6:	603b      	str	r3, [r7, #0]
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
   d53f8:	2104      	movs	r1, #4

namespace tflite {

TfLiteStatus GetRegistrationFromOpCode(
    const OperatorCode* opcode, const OpResolver& op_resolver,
    ErrorReporter* error_reporter, const TfLiteRegistration** registration) {
   d53fa:	4605      	mov	r5, r0
   d53fc:	4690      	mov	r8, r2
   d53fe:	f7ff f83b 	bl	d4478 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d5402:	b100      	cbz	r0, d5406 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x1a>
   d5404:	5628      	ldrsb	r0, [r5, r0]
    VT_BUILTIN_CODE = 4,
    VT_CUSTOM_CODE = 6,
    VT_VERSION = 8
  };
  BuiltinOperator builtin_code() const {
    return static_cast<BuiltinOperator>(GetField<int8_t>(VT_BUILTIN_CODE, 0));
   d5406:	b2c4      	uxtb	r4, r0
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
   d5408:	2108      	movs	r1, #8
   d540a:	4628      	mov	r0, r5
   d540c:	f7ff f834 	bl	d4478 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d5410:	b110      	cbz	r0, d5418 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x2c>
   d5412:	f855 9000 	ldr.w	r9, [r5, r0]
   d5416:	e001      	b.n	d541c <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x30>
   d5418:	f04f 0901 	mov.w	r9, #1
  TfLiteStatus status = kTfLiteOk;
  *registration = nullptr;
  auto builtin_code = opcode->builtin_code();
  int version = opcode->version();

  if (builtin_code > BuiltinOperator_MAX ||
   d541c:	2c79      	cmp	r4, #121	; 0x79
   d541e:	d905      	bls.n	d542c <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x40>
      builtin_code < BuiltinOperator_MIN) {
    error_reporter->Report(
        "Op builtin_code out of range: %d. Are you using old TFLite binary "
        "with newer model?",
        builtin_code);
   d5420:	4622      	mov	r2, r4
   d5422:	491b      	ldr	r1, [pc, #108]	; (d5490 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0xa4>)
   d5424:	4640      	mov	r0, r8
   d5426:	f7fe ffd5 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
   d542a:	e01f      	b.n	d546c <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x80>
    status = kTfLiteError;
  } else if (builtin_code != BuiltinOperator_CUSTOM) {
   d542c:	2c20      	cmp	r4, #32
   d542e:	d010      	beq.n	d5452 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x66>
    *registration = op_resolver.FindOp(builtin_code, version);
   d5430:	6833      	ldr	r3, [r6, #0]
   d5432:	464a      	mov	r2, r9
   d5434:	681b      	ldr	r3, [r3, #0]
   d5436:	4621      	mov	r1, r4
   d5438:	4630      	mov	r0, r6
   d543a:	4798      	blx	r3
   d543c:	6038      	str	r0, [r7, #0]
    if (*registration == nullptr) {
   d543e:	bb20      	cbnz	r0, d548a <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x9e>
      error_reporter->Report(
          "Didn't find op for builtin opcode '%s' version '%d'\n",
          EnumNameBuiltinOperator(builtin_code), version);
   d5440:	4a14      	ldr	r2, [pc, #80]	; (d5494 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0xa8>)
   d5442:	4915      	ldr	r1, [pc, #84]	; (d5498 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0xac>)
   d5444:	f852 2024 	ldr.w	r2, [r2, r4, lsl #2]
   d5448:	464b      	mov	r3, r9
   d544a:	4640      	mov	r0, r8
   d544c:	f7fe ffc2 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
   d5450:	e00c      	b.n	d546c <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x80>
  }

  template<typename P> P GetPointer(voffset_t field) {
    auto field_offset = GetOptionalFieldOffset(field);
   d5452:	2106      	movs	r1, #6
   d5454:	4628      	mov	r0, r5
   d5456:	f7ff f80f 	bl	d4478 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    auto p = data_ + field_offset;
   d545a:	182a      	adds	r2, r5, r0
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
   d545c:	b110      	cbz	r0, d5464 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x78>
   d545e:	5829      	ldr	r1, [r5, r0]
      status = kTfLiteError;
    }
  } else if (!opcode->custom_code()) {
   d5460:	1851      	adds	r1, r2, r1
   d5462:	d106      	bne.n	d5472 <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0x86>
    error_reporter->Report(
        "Operator with CUSTOM builtin_code has no custom_code.\n");
   d5464:	490d      	ldr	r1, [pc, #52]	; (d549c <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration+0xb0>)
   d5466:	4640      	mov	r0, r8
   d5468:	f7fe ffb4 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    status = kTfLiteError;
   d546c:	2001      	movs	r0, #1
   d546e:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
  } else {
    const char* name = opcode->custom_code()->c_str();
    *registration = op_resolver.FindOp(name, version);
   d5472:	6833      	ldr	r3, [r6, #0]
   d5474:	464a      	mov	r2, r9
   d5476:	685b      	ldr	r3, [r3, #4]
   d5478:	3104      	adds	r1, #4
   d547a:	4630      	mov	r0, r6
   d547c:	4798      	blx	r3
   d547e:	6038      	str	r0, [r7, #0]
      builtin_code < BuiltinOperator_MIN) {
    error_reporter->Report(
        "Op builtin_code out of range: %d. Are you using old TFLite binary "
        "with newer model?",
        builtin_code);
    status = kTfLiteError;
   d5480:	fab0 f080 	clz	r0, r0
   d5484:	0940      	lsrs	r0, r0, #5
   d5486:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
namespace tflite {

TfLiteStatus GetRegistrationFromOpCode(
    const OperatorCode* opcode, const OpResolver& op_resolver,
    ErrorReporter* error_reporter, const TfLiteRegistration** registration) {
  TfLiteStatus status = kTfLiteOk;
   d548a:	2000      	movs	r0, #0
      // while preparing ops.
      status = kTfLiteError;
    }
  }
  return status;
}
   d548c:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
   d5490:	000e8880 	.word	0x000e8880
   d5494:	000e8694 	.word	0x000e8694
   d5498:	000e88d4 	.word	0x000e88d4
   d549c:	000e8909 	.word	0x000e8909

000d54a0 <_ZN6tflite19ResetVariableTensorEP12TfLiteTensor>:

#include <string.h>

namespace tflite {

TfLiteStatus ResetVariableTensor(TfLiteTensor* tensor) {
   d54a0:	b530      	push	{r4, r5, lr}
  if (!tensor->is_variable) {
   d54a2:	f890 302d 	ldrb.w	r3, [r0, #45]	; 0x2d
   d54a6:	b16b      	cbz	r3, d54c4 <_ZN6tflite19ResetVariableTensorEP12TfLiteTensor+0x24>
    return kTfLiteOk;
  }
  // TODO(b/115961645): Implement - If a variable tensor has a buffer, reset it
  // to the value of the buffer.
  int value = 0;
  if (tensor->type == kTfLiteInt8) {
   d54a8:	7803      	ldrb	r3, [r0, #0]
#if __ANDROID__ || defined(__x86_64__) || defined(__i386__) || \
    defined(__i386) || defined(__x86__) || defined(__X86__) || \
    defined(_X86_) || defined(_M_IX86) || defined(_M_X64)
  memset(tensor->data.raw, value, tensor->bytes);
#else
  char* raw_ptr = tensor->data.raw;
   d54aa:	6844      	ldr	r4, [r0, #4]
    return kTfLiteOk;
  }
  // TODO(b/115961645): Implement - If a variable tensor has a buffer, reset it
  // to the value of the buffer.
  int value = 0;
  if (tensor->type == kTfLiteInt8) {
   d54ac:	2b09      	cmp	r3, #9
    value = tensor->params.zero_point;
   d54ae:	bf0c      	ite	eq
   d54b0:	6901      	ldreq	r1, [r0, #16]
  if (!tensor->is_variable) {
    return kTfLiteOk;
  }
  // TODO(b/115961645): Implement - If a variable tensor has a buffer, reset it
  // to the value of the buffer.
  int value = 0;
   d54b2:	2100      	movne	r1, #0
#if __ANDROID__ || defined(__x86_64__) || defined(__i386__) || \
    defined(__i386) || defined(__x86__) || defined(__X86__) || \
    defined(_X86_) || defined(_M_IX86) || defined(_M_X64)
  memset(tensor->data.raw, value, tensor->bytes);
#else
  char* raw_ptr = tensor->data.raw;
   d54b4:	4623      	mov	r3, r4
  for (int i = 0; i < tensor->bytes; ++i) {
   d54b6:	6985      	ldr	r5, [r0, #24]
   d54b8:	1b1a      	subs	r2, r3, r4
   d54ba:	4295      	cmp	r5, r2
   d54bc:	d902      	bls.n	d54c4 <_ZN6tflite19ResetVariableTensorEP12TfLiteTensor+0x24>
    *raw_ptr = value;
   d54be:	f803 1b01 	strb.w	r1, [r3], #1
    defined(__i386) || defined(__x86__) || defined(__X86__) || \
    defined(_X86_) || defined(_M_IX86) || defined(_M_X64)
  memset(tensor->data.raw, value, tensor->bytes);
#else
  char* raw_ptr = tensor->data.raw;
  for (int i = 0; i < tensor->bytes; ++i) {
   d54c2:	e7f8      	b.n	d54b6 <_ZN6tflite19ResetVariableTensorEP12TfLiteTensor+0x16>
    *raw_ptr = value;
    raw_ptr++;
  }
#endif
  return kTfLiteOk;
}
   d54c4:	2000      	movs	r0, #0
   d54c6:	bd30      	pop	{r4, r5, pc}

000d54c8 <_ZN12_GLOBAL__N_19StrCatStrEPciPKc>:
  return start;
}

// Appends a string to a string, in-place. You need to pass in the maximum
// string length as the second argument.
char* StrCatStr(char* main, int main_max_length, const char* to_append) {
   d54c8:	b530      	push	{r4, r5, lr}
   d54ca:	4604      	mov	r4, r0
   d54cc:	4623      	mov	r3, r4
   d54ce:	3401      	adds	r4, #1
  char* current = main;
  while (*current != 0) {
   d54d0:	781d      	ldrb	r5, [r3, #0]
   d54d2:	2d00      	cmp	r5, #0
   d54d4:	d1fa      	bne.n	d54cc <_ZN12_GLOBAL__N_19StrCatStrEPciPKc+0x4>
    ++current;
  }
  char* current_end = main + (main_max_length - 1);
   d54d6:	3901      	subs	r1, #1
   d54d8:	4408      	add	r0, r1
   d54da:	3a01      	subs	r2, #1
  while ((*to_append != 0) && (current < current_end)) {
   d54dc:	f812 1f01 	ldrb.w	r1, [r2, #1]!
   d54e0:	b121      	cbz	r1, d54ec <_ZN12_GLOBAL__N_19StrCatStrEPciPKc+0x24>
   d54e2:	4283      	cmp	r3, r0
   d54e4:	d202      	bcs.n	d54ec <_ZN12_GLOBAL__N_19StrCatStrEPciPKc+0x24>
    *current = *to_append;
   d54e6:	f803 1b01 	strb.w	r1, [r3], #1
  char* current = main;
  while (*current != 0) {
    ++current;
  }
  char* current_end = main + (main_max_length - 1);
  while ((*to_append != 0) && (current < current_end)) {
   d54ea:	e7f7      	b.n	d54dc <_ZN12_GLOBAL__N_19StrCatStrEPciPKc+0x14>
    *current = *to_append;
    ++current;
    ++to_append;
  }
  *current = 0;
   d54ec:	2200      	movs	r2, #0
   d54ee:	701a      	strb	r2, [r3, #0]
  return current;
}
   d54f0:	4618      	mov	r0, r3
   d54f2:	bd30      	pop	{r4, r5, pc}

000d54f4 <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci>:

// Populates the provided buffer with an ASCII representation of the number.
char* FastUInt32ToBufferLeft(uint32_t i, char* buffer, int base) {
   d54f4:	b530      	push	{r4, r5, lr}
   d54f6:	4603      	mov	r3, r0
   d54f8:	460c      	mov	r4, r1
  char* start = buffer;
  do {
    int32_t digit = i % base;
   d54fa:	fbb3 f5f2 	udiv	r5, r3, r2
   d54fe:	fb02 3315 	mls	r3, r2, r5, r3
    char character;
    if (digit < 10) {
   d5502:	2b09      	cmp	r3, #9
      character = '0' + digit;
   d5504:	bfd4      	ite	le
   d5506:	3330      	addle	r3, #48	; 0x30
    } else {
      character = 'a' + (digit - 10);
   d5508:	3357      	addgt	r3, #87	; 0x57
    }
    *buffer++ = character;
   d550a:	4620      	mov	r0, r4
    int32_t digit = i % base;
    char character;
    if (digit < 10) {
      character = '0' + digit;
    } else {
      character = 'a' + (digit - 10);
   d550c:	b2db      	uxtb	r3, r3
    }
    *buffer++ = character;
   d550e:	f800 3b01 	strb.w	r3, [r0], #1
    i /= base;
   d5512:	462b      	mov	r3, r5
  } while (i > 0);
   d5514:	b10d      	cbz	r5, d551a <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci+0x26>
   d5516:	4604      	mov	r4, r0
   d5518:	e7ef      	b.n	d54fa <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci+0x6>
  *buffer = 0;
   d551a:	7065      	strb	r5, [r4, #1]

// Reverses a zero-terminated string in-place.
char* ReverseStringInPlace(char* start, char* end) {
  char* p1 = start;
  char* p2 = end - 1;
  while (p1 < p2) {
   d551c:	42a1      	cmp	r1, r4
   d551e:	d206      	bcs.n	d552e <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci+0x3a>
    char tmp = *p1;
   d5520:	780b      	ldrb	r3, [r1, #0]
    *p1++ = *p2;
   d5522:	7822      	ldrb	r2, [r4, #0]
   d5524:	f801 2b01 	strb.w	r2, [r1], #1
    *p2-- = tmp;
   d5528:	f804 3901 	strb.w	r3, [r4], #-1
   d552c:	e7f6      	b.n	d551c <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci+0x28>
    i /= base;
  } while (i > 0);
  *buffer = 0;
  ReverseStringInPlace(start, buffer);
  return buffer;
}
   d552e:	bd30      	pop	{r4, r5, pc}

000d5530 <DebugLogInt32>:
  return current;
}

}  // namespace

extern "C" void DebugLogInt32(int32_t i) {
   d5530:	b500      	push	{lr}
}

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
   d5532:	2800      	cmp	r0, #0
  return current;
}

}  // namespace

extern "C" void DebugLogInt32(int32_t i) {
   d5534:	b08d      	sub	sp, #52	; 0x34

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
    *buffer++ = '-';
   d5536:	bfbd      	ittte	lt
   d5538:	232d      	movlt	r3, #45	; 0x2d
    u = -u;
   d553a:	4240      	neglt	r0, r0

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
    *buffer++ = '-';
   d553c:	f10d 0101 	addlt.w	r1, sp, #1
}

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
   d5540:	4669      	movge	r1, sp
    *buffer++ = '-';
    u = -u;
  }
  return FastUInt32ToBufferLeft(u, buffer, 10);
   d5542:	f04f 020a 	mov.w	r2, #10

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
    *buffer++ = '-';
   d5546:	bfb8      	it	lt
   d5548:	f88d 3000 	strblt.w	r3, [sp]
    u = -u;
  }
  return FastUInt32ToBufferLeft(u, buffer, 10);
   d554c:	f7ff ffd2 	bl	d54f4 <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci>
}  // namespace

extern "C" void DebugLogInt32(int32_t i) {
  char number_string[kFastToBufferSize];
  FastInt32ToBufferLeft(i, number_string);
  DebugLog(number_string);
   d5550:	4668      	mov	r0, sp
   d5552:	f000 ff61 	bl	d6418 <DebugLog>
}
   d5556:	b00d      	add	sp, #52	; 0x34
   d5558:	f85d fb04 	ldr.w	pc, [sp], #4

000d555c <DebugLogFloat>:
  char number_string[kFastToBufferSize];
  FastUInt32ToBufferLeft(i, number_string, 16);
  DebugLog(number_string);
}

extern "C" void DebugLogFloat(float i) {
   d555c:	b5f0      	push	{r4, r5, r6, r7, lr}
  const uint32_t sign_mask = 0x80000000;
  const uint32_t exponent_mask = 0x7f800000;
  const int32_t exponent_shift = 23;
  const int32_t exponent_bias = 127;
  const uint32_t fraction_mask = 0x007fffff;
  const uint32_t u = *reinterpret_cast<uint32_t*>(&f);
   d555e:	ee10 3a10 	vmov	r3, s0
  char number_string[kFastToBufferSize];
  FastUInt32ToBufferLeft(i, number_string, 16);
  DebugLog(number_string);
}

extern "C" void DebugLogFloat(float i) {
   d5562:	b09d      	sub	sp, #116	; 0x74
  const uint32_t u = *reinterpret_cast<uint32_t*>(&f);
  const int32_t exponent =
      ((u & exponent_mask) >> exponent_shift) - exponent_bias;
  const uint32_t fraction = (u & fraction_mask);
  // Expect ~0x2B1B9D3 for fraction.
  if (u & sign_mask) {
   d5564:	2b00      	cmp	r3, #0
  const int32_t exponent_shift = 23;
  const int32_t exponent_bias = 127;
  const uint32_t fraction_mask = 0x007fffff;
  const uint32_t u = *reinterpret_cast<uint32_t*>(&f);
  const int32_t exponent =
      ((u & exponent_mask) >> exponent_shift) - exponent_bias;
   d5566:	f3c3 54c7 	ubfx	r4, r3, #23, #8
  const uint32_t fraction = (u & fraction_mask);
   d556a:	f3c3 0716 	ubfx	r7, r3, #0, #23
  // Expect ~0x2B1B9D3 for fraction.
  if (u & sign_mask) {
    *current = '-';
   d556e:	bfbb      	ittet	lt
   d5570:	232d      	movlt	r3, #45	; 0x2d
   d5572:	f88d 3010 	strblt.w	r3, [sp, #16]
// Populates the provided buffer with ASCII representation of the float number.
// Avoids the use of any floating point instructions (since these aren't
// supported on many microcontrollers) and as a consequence prints values with
// power-of-two exponents.
char* FastFloatToBufferLeft(float f, char* buffer) {
  char* current = buffer;
   d5576:	ab04      	addge	r3, sp, #16
      ((u & exponent_mask) >> exponent_shift) - exponent_bias;
  const uint32_t fraction = (u & fraction_mask);
  // Expect ~0x2B1B9D3 for fraction.
  if (u & sign_mask) {
    *current = '-';
    current += 1;
   d5578:	f10d 0311 	addlt.w	r3, sp, #17
  const int32_t exponent_shift = 23;
  const int32_t exponent_bias = 127;
  const uint32_t fraction_mask = 0x007fffff;
  const uint32_t u = *reinterpret_cast<uint32_t*>(&f);
  const int32_t exponent =
      ((u & exponent_mask) >> exponent_shift) - exponent_bias;
   d557c:	3c7f      	subs	r4, #127	; 0x7f
  // Expect ~0x2B1B9D3 for fraction.
  if (u & sign_mask) {
    *current = '-';
    current += 1;
  }
  *current = 0;
   d557e:	2200      	movs	r2, #0
  // These are special cases for infinities and not-a-numbers.
  if (exponent == 128) {
   d5580:	2c80      	cmp	r4, #128	; 0x80
  // Expect ~0x2B1B9D3 for fraction.
  if (u & sign_mask) {
    *current = '-';
    current += 1;
  }
  *current = 0;
   d5582:	701a      	strb	r2, [r3, #0]
  // These are special cases for infinities and not-a-numbers.
  if (exponent == 128) {
   d5584:	d108      	bne.n	d5598 <DebugLogFloat+0x3c>
   d5586:	f10d 013f 	add.w	r1, sp, #63	; 0x3f
    if (fraction == 0) {
   d558a:	b90f      	cbnz	r7, d5590 <DebugLogFloat+0x34>
      current = StrCatStr(current, (current_end - current), "Inf");
   d558c:	4a2a      	ldr	r2, [pc, #168]	; (d5638 <DebugLogFloat+0xdc>)
   d558e:	e000      	b.n	d5592 <DebugLogFloat+0x36>
      return current;
    } else {
      current = StrCatStr(current, (current_end - current), "NaN");
   d5590:	4a2a      	ldr	r2, [pc, #168]	; (d563c <DebugLogFloat+0xe0>)
   d5592:	1ac9      	subs	r1, r1, r3
   d5594:	4618      	mov	r0, r3
   d5596:	e047      	b.n	d5628 <DebugLogFloat+0xcc>
  // We can approximate this using multiply-adds and right-shifts using the
  // values in this array. The 1. portion of the number string is printed out
  // in a fixed way before the fraction, below.
  const int32_t scale_shifts_size = 13;
  const int8_t scale_shifts[13] = {3,  4,  8,  11, 13, 14, 17,
                                   18, 19, 20, 21, 22, 23};
   d5598:	4a29      	ldr	r2, [pc, #164]	; (d5640 <DebugLogFloat+0xe4>)
   d559a:	466d      	mov	r5, sp
   d559c:	f102 0c08 	add.w	ip, r2, #8
   d55a0:	46ee      	mov	lr, sp
   d55a2:	6810      	ldr	r0, [r2, #0]
   d55a4:	6851      	ldr	r1, [r2, #4]
   d55a6:	462e      	mov	r6, r5
   d55a8:	c603      	stmia	r6!, {r0, r1}
   d55aa:	3208      	adds	r2, #8
   d55ac:	4562      	cmp	r2, ip
   d55ae:	4635      	mov	r5, r6
   d55b0:	d1f7      	bne.n	d55a2 <DebugLogFloat+0x46>
   d55b2:	6810      	ldr	r0, [r2, #0]
   d55b4:	7912      	ldrb	r2, [r2, #4]
   d55b6:	6030      	str	r0, [r6, #0]
   d55b8:	7132      	strb	r2, [r6, #4]
  uint32_t scaled_fraction = fraction;
   d55ba:	4638      	mov	r0, r7
  for (int i = 0; i < scale_shifts_size; ++i) {
   d55bc:	2200      	movs	r2, #0
    scaled_fraction += (fraction >> scale_shifts[i]);
   d55be:	f91e 1002 	ldrsb.w	r1, [lr, r2]
  // in a fixed way before the fraction, below.
  const int32_t scale_shifts_size = 13;
  const int8_t scale_shifts[13] = {3,  4,  8,  11, 13, 14, 17,
                                   18, 19, 20, 21, 22, 23};
  uint32_t scaled_fraction = fraction;
  for (int i = 0; i < scale_shifts_size; ++i) {
   d55c2:	3201      	adds	r2, #1
    scaled_fraction += (fraction >> scale_shifts[i]);
   d55c4:	fa27 f101 	lsr.w	r1, r7, r1
  // in a fixed way before the fraction, below.
  const int32_t scale_shifts_size = 13;
  const int8_t scale_shifts[13] = {3,  4,  8,  11, 13, 14, 17,
                                   18, 19, 20, 21, 22, 23};
  uint32_t scaled_fraction = fraction;
  for (int i = 0; i < scale_shifts_size; ++i) {
   d55c8:	2a0d      	cmp	r2, #13
    scaled_fraction += (fraction >> scale_shifts[i]);
   d55ca:	4408      	add	r0, r1
  // in a fixed way before the fraction, below.
  const int32_t scale_shifts_size = 13;
  const int8_t scale_shifts[13] = {3,  4,  8,  11, 13, 14, 17,
                                   18, 19, 20, 21, 22, 23};
  uint32_t scaled_fraction = fraction;
  for (int i = 0; i < scale_shifts_size; ++i) {
   d55cc:	d1f7      	bne.n	d55be <DebugLogFloat+0x62>
    scaled_fraction += (fraction >> scale_shifts[i]);
  }
  *current = '1';
   d55ce:	2231      	movs	r2, #49	; 0x31
   d55d0:	701a      	strb	r2, [r3, #0]
  current += 1;
  *current = '.';
   d55d2:	222e      	movs	r2, #46	; 0x2e
  current += 1;
   d55d4:	1c9e      	adds	r6, r3, #2
  for (int i = 0; i < scale_shifts_size; ++i) {
    scaled_fraction += (fraction >> scale_shifts[i]);
  }
  *current = '1';
  current += 1;
  *current = '.';
   d55d6:	705a      	strb	r2, [r3, #1]
  current += 1;
  *current = 0;
  current = StrCatUInt32(current, (current_end - current), scaled_fraction, 10);
   d55d8:	f10d 053f 	add.w	r5, sp, #63	; 0x3f
  }
  *current = '1';
  current += 1;
  *current = '.';
  current += 1;
  *current = 0;
   d55dc:	2200      	movs	r2, #0
   d55de:	709a      	strb	r2, [r3, #2]
  current = StrCatUInt32(current, (current_end - current), scaled_fraction, 10);
   d55e0:	1baf      	subs	r7, r5, r6
}

// Converts a number to a string and appends it to another.
char* StrCatUInt32(char* main, int main_max_length, uint32_t number, int base) {
  char number_string[kFastToBufferSize];
  FastUInt32ToBufferLeft(number, number_string, base);
   d55e2:	220a      	movs	r2, #10
   d55e4:	a910      	add	r1, sp, #64	; 0x40
   d55e6:	f7ff ff85 	bl	d54f4 <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci>
  return StrCatStr(main, main_max_length, number_string);
   d55ea:	aa10      	add	r2, sp, #64	; 0x40
   d55ec:	4639      	mov	r1, r7
   d55ee:	4630      	mov	r0, r6
   d55f0:	f7ff ff6a 	bl	d54c8 <_ZN12_GLOBAL__N_19StrCatStrEPciPKc>
  current += 1;
  *current = '.';
  current += 1;
  *current = 0;
  current = StrCatUInt32(current, (current_end - current), scaled_fraction, 10);
  current = StrCatStr(current, (current_end - current), "*2^");
   d55f4:	4a13      	ldr	r2, [pc, #76]	; (d5644 <DebugLogFloat+0xe8>)
   d55f6:	1a29      	subs	r1, r5, r0
   d55f8:	f7ff ff66 	bl	d54c8 <_ZN12_GLOBAL__N_19StrCatStrEPciPKc>
}

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
   d55fc:	2c00      	cmp	r4, #0
    *buffer++ = '-';
    u = -u;
   d55fe:	bfb8      	it	lt
   d5600:	4264      	neglt	r4, r4
  current += 1;
  *current = '.';
  current += 1;
  *current = 0;
  current = StrCatUInt32(current, (current_end - current), scaled_fraction, 10);
  current = StrCatStr(current, (current_end - current), "*2^");
   d5602:	4606      	mov	r6, r0
  current = StrCatInt32(current, (current_end - current), exponent);
   d5604:	eba5 0500 	sub.w	r5, r5, r0

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
    *buffer++ = '-';
   d5608:	bfba      	itte	lt
   d560a:	232d      	movlt	r3, #45	; 0x2d
   d560c:	f10d 0141 	addlt.w	r1, sp, #65	; 0x41
}

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
   d5610:	a910      	addge	r1, sp, #64	; 0x40
    *buffer++ = '-';
    u = -u;
  }
  return FastUInt32ToBufferLeft(u, buffer, 10);
   d5612:	f04f 020a 	mov.w	r2, #10
   d5616:	4620      	mov	r0, r4

// Populates the provided buffer with an ASCII representation of the number.
char* FastInt32ToBufferLeft(int32_t i, char* buffer) {
  uint32_t u = i;
  if (i < 0) {
    *buffer++ = '-';
   d5618:	bfb8      	it	lt
   d561a:	f88d 3040 	strblt.w	r3, [sp, #64]	; 0x40
    u = -u;
  }
  return FastUInt32ToBufferLeft(u, buffer, 10);
   d561e:	f7ff ff69 	bl	d54f4 <_ZN12_GLOBAL__N_122FastUInt32ToBufferLeftEmPci>

// Converts a number to a string and appends it to another.
char* StrCatInt32(char* main, int main_max_length, int32_t number) {
  char number_string[kFastToBufferSize];
  FastInt32ToBufferLeft(number, number_string);
  return StrCatStr(main, main_max_length, number_string);
   d5622:	aa10      	add	r2, sp, #64	; 0x40
   d5624:	4629      	mov	r1, r5
   d5626:	4630      	mov	r0, r6
   d5628:	f7ff ff4e 	bl	d54c8 <_ZN12_GLOBAL__N_19StrCatStrEPciPKc>
}

extern "C" void DebugLogFloat(float i) {
  char number_string[kFastToBufferSize];
  FastFloatToBufferLeft(i, number_string);
  DebugLog(number_string);
   d562c:	a804      	add	r0, sp, #16
   d562e:	f000 fef3 	bl	d6418 <DebugLog>
}
   d5632:	b01d      	add	sp, #116	; 0x74
   d5634:	bdf0      	pop	{r4, r5, r6, r7, pc}
   d5636:	bf00      	nop
   d5638:	000e8de8 	.word	0x000e8de8
   d563c:	000e8dec 	.word	0x000e8dec
   d5640:	000e8ddb 	.word	0x000e8ddb
   d5644:	000e8df0 	.word	0x000e8df0

000d5648 <_ZN6tflite14AlignPointerUpEPhj>:

uint8_t* AlignPointerUp(uint8_t* data, size_t alignment) {
  size_t data_as_size_t = reinterpret_cast<size_t>(data);
  uint8_t* aligned_result = reinterpret_cast<uint8_t*>(
      ((data_as_size_t + (alignment - 1)) / alignment) * alignment);
  return aligned_result;
   d5648:	1e4b      	subs	r3, r1, #1
   d564a:	4418      	add	r0, r3
   d564c:	fbb0 f0f1 	udiv	r0, r0, r1
}
   d5650:	4348      	muls	r0, r1
   d5652:	4770      	bx	lr

000d5654 <_ZN6tflite16AlignPointerDownEPhj>:

uint8_t* AlignPointerDown(uint8_t* data, size_t alignment) {
  size_t data_as_size_t = reinterpret_cast<size_t>(data);
  uint8_t* aligned_result =
      reinterpret_cast<uint8_t*>((data_as_size_t / alignment) * alignment);
  return aligned_result;
   d5654:	fbb0 f0f1 	udiv	r0, r0, r1
}
   d5658:	4348      	muls	r0, r1
   d565a:	4770      	bx	lr

000d565c <_ZN6tflite11AlignSizeUpEjj>:

size_t AlignSizeUp(size_t size, size_t alignment) {
  size_t aligned_size = (((size + (alignment - 1)) / alignment) * alignment);
  return aligned_size;
   d565c:	3801      	subs	r0, #1
   d565e:	4408      	add	r0, r1
   d5660:	fbb0 f0f1 	udiv	r0, r0, r1
}
   d5664:	4348      	muls	r0, r1
   d5666:	4770      	bx	lr

000d5668 <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE>:

TfLiteStatus TfLiteTypeSizeOf(TfLiteType type, size_t* size,
                              ErrorReporter* reporter) {
   d5668:	b538      	push	{r3, r4, r5, lr}
  switch (type) {
   d566a:	1e43      	subs	r3, r0, #1
  size_t aligned_size = (((size + (alignment - 1)) / alignment) * alignment);
  return aligned_size;
}

TfLiteStatus TfLiteTypeSizeOf(TfLiteType type, size_t* size,
                              ErrorReporter* reporter) {
   d566c:	4604      	mov	r4, r0
   d566e:	4615      	mov	r5, r2
  switch (type) {
   d5670:	2b08      	cmp	r3, #8
   d5672:	d810      	bhi.n	d5696 <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE+0x2e>
   d5674:	e8df f003 	tbb	[pc, r3]
   d5678:	0d0b0909 	.word	0x0d0b0909
   d567c:	0d050b0f 	.word	0x0d050b0f
   d5680:	0b          	.byte	0x0b
   d5681:	00          	.byte	0x00
    case kTfLiteFloat32:
      *size = sizeof(float);
      break;
    case kTfLiteInt16:
      *size = sizeof(int16_t);
   d5682:	2302      	movs	r3, #2
   d5684:	600b      	str	r3, [r1, #0]
    default:
      reporter->Report("Type %s (%d) not is not supported",
                       TfLiteTypeGetName(type), type);
      return kTfLiteError;
  }
  return kTfLiteOk;
   d5686:	2000      	movs	r0, #0
    case kTfLiteFloat32:
      *size = sizeof(float);
      break;
    case kTfLiteInt16:
      *size = sizeof(int16_t);
      break;
   d5688:	bd38      	pop	{r3, r4, r5, pc}
    case kTfLiteInt32:
      *size = sizeof(int32_t);
   d568a:	2304      	movs	r3, #4
   d568c:	e7fa      	b.n	d5684 <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE+0x1c>
      break;
    case kTfLiteInt64:
      *size = sizeof(int64_t);
      break;
    case kTfLiteBool:
      *size = sizeof(bool);
   d568e:	2301      	movs	r3, #1
   d5690:	e7f8      	b.n	d5684 <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE+0x1c>
      break;
    case kTfLiteComplex64:
      *size = sizeof(float) * 2;
   d5692:	2308      	movs	r3, #8
   d5694:	e7f6      	b.n	d5684 <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE+0x1c>
      break;
    default:
      reporter->Report("Type %s (%d) not is not supported",
   d5696:	f7fe fd41 	bl	d411c <TfLiteTypeGetName>
                       TfLiteTypeGetName(type), type);
   d569a:	4623      	mov	r3, r4
   d569c:	4602      	mov	r2, r0
   d569e:	4903      	ldr	r1, [pc, #12]	; (d56ac <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE+0x44>)
   d56a0:	4628      	mov	r0, r5
   d56a2:	f7fe fe97 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
   d56a6:	2001      	movs	r0, #1
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   d56a8:	bd38      	pop	{r3, r4, r5, pc}
   d56aa:	bf00      	nop
   d56ac:	000e8df4 	.word	0x000e8df4

000d56b0 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE>:

TfLiteStatus BytesRequiredForTensor(const tflite::Tensor& flatbuffer_tensor,
                                    size_t* bytes, size_t* type_size,
                                    ErrorReporter* error_reporter) {
   d56b0:	e92d 41f3 	stmdb	sp!, {r0, r1, r4, r5, r6, r7, r8, lr}
   d56b4:	460d      	mov	r5, r1
// "tables" use an offset table (possibly shared) that allows fields to be
// omitted and added at will, but uses an extra indirection to read.
class Table {
 public:
  const uint8_t *GetVTable() const {
    return data_ - ReadScalar<soffset_t>(data_);
   d56b6:	6801      	ldr	r1, [r0, #0]
   d56b8:	1a41      	subs	r1, r0, r1
   d56ba:	461f      	mov	r7, r3
   d56bc:	f8b1 e000 	ldrh.w	lr, [r1]
   d56c0:	4616      	mov	r6, r2
  int element_count = 1;
  for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
   d56c2:	2300      	movs	r3, #0
}

TfLiteStatus BytesRequiredForTensor(const tflite::Tensor& flatbuffer_tensor,
                                    size_t* bytes, size_t* type_size,
                                    ErrorReporter* error_reporter) {
  int element_count = 1;
   d56c4:	2401      	movs	r4, #1
    auto vtable = GetVTable();
    // The first element is the size of the vtable (fields + type id + itself).
    auto vtsize = ReadScalar<voffset_t>(vtable);
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
   d56c6:	f1be 0f04 	cmp.w	lr, #4
   d56ca:	bf8c      	ite	hi
   d56cc:	888a      	ldrhhi	r2, [r1, #4]
   d56ce:	2200      	movls	r2, #0
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
  }

  template<typename P> P GetPointer(voffset_t field) {
    auto field_offset = GetOptionalFieldOffset(field);
    auto p = data_ + field_offset;
   d56d0:	eb00 0802 	add.w	r8, r0, r2
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
   d56d4:	b362      	cbz	r2, d5730 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x80>
   d56d6:	f850 c002 	ldr.w	ip, [r0, r2]
   d56da:	eb08 020c 	add.w	r2, r8, ip
  for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
   d56de:	f858 c00c 	ldr.w	ip, [r8, ip]
   d56e2:	4563      	cmp	r3, ip
   d56e4:	d205      	bcs.n	d56f2 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x42>
template<typename T> struct IndirectHelper {
  typedef T return_type;
  typedef T mutable_return_type;
  static const size_t element_stride = sizeof(T);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    return EndianScalar((reinterpret_cast<const T *>(p))[i]);
   d56e6:	eb02 0283 	add.w	r2, r2, r3, lsl #2
   d56ea:	3301      	adds	r3, #1
    element_count *= flatbuffer_tensor.shape()->Get(n);
   d56ec:	6852      	ldr	r2, [r2, #4]
   d56ee:	4354      	muls	r4, r2

TfLiteStatus BytesRequiredForTensor(const tflite::Tensor& flatbuffer_tensor,
                                    size_t* bytes, size_t* type_size,
                                    ErrorReporter* error_reporter) {
  int element_count = 1;
  for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
   d56f0:	e7e9      	b.n	d56c6 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x16>
    auto vtable = GetVTable();
    // The first element is the size of the vtable (fields + type id + itself).
    auto vtsize = ReadScalar<voffset_t>(vtable);
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
   d56f2:	f1be 0f06 	cmp.w	lr, #6
   d56f6:	d903      	bls.n	d5700 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x50>
   d56f8:	88ca      	ldrh	r2, [r1, #6]
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d56fa:	b11a      	cbz	r2, d5704 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x54>
   d56fc:	5680      	ldrsb	r0, [r0, r2]
   d56fe:	e002      	b.n	d5706 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x56>
   d5700:	2000      	movs	r0, #0
   d5702:	e000      	b.n	d5706 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x56>
   d5704:	4610      	mov	r0, r2
    element_count *= flatbuffer_tensor.shape()->Get(n);
  }

  TfLiteType tf_lite_type;
  TF_LITE_ENSURE_STATUS(ConvertTensorType(flatbuffer_tensor.type(),
   d5706:	463a      	mov	r2, r7
   d5708:	f10d 0107 	add.w	r1, sp, #7
   d570c:	b2c0      	uxtb	r0, r0
   d570e:	f7fe febb 	bl	d4488 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
   d5712:	b108      	cbz	r0, d5718 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x68>
   d5714:	2001      	movs	r0, #1
   d5716:	e00d      	b.n	d5734 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x84>
                                          &tf_lite_type, error_reporter));
  TF_LITE_ENSURE_STATUS(
   d5718:	463a      	mov	r2, r7
   d571a:	4631      	mov	r1, r6
   d571c:	f89d 0007 	ldrb.w	r0, [sp, #7]
   d5720:	f7ff ffa2 	bl	d5668 <_ZN6tflite16TfLiteTypeSizeOfE10TfLiteTypePjPNS_13ErrorReporterE>
   d5724:	2800      	cmp	r0, #0
   d5726:	d1f5      	bne.n	d5714 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x64>
      TfLiteTypeSizeOf(tf_lite_type, type_size, error_reporter));
  *bytes = element_count * (*type_size);
   d5728:	6833      	ldr	r3, [r6, #0]
   d572a:	435c      	muls	r4, r3
   d572c:	602c      	str	r4, [r5, #0]
  return kTfLiteOk;
   d572e:	e001      	b.n	d5734 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x84>
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d5730:	6813      	ldr	r3, [r2, #0]
   d5732:	deff      	udf	#255	; 0xff
}
   d5734:	b002      	add	sp, #8
   d5736:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}

000d573a <_ZNK6tflite6Tensor11is_variableEv>:
    return GetPointer<const flatbuffers::String *>(VT_NAME);
  }
  const QuantizationParameters *quantization() const {
    return GetPointer<const QuantizationParameters *>(VT_QUANTIZATION);
  }
  bool is_variable() const {
   d573a:	b510      	push	{r4, lr}
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
   d573c:	210e      	movs	r1, #14
   d573e:	4604      	mov	r4, r0
   d5740:	f7fe fe9a 	bl	d4478 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d5744:	b100      	cbz	r0, d5748 <_ZNK6tflite6Tensor11is_variableEv+0xe>
   d5746:	5c20      	ldrb	r0, [r4, r0]
    return GetField<uint8_t>(VT_IS_VARIABLE, 0) != 0;
  }
   d5748:	3000      	adds	r0, #0
   d574a:	bf18      	it	ne
   d574c:	2001      	movne	r0, #1
   d574e:	bd10      	pop	{r4, pc}

000d5750 <_ZNK11flatbuffers6VectorIfE3GetEm>:
  uoffset_t Length() const { return size(); }

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
   d5750:	b508      	push	{r3, lr}
    FLATBUFFERS_ASSERT(i < size());
   d5752:	6803      	ldr	r3, [r0, #0]
   d5754:	4299      	cmp	r1, r3
   d5756:	d305      	bcc.n	d5764 <_ZNK11flatbuffers6VectorIfE3GetEm+0x14>
   d5758:	4b05      	ldr	r3, [pc, #20]	; (d5770 <_ZNK11flatbuffers6VectorIfE3GetEm+0x20>)
   d575a:	4a06      	ldr	r2, [pc, #24]	; (d5774 <_ZNK11flatbuffers6VectorIfE3GetEm+0x24>)
   d575c:	4806      	ldr	r0, [pc, #24]	; (d5778 <_ZNK11flatbuffers6VectorIfE3GetEm+0x28>)
   d575e:	21ed      	movs	r1, #237	; 0xed
   d5760:	f00e fe04 	bl	e436c <__assert_func>
template<typename T> struct IndirectHelper {
  typedef T return_type;
  typedef T mutable_return_type;
  static const size_t element_stride = sizeof(T);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    return EndianScalar((reinterpret_cast<const T *>(p))[i]);
   d5764:	eb00 0081 	add.w	r0, r0, r1, lsl #2
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
    return IndirectHelper<T>::Read(Data(), i);
  }
   d5768:	ed90 0a01 	vldr	s0, [r0, #4]
   d576c:	bd08      	pop	{r3, pc}
   d576e:	bf00      	nop
   d5770:	000e8511 	.word	0x000e8511
   d5774:	000e91e0 	.word	0x000e91e0
   d5778:	000e851c 	.word	0x000e851c

000d577c <_ZNK11flatbuffers6VectorIlE3GetEm>:
  uoffset_t Length() const { return size(); }

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
   d577c:	b508      	push	{r3, lr}
    FLATBUFFERS_ASSERT(i < size());
   d577e:	6803      	ldr	r3, [r0, #0]
   d5780:	4299      	cmp	r1, r3
   d5782:	d305      	bcc.n	d5790 <_ZNK11flatbuffers6VectorIlE3GetEm+0x14>
   d5784:	4b04      	ldr	r3, [pc, #16]	; (d5798 <_ZNK11flatbuffers6VectorIlE3GetEm+0x1c>)
   d5786:	4a05      	ldr	r2, [pc, #20]	; (d579c <_ZNK11flatbuffers6VectorIlE3GetEm+0x20>)
   d5788:	4805      	ldr	r0, [pc, #20]	; (d57a0 <_ZNK11flatbuffers6VectorIlE3GetEm+0x24>)
   d578a:	21ed      	movs	r1, #237	; 0xed
   d578c:	f00e fdee 	bl	e436c <__assert_func>
template<typename T> struct IndirectHelper {
  typedef T return_type;
  typedef T mutable_return_type;
  static const size_t element_stride = sizeof(T);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    return EndianScalar((reinterpret_cast<const T *>(p))[i]);
   d5790:	eb00 0081 	add.w	r0, r0, r1, lsl #2
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
    return IndirectHelper<T>::Read(Data(), i);
  }
   d5794:	6840      	ldr	r0, [r0, #4]
   d5796:	bd08      	pop	{r3, pc}
   d5798:	000e8511 	.word	0x000e8511
   d579c:	000e8e16 	.word	0x000e8e16
   d57a0:	000e851c 	.word	0x000e851c

000d57a4 <_ZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEm>:
  uoffset_t Length() const { return size(); }

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
   d57a4:	b508      	push	{r3, lr}
    FLATBUFFERS_ASSERT(i < size());
   d57a6:	6803      	ldr	r3, [r0, #0]
   d57a8:	4299      	cmp	r1, r3
   d57aa:	d305      	bcc.n	d57b8 <_ZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEm+0x14>
   d57ac:	4b06      	ldr	r3, [pc, #24]	; (d57c8 <_ZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEm+0x24>)
   d57ae:	4a07      	ldr	r2, [pc, #28]	; (d57cc <_ZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEm+0x28>)
   d57b0:	4807      	ldr	r0, [pc, #28]	; (d57d0 <_ZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEm+0x2c>)
   d57b2:	21ed      	movs	r1, #237	; 0xed
   d57b4:	f00e fdda 	bl	e436c <__assert_func>
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d57b8:	3004      	adds	r0, #4
template<typename T> struct IndirectHelper<Offset<T>> {
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
   d57ba:	eb00 0281 	add.w	r2, r0, r1, lsl #2
  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
    return IndirectHelper<T>::Read(Data(), i);
   d57be:	f850 0021 	ldr.w	r0, [r0, r1, lsl #2]
  }
   d57c2:	4410      	add	r0, r2
   d57c4:	bd08      	pop	{r3, pc}
   d57c6:	bf00      	nop
   d57c8:	000e8511 	.word	0x000e8511
   d57cc:	000e8fde 	.word	0x000e8fde
   d57d0:	000e851c 	.word	0x000e851c

000d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>:
  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
  }

  template<typename P> P GetPointer(voffset_t field) {
   d57d4:	b510      	push	{r4, lr}
   d57d6:	4604      	mov	r4, r0
    auto field_offset = GetOptionalFieldOffset(field);
   d57d8:	f7fe fe4e 	bl	d4478 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    auto p = data_ + field_offset;
   d57dc:	1822      	adds	r2, r4, r0
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
   d57de:	b108      	cbz	r0, d57e4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t+0x10>
   d57e0:	5823      	ldr	r3, [r4, r0]
   d57e2:	18d0      	adds	r0, r2, r3
  }
   d57e4:	bd10      	pop	{r4, pc}
	...

000d57e8 <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE>:
// requirement for SIMD extensions.
constexpr int kBufferAlignment = 16;

}  // namespace

MicroAllocator::MicroAllocator(TfLiteContext* context, const Model* model,
   d57e8:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
   d57ec:	9e07      	ldr	r6, [sp, #28]
// though we have enough information about lifetimes of the tensors to do so.
// This makes it pretty wasteful, so we should use a more intelligent method.
class SimpleMemoryAllocator {
 public:
  SimpleMemoryAllocator(uint8_t* buffer, size_t buffer_size)
      : data_size_(0), data_size_max_(buffer_size), data_(buffer) {}
   d57ee:	60c3      	str	r3, [r0, #12]
   d57f0:	460f      	mov	r7, r1
   d57f2:	2500      	movs	r5, #0
   d57f4:	9906      	ldr	r1, [sp, #24]
   d57f6:	6081      	str	r1, [r0, #8]
    : model_(model),
      memory_allocator_(tensor_arena, arena_size),
      error_reporter_(error_reporter),
      context_(context),
      arena_(tensor_arena),
      arena_size_(arena_size) {
   d57f8:	6183      	str	r3, [r0, #24]
   d57fa:	61c1      	str	r1, [r0, #28]
   d57fc:	6002      	str	r2, [r0, #0]
   d57fe:	6045      	str	r5, [r0, #4]
   d5800:	6106      	str	r6, [r0, #16]
   d5802:	6147      	str	r7, [r0, #20]
// requirement for SIMD extensions.
constexpr int kBufferAlignment = 16;

}  // namespace

MicroAllocator::MicroAllocator(TfLiteContext* context, const Model* model,
   d5804:	4604      	mov	r4, r0
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5806:	2108      	movs	r1, #8
   d5808:	4610      	mov	r0, r2
   d580a:	f7ff ffe3 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
      error_reporter_(error_reporter),
      context_(context),
      arena_(tensor_arena),
      arena_size_(arena_size) {
  auto* subgraphs = model->subgraphs();
  if (subgraphs->size() != 1) {
   d580e:	6803      	ldr	r3, [r0, #0]
   d5810:	2b01      	cmp	r3, #1
   d5812:	d004      	beq.n	d581e <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE+0x36>
    error_reporter->Report("Only 1 subgraph is currently supported.\n");
   d5814:	4917      	ldr	r1, [pc, #92]	; (d5874 <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE+0x8c>)
   d5816:	4630      	mov	r0, r6
   d5818:	f7fe fddc 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return;
   d581c:	e026      	b.n	d586c <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE+0x84>
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
   d581e:	6843      	ldr	r3, [r0, #4]
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d5820:	1d06      	adds	r6, r0, #4
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
   d5822:	441e      	add	r6, r3
  }
  subgraph_ = (*subgraphs)[0];
   d5824:	6226      	str	r6, [r4, #32]
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5826:	2104      	movs	r1, #4
   d5828:	4630      	mov	r0, r6
   d582a:	f7ff ffd3 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
   d582e:	210a      	movs	r1, #10
   d5830:	4680      	mov	r8, r0
  tensors_ = subgraph_->tensors();
   d5832:	62a0      	str	r0, [r4, #40]	; 0x28
   d5834:	4630      	mov	r0, r6
   d5836:	f7ff ffcd 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
  operators_ = subgraph_->operators();
   d583a:	6260      	str	r0, [r4, #36]	; 0x24
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d583c:	f8d8 3000 	ldr.w	r3, [r8]

  context_->tensors_size = tensors_->size();
   d5840:	603b      	str	r3, [r7, #0]
  context_->tensors =
      reinterpret_cast<TfLiteTensor*>(memory_allocator_.AllocateFromTail(
          sizeof(TfLiteTensor) * context_->tensors_size, 4));
   d5842:	6967      	ldr	r7, [r4, #20]
  tensors_ = subgraph_->tensors();
  operators_ = subgraph_->operators();

  context_->tensors_size = tensors_->size();
  context_->tensors =
      reinterpret_cast<TfLiteTensor*>(memory_allocator_.AllocateFromTail(
   d5844:	6839      	ldr	r1, [r7, #0]
   d5846:	2204      	movs	r2, #4
   d5848:	2638      	movs	r6, #56	; 0x38
   d584a:	4371      	muls	r1, r6
   d584c:	18a0      	adds	r0, r4, r2
   d584e:	f000 fdce 	bl	d63ee <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>
          sizeof(TfLiteTensor) * context_->tensors_size, 4));

  // Null all inputs so we can later perform a null check to avoid re-allocating
  // registered pre-allocated inputs.
  for (size_t i = 0; i < context_->tensors_size; ++i) {
   d5852:	462b      	mov	r3, r5
  operators_ = subgraph_->operators();

  context_->tensors_size = tensors_->size();
  context_->tensors =
      reinterpret_cast<TfLiteTensor*>(memory_allocator_.AllocateFromTail(
          sizeof(TfLiteTensor) * context_->tensors_size, 4));
   d5854:	60b8      	str	r0, [r7, #8]

  // Null all inputs so we can later perform a null check to avoid re-allocating
  // registered pre-allocated inputs.
  for (size_t i = 0; i < context_->tensors_size; ++i) {
    context_->tensors[i].data.raw = nullptr;
   d5856:	4629      	mov	r1, r5
      reinterpret_cast<TfLiteTensor*>(memory_allocator_.AllocateFromTail(
          sizeof(TfLiteTensor) * context_->tensors_size, 4));

  // Null all inputs so we can later perform a null check to avoid re-allocating
  // registered pre-allocated inputs.
  for (size_t i = 0; i < context_->tensors_size; ++i) {
   d5858:	6962      	ldr	r2, [r4, #20]
   d585a:	6810      	ldr	r0, [r2, #0]
   d585c:	4283      	cmp	r3, r0
   d585e:	d205      	bcs.n	d586c <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE+0x84>
    context_->tensors[i].data.raw = nullptr;
   d5860:	6892      	ldr	r2, [r2, #8]
   d5862:	fb06 2203 	mla	r2, r6, r3, r2
      reinterpret_cast<TfLiteTensor*>(memory_allocator_.AllocateFromTail(
          sizeof(TfLiteTensor) * context_->tensors_size, 4));

  // Null all inputs so we can later perform a null check to avoid re-allocating
  // registered pre-allocated inputs.
  for (size_t i = 0; i < context_->tensors_size; ++i) {
   d5866:	3301      	adds	r3, #1
    context_->tensors[i].data.raw = nullptr;
   d5868:	6051      	str	r1, [r2, #4]
      reinterpret_cast<TfLiteTensor*>(memory_allocator_.AllocateFromTail(
          sizeof(TfLiteTensor) * context_->tensors_size, 4));

  // Null all inputs so we can later perform a null check to avoid re-allocating
  // registered pre-allocated inputs.
  for (size_t i = 0; i < context_->tensors_size; ++i) {
   d586a:	e7f5      	b.n	d5858 <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE+0x70>
    context_->tensors[i].data.raw = nullptr;
  }
}
   d586c:	4620      	mov	r0, r4
   d586e:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   d5872:	bf00      	nop
   d5874:	000e90d4 	.word	0x000e90d4

000d5878 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh>:

TfLiteStatus MicroAllocator::InitializeRuntimeTensor(
    const tflite::Tensor& flatbuffer_tensor,
    const flatbuffers::Vector<flatbuffers::Offset<Buffer>>* buffers,
    ErrorReporter* error_reporter, TfLiteTensor* result,
    uint8_t* preallocated_buffer) {
   d5878:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d587c:	460d      	mov	r5, r1
   d587e:	b087      	sub	sp, #28
   d5880:	4607      	mov	r7, r0
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
   d5882:	2106      	movs	r1, #6
   d5884:	4628      	mov	r0, r5
   d5886:	4616      	mov	r6, r2
   d5888:	4698      	mov	r8, r3
   d588a:	9c10      	ldr	r4, [sp, #64]	; 0x40
   d588c:	f8dd 9044 	ldr.w	r9, [sp, #68]	; 0x44
   d5890:	f7fe fdf2 	bl	d4478 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d5894:	b100      	cbz	r0, d5898 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x20>
   d5896:	5628      	ldrsb	r0, [r5, r0]
  // Make sure the serialized type is one we know how to deal with, and convert
  // it from a flatbuffer enum into a constant used by the kernel C API.
  TF_LITE_ENSURE_STATUS(ConvertTensorType(flatbuffer_tensor.type(),
   d5898:	4642      	mov	r2, r8
   d589a:	4621      	mov	r1, r4
   d589c:	b2c0      	uxtb	r0, r0
   d589e:	f7fe fdf3 	bl	d4488 <_ZN6tflite17ConvertTensorTypeENS_10TensorTypeEP10TfLiteTypePNS_13ErrorReporterE>
   d58a2:	4682      	mov	sl, r0
   d58a4:	2800      	cmp	r0, #0
   d58a6:	f040 80e2 	bne.w	d5a6e <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1f6>
                                          &result->type, error_reporter));
  // Make sure we remember if the serialized tensor is designated as a variable.
  result->is_variable = flatbuffer_tensor.is_variable();
   d58aa:	4628      	mov	r0, r5
   d58ac:	f7ff ff45 	bl	d573a <_ZNK6tflite6Tensor11is_variableEv>
  // We need to figure out where the actual contents of this tensor are stored
  // in memory. We'll check to see if there's a serialized buffer (pretty much
  // the same as a constant op in TensorFlow) associated with this tensor first,
  // and if there is update the runtime structure to point to its location in
  // memory.
  result->data.raw = nullptr;
   d58b0:	f8c4 a004 	str.w	sl, [r4, #4]
  // Make sure the serialized type is one we know how to deal with, and convert
  // it from a flatbuffer enum into a constant used by the kernel C API.
  TF_LITE_ENSURE_STATUS(ConvertTensorType(flatbuffer_tensor.type(),
                                          &result->type, error_reporter));
  // Make sure we remember if the serialized tensor is designated as a variable.
  result->is_variable = flatbuffer_tensor.is_variable();
   d58b4:	f884 002d 	strb.w	r0, [r4, #45]	; 0x2d
  // in memory. We'll check to see if there's a serialized buffer (pretty much
  // the same as a constant op in TensorFlow) associated with this tensor first,
  // and if there is update the runtime structure to point to its location in
  // memory.
  result->data.raw = nullptr;
  result->bytes = 0;
   d58b8:	f8c4 a018 	str.w	sl, [r4, #24]
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
   d58bc:	2108      	movs	r1, #8
   d58be:	4628      	mov	r0, r5
   d58c0:	f7fe fdda 	bl	d4478 <_ZNK11flatbuffers5Table22GetOptionalFieldOffsetEt>
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d58c4:	b100      	cbz	r0, d58c8 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x50>
   d58c6:	5828      	ldr	r0, [r5, r0]

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
   d58c8:	6833      	ldr	r3, [r6, #0]
   d58ca:	4283      	cmp	r3, r0
   d58cc:	d802      	bhi.n	d58d4 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x5c>
   d58ce:	4b74      	ldr	r3, [pc, #464]	; (d5aa0 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x228>)
   d58d0:	4a74      	ldr	r2, [pc, #464]	; (d5aa4 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x22c>)
   d58d2:	e0ac      	b.n	d5a2e <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1b6>
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d58d4:	3604      	adds	r6, #4
template<typename T> struct IndirectHelper<Offset<T>> {
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
   d58d6:	eb06 0380 	add.w	r3, r6, r0, lsl #2
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
   d58da:	f856 0020 	ldr.w	r0, [r6, r0, lsl #2]
  // First see if there's any buffer information in the serialized tensor.
  if (auto* buffer = (*buffers)[flatbuffer_tensor.buffer()]) {
   d58de:	1818      	adds	r0, r3, r0
   d58e0:	d009      	beq.n	d58f6 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x7e>
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d58e2:	2104      	movs	r1, #4
   d58e4:	f7ff ff76 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
    // If we've found a buffer, does it have any data?
    if (auto* array = buffer->data()) {
   d58e8:	b128      	cbz	r0, d58f6 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x7e>
      // If it has any data, is the data size larger than zero?
      if (size_t array_size = array->size()) {
   d58ea:	6803      	ldr	r3, [r0, #0]
   d58ec:	b11b      	cbz	r3, d58f6 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x7e>
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d58ee:	3004      	adds	r0, #4
        // We've found a buffer with valid data, so update the runtime tensor
        // data structure to point to it.
        result->data.raw =
            const_cast<char*>(reinterpret_cast<const char*>(array->data()));
        // We set the data from a serialized buffer, so record tha.
        result->allocation_type = kTfLiteMmapRo;
   d58f0:	2301      	movs	r3, #1
      // If it has any data, is the data size larger than zero?
      if (size_t array_size = array->size()) {
        // We've found a buffer with valid data, so update the runtime tensor
        // data structure to point to it.
        result->data.raw =
            const_cast<char*>(reinterpret_cast<const char*>(array->data()));
   d58f2:	6060      	str	r0, [r4, #4]
        // We set the data from a serialized buffer, so record tha.
        result->allocation_type = kTfLiteMmapRo;
   d58f4:	7523      	strb	r3, [r4, #20]
    // it less ambiguous.
  }

  // TODO(petewarden): Some of these paths aren't getting enough testing
  // coverage, so we should figure out some tests that exercise them.
  if (!result->data.raw) {
   d58f6:	6863      	ldr	r3, [r4, #4]
   d58f8:	b933      	cbnz	r3, d5908 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x90>
    // The tensor contents haven't been set from a serialized buffer, so
    // make a note that they will be allocated from memory. The actual
    // allocation won't happen until later.
    result->allocation_type = kTfLiteArenaRw;
   d58fa:	2302      	movs	r3, #2
   d58fc:	7523      	strb	r3, [r4, #20]
    if (preallocated_buffer != nullptr) {
   d58fe:	f1b9 0f00 	cmp.w	r9, #0
   d5902:	d001      	beq.n	d5908 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x90>
      // If the client is supplying memory for the contents of the tensor
      // themselves, use it.
      // TODO(petewarden): Should we store the fact this is a client-allocated
      // buffer?
      result->data.raw = reinterpret_cast<char*>(preallocated_buffer);
   d5904:	f8c4 9004 	str.w	r9, [r4, #4]
    }
  }

  // Figure out what the size in bytes of the buffer is and store it.
  size_t type_size;
  TF_LITE_ENSURE_STATUS(BytesRequiredForTensor(
   d5908:	4643      	mov	r3, r8
   d590a:	aa05      	add	r2, sp, #20
   d590c:	f104 0118 	add.w	r1, r4, #24
   d5910:	4628      	mov	r0, r5
   d5912:	f7ff fecd 	bl	d56b0 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE>
   d5916:	4606      	mov	r6, r0
   d5918:	2800      	cmp	r0, #0
   d591a:	f040 80a8 	bne.w	d5a6e <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1f6>
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d591e:	2104      	movs	r1, #4
   d5920:	4628      	mov	r0, r5
   d5922:	f7ff ff57 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
      flatbuffer_tensor, &result->bytes, &type_size, error_reporter));
  // Copy the shape of the tensor from the serialized data into the runtime
  // form. We have to allocate memory for this.
  result->dims =
      reinterpret_cast<TfLiteIntArray*>(memory_allocator_.AllocateFromTail(
   d5926:	6801      	ldr	r1, [r0, #0]
   d5928:	f107 0b04 	add.w	fp, r7, #4
   d592c:	3101      	adds	r1, #1
   d592e:	2204      	movs	r2, #4
   d5930:	0089      	lsls	r1, r1, #2
   d5932:	4658      	mov	r0, fp
   d5934:	f000 fd5b 	bl	d63ee <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>
   d5938:	2104      	movs	r1, #4
   d593a:	4607      	mov	r7, r0
          sizeof(int) * (flatbuffer_tensor.shape()->Length() + 1),
          sizeof(int)));
   d593c:	60a0      	str	r0, [r4, #8]
   d593e:	4628      	mov	r0, r5
   d5940:	f7ff ff48 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
  result->dims->size = flatbuffer_tensor.shape()->Length();
   d5944:	6803      	ldr	r3, [r0, #0]
   d5946:	603b      	str	r3, [r7, #0]
  for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
   d5948:	4637      	mov	r7, r6
   d594a:	2104      	movs	r1, #4
   d594c:	4628      	mov	r0, r5
   d594e:	f7ff ff41 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
   d5952:	6803      	ldr	r3, [r0, #0]
   d5954:	42bb      	cmp	r3, r7
   d5956:	d90a      	bls.n	d596e <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0xf6>
    result->dims->data[n] = flatbuffer_tensor.shape()->Get(n);
   d5958:	f8d4 8008 	ldr.w	r8, [r4, #8]
   d595c:	4639      	mov	r1, r7
   d595e:	f7ff ff0d 	bl	d577c <_ZNK11flatbuffers6VectorIlE3GetEm>
   d5962:	eb08 0887 	add.w	r8, r8, r7, lsl #2
  result->dims =
      reinterpret_cast<TfLiteIntArray*>(memory_allocator_.AllocateFromTail(
          sizeof(int) * (flatbuffer_tensor.shape()->Length() + 1),
          sizeof(int)));
  result->dims->size = flatbuffer_tensor.shape()->Length();
  for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
   d5966:	3701      	adds	r7, #1
    result->dims->data[n] = flatbuffer_tensor.shape()->Get(n);
   d5968:	f8c8 0004 	str.w	r0, [r8, #4]
   d596c:	e7ed      	b.n	d594a <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0xd2>
   d596e:	210c      	movs	r1, #12
   d5970:	4628      	mov	r0, r5
   d5972:	f7ff ff2f 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
  }
  // Copy the quantization information from the serialized data.
  const auto* src_quantization = flatbuffer_tensor.quantization();
  if (src_quantization && src_quantization->scale() &&
      (src_quantization->scale()->size() > 0) &&
      src_quantization->zero_point() &&
   d5976:	4607      	mov	r7, r0
   d5978:	2800      	cmp	r0, #0
   d597a:	d066      	beq.n	d5a4a <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1d2>
   d597c:	2108      	movs	r1, #8
   d597e:	f7ff ff29 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
  for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
    result->dims->data[n] = flatbuffer_tensor.shape()->Get(n);
  }
  // Copy the quantization information from the serialized data.
  const auto* src_quantization = flatbuffer_tensor.quantization();
  if (src_quantization && src_quantization->scale() &&
   d5982:	4680      	mov	r8, r0
   d5984:	2800      	cmp	r0, #0
   d5986:	d060      	beq.n	d5a4a <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1d2>
   d5988:	6803      	ldr	r3, [r0, #0]
   d598a:	2b00      	cmp	r3, #0
   d598c:	d05d      	beq.n	d5a4a <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1d2>
   d598e:	210a      	movs	r1, #10
   d5990:	4638      	mov	r0, r7
   d5992:	f7ff ff1f 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
      (src_quantization->scale()->size() > 0) &&
   d5996:	2800      	cmp	r0, #0
   d5998:	d057      	beq.n	d5a4a <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1d2>
      src_quantization->zero_point() &&
   d599a:	6803      	ldr	r3, [r0, #0]
   d599c:	2b00      	cmp	r3, #0
   d599e:	d054      	beq.n	d5a4a <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1d2>
      (src_quantization->zero_point()->size() > 0)) {
    result->params.scale = src_quantization->scale()->Get(0);
   d59a0:	4640      	mov	r0, r8
   d59a2:	2100      	movs	r1, #0
   d59a4:	f7ff fed4 	bl	d5750 <_ZNK11flatbuffers6VectorIfE3GetEm>
   d59a8:	f104 090f 	add.w	r9, r4, #15
   d59ac:	ed84 0a03 	vstr	s0, [r4, #12]
    // This magic handles issues with little-endianness.
    for (unsigned int b = 0; b < sizeof(int64_t); ++b)
   d59b0:	f04f 0800 	mov.w	r8, #0
   d59b4:	210a      	movs	r1, #10
   d59b6:	4638      	mov	r0, r7
   d59b8:	f7ff ff0c 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
      *(reinterpret_cast<char*>(&result->params.zero_point) + b) =
          *(reinterpret_cast<const char*>(
                src_quantization->zero_point()->Data()) +
            b);
   d59bc:	4440      	add	r0, r8
      (src_quantization->scale()->size() > 0) &&
      src_quantization->zero_point() &&
      (src_quantization->zero_point()->size() > 0)) {
    result->params.scale = src_quantization->scale()->Get(0);
    // This magic handles issues with little-endianness.
    for (unsigned int b = 0; b < sizeof(int64_t); ++b)
   d59be:	f108 0801 	add.w	r8, r8, #1
      *(reinterpret_cast<char*>(&result->params.zero_point) + b) =
          *(reinterpret_cast<const char*>(
                src_quantization->zero_point()->Data()) +
            b);
   d59c2:	7903      	ldrb	r3, [r0, #4]
   d59c4:	f809 3f01 	strb.w	r3, [r9, #1]!
      (src_quantization->scale()->size() > 0) &&
      src_quantization->zero_point() &&
      (src_quantization->zero_point()->size() > 0)) {
    result->params.scale = src_quantization->scale()->Get(0);
    // This magic handles issues with little-endianness.
    for (unsigned int b = 0; b < sizeof(int64_t); ++b)
   d59c8:	f1b8 0f08 	cmp.w	r8, #8
   d59cc:	d1f2      	bne.n	d59b4 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x13c>
   d59ce:	4641      	mov	r1, r8
   d59d0:	4638      	mov	r0, r7
   d59d2:	f7ff feff 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
    // Populate per-channel quantization params.
    int channels = src_quantization->scale()->size();
    TfLiteAffineQuantization* quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            memory_allocator_.AllocateFromTail(sizeof(TfLiteAffineQuantization),
                                               sizeof(int)));
   d59d6:	2204      	movs	r2, #4
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d59d8:	f8d0 8000 	ldr.w	r8, [r0]
   d59dc:	210c      	movs	r1, #12
   d59de:	4658      	mov	r0, fp
   d59e0:	f000 fd05 	bl	d63ee <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>
    int* zero_point_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
   d59e4:	ea4f 0388 	mov.w	r3, r8, lsl #2
   d59e8:	f103 0904 	add.w	r9, r3, #4
            channels * sizeof(int) + sizeof(int), sizeof(int)));
   d59ec:	4649      	mov	r1, r9
    // Populate per-channel quantization params.
    int channels = src_quantization->scale()->size();
    TfLiteAffineQuantization* quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            memory_allocator_.AllocateFromTail(sizeof(TfLiteAffineQuantization),
                                               sizeof(int)));
   d59ee:	9001      	str	r0, [sp, #4]
    int* zero_point_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(int) + sizeof(int), sizeof(int)));
   d59f0:	2204      	movs	r2, #4
   d59f2:	4658      	mov	r0, fp
   d59f4:	f000 fcfb 	bl	d63ee <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>
    int* scale_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(float) + sizeof(int), sizeof(int)));
   d59f8:	4649      	mov	r1, r9
        reinterpret_cast<TfLiteAffineQuantization*>(
            memory_allocator_.AllocateFromTail(sizeof(TfLiteAffineQuantization),
                                               sizeof(int)));
    int* zero_point_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(int) + sizeof(int), sizeof(int)));
   d59fa:	4682      	mov	sl, r0
    int* scale_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(float) + sizeof(int), sizeof(int)));
   d59fc:	2204      	movs	r2, #4
   d59fe:	4658      	mov	r0, fp
   d5a00:	f000 fcf5 	bl	d63ee <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>
    zero_point_array[0] = channels;
    scale_array[0] = channels;
   d5a04:	4681      	mov	r9, r0
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(int) + sizeof(int), sizeof(int)));
    int* scale_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(float) + sizeof(int), sizeof(int)));
    zero_point_array[0] = channels;
   d5a06:	f8ca 8000 	str.w	r8, [sl]
    scale_array[0] = channels;
   d5a0a:	f849 8b04 	str.w	r8, [r9], #4
    int* zero_point_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(int) + sizeof(int), sizeof(int)));
    int* scale_array =
        reinterpret_cast<int*>(memory_allocator_.AllocateFromTail(
            channels * sizeof(float) + sizeof(int), sizeof(int)));
   d5a0e:	9002      	str	r0, [sp, #8]
   d5a10:	f8cd a00c 	str.w	sl, [sp, #12]
    zero_point_array[0] = channels;
    scale_array[0] = channels;
    int* zero_point_data = &zero_point_array[1];
    float* scale_data = reinterpret_cast<float*>(&scale_array[1]);
    for (int i = 0; i < channels; i++) {
   d5a14:	f04f 0b00 	mov.w	fp, #0
   d5a18:	45d8      	cmp	r8, fp
   d5a1a:	dd0c      	ble.n	d5a36 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1be>
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5a1c:	210a      	movs	r1, #10
   d5a1e:	4638      	mov	r0, r7
   d5a20:	f7ff fed8 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
   d5a24:	6801      	ldr	r1, [r0, #0]
   d5a26:	458b      	cmp	fp, r1
   d5a28:	d323      	bcc.n	d5a72 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1fa>
   d5a2a:	4b1d      	ldr	r3, [pc, #116]	; (d5aa0 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x228>)
   d5a2c:	4a1e      	ldr	r2, [pc, #120]	; (d5aa8 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x230>)
   d5a2e:	481f      	ldr	r0, [pc, #124]	; (d5aac <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x234>)
   d5a30:	21ed      	movs	r1, #237	; 0xed
   d5a32:	f00e fc9b 	bl	e436c <__assert_func>
      zero_point_data[i] = src_quantization->zero_point()->Get(i);
      scale_data[i] = src_quantization->scale()->Get(i);
    }
    quantization->scale = reinterpret_cast<TfLiteFloatArray*>(scale_array);
   d5a36:	9b01      	ldr	r3, [sp, #4]
   d5a38:	461a      	mov	r2, r3
   d5a3a:	9b02      	ldr	r3, [sp, #8]
   d5a3c:	6013      	str	r3, [r2, #0]
    quantization->zero_point =
        reinterpret_cast<TfLiteIntArray*>(zero_point_array);

    result->quantization = {kTfLiteAffineQuantization, quantization};
   d5a3e:	2301      	movs	r3, #1
      zero_point_data[i] = src_quantization->zero_point()->Get(i);
      scale_data[i] = src_quantization->scale()->Get(i);
    }
    quantization->scale = reinterpret_cast<TfLiteFloatArray*>(scale_array);
    quantization->zero_point =
        reinterpret_cast<TfLiteIntArray*>(zero_point_array);
   d5a40:	f8c2 a004 	str.w	sl, [r2, #4]

    result->quantization = {kTfLiteAffineQuantization, quantization};
   d5a44:	f884 3030 	strb.w	r3, [r4, #48]	; 0x30
   d5a48:	6362      	str	r2, [r4, #52]	; 0x34
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5a4a:	210a      	movs	r1, #10
   d5a4c:	4628      	mov	r0, r5
   d5a4e:	f7ff fec1 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
  }
  // Copy the name, if there is one.
  if (flatbuffer_tensor.name()->c_str() != nullptr) {
   d5a52:	3004      	adds	r0, #4
    result->name = flatbuffer_tensor.name()->c_str();
  } else {
    result->name = "<No name>";
   d5a54:	bf04      	itt	eq
   d5a56:	4b16      	ldreq	r3, [pc, #88]	; (d5ab0 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x238>)
   d5a58:	6223      	streq	r3, [r4, #32]
  }
  // These aren't used by the micro flavor of TFL, so set them to defaults.
  result->allocation = nullptr;
   d5a5a:	f04f 0300 	mov.w	r3, #0

    result->quantization = {kTfLiteAffineQuantization, quantization};
  }
  // Copy the name, if there is one.
  if (flatbuffer_tensor.name()->c_str() != nullptr) {
    result->name = flatbuffer_tensor.name()->c_str();
   d5a5e:	bf18      	it	ne
   d5a60:	6220      	strne	r0, [r4, #32]
  } else {
    result->name = "<No name>";
  }
  // These aren't used by the micro flavor of TFL, so set them to defaults.
  result->allocation = nullptr;
   d5a62:	61e3      	str	r3, [r4, #28]
  result->delegate = nullptr;
   d5a64:	6263      	str	r3, [r4, #36]	; 0x24
  result->buffer_handle = 0;
   d5a66:	62a3      	str	r3, [r4, #40]	; 0x28
  result->data_is_stale = false;
   d5a68:	f884 302c 	strb.w	r3, [r4, #44]	; 0x2c
   d5a6c:	e014      	b.n	d5a98 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x220>
    const flatbuffers::Vector<flatbuffers::Offset<Buffer>>* buffers,
    ErrorReporter* error_reporter, TfLiteTensor* result,
    uint8_t* preallocated_buffer) {
  // Make sure the serialized type is one we know how to deal with, and convert
  // it from a flatbuffer enum into a constant used by the kernel C API.
  TF_LITE_ENSURE_STATUS(ConvertTensorType(flatbuffer_tensor.type(),
   d5a6e:	2601      	movs	r6, #1
   d5a70:	e012      	b.n	d5a98 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x220>
template<typename T> struct IndirectHelper {
  typedef T return_type;
  typedef T mutable_return_type;
  static const size_t element_stride = sizeof(T);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    return EndianScalar((reinterpret_cast<const T *>(p))[i]);
   d5a72:	eb00 00cb 	add.w	r0, r0, fp, lsl #3
    zero_point_array[0] = channels;
    scale_array[0] = channels;
    int* zero_point_data = &zero_point_array[1];
    float* scale_data = reinterpret_cast<float*>(&scale_array[1]);
    for (int i = 0; i < channels; i++) {
      zero_point_data[i] = src_quantization->zero_point()->Get(i);
   d5a76:	9b03      	ldr	r3, [sp, #12]
   d5a78:	6841      	ldr	r1, [r0, #4]
   d5a7a:	f843 1f04 	str.w	r1, [r3, #4]!
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5a7e:	4638      	mov	r0, r7
   d5a80:	2108      	movs	r1, #8
   d5a82:	9303      	str	r3, [sp, #12]
   d5a84:	f7ff fea6 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
      scale_data[i] = src_quantization->scale()->Get(i);
   d5a88:	4659      	mov	r1, fp
   d5a8a:	f7ff fe61 	bl	d5750 <_ZNK11flatbuffers6VectorIfE3GetEm>
            channels * sizeof(float) + sizeof(int), sizeof(int)));
    zero_point_array[0] = channels;
    scale_array[0] = channels;
    int* zero_point_data = &zero_point_array[1];
    float* scale_data = reinterpret_cast<float*>(&scale_array[1]);
    for (int i = 0; i < channels; i++) {
   d5a8e:	f10b 0b01 	add.w	fp, fp, #1
      zero_point_data[i] = src_quantization->zero_point()->Get(i);
      scale_data[i] = src_quantization->scale()->Get(i);
   d5a92:	eca9 0a01 	vstmia	r9!, {s0}
   d5a96:	e7bf      	b.n	d5a18 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh+0x1a0>
  result->allocation = nullptr;
  result->delegate = nullptr;
  result->buffer_handle = 0;
  result->data_is_stale = false;
  return kTfLiteOk;
}
   d5a98:	4630      	mov	r0, r6
   d5a9a:	b007      	add	sp, #28
   d5a9c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   d5aa0:	000e8511 	.word	0x000e8511
   d5aa4:	000e92a8 	.word	0x000e92a8
   d5aa8:	000e939e 	.word	0x000e939e
   d5aac:	000e851c 	.word	0x000e851c
   d5ab0:	000e90fd 	.word	0x000e90fd

000d5ab4 <_ZN6tflite14MicroAllocator15AllocateTensorsEv>:
  const auto* tensor = tensors_->Get(tensor_index);
  return InitializeRuntimeTensor(*tensor, buffers, error_reporter_,
                                 &context_->tensors[tensor_index], buffer);
}

TfLiteStatus MicroAllocator::AllocateTensors() {
   d5ab4:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d5ab8:	6a83      	ldr	r3, [r0, #40]	; 0x28
   d5aba:	f8d3 9000 	ldr.w	r9, [r3]

  // It would be better not to allocate this memory for the lifetime of the
  // model, but we don't have a straightforward way to avoid it.
  TensorInfo* tensor_info =
      reinterpret_cast<TensorInfo*>(memory_allocator_.AllocateFromTail(
          sizeof(TensorInfo) * tensors_size, sizeof(TensorInfo)));
   d5abe:	2214      	movs	r2, #20
  const auto* tensor = tensors_->Get(tensor_index);
  return InitializeRuntimeTensor(*tensor, buffers, error_reporter_,
                                 &context_->tensors[tensor_index], buffer);
}

TfLiteStatus MicroAllocator::AllocateTensors() {
   d5ac0:	b091      	sub	sp, #68	; 0x44

  // It would be better not to allocate this memory for the lifetime of the
  // model, but we don't have a straightforward way to avoid it.
  TensorInfo* tensor_info =
      reinterpret_cast<TensorInfo*>(memory_allocator_.AllocateFromTail(
          sizeof(TensorInfo) * tensors_size, sizeof(TensorInfo)));
   d5ac2:	fb02 f109 	mul.w	r1, r2, r9
  const auto* tensor = tensors_->Get(tensor_index);
  return InitializeRuntimeTensor(*tensor, buffers, error_reporter_,
                                 &context_->tensors[tensor_index], buffer);
}

TfLiteStatus MicroAllocator::AllocateTensors() {
   d5ac6:	4604      	mov	r4, r0

  // It would be better not to allocate this memory for the lifetime of the
  // model, but we don't have a straightforward way to avoid it.
  TensorInfo* tensor_info =
      reinterpret_cast<TensorInfo*>(memory_allocator_.AllocateFromTail(
          sizeof(TensorInfo) * tensors_size, sizeof(TensorInfo)));
   d5ac8:	3004      	adds	r0, #4
   d5aca:	f000 fc90 	bl	d63ee <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5ace:	210c      	movs	r1, #12
   d5ad0:	4605      	mov	r5, r0
   d5ad2:	6820      	ldr	r0, [r4, #0]
   d5ad4:	f7ff fe7e 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
   d5ad8:	f105 0710 	add.w	r7, r5, #16

  const flatbuffers::Vector<flatbuffers::Offset<Buffer>>* buffers =
      model_->buffers();

  // Set up the runtime data structures for all tensors.
  for (size_t i = 0; i < tensors_size; ++i) {
   d5adc:	f04f 0800 	mov.w	r8, #0
   d5ae0:	9002      	str	r0, [sp, #8]
   d5ae2:	463e      	mov	r6, r7
    const bool is_variable = current->flatbuffer_tensor->is_variable();
    if (is_variable) {
      current->first_created = 0;
      current->last_used = operators_->size();
    } else {
      current->first_created = -1;
   d5ae4:	f04f 3aff 	mov.w	sl, #4294967295	; 0xffffffff
    TensorInfo* current = &tensor_info[i];
    current->flatbuffer_tensor = &(*(tensors_->Get(i)));
    current->runtime_tensor = &context_->tensors[i];
    const bool is_variable = current->flatbuffer_tensor->is_variable();
    if (is_variable) {
      current->first_created = 0;
   d5ae8:	46c3      	mov	fp, r8

  const flatbuffers::Vector<flatbuffers::Offset<Buffer>>* buffers =
      model_->buffers();

  // Set up the runtime data structures for all tensors.
  for (size_t i = 0; i < tensors_size; ++i) {
   d5aea:	45c8      	cmp	r8, r9
   d5aec:	d104      	bne.n	d5af8 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x44>
   d5aee:	2600      	movs	r6, #0
  }

  // First go through the inputs and figure out if they need to be allocated.
  for (size_t i = 0; i < subgraph_->inputs()->size(); ++i) {
    const int tensor_index = subgraph_->inputs()->Get(i);
    TensorInfo* current = &tensor_info[tensor_index];
   d5af0:	f04f 0914 	mov.w	r9, #20
    // Check for pre-allocated inputs.
    current->needs_allocating = (current->runtime_tensor->data.raw == nullptr);
    current->first_created = 0;
   d5af4:	46b0      	mov	r8, r6
   d5af6:	e03e      	b.n	d5b76 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0xc2>
      model_->buffers();

  // Set up the runtime data structures for all tensors.
  for (size_t i = 0; i < tensors_size; ++i) {
    TensorInfo* current = &tensor_info[i];
    current->flatbuffer_tensor = &(*(tensors_->Get(i)));
   d5af8:	6aa0      	ldr	r0, [r4, #40]	; 0x28
   d5afa:	4641      	mov	r1, r8
   d5afc:	f7ff fe52 	bl	d57a4 <_ZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEm>
   d5b00:	f846 0c10 	str.w	r0, [r6, #-16]
    current->runtime_tensor = &context_->tensors[i];
   d5b04:	6963      	ldr	r3, [r4, #20]
   d5b06:	689b      	ldr	r3, [r3, #8]
   d5b08:	2238      	movs	r2, #56	; 0x38
   d5b0a:	fb02 3308 	mla	r3, r2, r8, r3
   d5b0e:	f846 3c0c 	str.w	r3, [r6, #-12]
   d5b12:	9303      	str	r3, [sp, #12]
    const bool is_variable = current->flatbuffer_tensor->is_variable();
   d5b14:	f7ff fe11 	bl	d573a <_ZNK6tflite6Tensor11is_variableEv>
    if (is_variable) {
   d5b18:	9b03      	ldr	r3, [sp, #12]
   d5b1a:	b130      	cbz	r0, d5b2a <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x76>
      current->first_created = 0;
   d5b1c:	f846 bc08 	str.w	fp, [r6, #-8]
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d5b20:	6a62      	ldr	r2, [r4, #36]	; 0x24
      current->last_used = operators_->size();
   d5b22:	6812      	ldr	r2, [r2, #0]
   d5b24:	f846 2c04 	str.w	r2, [r6, #-4]
   d5b28:	e003      	b.n	d5b32 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x7e>
    } else {
      current->first_created = -1;
   d5b2a:	f846 ac08 	str.w	sl, [r6, #-8]
      current->last_used = -1;
   d5b2e:	f846 ac04 	str.w	sl, [r6, #-4]
    }
    current->needs_allocating = false;
   d5b32:	f886 b000 	strb.w	fp, [r6]
    // Preallocated inputs have already been set up earlier, so skip them.
    const bool is_preallocated_input =
        (current->runtime_tensor->data.raw != nullptr);
    if (!is_preallocated_input) {
   d5b36:	685a      	ldr	r2, [r3, #4]
   d5b38:	b11a      	cbz	r2, d5b42 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x8e>

  const flatbuffers::Vector<flatbuffers::Offset<Buffer>>* buffers =
      model_->buffers();

  // Set up the runtime data structures for all tensors.
  for (size_t i = 0; i < tensors_size; ++i) {
   d5b3a:	f108 0801 	add.w	r8, r8, #1
   d5b3e:	3614      	adds	r6, #20
   d5b40:	e7d3      	b.n	d5aea <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x36>
    current->needs_allocating = false;
    // Preallocated inputs have already been set up earlier, so skip them.
    const bool is_preallocated_input =
        (current->runtime_tensor->data.raw != nullptr);
    if (!is_preallocated_input) {
      TF_LITE_ENSURE_STATUS(InitializeRuntimeTensor(
   d5b42:	9201      	str	r2, [sp, #4]
   d5b44:	9300      	str	r3, [sp, #0]
   d5b46:	6923      	ldr	r3, [r4, #16]
   d5b48:	9a02      	ldr	r2, [sp, #8]
   d5b4a:	f856 1c10 	ldr.w	r1, [r6, #-16]
   d5b4e:	4620      	mov	r0, r4
   d5b50:	f7ff fe92 	bl	d5878 <_ZN6tflite14MicroAllocator23InitializeRuntimeTensorERKNS_6TensorEPKN11flatbuffers6VectorINS4_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensorPh>
   d5b54:	2800      	cmp	r0, #0
   d5b56:	d0f0      	beq.n	d5b3a <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x86>
   d5b58:	e101      	b.n	d5d5e <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2aa>
    }
  }

  // First go through the inputs and figure out if they need to be allocated.
  for (size_t i = 0; i < subgraph_->inputs()->size(); ++i) {
    const int tensor_index = subgraph_->inputs()->Get(i);
   d5b5a:	4631      	mov	r1, r6
   d5b5c:	f7ff fe0e 	bl	d577c <_ZNK11flatbuffers6VectorIlE3GetEm>
    TensorInfo* current = &tensor_info[tensor_index];
   d5b60:	fb09 5000 	mla	r0, r9, r0, r5
          current->runtime_tensor, nullptr));
    }
  }

  // First go through the inputs and figure out if they need to be allocated.
  for (size_t i = 0; i < subgraph_->inputs()->size(); ++i) {
   d5b64:	3601      	adds	r6, #1
    const int tensor_index = subgraph_->inputs()->Get(i);
    TensorInfo* current = &tensor_info[tensor_index];
    // Check for pre-allocated inputs.
    current->needs_allocating = (current->runtime_tensor->data.raw == nullptr);
   d5b66:	6843      	ldr	r3, [r0, #4]
   d5b68:	685b      	ldr	r3, [r3, #4]
    current->first_created = 0;
   d5b6a:	f8c0 8008 	str.w	r8, [r0, #8]
  // First go through the inputs and figure out if they need to be allocated.
  for (size_t i = 0; i < subgraph_->inputs()->size(); ++i) {
    const int tensor_index = subgraph_->inputs()->Get(i);
    TensorInfo* current = &tensor_info[tensor_index];
    // Check for pre-allocated inputs.
    current->needs_allocating = (current->runtime_tensor->data.raw == nullptr);
   d5b6e:	fab3 f383 	clz	r3, r3
   d5b72:	095b      	lsrs	r3, r3, #5
   d5b74:	7403      	strb	r3, [r0, #16]
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5b76:	2106      	movs	r1, #6
   d5b78:	6a20      	ldr	r0, [r4, #32]
   d5b7a:	f7ff fe2b 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
          current->runtime_tensor, nullptr));
    }
  }

  // First go through the inputs and figure out if they need to be allocated.
  for (size_t i = 0; i < subgraph_->inputs()->size(); ++i) {
   d5b7e:	6803      	ldr	r3, [r0, #0]
   d5b80:	429e      	cmp	r6, r3
   d5b82:	d3ea      	bcc.n	d5b5a <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0xa6>
   d5b84:	2600      	movs	r6, #0

  // Mark all outputs as persistent to the end of the invocation.
  for (size_t i = 0; i < subgraph_->outputs()->size(); ++i) {
    const int tensor_index = subgraph_->outputs()->Get(i);
    TensorInfo* current = &tensor_info[tensor_index];
    current->last_used = operators_->size() - 1;
   d5b86:	f04f 0914 	mov.w	r9, #20
   d5b8a:	2108      	movs	r1, #8
   d5b8c:	6a20      	ldr	r0, [r4, #32]
   d5b8e:	f7ff fe21 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
    current->needs_allocating = (current->runtime_tensor->data.raw == nullptr);
    current->first_created = 0;
  }

  // Mark all outputs as persistent to the end of the invocation.
  for (size_t i = 0; i < subgraph_->outputs()->size(); ++i) {
   d5b92:	6803      	ldr	r3, [r0, #0]
   d5b94:	f8d4 8024 	ldr.w	r8, [r4, #36]	; 0x24
   d5b98:	429e      	cmp	r6, r3
   d5b9a:	d20a      	bcs.n	d5bb2 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0xfe>
    const int tensor_index = subgraph_->outputs()->Get(i);
   d5b9c:	4631      	mov	r1, r6
   d5b9e:	f7ff fded 	bl	d577c <_ZNK11flatbuffers6VectorIlE3GetEm>
    TensorInfo* current = &tensor_info[tensor_index];
    current->last_used = operators_->size() - 1;
   d5ba2:	f8d8 3000 	ldr.w	r3, [r8]
   d5ba6:	fb09 5000 	mla	r0, r9, r0, r5
   d5baa:	3b01      	subs	r3, #1
   d5bac:	60c3      	str	r3, [r0, #12]
    current->needs_allocating = (current->runtime_tensor->data.raw == nullptr);
    current->first_created = 0;
  }

  // Mark all outputs as persistent to the end of the invocation.
  for (size_t i = 0; i < subgraph_->outputs()->size(); ++i) {
   d5bae:	3601      	adds	r6, #1
   d5bb0:	e7eb      	b.n	d5b8a <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0xd6>
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d5bb2:	f8d8 6000 	ldr.w	r6, [r8]
    TensorInfo* current = &tensor_info[tensor_index];
    current->last_used = operators_->size() - 1;
  }

  // Figure out when the first and last use of each tensor is.
  for (int i = (operators_->size() - 1); i >= 0; --i) {
   d5bb6:	f106 38ff 	add.w	r8, r6, #4294967295	; 0xffffffff
   d5bba:	f106 4680 	add.w	r6, r6, #1073741824	; 0x40000000
   d5bbe:	3e01      	subs	r6, #1
   d5bc0:	00b6      	lsls	r6, r6, #2
    const auto* op = operators_->Get(i);
    for (size_t n = 0; n < op->inputs()->size(); ++n) {
      const int tensor_index = op->inputs()->Get(n);
      TensorInfo* current = &tensor_info[tensor_index];
   d5bc2:	f04f 0914 	mov.w	r9, #20
    TensorInfo* current = &tensor_info[tensor_index];
    current->last_used = operators_->size() - 1;
  }

  // Figure out when the first and last use of each tensor is.
  for (int i = (operators_->size() - 1); i >= 0; --i) {
   d5bc6:	f1b8 0f00 	cmp.w	r8, #0
   d5bca:	da03      	bge.n	d5bd4 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x120>
   d5bcc:	462b      	mov	r3, r5
   d5bce:	2200      	movs	r2, #0
          "Logic error in memory planner, tensor %d has an invalid lifetime",
          i);
      return kTfLiteError;
    }
    if (!is_read_only && !is_preallocated_input) {
      current->needs_allocating = true;
   d5bd0:	2001      	movs	r0, #1
   d5bd2:	e04c      	b.n	d5c6e <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x1ba>
    current->last_used = operators_->size() - 1;
  }

  // Figure out when the first and last use of each tensor is.
  for (int i = (operators_->size() - 1); i >= 0; --i) {
    const auto* op = operators_->Get(i);
   d5bd4:	6a63      	ldr	r3, [r4, #36]	; 0x24

  typedef typename IndirectHelper<T>::return_type return_type;
  typedef typename IndirectHelper<T>::mutable_return_type mutable_return_type;

  return_type Get(uoffset_t i) const {
    FLATBUFFERS_ASSERT(i < size());
   d5bd6:	681a      	ldr	r2, [r3, #0]
   d5bd8:	4590      	cmp	r8, r2
   d5bda:	d305      	bcc.n	d5be8 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x134>
   d5bdc:	4b66      	ldr	r3, [pc, #408]	; (d5d78 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2c4>)
   d5bde:	4a67      	ldr	r2, [pc, #412]	; (d5d7c <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2c8>)
   d5be0:	4867      	ldr	r0, [pc, #412]	; (d5d80 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2cc>)
   d5be2:	21ed      	movs	r1, #237	; 0xed
   d5be4:	f00e fbc2 	bl	e436c <__assert_func>
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d5be8:	3304      	adds	r3, #4
template<typename T> struct IndirectHelper<Offset<T>> {
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
   d5bea:	eb03 0b06 	add.w	fp, r3, r6
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
   d5bee:	599b      	ldr	r3, [r3, r6]
    for (size_t n = 0; n < op->inputs()->size(); ++n) {
   d5bf0:	f04f 0a00 	mov.w	sl, #0
   d5bf4:	449b      	add	fp, r3
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5bf6:	2106      	movs	r1, #6
   d5bf8:	4658      	mov	r0, fp
   d5bfa:	f7ff fdeb 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
   d5bfe:	6803      	ldr	r3, [r0, #0]
   d5c00:	459a      	cmp	sl, r3
   d5c02:	d302      	bcc.n	d5c0a <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x156>
   d5c04:	f04f 0a00 	mov.w	sl, #0
   d5c08:	e01a      	b.n	d5c40 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x18c>
      const int tensor_index = op->inputs()->Get(n);
   d5c0a:	4651      	mov	r1, sl
   d5c0c:	f7ff fdb6 	bl	d577c <_ZNK11flatbuffers6VectorIlE3GetEm>
      TensorInfo* current = &tensor_info[tensor_index];
   d5c10:	fb09 5000 	mla	r0, r9, r0, r5
      if ((current->last_used == -1) || (current->last_used > i)) {
   d5c14:	68c3      	ldr	r3, [r0, #12]
   d5c16:	1c59      	adds	r1, r3, #1
   d5c18:	d001      	beq.n	d5c1e <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x16a>
   d5c1a:	4598      	cmp	r8, r3
   d5c1c:	da01      	bge.n	d5c22 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x16e>
        current->last_used = i;
   d5c1e:	f8c0 800c 	str.w	r8, [r0, #12]
  }

  // Figure out when the first and last use of each tensor is.
  for (int i = (operators_->size() - 1); i >= 0; --i) {
    const auto* op = operators_->Get(i);
    for (size_t n = 0; n < op->inputs()->size(); ++n) {
   d5c22:	f10a 0a01 	add.w	sl, sl, #1
   d5c26:	e7e6      	b.n	d5bf6 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x142>
      if ((current->last_used == -1) || (current->last_used > i)) {
        current->last_used = i;
      }
    }
    for (size_t n = 0; n < op->outputs()->size(); ++n) {
      const int tensor_index = op->outputs()->Get(n);
   d5c28:	4651      	mov	r1, sl
   d5c2a:	f7ff fda7 	bl	d577c <_ZNK11flatbuffers6VectorIlE3GetEm>
      TensorInfo* current = &tensor_info[tensor_index];
   d5c2e:	fb09 5000 	mla	r0, r9, r0, r5
      if ((current->first_created == -1) || (current->first_created < i)) {
   d5c32:	6883      	ldr	r3, [r0, #8]
   d5c34:	1c5a      	adds	r2, r3, #1
   d5c36:	d00b      	beq.n	d5c50 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x19c>
   d5c38:	4598      	cmp	r8, r3
   d5c3a:	dc09      	bgt.n	d5c50 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x19c>
      TensorInfo* current = &tensor_info[tensor_index];
      if ((current->last_used == -1) || (current->last_used > i)) {
        current->last_used = i;
      }
    }
    for (size_t n = 0; n < op->outputs()->size(); ++n) {
   d5c3c:	f10a 0a01 	add.w	sl, sl, #1
   d5c40:	2108      	movs	r1, #8
   d5c42:	4658      	mov	r0, fp
   d5c44:	f7ff fdc6 	bl	d57d4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite6BufferEEEEEEET_t>
   d5c48:	6803      	ldr	r3, [r0, #0]
   d5c4a:	459a      	cmp	sl, r3
   d5c4c:	d3ec      	bcc.n	d5c28 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x174>
   d5c4e:	e002      	b.n	d5c56 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x1a2>
      const int tensor_index = op->outputs()->Get(n);
      TensorInfo* current = &tensor_info[tensor_index];
      if ((current->first_created == -1) || (current->first_created < i)) {
        current->first_created = i;
   d5c50:	f8c0 8008 	str.w	r8, [r0, #8]
   d5c54:	e7f2      	b.n	d5c3c <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x188>
    TensorInfo* current = &tensor_info[tensor_index];
    current->last_used = operators_->size() - 1;
  }

  // Figure out when the first and last use of each tensor is.
  for (int i = (operators_->size() - 1); i >= 0; --i) {
   d5c56:	f108 38ff 	add.w	r8, r8, #4294967295	; 0xffffffff
   d5c5a:	3e04      	subs	r6, #4
   d5c5c:	e7b3      	b.n	d5bc6 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x112>

  // Work out which tensors need to be allocated.
  for (size_t i = 0; i < tensors_->size(); ++i) {
    TensorInfo* current = &tensor_info[i];
    const bool is_read_only =
        (current->first_created == -1) && (current->last_used != -1);
   d5c5e:	6899      	ldr	r1, [r3, #8]
   d5c60:	3101      	adds	r1, #1
   d5c62:	68d9      	ldr	r1, [r3, #12]
   d5c64:	d175      	bne.n	d5d52 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x29e>
   d5c66:	3101      	adds	r1, #1
   d5c68:	d075      	beq.n	d5d56 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2a2>
      }
    }
  }

  // Work out which tensors need to be allocated.
  for (size_t i = 0; i < tensors_->size(); ++i) {
   d5c6a:	3201      	adds	r2, #1
   d5c6c:	3314      	adds	r3, #20
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d5c6e:	6aa1      	ldr	r1, [r4, #40]	; 0x28
   d5c70:	6809      	ldr	r1, [r1, #0]
   d5c72:	428a      	cmp	r2, r1
   d5c74:	d3f3      	bcc.n	d5c5e <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x1aa>
    if (!is_read_only && !is_preallocated_input) {
      current->needs_allocating = true;
    }
  }

  uint8_t* aligned_arena = AlignPointerUp(arena_, kBufferAlignment);
   d5c76:	2110      	movs	r1, #16
   d5c78:	69a0      	ldr	r0, [r4, #24]
   d5c7a:	f7ff fce5 	bl	d5648 <_ZN6tflite14AlignPointerUpEPhj>
  const size_t alignment_loss = (aligned_arena - arena_);

  int remaining_arena_size =
      arena_size_ - (memory_allocator_.GetDataSize() + alignment_loss);
   d5c7e:	69e3      	ldr	r3, [r4, #28]
   d5c80:	6866      	ldr	r6, [r4, #4]
   d5c82:	1b9e      	subs	r6, r3, r6
   d5c84:	69a3      	ldr	r3, [r4, #24]
   d5c86:	1ac3      	subs	r3, r0, r3
   d5c88:	1af6      	subs	r6, r6, r3
  GreedyMemoryPlanner planner(aligned_arena, remaining_arena_size);
   d5c8a:	4601      	mov	r1, r0
    if (!is_read_only && !is_preallocated_input) {
      current->needs_allocating = true;
    }
  }

  uint8_t* aligned_arena = AlignPointerUp(arena_, kBufferAlignment);
   d5c8c:	4680      	mov	r8, r0
  const size_t alignment_loss = (aligned_arena - arena_);

  int remaining_arena_size =
      arena_size_ - (memory_allocator_.GetDataSize() + alignment_loss);
  GreedyMemoryPlanner planner(aligned_arena, remaining_arena_size);
   d5c8e:	4632      	mov	r2, r6
   d5c90:	a806      	add	r0, sp, #24
   d5c92:	f00d fd3d 	bl	e3710 <_ZN6tflite19GreedyMemoryPlannerC1EPhi>

  // Add the tensors to our allocation plan.
  for (size_t i = 0; i < tensors_->size(); ++i) {
   d5c96:	f04f 0900 	mov.w	r9, #0
   d5c9a:	6aa3      	ldr	r3, [r4, #40]	; 0x28
   d5c9c:	681b      	ldr	r3, [r3, #0]
   d5c9e:	4599      	cmp	r9, r3
   d5ca0:	d21b      	bcs.n	d5cda <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x226>
    TensorInfo* current = &tensor_info[i];
    if (current->needs_allocating) {
   d5ca2:	783b      	ldrb	r3, [r7, #0]
   d5ca4:	b1ab      	cbz	r3, d5cd2 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x21e>
      size_t bytes_required;
      size_t type_size;
      TF_LITE_ENSURE_STATUS(BytesRequiredForTensor(*current->flatbuffer_tensor,
   d5ca6:	6923      	ldr	r3, [r4, #16]
   d5ca8:	f857 0c10 	ldr.w	r0, [r7, #-16]
   d5cac:	aa05      	add	r2, sp, #20
   d5cae:	a904      	add	r1, sp, #16
   d5cb0:	f7ff fcfe 	bl	d56b0 <_ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE>
   d5cb4:	bb00      	cbnz	r0, d5cf8 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x244>
                                                   &bytes_required, &type_size,
                                                   error_reporter_));
      size_t aligned_bytes_required =
          AlignSizeUp(bytes_required, kBufferAlignment);
   d5cb6:	2110      	movs	r1, #16
   d5cb8:	9804      	ldr	r0, [sp, #16]
   d5cba:	f7ff fccf 	bl	d565c <_ZN6tflite11AlignSizeUpEjj>
      planner.AddBuffer(error_reporter_, aligned_bytes_required,
                        current->first_created, current->last_used);
   d5cbe:	f857 3c04 	ldr.w	r3, [r7, #-4]
   d5cc2:	9300      	str	r3, [sp, #0]
   d5cc4:	4602      	mov	r2, r0
   d5cc6:	f857 3c08 	ldr.w	r3, [r7, #-8]
   d5cca:	6921      	ldr	r1, [r4, #16]
   d5ccc:	a806      	add	r0, sp, #24
   d5cce:	f00d fce3 	bl	e3698 <_ZN6tflite19GreedyMemoryPlanner9AddBufferEPNS_13ErrorReporterEiii>
  int remaining_arena_size =
      arena_size_ - (memory_allocator_.GetDataSize() + alignment_loss);
  GreedyMemoryPlanner planner(aligned_arena, remaining_arena_size);

  // Add the tensors to our allocation plan.
  for (size_t i = 0; i < tensors_->size(); ++i) {
   d5cd2:	f109 0901 	add.w	r9, r9, #1
   d5cd6:	3714      	adds	r7, #20
   d5cd8:	e7df      	b.n	d5c9a <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x1e6>
                        current->first_created, current->last_used);
    }
  }

  // Make sure we have enough room.
  if (planner.GetMaximumMemorySize() > remaining_arena_size) {
   d5cda:	a806      	add	r0, sp, #24
   d5cdc:	f00d fe01 	bl	e38e2 <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv>
   d5ce0:	4286      	cmp	r6, r0
   d5ce2:	da0b      	bge.n	d5cfc <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x248>
    error_reporter_->Report(
   d5ce4:	a806      	add	r0, sp, #24
   d5ce6:	6924      	ldr	r4, [r4, #16]
   d5ce8:	f00d fdfb 	bl	e38e2 <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv>
        "Arena size is too small for activation buffers. Needed %d but only %d "
        "was available.",
        planner.GetMaximumMemorySize(), remaining_arena_size);
   d5cec:	4633      	mov	r3, r6
   d5cee:	4602      	mov	r2, r0
   d5cf0:	4924      	ldr	r1, [pc, #144]	; (d5d84 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2d0>)
   d5cf2:	4620      	mov	r0, r4
   d5cf4:	f7fe fb6e 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return kTfLiteError;
   d5cf8:	2401      	movs	r4, #1
   d5cfa:	e026      	b.n	d5d4a <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x296>
   d5cfc:	2600      	movs	r6, #0
   d5cfe:	4637      	mov	r7, r6
   d5d00:	6aa3      	ldr	r3, [r4, #40]	; 0x28
  }

  // Figure out the actual memory addresses for each buffer, based on the plan.
  int planner_index = 0;
  for (size_t i = 0; i < tensors_->size(); ++i) {
   d5d02:	681b      	ldr	r3, [r3, #0]
   d5d04:	429e      	cmp	r6, r3
   d5d06:	d21f      	bcs.n	d5d48 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x294>
    TensorInfo* current = &tensor_info[i];
    if (current->needs_allocating) {
   d5d08:	7c2b      	ldrb	r3, [r5, #16]
   d5d0a:	b163      	cbz	r3, d5d26 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x272>
      int offset;
      TF_LITE_ENSURE_STATUS(
   d5d0c:	ab05      	add	r3, sp, #20
   d5d0e:	463a      	mov	r2, r7
   d5d10:	6921      	ldr	r1, [r4, #16]
   d5d12:	a806      	add	r0, sp, #24
   d5d14:	f00d fe00 	bl	e3918 <_ZN6tflite19GreedyMemoryPlanner18GetOffsetForBufferEPNS_13ErrorReporterEiPi>
   d5d18:	2800      	cmp	r0, #0
   d5d1a:	d1ed      	bne.n	d5cf8 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x244>
          planner.GetOffsetForBuffer(error_reporter_, planner_index, &offset));
      current->runtime_tensor->data.uint8 = aligned_arena + offset;
   d5d1c:	9b05      	ldr	r3, [sp, #20]
   d5d1e:	686a      	ldr	r2, [r5, #4]
   d5d20:	4443      	add	r3, r8
   d5d22:	6053      	str	r3, [r2, #4]
      ++planner_index;
   d5d24:	3701      	adds	r7, #1
    }
    // Set default value for variable tensors:
    if (current->flatbuffer_tensor->is_variable()) {
   d5d26:	6828      	ldr	r0, [r5, #0]
   d5d28:	f7ff fd07 	bl	d573a <_ZNK6tflite6Tensor11is_variableEv>
   d5d2c:	b148      	cbz	r0, d5d42 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x28e>
      if (current->runtime_tensor->data.uint8 == nullptr) {
   d5d2e:	6868      	ldr	r0, [r5, #4]
   d5d30:	6843      	ldr	r3, [r0, #4]
   d5d32:	b923      	cbnz	r3, d5d3e <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x28a>
        error_reporter_->Report("Variable is not allocated");
   d5d34:	4914      	ldr	r1, [pc, #80]	; (d5d88 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2d4>)
   d5d36:	6920      	ldr	r0, [r4, #16]
   d5d38:	f7fe fb4c 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
   d5d3c:	e7dc      	b.n	d5cf8 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x244>
        return kTfLiteError;
      }
      tflite::ResetVariableTensor(current->runtime_tensor);
   d5d3e:	f7ff fbaf 	bl	d54a0 <_ZN6tflite19ResetVariableTensorEP12TfLiteTensor>
    return kTfLiteError;
  }

  // Figure out the actual memory addresses for each buffer, based on the plan.
  int planner_index = 0;
  for (size_t i = 0; i < tensors_->size(); ++i) {
   d5d42:	3601      	adds	r6, #1
   d5d44:	3514      	adds	r5, #20
   d5d46:	e7db      	b.n	d5d00 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x24c>
      }
      tflite::ResetVariableTensor(current->runtime_tensor);
    }
  }

  return kTfLiteOk;
   d5d48:	2400      	movs	r4, #0
  uint8_t* aligned_arena = AlignPointerUp(arena_, kBufferAlignment);
  const size_t alignment_loss = (aligned_arena - arena_);

  int remaining_arena_size =
      arena_size_ - (memory_allocator_.GetDataSize() + alignment_loss);
  GreedyMemoryPlanner planner(aligned_arena, remaining_arena_size);
   d5d4a:	a806      	add	r0, sp, #24
   d5d4c:	f00d fc9a 	bl	e3684 <_ZN6tflite19GreedyMemoryPlannerD1Ev>
   d5d50:	e00e      	b.n	d5d70 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2bc>
        (current->first_created == -1) && (current->last_used != -1);
    const bool is_preallocated_input =
        (current->runtime_tensor->data.raw != nullptr);
    const bool has_partial_lifetime =
        !is_read_only &&
        ((current->first_created == -1) || (current->last_used == -1));
   d5d52:	3101      	adds	r1, #1
   d5d54:	d105      	bne.n	d5d62 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2ae>
    if (has_partial_lifetime) {
      error_reporter_->Report(
          "Logic error in memory planner, tensor %d has an invalid lifetime",
          i);
   d5d56:	490d      	ldr	r1, [pc, #52]	; (d5d8c <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2d8>)
   d5d58:	6920      	ldr	r0, [r4, #16]
   d5d5a:	f7fe fb3b 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
      return kTfLiteError;
   d5d5e:	2401      	movs	r4, #1
   d5d60:	e006      	b.n	d5d70 <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x2bc>
  for (size_t i = 0; i < tensors_->size(); ++i) {
    TensorInfo* current = &tensor_info[i];
    const bool is_read_only =
        (current->first_created == -1) && (current->last_used != -1);
    const bool is_preallocated_input =
        (current->runtime_tensor->data.raw != nullptr);
   d5d62:	6859      	ldr	r1, [r3, #4]
      error_reporter_->Report(
          "Logic error in memory planner, tensor %d has an invalid lifetime",
          i);
      return kTfLiteError;
    }
    if (!is_read_only && !is_preallocated_input) {
   d5d64:	6849      	ldr	r1, [r1, #4]
   d5d66:	2900      	cmp	r1, #0
   d5d68:	f47f af7f 	bne.w	d5c6a <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x1b6>
      current->needs_allocating = true;
   d5d6c:	7418      	strb	r0, [r3, #16]
   d5d6e:	e77c      	b.n	d5c6a <_ZN6tflite14MicroAllocator15AllocateTensorsEv+0x1b6>
      tflite::ResetVariableTensor(current->runtime_tensor);
    }
  }

  return kTfLiteOk;
}
   d5d70:	4620      	mov	r0, r4
   d5d72:	b011      	add	sp, #68	; 0x44
   d5d74:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   d5d78:	000e8511 	.word	0x000e8511
   d5d7c:	000e8ee4 	.word	0x000e8ee4
   d5d80:	000e851c 	.word	0x000e851c
   d5d84:	000e9130 	.word	0x000e9130
   d5d88:	000e9185 	.word	0x000e9185
   d5d8c:	000e919f 	.word	0x000e919f

000d5d90 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list>:
  }
  DebugLog("\r\n");
}
}  // namespace

int MicroErrorReporter::Report(const char* format, va_list args) {
   d5d90:	b5f0      	push	{r4, r5, r6, r7, lr}
namespace tflite {
namespace {
void DebugLogPrintf(const char* format, va_list args) {
  const int output_cache_size = 64;
  char output_cache[output_cache_size + 1];
  int output_cache_index = 0;
   d5d92:	2300      	movs	r3, #0
  }
  DebugLog("\r\n");
}
}  // namespace

int MicroErrorReporter::Report(const char* format, va_list args) {
   d5d94:	b093      	sub	sp, #76	; 0x4c
   d5d96:	460d      	mov	r5, r1
   d5d98:	4614      	mov	r4, r2
    } else {
      output_cache[output_cache_index] = *current;
      output_cache_index += 1;
    }
    if (output_cache_index >= output_cache_size) {
      output_cache[output_cache_index] = 0;
   d5d9a:	461f      	mov	r7, r3
void DebugLogPrintf(const char* format, va_list args) {
  const int output_cache_size = 64;
  char output_cache[output_cache_size + 1];
  int output_cache_index = 0;
  const char* current = format;
  while (*current != 0) {
   d5d9c:	782a      	ldrb	r2, [r5, #0]
   d5d9e:	2a00      	cmp	r2, #0
   d5da0:	d041      	beq.n	d5e26 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x96>
    if (*current == '%') {
   d5da2:	2a25      	cmp	r2, #37	; 0x25
   d5da4:	d12e      	bne.n	d5e04 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x74>
      const char next = *(current + 1);
   d5da6:	786e      	ldrb	r6, [r5, #1]
      if ((next == 'd') || (next == 's') || (next == 'f')) {
   d5da8:	f006 02fd 	and.w	r2, r6, #253	; 0xfd
   d5dac:	2a64      	cmp	r2, #100	; 0x64
   d5dae:	d001      	beq.n	d5db4 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x24>
   d5db0:	2e73      	cmp	r6, #115	; 0x73
   d5db2:	d12c      	bne.n	d5e0e <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x7e>
        current += 1;
   d5db4:	3501      	adds	r5, #1
        if (output_cache_index > 0) {
   d5db6:	b133      	cbz	r3, d5dc6 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x36>
          output_cache[output_cache_index] = 0;
   d5db8:	aa12      	add	r2, sp, #72	; 0x48
   d5dba:	4413      	add	r3, r2
          DebugLog(output_cache);
   d5dbc:	a801      	add	r0, sp, #4
    if (*current == '%') {
      const char next = *(current + 1);
      if ((next == 'd') || (next == 's') || (next == 'f')) {
        current += 1;
        if (output_cache_index > 0) {
          output_cache[output_cache_index] = 0;
   d5dbe:	f803 7c44 	strb.w	r7, [r3, #-68]
          DebugLog(output_cache);
   d5dc2:	f000 fb29 	bl	d6418 <DebugLog>
          output_cache_index = 0;
        }
        if (next == 'd') {
   d5dc6:	2e64      	cmp	r6, #100	; 0x64
   d5dc8:	d104      	bne.n	d5dd4 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x44>
          DebugLogInt32(va_arg(args, int));
   d5dca:	6820      	ldr	r0, [r4, #0]
   d5dcc:	1d26      	adds	r6, r4, #4
   d5dce:	f7ff fbaf 	bl	d5530 <DebugLogInt32>
   d5dd2:	e005      	b.n	d5de0 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x50>
        } else if (next == 's') {
   d5dd4:	2e73      	cmp	r6, #115	; 0x73
   d5dd6:	d105      	bne.n	d5de4 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x54>
          DebugLog(va_arg(args, char*));
   d5dd8:	6820      	ldr	r0, [r4, #0]
   d5dda:	1d26      	adds	r6, r4, #4
   d5ddc:	f000 fb1c 	bl	d6418 <DebugLog>
   d5de0:	4634      	mov	r4, r6
   d5de2:	e01d      	b.n	d5e20 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x90>
        } else if (next == 'f') {
   d5de4:	2e66      	cmp	r6, #102	; 0x66
   d5de6:	d11b      	bne.n	d5e20 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x90>
          DebugLogFloat(va_arg(args, double));
   d5de8:	1de2      	adds	r2, r4, #7
   d5dea:	f022 0207 	bic.w	r2, r2, #7
   d5dee:	e9d2 0100 	ldrd	r0, r1, [r2]
   d5df2:	f102 0408 	add.w	r4, r2, #8
   d5df6:	f011 fc1f 	bl	e7638 <__aeabi_d2f>
   d5dfa:	ee00 0a10 	vmov	s0, r0
   d5dfe:	f7ff fbad 	bl	d555c <DebugLogFloat>
   d5e02:	e00d      	b.n	d5e20 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x90>
        }
      }
    } else {
      output_cache[output_cache_index] = *current;
   d5e04:	a912      	add	r1, sp, #72	; 0x48
   d5e06:	4419      	add	r1, r3
      output_cache_index += 1;
   d5e08:	3301      	adds	r3, #1
        } else if (next == 'f') {
          DebugLogFloat(va_arg(args, double));
        }
      }
    } else {
      output_cache[output_cache_index] = *current;
   d5e0a:	f801 2c44 	strb.w	r2, [r1, #-68]
      output_cache_index += 1;
    }
    if (output_cache_index >= output_cache_size) {
   d5e0e:	2b3f      	cmp	r3, #63	; 0x3f
   d5e10:	dd07      	ble.n	d5e22 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0x92>
      output_cache[output_cache_index] = 0;
   d5e12:	aa12      	add	r2, sp, #72	; 0x48
   d5e14:	4413      	add	r3, r2
      DebugLog(output_cache);
   d5e16:	a801      	add	r0, sp, #4
    } else {
      output_cache[output_cache_index] = *current;
      output_cache_index += 1;
    }
    if (output_cache_index >= output_cache_size) {
      output_cache[output_cache_index] = 0;
   d5e18:	f803 7c44 	strb.w	r7, [r3, #-68]
      DebugLog(output_cache);
   d5e1c:	f000 fafc 	bl	d6418 <DebugLog>
      output_cache_index = 0;
   d5e20:	2300      	movs	r3, #0
    }
    current += 1;
   d5e22:	3501      	adds	r5, #1
   d5e24:	e7ba      	b.n	d5d9c <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0xc>
  }
  if (output_cache_index > 0) {
   d5e26:	b133      	cbz	r3, d5e36 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0xa6>
    output_cache[output_cache_index] = 0;
   d5e28:	a912      	add	r1, sp, #72	; 0x48
   d5e2a:	440b      	add	r3, r1
    DebugLog(output_cache);
   d5e2c:	a801      	add	r0, sp, #4
      output_cache_index = 0;
    }
    current += 1;
  }
  if (output_cache_index > 0) {
    output_cache[output_cache_index] = 0;
   d5e2e:	f803 2c44 	strb.w	r2, [r3, #-68]
    DebugLog(output_cache);
   d5e32:	f000 faf1 	bl	d6418 <DebugLog>
    output_cache_index = 0;
  }
  DebugLog("\r\n");
   d5e36:	4803      	ldr	r0, [pc, #12]	; (d5e44 <_ZN6tflite18MicroErrorReporter6ReportEPKcSt9__va_list+0xb4>)
   d5e38:	f000 faee 	bl	d6418 <DebugLog>
}  // namespace

int MicroErrorReporter::Report(const char* format, va_list args) {
  DebugLogPrintf(format, args);
  return 0;
}
   d5e3c:	2000      	movs	r0, #0
   d5e3e:	b013      	add	sp, #76	; 0x4c
   d5e40:	bdf0      	pop	{r4, r5, r6, r7, pc}
   d5e42:	bf00      	nop
   d5e44:	000e948c 	.word	0x000e948c

000d5e48 <_ZN6tflite12_GLOBAL__N_118StackDataAllocator8AllocateEj>:
namespace {
const int kStackDataAllocatorSize = 128;
class StackDataAllocator : public BuiltinDataAllocator {
 public:
  void* Allocate(size_t size) override {
    if (size > kStackDataAllocatorSize) {
   d5e48:	2980      	cmp	r1, #128	; 0x80
      return nullptr;
    } else {
      return data_;
   d5e4a:	bf94      	ite	ls
   d5e4c:	3004      	addls	r0, #4
const int kStackDataAllocatorSize = 128;
class StackDataAllocator : public BuiltinDataAllocator {
 public:
  void* Allocate(size_t size) override {
    if (size > kStackDataAllocatorSize) {
      return nullptr;
   d5e4e:	2000      	movhi	r0, #0
    } else {
      return data_;
    }
  }
   d5e50:	4770      	bx	lr

000d5e52 <_ZN6tflite12_GLOBAL__N_118StackDataAllocator10DeallocateEPv>:
  void Deallocate(void* data) override {
   d5e52:	4770      	bx	lr

000d5e54 <_ZN6tflite12_GLOBAL__N_118StackDataAllocatorD1Ev>:
#include "tensorflow/lite/experimental/micro/compatibility.h"

namespace tflite {
namespace {
const int kStackDataAllocatorSize = 128;
class StackDataAllocator : public BuiltinDataAllocator {
   d5e54:	4770      	bx	lr

000d5e56 <_ZN6tflite12_GLOBAL__N_113ReportOpErrorEP13TfLiteContextPKcz>:
  } else {
    return EnumNameBuiltinOperator(BuiltinOperator(registration->builtin_code));
  }
}

void ReportOpError(struct TfLiteContext* context, const char* format, ...) {
   d5e56:	b40e      	push	{r1, r2, r3}
   d5e58:	b503      	push	{r0, r1, lr}
  MicroInterpreter* interpreter =
      static_cast<MicroInterpreter*>(context->impl_);
   d5e5a:	68c3      	ldr	r3, [r0, #12]
   d5e5c:	6898      	ldr	r0, [r3, #8]
  } else {
    return EnumNameBuiltinOperator(BuiltinOperator(registration->builtin_code));
  }
}

void ReportOpError(struct TfLiteContext* context, const char* format, ...) {
   d5e5e:	aa03      	add	r2, sp, #12
  MicroInterpreter* interpreter =
      static_cast<MicroInterpreter*>(context->impl_);
  va_list args;
  va_start(args, format);
  interpreter->error_reporter()->Report(format, args);
   d5e60:	6803      	ldr	r3, [r0, #0]
  } else {
    return EnumNameBuiltinOperator(BuiltinOperator(registration->builtin_code));
  }
}

void ReportOpError(struct TfLiteContext* context, const char* format, ...) {
   d5e62:	f852 1b04 	ldr.w	r1, [r2], #4
  MicroInterpreter* interpreter =
      static_cast<MicroInterpreter*>(context->impl_);
  va_list args;
  va_start(args, format);
   d5e66:	9201      	str	r2, [sp, #4]
  interpreter->error_reporter()->Report(format, args);
   d5e68:	689b      	ldr	r3, [r3, #8]
   d5e6a:	4798      	blx	r3
  va_end(args);
}
   d5e6c:	b002      	add	sp, #8
   d5e6e:	f85d eb04 	ldr.w	lr, [sp], #4
   d5e72:	b003      	add	sp, #12
   d5e74:	4770      	bx	lr

000d5e76 <_ZN6tflite12_GLOBAL__N_118StackDataAllocatorD0Ev>:
#include "tensorflow/lite/experimental/micro/compatibility.h"

namespace tflite {
namespace {
const int kStackDataAllocatorSize = 128;
class StackDataAllocator : public BuiltinDataAllocator {
   d5e76:	b510      	push	{r4, lr}
   d5e78:	2184      	movs	r1, #132	; 0x84
   d5e7a:	4604      	mov	r4, r0
   d5e7c:	f00e ffbf 	bl	e4dfe <_ZdlPvj>
   d5e80:	4620      	mov	r0, r4
   d5e82:	bd10      	pop	{r4, pc}

000d5e84 <_ZN6tflite16MicroInterpreter15AllocateTensorsEv>:
TfLiteStatus MicroInterpreter::RegisterPreallocatedInput(uint8_t* buffer,
                                                         size_t input_index) {
  return allocator_.RegisterPreallocatedInput(buffer, input_index);
}

TfLiteStatus MicroInterpreter::AllocateTensors() {
   d5e84:	b510      	push	{r4, lr}
   d5e86:	4604      	mov	r4, r0
  TfLiteStatus status = allocator_.AllocateTensors();
   d5e88:	3044      	adds	r0, #68	; 0x44
   d5e8a:	f7ff fe13 	bl	d5ab4 <_ZN6tflite14MicroAllocator15AllocateTensorsEv>
   d5e8e:	2301      	movs	r3, #1
  TF_LITE_ENSURE_OK(&context_, status);
   d5e90:	b910      	cbnz	r0, d5e98 <_ZN6tflite16MicroInterpreter15AllocateTensorsEv+0x14>
  tensors_allocated_ = true;
   d5e92:	f884 3070 	strb.w	r3, [r4, #112]	; 0x70
  return kTfLiteOk;
   d5e96:	bd10      	pop	{r4, pc}
  return allocator_.RegisterPreallocatedInput(buffer, input_index);
}

TfLiteStatus MicroInterpreter::AllocateTensors() {
  TfLiteStatus status = allocator_.AllocateTensors();
  TF_LITE_ENSURE_OK(&context_, status);
   d5e98:	4618      	mov	r0, r3
  tensors_allocated_ = true;
  return kTfLiteOk;
}
   d5e9a:	bd10      	pop	{r4, pc}

000d5e9c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>:
// "tables" use an offset table (possibly shared) that allows fields to be
// omitted and added at will, but uses an extra indirection to read.
class Table {
 public:
  const uint8_t *GetVTable() const {
    return data_ - ReadScalar<soffset_t>(data_);
   d5e9c:	6803      	ldr	r3, [r0, #0]
   d5e9e:	1ac3      	subs	r3, r0, r3
    auto vtable = GetVTable();
    // The first element is the size of the vtable (fields + type id + itself).
    auto vtsize = ReadScalar<voffset_t>(vtable);
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
   d5ea0:	881a      	ldrh	r2, [r3, #0]
   d5ea2:	428a      	cmp	r2, r1
   d5ea4:	bf8c      	ite	hi
   d5ea6:	5a5b      	ldrhhi	r3, [r3, r1]
   d5ea8:	2300      	movls	r3, #0
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
  }

  template<typename P> P GetPointer(voffset_t field) {
    auto field_offset = GetOptionalFieldOffset(field);
    auto p = data_ + field_offset;
   d5eaa:	18c2      	adds	r2, r0, r3
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
   d5eac:	b113      	cbz	r3, d5eb4 <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t+0x18>
   d5eae:	58c3      	ldr	r3, [r0, r3]
   d5eb0:	18d0      	adds	r0, r2, r3
   d5eb2:	4770      	bx	lr
   d5eb4:	4618      	mov	r0, r3
  }
   d5eb6:	4770      	bx	lr

000d5eb8 <_ZN6tflite16MicroInterpreterC1EPKNS_5ModelERKNS_10OpResolverEPhjPNS_13ErrorReporterE>:
  va_end(args);
}

}  // namespace

MicroInterpreter::MicroInterpreter(const Model* model,
   d5eb8:	e92d 41f3 	stmdb	sp!, {r0, r1, r4, r5, r6, r7, r8, lr}
   d5ebc:	4604      	mov	r4, r0
   d5ebe:	9d09      	ldr	r5, [sp, #36]	; 0x24
      op_resolver_(op_resolver),
      error_reporter_(error_reporter),
      context_(),
      allocator_(&context_, model_, tensor_arena, tensor_arena_size,
                 error_reporter_),
      tensors_allocated_(false) {
   d5ec0:	6021      	str	r1, [r4, #0]
   d5ec2:	f100 070c 	add.w	r7, r0, #12
  va_end(args);
}

}  // namespace

MicroInterpreter::MicroInterpreter(const Model* model,
   d5ec6:	460e      	mov	r6, r1
      op_resolver_(op_resolver),
      error_reporter_(error_reporter),
      context_(),
      allocator_(&context_, model_, tensor_arena, tensor_arena_size,
                 error_reporter_),
      tensors_allocated_(false) {
   d5ec8:	6042      	str	r2, [r0, #4]
   d5eca:	6085      	str	r5, [r0, #8]
   d5ecc:	2238      	movs	r2, #56	; 0x38
   d5ece:	2100      	movs	r1, #0
   d5ed0:	4638      	mov	r0, r7
  va_end(args);
}

}  // namespace

MicroInterpreter::MicroInterpreter(const Model* model,
   d5ed2:	4698      	mov	r8, r3
      op_resolver_(op_resolver),
      error_reporter_(error_reporter),
      context_(),
      allocator_(&context_, model_, tensor_arena, tensor_arena_size,
                 error_reporter_),
      tensors_allocated_(false) {
   d5ed4:	f011 fc78 	bl	e77c8 <memset>
   d5ed8:	9b08      	ldr	r3, [sp, #32]
   d5eda:	9300      	str	r3, [sp, #0]
   d5edc:	4632      	mov	r2, r6
   d5ede:	4639      	mov	r1, r7
   d5ee0:	4643      	mov	r3, r8
   d5ee2:	9501      	str	r5, [sp, #4]
   d5ee4:	f104 0044 	add.w	r0, r4, #68	; 0x44
   d5ee8:	2700      	movs	r7, #0
   d5eea:	f7ff fc7d 	bl	d57e8 <_ZN6tflite14MicroAllocatorC1EP13TfLiteContextPKNS_5ModelEPhjPNS_13ErrorReporterE>
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5eee:	4630      	mov	r0, r6
   d5ef0:	f884 7070 	strb.w	r7, [r4, #112]	; 0x70
   d5ef4:	2108      	movs	r1, #8
   d5ef6:	f7ff ffd1 	bl	d5e9c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
  auto* subgraphs = model->subgraphs();
  if (subgraphs->size() != 1) {
   d5efa:	6806      	ldr	r6, [r0, #0]
   d5efc:	2e01      	cmp	r6, #1
   d5efe:	d007      	beq.n	d5f10 <_ZN6tflite16MicroInterpreterC1EPKNS_5ModelERKNS_10OpResolverEPhjPNS_13ErrorReporterE+0x58>
    error_reporter->Report("Only 1 subgraph is currently supported.\n");
   d5f00:	490f      	ldr	r1, [pc, #60]	; (d5f40 <_ZN6tflite16MicroInterpreterC1EPKNS_5ModelERKNS_10OpResolverEPhjPNS_13ErrorReporterE+0x88>)
   d5f02:	4628      	mov	r0, r5
   d5f04:	f7fe fa66 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    initialization_status_ = kTfLiteError;
   d5f08:	2301      	movs	r3, #1
   d5f0a:	f884 3071 	strb.w	r3, [r4, #113]	; 0x71
    return;
   d5f0e:	e013      	b.n	d5f38 <_ZN6tflite16MicroInterpreterC1EPKNS_5ModelERKNS_10OpResolverEPhjPNS_13ErrorReporterE+0x80>
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
   d5f10:	6843      	ldr	r3, [r0, #4]
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d5f12:	1d05      	adds	r5, r0, #4
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
   d5f14:	441d      	add	r5, r3
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5f16:	2104      	movs	r1, #4
  }
  subgraph_ = (*subgraphs)[0];
   d5f18:	67e5      	str	r5, [r4, #124]	; 0x7c
   d5f1a:	4628      	mov	r0, r5
   d5f1c:	f7ff ffbe 	bl	d5e9c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
   d5f20:	210a      	movs	r1, #10
  tensors_ = subgraph_->tensors();
   d5f22:	6760      	str	r0, [r4, #116]	; 0x74
   d5f24:	4628      	mov	r0, r5
   d5f26:	f7ff ffb9 	bl	d5e9c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
  operators_ = subgraph_->operators();

  context_.impl_ = static_cast<void*>(this);
  context_.ReportError = ReportOpError;
   d5f2a:	4b06      	ldr	r3, [pc, #24]	; (d5f44 <_ZN6tflite16MicroInterpreterC1EPKNS_5ModelERKNS_10OpResolverEPhjPNS_13ErrorReporterE+0x8c>)
    initialization_status_ = kTfLiteError;
    return;
  }
  subgraph_ = (*subgraphs)[0];
  tensors_ = subgraph_->tensors();
  operators_ = subgraph_->operators();
   d5f2c:	67a0      	str	r0, [r4, #120]	; 0x78

  context_.impl_ = static_cast<void*>(this);
   d5f2e:	61a4      	str	r4, [r4, #24]
  context_.ReportError = ReportOpError;
   d5f30:	6223      	str	r3, [r4, #32]
  context_.recommended_num_threads = 1;
   d5f32:	6326      	str	r6, [r4, #48]	; 0x30
      if (thisTensor->allocation_type == kTfLiteMmapRo)
        CorrectTensorEndianness(thisTensor);
    }
  }

  initialization_status_ = kTfLiteOk;
   d5f34:	f884 7071 	strb.w	r7, [r4, #113]	; 0x71
}
   d5f38:	4620      	mov	r0, r4
   d5f3a:	b002      	add	sp, #8
   d5f3c:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   d5f40:	000e90d4 	.word	0x000e90d4
   d5f44:	000d5e57 	.word	0x000d5e57

000d5f48 <_ZN6tflite16MicroInterpreter6outputEj>:
    return nullptr;
  }
  return &(context_.tensors[inputs->Get(index)]);
}

TfLiteTensor* MicroInterpreter::output(size_t index) {
   d5f48:	b538      	push	{r3, r4, r5, lr}
   d5f4a:	460d      	mov	r5, r1
   d5f4c:	4604      	mov	r4, r0
   d5f4e:	2108      	movs	r1, #8
   d5f50:	6fc0      	ldr	r0, [r0, #124]	; 0x7c
   d5f52:	f7ff ffa3 	bl	d5e9c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d5f56:	6803      	ldr	r3, [r0, #0]
  const flatbuffers::Vector<int32_t>* outputs = subgraph_->outputs();
  const size_t length = outputs->size();
  if ((index < 0) || (index >= outputs->size())) {
   d5f58:	42ab      	cmp	r3, r5
   d5f5a:	d806      	bhi.n	d5f6a <_ZN6tflite16MicroInterpreter6outputEj+0x22>
    error_reporter_->Report("Output index %d out of range (length is %d)",
                            index, length);
   d5f5c:	462a      	mov	r2, r5
   d5f5e:	4907      	ldr	r1, [pc, #28]	; (d5f7c <_ZN6tflite16MicroInterpreter6outputEj+0x34>)
   d5f60:	68a0      	ldr	r0, [r4, #8]
   d5f62:	f7fe fa37 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return nullptr;
   d5f66:	2000      	movs	r0, #0
   d5f68:	bd38      	pop	{r3, r4, r5, pc}
  }
  return &(context_.tensors[outputs->Get(index)]);
   d5f6a:	4629      	mov	r1, r5
   d5f6c:	f7ff fc06 	bl	d577c <_ZNK11flatbuffers6VectorIlE3GetEm>
   d5f70:	6962      	ldr	r2, [r4, #20]
   d5f72:	2338      	movs	r3, #56	; 0x38
   d5f74:	fb03 2000 	mla	r0, r3, r0, r2
}
   d5f78:	bd38      	pop	{r3, r4, r5, pc}
   d5f7a:	bf00      	nop
   d5f7c:	000e94bb 	.word	0x000e94bb

000d5f80 <_ZN6tflite16MicroInterpreter5inputEj>:
    }
  }
  return status;
}

TfLiteTensor* MicroInterpreter::input(size_t index) {
   d5f80:	b538      	push	{r3, r4, r5, lr}
   d5f82:	460d      	mov	r5, r1
   d5f84:	4604      	mov	r4, r0
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5f86:	2106      	movs	r1, #6
   d5f88:	6fc0      	ldr	r0, [r0, #124]	; 0x7c
   d5f8a:	f7ff ff87 	bl	d5e9c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d5f8e:	6803      	ldr	r3, [r0, #0]
  const flatbuffers::Vector<int32_t>* inputs = subgraph_->inputs();
  const size_t length = inputs->size();
  if ((index < 0) || (index >= length)) {
   d5f90:	429d      	cmp	r5, r3
   d5f92:	d306      	bcc.n	d5fa2 <_ZN6tflite16MicroInterpreter5inputEj+0x22>
    error_reporter_->Report("Input index %d out of range (length is %d)", index,
                            length);
   d5f94:	462a      	mov	r2, r5
   d5f96:	4907      	ldr	r1, [pc, #28]	; (d5fb4 <_ZN6tflite16MicroInterpreter5inputEj+0x34>)
   d5f98:	68a0      	ldr	r0, [r4, #8]
   d5f9a:	f7fe fa1b 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return nullptr;
   d5f9e:	2000      	movs	r0, #0
   d5fa0:	bd38      	pop	{r3, r4, r5, pc}
  }
  return &(context_.tensors[inputs->Get(index)]);
   d5fa2:	4629      	mov	r1, r5
   d5fa4:	f7ff fbea 	bl	d577c <_ZNK11flatbuffers6VectorIlE3GetEm>
   d5fa8:	6962      	ldr	r2, [r4, #20]
   d5faa:	2338      	movs	r3, #56	; 0x38
   d5fac:	fb03 2000 	mla	r0, r3, r0, r2
}
   d5fb0:	bd38      	pop	{r3, r4, r5, pc}
   d5fb2:	bf00      	nop
   d5fb4:	000e94e7 	.word	0x000e94e7

000d5fb8 <_ZN6tflite16MicroInterpreter6InvokeEv>:
  TF_LITE_ENSURE_OK(&context_, status);
  tensors_allocated_ = true;
  return kTfLiteOk;
}

TfLiteStatus MicroInterpreter::Invoke() {
   d5fb8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  if (initialization_status_ != kTfLiteOk) {
   d5fbc:	f890 5071 	ldrb.w	r5, [r0, #113]	; 0x71
  TF_LITE_ENSURE_OK(&context_, status);
  tensors_allocated_ = true;
  return kTfLiteOk;
}

TfLiteStatus MicroInterpreter::Invoke() {
   d5fc0:	b0c5      	sub	sp, #276	; 0x114
   d5fc2:	4604      	mov	r4, r0
  if (initialization_status_ != kTfLiteOk) {
   d5fc4:	b125      	cbz	r5, d5fd0 <_ZN6tflite16MicroInterpreter6InvokeEv+0x18>
    error_reporter_->Report("Invoke() called after initialization failed\n");
   d5fc6:	4976      	ldr	r1, [pc, #472]	; (d61a0 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1e8>)
   d5fc8:	6880      	ldr	r0, [r0, #8]
   d5fca:	f7fe fa03 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
   d5fce:	e0a2      	b.n	d6116 <_ZN6tflite16MicroInterpreter6InvokeEv+0x15e>
    return kTfLiteError;
  }

  // Ensure tensors are allocated before the interpreter is invoked to avoid
  // difficult to debug segfaults.
  if (!tensors_allocated_) {
   d5fd0:	f890 3070 	ldrb.w	r3, [r0, #112]	; 0x70
   d5fd4:	b90b      	cbnz	r3, d5fda <_ZN6tflite16MicroInterpreter6InvokeEv+0x22>
    AllocateTensors();
   d5fd6:	f7ff ff55 	bl	d5e84 <_ZN6tflite16MicroInterpreter15AllocateTensorsEv>
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5fda:	2106      	movs	r1, #6
   d5fdc:	6820      	ldr	r0, [r4, #0]
   d5fde:	f7ff ff5d 	bl	d5e9c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
  }
  TfLiteStatus status = kTfLiteOk;
  auto opcodes = model_->operator_codes();
  for (size_t i = 0; i < operators_->size(); ++i) {
   d5fe2:	2600      	movs	r6, #0
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d5fe4:	1d03      	adds	r3, r0, #4
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d5fe6:	4683      	mov	fp, r0
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d5fe8:	46b2      	mov	sl, r6
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d5fea:	9304      	str	r3, [sp, #16]
   d5fec:	6fa3      	ldr	r3, [r4, #120]	; 0x78
   d5fee:	681a      	ldr	r2, [r3, #0]
   d5ff0:	4296      	cmp	r6, r2
   d5ff2:	f080 80d1 	bcs.w	d6198 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1e0>
   d5ff6:	3304      	adds	r3, #4
template<typename T> struct IndirectHelper<Offset<T>> {
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
   d5ff8:	eb03 0286 	add.w	r2, r3, r6, lsl #2
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
   d5ffc:	f853 3026 	ldr.w	r3, [r3, r6, lsl #2]
   d6000:	18d7      	adds	r7, r2, r3
// "tables" use an offset table (possibly shared) that allows fields to be
// omitted and added at will, but uses an extra indirection to read.
class Table {
 public:
  const uint8_t *GetVTable() const {
    return data_ - ReadScalar<soffset_t>(data_);
   d6002:	58d3      	ldr	r3, [r2, r3]
   d6004:	1afb      	subs	r3, r7, r3
    auto vtable = GetVTable();
    // The first element is the size of the vtable (fields + type id + itself).
    auto vtsize = ReadScalar<voffset_t>(vtable);
    // If the field we're accessing is outside the vtable, we're reading older
    // data, so it's the same as if the offset was 0 (not present).
    return field < vtsize ? ReadScalar<voffset_t>(vtable + field) : 0;
   d6006:	881a      	ldrh	r2, [r3, #0]
   d6008:	2a04      	cmp	r2, #4
   d600a:	d904      	bls.n	d6016 <_ZN6tflite16MicroInterpreter6InvokeEv+0x5e>
   d600c:	889b      	ldrh	r3, [r3, #4]
  }

  template<typename T> T GetField(voffset_t field, T defaultval) const {
    auto field_offset = GetOptionalFieldOffset(field);
    return field_offset ? ReadScalar<T>(data_ + field_offset) : defaultval;
   d600e:	b12b      	cbz	r3, d601c <_ZN6tflite16MicroInterpreter6InvokeEv+0x64>
   d6010:	f857 8003 	ldr.w	r8, [r7, r3]
   d6014:	e003      	b.n	d601e <_ZN6tflite16MicroInterpreter6InvokeEv+0x66>
   d6016:	f04f 0800 	mov.w	r8, #0
   d601a:	e000      	b.n	d601e <_ZN6tflite16MicroInterpreter6InvokeEv+0x66>
   d601c:	4698      	mov	r8, r3
    const auto* op = operators_->Get(i);
    size_t index = op->opcode_index();
    if (index < 0 || index >= opcodes->size()) {
   d601e:	f8db 3000 	ldr.w	r3, [fp]
   d6022:	4543      	cmp	r3, r8
   d6024:	d802      	bhi.n	d602c <_ZN6tflite16MicroInterpreter6InvokeEv+0x74>
      error_reporter_->Report("Missing registration for opcode_index %d\n",
                              index);
   d6026:	4642      	mov	r2, r8
   d6028:	495e      	ldr	r1, [pc, #376]	; (d61a4 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1ec>)
   d602a:	e025      	b.n	d6078 <_ZN6tflite16MicroInterpreter6InvokeEv+0xc0>
template<typename T> struct IndirectHelper<Offset<T>> {
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
   d602c:	9b04      	ldr	r3, [sp, #16]
      return kTfLiteError;
    }
    auto opcode = (*opcodes)[index];
    const TfLiteRegistration* registration = nullptr;
    status = GetRegistrationFromOpCode(opcode, op_resolver_, error_reporter_,
                                       &registration);
   d602e:	68a2      	ldr	r2, [r4, #8]
    return reinterpret_cast<return_type>(p + ReadScalar<uoffset_t>(p));
   d6030:	f853 e028 	ldr.w	lr, [r3, r8, lsl #2]
   d6034:	6861      	ldr	r1, [r4, #4]
template<typename T> struct IndirectHelper<Offset<T>> {
  typedef const T *return_type;
  typedef T *mutable_return_type;
  static const size_t element_stride = sizeof(uoffset_t);
  static return_type Read(const uint8_t *p, uoffset_t i) {
    p += i * sizeof(uoffset_t);
   d6036:	eb03 0088 	add.w	r0, r3, r8, lsl #2
      error_reporter_->Report("Missing registration for opcode_index %d\n",
                              index);
      return kTfLiteError;
    }
    auto opcode = (*opcodes)[index];
    const TfLiteRegistration* registration = nullptr;
   d603a:	ab44      	add	r3, sp, #272	; 0x110
    status = GetRegistrationFromOpCode(opcode, op_resolver_, error_reporter_,
                                       &registration);
   d603c:	4470      	add	r0, lr
      error_reporter_->Report("Missing registration for opcode_index %d\n",
                              index);
      return kTfLiteError;
    }
    auto opcode = (*opcodes)[index];
    const TfLiteRegistration* registration = nullptr;
   d603e:	f843 adf4 	str.w	sl, [r3, #-244]!
    status = GetRegistrationFromOpCode(opcode, op_resolver_, error_reporter_,
                                       &registration);
   d6042:	f7ff f9d3 	bl	d53ec <_ZN6tflite25GetRegistrationFromOpCodeEPKNS_12OperatorCodeERKNS_10OpResolverEPNS_13ErrorReporterEPPK18TfLiteRegistration>
    if (status != kTfLiteOk) {
   d6046:	2800      	cmp	r0, #0
   d6048:	d167      	bne.n	d611a <_ZN6tflite16MicroInterpreter6InvokeEv+0x162>
      return status;
    }
    if (registration == nullptr) {
   d604a:	9b07      	ldr	r3, [sp, #28]
   d604c:	b913      	cbnz	r3, d6054 <_ZN6tflite16MicroInterpreter6InvokeEv+0x9c>
      error_reporter_->Report("Skipping op for opcode_index %d\n", index);
   d604e:	4642      	mov	r2, r8
   d6050:	4955      	ldr	r1, [pc, #340]	; (d61a8 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1f0>)
   d6052:	e011      	b.n	d6078 <_ZN6tflite16MicroInterpreter6InvokeEv+0xc0>
      return kTfLiteError;
    }
    BuiltinOperator op_type =
        static_cast<BuiltinOperator>(registration->builtin_code);
   d6054:	f893 8014 	ldrb.w	r8, [r3, #20]

    if (op_type != BuiltinOperator_CUSTOM && op->custom_options()) {
   d6058:	f1b8 0f20 	cmp.w	r8, #32
   d605c:	d010      	beq.n	d6080 <_ZN6tflite16MicroInterpreter6InvokeEv+0xc8>
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d605e:	210e      	movs	r1, #14
   d6060:	4638      	mov	r0, r7
   d6062:	f7ff ff1b 	bl	d5e9c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
   d6066:	b158      	cbz	r0, d6080 <_ZN6tflite16MicroInterpreter6InvokeEv+0xc8>
  };
  return names;
}

inline const char *EnumNameBuiltinOperator(BuiltinOperator e) {
  if (e < BuiltinOperator_ADD || e > BuiltinOperator_NON_MAX_SUPPRESSION_V5) return "";
   d6068:	f1b8 0f79 	cmp.w	r8, #121	; 0x79
   d606c:	f200 8092 	bhi.w	d6194 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1dc>
  const size_t index = static_cast<size_t>(e);
  return EnumNamesBuiltinOperator()[index];
   d6070:	4b4e      	ldr	r3, [pc, #312]	; (d61ac <_ZN6tflite16MicroInterpreter6InvokeEv+0x1f4>)
   d6072:	f853 2028 	ldr.w	r2, [r3, r8, lsl #2]
      error_reporter_->Report(
          "Unsupported behavior: found builtin operator %s with custom "
          "options.\n",
          EnumNameBuiltinOperator(op_type));
   d6076:	494e      	ldr	r1, [pc, #312]	; (d61b0 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1f8>)
   d6078:	68a0      	ldr	r0, [r4, #8]
   d607a:	f7fe f9ab 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
   d607e:	e04a      	b.n	d6116 <_ZN6tflite16MicroInterpreter6InvokeEv+0x15e>
#include "tensorflow/lite/experimental/micro/compatibility.h"

namespace tflite {
namespace {
const int kStackDataAllocatorSize = 128;
class StackDataAllocator : public BuiltinDataAllocator {
   d6080:	4b4c      	ldr	r3, [pc, #304]	; (d61b4 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1fc>)
   d6082:	9323      	str	r3, [sp, #140]	; 0x8c
   d6084:	210e      	movs	r1, #14
   d6086:	4638      	mov	r0, r7
      return kTfLiteError;
    }
    StackDataAllocator stack_data_allocator;
    const char* custom_data = nullptr;
    size_t custom_data_size = 0;
    unsigned char* builtin_data = nullptr;
   d6088:	f8cd a020 	str.w	sl, [sp, #32]
   d608c:	f7ff ff06 	bl	d5e9c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
    if (op->custom_options()) {
   d6090:	2800      	cmp	r0, #0
   d6092:	d044      	beq.n	d611e <_ZN6tflite16MicroInterpreter6InvokeEv+0x166>
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d6094:	1d03      	adds	r3, r0, #4
  typedef VectorIterator<T, typename IndirectHelper<T>::return_type>
      const_iterator;
  typedef VectorReverseIterator<iterator> reverse_iterator;
  typedef VectorReverseIterator<const_iterator> const_reverse_iterator;

  uoffset_t size() const { return EndianScalar(length_); }
   d6096:	f8d0 9000 	ldr.w	r9, [r0]
    return const_cast<mutable_return_type>(IndirectHelper<T>::Read(Data(), i));
  }

  // The raw data in little endian format. Use with care.
  const uint8_t *Data() const {
    return reinterpret_cast<const uint8_t *>(&length_ + 1);
   d609a:	9303      	str	r3, [sp, #12]
                                        (void**)(&builtin_data)));
    }

    const char* init_data;
    size_t init_data_size;
    if (registration->builtin_code == BuiltinOperator_CUSTOM) {
   d609c:	9807      	ldr	r0, [sp, #28]
   d609e:	6942      	ldr	r2, [r0, #20]
    } else {
      init_data = reinterpret_cast<const char*>(builtin_data);
      init_data_size = 0;
    }
    void* user_data = nullptr;
    if (registration->init) {
   d60a0:	6803      	ldr	r3, [r0, #0]
                                        (void**)(&builtin_data)));
    }

    const char* init_data;
    size_t init_data_size;
    if (registration->builtin_code == BuiltinOperator_CUSTOM) {
   d60a2:	2a20      	cmp	r2, #32
      init_data = custom_data;
      init_data_size = custom_data_size;
    } else {
      init_data = reinterpret_cast<const char*>(builtin_data);
   d60a4:	bf15      	itete	ne
   d60a6:	9908      	ldrne	r1, [sp, #32]
    }

    const char* init_data;
    size_t init_data_size;
    if (registration->builtin_code == BuiltinOperator_CUSTOM) {
      init_data = custom_data;
   d60a8:	9903      	ldreq	r1, [sp, #12]
      init_data_size = custom_data_size;
    } else {
      init_data = reinterpret_cast<const char*>(builtin_data);
      init_data_size = 0;
   d60aa:	2200      	movne	r2, #0

    const char* init_data;
    size_t init_data_size;
    if (registration->builtin_code == BuiltinOperator_CUSTOM) {
      init_data = custom_data;
      init_data_size = custom_data_size;
   d60ac:	464a      	moveq	r2, r9
    } else {
      init_data = reinterpret_cast<const char*>(builtin_data);
      init_data_size = 0;
    }
    void* user_data = nullptr;
    if (registration->init) {
   d60ae:	2b00      	cmp	r3, #0
   d60b0:	d042      	beq.n	d6138 <_ZN6tflite16MicroInterpreter6InvokeEv+0x180>
      user_data = registration->init(&context_, init_data, init_data_size);
   d60b2:	f104 000c 	add.w	r0, r4, #12
   d60b6:	4798      	blx	r3
   d60b8:	4680      	mov	r8, r0
    auto p = data_ + field_offset;
    return field_offset ? reinterpret_cast<P>(p + ReadScalar<uoffset_t>(p))
                        : nullptr;
  }
  template<typename P> P GetPointer(voffset_t field) const {
    return const_cast<Table *>(this)->GetPointer<P>(field);
   d60ba:	2106      	movs	r1, #6
   d60bc:	4638      	mov	r0, r7
   d60be:	f7ff feed 	bl	d5e9c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
   d60c2:	2108      	movs	r1, #8
   d60c4:	9005      	str	r0, [sp, #20]
   d60c6:	4638      	mov	r0, r7
   d60c8:	f7ff fee8 	bl	d5e9c <_ZN11flatbuffers5Table10GetPointerIPKNS_6VectorINS_6OffsetIN6tflite8SubGraphEEEEEEET_t>
    TfLiteIntArray* temporaries_array =
        reinterpret_cast<TfLiteIntArray*>(temporaries_data);
    temporaries_array->size = 0;

    TfLiteNode node;
    node.inputs = inputs_array;
   d60cc:	9a05      	ldr	r2, [sp, #20]
    node.outputs = outputs_array;
    node.temporaries = temporaries_array;
    node.user_data = user_data;
    node.builtin_data = reinterpret_cast<void*>(builtin_data);
    node.custom_initial_data = custom_data;
   d60ce:	9b03      	ldr	r3, [sp, #12]
    TfLiteIntArray* temporaries_array =
        reinterpret_cast<TfLiteIntArray*>(temporaries_data);
    temporaries_array->size = 0;

    TfLiteNode node;
    node.inputs = inputs_array;
   d60d0:	9209      	str	r2, [sp, #36]	; 0x24
    node.outputs = outputs_array;
    node.temporaries = temporaries_array;
   d60d2:	aa12      	add	r2, sp, #72	; 0x48
   d60d4:	920c      	str	r2, [sp, #48]	; 0x30
    node.user_data = user_data;
    node.builtin_data = reinterpret_cast<void*>(builtin_data);
    node.custom_initial_data = custom_data;
   d60d6:	930f      	str	r3, [sp, #60]	; 0x3c
    TfLiteNode node;
    node.inputs = inputs_array;
    node.outputs = outputs_array;
    node.temporaries = temporaries_array;
    node.user_data = user_data;
    node.builtin_data = reinterpret_cast<void*>(builtin_data);
   d60d8:	9a08      	ldr	r2, [sp, #32]
    node.custom_initial_data = custom_data;
    node.custom_initial_data_size = custom_data_size;
    node.delegate = nullptr;
    if (registration->prepare) {
   d60da:	9b07      	ldr	r3, [sp, #28]

    TfLiteNode node;
    node.inputs = inputs_array;
    node.outputs = outputs_array;
    node.temporaries = temporaries_array;
    node.user_data = user_data;
   d60dc:	f8cd 8034 	str.w	r8, [sp, #52]	; 0x34
    node.builtin_data = reinterpret_cast<void*>(builtin_data);
   d60e0:	920e      	str	r2, [sp, #56]	; 0x38
    node.custom_initial_data = custom_data;
    node.custom_initial_data_size = custom_data_size;
   d60e2:	f8cd 9040 	str.w	r9, [sp, #64]	; 0x40
    node.delegate = nullptr;
   d60e6:	f8cd a044 	str.w	sl, [sp, #68]	; 0x44
    if (registration->prepare) {
   d60ea:	689b      	ldr	r3, [r3, #8]

    const int kMaxTemporaries = 16;
    int temporaries_data[kMaxTemporaries + 1];
    TfLiteIntArray* temporaries_array =
        reinterpret_cast<TfLiteIntArray*>(temporaries_data);
    temporaries_array->size = 0;
   d60ec:	f8cd a048 	str.w	sl, [sp, #72]	; 0x48

    TfLiteNode node;
    node.inputs = inputs_array;
    node.outputs = outputs_array;
   d60f0:	900a      	str	r0, [sp, #40]	; 0x28
    node.user_data = user_data;
    node.builtin_data = reinterpret_cast<void*>(builtin_data);
    node.custom_initial_data = custom_data;
    node.custom_initial_data_size = custom_data_size;
    node.delegate = nullptr;
    if (registration->prepare) {
   d60f2:	b35b      	cbz	r3, d614c <_ZN6tflite16MicroInterpreter6InvokeEv+0x194>
      TfLiteStatus prepare_status = registration->prepare(&context_, &node);
   d60f4:	a909      	add	r1, sp, #36	; 0x24
   d60f6:	f104 000c 	add.w	r0, r4, #12
   d60fa:	4798      	blx	r3
      if (prepare_status != kTfLiteOk) {
   d60fc:	4601      	mov	r1, r0
   d60fe:	b328      	cbz	r0, d614c <_ZN6tflite16MicroInterpreter6InvokeEv+0x194>
        error_reporter_->Report(
   d6100:	9a07      	ldr	r2, [sp, #28]
   d6102:	68a0      	ldr	r0, [r4, #8]

  TF_LITE_REMOVE_VIRTUAL_DELETE
};

const char* OpNameFromRegistration(const TfLiteRegistration* registration) {
  if (registration->builtin_code == BuiltinOperator_CUSTOM) {
   d6104:	6953      	ldr	r3, [r2, #20]
   d6106:	2b20      	cmp	r3, #32
   d6108:	d118      	bne.n	d613c <_ZN6tflite16MicroInterpreter6InvokeEv+0x184>
    return registration->custom_name;
   d610a:	6992      	ldr	r2, [r2, #24]
    if (registration->prepare) {
      TfLiteStatus prepare_status = registration->prepare(&context_, &node);
      if (prepare_status != kTfLiteOk) {
        error_reporter_->Report(
            "Node %s (number %d) failed to prepare with status %d",
            OpNameFromRegistration(registration), i, prepare_status);
   d610c:	9100      	str	r1, [sp, #0]
   d610e:	492a      	ldr	r1, [pc, #168]	; (d61b8 <_ZN6tflite16MicroInterpreter6InvokeEv+0x200>)
   d6110:	4633      	mov	r3, r6
    if (registration->invoke) {
      TfLiteStatus invoke_status = registration->invoke(&context_, &node);
      if (invoke_status != kTfLiteOk) {
        error_reporter_->Report(
            "Node %s (number %d) failed to invoke with status %d",
            OpNameFromRegistration(registration), i, invoke_status);
   d6112:	f7fe f95f 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    if (op_type != BuiltinOperator_CUSTOM && op->custom_options()) {
      error_reporter_->Report(
          "Unsupported behavior: found builtin operator %s with custom "
          "options.\n",
          EnumNameBuiltinOperator(op_type));
      return kTfLiteError;
   d6116:	2501      	movs	r5, #1
   d6118:	e03e      	b.n	d6198 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1e0>
      return kTfLiteError;
    }
    auto opcode = (*opcodes)[index];
    const TfLiteRegistration* registration = nullptr;
    status = GetRegistrationFromOpCode(opcode, op_resolver_, error_reporter_,
                                       &registration);
   d611a:	4605      	mov	r5, r0
   d611c:	e03c      	b.n	d6198 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1e0>
    unsigned char* builtin_data = nullptr;
    if (op->custom_options()) {
      custom_data = reinterpret_cast<const char*>(op->custom_options()->data());
      custom_data_size = op->custom_options()->size();
    } else {
      TF_LITE_ENSURE_STATUS(ParseOpData(op, op_type, error_reporter_,
   d611e:	ab08      	add	r3, sp, #32
   d6120:	9300      	str	r3, [sp, #0]
   d6122:	68a2      	ldr	r2, [r4, #8]
   d6124:	ab23      	add	r3, sp, #140	; 0x8c
   d6126:	4641      	mov	r1, r8
   d6128:	4638      	mov	r0, r7
   d612a:	f7fe fa3f 	bl	d45ac <_ZN6tflite11ParseOpDataEPKNS_8OperatorENS_15BuiltinOperatorEPNS_13ErrorReporterEPNS_20BuiltinDataAllocatorEPPv>
   d612e:	2800      	cmp	r0, #0
   d6130:	d1f1      	bne.n	d6116 <_ZN6tflite16MicroInterpreter6InvokeEv+0x15e>
          EnumNameBuiltinOperator(op_type));
      return kTfLiteError;
    }
    StackDataAllocator stack_data_allocator;
    const char* custom_data = nullptr;
    size_t custom_data_size = 0;
   d6132:	4681      	mov	r9, r0
          "options.\n",
          EnumNameBuiltinOperator(op_type));
      return kTfLiteError;
    }
    StackDataAllocator stack_data_allocator;
    const char* custom_data = nullptr;
   d6134:	9003      	str	r0, [sp, #12]
   d6136:	e7b1      	b.n	d609c <_ZN6tflite16MicroInterpreter6InvokeEv+0xe4>
      init_data_size = custom_data_size;
    } else {
      init_data = reinterpret_cast<const char*>(builtin_data);
      init_data_size = 0;
    }
    void* user_data = nullptr;
   d6138:	4698      	mov	r8, r3
   d613a:	e7be      	b.n	d60ba <_ZN6tflite16MicroInterpreter6InvokeEv+0x102>
  };
  return names;
}

inline const char *EnumNameBuiltinOperator(BuiltinOperator e) {
  if (e < BuiltinOperator_ADD || e > BuiltinOperator_NON_MAX_SUPPRESSION_V5) return "";
   d613c:	b2db      	uxtb	r3, r3
   d613e:	2b79      	cmp	r3, #121	; 0x79
  const size_t index = static_cast<size_t>(e);
  return EnumNamesBuiltinOperator()[index];
   d6140:	bf96      	itet	ls
   d6142:	4a1a      	ldrls	r2, [pc, #104]	; (d61ac <_ZN6tflite16MicroInterpreter6InvokeEv+0x1f4>)
  };
  return names;
}

inline const char *EnumNameBuiltinOperator(BuiltinOperator e) {
  if (e < BuiltinOperator_ADD || e > BuiltinOperator_NON_MAX_SUPPRESSION_V5) return "";
   d6144:	4a1d      	ldrhi	r2, [pc, #116]	; (d61bc <_ZN6tflite16MicroInterpreter6InvokeEv+0x204>)
  const size_t index = static_cast<size_t>(e);
  return EnumNamesBuiltinOperator()[index];
   d6146:	f852 2023 	ldrls.w	r2, [r2, r3, lsl #2]
   d614a:	e7df      	b.n	d610c <_ZN6tflite16MicroInterpreter6InvokeEv+0x154>
            OpNameFromRegistration(registration), i, prepare_status);
        return kTfLiteError;
      }
    }

    if (registration->invoke) {
   d614c:	9b07      	ldr	r3, [sp, #28]
   d614e:	68db      	ldr	r3, [r3, #12]
   d6150:	b1bb      	cbz	r3, d6182 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1ca>
      TfLiteStatus invoke_status = registration->invoke(&context_, &node);
   d6152:	a909      	add	r1, sp, #36	; 0x24
   d6154:	f104 000c 	add.w	r0, r4, #12
   d6158:	4798      	blx	r3
      if (invoke_status != kTfLiteOk) {
   d615a:	4601      	mov	r1, r0
   d615c:	b188      	cbz	r0, d6182 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1ca>
        error_reporter_->Report(
   d615e:	9a07      	ldr	r2, [sp, #28]
   d6160:	68a0      	ldr	r0, [r4, #8]

  TF_LITE_REMOVE_VIRTUAL_DELETE
};

const char* OpNameFromRegistration(const TfLiteRegistration* registration) {
  if (registration->builtin_code == BuiltinOperator_CUSTOM) {
   d6162:	6953      	ldr	r3, [r2, #20]
   d6164:	2b20      	cmp	r3, #32
   d6166:	d101      	bne.n	d616c <_ZN6tflite16MicroInterpreter6InvokeEv+0x1b4>
    return registration->custom_name;
   d6168:	6992      	ldr	r2, [r2, #24]
   d616a:	e006      	b.n	d617a <_ZN6tflite16MicroInterpreter6InvokeEv+0x1c2>
  };
  return names;
}

inline const char *EnumNameBuiltinOperator(BuiltinOperator e) {
  if (e < BuiltinOperator_ADD || e > BuiltinOperator_NON_MAX_SUPPRESSION_V5) return "";
   d616c:	b2db      	uxtb	r3, r3
   d616e:	2b79      	cmp	r3, #121	; 0x79
  const size_t index = static_cast<size_t>(e);
  return EnumNamesBuiltinOperator()[index];
   d6170:	bf96      	itet	ls
   d6172:	4a0e      	ldrls	r2, [pc, #56]	; (d61ac <_ZN6tflite16MicroInterpreter6InvokeEv+0x1f4>)
  };
  return names;
}

inline const char *EnumNameBuiltinOperator(BuiltinOperator e) {
  if (e < BuiltinOperator_ADD || e > BuiltinOperator_NON_MAX_SUPPRESSION_V5) return "";
   d6174:	4a11      	ldrhi	r2, [pc, #68]	; (d61bc <_ZN6tflite16MicroInterpreter6InvokeEv+0x204>)
  const size_t index = static_cast<size_t>(e);
  return EnumNamesBuiltinOperator()[index];
   d6176:	f852 2023 	ldrls.w	r2, [r2, r3, lsl #2]
    if (registration->invoke) {
      TfLiteStatus invoke_status = registration->invoke(&context_, &node);
      if (invoke_status != kTfLiteOk) {
        error_reporter_->Report(
            "Node %s (number %d) failed to invoke with status %d",
            OpNameFromRegistration(registration), i, invoke_status);
   d617a:	9100      	str	r1, [sp, #0]
   d617c:	4633      	mov	r3, r6
   d617e:	4910      	ldr	r1, [pc, #64]	; (d61c0 <_ZN6tflite16MicroInterpreter6InvokeEv+0x208>)
   d6180:	e7c7      	b.n	d6112 <_ZN6tflite16MicroInterpreter6InvokeEv+0x15a>
        return kTfLiteError;
      }
    }

    if (registration->free) {
   d6182:	9b07      	ldr	r3, [sp, #28]
   d6184:	685b      	ldr	r3, [r3, #4]
   d6186:	b11b      	cbz	r3, d6190 <_ZN6tflite16MicroInterpreter6InvokeEv+0x1d8>
      registration->free(&context_, user_data);
   d6188:	4641      	mov	r1, r8
   d618a:	f104 000c 	add.w	r0, r4, #12
   d618e:	4798      	blx	r3
  if (!tensors_allocated_) {
    AllocateTensors();
  }
  TfLiteStatus status = kTfLiteOk;
  auto opcodes = model_->operator_codes();
  for (size_t i = 0; i < operators_->size(); ++i) {
   d6190:	3601      	adds	r6, #1
   d6192:	e72b      	b.n	d5fec <_ZN6tflite16MicroInterpreter6InvokeEv+0x34>
  };
  return names;
}

inline const char *EnumNameBuiltinOperator(BuiltinOperator e) {
  if (e < BuiltinOperator_ADD || e > BuiltinOperator_NON_MAX_SUPPRESSION_V5) return "";
   d6194:	4a09      	ldr	r2, [pc, #36]	; (d61bc <_ZN6tflite16MicroInterpreter6InvokeEv+0x204>)
   d6196:	e76e      	b.n	d6076 <_ZN6tflite16MicroInterpreter6InvokeEv+0xbe>
    if (registration->free) {
      registration->free(&context_, user_data);
    }
  }
  return status;
}
   d6198:	4628      	mov	r0, r5
   d619a:	b045      	add	sp, #276	; 0x114
   d619c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   d61a0:	000e9512 	.word	0x000e9512
   d61a4:	000e953f 	.word	0x000e953f
   d61a8:	000e9569 	.word	0x000e9569
   d61ac:	000e8694 	.word	0x000e8694
   d61b0:	000e958a 	.word	0x000e958a
   d61b4:	000e9644 	.word	0x000e9644
   d61b8:	000e95d0 	.word	0x000e95d0
   d61bc:	000e948e 	.word	0x000e948e
   d61c0:	000e9605 	.word	0x000e9605

000d61c4 <_ZNK6tflite22MicroMutableOpResolver6FindOpENS_15BuiltinOperatorEi>:
#include "tensorflow/lite/experimental/micro/micro_mutable_op_resolver.h"

namespace tflite {

const TfLiteRegistration* MicroMutableOpResolver::FindOp(
    tflite::BuiltinOperator op, int version) const {
   d61c4:	b570      	push	{r4, r5, r6, lr}
  for (int i = 0; i < registrations_len_; ++i) {
   d61c6:	f241 0304 	movw	r3, #4100	; 0x1004
   d61ca:	2400      	movs	r4, #0
   d61cc:	58c5      	ldr	r5, [r0, r3]
   d61ce:	4603      	mov	r3, r0
   d61d0:	42ac      	cmp	r4, r5
   d61d2:	da0c      	bge.n	d61ee <_ZNK6tflite22MicroMutableOpResolver6FindOpENS_15BuiltinOperatorEi+0x2a>
    const TfLiteRegistration& registration = registrations_[i];
    if ((registration.builtin_code == op) &&
   d61d4:	699e      	ldr	r6, [r3, #24]
   d61d6:	428e      	cmp	r6, r1
   d61d8:	d106      	bne.n	d61e8 <_ZNK6tflite22MicroMutableOpResolver6FindOpENS_15BuiltinOperatorEi+0x24>
   d61da:	6a1e      	ldr	r6, [r3, #32]
   d61dc:	4296      	cmp	r6, r2
   d61de:	d103      	bne.n	d61e8 <_ZNK6tflite22MicroMutableOpResolver6FindOpENS_15BuiltinOperatorEi+0x24>
namespace tflite {

const TfLiteRegistration* MicroMutableOpResolver::FindOp(
    tflite::BuiltinOperator op, int version) const {
  for (int i = 0; i < registrations_len_; ++i) {
    const TfLiteRegistration& registration = registrations_[i];
   d61e0:	eb00 1044 	add.w	r0, r0, r4, lsl #5
   d61e4:	3004      	adds	r0, #4
   d61e6:	bd70      	pop	{r4, r5, r6, pc}

namespace tflite {

const TfLiteRegistration* MicroMutableOpResolver::FindOp(
    tflite::BuiltinOperator op, int version) const {
  for (int i = 0; i < registrations_len_; ++i) {
   d61e8:	3401      	adds	r4, #1
   d61ea:	3320      	adds	r3, #32
   d61ec:	e7f0      	b.n	d61d0 <_ZNK6tflite22MicroMutableOpResolver6FindOpENS_15BuiltinOperatorEi+0xc>
    if ((registration.builtin_code == op) &&
        (registration.version == version)) {
      return &registration;
    }
  }
  return nullptr;
   d61ee:	2000      	movs	r0, #0
}
   d61f0:	bd70      	pop	{r4, r5, r6, pc}

000d61f2 <_ZNK6tflite22MicroMutableOpResolver6FindOpEPKci>:

const TfLiteRegistration* MicroMutableOpResolver::FindOp(const char* op,
                                                         int version) const {
   d61f2:	e92d 43f8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, lr}
  for (int i = 0; i < registrations_len_; ++i) {
   d61f6:	f241 0304 	movw	r3, #4100	; 0x1004
  }
  return nullptr;
}

const TfLiteRegistration* MicroMutableOpResolver::FindOp(const char* op,
                                                         int version) const {
   d61fa:	4604      	mov	r4, r0
  for (int i = 0; i < registrations_len_; ++i) {
   d61fc:	58c7      	ldr	r7, [r0, r3]
  }
  return nullptr;
}

const TfLiteRegistration* MicroMutableOpResolver::FindOp(const char* op,
                                                         int version) const {
   d61fe:	4688      	mov	r8, r1
   d6200:	4691      	mov	r9, r2
   d6202:	4605      	mov	r5, r0
  for (int i = 0; i < registrations_len_; ++i) {
   d6204:	2600      	movs	r6, #0
   d6206:	42be      	cmp	r6, r7
   d6208:	da12      	bge.n	d6230 <_ZNK6tflite22MicroMutableOpResolver6FindOpEPKci+0x3e>
    const TfLiteRegistration& registration = registrations_[i];
    if ((registration.builtin_code == BuiltinOperator_CUSTOM) &&
   d620a:	69ab      	ldr	r3, [r5, #24]
   d620c:	2b20      	cmp	r3, #32
   d620e:	d10c      	bne.n	d622a <_ZNK6tflite22MicroMutableOpResolver6FindOpEPKci+0x38>
        (strcmp(registration.custom_name, op) == 0) &&
   d6210:	4641      	mov	r1, r8
   d6212:	69e8      	ldr	r0, [r5, #28]
   d6214:	f011 fb08 	bl	e7828 <strcmp>

const TfLiteRegistration* MicroMutableOpResolver::FindOp(const char* op,
                                                         int version) const {
  for (int i = 0; i < registrations_len_; ++i) {
    const TfLiteRegistration& registration = registrations_[i];
    if ((registration.builtin_code == BuiltinOperator_CUSTOM) &&
   d6218:	b938      	cbnz	r0, d622a <_ZNK6tflite22MicroMutableOpResolver6FindOpEPKci+0x38>
        (strcmp(registration.custom_name, op) == 0) &&
   d621a:	6a2b      	ldr	r3, [r5, #32]
   d621c:	454b      	cmp	r3, r9
   d621e:	d104      	bne.n	d622a <_ZNK6tflite22MicroMutableOpResolver6FindOpEPKci+0x38>
}

const TfLiteRegistration* MicroMutableOpResolver::FindOp(const char* op,
                                                         int version) const {
  for (int i = 0; i < registrations_len_; ++i) {
    const TfLiteRegistration& registration = registrations_[i];
   d6220:	eb04 1046 	add.w	r0, r4, r6, lsl #5
   d6224:	3004      	adds	r0, #4
   d6226:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}
  return nullptr;
}

const TfLiteRegistration* MicroMutableOpResolver::FindOp(const char* op,
                                                         int version) const {
  for (int i = 0; i < registrations_len_; ++i) {
   d622a:	3601      	adds	r6, #1
   d622c:	3520      	adds	r5, #32
   d622e:	e7ea      	b.n	d6206 <_ZNK6tflite22MicroMutableOpResolver6FindOpEPKci+0x14>
        (strcmp(registration.custom_name, op) == 0) &&
        (registration.version == version)) {
      return &registration;
    }
  }
  return nullptr;
   d6230:	2000      	movs	r0, #0
}
   d6232:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}

000d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>:

void MicroMutableOpResolver::AddBuiltin(tflite::BuiltinOperator op,
                                        TfLiteRegistration* registration,
                                        int min_version, int max_version) {
  for (int version = min_version; version <= max_version; ++version) {
    if (registrations_len_ >= TFLITE_REGISTRATIONS_MAX) {
   d6236:	f500 5c80 	add.w	ip, r0, #4096	; 0x1000
  return nullptr;
}

void MicroMutableOpResolver::AddBuiltin(tflite::BuiltinOperator op,
                                        TfLiteRegistration* registration,
                                        int min_version, int max_version) {
   d623a:	e92d 47f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, lr}
  for (int version = min_version; version <= max_version; ++version) {
    if (registrations_len_ >= TFLITE_REGISTRATIONS_MAX) {
   d623e:	f10c 0c04 	add.w	ip, ip, #4
  return nullptr;
}

void MicroMutableOpResolver::AddBuiltin(tflite::BuiltinOperator op,
                                        TfLiteRegistration* registration,
                                        int min_version, int max_version) {
   d6242:	4680      	mov	r8, r0
   d6244:	4689      	mov	r9, r1
   d6246:	4692      	mov	sl, r2
   d6248:	461f      	mov	r7, r3
  for (int version = min_version; version <= max_version; ++version) {
   d624a:	9b08      	ldr	r3, [sp, #32]
   d624c:	429f      	cmp	r7, r3
   d624e:	dc15      	bgt.n	d627c <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii+0x46>
    if (registrations_len_ >= TFLITE_REGISTRATIONS_MAX) {
   d6250:	f8dc 6000 	ldr.w	r6, [ip]
   d6254:	2e7f      	cmp	r6, #127	; 0x7f
   d6256:	dc11      	bgt.n	d627c <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii+0x46>
      // TODO(petewarden) - Add error reporting hooks so we can report this!
      return;
    }
    TfLiteRegistration* new_registration = &registrations_[registrations_len_];
    registrations_len_ += 1;
   d6258:	1c73      	adds	r3, r6, #1
   d625a:	f8cc 3000 	str.w	r3, [ip]

    *new_registration = *registration;
   d625e:	4655      	mov	r5, sl
   d6260:	cd0f      	ldmia	r5!, {r0, r1, r2, r3}
   d6262:	eb08 1646 	add.w	r6, r8, r6, lsl #5
   d6266:	1d34      	adds	r4, r6, #4
   d6268:	c40f      	stmia	r4!, {r0, r1, r2, r3}
   d626a:	e895 000f 	ldmia.w	r5, {r0, r1, r2, r3}
   d626e:	e884 000f 	stmia.w	r4, {r0, r1, r2, r3}
    new_registration->builtin_code = op;
    new_registration->version = version;
   d6272:	6237      	str	r7, [r6, #32]
    }
    TfLiteRegistration* new_registration = &registrations_[registrations_len_];
    registrations_len_ += 1;

    *new_registration = *registration;
    new_registration->builtin_code = op;
   d6274:	f8c6 9018 	str.w	r9, [r6, #24]
}

void MicroMutableOpResolver::AddBuiltin(tflite::BuiltinOperator op,
                                        TfLiteRegistration* registration,
                                        int min_version, int max_version) {
  for (int version = min_version; version <= max_version; ++version) {
   d6278:	3701      	adds	r7, #1
   d627a:	e7e6      	b.n	d624a <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii+0x14>
   d627c:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}

000d6280 <_ZN6tflite12ElementCountERK14TfLiteIntArray>:
static const int8_t kAsymmetricInt8Max = 127;
static const int kSymmetricInt8Scale = kAsymmetricInt8Max;

}  // namespace

int ElementCount(const TfLiteIntArray& dims) {
   d6280:	b510      	push	{r4, lr}
   d6282:	4603      	mov	r3, r0
  int result = 1;
  for (int i = 0; i < dims.size; ++i) {
   d6284:	6804      	ldr	r4, [r0, #0]
   d6286:	2200      	movs	r2, #0
static const int kSymmetricInt8Scale = kAsymmetricInt8Max;

}  // namespace

int ElementCount(const TfLiteIntArray& dims) {
  int result = 1;
   d6288:	2001      	movs	r0, #1
  for (int i = 0; i < dims.size; ++i) {
   d628a:	42a2      	cmp	r2, r4
   d628c:	da04      	bge.n	d6298 <_ZN6tflite12ElementCountERK14TfLiteIntArray+0x18>
    result *= dims.data[i];
   d628e:	f853 1f04 	ldr.w	r1, [r3, #4]!

}  // namespace

int ElementCount(const TfLiteIntArray& dims) {
  int result = 1;
  for (int i = 0; i < dims.size; ++i) {
   d6292:	3201      	adds	r2, #1
    result *= dims.data[i];
   d6294:	4348      	muls	r0, r1

}  // namespace

int ElementCount(const TfLiteIntArray& dims) {
  int result = 1;
  for (int i = 0; i < dims.size; ++i) {
   d6296:	e7f8      	b.n	d628a <_ZN6tflite12ElementCountERK14TfLiteIntArray+0xa>
    result *= dims.data[i];
  }
  return result;
}
   d6298:	bd10      	pop	{r4, pc}
	...

000d629c <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf>:

void SignedSymmetricPerChannelQuantize(const float* values,
                                       TfLiteIntArray* dims,
                                       int quantized_dimension,
                                       int8_t* quantized_values,
                                       float* scaling_factors) {
   d629c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d62a0:	ed2d 8b04 	vpush	{d8-d9}
   d62a4:	b087      	sub	sp, #28
   d62a6:	4688      	mov	r8, r1
   d62a8:	4693      	mov	fp, r2
   d62aa:	4682      	mov	sl, r0
  int input_size = ElementCount(*dims);
   d62ac:	4608      	mov	r0, r1

void SignedSymmetricPerChannelQuantize(const float* values,
                                       TfLiteIntArray* dims,
                                       int quantized_dimension,
                                       int8_t* quantized_values,
                                       float* scaling_factors) {
   d62ae:	9302      	str	r3, [sp, #8]
  int input_size = ElementCount(*dims);
   d62b0:	f7ff ffe6 	bl	d6280 <_ZN6tflite12ElementCountERK14TfLiteIntArray>
  int channel_count = dims->data[quantized_dimension];
   d62b4:	eb08 038b 	add.w	r3, r8, fp, lsl #2
   d62b8:	9f14      	ldr	r7, [sp, #80]	; 0x50
   d62ba:	685b      	ldr	r3, [r3, #4]
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
   d62bc:	eddf 9a40 	vldr	s19, [pc, #256]	; d63c0 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x124>
                                       TfLiteIntArray* dims,
                                       int quantized_dimension,
                                       int8_t* quantized_values,
                                       float* scaling_factors) {
  int input_size = ElementCount(*dims);
  int channel_count = dims->data[quantized_dimension];
   d62c0:	9301      	str	r3, [sp, #4]
  int per_channel_size = input_size / channel_count;
  for (int channel = 0; channel < channel_count; channel++) {
   d62c2:	2500      	movs	r5, #0
                                       int quantized_dimension,
                                       int8_t* quantized_values,
                                       float* scaling_factors) {
  int input_size = ElementCount(*dims);
  int channel_count = dims->data[quantized_dimension];
  int per_channel_size = input_size / channel_count;
   d62c4:	fb90 f6f3 	sdiv	r6, r0, r3
  for (int channel = 0; channel < channel_count; channel++) {
   d62c8:	9b01      	ldr	r3, [sp, #4]
   d62ca:	429d      	cmp	r5, r3
   d62cc:	da73      	bge.n	d63b6 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x11a>
   d62ce:	4641      	mov	r1, r8
   d62d0:	2200      	movs	r2, #0
   d62d2:	f04f 0901 	mov.w	r9, #1
    float min = 0;
    float max = 0;
    int stride = 1;
    for (int i = 0; i < quantized_dimension; i++) {
   d62d6:	455a      	cmp	r2, fp
   d62d8:	da05      	bge.n	d62e6 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x4a>
      stride *= dims->data[i];
   d62da:	f851 0f04 	ldr.w	r0, [r1, #4]!
  int per_channel_size = input_size / channel_count;
  for (int channel = 0; channel < channel_count; channel++) {
    float min = 0;
    float max = 0;
    int stride = 1;
    for (int i = 0; i < quantized_dimension; i++) {
   d62de:	3201      	adds	r2, #1
      stride *= dims->data[i];
   d62e0:	fb00 f909 	mul.w	r9, r0, r9
  int per_channel_size = input_size / channel_count;
  for (int channel = 0; channel < channel_count; channel++) {
    float min = 0;
    float max = 0;
    int stride = 1;
    for (int i = 0; i < quantized_dimension; i++) {
   d62e4:	e7f7      	b.n	d62d6 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x3a>
  int input_size = ElementCount(*dims);
  int channel_count = dims->data[quantized_dimension];
  int per_channel_size = input_size / channel_count;
  for (int channel = 0; channel < channel_count; channel++) {
    float min = 0;
    float max = 0;
   d62e6:	ed9f 8a37 	vldr	s16, [pc, #220]	; d63c4 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x128>
      stride *= dims->data[i];
    }
    int channel_stride = per_channel_size / stride;
    // Calculate scales for each channel.
    for (int i = 0; i < per_channel_size; i++) {
      int idx = channel * channel_stride + i * stride;
   d62ea:	fb96 f4f9 	sdiv	r4, r6, r9
   d62ee:	436c      	muls	r4, r5
   d62f0:	ea4f 0089 	mov.w	r0, r9, lsl #2
   d62f4:	eb0a 0284 	add.w	r2, sl, r4, lsl #2
    for (int i = 0; i < quantized_dimension; i++) {
      stride *= dims->data[i];
    }
    int channel_stride = per_channel_size / stride;
    // Calculate scales for each channel.
    for (int i = 0; i < per_channel_size; i++) {
   d62f8:	2100      	movs	r1, #0
                                       float* scaling_factors) {
  int input_size = ElementCount(*dims);
  int channel_count = dims->data[quantized_dimension];
  int per_channel_size = input_size / channel_count;
  for (int channel = 0; channel < channel_count; channel++) {
    float min = 0;
   d62fa:	eef0 8a48 	vmov.f32	s17, s16
    for (int i = 0; i < quantized_dimension; i++) {
      stride *= dims->data[i];
    }
    int channel_stride = per_channel_size / stride;
    // Calculate scales for each channel.
    for (int i = 0; i < per_channel_size; i++) {
   d62fe:	42b1      	cmp	r1, r6
   d6300:	9005      	str	r0, [sp, #20]
   d6302:	9104      	str	r1, [sp, #16]
   d6304:	da18      	bge.n	d6338 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x9c>
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
   d6306:	ed92 9a00 	vldr	s18, [r2]
   d630a:	9203      	str	r2, [sp, #12]
   d630c:	eef0 0a49 	vmov.f32	s1, s18
   d6310:	eeb0 0a68 	vmov.f32	s0, s17
   d6314:	f00f f8d2 	bl	e54bc <fminf>
      max = fmaxf(max, values[idx]);
   d6318:	eef0 0a49 	vmov.f32	s1, s18
    }
    int channel_stride = per_channel_size / stride;
    // Calculate scales for each channel.
    for (int i = 0; i < per_channel_size; i++) {
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
   d631c:	eef0 8a40 	vmov.f32	s17, s0
      max = fmaxf(max, values[idx]);
   d6320:	eeb0 0a48 	vmov.f32	s0, s16
   d6324:	f00f f8ac 	bl	e5480 <fmaxf>
    for (int i = 0; i < quantized_dimension; i++) {
      stride *= dims->data[i];
    }
    int channel_stride = per_channel_size / stride;
    // Calculate scales for each channel.
    for (int i = 0; i < per_channel_size; i++) {
   d6328:	9904      	ldr	r1, [sp, #16]
   d632a:	9a03      	ldr	r2, [sp, #12]
   d632c:	9805      	ldr	r0, [sp, #20]
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
   d632e:	eeb0 8a40 	vmov.f32	s16, s0
    for (int i = 0; i < quantized_dimension; i++) {
      stride *= dims->data[i];
    }
    int channel_stride = per_channel_size / stride;
    // Calculate scales for each channel.
    for (int i = 0; i < per_channel_size; i++) {
   d6332:	3101      	adds	r1, #1
   d6334:	4402      	add	r2, r0
   d6336:	e7e2      	b.n	d62fe <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x62>
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
   d6338:	eef0 0ac8 	vabs.f32	s1, s16
   d633c:	eeb0 0ae8 	vabs.f32	s0, s17
   d6340:	f00f f89e 	bl	e5480 <fmaxf>
   d6344:	ee80 0a29 	vdiv.f32	s0, s0, s19
      int idx = channel * channel_stride + i * stride;
      const int32_t quantized_value =
          static_cast<int32_t>(roundf(values[idx] / scaling_factors[channel]));
      // Clamp: just in case some odd numeric offset.
      quantized_values[idx] = fminf(
          kSymmetricInt8Scale, fmaxf(-kSymmetricInt8Scale, quantized_value));
   d6348:	9b02      	ldr	r3, [sp, #8]
   d634a:	ebc9 0004 	rsb	r0, r9, r4
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
   d634e:	2200      	movs	r2, #0
      int idx = channel * channel_stride + i * stride;
      const int32_t quantized_value =
          static_cast<int32_t>(roundf(values[idx] / scaling_factors[channel]));
      // Clamp: just in case some odd numeric offset.
      quantized_values[idx] = fminf(
          kSymmetricInt8Scale, fmaxf(-kSymmetricInt8Scale, quantized_value));
   d6350:	ebc9 0404 	rsb	r4, r9, r4
   d6354:	0080      	lsls	r0, r0, #2
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
    for (int i = 0; i < per_channel_size; i++) {
   d6356:	4611      	mov	r1, r2
      int idx = channel * channel_stride + i * stride;
      const int32_t quantized_value =
          static_cast<int32_t>(roundf(values[idx] / scaling_factors[channel]));
      // Clamp: just in case some odd numeric offset.
      quantized_values[idx] = fminf(
          kSymmetricInt8Scale, fmaxf(-kSymmetricInt8Scale, quantized_value));
   d6358:	441c      	add	r4, r3
      int idx = channel * channel_stride + i * stride;
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
   d635a:	eca7 0a01 	vstmia	r7!, {s0}
    for (int i = 0; i < per_channel_size; i++) {
   d635e:	42b1      	cmp	r1, r6
   d6360:	444a      	add	r2, r9
   d6362:	9105      	str	r1, [sp, #20]
   d6364:	da25      	bge.n	d63b2 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x116>
      int idx = channel * channel_stride + i * stride;
      const int32_t quantized_value =
          static_cast<int32_t>(roundf(values[idx] / scaling_factors[channel]));
   d6366:	eb00 0e82 	add.w	lr, r0, r2, lsl #2
   d636a:	44d6      	add	lr, sl
   d636c:	ed9e 0a00 	vldr	s0, [lr]
   d6370:	ed57 7a01 	vldr	s15, [r7, #-4]
   d6374:	9204      	str	r2, [sp, #16]
   d6376:	ee80 0a27 	vdiv.f32	s0, s0, s15
   d637a:	9003      	str	r0, [sp, #12]
   d637c:	f00f f8d8 	bl	e5530 <roundf>
      // Clamp: just in case some odd numeric offset.
      quantized_values[idx] = fminf(
   d6380:	eefd 0ac0 	vcvt.s32.f32	s1, s0
   d6384:	ed9f 0a10 	vldr	s0, [pc, #64]	; d63c8 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x12c>
   d6388:	eef8 0ae0 	vcvt.f32.s32	s1, s1
   d638c:	f00f f878 	bl	e5480 <fmaxf>
   d6390:	eef0 0a40 	vmov.f32	s1, s0
   d6394:	ed9f 0a0a 	vldr	s0, [pc, #40]	; d63c0 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x124>
   d6398:	f00f f890 	bl	e54bc <fminf>
          kSymmetricInt8Scale, fmaxf(-kSymmetricInt8Scale, quantized_value));
   d639c:	eefd 7ac0 	vcvt.s32.f32	s15, s0
   d63a0:	9a04      	ldr	r2, [sp, #16]
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
    for (int i = 0; i < per_channel_size; i++) {
   d63a2:	9905      	ldr	r1, [sp, #20]
   d63a4:	9803      	ldr	r0, [sp, #12]
      int idx = channel * channel_stride + i * stride;
      const int32_t quantized_value =
          static_cast<int32_t>(roundf(values[idx] / scaling_factors[channel]));
      // Clamp: just in case some odd numeric offset.
      quantized_values[idx] = fminf(
          kSymmetricInt8Scale, fmaxf(-kSymmetricInt8Scale, quantized_value));
   d63a6:	ee17 ea90 	vmov	lr, s15
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
    for (int i = 0; i < per_channel_size; i++) {
   d63aa:	3101      	adds	r1, #1
      int idx = channel * channel_stride + i * stride;
      const int32_t quantized_value =
          static_cast<int32_t>(roundf(values[idx] / scaling_factors[channel]));
      // Clamp: just in case some odd numeric offset.
      quantized_values[idx] = fminf(
          kSymmetricInt8Scale, fmaxf(-kSymmetricInt8Scale, quantized_value));
   d63ac:	f804 e002 	strb.w	lr, [r4, r2]
      min = fminf(min, values[idx]);
      max = fmaxf(max, values[idx]);
    }
    scaling_factors[channel] =
        fmaxf(fabs(min), fabs(max)) / kSymmetricInt8Scale;
    for (int i = 0; i < per_channel_size; i++) {
   d63b0:	e7d5      	b.n	d635e <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0xc2>
                                       int8_t* quantized_values,
                                       float* scaling_factors) {
  int input_size = ElementCount(*dims);
  int channel_count = dims->data[quantized_dimension];
  int per_channel_size = input_size / channel_count;
  for (int channel = 0; channel < channel_count; channel++) {
   d63b2:	3501      	adds	r5, #1
   d63b4:	e788      	b.n	d62c8 <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf+0x2c>
      // Clamp: just in case some odd numeric offset.
      quantized_values[idx] = fminf(
          kSymmetricInt8Scale, fmaxf(-kSymmetricInt8Scale, quantized_value));
    }
  }
}
   d63b6:	b007      	add	sp, #28
   d63b8:	ecbd 8b04 	vpop	{d8-d9}
   d63bc:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   d63c0:	42fe0000 	.word	0x42fe0000
   d63c4:	00000000 	.word	0x00000000
   d63c8:	c2fe0000 	.word	0xc2fe0000

000d63cc <_ZN6tflite19SymmetricDequantizeEPKaifPf>:
                          scaling_factor);
}

void SymmetricDequantize(const int8_t* values, const int size,
                         const float dequantization_scale,
                         float* dequantized_values) {
   d63cc:	b510      	push	{r4, lr}
   d63ce:	4603      	mov	r3, r0
  for (int i = 0; i < size; ++i) {
   d63d0:	1a1c      	subs	r4, r3, r0
   d63d2:	42a1      	cmp	r1, r4
   d63d4:	dd0a      	ble.n	d63ec <_ZN6tflite19SymmetricDequantizeEPKaifPf+0x20>
    dequantized_values[i] = values[i] * dequantization_scale;
   d63d6:	f913 4b01 	ldrsb.w	r4, [r3], #1
   d63da:	ee07 4a90 	vmov	s15, r4
   d63de:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   d63e2:	ee67 7a80 	vmul.f32	s15, s15, s0
   d63e6:	ece2 7a01 	vstmia	r2!, {s15}
}

void SymmetricDequantize(const int8_t* values, const int size,
                         const float dequantization_scale,
                         float* dequantized_values) {
  for (int i = 0; i < size; ++i) {
   d63ea:	e7f1      	b.n	d63d0 <_ZN6tflite19SymmetricDequantizeEPKaifPf+0x4>
   d63ec:	bd10      	pop	{r4, pc}

000d63ee <_ZN6tflite21SimpleMemoryAllocator16AllocateFromTailEjj>:
#include "tensorflow/lite/experimental/micro/memory_helpers.h"

namespace tflite {

uint8_t* SimpleMemoryAllocator::AllocateFromTail(size_t size,
                                                 size_t alignment) {
   d63ee:	b538      	push	{r3, r4, r5, lr}
  uint8_t* previous_free = (data_ + data_size_max_) - data_size_;
   d63f0:	6804      	ldr	r4, [r0, #0]
   d63f2:	6843      	ldr	r3, [r0, #4]
   d63f4:	1b1b      	subs	r3, r3, r4
   d63f6:	6884      	ldr	r4, [r0, #8]
   d63f8:	441c      	add	r4, r3
#include "tensorflow/lite/experimental/micro/memory_helpers.h"

namespace tflite {

uint8_t* SimpleMemoryAllocator::AllocateFromTail(size_t size,
                                                 size_t alignment) {
   d63fa:	4605      	mov	r5, r0
  uint8_t* previous_free = (data_ + data_size_max_) - data_size_;
  uint8_t* current_data = previous_free - size;
  uint8_t* aligned_result = AlignPointerDown(current_data, alignment);
   d63fc:	1a60      	subs	r0, r4, r1
   d63fe:	4611      	mov	r1, r2
   d6400:	f7ff f928 	bl	d5654 <_ZN6tflite16AlignPointerDownEPhj>
  size_t aligned_size = (previous_free - aligned_result);
  if ((data_size_ + aligned_size) > data_size_max_) {
   d6404:	682b      	ldr	r3, [r5, #0]
   d6406:	1a24      	subs	r4, r4, r0
   d6408:	441c      	add	r4, r3
   d640a:	686b      	ldr	r3, [r5, #4]
   d640c:	429c      	cmp	r4, r3
    // TODO(petewarden): Add error reporting beyond returning null!
    return nullptr;
  }
  data_size_ += aligned_size;
   d640e:	bf94      	ite	ls
   d6410:	602c      	strls	r4, [r5, #0]
  uint8_t* current_data = previous_free - size;
  uint8_t* aligned_result = AlignPointerDown(current_data, alignment);
  size_t aligned_size = (previous_free - aligned_result);
  if ((data_size_ + aligned_size) > data_size_max_) {
    // TODO(petewarden): Add error reporting beyond returning null!
    return nullptr;
   d6412:	2000      	movhi	r0, #0
  }
  data_size_ += aligned_size;
  return aligned_result;
}
   d6414:	bd38      	pop	{r3, r4, r5, pc}
	...

000d6418 <DebugLog>:
#define DEBUG_SERIAL_OBJECT (Serial)
#endif

// On Arduino platforms, we set up a serial port and write to it for debug
// logging.
extern "C" void DebugLog(const char* s) {
   d6418:	b538      	push	{r3, r4, r5, lr}
  static bool is_initialized = false;
  if (!is_initialized) {
   d641a:	4c09      	ldr	r4, [pc, #36]	; (d6440 <DebugLog+0x28>)
   d641c:	7823      	ldrb	r3, [r4, #0]
#define DEBUG_SERIAL_OBJECT (Serial)
#endif

// On Arduino platforms, we set up a serial port and write to it for debug
// logging.
extern "C" void DebugLog(const char* s) {
   d641e:	4605      	mov	r5, r0
  static bool is_initialized = false;
  if (!is_initialized) {
   d6420:	b93b      	cbnz	r3, d6432 <DebugLog+0x1a>
    DEBUG_SERIAL_OBJECT.begin(9600);
   d6422:	f00e fbb1 	bl	e4b88 <_Z16_fetch_usbserialv>
   d6426:	f44f 5116 	mov.w	r1, #9600	; 0x2580
   d642a:	f00e fba1 	bl	e4b70 <_ZN9USBSerial5beginEl>
    is_initialized = true;
   d642e:	2301      	movs	r3, #1
   d6430:	7023      	strb	r3, [r4, #0]
  }
  DEBUG_SERIAL_OBJECT.print(s);
   d6432:	f00e fba9 	bl	e4b88 <_Z16_fetch_usbserialv>
   d6436:	4629      	mov	r1, r5
}
   d6438:	e8bd 4038 	ldmia.w	sp!, {r3, r4, r5, lr}
  static bool is_initialized = false;
  if (!is_initialized) {
    DEBUG_SERIAL_OBJECT.begin(9600);
    is_initialized = true;
  }
  DEBUG_SERIAL_OBJECT.print(s);
   d643c:	f00e ba26 	b.w	e488c <_ZN5Print5printEPKc>
   d6440:	2003dcc1 	.word	0x2003dcc1

000d6444 <_GLOBAL__sub_I_DebugLog>:
   d6444:	f00d bde6 	b.w	e4014 <HAL_Pin_Map>

000d6448 <_ZN6tflite3ops5micro3add4InitEP13TfLiteContextPKcj>:
  int32 output_offset;
};

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
   d6448:	2000      	movs	r0, #0
   d644a:	4770      	bx	lr

000d644c <_ZN6tflite3ops5micro3add4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
   d644c:	4770      	bx	lr

000d644e <_ZN6tflite3ops5micro3add7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   d644e:	2000      	movs	r0, #0
   d6450:	4770      	bx	lr

000d6452 <_ZN6tflite12RuntimeShapeD1Ev>:
  bool operator==(const RuntimeShape& comp) const {
    return this->size_ == comp.size_ &&
           std::memcmp(DimsData(), comp.DimsData(), size_ * sizeof(int32)) == 0;
  }

  ~RuntimeShape() {
   d6452:	b510      	push	{r4, lr}
    if (size_ > kMaxSmallSize) {
   d6454:	6803      	ldr	r3, [r0, #0]
   d6456:	2b04      	cmp	r3, #4
  bool operator==(const RuntimeShape& comp) const {
    return this->size_ == comp.size_ &&
           std::memcmp(DimsData(), comp.DimsData(), size_ * sizeof(int32)) == 0;
  }

  ~RuntimeShape() {
   d6458:	4604      	mov	r4, r0
    if (size_ > kMaxSmallSize) {
   d645a:	dd03      	ble.n	d6464 <_ZN6tflite12RuntimeShapeD1Ev+0x12>
#ifdef TF_LITE_STATIC_MEMORY
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
   d645c:	6840      	ldr	r0, [r0, #4]
   d645e:	b108      	cbz	r0, d6464 <_ZN6tflite12RuntimeShapeD1Ev+0x12>
   d6460:	f7fd fe21 	bl	d40a6 <_ZdaPv>
#endif  // TF_LITE_STATIC_MEMORY
    }
  }
   d6464:	4620      	mov	r0, r4
   d6466:	bd10      	pop	{r4, pc}

000d6468 <_ZNK6tflite12RuntimeShape4DimsEi>:

  inline int32 DimensionsCount() const { return size_; }
  inline int32 Dims(int i) const {
    TFLITE_DCHECK_GE(i, 0);
   d6468:	2900      	cmp	r1, #0
#endif  // TF_LITE_STATIC_MEMORY
    }
  }

  inline int32 DimensionsCount() const { return size_; }
  inline int32 Dims(int i) const {
   d646a:	b508      	push	{r3, lr}
    TFLITE_DCHECK_GE(i, 0);
   d646c:	da01      	bge.n	d6472 <_ZNK6tflite12RuntimeShape4DimsEi+0xa>
   d646e:	f00d ff6d 	bl	e434c <abort>
    TFLITE_DCHECK_LT(i, size_);
   d6472:	6803      	ldr	r3, [r0, #0]
   d6474:	428b      	cmp	r3, r1
   d6476:	ddfa      	ble.n	d646e <_ZNK6tflite12RuntimeShape4DimsEi+0x6>
    return size_ > kMaxSmallSize ? dims_pointer_[i] : dims_[i];
   d6478:	2b04      	cmp	r3, #4
   d647a:	bfc7      	ittee	gt
   d647c:	6843      	ldrgt	r3, [r0, #4]
   d647e:	f853 0021 	ldrgt.w	r0, [r3, r1, lsl #2]
   d6482:	eb00 0081 	addle.w	r0, r0, r1, lsl #2
   d6486:	6840      	ldrle	r0, [r0, #4]
  }
   d6488:	bd08      	pop	{r3, pc}

000d648a <_ZN6tflite12RuntimeShape6SetDimEil>:
  inline void SetDim(int i, int32 val) {
    TFLITE_DCHECK_GE(i, 0);
   d648a:	2900      	cmp	r1, #0
  inline int32 Dims(int i) const {
    TFLITE_DCHECK_GE(i, 0);
    TFLITE_DCHECK_LT(i, size_);
    return size_ > kMaxSmallSize ? dims_pointer_[i] : dims_[i];
  }
  inline void SetDim(int i, int32 val) {
   d648c:	b508      	push	{r3, lr}
    TFLITE_DCHECK_GE(i, 0);
   d648e:	da01      	bge.n	d6494 <_ZN6tflite12RuntimeShape6SetDimEil+0xa>
   d6490:	f00d ff5c 	bl	e434c <abort>
    TFLITE_DCHECK_LT(i, size_);
   d6494:	6803      	ldr	r3, [r0, #0]
   d6496:	428b      	cmp	r3, r1
   d6498:	ddfa      	ble.n	d6490 <_ZN6tflite12RuntimeShape6SetDimEil+0x6>
    if (size_ > kMaxSmallSize) {
   d649a:	2b04      	cmp	r3, #4
      dims_pointer_[i] = val;
   d649c:	bfcb      	itete	gt
   d649e:	6843      	ldrgt	r3, [r0, #4]
    } else {
      dims_[i] = val;
   d64a0:	eb00 0081 	addle.w	r0, r0, r1, lsl #2
  }
  inline void SetDim(int i, int32 val) {
    TFLITE_DCHECK_GE(i, 0);
    TFLITE_DCHECK_LT(i, size_);
    if (size_ > kMaxSmallSize) {
      dims_pointer_[i] = val;
   d64a4:	f843 2021 	strgt.w	r2, [r3, r1, lsl #2]
    } else {
      dims_[i] = val;
   d64a8:	6042      	strle	r2, [r0, #4]
   d64aa:	bd08      	pop	{r3, pc}

000d64ac <_ZN6tflite12RuntimeShape6ResizeEi>:
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  // The caller must ensure that the shape is no bigger than 4-D.
  inline const int32* DimsDataUpTo4D() const { return dims_; }

  inline void Resize(int dimensions_count) {
   d64ac:	b538      	push	{r3, r4, r5, lr}
    if (size_ > kMaxSmallSize) {
   d64ae:	6803      	ldr	r3, [r0, #0]
   d64b0:	2b04      	cmp	r3, #4
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  // The caller must ensure that the shape is no bigger than 4-D.
  inline const int32* DimsDataUpTo4D() const { return dims_; }

  inline void Resize(int dimensions_count) {
   d64b2:	4605      	mov	r5, r0
   d64b4:	460c      	mov	r4, r1
    if (size_ > kMaxSmallSize) {
   d64b6:	dd03      	ble.n	d64c0 <_ZN6tflite12RuntimeShape6ResizeEi+0x14>
#ifdef TF_LITE_STATIC_MEMORY
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
   d64b8:	6840      	ldr	r0, [r0, #4]
   d64ba:	b108      	cbz	r0, d64c0 <_ZN6tflite12RuntimeShape6ResizeEi+0x14>
   d64bc:	f7fd fdf3 	bl	d40a6 <_ZdaPv>
#endif  // TF_LITE_STATIC_MEMORY
    }
    size_ = dimensions_count;
    if (dimensions_count > kMaxSmallSize) {
   d64c0:	2c04      	cmp	r4, #4
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
#endif  // TF_LITE_STATIC_MEMORY
    }
    size_ = dimensions_count;
   d64c2:	602c      	str	r4, [r5, #0]
    if (dimensions_count > kMaxSmallSize) {
   d64c4:	dd08      	ble.n	d64d8 <_ZN6tflite12RuntimeShape6ResizeEi+0x2c>
#ifdef TF_LITE_STATIC_MEMORY
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      dims_pointer_ = new int32[dimensions_count];
   d64c6:	f1b4 5ffe 	cmp.w	r4, #532676608	; 0x1fc00000
   d64ca:	bfd4      	ite	le
   d64cc:	00a0      	lslle	r0, r4, #2
   d64ce:	f04f 30ff 	movgt.w	r0, #4294967295	; 0xffffffff
   d64d2:	f7fd fde4 	bl	d409e <_Znaj>
   d64d6:	6068      	str	r0, [r5, #4]
   d64d8:	bd38      	pop	{r3, r4, r5, pc}

000d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>:

 private:
  // For use only by ExtendedShape(), written to guarantee (return-value) copy
  // elision in C++17.
  // This creates a shape padded to the desired size with the specified value.
  RuntimeShape(int new_shape_size, const RuntimeShape& shape, int pad_value)
   d64da:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
      : size_(0) {
   d64de:	2500      	movs	r5, #0
   d64e0:	6005      	str	r5, [r0, #0]

 private:
  // For use only by ExtendedShape(), written to guarantee (return-value) copy
  // elision in C++17.
  // This creates a shape padded to the desired size with the specified value.
  RuntimeShape(int new_shape_size, const RuntimeShape& shape, int pad_value)
   d64e2:	4698      	mov	r8, r3
      : size_(0) {
    // If the following check fails, it is likely because a 4D-only kernel is
    // being used with an array of larger dimension count.
    TFLITE_CHECK_GE(new_shape_size, shape.DimensionsCount());
   d64e4:	6813      	ldr	r3, [r2, #0]
   d64e6:	428b      	cmp	r3, r1

 private:
  // For use only by ExtendedShape(), written to guarantee (return-value) copy
  // elision in C++17.
  // This creates a shape padded to the desired size with the specified value.
  RuntimeShape(int new_shape_size, const RuntimeShape& shape, int pad_value)
   d64e8:	4606      	mov	r6, r0
   d64ea:	460f      	mov	r7, r1
   d64ec:	4614      	mov	r4, r2
      : size_(0) {
    // If the following check fails, it is likely because a 4D-only kernel is
    // being used with an array of larger dimension count.
    TFLITE_CHECK_GE(new_shape_size, shape.DimensionsCount());
   d64ee:	dd01      	ble.n	d64f4 <_ZN6tflite12RuntimeShapeC1EiRKS0_i+0x1a>
   d64f0:	f00d ff2c 	bl	e434c <abort>
    Resize(new_shape_size);
   d64f4:	f7ff ffda 	bl	d64ac <_ZN6tflite12RuntimeShape6ResizeEi>
    const int size_increase = new_shape_size - shape.DimensionsCount();
   d64f8:	6820      	ldr	r0, [r4, #0]
   d64fa:	1a3f      	subs	r7, r7, r0
    for (int i = 0; i < size_increase; ++i) {
   d64fc:	42bd      	cmp	r5, r7
   d64fe:	da06      	bge.n	d650e <_ZN6tflite12RuntimeShapeC1EiRKS0_i+0x34>
      SetDim(i, pad_value);
   d6500:	4629      	mov	r1, r5
   d6502:	4642      	mov	r2, r8
   d6504:	4630      	mov	r0, r6
   d6506:	f7ff ffc0 	bl	d648a <_ZN6tflite12RuntimeShape6SetDimEil>
    // If the following check fails, it is likely because a 4D-only kernel is
    // being used with an array of larger dimension count.
    TFLITE_CHECK_GE(new_shape_size, shape.DimensionsCount());
    Resize(new_shape_size);
    const int size_increase = new_shape_size - shape.DimensionsCount();
    for (int i = 0; i < size_increase; ++i) {
   d650a:	3501      	adds	r5, #1
   d650c:	e7f6      	b.n	d64fc <_ZN6tflite12RuntimeShapeC1EiRKS0_i+0x22>
      dims_[i] = val;
    }
  }

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d650e:	6833      	ldr	r3, [r6, #0]
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d6510:	6822      	ldr	r2, [r4, #0]
      dims_[i] = val;
    }
  }

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d6512:	2b04      	cmp	r3, #4
   d6514:	bfcc      	ite	gt
   d6516:	6870      	ldrgt	r0, [r6, #4]
   d6518:	1d30      	addle	r0, r6, #4
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d651a:	2a04      	cmp	r2, #4
    Resize(new_shape_size);
    const int size_increase = new_shape_size - shape.DimensionsCount();
    for (int i = 0; i < size_increase; ++i) {
      SetDim(i, pad_value);
    }
    std::memcpy(DimsData() + size_increase, shape.DimsData(),
   d651c:	eb00 0087 	add.w	r0, r0, r7, lsl #2

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d6520:	bfcc      	ite	gt
   d6522:	6861      	ldrgt	r1, [r4, #4]
   d6524:	1d21      	addle	r1, r4, #4
    const int size_increase = new_shape_size - shape.DimensionsCount();
    for (int i = 0; i < size_increase; ++i) {
      SetDim(i, pad_value);
    }
    std::memcpy(DimsData() + size_increase, shape.DimsData(),
                sizeof(int32) * shape.DimensionsCount());
   d6526:	0092      	lsls	r2, r2, #2
   d6528:	f011 f943 	bl	e77b2 <memcpy>
  }
   d652c:	4630      	mov	r0, r6
   d652e:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}

000d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>:
    }
  }
  return offset;
}

inline int Offset(const RuntimeShape& shape, int i0, int i1, int i2, int i3) {
   d6532:	b570      	push	{r4, r5, r6, lr}
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), 4);
   d6534:	6805      	ldr	r5, [r0, #0]
    }
  }
  return offset;
}

inline int Offset(const RuntimeShape& shape, int i0, int i1, int i2, int i3) {
   d6536:	9c04      	ldr	r4, [sp, #16]
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), 4);
   d6538:	2d04      	cmp	r5, #4
   d653a:	d001      	beq.n	d6540 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xe>
   d653c:	f00d ff06 	bl	e434c <abort>
  const int* dims_data = reinterpret_cast<const int*>(shape.DimsDataUpTo4D());
  TFLITE_DCHECK(i0 >= 0 && i0 < dims_data[0]);
   d6540:	2900      	cmp	r1, #0
   d6542:	dbfb      	blt.n	d653c <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
   d6544:	6845      	ldr	r5, [r0, #4]
   d6546:	42a9      	cmp	r1, r5
   d6548:	daf8      	bge.n	d653c <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
  TFLITE_DCHECK(i1 >= 0 && i1 < dims_data[1]);
   d654a:	2a00      	cmp	r2, #0
   d654c:	dbf6      	blt.n	d653c <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
   d654e:	6886      	ldr	r6, [r0, #8]
   d6550:	42b2      	cmp	r2, r6
   d6552:	daf3      	bge.n	d653c <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
  TFLITE_DCHECK(i2 >= 0 && i2 < dims_data[2]);
   d6554:	2b00      	cmp	r3, #0
   d6556:	dbf1      	blt.n	d653c <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
   d6558:	68c5      	ldr	r5, [r0, #12]
   d655a:	42ab      	cmp	r3, r5
   d655c:	daee      	bge.n	d653c <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
  TFLITE_DCHECK(i3 >= 0 && i3 < dims_data[3]);
   d655e:	2c00      	cmp	r4, #0
   d6560:	dbec      	blt.n	d653c <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
   d6562:	6900      	ldr	r0, [r0, #16]
   d6564:	4284      	cmp	r4, r0
   d6566:	dae9      	bge.n	d653c <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii+0xa>
  return ((i0 * dims_data[1] + i1) * dims_data[2] + i2) * dims_data[3] + i3;
   d6568:	fb06 2101 	mla	r1, r6, r1, r2
   d656c:	fb05 3301 	mla	r3, r5, r1, r3
}
   d6570:	fb00 4003 	mla	r0, r0, r3, r4
   d6574:	bd70      	pop	{r4, r5, r6, pc}

000d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>:
  return shape.FlatSize();
}

inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0,
                            const RuntimeShape& check_shape_1) {
   d6576:	e92d 43f8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, lr}
   d657a:	6805      	ldr	r5, [r0, #0]
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   d657c:	680b      	ldr	r3, [r1, #0]
   d657e:	429d      	cmp	r5, r3
  return shape.FlatSize();
}

inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0,
                            const RuntimeShape& check_shape_1) {
   d6580:	4604      	mov	r4, r0
   d6582:	4688      	mov	r8, r1
   d6584:	4617      	mov	r7, r2
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   d6586:	d101      	bne.n	d658c <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x16>
   d6588:	2600      	movs	r6, #0
   d658a:	e00d      	b.n	d65a8 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x32>
   d658c:	f00d fede 	bl	e434c <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   d6590:	4631      	mov	r1, r6
   d6592:	4620      	mov	r0, r4
   d6594:	f7ff ff68 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d6598:	4631      	mov	r1, r6
   d659a:	4681      	mov	r9, r0
   d659c:	4640      	mov	r0, r8
   d659e:	f7ff ff63 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d65a2:	4581      	cmp	r9, r0
   d65a4:	d1f2      	bne.n	d658c <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x16>
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0,
                            const RuntimeShape& check_shape_1) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   d65a6:	3601      	adds	r6, #1
   d65a8:	42ae      	cmp	r6, r5
   d65aa:	dbf1      	blt.n	d6590 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x1a>

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   d65ac:	683b      	ldr	r3, [r7, #0]
   d65ae:	429d      	cmp	r5, r3
   d65b0:	d1ec      	bne.n	d658c <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x16>
   d65b2:	2600      	movs	r6, #0
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   d65b4:	42b5      	cmp	r5, r6
   d65b6:	dd0c      	ble.n	d65d2 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x5c>
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   d65b8:	4631      	mov	r1, r6
   d65ba:	4620      	mov	r0, r4
   d65bc:	f7ff ff54 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d65c0:	4631      	mov	r1, r6
   d65c2:	4680      	mov	r8, r0
   d65c4:	4638      	mov	r0, r7
   d65c6:	f7ff ff4f 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d65ca:	4580      	cmp	r8, r0
   d65cc:	d1de      	bne.n	d658c <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x16>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   d65ce:	3601      	adds	r6, #1
   d65d0:	e7f0      	b.n	d65b4 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x3e>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d65d2:	2d04      	cmp	r5, #4
   d65d4:	bfcc      	ite	gt
   d65d6:	6864      	ldrgt	r4, [r4, #4]
   d65d8:	3404      	addle	r4, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   d65da:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
   d65dc:	2001      	movs	r0, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   d65de:	429d      	cmp	r5, r3
   d65e0:	dd04      	ble.n	d65ec <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x76>
      buffer_size *= dims_data[i];
   d65e2:	f854 2023 	ldr.w	r2, [r4, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   d65e6:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
   d65e8:	4350      	muls	r0, r2
   d65ea:	e7f8      	b.n	d65de <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_+0x68>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
  }
  return MatchingFlatSize(shape, check_shape_1);
}
   d65ec:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}

000d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>:
// This function implements the same computation as the ARMv7 NEON VQRDMULH
// instruction.
template <>
inline std::int32_t SaturatingRoundingDoublingHighMul(std::int32_t a,
                                                      std::int32_t b) {
  bool overflow = a == b && a == std::numeric_limits<std::int32_t>::min();
   d65f0:	4288      	cmp	r0, r1
  }
#endif
}

inline int32 MultiplyByQuantizedMultiplierSmallerThanOneExp(
    int32 x, int32 quantized_multiplier, int left_shift) {
   d65f2:	b570      	push	{r4, r5, r6, lr}
   d65f4:	d104      	bne.n	d6600 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x10>
   d65f6:	f100 4300 	add.w	r3, r0, #2147483648	; 0x80000000
   d65fa:	425e      	negs	r6, r3
   d65fc:	415e      	adcs	r6, r3
   d65fe:	e000      	b.n	d6602 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x12>
   d6600:	2600      	movs	r6, #0
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
   d6602:	fb80 4501 	smull	r4, r5, r0, r1
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
   d6606:	2c00      	cmp	r4, #0
   d6608:	f175 0300 	sbcs.w	r3, r5, #0
   d660c:	4b1c      	ldr	r3, [pc, #112]	; (d6680 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x90>)
   d660e:	bfa8      	it	ge
   d6610:	f04f 4380 	movge.w	r3, #1073741824	; 0x40000000
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
   d6614:	b97e      	cbnz	r6, d6636 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x46>
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
   d6616:	18e4      	adds	r4, r4, r3
   d6618:	eb45 75e3 	adc.w	r5, r5, r3, asr #31
   d661c:	2c00      	cmp	r4, #0
   d661e:	f175 0300 	sbcs.w	r3, r5, #0
   d6622:	da04      	bge.n	d662e <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x3e>
   d6624:	f06f 4000 	mvn.w	r0, #2147483648	; 0x80000000
   d6628:	2100      	movs	r1, #0
   d662a:	1824      	adds	r4, r4, r0
   d662c:	414d      	adcs	r5, r1
   d662e:	0fe4      	lsrs	r4, r4, #31
   d6630:	ea44 0445 	orr.w	r4, r4, r5, lsl #1
   d6634:	e001      	b.n	d663a <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x4a>
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
   d6636:	f06f 4400 	mvn.w	r4, #2147483648	; 0x80000000
  using gemmlowp::RoundingDivideByPOT;
  using gemmlowp::SaturatingRoundingDoublingHighMul;
  return RoundingDivideByPOT(
   d663a:	4255      	negs	r5, r2

// Correctly-rounded-to-nearest division by a power-of-two.
// Also known as a rounding arithmetic right shift.
template <typename IntegerType>
inline IntegerType RoundingDivideByPOT(IntegerType x, int exponent) {
  assert(exponent >= 0);
   d663c:	2d00      	cmp	r5, #0
   d663e:	da04      	bge.n	d664a <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x5a>
   d6640:	4b10      	ldr	r3, [pc, #64]	; (d6684 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x94>)
   d6642:	4a11      	ldr	r2, [pc, #68]	; (d6688 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x98>)
   d6644:	f44f 71b3 	mov.w	r1, #358	; 0x166
   d6648:	e005      	b.n	d6656 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x66>
  assert(exponent <= 31);
   d664a:	2d1f      	cmp	r5, #31
   d664c:	dd06      	ble.n	d665c <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x6c>
   d664e:	4b0f      	ldr	r3, [pc, #60]	; (d668c <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x9c>)
   d6650:	4a0d      	ldr	r2, [pc, #52]	; (d6688 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0x98>)
   d6652:	f240 1167 	movw	r1, #359	; 0x167
   d6656:	480e      	ldr	r0, [pc, #56]	; (d6690 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli+0xa0>)
   d6658:	f00d fe88 	bl	e436c <__assert_func>
  const IntegerType mask = Dup<IntegerType>((1ll << exponent) - 1);
   d665c:	462a      	mov	r2, r5
   d665e:	2001      	movs	r0, #1
   d6660:	2100      	movs	r1, #0
   d6662:	f010 fb43 	bl	e6cec <__aeabi_llsl>
   d6666:	3801      	subs	r0, #1
      SaturatingRoundingDoublingHighMul(x, quantized_multiplier), -left_shift);
   d6668:	ea00 0304 	and.w	r3, r0, r4
   d666c:	1040      	asrs	r0, r0, #1
   d666e:	eb00 70d4 	add.w	r0, r0, r4, lsr #31
   d6672:	412c      	asrs	r4, r5
}
   d6674:	4283      	cmp	r3, r0
   d6676:	bfd4      	ite	le
   d6678:	4620      	movle	r0, r4
   d667a:	1c60      	addgt	r0, r4, #1
   d667c:	bd70      	pop	{r4, r5, r6, pc}
   d667e:	bf00      	nop
   d6680:	c0000001 	.word	0xc0000001
   d6684:	000e9654 	.word	0x000e9654
   d6688:	000e9743 	.word	0x000e9743
   d668c:	000e9701 	.word	0x000e9701
   d6690:	000e9662 	.word	0x000e9662

000d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>:
// DO NOT USE THIS FUNCTION FOR NEW FUNCTIONALITY BEYOND IMPLEMENTING
// BROADCASTING.
//
// Same as Offset(), except takes as NdArrayDesc<N> instead of Dims<N>.
inline int SubscriptToIndex(const NdArrayDesc<4>& desc, int i0, int i1, int i2,
                            int i3) {
   d6694:	b570      	push	{r4, r5, r6, lr}
  TFLITE_DCHECK(i0 >= 0 && i0 < desc.extents[0]);
   d6696:	2900      	cmp	r1, #0
// DO NOT USE THIS FUNCTION FOR NEW FUNCTIONALITY BEYOND IMPLEMENTING
// BROADCASTING.
//
// Same as Offset(), except takes as NdArrayDesc<N> instead of Dims<N>.
inline int SubscriptToIndex(const NdArrayDesc<4>& desc, int i0, int i1, int i2,
                            int i3) {
   d6698:	9c04      	ldr	r4, [sp, #16]
  TFLITE_DCHECK(i0 >= 0 && i0 < desc.extents[0]);
   d669a:	db02      	blt.n	d66a2 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
   d669c:	6805      	ldr	r5, [r0, #0]
   d669e:	42a9      	cmp	r1, r5
   d66a0:	db01      	blt.n	d66a6 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0x12>
   d66a2:	f00d fe53 	bl	e434c <abort>
  TFLITE_DCHECK(i1 >= 0 && i1 < desc.extents[1]);
   d66a6:	2a00      	cmp	r2, #0
   d66a8:	dbfb      	blt.n	d66a2 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
   d66aa:	6845      	ldr	r5, [r0, #4]
   d66ac:	42aa      	cmp	r2, r5
   d66ae:	daf8      	bge.n	d66a2 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
  TFLITE_DCHECK(i2 >= 0 && i2 < desc.extents[2]);
   d66b0:	2b00      	cmp	r3, #0
   d66b2:	dbf6      	blt.n	d66a2 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
   d66b4:	6885      	ldr	r5, [r0, #8]
   d66b6:	42ab      	cmp	r3, r5
   d66b8:	daf3      	bge.n	d66a2 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
  TFLITE_DCHECK(i3 >= 0 && i3 < desc.extents[3]);
   d66ba:	2c00      	cmp	r4, #0
   d66bc:	dbf1      	blt.n	d66a2 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
   d66be:	68c5      	ldr	r5, [r0, #12]
   d66c0:	42ac      	cmp	r4, r5
   d66c2:	daee      	bge.n	d66a2 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii+0xe>
  return i0 * desc.strides[0] + i1 * desc.strides[1] + i2 * desc.strides[2] +
         i3 * desc.strides[3];
   d66c4:	6945      	ldr	r5, [r0, #20]
   d66c6:	6906      	ldr	r6, [r0, #16]
   d66c8:	4355      	muls	r5, r2
   d66ca:	6982      	ldr	r2, [r0, #24]
   d66cc:	69c0      	ldr	r0, [r0, #28]
   d66ce:	fb06 5101 	mla	r1, r6, r1, r5
   d66d2:	fb02 1303 	mla	r3, r2, r3, r1
}
   d66d6:	fb00 3004 	mla	r0, r0, r4, r3
   d66da:	bd70      	pop	{r4, r5, r6, pc}

000d66dc <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph>:
}

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const uint8* input1_data,
                const RuntimeShape& input2_shape, const uint8* input2_data,
                const RuntimeShape& output_shape, uint8* output_data) {
   d66dc:	e92d 4ff8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d66e0:	4604      	mov	r4, r0
   d66e2:	4690      	mov	r8, r2
   d66e4:	4608      	mov	r0, r1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
   d66e6:	6b22      	ldr	r2, [r4, #48]	; 0x30
   d66e8:	6ae1      	ldr	r1, [r4, #44]	; 0x2c
}

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const uint8* input1_data,
                const RuntimeShape& input2_shape, const uint8* input2_data,
                const RuntimeShape& output_shape, uint8* output_data) {
   d66ea:	9f0a      	ldr	r7, [sp, #40]	; 0x28
   d66ec:	9e0c      	ldr	r6, [sp, #48]	; 0x30
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
   d66ee:	4291      	cmp	r1, r2
   d66f0:	dd01      	ble.n	d66f6 <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1a>

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const uint8* input1_data,
                const RuntimeShape& input2_shape, const uint8* input2_data,
                const RuntimeShape& output_shape, uint8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
   d66f2:	f00d fe2b 	bl	e434c <abort>
                   params.quantized_activation_max);
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d66f6:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   d66f8:	4619      	mov	r1, r3
   d66fa:	f7ff ff3c 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>

  TFLITE_DCHECK_GT(params.input1_offset, -256);
   d66fe:	6862      	ldr	r2, [r4, #4]
   d6700:	f112 0fff 	cmn.w	r2, #255	; 0xff
                const RuntimeShape& input2_shape, const uint8* input2_data,
                const RuntimeShape& output_shape, uint8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d6704:	4681      	mov	r9, r0

  TFLITE_DCHECK_GT(params.input1_offset, -256);
   d6706:	dbf4      	blt.n	d66f2 <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x16>
  TFLITE_DCHECK_GT(params.input2_offset, -256);
  TFLITE_DCHECK_LT(params.input1_offset, 256);
   d6708:	2aff      	cmp	r2, #255	; 0xff
                   params.quantized_activation_max);
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);

  TFLITE_DCHECK_GT(params.input1_offset, -256);
  TFLITE_DCHECK_GT(params.input2_offset, -256);
   d670a:	68a3      	ldr	r3, [r4, #8]
  TFLITE_DCHECK_LT(params.input1_offset, 256);
   d670c:	dcf1      	bgt.n	d66f2 <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x16>
  TFLITE_DCHECK_LT(params.input2_offset, 256);
   d670e:	33ff      	adds	r3, #255	; 0xff
   d6710:	f5b3 7fff 	cmp.w	r3, #510	; 0x1fe
   d6714:	d8ed      	bhi.n	d66f2 <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x16>
   d6716:	2500      	movs	r5, #0
  TFLITE_DCHECK_GT(params.input1_offset, -256);
  TFLITE_DCHECK_GT(params.input2_offset, -256);
  TFLITE_DCHECK_LT(params.input1_offset, 256);
  TFLITE_DCHECK_LT(params.input2_offset, 256);

  for (int i = 0; i < size; ++i) {
   d6718:	45a9      	cmp	r9, r5
   d671a:	dd28      	ble.n	d676e <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x92>
    const int32 input1_val = params.input1_offset + input1_data[i];
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
   d671c:	f817 a005 	ldrb.w	sl, [r7, r5]
   d6720:	68a3      	ldr	r3, [r4, #8]
  TFLITE_DCHECK_LT(params.input2_offset, 256);

  for (int i = 0; i < size; ++i) {
    const int32 input1_val = params.input1_offset + input1_data[i];
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
   d6722:	69a0      	ldr	r0, [r4, #24]
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, params.input1_multiplier, params.input1_shift);
   d6724:	f818 2005 	ldrb.w	r2, [r8, r5]
   d6728:	69e1      	ldr	r1, [r4, #28]

  for (int i = 0; i < size; ++i) {
    const int32 input1_val = params.input1_offset + input1_data[i];
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
   d672a:	4453      	add	r3, sl
   d672c:	fa03 fa00 	lsl.w	sl, r3, r0
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, params.input1_multiplier, params.input1_shift);
   d6730:	6863      	ldr	r3, [r4, #4]
   d6732:	4413      	add	r3, r2
   d6734:	fa03 f000 	lsl.w	r0, r3, r0
   d6738:	6a22      	ldr	r2, [r4, #32]
   d673a:	f7ff ff59 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, params.input2_multiplier, params.input2_shift);
   d673e:	6aa2      	ldr	r2, [r4, #40]	; 0x28
   d6740:	6a61      	ldr	r1, [r4, #36]	; 0x24
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, params.input1_multiplier, params.input1_shift);
   d6742:	4683      	mov	fp, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, params.input2_multiplier, params.input2_shift);
   d6744:	4650      	mov	r0, sl
   d6746:	f7ff ff53 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 raw_sum = scaled_input1_val + scaled_input2_val;
    const int32 raw_output =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
   d674a:	6962      	ldr	r2, [r4, #20]
   d674c:	6921      	ldr	r1, [r4, #16]
   d674e:	4458      	add	r0, fp
   d6750:	f7ff ff4e 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
            raw_sum, params.output_multiplier, params.output_shift) +
        params.output_offset;
    const int32 clamped_output =
        std::min(params.quantized_activation_max,
                 std::max(params.quantized_activation_min, raw_output));
    output_data[i] = static_cast<uint8>(clamped_output);
   d6754:	68e3      	ldr	r3, [r4, #12]
   d6756:	4418      	add	r0, r3
   d6758:	6ae3      	ldr	r3, [r4, #44]	; 0x2c
   d675a:	4298      	cmp	r0, r3
   d675c:	bfb8      	it	lt
   d675e:	4618      	movlt	r0, r3
   d6760:	6b23      	ldr	r3, [r4, #48]	; 0x30
   d6762:	4283      	cmp	r3, r0
   d6764:	bfa8      	it	ge
   d6766:	4603      	movge	r3, r0
   d6768:	5573      	strb	r3, [r6, r5]
  TFLITE_DCHECK_GT(params.input1_offset, -256);
  TFLITE_DCHECK_GT(params.input2_offset, -256);
  TFLITE_DCHECK_LT(params.input1_offset, 256);
  TFLITE_DCHECK_LT(params.input2_offset, 256);

  for (int i = 0; i < size; ++i) {
   d676a:	3501      	adds	r5, #1
   d676c:	e7d4      	b.n	d6718 <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x3c>
   d676e:	e8bd 8ff8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d6772 <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa>:
}

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const int8_t* input1_data,
                const RuntimeShape& input2_shape, const int8_t* input2_data,
                const RuntimeShape& output_shape, int8_t* output_data) {
   d6772:	e92d 4ff8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d6776:	4604      	mov	r4, r0
   d6778:	4690      	mov	r8, r2
   d677a:	4608      	mov	r0, r1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
   d677c:	6b22      	ldr	r2, [r4, #48]	; 0x30
   d677e:	6ae1      	ldr	r1, [r4, #44]	; 0x2c
}

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const int8_t* input1_data,
                const RuntimeShape& input2_shape, const int8_t* input2_data,
                const RuntimeShape& output_shape, int8_t* output_data) {
   d6780:	9f0a      	ldr	r7, [sp, #40]	; 0x28
   d6782:	9e0c      	ldr	r6, [sp, #48]	; 0x30
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
   d6784:	4291      	cmp	r1, r2
   d6786:	dd01      	ble.n	d678c <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x1a>

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const int8_t* input1_data,
                const RuntimeShape& input2_shape, const int8_t* input2_data,
                const RuntimeShape& output_shape, int8_t* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
   d6788:	f00d fde0 	bl	e434c <abort>
                   params.quantized_activation_max);
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d678c:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   d678e:	4619      	mov	r1, r3
   d6790:	f7ff fef1 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>

  const int32_t int8_max_value = std::numeric_limits<int8_t>::max();
  TFLITE_DCHECK_GE(params.input1_offset, -1 * int8_max_value);
   d6794:	6862      	ldr	r2, [r4, #4]
   d6796:	f112 0f7f 	cmn.w	r2, #127	; 0x7f
                const RuntimeShape& input2_shape, const int8_t* input2_data,
                const RuntimeShape& output_shape, int8_t* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d679a:	4681      	mov	r9, r0

  const int32_t int8_max_value = std::numeric_limits<int8_t>::max();
  TFLITE_DCHECK_GE(params.input1_offset, -1 * int8_max_value);
   d679c:	dbf4      	blt.n	d6788 <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x16>
  TFLITE_DCHECK_GE(params.input2_offset, -1 * int8_max_value);
  TFLITE_DCHECK_LE(params.input1_offset, int8_max_value);
   d679e:	2a7f      	cmp	r2, #127	; 0x7f
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);

  const int32_t int8_max_value = std::numeric_limits<int8_t>::max();
  TFLITE_DCHECK_GE(params.input1_offset, -1 * int8_max_value);
  TFLITE_DCHECK_GE(params.input2_offset, -1 * int8_max_value);
   d67a0:	68a3      	ldr	r3, [r4, #8]
  TFLITE_DCHECK_LE(params.input1_offset, int8_max_value);
   d67a2:	dcf1      	bgt.n	d6788 <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x16>
  TFLITE_DCHECK_LE(params.input2_offset, int8_max_value);
   d67a4:	337f      	adds	r3, #127	; 0x7f
   d67a6:	2bfe      	cmp	r3, #254	; 0xfe
   d67a8:	d8ee      	bhi.n	d6788 <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x16>
   d67aa:	2500      	movs	r5, #0
  TFLITE_DCHECK_GE(params.input1_offset, -1 * int8_max_value);
  TFLITE_DCHECK_GE(params.input2_offset, -1 * int8_max_value);
  TFLITE_DCHECK_LE(params.input1_offset, int8_max_value);
  TFLITE_DCHECK_LE(params.input2_offset, int8_max_value);

  for (int i = 0; i < size; ++i) {
   d67ac:	45a9      	cmp	r9, r5
   d67ae:	dd28      	ble.n	d6802 <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x90>
    const int32 input1_val = params.input1_offset + input1_data[i];
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
   d67b0:	f917 a005 	ldrsb.w	sl, [r7, r5]
   d67b4:	68a3      	ldr	r3, [r4, #8]
  TFLITE_DCHECK_LE(params.input2_offset, int8_max_value);

  for (int i = 0; i < size; ++i) {
    const int32 input1_val = params.input1_offset + input1_data[i];
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
   d67b6:	69a0      	ldr	r0, [r4, #24]
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, params.input1_multiplier, params.input1_shift);
   d67b8:	f918 2005 	ldrsb.w	r2, [r8, r5]
   d67bc:	69e1      	ldr	r1, [r4, #28]

  for (int i = 0; i < size; ++i) {
    const int32 input1_val = params.input1_offset + input1_data[i];
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
   d67be:	4453      	add	r3, sl
   d67c0:	fa03 fa00 	lsl.w	sl, r3, r0
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, params.input1_multiplier, params.input1_shift);
   d67c4:	6863      	ldr	r3, [r4, #4]
   d67c6:	4413      	add	r3, r2
   d67c8:	fa03 f000 	lsl.w	r0, r3, r0
   d67cc:	6a22      	ldr	r2, [r4, #32]
   d67ce:	f7ff ff0f 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, params.input2_multiplier, params.input2_shift);
   d67d2:	6aa2      	ldr	r2, [r4, #40]	; 0x28
   d67d4:	6a61      	ldr	r1, [r4, #36]	; 0x24
    const int32 input2_val = params.input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);
    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, params.input1_multiplier, params.input1_shift);
   d67d6:	4683      	mov	fp, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, params.input2_multiplier, params.input2_shift);
   d67d8:	4650      	mov	r0, sl
   d67da:	f7ff ff09 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    const int32 raw_sum = scaled_input1_val + scaled_input2_val;
    const int32 raw_output =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
   d67de:	6962      	ldr	r2, [r4, #20]
   d67e0:	6921      	ldr	r1, [r4, #16]
   d67e2:	4458      	add	r0, fp
   d67e4:	f7ff ff04 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
            raw_sum, params.output_multiplier, params.output_shift) +
        params.output_offset;
    const int32 clamped_output =
        std::min(params.quantized_activation_max,
                 std::max(params.quantized_activation_min, raw_output));
    output_data[i] = static_cast<int8_t>(clamped_output);
   d67e8:	68e3      	ldr	r3, [r4, #12]
   d67ea:	4418      	add	r0, r3
   d67ec:	6ae3      	ldr	r3, [r4, #44]	; 0x2c
   d67ee:	4298      	cmp	r0, r3
   d67f0:	bfb8      	it	lt
   d67f2:	4618      	movlt	r0, r3
   d67f4:	6b23      	ldr	r3, [r4, #48]	; 0x30
   d67f6:	4283      	cmp	r3, r0
   d67f8:	bfa8      	it	ge
   d67fa:	4603      	movge	r3, r0
   d67fc:	5573      	strb	r3, [r6, r5]
  TFLITE_DCHECK_GE(params.input1_offset, -1 * int8_max_value);
  TFLITE_DCHECK_GE(params.input2_offset, -1 * int8_max_value);
  TFLITE_DCHECK_LE(params.input1_offset, int8_max_value);
  TFLITE_DCHECK_LE(params.input2_offset, int8_max_value);

  for (int i = 0; i < size; ++i) {
   d67fe:	3501      	adds	r5, #1
   d6800:	e7d4      	b.n	d67ac <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x3a>
   d6802:	e8bd 8ff8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d6806 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE>:
//
// Returns true iff there is some sort of broadcast, which includes five-fold
// patterns and falling back to generic broadcast.
inline bool ProcessBroadcastShapes(const RuntimeShape& shape0,
                                   const RuntimeShape& shape1,
                                   tflite::ArithmeticParams* params) {
   d6806:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
   d680a:	b091      	sub	sp, #68	; 0x44
   d680c:	6804      	ldr	r4, [r0, #0]
   d680e:	680b      	ldr	r3, [r1, #0]
      dims_pointer_ = new int32[dimensions_count];
#endif  // TF_LITE_STATIC_MEMORY
    }
  }

  RuntimeShape(int shape_size, int32 value) : size_(0) {
   d6810:	af10      	add	r7, sp, #64	; 0x40
   d6812:	2600      	movs	r6, #0
   d6814:	429c      	cmp	r4, r3
   d6816:	f847 6d3c 	str.w	r6, [r7, #-60]!
   d681a:	bfb8      	it	lt
   d681c:	461c      	movlt	r4, r3
  const int dims_count =
      std::max(shape0.DimensionsCount(), shape1.DimensionsCount());

  params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
   d681e:	2304      	movs	r3, #4
//
// Returns true iff there is some sort of broadcast, which includes five-fold
// patterns and falling back to generic broadcast.
inline bool ProcessBroadcastShapes(const RuntimeShape& shape0,
                                   const RuntimeShape& shape1,
                                   tflite::ArithmeticParams* params) {
   d6820:	4681      	mov	r9, r0
   d6822:	4688      	mov	r8, r1
  const int dims_count =
      std::max(shape0.DimensionsCount(), shape1.DimensionsCount());

  params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
   d6824:	7013      	strb	r3, [r2, #0]
    Resize(shape_size);
   d6826:	4621      	mov	r1, r4
   d6828:	4638      	mov	r0, r7
//
// Returns true iff there is some sort of broadcast, which includes five-fold
// patterns and falling back to generic broadcast.
inline bool ProcessBroadcastShapes(const RuntimeShape& shape0,
                                   const RuntimeShape& shape1,
                                   tflite::ArithmeticParams* params) {
   d682a:	4615      	mov	r5, r2
   d682c:	f7ff fe3e 	bl	d64ac <_ZN6tflite12RuntimeShape6ResizeEi>
    for (int i = 0; i < shape_size; ++i) {
   d6830:	42a6      	cmp	r6, r4
   d6832:	da06      	bge.n	d6842 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x3c>
      SetDim(i, value);
   d6834:	4631      	mov	r1, r6
   d6836:	2201      	movs	r2, #1
   d6838:	4638      	mov	r0, r7
   d683a:	f7ff fe26 	bl	d648a <_ZN6tflite12RuntimeShape6SetDimEil>
    }
  }

  RuntimeShape(int shape_size, int32 value) : size_(0) {
    Resize(shape_size);
    for (int i = 0; i < shape_size; ++i) {
   d683e:	3601      	adds	r6, #1
   d6840:	e7f6      	b.n	d6830 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x2a>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
   d6842:	2301      	movs	r3, #1
   d6844:	464a      	mov	r2, r9
   d6846:	4621      	mov	r1, r4
   d6848:	a806      	add	r0, sp, #24
   d684a:	f7ff fe46 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d684e:	2301      	movs	r3, #1
   d6850:	4642      	mov	r2, r8
   d6852:	4621      	mov	r1, r4
   d6854:	a80b      	add	r0, sp, #44	; 0x2c
   d6856:	f7ff fe40 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
    }
    std::memcpy(DimsData(), other.DimsData(), sizeof(int32) * size_);
  }

  bool operator==(const RuntimeShape& comp) const {
    return this->size_ == comp.size_ &&
   d685a:	9a06      	ldr	r2, [sp, #24]
   d685c:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   d685e:	429a      	cmp	r2, r3
   d6860:	d10d      	bne.n	d687e <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x78>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d6862:	2a04      	cmp	r2, #4
   d6864:	bfc7      	ittee	gt
   d6866:	9807      	ldrgt	r0, [sp, #28]
   d6868:	990c      	ldrgt	r1, [sp, #48]	; 0x30
   d686a:	a807      	addle	r0, sp, #28
   d686c:	a90c      	addle	r1, sp, #48	; 0x30
    std::memcpy(DimsData(), other.DimsData(), sizeof(int32) * size_);
  }

  bool operator==(const RuntimeShape& comp) const {
    return this->size_ == comp.size_ &&
           std::memcmp(DimsData(), comp.DimsData(), size_ * sizeof(int32)) == 0;
   d686e:	0092      	lsls	r2, r2, #2
   d6870:	f010 ff90 	bl	e7794 <memcmp>
    }
    std::memcpy(DimsData(), other.DimsData(), sizeof(int32) * size_);
  }

  bool operator==(const RuntimeShape& comp) const {
    return this->size_ == comp.size_ &&
   d6874:	b918      	cbnz	r0, d687e <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x78>
  auto extended_shape0 = RuntimeShape::ExtendedShape(dims_count, shape0);
  auto extended_shape1 = RuntimeShape::ExtendedShape(dims_count, shape1);

  // Check for "exact" match, implicitly accepting any scalar shapes.
  if (extended_shape0 == extended_shape1) {
    params->broadcast_category = BroadcastableOpCategory::kNonBroadcast;
   d6876:	2301      	movs	r3, #1
   d6878:	702b      	strb	r3, [r5, #0]

  if (params->broadcast_category !=
          BroadcastableOpCategory::kFirstInputBroadcastsFast &&
      params->broadcast_category !=
          BroadcastableOpCategory::kSecondInputBroadcastsFast) {
    return false;
   d687a:	2400      	movs	r4, #0
   d687c:	e08c      	b.n	d6998 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x192>
  if (extended_shape0 == extended_shape1) {
    params->broadcast_category = BroadcastableOpCategory::kNonBroadcast;
    return false;
  }

  for (int i = dims_count - 1; i >= 0; --i) {
   d687e:	3c01      	subs	r4, #1
   d6880:	4626      	mov	r6, r4
   d6882:	2e00      	cmp	r6, #0
   d6884:	db13      	blt.n	d68ae <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0xa8>
    if (extended_shape0.Dims(i) == extended_shape1.Dims(i)) {
   d6886:	4631      	mov	r1, r6
   d6888:	a806      	add	r0, sp, #24
   d688a:	f7ff fded 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d688e:	4631      	mov	r1, r6
   d6890:	4680      	mov	r8, r0
   d6892:	a80b      	add	r0, sp, #44	; 0x2c
   d6894:	f7ff fde8 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d6898:	4580      	cmp	r8, r0
   d689a:	d04d      	beq.n	d6938 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x132>
      continue;
    } else if (extended_shape0.Dims(i) == 1) {
   d689c:	f1b8 0f01 	cmp.w	r8, #1
   d68a0:	d101      	bne.n	d68a6 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0xa0>
      params->broadcast_category =
          BroadcastableOpCategory::kFirstInputBroadcastsFast;
   d68a2:	2302      	movs	r3, #2
   d68a4:	e002      	b.n	d68ac <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0xa6>
      break;
    } else if (extended_shape1.Dims(i) == 1) {
   d68a6:	2801      	cmp	r0, #1
   d68a8:	d143      	bne.n	d6932 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x12c>
      params->broadcast_category =
          BroadcastableOpCategory::kSecondInputBroadcastsFast;
   d68aa:	2303      	movs	r3, #3
   d68ac:	702b      	strb	r3, [r5, #0]
      params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
      return true;
    }
  }

  if (params->broadcast_category !=
   d68ae:	782b      	ldrb	r3, [r5, #0]
   d68b0:	1e9a      	subs	r2, r3, #2
   d68b2:	2a01      	cmp	r2, #1
   d68b4:	d8e1      	bhi.n	d687a <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x74>
  // From this point it is assumed contractually that corresponding dimensions
  // in shape0 and shape1 are either (a) equal or (b) one or other equals 1.
  const bool swap_inputs = params->broadcast_category ==
                           BroadcastableOpCategory::kSecondInputBroadcastsFast;
  const RuntimeShape* shape_a =
      swap_inputs ? &extended_shape1 : &extended_shape0;
   d68b6:	2b03      	cmp	r3, #3
  const RuntimeShape* shape_b =
      swap_inputs ? &extended_shape0 : &extended_shape1;

  int i = dims_count - 1;
  params->broadcast_shape[0] = 1;
   d68b8:	f04f 0301 	mov.w	r3, #1
  // From this point it is assumed contractually that corresponding dimensions
  // in shape0 and shape1 are either (a) equal or (b) one or other equals 1.
  const bool swap_inputs = params->broadcast_category ==
                           BroadcastableOpCategory::kSecondInputBroadcastsFast;
  const RuntimeShape* shape_a =
      swap_inputs ? &extended_shape1 : &extended_shape0;
   d68bc:	bf07      	ittee	eq
   d68be:	f10d 082c 	addeq.w	r8, sp, #44	; 0x2c
  const RuntimeShape* shape_b =
      swap_inputs ? &extended_shape0 : &extended_shape1;
   d68c2:	ae06      	addeq	r6, sp, #24
  // From this point it is assumed contractually that corresponding dimensions
  // in shape0 and shape1 are either (a) equal or (b) one or other equals 1.
  const bool swap_inputs = params->broadcast_category ==
                           BroadcastableOpCategory::kSecondInputBroadcastsFast;
  const RuntimeShape* shape_a =
      swap_inputs ? &extended_shape1 : &extended_shape0;
   d68c4:	f10d 0818 	addne.w	r8, sp, #24
  const RuntimeShape* shape_b =
      swap_inputs ? &extended_shape0 : &extended_shape1;
   d68c8:	ae0b      	addne	r6, sp, #44	; 0x2c

  int i = dims_count - 1;
  params->broadcast_shape[0] = 1;
   d68ca:	63eb      	str	r3, [r5, #60]	; 0x3c
  params->broadcast_shape[1] = 1;
   d68cc:	642b      	str	r3, [r5, #64]	; 0x40
  params->broadcast_shape[2] = 1;
   d68ce:	646b      	str	r3, [r5, #68]	; 0x44
  params->broadcast_shape[3] = 1;
   d68d0:	64ab      	str	r3, [r5, #72]	; 0x48
  params->broadcast_shape[4] = 1;
   d68d2:	64eb      	str	r3, [r5, #76]	; 0x4c
  // y_0 is greedy: include dims if both or neither equal 1: in other words,
  // test for equality rather than (shape_a->Dims(i) != 1).
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
   d68d4:	2c00      	cmp	r4, #0
   d68d6:	db5e      	blt.n	d6996 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x190>
   d68d8:	4621      	mov	r1, r4
   d68da:	4640      	mov	r0, r8
   d68dc:	f7ff fdc4 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d68e0:	4621      	mov	r1, r4
   d68e2:	4681      	mov	r9, r0
   d68e4:	4630      	mov	r0, r6
   d68e6:	f7ff fdbf 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d68ea:	4581      	cmp	r9, r0
   d68ec:	d026      	beq.n	d693c <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x136>
    params->broadcast_shape[4] *= shape_b->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).  If it is input_b
  // that has the unit dimension, the next two loops are not entered.
  while (i >= 0 && shape_a->Dims(i) == 1) {
   d68ee:	4621      	mov	r1, r4
   d68f0:	4640      	mov	r0, r8
   d68f2:	f7ff fdb9 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d68f6:	2801      	cmp	r0, #1
   d68f8:	d026      	beq.n	d6948 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x142>
    params->broadcast_shape[3] *= shape_b->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
   d68fa:	4621      	mov	r1, r4
   d68fc:	4640      	mov	r0, r8
   d68fe:	f7ff fdb3 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d6902:	4621      	mov	r1, r4
   d6904:	4681      	mov	r9, r0
   d6906:	4630      	mov	r0, r6
   d6908:	f7ff fdae 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d690c:	4581      	cmp	r9, r0
   d690e:	d027      	beq.n	d6960 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x15a>
    params->broadcast_shape[2] *= shape_a->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).
  while (i >= 0 && shape_b->Dims(i) == 1) {
   d6910:	4621      	mov	r1, r4
   d6912:	4630      	mov	r0, r6
   d6914:	f7ff fda8 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d6918:	2801      	cmp	r0, #1
   d691a:	d029      	beq.n	d6970 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x16a>
    params->broadcast_shape[1] *= shape_a->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
   d691c:	4621      	mov	r1, r4
   d691e:	4640      	mov	r0, r8
   d6920:	f7ff fda2 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d6924:	4621      	mov	r1, r4
   d6926:	4681      	mov	r9, r0
   d6928:	4630      	mov	r0, r6
   d692a:	f7ff fd9d 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d692e:	4581      	cmp	r9, r0
   d6930:	d02a      	beq.n	d6988 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x182>
          BroadcastableOpCategory::kSecondInputBroadcastsFast;
      break;
    } else {
      // This case is erroneous: there is a dimension that does not match and
      // is not a broadcast from one shape to the other.
      params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
   d6932:	2304      	movs	r3, #4
   d6934:	702b      	strb	r3, [r5, #0]
   d6936:	e02e      	b.n	d6996 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x190>
  if (extended_shape0 == extended_shape1) {
    params->broadcast_category = BroadcastableOpCategory::kNonBroadcast;
    return false;
  }

  for (int i = dims_count - 1; i >= 0; --i) {
   d6938:	3e01      	subs	r6, #1
   d693a:	e7a2      	b.n	d6882 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x7c>
  params->broadcast_shape[3] = 1;
  params->broadcast_shape[4] = 1;
  // y_0 is greedy: include dims if both or neither equal 1: in other words,
  // test for equality rather than (shape_a->Dims(i) != 1).
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
    params->broadcast_shape[4] *= shape_b->Dims(i);
   d693c:	6ceb      	ldr	r3, [r5, #76]	; 0x4c
   d693e:	fb09 f303 	mul.w	r3, r9, r3
   d6942:	64eb      	str	r3, [r5, #76]	; 0x4c
    --i;
   d6944:	3c01      	subs	r4, #1
  params->broadcast_shape[2] = 1;
  params->broadcast_shape[3] = 1;
  params->broadcast_shape[4] = 1;
  // y_0 is greedy: include dims if both or neither equal 1: in other words,
  // test for equality rather than (shape_a->Dims(i) != 1).
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
   d6946:	e7c5      	b.n	d68d4 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0xce>
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).  If it is input_b
  // that has the unit dimension, the next two loops are not entered.
  while (i >= 0 && shape_a->Dims(i) == 1) {
    params->broadcast_shape[3] *= shape_b->Dims(i);
   d6948:	4621      	mov	r1, r4
   d694a:	4630      	mov	r0, r6
   d694c:	f7ff fd8c 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d6950:	6cab      	ldr	r3, [r5, #72]	; 0x48
    params->broadcast_shape[4] *= shape_b->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).  If it is input_b
  // that has the unit dimension, the next two loops are not entered.
  while (i >= 0 && shape_a->Dims(i) == 1) {
   d6952:	f114 34ff 	adds.w	r4, r4, #4294967295	; 0xffffffff
    params->broadcast_shape[3] *= shape_b->Dims(i);
   d6956:	fb00 f003 	mul.w	r0, r0, r3
   d695a:	64a8      	str	r0, [r5, #72]	; 0x48
    params->broadcast_shape[4] *= shape_b->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).  If it is input_b
  // that has the unit dimension, the next two loops are not entered.
  while (i >= 0 && shape_a->Dims(i) == 1) {
   d695c:	d2c7      	bcs.n	d68ee <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0xe8>
   d695e:	e01a      	b.n	d6996 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x190>
    params->broadcast_shape[3] *= shape_b->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
    params->broadcast_shape[2] *= shape_a->Dims(i);
   d6960:	6c6b      	ldr	r3, [r5, #68]	; 0x44
  // that has the unit dimension, the next two loops are not entered.
  while (i >= 0 && shape_a->Dims(i) == 1) {
    params->broadcast_shape[3] *= shape_b->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
   d6962:	f114 34ff 	adds.w	r4, r4, #4294967295	; 0xffffffff
    params->broadcast_shape[2] *= shape_a->Dims(i);
   d6966:	fb09 f303 	mul.w	r3, r9, r3
   d696a:	646b      	str	r3, [r5, #68]	; 0x44
  // that has the unit dimension, the next two loops are not entered.
  while (i >= 0 && shape_a->Dims(i) == 1) {
    params->broadcast_shape[3] *= shape_b->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
   d696c:	d2c5      	bcs.n	d68fa <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0xf4>
   d696e:	e012      	b.n	d6996 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x190>
    params->broadcast_shape[2] *= shape_a->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).
  while (i >= 0 && shape_b->Dims(i) == 1) {
    params->broadcast_shape[1] *= shape_a->Dims(i);
   d6970:	4621      	mov	r1, r4
   d6972:	4640      	mov	r0, r8
   d6974:	f7ff fd78 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d6978:	6c2b      	ldr	r3, [r5, #64]	; 0x40
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
    params->broadcast_shape[2] *= shape_a->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).
  while (i >= 0 && shape_b->Dims(i) == 1) {
   d697a:	f114 34ff 	adds.w	r4, r4, #4294967295	; 0xffffffff
    params->broadcast_shape[1] *= shape_a->Dims(i);
   d697e:	fb00 f003 	mul.w	r0, r0, r3
   d6982:	6428      	str	r0, [r5, #64]	; 0x40
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
    params->broadcast_shape[2] *= shape_a->Dims(i);
    --i;
  }
  // Here either input_a or input_b has dim of 1 (if i >= 0).
  while (i >= 0 && shape_b->Dims(i) == 1) {
   d6984:	d2c4      	bcs.n	d6910 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x10a>
   d6986:	e006      	b.n	d6996 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x190>
    params->broadcast_shape[1] *= shape_a->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
    params->broadcast_shape[0] *= shape_b->Dims(i);
   d6988:	6beb      	ldr	r3, [r5, #60]	; 0x3c
  // Here either input_a or input_b has dim of 1 (if i >= 0).
  while (i >= 0 && shape_b->Dims(i) == 1) {
    params->broadcast_shape[1] *= shape_a->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
   d698a:	f114 34ff 	adds.w	r4, r4, #4294967295	; 0xffffffff
    params->broadcast_shape[0] *= shape_b->Dims(i);
   d698e:	fb09 f303 	mul.w	r3, r9, r3
   d6992:	63eb      	str	r3, [r5, #60]	; 0x3c
  // Here either input_a or input_b has dim of 1 (if i >= 0).
  while (i >= 0 && shape_b->Dims(i) == 1) {
    params->broadcast_shape[1] *= shape_a->Dims(i);
    --i;
  }
  while (i >= 0 && shape_a->Dims(i) == shape_b->Dims(i)) {
   d6994:	d2c2      	bcs.n	d691c <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE+0x116>
  // Rarer case is when the broadcast dimensions cannot be handled by a fivefold
  // loop.
  if (i >= 0) {
    params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
  }
  return true;
   d6996:	2401      	movs	r4, #1

  params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
  RuntimeShape scalar_shape(dims_count, 1);

  auto extended_shape0 = RuntimeShape::ExtendedShape(dims_count, shape0);
  auto extended_shape1 = RuntimeShape::ExtendedShape(dims_count, shape1);
   d6998:	a80b      	add	r0, sp, #44	; 0x2c
   d699a:	f7ff fd5a 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
      std::max(shape0.DimensionsCount(), shape1.DimensionsCount());

  params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
  RuntimeShape scalar_shape(dims_count, 1);

  auto extended_shape0 = RuntimeShape::ExtendedShape(dims_count, shape0);
   d699e:	a806      	add	r0, sp, #24
   d69a0:	f7ff fd57 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                                   tflite::ArithmeticParams* params) {
  const int dims_count =
      std::max(shape0.DimensionsCount(), shape1.DimensionsCount());

  params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
  RuntimeShape scalar_shape(dims_count, 1);
   d69a4:	4638      	mov	r0, r7
   d69a6:	f7ff fd54 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  // loop.
  if (i >= 0) {
    params->broadcast_category = BroadcastableOpCategory::kGenericBroadcast;
  }
  return true;
}
   d69aa:	4620      	mov	r0, r4
   d69ac:	b011      	add	sp, #68	; 0x44
   d69ae:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}

000d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>:
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
}

inline RuntimeShape GetTensorShape(const TfLiteTensor* tensor) {
   d69b2:	b570      	push	{r4, r5, r6, lr}
   d69b4:	4604      	mov	r4, r0
  if (tensor == nullptr) {
   d69b6:	b909      	cbnz	r1, d69bc <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor+0xa>
  // larger shapes are separately allocated.
  static constexpr int kMaxSmallSize = 4;

  RuntimeShape& operator=(RuntimeShape const&) = delete;

  RuntimeShape() : size_(0) {}
   d69b8:	6001      	str	r1, [r0, #0]
   d69ba:	e010      	b.n	d69de <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor+0x2c>
    return RuntimeShape();
  }

  TfLiteIntArray* dims = tensor->dims;
  const int dims_size = dims->size;
   d69bc:	688d      	ldr	r5, [r1, #8]
   d69be:	f855 6b04 	ldr.w	r6, [r5], #4
    for (int i = 0; i < shape_size; ++i) {
      SetDim(i, value);
    }
  }

  RuntimeShape(int dimensions_count, const int32* dims_data) : size_(0) {
   d69c2:	2300      	movs	r3, #0
   d69c4:	6003      	str	r3, [r0, #0]
#endif  // TF_LITE_STATIC_MEMORY
    }
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
    Resize(dimensions_count);
   d69c6:	4631      	mov	r1, r6
   d69c8:	f7ff fd70 	bl	d64ac <_ZN6tflite12RuntimeShape6ResizeEi>
      dims_[i] = val;
    }
  }

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d69cc:	6823      	ldr	r3, [r4, #0]
   d69ce:	2b04      	cmp	r3, #4
   d69d0:	bfcc      	ite	gt
   d69d2:	6860      	ldrgt	r0, [r4, #4]
   d69d4:	1d20      	addle	r0, r4, #4
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
    Resize(dimensions_count);
    int32* dst_dims = DimsData();
    std::memcpy(dst_dims, dims_data, dimensions_count * sizeof(int32));
   d69d6:	00b2      	lsls	r2, r6, #2
   d69d8:	4629      	mov	r1, r5
   d69da:	f010 feea 	bl	e77b2 <memcpy>
  const int32_t* dims_data = reinterpret_cast<const int32_t*>(dims->data);
  return RuntimeShape(dims_size, dims_data);
}
   d69de:	4620      	mov	r0, r4
   d69e0:	bd70      	pop	{r4, r5, r6, pc}
	...

000d69e4 <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE>:

TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteAddParams* params,
                             const TfLiteTensor* input1,
                             const TfLiteTensor* input2, TfLiteTensor* output,
                             OpData* data) {
   d69e4:	e92d 47f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, lr}
  data->requires_broadcast = !HaveSameShapes(input1, input2);
   d69e8:	4610      	mov	r0, r2
}

TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteAddParams* params,
                             const TfLiteTensor* input1,
                             const TfLiteTensor* input2, TfLiteTensor* output,
                             OpData* data) {
   d69ea:	ed2d 8b06 	vpush	{d8-d10}
   d69ee:	4688      	mov	r8, r1
  data->requires_broadcast = !HaveSameShapes(input1, input2);
   d69f0:	4619      	mov	r1, r3
}

TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteAddParams* params,
                             const TfLiteTensor* input1,
                             const TfLiteTensor* input2, TfLiteTensor* output,
                             OpData* data) {
   d69f2:	461e      	mov	r6, r3
   d69f4:	9d0e      	ldr	r5, [sp, #56]	; 0x38
   d69f6:	9c0f      	ldr	r4, [sp, #60]	; 0x3c
   d69f8:	4617      	mov	r7, r2
  data->requires_broadcast = !HaveSameShapes(input1, input2);
   d69fa:	f00d f9c1 	bl	e3d80 <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
   d69fe:	f080 0001 	eor.w	r0, r0, #1
   d6a02:	7020      	strb	r0, [r4, #0]

  if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt8) {
   d6a04:	782b      	ldrb	r3, [r5, #0]
   d6a06:	2b03      	cmp	r3, #3
   d6a08:	d001      	beq.n	d6a0e <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE+0x2a>
   d6a0a:	2b09      	cmp	r3, #9
   d6a0c:	d16f      	bne.n	d6aee <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE+0x10a>
    // 8bit -> 8bit general quantized path, with general rescalings
    data->input1_offset = -input1->params.zero_point;
   d6a0e:	693b      	ldr	r3, [r7, #16]
   d6a10:	425b      	negs	r3, r3
   d6a12:	62a3      	str	r3, [r4, #40]	; 0x28
    data->input2_offset = -input2->params.zero_point;
   d6a14:	6933      	ldr	r3, [r6, #16]
   d6a16:	425b      	negs	r3, r3
   d6a18:	62e3      	str	r3, [r4, #44]	; 0x2c
    data->output_offset = output->params.zero_point;
   d6a1a:	692b      	ldr	r3, [r5, #16]
   d6a1c:	6323      	str	r3, [r4, #48]	; 0x30
    data->left_shift = 20;
   d6a1e:	2314      	movs	r3, #20
   d6a20:	6263      	str	r3, [r4, #36]	; 0x24
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   d6a22:	ed97 8a03 	vldr	s16, [r7, #12]
   d6a26:	edd6 8a03 	vldr	s17, [r6, #12]
    const double twice_max_input_scale =
        2 * std::max(input1->params.scale, input2->params.scale);
   d6a2a:	eeb4 8ae8 	vcmpe.f32	s16, s17
   d6a2e:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d6a32:	bf54      	ite	pl
   d6a34:	eef0 7a48 	vmovpl.f32	s15, s16
   d6a38:	eef0 7a68 	vmovmi.f32	s15, s17
   d6a3c:	ee77 7aa7 	vadd.f32	s15, s15, s15
        input2->params.scale / twice_max_input_scale;
    const double real_output_multiplier =
        twice_max_input_scale /
        ((1 << data->left_shift) * output->params.scale);

    QuantizeMultiplierSmallerThanOneExp(
   d6a40:	f104 0a04 	add.w	sl, r4, #4
    data->input1_offset = -input1->params.zero_point;
    data->input2_offset = -input2->params.zero_point;
    data->output_offset = output->params.zero_point;
    data->left_shift = 20;
    const double twice_max_input_scale =
        2 * std::max(input1->params.scale, input2->params.scale);
   d6a44:	ee17 0a90 	vmov	r0, s15
   d6a48:	f010 fac0 	bl	e6fcc <__aeabi_f2d>
   d6a4c:	4606      	mov	r6, r0
   d6a4e:	460f      	mov	r7, r1
    const double real_input1_multiplier =
        input1->params.scale / twice_max_input_scale;
    const double real_input2_multiplier =
        input2->params.scale / twice_max_input_scale;
   d6a50:	ee18 0a90 	vmov	r0, s17
   d6a54:	f010 faba 	bl	e6fcc <__aeabi_f2d>
   d6a58:	4632      	mov	r2, r6
   d6a5a:	463b      	mov	r3, r7
   d6a5c:	f010 fc34 	bl	e72c8 <__aeabi_ddiv>
    const double real_output_multiplier =
        twice_max_input_scale /
        ((1 << data->left_shift) * output->params.scale);
   d6a60:	ed95 7a03 	vldr	s14, [r5, #12]
   d6a64:	eddf 7a24 	vldr	s15, [pc, #144]	; d6af8 <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE+0x114>
   d6a68:	ee67 7a27 	vmul.f32	s15, s14, s15
    const double twice_max_input_scale =
        2 * std::max(input1->params.scale, input2->params.scale);
    const double real_input1_multiplier =
        input1->params.scale / twice_max_input_scale;
    const double real_input2_multiplier =
        input2->params.scale / twice_max_input_scale;
   d6a6c:	ec41 0b1a 	vmov	d10, r0, r1
    const double real_output_multiplier =
        twice_max_input_scale /
        ((1 << data->left_shift) * output->params.scale);
   d6a70:	ee17 0a90 	vmov	r0, s15
   d6a74:	f010 faaa 	bl	e6fcc <__aeabi_f2d>
   d6a78:	4602      	mov	r2, r0
   d6a7a:	460b      	mov	r3, r1
   d6a7c:	4630      	mov	r0, r6
   d6a7e:	4639      	mov	r1, r7
   d6a80:	f010 fc22 	bl	e72c8 <__aeabi_ddiv>
   d6a84:	ec41 0b19 	vmov	d9, r0, r1

    QuantizeMultiplierSmallerThanOneExp(
        real_input1_multiplier, &data->input1_multiplier, &data->input1_shift);
   d6a88:	ee18 0a10 	vmov	r0, s16
   d6a8c:	f010 fa9e 	bl	e6fcc <__aeabi_f2d>
   d6a90:	4632      	mov	r2, r6
   d6a92:	463b      	mov	r3, r7
   d6a94:	f010 fc18 	bl	e72c8 <__aeabi_ddiv>
        input2->params.scale / twice_max_input_scale;
    const double real_output_multiplier =
        twice_max_input_scale /
        ((1 << data->left_shift) * output->params.scale);

    QuantizeMultiplierSmallerThanOneExp(
   d6a98:	f104 0914 	add.w	r9, r4, #20
        real_input1_multiplier, &data->input1_multiplier, &data->input1_shift);
   d6a9c:	ec41 0b10 	vmov	d0, r0, r1
   d6aa0:	4651      	mov	r1, sl
   d6aa2:	4648      	mov	r0, r9
   d6aa4:	f00d f9e0 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>

    QuantizeMultiplierSmallerThanOneExp(
        real_input2_multiplier, &data->input2_multiplier, &data->input2_shift);
   d6aa8:	eeb0 0a4a 	vmov.f32	s0, s20
   d6aac:	eef0 0a6a 	vmov.f32	s1, s21
   d6ab0:	f104 0108 	add.w	r1, r4, #8
   d6ab4:	f104 0018 	add.w	r0, r4, #24
   d6ab8:	f00d f9d6 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>

    QuantizeMultiplierSmallerThanOneExp(
        real_output_multiplier, &data->output_multiplier, &data->output_shift);
   d6abc:	eeb0 0a49 	vmov.f32	s0, s18
   d6ac0:	eef0 0a69 	vmov.f32	s1, s19
   d6ac4:	f104 0120 	add.w	r1, r4, #32
   d6ac8:	f104 001c 	add.w	r0, r4, #28
   d6acc:	f00d f9cc 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>

    if (output->type == kTfLiteUInt8) {
   d6ad0:	782b      	ldrb	r3, [r5, #0]
   d6ad2:	f898 0000 	ldrb.w	r0, [r8]
   d6ad6:	2b03      	cmp	r3, #3
      CalculateActivationRangeUint8(params->activation, output,
                                    &data->output_activation_min,
                                    &data->output_activation_max);
   d6ad8:	4629      	mov	r1, r5
   d6ada:	f104 0310 	add.w	r3, r4, #16
   d6ade:	f104 020c 	add.w	r2, r4, #12
        real_input2_multiplier, &data->input2_multiplier, &data->input2_shift);

    QuantizeMultiplierSmallerThanOneExp(
        real_output_multiplier, &data->output_multiplier, &data->output_shift);

    if (output->type == kTfLiteUInt8) {
   d6ae2:	d102      	bne.n	d6aea <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE+0x106>
      CalculateActivationRangeUint8(params->activation, output,
                                    &data->output_activation_min,
                                    &data->output_activation_max);
   d6ae4:	f00d f846 	bl	e3b74 <_ZN6tflite29CalculateActivationRangeUint8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>
   d6ae8:	e001      	b.n	d6aee <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE+0x10a>
    } else {
      CalculateActivationRangeInt8(params->activation, output,
                                   &data->output_activation_min,
                                   &data->output_activation_max);
   d6aea:	f00d f93d 	bl	e3d68 <_ZN6tflite28CalculateActivationRangeInt8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>
    }
  }

  return kTfLiteOk;
}
   d6aee:	ecbd 8b06 	vpop	{d8-d10}
   d6af2:	2000      	movs	r0, #0
   d6af4:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}
   d6af8:	49800000 	.word	0x49800000

000d6afc <_ZN6tflite3ops5micro12Register_ADDEv>:
}  // namespace add

TfLiteRegistration* Register_ADD() {
  static TfLiteRegistration r = {add::Init, add::Free, add::Prepare, add::Eval};
  return &r;
}
   d6afc:	4800      	ldr	r0, [pc, #0]	; (d6b00 <_ZN6tflite3ops5micro12Register_ADDEv+0x4>)
   d6afe:	4770      	bx	lr
   d6b00:	2003be7c 	.word	0x2003be7c

000d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>:
    }
  }
}

template <int N>
inline void NdArrayDescsForElementwiseBroadcast(
   d6b04:	e92d 47f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, lr}
   d6b08:	460e      	mov	r6, r1
   d6b0a:	b08a      	sub	sp, #40	; 0x28
   d6b0c:	461d      	mov	r5, r3
    const RuntimeShape& input0_shape, const RuntimeShape& input1_shape,
    NdArrayDesc<N>* desc0_out, NdArrayDesc<N>* desc1_out) {
  TFLITE_DCHECK(desc0_out != nullptr);
   d6b0e:	4614      	mov	r4, r2
   d6b10:	b90a      	cbnz	r2, d6b16 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0x12>
   d6b12:	f00d fc1b 	bl	e434c <abort>
  TFLITE_DCHECK(desc1_out != nullptr);
   d6b16:	2b00      	cmp	r3, #0
   d6b18:	d0fb      	beq.n	d6b12 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0xe>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
   d6b1a:	4602      	mov	r2, r0
   d6b1c:	2301      	movs	r3, #1
   d6b1e:	2104      	movs	r1, #4
   d6b20:	4668      	mov	r0, sp
   d6b22:	f7ff fcda 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d6b26:	4632      	mov	r2, r6
   d6b28:	2301      	movs	r3, #1
   d6b2a:	2104      	movs	r1, #4
   d6b2c:	a805      	add	r0, sp, #20
   d6b2e:	f7ff fcd4 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
   d6b32:	f04f 0901 	mov.w	r9, #1
   d6b36:	46a0      	mov	r8, r4
   d6b38:	462f      	mov	r7, r5
  for (int i = N - 1; i >= 0; --i) {
   d6b3a:	2603      	movs	r6, #3

  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
   d6b3c:	46ca      	mov	sl, r9
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
   d6b3e:	4631      	mov	r1, r6
   d6b40:	4668      	mov	r0, sp
   d6b42:	f7ff fc91 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
    desc0_out->strides[i] = desc0_stride;
   d6b46:	f8c8 a01c 	str.w	sl, [r8, #28]

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
   d6b4a:	f8c8 000c 	str.w	r0, [r8, #12]
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
   d6b4e:	4631      	mov	r1, r6
   d6b50:	4668      	mov	r0, sp
   d6b52:	f7ff fc89 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
   d6b56:	4631      	mov	r1, r6
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
   d6b58:	fb00 fa0a 	mul.w	sl, r0, sl
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
   d6b5c:	a805      	add	r0, sp, #20
   d6b5e:	f7ff fc83 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
    desc1_out->strides[i] = desc1_stride;
   d6b62:	f8c7 901c 	str.w	r9, [r7, #28]
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
   d6b66:	60f8      	str	r0, [r7, #12]
    desc1_out->strides[i] = desc1_stride;
    desc1_stride *= extended_input1_shape.Dims(i);
   d6b68:	4631      	mov	r1, r6
   d6b6a:	a805      	add	r0, sp, #20
   d6b6c:	f7ff fc7c 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
   d6b70:	3e01      	subs	r6, #1
   d6b72:	1c73      	adds	r3, r6, #1
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
    desc1_out->strides[i] = desc1_stride;
    desc1_stride *= extended_input1_shape.Dims(i);
   d6b74:	fb00 f909 	mul.w	r9, r0, r9
   d6b78:	f1a8 0804 	sub.w	r8, r8, #4
   d6b7c:	f1a7 0704 	sub.w	r7, r7, #4
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
   d6b80:	d1dd      	bne.n	d6b3e <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0x3a>
   d6b82:	2600      	movs	r6, #0
   d6b84:	3510      	adds	r5, #16
   d6b86:	3410      	adds	r4, #16
   d6b88:	46b0      	mov	r8, r6

  // Walk over each dimension. If the extents are equal do nothing.
  // Otherwise, set the desc with extent 1 to have extent equal to the other and
  // stride 0.
  for (int i = 0; i < N; ++i) {
    const int extent0 = extended_input0_shape.Dims(i);
   d6b8a:	4631      	mov	r1, r6
   d6b8c:	4668      	mov	r0, sp
   d6b8e:	f7ff fc6b 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
    const int extent1 = extended_input1_shape.Dims(i);
   d6b92:	4631      	mov	r1, r6

  // Walk over each dimension. If the extents are equal do nothing.
  // Otherwise, set the desc with extent 1 to have extent equal to the other and
  // stride 0.
  for (int i = 0; i < N; ++i) {
    const int extent0 = extended_input0_shape.Dims(i);
   d6b94:	4607      	mov	r7, r0
    const int extent1 = extended_input1_shape.Dims(i);
   d6b96:	a805      	add	r0, sp, #20
   d6b98:	f7ff fc66 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
    if (extent0 != extent1) {
   d6b9c:	4287      	cmp	r7, r0
   d6b9e:	d00c      	beq.n	d6bba <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0xb6>
      if (extent0 == 1) {
   d6ba0:	2f01      	cmp	r7, #1
   d6ba2:	d104      	bne.n	d6bae <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0xaa>
        desc0_out->strides[i] = 0;
   d6ba4:	f8c4 8000 	str.w	r8, [r4]
        desc0_out->extents[i] = extent1;
   d6ba8:	f844 0c10 	str.w	r0, [r4, #-16]
   d6bac:	e005      	b.n	d6bba <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0xb6>
      } else {
        TFLITE_DCHECK_EQ(extent1, 1);
   d6bae:	2801      	cmp	r0, #1
   d6bb0:	d1af      	bne.n	d6b12 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0xe>
        desc1_out->strides[i] = 0;
   d6bb2:	f8c5 8000 	str.w	r8, [r5]
        desc1_out->extents[i] = extent0;
   d6bb6:	f845 7c10 	str.w	r7, [r5, #-16]
  }

  // Walk over each dimension. If the extents are equal do nothing.
  // Otherwise, set the desc with extent 1 to have extent equal to the other and
  // stride 0.
  for (int i = 0; i < N; ++i) {
   d6bba:	3601      	adds	r6, #1
   d6bbc:	2e04      	cmp	r6, #4
   d6bbe:	f105 0504 	add.w	r5, r5, #4
   d6bc2:	f104 0404 	add.w	r4, r4, #4
   d6bc6:	d1e0      	bne.n	d6b8a <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_+0x86>
    NdArrayDesc<N>* desc0_out, NdArrayDesc<N>* desc1_out) {
  TFLITE_DCHECK(desc0_out != nullptr);
  TFLITE_DCHECK(desc1_out != nullptr);

  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);
   d6bc8:	a805      	add	r0, sp, #20
   d6bca:	f7ff fc42 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
    const RuntimeShape& input0_shape, const RuntimeShape& input1_shape,
    NdArrayDesc<N>* desc0_out, NdArrayDesc<N>* desc1_out) {
  TFLITE_DCHECK(desc0_out != nullptr);
  TFLITE_DCHECK(desc1_out != nullptr);

  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
   d6bce:	4668      	mov	r0, sp
   d6bd0:	f7ff fc3f 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
        desc1_out->strides[i] = 0;
        desc1_out->extents[i] = extent0;
      }
    }
  }
}
   d6bd4:	b00a      	add	sp, #40	; 0x28
   d6bd6:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}

000d6bda <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf>:
                               const RuntimeShape& input1_shape,
                               const float* input1_data,
                               const RuntimeShape& input2_shape,
                               const float* input2_data,
                               const RuntimeShape& output_shape,
                               float* output_data) {
   d6bda:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d6bde:	b09b      	sub	sp, #108	; 0x6c
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
   d6be0:	ad0a      	add	r5, sp, #40	; 0x28
                               const RuntimeShape& input1_shape,
                               const float* input1_data,
                               const RuntimeShape& input2_shape,
                               const float* input2_data,
                               const RuntimeShape& output_shape,
                               float* output_data) {
   d6be2:	9203      	str	r2, [sp, #12]
   d6be4:	4683      	mov	fp, r0
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
   d6be6:	462a      	mov	r2, r5
                               const RuntimeShape& input1_shape,
                               const float* input1_data,
                               const RuntimeShape& input2_shape,
                               const float* input2_data,
                               const RuntimeShape& output_shape,
                               float* output_data) {
   d6be8:	4608      	mov	r0, r1
   d6bea:	4619      	mov	r1, r3
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
   d6bec:	ab12      	add	r3, sp, #72	; 0x48
   d6bee:	f7ff ff89 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
   d6bf2:	2301      	movs	r3, #1
   d6bf4:	9a25      	ldr	r2, [sp, #148]	; 0x94
   d6bf6:	2104      	movs	r1, #4
   d6bf8:	a805      	add	r0, sp, #20
   d6bfa:	f7ff fc6e 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   d6bfe:	2400      	movs	r4, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              ActivationFunctionWithMinMax(
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
   d6c00:	462f      	mov	r7, r5
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   d6c02:	2100      	movs	r1, #0
   d6c04:	a805      	add	r0, sp, #20
   d6c06:	f7ff fc2f 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d6c0a:	4284      	cmp	r4, r0
   d6c0c:	da61      	bge.n	d6cd2 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0xf8>
   d6c0e:	2500      	movs	r5, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
   d6c10:	f10d 0914 	add.w	r9, sp, #20
   d6c14:	2101      	movs	r1, #1
   d6c16:	4648      	mov	r0, r9
   d6c18:	f7ff fc26 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d6c1c:	4285      	cmp	r5, r0
   d6c1e:	da56      	bge.n	d6cce <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0xf4>
   d6c20:	f04f 0800 	mov.w	r8, #0
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
   d6c24:	464e      	mov	r6, r9
   d6c26:	2102      	movs	r1, #2
   d6c28:	4630      	mov	r0, r6
   d6c2a:	f7ff fc1d 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d6c2e:	4580      	cmp	r8, r0
   d6c30:	da4b      	bge.n	d6cca <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0xf0>
   d6c32:	f04f 0900 	mov.w	r9, #0
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   d6c36:	2103      	movs	r1, #3
   d6c38:	4630      	mov	r0, r6
   d6c3a:	f7ff fc15 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d6c3e:	4581      	cmp	r9, r0
   d6c40:	da40      	bge.n	d6cc4 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0xea>
          output_data[Offset(extended_output_shape, b, y, x, c)] =
   d6c42:	f8cd 9000 	str.w	r9, [sp]
   d6c46:	4643      	mov	r3, r8
   d6c48:	462a      	mov	r2, r5
   d6c4a:	4621      	mov	r1, r4
   d6c4c:	4630      	mov	r0, r6
   d6c4e:	f7ff fc70 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   d6c52:	9b26      	ldr	r3, [sp, #152]	; 0x98
              ActivationFunctionWithMinMax(
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
   d6c54:	f8cd 9000 	str.w	r9, [sp]
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          output_data[Offset(extended_output_shape, b, y, x, c)] =
   d6c58:	eb03 0380 	add.w	r3, r3, r0, lsl #2
   d6c5c:	9302      	str	r3, [sp, #8]
              ActivationFunctionWithMinMax(
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
   d6c5e:	462a      	mov	r2, r5
   d6c60:	4643      	mov	r3, r8
   d6c62:	4621      	mov	r1, r4
   d6c64:	4638      	mov	r0, r7
   d6c66:	f7ff fd15 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                      input2_data[SubscriptToIndex(desc2, b, y, x, c)],
   d6c6a:	f8cd 9000 	str.w	r9, [sp]
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              ActivationFunctionWithMinMax(
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
   d6c6e:	4682      	mov	sl, r0
                      input2_data[SubscriptToIndex(desc2, b, y, x, c)],
   d6c70:	4643      	mov	r3, r8
   d6c72:	462a      	mov	r2, r5
   d6c74:	4621      	mov	r1, r4
   d6c76:	a812      	add	r0, sp, #72	; 0x48
   d6c78:	f7ff fd0c 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              ActivationFunctionWithMinMax(
   d6c7c:	9b03      	ldr	r3, [sp, #12]
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
                      input2_data[SubscriptToIndex(desc2, b, y, x, c)],
                  params.float_activation_min, params.float_activation_max);
   d6c7e:	eddb 6a0e 	vldr	s13, [fp, #56]	; 0x38
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              ActivationFunctionWithMinMax(
   d6c82:	eb03 0a8a 	add.w	sl, r3, sl, lsl #2
   d6c86:	9b24      	ldr	r3, [sp, #144]	; 0x90
   d6c88:	edda 7a00 	vldr	s15, [sl]
   d6c8c:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   d6c90:	ed90 7a00 	vldr	s14, [r0]
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
                      input2_data[SubscriptToIndex(desc2, b, y, x, c)],
                  params.float_activation_min, params.float_activation_max);
   d6c94:	9b02      	ldr	r3, [sp, #8]
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              ActivationFunctionWithMinMax(
   d6c96:	ee37 7a87 	vadd.f32	s14, s15, s14
                  input1_data[SubscriptToIndex(desc1, b, y, x, c)] +
                      input2_data[SubscriptToIndex(desc2, b, y, x, c)],
                  params.float_activation_min, params.float_activation_max);
   d6c9a:	eddb 7a0d 	vldr	s15, [fp, #52]	; 0x34
	return __b;
      return __a;
   d6c9e:	eeb4 7ae7 	vcmpe.f32	s14, s15
   d6ca2:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d6ca6:	bf58      	it	pl
   d6ca8:	eef0 7a47 	vmovpl.f32	s15, s14
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   d6cac:	eef4 6a67 	vcmp.f32	s13, s15
   d6cb0:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d6cb4:	bf48      	it	mi
   d6cb6:	eef0 7a66 	vmovmi.f32	s15, s13
   d6cba:	edc3 7a00 	vstr	s15, [r3]
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   d6cbe:	f109 0901 	add.w	r9, r9, #1
   d6cc2:	e7b8      	b.n	d6c36 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0x5c>
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
   d6cc4:	f108 0801 	add.w	r8, r8, #1
   d6cc8:	e7ad      	b.n	d6c26 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0x4c>
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
   d6cca:	3501      	adds	r5, #1
   d6ccc:	e7a0      	b.n	d6c10 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0x36>
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   d6cce:	3401      	adds	r4, #1
   d6cd0:	e797      	b.n	d6c02 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf+0x28>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
  const RuntimeShape extended_output_shape =
      RuntimeShape::ExtendedShape(4, output_shape);
   d6cd2:	a805      	add	r0, sp, #20
   d6cd4:	f7ff fbbd 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                  params.float_activation_min, params.float_activation_max);
        }
      }
    }
  }
}
   d6cd8:	b01b      	add	sp, #108	; 0x6c
   d6cda:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

000d6ce0 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_>:
  return kTfLiteOk;
}

void EvalAdd(TfLiteContext* context, TfLiteNode* node, TfLiteAddParams* params,
             const OpData* data, const TfLiteTensor* input1,
             const TfLiteTensor* input2, TfLiteTensor* output) {
   d6ce0:	b5f0      	push	{r4, r5, r6, r7, lr}
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
   d6ce2:	7812      	ldrb	r2, [r2, #0]
  return kTfLiteOk;
}

void EvalAdd(TfLiteContext* context, TfLiteNode* node, TfLiteAddParams* params,
             const OpData* data, const TfLiteTensor* input1,
             const TfLiteTensor* input2, TfLiteTensor* output) {
   d6ce4:	b0a9      	sub	sp, #164	; 0xa4
// Calculates the useful range of an activation layer given its activation
// tensor.a
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
   d6ce6:	2a01      	cmp	r2, #1
   d6ce8:	9e2e      	ldr	r6, [sp, #184]	; 0xb8
   d6cea:	9d2f      	ldr	r5, [sp, #188]	; 0xbc
   d6cec:	9c30      	ldr	r4, [sp, #192]	; 0xc0
   d6cee:	d011      	beq.n	d6d14 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x34>
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
   d6cf0:	2a03      	cmp	r2, #3
   d6cf2:	d012      	beq.n	d6d1a <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x3a>
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
    *activation_max = std::numeric_limits<T>::max();
   d6cf4:	ed9f 7a3c 	vldr	s14, [pc, #240]	; d6de8 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x108>
   d6cf8:	eddf 6a3c 	vldr	s13, [pc, #240]	; d6dec <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x10c>
   d6cfc:	2a02      	cmp	r2, #2
   d6cfe:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   d6d02:	bf18      	it	ne
   d6d04:	eef0 7a47 	vmovne.f32	s15, s14
   d6d08:	eebf 7a00 	vmov.f32	s14, #240	; 0xbf800000 -1.0
   d6d0c:	bf18      	it	ne
   d6d0e:	eeb0 7a66 	vmovne.f32	s14, s13
   d6d12:	e006      	b.n	d6d22 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x42>
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
   d6d14:	eddf 7a34 	vldr	s15, [pc, #208]	; d6de8 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x108>
   d6d18:	e001      	b.n	d6d1e <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x3e>
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
    *activation_max = 6;
   d6d1a:	eef1 7a08 	vmov.f32	s15, #24	; 0x40c00000  6.0
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
   d6d1e:	ed9f 7a34 	vldr	s14, [pc, #208]	; d6df0 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x110>
#define TF_LITE_ADD(opname)                                                   \
  reference_ops::opname(op_params, GetTensorShape(input1),                    \
                        GetTensorData<float>(input1), GetTensorShape(input2), \
                        GetTensorData<float>(input2), GetTensorShape(output), \
                        GetTensorData<float>(output))
  if (data->requires_broadcast) {
   d6d22:	781b      	ldrb	r3, [r3, #0]
  int output_shift;
};

template <typename P>
inline void SetActivationParams(float min, float max, P* params) {
  params->float_activation_min = min;
   d6d24:	ed8d 7a21 	vstr	s14, [sp, #132]	; 0x84
  params->float_activation_max = max;
   d6d28:	edcd 7a22 	vstr	s15, [sp, #136]	; 0x88
   d6d2c:	af0f      	add	r7, sp, #60	; 0x3c
    TF_LITE_ADD(BroadcastAdd4DSlow);
   d6d2e:	4631      	mov	r1, r6
   d6d30:	a805      	add	r0, sp, #20
#define TF_LITE_ADD(opname)                                                   \
  reference_ops::opname(op_params, GetTensorShape(input1),                    \
                        GetTensorData<float>(input1), GetTensorShape(input2), \
                        GetTensorData<float>(input2), GetTensorShape(output), \
                        GetTensorData<float>(output))
  if (data->requires_broadcast) {
   d6d32:	b1cb      	cbz	r3, d6d68 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x88>
    TF_LITE_ADD(BroadcastAdd4DSlow);
   d6d34:	f7ff fe3d 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d6d38:	b106      	cbz	r6, d6d3c <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x5c>
   d6d3a:	6876      	ldr	r6, [r6, #4]
   d6d3c:	4629      	mov	r1, r5
   d6d3e:	a80a      	add	r0, sp, #40	; 0x28
   d6d40:	f7ff fe37 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d6d44:	b105      	cbz	r5, d6d48 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x68>
   d6d46:	686d      	ldr	r5, [r5, #4]
   d6d48:	4621      	mov	r1, r4
   d6d4a:	4638      	mov	r0, r7
   d6d4c:	f7ff fe31 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d6d50:	b104      	cbz	r4, d6d54 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x74>
   d6d52:	6864      	ldr	r4, [r4, #4]
   d6d54:	9402      	str	r4, [sp, #8]
   d6d56:	e88d 00a0 	stmia.w	sp, {r5, r7}
   d6d5a:	ab0a      	add	r3, sp, #40	; 0x28
   d6d5c:	4632      	mov	r2, r6
   d6d5e:	a905      	add	r1, sp, #20
   d6d60:	a814      	add	r0, sp, #80	; 0x50
   d6d62:	f7ff ff3a 	bl	d6bda <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_Pf>
   d6d66:	e033      	b.n	d6dd0 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0xf0>
  } else {
    TF_LITE_ADD(Add);
   d6d68:	f7ff fe23 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d6d6c:	b106      	cbz	r6, d6d70 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x90>
   d6d6e:	6876      	ldr	r6, [r6, #4]
   d6d70:	4629      	mov	r1, r5
   d6d72:	a80a      	add	r0, sp, #40	; 0x28
   d6d74:	f7ff fe1d 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d6d78:	b105      	cbz	r5, d6d7c <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x9c>
   d6d7a:	686d      	ldr	r5, [r5, #4]
   d6d7c:	4621      	mov	r1, r4
   d6d7e:	4638      	mov	r0, r7
   d6d80:	f7ff fe17 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d6d84:	b104      	cbz	r4, d6d88 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0xa8>
   d6d86:	6864      	ldr	r4, [r4, #4]

inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const float* input1_data,
                const RuntimeShape& input2_shape, const float* input2_data,
                const RuntimeShape& output_shape, float* output_data) {
  const int size = MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d6d88:	463a      	mov	r2, r7
   d6d8a:	a90a      	add	r1, sp, #40	; 0x28
   d6d8c:	a805      	add	r0, sp, #20
   d6d8e:	f7ff fbf2 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
  for (int i = 0; i < size; i++) {
   d6d92:	2300      	movs	r3, #0
   d6d94:	4298      	cmp	r0, r3
   d6d96:	dd1b      	ble.n	d6dd0 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0xf0>
    auto x = input1_data[i] + input2_data[i];
   d6d98:	ecf6 7a01 	vldmia	r6!, {s15}
   d6d9c:	ecb5 7a01 	vldmia	r5!, {s14}
    output_data[i] = ActivationFunctionWithMinMax(
        x, params.float_activation_min, params.float_activation_max);
   d6da0:	eddd 6a22 	vldr	s13, [sp, #136]	; 0x88
                const RuntimeShape& input1_shape, const float* input1_data,
                const RuntimeShape& input2_shape, const float* input2_data,
                const RuntimeShape& output_shape, float* output_data) {
  const int size = MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int i = 0; i < size; i++) {
    auto x = input1_data[i] + input2_data[i];
   d6da4:	ee37 7a87 	vadd.f32	s14, s15, s14
    output_data[i] = ActivationFunctionWithMinMax(
        x, params.float_activation_min, params.float_activation_max);
   d6da8:	eddd 7a21 	vldr	s15, [sp, #132]	; 0x84
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
	return __b;
      return __a;
   d6dac:	eeb4 7ae7 	vcmpe.f32	s14, s15
   d6db0:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d6db4:	bf58      	it	pl
   d6db6:	eef0 7a47 	vmovpl.f32	s15, s14
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   d6dba:	eef4 6a67 	vcmp.f32	s13, s15
   d6dbe:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d6dc2:	bf48      	it	mi
   d6dc4:	eef0 7a66 	vmovmi.f32	s15, s13
   d6dc8:	ece4 7a01 	vstmia	r4!, {s15}
inline void Add(const ArithmeticParams& params,
                const RuntimeShape& input1_shape, const float* input1_data,
                const RuntimeShape& input2_shape, const float* input2_data,
                const RuntimeShape& output_shape, float* output_data) {
  const int size = MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int i = 0; i < size; i++) {
   d6dcc:	3301      	adds	r3, #1
   d6dce:	e7e1      	b.n	d6d94 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0xb4>
   d6dd0:	4638      	mov	r0, r7
   d6dd2:	f7ff fb3e 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   d6dd6:	a80a      	add	r0, sp, #40	; 0x28
   d6dd8:	f7ff fb3b 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   d6ddc:	a805      	add	r0, sp, #20
   d6dde:	f7ff fb38 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  }
#undef TF_LITE_ADD
}
   d6de2:	b029      	add	sp, #164	; 0xa4
   d6de4:	bdf0      	pop	{r4, r5, r6, r7, pc}
   d6de6:	bf00      	nop
   d6de8:	7f7fffff 	.word	0x7f7fffff
   d6dec:	ff7fffff 	.word	0xff7fffff
   d6df0:	00000000 	.word	0x00000000

000d6df4 <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa>:
                               const RuntimeShape& input1_shape,
                               const int8_t* input1_data,
                               const RuntimeShape& input2_shape,
                               const int8_t* input2_data,
                               const RuntimeShape& output_shape,
                               int8_t* output_data) {
   d6df4:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d6df8:	b09b      	sub	sp, #108	; 0x6c
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
   d6dfa:	ae0a      	add	r6, sp, #40	; 0x28
                               const RuntimeShape& input1_shape,
                               const int8_t* input1_data,
                               const RuntimeShape& input2_shape,
                               const int8_t* input2_data,
                               const RuntimeShape& output_shape,
                               int8_t* output_data) {
   d6dfc:	4604      	mov	r4, r0
   d6dfe:	4693      	mov	fp, r2
   d6e00:	4608      	mov	r0, r1
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
   d6e02:	4632      	mov	r2, r6
                               const RuntimeShape& input1_shape,
                               const int8_t* input1_data,
                               const RuntimeShape& input2_shape,
                               const int8_t* input2_data,
                               const RuntimeShape& output_shape,
                               int8_t* output_data) {
   d6e04:	4619      	mov	r1, r3
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
   d6e06:	ab12      	add	r3, sp, #72	; 0x48
   d6e08:	f7ff fe7c 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
   d6e0c:	2301      	movs	r3, #1
   d6e0e:	9a25      	ldr	r2, [sp, #148]	; 0x94
   d6e10:	2104      	movs	r1, #4
   d6e12:	a805      	add	r0, sp, #20
   d6e14:	f7ff fb61 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   d6e18:	2500      	movs	r5, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          const int32_t input1_val =
              params.input1_offset +
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d6e1a:	9602      	str	r6, [sp, #8]
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   d6e1c:	2100      	movs	r1, #0
   d6e1e:	a805      	add	r0, sp, #20
   d6e20:	f7ff fb22 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d6e24:	4285      	cmp	r5, r0
   d6e26:	da65      	bge.n	d6ef4 <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x100>
   d6e28:	2600      	movs	r6, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
   d6e2a:	f10d 0814 	add.w	r8, sp, #20
   d6e2e:	2101      	movs	r1, #1
   d6e30:	4640      	mov	r0, r8
   d6e32:	f7ff fb19 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d6e36:	4286      	cmp	r6, r0
   d6e38:	da5a      	bge.n	d6ef0 <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0xfc>
   d6e3a:	2700      	movs	r7, #0
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
   d6e3c:	2102      	movs	r1, #2
   d6e3e:	4640      	mov	r0, r8
   d6e40:	f7ff fb12 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d6e44:	4287      	cmp	r7, r0
   d6e46:	da51      	bge.n	d6eec <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0xf8>
   d6e48:	f04f 0900 	mov.w	r9, #0
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   d6e4c:	2103      	movs	r1, #3
   d6e4e:	4640      	mov	r0, r8
   d6e50:	f7ff fb0a 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d6e54:	4581      	cmp	r9, r0
   d6e56:	da47      	bge.n	d6ee8 <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0xf4>
          const int32_t input1_val =
              params.input1_offset +
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d6e58:	f8cd 9000 	str.w	r9, [sp]
   d6e5c:	463b      	mov	r3, r7
   d6e5e:	4632      	mov	r2, r6
   d6e60:	4629      	mov	r1, r5
   d6e62:	9802      	ldr	r0, [sp, #8]
   d6e64:	f7ff fc16 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   d6e68:	6863      	ldr	r3, [r4, #4]
   d6e6a:	f91b a000 	ldrsb.w	sl, [fp, r0]
          const int32_t input2_val =
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d6e6e:	f8cd 9000 	str.w	r9, [sp]
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          const int32_t input1_val =
              params.input1_offset +
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d6e72:	449a      	add	sl, r3
          const int32_t input2_val =
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d6e74:	4632      	mov	r2, r6
   d6e76:	463b      	mov	r3, r7
   d6e78:	4629      	mov	r1, r5
   d6e7a:	a812      	add	r0, sp, #72	; 0x48
   d6e7c:	f7ff fc0a 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32_t shifted_input1_val =
              input1_val * (1 << params.left_shift);
          const int32_t shifted_input2_val =
              input2_val * (1 << params.left_shift);
   d6e80:	9b24      	ldr	r3, [sp, #144]	; 0x90
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32_t input2_val =
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32_t shifted_input1_val =
              input1_val * (1 << params.left_shift);
   d6e82:	f8d4 e018 	ldr.w	lr, [r4, #24]
          const int32_t shifted_input2_val =
              input2_val * (1 << params.left_shift);
   d6e86:	561a      	ldrsb	r2, [r3, r0]
   d6e88:	68a3      	ldr	r3, [r4, #8]
          const int32_t scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
   d6e8a:	69e1      	ldr	r1, [r4, #28]
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32_t shifted_input1_val =
              input1_val * (1 << params.left_shift);
          const int32_t shifted_input2_val =
              input2_val * (1 << params.left_shift);
   d6e8c:	4413      	add	r3, r2
   d6e8e:	fa03 f30e 	lsl.w	r3, r3, lr
          const int32_t scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
   d6e92:	fa0a f00e 	lsl.w	r0, sl, lr
   d6e96:	6a22      	ldr	r2, [r4, #32]
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32_t shifted_input1_val =
              input1_val * (1 << params.left_shift);
          const int32_t shifted_input2_val =
              input2_val * (1 << params.left_shift);
   d6e98:	9303      	str	r3, [sp, #12]
          const int32_t scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
   d6e9a:	f7ff fba9 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32_t scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, params.input2_multiplier,
                  params.input2_shift);
   d6e9e:	9b03      	ldr	r3, [sp, #12]
   d6ea0:	6aa2      	ldr	r2, [r4, #40]	; 0x28
   d6ea2:	6a61      	ldr	r1, [r4, #36]	; 0x24
          const int32_t shifted_input2_val =
              input2_val * (1 << params.left_shift);
          const int32_t scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
   d6ea4:	4682      	mov	sl, r0
          const int32_t scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, params.input2_multiplier,
                  params.input2_shift);
   d6ea6:	4618      	mov	r0, r3
   d6ea8:	f7ff fba2 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32_t raw_sum = scaled_input1_val + scaled_input2_val;
          const int32_t raw_output =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
   d6eac:	6962      	ldr	r2, [r4, #20]
   d6eae:	6921      	ldr	r1, [r4, #16]
   d6eb0:	4450      	add	r0, sl
   d6eb2:	f7ff fb9d 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
   d6eb6:	68e3      	ldr	r3, [r4, #12]
                  raw_sum, params.output_multiplier, params.output_shift) +
              params.output_offset;
          const int32_t clamped_output =
              std::min(params.quantized_activation_max,
                       std::max(params.quantized_activation_min, raw_output));
          output_data[Offset(extended_output_shape, b, y, x, c)] =
   d6eb8:	f8cd 9000 	str.w	r9, [sp]
   d6ebc:	4418      	add	r0, r3
   d6ebe:	6ae3      	ldr	r3, [r4, #44]	; 0x2c
   d6ec0:	4283      	cmp	r3, r0
   d6ec2:	bfb8      	it	lt
   d6ec4:	4603      	movlt	r3, r0
   d6ec6:	6b20      	ldr	r0, [r4, #48]	; 0x30
   d6ec8:	4283      	cmp	r3, r0
   d6eca:	bfa8      	it	ge
   d6ecc:	4603      	movge	r3, r0
   d6ece:	469a      	mov	sl, r3
   d6ed0:	4632      	mov	r2, r6
   d6ed2:	463b      	mov	r3, r7
   d6ed4:	4629      	mov	r1, r5
   d6ed6:	4640      	mov	r0, r8
   d6ed8:	f7ff fb2b 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<int8_t>(clamped_output);
   d6edc:	9b26      	ldr	r3, [sp, #152]	; 0x98
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   d6ede:	f109 0901 	add.w	r9, r9, #1
              params.output_offset;
          const int32_t clamped_output =
              std::min(params.quantized_activation_max,
                       std::max(params.quantized_activation_min, raw_output));
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              static_cast<int8_t>(clamped_output);
   d6ee2:	f803 a000 	strb.w	sl, [r3, r0]
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   d6ee6:	e7b1      	b.n	d6e4c <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x58>
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
   d6ee8:	3701      	adds	r7, #1
   d6eea:	e7a7      	b.n	d6e3c <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x48>
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
   d6eec:	3601      	adds	r6, #1
   d6eee:	e79c      	b.n	d6e2a <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x36>
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   d6ef0:	3501      	adds	r5, #1
   d6ef2:	e793      	b.n	d6e1c <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa+0x28>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
  const RuntimeShape extended_output_shape =
      RuntimeShape::ExtendedShape(4, output_shape);
   d6ef4:	a805      	add	r0, sp, #20
   d6ef6:	f7ff faac 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
              static_cast<int8_t>(clamped_output);
        }
      }
    }
  }
}
   d6efa:	b01b      	add	sp, #108	; 0x6c
   d6efc:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d6f00 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph>:
                               const RuntimeShape& input1_shape,
                               const uint8* input1_data,
                               const RuntimeShape& input2_shape,
                               const uint8* input2_data,
                               const RuntimeShape& output_shape,
                               uint8* output_data) {
   d6f00:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d6f04:	b09b      	sub	sp, #108	; 0x6c
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
   d6f06:	ae0a      	add	r6, sp, #40	; 0x28
                               const RuntimeShape& input1_shape,
                               const uint8* input1_data,
                               const RuntimeShape& input2_shape,
                               const uint8* input2_data,
                               const RuntimeShape& output_shape,
                               uint8* output_data) {
   d6f08:	4604      	mov	r4, r0
   d6f0a:	4693      	mov	fp, r2
   d6f0c:	4608      	mov	r0, r1
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
   d6f0e:	4632      	mov	r2, r6
                               const RuntimeShape& input1_shape,
                               const uint8* input1_data,
                               const RuntimeShape& input2_shape,
                               const uint8* input2_data,
                               const RuntimeShape& output_shape,
                               uint8* output_data) {
   d6f10:	4619      	mov	r1, r3
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
   d6f12:	ab12      	add	r3, sp, #72	; 0x48
   d6f14:	f7ff fdf6 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
   d6f18:	2301      	movs	r3, #1
   d6f1a:	9a25      	ldr	r2, [sp, #148]	; 0x94
   d6f1c:	2104      	movs	r1, #4
   d6f1e:	a805      	add	r0, sp, #20
   d6f20:	f7ff fadb 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   d6f24:	2500      	movs	r5, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          const int32 input1_val =
              params.input1_offset +
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d6f26:	9602      	str	r6, [sp, #8]
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   d6f28:	2100      	movs	r1, #0
   d6f2a:	a805      	add	r0, sp, #20
   d6f2c:	f7ff fa9c 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d6f30:	4285      	cmp	r5, r0
   d6f32:	da65      	bge.n	d7000 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x100>
   d6f34:	2600      	movs	r6, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
   d6f36:	f10d 0814 	add.w	r8, sp, #20
   d6f3a:	2101      	movs	r1, #1
   d6f3c:	4640      	mov	r0, r8
   d6f3e:	f7ff fa93 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d6f42:	4286      	cmp	r6, r0
   d6f44:	da5a      	bge.n	d6ffc <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0xfc>
   d6f46:	2700      	movs	r7, #0
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
   d6f48:	2102      	movs	r1, #2
   d6f4a:	4640      	mov	r0, r8
   d6f4c:	f7ff fa8c 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d6f50:	4287      	cmp	r7, r0
   d6f52:	da51      	bge.n	d6ff8 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0xf8>
   d6f54:	f04f 0900 	mov.w	r9, #0
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   d6f58:	2103      	movs	r1, #3
   d6f5a:	4640      	mov	r0, r8
   d6f5c:	f7ff fa84 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d6f60:	4581      	cmp	r9, r0
   d6f62:	da47      	bge.n	d6ff4 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0xf4>
          const int32 input1_val =
              params.input1_offset +
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d6f64:	f8cd 9000 	str.w	r9, [sp]
   d6f68:	463b      	mov	r3, r7
   d6f6a:	4632      	mov	r2, r6
   d6f6c:	4629      	mov	r1, r5
   d6f6e:	9802      	ldr	r0, [sp, #8]
   d6f70:	f7ff fb90 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   d6f74:	6863      	ldr	r3, [r4, #4]
   d6f76:	f81b a000 	ldrb.w	sl, [fp, r0]
          const int32 input2_val =
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d6f7a:	f8cd 9000 	str.w	r9, [sp]
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          const int32 input1_val =
              params.input1_offset +
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d6f7e:	449a      	add	sl, r3
          const int32 input2_val =
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d6f80:	4632      	mov	r2, r6
   d6f82:	463b      	mov	r3, r7
   d6f84:	4629      	mov	r1, r5
   d6f86:	a812      	add	r0, sp, #72	; 0x48
   d6f88:	f7ff fb84 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val =
              input1_val * (1 << params.left_shift);
          const int32 shifted_input2_val =
              input2_val * (1 << params.left_shift);
   d6f8c:	9b24      	ldr	r3, [sp, #144]	; 0x90
              input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val =
              input1_val * (1 << params.left_shift);
   d6f8e:	f8d4 e018 	ldr.w	lr, [r4, #24]
          const int32 shifted_input2_val =
              input2_val * (1 << params.left_shift);
   d6f92:	5c1a      	ldrb	r2, [r3, r0]
   d6f94:	68a3      	ldr	r3, [r4, #8]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
   d6f96:	69e1      	ldr	r1, [r4, #28]
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val =
              input1_val * (1 << params.left_shift);
          const int32 shifted_input2_val =
              input2_val * (1 << params.left_shift);
   d6f98:	4413      	add	r3, r2
   d6f9a:	fa03 f30e 	lsl.w	r3, r3, lr
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
   d6f9e:	fa0a f00e 	lsl.w	r0, sl, lr
   d6fa2:	6a22      	ldr	r2, [r4, #32]
              params.input2_offset +
              input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val =
              input1_val * (1 << params.left_shift);
          const int32 shifted_input2_val =
              input2_val * (1 << params.left_shift);
   d6fa4:	9303      	str	r3, [sp, #12]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
   d6fa6:	f7ff fb23 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, params.input2_multiplier,
                  params.input2_shift);
   d6faa:	9b03      	ldr	r3, [sp, #12]
   d6fac:	6aa2      	ldr	r2, [r4, #40]	; 0x28
   d6fae:	6a61      	ldr	r1, [r4, #36]	; 0x24
          const int32 shifted_input2_val =
              input2_val * (1 << params.left_shift);
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, params.input1_multiplier,
                  params.input1_shift);
   d6fb0:	4682      	mov	sl, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, params.input2_multiplier,
                  params.input2_shift);
   d6fb2:	4618      	mov	r0, r3
   d6fb4:	f7ff fb1c 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 raw_sum = scaled_input1_val + scaled_input2_val;
          const int32 raw_output =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
   d6fb8:	6962      	ldr	r2, [r4, #20]
   d6fba:	6921      	ldr	r1, [r4, #16]
   d6fbc:	4450      	add	r0, sl
   d6fbe:	f7ff fb17 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
   d6fc2:	68e3      	ldr	r3, [r4, #12]
                  raw_sum, params.output_multiplier, params.output_shift) +
              params.output_offset;
          const int32 clamped_output =
              std::min(params.quantized_activation_max,
                       std::max(params.quantized_activation_min, raw_output));
          output_data[Offset(extended_output_shape, b, y, x, c)] =
   d6fc4:	f8cd 9000 	str.w	r9, [sp]
   d6fc8:	4418      	add	r0, r3
   d6fca:	6ae3      	ldr	r3, [r4, #44]	; 0x2c
   d6fcc:	4283      	cmp	r3, r0
   d6fce:	bfb8      	it	lt
   d6fd0:	4603      	movlt	r3, r0
   d6fd2:	6b20      	ldr	r0, [r4, #48]	; 0x30
   d6fd4:	4283      	cmp	r3, r0
   d6fd6:	bfa8      	it	ge
   d6fd8:	4603      	movge	r3, r0
   d6fda:	469a      	mov	sl, r3
   d6fdc:	4632      	mov	r2, r6
   d6fde:	463b      	mov	r3, r7
   d6fe0:	4629      	mov	r1, r5
   d6fe2:	4640      	mov	r0, r8
   d6fe4:	f7ff faa5 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<uint8>(clamped_output);
   d6fe8:	9b26      	ldr	r3, [sp, #152]	; 0x98
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   d6fea:	f109 0901 	add.w	r9, r9, #1
              params.output_offset;
          const int32 clamped_output =
              std::min(params.quantized_activation_max,
                       std::max(params.quantized_activation_min, raw_output));
          output_data[Offset(extended_output_shape, b, y, x, c)] =
              static_cast<uint8>(clamped_output);
   d6fee:	f803 a000 	strb.w	sl, [r3, r0]
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   d6ff2:	e7b1      	b.n	d6f58 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x58>
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
   d6ff4:	3701      	adds	r7, #1
   d6ff6:	e7a7      	b.n	d6f48 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x48>
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
   d6ff8:	3601      	adds	r6, #1
   d6ffa:	e79c      	b.n	d6f36 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x36>
  // first dimension has smallest stride.
  //
  // We name our variables by their Tensorflow convention, but generate C code
  // nesting loops such that the innermost loop has the smallest stride for the
  // best cache behavior.
  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   d6ffc:	3501      	adds	r5, #1
   d6ffe:	e793      	b.n	d6f28 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x28>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input1_shape, input2_shape, &desc1,
                                      &desc2);
  const RuntimeShape extended_output_shape =
      RuntimeShape::ExtendedShape(4, output_shape);
   d7000:	a805      	add	r0, sp, #20
   d7002:	f7ff fa26 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
              static_cast<uint8>(clamped_output);
        }
      }
    }
  }
}
   d7006:	b01b      	add	sp, #108	; 0x6c
   d7008:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d700c <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4>:

TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
   d700c:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
   d7010:	b0a8      	sub	sp, #160	; 0xa0
   d7012:	461e      	mov	r6, r3
                              const TfLiteTensor* input1,
                              const TfLiteTensor* input2,
                              TfLiteTensor* output) {
  if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt8) {
    tflite::ArithmeticParams op_params;
    op_params.left_shift = data->left_shift;
   d7014:	6a43      	ldr	r3, [r0, #36]	; 0x24
   d7016:	931a      	str	r3, [sp, #104]	; 0x68
    op_params.input1_offset = data->input1_offset;
   d7018:	6a83      	ldr	r3, [r0, #40]	; 0x28
   d701a:	9315      	str	r3, [sp, #84]	; 0x54
    op_params.input1_multiplier = data->input1_multiplier;
   d701c:	6943      	ldr	r3, [r0, #20]
   d701e:	931b      	str	r3, [sp, #108]	; 0x6c
    op_params.input1_shift = data->input1_shift;
   d7020:	6843      	ldr	r3, [r0, #4]
   d7022:	931c      	str	r3, [sp, #112]	; 0x70
    op_params.input2_offset = data->input2_offset;
   d7024:	6ac3      	ldr	r3, [r0, #44]	; 0x2c
   d7026:	9316      	str	r3, [sp, #88]	; 0x58
    op_params.input2_multiplier = data->input2_multiplier;
   d7028:	6983      	ldr	r3, [r0, #24]
   d702a:	931d      	str	r3, [sp, #116]	; 0x74
    op_params.input2_shift = data->input2_shift;
   d702c:	6883      	ldr	r3, [r0, #8]
   d702e:	931e      	str	r3, [sp, #120]	; 0x78
    op_params.output_offset = data->output_offset;
   d7030:	6b03      	ldr	r3, [r0, #48]	; 0x30
   d7032:	9317      	str	r3, [sp, #92]	; 0x5c
    op_params.output_multiplier = data->output_multiplier;
   d7034:	69c3      	ldr	r3, [r0, #28]
   d7036:	9318      	str	r3, [sp, #96]	; 0x60
    op_params.output_shift = data->output_shift;
   d7038:	6a03      	ldr	r3, [r0, #32]
   d703a:	9319      	str	r3, [sp, #100]	; 0x64
    TF_LITE_ADD(Add);
  }
#undef TF_LITE_ADD
}

TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
   d703c:	4614      	mov	r4, r2
    op_params.input2_shift = data->input2_shift;
    op_params.output_offset = data->output_offset;
    op_params.output_multiplier = data->output_multiplier;
    op_params.output_shift = data->output_shift;
    SetActivationParams(data->output_activation_min,
                        data->output_activation_max, &op_params);
   d703e:	6903      	ldr	r3, [r0, #16]
  params->float_activation_max = max;
}

template <typename P>
inline void SetActivationParams(int32 min, int32 max, P* params) {
  params->quantized_activation_min = min;
   d7040:	68c2      	ldr	r2, [r0, #12]
  params->quantized_activation_max = max;
   d7042:	9320      	str	r3, [sp, #128]	; 0x80
    bool need_broadcast = reference_ops::ProcessBroadcastShapes(
        GetTensorShape(input1), GetTensorShape(input2), &op_params);
   d7044:	a80f      	add	r0, sp, #60	; 0x3c
    TF_LITE_ADD(Add);
  }
#undef TF_LITE_ADD
}

TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
   d7046:	460d      	mov	r5, r1
  params->float_activation_max = max;
}

template <typename P>
inline void SetActivationParams(int32 min, int32 max, P* params) {
  params->quantized_activation_min = min;
   d7048:	921f      	str	r2, [sp, #124]	; 0x7c
    op_params.output_multiplier = data->output_multiplier;
    op_params.output_shift = data->output_shift;
    SetActivationParams(data->output_activation_min,
                        data->output_activation_max, &op_params);
    bool need_broadcast = reference_ops::ProcessBroadcastShapes(
        GetTensorShape(input1), GetTensorShape(input2), &op_params);
   d704a:	f7ff fcb2 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d704e:	4621      	mov	r1, r4
   d7050:	a80a      	add	r0, sp, #40	; 0x28
   d7052:	f7ff fcae 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d7056:	a90a      	add	r1, sp, #40	; 0x28
   d7058:	aa14      	add	r2, sp, #80	; 0x50
   d705a:	a80f      	add	r0, sp, #60	; 0x3c
   d705c:	f7ff fbd3 	bl	d6806 <_ZN6tflite13reference_ops22ProcessBroadcastShapesERKNS_12RuntimeShapeES3_PNS_16ArithmeticParamsE>
   d7060:	4680      	mov	r8, r0
   d7062:	a80a      	add	r0, sp, #40	; 0x28
   d7064:	f7ff f9f5 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   d7068:	a80f      	add	r0, sp, #60	; 0x3c
   d706a:	f7ff f9f2 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
#define TF_LITE_ADD(type, opname, dtype)                             \
  type::opname(op_params, GetTensorShape(input1),                    \
               GetTensorData<dtype>(input1), GetTensorShape(input2), \
               GetTensorData<dtype>(input2), GetTensorShape(output), \
               GetTensorData<dtype>(output));
    if (output->type == kTfLiteInt8) {
   d706e:	7833      	ldrb	r3, [r6, #0]
   d7070:	2b09      	cmp	r3, #9
      if (need_broadcast) {
        TF_LITE_ADD(reference_integer_ops, BroadcastAdd4DSlow, int8_t);
   d7072:	4629      	mov	r1, r5
   d7074:	a80f      	add	r0, sp, #60	; 0x3c
   d7076:	af05      	add	r7, sp, #20
#define TF_LITE_ADD(type, opname, dtype)                             \
  type::opname(op_params, GetTensorShape(input1),                    \
               GetTensorData<dtype>(input1), GetTensorShape(input2), \
               GetTensorData<dtype>(input2), GetTensorShape(output), \
               GetTensorData<dtype>(output));
    if (output->type == kTfLiteInt8) {
   d7078:	d134      	bne.n	d70e4 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0xd8>
      if (need_broadcast) {
   d707a:	f1b8 0f00 	cmp.w	r8, #0
   d707e:	d018      	beq.n	d70b2 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0xa6>
        TF_LITE_ADD(reference_integer_ops, BroadcastAdd4DSlow, int8_t);
   d7080:	f7ff fc97 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d7084:	b105      	cbz	r5, d7088 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x7c>
   d7086:	686d      	ldr	r5, [r5, #4]
   d7088:	4621      	mov	r1, r4
   d708a:	a80a      	add	r0, sp, #40	; 0x28
   d708c:	f7ff fc91 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d7090:	b104      	cbz	r4, d7094 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x88>
   d7092:	6864      	ldr	r4, [r4, #4]
   d7094:	4631      	mov	r1, r6
   d7096:	4638      	mov	r0, r7
   d7098:	f7ff fc8b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d709c:	6873      	ldr	r3, [r6, #4]
   d709e:	9302      	str	r3, [sp, #8]
   d70a0:	e88d 0090 	stmia.w	sp, {r4, r7}
   d70a4:	ab0a      	add	r3, sp, #40	; 0x28
   d70a6:	462a      	mov	r2, r5
   d70a8:	a90f      	add	r1, sp, #60	; 0x3c
   d70aa:	a814      	add	r0, sp, #80	; 0x50
   d70ac:	f7ff fea2 	bl	d6df4 <_ZN6tflite21reference_integer_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa>
   d70b0:	e04c      	b.n	d714c <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x140>
      } else {
        TF_LITE_ADD(reference_integer_ops, Add, int8_t);
   d70b2:	f7ff fc7e 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d70b6:	b105      	cbz	r5, d70ba <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0xae>
   d70b8:	686d      	ldr	r5, [r5, #4]
   d70ba:	4621      	mov	r1, r4
   d70bc:	a80a      	add	r0, sp, #40	; 0x28
   d70be:	f7ff fc78 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d70c2:	b104      	cbz	r4, d70c6 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0xba>
   d70c4:	6864      	ldr	r4, [r4, #4]
   d70c6:	4631      	mov	r1, r6
   d70c8:	4638      	mov	r0, r7
   d70ca:	f7ff fc72 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d70ce:	6873      	ldr	r3, [r6, #4]
   d70d0:	9302      	str	r3, [sp, #8]
   d70d2:	e88d 0090 	stmia.w	sp, {r4, r7}
   d70d6:	ab0a      	add	r3, sp, #40	; 0x28
   d70d8:	462a      	mov	r2, r5
   d70da:	a90f      	add	r1, sp, #60	; 0x3c
   d70dc:	a814      	add	r0, sp, #80	; 0x50
   d70de:	f7ff fb48 	bl	d6772 <_ZN6tflite21reference_integer_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_Pa>
   d70e2:	e033      	b.n	d714c <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x140>
      }
    } else {
      if (need_broadcast) {
   d70e4:	f1b8 0f00 	cmp.w	r8, #0
   d70e8:	d018      	beq.n	d711c <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x110>
        TF_LITE_ADD(reference_ops, BroadcastAdd4DSlow, uint8_t);
   d70ea:	f7ff fc62 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d70ee:	b105      	cbz	r5, d70f2 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0xe6>
   d70f0:	686d      	ldr	r5, [r5, #4]
   d70f2:	4621      	mov	r1, r4
   d70f4:	a80a      	add	r0, sp, #40	; 0x28
   d70f6:	f7ff fc5c 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d70fa:	b104      	cbz	r4, d70fe <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0xf2>
   d70fc:	6864      	ldr	r4, [r4, #4]
   d70fe:	4631      	mov	r1, r6
   d7100:	4638      	mov	r0, r7
   d7102:	f7ff fc56 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d7106:	6873      	ldr	r3, [r6, #4]
   d7108:	9302      	str	r3, [sp, #8]
   d710a:	e88d 0090 	stmia.w	sp, {r4, r7}
   d710e:	ab0a      	add	r3, sp, #40	; 0x28
   d7110:	462a      	mov	r2, r5
   d7112:	a90f      	add	r1, sp, #60	; 0x3c
   d7114:	a814      	add	r0, sp, #80	; 0x50
   d7116:	f7ff fef3 	bl	d6f00 <_ZN6tflite13reference_ops18BroadcastAdd4DSlowERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph>
   d711a:	e017      	b.n	d714c <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x140>
      } else {
        TF_LITE_ADD(reference_ops, Add, uint8_t);
   d711c:	f7ff fc49 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d7120:	b105      	cbz	r5, d7124 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x118>
   d7122:	686d      	ldr	r5, [r5, #4]
   d7124:	4621      	mov	r1, r4
   d7126:	a80a      	add	r0, sp, #40	; 0x28
   d7128:	f7ff fc43 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d712c:	b104      	cbz	r4, d7130 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4+0x124>
   d712e:	6864      	ldr	r4, [r4, #4]
   d7130:	4631      	mov	r1, r6
   d7132:	4638      	mov	r0, r7
   d7134:	f7ff fc3d 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d7138:	6873      	ldr	r3, [r6, #4]
   d713a:	9302      	str	r3, [sp, #8]
   d713c:	e88d 0090 	stmia.w	sp, {r4, r7}
   d7140:	ab0a      	add	r3, sp, #40	; 0x28
   d7142:	462a      	mov	r2, r5
   d7144:	a90f      	add	r1, sp, #60	; 0x3c
   d7146:	a814      	add	r0, sp, #80	; 0x50
   d7148:	f7ff fac8 	bl	d66dc <_ZN6tflite13reference_ops3AddERKNS_16ArithmeticParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph>
   d714c:	4638      	mov	r0, r7
   d714e:	f7ff f980 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   d7152:	a80a      	add	r0, sp, #40	; 0x28
   d7154:	f7ff f97d 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   d7158:	a80f      	add	r0, sp, #60	; 0x3c
   d715a:	f7ff f97a 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
    }
#undef TF_LITE_ADD
  }

  return kTfLiteOk;
}
   d715e:	b028      	add	sp, #160	; 0xa0
   d7160:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}

000d7164 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_>:

TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
                              TfLiteAddParams* params, const OpData* data,
                              const TfLiteTensor* input1,
                              const TfLiteTensor* input2,
                              TfLiteTensor* output) {
   d7164:	b508      	push	{r3, lr}
   d7166:	4618      	mov	r0, r3
   d7168:	9b04      	ldr	r3, [sp, #16]
  if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt8) {
   d716a:	781a      	ldrb	r2, [r3, #0]
   d716c:	2a03      	cmp	r2, #3
   d716e:	d001      	beq.n	d7174 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x10>
   d7170:	2a09      	cmp	r2, #9
   d7172:	d103      	bne.n	d717c <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_+0x18>
   d7174:	9a03      	ldr	r2, [sp, #12]
   d7176:	9902      	ldr	r1, [sp, #8]
   d7178:	f7ff ff48 	bl	d700c <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_.part.4>
    }
#undef TF_LITE_ADD
  }

  return kTfLiteOk;
}
   d717c:	2000      	movs	r0, #0
   d717e:	bd08      	pop	{r3, pc}

000d7180 <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   d7180:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d7184:	680a      	ldr	r2, [r1, #0]
   d7186:	f8d0 8008 	ldr.w	r8, [r0, #8]
  auto* params = reinterpret_cast<TfLiteAddParams*>(node->builtin_data);
   d718a:	f8d1 9014 	ldr.w	r9, [r1, #20]
  }

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   d718e:	460f      	mov	r7, r1
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d7190:	6851      	ldr	r1, [r2, #4]
   d7192:	6892      	ldr	r2, [r2, #8]
   d7194:	b095      	sub	sp, #84	; 0x54
   d7196:	2338      	movs	r3, #56	; 0x38
   d7198:	fb03 8202 	mla	r2, r3, r2, r8
   d719c:	9204      	str	r2, [sp, #16]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d719e:	687a      	ldr	r2, [r7, #4]
   d71a0:	6854      	ldr	r4, [r2, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d71a2:	fb03 8b01 	mla	fp, r3, r1, r8
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d71a6:	4363      	muls	r3, r4
   d71a8:	eb08 0403 	add.w	r4, r8, r3
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  OpData data;
  TF_LITE_ENSURE_STATUS(
   d71ac:	f10d 0a1c 	add.w	sl, sp, #28
   d71b0:	9305      	str	r3, [sp, #20]
   d71b2:	e88d 0410 	stmia.w	sp, {r4, sl}
   d71b6:	9b04      	ldr	r3, [sp, #16]
   d71b8:	465a      	mov	r2, fp
   d71ba:	4649      	mov	r1, r9
  }

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   d71bc:	4606      	mov	r6, r0
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  OpData data;
  TF_LITE_ENSURE_STATUS(
   d71be:	f7ff fc11 	bl	d69e4 <_ZN6tflite3ops5micro3add15CalculateOpDataEP13TfLiteContextP15TfLiteAddParamsPK12TfLiteTensorS9_PS7_PNS2_6OpDataE>
   d71c2:	4605      	mov	r5, r0
   d71c4:	bb38      	cbnz	r0, d7216 <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0x96>
      CalculateOpData(context, params, input1, input2, output, &data));

  if (output->type == kTfLiteFloat32) {
   d71c6:	9b05      	ldr	r3, [sp, #20]
   d71c8:	f818 3003 	ldrb.w	r3, [r8, r3]
   d71cc:	2b01      	cmp	r3, #1
   d71ce:	d10b      	bne.n	d71e8 <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0x68>
    EvalAdd(context, node, params, &data, input1, input2, output);
   d71d0:	9b04      	ldr	r3, [sp, #16]
   d71d2:	9301      	str	r3, [sp, #4]
   d71d4:	9402      	str	r4, [sp, #8]
   d71d6:	f8cd b000 	str.w	fp, [sp]
   d71da:	4653      	mov	r3, sl
   d71dc:	464a      	mov	r2, r9
   d71de:	4639      	mov	r1, r7
   d71e0:	4630      	mov	r0, r6
   d71e2:	f7ff fd7d 	bl	d6ce0 <_ZN6tflite3ops5micro3add7EvalAddEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_>
   d71e6:	e017      	b.n	d7218 <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0x98>
  } else if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt8) {
   d71e8:	2b03      	cmp	r3, #3
   d71ea:	d001      	beq.n	d71f0 <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0x70>
   d71ec:	2b09      	cmp	r3, #9
   d71ee:	d10e      	bne.n	d720e <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0x8e>
    TF_LITE_ENSURE_OK(context, EvalAddQuantized(context, node, params, &data,
   d71f0:	9b04      	ldr	r3, [sp, #16]
   d71f2:	9301      	str	r3, [sp, #4]
   d71f4:	9402      	str	r4, [sp, #8]
   d71f6:	f8cd b000 	str.w	fp, [sp]
   d71fa:	4653      	mov	r3, sl
   d71fc:	464a      	mov	r2, r9
   d71fe:	4639      	mov	r1, r7
   d7200:	4630      	mov	r0, r6
   d7202:	f7ff ffaf 	bl	d7164 <_ZN6tflite3ops5micro3add16EvalAddQuantizedEP13TfLiteContextP10TfLiteNodeP15TfLiteAddParamsPKNS2_6OpDataEPK12TfLiteTensorSE_PSC_>
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  OpData data;
  TF_LITE_ENSURE_STATUS(
   d7206:	1c05      	adds	r5, r0, #0
   d7208:	bf18      	it	ne
   d720a:	2501      	movne	r5, #1
   d720c:	e004      	b.n	d7218 <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0x98>
  } else if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt8) {
    TF_LITE_ENSURE_OK(context, EvalAddQuantized(context, node, params, &data,
                                                input1, input2, output));
  } else {
    context->ReportError(context,
                         "Inputs and outputs not all float|uint8|int8 types.");
   d720e:	6973      	ldr	r3, [r6, #20]
   d7210:	4903      	ldr	r1, [pc, #12]	; (d7220 <_ZN6tflite3ops5micro3add4EvalEP13TfLiteContextP10TfLiteNode+0xa0>)
   d7212:	4630      	mov	r0, r6
   d7214:	4798      	blx	r3
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  OpData data;
  TF_LITE_ENSURE_STATUS(
   d7216:	2501      	movs	r5, #1
                         "Inputs and outputs not all float|uint8|int8 types.");
    return kTfLiteError;
  }

  return kTfLiteOk;
}
   d7218:	4628      	mov	r0, r5
   d721a:	b015      	add	sp, #84	; 0x54
   d721c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   d7220:	000e9710 	.word	0x000e9710

000d7224 <_ZN6tflite3ops5micro14AllOpsResolverC1Ev>:
#define TFLITE_REGISTRATIONS_MAX (128)
#endif

namespace tflite {

class MicroMutableOpResolver : public OpResolver {
   d7224:	f241 0304 	movw	r3, #4100	; 0x1004
TfLiteRegistration* Register_UNPACK();
TfLiteRegistration* Register_NEG();
TfLiteRegistration* Register_ADD();
TfLiteRegistration* Register_QUANTIZE();
TfLiteRegistration* Register_DEQUANTIZE();
AllOpsResolver::AllOpsResolver() {
   d7228:	b5f7      	push	{r0, r1, r2, r4, r5, r6, r7, lr}
   d722a:	2700      	movs	r7, #0
   d722c:	50c7      	str	r7, [r0, r3]
   d722e:	4bbd      	ldr	r3, [pc, #756]	; (d7524 <_ZN6tflite3ops5micro14AllOpsResolverC1Ev+0x300>)
   d7230:	6003      	str	r3, [r0, #0]
   d7232:	4605      	mov	r5, r0
  AddBuiltin(BuiltinOperator_DEPTHWISE_CONV_2D, Register_DEPTHWISE_CONV_2D());
   d7234:	f00b fc1a 	bl	e2a6c <_ZN6tflite3ops5micro26Register_DEPTHWISE_CONV_2DEv>
   d7238:	2401      	movs	r4, #1
   d723a:	4623      	mov	r3, r4
   d723c:	4602      	mov	r2, r0
   d723e:	2104      	movs	r1, #4
   d7240:	4628      	mov	r0, r5
   d7242:	9400      	str	r4, [sp, #0]
   d7244:	f7fe fff7 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_FULLY_CONNECTED, Register_FULLY_CONNECTED(),
   d7248:	f006 f948 	bl	dd4dc <_ZN6tflite3ops5micro24Register_FULLY_CONNECTEDEv>
             /* min_version */ 1,
             /* max_version */ 4);
   d724c:	2604      	movs	r6, #4
   d724e:	4623      	mov	r3, r4
   d7250:	4602      	mov	r2, r0
   d7252:	2109      	movs	r1, #9
   d7254:	4628      	mov	r0, r5
   d7256:	9600      	str	r6, [sp, #0]
   d7258:	f7fe ffed 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_MAX_POOL_2D, Register_MAX_POOL_2D());
   d725c:	f008 f840 	bl	df2e0 <_ZN6tflite3ops5micro20Register_MAX_POOL_2DEv>
   d7260:	4623      	mov	r3, r4
   d7262:	4602      	mov	r2, r0
   d7264:	2111      	movs	r1, #17
   d7266:	4628      	mov	r0, r5
   d7268:	9400      	str	r4, [sp, #0]
   d726a:	f7fe ffe4 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_SOFTMAX, Register_SOFTMAX());
   d726e:	f008 fef9 	bl	e0064 <_ZN6tflite3ops5micro16Register_SOFTMAXEv>
   d7272:	4623      	mov	r3, r4
   d7274:	4602      	mov	r2, r0
   d7276:	2119      	movs	r1, #25
   d7278:	4628      	mov	r0, r5
   d727a:	9400      	str	r4, [sp, #0]
   d727c:	f7fe ffdb 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LOGISTIC, Register_LOGISTIC());
   d7280:	f006 fb66 	bl	dd950 <_ZN6tflite3ops5micro17Register_LOGISTICEv>
   d7284:	4623      	mov	r3, r4
   d7286:	4602      	mov	r2, r0
   d7288:	210e      	movs	r1, #14
   d728a:	4628      	mov	r0, r5
   d728c:	9400      	str	r4, [sp, #0]
   d728e:	f7fe ffd2 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_SVDF, Register_SVDF());
   d7292:	f00a ffad 	bl	e21f0 <_ZN6tflite3ops5micro13Register_SVDFEv>
   d7296:	4623      	mov	r3, r4
   d7298:	4602      	mov	r2, r0
   d729a:	211b      	movs	r1, #27
   d729c:	4628      	mov	r0, r5
   d729e:	9400      	str	r4, [sp, #0]
   d72a0:	f7fe ffc9 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_CONV_2D, Register_CONV_2D());
   d72a4:	f005 fb08 	bl	dc8b8 <_ZN6tflite3ops5micro16Register_CONV_2DEv>
   d72a8:	4623      	mov	r3, r4
   d72aa:	4602      	mov	r2, r0
   d72ac:	2103      	movs	r1, #3
   d72ae:	4628      	mov	r0, r5
   d72b0:	9400      	str	r4, [sp, #0]
   d72b2:	f7fe ffc0 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_AVERAGE_POOL_2D, Register_AVERAGE_POOL_2D());
   d72b6:	f008 f80f 	bl	df2d8 <_ZN6tflite3ops5micro24Register_AVERAGE_POOL_2DEv>
   d72ba:	4623      	mov	r3, r4
   d72bc:	4602      	mov	r2, r0
   d72be:	4621      	mov	r1, r4
   d72c0:	4628      	mov	r0, r5
   d72c2:	9400      	str	r4, [sp, #0]
   d72c4:	f7fe ffb7 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_ABS, Register_ABS());
   d72c8:	f005 fd4a 	bl	dcd60 <_ZN6tflite3ops5micro12Register_ABSEv>
   d72cc:	4623      	mov	r3, r4
   d72ce:	4602      	mov	r2, r0
   d72d0:	2165      	movs	r1, #101	; 0x65
   d72d2:	4628      	mov	r0, r5
   d72d4:	9400      	str	r4, [sp, #0]
   d72d6:	f7fe ffae 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_SIN, Register_SIN());
   d72da:	f005 fd45 	bl	dcd68 <_ZN6tflite3ops5micro12Register_SINEv>
   d72de:	4623      	mov	r3, r4
   d72e0:	4602      	mov	r2, r0
   d72e2:	2142      	movs	r1, #66	; 0x42
   d72e4:	4628      	mov	r0, r5
   d72e6:	9400      	str	r4, [sp, #0]
   d72e8:	f7fe ffa5 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_COS, Register_COS());
   d72ec:	f005 fd40 	bl	dcd70 <_ZN6tflite3ops5micro12Register_COSEv>
   d72f0:	4623      	mov	r3, r4
   d72f2:	4602      	mov	r2, r0
   d72f4:	216c      	movs	r1, #108	; 0x6c
   d72f6:	4628      	mov	r0, r5
   d72f8:	9400      	str	r4, [sp, #0]
   d72fa:	f7fe ff9c 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LOG, Register_LOG());
   d72fe:	f005 fd3b 	bl	dcd78 <_ZN6tflite3ops5micro12Register_LOGEv>
   d7302:	4623      	mov	r3, r4
   d7304:	4602      	mov	r2, r0
   d7306:	2149      	movs	r1, #73	; 0x49
   d7308:	4628      	mov	r0, r5
   d730a:	9400      	str	r4, [sp, #0]
   d730c:	f7fe ff93 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_SQRT, Register_SQRT());
   d7310:	f005 fd36 	bl	dcd80 <_ZN6tflite3ops5micro13Register_SQRTEv>
   d7314:	4623      	mov	r3, r4
   d7316:	4602      	mov	r2, r0
   d7318:	214b      	movs	r1, #75	; 0x4b
   d731a:	4628      	mov	r0, r5
   d731c:	9400      	str	r4, [sp, #0]
   d731e:	f7fe ff8a 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_RSQRT, Register_RSQRT());
   d7322:	f005 fd31 	bl	dcd88 <_ZN6tflite3ops5micro14Register_RSQRTEv>
   d7326:	4623      	mov	r3, r4
   d7328:	4602      	mov	r2, r0
   d732a:	214c      	movs	r1, #76	; 0x4c
   d732c:	4628      	mov	r0, r5
   d732e:	9400      	str	r4, [sp, #0]
   d7330:	f7fe ff81 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_SQUARE, Register_SQUARE());
   d7334:	f005 fd2c 	bl	dcd90 <_ZN6tflite3ops5micro15Register_SQUAREEv>
   d7338:	4623      	mov	r3, r4
   d733a:	4602      	mov	r2, r0
   d733c:	215c      	movs	r1, #92	; 0x5c
   d733e:	4628      	mov	r0, r5
   d7340:	9400      	str	r4, [sp, #0]
   d7342:	f7fe ff78 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_PRELU, Register_PRELU());
   d7346:	f007 ffd1 	bl	df2ec <_ZN6tflite3ops5micro14Register_PRELUEv>
   d734a:	4623      	mov	r3, r4
   d734c:	4602      	mov	r2, r0
   d734e:	2136      	movs	r1, #54	; 0x36
   d7350:	4628      	mov	r0, r5
   d7352:	9400      	str	r4, [sp, #0]
   d7354:	f7fe ff6f 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_FLOOR, Register_FLOOR());
   d7358:	f005 fd8e 	bl	dce78 <_ZN6tflite3ops5micro14Register_FLOOREv>
   d735c:	4623      	mov	r3, r4
   d735e:	4602      	mov	r2, r0
   d7360:	2108      	movs	r1, #8
   d7362:	4628      	mov	r0, r5
   d7364:	9400      	str	r4, [sp, #0]
   d7366:	f7fe ff66 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_MAXIMUM, Register_MAXIMUM());
   d736a:	f006 fb2f 	bl	dd9cc <_ZN6tflite3ops5micro16Register_MAXIMUMEv>
   d736e:	4623      	mov	r3, r4
   d7370:	4602      	mov	r2, r0
   d7372:	2137      	movs	r1, #55	; 0x37
   d7374:	4628      	mov	r0, r5
   d7376:	9400      	str	r4, [sp, #0]
   d7378:	f7fe ff5d 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_MINIMUM, Register_MINIMUM());
   d737c:	f006 fb2a 	bl	dd9d4 <_ZN6tflite3ops5micro16Register_MINIMUMEv>
   d7380:	4623      	mov	r3, r4
   d7382:	4602      	mov	r2, r0
   d7384:	2139      	movs	r1, #57	; 0x39
   d7386:	4628      	mov	r0, r5
   d7388:	9400      	str	r4, [sp, #0]
   d738a:	f7fe ff54 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_ARG_MAX, Register_ARG_MAX());
   d738e:	f000 f8cd 	bl	d752c <_ZN6tflite3ops5micro16Register_ARG_MAXEv>
   d7392:	4623      	mov	r3, r4
   d7394:	4602      	mov	r2, r0
   d7396:	2138      	movs	r1, #56	; 0x38
   d7398:	4628      	mov	r0, r5
   d739a:	9400      	str	r4, [sp, #0]
   d739c:	f7fe ff4b 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_ARG_MIN, Register_ARG_MIN());
   d73a0:	f000 f8c8 	bl	d7534 <_ZN6tflite3ops5micro16Register_ARG_MINEv>
   d73a4:	4623      	mov	r3, r4
   d73a6:	4602      	mov	r2, r0
   d73a8:	214f      	movs	r1, #79	; 0x4f
   d73aa:	4628      	mov	r0, r5
   d73ac:	9400      	str	r4, [sp, #0]
   d73ae:	f7fe ff42 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LOGICAL_OR, Register_LOGICAL_OR());
   d73b2:	f006 f8a1 	bl	dd4f8 <_ZN6tflite3ops5micro19Register_LOGICAL_OREv>
   d73b6:	4623      	mov	r3, r4
   d73b8:	4602      	mov	r2, r0
   d73ba:	2154      	movs	r1, #84	; 0x54
   d73bc:	4628      	mov	r0, r5
   d73be:	9400      	str	r4, [sp, #0]
   d73c0:	f7fe ff39 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LOGICAL_AND, Register_LOGICAL_AND());
   d73c4:	f006 f89c 	bl	dd500 <_ZN6tflite3ops5micro20Register_LOGICAL_ANDEv>
   d73c8:	4623      	mov	r3, r4
   d73ca:	4602      	mov	r2, r0
   d73cc:	2156      	movs	r1, #86	; 0x56
   d73ce:	4628      	mov	r0, r5
   d73d0:	9400      	str	r4, [sp, #0]
   d73d2:	f7fe ff30 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LOGICAL_NOT, Register_LOGICAL_NOT());
   d73d6:	f005 fcdf 	bl	dcd98 <_ZN6tflite3ops5micro20Register_LOGICAL_NOTEv>
   d73da:	4623      	mov	r3, r4
   d73dc:	4602      	mov	r2, r0
   d73de:	2157      	movs	r1, #87	; 0x57
   d73e0:	4628      	mov	r0, r5
   d73e2:	9400      	str	r4, [sp, #0]
   d73e4:	f7fe ff27 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_RESHAPE, Register_RESHAPE());
   d73e8:	f008 fb7a 	bl	dfae0 <_ZN6tflite3ops5micro16Register_RESHAPEEv>
   d73ec:	4623      	mov	r3, r4
   d73ee:	4602      	mov	r2, r0
   d73f0:	2116      	movs	r1, #22
   d73f2:	4628      	mov	r0, r5
   d73f4:	9400      	str	r4, [sp, #0]
   d73f6:	f7fe ff1e 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_EQUAL, Register_EQUAL());
   d73fa:	f000 fd53 	bl	d7ea4 <_ZN6tflite3ops5micro14Register_EQUALEv>
   d73fe:	4623      	mov	r3, r4
   d7400:	4602      	mov	r2, r0
   d7402:	2147      	movs	r1, #71	; 0x47
   d7404:	4628      	mov	r0, r5
   d7406:	9400      	str	r4, [sp, #0]
   d7408:	f7fe ff15 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_NOT_EQUAL, Register_NOT_EQUAL());
   d740c:	f000 fd4e 	bl	d7eac <_ZN6tflite3ops5micro18Register_NOT_EQUALEv>
   d7410:	4623      	mov	r3, r4
   d7412:	4602      	mov	r2, r0
   d7414:	2148      	movs	r1, #72	; 0x48
   d7416:	4628      	mov	r0, r5
   d7418:	9400      	str	r4, [sp, #0]
   d741a:	f7fe ff0c 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_GREATER, Register_GREATER());
   d741e:	f000 fd49 	bl	d7eb4 <_ZN6tflite3ops5micro16Register_GREATEREv>
   d7422:	4623      	mov	r3, r4
   d7424:	4602      	mov	r2, r0
   d7426:	213d      	movs	r1, #61	; 0x3d
   d7428:	4628      	mov	r0, r5
   d742a:	9400      	str	r4, [sp, #0]
   d742c:	f7fe ff03 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_GREATER_EQUAL, Register_GREATER_EQUAL());
   d7430:	f000 fd44 	bl	d7ebc <_ZN6tflite3ops5micro22Register_GREATER_EQUALEv>
   d7434:	4623      	mov	r3, r4
   d7436:	4602      	mov	r2, r0
   d7438:	213e      	movs	r1, #62	; 0x3e
   d743a:	4628      	mov	r0, r5
   d743c:	9400      	str	r4, [sp, #0]
   d743e:	f7fe fefa 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LESS, Register_LESS());
   d7442:	f000 fd3f 	bl	d7ec4 <_ZN6tflite3ops5micro13Register_LESSEv>
   d7446:	4623      	mov	r3, r4
   d7448:	4602      	mov	r2, r0
   d744a:	213a      	movs	r1, #58	; 0x3a
   d744c:	4628      	mov	r0, r5
   d744e:	9400      	str	r4, [sp, #0]
   d7450:	f7fe fef1 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_LESS_EQUAL, Register_LESS_EQUAL());
   d7454:	f000 fd3a 	bl	d7ecc <_ZN6tflite3ops5micro19Register_LESS_EQUALEv>
   d7458:	4623      	mov	r3, r4
   d745a:	4602      	mov	r2, r0
   d745c:	213f      	movs	r1, #63	; 0x3f
   d745e:	4628      	mov	r0, r5
   d7460:	9400      	str	r4, [sp, #0]
   d7462:	f7fe fee8 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_CEIL, Register_CEIL());
   d7466:	f000 fd19 	bl	d7e9c <_ZN6tflite3ops5micro13Register_CEILEv>
   d746a:	4623      	mov	r3, r4
   d746c:	4602      	mov	r2, r0
   d746e:	2168      	movs	r1, #104	; 0x68
   d7470:	4628      	mov	r0, r5
   d7472:	9400      	str	r4, [sp, #0]
   d7474:	f7fe fedf 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_ROUND, Register_ROUND());
   d7478:	f008 fc5e 	bl	dfd38 <_ZN6tflite3ops5micro14Register_ROUNDEv>
   d747c:	4623      	mov	r3, r4
   d747e:	4602      	mov	r2, r0
   d7480:	2174      	movs	r1, #116	; 0x74
   d7482:	4628      	mov	r0, r5
   d7484:	9400      	str	r4, [sp, #0]
   d7486:	f7fe fed6 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_STRIDED_SLICE, Register_STRIDED_SLICE());
   d748a:	f009 fe37 	bl	e10fc <_ZN6tflite3ops5micro22Register_STRIDED_SLICEEv>
   d748e:	4623      	mov	r3, r4
   d7490:	4602      	mov	r2, r0
   d7492:	212d      	movs	r1, #45	; 0x2d
   d7494:	4628      	mov	r0, r5
   d7496:	9400      	str	r4, [sp, #0]
   d7498:	f7fe fecd 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_PACK, Register_PACK());
   d749c:	f007 f926 	bl	de6ec <_ZN6tflite3ops5micro13Register_PACKEv>
   d74a0:	4623      	mov	r3, r4
   d74a2:	4602      	mov	r2, r0
   d74a4:	2153      	movs	r1, #83	; 0x53
   d74a6:	4628      	mov	r0, r5
   d74a8:	9400      	str	r4, [sp, #0]
   d74aa:	f7fe fec4 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_SPLIT, Register_SPLIT(),
   d74ae:	f009 f913 	bl	e06d8 <_ZN6tflite3ops5micro14Register_SPLITEv>
             /* min_version */ 1,
             /* max_version */ 3);
   d74b2:	2303      	movs	r3, #3
   d74b4:	4602      	mov	r2, r0
   d74b6:	9300      	str	r3, [sp, #0]
   d74b8:	2131      	movs	r1, #49	; 0x31
   d74ba:	4623      	mov	r3, r4
   d74bc:	4628      	mov	r0, r5
   d74be:	f7fe feba 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_UNPACK, Register_UNPACK());
   d74c2:	f00b f87d 	bl	e25c0 <_ZN6tflite3ops5micro15Register_UNPACKEv>
   d74c6:	4623      	mov	r3, r4
   d74c8:	4602      	mov	r2, r0
   d74ca:	2158      	movs	r1, #88	; 0x58
   d74cc:	4628      	mov	r0, r5
   d74ce:	9400      	str	r4, [sp, #0]
   d74d0:	f7fe feb1 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_NEG, Register_NEG());
   d74d4:	f006 fecc 	bl	de270 <_ZN6tflite3ops5micro12Register_NEGEv>
   d74d8:	4623      	mov	r3, r4
   d74da:	4602      	mov	r2, r0
   d74dc:	213b      	movs	r1, #59	; 0x3b
   d74de:	4628      	mov	r0, r5
   d74e0:	9400      	str	r4, [sp, #0]
   d74e2:	f7fe fea8 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_ADD, Register_ADD());
   d74e6:	f7ff fb09 	bl	d6afc <_ZN6tflite3ops5micro12Register_ADDEv>
   d74ea:	4623      	mov	r3, r4
   d74ec:	4602      	mov	r2, r0
   d74ee:	4639      	mov	r1, r7
   d74f0:	4628      	mov	r0, r5
   d74f2:	9400      	str	r4, [sp, #0]
   d74f4:	f7fe fe9f 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_QUANTIZE, Register_QUANTIZE(), 1, 4);
   d74f8:	f008 fa0e 	bl	df918 <_ZN6tflite3ops5micro17Register_QUANTIZEEv>
   d74fc:	4623      	mov	r3, r4
   d74fe:	4602      	mov	r2, r0
   d7500:	2172      	movs	r1, #114	; 0x72
   d7502:	4628      	mov	r0, r5
   d7504:	9600      	str	r6, [sp, #0]
   d7506:	f7fe fe96 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
  AddBuiltin(BuiltinOperator_DEQUANTIZE, Register_DEQUANTIZE(), 1, 4);
   d750a:	f005 fadd 	bl	dcac8 <_ZN6tflite3ops5micro19Register_DEQUANTIZEEv>
   d750e:	9600      	str	r6, [sp, #0]
   d7510:	4602      	mov	r2, r0
   d7512:	4623      	mov	r3, r4
   d7514:	4628      	mov	r0, r5
   d7516:	2106      	movs	r1, #6
   d7518:	f7fe fe8d 	bl	d6236 <_ZN6tflite22MicroMutableOpResolver10AddBuiltinENS_15BuiltinOperatorEP18TfLiteRegistrationii>
}
   d751c:	4628      	mov	r0, r5
   d751e:	b003      	add	sp, #12
   d7520:	bdf0      	pop	{r4, r5, r6, r7, pc}
   d7522:	bf00      	nop
   d7524:	000e97a8 	.word	0x000e97a8

000d7528 <_ZN6tflite3ops5micro11arg_min_max7PrepareEP13TfLiteContextP10TfLiteNode>:
constexpr int kAxis = 1;
constexpr int kOutputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   d7528:	2000      	movs	r0, #0
   d752a:	4770      	bx	lr

000d752c <_ZN6tflite3ops5micro16Register_ARG_MAXEv>:

TfLiteRegistration* Register_ARG_MAX() {
  static TfLiteRegistration r = {nullptr, nullptr, arg_min_max::Prepare,
                                 arg_min_max::ArgMaxEval};
  return &r;
}
   d752c:	4800      	ldr	r0, [pc, #0]	; (d7530 <_ZN6tflite3ops5micro16Register_ARG_MAXEv+0x4>)
   d752e:	4770      	bx	lr
   d7530:	2003be9c 	.word	0x2003be9c

000d7534 <_ZN6tflite3ops5micro16Register_ARG_MINEv>:

TfLiteRegistration* Register_ARG_MIN() {
  static TfLiteRegistration r = {nullptr, nullptr, arg_min_max::Prepare,
                                 arg_min_max::ArgMinEval};
  return &r;
}
   d7534:	4800      	ldr	r0, [pc, #0]	; (d7538 <_ZN6tflite3ops5micro16Register_ARG_MINEv+0x4>)
   d7536:	4770      	bx	lr
   d7538:	2003bebc 	.word	0x2003bebc

000d753c <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>:
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d753c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d7540:	6805      	ldr	r5, [r0, #0]
   d7542:	b087      	sub	sp, #28
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d7544:	2d00      	cmp	r5, #0
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d7546:	4606      	mov	r6, r0
   d7548:	9105      	str	r1, [sp, #20]
   d754a:	461f      	mov	r7, r3
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d754c:	dc01      	bgt.n	d7552 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x16>
   d754e:	f00c fefd 	bl	e434c <abort>
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
   d7552:	6839      	ldr	r1, [r7, #0]
   d7554:	1e6b      	subs	r3, r5, #1
   d7556:	428b      	cmp	r3, r1
   d7558:	d1f9      	bne.n	d754e <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
   d755a:	6814      	ldr	r4, [r2, #0]
  if (axis < 0) {
   d755c:	2c00      	cmp	r4, #0
    axis += input1_shape.DimensionsCount();
   d755e:	bfb8      	it	lt
   d7560:	1964      	addlt	r4, r4, r5
  }
  const int axis_size = input1_shape.Dims(axis);
   d7562:	4621      	mov	r1, r4
   d7564:	f7fe ff80 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d7568:	f04f 0a00 	mov.w	sl, #0
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
  if (axis < 0) {
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);
   d756c:	4680      	mov	r8, r0

  int outer_size = 1;
   d756e:	f04f 0901 	mov.w	r9, #1
  for (int i = 0; i < axis; ++i) {
   d7572:	4554      	cmp	r4, sl
   d7574:	dd0f      	ble.n	d7596 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x5a>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
   d7576:	4651      	mov	r1, sl
   d7578:	4630      	mov	r0, r6
   d757a:	f7fe ff75 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d757e:	4651      	mov	r1, sl
   d7580:	4683      	mov	fp, r0
   d7582:	4638      	mov	r0, r7
   d7584:	f7fe ff70 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d7588:	4583      	cmp	fp, r0
   d758a:	d1e0      	bne.n	d754e <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    outer_size *= input1_shape.Dims(i);
   d758c:	fb0b f909 	mul.w	r9, fp, r9
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d7590:	f10a 0a01 	add.w	sl, sl, #1
   d7594:	e7ed      	b.n	d7572 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x36>
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d7596:	f104 0a01 	add.w	sl, r4, #1
  for (int i = 0; i < axis; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
   d759a:	2401      	movs	r4, #1
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d759c:	45aa      	cmp	sl, r5
   d759e:	da10      	bge.n	d75c2 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x86>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
   d75a0:	4651      	mov	r1, sl
   d75a2:	4630      	mov	r0, r6
   d75a4:	f7fe ff60 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d75a8:	f10a 31ff 	add.w	r1, sl, #4294967295	; 0xffffffff
   d75ac:	4683      	mov	fp, r0
   d75ae:	4638      	mov	r0, r7
   d75b0:	f7fe ff5a 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d75b4:	4583      	cmp	fp, r0
   d75b6:	d1ca      	bne.n	d754e <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    inner_size *= input1_shape.Dims(i);
   d75b8:	fb0b f404 	mul.w	r4, fp, r4
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d75bc:	f10a 0a01 	add.w	sl, sl, #1
   d75c0:	e7ec      	b.n	d759c <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x60>
   d75c2:	fb04 f308 	mul.w	r3, r4, r8
   d75c6:	9304      	str	r3, [sp, #16]
   d75c8:	f8dd e040 	ldr.w	lr, [sp, #64]	; 0x40
   d75cc:	2300      	movs	r3, #0
   d75ce:	ea4f 0a84 	mov.w	sl, r4, lsl #2
   d75d2:	461f      	mov	r7, r3
   d75d4:	469c      	mov	ip, r3
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d75d6:	45e1      	cmp	r9, ip
   d75d8:	dd31      	ble.n	d763e <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x102>
   d75da:	9a05      	ldr	r2, [sp, #20]
   d75dc:	eb02 0183 	add.w	r1, r2, r3, lsl #2
   d75e0:	fb07 4204 	mla	r2, r7, r4, r4
   d75e4:	1ad2      	subs	r2, r2, r3
   d75e6:	0092      	lsls	r2, r2, #2
   d75e8:	9202      	str	r2, [sp, #8]
   d75ea:	2000      	movs	r0, #0
    for (int inner = 0; inner < inner_size; ++inner) {
   d75ec:	4284      	cmp	r4, r0
   d75ee:	dd1f      	ble.n	d7630 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xf4>
   d75f0:	9a02      	ldr	r2, [sp, #8]
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
   d75f2:	edd1 7a00 	vldr	s15, [r1]
   d75f6:	188a      	adds	r2, r1, r2
   d75f8:	2500      	movs	r5, #0
   d75fa:	9203      	str	r2, [sp, #12]
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d75fc:	2601      	movs	r6, #1
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
   d75fe:	9501      	str	r5, [sp, #4]
      for (int i = 1; i < axis_size; ++i) {
   d7600:	4546      	cmp	r6, r8
   d7602:	da0f      	bge.n	d7624 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xe8>
   d7604:	9a03      	ldr	r2, [sp, #12]
   d7606:	eb02 0b05 	add.w	fp, r2, r5
   d760a:	ed9b 7a00 	vldr	s14, [fp]
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
   d760e:	eef4 7ac7 	vcmpe.f32	s15, s14
   d7612:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d7616:	bf44      	itt	mi
   d7618:	9601      	strmi	r6, [sp, #4]
          min_max_value = curr_value;
   d761a:	eef0 7a47 	vmovmi.f32	s15, s14
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d761e:	3601      	adds	r6, #1
   d7620:	4455      	add	r5, sl
   d7622:	e7ed      	b.n	d7600 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xc4>
        if (cmp(curr_value, min_max_value)) {
          min_max_value = curr_value;
          min_max_index = static_cast<T2>(i);
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
   d7624:	9a01      	ldr	r2, [sp, #4]
   d7626:	f84e 2020 	str.w	r2, [lr, r0, lsl #2]
   d762a:	3104      	adds	r1, #4
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
   d762c:	3001      	adds	r0, #1
   d762e:	e7dd      	b.n	d75ec <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xb0>
   d7630:	9a04      	ldr	r2, [sp, #16]
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d7632:	f10c 0c01 	add.w	ip, ip, #1
   d7636:	44d6      	add	lr, sl
   d7638:	4447      	add	r7, r8
   d763a:	4413      	add	r3, r2
   d763c:	e7cb      	b.n	d75d6 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x9a>
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
    }
  }
}
   d763e:	b007      	add	sp, #28
   d7640:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d7644 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>:
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d7644:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d7648:	6805      	ldr	r5, [r0, #0]
   d764a:	b087      	sub	sp, #28
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d764c:	2d00      	cmp	r5, #0
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d764e:	4606      	mov	r6, r0
   d7650:	9105      	str	r1, [sp, #20]
   d7652:	461f      	mov	r7, r3
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d7654:	dc01      	bgt.n	d765a <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x16>
   d7656:	f00c fe79 	bl	e434c <abort>
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
   d765a:	6839      	ldr	r1, [r7, #0]
   d765c:	1e6b      	subs	r3, r5, #1
   d765e:	428b      	cmp	r3, r1
   d7660:	d1f9      	bne.n	d7656 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
   d7662:	6814      	ldr	r4, [r2, #0]
  if (axis < 0) {
   d7664:	2c00      	cmp	r4, #0
    axis += input1_shape.DimensionsCount();
   d7666:	bfb8      	it	lt
   d7668:	1964      	addlt	r4, r4, r5
  }
  const int axis_size = input1_shape.Dims(axis);
   d766a:	4621      	mov	r1, r4
   d766c:	f7fe fefc 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d7670:	f04f 0a00 	mov.w	sl, #0
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
  if (axis < 0) {
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);
   d7674:	4680      	mov	r8, r0

  int outer_size = 1;
   d7676:	f04f 0901 	mov.w	r9, #1
  for (int i = 0; i < axis; ++i) {
   d767a:	4554      	cmp	r4, sl
   d767c:	dd0f      	ble.n	d769e <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x5a>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
   d767e:	4651      	mov	r1, sl
   d7680:	4630      	mov	r0, r6
   d7682:	f7fe fef1 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d7686:	4651      	mov	r1, sl
   d7688:	4683      	mov	fp, r0
   d768a:	4638      	mov	r0, r7
   d768c:	f7fe feec 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d7690:	4583      	cmp	fp, r0
   d7692:	d1e0      	bne.n	d7656 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    outer_size *= input1_shape.Dims(i);
   d7694:	fb0b f909 	mul.w	r9, fp, r9
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d7698:	f10a 0a01 	add.w	sl, sl, #1
   d769c:	e7ed      	b.n	d767a <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x36>
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d769e:	f104 0a01 	add.w	sl, r4, #1
  for (int i = 0; i < axis; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
   d76a2:	2401      	movs	r4, #1
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d76a4:	45aa      	cmp	sl, r5
   d76a6:	da10      	bge.n	d76ca <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x86>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
   d76a8:	4651      	mov	r1, sl
   d76aa:	4630      	mov	r0, r6
   d76ac:	f7fe fedc 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d76b0:	f10a 31ff 	add.w	r1, sl, #4294967295	; 0xffffffff
   d76b4:	4683      	mov	fp, r0
   d76b6:	4638      	mov	r0, r7
   d76b8:	f7fe fed6 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d76bc:	4583      	cmp	fp, r0
   d76be:	d1ca      	bne.n	d7656 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    inner_size *= input1_shape.Dims(i);
   d76c0:	fb0b f404 	mul.w	r4, fp, r4
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d76c4:	f10a 0a01 	add.w	sl, sl, #1
   d76c8:	e7ec      	b.n	d76a4 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x60>
   d76ca:	fb04 f308 	mul.w	r3, r4, r8
   d76ce:	9304      	str	r3, [sp, #16]
   d76d0:	f8dd e040 	ldr.w	lr, [sp, #64]	; 0x40
   d76d4:	2300      	movs	r3, #0
   d76d6:	ea4f 0a84 	mov.w	sl, r4, lsl #2
   d76da:	461f      	mov	r7, r3
   d76dc:	469c      	mov	ip, r3
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d76de:	45e1      	cmp	r9, ip
   d76e0:	dd31      	ble.n	d7746 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x102>
   d76e2:	9a05      	ldr	r2, [sp, #20]
   d76e4:	eb02 0183 	add.w	r1, r2, r3, lsl #2
   d76e8:	fb07 4204 	mla	r2, r7, r4, r4
   d76ec:	1ad2      	subs	r2, r2, r3
   d76ee:	0092      	lsls	r2, r2, #2
   d76f0:	9202      	str	r2, [sp, #8]
   d76f2:	2000      	movs	r0, #0
    for (int inner = 0; inner < inner_size; ++inner) {
   d76f4:	4284      	cmp	r4, r0
   d76f6:	dd1f      	ble.n	d7738 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xf4>
   d76f8:	9a02      	ldr	r2, [sp, #8]
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
   d76fa:	edd1 7a00 	vldr	s15, [r1]
   d76fe:	188a      	adds	r2, r1, r2
   d7700:	2500      	movs	r5, #0
   d7702:	9203      	str	r2, [sp, #12]
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d7704:	2601      	movs	r6, #1
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
   d7706:	9501      	str	r5, [sp, #4]
      for (int i = 1; i < axis_size; ++i) {
   d7708:	4546      	cmp	r6, r8
   d770a:	da0f      	bge.n	d772c <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xe8>
   d770c:	9a03      	ldr	r2, [sp, #12]
   d770e:	eb02 0b05 	add.w	fp, r2, r5
   d7712:	ed9b 7a00 	vldr	s14, [fp]
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
   d7716:	eef4 7ac7 	vcmpe.f32	s15, s14
   d771a:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d771e:	bfc4      	itt	gt
   d7720:	9601      	strgt	r6, [sp, #4]
          min_max_value = curr_value;
   d7722:	eef0 7a47 	vmovgt.f32	s15, s14
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d7726:	3601      	adds	r6, #1
   d7728:	4455      	add	r5, sl
   d772a:	e7ed      	b.n	d7708 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xc4>
        if (cmp(curr_value, min_max_value)) {
          min_max_value = curr_value;
          min_max_index = static_cast<T2>(i);
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
   d772c:	9a01      	ldr	r2, [sp, #4]
   d772e:	f84e 2020 	str.w	r2, [lr, r0, lsl #2]
   d7732:	3104      	adds	r1, #4
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
   d7734:	3001      	adds	r0, #1
   d7736:	e7dd      	b.n	d76f4 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xb0>
   d7738:	9a04      	ldr	r2, [sp, #16]
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d773a:	f10c 0c01 	add.w	ip, ip, #1
   d773e:	44d6      	add	lr, sl
   d7740:	4447      	add	r7, r8
   d7742:	4413      	add	r3, r2
   d7744:	e7cb      	b.n	d76de <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x9a>
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
    }
  }
}
   d7746:	b007      	add	sp, #28
   d7748:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d774c <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>:
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d774c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d7750:	6806      	ldr	r6, [r0, #0]
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d7752:	2e00      	cmp	r6, #0
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d7754:	b087      	sub	sp, #28
   d7756:	4681      	mov	r9, r0
   d7758:	460f      	mov	r7, r1
   d775a:	469a      	mov	sl, r3
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d775c:	dc01      	bgt.n	d7762 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x16>
   d775e:	f00c fdf5 	bl	e434c <abort>
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
   d7762:	f8da 1000 	ldr.w	r1, [sl]
   d7766:	1e73      	subs	r3, r6, #1
   d7768:	428b      	cmp	r3, r1
   d776a:	d1f8      	bne.n	d775e <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
   d776c:	6814      	ldr	r4, [r2, #0]
  if (axis < 0) {
   d776e:	2c00      	cmp	r4, #0
    axis += input1_shape.DimensionsCount();
   d7770:	bfb8      	it	lt
   d7772:	19a4      	addlt	r4, r4, r6
  }
  const int axis_size = input1_shape.Dims(axis);
   d7774:	4621      	mov	r1, r4
   d7776:	f7fe fe77 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d777a:	f04f 0b00 	mov.w	fp, #0
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
  if (axis < 0) {
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);
   d777e:	4605      	mov	r5, r0

  int outer_size = 1;
   d7780:	f04f 0801 	mov.w	r8, #1
  for (int i = 0; i < axis; ++i) {
   d7784:	455c      	cmp	r4, fp
   d7786:	dd10      	ble.n	d77aa <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x5e>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
   d7788:	4659      	mov	r1, fp
   d778a:	4648      	mov	r0, r9
   d778c:	f7fe fe6c 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d7790:	4659      	mov	r1, fp
   d7792:	9001      	str	r0, [sp, #4]
   d7794:	4650      	mov	r0, sl
   d7796:	f7fe fe67 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d779a:	9b01      	ldr	r3, [sp, #4]
   d779c:	4283      	cmp	r3, r0
   d779e:	d1de      	bne.n	d775e <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    outer_size *= input1_shape.Dims(i);
   d77a0:	fb03 f808 	mul.w	r8, r3, r8
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d77a4:	f10b 0b01 	add.w	fp, fp, #1
   d77a8:	e7ec      	b.n	d7784 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x38>
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d77aa:	f104 0b01 	add.w	fp, r4, #1
  for (int i = 0; i < axis; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
   d77ae:	2401      	movs	r4, #1
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d77b0:	45b3      	cmp	fp, r6
   d77b2:	da10      	bge.n	d77d6 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x8a>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
   d77b4:	4659      	mov	r1, fp
   d77b6:	4648      	mov	r0, r9
   d77b8:	f7fe fe56 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d77bc:	f10b 31ff 	add.w	r1, fp, #4294967295	; 0xffffffff
   d77c0:	9001      	str	r0, [sp, #4]
   d77c2:	4650      	mov	r0, sl
   d77c4:	f7fe fe50 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d77c8:	9b01      	ldr	r3, [sp, #4]
   d77ca:	4283      	cmp	r3, r0
   d77cc:	d1c7      	bne.n	d775e <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    inner_size *= input1_shape.Dims(i);
   d77ce:	435c      	muls	r4, r3
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d77d0:	f10b 0b01 	add.w	fp, fp, #1
   d77d4:	e7ec      	b.n	d77b0 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x64>
   d77d6:	00a3      	lsls	r3, r4, #2
   d77d8:	9302      	str	r3, [sp, #8]
   d77da:	2200      	movs	r2, #0
   d77dc:	fb05 f304 	mul.w	r3, r5, r4
   d77e0:	f8dd e040 	ldr.w	lr, [sp, #64]	; 0x40
   d77e4:	9304      	str	r3, [sp, #16]
   d77e6:	9701      	str	r7, [sp, #4]
   d77e8:	4694      	mov	ip, r2
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d77ea:	45e0      	cmp	r8, ip
   d77ec:	dd29      	ble.n	d7842 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xf6>
   d77ee:	fb02 4304 	mla	r3, r2, r4, r4
   d77f2:	9305      	str	r3, [sp, #20]
   d77f4:	2300      	movs	r3, #0
    for (int inner = 0; inner < inner_size; ++inner) {
   d77f6:	429c      	cmp	r4, r3
   d77f8:	dd19      	ble.n	d782e <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xe2>
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
   d77fa:	9901      	ldr	r1, [sp, #4]
   d77fc:	f811 a003 	ldrb.w	sl, [r1, r3]
   d7800:	9905      	ldr	r1, [sp, #20]
   d7802:	1859      	adds	r1, r3, r1
   d7804:	1879      	adds	r1, r7, r1
   d7806:	9103      	str	r1, [sp, #12]
   d7808:	2100      	movs	r1, #0
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d780a:	2001      	movs	r0, #1
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
   d780c:	4689      	mov	r9, r1
      for (int i = 1; i < axis_size; ++i) {
   d780e:	42a8      	cmp	r0, r5
   d7810:	da09      	bge.n	d7826 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xda>
   d7812:	9e03      	ldr	r6, [sp, #12]
   d7814:	f816 b001 	ldrb.w	fp, [r6, r1]
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
   d7818:	45da      	cmp	sl, fp
   d781a:	bf3c      	itt	cc
   d781c:	4681      	movcc	r9, r0
   d781e:	46da      	movcc	sl, fp
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d7820:	3001      	adds	r0, #1
   d7822:	4421      	add	r1, r4
   d7824:	e7f3      	b.n	d780e <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xc2>
        if (cmp(curr_value, min_max_value)) {
          min_max_value = curr_value;
          min_max_index = static_cast<T2>(i);
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
   d7826:	f84e 9023 	str.w	r9, [lr, r3, lsl #2]
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
   d782a:	3301      	adds	r3, #1
   d782c:	e7e3      	b.n	d77f6 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xaa>
   d782e:	9b02      	ldr	r3, [sp, #8]
   d7830:	9901      	ldr	r1, [sp, #4]
   d7832:	449e      	add	lr, r3
   d7834:	9b04      	ldr	r3, [sp, #16]
   d7836:	4419      	add	r1, r3
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d7838:	f10c 0c01 	add.w	ip, ip, #1
   d783c:	9101      	str	r1, [sp, #4]
   d783e:	442a      	add	r2, r5
   d7840:	e7d3      	b.n	d77ea <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x9e>
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
    }
  }
}
   d7842:	b007      	add	sp, #28
   d7844:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d7848 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>:
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d7848:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d784c:	6806      	ldr	r6, [r0, #0]
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d784e:	2e00      	cmp	r6, #0
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d7850:	b087      	sub	sp, #28
   d7852:	4681      	mov	r9, r0
   d7854:	460f      	mov	r7, r1
   d7856:	469a      	mov	sl, r3
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d7858:	dc01      	bgt.n	d785e <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x16>
   d785a:	f00c fd77 	bl	e434c <abort>
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
   d785e:	f8da 1000 	ldr.w	r1, [sl]
   d7862:	1e73      	subs	r3, r6, #1
   d7864:	428b      	cmp	r3, r1
   d7866:	d1f8      	bne.n	d785a <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
   d7868:	6814      	ldr	r4, [r2, #0]
  if (axis < 0) {
   d786a:	2c00      	cmp	r4, #0
    axis += input1_shape.DimensionsCount();
   d786c:	bfb8      	it	lt
   d786e:	19a4      	addlt	r4, r4, r6
  }
  const int axis_size = input1_shape.Dims(axis);
   d7870:	4621      	mov	r1, r4
   d7872:	f7fe fdf9 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d7876:	f04f 0b00 	mov.w	fp, #0
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
  if (axis < 0) {
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);
   d787a:	4605      	mov	r5, r0

  int outer_size = 1;
   d787c:	f04f 0801 	mov.w	r8, #1
  for (int i = 0; i < axis; ++i) {
   d7880:	455c      	cmp	r4, fp
   d7882:	dd10      	ble.n	d78a6 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x5e>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
   d7884:	4659      	mov	r1, fp
   d7886:	4648      	mov	r0, r9
   d7888:	f7fe fdee 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d788c:	4659      	mov	r1, fp
   d788e:	9001      	str	r0, [sp, #4]
   d7890:	4650      	mov	r0, sl
   d7892:	f7fe fde9 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d7896:	9b01      	ldr	r3, [sp, #4]
   d7898:	4283      	cmp	r3, r0
   d789a:	d1de      	bne.n	d785a <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    outer_size *= input1_shape.Dims(i);
   d789c:	fb03 f808 	mul.w	r8, r3, r8
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d78a0:	f10b 0b01 	add.w	fp, fp, #1
   d78a4:	e7ec      	b.n	d7880 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x38>
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d78a6:	f104 0b01 	add.w	fp, r4, #1
  for (int i = 0; i < axis; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
   d78aa:	2401      	movs	r4, #1
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d78ac:	45b3      	cmp	fp, r6
   d78ae:	da10      	bge.n	d78d2 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x8a>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
   d78b0:	4659      	mov	r1, fp
   d78b2:	4648      	mov	r0, r9
   d78b4:	f7fe fdd8 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d78b8:	f10b 31ff 	add.w	r1, fp, #4294967295	; 0xffffffff
   d78bc:	9001      	str	r0, [sp, #4]
   d78be:	4650      	mov	r0, sl
   d78c0:	f7fe fdd2 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d78c4:	9b01      	ldr	r3, [sp, #4]
   d78c6:	4283      	cmp	r3, r0
   d78c8:	d1c7      	bne.n	d785a <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    inner_size *= input1_shape.Dims(i);
   d78ca:	435c      	muls	r4, r3
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d78cc:	f10b 0b01 	add.w	fp, fp, #1
   d78d0:	e7ec      	b.n	d78ac <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x64>
   d78d2:	00a3      	lsls	r3, r4, #2
   d78d4:	9302      	str	r3, [sp, #8]
   d78d6:	2200      	movs	r2, #0
   d78d8:	fb05 f304 	mul.w	r3, r5, r4
   d78dc:	f8dd e040 	ldr.w	lr, [sp, #64]	; 0x40
   d78e0:	9304      	str	r3, [sp, #16]
   d78e2:	9701      	str	r7, [sp, #4]
   d78e4:	4694      	mov	ip, r2
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d78e6:	45e0      	cmp	r8, ip
   d78e8:	dd29      	ble.n	d793e <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xf6>
   d78ea:	fb02 4304 	mla	r3, r2, r4, r4
   d78ee:	9305      	str	r3, [sp, #20]
   d78f0:	2300      	movs	r3, #0
    for (int inner = 0; inner < inner_size; ++inner) {
   d78f2:	429c      	cmp	r4, r3
   d78f4:	dd19      	ble.n	d792a <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xe2>
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
   d78f6:	9901      	ldr	r1, [sp, #4]
   d78f8:	f811 a003 	ldrb.w	sl, [r1, r3]
   d78fc:	9905      	ldr	r1, [sp, #20]
   d78fe:	1859      	adds	r1, r3, r1
   d7900:	1879      	adds	r1, r7, r1
   d7902:	9103      	str	r1, [sp, #12]
   d7904:	2100      	movs	r1, #0
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d7906:	2001      	movs	r0, #1
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
   d7908:	4689      	mov	r9, r1
      for (int i = 1; i < axis_size; ++i) {
   d790a:	42a8      	cmp	r0, r5
   d790c:	da09      	bge.n	d7922 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xda>
   d790e:	9e03      	ldr	r6, [sp, #12]
   d7910:	f816 b001 	ldrb.w	fp, [r6, r1]
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
   d7914:	45da      	cmp	sl, fp
   d7916:	bf84      	itt	hi
   d7918:	4681      	movhi	r9, r0
   d791a:	46da      	movhi	sl, fp
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d791c:	3001      	adds	r0, #1
   d791e:	4421      	add	r1, r4
   d7920:	e7f3      	b.n	d790a <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xc2>
        if (cmp(curr_value, min_max_value)) {
          min_max_value = curr_value;
          min_max_index = static_cast<T2>(i);
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
   d7922:	f84e 9023 	str.w	r9, [lr, r3, lsl #2]
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
   d7926:	3301      	adds	r3, #1
   d7928:	e7e3      	b.n	d78f2 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xaa>
   d792a:	9b02      	ldr	r3, [sp, #8]
   d792c:	9901      	ldr	r1, [sp, #4]
   d792e:	449e      	add	lr, r3
   d7930:	9b04      	ldr	r3, [sp, #16]
   d7932:	4419      	add	r1, r3
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d7934:	f10c 0c01 	add.w	ip, ip, #1
   d7938:	9101      	str	r1, [sp, #4]
   d793a:	442a      	add	r2, r5
   d793c:	e7d3      	b.n	d78e6 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x9e>
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
    }
  }
}
   d793e:	b007      	add	sp, #28
   d7940:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d7944 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>:
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d7944:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d7948:	6806      	ldr	r6, [r0, #0]
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d794a:	2e00      	cmp	r6, #0
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d794c:	b087      	sub	sp, #28
   d794e:	4681      	mov	r9, r0
   d7950:	460f      	mov	r7, r1
   d7952:	469a      	mov	sl, r3
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d7954:	dc01      	bgt.n	d795a <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x16>
   d7956:	f00c fcf9 	bl	e434c <abort>
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
   d795a:	f8da 1000 	ldr.w	r1, [sl]
   d795e:	1e73      	subs	r3, r6, #1
   d7960:	428b      	cmp	r3, r1
   d7962:	d1f8      	bne.n	d7956 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
   d7964:	6814      	ldr	r4, [r2, #0]
  if (axis < 0) {
   d7966:	2c00      	cmp	r4, #0
    axis += input1_shape.DimensionsCount();
   d7968:	bfb8      	it	lt
   d796a:	19a4      	addlt	r4, r4, r6
  }
  const int axis_size = input1_shape.Dims(axis);
   d796c:	4621      	mov	r1, r4
   d796e:	f7fe fd7b 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d7972:	f04f 0b00 	mov.w	fp, #0
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
  if (axis < 0) {
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);
   d7976:	4605      	mov	r5, r0

  int outer_size = 1;
   d7978:	f04f 0801 	mov.w	r8, #1
  for (int i = 0; i < axis; ++i) {
   d797c:	455c      	cmp	r4, fp
   d797e:	dd10      	ble.n	d79a2 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x5e>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
   d7980:	4659      	mov	r1, fp
   d7982:	4648      	mov	r0, r9
   d7984:	f7fe fd70 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d7988:	4659      	mov	r1, fp
   d798a:	9001      	str	r0, [sp, #4]
   d798c:	4650      	mov	r0, sl
   d798e:	f7fe fd6b 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d7992:	9b01      	ldr	r3, [sp, #4]
   d7994:	4283      	cmp	r3, r0
   d7996:	d1de      	bne.n	d7956 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    outer_size *= input1_shape.Dims(i);
   d7998:	fb03 f808 	mul.w	r8, r3, r8
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d799c:	f10b 0b01 	add.w	fp, fp, #1
   d79a0:	e7ec      	b.n	d797c <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x38>
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d79a2:	f104 0b01 	add.w	fp, r4, #1
  for (int i = 0; i < axis; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
   d79a6:	2401      	movs	r4, #1
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d79a8:	45b3      	cmp	fp, r6
   d79aa:	da10      	bge.n	d79ce <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x8a>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
   d79ac:	4659      	mov	r1, fp
   d79ae:	4648      	mov	r0, r9
   d79b0:	f7fe fd5a 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d79b4:	f10b 31ff 	add.w	r1, fp, #4294967295	; 0xffffffff
   d79b8:	9001      	str	r0, [sp, #4]
   d79ba:	4650      	mov	r0, sl
   d79bc:	f7fe fd54 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d79c0:	9b01      	ldr	r3, [sp, #4]
   d79c2:	4283      	cmp	r3, r0
   d79c4:	d1c7      	bne.n	d7956 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    inner_size *= input1_shape.Dims(i);
   d79c6:	435c      	muls	r4, r3
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d79c8:	f10b 0b01 	add.w	fp, fp, #1
   d79cc:	e7ec      	b.n	d79a8 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x64>
   d79ce:	00a3      	lsls	r3, r4, #2
   d79d0:	9302      	str	r3, [sp, #8]
   d79d2:	2200      	movs	r2, #0
   d79d4:	fb05 f304 	mul.w	r3, r5, r4
   d79d8:	f8dd e040 	ldr.w	lr, [sp, #64]	; 0x40
   d79dc:	9304      	str	r3, [sp, #16]
   d79de:	9701      	str	r7, [sp, #4]
   d79e0:	4694      	mov	ip, r2
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d79e2:	45e0      	cmp	r8, ip
   d79e4:	dd29      	ble.n	d7a3a <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xf6>
   d79e6:	fb02 4304 	mla	r3, r2, r4, r4
   d79ea:	9305      	str	r3, [sp, #20]
   d79ec:	2300      	movs	r3, #0
    for (int inner = 0; inner < inner_size; ++inner) {
   d79ee:	429c      	cmp	r4, r3
   d79f0:	dd19      	ble.n	d7a26 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xe2>
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
   d79f2:	9901      	ldr	r1, [sp, #4]
   d79f4:	f911 a003 	ldrsb.w	sl, [r1, r3]
   d79f8:	9905      	ldr	r1, [sp, #20]
   d79fa:	1859      	adds	r1, r3, r1
   d79fc:	1879      	adds	r1, r7, r1
   d79fe:	9103      	str	r1, [sp, #12]
   d7a00:	2100      	movs	r1, #0
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d7a02:	2001      	movs	r0, #1
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
   d7a04:	4689      	mov	r9, r1
      for (int i = 1; i < axis_size; ++i) {
   d7a06:	42a8      	cmp	r0, r5
   d7a08:	da09      	bge.n	d7a1e <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xda>
   d7a0a:	9e03      	ldr	r6, [sp, #12]
   d7a0c:	f916 b001 	ldrsb.w	fp, [r6, r1]
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
   d7a10:	45da      	cmp	sl, fp
   d7a12:	bfbc      	itt	lt
   d7a14:	4681      	movlt	r9, r0
   d7a16:	46da      	movlt	sl, fp
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d7a18:	3001      	adds	r0, #1
   d7a1a:	4421      	add	r1, r4
   d7a1c:	e7f3      	b.n	d7a06 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xc2>
        if (cmp(curr_value, min_max_value)) {
          min_max_value = curr_value;
          min_max_index = static_cast<T2>(i);
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
   d7a1e:	f84e 9023 	str.w	r9, [lr, r3, lsl #2]
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
   d7a22:	3301      	adds	r3, #1
   d7a24:	e7e3      	b.n	d79ee <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xaa>
   d7a26:	9b02      	ldr	r3, [sp, #8]
   d7a28:	9901      	ldr	r1, [sp, #4]
   d7a2a:	449e      	add	lr, r3
   d7a2c:	9b04      	ldr	r3, [sp, #16]
   d7a2e:	4419      	add	r1, r3
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d7a30:	f10c 0c01 	add.w	ip, ip, #1
   d7a34:	9101      	str	r1, [sp, #4]
   d7a36:	442a      	add	r2, r5
   d7a38:	e7d3      	b.n	d79e2 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x9e>
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
    }
  }
}
   d7a3a:	b007      	add	sp, #28
   d7a3c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d7a40 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>:
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d7a40:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d7a44:	6806      	ldr	r6, [r0, #0]
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d7a46:	2e00      	cmp	r6, #0
namespace tflite {

namespace reference_ops {

template <typename T1, typename T2, typename T3, typename Cmp>
void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
   d7a48:	b087      	sub	sp, #28
   d7a4a:	4681      	mov	r9, r0
   d7a4c:	460f      	mov	r7, r1
   d7a4e:	469a      	mov	sl, r3
               const T3* input2_data, const RuntimeShape& output_shape,
               T2* output_data, const Cmp& cmp) {
  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);
   d7a50:	dc01      	bgt.n	d7a56 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x16>
   d7a52:	f00c fc7b 	bl	e434c <abort>
  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,
   d7a56:	f8da 1000 	ldr.w	r1, [sl]
   d7a5a:	1e73      	subs	r3, r6, #1
   d7a5c:	428b      	cmp	r3, r1
   d7a5e:	d1f8      	bne.n	d7a52 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
   d7a60:	6814      	ldr	r4, [r2, #0]
  if (axis < 0) {
   d7a62:	2c00      	cmp	r4, #0
    axis += input1_shape.DimensionsCount();
   d7a64:	bfb8      	it	lt
   d7a66:	19a4      	addlt	r4, r4, r6
  }
  const int axis_size = input1_shape.Dims(axis);
   d7a68:	4621      	mov	r1, r4
   d7a6a:	f7fe fcfd 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d7a6e:	f04f 0b00 	mov.w	fp, #0
                   output_shape.DimensionsCount());
  int axis = input2_data[0];
  if (axis < 0) {
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);
   d7a72:	4605      	mov	r5, r0

  int outer_size = 1;
   d7a74:	f04f 0801 	mov.w	r8, #1
  for (int i = 0; i < axis; ++i) {
   d7a78:	455c      	cmp	r4, fp
   d7a7a:	dd10      	ble.n	d7a9e <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x5e>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
   d7a7c:	4659      	mov	r1, fp
   d7a7e:	4648      	mov	r0, r9
   d7a80:	f7fe fcf2 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d7a84:	4659      	mov	r1, fp
   d7a86:	9001      	str	r0, [sp, #4]
   d7a88:	4650      	mov	r0, sl
   d7a8a:	f7fe fced 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d7a8e:	9b01      	ldr	r3, [sp, #4]
   d7a90:	4283      	cmp	r3, r0
   d7a92:	d1de      	bne.n	d7a52 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    outer_size *= input1_shape.Dims(i);
   d7a94:	fb03 f808 	mul.w	r8, r3, r8
    axis += input1_shape.DimensionsCount();
  }
  const int axis_size = input1_shape.Dims(axis);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   d7a98:	f10b 0b01 	add.w	fp, fp, #1
   d7a9c:	e7ec      	b.n	d7a78 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x38>
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d7a9e:	f104 0b01 	add.w	fp, r4, #1
  for (int i = 0; i < axis; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
   d7aa2:	2401      	movs	r4, #1
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d7aa4:	45b3      	cmp	fp, r6
   d7aa6:	da10      	bge.n	d7aca <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x8a>
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
   d7aa8:	4659      	mov	r1, fp
   d7aaa:	4648      	mov	r0, r9
   d7aac:	f7fe fcdc 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d7ab0:	f10b 31ff 	add.w	r1, fp, #4294967295	; 0xffffffff
   d7ab4:	9001      	str	r0, [sp, #4]
   d7ab6:	4650      	mov	r0, sl
   d7ab8:	f7fe fcd6 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d7abc:	9b01      	ldr	r3, [sp, #4]
   d7abe:	4283      	cmp	r3, r0
   d7ac0:	d1c7      	bne.n	d7a52 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x12>
    inner_size *= input1_shape.Dims(i);
   d7ac2:	435c      	muls	r4, r3
    outer_size *= input1_shape.Dims(i);
  }

  int inner_size = 1;
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
   d7ac4:	f10b 0b01 	add.w	fp, fp, #1
   d7ac8:	e7ec      	b.n	d7aa4 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x64>
   d7aca:	00a3      	lsls	r3, r4, #2
   d7acc:	9302      	str	r3, [sp, #8]
   d7ace:	2200      	movs	r2, #0
   d7ad0:	fb05 f304 	mul.w	r3, r5, r4
   d7ad4:	f8dd e040 	ldr.w	lr, [sp, #64]	; 0x40
   d7ad8:	9304      	str	r3, [sp, #16]
   d7ada:	9701      	str	r7, [sp, #4]
   d7adc:	4694      	mov	ip, r2
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d7ade:	45e0      	cmp	r8, ip
   d7ae0:	dd29      	ble.n	d7b36 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xf6>
   d7ae2:	fb02 4304 	mla	r3, r2, r4, r4
   d7ae6:	9305      	str	r3, [sp, #20]
   d7ae8:	2300      	movs	r3, #0
    for (int inner = 0; inner < inner_size; ++inner) {
   d7aea:	429c      	cmp	r4, r3
   d7aec:	dd19      	ble.n	d7b22 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xe2>
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
   d7aee:	9901      	ldr	r1, [sp, #4]
   d7af0:	f911 a003 	ldrsb.w	sl, [r1, r3]
   d7af4:	9905      	ldr	r1, [sp, #20]
   d7af6:	1859      	adds	r1, r3, r1
   d7af8:	1879      	adds	r1, r7, r1
   d7afa:	9103      	str	r1, [sp, #12]
   d7afc:	2100      	movs	r1, #0
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d7afe:	2001      	movs	r0, #1
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
   d7b00:	4689      	mov	r9, r1
      for (int i = 1; i < axis_size; ++i) {
   d7b02:	42a8      	cmp	r0, r5
   d7b04:	da09      	bge.n	d7b1a <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xda>
   d7b06:	9e03      	ldr	r6, [sp, #12]
   d7b08:	f916 b001 	ldrsb.w	fp, [r6, r1]
        const auto& curr_value =
            input1_data[(outer * axis_size + i) * inner_size + inner];
        if (cmp(curr_value, min_max_value)) {
   d7b0c:	45da      	cmp	sl, fp
   d7b0e:	bfc4      	itt	gt
   d7b10:	4681      	movgt	r9, r0
   d7b12:	46da      	movgt	sl, fp
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
      auto min_max_value = input1_data[outer * axis_size * inner_size + inner];
      T2 min_max_index = 0;
      for (int i = 1; i < axis_size; ++i) {
   d7b14:	3001      	adds	r0, #1
   d7b16:	4421      	add	r1, r4
   d7b18:	e7f3      	b.n	d7b02 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xc2>
        if (cmp(curr_value, min_max_value)) {
          min_max_value = curr_value;
          min_max_index = static_cast<T2>(i);
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
   d7b1a:	f84e 9023 	str.w	r9, [lr, r3, lsl #2]
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
    for (int inner = 0; inner < inner_size; ++inner) {
   d7b1e:	3301      	adds	r3, #1
   d7b20:	e7e3      	b.n	d7aea <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0xaa>
   d7b22:	9b02      	ldr	r3, [sp, #8]
   d7b24:	9901      	ldr	r1, [sp, #4]
   d7b26:	449e      	add	lr, r3
   d7b28:	9b04      	ldr	r3, [sp, #16]
   d7b2a:	4419      	add	r1, r3
  const int dims_count = input1_shape.DimensionsCount();
  for (int i = axis + 1; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));
    inner_size *= input1_shape.Dims(i);
  }
  for (int outer = 0; outer < outer_size; ++outer) {
   d7b2c:	f10c 0c01 	add.w	ip, ip, #1
   d7b30:	9101      	str	r1, [sp, #4]
   d7b32:	442a      	add	r2, r5
   d7b34:	e7d3      	b.n	d7ade <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_+0x9e>
        }
      }
      output_data[outer * inner_size + inner] = min_max_index;
    }
  }
}
   d7b36:	b007      	add	sp, #28
   d7b38:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d7b3c <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb>:
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
                             output_shape, output_data, micro::Less());
  }
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node, bool is_arg_max) {
   d7b3c:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
   d7b40:	680e      	ldr	r6, [r1, #0]
   d7b42:	4604      	mov	r4, r0
   d7b44:	4617      	mov	r7, r2
   d7b46:	6882      	ldr	r2, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d7b48:	68b0      	ldr	r0, [r6, #8]
   d7b4a:	2338      	movs	r3, #56	; 0x38
   d7b4c:	4358      	muls	r0, r3
   d7b4e:	eb02 0800 	add.w	r8, r2, r0

#define TF_LITE_ARG_MIN_MAX(data_type, axis_type, output_type)            \
  ArgMinMaxHelper(GetTensorShape(input), GetTensorData<data_type>(input), \
                  GetTensorData<axis_type>(axis), GetTensorShape(output), \
                  GetTensorData<output_type>(output), is_arg_max)
  if (axis->type == kTfLiteInt32) {
   d7b52:	5c10      	ldrb	r0, [r2, r0]
   d7b54:	2802      	cmp	r0, #2
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
                             output_shape, output_data, micro::Less());
  }
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node, bool is_arg_max) {
   d7b56:	b08e      	sub	sp, #56	; 0x38

#define TF_LITE_ARG_MIN_MAX(data_type, axis_type, output_type)            \
  ArgMinMaxHelper(GetTensorShape(input), GetTensorData<data_type>(input), \
                  GetTensorData<axis_type>(axis), GetTensorShape(output), \
                  GetTensorData<output_type>(output), is_arg_max)
  if (axis->type == kTfLiteInt32) {
   d7b58:	d16b      	bne.n	d7c32 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xf6>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d7b5a:	6849      	ldr	r1, [r1, #4]
   d7b5c:	6849      	ldr	r1, [r1, #4]
   d7b5e:	4359      	muls	r1, r3
   d7b60:	1855      	adds	r5, r2, r1
    if (output->type == kTfLiteInt32) {
   d7b62:	5c50      	ldrb	r0, [r2, r1]
   d7b64:	2802      	cmp	r0, #2
   d7b66:	d164      	bne.n	d7c32 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xf6>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d7b68:	6871      	ldr	r1, [r6, #4]
   d7b6a:	434b      	muls	r3, r1
   d7b6c:	18d6      	adds	r6, r2, r3
      switch (input->type) {
   d7b6e:	5cd0      	ldrb	r0, [r2, r3]
   d7b70:	2803      	cmp	r0, #3
   d7b72:	d01d      	beq.n	d7bb0 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0x74>
   d7b74:	2809      	cmp	r0, #9
   d7b76:	d035      	beq.n	d7be4 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xa8>
   d7b78:	2801      	cmp	r0, #1
   d7b7a:	d154      	bne.n	d7c26 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xea>
        case kTfLiteFloat32:
          TF_LITE_ARG_MIN_MAX(float, int32_t, int32_t);
   d7b7c:	4631      	mov	r1, r6
   d7b7e:	a804      	add	r0, sp, #16
   d7b80:	f7fe ff17 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d7b84:	6874      	ldr	r4, [r6, #4]
   d7b86:	f8d8 6004 	ldr.w	r6, [r8, #4]
   d7b8a:	4629      	mov	r1, r5
   d7b8c:	a809      	add	r0, sp, #36	; 0x24
   d7b8e:	f7fe ff10 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d7b92:	686b      	ldr	r3, [r5, #4]
inline void ArgMinMaxHelper(const RuntimeShape& input1_shape,
                            const T1* input1_data, const T3* input2_data,
                            const RuntimeShape& output_shape, T2* output_data,
                            bool is_arg_max) {
  if (is_arg_max) {
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
   d7b94:	9300      	str	r3, [sp, #0]
   d7b96:	aa03      	add	r2, sp, #12
   d7b98:	9201      	str	r2, [sp, #4]
   d7b9a:	ab09      	add	r3, sp, #36	; 0x24
   d7b9c:	4632      	mov	r2, r6
   d7b9e:	4621      	mov	r1, r4
   d7ba0:	a804      	add	r0, sp, #16
template <typename T1, typename T2, typename T3>
inline void ArgMinMaxHelper(const RuntimeShape& input1_shape,
                            const T1* input1_data, const T3* input2_data,
                            const RuntimeShape& output_shape, T2* output_data,
                            bool is_arg_max) {
  if (is_arg_max) {
   d7ba2:	b117      	cbz	r7, d7baa <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0x6e>
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
   d7ba4:	f7ff fcca 	bl	d753c <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>
   d7ba8:	e035      	b.n	d7c16 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xda>
                             output_shape, output_data, micro::Greater());
  } else {
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
   d7baa:	f7ff fd4b 	bl	d7644 <_ZN6tflite13reference_ops9ArgMinMaxIfllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>
   d7bae:	e032      	b.n	d7c16 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xda>
      switch (input->type) {
        case kTfLiteFloat32:
          TF_LITE_ARG_MIN_MAX(float, int32_t, int32_t);
          break;
        case kTfLiteUInt8:
          TF_LITE_ARG_MIN_MAX(uint8_t, int32_t, int32_t);
   d7bb0:	4631      	mov	r1, r6
   d7bb2:	a804      	add	r0, sp, #16
   d7bb4:	f7fe fefd 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d7bb8:	6874      	ldr	r4, [r6, #4]
   d7bba:	f8d8 6004 	ldr.w	r6, [r8, #4]
   d7bbe:	4629      	mov	r1, r5
   d7bc0:	a809      	add	r0, sp, #36	; 0x24
   d7bc2:	f7fe fef6 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d7bc6:	686b      	ldr	r3, [r5, #4]
inline void ArgMinMaxHelper(const RuntimeShape& input1_shape,
                            const T1* input1_data, const T3* input2_data,
                            const RuntimeShape& output_shape, T2* output_data,
                            bool is_arg_max) {
  if (is_arg_max) {
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
   d7bc8:	9300      	str	r3, [sp, #0]
   d7bca:	aa03      	add	r2, sp, #12
   d7bcc:	9201      	str	r2, [sp, #4]
   d7bce:	ab09      	add	r3, sp, #36	; 0x24
   d7bd0:	4632      	mov	r2, r6
   d7bd2:	4621      	mov	r1, r4
   d7bd4:	a804      	add	r0, sp, #16
template <typename T1, typename T2, typename T3>
inline void ArgMinMaxHelper(const RuntimeShape& input1_shape,
                            const T1* input1_data, const T3* input2_data,
                            const RuntimeShape& output_shape, T2* output_data,
                            bool is_arg_max) {
  if (is_arg_max) {
   d7bd6:	b117      	cbz	r7, d7bde <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xa2>
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
   d7bd8:	f7ff fdb8 	bl	d774c <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>
   d7bdc:	e01b      	b.n	d7c16 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xda>
                             output_shape, output_data, micro::Greater());
  } else {
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
   d7bde:	f7ff fe33 	bl	d7848 <_ZN6tflite13reference_ops9ArgMinMaxIhllNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>
   d7be2:	e018      	b.n	d7c16 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xda>
          break;
        case kTfLiteUInt8:
          TF_LITE_ARG_MIN_MAX(uint8_t, int32_t, int32_t);
          break;
        case kTfLiteInt8:
          TF_LITE_ARG_MIN_MAX(int8_t, int32_t, int32_t);
   d7be4:	4631      	mov	r1, r6
   d7be6:	a804      	add	r0, sp, #16
   d7be8:	f7fe fee3 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d7bec:	6874      	ldr	r4, [r6, #4]
   d7bee:	f8d8 6004 	ldr.w	r6, [r8, #4]
   d7bf2:	4629      	mov	r1, r5
   d7bf4:	a809      	add	r0, sp, #36	; 0x24
   d7bf6:	f7fe fedc 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d7bfa:	686b      	ldr	r3, [r5, #4]
inline void ArgMinMaxHelper(const RuntimeShape& input1_shape,
                            const T1* input1_data, const T3* input2_data,
                            const RuntimeShape& output_shape, T2* output_data,
                            bool is_arg_max) {
  if (is_arg_max) {
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
   d7bfc:	9300      	str	r3, [sp, #0]
   d7bfe:	aa03      	add	r2, sp, #12
   d7c00:	9201      	str	r2, [sp, #4]
   d7c02:	ab09      	add	r3, sp, #36	; 0x24
   d7c04:	4632      	mov	r2, r6
   d7c06:	4621      	mov	r1, r4
   d7c08:	a804      	add	r0, sp, #16
template <typename T1, typename T2, typename T3>
inline void ArgMinMaxHelper(const RuntimeShape& input1_shape,
                            const T1* input1_data, const T3* input2_data,
                            const RuntimeShape& output_shape, T2* output_data,
                            bool is_arg_max) {
  if (is_arg_max) {
   d7c0a:	b117      	cbz	r7, d7c12 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xd6>
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
   d7c0c:	f7ff fe9a 	bl	d7944 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro7GreaterEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>
   d7c10:	e001      	b.n	d7c16 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0xda>
                             output_shape, output_data, micro::Greater());
  } else {
    reference_ops::ArgMinMax(input1_shape, input1_data, input2_data,
   d7c12:	f7ff ff15 	bl	d7a40 <_ZN6tflite13reference_ops9ArgMinMaxIallNS_3ops5micro4LessEEEvRKNS_12RuntimeShapeEPKT_PKT1_S7_PT0_RKT2_>
          break;
        case kTfLiteUInt8:
          TF_LITE_ARG_MIN_MAX(uint8_t, int32_t, int32_t);
          break;
        case kTfLiteInt8:
          TF_LITE_ARG_MIN_MAX(int8_t, int32_t, int32_t);
   d7c16:	a809      	add	r0, sp, #36	; 0x24
   d7c18:	f7fe fc1b 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   d7c1c:	a804      	add	r0, sp, #16
   d7c1e:	f7fe fc18 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
    return kTfLiteError;
  }

#undef TF_LITE_ARG_MIN_MAX

  return kTfLiteOk;
   d7c22:	2000      	movs	r0, #0
        case kTfLiteUInt8:
          TF_LITE_ARG_MIN_MAX(uint8_t, int32_t, int32_t);
          break;
        case kTfLiteInt8:
          TF_LITE_ARG_MIN_MAX(int8_t, int32_t, int32_t);
          break;
   d7c24:	e00d      	b.n	d7c42 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0x106>
        default:
          context->ReportError(context,
   d7c26:	6965      	ldr	r5, [r4, #20]
   d7c28:	f7fc fa78 	bl	d411c <TfLiteTypeGetName>
                               "Only float32, uint8 and int8 are "
                               "supported currently, got %s.",
                               TfLiteTypeGetName(input->type));
   d7c2c:	4906      	ldr	r1, [pc, #24]	; (d7c48 <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0x10c>)
   d7c2e:	4602      	mov	r2, r0
   d7c30:	e004      	b.n	d7c3c <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0x100>
                           "Only int32 are supported currently, got %s.",
                           TfLiteTypeGetName(output->type));
      return kTfLiteError;
    }
  } else {
    context->ReportError(context, "Only int32 are supported currently, got %s.",
   d7c32:	6965      	ldr	r5, [r4, #20]
   d7c34:	f7fc fa72 	bl	d411c <TfLiteTypeGetName>
                         TfLiteTypeGetName(axis->type));
   d7c38:	4904      	ldr	r1, [pc, #16]	; (d7c4c <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb+0x110>)
   d7c3a:	4602      	mov	r2, r0
   d7c3c:	4620      	mov	r0, r4
   d7c3e:	47a8      	blx	r5
    return kTfLiteError;
   d7c40:	2001      	movs	r0, #1
  }

#undef TF_LITE_ARG_MIN_MAX

  return kTfLiteOk;
}
   d7c42:	b00e      	add	sp, #56	; 0x38
   d7c44:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   d7c48:	000e97b8 	.word	0x000e97b8
   d7c4c:	000e97f6 	.word	0x000e97f6

000d7c50 <_ZN6tflite3ops5micro11arg_min_max10ArgMinEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus ArgMinEval(TfLiteContext* context, TfLiteNode* node) {
  return Eval(context, node, false);
   d7c50:	2200      	movs	r2, #0
   d7c52:	f7ff bf73 	b.w	d7b3c <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb>

000d7c56 <_ZN6tflite3ops5micro11arg_min_max10ArgMaxEvalEP13TfLiteContextP10TfLiteNode>:
}

TfLiteStatus ArgMaxEval(TfLiteContext* context, TfLiteNode* node) {
  return Eval(context, node, true);
   d7c56:	2201      	movs	r2, #1
   d7c58:	f7ff bf70 	b.w	d7b3c <_ZN6tflite3ops5micro11arg_min_max4EvalEP13TfLiteContextP10TfLiteNodeb>

000d7c5c <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode>:
namespace ceil {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   d7c5c:	b5f0      	push	{r4, r5, r6, r7, lr}
   d7c5e:	680b      	ldr	r3, [r1, #0]
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
   d7c60:	681e      	ldr	r6, [r3, #0]
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   d7c62:	2e01      	cmp	r6, #1
namespace ceil {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   d7c64:	b085      	sub	sp, #20
   d7c66:	4605      	mov	r5, r0
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   d7c68:	d009      	beq.n	d7c7e <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x22>
   d7c6a:	4b3b      	ldr	r3, [pc, #236]	; (d7d58 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xfc>)
   d7c6c:	9301      	str	r3, [sp, #4]
   d7c6e:	2401      	movs	r4, #1
   d7c70:	4b3a      	ldr	r3, [pc, #232]	; (d7d5c <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x100>)
   d7c72:	9300      	str	r3, [sp, #0]
   d7c74:	9403      	str	r4, [sp, #12]
   d7c76:	9602      	str	r6, [sp, #8]
   d7c78:	6945      	ldr	r5, [r0, #20]
   d7c7a:	2321      	movs	r3, #33	; 0x21
   d7c7c:	e01e      	b.n	d7cbc <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
   d7c7e:	f8d1 e004 	ldr.w	lr, [r1, #4]
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
   d7c82:	f8de 4000 	ldr.w	r4, [lr]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
   d7c86:	2c01      	cmp	r4, #1
   d7c88:	d008      	beq.n	d7c9c <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x40>
   d7c8a:	4b33      	ldr	r3, [pc, #204]	; (d7d58 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xfc>)
   d7c8c:	9301      	str	r3, [sp, #4]
   d7c8e:	4b34      	ldr	r3, [pc, #208]	; (d7d60 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x104>)
   d7c90:	9300      	str	r3, [sp, #0]
   d7c92:	9603      	str	r6, [sp, #12]
   d7c94:	9402      	str	r4, [sp, #8]
   d7c96:	6944      	ldr	r4, [r0, #20]
   d7c98:	2322      	movs	r3, #34	; 0x22
   d7c9a:	e022      	b.n	d7ce2 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x86>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d7c9c:	6859      	ldr	r1, [r3, #4]
   d7c9e:	6882      	ldr	r2, [r0, #8]
   d7ca0:	2338      	movs	r3, #56	; 0x38
   d7ca2:	4359      	muls	r1, r3
   d7ca4:	1857      	adds	r7, r2, r1
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
   d7ca6:	5c56      	ldrb	r6, [r2, r1]
   d7ca8:	2e01      	cmp	r6, #1
   d7caa:	d00b      	beq.n	d7cc4 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x68>
   d7cac:	4b2d      	ldr	r3, [pc, #180]	; (d7d64 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x108>)
   d7cae:	9301      	str	r3, [sp, #4]
   d7cb0:	4b2d      	ldr	r3, [pc, #180]	; (d7d68 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x10c>)
   d7cb2:	9300      	str	r3, [sp, #0]
   d7cb4:	9403      	str	r4, [sp, #12]
   d7cb6:	9602      	str	r6, [sp, #8]
   d7cb8:	6945      	ldr	r5, [r0, #20]
   d7cba:	2323      	movs	r3, #35	; 0x23
   d7cbc:	4a2b      	ldr	r2, [pc, #172]	; (d7d6c <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x110>)
   d7cbe:	492c      	ldr	r1, [pc, #176]	; (d7d70 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x114>)
   d7cc0:	47a8      	blx	r5
   d7cc2:	e042      	b.n	d7d4a <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xee>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d7cc4:	f8de 1004 	ldr.w	r1, [lr, #4]
   d7cc8:	434b      	muls	r3, r1
   d7cca:	18d1      	adds	r1, r2, r3
  TF_LITE_ENSURE_EQ(context, output->type, input->type);
   d7ccc:	5cd4      	ldrb	r4, [r2, r3]
   d7cce:	2c01      	cmp	r4, #1
   d7cd0:	d00a      	beq.n	d7ce8 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x8c>
   d7cd2:	4b25      	ldr	r3, [pc, #148]	; (d7d68 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x10c>)
   d7cd4:	9301      	str	r3, [sp, #4]
   d7cd6:	4b27      	ldr	r3, [pc, #156]	; (d7d74 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x118>)
   d7cd8:	9300      	str	r3, [sp, #0]
   d7cda:	9603      	str	r6, [sp, #12]
   d7cdc:	9402      	str	r4, [sp, #8]
   d7cde:	6944      	ldr	r4, [r0, #20]
   d7ce0:	2324      	movs	r3, #36	; 0x24
   d7ce2:	4a22      	ldr	r2, [pc, #136]	; (d7d6c <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x110>)
   d7ce4:	4922      	ldr	r1, [pc, #136]	; (d7d70 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x114>)
   d7ce6:	e02f      	b.n	d7d48 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xec>
  TF_LITE_ENSURE_EQ(context, output->bytes, input->bytes);
   d7ce8:	698b      	ldr	r3, [r1, #24]
   d7cea:	69ba      	ldr	r2, [r7, #24]
   d7cec:	4293      	cmp	r3, r2
   d7cee:	d008      	beq.n	d7d02 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xa6>
   d7cf0:	9302      	str	r3, [sp, #8]
   d7cf2:	4b21      	ldr	r3, [pc, #132]	; (d7d78 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x11c>)
   d7cf4:	9301      	str	r3, [sp, #4]
   d7cf6:	4b21      	ldr	r3, [pc, #132]	; (d7d7c <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x120>)
   d7cf8:	9300      	str	r3, [sp, #0]
   d7cfa:	9203      	str	r2, [sp, #12]
   d7cfc:	6945      	ldr	r5, [r0, #20]
   d7cfe:	2325      	movs	r3, #37	; 0x25
   d7d00:	e7dc      	b.n	d7cbc <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
  TF_LITE_ENSURE_EQ(context, output->dims->size, input->dims->size);
   d7d02:	688b      	ldr	r3, [r1, #8]
   d7d04:	68ba      	ldr	r2, [r7, #8]
   d7d06:	681e      	ldr	r6, [r3, #0]
   d7d08:	6811      	ldr	r1, [r2, #0]
   d7d0a:	428e      	cmp	r6, r1
   d7d0c:	d008      	beq.n	d7d20 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xc4>
   d7d0e:	4b1c      	ldr	r3, [pc, #112]	; (d7d80 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x124>)
   d7d10:	9301      	str	r3, [sp, #4]
   d7d12:	4b1c      	ldr	r3, [pc, #112]	; (d7d84 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x128>)
   d7d14:	9300      	str	r3, [sp, #0]
   d7d16:	9103      	str	r1, [sp, #12]
   d7d18:	9602      	str	r6, [sp, #8]
   d7d1a:	6945      	ldr	r5, [r0, #20]
   d7d1c:	2326      	movs	r3, #38	; 0x26
   d7d1e:	e7cd      	b.n	d7cbc <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
   d7d20:	2100      	movs	r1, #0
  for (int i = 0; i < output->dims->size; ++i) {
   d7d22:	42b1      	cmp	r1, r6
   d7d24:	da15      	bge.n	d7d52 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xf6>
    TF_LITE_ENSURE_EQ(context, output->dims->data[i], input->dims->data[i]);
   d7d26:	f853 0f04 	ldr.w	r0, [r3, #4]!
   d7d2a:	f852 4f04 	ldr.w	r4, [r2, #4]!
   d7d2e:	42a0      	cmp	r0, r4
   d7d30:	d00d      	beq.n	d7d4e <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xf2>
   d7d32:	4b15      	ldr	r3, [pc, #84]	; (d7d88 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x12c>)
   d7d34:	9301      	str	r3, [sp, #4]
   d7d36:	4b15      	ldr	r3, [pc, #84]	; (d7d8c <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x130>)
   d7d38:	9002      	str	r0, [sp, #8]
   d7d3a:	9300      	str	r3, [sp, #0]
   d7d3c:	9403      	str	r4, [sp, #12]
   d7d3e:	696c      	ldr	r4, [r5, #20]
   d7d40:	4a0a      	ldr	r2, [pc, #40]	; (d7d6c <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x110>)
   d7d42:	490b      	ldr	r1, [pc, #44]	; (d7d70 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0x114>)
   d7d44:	2328      	movs	r3, #40	; 0x28
   d7d46:	4628      	mov	r0, r5
   d7d48:	47a0      	blx	r4
   d7d4a:	2001      	movs	r0, #1
   d7d4c:	e002      	b.n	d7d54 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xf8>
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
  TF_LITE_ENSURE_EQ(context, output->type, input->type);
  TF_LITE_ENSURE_EQ(context, output->bytes, input->bytes);
  TF_LITE_ENSURE_EQ(context, output->dims->size, input->dims->size);
  for (int i = 0; i < output->dims->size; ++i) {
   d7d4e:	3101      	adds	r1, #1
   d7d50:	e7e7      	b.n	d7d22 <_ZN6tflite3ops5micro4ceil7PrepareEP13TfLiteContextP10TfLiteNode+0xc6>
    TF_LITE_ENSURE_EQ(context, output->dims->data[i], input->dims->data[i]);
  }
  return kTfLiteOk;
   d7d52:	2000      	movs	r0, #0
}
   d7d54:	b005      	add	sp, #20
   d7d56:	bdf0      	pop	{r4, r5, r6, r7, pc}
   d7d58:	000eb295 	.word	0x000eb295
   d7d5c:	000e98e2 	.word	0x000e98e2
   d7d60:	000e98f2 	.word	0x000e98f2
   d7d64:	000ea137 	.word	0x000ea137
   d7d68:	000e9903 	.word	0x000e9903
   d7d6c:	000e9822 	.word	0x000e9822
   d7d70:	000e98c8 	.word	0x000e98c8
   d7d74:	000e990f 	.word	0x000e990f
   d7d78:	000e991c 	.word	0x000e991c
   d7d7c:	000e9929 	.word	0x000e9929
   d7d80:	000e9937 	.word	0x000e9937
   d7d84:	000e9949 	.word	0x000e9949
   d7d88:	000e995c 	.word	0x000e995c
   d7d8c:	000e9971 	.word	0x000e9971

000d7d90 <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl>:
      dims_pointer_ = new int32[dimensions_count];
#endif  // TF_LITE_STATIC_MEMORY
    }
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
   d7d90:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
  }
  // The caller must ensure that the shape is no bigger than 4-D.
  inline const int32* DimsDataUpTo4D() const { return dims_; }

  inline void Resize(int dimensions_count) {
    if (size_ > kMaxSmallSize) {
   d7d92:	6803      	ldr	r3, [r0, #0]
   d7d94:	2b04      	cmp	r3, #4
      dims_pointer_ = new int32[dimensions_count];
#endif  // TF_LITE_STATIC_MEMORY
    }
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
   d7d96:	4604      	mov	r4, r0
   d7d98:	460d      	mov	r5, r1
   d7d9a:	4617      	mov	r7, r2
  }
  // The caller must ensure that the shape is no bigger than 4-D.
  inline const int32* DimsDataUpTo4D() const { return dims_; }

  inline void Resize(int dimensions_count) {
    if (size_ > kMaxSmallSize) {
   d7d9c:	dd03      	ble.n	d7da6 <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl+0x16>
#ifdef TF_LITE_STATIC_MEMORY
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
   d7d9e:	6840      	ldr	r0, [r0, #4]
   d7da0:	b108      	cbz	r0, d7da6 <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl+0x16>
   d7da2:	f7fc f980 	bl	d40a6 <_ZdaPv>
#endif  // TF_LITE_STATIC_MEMORY
    }
    size_ = dimensions_count;
    if (dimensions_count > kMaxSmallSize) {
   d7da6:	2d04      	cmp	r5, #4
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
#endif  // TF_LITE_STATIC_MEMORY
    }
    size_ = dimensions_count;
   d7da8:	6025      	str	r5, [r4, #0]
   d7daa:	ea4f 0685 	mov.w	r6, r5, lsl #2
    if (dimensions_count > kMaxSmallSize) {
   d7dae:	dd08      	ble.n	d7dc2 <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl+0x32>
#ifdef TF_LITE_STATIC_MEMORY
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      dims_pointer_ = new int32[dimensions_count];
   d7db0:	f1b5 5ffe 	cmp.w	r5, #532676608	; 0x1fc00000
   d7db4:	bfd4      	ite	le
   d7db6:	4630      	movle	r0, r6
   d7db8:	f04f 30ff 	movgt.w	r0, #4294967295	; 0xffffffff
   d7dbc:	f7fc f96f 	bl	d409e <_Znaj>
   d7dc0:	6060      	str	r0, [r4, #4]
      dims_[i] = val;
    }
  }

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d7dc2:	6823      	ldr	r3, [r4, #0]
   d7dc4:	2b04      	cmp	r3, #4
   d7dc6:	bfcc      	ite	gt
   d7dc8:	6860      	ldrgt	r0, [r4, #4]
   d7dca:	1d20      	addle	r0, r4, #4
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
    Resize(dimensions_count);
    int32* dst_dims = DimsData();
    std::memcpy(dst_dims, dims_data, dimensions_count * sizeof(int32));
   d7dcc:	4632      	mov	r2, r6
   d7dce:	4639      	mov	r1, r7
  }
   d7dd0:	e8bd 40f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, lr}
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
    Resize(dimensions_count);
    int32* dst_dims = DimsData();
    std::memcpy(dst_dims, dims_data, dimensions_count * sizeof(int32));
   d7dd4:	f00f bced 	b.w	e77b2 <memcpy>

000d7dd8 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   d7dd8:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d7ddc:	680a      	ldr	r2, [r1, #0]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d7dde:	6849      	ldr	r1, [r1, #4]
   d7de0:	6883      	ldr	r3, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d7de2:	6855      	ldr	r5, [r2, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d7de4:	684c      	ldr	r4, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d7de6:	2238      	movs	r2, #56	; 0x38
   d7de8:	fb02 3505 	mla	r5, r2, r5, r3
   d7dec:	b08a      	sub	sp, #40	; 0x28
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d7dee:	fb02 3404 	mla	r4, r2, r4, r3
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
}

inline RuntimeShape GetTensorShape(const TfLiteTensor* tensor) {
  if (tensor == nullptr) {
   d7df2:	b90d      	cbnz	r5, d7df8 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x20>
  // larger shapes are separately allocated.
  static constexpr int kMaxSmallSize = 4;

  RuntimeShape& operator=(RuntimeShape const&) = delete;

  RuntimeShape() : size_(0) {}
   d7df4:	9500      	str	r5, [sp, #0]
   d7df6:	e009      	b.n	d7e0c <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x34>
    return RuntimeShape();
  }

  TfLiteIntArray* dims = tensor->dims;
  const int dims_size = dims->size;
   d7df8:	68aa      	ldr	r2, [r5, #8]
    for (int i = 0; i < shape_size; ++i) {
      SetDim(i, value);
    }
  }

  RuntimeShape(int dimensions_count, const int32* dims_data) : size_(0) {
   d7dfa:	a80a      	add	r0, sp, #40	; 0x28
   d7dfc:	2300      	movs	r3, #0
   d7dfe:	f852 1b04 	ldr.w	r1, [r2], #4
   d7e02:	f840 3d28 	str.w	r3, [r0, #-40]!
    ReplaceWith(dimensions_count, dims_data);
   d7e06:	f7ff ffc3 	bl	d7d90 <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d7e0a:	686d      	ldr	r5, [r5, #4]
}

inline RuntimeShape GetTensorShape(const TfLiteTensor* tensor) {
  if (tensor == nullptr) {
   d7e0c:	b90c      	cbnz	r4, d7e12 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x3a>
  // larger shapes are separately allocated.
  static constexpr int kMaxSmallSize = 4;

  RuntimeShape& operator=(RuntimeShape const&) = delete;

  RuntimeShape() : size_(0) {}
   d7e0e:	9405      	str	r4, [sp, #20]
   d7e10:	e009      	b.n	d7e26 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x4e>
    return RuntimeShape();
  }

  TfLiteIntArray* dims = tensor->dims;
  const int dims_size = dims->size;
   d7e12:	68a2      	ldr	r2, [r4, #8]
    for (int i = 0; i < shape_size; ++i) {
      SetDim(i, value);
    }
  }

  RuntimeShape(int dimensions_count, const int32* dims_data) : size_(0) {
   d7e14:	a80a      	add	r0, sp, #40	; 0x28
   d7e16:	2300      	movs	r3, #0
   d7e18:	f852 1b04 	ldr.w	r1, [r2], #4
   d7e1c:	f840 3d14 	str.w	r3, [r0, #-20]!
    ReplaceWith(dimensions_count, dims_data);
   d7e20:	f7ff ffb6 	bl	d7d90 <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d7e24:	6864      	ldr	r4, [r4, #4]
   d7e26:	9f00      	ldr	r7, [sp, #0]

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   d7e28:	9b05      	ldr	r3, [sp, #20]
   d7e2a:	429f      	cmp	r7, r3
   d7e2c:	d101      	bne.n	d7e32 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x5a>
   d7e2e:	2600      	movs	r6, #0
   d7e30:	e00d      	b.n	d7e4e <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x76>
   d7e32:	f00c fa8b 	bl	e434c <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   d7e36:	4631      	mov	r1, r6
   d7e38:	4668      	mov	r0, sp
   d7e3a:	f7fe fb15 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d7e3e:	4631      	mov	r1, r6
   d7e40:	4680      	mov	r8, r0
   d7e42:	a805      	add	r0, sp, #20
   d7e44:	f7fe fb10 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d7e48:	4580      	cmp	r8, r0
   d7e4a:	d1f2      	bne.n	d7e32 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x5a>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   d7e4c:	3601      	adds	r6, #1
   d7e4e:	42b7      	cmp	r7, r6
   d7e50:	dcf1      	bgt.n	d7e36 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x5e>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   d7e52:	2f04      	cmp	r7, #4
   d7e54:	bfcc      	ite	gt
   d7e56:	9a01      	ldrgt	r2, [sp, #4]
   d7e58:	aa01      	addle	r2, sp, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   d7e5a:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
   d7e5c:	f04f 0801 	mov.w	r8, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   d7e60:	429f      	cmp	r7, r3
   d7e62:	dd05      	ble.n	d7e70 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x98>
      buffer_size *= dims_data[i];
   d7e64:	f852 1023 	ldr.w	r1, [r2, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   d7e68:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
   d7e6a:	fb01 f808 	mul.w	r8, r1, r8
   d7e6e:	e7f7      	b.n	d7e60 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x88>
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   d7e70:	2600      	movs	r6, #0

inline void Ceil(const RuntimeShape& input_shape, const float* input_data,
                 const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; ++i) {
   d7e72:	4546      	cmp	r6, r8
   d7e74:	da07      	bge.n	d7e86 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0xae>
  using ::ceil;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  ceil(float __x)
  { return __builtin_ceilf(__x); }
   d7e76:	ecb5 0a01 	vldmia	r5!, {s0}
   d7e7a:	f00d fa2b 	bl	e52d4 <ceilf>
   d7e7e:	3601      	adds	r6, #1
    output_data[i] = std::ceil(input_data[i]);
   d7e80:	eca4 0a01 	vstmia	r4!, {s0}
   d7e84:	e7f5      	b.n	d7e72 <_ZN6tflite3ops5micro4ceil4EvalEP13TfLiteContextP10TfLiteNode+0x9a>
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  reference_ops::Ceil(GetTensorShape(input), GetTensorData<float>(input),
                      GetTensorShape(output), GetTensorData<float>(output));
   d7e86:	a805      	add	r0, sp, #20
   d7e88:	f7fe fae3 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  reference_ops::Ceil(GetTensorShape(input), GetTensorData<float>(input),
   d7e8c:	4668      	mov	r0, sp
   d7e8e:	f7fe fae0 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                      GetTensorShape(output), GetTensorData<float>(output));

  return kTfLiteOk;
}
   d7e92:	2000      	movs	r0, #0
   d7e94:	b00a      	add	sp, #40	; 0x28
   d7e96:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
	...

000d7e9c <_ZN6tflite3ops5micro13Register_CEILEv>:

TfLiteRegistration* Register_CEIL() {
  static TfLiteRegistration r = {/*init=*/nullptr,
                                 /*free=*/nullptr, ceil::Prepare, ceil::Eval};
  return &r;
}
   d7e9c:	4800      	ldr	r0, [pc, #0]	; (d7ea0 <_ZN6tflite3ops5micro13Register_CEILEv+0x4>)
   d7e9e:	4770      	bx	lr
   d7ea0:	2003bedc 	.word	0x2003bedc

000d7ea4 <_ZN6tflite3ops5micro14Register_EQUALEv>:

TfLiteRegistration* Register_EQUAL() {
  static TfLiteRegistration r = {nullptr, nullptr, nullptr,
                                 comparisons::EqualEval};
  return &r;
}
   d7ea4:	4800      	ldr	r0, [pc, #0]	; (d7ea8 <_ZN6tflite3ops5micro14Register_EQUALEv+0x4>)
   d7ea6:	4770      	bx	lr
   d7ea8:	2003bf1c 	.word	0x2003bf1c

000d7eac <_ZN6tflite3ops5micro18Register_NOT_EQUALEv>:

TfLiteRegistration* Register_NOT_EQUAL() {
  static TfLiteRegistration r = {nullptr, nullptr, nullptr,
                                 comparisons::NotEqualEval};
  return &r;
}
   d7eac:	4800      	ldr	r0, [pc, #0]	; (d7eb0 <_ZN6tflite3ops5micro18Register_NOT_EQUALEv+0x4>)
   d7eae:	4770      	bx	lr
   d7eb0:	2003bf5c 	.word	0x2003bf5c

000d7eb4 <_ZN6tflite3ops5micro16Register_GREATEREv>:

TfLiteRegistration* Register_GREATER() {
  static TfLiteRegistration r = {nullptr, nullptr, nullptr,
                                 comparisons::GreaterEval};
  return &r;
}
   d7eb4:	4800      	ldr	r0, [pc, #0]	; (d7eb8 <_ZN6tflite3ops5micro16Register_GREATEREv+0x4>)
   d7eb6:	4770      	bx	lr
   d7eb8:	2003befc 	.word	0x2003befc

000d7ebc <_ZN6tflite3ops5micro22Register_GREATER_EQUALEv>:

TfLiteRegistration* Register_GREATER_EQUAL() {
  static TfLiteRegistration r = {nullptr, nullptr, nullptr,
                                 comparisons::GreaterEqualEval};
  return &r;
}
   d7ebc:	4800      	ldr	r0, [pc, #0]	; (d7ec0 <_ZN6tflite3ops5micro22Register_GREATER_EQUALEv+0x4>)
   d7ebe:	4770      	bx	lr
   d7ec0:	2003bf7c 	.word	0x2003bf7c

000d7ec4 <_ZN6tflite3ops5micro13Register_LESSEv>:

TfLiteRegistration* Register_LESS() {
  static TfLiteRegistration r = {nullptr, nullptr, nullptr,
                                 comparisons::LessEval};
  return &r;
}
   d7ec4:	4800      	ldr	r0, [pc, #0]	; (d7ec8 <_ZN6tflite3ops5micro13Register_LESSEv+0x4>)
   d7ec6:	4770      	bx	lr
   d7ec8:	2003bf3c 	.word	0x2003bf3c

000d7ecc <_ZN6tflite3ops5micro19Register_LESS_EQUALEv>:

TfLiteRegistration* Register_LESS_EQUAL() {
  static TfLiteRegistration r = {nullptr, nullptr, nullptr,
                                 comparisons::LessEqualEval};
  return &r;
}
   d7ecc:	4800      	ldr	r0, [pc, #0]	; (d7ed0 <_ZN6tflite3ops5micro19Register_LESS_EQUALEv+0x4>)
   d7ece:	4770      	bx	lr
   d7ed0:	2003bf9c 	.word	0x2003bf9c

000d7ed4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d7ed4:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d7ed8:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d7eda:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d7edc:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d7ede:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d7ee0:	4691      	mov	r9, r2
   d7ee2:	460c      	mov	r4, r1
   d7ee4:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d7ee6:	dd01      	ble.n	d7eec <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d7ee8:	f00c fa30 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d7eec:	682b      	ldr	r3, [r5, #0]
   d7eee:	2b04      	cmp	r3, #4
   d7ef0:	dcfa      	bgt.n	d7ee8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d7ef2:	6813      	ldr	r3, [r2, #0]
   d7ef4:	2b04      	cmp	r3, #4
   d7ef6:	dcf7      	bgt.n	d7ee8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
   d7ef8:	2301      	movs	r3, #1
   d7efa:	2104      	movs	r1, #4
   d7efc:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d7efe:	f10d 0820 	add.w	r8, sp, #32
   d7f02:	f7fe faea 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d7f06:	4620      	mov	r0, r4
   d7f08:	ab10      	add	r3, sp, #64	; 0x40
   d7f0a:	4642      	mov	r2, r8
   d7f0c:	4629      	mov	r1, r5
   d7f0e:	f7fe fdf9 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d7f12:	2400      	movs	r4, #0
   d7f14:	2100      	movs	r1, #0
   d7f16:	a803      	add	r0, sp, #12
   d7f18:	f7fe faa6 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d7f1c:	4284      	cmp	r4, r0
   d7f1e:	da3d      	bge.n	d7f9c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
   d7f20:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d7f22:	2101      	movs	r1, #1
   d7f24:	a803      	add	r0, sp, #12
   d7f26:	f7fe fa9f 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d7f2a:	4285      	cmp	r5, r0
   d7f2c:	da34      	bge.n	d7f98 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
   d7f2e:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d7f30:	2102      	movs	r1, #2
   d7f32:	a803      	add	r0, sp, #12
   d7f34:	f7fe fa98 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d7f38:	4286      	cmp	r6, r0
   d7f3a:	da2b      	bge.n	d7f94 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
   d7f3c:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d7f3e:	2103      	movs	r1, #3
   d7f40:	a803      	add	r0, sp, #12
   d7f42:	f7fe fa91 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d7f46:	4287      	cmp	r7, r0
   d7f48:	da22      	bge.n	d7f90 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xbc>
          output_data[Offset(output_shape, b, y, x, c)] =
   d7f4a:	9700      	str	r7, [sp, #0]
   d7f4c:	4633      	mov	r3, r6
   d7f4e:	462a      	mov	r2, r5
   d7f50:	4621      	mov	r1, r4
   d7f52:	a803      	add	r0, sp, #12
   d7f54:	f7fe faed 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d7f58:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d7f5a:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d7f5c:	4633      	mov	r3, r6
   d7f5e:	462a      	mov	r2, r5
   d7f60:	4621      	mov	r1, r4
   d7f62:	4640      	mov	r0, r8
   d7f64:	f7fe fb96 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d7f68:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d7f6a:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d7f6c:	4633      	mov	r3, r6
   d7f6e:	462a      	mov	r2, r5
   d7f70:	4621      	mov	r1, r4
   d7f72:	a810      	add	r0, sp, #64	; 0x40
   d7f74:	f7fe fb8e 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d7f78:	9a22      	ldr	r2, [sp, #136]	; 0x88
   d7f7a:	f819 300b 	ldrb.w	r3, [r9, fp]
   d7f7e:	5c12      	ldrb	r2, [r2, r0]
   d7f80:	1a9a      	subs	r2, r3, r2
   d7f82:	4253      	negs	r3, r2
   d7f84:	4153      	adcs	r3, r2
   d7f86:	9a24      	ldr	r2, [sp, #144]	; 0x90
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d7f88:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
   d7f8a:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d7f8e:	e7d6      	b.n	d7f3e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d7f90:	3601      	adds	r6, #1
   d7f92:	e7cd      	b.n	d7f30 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d7f94:	3501      	adds	r5, #1
   d7f96:	e7c4      	b.n	d7f22 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d7f98:	3401      	adds	r4, #1
   d7f9a:	e7bb      	b.n	d7f14 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d7f9c:	a803      	add	r0, sp, #12
   d7f9e:	f7fe fa58 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d7fa2:	b019      	add	sp, #100	; 0x64
   d7fa4:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d7fa8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d7fa8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d7fac:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d7fae:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d7fb0:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d7fb2:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d7fb4:	4692      	mov	sl, r2
   d7fb6:	460c      	mov	r4, r1
   d7fb8:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d7fba:	dd01      	ble.n	d7fc0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d7fbc:	f00c f9c6 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d7fc0:	682b      	ldr	r3, [r5, #0]
   d7fc2:	2b04      	cmp	r3, #4
   d7fc4:	dcfa      	bgt.n	d7fbc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d7fc6:	6813      	ldr	r3, [r2, #0]
   d7fc8:	2b04      	cmp	r3, #4
   d7fca:	dcf7      	bgt.n	d7fbc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d7fcc:	2301      	movs	r3, #1
   d7fce:	2104      	movs	r1, #4
   d7fd0:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d7fd2:	f10d 0820 	add.w	r8, sp, #32
   d7fd6:	f7fe fa80 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d7fda:	4620      	mov	r0, r4
   d7fdc:	ab10      	add	r3, sp, #64	; 0x40
   d7fde:	4642      	mov	r2, r8
   d7fe0:	4629      	mov	r1, r5
   d7fe2:	f7fe fd8f 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d7fe6:	2400      	movs	r4, #0
   d7fe8:	2100      	movs	r1, #0
   d7fea:	a803      	add	r0, sp, #12
   d7fec:	f7fe fa3c 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d7ff0:	4284      	cmp	r4, r0
   d7ff2:	da46      	bge.n	d8082 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xda>
   d7ff4:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d7ff6:	2101      	movs	r1, #1
   d7ff8:	a803      	add	r0, sp, #12
   d7ffa:	f7fe fa35 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d7ffe:	4285      	cmp	r5, r0
   d8000:	da3d      	bge.n	d807e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd6>
   d8002:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8004:	2102      	movs	r1, #2
   d8006:	a803      	add	r0, sp, #12
   d8008:	f7fe fa2e 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d800c:	4286      	cmp	r6, r0
   d800e:	da34      	bge.n	d807a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd2>
   d8010:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8012:	2103      	movs	r1, #3
   d8014:	a803      	add	r0, sp, #12
   d8016:	f7fe fa27 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d801a:	4287      	cmp	r7, r0
   d801c:	da2b      	bge.n	d8076 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xce>
          output_data[Offset(output_shape, b, y, x, c)] =
   d801e:	9700      	str	r7, [sp, #0]
   d8020:	4633      	mov	r3, r6
   d8022:	462a      	mov	r2, r5
   d8024:	4621      	mov	r1, r4
   d8026:	a803      	add	r0, sp, #12
   d8028:	f7fe fa83 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d802c:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d802e:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8030:	4633      	mov	r3, r6
   d8032:	462a      	mov	r2, r5
   d8034:	4621      	mov	r1, r4
   d8036:	4640      	mov	r0, r8
   d8038:	f7fe fb2c 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d803c:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d803e:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8040:	4633      	mov	r3, r6
   d8042:	462a      	mov	r2, r5
   d8044:	4621      	mov	r1, r4
   d8046:	a810      	add	r0, sp, #64	; 0x40
   d8048:	f7fe fb24 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d804c:	9b22      	ldr	r3, [sp, #136]	; 0x88

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d804e:	9a24      	ldr	r2, [sp, #144]	; 0x90
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8050:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   d8054:	eb0a 0989 	add.w	r9, sl, r9, lsl #2

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8058:	ed99 7a00 	vldr	s14, [r9]
   d805c:	edd0 7a00 	vldr	s15, [r0]
   d8060:	eeb4 7a67 	vcmp.f32	s14, s15
   d8064:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d8068:	bf0c      	ite	eq
   d806a:	2301      	moveq	r3, #1
   d806c:	2300      	movne	r3, #0
   d806e:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8072:	3701      	adds	r7, #1
   d8074:	e7cd      	b.n	d8012 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8076:	3601      	adds	r6, #1
   d8078:	e7c4      	b.n	d8004 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d807a:	3501      	adds	r5, #1
   d807c:	e7bb      	b.n	d7ff6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d807e:	3401      	adds	r4, #1
   d8080:	e7b2      	b.n	d7fe8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8082:	a803      	add	r0, sp, #12
   d8084:	f7fe f9e5 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8088:	b019      	add	sp, #100	; 0x64
   d808a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d808e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d808e:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8092:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8094:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8096:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8098:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d809a:	4691      	mov	r9, r2
   d809c:	460c      	mov	r4, r1
   d809e:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d80a0:	dd01      	ble.n	d80a6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d80a2:	f00c f953 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d80a6:	682b      	ldr	r3, [r5, #0]
   d80a8:	2b04      	cmp	r3, #4
   d80aa:	dcfa      	bgt.n	d80a2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d80ac:	6813      	ldr	r3, [r2, #0]
   d80ae:	2b04      	cmp	r3, #4
   d80b0:	dcf7      	bgt.n	d80a2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d80b2:	2301      	movs	r3, #1
   d80b4:	2104      	movs	r1, #4
   d80b6:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d80b8:	f10d 0820 	add.w	r8, sp, #32
   d80bc:	f7fe fa0d 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d80c0:	4620      	mov	r0, r4
   d80c2:	ab10      	add	r3, sp, #64	; 0x40
   d80c4:	4642      	mov	r2, r8
   d80c6:	4629      	mov	r1, r5
   d80c8:	f7fe fd1c 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d80cc:	2400      	movs	r4, #0
   d80ce:	2100      	movs	r1, #0
   d80d0:	a803      	add	r0, sp, #12
   d80d2:	f7fe f9c9 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d80d6:	4284      	cmp	r4, r0
   d80d8:	da3e      	bge.n	d8158 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xca>
   d80da:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d80dc:	2101      	movs	r1, #1
   d80de:	a803      	add	r0, sp, #12
   d80e0:	f7fe f9c2 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d80e4:	4285      	cmp	r5, r0
   d80e6:	da35      	bge.n	d8154 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc6>
   d80e8:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d80ea:	2102      	movs	r1, #2
   d80ec:	a803      	add	r0, sp, #12
   d80ee:	f7fe f9bb 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d80f2:	4286      	cmp	r6, r0
   d80f4:	da2c      	bge.n	d8150 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc2>
   d80f6:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d80f8:	2103      	movs	r1, #3
   d80fa:	a803      	add	r0, sp, #12
   d80fc:	f7fe f9b4 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8100:	4287      	cmp	r7, r0
   d8102:	da23      	bge.n	d814c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xbe>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8104:	9700      	str	r7, [sp, #0]
   d8106:	4633      	mov	r3, r6
   d8108:	462a      	mov	r2, r5
   d810a:	4621      	mov	r1, r4
   d810c:	a803      	add	r0, sp, #12
   d810e:	f7fe fa10 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8112:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8114:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8116:	4633      	mov	r3, r6
   d8118:	462a      	mov	r2, r5
   d811a:	4621      	mov	r1, r4
   d811c:	4640      	mov	r0, r8
   d811e:	f7fe fab9 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8122:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8124:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8126:	4633      	mov	r3, r6
   d8128:	462a      	mov	r2, r5
   d812a:	4621      	mov	r1, r4
   d812c:	a810      	add	r0, sp, #64	; 0x40
   d812e:	f7fe fab1 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8132:	9a22      	ldr	r2, [sp, #136]	; 0x88
   d8134:	f859 302b 	ldr.w	r3, [r9, fp, lsl #2]
   d8138:	f852 2020 	ldr.w	r2, [r2, r0, lsl #2]
   d813c:	1a9a      	subs	r2, r3, r2
   d813e:	4253      	negs	r3, r2
   d8140:	4153      	adcs	r3, r2
   d8142:	9a24      	ldr	r2, [sp, #144]	; 0x90
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8144:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
   d8146:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d814a:	e7d5      	b.n	d80f8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d814c:	3601      	adds	r6, #1
   d814e:	e7cc      	b.n	d80ea <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8150:	3501      	adds	r5, #1
   d8152:	e7c3      	b.n	d80dc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8154:	3401      	adds	r4, #1
   d8156:	e7ba      	b.n	d80ce <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8158:	a803      	add	r0, sp, #12
   d815a:	f7fe f97a 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d815e:	b019      	add	sp, #100	; 0x64
   d8160:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8164 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8164:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8168:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d816a:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d816c:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d816e:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8170:	4692      	mov	sl, r2
   d8172:	460c      	mov	r4, r1
   d8174:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8176:	dd01      	ble.n	d817c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8178:	f00c f8e8 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d817c:	682b      	ldr	r3, [r5, #0]
   d817e:	2b04      	cmp	r3, #4
   d8180:	dcfa      	bgt.n	d8178 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8182:	6813      	ldr	r3, [r2, #0]
   d8184:	2b04      	cmp	r3, #4
   d8186:	dcf7      	bgt.n	d8178 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8188:	2301      	movs	r3, #1
   d818a:	2104      	movs	r1, #4
   d818c:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d818e:	f10d 0820 	add.w	r8, sp, #32
   d8192:	f7fe f9a2 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8196:	4620      	mov	r0, r4
   d8198:	ab10      	add	r3, sp, #64	; 0x40
   d819a:	4642      	mov	r2, r8
   d819c:	4629      	mov	r1, r5
   d819e:	f7fe fcb1 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d81a2:	2400      	movs	r4, #0
   d81a4:	2100      	movs	r1, #0
   d81a6:	a803      	add	r0, sp, #12
   d81a8:	f7fe f95e 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d81ac:	4284      	cmp	r4, r0
   d81ae:	da45      	bge.n	d823c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd8>
   d81b0:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d81b2:	2101      	movs	r1, #1
   d81b4:	a803      	add	r0, sp, #12
   d81b6:	f7fe f957 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d81ba:	4285      	cmp	r5, r0
   d81bc:	da3c      	bge.n	d8238 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd4>
   d81be:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d81c0:	2102      	movs	r1, #2
   d81c2:	a803      	add	r0, sp, #12
   d81c4:	f7fe f950 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d81c8:	4286      	cmp	r6, r0
   d81ca:	da33      	bge.n	d8234 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd0>
   d81cc:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d81ce:	2103      	movs	r1, #3
   d81d0:	a803      	add	r0, sp, #12
   d81d2:	f7fe f949 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d81d6:	4287      	cmp	r7, r0
   d81d8:	da2a      	bge.n	d8230 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
          output_data[Offset(output_shape, b, y, x, c)] =
   d81da:	9700      	str	r7, [sp, #0]
   d81dc:	4633      	mov	r3, r6
   d81de:	462a      	mov	r2, r5
   d81e0:	4621      	mov	r1, r4
   d81e2:	a803      	add	r0, sp, #12
   d81e4:	f7fe f9a5 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d81e8:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d81ea:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d81ec:	4633      	mov	r3, r6
   d81ee:	462a      	mov	r2, r5
   d81f0:	4621      	mov	r1, r4
   d81f2:	4640      	mov	r0, r8
   d81f4:	f7fe fa4e 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d81f8:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d81fa:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d81fc:	4633      	mov	r3, r6
   d81fe:	462a      	mov	r2, r5
   d8200:	4621      	mov	r1, r4
   d8202:	a810      	add	r0, sp, #64	; 0x40
   d8204:	f7fe fa46 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8208:	9b22      	ldr	r3, [sp, #136]	; 0x88
   d820a:	eb0a 09c9 	add.w	r9, sl, r9, lsl #3
   d820e:	eb03 03c0 	add.w	r3, r3, r0, lsl #3

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8212:	e9d3 2300 	ldrd	r2, r3, [r3]
   d8216:	e9d9 0100 	ldrd	r0, r1, [r9]
   d821a:	4299      	cmp	r1, r3
   d821c:	bf08      	it	eq
   d821e:	4290      	cmpeq	r0, r2
   d8220:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d8222:	bf0c      	ite	eq
   d8224:	2301      	moveq	r3, #1
   d8226:	2300      	movne	r3, #0
   d8228:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d822c:	3701      	adds	r7, #1
   d822e:	e7ce      	b.n	d81ce <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8230:	3601      	adds	r6, #1
   d8232:	e7c5      	b.n	d81c0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8234:	3501      	adds	r5, #1
   d8236:	e7bc      	b.n	d81b2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8238:	3401      	adds	r4, #1
   d823a:	e7b3      	b.n	d81a4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d823c:	a803      	add	r0, sp, #12
   d823e:	f7fe f908 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8242:	b019      	add	sp, #100	; 0x64
   d8244:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8248 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8248:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d824c:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d824e:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8250:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8252:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8254:	4691      	mov	r9, r2
   d8256:	460c      	mov	r4, r1
   d8258:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d825a:	dd01      	ble.n	d8260 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d825c:	f00c f876 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8260:	682b      	ldr	r3, [r5, #0]
   d8262:	2b04      	cmp	r3, #4
   d8264:	dcfa      	bgt.n	d825c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8266:	6813      	ldr	r3, [r2, #0]
   d8268:	2b04      	cmp	r3, #4
   d826a:	dcf7      	bgt.n	d825c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d826c:	2301      	movs	r3, #1
   d826e:	2104      	movs	r1, #4
   d8270:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8272:	f10d 0820 	add.w	r8, sp, #32
   d8276:	f7fe f930 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d827a:	4620      	mov	r0, r4
   d827c:	ab10      	add	r3, sp, #64	; 0x40
   d827e:	4642      	mov	r2, r8
   d8280:	4629      	mov	r1, r5
   d8282:	f7fe fc3f 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8286:	2400      	movs	r4, #0
   d8288:	2100      	movs	r1, #0
   d828a:	a803      	add	r0, sp, #12
   d828c:	f7fe f8ec 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8290:	4284      	cmp	r4, r0
   d8292:	da3b      	bge.n	d830c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
   d8294:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8296:	2101      	movs	r1, #1
   d8298:	a803      	add	r0, sp, #12
   d829a:	f7fe f8e5 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d829e:	4285      	cmp	r5, r0
   d82a0:	da32      	bge.n	d8308 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
   d82a2:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d82a4:	2102      	movs	r1, #2
   d82a6:	a803      	add	r0, sp, #12
   d82a8:	f7fe f8de 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d82ac:	4286      	cmp	r6, r0
   d82ae:	da29      	bge.n	d8304 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xbc>
   d82b0:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d82b2:	2103      	movs	r1, #3
   d82b4:	a803      	add	r0, sp, #12
   d82b6:	f7fe f8d7 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d82ba:	4287      	cmp	r7, r0
   d82bc:	da20      	bge.n	d8300 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xb8>
          output_data[Offset(output_shape, b, y, x, c)] =
   d82be:	9700      	str	r7, [sp, #0]
   d82c0:	4633      	mov	r3, r6
   d82c2:	462a      	mov	r2, r5
   d82c4:	4621      	mov	r1, r4
   d82c6:	a803      	add	r0, sp, #12
   d82c8:	f7fe f933 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d82cc:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d82ce:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d82d0:	4633      	mov	r3, r6
   d82d2:	462a      	mov	r2, r5
   d82d4:	4621      	mov	r1, r4
   d82d6:	4640      	mov	r0, r8
   d82d8:	f7fe f9dc 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d82dc:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d82de:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d82e0:	4633      	mov	r3, r6
   d82e2:	462a      	mov	r2, r5
   d82e4:	4621      	mov	r1, r4
   d82e6:	a810      	add	r0, sp, #64	; 0x40
   d82e8:	f7fe f9d4 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d82ec:	9b22      	ldr	r3, [sp, #136]	; 0x88
   d82ee:	f819 200b 	ldrb.w	r2, [r9, fp]
   d82f2:	5c1b      	ldrb	r3, [r3, r0]
   d82f4:	4053      	eors	r3, r2
   d82f6:	9a24      	ldr	r2, [sp, #144]	; 0x90
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d82f8:	3701      	adds	r7, #1
          output_data[Offset(output_shape, b, y, x, c)] =
   d82fa:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d82fe:	e7d8      	b.n	d82b2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8300:	3601      	adds	r6, #1
   d8302:	e7cf      	b.n	d82a4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8304:	3501      	adds	r5, #1
   d8306:	e7c6      	b.n	d8296 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8308:	3401      	adds	r4, #1
   d830a:	e7bd      	b.n	d8288 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d830c:	a803      	add	r0, sp, #12
   d830e:	f7fe f8a0 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8312:	b019      	add	sp, #100	; 0x64
   d8314:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8318 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8318:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d831c:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d831e:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8320:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8322:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8324:	4692      	mov	sl, r2
   d8326:	460c      	mov	r4, r1
   d8328:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d832a:	dd01      	ble.n	d8330 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d832c:	f00c f80e 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8330:	682b      	ldr	r3, [r5, #0]
   d8332:	2b04      	cmp	r3, #4
   d8334:	dcfa      	bgt.n	d832c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8336:	6813      	ldr	r3, [r2, #0]
   d8338:	2b04      	cmp	r3, #4
   d833a:	dcf7      	bgt.n	d832c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d833c:	2301      	movs	r3, #1
   d833e:	2104      	movs	r1, #4
   d8340:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8342:	f10d 0820 	add.w	r8, sp, #32
   d8346:	f7fe f8c8 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d834a:	4620      	mov	r0, r4
   d834c:	ab10      	add	r3, sp, #64	; 0x40
   d834e:	4642      	mov	r2, r8
   d8350:	4629      	mov	r1, r5
   d8352:	f7fe fbd7 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8356:	2400      	movs	r4, #0
   d8358:	2100      	movs	r1, #0
   d835a:	a803      	add	r0, sp, #12
   d835c:	f7fe f884 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8360:	4284      	cmp	r4, r0
   d8362:	da46      	bge.n	d83f2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xda>
   d8364:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8366:	2101      	movs	r1, #1
   d8368:	a803      	add	r0, sp, #12
   d836a:	f7fe f87d 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d836e:	4285      	cmp	r5, r0
   d8370:	da3d      	bge.n	d83ee <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd6>
   d8372:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8374:	2102      	movs	r1, #2
   d8376:	a803      	add	r0, sp, #12
   d8378:	f7fe f876 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d837c:	4286      	cmp	r6, r0
   d837e:	da34      	bge.n	d83ea <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd2>
   d8380:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8382:	2103      	movs	r1, #3
   d8384:	a803      	add	r0, sp, #12
   d8386:	f7fe f86f 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d838a:	4287      	cmp	r7, r0
   d838c:	da2b      	bge.n	d83e6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xce>
          output_data[Offset(output_shape, b, y, x, c)] =
   d838e:	9700      	str	r7, [sp, #0]
   d8390:	4633      	mov	r3, r6
   d8392:	462a      	mov	r2, r5
   d8394:	4621      	mov	r1, r4
   d8396:	a803      	add	r0, sp, #12
   d8398:	f7fe f8cb 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d839c:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d839e:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d83a0:	4633      	mov	r3, r6
   d83a2:	462a      	mov	r2, r5
   d83a4:	4621      	mov	r1, r4
   d83a6:	4640      	mov	r0, r8
   d83a8:	f7fe f974 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d83ac:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d83ae:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d83b0:	4633      	mov	r3, r6
   d83b2:	462a      	mov	r2, r5
   d83b4:	4621      	mov	r1, r4
   d83b6:	a810      	add	r0, sp, #64	; 0x40
   d83b8:	f7fe f96c 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d83bc:	9b22      	ldr	r3, [sp, #136]	; 0x88

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d83be:	9a24      	ldr	r2, [sp, #144]	; 0x90
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d83c0:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   d83c4:	eb0a 0989 	add.w	r9, sl, r9, lsl #2

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d83c8:	ed99 7a00 	vldr	s14, [r9]
   d83cc:	edd0 7a00 	vldr	s15, [r0]
   d83d0:	eeb4 7a67 	vcmp.f32	s14, s15
   d83d4:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d83d8:	bf14      	ite	ne
   d83da:	2301      	movne	r3, #1
   d83dc:	2300      	moveq	r3, #0
   d83de:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d83e2:	3701      	adds	r7, #1
   d83e4:	e7cd      	b.n	d8382 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d83e6:	3601      	adds	r6, #1
   d83e8:	e7c4      	b.n	d8374 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d83ea:	3501      	adds	r5, #1
   d83ec:	e7bb      	b.n	d8366 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d83ee:	3401      	adds	r4, #1
   d83f0:	e7b2      	b.n	d8358 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d83f2:	a803      	add	r0, sp, #12
   d83f4:	f7fe f82d 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d83f8:	b019      	add	sp, #100	; 0x64
   d83fa:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d83fe <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d83fe:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8402:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8404:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8406:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8408:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d840a:	4691      	mov	r9, r2
   d840c:	460c      	mov	r4, r1
   d840e:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8410:	dd01      	ble.n	d8416 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8412:	f00b ff9b 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8416:	682b      	ldr	r3, [r5, #0]
   d8418:	2b04      	cmp	r3, #4
   d841a:	dcfa      	bgt.n	d8412 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d841c:	6813      	ldr	r3, [r2, #0]
   d841e:	2b04      	cmp	r3, #4
   d8420:	dcf7      	bgt.n	d8412 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8422:	2301      	movs	r3, #1
   d8424:	2104      	movs	r1, #4
   d8426:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8428:	f10d 0820 	add.w	r8, sp, #32
   d842c:	f7fe f855 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8430:	4620      	mov	r0, r4
   d8432:	ab10      	add	r3, sp, #64	; 0x40
   d8434:	4642      	mov	r2, r8
   d8436:	4629      	mov	r1, r5
   d8438:	f7fe fb64 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d843c:	2400      	movs	r4, #0
   d843e:	2100      	movs	r1, #0
   d8440:	a803      	add	r0, sp, #12
   d8442:	f7fe f811 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8446:	4284      	cmp	r4, r0
   d8448:	da3e      	bge.n	d84c8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xca>
   d844a:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d844c:	2101      	movs	r1, #1
   d844e:	a803      	add	r0, sp, #12
   d8450:	f7fe f80a 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8454:	4285      	cmp	r5, r0
   d8456:	da35      	bge.n	d84c4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc6>
   d8458:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d845a:	2102      	movs	r1, #2
   d845c:	a803      	add	r0, sp, #12
   d845e:	f7fe f803 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8462:	4286      	cmp	r6, r0
   d8464:	da2c      	bge.n	d84c0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc2>
   d8466:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8468:	2103      	movs	r1, #3
   d846a:	a803      	add	r0, sp, #12
   d846c:	f7fd fffc 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8470:	4287      	cmp	r7, r0
   d8472:	da23      	bge.n	d84bc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xbe>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8474:	9700      	str	r7, [sp, #0]
   d8476:	4633      	mov	r3, r6
   d8478:	462a      	mov	r2, r5
   d847a:	4621      	mov	r1, r4
   d847c:	a803      	add	r0, sp, #12
   d847e:	f7fe f858 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8482:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8484:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8486:	4633      	mov	r3, r6
   d8488:	462a      	mov	r2, r5
   d848a:	4621      	mov	r1, r4
   d848c:	4640      	mov	r0, r8
   d848e:	f7fe f901 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8492:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8494:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8496:	4633      	mov	r3, r6
   d8498:	462a      	mov	r2, r5
   d849a:	4621      	mov	r1, r4
   d849c:	a810      	add	r0, sp, #64	; 0x40
   d849e:	f7fe f8f9 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d84a2:	9a22      	ldr	r2, [sp, #136]	; 0x88
   d84a4:	f859 302b 	ldr.w	r3, [r9, fp, lsl #2]
   d84a8:	f852 2020 	ldr.w	r2, [r2, r0, lsl #2]
   d84ac:	1a9b      	subs	r3, r3, r2
   d84ae:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d84b0:	bf18      	it	ne
   d84b2:	2301      	movne	r3, #1
   d84b4:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d84b8:	3701      	adds	r7, #1
   d84ba:	e7d5      	b.n	d8468 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d84bc:	3601      	adds	r6, #1
   d84be:	e7cc      	b.n	d845a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d84c0:	3501      	adds	r5, #1
   d84c2:	e7c3      	b.n	d844c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d84c4:	3401      	adds	r4, #1
   d84c6:	e7ba      	b.n	d843e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d84c8:	a803      	add	r0, sp, #12
   d84ca:	f7fd ffc2 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d84ce:	b019      	add	sp, #100	; 0x64
   d84d0:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d84d4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d84d4:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d84d8:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d84da:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d84dc:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d84de:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d84e0:	4692      	mov	sl, r2
   d84e2:	460c      	mov	r4, r1
   d84e4:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d84e6:	dd01      	ble.n	d84ec <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d84e8:	f00b ff30 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d84ec:	682b      	ldr	r3, [r5, #0]
   d84ee:	2b04      	cmp	r3, #4
   d84f0:	dcfa      	bgt.n	d84e8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d84f2:	6813      	ldr	r3, [r2, #0]
   d84f4:	2b04      	cmp	r3, #4
   d84f6:	dcf7      	bgt.n	d84e8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d84f8:	2301      	movs	r3, #1
   d84fa:	2104      	movs	r1, #4
   d84fc:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d84fe:	f10d 0820 	add.w	r8, sp, #32
   d8502:	f7fd ffea 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8506:	4620      	mov	r0, r4
   d8508:	ab10      	add	r3, sp, #64	; 0x40
   d850a:	4642      	mov	r2, r8
   d850c:	4629      	mov	r1, r5
   d850e:	f7fe faf9 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8512:	2400      	movs	r4, #0
   d8514:	2100      	movs	r1, #0
   d8516:	a803      	add	r0, sp, #12
   d8518:	f7fd ffa6 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d851c:	4284      	cmp	r4, r0
   d851e:	da45      	bge.n	d85ac <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd8>
   d8520:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8522:	2101      	movs	r1, #1
   d8524:	a803      	add	r0, sp, #12
   d8526:	f7fd ff9f 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d852a:	4285      	cmp	r5, r0
   d852c:	da3c      	bge.n	d85a8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd4>
   d852e:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8530:	2102      	movs	r1, #2
   d8532:	a803      	add	r0, sp, #12
   d8534:	f7fd ff98 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8538:	4286      	cmp	r6, r0
   d853a:	da33      	bge.n	d85a4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd0>
   d853c:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d853e:	2103      	movs	r1, #3
   d8540:	a803      	add	r0, sp, #12
   d8542:	f7fd ff91 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8546:	4287      	cmp	r7, r0
   d8548:	da2a      	bge.n	d85a0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
          output_data[Offset(output_shape, b, y, x, c)] =
   d854a:	9700      	str	r7, [sp, #0]
   d854c:	4633      	mov	r3, r6
   d854e:	462a      	mov	r2, r5
   d8550:	4621      	mov	r1, r4
   d8552:	a803      	add	r0, sp, #12
   d8554:	f7fd ffed 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8558:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d855a:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d855c:	4633      	mov	r3, r6
   d855e:	462a      	mov	r2, r5
   d8560:	4621      	mov	r1, r4
   d8562:	4640      	mov	r0, r8
   d8564:	f7fe f896 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8568:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d856a:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d856c:	4633      	mov	r3, r6
   d856e:	462a      	mov	r2, r5
   d8570:	4621      	mov	r1, r4
   d8572:	a810      	add	r0, sp, #64	; 0x40
   d8574:	f7fe f88e 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8578:	9b22      	ldr	r3, [sp, #136]	; 0x88
   d857a:	eb0a 09c9 	add.w	r9, sl, r9, lsl #3
   d857e:	eb03 03c0 	add.w	r3, r3, r0, lsl #3

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8582:	e9d3 2300 	ldrd	r2, r3, [r3]
   d8586:	e9d9 0100 	ldrd	r0, r1, [r9]
   d858a:	4299      	cmp	r1, r3
   d858c:	bf08      	it	eq
   d858e:	4290      	cmpeq	r0, r2
   d8590:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d8592:	bf14      	ite	ne
   d8594:	2301      	movne	r3, #1
   d8596:	2300      	moveq	r3, #0
   d8598:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d859c:	3701      	adds	r7, #1
   d859e:	e7ce      	b.n	d853e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d85a0:	3601      	adds	r6, #1
   d85a2:	e7c5      	b.n	d8530 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d85a4:	3501      	adds	r5, #1
   d85a6:	e7bc      	b.n	d8522 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d85a8:	3401      	adds	r4, #1
   d85aa:	e7b3      	b.n	d8514 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d85ac:	a803      	add	r0, sp, #12
   d85ae:	f7fd ff50 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d85b2:	b019      	add	sp, #100	; 0x64
   d85b4:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d85b8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d85b8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d85bc:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d85be:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d85c0:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d85c2:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d85c4:	4692      	mov	sl, r2
   d85c6:	460c      	mov	r4, r1
   d85c8:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d85ca:	dd01      	ble.n	d85d0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d85cc:	f00b febe 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d85d0:	682b      	ldr	r3, [r5, #0]
   d85d2:	2b04      	cmp	r3, #4
   d85d4:	dcfa      	bgt.n	d85cc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d85d6:	6813      	ldr	r3, [r2, #0]
   d85d8:	2b04      	cmp	r3, #4
   d85da:	dcf7      	bgt.n	d85cc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d85dc:	2301      	movs	r3, #1
   d85de:	2104      	movs	r1, #4
   d85e0:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d85e2:	f10d 0820 	add.w	r8, sp, #32
   d85e6:	f7fd ff78 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d85ea:	4620      	mov	r0, r4
   d85ec:	ab10      	add	r3, sp, #64	; 0x40
   d85ee:	4642      	mov	r2, r8
   d85f0:	4629      	mov	r1, r5
   d85f2:	f7fe fa87 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d85f6:	2400      	movs	r4, #0
   d85f8:	2100      	movs	r1, #0
   d85fa:	a803      	add	r0, sp, #12
   d85fc:	f7fd ff34 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8600:	4284      	cmp	r4, r0
   d8602:	da46      	bge.n	d8692 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xda>
   d8604:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8606:	2101      	movs	r1, #1
   d8608:	a803      	add	r0, sp, #12
   d860a:	f7fd ff2d 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d860e:	4285      	cmp	r5, r0
   d8610:	da3d      	bge.n	d868e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd6>
   d8612:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8614:	2102      	movs	r1, #2
   d8616:	a803      	add	r0, sp, #12
   d8618:	f7fd ff26 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d861c:	4286      	cmp	r6, r0
   d861e:	da34      	bge.n	d868a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd2>
   d8620:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8622:	2103      	movs	r1, #3
   d8624:	a803      	add	r0, sp, #12
   d8626:	f7fd ff1f 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d862a:	4287      	cmp	r7, r0
   d862c:	da2b      	bge.n	d8686 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xce>
          output_data[Offset(output_shape, b, y, x, c)] =
   d862e:	9700      	str	r7, [sp, #0]
   d8630:	4633      	mov	r3, r6
   d8632:	462a      	mov	r2, r5
   d8634:	4621      	mov	r1, r4
   d8636:	a803      	add	r0, sp, #12
   d8638:	f7fd ff7b 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d863c:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d863e:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8640:	4633      	mov	r3, r6
   d8642:	462a      	mov	r2, r5
   d8644:	4621      	mov	r1, r4
   d8646:	4640      	mov	r0, r8
   d8648:	f7fe f824 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d864c:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d864e:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8650:	4633      	mov	r3, r6
   d8652:	462a      	mov	r2, r5
   d8654:	4621      	mov	r1, r4
   d8656:	a810      	add	r0, sp, #64	; 0x40
   d8658:	f7fe f81c 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d865c:	9b22      	ldr	r3, [sp, #136]	; 0x88

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d865e:	9a24      	ldr	r2, [sp, #144]	; 0x90
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8660:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   d8664:	eb0a 0989 	add.w	r9, sl, r9, lsl #2

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8668:	ed99 7a00 	vldr	s14, [r9]
   d866c:	edd0 7a00 	vldr	s15, [r0]
   d8670:	eeb4 7ae7 	vcmpe.f32	s14, s15
   d8674:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d8678:	bfcc      	ite	gt
   d867a:	2301      	movgt	r3, #1
   d867c:	2300      	movle	r3, #0
   d867e:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8682:	3701      	adds	r7, #1
   d8684:	e7cd      	b.n	d8622 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8686:	3601      	adds	r6, #1
   d8688:	e7c4      	b.n	d8614 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d868a:	3501      	adds	r5, #1
   d868c:	e7bb      	b.n	d8606 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d868e:	3401      	adds	r4, #1
   d8690:	e7b2      	b.n	d85f8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8692:	a803      	add	r0, sp, #12
   d8694:	f7fd fedd 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8698:	b019      	add	sp, #100	; 0x64
   d869a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d869e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d869e:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d86a2:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d86a4:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d86a6:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d86a8:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d86aa:	4691      	mov	r9, r2
   d86ac:	460c      	mov	r4, r1
   d86ae:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d86b0:	dd01      	ble.n	d86b6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d86b2:	f00b fe4b 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d86b6:	682b      	ldr	r3, [r5, #0]
   d86b8:	2b04      	cmp	r3, #4
   d86ba:	dcfa      	bgt.n	d86b2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d86bc:	6813      	ldr	r3, [r2, #0]
   d86be:	2b04      	cmp	r3, #4
   d86c0:	dcf7      	bgt.n	d86b2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d86c2:	2301      	movs	r3, #1
   d86c4:	2104      	movs	r1, #4
   d86c6:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d86c8:	f10d 0820 	add.w	r8, sp, #32
   d86cc:	f7fd ff05 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d86d0:	4620      	mov	r0, r4
   d86d2:	ab10      	add	r3, sp, #64	; 0x40
   d86d4:	4642      	mov	r2, r8
   d86d6:	4629      	mov	r1, r5
   d86d8:	f7fe fa14 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d86dc:	2400      	movs	r4, #0
   d86de:	2100      	movs	r1, #0
   d86e0:	a803      	add	r0, sp, #12
   d86e2:	f7fd fec1 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d86e6:	4284      	cmp	r4, r0
   d86e8:	da3f      	bge.n	d876a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
   d86ea:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d86ec:	2101      	movs	r1, #1
   d86ee:	a803      	add	r0, sp, #12
   d86f0:	f7fd feba 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d86f4:	4285      	cmp	r5, r0
   d86f6:	da36      	bge.n	d8766 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
   d86f8:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d86fa:	2102      	movs	r1, #2
   d86fc:	a803      	add	r0, sp, #12
   d86fe:	f7fd feb3 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8702:	4286      	cmp	r6, r0
   d8704:	da2d      	bge.n	d8762 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
   d8706:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8708:	2103      	movs	r1, #3
   d870a:	a803      	add	r0, sp, #12
   d870c:	f7fd feac 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8710:	4287      	cmp	r7, r0
   d8712:	da24      	bge.n	d875e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8714:	9700      	str	r7, [sp, #0]
   d8716:	4633      	mov	r3, r6
   d8718:	462a      	mov	r2, r5
   d871a:	4621      	mov	r1, r4
   d871c:	a803      	add	r0, sp, #12
   d871e:	f7fd ff08 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8722:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8724:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8726:	4633      	mov	r3, r6
   d8728:	462a      	mov	r2, r5
   d872a:	4621      	mov	r1, r4
   d872c:	4640      	mov	r0, r8
   d872e:	f7fd ffb1 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8732:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8734:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8736:	4633      	mov	r3, r6
   d8738:	462a      	mov	r2, r5
   d873a:	4621      	mov	r1, r4
   d873c:	a810      	add	r0, sp, #64	; 0x40
   d873e:	f7fd ffa9 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8742:	9a22      	ldr	r2, [sp, #136]	; 0x88
   d8744:	f859 302b 	ldr.w	r3, [r9, fp, lsl #2]
   d8748:	f852 2020 	ldr.w	r2, [r2, r0, lsl #2]
   d874c:	4293      	cmp	r3, r2
   d874e:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d8750:	bfd4      	ite	le
   d8752:	2300      	movle	r3, #0
   d8754:	2301      	movgt	r3, #1
   d8756:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d875a:	3701      	adds	r7, #1
   d875c:	e7d4      	b.n	d8708 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d875e:	3601      	adds	r6, #1
   d8760:	e7cb      	b.n	d86fa <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8762:	3501      	adds	r5, #1
   d8764:	e7c2      	b.n	d86ec <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8766:	3401      	adds	r4, #1
   d8768:	e7b9      	b.n	d86de <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d876a:	a803      	add	r0, sp, #12
   d876c:	f7fd fe71 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8770:	b019      	add	sp, #100	; 0x64
   d8772:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8776 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8776:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d877a:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d877c:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d877e:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8780:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8782:	4692      	mov	sl, r2
   d8784:	460c      	mov	r4, r1
   d8786:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8788:	dd01      	ble.n	d878e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d878a:	f00b fddf 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d878e:	682b      	ldr	r3, [r5, #0]
   d8790:	2b04      	cmp	r3, #4
   d8792:	dcfa      	bgt.n	d878a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8794:	6813      	ldr	r3, [r2, #0]
   d8796:	2b04      	cmp	r3, #4
   d8798:	dcf7      	bgt.n	d878a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d879a:	2301      	movs	r3, #1
   d879c:	2104      	movs	r1, #4
   d879e:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d87a0:	f10d 0820 	add.w	r8, sp, #32
   d87a4:	f7fd fe99 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d87a8:	4620      	mov	r0, r4
   d87aa:	ab10      	add	r3, sp, #64	; 0x40
   d87ac:	4642      	mov	r2, r8
   d87ae:	4629      	mov	r1, r5
   d87b0:	f7fe f9a8 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d87b4:	2400      	movs	r4, #0
   d87b6:	2100      	movs	r1, #0
   d87b8:	a803      	add	r0, sp, #12
   d87ba:	f7fd fe55 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d87be:	4284      	cmp	r4, r0
   d87c0:	da45      	bge.n	d884e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd8>
   d87c2:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d87c4:	2101      	movs	r1, #1
   d87c6:	a803      	add	r0, sp, #12
   d87c8:	f7fd fe4e 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d87cc:	4285      	cmp	r5, r0
   d87ce:	da3c      	bge.n	d884a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd4>
   d87d0:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d87d2:	2102      	movs	r1, #2
   d87d4:	a803      	add	r0, sp, #12
   d87d6:	f7fd fe47 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d87da:	4286      	cmp	r6, r0
   d87dc:	da33      	bge.n	d8846 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd0>
   d87de:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d87e0:	2103      	movs	r1, #3
   d87e2:	a803      	add	r0, sp, #12
   d87e4:	f7fd fe40 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d87e8:	4287      	cmp	r7, r0
   d87ea:	da2a      	bge.n	d8842 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
          output_data[Offset(output_shape, b, y, x, c)] =
   d87ec:	9700      	str	r7, [sp, #0]
   d87ee:	4633      	mov	r3, r6
   d87f0:	462a      	mov	r2, r5
   d87f2:	4621      	mov	r1, r4
   d87f4:	a803      	add	r0, sp, #12
   d87f6:	f7fd fe9c 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d87fa:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d87fc:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d87fe:	4633      	mov	r3, r6
   d8800:	462a      	mov	r2, r5
   d8802:	4621      	mov	r1, r4
   d8804:	4640      	mov	r0, r8
   d8806:	f7fd ff45 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d880a:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d880c:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d880e:	4633      	mov	r3, r6
   d8810:	462a      	mov	r2, r5
   d8812:	4621      	mov	r1, r4
   d8814:	a810      	add	r0, sp, #64	; 0x40
   d8816:	f7fd ff3d 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d881a:	9b22      	ldr	r3, [sp, #136]	; 0x88
   d881c:	eb0a 09c9 	add.w	r9, sl, r9, lsl #3
   d8820:	eb03 00c0 	add.w	r0, r3, r0, lsl #3

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8824:	e9d0 0100 	ldrd	r0, r1, [r0]
   d8828:	e9d9 2300 	ldrd	r2, r3, [r9]
   d882c:	4290      	cmp	r0, r2
   d882e:	eb71 0303 	sbcs.w	r3, r1, r3
   d8832:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d8834:	bfb4      	ite	lt
   d8836:	2301      	movlt	r3, #1
   d8838:	2300      	movge	r3, #0
   d883a:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d883e:	3701      	adds	r7, #1
   d8840:	e7ce      	b.n	d87e0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8842:	3601      	adds	r6, #1
   d8844:	e7c5      	b.n	d87d2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8846:	3501      	adds	r5, #1
   d8848:	e7bc      	b.n	d87c4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d884a:	3401      	adds	r4, #1
   d884c:	e7b3      	b.n	d87b6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d884e:	a803      	add	r0, sp, #12
   d8850:	f7fd fdff 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8854:	b019      	add	sp, #100	; 0x64
   d8856:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d885a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d885a:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d885e:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8860:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8862:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8864:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8866:	4692      	mov	sl, r2
   d8868:	460c      	mov	r4, r1
   d886a:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d886c:	dd01      	ble.n	d8872 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d886e:	f00b fd6d 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8872:	682b      	ldr	r3, [r5, #0]
   d8874:	2b04      	cmp	r3, #4
   d8876:	dcfa      	bgt.n	d886e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8878:	6813      	ldr	r3, [r2, #0]
   d887a:	2b04      	cmp	r3, #4
   d887c:	dcf7      	bgt.n	d886e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d887e:	2301      	movs	r3, #1
   d8880:	2104      	movs	r1, #4
   d8882:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8884:	f10d 0820 	add.w	r8, sp, #32
   d8888:	f7fd fe27 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d888c:	4620      	mov	r0, r4
   d888e:	ab10      	add	r3, sp, #64	; 0x40
   d8890:	4642      	mov	r2, r8
   d8892:	4629      	mov	r1, r5
   d8894:	f7fe f936 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8898:	2400      	movs	r4, #0
   d889a:	2100      	movs	r1, #0
   d889c:	a803      	add	r0, sp, #12
   d889e:	f7fd fde3 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d88a2:	4284      	cmp	r4, r0
   d88a4:	da46      	bge.n	d8934 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xda>
   d88a6:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d88a8:	2101      	movs	r1, #1
   d88aa:	a803      	add	r0, sp, #12
   d88ac:	f7fd fddc 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d88b0:	4285      	cmp	r5, r0
   d88b2:	da3d      	bge.n	d8930 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd6>
   d88b4:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d88b6:	2102      	movs	r1, #2
   d88b8:	a803      	add	r0, sp, #12
   d88ba:	f7fd fdd5 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d88be:	4286      	cmp	r6, r0
   d88c0:	da34      	bge.n	d892c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd2>
   d88c2:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d88c4:	2103      	movs	r1, #3
   d88c6:	a803      	add	r0, sp, #12
   d88c8:	f7fd fdce 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d88cc:	4287      	cmp	r7, r0
   d88ce:	da2b      	bge.n	d8928 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xce>
          output_data[Offset(output_shape, b, y, x, c)] =
   d88d0:	9700      	str	r7, [sp, #0]
   d88d2:	4633      	mov	r3, r6
   d88d4:	462a      	mov	r2, r5
   d88d6:	4621      	mov	r1, r4
   d88d8:	a803      	add	r0, sp, #12
   d88da:	f7fd fe2a 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d88de:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d88e0:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d88e2:	4633      	mov	r3, r6
   d88e4:	462a      	mov	r2, r5
   d88e6:	4621      	mov	r1, r4
   d88e8:	4640      	mov	r0, r8
   d88ea:	f7fd fed3 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d88ee:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d88f0:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d88f2:	4633      	mov	r3, r6
   d88f4:	462a      	mov	r2, r5
   d88f6:	4621      	mov	r1, r4
   d88f8:	a810      	add	r0, sp, #64	; 0x40
   d88fa:	f7fd fecb 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d88fe:	9b22      	ldr	r3, [sp, #136]	; 0x88

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8900:	9a24      	ldr	r2, [sp, #144]	; 0x90
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8902:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   d8906:	eb0a 0989 	add.w	r9, sl, r9, lsl #2

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d890a:	ed99 7a00 	vldr	s14, [r9]
   d890e:	edd0 7a00 	vldr	s15, [r0]
   d8912:	eeb4 7ae7 	vcmpe.f32	s14, s15
   d8916:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d891a:	bfac      	ite	ge
   d891c:	2301      	movge	r3, #1
   d891e:	2300      	movlt	r3, #0
   d8920:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8924:	3701      	adds	r7, #1
   d8926:	e7cd      	b.n	d88c4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8928:	3601      	adds	r6, #1
   d892a:	e7c4      	b.n	d88b6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d892c:	3501      	adds	r5, #1
   d892e:	e7bb      	b.n	d88a8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8930:	3401      	adds	r4, #1
   d8932:	e7b2      	b.n	d889a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8934:	a803      	add	r0, sp, #12
   d8936:	f7fd fd8c 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d893a:	b019      	add	sp, #100	; 0x64
   d893c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8940 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8940:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8944:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8946:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8948:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d894a:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d894c:	4691      	mov	r9, r2
   d894e:	460c      	mov	r4, r1
   d8950:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8952:	dd01      	ble.n	d8958 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8954:	f00b fcfa 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8958:	682b      	ldr	r3, [r5, #0]
   d895a:	2b04      	cmp	r3, #4
   d895c:	dcfa      	bgt.n	d8954 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d895e:	6813      	ldr	r3, [r2, #0]
   d8960:	2b04      	cmp	r3, #4
   d8962:	dcf7      	bgt.n	d8954 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8964:	2301      	movs	r3, #1
   d8966:	2104      	movs	r1, #4
   d8968:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d896a:	f10d 0820 	add.w	r8, sp, #32
   d896e:	f7fd fdb4 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8972:	4620      	mov	r0, r4
   d8974:	ab10      	add	r3, sp, #64	; 0x40
   d8976:	4642      	mov	r2, r8
   d8978:	4629      	mov	r1, r5
   d897a:	f7fe f8c3 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d897e:	2400      	movs	r4, #0
   d8980:	2100      	movs	r1, #0
   d8982:	a803      	add	r0, sp, #12
   d8984:	f7fd fd70 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8988:	4284      	cmp	r4, r0
   d898a:	da3f      	bge.n	d8a0c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
   d898c:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d898e:	2101      	movs	r1, #1
   d8990:	a803      	add	r0, sp, #12
   d8992:	f7fd fd69 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8996:	4285      	cmp	r5, r0
   d8998:	da36      	bge.n	d8a08 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
   d899a:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d899c:	2102      	movs	r1, #2
   d899e:	a803      	add	r0, sp, #12
   d89a0:	f7fd fd62 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d89a4:	4286      	cmp	r6, r0
   d89a6:	da2d      	bge.n	d8a04 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
   d89a8:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d89aa:	2103      	movs	r1, #3
   d89ac:	a803      	add	r0, sp, #12
   d89ae:	f7fd fd5b 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d89b2:	4287      	cmp	r7, r0
   d89b4:	da24      	bge.n	d8a00 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
          output_data[Offset(output_shape, b, y, x, c)] =
   d89b6:	9700      	str	r7, [sp, #0]
   d89b8:	4633      	mov	r3, r6
   d89ba:	462a      	mov	r2, r5
   d89bc:	4621      	mov	r1, r4
   d89be:	a803      	add	r0, sp, #12
   d89c0:	f7fd fdb7 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d89c4:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d89c6:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d89c8:	4633      	mov	r3, r6
   d89ca:	462a      	mov	r2, r5
   d89cc:	4621      	mov	r1, r4
   d89ce:	4640      	mov	r0, r8
   d89d0:	f7fd fe60 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d89d4:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d89d6:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d89d8:	4633      	mov	r3, r6
   d89da:	462a      	mov	r2, r5
   d89dc:	4621      	mov	r1, r4
   d89de:	a810      	add	r0, sp, #64	; 0x40
   d89e0:	f7fd fe58 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d89e4:	9a22      	ldr	r2, [sp, #136]	; 0x88
   d89e6:	f859 302b 	ldr.w	r3, [r9, fp, lsl #2]
   d89ea:	f852 2020 	ldr.w	r2, [r2, r0, lsl #2]
   d89ee:	4293      	cmp	r3, r2
   d89f0:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d89f2:	bfb4      	ite	lt
   d89f4:	2300      	movlt	r3, #0
   d89f6:	2301      	movge	r3, #1
   d89f8:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d89fc:	3701      	adds	r7, #1
   d89fe:	e7d4      	b.n	d89aa <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8a00:	3601      	adds	r6, #1
   d8a02:	e7cb      	b.n	d899c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8a04:	3501      	adds	r5, #1
   d8a06:	e7c2      	b.n	d898e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8a08:	3401      	adds	r4, #1
   d8a0a:	e7b9      	b.n	d8980 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8a0c:	a803      	add	r0, sp, #12
   d8a0e:	f7fd fd20 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8a12:	b019      	add	sp, #100	; 0x64
   d8a14:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8a18 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8a18:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8a1c:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8a1e:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8a20:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8a22:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8a24:	4692      	mov	sl, r2
   d8a26:	460c      	mov	r4, r1
   d8a28:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8a2a:	dd01      	ble.n	d8a30 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8a2c:	f00b fc8e 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8a30:	682b      	ldr	r3, [r5, #0]
   d8a32:	2b04      	cmp	r3, #4
   d8a34:	dcfa      	bgt.n	d8a2c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8a36:	6813      	ldr	r3, [r2, #0]
   d8a38:	2b04      	cmp	r3, #4
   d8a3a:	dcf7      	bgt.n	d8a2c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8a3c:	2301      	movs	r3, #1
   d8a3e:	2104      	movs	r1, #4
   d8a40:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8a42:	f10d 0820 	add.w	r8, sp, #32
   d8a46:	f7fd fd48 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8a4a:	4620      	mov	r0, r4
   d8a4c:	ab10      	add	r3, sp, #64	; 0x40
   d8a4e:	4642      	mov	r2, r8
   d8a50:	4629      	mov	r1, r5
   d8a52:	f7fe f857 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8a56:	2400      	movs	r4, #0
   d8a58:	2100      	movs	r1, #0
   d8a5a:	a803      	add	r0, sp, #12
   d8a5c:	f7fd fd04 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8a60:	4284      	cmp	r4, r0
   d8a62:	da45      	bge.n	d8af0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd8>
   d8a64:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8a66:	2101      	movs	r1, #1
   d8a68:	a803      	add	r0, sp, #12
   d8a6a:	f7fd fcfd 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8a6e:	4285      	cmp	r5, r0
   d8a70:	da3c      	bge.n	d8aec <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd4>
   d8a72:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8a74:	2102      	movs	r1, #2
   d8a76:	a803      	add	r0, sp, #12
   d8a78:	f7fd fcf6 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8a7c:	4286      	cmp	r6, r0
   d8a7e:	da33      	bge.n	d8ae8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd0>
   d8a80:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8a82:	2103      	movs	r1, #3
   d8a84:	a803      	add	r0, sp, #12
   d8a86:	f7fd fcef 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8a8a:	4287      	cmp	r7, r0
   d8a8c:	da2a      	bge.n	d8ae4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8a8e:	9700      	str	r7, [sp, #0]
   d8a90:	4633      	mov	r3, r6
   d8a92:	462a      	mov	r2, r5
   d8a94:	4621      	mov	r1, r4
   d8a96:	a803      	add	r0, sp, #12
   d8a98:	f7fd fd4b 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8a9c:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8a9e:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8aa0:	4633      	mov	r3, r6
   d8aa2:	462a      	mov	r2, r5
   d8aa4:	4621      	mov	r1, r4
   d8aa6:	4640      	mov	r0, r8
   d8aa8:	f7fd fdf4 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8aac:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8aae:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8ab0:	4633      	mov	r3, r6
   d8ab2:	462a      	mov	r2, r5
   d8ab4:	4621      	mov	r1, r4
   d8ab6:	a810      	add	r0, sp, #64	; 0x40
   d8ab8:	f7fd fdec 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8abc:	9b22      	ldr	r3, [sp, #136]	; 0x88
   d8abe:	eb0a 09c9 	add.w	r9, sl, r9, lsl #3
   d8ac2:	eb03 03c0 	add.w	r3, r3, r0, lsl #3

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8ac6:	e9d3 2300 	ldrd	r2, r3, [r3]
   d8aca:	e9d9 0100 	ldrd	r0, r1, [r9]
   d8ace:	4290      	cmp	r0, r2
   d8ad0:	eb71 0303 	sbcs.w	r3, r1, r3
   d8ad4:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d8ad6:	bfac      	ite	ge
   d8ad8:	2301      	movge	r3, #1
   d8ada:	2300      	movlt	r3, #0
   d8adc:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8ae0:	3701      	adds	r7, #1
   d8ae2:	e7ce      	b.n	d8a82 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8ae4:	3601      	adds	r6, #1
   d8ae6:	e7c5      	b.n	d8a74 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8ae8:	3501      	adds	r5, #1
   d8aea:	e7bc      	b.n	d8a66 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8aec:	3401      	adds	r4, #1
   d8aee:	e7b3      	b.n	d8a58 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8af0:	a803      	add	r0, sp, #12
   d8af2:	f7fd fcae 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8af6:	b019      	add	sp, #100	; 0x64
   d8af8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8afc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8afc:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8b00:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8b02:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8b04:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8b06:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8b08:	4692      	mov	sl, r2
   d8b0a:	460c      	mov	r4, r1
   d8b0c:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8b0e:	dd01      	ble.n	d8b14 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8b10:	f00b fc1c 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8b14:	682b      	ldr	r3, [r5, #0]
   d8b16:	2b04      	cmp	r3, #4
   d8b18:	dcfa      	bgt.n	d8b10 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8b1a:	6813      	ldr	r3, [r2, #0]
   d8b1c:	2b04      	cmp	r3, #4
   d8b1e:	dcf7      	bgt.n	d8b10 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8b20:	2301      	movs	r3, #1
   d8b22:	2104      	movs	r1, #4
   d8b24:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8b26:	f10d 0820 	add.w	r8, sp, #32
   d8b2a:	f7fd fcd6 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8b2e:	4620      	mov	r0, r4
   d8b30:	ab10      	add	r3, sp, #64	; 0x40
   d8b32:	4642      	mov	r2, r8
   d8b34:	4629      	mov	r1, r5
   d8b36:	f7fd ffe5 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8b3a:	2400      	movs	r4, #0
   d8b3c:	2100      	movs	r1, #0
   d8b3e:	a803      	add	r0, sp, #12
   d8b40:	f7fd fc92 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8b44:	4284      	cmp	r4, r0
   d8b46:	da46      	bge.n	d8bd6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xda>
   d8b48:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8b4a:	2101      	movs	r1, #1
   d8b4c:	a803      	add	r0, sp, #12
   d8b4e:	f7fd fc8b 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8b52:	4285      	cmp	r5, r0
   d8b54:	da3d      	bge.n	d8bd2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd6>
   d8b56:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8b58:	2102      	movs	r1, #2
   d8b5a:	a803      	add	r0, sp, #12
   d8b5c:	f7fd fc84 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8b60:	4286      	cmp	r6, r0
   d8b62:	da34      	bge.n	d8bce <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd2>
   d8b64:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8b66:	2103      	movs	r1, #3
   d8b68:	a803      	add	r0, sp, #12
   d8b6a:	f7fd fc7d 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8b6e:	4287      	cmp	r7, r0
   d8b70:	da2b      	bge.n	d8bca <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xce>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8b72:	9700      	str	r7, [sp, #0]
   d8b74:	4633      	mov	r3, r6
   d8b76:	462a      	mov	r2, r5
   d8b78:	4621      	mov	r1, r4
   d8b7a:	a803      	add	r0, sp, #12
   d8b7c:	f7fd fcd9 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8b80:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8b82:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8b84:	4633      	mov	r3, r6
   d8b86:	462a      	mov	r2, r5
   d8b88:	4621      	mov	r1, r4
   d8b8a:	4640      	mov	r0, r8
   d8b8c:	f7fd fd82 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8b90:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8b92:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8b94:	4633      	mov	r3, r6
   d8b96:	462a      	mov	r2, r5
   d8b98:	4621      	mov	r1, r4
   d8b9a:	a810      	add	r0, sp, #64	; 0x40
   d8b9c:	f7fd fd7a 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8ba0:	9b22      	ldr	r3, [sp, #136]	; 0x88

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8ba2:	9a24      	ldr	r2, [sp, #144]	; 0x90
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8ba4:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   d8ba8:	eb0a 0989 	add.w	r9, sl, r9, lsl #2

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8bac:	ed99 7a00 	vldr	s14, [r9]
   d8bb0:	edd0 7a00 	vldr	s15, [r0]
   d8bb4:	eeb4 7ae7 	vcmpe.f32	s14, s15
   d8bb8:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d8bbc:	bf4c      	ite	mi
   d8bbe:	2301      	movmi	r3, #1
   d8bc0:	2300      	movpl	r3, #0
   d8bc2:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8bc6:	3701      	adds	r7, #1
   d8bc8:	e7cd      	b.n	d8b66 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8bca:	3601      	adds	r6, #1
   d8bcc:	e7c4      	b.n	d8b58 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8bce:	3501      	adds	r5, #1
   d8bd0:	e7bb      	b.n	d8b4a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8bd2:	3401      	adds	r4, #1
   d8bd4:	e7b2      	b.n	d8b3c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8bd6:	a803      	add	r0, sp, #12
   d8bd8:	f7fd fc3b 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8bdc:	b019      	add	sp, #100	; 0x64
   d8bde:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8be2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8be2:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8be6:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8be8:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8bea:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8bec:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8bee:	4691      	mov	r9, r2
   d8bf0:	460c      	mov	r4, r1
   d8bf2:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8bf4:	dd01      	ble.n	d8bfa <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8bf6:	f00b fba9 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8bfa:	682b      	ldr	r3, [r5, #0]
   d8bfc:	2b04      	cmp	r3, #4
   d8bfe:	dcfa      	bgt.n	d8bf6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8c00:	6813      	ldr	r3, [r2, #0]
   d8c02:	2b04      	cmp	r3, #4
   d8c04:	dcf7      	bgt.n	d8bf6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8c06:	2301      	movs	r3, #1
   d8c08:	2104      	movs	r1, #4
   d8c0a:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8c0c:	f10d 0820 	add.w	r8, sp, #32
   d8c10:	f7fd fc63 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8c14:	4620      	mov	r0, r4
   d8c16:	ab10      	add	r3, sp, #64	; 0x40
   d8c18:	4642      	mov	r2, r8
   d8c1a:	4629      	mov	r1, r5
   d8c1c:	f7fd ff72 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8c20:	2400      	movs	r4, #0
   d8c22:	2100      	movs	r1, #0
   d8c24:	a803      	add	r0, sp, #12
   d8c26:	f7fd fc1f 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8c2a:	4284      	cmp	r4, r0
   d8c2c:	da3f      	bge.n	d8cae <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
   d8c2e:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8c30:	2101      	movs	r1, #1
   d8c32:	a803      	add	r0, sp, #12
   d8c34:	f7fd fc18 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8c38:	4285      	cmp	r5, r0
   d8c3a:	da36      	bge.n	d8caa <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
   d8c3c:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8c3e:	2102      	movs	r1, #2
   d8c40:	a803      	add	r0, sp, #12
   d8c42:	f7fd fc11 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8c46:	4286      	cmp	r6, r0
   d8c48:	da2d      	bge.n	d8ca6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
   d8c4a:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8c4c:	2103      	movs	r1, #3
   d8c4e:	a803      	add	r0, sp, #12
   d8c50:	f7fd fc0a 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8c54:	4287      	cmp	r7, r0
   d8c56:	da24      	bge.n	d8ca2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8c58:	9700      	str	r7, [sp, #0]
   d8c5a:	4633      	mov	r3, r6
   d8c5c:	462a      	mov	r2, r5
   d8c5e:	4621      	mov	r1, r4
   d8c60:	a803      	add	r0, sp, #12
   d8c62:	f7fd fc66 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8c66:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8c68:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8c6a:	4633      	mov	r3, r6
   d8c6c:	462a      	mov	r2, r5
   d8c6e:	4621      	mov	r1, r4
   d8c70:	4640      	mov	r0, r8
   d8c72:	f7fd fd0f 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8c76:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8c78:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8c7a:	4633      	mov	r3, r6
   d8c7c:	462a      	mov	r2, r5
   d8c7e:	4621      	mov	r1, r4
   d8c80:	a810      	add	r0, sp, #64	; 0x40
   d8c82:	f7fd fd07 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8c86:	9a22      	ldr	r2, [sp, #136]	; 0x88
   d8c88:	f859 302b 	ldr.w	r3, [r9, fp, lsl #2]
   d8c8c:	f852 2020 	ldr.w	r2, [r2, r0, lsl #2]
   d8c90:	4293      	cmp	r3, r2
   d8c92:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d8c94:	bfac      	ite	ge
   d8c96:	2300      	movge	r3, #0
   d8c98:	2301      	movlt	r3, #1
   d8c9a:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8c9e:	3701      	adds	r7, #1
   d8ca0:	e7d4      	b.n	d8c4c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8ca2:	3601      	adds	r6, #1
   d8ca4:	e7cb      	b.n	d8c3e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8ca6:	3501      	adds	r5, #1
   d8ca8:	e7c2      	b.n	d8c30 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8caa:	3401      	adds	r4, #1
   d8cac:	e7b9      	b.n	d8c22 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8cae:	a803      	add	r0, sp, #12
   d8cb0:	f7fd fbcf 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8cb4:	b019      	add	sp, #100	; 0x64
   d8cb6:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8cba <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8cba:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8cbe:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8cc0:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8cc2:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8cc4:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8cc6:	4692      	mov	sl, r2
   d8cc8:	460c      	mov	r4, r1
   d8cca:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8ccc:	dd01      	ble.n	d8cd2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8cce:	f00b fb3d 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8cd2:	682b      	ldr	r3, [r5, #0]
   d8cd4:	2b04      	cmp	r3, #4
   d8cd6:	dcfa      	bgt.n	d8cce <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8cd8:	6813      	ldr	r3, [r2, #0]
   d8cda:	2b04      	cmp	r3, #4
   d8cdc:	dcf7      	bgt.n	d8cce <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8cde:	2301      	movs	r3, #1
   d8ce0:	2104      	movs	r1, #4
   d8ce2:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8ce4:	f10d 0820 	add.w	r8, sp, #32
   d8ce8:	f7fd fbf7 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8cec:	4620      	mov	r0, r4
   d8cee:	ab10      	add	r3, sp, #64	; 0x40
   d8cf0:	4642      	mov	r2, r8
   d8cf2:	4629      	mov	r1, r5
   d8cf4:	f7fd ff06 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8cf8:	2400      	movs	r4, #0
   d8cfa:	2100      	movs	r1, #0
   d8cfc:	a803      	add	r0, sp, #12
   d8cfe:	f7fd fbb3 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8d02:	4284      	cmp	r4, r0
   d8d04:	da45      	bge.n	d8d92 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd8>
   d8d06:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8d08:	2101      	movs	r1, #1
   d8d0a:	a803      	add	r0, sp, #12
   d8d0c:	f7fd fbac 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8d10:	4285      	cmp	r5, r0
   d8d12:	da3c      	bge.n	d8d8e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd4>
   d8d14:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8d16:	2102      	movs	r1, #2
   d8d18:	a803      	add	r0, sp, #12
   d8d1a:	f7fd fba5 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8d1e:	4286      	cmp	r6, r0
   d8d20:	da33      	bge.n	d8d8a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd0>
   d8d22:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8d24:	2103      	movs	r1, #3
   d8d26:	a803      	add	r0, sp, #12
   d8d28:	f7fd fb9e 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8d2c:	4287      	cmp	r7, r0
   d8d2e:	da2a      	bge.n	d8d86 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8d30:	9700      	str	r7, [sp, #0]
   d8d32:	4633      	mov	r3, r6
   d8d34:	462a      	mov	r2, r5
   d8d36:	4621      	mov	r1, r4
   d8d38:	a803      	add	r0, sp, #12
   d8d3a:	f7fd fbfa 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8d3e:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8d40:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8d42:	4633      	mov	r3, r6
   d8d44:	462a      	mov	r2, r5
   d8d46:	4621      	mov	r1, r4
   d8d48:	4640      	mov	r0, r8
   d8d4a:	f7fd fca3 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8d4e:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8d50:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8d52:	4633      	mov	r3, r6
   d8d54:	462a      	mov	r2, r5
   d8d56:	4621      	mov	r1, r4
   d8d58:	a810      	add	r0, sp, #64	; 0x40
   d8d5a:	f7fd fc9b 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8d5e:	9b22      	ldr	r3, [sp, #136]	; 0x88
   d8d60:	eb0a 09c9 	add.w	r9, sl, r9, lsl #3
   d8d64:	eb03 03c0 	add.w	r3, r3, r0, lsl #3

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8d68:	e9d3 2300 	ldrd	r2, r3, [r3]
   d8d6c:	e9d9 0100 	ldrd	r0, r1, [r9]
   d8d70:	4290      	cmp	r0, r2
   d8d72:	eb71 0303 	sbcs.w	r3, r1, r3
   d8d76:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d8d78:	bfb4      	ite	lt
   d8d7a:	2301      	movlt	r3, #1
   d8d7c:	2300      	movge	r3, #0
   d8d7e:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8d82:	3701      	adds	r7, #1
   d8d84:	e7ce      	b.n	d8d24 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8d86:	3601      	adds	r6, #1
   d8d88:	e7c5      	b.n	d8d16 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8d8a:	3501      	adds	r5, #1
   d8d8c:	e7bc      	b.n	d8d08 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8d8e:	3401      	adds	r4, #1
   d8d90:	e7b3      	b.n	d8cfa <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8d92:	a803      	add	r0, sp, #12
   d8d94:	f7fd fb5d 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8d98:	b019      	add	sp, #100	; 0x64
   d8d9a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8d9e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8d9e:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8da2:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8da4:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8da6:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8da8:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8daa:	4692      	mov	sl, r2
   d8dac:	460c      	mov	r4, r1
   d8dae:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8db0:	dd01      	ble.n	d8db6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8db2:	f00b facb 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8db6:	682b      	ldr	r3, [r5, #0]
   d8db8:	2b04      	cmp	r3, #4
   d8dba:	dcfa      	bgt.n	d8db2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8dbc:	6813      	ldr	r3, [r2, #0]
   d8dbe:	2b04      	cmp	r3, #4
   d8dc0:	dcf7      	bgt.n	d8db2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8dc2:	2301      	movs	r3, #1
   d8dc4:	2104      	movs	r1, #4
   d8dc6:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8dc8:	f10d 0820 	add.w	r8, sp, #32
   d8dcc:	f7fd fb85 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8dd0:	4620      	mov	r0, r4
   d8dd2:	ab10      	add	r3, sp, #64	; 0x40
   d8dd4:	4642      	mov	r2, r8
   d8dd6:	4629      	mov	r1, r5
   d8dd8:	f7fd fe94 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8ddc:	2400      	movs	r4, #0
   d8dde:	2100      	movs	r1, #0
   d8de0:	a803      	add	r0, sp, #12
   d8de2:	f7fd fb41 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8de6:	4284      	cmp	r4, r0
   d8de8:	da46      	bge.n	d8e78 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xda>
   d8dea:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8dec:	2101      	movs	r1, #1
   d8dee:	a803      	add	r0, sp, #12
   d8df0:	f7fd fb3a 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8df4:	4285      	cmp	r5, r0
   d8df6:	da3d      	bge.n	d8e74 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd6>
   d8df8:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8dfa:	2102      	movs	r1, #2
   d8dfc:	a803      	add	r0, sp, #12
   d8dfe:	f7fd fb33 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8e02:	4286      	cmp	r6, r0
   d8e04:	da34      	bge.n	d8e70 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd2>
   d8e06:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8e08:	2103      	movs	r1, #3
   d8e0a:	a803      	add	r0, sp, #12
   d8e0c:	f7fd fb2c 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8e10:	4287      	cmp	r7, r0
   d8e12:	da2b      	bge.n	d8e6c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xce>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8e14:	9700      	str	r7, [sp, #0]
   d8e16:	4633      	mov	r3, r6
   d8e18:	462a      	mov	r2, r5
   d8e1a:	4621      	mov	r1, r4
   d8e1c:	a803      	add	r0, sp, #12
   d8e1e:	f7fd fb88 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8e22:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8e24:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8e26:	4633      	mov	r3, r6
   d8e28:	462a      	mov	r2, r5
   d8e2a:	4621      	mov	r1, r4
   d8e2c:	4640      	mov	r0, r8
   d8e2e:	f7fd fc31 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8e32:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8e34:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8e36:	4633      	mov	r3, r6
   d8e38:	462a      	mov	r2, r5
   d8e3a:	4621      	mov	r1, r4
   d8e3c:	a810      	add	r0, sp, #64	; 0x40
   d8e3e:	f7fd fc29 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8e42:	9b22      	ldr	r3, [sp, #136]	; 0x88

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8e44:	9a24      	ldr	r2, [sp, #144]	; 0x90
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8e46:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   d8e4a:	eb0a 0989 	add.w	r9, sl, r9, lsl #2

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8e4e:	ed99 7a00 	vldr	s14, [r9]
   d8e52:	edd0 7a00 	vldr	s15, [r0]
   d8e56:	eeb4 7ae7 	vcmpe.f32	s14, s15
   d8e5a:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d8e5e:	bf94      	ite	ls
   d8e60:	2301      	movls	r3, #1
   d8e62:	2300      	movhi	r3, #0
   d8e64:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8e68:	3701      	adds	r7, #1
   d8e6a:	e7cd      	b.n	d8e08 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8e6c:	3601      	adds	r6, #1
   d8e6e:	e7c4      	b.n	d8dfa <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8e70:	3501      	adds	r5, #1
   d8e72:	e7bb      	b.n	d8dec <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8e74:	3401      	adds	r4, #1
   d8e76:	e7b2      	b.n	d8dde <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8e78:	a803      	add	r0, sp, #12
   d8e7a:	f7fd faea 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8e7e:	b019      	add	sp, #100	; 0x64
   d8e80:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8e84 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8e84:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8e88:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8e8a:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8e8c:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8e8e:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8e90:	4691      	mov	r9, r2
   d8e92:	460c      	mov	r4, r1
   d8e94:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8e96:	dd01      	ble.n	d8e9c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8e98:	f00b fa58 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8e9c:	682b      	ldr	r3, [r5, #0]
   d8e9e:	2b04      	cmp	r3, #4
   d8ea0:	dcfa      	bgt.n	d8e98 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8ea2:	6813      	ldr	r3, [r2, #0]
   d8ea4:	2b04      	cmp	r3, #4
   d8ea6:	dcf7      	bgt.n	d8e98 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8ea8:	2301      	movs	r3, #1
   d8eaa:	2104      	movs	r1, #4
   d8eac:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8eae:	f10d 0820 	add.w	r8, sp, #32
   d8eb2:	f7fd fb12 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8eb6:	4620      	mov	r0, r4
   d8eb8:	ab10      	add	r3, sp, #64	; 0x40
   d8eba:	4642      	mov	r2, r8
   d8ebc:	4629      	mov	r1, r5
   d8ebe:	f7fd fe21 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8ec2:	2400      	movs	r4, #0
   d8ec4:	2100      	movs	r1, #0
   d8ec6:	a803      	add	r0, sp, #12
   d8ec8:	f7fd face 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8ecc:	4284      	cmp	r4, r0
   d8ece:	da3f      	bge.n	d8f50 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
   d8ed0:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8ed2:	2101      	movs	r1, #1
   d8ed4:	a803      	add	r0, sp, #12
   d8ed6:	f7fd fac7 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8eda:	4285      	cmp	r5, r0
   d8edc:	da36      	bge.n	d8f4c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc8>
   d8ede:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8ee0:	2102      	movs	r1, #2
   d8ee2:	a803      	add	r0, sp, #12
   d8ee4:	f7fd fac0 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8ee8:	4286      	cmp	r6, r0
   d8eea:	da2d      	bge.n	d8f48 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc4>
   d8eec:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8eee:	2103      	movs	r1, #3
   d8ef0:	a803      	add	r0, sp, #12
   d8ef2:	f7fd fab9 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8ef6:	4287      	cmp	r7, r0
   d8ef8:	da24      	bge.n	d8f44 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xc0>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8efa:	9700      	str	r7, [sp, #0]
   d8efc:	4633      	mov	r3, r6
   d8efe:	462a      	mov	r2, r5
   d8f00:	4621      	mov	r1, r4
   d8f02:	a803      	add	r0, sp, #12
   d8f04:	f7fd fb15 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8f08:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8f0a:	4682      	mov	sl, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8f0c:	4633      	mov	r3, r6
   d8f0e:	462a      	mov	r2, r5
   d8f10:	4621      	mov	r1, r4
   d8f12:	4640      	mov	r0, r8
   d8f14:	f7fd fbbe 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8f18:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8f1a:	4683      	mov	fp, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8f1c:	4633      	mov	r3, r6
   d8f1e:	462a      	mov	r2, r5
   d8f20:	4621      	mov	r1, r4
   d8f22:	a810      	add	r0, sp, #64	; 0x40
   d8f24:	f7fd fbb6 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8f28:	9a22      	ldr	r2, [sp, #136]	; 0x88
   d8f2a:	f859 302b 	ldr.w	r3, [r9, fp, lsl #2]
   d8f2e:	f852 2020 	ldr.w	r2, [r2, r0, lsl #2]
   d8f32:	4293      	cmp	r3, r2
   d8f34:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d8f36:	bfcc      	ite	gt
   d8f38:	2300      	movgt	r3, #0
   d8f3a:	2301      	movle	r3, #1
   d8f3c:	f802 300a 	strb.w	r3, [r2, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8f40:	3701      	adds	r7, #1
   d8f42:	e7d4      	b.n	d8eee <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8f44:	3601      	adds	r6, #1
   d8f46:	e7cb      	b.n	d8ee0 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8f48:	3501      	adds	r5, #1
   d8f4a:	e7c2      	b.n	d8ed2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8f4c:	3401      	adds	r4, #1
   d8f4e:	e7b9      	b.n	d8ec4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d8f50:	a803      	add	r0, sp, #12
   d8f52:	f7fd fa7e 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d8f56:	b019      	add	sp, #100	; 0x64
   d8f58:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d8f5c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8f5c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d8f60:	461d      	mov	r5, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8f62:	680b      	ldr	r3, [r1, #0]
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8f64:	b099      	sub	sp, #100	; 0x64
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8f66:	2b04      	cmp	r3, #4
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
  }
}

template <typename T, ComparisonFn<T> F>
inline void BroadcastComparison4DSlowImpl(
   d8f68:	4692      	mov	sl, r2
   d8f6a:	460c      	mov	r4, r1
   d8f6c:	9a23      	ldr	r2, [sp, #140]	; 0x8c
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d8f6e:	dd01      	ble.n	d8f74 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x18>
   d8f70:	f00b f9ec 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d8f74:	682b      	ldr	r3, [r5, #0]
   d8f76:	2b04      	cmp	r3, #4
   d8f78:	dcfa      	bgt.n	d8f70 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d8f7a:	6813      	ldr	r3, [r2, #0]
   d8f7c:	2b04      	cmp	r3, #4
   d8f7e:	dcf7      	bgt.n	d8f70 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x14>
   d8f80:	2301      	movs	r3, #1
   d8f82:	2104      	movs	r1, #4
   d8f84:	a803      	add	r0, sp, #12
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d8f86:	f10d 0820 	add.w	r8, sp, #32
   d8f8a:	f7fd faa6 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d8f8e:	4620      	mov	r0, r4
   d8f90:	ab10      	add	r3, sp, #64	; 0x40
   d8f92:	4642      	mov	r2, r8
   d8f94:	4629      	mov	r1, r5
   d8f96:	f7fd fdb5 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d8f9a:	2400      	movs	r4, #0
   d8f9c:	2100      	movs	r1, #0
   d8f9e:	a803      	add	r0, sp, #12
   d8fa0:	f7fd fa62 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8fa4:	4284      	cmp	r4, r0
   d8fa6:	da45      	bge.n	d9034 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd8>
   d8fa8:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d8faa:	2101      	movs	r1, #1
   d8fac:	a803      	add	r0, sp, #12
   d8fae:	f7fd fa5b 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8fb2:	4285      	cmp	r5, r0
   d8fb4:	da3c      	bge.n	d9030 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd4>
   d8fb6:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d8fb8:	2102      	movs	r1, #2
   d8fba:	a803      	add	r0, sp, #12
   d8fbc:	f7fd fa54 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8fc0:	4286      	cmp	r6, r0
   d8fc2:	da33      	bge.n	d902c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xd0>
   d8fc4:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d8fc6:	2103      	movs	r1, #3
   d8fc8:	a803      	add	r0, sp, #12
   d8fca:	f7fd fa4d 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d8fce:	4287      	cmp	r7, r0
   d8fd0:	da2a      	bge.n	d9028 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0xcc>
          output_data[Offset(output_shape, b, y, x, c)] =
   d8fd2:	9700      	str	r7, [sp, #0]
   d8fd4:	4633      	mov	r3, r6
   d8fd6:	462a      	mov	r2, r5
   d8fd8:	4621      	mov	r1, r4
   d8fda:	a803      	add	r0, sp, #12
   d8fdc:	f7fd faa9 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8fe0:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d8fe2:	4683      	mov	fp, r0
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8fe4:	4633      	mov	r3, r6
   d8fe6:	462a      	mov	r2, r5
   d8fe8:	4621      	mov	r1, r4
   d8fea:	4640      	mov	r0, r8
   d8fec:	f7fd fb52 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8ff0:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d8ff2:	4681      	mov	r9, r0
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
   d8ff4:	4633      	mov	r3, r6
   d8ff6:	462a      	mov	r2, r5
   d8ff8:	4621      	mov	r1, r4
   d8ffa:	a810      	add	r0, sp, #64	; 0x40
   d8ffc:	f7fd fb4a 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
              F(input1_data[SubscriptToIndex(desc1, b, y, x, c)],
   d9000:	9b22      	ldr	r3, [sp, #136]	; 0x88
   d9002:	eb0a 09c9 	add.w	r9, sl, r9, lsl #3
   d9006:	eb03 00c0 	add.w	r0, r3, r0, lsl #3

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          output_data[Offset(output_shape, b, y, x, c)] =
   d900a:	e9d0 0100 	ldrd	r0, r1, [r0]
   d900e:	e9d9 2300 	ldrd	r2, r3, [r9]
   d9012:	4290      	cmp	r0, r2
   d9014:	eb71 0303 	sbcs.w	r3, r1, r3
   d9018:	9a24      	ldr	r2, [sp, #144]	; 0x90
   d901a:	bfac      	ite	ge
   d901c:	2301      	movge	r3, #1
   d901e:	2300      	movlt	r3, #0
   d9020:	f802 300b 	strb.w	r3, [r2, fp]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d9024:	3701      	adds	r7, #1
   d9026:	e7ce      	b.n	d8fc6 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6a>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d9028:	3601      	adds	r6, #1
   d902a:	e7c5      	b.n	d8fb8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5c>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d902c:	3501      	adds	r5, #1
   d902e:	e7bc      	b.n	d8faa <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x4e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d9030:	3401      	adds	r4, #1
   d9032:	e7b3      	b.n	d8f9c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x40>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d9034:	a803      	add	r0, sp, #12
   d9036:	f7fd fa0c 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                input2_data[SubscriptToIndex(desc2, b, y, x, c)]);
        }
      }
    }
  }
}
   d903a:	b019      	add	sp, #100	; 0x64
   d903c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d9040 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d9040:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d9044:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d9046:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d9048:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d904a:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d904c:	9208      	str	r2, [sp, #32]
   d904e:	4604      	mov	r4, r0
   d9050:	460e      	mov	r6, r1
   d9052:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d9054:	dd01      	ble.n	d905a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   d9056:	f00b f979 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d905a:	683b      	ldr	r3, [r7, #0]
   d905c:	2b04      	cmp	r3, #4
   d905e:	dcfa      	bgt.n	d9056 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d9060:	6813      	ldr	r3, [r2, #0]
   d9062:	2b04      	cmp	r3, #4
   d9064:	dcf7      	bgt.n	d9056 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   d9066:	2301      	movs	r3, #1
   d9068:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d906a:	ad10      	add	r5, sp, #64	; 0x40
   d906c:	a80b      	add	r0, sp, #44	; 0x2c
   d906e:	f7fd fa34 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d9072:	ab18      	add	r3, sp, #96	; 0x60
   d9074:	462a      	mov	r2, r5
   d9076:	4639      	mov	r1, r7
   d9078:	4630      	mov	r0, r6
   d907a:	f7fd fd43 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   d907e:	6863      	ldr	r3, [r4, #4]
   d9080:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   d9082:	68a3      	ldr	r3, [r4, #8]
   d9084:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   d9086:	68e3      	ldr	r3, [r4, #12]
   d9088:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   d908a:	6923      	ldr	r3, [r4, #16]
   d908c:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   d908e:	6963      	ldr	r3, [r4, #20]
   d9090:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   d9092:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   d9094:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   d9098:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d909a:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d909c:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d909e:	2100      	movs	r1, #0
   d90a0:	a80b      	add	r0, sp, #44	; 0x2c
   d90a2:	f7fd f9e1 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d90a6:	4284      	cmp	r4, r0
   d90a8:	da59      	bge.n	d915e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   d90aa:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d90ac:	af0b      	add	r7, sp, #44	; 0x2c
   d90ae:	2101      	movs	r1, #1
   d90b0:	4638      	mov	r0, r7
   d90b2:	f7fd f9d9 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d90b6:	4285      	cmp	r5, r0
   d90b8:	da4f      	bge.n	d915a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   d90ba:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d90bc:	2102      	movs	r1, #2
   d90be:	4638      	mov	r0, r7
   d90c0:	f7fd f9d2 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d90c4:	4286      	cmp	r6, r0
   d90c6:	da46      	bge.n	d9156 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   d90c8:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d90cc:	2103      	movs	r1, #3
   d90ce:	4638      	mov	r0, r7
   d90d0:	f7fd f9ca 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d90d4:	4580      	cmp	r8, r0
   d90d6:	da3c      	bge.n	d9152 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d90d8:	f8cd 8000 	str.w	r8, [sp]
   d90dc:	4633      	mov	r3, r6
   d90de:	462a      	mov	r2, r5
   d90e0:	4621      	mov	r1, r4
   d90e2:	9809      	ldr	r0, [sp, #36]	; 0x24
   d90e4:	f7fd fad6 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   d90e8:	9b08      	ldr	r3, [sp, #32]
   d90ea:	f813 9000 	ldrb.w	r9, [r3, r0]
   d90ee:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d90f0:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d90f4:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d90f6:	462a      	mov	r2, r5
   d90f8:	4633      	mov	r3, r6
   d90fa:	4621      	mov	r1, r4
   d90fc:	a818      	add	r0, sp, #96	; 0x60
   d90fe:	f7fd fac9 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9102:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d9104:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9106:	f813 b000 	ldrb.w	fp, [r3, r0]
   d910a:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d910c:	9903      	ldr	r1, [sp, #12]
   d910e:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9112:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d9114:	f7fd fa6c 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9118:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d911c:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   d911e:	9a07      	ldr	r2, [sp, #28]
   d9120:	9906      	ldr	r1, [sp, #24]
   d9122:	4658      	mov	r0, fp
   d9124:	f7fd fa64 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   d9128:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   d912c:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   d912e:	4633      	mov	r3, r6
   d9130:	462a      	mov	r2, r5
   d9132:	4621      	mov	r1, r4
   d9134:	4638      	mov	r0, r7
   d9136:	f7fd f9fc 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   d913a:	ebcb 0309 	rsb	r3, fp, r9
   d913e:	f1d3 0900 	rsbs	r9, r3, #0
   d9142:	eb49 0903 	adc.w	r9, r9, r3
   d9146:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d9148:	f108 0801 	add.w	r8, r8, #1
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
          output_data[Offset(output_shape, b, y, x, c)] =
   d914c:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d9150:	e7bc      	b.n	d90cc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d9152:	3601      	adds	r6, #1
   d9154:	e7b2      	b.n	d90bc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d9156:	3501      	adds	r5, #1
   d9158:	e7a8      	b.n	d90ac <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d915a:	3401      	adds	r4, #1
   d915c:	e79f      	b.n	d909e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d915e:	a80b      	add	r0, sp, #44	; 0x2c
   d9160:	f7fd f977 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   d9164:	b021      	add	sp, #132	; 0x84
   d9166:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d916a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d916a:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d916e:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d9170:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d9172:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d9174:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d9176:	9208      	str	r2, [sp, #32]
   d9178:	4604      	mov	r4, r0
   d917a:	460e      	mov	r6, r1
   d917c:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d917e:	dd01      	ble.n	d9184 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   d9180:	f00b f8e4 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d9184:	683b      	ldr	r3, [r7, #0]
   d9186:	2b04      	cmp	r3, #4
   d9188:	dcfa      	bgt.n	d9180 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d918a:	6813      	ldr	r3, [r2, #0]
   d918c:	2b04      	cmp	r3, #4
   d918e:	dcf7      	bgt.n	d9180 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   d9190:	2301      	movs	r3, #1
   d9192:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d9194:	ad10      	add	r5, sp, #64	; 0x40
   d9196:	a80b      	add	r0, sp, #44	; 0x2c
   d9198:	f7fd f99f 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d919c:	ab18      	add	r3, sp, #96	; 0x60
   d919e:	462a      	mov	r2, r5
   d91a0:	4639      	mov	r1, r7
   d91a2:	4630      	mov	r0, r6
   d91a4:	f7fd fcae 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   d91a8:	6863      	ldr	r3, [r4, #4]
   d91aa:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   d91ac:	68a3      	ldr	r3, [r4, #8]
   d91ae:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   d91b0:	68e3      	ldr	r3, [r4, #12]
   d91b2:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   d91b4:	6923      	ldr	r3, [r4, #16]
   d91b6:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   d91b8:	6963      	ldr	r3, [r4, #20]
   d91ba:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   d91bc:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   d91be:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   d91c2:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d91c4:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d91c6:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d91c8:	2100      	movs	r1, #0
   d91ca:	a80b      	add	r0, sp, #44	; 0x2c
   d91cc:	f7fd f94c 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d91d0:	4284      	cmp	r4, r0
   d91d2:	da59      	bge.n	d9288 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   d91d4:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d91d6:	af0b      	add	r7, sp, #44	; 0x2c
   d91d8:	2101      	movs	r1, #1
   d91da:	4638      	mov	r0, r7
   d91dc:	f7fd f944 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d91e0:	4285      	cmp	r5, r0
   d91e2:	da4f      	bge.n	d9284 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   d91e4:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d91e6:	2102      	movs	r1, #2
   d91e8:	4638      	mov	r0, r7
   d91ea:	f7fd f93d 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d91ee:	4286      	cmp	r6, r0
   d91f0:	da46      	bge.n	d9280 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   d91f2:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d91f6:	2103      	movs	r1, #3
   d91f8:	4638      	mov	r0, r7
   d91fa:	f7fd f935 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d91fe:	4580      	cmp	r8, r0
   d9200:	da3c      	bge.n	d927c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d9202:	f8cd 8000 	str.w	r8, [sp]
   d9206:	4633      	mov	r3, r6
   d9208:	462a      	mov	r2, r5
   d920a:	4621      	mov	r1, r4
   d920c:	9809      	ldr	r0, [sp, #36]	; 0x24
   d920e:	f7fd fa41 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   d9212:	9b08      	ldr	r3, [sp, #32]
   d9214:	f913 9000 	ldrsb.w	r9, [r3, r0]
   d9218:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d921a:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d921e:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d9220:	462a      	mov	r2, r5
   d9222:	4633      	mov	r3, r6
   d9224:	4621      	mov	r1, r4
   d9226:	a818      	add	r0, sp, #96	; 0x60
   d9228:	f7fd fa34 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d922c:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d922e:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9230:	f913 b000 	ldrsb.w	fp, [r3, r0]
   d9234:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d9236:	9903      	ldr	r1, [sp, #12]
   d9238:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d923c:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d923e:	f7fd f9d7 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9242:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d9246:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   d9248:	9a07      	ldr	r2, [sp, #28]
   d924a:	9906      	ldr	r1, [sp, #24]
   d924c:	4658      	mov	r0, fp
   d924e:	f7fd f9cf 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   d9252:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   d9256:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   d9258:	4633      	mov	r3, r6
   d925a:	462a      	mov	r2, r5
   d925c:	4621      	mov	r1, r4
   d925e:	4638      	mov	r0, r7
   d9260:	f7fd f967 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   d9264:	ebcb 0309 	rsb	r3, fp, r9
   d9268:	f1d3 0900 	rsbs	r9, r3, #0
   d926c:	eb49 0903 	adc.w	r9, r9, r3
   d9270:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d9272:	f108 0801 	add.w	r8, r8, #1
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
          output_data[Offset(output_shape, b, y, x, c)] =
   d9276:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d927a:	e7bc      	b.n	d91f6 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d927c:	3601      	adds	r6, #1
   d927e:	e7b2      	b.n	d91e6 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d9280:	3501      	adds	r5, #1
   d9282:	e7a8      	b.n	d91d6 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d9284:	3401      	adds	r4, #1
   d9286:	e79f      	b.n	d91c8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d9288:	a80b      	add	r0, sp, #44	; 0x2c
   d928a:	f7fd f8e2 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   d928e:	b021      	add	sp, #132	; 0x84
   d9290:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d9294 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode>:
              op_params, GetTensorShape(input1), GetTensorData<type>(input1), \
              GetTensorShape(input2), GetTensorData<type>(input2),            \
              GetTensorShape(output), GetTensorData<bool>(output));           \
  }

TfLiteStatus EqualEval(TfLiteContext* context, TfLiteNode* node) {
   d9294:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d9298:	680a      	ldr	r2, [r1, #0]
   d929a:	f8d0 9008 	ldr.w	r9, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d929e:	6895      	ldr	r5, [r2, #8]
   d92a0:	4682      	mov	sl, r0
   d92a2:	6850      	ldr	r0, [r2, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d92a4:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d92a6:	2338      	movs	r3, #56	; 0x38
   d92a8:	fb03 f800 	mul.w	r8, r3, r0
   d92ac:	fb03 9505 	mla	r5, r3, r5, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d92b0:	6854      	ldr	r4, [r2, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d92b2:	eb09 0608 	add.w	r6, r9, r8
   d92b6:	b0ab      	sub	sp, #172	; 0xac
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   d92b8:	4629      	mov	r1, r5
   d92ba:	4630      	mov	r0, r6
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d92bc:	fb03 9404 	mla	r4, r3, r4, r9
   d92c0:	f00a fd5e 	bl	e3d80 <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
  switch (input1->type) {
   d92c4:	f819 2008 	ldrb.w	r2, [r9, r8]

TfLiteStatus EqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   d92c8:	f080 0001 	eor.w	r0, r0, #1
  switch (input1->type) {
   d92cc:	1e53      	subs	r3, r2, #1

TfLiteStatus EqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   d92ce:	b2c7      	uxtb	r7, r0
  switch (input1->type) {
   d92d0:	2b08      	cmp	r3, #8
   d92d2:	f200 8274 	bhi.w	d97be <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x52a>
   d92d6:	e8df f013 	tbh	[pc, r3, lsl #1]
   d92da:	0057      	.short	0x0057
   d92dc:	013f00a5 	.word	0x013f00a5
   d92e0:	027200f1 	.word	0x027200f1
   d92e4:	02720009 	.word	0x02720009
   d92e8:	01d30272 	.word	0x01d30272
   d92ec:	f10d 0874 	add.w	r8, sp, #116	; 0x74
    case kTfLiteBool:
      TF_LITE_COMPARISON(bool, Equal, requires_broadcast);
   d92f0:	4631      	mov	r1, r6
   d92f2:	b1cf      	cbz	r7, d9328 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x94>
   d92f4:	a813      	add	r0, sp, #76	; 0x4c
   d92f6:	f7fd fb5c 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d92fa:	4629      	mov	r1, r5
   d92fc:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d92fe:	6876      	ldr	r6, [r6, #4]
   d9300:	f7fd fb57 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9304:	b105      	cbz	r5, d9308 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x74>
   d9306:	686d      	ldr	r5, [r5, #4]
   d9308:	4621      	mov	r1, r4
   d930a:	4640      	mov	r0, r8
   d930c:	f7fd fb51 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9310:	b104      	cbz	r4, d9314 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x80>
   d9312:	6864      	ldr	r4, [r4, #4]
      bool* output_data) {                                                     \
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
   d9314:	9402      	str	r4, [sp, #8]
   d9316:	e88d 0120 	stmia.w	sp, {r5, r8}
   d931a:	ab18      	add	r3, sp, #96	; 0x60
   d931c:	4632      	mov	r2, r6
   d931e:	a913      	add	r1, sp, #76	; 0x4c
   d9320:	a822      	add	r0, sp, #136	; 0x88
   d9322:	f7fe fdd7 	bl	d7ed4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_7EqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d9326:	e1ed      	b.n	d9704 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x470>
   d9328:	a818      	add	r0, sp, #96	; 0x60
   d932a:	f7fd fb42 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d932e:	4629      	mov	r1, r5
   d9330:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9332:	6876      	ldr	r6, [r6, #4]
   d9334:	f7fd fb3d 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9338:	b105      	cbz	r5, d933c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0xa8>
   d933a:	686d      	ldr	r5, [r5, #4]
   d933c:	4621      	mov	r1, r4
   d933e:	a822      	add	r0, sp, #136	; 0x88
   d9340:	f7fd fb37 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9344:	b104      	cbz	r4, d9348 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0xb4>
   d9346:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9348:	4641      	mov	r1, r8
   d934a:	aa22      	add	r2, sp, #136	; 0x88
   d934c:	a818      	add	r0, sp, #96	; 0x60
   d934e:	f7fd f912 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d9352:	3d01      	subs	r5, #1
   d9354:	1e73      	subs	r3, r6, #1
   d9356:	17c1      	asrs	r1, r0, #31
   d9358:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
   d935a:	2600      	movs	r6, #0
   d935c:	2700      	movs	r7, #0
   d935e:	4286      	cmp	r6, r0
   d9360:	eb77 0201 	sbcs.w	r2, r7, r1
   d9364:	f280 8232 	bge.w	d97cc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x538>
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9368:	f813 2f01 	ldrb.w	r2, [r3, #1]!
   d936c:	f815 ef01 	ldrb.w	lr, [r5, #1]!
   d9370:	ebce 0c02 	rsb	ip, lr, r2
   d9374:	f1dc 0200 	rsbs	r2, ip, #0
   d9378:	eb42 020c 	adc.w	r2, r2, ip
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d937c:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   d937e:	f804 2f01 	strb.w	r2, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9382:	f147 0700 	adc.w	r7, r7, #0
   d9386:	e7ea      	b.n	d935e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0xca>
   d9388:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteFloat32:
      TF_LITE_COMPARISON(float, Equal, requires_broadcast);
   d938c:	4631      	mov	r1, r6
   d938e:	b1cf      	cbz	r7, d93c4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x130>
   d9390:	a813      	add	r0, sp, #76	; 0x4c
   d9392:	f7fd fb0e 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9396:	4629      	mov	r1, r5
   d9398:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d939a:	6876      	ldr	r6, [r6, #4]
   d939c:	f7fd fb09 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d93a0:	b105      	cbz	r5, d93a4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x110>
   d93a2:	686d      	ldr	r5, [r5, #4]
   d93a4:	4621      	mov	r1, r4
   d93a6:	4640      	mov	r0, r8
   d93a8:	f7fd fb03 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d93ac:	b104      	cbz	r4, d93b0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x11c>
   d93ae:	6864      	ldr	r4, [r4, #4]
      bool* output_data) {                                                     \
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
   d93b0:	9402      	str	r4, [sp, #8]
   d93b2:	e88d 0120 	stmia.w	sp, {r5, r8}
   d93b6:	ab18      	add	r3, sp, #96	; 0x60
   d93b8:	4632      	mov	r2, r6
   d93ba:	a913      	add	r1, sp, #76	; 0x4c
   d93bc:	a822      	add	r0, sp, #136	; 0x88
   d93be:	f7fe fdf3 	bl	d7fa8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_7EqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d93c2:	e19f      	b.n	d9704 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x470>
   d93c4:	a818      	add	r0, sp, #96	; 0x60
   d93c6:	f7fd faf4 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d93ca:	4629      	mov	r1, r5
   d93cc:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d93ce:	6876      	ldr	r6, [r6, #4]
   d93d0:	f7fd faef 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d93d4:	b105      	cbz	r5, d93d8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x144>
   d93d6:	686d      	ldr	r5, [r5, #4]
   d93d8:	4621      	mov	r1, r4
   d93da:	a822      	add	r0, sp, #136	; 0x88
   d93dc:	f7fd fae9 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d93e0:	b104      	cbz	r4, d93e4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x150>
   d93e2:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d93e4:	4641      	mov	r1, r8
   d93e6:	aa22      	add	r2, sp, #136	; 0x88
   d93e8:	a818      	add	r0, sp, #96	; 0x60
   d93ea:	f7fd f8c4 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d93ee:	3c01      	subs	r4, #1
   d93f0:	4633      	mov	r3, r6
   d93f2:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   d93f4:	2600      	movs	r6, #0
   d93f6:	2700      	movs	r7, #0
   d93f8:	4286      	cmp	r6, r0
   d93fa:	eb77 0201 	sbcs.w	r2, r7, r1
   d93fe:	f280 81e5 	bge.w	d97cc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x538>
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9402:	ecb3 7a01 	vldmia	r3!, {s14}
   d9406:	ecf5 7a01 	vldmia	r5!, {s15}
   d940a:	eeb4 7a67 	vcmp.f32	s14, s15
   d940e:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d9412:	bf0c      	ite	eq
   d9414:	2201      	moveq	r2, #1
   d9416:	2200      	movne	r2, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9418:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   d941a:	f804 2f01 	strb.w	r2, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d941e:	f147 0700 	adc.w	r7, r7, #0
   d9422:	e7e9      	b.n	d93f8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x164>
   d9424:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, Equal, requires_broadcast);
   d9428:	4631      	mov	r1, r6
   d942a:	b1cf      	cbz	r7, d9460 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x1cc>
   d942c:	a813      	add	r0, sp, #76	; 0x4c
   d942e:	f7fd fac0 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9432:	4629      	mov	r1, r5
   d9434:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9436:	6876      	ldr	r6, [r6, #4]
   d9438:	f7fd fabb 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d943c:	b105      	cbz	r5, d9440 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x1ac>
   d943e:	686d      	ldr	r5, [r5, #4]
   d9440:	4621      	mov	r1, r4
   d9442:	4640      	mov	r0, r8
   d9444:	f7fd fab5 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9448:	b104      	cbz	r4, d944c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x1b8>
   d944a:	6864      	ldr	r4, [r4, #4]
      bool* output_data) {                                                     \
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
   d944c:	9402      	str	r4, [sp, #8]
   d944e:	e88d 0120 	stmia.w	sp, {r5, r8}
   d9452:	ab18      	add	r3, sp, #96	; 0x60
   d9454:	4632      	mov	r2, r6
   d9456:	a913      	add	r1, sp, #76	; 0x4c
   d9458:	a822      	add	r0, sp, #136	; 0x88
   d945a:	f7fe fe18 	bl	d808e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d945e:	e151      	b.n	d9704 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x470>
   d9460:	a818      	add	r0, sp, #96	; 0x60
   d9462:	f7fd faa6 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9466:	4629      	mov	r1, r5
   d9468:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d946a:	6876      	ldr	r6, [r6, #4]
   d946c:	f7fd faa1 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9470:	b105      	cbz	r5, d9474 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x1e0>
   d9472:	686d      	ldr	r5, [r5, #4]
   d9474:	4621      	mov	r1, r4
   d9476:	a822      	add	r0, sp, #136	; 0x88
   d9478:	f7fd fa9b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d947c:	b104      	cbz	r4, d9480 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x1ec>
   d947e:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9480:	aa22      	add	r2, sp, #136	; 0x88
   d9482:	4641      	mov	r1, r8
   d9484:	a818      	add	r0, sp, #96	; 0x60
   d9486:	f7fd f876 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d948a:	3c01      	subs	r4, #1
   d948c:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   d948e:	2200      	movs	r2, #0
   d9490:	2300      	movs	r3, #0
   d9492:	4282      	cmp	r2, r0
   d9494:	eb73 0701 	sbcs.w	r7, r3, r1
   d9498:	f280 8198 	bge.w	d97cc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x538>
    output_data[i] = F(input1_data[i], input2_data[i]);
   d949c:	f856 7022 	ldr.w	r7, [r6, r2, lsl #2]
   d94a0:	f855 e022 	ldr.w	lr, [r5, r2, lsl #2]
   d94a4:	ebce 0e07 	rsb	lr, lr, r7
   d94a8:	f1de 0700 	rsbs	r7, lr, #0
   d94ac:	eb47 070e 	adc.w	r7, r7, lr
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d94b0:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   d94b2:	f804 7f01 	strb.w	r7, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d94b6:	f143 0300 	adc.w	r3, r3, #0
   d94ba:	e7ea      	b.n	d9492 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x1fe>
   d94bc:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, Equal, requires_broadcast);
   d94c0:	4631      	mov	r1, r6
   d94c2:	b1cf      	cbz	r7, d94f8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x264>
   d94c4:	a813      	add	r0, sp, #76	; 0x4c
   d94c6:	f7fd fa74 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d94ca:	4629      	mov	r1, r5
   d94cc:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d94ce:	6876      	ldr	r6, [r6, #4]
   d94d0:	f7fd fa6f 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d94d4:	b105      	cbz	r5, d94d8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x244>
   d94d6:	686d      	ldr	r5, [r5, #4]
   d94d8:	4621      	mov	r1, r4
   d94da:	4640      	mov	r0, r8
   d94dc:	f7fd fa69 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d94e0:	b104      	cbz	r4, d94e4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x250>
   d94e2:	6864      	ldr	r4, [r4, #4]
      bool* output_data) {                                                     \
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
   d94e4:	9402      	str	r4, [sp, #8]
   d94e6:	e88d 0120 	stmia.w	sp, {r5, r8}
   d94ea:	ab18      	add	r3, sp, #96	; 0x60
   d94ec:	4632      	mov	r2, r6
   d94ee:	a913      	add	r1, sp, #76	; 0x4c
   d94f0:	a822      	add	r0, sp, #136	; 0x88
   d94f2:	f7fe fe37 	bl	d8164 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_7EqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d94f6:	e105      	b.n	d9704 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x470>
   d94f8:	a818      	add	r0, sp, #96	; 0x60
   d94fa:	f7fd fa5a 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d94fe:	4629      	mov	r1, r5
   d9500:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9502:	6876      	ldr	r6, [r6, #4]
   d9504:	f7fd fa55 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9508:	b105      	cbz	r5, d950c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x278>
   d950a:	686d      	ldr	r5, [r5, #4]
   d950c:	4621      	mov	r1, r4
   d950e:	a822      	add	r0, sp, #136	; 0x88
   d9510:	f7fd fa4f 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9514:	b104      	cbz	r4, d9518 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x284>
   d9516:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9518:	aa22      	add	r2, sp, #136	; 0x88
   d951a:	4641      	mov	r1, r8
   d951c:	a818      	add	r0, sp, #96	; 0x60
   d951e:	f7fd f82a 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d9522:	3d08      	subs	r5, #8
   d9524:	17c1      	asrs	r1, r0, #31
   d9526:	f1a6 0e08 	sub.w	lr, r6, #8
   d952a:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
   d952c:	2200      	movs	r2, #0
   d952e:	2300      	movs	r3, #0
   d9530:	4282      	cmp	r2, r0
   d9532:	eb73 0601 	sbcs.w	r6, r3, r1
   d9536:	f280 8149 	bge.w	d97cc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x538>
    output_data[i] = F(input1_data[i], input2_data[i]);
   d953a:	e9f5 6702 	ldrd	r6, r7, [r5, #8]!
   d953e:	e9fe ab02 	ldrd	sl, fp, [lr, #8]!
   d9542:	45bb      	cmp	fp, r7
   d9544:	bf06      	itte	eq
   d9546:	45b2      	cmpeq	sl, r6
   d9548:	2601      	moveq	r6, #1
   d954a:	2600      	movne	r6, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d954c:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   d954e:	f804 6f01 	strb.w	r6, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9552:	f143 0300 	adc.w	r3, r3, #0
   d9556:	e7eb      	b.n	d9530 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x29c>
            GetTensorData<input_dtype>(input2), GetTensorShape(output),        \
            GetTensorData<bool>(output));                                      \
      }                                                                        \
    }                                                                          \
  }
TF_LITE_QUANTIZE_COMPARISON(Equal);
   d9558:	6933      	ldr	r3, [r6, #16]
   d955a:	68f0      	ldr	r0, [r6, #12]
   d955c:	f1c3 0900 	rsb	r9, r3, #0
   d9560:	692b      	ldr	r3, [r5, #16]
   d9562:	f1c3 0800 	rsb	r8, r3, #0
   d9566:	f00d fd31 	bl	e6fcc <__aeabi_f2d>
   d956a:	ec41 0b10 	vmov	d0, r0, r1
   d956e:	a910      	add	r1, sp, #64	; 0x40
   d9570:	a80f      	add	r0, sp, #60	; 0x3c
   d9572:	f00a fc79 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   d9576:	68e8      	ldr	r0, [r5, #12]
   d9578:	f00d fd28 	bl	e6fcc <__aeabi_f2d>
   d957c:	ec41 0b10 	vmov	d0, r0, r1
   d9580:	a912      	add	r1, sp, #72	; 0x48
   d9582:	a811      	add	r0, sp, #68	; 0x44
   d9584:	f00a fc70 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   d9588:	2308      	movs	r3, #8
   d958a:	9322      	str	r3, [sp, #136]	; 0x88
   d958c:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   d958e:	9324      	str	r3, [sp, #144]	; 0x90
   d9590:	9b10      	ldr	r3, [sp, #64]	; 0x40
   d9592:	9325      	str	r3, [sp, #148]	; 0x94
   d9594:	9b11      	ldr	r3, [sp, #68]	; 0x44
   d9596:	9327      	str	r3, [sp, #156]	; 0x9c
   d9598:	9b12      	ldr	r3, [sp, #72]	; 0x48
   d959a:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   d959e:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   d95a2:	9328      	str	r3, [sp, #160]	; 0xa0
   d95a4:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   d95a8:	4631      	mov	r1, r6
   d95aa:	a813      	add	r0, sp, #76	; 0x4c
   d95ac:	b1bf      	cbz	r7, d95de <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x34a>
   d95ae:	f7fd fa00 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d95b2:	4629      	mov	r1, r5
   d95b4:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d95b6:	6876      	ldr	r6, [r6, #4]
   d95b8:	f7fd f9fb 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d95bc:	4621      	mov	r1, r4
   d95be:	4640      	mov	r0, r8
   d95c0:	686d      	ldr	r5, [r5, #4]
   d95c2:	f7fd f9f6 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d95c6:	b104      	cbz	r4, d95ca <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x336>
   d95c8:	6864      	ldr	r4, [r4, #4]
      bool* output_data) {                                                     \
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
   d95ca:	9402      	str	r4, [sp, #8]
   d95cc:	e88d 0120 	stmia.w	sp, {r5, r8}
   d95d0:	ab18      	add	r3, sp, #96	; 0x60
   d95d2:	4632      	mov	r2, r6
   d95d4:	a913      	add	r1, sp, #76	; 0x4c
   d95d6:	a822      	add	r0, sp, #136	; 0x88
   d95d8:	f7ff fd32 	bl	d9040 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d95dc:	e092      	b.n	d9704 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x470>
   d95de:	f7fd f9e8 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d95e2:	6873      	ldr	r3, [r6, #4]
   d95e4:	9305      	str	r3, [sp, #20]
   d95e6:	4629      	mov	r1, r5
   d95e8:	a818      	add	r0, sp, #96	; 0x60
   d95ea:	f7fd f9e2 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d95ee:	4621      	mov	r1, r4
   d95f0:	4640      	mov	r0, r8
   d95f2:	f8d5 b004 	ldr.w	fp, [r5, #4]
   d95f6:	f7fd f9dc 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d95fa:	b104      	cbz	r4, d95fe <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x36a>
   d95fc:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   d95fe:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   d9600:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   d9602:	9b24      	ldr	r3, [sp, #144]	; 0x90
   d9604:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   d9606:	9b25      	ldr	r3, [sp, #148]	; 0x94
   d9608:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   d960a:	9b26      	ldr	r3, [sp, #152]	; 0x98
   d960c:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   d960e:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   d9610:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9612:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   d9614:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   d9616:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9618:	a918      	add	r1, sp, #96	; 0x60
   d961a:	a813      	add	r0, sp, #76	; 0x4c
   d961c:	f7fc ffab 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d9620:	4602      	mov	r2, r0
   d9622:	17c3      	asrs	r3, r0, #31
   d9624:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   d9628:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d962a:	f04f 0800 	mov.w	r8, #0
   d962e:	f04f 0900 	mov.w	r9, #0
   d9632:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   d9636:	4590      	cmp	r8, r2
   d9638:	eb79 0303 	sbcs.w	r3, r9, r3
   d963c:	f280 80b4 	bge.w	d97a8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x514>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9640:	f81b 5008 	ldrb.w	r5, [fp, r8]
   d9644:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d9646:	9a08      	ldr	r2, [sp, #32]
   d9648:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d964a:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d964c:	9b05      	ldr	r3, [sp, #20]
   d964e:	f813 0008 	ldrb.w	r0, [r3, r8]
   d9652:	9b06      	ldr	r3, [sp, #24]
   d9654:	4418      	add	r0, r3
   d9656:	40b8      	lsls	r0, r7
   d9658:	f7fc ffca 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d965c:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d965e:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   d9660:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   d9662:	990a      	ldr	r1, [sp, #40]	; 0x28
   d9664:	4628      	mov	r0, r5
   d9666:	f7fc ffc3 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   d966a:	ebc0 020a 	rsb	r2, r0, sl
   d966e:	4250      	negs	r0, r2
   d9670:	4150      	adcs	r0, r2
   d9672:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9676:	f118 0801 	adds.w	r8, r8, #1
   d967a:	f149 0900 	adc.w	r9, r9, #0
   d967e:	e7d8      	b.n	d9632 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x39e>
   d9680:	6933      	ldr	r3, [r6, #16]
   d9682:	68f0      	ldr	r0, [r6, #12]
   d9684:	f1c3 0900 	rsb	r9, r3, #0
   d9688:	692b      	ldr	r3, [r5, #16]
   d968a:	f1c3 0800 	rsb	r8, r3, #0
   d968e:	f00d fc9d 	bl	e6fcc <__aeabi_f2d>
   d9692:	ec41 0b10 	vmov	d0, r0, r1
   d9696:	a910      	add	r1, sp, #64	; 0x40
   d9698:	a80f      	add	r0, sp, #60	; 0x3c
   d969a:	f00a fbe5 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   d969e:	68e8      	ldr	r0, [r5, #12]
   d96a0:	f00d fc94 	bl	e6fcc <__aeabi_f2d>
   d96a4:	ec41 0b10 	vmov	d0, r0, r1
   d96a8:	a912      	add	r1, sp, #72	; 0x48
   d96aa:	a811      	add	r0, sp, #68	; 0x44
   d96ac:	f00a fbdc 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   d96b0:	2308      	movs	r3, #8
   d96b2:	9322      	str	r3, [sp, #136]	; 0x88
   d96b4:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   d96b6:	9324      	str	r3, [sp, #144]	; 0x90
   d96b8:	9b10      	ldr	r3, [sp, #64]	; 0x40
   d96ba:	9325      	str	r3, [sp, #148]	; 0x94
   d96bc:	9b11      	ldr	r3, [sp, #68]	; 0x44
   d96be:	9327      	str	r3, [sp, #156]	; 0x9c
   d96c0:	9b12      	ldr	r3, [sp, #72]	; 0x48
   d96c2:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   d96c6:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   d96ca:	9328      	str	r3, [sp, #160]	; 0xa0
   d96cc:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   d96d0:	4631      	mov	r1, r6
   d96d2:	a813      	add	r0, sp, #76	; 0x4c
   d96d4:	b1c7      	cbz	r7, d9708 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x474>
   d96d6:	f7fd f96c 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d96da:	4629      	mov	r1, r5
   d96dc:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d96de:	6876      	ldr	r6, [r6, #4]
   d96e0:	f7fd f967 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d96e4:	4621      	mov	r1, r4
   d96e6:	4640      	mov	r0, r8
   d96e8:	686d      	ldr	r5, [r5, #4]
   d96ea:	f7fd f962 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d96ee:	b104      	cbz	r4, d96f2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x45e>
   d96f0:	6864      	ldr	r4, [r4, #4]
      bool* output_data) {                                                     \
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
   d96f2:	9402      	str	r4, [sp, #8]
   d96f4:	e88d 0120 	stmia.w	sp, {r5, r8}
   d96f8:	ab18      	add	r3, sp, #96	; 0x60
   d96fa:	4632      	mov	r2, r6
   d96fc:	a913      	add	r1, sp, #76	; 0x4c
   d96fe:	a822      	add	r0, sp, #136	; 0x88
   d9700:	f7ff fd33 	bl	d916a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_7EqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d9704:	4640      	mov	r0, r8
   d9706:	e050      	b.n	d97aa <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x516>
   d9708:	f7fd f953 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d970c:	6873      	ldr	r3, [r6, #4]
   d970e:	9305      	str	r3, [sp, #20]
   d9710:	4629      	mov	r1, r5
   d9712:	a818      	add	r0, sp, #96	; 0x60
   d9714:	f7fd f94d 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9718:	4621      	mov	r1, r4
   d971a:	4640      	mov	r0, r8
   d971c:	f8d5 b004 	ldr.w	fp, [r5, #4]
   d9720:	f7fd f947 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9724:	b104      	cbz	r4, d9728 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x494>
   d9726:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   d9728:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   d972a:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   d972c:	9b24      	ldr	r3, [sp, #144]	; 0x90
   d972e:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   d9730:	9b25      	ldr	r3, [sp, #148]	; 0x94
   d9732:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   d9734:	9b26      	ldr	r3, [sp, #152]	; 0x98
   d9736:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   d9738:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   d973a:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d973c:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   d973e:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   d9740:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9742:	a918      	add	r1, sp, #96	; 0x60
   d9744:	a813      	add	r0, sp, #76	; 0x4c
   d9746:	f7fc ff16 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d974a:	4602      	mov	r2, r0
   d974c:	17c3      	asrs	r3, r0, #31
   d974e:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   d9752:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9754:	f04f 0800 	mov.w	r8, #0
   d9758:	f04f 0900 	mov.w	r9, #0
   d975c:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   d9760:	4590      	cmp	r8, r2
   d9762:	eb79 0303 	sbcs.w	r3, r9, r3
   d9766:	da1f      	bge.n	d97a8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x514>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9768:	f91b 5008 	ldrsb.w	r5, [fp, r8]
   d976c:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d976e:	9a08      	ldr	r2, [sp, #32]
   d9770:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9772:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d9774:	9b05      	ldr	r3, [sp, #20]
   d9776:	f913 0008 	ldrsb.w	r0, [r3, r8]
   d977a:	9b06      	ldr	r3, [sp, #24]
   d977c:	4418      	add	r0, r3
   d977e:	40b8      	lsls	r0, r7
   d9780:	f7fc ff36 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9784:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d9786:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   d9788:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   d978a:	990a      	ldr	r1, [sp, #40]	; 0x28
   d978c:	4628      	mov	r0, r5
   d978e:	f7fc ff2f 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   d9792:	ebc0 030a 	rsb	r3, r0, sl
   d9796:	4258      	negs	r0, r3
   d9798:	4158      	adcs	r0, r3
   d979a:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d979e:	f118 0801 	adds.w	r8, r8, #1
   d97a2:	f149 0900 	adc.w	r9, r9, #0
   d97a6:	e7d9      	b.n	d975c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x4c8>
   d97a8:	a81d      	add	r0, sp, #116	; 0x74
   d97aa:	f7fc fe52 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   d97ae:	a818      	add	r0, sp, #96	; 0x60
   d97b0:	f7fc fe4f 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   d97b4:	a813      	add	r0, sp, #76	; 0x4c
   d97b6:	f7fc fe4c 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(
          context, "Does not support type %d, requires bool|float|int|uint8",
          input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
   d97ba:	2000      	movs	r0, #0
   d97bc:	e00e      	b.n	d97dc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x548>
                                 requires_broadcast);
      break;
    default:
      context->ReportError(
          context, "Does not support type %d, requires bool|float|int|uint8",
          input1->type);
   d97be:	4650      	mov	r0, sl
   d97c0:	f8da 3014 	ldr.w	r3, [sl, #20]
   d97c4:	4907      	ldr	r1, [pc, #28]	; (d97e4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x550>)
   d97c6:	4798      	blx	r3
      return kTfLiteError;
   d97c8:	2001      	movs	r0, #1
   d97ca:	e007      	b.n	d97dc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x548>
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, Equal, requires_broadcast);
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, Equal, requires_broadcast);
   d97cc:	a822      	add	r0, sp, #136	; 0x88
   d97ce:	f7fc fe40 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   d97d2:	4640      	mov	r0, r8
   d97d4:	f7fc fe3d 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   d97d8:	a818      	add	r0, sp, #96	; 0x60
   d97da:	e7ec      	b.n	d97b6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_19EqualEvalEP13TfLiteContextP10TfLiteNode+0x522>
          context, "Does not support type %d, requires bool|float|int|uint8",
          input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   d97dc:	b02b      	add	sp, #172	; 0xac
   d97de:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   d97e2:	bf00      	nop
   d97e4:	000e9987 	.word	0x000e9987

000d97e8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d97e8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d97ec:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d97ee:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d97f0:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d97f2:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d97f4:	9208      	str	r2, [sp, #32]
   d97f6:	4604      	mov	r4, r0
   d97f8:	460e      	mov	r6, r1
   d97fa:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d97fc:	dd01      	ble.n	d9802 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   d97fe:	f00a fda5 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d9802:	683b      	ldr	r3, [r7, #0]
   d9804:	2b04      	cmp	r3, #4
   d9806:	dcfa      	bgt.n	d97fe <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d9808:	6813      	ldr	r3, [r2, #0]
   d980a:	2b04      	cmp	r3, #4
   d980c:	dcf7      	bgt.n	d97fe <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   d980e:	2301      	movs	r3, #1
   d9810:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d9812:	ad10      	add	r5, sp, #64	; 0x40
   d9814:	a80b      	add	r0, sp, #44	; 0x2c
   d9816:	f7fc fe60 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d981a:	ab18      	add	r3, sp, #96	; 0x60
   d981c:	462a      	mov	r2, r5
   d981e:	4639      	mov	r1, r7
   d9820:	4630      	mov	r0, r6
   d9822:	f7fd f96f 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   d9826:	6863      	ldr	r3, [r4, #4]
   d9828:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   d982a:	68a3      	ldr	r3, [r4, #8]
   d982c:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   d982e:	68e3      	ldr	r3, [r4, #12]
   d9830:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   d9832:	6923      	ldr	r3, [r4, #16]
   d9834:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   d9836:	6963      	ldr	r3, [r4, #20]
   d9838:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   d983a:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   d983c:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   d9840:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d9842:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d9844:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d9846:	2100      	movs	r1, #0
   d9848:	a80b      	add	r0, sp, #44	; 0x2c
   d984a:	f7fc fe0d 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d984e:	4284      	cmp	r4, r0
   d9850:	da58      	bge.n	d9904 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11c>
   d9852:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d9854:	af0b      	add	r7, sp, #44	; 0x2c
   d9856:	2101      	movs	r1, #1
   d9858:	4638      	mov	r0, r7
   d985a:	f7fc fe05 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d985e:	4285      	cmp	r5, r0
   d9860:	da4e      	bge.n	d9900 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x118>
   d9862:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d9864:	2102      	movs	r1, #2
   d9866:	4638      	mov	r0, r7
   d9868:	f7fc fdfe 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d986c:	4286      	cmp	r6, r0
   d986e:	da45      	bge.n	d98fc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x114>
   d9870:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d9874:	2103      	movs	r1, #3
   d9876:	4638      	mov	r0, r7
   d9878:	f7fc fdf6 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d987c:	4580      	cmp	r8, r0
   d987e:	da3b      	bge.n	d98f8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x110>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d9880:	f8cd 8000 	str.w	r8, [sp]
   d9884:	4633      	mov	r3, r6
   d9886:	462a      	mov	r2, r5
   d9888:	4621      	mov	r1, r4
   d988a:	9809      	ldr	r0, [sp, #36]	; 0x24
   d988c:	f7fc ff02 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   d9890:	9b08      	ldr	r3, [sp, #32]
   d9892:	f813 9000 	ldrb.w	r9, [r3, r0]
   d9896:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d9898:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d989c:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d989e:	462a      	mov	r2, r5
   d98a0:	4633      	mov	r3, r6
   d98a2:	4621      	mov	r1, r4
   d98a4:	a818      	add	r0, sp, #96	; 0x60
   d98a6:	f7fc fef5 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d98aa:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d98ac:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d98ae:	f813 b000 	ldrb.w	fp, [r3, r0]
   d98b2:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d98b4:	9903      	ldr	r1, [sp, #12]
   d98b6:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d98ba:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d98bc:	f7fc fe98 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d98c0:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d98c4:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   d98c6:	9a07      	ldr	r2, [sp, #28]
   d98c8:	9906      	ldr	r1, [sp, #24]
   d98ca:	4658      	mov	r0, fp
   d98cc:	f7fc fe90 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   d98d0:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   d98d4:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   d98d6:	4633      	mov	r3, r6
   d98d8:	462a      	mov	r2, r5
   d98da:	4621      	mov	r1, r4
   d98dc:	4638      	mov	r0, r7
   d98de:	f7fc fe28 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   d98e2:	ebb9 090b 	subs.w	r9, r9, fp
   d98e6:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   d98e8:	bf18      	it	ne
   d98ea:	f04f 0901 	movne.w	r9, #1
   d98ee:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d98f2:	f108 0801 	add.w	r8, r8, #1
   d98f6:	e7bd      	b.n	d9874 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d98f8:	3601      	adds	r6, #1
   d98fa:	e7b3      	b.n	d9864 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d98fc:	3501      	adds	r5, #1
   d98fe:	e7a9      	b.n	d9854 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d9900:	3401      	adds	r4, #1
   d9902:	e7a0      	b.n	d9846 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d9904:	a80b      	add	r0, sp, #44	; 0x2c
   d9906:	f7fc fda4 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   d990a:	b021      	add	sp, #132	; 0x84
   d990c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d9910 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d9910:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d9914:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d9916:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d9918:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d991a:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d991c:	9208      	str	r2, [sp, #32]
   d991e:	4604      	mov	r4, r0
   d9920:	460e      	mov	r6, r1
   d9922:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d9924:	dd01      	ble.n	d992a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   d9926:	f00a fd11 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d992a:	683b      	ldr	r3, [r7, #0]
   d992c:	2b04      	cmp	r3, #4
   d992e:	dcfa      	bgt.n	d9926 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d9930:	6813      	ldr	r3, [r2, #0]
   d9932:	2b04      	cmp	r3, #4
   d9934:	dcf7      	bgt.n	d9926 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   d9936:	2301      	movs	r3, #1
   d9938:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d993a:	ad10      	add	r5, sp, #64	; 0x40
   d993c:	a80b      	add	r0, sp, #44	; 0x2c
   d993e:	f7fc fdcc 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d9942:	ab18      	add	r3, sp, #96	; 0x60
   d9944:	462a      	mov	r2, r5
   d9946:	4639      	mov	r1, r7
   d9948:	4630      	mov	r0, r6
   d994a:	f7fd f8db 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   d994e:	6863      	ldr	r3, [r4, #4]
   d9950:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   d9952:	68a3      	ldr	r3, [r4, #8]
   d9954:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   d9956:	68e3      	ldr	r3, [r4, #12]
   d9958:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   d995a:	6923      	ldr	r3, [r4, #16]
   d995c:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   d995e:	6963      	ldr	r3, [r4, #20]
   d9960:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   d9962:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   d9964:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   d9968:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d996a:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d996c:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d996e:	2100      	movs	r1, #0
   d9970:	a80b      	add	r0, sp, #44	; 0x2c
   d9972:	f7fc fd79 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d9976:	4284      	cmp	r4, r0
   d9978:	da58      	bge.n	d9a2c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11c>
   d997a:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d997c:	af0b      	add	r7, sp, #44	; 0x2c
   d997e:	2101      	movs	r1, #1
   d9980:	4638      	mov	r0, r7
   d9982:	f7fc fd71 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d9986:	4285      	cmp	r5, r0
   d9988:	da4e      	bge.n	d9a28 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x118>
   d998a:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d998c:	2102      	movs	r1, #2
   d998e:	4638      	mov	r0, r7
   d9990:	f7fc fd6a 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d9994:	4286      	cmp	r6, r0
   d9996:	da45      	bge.n	d9a24 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x114>
   d9998:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d999c:	2103      	movs	r1, #3
   d999e:	4638      	mov	r0, r7
   d99a0:	f7fc fd62 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d99a4:	4580      	cmp	r8, r0
   d99a6:	da3b      	bge.n	d9a20 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x110>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d99a8:	f8cd 8000 	str.w	r8, [sp]
   d99ac:	4633      	mov	r3, r6
   d99ae:	462a      	mov	r2, r5
   d99b0:	4621      	mov	r1, r4
   d99b2:	9809      	ldr	r0, [sp, #36]	; 0x24
   d99b4:	f7fc fe6e 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   d99b8:	9b08      	ldr	r3, [sp, #32]
   d99ba:	f913 9000 	ldrsb.w	r9, [r3, r0]
   d99be:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d99c0:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d99c4:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   d99c6:	462a      	mov	r2, r5
   d99c8:	4633      	mov	r3, r6
   d99ca:	4621      	mov	r1, r4
   d99cc:	a818      	add	r0, sp, #96	; 0x60
   d99ce:	f7fc fe61 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d99d2:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d99d4:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d99d6:	f913 b000 	ldrsb.w	fp, [r3, r0]
   d99da:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d99dc:	9903      	ldr	r1, [sp, #12]
   d99de:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d99e2:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d99e4:	f7fc fe04 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d99e8:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   d99ec:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   d99ee:	9a07      	ldr	r2, [sp, #28]
   d99f0:	9906      	ldr	r1, [sp, #24]
   d99f2:	4658      	mov	r0, fp
   d99f4:	f7fc fdfc 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   d99f8:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   d99fc:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   d99fe:	4633      	mov	r3, r6
   d9a00:	462a      	mov	r2, r5
   d9a02:	4621      	mov	r1, r4
   d9a04:	4638      	mov	r0, r7
   d9a06:	f7fc fd94 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   d9a0a:	ebb9 090b 	subs.w	r9, r9, fp
   d9a0e:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   d9a10:	bf18      	it	ne
   d9a12:	f04f 0901 	movne.w	r9, #1
   d9a16:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   d9a1a:	f108 0801 	add.w	r8, r8, #1
   d9a1e:	e7bd      	b.n	d999c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d9a20:	3601      	adds	r6, #1
   d9a22:	e7b3      	b.n	d998c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d9a24:	3501      	adds	r5, #1
   d9a26:	e7a9      	b.n	d997c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d9a28:	3401      	adds	r4, #1
   d9a2a:	e7a0      	b.n	d996e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   d9a2c:	a80b      	add	r0, sp, #44	; 0x2c
   d9a2e:	f7fc fd10 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   d9a32:	b021      	add	sp, #132	; 0x84
   d9a34:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000d9a38 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode>:

// TODO(renjieliu): Refactor the logic to avoid duplications.
TfLiteStatus NotEqualEval(TfLiteContext* context, TfLiteNode* node) {
   d9a38:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d9a3c:	680a      	ldr	r2, [r1, #0]
   d9a3e:	f8d0 9008 	ldr.w	r9, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d9a42:	6895      	ldr	r5, [r2, #8]
   d9a44:	4682      	mov	sl, r0
   d9a46:	6850      	ldr	r0, [r2, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d9a48:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d9a4a:	2338      	movs	r3, #56	; 0x38
   d9a4c:	fb03 f800 	mul.w	r8, r3, r0
   d9a50:	fb03 9505 	mla	r5, r3, r5, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d9a54:	6854      	ldr	r4, [r2, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   d9a56:	eb09 0608 	add.w	r6, r9, r8
   d9a5a:	b0ab      	sub	sp, #172	; 0xac
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   d9a5c:	4629      	mov	r1, r5
   d9a5e:	4630      	mov	r0, r6
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   d9a60:	fb03 9404 	mla	r4, r3, r4, r9
   d9a64:	f00a f98c 	bl	e3d80 <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
  switch (input1->type) {
   d9a68:	f819 2008 	ldrb.w	r2, [r9, r8]
// TODO(renjieliu): Refactor the logic to avoid duplications.
TfLiteStatus NotEqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   d9a6c:	f080 0001 	eor.w	r0, r0, #1
  switch (input1->type) {
   d9a70:	1e53      	subs	r3, r2, #1
// TODO(renjieliu): Refactor the logic to avoid duplications.
TfLiteStatus NotEqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   d9a72:	b2c7      	uxtb	r7, r0
  switch (input1->type) {
   d9a74:	2b08      	cmp	r3, #8
   d9a76:	f200 826e 	bhi.w	d9f56 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x51e>
   d9a7a:	e8df f013 	tbh	[pc, r3, lsl #1]
   d9a7e:	0053      	.short	0x0053
   d9a80:	013900a1 	.word	0x013900a1
   d9a84:	026c00eb 	.word	0x026c00eb
   d9a88:	026c0009 	.word	0x026c0009
   d9a8c:	01cd026c 	.word	0x01cd026c
   d9a90:	f10d 0874 	add.w	r8, sp, #116	; 0x74
    case kTfLiteBool:
      TF_LITE_COMPARISON(bool, NotEqual, requires_broadcast);
   d9a94:	4631      	mov	r1, r6
   d9a96:	b1cf      	cbz	r7, d9acc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x94>
   d9a98:	a813      	add	r0, sp, #76	; 0x4c
   d9a9a:	f7fc ff8a 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9a9e:	4629      	mov	r1, r5
   d9aa0:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9aa2:	6876      	ldr	r6, [r6, #4]
   d9aa4:	f7fc ff85 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9aa8:	b105      	cbz	r5, d9aac <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x74>
   d9aaa:	686d      	ldr	r5, [r5, #4]
   d9aac:	4621      	mov	r1, r4
   d9aae:	4640      	mov	r0, r8
   d9ab0:	f7fc ff7f 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9ab4:	b104      	cbz	r4, d9ab8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x80>
   d9ab6:	6864      	ldr	r4, [r4, #4]
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
   d9ab8:	9402      	str	r4, [sp, #8]
   d9aba:	e88d 0120 	stmia.w	sp, {r5, r8}
   d9abe:	ab18      	add	r3, sp, #96	; 0x60
   d9ac0:	4632      	mov	r2, r6
   d9ac2:	a913      	add	r1, sp, #76	; 0x4c
   d9ac4:	a822      	add	r0, sp, #136	; 0x88
   d9ac6:	f7fe fbbf 	bl	d8248 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIbXadL_ZNS0_10NotEqualFnIbEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d9aca:	e1e7      	b.n	d9e9c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x464>
   d9acc:	a818      	add	r0, sp, #96	; 0x60
   d9ace:	f7fc ff70 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9ad2:	4629      	mov	r1, r5
   d9ad4:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9ad6:	6876      	ldr	r6, [r6, #4]
   d9ad8:	f7fc ff6b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9adc:	b105      	cbz	r5, d9ae0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0xa8>
   d9ade:	686d      	ldr	r5, [r5, #4]
   d9ae0:	4621      	mov	r1, r4
   d9ae2:	a822      	add	r0, sp, #136	; 0x88
   d9ae4:	f7fc ff65 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9ae8:	b104      	cbz	r4, d9aec <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0xb4>
   d9aea:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9aec:	4641      	mov	r1, r8
   d9aee:	aa22      	add	r2, sp, #136	; 0x88
   d9af0:	a818      	add	r0, sp, #96	; 0x60
   d9af2:	f7fc fd40 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d9af6:	3d01      	subs	r5, #1
   d9af8:	1e73      	subs	r3, r6, #1
   d9afa:	17c1      	asrs	r1, r0, #31
   d9afc:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
   d9afe:	2600      	movs	r6, #0
   d9b00:	2700      	movs	r7, #0
   d9b02:	4286      	cmp	r6, r0
   d9b04:	eb77 0201 	sbcs.w	r2, r7, r1
   d9b08:	f280 822c 	bge.w	d9f64 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x52c>
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9b0c:	f813 ef01 	ldrb.w	lr, [r3, #1]!
   d9b10:	f815 2f01 	ldrb.w	r2, [r5, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9b14:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9b16:	ea8e 0202 	eor.w	r2, lr, r2
   d9b1a:	f804 2f01 	strb.w	r2, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9b1e:	f147 0700 	adc.w	r7, r7, #0
   d9b22:	e7ee      	b.n	d9b02 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0xca>
   d9b24:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteFloat32:
      TF_LITE_COMPARISON(float, NotEqual, requires_broadcast);
   d9b28:	4631      	mov	r1, r6
   d9b2a:	b1cf      	cbz	r7, d9b60 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x128>
   d9b2c:	a813      	add	r0, sp, #76	; 0x4c
   d9b2e:	f7fc ff40 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9b32:	4629      	mov	r1, r5
   d9b34:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9b36:	6876      	ldr	r6, [r6, #4]
   d9b38:	f7fc ff3b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9b3c:	b105      	cbz	r5, d9b40 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x108>
   d9b3e:	686d      	ldr	r5, [r5, #4]
   d9b40:	4621      	mov	r1, r4
   d9b42:	4640      	mov	r0, r8
   d9b44:	f7fc ff35 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9b48:	b104      	cbz	r4, d9b4c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x114>
   d9b4a:	6864      	ldr	r4, [r4, #4]
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
   d9b4c:	9402      	str	r4, [sp, #8]
   d9b4e:	e88d 0120 	stmia.w	sp, {r5, r8}
   d9b52:	ab18      	add	r3, sp, #96	; 0x60
   d9b54:	4632      	mov	r2, r6
   d9b56:	a913      	add	r1, sp, #76	; 0x4c
   d9b58:	a822      	add	r0, sp, #136	; 0x88
   d9b5a:	f7fe fbdd 	bl	d8318 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_10NotEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d9b5e:	e19d      	b.n	d9e9c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x464>
   d9b60:	a818      	add	r0, sp, #96	; 0x60
   d9b62:	f7fc ff26 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9b66:	4629      	mov	r1, r5
   d9b68:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9b6a:	6876      	ldr	r6, [r6, #4]
   d9b6c:	f7fc ff21 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9b70:	b105      	cbz	r5, d9b74 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x13c>
   d9b72:	686d      	ldr	r5, [r5, #4]
   d9b74:	4621      	mov	r1, r4
   d9b76:	a822      	add	r0, sp, #136	; 0x88
   d9b78:	f7fc ff1b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9b7c:	b104      	cbz	r4, d9b80 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x148>
   d9b7e:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9b80:	4641      	mov	r1, r8
   d9b82:	aa22      	add	r2, sp, #136	; 0x88
   d9b84:	a818      	add	r0, sp, #96	; 0x60
   d9b86:	f7fc fcf6 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d9b8a:	3c01      	subs	r4, #1
   d9b8c:	4633      	mov	r3, r6
   d9b8e:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   d9b90:	2600      	movs	r6, #0
   d9b92:	2700      	movs	r7, #0
   d9b94:	4286      	cmp	r6, r0
   d9b96:	eb77 0201 	sbcs.w	r2, r7, r1
   d9b9a:	f280 81e3 	bge.w	d9f64 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x52c>
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9b9e:	ecb3 7a01 	vldmia	r3!, {s14}
   d9ba2:	ecf5 7a01 	vldmia	r5!, {s15}
   d9ba6:	eeb4 7a67 	vcmp.f32	s14, s15
   d9baa:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   d9bae:	bf14      	ite	ne
   d9bb0:	2201      	movne	r2, #1
   d9bb2:	2200      	moveq	r2, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9bb4:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9bb6:	f804 2f01 	strb.w	r2, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9bba:	f147 0700 	adc.w	r7, r7, #0
   d9bbe:	e7e9      	b.n	d9b94 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x15c>
   d9bc0:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, NotEqual, requires_broadcast);
   d9bc4:	4631      	mov	r1, r6
   d9bc6:	b1cf      	cbz	r7, d9bfc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x1c4>
   d9bc8:	a813      	add	r0, sp, #76	; 0x4c
   d9bca:	f7fc fef2 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9bce:	4629      	mov	r1, r5
   d9bd0:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9bd2:	6876      	ldr	r6, [r6, #4]
   d9bd4:	f7fc feed 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9bd8:	b105      	cbz	r5, d9bdc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x1a4>
   d9bda:	686d      	ldr	r5, [r5, #4]
   d9bdc:	4621      	mov	r1, r4
   d9bde:	4640      	mov	r0, r8
   d9be0:	f7fc fee7 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9be4:	b104      	cbz	r4, d9be8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x1b0>
   d9be6:	6864      	ldr	r4, [r4, #4]
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
   d9be8:	9402      	str	r4, [sp, #8]
   d9bea:	e88d 0120 	stmia.w	sp, {r5, r8}
   d9bee:	ab18      	add	r3, sp, #96	; 0x60
   d9bf0:	4632      	mov	r2, r6
   d9bf2:	a913      	add	r1, sp, #76	; 0x4c
   d9bf4:	a822      	add	r0, sp, #136	; 0x88
   d9bf6:	f7fe fc02 	bl	d83fe <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d9bfa:	e14f      	b.n	d9e9c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x464>
   d9bfc:	a818      	add	r0, sp, #96	; 0x60
   d9bfe:	f7fc fed8 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9c02:	4629      	mov	r1, r5
   d9c04:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9c06:	6876      	ldr	r6, [r6, #4]
   d9c08:	f7fc fed3 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9c0c:	b105      	cbz	r5, d9c10 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x1d8>
   d9c0e:	686d      	ldr	r5, [r5, #4]
   d9c10:	4621      	mov	r1, r4
   d9c12:	a822      	add	r0, sp, #136	; 0x88
   d9c14:	f7fc fecd 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9c18:	b104      	cbz	r4, d9c1c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x1e4>
   d9c1a:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9c1c:	aa22      	add	r2, sp, #136	; 0x88
   d9c1e:	4641      	mov	r1, r8
   d9c20:	a818      	add	r0, sp, #96	; 0x60
   d9c22:	f7fc fca8 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d9c26:	3c01      	subs	r4, #1
   d9c28:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   d9c2a:	2200      	movs	r2, #0
   d9c2c:	2300      	movs	r3, #0
   d9c2e:	4282      	cmp	r2, r0
   d9c30:	eb73 0701 	sbcs.w	r7, r3, r1
   d9c34:	f280 8196 	bge.w	d9f64 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x52c>
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9c38:	f856 7022 	ldr.w	r7, [r6, r2, lsl #2]
   d9c3c:	f855 e022 	ldr.w	lr, [r5, r2, lsl #2]
   d9c40:	ebb7 070e 	subs.w	r7, r7, lr
   d9c44:	bf18      	it	ne
   d9c46:	2701      	movne	r7, #1
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9c48:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9c4a:	f804 7f01 	strb.w	r7, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9c4e:	f143 0300 	adc.w	r3, r3, #0
   d9c52:	e7ec      	b.n	d9c2e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x1f6>
   d9c54:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, NotEqual, requires_broadcast);
   d9c58:	4631      	mov	r1, r6
   d9c5a:	b1cf      	cbz	r7, d9c90 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x258>
   d9c5c:	a813      	add	r0, sp, #76	; 0x4c
   d9c5e:	f7fc fea8 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9c62:	4629      	mov	r1, r5
   d9c64:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9c66:	6876      	ldr	r6, [r6, #4]
   d9c68:	f7fc fea3 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9c6c:	b105      	cbz	r5, d9c70 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x238>
   d9c6e:	686d      	ldr	r5, [r5, #4]
   d9c70:	4621      	mov	r1, r4
   d9c72:	4640      	mov	r0, r8
   d9c74:	f7fc fe9d 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9c78:	b104      	cbz	r4, d9c7c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x244>
   d9c7a:	6864      	ldr	r4, [r4, #4]
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
   d9c7c:	9402      	str	r4, [sp, #8]
   d9c7e:	e88d 0120 	stmia.w	sp, {r5, r8}
   d9c82:	ab18      	add	r3, sp, #96	; 0x60
   d9c84:	4632      	mov	r2, r6
   d9c86:	a913      	add	r1, sp, #76	; 0x4c
   d9c88:	a822      	add	r0, sp, #136	; 0x88
   d9c8a:	f7fe fc23 	bl	d84d4 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_10NotEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d9c8e:	e105      	b.n	d9e9c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x464>
   d9c90:	a818      	add	r0, sp, #96	; 0x60
   d9c92:	f7fc fe8e 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9c96:	4629      	mov	r1, r5
   d9c98:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9c9a:	6876      	ldr	r6, [r6, #4]
   d9c9c:	f7fc fe89 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9ca0:	b105      	cbz	r5, d9ca4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x26c>
   d9ca2:	686d      	ldr	r5, [r5, #4]
   d9ca4:	4621      	mov	r1, r4
   d9ca6:	a822      	add	r0, sp, #136	; 0x88
   d9ca8:	f7fc fe83 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9cac:	b104      	cbz	r4, d9cb0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x278>
   d9cae:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9cb0:	aa22      	add	r2, sp, #136	; 0x88
   d9cb2:	4641      	mov	r1, r8
   d9cb4:	a818      	add	r0, sp, #96	; 0x60
   d9cb6:	f7fc fc5e 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d9cba:	3d08      	subs	r5, #8
   d9cbc:	17c1      	asrs	r1, r0, #31
   d9cbe:	f1a6 0e08 	sub.w	lr, r6, #8
   d9cc2:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
   d9cc4:	2200      	movs	r2, #0
   d9cc6:	2300      	movs	r3, #0
   d9cc8:	4282      	cmp	r2, r0
   d9cca:	eb73 0601 	sbcs.w	r6, r3, r1
   d9cce:	f280 8149 	bge.w	d9f64 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x52c>
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9cd2:	e9f5 6702 	ldrd	r6, r7, [r5, #8]!
   d9cd6:	e9fe ab02 	ldrd	sl, fp, [lr, #8]!
   d9cda:	45bb      	cmp	fp, r7
   d9cdc:	bf0a      	itet	eq
   d9cde:	45b2      	cmpeq	sl, r6
   d9ce0:	2601      	movne	r6, #1
   d9ce2:	2600      	moveq	r6, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9ce4:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   d9ce6:	f804 6f01 	strb.w	r6, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9cea:	f143 0300 	adc.w	r3, r3, #0
   d9cee:	e7eb      	b.n	d9cc8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x290>
            GetTensorData<bool>(output));                                      \
      }                                                                        \
    }                                                                          \
  }
TF_LITE_QUANTIZE_COMPARISON(Equal);
TF_LITE_QUANTIZE_COMPARISON(NotEqual);
   d9cf0:	6933      	ldr	r3, [r6, #16]
   d9cf2:	68f0      	ldr	r0, [r6, #12]
   d9cf4:	f1c3 0900 	rsb	r9, r3, #0
   d9cf8:	692b      	ldr	r3, [r5, #16]
   d9cfa:	f1c3 0800 	rsb	r8, r3, #0
   d9cfe:	f00d f965 	bl	e6fcc <__aeabi_f2d>
   d9d02:	ec41 0b10 	vmov	d0, r0, r1
   d9d06:	a910      	add	r1, sp, #64	; 0x40
   d9d08:	a80f      	add	r0, sp, #60	; 0x3c
   d9d0a:	f00a f8ad 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   d9d0e:	68e8      	ldr	r0, [r5, #12]
   d9d10:	f00d f95c 	bl	e6fcc <__aeabi_f2d>
   d9d14:	ec41 0b10 	vmov	d0, r0, r1
   d9d18:	a912      	add	r1, sp, #72	; 0x48
   d9d1a:	a811      	add	r0, sp, #68	; 0x44
   d9d1c:	f00a f8a4 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   d9d20:	2308      	movs	r3, #8
   d9d22:	9322      	str	r3, [sp, #136]	; 0x88
   d9d24:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   d9d26:	9324      	str	r3, [sp, #144]	; 0x90
   d9d28:	9b10      	ldr	r3, [sp, #64]	; 0x40
   d9d2a:	9325      	str	r3, [sp, #148]	; 0x94
   d9d2c:	9b11      	ldr	r3, [sp, #68]	; 0x44
   d9d2e:	9327      	str	r3, [sp, #156]	; 0x9c
   d9d30:	9b12      	ldr	r3, [sp, #72]	; 0x48
   d9d32:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   d9d36:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   d9d3a:	9328      	str	r3, [sp, #160]	; 0xa0
   d9d3c:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   d9d40:	4631      	mov	r1, r6
   d9d42:	a813      	add	r0, sp, #76	; 0x4c
   d9d44:	b1bf      	cbz	r7, d9d76 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x33e>
   d9d46:	f7fc fe34 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9d4a:	4629      	mov	r1, r5
   d9d4c:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9d4e:	6876      	ldr	r6, [r6, #4]
   d9d50:	f7fc fe2f 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9d54:	4621      	mov	r1, r4
   d9d56:	4640      	mov	r0, r8
   d9d58:	686d      	ldr	r5, [r5, #4]
   d9d5a:	f7fc fe2a 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9d5e:	b104      	cbz	r4, d9d62 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x32a>
   d9d60:	6864      	ldr	r4, [r4, #4]
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
   d9d62:	9402      	str	r4, [sp, #8]
   d9d64:	e88d 0120 	stmia.w	sp, {r5, r8}
   d9d68:	ab18      	add	r3, sp, #96	; 0x60
   d9d6a:	4632      	mov	r2, r6
   d9d6c:	a913      	add	r1, sp, #76	; 0x4c
   d9d6e:	a822      	add	r0, sp, #136	; 0x88
   d9d70:	f7ff fd3a 	bl	d97e8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d9d74:	e092      	b.n	d9e9c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x464>
   d9d76:	f7fc fe1c 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9d7a:	6873      	ldr	r3, [r6, #4]
   d9d7c:	9305      	str	r3, [sp, #20]
   d9d7e:	4629      	mov	r1, r5
   d9d80:	a818      	add	r0, sp, #96	; 0x60
   d9d82:	f7fc fe16 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9d86:	4621      	mov	r1, r4
   d9d88:	4640      	mov	r0, r8
   d9d8a:	f8d5 b004 	ldr.w	fp, [r5, #4]
   d9d8e:	f7fc fe10 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9d92:	b104      	cbz	r4, d9d96 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x35e>
   d9d94:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   d9d96:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   d9d98:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   d9d9a:	9b24      	ldr	r3, [sp, #144]	; 0x90
   d9d9c:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   d9d9e:	9b25      	ldr	r3, [sp, #148]	; 0x94
   d9da0:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   d9da2:	9b26      	ldr	r3, [sp, #152]	; 0x98
   d9da4:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   d9da6:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   d9da8:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9daa:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   d9dac:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   d9dae:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9db0:	a918      	add	r1, sp, #96	; 0x60
   d9db2:	a813      	add	r0, sp, #76	; 0x4c
   d9db4:	f7fc fbdf 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d9db8:	4602      	mov	r2, r0
   d9dba:	17c3      	asrs	r3, r0, #31
   d9dbc:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   d9dc0:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9dc2:	f04f 0800 	mov.w	r8, #0
   d9dc6:	f04f 0900 	mov.w	r9, #0
   d9dca:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   d9dce:	4590      	cmp	r8, r2
   d9dd0:	eb79 0303 	sbcs.w	r3, r9, r3
   d9dd4:	f280 80b4 	bge.w	d9f40 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x508>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9dd8:	f81b 5008 	ldrb.w	r5, [fp, r8]
   d9ddc:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d9dde:	9a08      	ldr	r2, [sp, #32]
   d9de0:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9de2:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d9de4:	9b05      	ldr	r3, [sp, #20]
   d9de6:	f813 0008 	ldrb.w	r0, [r3, r8]
   d9dea:	9b06      	ldr	r3, [sp, #24]
   d9dec:	4418      	add	r0, r3
   d9dee:	40b8      	lsls	r0, r7
   d9df0:	f7fc fbfe 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9df4:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d9df6:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   d9df8:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   d9dfa:	990a      	ldr	r1, [sp, #40]	; 0x28
   d9dfc:	4628      	mov	r0, r5
   d9dfe:	f7fc fbf7 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   d9e02:	ebba 0000 	subs.w	r0, sl, r0
   d9e06:	bf18      	it	ne
   d9e08:	2001      	movne	r0, #1
   d9e0a:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9e0e:	f118 0801 	adds.w	r8, r8, #1
   d9e12:	f149 0900 	adc.w	r9, r9, #0
   d9e16:	e7d8      	b.n	d9dca <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x392>
   d9e18:	6933      	ldr	r3, [r6, #16]
   d9e1a:	68f0      	ldr	r0, [r6, #12]
   d9e1c:	f1c3 0900 	rsb	r9, r3, #0
   d9e20:	692b      	ldr	r3, [r5, #16]
   d9e22:	f1c3 0800 	rsb	r8, r3, #0
   d9e26:	f00d f8d1 	bl	e6fcc <__aeabi_f2d>
   d9e2a:	ec41 0b10 	vmov	d0, r0, r1
   d9e2e:	a910      	add	r1, sp, #64	; 0x40
   d9e30:	a80f      	add	r0, sp, #60	; 0x3c
   d9e32:	f00a f819 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   d9e36:	68e8      	ldr	r0, [r5, #12]
   d9e38:	f00d f8c8 	bl	e6fcc <__aeabi_f2d>
   d9e3c:	ec41 0b10 	vmov	d0, r0, r1
   d9e40:	a912      	add	r1, sp, #72	; 0x48
   d9e42:	a811      	add	r0, sp, #68	; 0x44
   d9e44:	f00a f810 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   d9e48:	2308      	movs	r3, #8
   d9e4a:	9322      	str	r3, [sp, #136]	; 0x88
   d9e4c:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   d9e4e:	9324      	str	r3, [sp, #144]	; 0x90
   d9e50:	9b10      	ldr	r3, [sp, #64]	; 0x40
   d9e52:	9325      	str	r3, [sp, #148]	; 0x94
   d9e54:	9b11      	ldr	r3, [sp, #68]	; 0x44
   d9e56:	9327      	str	r3, [sp, #156]	; 0x9c
   d9e58:	9b12      	ldr	r3, [sp, #72]	; 0x48
   d9e5a:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   d9e5e:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   d9e62:	9328      	str	r3, [sp, #160]	; 0xa0
   d9e64:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   d9e68:	4631      	mov	r1, r6
   d9e6a:	a813      	add	r0, sp, #76	; 0x4c
   d9e6c:	b1c7      	cbz	r7, d9ea0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x468>
   d9e6e:	f7fc fda0 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9e72:	4629      	mov	r1, r5
   d9e74:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9e76:	6876      	ldr	r6, [r6, #4]
   d9e78:	f7fc fd9b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9e7c:	4621      	mov	r1, r4
   d9e7e:	4640      	mov	r0, r8
   d9e80:	686d      	ldr	r5, [r5, #4]
   d9e82:	f7fc fd96 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9e86:	b104      	cbz	r4, d9e8a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x452>
   d9e88:	6864      	ldr	r4, [r4, #4]
    BroadcastComparison4DSlowWithScaling<T, name##Fn>(                         \
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
   d9e8a:	9402      	str	r4, [sp, #8]
   d9e8c:	e88d 0120 	stmia.w	sp, {r5, r8}
   d9e90:	ab18      	add	r3, sp, #96	; 0x60
   d9e92:	4632      	mov	r2, r6
   d9e94:	a913      	add	r1, sp, #76	; 0x4c
   d9e96:	a822      	add	r0, sp, #136	; 0x88
   d9e98:	f7ff fd3a 	bl	d9910 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_10NotEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   d9e9c:	4640      	mov	r0, r8
   d9e9e:	e050      	b.n	d9f42 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x50a>
   d9ea0:	f7fc fd87 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   d9ea4:	6873      	ldr	r3, [r6, #4]
   d9ea6:	9305      	str	r3, [sp, #20]
   d9ea8:	4629      	mov	r1, r5
   d9eaa:	a818      	add	r0, sp, #96	; 0x60
   d9eac:	f7fc fd81 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   d9eb0:	4621      	mov	r1, r4
   d9eb2:	4640      	mov	r0, r8
   d9eb4:	f8d5 b004 	ldr.w	fp, [r5, #4]
   d9eb8:	f7fc fd7b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   d9ebc:	b104      	cbz	r4, d9ec0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x488>
   d9ebe:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   d9ec0:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   d9ec2:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   d9ec4:	9b24      	ldr	r3, [sp, #144]	; 0x90
   d9ec6:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   d9ec8:	9b25      	ldr	r3, [sp, #148]	; 0x94
   d9eca:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   d9ecc:	9b26      	ldr	r3, [sp, #152]	; 0x98
   d9ece:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   d9ed0:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   d9ed2:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9ed4:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   d9ed6:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   d9ed8:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   d9eda:	a918      	add	r1, sp, #96	; 0x60
   d9edc:	a813      	add	r0, sp, #76	; 0x4c
   d9ede:	f7fc fb4a 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   d9ee2:	4602      	mov	r2, r0
   d9ee4:	17c3      	asrs	r3, r0, #31
   d9ee6:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   d9eea:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9eec:	f04f 0800 	mov.w	r8, #0
   d9ef0:	f04f 0900 	mov.w	r9, #0
   d9ef4:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   d9ef8:	4590      	cmp	r8, r2
   d9efa:	eb79 0303 	sbcs.w	r3, r9, r3
   d9efe:	da1f      	bge.n	d9f40 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x508>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9f00:	f91b 5008 	ldrsb.w	r5, [fp, r8]
   d9f04:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d9f06:	9a08      	ldr	r2, [sp, #32]
   d9f08:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9f0a:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d9f0c:	9b05      	ldr	r3, [sp, #20]
   d9f0e:	f913 0008 	ldrsb.w	r0, [r3, r8]
   d9f12:	9b06      	ldr	r3, [sp, #24]
   d9f14:	4418      	add	r0, r3
   d9f16:	40b8      	lsls	r0, r7
   d9f18:	f7fc fb6a 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   d9f1c:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   d9f1e:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   d9f20:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   d9f22:	990a      	ldr	r1, [sp, #40]	; 0x28
   d9f24:	4628      	mov	r0, r5
   d9f26:	f7fc fb63 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   d9f2a:	ebba 0000 	subs.w	r0, sl, r0
   d9f2e:	bf18      	it	ne
   d9f30:	2001      	movne	r0, #1
   d9f32:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   d9f36:	f118 0801 	adds.w	r8, r8, #1
   d9f3a:	f149 0900 	adc.w	r9, r9, #0
   d9f3e:	e7d9      	b.n	d9ef4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x4bc>
   d9f40:	a81d      	add	r0, sp, #116	; 0x74
   d9f42:	f7fc fa86 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   d9f46:	a818      	add	r0, sp, #96	; 0x60
   d9f48:	f7fc fa83 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   d9f4c:	a813      	add	r0, sp, #76	; 0x4c
   d9f4e:	f7fc fa80 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(
          context, "Does not support type %d, requires bool|float|int|uint8",
          input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
   d9f52:	2000      	movs	r0, #0
   d9f54:	e00e      	b.n	d9f74 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x53c>
                                    requires_broadcast);
      break;
    default:
      context->ReportError(
          context, "Does not support type %d, requires bool|float|int|uint8",
          input1->type);
   d9f56:	4650      	mov	r0, sl
   d9f58:	f8da 3014 	ldr.w	r3, [sl, #20]
   d9f5c:	4907      	ldr	r1, [pc, #28]	; (d9f7c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x544>)
   d9f5e:	4798      	blx	r3
      return kTfLiteError;
   d9f60:	2001      	movs	r0, #1
   d9f62:	e007      	b.n	d9f74 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x53c>
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, NotEqual, requires_broadcast);
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, NotEqual, requires_broadcast);
   d9f64:	a822      	add	r0, sp, #136	; 0x88
   d9f66:	f7fc fa74 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   d9f6a:	4640      	mov	r0, r8
   d9f6c:	f7fc fa71 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   d9f70:	a818      	add	r0, sp, #96	; 0x60
   d9f72:	e7ec      	b.n	d9f4e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_112NotEqualEvalEP13TfLiteContextP10TfLiteNode+0x516>
          context, "Does not support type %d, requires bool|float|int|uint8",
          input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   d9f74:	b02b      	add	sp, #172	; 0xac
   d9f76:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   d9f7a:	bf00      	nop
   d9f7c:	000e9987 	.word	0x000e9987

000d9f80 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d9f80:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   d9f84:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d9f86:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d9f88:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d9f8a:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   d9f8c:	9208      	str	r2, [sp, #32]
   d9f8e:	4604      	mov	r4, r0
   d9f90:	460e      	mov	r6, r1
   d9f92:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   d9f94:	dd01      	ble.n	d9f9a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   d9f96:	f00a f9d9 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   d9f9a:	683b      	ldr	r3, [r7, #0]
   d9f9c:	2b04      	cmp	r3, #4
   d9f9e:	dcfa      	bgt.n	d9f96 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   d9fa0:	6813      	ldr	r3, [r2, #0]
   d9fa2:	2b04      	cmp	r3, #4
   d9fa4:	dcf7      	bgt.n	d9f96 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   d9fa6:	2301      	movs	r3, #1
   d9fa8:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   d9faa:	ad10      	add	r5, sp, #64	; 0x40
   d9fac:	a80b      	add	r0, sp, #44	; 0x2c
   d9fae:	f7fc fa94 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   d9fb2:	ab18      	add	r3, sp, #96	; 0x60
   d9fb4:	462a      	mov	r2, r5
   d9fb6:	4639      	mov	r1, r7
   d9fb8:	4630      	mov	r0, r6
   d9fba:	f7fc fda3 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   d9fbe:	6863      	ldr	r3, [r4, #4]
   d9fc0:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   d9fc2:	68a3      	ldr	r3, [r4, #8]
   d9fc4:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   d9fc6:	68e3      	ldr	r3, [r4, #12]
   d9fc8:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   d9fca:	6923      	ldr	r3, [r4, #16]
   d9fcc:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   d9fce:	6963      	ldr	r3, [r4, #20]
   d9fd0:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   d9fd2:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   d9fd4:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   d9fd8:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d9fda:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   d9fdc:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   d9fde:	2100      	movs	r1, #0
   d9fe0:	a80b      	add	r0, sp, #44	; 0x2c
   d9fe2:	f7fc fa41 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d9fe6:	4284      	cmp	r4, r0
   d9fe8:	da59      	bge.n	da09e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   d9fea:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   d9fec:	af0b      	add	r7, sp, #44	; 0x2c
   d9fee:	2101      	movs	r1, #1
   d9ff0:	4638      	mov	r0, r7
   d9ff2:	f7fc fa39 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   d9ff6:	4285      	cmp	r5, r0
   d9ff8:	da4f      	bge.n	da09a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   d9ffa:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   d9ffc:	2102      	movs	r1, #2
   d9ffe:	4638      	mov	r0, r7
   da000:	f7fc fa32 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   da004:	4286      	cmp	r6, r0
   da006:	da46      	bge.n	da096 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   da008:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   da00c:	2103      	movs	r1, #3
   da00e:	4638      	mov	r0, r7
   da010:	f7fc fa2a 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   da014:	4580      	cmp	r8, r0
   da016:	da3c      	bge.n	da092 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da018:	f8cd 8000 	str.w	r8, [sp]
   da01c:	4633      	mov	r3, r6
   da01e:	462a      	mov	r2, r5
   da020:	4621      	mov	r1, r4
   da022:	9809      	ldr	r0, [sp, #36]	; 0x24
   da024:	f7fc fb36 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   da028:	9b08      	ldr	r3, [sp, #32]
   da02a:	f813 9000 	ldrb.w	r9, [r3, r0]
   da02e:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   da030:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da034:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   da036:	462a      	mov	r2, r5
   da038:	4633      	mov	r3, r6
   da03a:	4621      	mov	r1, r4
   da03c:	a818      	add	r0, sp, #96	; 0x60
   da03e:	f7fc fb29 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da042:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da044:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da046:	f813 b000 	ldrb.w	fp, [r3, r0]
   da04a:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da04c:	9903      	ldr	r1, [sp, #12]
   da04e:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da052:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da054:	f7fc facc 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da058:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da05c:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   da05e:	9a07      	ldr	r2, [sp, #28]
   da060:	9906      	ldr	r1, [sp, #24]
   da062:	4658      	mov	r0, fp
   da064:	f7fc fac4 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   da068:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   da06c:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   da06e:	4633      	mov	r3, r6
   da070:	462a      	mov	r2, r5
   da072:	4621      	mov	r1, r4
   da074:	4638      	mov	r0, r7
   da076:	f7fc fa5c 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   da07a:	45d9      	cmp	r9, fp
   da07c:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   da07e:	bfd4      	ite	le
   da080:	f04f 0900 	movle.w	r9, #0
   da084:	f04f 0901 	movgt.w	r9, #1
   da088:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   da08c:	f108 0801 	add.w	r8, r8, #1
   da090:	e7bc      	b.n	da00c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   da092:	3601      	adds	r6, #1
   da094:	e7b2      	b.n	d9ffc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   da096:	3501      	adds	r5, #1
   da098:	e7a8      	b.n	d9fec <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da09a:	3401      	adds	r4, #1
   da09c:	e79f      	b.n	d9fde <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   da09e:	a80b      	add	r0, sp, #44	; 0x2c
   da0a0:	f7fc f9d7 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   da0a4:	b021      	add	sp, #132	; 0x84
   da0a6:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000da0aa <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   da0aa:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   da0ae:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   da0b0:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   da0b2:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   da0b4:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   da0b6:	9208      	str	r2, [sp, #32]
   da0b8:	4604      	mov	r4, r0
   da0ba:	460e      	mov	r6, r1
   da0bc:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   da0be:	dd01      	ble.n	da0c4 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   da0c0:	f00a f944 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   da0c4:	683b      	ldr	r3, [r7, #0]
   da0c6:	2b04      	cmp	r3, #4
   da0c8:	dcfa      	bgt.n	da0c0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   da0ca:	6813      	ldr	r3, [r2, #0]
   da0cc:	2b04      	cmp	r3, #4
   da0ce:	dcf7      	bgt.n	da0c0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   da0d0:	2301      	movs	r3, #1
   da0d2:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   da0d4:	ad10      	add	r5, sp, #64	; 0x40
   da0d6:	a80b      	add	r0, sp, #44	; 0x2c
   da0d8:	f7fc f9ff 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   da0dc:	ab18      	add	r3, sp, #96	; 0x60
   da0de:	462a      	mov	r2, r5
   da0e0:	4639      	mov	r1, r7
   da0e2:	4630      	mov	r0, r6
   da0e4:	f7fc fd0e 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   da0e8:	6863      	ldr	r3, [r4, #4]
   da0ea:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   da0ec:	68a3      	ldr	r3, [r4, #8]
   da0ee:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   da0f0:	68e3      	ldr	r3, [r4, #12]
   da0f2:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   da0f4:	6923      	ldr	r3, [r4, #16]
   da0f6:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   da0f8:	6963      	ldr	r3, [r4, #20]
   da0fa:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   da0fc:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   da0fe:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   da102:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da104:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da106:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da108:	2100      	movs	r1, #0
   da10a:	a80b      	add	r0, sp, #44	; 0x2c
   da10c:	f7fc f9ac 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   da110:	4284      	cmp	r4, r0
   da112:	da59      	bge.n	da1c8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   da114:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   da116:	af0b      	add	r7, sp, #44	; 0x2c
   da118:	2101      	movs	r1, #1
   da11a:	4638      	mov	r0, r7
   da11c:	f7fc f9a4 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   da120:	4285      	cmp	r5, r0
   da122:	da4f      	bge.n	da1c4 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   da124:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   da126:	2102      	movs	r1, #2
   da128:	4638      	mov	r0, r7
   da12a:	f7fc f99d 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   da12e:	4286      	cmp	r6, r0
   da130:	da46      	bge.n	da1c0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   da132:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   da136:	2103      	movs	r1, #3
   da138:	4638      	mov	r0, r7
   da13a:	f7fc f995 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   da13e:	4580      	cmp	r8, r0
   da140:	da3c      	bge.n	da1bc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da142:	f8cd 8000 	str.w	r8, [sp]
   da146:	4633      	mov	r3, r6
   da148:	462a      	mov	r2, r5
   da14a:	4621      	mov	r1, r4
   da14c:	9809      	ldr	r0, [sp, #36]	; 0x24
   da14e:	f7fc faa1 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   da152:	9b08      	ldr	r3, [sp, #32]
   da154:	f913 9000 	ldrsb.w	r9, [r3, r0]
   da158:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   da15a:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da15e:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   da160:	462a      	mov	r2, r5
   da162:	4633      	mov	r3, r6
   da164:	4621      	mov	r1, r4
   da166:	a818      	add	r0, sp, #96	; 0x60
   da168:	f7fc fa94 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da16c:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da16e:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da170:	f913 b000 	ldrsb.w	fp, [r3, r0]
   da174:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da176:	9903      	ldr	r1, [sp, #12]
   da178:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da17c:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da17e:	f7fc fa37 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da182:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da186:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   da188:	9a07      	ldr	r2, [sp, #28]
   da18a:	9906      	ldr	r1, [sp, #24]
   da18c:	4658      	mov	r0, fp
   da18e:	f7fc fa2f 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   da192:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   da196:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   da198:	4633      	mov	r3, r6
   da19a:	462a      	mov	r2, r5
   da19c:	4621      	mov	r1, r4
   da19e:	4638      	mov	r0, r7
   da1a0:	f7fc f9c7 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   da1a4:	45d9      	cmp	r9, fp
   da1a6:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   da1a8:	bfd4      	ite	le
   da1aa:	f04f 0900 	movle.w	r9, #0
   da1ae:	f04f 0901 	movgt.w	r9, #1
   da1b2:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   da1b6:	f108 0801 	add.w	r8, r8, #1
   da1ba:	e7bc      	b.n	da136 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   da1bc:	3601      	adds	r6, #1
   da1be:	e7b2      	b.n	da126 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   da1c0:	3501      	adds	r5, #1
   da1c2:	e7a8      	b.n	da116 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da1c4:	3401      	adds	r4, #1
   da1c6:	e79f      	b.n	da108 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   da1c8:	a80b      	add	r0, sp, #44	; 0x2c
   da1ca:	f7fc f942 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   da1ce:	b021      	add	sp, #132	; 0x84
   da1d0:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000da1d4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus GreaterEval(TfLiteContext* context, TfLiteNode* node) {
   da1d4:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   da1d8:	680a      	ldr	r2, [r1, #0]
   da1da:	f8d0 9008 	ldr.w	r9, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   da1de:	6895      	ldr	r5, [r2, #8]
   da1e0:	4682      	mov	sl, r0
   da1e2:	6850      	ldr	r0, [r2, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   da1e4:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   da1e6:	2338      	movs	r3, #56	; 0x38
   da1e8:	fb03 f800 	mul.w	r8, r3, r0
   da1ec:	fb03 9505 	mla	r5, r3, r5, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   da1f0:	6854      	ldr	r4, [r2, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   da1f2:	eb09 0608 	add.w	r6, r9, r8
   da1f6:	b0ab      	sub	sp, #172	; 0xac
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   da1f8:	4629      	mov	r1, r5
   da1fa:	4630      	mov	r0, r6
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   da1fc:	fb03 9404 	mla	r4, r3, r4, r9
   da200:	f009 fdbe 	bl	e3d80 <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
  switch (input1->type) {
   da204:	f819 2008 	ldrb.w	r2, [r9, r8]

TfLiteStatus GreaterEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   da208:	f080 0001 	eor.w	r0, r0, #1
  switch (input1->type) {
   da20c:	1e53      	subs	r3, r2, #1

TfLiteStatus GreaterEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   da20e:	b2c7      	uxtb	r7, r0
  switch (input1->type) {
   da210:	2b08      	cmp	r3, #8
   da212:	f200 8225 	bhi.w	da660 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x48c>
   da216:	e8df f013 	tbh	[pc, r3, lsl #1]
   da21a:	0009      	.short	0x0009
   da21c:	00f00057 	.word	0x00f00057
   da220:	022300a1 	.word	0x022300a1
   da224:	02230223 	.word	0x02230223
   da228:	01840223 	.word	0x01840223
   da22c:	f10d 0874 	add.w	r8, sp, #116	; 0x74
    case kTfLiteFloat32:
      TF_LITE_COMPARISON(float, Greater, requires_broadcast);
   da230:	4631      	mov	r1, r6
   da232:	b1cf      	cbz	r7, da268 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x94>
   da234:	a813      	add	r0, sp, #76	; 0x4c
   da236:	f7fc fbbc 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da23a:	4629      	mov	r1, r5
   da23c:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da23e:	6876      	ldr	r6, [r6, #4]
   da240:	f7fc fbb7 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da244:	b105      	cbz	r5, da248 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x74>
   da246:	686d      	ldr	r5, [r5, #4]
   da248:	4621      	mov	r1, r4
   da24a:	4640      	mov	r0, r8
   da24c:	f7fc fbb1 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da250:	b104      	cbz	r4, da254 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x80>
   da252:	6864      	ldr	r4, [r4, #4]
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
   da254:	9402      	str	r4, [sp, #8]
   da256:	e88d 0120 	stmia.w	sp, {r5, r8}
   da25a:	ab18      	add	r3, sp, #96	; 0x60
   da25c:	4632      	mov	r2, r6
   da25e:	a913      	add	r1, sp, #76	; 0x4c
   da260:	a822      	add	r0, sp, #136	; 0x88
   da262:	f7fe f9a9 	bl	d85b8 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_9GreaterFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   da266:	e19e      	b.n	da5a6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   da268:	a818      	add	r0, sp, #96	; 0x60
   da26a:	f7fc fba2 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da26e:	4629      	mov	r1, r5
   da270:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da272:	6876      	ldr	r6, [r6, #4]
   da274:	f7fc fb9d 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da278:	b105      	cbz	r5, da27c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0xa8>
   da27a:	686d      	ldr	r5, [r5, #4]
   da27c:	4621      	mov	r1, r4
   da27e:	a822      	add	r0, sp, #136	; 0x88
   da280:	f7fc fb97 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da284:	b104      	cbz	r4, da288 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0xb4>
   da286:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   da288:	4641      	mov	r1, r8
   da28a:	aa22      	add	r2, sp, #136	; 0x88
   da28c:	a818      	add	r0, sp, #96	; 0x60
   da28e:	f7fc f972 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   da292:	3c01      	subs	r4, #1
   da294:	4633      	mov	r3, r6
   da296:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   da298:	2600      	movs	r6, #0
   da29a:	2700      	movs	r7, #0
   da29c:	4286      	cmp	r6, r0
   da29e:	eb77 0201 	sbcs.w	r2, r7, r1
   da2a2:	f280 81e4 	bge.w	da66e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   da2a6:	ecb3 7a01 	vldmia	r3!, {s14}
   da2aa:	ecf5 7a01 	vldmia	r5!, {s15}
   da2ae:	eeb4 7ae7 	vcmpe.f32	s14, s15
   da2b2:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   da2b6:	bfcc      	ite	gt
   da2b8:	2201      	movgt	r2, #1
   da2ba:	2200      	movle	r2, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da2bc:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   da2be:	f804 2f01 	strb.w	r2, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da2c2:	f147 0700 	adc.w	r7, r7, #0
   da2c6:	e7e9      	b.n	da29c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0xc8>
   da2c8:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, Greater, requires_broadcast);
   da2cc:	4631      	mov	r1, r6
   da2ce:	b1cf      	cbz	r7, da304 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x130>
   da2d0:	a813      	add	r0, sp, #76	; 0x4c
   da2d2:	f7fc fb6e 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da2d6:	4629      	mov	r1, r5
   da2d8:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da2da:	6876      	ldr	r6, [r6, #4]
   da2dc:	f7fc fb69 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da2e0:	b105      	cbz	r5, da2e4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x110>
   da2e2:	686d      	ldr	r5, [r5, #4]
   da2e4:	4621      	mov	r1, r4
   da2e6:	4640      	mov	r0, r8
   da2e8:	f7fc fb63 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da2ec:	b104      	cbz	r4, da2f0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x11c>
   da2ee:	6864      	ldr	r4, [r4, #4]
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
   da2f0:	9402      	str	r4, [sp, #8]
   da2f2:	e88d 0120 	stmia.w	sp, {r5, r8}
   da2f6:	ab18      	add	r3, sp, #96	; 0x60
   da2f8:	4632      	mov	r2, r6
   da2fa:	a913      	add	r1, sp, #76	; 0x4c
   da2fc:	a822      	add	r0, sp, #136	; 0x88
   da2fe:	f7fe f9ce 	bl	d869e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   da302:	e150      	b.n	da5a6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   da304:	a818      	add	r0, sp, #96	; 0x60
   da306:	f7fc fb54 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da30a:	4629      	mov	r1, r5
   da30c:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da30e:	6876      	ldr	r6, [r6, #4]
   da310:	f7fc fb4f 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da314:	b105      	cbz	r5, da318 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x144>
   da316:	686d      	ldr	r5, [r5, #4]
   da318:	4621      	mov	r1, r4
   da31a:	a822      	add	r0, sp, #136	; 0x88
   da31c:	f7fc fb49 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da320:	b104      	cbz	r4, da324 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x150>
   da322:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   da324:	aa22      	add	r2, sp, #136	; 0x88
   da326:	4641      	mov	r1, r8
   da328:	a818      	add	r0, sp, #96	; 0x60
   da32a:	f7fc f924 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   da32e:	3c01      	subs	r4, #1
   da330:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   da332:	2200      	movs	r2, #0
   da334:	2300      	movs	r3, #0
   da336:	4282      	cmp	r2, r0
   da338:	eb73 0701 	sbcs.w	r7, r3, r1
   da33c:	f280 8197 	bge.w	da66e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   da340:	f856 7022 	ldr.w	r7, [r6, r2, lsl #2]
   da344:	f855 e022 	ldr.w	lr, [r5, r2, lsl #2]
   da348:	4577      	cmp	r7, lr
   da34a:	bfd4      	ite	le
   da34c:	2700      	movle	r7, #0
   da34e:	2701      	movgt	r7, #1
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da350:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   da352:	f804 7f01 	strb.w	r7, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da356:	f143 0300 	adc.w	r3, r3, #0
   da35a:	e7ec      	b.n	da336 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x162>
   da35c:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, Greater, requires_broadcast);
   da360:	4631      	mov	r1, r6
   da362:	b1cf      	cbz	r7, da398 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x1c4>
   da364:	a813      	add	r0, sp, #76	; 0x4c
   da366:	f7fc fb24 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da36a:	4629      	mov	r1, r5
   da36c:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da36e:	6876      	ldr	r6, [r6, #4]
   da370:	f7fc fb1f 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da374:	b105      	cbz	r5, da378 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x1a4>
   da376:	686d      	ldr	r5, [r5, #4]
   da378:	4621      	mov	r1, r4
   da37a:	4640      	mov	r0, r8
   da37c:	f7fc fb19 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da380:	b104      	cbz	r4, da384 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x1b0>
   da382:	6864      	ldr	r4, [r4, #4]
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
   da384:	9402      	str	r4, [sp, #8]
   da386:	e88d 0120 	stmia.w	sp, {r5, r8}
   da38a:	ab18      	add	r3, sp, #96	; 0x60
   da38c:	4632      	mov	r2, r6
   da38e:	a913      	add	r1, sp, #76	; 0x4c
   da390:	a822      	add	r0, sp, #136	; 0x88
   da392:	f7fe f9f0 	bl	d8776 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_9GreaterFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   da396:	e106      	b.n	da5a6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   da398:	a818      	add	r0, sp, #96	; 0x60
   da39a:	f7fc fb0a 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da39e:	4629      	mov	r1, r5
   da3a0:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da3a2:	6876      	ldr	r6, [r6, #4]
   da3a4:	f7fc fb05 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da3a8:	b105      	cbz	r5, da3ac <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x1d8>
   da3aa:	686d      	ldr	r5, [r5, #4]
   da3ac:	4621      	mov	r1, r4
   da3ae:	a822      	add	r0, sp, #136	; 0x88
   da3b0:	f7fc faff 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da3b4:	b104      	cbz	r4, da3b8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x1e4>
   da3b6:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   da3b8:	aa22      	add	r2, sp, #136	; 0x88
   da3ba:	4641      	mov	r1, r8
   da3bc:	a818      	add	r0, sp, #96	; 0x60
   da3be:	f7fc f8da 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   da3c2:	3d08      	subs	r5, #8
   da3c4:	17c1      	asrs	r1, r0, #31
   da3c6:	f1a6 0e08 	sub.w	lr, r6, #8
   da3ca:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
   da3cc:	2200      	movs	r2, #0
   da3ce:	2300      	movs	r3, #0
   da3d0:	4282      	cmp	r2, r0
   da3d2:	eb73 0601 	sbcs.w	r6, r3, r1
   da3d6:	f280 814a 	bge.w	da66e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   da3da:	e9fe 6702 	ldrd	r6, r7, [lr, #8]!
   da3de:	e9f5 ab02 	ldrd	sl, fp, [r5, #8]!
   da3e2:	45b2      	cmp	sl, r6
   da3e4:	eb7b 0607 	sbcs.w	r6, fp, r7
   da3e8:	bfb4      	ite	lt
   da3ea:	2601      	movlt	r6, #1
   da3ec:	2600      	movge	r6, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da3ee:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   da3f0:	f804 6f01 	strb.w	r6, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da3f4:	f143 0300 	adc.w	r3, r3, #0
   da3f8:	e7ea      	b.n	da3d0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x1fc>
      }                                                                        \
    }                                                                          \
  }
TF_LITE_QUANTIZE_COMPARISON(Equal);
TF_LITE_QUANTIZE_COMPARISON(NotEqual);
TF_LITE_QUANTIZE_COMPARISON(Greater);
   da3fa:	6933      	ldr	r3, [r6, #16]
   da3fc:	68f0      	ldr	r0, [r6, #12]
   da3fe:	f1c3 0900 	rsb	r9, r3, #0
   da402:	692b      	ldr	r3, [r5, #16]
   da404:	f1c3 0800 	rsb	r8, r3, #0
   da408:	f00c fde0 	bl	e6fcc <__aeabi_f2d>
   da40c:	ec41 0b10 	vmov	d0, r0, r1
   da410:	a910      	add	r1, sp, #64	; 0x40
   da412:	a80f      	add	r0, sp, #60	; 0x3c
   da414:	f009 fd28 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   da418:	68e8      	ldr	r0, [r5, #12]
   da41a:	f00c fdd7 	bl	e6fcc <__aeabi_f2d>
   da41e:	ec41 0b10 	vmov	d0, r0, r1
   da422:	a912      	add	r1, sp, #72	; 0x48
   da424:	a811      	add	r0, sp, #68	; 0x44
   da426:	f009 fd1f 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   da42a:	2308      	movs	r3, #8
   da42c:	9322      	str	r3, [sp, #136]	; 0x88
   da42e:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   da430:	9324      	str	r3, [sp, #144]	; 0x90
   da432:	9b10      	ldr	r3, [sp, #64]	; 0x40
   da434:	9325      	str	r3, [sp, #148]	; 0x94
   da436:	9b11      	ldr	r3, [sp, #68]	; 0x44
   da438:	9327      	str	r3, [sp, #156]	; 0x9c
   da43a:	9b12      	ldr	r3, [sp, #72]	; 0x48
   da43c:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   da440:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   da444:	9328      	str	r3, [sp, #160]	; 0xa0
   da446:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   da44a:	4631      	mov	r1, r6
   da44c:	a813      	add	r0, sp, #76	; 0x4c
   da44e:	b1bf      	cbz	r7, da480 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x2ac>
   da450:	f7fc faaf 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da454:	4629      	mov	r1, r5
   da456:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da458:	6876      	ldr	r6, [r6, #4]
   da45a:	f7fc faaa 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da45e:	4621      	mov	r1, r4
   da460:	4640      	mov	r0, r8
   da462:	686d      	ldr	r5, [r5, #4]
   da464:	f7fc faa5 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da468:	b104      	cbz	r4, da46c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x298>
   da46a:	6864      	ldr	r4, [r4, #4]
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
   da46c:	9402      	str	r4, [sp, #8]
   da46e:	e88d 0120 	stmia.w	sp, {r5, r8}
   da472:	ab18      	add	r3, sp, #96	; 0x60
   da474:	4632      	mov	r2, r6
   da476:	a913      	add	r1, sp, #76	; 0x4c
   da478:	a822      	add	r0, sp, #136	; 0x88
   da47a:	f7ff fd81 	bl	d9f80 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   da47e:	e092      	b.n	da5a6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   da480:	f7fc fa97 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da484:	6873      	ldr	r3, [r6, #4]
   da486:	9305      	str	r3, [sp, #20]
   da488:	4629      	mov	r1, r5
   da48a:	a818      	add	r0, sp, #96	; 0x60
   da48c:	f7fc fa91 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da490:	4621      	mov	r1, r4
   da492:	4640      	mov	r0, r8
   da494:	f8d5 b004 	ldr.w	fp, [r5, #4]
   da498:	f7fc fa8b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da49c:	b104      	cbz	r4, da4a0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x2cc>
   da49e:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   da4a0:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   da4a2:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   da4a4:	9b24      	ldr	r3, [sp, #144]	; 0x90
   da4a6:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   da4a8:	9b25      	ldr	r3, [sp, #148]	; 0x94
   da4aa:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   da4ac:	9b26      	ldr	r3, [sp, #152]	; 0x98
   da4ae:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   da4b0:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   da4b2:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   da4b4:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   da4b6:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   da4b8:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   da4ba:	a918      	add	r1, sp, #96	; 0x60
   da4bc:	a813      	add	r0, sp, #76	; 0x4c
   da4be:	f7fc f85a 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   da4c2:	4602      	mov	r2, r0
   da4c4:	17c3      	asrs	r3, r0, #31
   da4c6:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   da4ca:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da4cc:	f04f 0800 	mov.w	r8, #0
   da4d0:	f04f 0900 	mov.w	r9, #0
   da4d4:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   da4d8:	4590      	cmp	r8, r2
   da4da:	eb79 0303 	sbcs.w	r3, r9, r3
   da4de:	f280 80b4 	bge.w	da64a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x476>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da4e2:	f81b 5008 	ldrb.w	r5, [fp, r8]
   da4e6:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   da4e8:	9a08      	ldr	r2, [sp, #32]
   da4ea:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da4ec:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   da4ee:	9b05      	ldr	r3, [sp, #20]
   da4f0:	f813 0008 	ldrb.w	r0, [r3, r8]
   da4f4:	9b06      	ldr	r3, [sp, #24]
   da4f6:	4418      	add	r0, r3
   da4f8:	40b8      	lsls	r0, r7
   da4fa:	f7fc f879 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da4fe:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   da500:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   da502:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   da504:	990a      	ldr	r1, [sp, #40]	; 0x28
   da506:	4628      	mov	r0, r5
   da508:	f7fc f872 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   da50c:	4582      	cmp	sl, r0
   da50e:	bfd4      	ite	le
   da510:	2000      	movle	r0, #0
   da512:	2001      	movgt	r0, #1
   da514:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da518:	f118 0801 	adds.w	r8, r8, #1
   da51c:	f149 0900 	adc.w	r9, r9, #0
   da520:	e7d8      	b.n	da4d4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x300>
   da522:	6933      	ldr	r3, [r6, #16]
   da524:	68f0      	ldr	r0, [r6, #12]
   da526:	f1c3 0900 	rsb	r9, r3, #0
   da52a:	692b      	ldr	r3, [r5, #16]
   da52c:	f1c3 0800 	rsb	r8, r3, #0
   da530:	f00c fd4c 	bl	e6fcc <__aeabi_f2d>
   da534:	ec41 0b10 	vmov	d0, r0, r1
   da538:	a910      	add	r1, sp, #64	; 0x40
   da53a:	a80f      	add	r0, sp, #60	; 0x3c
   da53c:	f009 fc94 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   da540:	68e8      	ldr	r0, [r5, #12]
   da542:	f00c fd43 	bl	e6fcc <__aeabi_f2d>
   da546:	ec41 0b10 	vmov	d0, r0, r1
   da54a:	a912      	add	r1, sp, #72	; 0x48
   da54c:	a811      	add	r0, sp, #68	; 0x44
   da54e:	f009 fc8b 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   da552:	2308      	movs	r3, #8
   da554:	9322      	str	r3, [sp, #136]	; 0x88
   da556:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   da558:	9324      	str	r3, [sp, #144]	; 0x90
   da55a:	9b10      	ldr	r3, [sp, #64]	; 0x40
   da55c:	9325      	str	r3, [sp, #148]	; 0x94
   da55e:	9b11      	ldr	r3, [sp, #68]	; 0x44
   da560:	9327      	str	r3, [sp, #156]	; 0x9c
   da562:	9b12      	ldr	r3, [sp, #72]	; 0x48
   da564:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   da568:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   da56c:	9328      	str	r3, [sp, #160]	; 0xa0
   da56e:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   da572:	4631      	mov	r1, r6
   da574:	a813      	add	r0, sp, #76	; 0x4c
   da576:	b1c7      	cbz	r7, da5aa <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3d6>
   da578:	f7fc fa1b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da57c:	4629      	mov	r1, r5
   da57e:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da580:	6876      	ldr	r6, [r6, #4]
   da582:	f7fc fa16 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da586:	4621      	mov	r1, r4
   da588:	4640      	mov	r0, r8
   da58a:	686d      	ldr	r5, [r5, #4]
   da58c:	f7fc fa11 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da590:	b104      	cbz	r4, da594 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3c0>
   da592:	6864      	ldr	r4, [r4, #4]
        op_params, input1_shape, input1_data, input2_shape, input2_data,       \
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
   da594:	9402      	str	r4, [sp, #8]
   da596:	e88d 0120 	stmia.w	sp, {r5, r8}
   da59a:	ab18      	add	r3, sp, #96	; 0x60
   da59c:	4632      	mov	r2, r6
   da59e:	a913      	add	r1, sp, #76	; 0x4c
   da5a0:	a822      	add	r0, sp, #136	; 0x88
   da5a2:	f7ff fd82 	bl	da0aa <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_9GreaterFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   da5a6:	4640      	mov	r0, r8
   da5a8:	e050      	b.n	da64c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x478>
   da5aa:	f7fc fa02 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da5ae:	6873      	ldr	r3, [r6, #4]
   da5b0:	9305      	str	r3, [sp, #20]
   da5b2:	4629      	mov	r1, r5
   da5b4:	a818      	add	r0, sp, #96	; 0x60
   da5b6:	f7fc f9fc 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da5ba:	4621      	mov	r1, r4
   da5bc:	4640      	mov	r0, r8
   da5be:	f8d5 b004 	ldr.w	fp, [r5, #4]
   da5c2:	f7fc f9f6 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da5c6:	b104      	cbz	r4, da5ca <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x3f6>
   da5c8:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   da5ca:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   da5cc:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   da5ce:	9b24      	ldr	r3, [sp, #144]	; 0x90
   da5d0:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   da5d2:	9b25      	ldr	r3, [sp, #148]	; 0x94
   da5d4:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   da5d6:	9b26      	ldr	r3, [sp, #152]	; 0x98
   da5d8:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   da5da:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   da5dc:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   da5de:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   da5e0:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   da5e2:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   da5e4:	a918      	add	r1, sp, #96	; 0x60
   da5e6:	a813      	add	r0, sp, #76	; 0x4c
   da5e8:	f7fb ffc5 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   da5ec:	4602      	mov	r2, r0
   da5ee:	17c3      	asrs	r3, r0, #31
   da5f0:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   da5f4:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da5f6:	f04f 0800 	mov.w	r8, #0
   da5fa:	f04f 0900 	mov.w	r9, #0
   da5fe:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   da602:	4590      	cmp	r8, r2
   da604:	eb79 0303 	sbcs.w	r3, r9, r3
   da608:	da1f      	bge.n	da64a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x476>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da60a:	f91b 5008 	ldrsb.w	r5, [fp, r8]
   da60e:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   da610:	9a08      	ldr	r2, [sp, #32]
   da612:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da614:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   da616:	9b05      	ldr	r3, [sp, #20]
   da618:	f913 0008 	ldrsb.w	r0, [r3, r8]
   da61c:	9b06      	ldr	r3, [sp, #24]
   da61e:	4418      	add	r0, r3
   da620:	40b8      	lsls	r0, r7
   da622:	f7fb ffe5 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da626:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   da628:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   da62a:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   da62c:	990a      	ldr	r1, [sp, #40]	; 0x28
   da62e:	4628      	mov	r0, r5
   da630:	f7fb ffde 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   da634:	4582      	cmp	sl, r0
   da636:	bfd4      	ite	le
   da638:	2000      	movle	r0, #0
   da63a:	2001      	movgt	r0, #1
   da63c:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da640:	f118 0801 	adds.w	r8, r8, #1
   da644:	f149 0900 	adc.w	r9, r9, #0
   da648:	e7d9      	b.n	da5fe <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x42a>
   da64a:	a81d      	add	r0, sp, #116	; 0x74
   da64c:	f7fb ff01 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   da650:	a818      	add	r0, sp, #96	; 0x60
   da652:	f7fb fefe 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   da656:	a813      	add	r0, sp, #76	; 0x4c
   da658:	f7fb fefb 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
   da65c:	2000      	movs	r0, #0
   da65e:	e00e      	b.n	da67e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x4aa>
                                   requires_broadcast);
      break;
    default:
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
   da660:	4650      	mov	r0, sl
   da662:	f8da 3014 	ldr.w	r3, [sl, #20]
   da666:	4907      	ldr	r1, [pc, #28]	; (da684 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x4b0>)
   da668:	4798      	blx	r3
      return kTfLiteError;
   da66a:	2001      	movs	r0, #1
   da66c:	e007      	b.n	da67e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x4aa>
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, Greater, requires_broadcast);
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, Greater, requires_broadcast);
   da66e:	a822      	add	r0, sp, #136	; 0x88
   da670:	f7fb feef 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   da674:	4640      	mov	r0, r8
   da676:	f7fb feec 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   da67a:	a818      	add	r0, sp, #96	; 0x60
   da67c:	e7ec      	b.n	da658 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_111GreaterEvalEP13TfLiteContextP10TfLiteNode+0x484>
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   da67e:	b02b      	add	sp, #172	; 0xac
   da680:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   da684:	000e99bf 	.word	0x000e99bf

000da688 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   da688:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   da68c:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   da68e:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   da690:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   da692:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   da694:	9208      	str	r2, [sp, #32]
   da696:	4604      	mov	r4, r0
   da698:	460e      	mov	r6, r1
   da69a:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   da69c:	dd01      	ble.n	da6a2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   da69e:	f009 fe55 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   da6a2:	683b      	ldr	r3, [r7, #0]
   da6a4:	2b04      	cmp	r3, #4
   da6a6:	dcfa      	bgt.n	da69e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   da6a8:	6813      	ldr	r3, [r2, #0]
   da6aa:	2b04      	cmp	r3, #4
   da6ac:	dcf7      	bgt.n	da69e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   da6ae:	2301      	movs	r3, #1
   da6b0:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   da6b2:	ad10      	add	r5, sp, #64	; 0x40
   da6b4:	a80b      	add	r0, sp, #44	; 0x2c
   da6b6:	f7fb ff10 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   da6ba:	ab18      	add	r3, sp, #96	; 0x60
   da6bc:	462a      	mov	r2, r5
   da6be:	4639      	mov	r1, r7
   da6c0:	4630      	mov	r0, r6
   da6c2:	f7fc fa1f 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   da6c6:	6863      	ldr	r3, [r4, #4]
   da6c8:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   da6ca:	68a3      	ldr	r3, [r4, #8]
   da6cc:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   da6ce:	68e3      	ldr	r3, [r4, #12]
   da6d0:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   da6d2:	6923      	ldr	r3, [r4, #16]
   da6d4:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   da6d6:	6963      	ldr	r3, [r4, #20]
   da6d8:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   da6da:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   da6dc:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   da6e0:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da6e2:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da6e4:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da6e6:	2100      	movs	r1, #0
   da6e8:	a80b      	add	r0, sp, #44	; 0x2c
   da6ea:	f7fb febd 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   da6ee:	4284      	cmp	r4, r0
   da6f0:	da59      	bge.n	da7a6 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   da6f2:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   da6f4:	af0b      	add	r7, sp, #44	; 0x2c
   da6f6:	2101      	movs	r1, #1
   da6f8:	4638      	mov	r0, r7
   da6fa:	f7fb feb5 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   da6fe:	4285      	cmp	r5, r0
   da700:	da4f      	bge.n	da7a2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   da702:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   da704:	2102      	movs	r1, #2
   da706:	4638      	mov	r0, r7
   da708:	f7fb feae 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   da70c:	4286      	cmp	r6, r0
   da70e:	da46      	bge.n	da79e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   da710:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   da714:	2103      	movs	r1, #3
   da716:	4638      	mov	r0, r7
   da718:	f7fb fea6 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   da71c:	4580      	cmp	r8, r0
   da71e:	da3c      	bge.n	da79a <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da720:	f8cd 8000 	str.w	r8, [sp]
   da724:	4633      	mov	r3, r6
   da726:	462a      	mov	r2, r5
   da728:	4621      	mov	r1, r4
   da72a:	9809      	ldr	r0, [sp, #36]	; 0x24
   da72c:	f7fb ffb2 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   da730:	9b08      	ldr	r3, [sp, #32]
   da732:	f813 9000 	ldrb.w	r9, [r3, r0]
   da736:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   da738:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da73c:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   da73e:	462a      	mov	r2, r5
   da740:	4633      	mov	r3, r6
   da742:	4621      	mov	r1, r4
   da744:	a818      	add	r0, sp, #96	; 0x60
   da746:	f7fb ffa5 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da74a:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da74c:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da74e:	f813 b000 	ldrb.w	fp, [r3, r0]
   da752:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da754:	9903      	ldr	r1, [sp, #12]
   da756:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da75a:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da75c:	f7fb ff48 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da760:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da764:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   da766:	9a07      	ldr	r2, [sp, #28]
   da768:	9906      	ldr	r1, [sp, #24]
   da76a:	4658      	mov	r0, fp
   da76c:	f7fb ff40 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   da770:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   da774:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   da776:	4633      	mov	r3, r6
   da778:	462a      	mov	r2, r5
   da77a:	4621      	mov	r1, r4
   da77c:	4638      	mov	r0, r7
   da77e:	f7fb fed8 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   da782:	45d9      	cmp	r9, fp
   da784:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   da786:	bfb4      	ite	lt
   da788:	f04f 0900 	movlt.w	r9, #0
   da78c:	f04f 0901 	movge.w	r9, #1
   da790:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   da794:	f108 0801 	add.w	r8, r8, #1
   da798:	e7bc      	b.n	da714 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   da79a:	3601      	adds	r6, #1
   da79c:	e7b2      	b.n	da704 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   da79e:	3501      	adds	r5, #1
   da7a0:	e7a8      	b.n	da6f4 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da7a2:	3401      	adds	r4, #1
   da7a4:	e79f      	b.n	da6e6 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   da7a6:	a80b      	add	r0, sp, #44	; 0x2c
   da7a8:	f7fb fe53 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   da7ac:	b021      	add	sp, #132	; 0x84
   da7ae:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000da7b2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   da7b2:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   da7b6:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   da7b8:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   da7ba:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   da7bc:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   da7be:	9208      	str	r2, [sp, #32]
   da7c0:	4604      	mov	r4, r0
   da7c2:	460e      	mov	r6, r1
   da7c4:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   da7c6:	dd01      	ble.n	da7cc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   da7c8:	f009 fdc0 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   da7cc:	683b      	ldr	r3, [r7, #0]
   da7ce:	2b04      	cmp	r3, #4
   da7d0:	dcfa      	bgt.n	da7c8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   da7d2:	6813      	ldr	r3, [r2, #0]
   da7d4:	2b04      	cmp	r3, #4
   da7d6:	dcf7      	bgt.n	da7c8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   da7d8:	2301      	movs	r3, #1
   da7da:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   da7dc:	ad10      	add	r5, sp, #64	; 0x40
   da7de:	a80b      	add	r0, sp, #44	; 0x2c
   da7e0:	f7fb fe7b 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   da7e4:	ab18      	add	r3, sp, #96	; 0x60
   da7e6:	462a      	mov	r2, r5
   da7e8:	4639      	mov	r1, r7
   da7ea:	4630      	mov	r0, r6
   da7ec:	f7fc f98a 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   da7f0:	6863      	ldr	r3, [r4, #4]
   da7f2:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   da7f4:	68a3      	ldr	r3, [r4, #8]
   da7f6:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   da7f8:	68e3      	ldr	r3, [r4, #12]
   da7fa:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   da7fc:	6923      	ldr	r3, [r4, #16]
   da7fe:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   da800:	6963      	ldr	r3, [r4, #20]
   da802:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   da804:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   da806:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   da80a:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da80c:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da80e:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da810:	2100      	movs	r1, #0
   da812:	a80b      	add	r0, sp, #44	; 0x2c
   da814:	f7fb fe28 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   da818:	4284      	cmp	r4, r0
   da81a:	da59      	bge.n	da8d0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   da81c:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   da81e:	af0b      	add	r7, sp, #44	; 0x2c
   da820:	2101      	movs	r1, #1
   da822:	4638      	mov	r0, r7
   da824:	f7fb fe20 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   da828:	4285      	cmp	r5, r0
   da82a:	da4f      	bge.n	da8cc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   da82c:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   da82e:	2102      	movs	r1, #2
   da830:	4638      	mov	r0, r7
   da832:	f7fb fe19 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   da836:	4286      	cmp	r6, r0
   da838:	da46      	bge.n	da8c8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   da83a:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   da83e:	2103      	movs	r1, #3
   da840:	4638      	mov	r0, r7
   da842:	f7fb fe11 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   da846:	4580      	cmp	r8, r0
   da848:	da3c      	bge.n	da8c4 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da84a:	f8cd 8000 	str.w	r8, [sp]
   da84e:	4633      	mov	r3, r6
   da850:	462a      	mov	r2, r5
   da852:	4621      	mov	r1, r4
   da854:	9809      	ldr	r0, [sp, #36]	; 0x24
   da856:	f7fb ff1d 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   da85a:	9b08      	ldr	r3, [sp, #32]
   da85c:	f913 9000 	ldrsb.w	r9, [r3, r0]
   da860:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   da862:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   da866:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   da868:	462a      	mov	r2, r5
   da86a:	4633      	mov	r3, r6
   da86c:	4621      	mov	r1, r4
   da86e:	a818      	add	r0, sp, #96	; 0x60
   da870:	f7fb ff10 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da874:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da876:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da878:	f913 b000 	ldrsb.w	fp, [r3, r0]
   da87c:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da87e:	9903      	ldr	r1, [sp, #12]
   da880:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da884:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da886:	f7fb feb3 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   da88a:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   da88e:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   da890:	9a07      	ldr	r2, [sp, #28]
   da892:	9906      	ldr	r1, [sp, #24]
   da894:	4658      	mov	r0, fp
   da896:	f7fb feab 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   da89a:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   da89e:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   da8a0:	4633      	mov	r3, r6
   da8a2:	462a      	mov	r2, r5
   da8a4:	4621      	mov	r1, r4
   da8a6:	4638      	mov	r0, r7
   da8a8:	f7fb fe43 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   da8ac:	45d9      	cmp	r9, fp
   da8ae:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   da8b0:	bfb4      	ite	lt
   da8b2:	f04f 0900 	movlt.w	r9, #0
   da8b6:	f04f 0901 	movge.w	r9, #1
   da8ba:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   da8be:	f108 0801 	add.w	r8, r8, #1
   da8c2:	e7bc      	b.n	da83e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   da8c4:	3601      	adds	r6, #1
   da8c6:	e7b2      	b.n	da82e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   da8c8:	3501      	adds	r5, #1
   da8ca:	e7a8      	b.n	da81e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   da8cc:	3401      	adds	r4, #1
   da8ce:	e79f      	b.n	da810 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   da8d0:	a80b      	add	r0, sp, #44	; 0x2c
   da8d2:	f7fb fdbe 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   da8d6:	b021      	add	sp, #132	; 0x84
   da8d8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000da8dc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus GreaterEqualEval(TfLiteContext* context, TfLiteNode* node) {
   da8dc:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   da8e0:	680a      	ldr	r2, [r1, #0]
   da8e2:	f8d0 9008 	ldr.w	r9, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   da8e6:	6895      	ldr	r5, [r2, #8]
   da8e8:	4682      	mov	sl, r0
   da8ea:	6850      	ldr	r0, [r2, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   da8ec:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   da8ee:	2338      	movs	r3, #56	; 0x38
   da8f0:	fb03 f800 	mul.w	r8, r3, r0
   da8f4:	fb03 9505 	mla	r5, r3, r5, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   da8f8:	6854      	ldr	r4, [r2, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   da8fa:	eb09 0608 	add.w	r6, r9, r8
   da8fe:	b0ab      	sub	sp, #172	; 0xac
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   da900:	4629      	mov	r1, r5
   da902:	4630      	mov	r0, r6
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   da904:	fb03 9404 	mla	r4, r3, r4, r9
   da908:	f009 fa3a 	bl	e3d80 <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
  switch (input1->type) {
   da90c:	f819 2008 	ldrb.w	r2, [r9, r8]

TfLiteStatus GreaterEqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   da910:	f080 0001 	eor.w	r0, r0, #1
  switch (input1->type) {
   da914:	1e53      	subs	r3, r2, #1

TfLiteStatus GreaterEqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   da916:	b2c7      	uxtb	r7, r0
  switch (input1->type) {
   da918:	2b08      	cmp	r3, #8
   da91a:	f200 8225 	bhi.w	dad68 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x48c>
   da91e:	e8df f013 	tbh	[pc, r3, lsl #1]
   da922:	0009      	.short	0x0009
   da924:	00f00057 	.word	0x00f00057
   da928:	022300a1 	.word	0x022300a1
   da92c:	02230223 	.word	0x02230223
   da930:	01840223 	.word	0x01840223
   da934:	f10d 0874 	add.w	r8, sp, #116	; 0x74
    case kTfLiteFloat32:
      TF_LITE_COMPARISON(float, GreaterEqual, requires_broadcast);
   da938:	4631      	mov	r1, r6
   da93a:	b1cf      	cbz	r7, da970 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x94>
   da93c:	a813      	add	r0, sp, #76	; 0x4c
   da93e:	f7fc f838 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da942:	4629      	mov	r1, r5
   da944:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da946:	6876      	ldr	r6, [r6, #4]
   da948:	f7fc f833 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da94c:	b105      	cbz	r5, da950 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x74>
   da94e:	686d      	ldr	r5, [r5, #4]
   da950:	4621      	mov	r1, r4
   da952:	4640      	mov	r0, r8
   da954:	f7fc f82d 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da958:	b104      	cbz	r4, da95c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x80>
   da95a:	6864      	ldr	r4, [r4, #4]
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
   da95c:	9402      	str	r4, [sp, #8]
   da95e:	e88d 0120 	stmia.w	sp, {r5, r8}
   da962:	ab18      	add	r3, sp, #96	; 0x60
   da964:	4632      	mov	r2, r6
   da966:	a913      	add	r1, sp, #76	; 0x4c
   da968:	a822      	add	r0, sp, #136	; 0x88
   da96a:	f7fd ff76 	bl	d885a <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_14GreaterEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   da96e:	e19e      	b.n	dacae <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   da970:	a818      	add	r0, sp, #96	; 0x60
   da972:	f7fc f81e 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da976:	4629      	mov	r1, r5
   da978:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da97a:	6876      	ldr	r6, [r6, #4]
   da97c:	f7fc f819 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da980:	b105      	cbz	r5, da984 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0xa8>
   da982:	686d      	ldr	r5, [r5, #4]
   da984:	4621      	mov	r1, r4
   da986:	a822      	add	r0, sp, #136	; 0x88
   da988:	f7fc f813 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da98c:	b104      	cbz	r4, da990 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0xb4>
   da98e:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   da990:	4641      	mov	r1, r8
   da992:	aa22      	add	r2, sp, #136	; 0x88
   da994:	a818      	add	r0, sp, #96	; 0x60
   da996:	f7fb fdee 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   da99a:	3c01      	subs	r4, #1
   da99c:	4633      	mov	r3, r6
   da99e:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   da9a0:	2600      	movs	r6, #0
   da9a2:	2700      	movs	r7, #0
   da9a4:	4286      	cmp	r6, r0
   da9a6:	eb77 0201 	sbcs.w	r2, r7, r1
   da9aa:	f280 81e4 	bge.w	dad76 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   da9ae:	ecb3 7a01 	vldmia	r3!, {s14}
   da9b2:	ecf5 7a01 	vldmia	r5!, {s15}
   da9b6:	eeb4 7ae7 	vcmpe.f32	s14, s15
   da9ba:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   da9be:	bfac      	ite	ge
   da9c0:	2201      	movge	r2, #1
   da9c2:	2200      	movlt	r2, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da9c4:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   da9c6:	f804 2f01 	strb.w	r2, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   da9ca:	f147 0700 	adc.w	r7, r7, #0
   da9ce:	e7e9      	b.n	da9a4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0xc8>
   da9d0:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, GreaterEqual, requires_broadcast);
   da9d4:	4631      	mov	r1, r6
   da9d6:	b1cf      	cbz	r7, daa0c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x130>
   da9d8:	a813      	add	r0, sp, #76	; 0x4c
   da9da:	f7fb ffea 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da9de:	4629      	mov	r1, r5
   da9e0:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   da9e2:	6876      	ldr	r6, [r6, #4]
   da9e4:	f7fb ffe5 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   da9e8:	b105      	cbz	r5, da9ec <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x110>
   da9ea:	686d      	ldr	r5, [r5, #4]
   da9ec:	4621      	mov	r1, r4
   da9ee:	4640      	mov	r0, r8
   da9f0:	f7fb ffdf 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   da9f4:	b104      	cbz	r4, da9f8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x11c>
   da9f6:	6864      	ldr	r4, [r4, #4]
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
   da9f8:	9402      	str	r4, [sp, #8]
   da9fa:	e88d 0120 	stmia.w	sp, {r5, r8}
   da9fe:	ab18      	add	r3, sp, #96	; 0x60
   daa00:	4632      	mov	r2, r6
   daa02:	a913      	add	r1, sp, #76	; 0x4c
   daa04:	a822      	add	r0, sp, #136	; 0x88
   daa06:	f7fd ff9b 	bl	d8940 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   daa0a:	e150      	b.n	dacae <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   daa0c:	a818      	add	r0, sp, #96	; 0x60
   daa0e:	f7fb ffd0 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   daa12:	4629      	mov	r1, r5
   daa14:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   daa16:	6876      	ldr	r6, [r6, #4]
   daa18:	f7fb ffcb 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   daa1c:	b105      	cbz	r5, daa20 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x144>
   daa1e:	686d      	ldr	r5, [r5, #4]
   daa20:	4621      	mov	r1, r4
   daa22:	a822      	add	r0, sp, #136	; 0x88
   daa24:	f7fb ffc5 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   daa28:	b104      	cbz	r4, daa2c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x150>
   daa2a:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   daa2c:	aa22      	add	r2, sp, #136	; 0x88
   daa2e:	4641      	mov	r1, r8
   daa30:	a818      	add	r0, sp, #96	; 0x60
   daa32:	f7fb fda0 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   daa36:	3c01      	subs	r4, #1
   daa38:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   daa3a:	2200      	movs	r2, #0
   daa3c:	2300      	movs	r3, #0
   daa3e:	4282      	cmp	r2, r0
   daa40:	eb73 0701 	sbcs.w	r7, r3, r1
   daa44:	f280 8197 	bge.w	dad76 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   daa48:	f856 7022 	ldr.w	r7, [r6, r2, lsl #2]
   daa4c:	f855 e022 	ldr.w	lr, [r5, r2, lsl #2]
   daa50:	4577      	cmp	r7, lr
   daa52:	bfb4      	ite	lt
   daa54:	2700      	movlt	r7, #0
   daa56:	2701      	movge	r7, #1
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   daa58:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   daa5a:	f804 7f01 	strb.w	r7, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   daa5e:	f143 0300 	adc.w	r3, r3, #0
   daa62:	e7ec      	b.n	daa3e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x162>
   daa64:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, GreaterEqual, requires_broadcast);
   daa68:	4631      	mov	r1, r6
   daa6a:	b1cf      	cbz	r7, daaa0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x1c4>
   daa6c:	a813      	add	r0, sp, #76	; 0x4c
   daa6e:	f7fb ffa0 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   daa72:	4629      	mov	r1, r5
   daa74:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   daa76:	6876      	ldr	r6, [r6, #4]
   daa78:	f7fb ff9b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   daa7c:	b105      	cbz	r5, daa80 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x1a4>
   daa7e:	686d      	ldr	r5, [r5, #4]
   daa80:	4621      	mov	r1, r4
   daa82:	4640      	mov	r0, r8
   daa84:	f7fb ff95 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   daa88:	b104      	cbz	r4, daa8c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x1b0>
   daa8a:	6864      	ldr	r4, [r4, #4]
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
   daa8c:	9402      	str	r4, [sp, #8]
   daa8e:	e88d 0120 	stmia.w	sp, {r5, r8}
   daa92:	ab18      	add	r3, sp, #96	; 0x60
   daa94:	4632      	mov	r2, r6
   daa96:	a913      	add	r1, sp, #76	; 0x4c
   daa98:	a822      	add	r0, sp, #136	; 0x88
   daa9a:	f7fd ffbd 	bl	d8a18 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_14GreaterEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   daa9e:	e106      	b.n	dacae <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   daaa0:	a818      	add	r0, sp, #96	; 0x60
   daaa2:	f7fb ff86 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   daaa6:	4629      	mov	r1, r5
   daaa8:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   daaaa:	6876      	ldr	r6, [r6, #4]
   daaac:	f7fb ff81 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   daab0:	b105      	cbz	r5, daab4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x1d8>
   daab2:	686d      	ldr	r5, [r5, #4]
   daab4:	4621      	mov	r1, r4
   daab6:	a822      	add	r0, sp, #136	; 0x88
   daab8:	f7fb ff7b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   daabc:	b104      	cbz	r4, daac0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x1e4>
   daabe:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   daac0:	aa22      	add	r2, sp, #136	; 0x88
   daac2:	4641      	mov	r1, r8
   daac4:	a818      	add	r0, sp, #96	; 0x60
   daac6:	f7fb fd56 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   daaca:	3d08      	subs	r5, #8
   daacc:	17c1      	asrs	r1, r0, #31
   daace:	f1a6 0e08 	sub.w	lr, r6, #8
   daad2:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
   daad4:	2200      	movs	r2, #0
   daad6:	2300      	movs	r3, #0
   daad8:	4282      	cmp	r2, r0
   daada:	eb73 0601 	sbcs.w	r6, r3, r1
   daade:	f280 814a 	bge.w	dad76 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   daae2:	e9fe ab02 	ldrd	sl, fp, [lr, #8]!
   daae6:	e9f5 6702 	ldrd	r6, r7, [r5, #8]!
   daaea:	45b2      	cmp	sl, r6
   daaec:	eb7b 0607 	sbcs.w	r6, fp, r7
   daaf0:	bfac      	ite	ge
   daaf2:	2601      	movge	r6, #1
   daaf4:	2600      	movlt	r6, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   daaf6:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   daaf8:	f804 6f01 	strb.w	r6, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   daafc:	f143 0300 	adc.w	r3, r3, #0
   dab00:	e7ea      	b.n	daad8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x1fc>
    }                                                                          \
  }
TF_LITE_QUANTIZE_COMPARISON(Equal);
TF_LITE_QUANTIZE_COMPARISON(NotEqual);
TF_LITE_QUANTIZE_COMPARISON(Greater);
TF_LITE_QUANTIZE_COMPARISON(GreaterEqual);
   dab02:	6933      	ldr	r3, [r6, #16]
   dab04:	68f0      	ldr	r0, [r6, #12]
   dab06:	f1c3 0900 	rsb	r9, r3, #0
   dab0a:	692b      	ldr	r3, [r5, #16]
   dab0c:	f1c3 0800 	rsb	r8, r3, #0
   dab10:	f00c fa5c 	bl	e6fcc <__aeabi_f2d>
   dab14:	ec41 0b10 	vmov	d0, r0, r1
   dab18:	a910      	add	r1, sp, #64	; 0x40
   dab1a:	a80f      	add	r0, sp, #60	; 0x3c
   dab1c:	f009 f9a4 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   dab20:	68e8      	ldr	r0, [r5, #12]
   dab22:	f00c fa53 	bl	e6fcc <__aeabi_f2d>
   dab26:	ec41 0b10 	vmov	d0, r0, r1
   dab2a:	a912      	add	r1, sp, #72	; 0x48
   dab2c:	a811      	add	r0, sp, #68	; 0x44
   dab2e:	f009 f99b 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   dab32:	2308      	movs	r3, #8
   dab34:	9322      	str	r3, [sp, #136]	; 0x88
   dab36:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   dab38:	9324      	str	r3, [sp, #144]	; 0x90
   dab3a:	9b10      	ldr	r3, [sp, #64]	; 0x40
   dab3c:	9325      	str	r3, [sp, #148]	; 0x94
   dab3e:	9b11      	ldr	r3, [sp, #68]	; 0x44
   dab40:	9327      	str	r3, [sp, #156]	; 0x9c
   dab42:	9b12      	ldr	r3, [sp, #72]	; 0x48
   dab44:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   dab48:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   dab4c:	9328      	str	r3, [sp, #160]	; 0xa0
   dab4e:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   dab52:	4631      	mov	r1, r6
   dab54:	a813      	add	r0, sp, #76	; 0x4c
   dab56:	b1bf      	cbz	r7, dab88 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x2ac>
   dab58:	f7fb ff2b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dab5c:	4629      	mov	r1, r5
   dab5e:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dab60:	6876      	ldr	r6, [r6, #4]
   dab62:	f7fb ff26 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dab66:	4621      	mov	r1, r4
   dab68:	4640      	mov	r0, r8
   dab6a:	686d      	ldr	r5, [r5, #4]
   dab6c:	f7fb ff21 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dab70:	b104      	cbz	r4, dab74 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x298>
   dab72:	6864      	ldr	r4, [r4, #4]
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
   dab74:	9402      	str	r4, [sp, #8]
   dab76:	e88d 0120 	stmia.w	sp, {r5, r8}
   dab7a:	ab18      	add	r3, sp, #96	; 0x60
   dab7c:	4632      	mov	r2, r6
   dab7e:	a913      	add	r1, sp, #76	; 0x4c
   dab80:	a822      	add	r0, sp, #136	; 0x88
   dab82:	f7ff fd81 	bl	da688 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   dab86:	e092      	b.n	dacae <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   dab88:	f7fb ff13 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dab8c:	6873      	ldr	r3, [r6, #4]
   dab8e:	9305      	str	r3, [sp, #20]
   dab90:	4629      	mov	r1, r5
   dab92:	a818      	add	r0, sp, #96	; 0x60
   dab94:	f7fb ff0d 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dab98:	4621      	mov	r1, r4
   dab9a:	4640      	mov	r0, r8
   dab9c:	f8d5 b004 	ldr.w	fp, [r5, #4]
   daba0:	f7fb ff07 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   daba4:	b104      	cbz	r4, daba8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x2cc>
   daba6:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   daba8:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   dabaa:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   dabac:	9b24      	ldr	r3, [sp, #144]	; 0x90
   dabae:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   dabb0:	9b25      	ldr	r3, [sp, #148]	; 0x94
   dabb2:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   dabb4:	9b26      	ldr	r3, [sp, #152]	; 0x98
   dabb6:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   dabb8:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   dabba:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   dabbc:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   dabbe:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   dabc0:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   dabc2:	a918      	add	r1, sp, #96	; 0x60
   dabc4:	a813      	add	r0, sp, #76	; 0x4c
   dabc6:	f7fb fcd6 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   dabca:	4602      	mov	r2, r0
   dabcc:	17c3      	asrs	r3, r0, #31
   dabce:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   dabd2:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   dabd4:	f04f 0800 	mov.w	r8, #0
   dabd8:	f04f 0900 	mov.w	r9, #0
   dabdc:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   dabe0:	4590      	cmp	r8, r2
   dabe2:	eb79 0303 	sbcs.w	r3, r9, r3
   dabe6:	f280 80b4 	bge.w	dad52 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x476>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dabea:	f81b 5008 	ldrb.w	r5, [fp, r8]
   dabee:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dabf0:	9a08      	ldr	r2, [sp, #32]
   dabf2:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dabf4:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dabf6:	9b05      	ldr	r3, [sp, #20]
   dabf8:	f813 0008 	ldrb.w	r0, [r3, r8]
   dabfc:	9b06      	ldr	r3, [sp, #24]
   dabfe:	4418      	add	r0, r3
   dac00:	40b8      	lsls	r0, r7
   dac02:	f7fb fcf5 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dac06:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dac08:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   dac0a:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   dac0c:	990a      	ldr	r1, [sp, #40]	; 0x28
   dac0e:	4628      	mov	r0, r5
   dac10:	f7fb fcee 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   dac14:	4582      	cmp	sl, r0
   dac16:	bfb4      	ite	lt
   dac18:	2000      	movlt	r0, #0
   dac1a:	2001      	movge	r0, #1
   dac1c:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   dac20:	f118 0801 	adds.w	r8, r8, #1
   dac24:	f149 0900 	adc.w	r9, r9, #0
   dac28:	e7d8      	b.n	dabdc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x300>
   dac2a:	6933      	ldr	r3, [r6, #16]
   dac2c:	68f0      	ldr	r0, [r6, #12]
   dac2e:	f1c3 0900 	rsb	r9, r3, #0
   dac32:	692b      	ldr	r3, [r5, #16]
   dac34:	f1c3 0800 	rsb	r8, r3, #0
   dac38:	f00c f9c8 	bl	e6fcc <__aeabi_f2d>
   dac3c:	ec41 0b10 	vmov	d0, r0, r1
   dac40:	a910      	add	r1, sp, #64	; 0x40
   dac42:	a80f      	add	r0, sp, #60	; 0x3c
   dac44:	f009 f910 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   dac48:	68e8      	ldr	r0, [r5, #12]
   dac4a:	f00c f9bf 	bl	e6fcc <__aeabi_f2d>
   dac4e:	ec41 0b10 	vmov	d0, r0, r1
   dac52:	a912      	add	r1, sp, #72	; 0x48
   dac54:	a811      	add	r0, sp, #68	; 0x44
   dac56:	f009 f907 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   dac5a:	2308      	movs	r3, #8
   dac5c:	9322      	str	r3, [sp, #136]	; 0x88
   dac5e:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   dac60:	9324      	str	r3, [sp, #144]	; 0x90
   dac62:	9b10      	ldr	r3, [sp, #64]	; 0x40
   dac64:	9325      	str	r3, [sp, #148]	; 0x94
   dac66:	9b11      	ldr	r3, [sp, #68]	; 0x44
   dac68:	9327      	str	r3, [sp, #156]	; 0x9c
   dac6a:	9b12      	ldr	r3, [sp, #72]	; 0x48
   dac6c:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   dac70:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   dac74:	9328      	str	r3, [sp, #160]	; 0xa0
   dac76:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   dac7a:	4631      	mov	r1, r6
   dac7c:	a813      	add	r0, sp, #76	; 0x4c
   dac7e:	b1c7      	cbz	r7, dacb2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d6>
   dac80:	f7fb fe97 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dac84:	4629      	mov	r1, r5
   dac86:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dac88:	6876      	ldr	r6, [r6, #4]
   dac8a:	f7fb fe92 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dac8e:	4621      	mov	r1, r4
   dac90:	4640      	mov	r0, r8
   dac92:	686d      	ldr	r5, [r5, #4]
   dac94:	f7fb fe8d 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dac98:	b104      	cbz	r4, dac9c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3c0>
   dac9a:	6864      	ldr	r4, [r4, #4]
        output_shape, output_data);                                            \
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
   dac9c:	9402      	str	r4, [sp, #8]
   dac9e:	e88d 0120 	stmia.w	sp, {r5, r8}
   daca2:	ab18      	add	r3, sp, #96	; 0x60
   daca4:	4632      	mov	r2, r6
   daca6:	a913      	add	r1, sp, #76	; 0x4c
   daca8:	a822      	add	r0, sp, #136	; 0x88
   dacaa:	f7ff fd82 	bl	da7b2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_14GreaterEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   dacae:	4640      	mov	r0, r8
   dacb0:	e050      	b.n	dad54 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x478>
   dacb2:	f7fb fe7e 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dacb6:	6873      	ldr	r3, [r6, #4]
   dacb8:	9305      	str	r3, [sp, #20]
   dacba:	4629      	mov	r1, r5
   dacbc:	a818      	add	r0, sp, #96	; 0x60
   dacbe:	f7fb fe78 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dacc2:	4621      	mov	r1, r4
   dacc4:	4640      	mov	r0, r8
   dacc6:	f8d5 b004 	ldr.w	fp, [r5, #4]
   dacca:	f7fb fe72 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dacce:	b104      	cbz	r4, dacd2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x3f6>
   dacd0:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   dacd2:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   dacd4:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   dacd6:	9b24      	ldr	r3, [sp, #144]	; 0x90
   dacd8:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   dacda:	9b25      	ldr	r3, [sp, #148]	; 0x94
   dacdc:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   dacde:	9b26      	ldr	r3, [sp, #152]	; 0x98
   dace0:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   dace2:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   dace4:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   dace6:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   dace8:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   dacea:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   dacec:	a918      	add	r1, sp, #96	; 0x60
   dacee:	a813      	add	r0, sp, #76	; 0x4c
   dacf0:	f7fb fc41 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   dacf4:	4602      	mov	r2, r0
   dacf6:	17c3      	asrs	r3, r0, #31
   dacf8:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   dacfc:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   dacfe:	f04f 0800 	mov.w	r8, #0
   dad02:	f04f 0900 	mov.w	r9, #0
   dad06:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   dad0a:	4590      	cmp	r8, r2
   dad0c:	eb79 0303 	sbcs.w	r3, r9, r3
   dad10:	da1f      	bge.n	dad52 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x476>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dad12:	f91b 5008 	ldrsb.w	r5, [fp, r8]
   dad16:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dad18:	9a08      	ldr	r2, [sp, #32]
   dad1a:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dad1c:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dad1e:	9b05      	ldr	r3, [sp, #20]
   dad20:	f913 0008 	ldrsb.w	r0, [r3, r8]
   dad24:	9b06      	ldr	r3, [sp, #24]
   dad26:	4418      	add	r0, r3
   dad28:	40b8      	lsls	r0, r7
   dad2a:	f7fb fc61 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dad2e:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dad30:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   dad32:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   dad34:	990a      	ldr	r1, [sp, #40]	; 0x28
   dad36:	4628      	mov	r0, r5
   dad38:	f7fb fc5a 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   dad3c:	4582      	cmp	sl, r0
   dad3e:	bfb4      	ite	lt
   dad40:	2000      	movlt	r0, #0
   dad42:	2001      	movge	r0, #1
   dad44:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   dad48:	f118 0801 	adds.w	r8, r8, #1
   dad4c:	f149 0900 	adc.w	r9, r9, #0
   dad50:	e7d9      	b.n	dad06 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x42a>
   dad52:	a81d      	add	r0, sp, #116	; 0x74
   dad54:	f7fb fb7d 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   dad58:	a818      	add	r0, sp, #96	; 0x60
   dad5a:	f7fb fb7a 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   dad5e:	a813      	add	r0, sp, #76	; 0x4c
   dad60:	f7fb fb77 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
   dad64:	2000      	movs	r0, #0
   dad66:	e00e      	b.n	dad86 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x4aa>
                                        requires_broadcast);
      break;
    default:
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
   dad68:	4650      	mov	r0, sl
   dad6a:	f8da 3014 	ldr.w	r3, [sl, #20]
   dad6e:	4907      	ldr	r1, [pc, #28]	; (dad8c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x4b0>)
   dad70:	4798      	blx	r3
      return kTfLiteError;
   dad72:	2001      	movs	r0, #1
   dad74:	e007      	b.n	dad86 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x4aa>
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, GreaterEqual, requires_broadcast);
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, GreaterEqual, requires_broadcast);
   dad76:	a822      	add	r0, sp, #136	; 0x88
   dad78:	f7fb fb6b 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   dad7c:	4640      	mov	r0, r8
   dad7e:	f7fb fb68 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   dad82:	a818      	add	r0, sp, #96	; 0x60
   dad84:	e7ec      	b.n	dad60 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_116GreaterEqualEvalEP13TfLiteContextP10TfLiteNode+0x484>
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   dad86:	b02b      	add	sp, #172	; 0xac
   dad88:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dad8c:	000e99bf 	.word	0x000e99bf

000dad90 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   dad90:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dad94:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   dad96:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   dad98:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   dad9a:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   dad9c:	9208      	str	r2, [sp, #32]
   dad9e:	4604      	mov	r4, r0
   dada0:	460e      	mov	r6, r1
   dada2:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   dada4:	dd01      	ble.n	dadaa <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   dada6:	f009 fad1 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   dadaa:	683b      	ldr	r3, [r7, #0]
   dadac:	2b04      	cmp	r3, #4
   dadae:	dcfa      	bgt.n	dada6 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   dadb0:	6813      	ldr	r3, [r2, #0]
   dadb2:	2b04      	cmp	r3, #4
   dadb4:	dcf7      	bgt.n	dada6 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   dadb6:	2301      	movs	r3, #1
   dadb8:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   dadba:	ad10      	add	r5, sp, #64	; 0x40
   dadbc:	a80b      	add	r0, sp, #44	; 0x2c
   dadbe:	f7fb fb8c 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   dadc2:	ab18      	add	r3, sp, #96	; 0x60
   dadc4:	462a      	mov	r2, r5
   dadc6:	4639      	mov	r1, r7
   dadc8:	4630      	mov	r0, r6
   dadca:	f7fb fe9b 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   dadce:	6863      	ldr	r3, [r4, #4]
   dadd0:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   dadd2:	68a3      	ldr	r3, [r4, #8]
   dadd4:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   dadd6:	68e3      	ldr	r3, [r4, #12]
   dadd8:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   dadda:	6923      	ldr	r3, [r4, #16]
   daddc:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   dadde:	6963      	ldr	r3, [r4, #20]
   dade0:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   dade2:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   dade4:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   dade8:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   dadea:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   dadec:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   dadee:	2100      	movs	r1, #0
   dadf0:	a80b      	add	r0, sp, #44	; 0x2c
   dadf2:	f7fb fb39 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dadf6:	4284      	cmp	r4, r0
   dadf8:	da59      	bge.n	daeae <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   dadfa:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   dadfc:	af0b      	add	r7, sp, #44	; 0x2c
   dadfe:	2101      	movs	r1, #1
   dae00:	4638      	mov	r0, r7
   dae02:	f7fb fb31 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dae06:	4285      	cmp	r5, r0
   dae08:	da4f      	bge.n	daeaa <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   dae0a:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   dae0c:	2102      	movs	r1, #2
   dae0e:	4638      	mov	r0, r7
   dae10:	f7fb fb2a 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dae14:	4286      	cmp	r6, r0
   dae16:	da46      	bge.n	daea6 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   dae18:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dae1c:	2103      	movs	r1, #3
   dae1e:	4638      	mov	r0, r7
   dae20:	f7fb fb22 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dae24:	4580      	cmp	r8, r0
   dae26:	da3c      	bge.n	daea2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   dae28:	f8cd 8000 	str.w	r8, [sp]
   dae2c:	4633      	mov	r3, r6
   dae2e:	462a      	mov	r2, r5
   dae30:	4621      	mov	r1, r4
   dae32:	9809      	ldr	r0, [sp, #36]	; 0x24
   dae34:	f7fb fc2e 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   dae38:	9b08      	ldr	r3, [sp, #32]
   dae3a:	f813 9000 	ldrb.w	r9, [r3, r0]
   dae3e:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   dae40:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   dae44:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   dae46:	462a      	mov	r2, r5
   dae48:	4633      	mov	r3, r6
   dae4a:	4621      	mov	r1, r4
   dae4c:	a818      	add	r0, sp, #96	; 0x60
   dae4e:	f7fb fc21 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dae52:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   dae54:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dae56:	f813 b000 	ldrb.w	fp, [r3, r0]
   dae5a:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   dae5c:	9903      	ldr	r1, [sp, #12]
   dae5e:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dae62:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   dae64:	f7fb fbc4 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dae68:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   dae6c:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   dae6e:	9a07      	ldr	r2, [sp, #28]
   dae70:	9906      	ldr	r1, [sp, #24]
   dae72:	4658      	mov	r0, fp
   dae74:	f7fb fbbc 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   dae78:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   dae7c:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   dae7e:	4633      	mov	r3, r6
   dae80:	462a      	mov	r2, r5
   dae82:	4621      	mov	r1, r4
   dae84:	4638      	mov	r0, r7
   dae86:	f7fb fb54 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   dae8a:	45d9      	cmp	r9, fp
   dae8c:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   dae8e:	bfac      	ite	ge
   dae90:	f04f 0900 	movge.w	r9, #0
   dae94:	f04f 0901 	movlt.w	r9, #1
   dae98:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dae9c:	f108 0801 	add.w	r8, r8, #1
   daea0:	e7bc      	b.n	dae1c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   daea2:	3601      	adds	r6, #1
   daea4:	e7b2      	b.n	dae0c <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   daea6:	3501      	adds	r5, #1
   daea8:	e7a8      	b.n	dadfc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   daeaa:	3401      	adds	r4, #1
   daeac:	e79f      	b.n	dadee <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   daeae:	a80b      	add	r0, sp, #44	; 0x2c
   daeb0:	f7fb facf 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   daeb4:	b021      	add	sp, #132	; 0x84
   daeb6:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000daeba <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   daeba:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   daebe:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   daec0:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   daec2:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   daec4:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   daec6:	9208      	str	r2, [sp, #32]
   daec8:	4604      	mov	r4, r0
   daeca:	460e      	mov	r6, r1
   daecc:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   daece:	dd01      	ble.n	daed4 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   daed0:	f009 fa3c 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   daed4:	683b      	ldr	r3, [r7, #0]
   daed6:	2b04      	cmp	r3, #4
   daed8:	dcfa      	bgt.n	daed0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   daeda:	6813      	ldr	r3, [r2, #0]
   daedc:	2b04      	cmp	r3, #4
   daede:	dcf7      	bgt.n	daed0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   daee0:	2301      	movs	r3, #1
   daee2:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   daee4:	ad10      	add	r5, sp, #64	; 0x40
   daee6:	a80b      	add	r0, sp, #44	; 0x2c
   daee8:	f7fb faf7 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   daeec:	ab18      	add	r3, sp, #96	; 0x60
   daeee:	462a      	mov	r2, r5
   daef0:	4639      	mov	r1, r7
   daef2:	4630      	mov	r0, r6
   daef4:	f7fb fe06 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   daef8:	6863      	ldr	r3, [r4, #4]
   daefa:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   daefc:	68a3      	ldr	r3, [r4, #8]
   daefe:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   daf00:	68e3      	ldr	r3, [r4, #12]
   daf02:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   daf04:	6923      	ldr	r3, [r4, #16]
   daf06:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   daf08:	6963      	ldr	r3, [r4, #20]
   daf0a:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   daf0c:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   daf0e:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   daf12:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   daf14:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   daf16:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   daf18:	2100      	movs	r1, #0
   daf1a:	a80b      	add	r0, sp, #44	; 0x2c
   daf1c:	f7fb faa4 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   daf20:	4284      	cmp	r4, r0
   daf22:	da59      	bge.n	dafd8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   daf24:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   daf26:	af0b      	add	r7, sp, #44	; 0x2c
   daf28:	2101      	movs	r1, #1
   daf2a:	4638      	mov	r0, r7
   daf2c:	f7fb fa9c 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   daf30:	4285      	cmp	r5, r0
   daf32:	da4f      	bge.n	dafd4 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   daf34:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   daf36:	2102      	movs	r1, #2
   daf38:	4638      	mov	r0, r7
   daf3a:	f7fb fa95 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   daf3e:	4286      	cmp	r6, r0
   daf40:	da46      	bge.n	dafd0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   daf42:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   daf46:	2103      	movs	r1, #3
   daf48:	4638      	mov	r0, r7
   daf4a:	f7fb fa8d 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   daf4e:	4580      	cmp	r8, r0
   daf50:	da3c      	bge.n	dafcc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   daf52:	f8cd 8000 	str.w	r8, [sp]
   daf56:	4633      	mov	r3, r6
   daf58:	462a      	mov	r2, r5
   daf5a:	4621      	mov	r1, r4
   daf5c:	9809      	ldr	r0, [sp, #36]	; 0x24
   daf5e:	f7fb fb99 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   daf62:	9b08      	ldr	r3, [sp, #32]
   daf64:	f913 9000 	ldrsb.w	r9, [r3, r0]
   daf68:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   daf6a:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   daf6e:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   daf70:	462a      	mov	r2, r5
   daf72:	4633      	mov	r3, r6
   daf74:	4621      	mov	r1, r4
   daf76:	a818      	add	r0, sp, #96	; 0x60
   daf78:	f7fb fb8c 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   daf7c:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   daf7e:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   daf80:	f913 b000 	ldrsb.w	fp, [r3, r0]
   daf84:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   daf86:	9903      	ldr	r1, [sp, #12]
   daf88:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   daf8c:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   daf8e:	f7fb fb2f 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   daf92:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   daf96:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   daf98:	9a07      	ldr	r2, [sp, #28]
   daf9a:	9906      	ldr	r1, [sp, #24]
   daf9c:	4658      	mov	r0, fp
   daf9e:	f7fb fb27 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   dafa2:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   dafa6:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   dafa8:	4633      	mov	r3, r6
   dafaa:	462a      	mov	r2, r5
   dafac:	4621      	mov	r1, r4
   dafae:	4638      	mov	r0, r7
   dafb0:	f7fb fabf 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   dafb4:	45d9      	cmp	r9, fp
   dafb6:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   dafb8:	bfac      	ite	ge
   dafba:	f04f 0900 	movge.w	r9, #0
   dafbe:	f04f 0901 	movlt.w	r9, #1
   dafc2:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dafc6:	f108 0801 	add.w	r8, r8, #1
   dafca:	e7bc      	b.n	daf46 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   dafcc:	3601      	adds	r6, #1
   dafce:	e7b2      	b.n	daf36 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   dafd0:	3501      	adds	r5, #1
   dafd2:	e7a8      	b.n	daf26 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   dafd4:	3401      	adds	r4, #1
   dafd6:	e79f      	b.n	daf18 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   dafd8:	a80b      	add	r0, sp, #44	; 0x2c
   dafda:	f7fb fa3a 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   dafde:	b021      	add	sp, #132	; 0x84
   dafe0:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dafe4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus LessEval(TfLiteContext* context, TfLiteNode* node) {
   dafe4:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dafe8:	680a      	ldr	r2, [r1, #0]
   dafea:	f8d0 9008 	ldr.w	r9, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dafee:	6895      	ldr	r5, [r2, #8]
   daff0:	4682      	mov	sl, r0
   daff2:	6850      	ldr	r0, [r2, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   daff4:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   daff6:	2338      	movs	r3, #56	; 0x38
   daff8:	fb03 f800 	mul.w	r8, r3, r0
   daffc:	fb03 9505 	mla	r5, r3, r5, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   db000:	6854      	ldr	r4, [r2, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   db002:	eb09 0608 	add.w	r6, r9, r8
   db006:	b0ab      	sub	sp, #172	; 0xac
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   db008:	4629      	mov	r1, r5
   db00a:	4630      	mov	r0, r6
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   db00c:	fb03 9404 	mla	r4, r3, r4, r9
   db010:	f008 feb6 	bl	e3d80 <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
  switch (input1->type) {
   db014:	f819 2008 	ldrb.w	r2, [r9, r8]

TfLiteStatus LessEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   db018:	f080 0001 	eor.w	r0, r0, #1
  switch (input1->type) {
   db01c:	1e53      	subs	r3, r2, #1

TfLiteStatus LessEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   db01e:	b2c7      	uxtb	r7, r0
  switch (input1->type) {
   db020:	2b08      	cmp	r3, #8
   db022:	f200 8225 	bhi.w	db470 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x48c>
   db026:	e8df f013 	tbh	[pc, r3, lsl #1]
   db02a:	0009      	.short	0x0009
   db02c:	00f00057 	.word	0x00f00057
   db030:	022300a1 	.word	0x022300a1
   db034:	02230223 	.word	0x02230223
   db038:	01840223 	.word	0x01840223
   db03c:	f10d 0874 	add.w	r8, sp, #116	; 0x74
    case kTfLiteFloat32:
      TF_LITE_COMPARISON(float, Less, requires_broadcast);
   db040:	4631      	mov	r1, r6
   db042:	b1cf      	cbz	r7, db078 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x94>
   db044:	a813      	add	r0, sp, #76	; 0x4c
   db046:	f7fb fcb4 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db04a:	4629      	mov	r1, r5
   db04c:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db04e:	6876      	ldr	r6, [r6, #4]
   db050:	f7fb fcaf 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db054:	b105      	cbz	r5, db058 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x74>
   db056:	686d      	ldr	r5, [r5, #4]
   db058:	4621      	mov	r1, r4
   db05a:	4640      	mov	r0, r8
   db05c:	f7fb fca9 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db060:	b104      	cbz	r4, db064 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x80>
   db062:	6864      	ldr	r4, [r4, #4]
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
   db064:	9402      	str	r4, [sp, #8]
   db066:	e88d 0120 	stmia.w	sp, {r5, r8}
   db06a:	ab18      	add	r3, sp, #96	; 0x60
   db06c:	4632      	mov	r2, r6
   db06e:	a913      	add	r1, sp, #76	; 0x4c
   db070:	a822      	add	r0, sp, #136	; 0x88
   db072:	f7fd fd43 	bl	d8afc <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_6LessFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   db076:	e19e      	b.n	db3b6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   db078:	a818      	add	r0, sp, #96	; 0x60
   db07a:	f7fb fc9a 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db07e:	4629      	mov	r1, r5
   db080:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db082:	6876      	ldr	r6, [r6, #4]
   db084:	f7fb fc95 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db088:	b105      	cbz	r5, db08c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0xa8>
   db08a:	686d      	ldr	r5, [r5, #4]
   db08c:	4621      	mov	r1, r4
   db08e:	a822      	add	r0, sp, #136	; 0x88
   db090:	f7fb fc8f 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db094:	b104      	cbz	r4, db098 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0xb4>
   db096:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db098:	4641      	mov	r1, r8
   db09a:	aa22      	add	r2, sp, #136	; 0x88
   db09c:	a818      	add	r0, sp, #96	; 0x60
   db09e:	f7fb fa6a 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   db0a2:	3c01      	subs	r4, #1
   db0a4:	4633      	mov	r3, r6
   db0a6:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   db0a8:	2600      	movs	r6, #0
   db0aa:	2700      	movs	r7, #0
   db0ac:	4286      	cmp	r6, r0
   db0ae:	eb77 0201 	sbcs.w	r2, r7, r1
   db0b2:	f280 81e4 	bge.w	db47e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   db0b6:	ecb3 7a01 	vldmia	r3!, {s14}
   db0ba:	ecf5 7a01 	vldmia	r5!, {s15}
   db0be:	eeb4 7ae7 	vcmpe.f32	s14, s15
   db0c2:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   db0c6:	bf4c      	ite	mi
   db0c8:	2201      	movmi	r2, #1
   db0ca:	2200      	movpl	r2, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db0cc:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   db0ce:	f804 2f01 	strb.w	r2, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db0d2:	f147 0700 	adc.w	r7, r7, #0
   db0d6:	e7e9      	b.n	db0ac <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0xc8>
   db0d8:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, Less, requires_broadcast);
   db0dc:	4631      	mov	r1, r6
   db0de:	b1cf      	cbz	r7, db114 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x130>
   db0e0:	a813      	add	r0, sp, #76	; 0x4c
   db0e2:	f7fb fc66 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db0e6:	4629      	mov	r1, r5
   db0e8:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db0ea:	6876      	ldr	r6, [r6, #4]
   db0ec:	f7fb fc61 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db0f0:	b105      	cbz	r5, db0f4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x110>
   db0f2:	686d      	ldr	r5, [r5, #4]
   db0f4:	4621      	mov	r1, r4
   db0f6:	4640      	mov	r0, r8
   db0f8:	f7fb fc5b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db0fc:	b104      	cbz	r4, db100 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x11c>
   db0fe:	6864      	ldr	r4, [r4, #4]
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
   db100:	9402      	str	r4, [sp, #8]
   db102:	e88d 0120 	stmia.w	sp, {r5, r8}
   db106:	ab18      	add	r3, sp, #96	; 0x60
   db108:	4632      	mov	r2, r6
   db10a:	a913      	add	r1, sp, #76	; 0x4c
   db10c:	a822      	add	r0, sp, #136	; 0x88
   db10e:	f7fd fd68 	bl	d8be2 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   db112:	e150      	b.n	db3b6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   db114:	a818      	add	r0, sp, #96	; 0x60
   db116:	f7fb fc4c 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db11a:	4629      	mov	r1, r5
   db11c:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db11e:	6876      	ldr	r6, [r6, #4]
   db120:	f7fb fc47 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db124:	b105      	cbz	r5, db128 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x144>
   db126:	686d      	ldr	r5, [r5, #4]
   db128:	4621      	mov	r1, r4
   db12a:	a822      	add	r0, sp, #136	; 0x88
   db12c:	f7fb fc41 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db130:	b104      	cbz	r4, db134 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x150>
   db132:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db134:	aa22      	add	r2, sp, #136	; 0x88
   db136:	4641      	mov	r1, r8
   db138:	a818      	add	r0, sp, #96	; 0x60
   db13a:	f7fb fa1c 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   db13e:	3c01      	subs	r4, #1
   db140:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   db142:	2200      	movs	r2, #0
   db144:	2300      	movs	r3, #0
   db146:	4282      	cmp	r2, r0
   db148:	eb73 0701 	sbcs.w	r7, r3, r1
   db14c:	f280 8197 	bge.w	db47e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   db150:	f856 7022 	ldr.w	r7, [r6, r2, lsl #2]
   db154:	f855 e022 	ldr.w	lr, [r5, r2, lsl #2]
   db158:	4577      	cmp	r7, lr
   db15a:	bfac      	ite	ge
   db15c:	2700      	movge	r7, #0
   db15e:	2701      	movlt	r7, #1
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db160:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   db162:	f804 7f01 	strb.w	r7, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db166:	f143 0300 	adc.w	r3, r3, #0
   db16a:	e7ec      	b.n	db146 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x162>
   db16c:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, Less, requires_broadcast);
   db170:	4631      	mov	r1, r6
   db172:	b1cf      	cbz	r7, db1a8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x1c4>
   db174:	a813      	add	r0, sp, #76	; 0x4c
   db176:	f7fb fc1c 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db17a:	4629      	mov	r1, r5
   db17c:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db17e:	6876      	ldr	r6, [r6, #4]
   db180:	f7fb fc17 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db184:	b105      	cbz	r5, db188 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x1a4>
   db186:	686d      	ldr	r5, [r5, #4]
   db188:	4621      	mov	r1, r4
   db18a:	4640      	mov	r0, r8
   db18c:	f7fb fc11 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db190:	b104      	cbz	r4, db194 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x1b0>
   db192:	6864      	ldr	r4, [r4, #4]
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
   db194:	9402      	str	r4, [sp, #8]
   db196:	e88d 0120 	stmia.w	sp, {r5, r8}
   db19a:	ab18      	add	r3, sp, #96	; 0x60
   db19c:	4632      	mov	r2, r6
   db19e:	a913      	add	r1, sp, #76	; 0x4c
   db1a0:	a822      	add	r0, sp, #136	; 0x88
   db1a2:	f7fd fd8a 	bl	d8cba <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_6LessFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   db1a6:	e106      	b.n	db3b6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   db1a8:	a818      	add	r0, sp, #96	; 0x60
   db1aa:	f7fb fc02 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db1ae:	4629      	mov	r1, r5
   db1b0:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db1b2:	6876      	ldr	r6, [r6, #4]
   db1b4:	f7fb fbfd 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db1b8:	b105      	cbz	r5, db1bc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x1d8>
   db1ba:	686d      	ldr	r5, [r5, #4]
   db1bc:	4621      	mov	r1, r4
   db1be:	a822      	add	r0, sp, #136	; 0x88
   db1c0:	f7fb fbf7 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db1c4:	b104      	cbz	r4, db1c8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x1e4>
   db1c6:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db1c8:	aa22      	add	r2, sp, #136	; 0x88
   db1ca:	4641      	mov	r1, r8
   db1cc:	a818      	add	r0, sp, #96	; 0x60
   db1ce:	f7fb f9d2 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   db1d2:	3d08      	subs	r5, #8
   db1d4:	17c1      	asrs	r1, r0, #31
   db1d6:	f1a6 0e08 	sub.w	lr, r6, #8
   db1da:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
   db1dc:	2200      	movs	r2, #0
   db1de:	2300      	movs	r3, #0
   db1e0:	4282      	cmp	r2, r0
   db1e2:	eb73 0601 	sbcs.w	r6, r3, r1
   db1e6:	f280 814a 	bge.w	db47e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   db1ea:	e9fe ab02 	ldrd	sl, fp, [lr, #8]!
   db1ee:	e9f5 6702 	ldrd	r6, r7, [r5, #8]!
   db1f2:	45b2      	cmp	sl, r6
   db1f4:	eb7b 0607 	sbcs.w	r6, fp, r7
   db1f8:	bfb4      	ite	lt
   db1fa:	2601      	movlt	r6, #1
   db1fc:	2600      	movge	r6, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db1fe:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   db200:	f804 6f01 	strb.w	r6, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db204:	f143 0300 	adc.w	r3, r3, #0
   db208:	e7ea      	b.n	db1e0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x1fc>
  }
TF_LITE_QUANTIZE_COMPARISON(Equal);
TF_LITE_QUANTIZE_COMPARISON(NotEqual);
TF_LITE_QUANTIZE_COMPARISON(Greater);
TF_LITE_QUANTIZE_COMPARISON(GreaterEqual);
TF_LITE_QUANTIZE_COMPARISON(Less);
   db20a:	6933      	ldr	r3, [r6, #16]
   db20c:	68f0      	ldr	r0, [r6, #12]
   db20e:	f1c3 0900 	rsb	r9, r3, #0
   db212:	692b      	ldr	r3, [r5, #16]
   db214:	f1c3 0800 	rsb	r8, r3, #0
   db218:	f00b fed8 	bl	e6fcc <__aeabi_f2d>
   db21c:	ec41 0b10 	vmov	d0, r0, r1
   db220:	a910      	add	r1, sp, #64	; 0x40
   db222:	a80f      	add	r0, sp, #60	; 0x3c
   db224:	f008 fe20 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   db228:	68e8      	ldr	r0, [r5, #12]
   db22a:	f00b fecf 	bl	e6fcc <__aeabi_f2d>
   db22e:	ec41 0b10 	vmov	d0, r0, r1
   db232:	a912      	add	r1, sp, #72	; 0x48
   db234:	a811      	add	r0, sp, #68	; 0x44
   db236:	f008 fe17 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   db23a:	2308      	movs	r3, #8
   db23c:	9322      	str	r3, [sp, #136]	; 0x88
   db23e:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   db240:	9324      	str	r3, [sp, #144]	; 0x90
   db242:	9b10      	ldr	r3, [sp, #64]	; 0x40
   db244:	9325      	str	r3, [sp, #148]	; 0x94
   db246:	9b11      	ldr	r3, [sp, #68]	; 0x44
   db248:	9327      	str	r3, [sp, #156]	; 0x9c
   db24a:	9b12      	ldr	r3, [sp, #72]	; 0x48
   db24c:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   db250:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   db254:	9328      	str	r3, [sp, #160]	; 0xa0
   db256:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   db25a:	4631      	mov	r1, r6
   db25c:	a813      	add	r0, sp, #76	; 0x4c
   db25e:	b1bf      	cbz	r7, db290 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x2ac>
   db260:	f7fb fba7 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db264:	4629      	mov	r1, r5
   db266:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db268:	6876      	ldr	r6, [r6, #4]
   db26a:	f7fb fba2 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db26e:	4621      	mov	r1, r4
   db270:	4640      	mov	r0, r8
   db272:	686d      	ldr	r5, [r5, #4]
   db274:	f7fb fb9d 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db278:	b104      	cbz	r4, db27c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x298>
   db27a:	6864      	ldr	r4, [r4, #4]
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
   db27c:	9402      	str	r4, [sp, #8]
   db27e:	e88d 0120 	stmia.w	sp, {r5, r8}
   db282:	ab18      	add	r3, sp, #96	; 0x60
   db284:	4632      	mov	r2, r6
   db286:	a913      	add	r1, sp, #76	; 0x4c
   db288:	a822      	add	r0, sp, #136	; 0x88
   db28a:	f7ff fd81 	bl	dad90 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   db28e:	e092      	b.n	db3b6 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   db290:	f7fb fb8f 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db294:	6873      	ldr	r3, [r6, #4]
   db296:	9305      	str	r3, [sp, #20]
   db298:	4629      	mov	r1, r5
   db29a:	a818      	add	r0, sp, #96	; 0x60
   db29c:	f7fb fb89 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db2a0:	4621      	mov	r1, r4
   db2a2:	4640      	mov	r0, r8
   db2a4:	f8d5 b004 	ldr.w	fp, [r5, #4]
   db2a8:	f7fb fb83 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db2ac:	b104      	cbz	r4, db2b0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x2cc>
   db2ae:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   db2b0:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   db2b2:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   db2b4:	9b24      	ldr	r3, [sp, #144]	; 0x90
   db2b6:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   db2b8:	9b25      	ldr	r3, [sp, #148]	; 0x94
   db2ba:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   db2bc:	9b26      	ldr	r3, [sp, #152]	; 0x98
   db2be:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   db2c0:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   db2c2:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db2c4:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   db2c6:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   db2c8:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db2ca:	a918      	add	r1, sp, #96	; 0x60
   db2cc:	a813      	add	r0, sp, #76	; 0x4c
   db2ce:	f7fb f952 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   db2d2:	4602      	mov	r2, r0
   db2d4:	17c3      	asrs	r3, r0, #31
   db2d6:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   db2da:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db2dc:	f04f 0800 	mov.w	r8, #0
   db2e0:	f04f 0900 	mov.w	r9, #0
   db2e4:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   db2e8:	4590      	cmp	r8, r2
   db2ea:	eb79 0303 	sbcs.w	r3, r9, r3
   db2ee:	f280 80b4 	bge.w	db45a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x476>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db2f2:	f81b 5008 	ldrb.w	r5, [fp, r8]
   db2f6:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   db2f8:	9a08      	ldr	r2, [sp, #32]
   db2fa:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db2fc:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   db2fe:	9b05      	ldr	r3, [sp, #20]
   db300:	f813 0008 	ldrb.w	r0, [r3, r8]
   db304:	9b06      	ldr	r3, [sp, #24]
   db306:	4418      	add	r0, r3
   db308:	40b8      	lsls	r0, r7
   db30a:	f7fb f971 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db30e:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   db310:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   db312:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   db314:	990a      	ldr	r1, [sp, #40]	; 0x28
   db316:	4628      	mov	r0, r5
   db318:	f7fb f96a 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   db31c:	4582      	cmp	sl, r0
   db31e:	bfac      	ite	ge
   db320:	2000      	movge	r0, #0
   db322:	2001      	movlt	r0, #1
   db324:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db328:	f118 0801 	adds.w	r8, r8, #1
   db32c:	f149 0900 	adc.w	r9, r9, #0
   db330:	e7d8      	b.n	db2e4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x300>
   db332:	6933      	ldr	r3, [r6, #16]
   db334:	68f0      	ldr	r0, [r6, #12]
   db336:	f1c3 0900 	rsb	r9, r3, #0
   db33a:	692b      	ldr	r3, [r5, #16]
   db33c:	f1c3 0800 	rsb	r8, r3, #0
   db340:	f00b fe44 	bl	e6fcc <__aeabi_f2d>
   db344:	ec41 0b10 	vmov	d0, r0, r1
   db348:	a910      	add	r1, sp, #64	; 0x40
   db34a:	a80f      	add	r0, sp, #60	; 0x3c
   db34c:	f008 fd8c 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   db350:	68e8      	ldr	r0, [r5, #12]
   db352:	f00b fe3b 	bl	e6fcc <__aeabi_f2d>
   db356:	ec41 0b10 	vmov	d0, r0, r1
   db35a:	a912      	add	r1, sp, #72	; 0x48
   db35c:	a811      	add	r0, sp, #68	; 0x44
   db35e:	f008 fd83 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   db362:	2308      	movs	r3, #8
   db364:	9322      	str	r3, [sp, #136]	; 0x88
   db366:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   db368:	9324      	str	r3, [sp, #144]	; 0x90
   db36a:	9b10      	ldr	r3, [sp, #64]	; 0x40
   db36c:	9325      	str	r3, [sp, #148]	; 0x94
   db36e:	9b11      	ldr	r3, [sp, #68]	; 0x44
   db370:	9327      	str	r3, [sp, #156]	; 0x9c
   db372:	9b12      	ldr	r3, [sp, #72]	; 0x48
   db374:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   db378:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   db37c:	9328      	str	r3, [sp, #160]	; 0xa0
   db37e:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   db382:	4631      	mov	r1, r6
   db384:	a813      	add	r0, sp, #76	; 0x4c
   db386:	b1c7      	cbz	r7, db3ba <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3d6>
   db388:	f7fb fb13 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db38c:	4629      	mov	r1, r5
   db38e:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db390:	6876      	ldr	r6, [r6, #4]
   db392:	f7fb fb0e 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db396:	4621      	mov	r1, r4
   db398:	4640      	mov	r0, r8
   db39a:	686d      	ldr	r5, [r5, #4]
   db39c:	f7fb fb09 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db3a0:	b104      	cbz	r4, db3a4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3c0>
   db3a2:	6864      	ldr	r4, [r4, #4]
  }
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
   db3a4:	9402      	str	r4, [sp, #8]
   db3a6:	e88d 0120 	stmia.w	sp, {r5, r8}
   db3aa:	ab18      	add	r3, sp, #96	; 0x60
   db3ac:	4632      	mov	r2, r6
   db3ae:	a913      	add	r1, sp, #76	; 0x4c
   db3b0:	a822      	add	r0, sp, #136	; 0x88
   db3b2:	f7ff fd82 	bl	daeba <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_6LessFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   db3b6:	4640      	mov	r0, r8
   db3b8:	e050      	b.n	db45c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x478>
   db3ba:	f7fb fafa 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db3be:	6873      	ldr	r3, [r6, #4]
   db3c0:	9305      	str	r3, [sp, #20]
   db3c2:	4629      	mov	r1, r5
   db3c4:	a818      	add	r0, sp, #96	; 0x60
   db3c6:	f7fb faf4 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db3ca:	4621      	mov	r1, r4
   db3cc:	4640      	mov	r0, r8
   db3ce:	f8d5 b004 	ldr.w	fp, [r5, #4]
   db3d2:	f7fb faee 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db3d6:	b104      	cbz	r4, db3da <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x3f6>
   db3d8:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   db3da:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   db3dc:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   db3de:	9b24      	ldr	r3, [sp, #144]	; 0x90
   db3e0:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   db3e2:	9b25      	ldr	r3, [sp, #148]	; 0x94
   db3e4:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   db3e6:	9b26      	ldr	r3, [sp, #152]	; 0x98
   db3e8:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   db3ea:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   db3ec:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db3ee:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   db3f0:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   db3f2:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db3f4:	a918      	add	r1, sp, #96	; 0x60
   db3f6:	a813      	add	r0, sp, #76	; 0x4c
   db3f8:	f7fb f8bd 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   db3fc:	4602      	mov	r2, r0
   db3fe:	17c3      	asrs	r3, r0, #31
   db400:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   db404:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db406:	f04f 0800 	mov.w	r8, #0
   db40a:	f04f 0900 	mov.w	r9, #0
   db40e:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   db412:	4590      	cmp	r8, r2
   db414:	eb79 0303 	sbcs.w	r3, r9, r3
   db418:	da1f      	bge.n	db45a <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x476>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db41a:	f91b 5008 	ldrsb.w	r5, [fp, r8]
   db41e:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   db420:	9a08      	ldr	r2, [sp, #32]
   db422:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db424:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   db426:	9b05      	ldr	r3, [sp, #20]
   db428:	f913 0008 	ldrsb.w	r0, [r3, r8]
   db42c:	9b06      	ldr	r3, [sp, #24]
   db42e:	4418      	add	r0, r3
   db430:	40b8      	lsls	r0, r7
   db432:	f7fb f8dd 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db436:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   db438:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   db43a:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   db43c:	990a      	ldr	r1, [sp, #40]	; 0x28
   db43e:	4628      	mov	r0, r5
   db440:	f7fb f8d6 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   db444:	4582      	cmp	sl, r0
   db446:	bfac      	ite	ge
   db448:	2000      	movge	r0, #0
   db44a:	2001      	movlt	r0, #1
   db44c:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db450:	f118 0801 	adds.w	r8, r8, #1
   db454:	f149 0900 	adc.w	r9, r9, #0
   db458:	e7d9      	b.n	db40e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x42a>
   db45a:	a81d      	add	r0, sp, #116	; 0x74
   db45c:	f7fa fff9 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   db460:	a818      	add	r0, sp, #96	; 0x60
   db462:	f7fa fff6 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   db466:	a813      	add	r0, sp, #76	; 0x4c
   db468:	f7fa fff3 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
   db46c:	2000      	movs	r0, #0
   db46e:	e00e      	b.n	db48e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x4aa>
                                requires_broadcast);
      break;
    default:
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
   db470:	4650      	mov	r0, sl
   db472:	f8da 3014 	ldr.w	r3, [sl, #20]
   db476:	4907      	ldr	r1, [pc, #28]	; (db494 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x4b0>)
   db478:	4798      	blx	r3
      return kTfLiteError;
   db47a:	2001      	movs	r0, #1
   db47c:	e007      	b.n	db48e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x4aa>
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, Less, requires_broadcast);
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, Less, requires_broadcast);
   db47e:	a822      	add	r0, sp, #136	; 0x88
   db480:	f7fa ffe7 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   db484:	4640      	mov	r0, r8
   db486:	f7fa ffe4 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   db48a:	a818      	add	r0, sp, #96	; 0x60
   db48c:	e7ec      	b.n	db468 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_18LessEvalEP13TfLiteContextP10TfLiteNode+0x484>
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   db48e:	b02b      	add	sp, #172	; 0xac
   db490:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   db494:	000e99bf 	.word	0x000e99bf

000db498 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   db498:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   db49c:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   db49e:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   db4a0:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   db4a2:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   db4a4:	9208      	str	r2, [sp, #32]
   db4a6:	4604      	mov	r4, r0
   db4a8:	460e      	mov	r6, r1
   db4aa:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   db4ac:	dd01      	ble.n	db4b2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   db4ae:	f008 ff4d 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   db4b2:	683b      	ldr	r3, [r7, #0]
   db4b4:	2b04      	cmp	r3, #4
   db4b6:	dcfa      	bgt.n	db4ae <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   db4b8:	6813      	ldr	r3, [r2, #0]
   db4ba:	2b04      	cmp	r3, #4
   db4bc:	dcf7      	bgt.n	db4ae <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   db4be:	2301      	movs	r3, #1
   db4c0:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   db4c2:	ad10      	add	r5, sp, #64	; 0x40
   db4c4:	a80b      	add	r0, sp, #44	; 0x2c
   db4c6:	f7fb f808 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   db4ca:	ab18      	add	r3, sp, #96	; 0x60
   db4cc:	462a      	mov	r2, r5
   db4ce:	4639      	mov	r1, r7
   db4d0:	4630      	mov	r0, r6
   db4d2:	f7fb fb17 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   db4d6:	6863      	ldr	r3, [r4, #4]
   db4d8:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   db4da:	68a3      	ldr	r3, [r4, #8]
   db4dc:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   db4de:	68e3      	ldr	r3, [r4, #12]
   db4e0:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   db4e2:	6923      	ldr	r3, [r4, #16]
   db4e4:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   db4e6:	6963      	ldr	r3, [r4, #20]
   db4e8:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   db4ea:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   db4ec:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   db4f0:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   db4f2:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   db4f4:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   db4f6:	2100      	movs	r1, #0
   db4f8:	a80b      	add	r0, sp, #44	; 0x2c
   db4fa:	f7fa ffb5 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   db4fe:	4284      	cmp	r4, r0
   db500:	da59      	bge.n	db5b6 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   db502:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   db504:	af0b      	add	r7, sp, #44	; 0x2c
   db506:	2101      	movs	r1, #1
   db508:	4638      	mov	r0, r7
   db50a:	f7fa ffad 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   db50e:	4285      	cmp	r5, r0
   db510:	da4f      	bge.n	db5b2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   db512:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   db514:	2102      	movs	r1, #2
   db516:	4638      	mov	r0, r7
   db518:	f7fa ffa6 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   db51c:	4286      	cmp	r6, r0
   db51e:	da46      	bge.n	db5ae <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   db520:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   db524:	2103      	movs	r1, #3
   db526:	4638      	mov	r0, r7
   db528:	f7fa ff9e 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   db52c:	4580      	cmp	r8, r0
   db52e:	da3c      	bge.n	db5aa <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   db530:	f8cd 8000 	str.w	r8, [sp]
   db534:	4633      	mov	r3, r6
   db536:	462a      	mov	r2, r5
   db538:	4621      	mov	r1, r4
   db53a:	9809      	ldr	r0, [sp, #36]	; 0x24
   db53c:	f7fb f8aa 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   db540:	9b08      	ldr	r3, [sp, #32]
   db542:	f813 9000 	ldrb.w	r9, [r3, r0]
   db546:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   db548:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   db54c:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   db54e:	462a      	mov	r2, r5
   db550:	4633      	mov	r3, r6
   db552:	4621      	mov	r1, r4
   db554:	a818      	add	r0, sp, #96	; 0x60
   db556:	f7fb f89d 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db55a:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   db55c:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db55e:	f813 b000 	ldrb.w	fp, [r3, r0]
   db562:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   db564:	9903      	ldr	r1, [sp, #12]
   db566:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db56a:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   db56c:	f7fb f840 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db570:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   db574:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   db576:	9a07      	ldr	r2, [sp, #28]
   db578:	9906      	ldr	r1, [sp, #24]
   db57a:	4658      	mov	r0, fp
   db57c:	f7fb f838 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   db580:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   db584:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   db586:	4633      	mov	r3, r6
   db588:	462a      	mov	r2, r5
   db58a:	4621      	mov	r1, r4
   db58c:	4638      	mov	r0, r7
   db58e:	f7fa ffd0 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   db592:	45d9      	cmp	r9, fp
   db594:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   db596:	bfcc      	ite	gt
   db598:	f04f 0900 	movgt.w	r9, #0
   db59c:	f04f 0901 	movle.w	r9, #1
   db5a0:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   db5a4:	f108 0801 	add.w	r8, r8, #1
   db5a8:	e7bc      	b.n	db524 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   db5aa:	3601      	adds	r6, #1
   db5ac:	e7b2      	b.n	db514 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   db5ae:	3501      	adds	r5, #1
   db5b0:	e7a8      	b.n	db504 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   db5b2:	3401      	adds	r4, #1
   db5b4:	e79f      	b.n	db4f6 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   db5b6:	a80b      	add	r0, sp, #44	; 0x2c
   db5b8:	f7fa ff4b 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   db5bc:	b021      	add	sp, #132	; 0x84
   db5be:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000db5c2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>:
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   db5c2:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   db5c6:	461f      	mov	r7, r3
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   db5c8:	680b      	ldr	r3, [r1, #0]
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   db5ca:	b0a1      	sub	sp, #132	; 0x84
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   db5cc:	2b04      	cmp	r3, #4
                                          input2_shape, input2_data,
                                          output_shape, output_data);
}

template <typename T, ComparisonFn<int32> F>
inline void BroadcastComparison4DSlowWithScaling(
   db5ce:	9208      	str	r2, [sp, #32]
   db5d0:	4604      	mov	r4, r0
   db5d2:	460e      	mov	r6, r1
   db5d4:	9a2b      	ldr	r2, [sp, #172]	; 0xac
    const ComparisonParams& op_params,
    const RuntimeShape& unextended_input1_shape, const T* input1_data,
    const RuntimeShape& unextended_input2_shape, const T* input2_data,
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   db5d6:	dd01      	ble.n	db5dc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x1a>
   db5d8:	f008 feb8 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   db5dc:	683b      	ldr	r3, [r7, #0]
   db5de:	2b04      	cmp	r3, #4
   db5e0:	dcfa      	bgt.n	db5d8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   db5e2:	6813      	ldr	r3, [r2, #0]
   db5e4:	2b04      	cmp	r3, #4
   db5e6:	dcf7      	bgt.n	db5d8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x16>
   db5e8:	2301      	movs	r3, #1
   db5ea:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   db5ec:	ad10      	add	r5, sp, #64	; 0x40
   db5ee:	a80b      	add	r0, sp, #44	; 0x2c
   db5f0:	f7fa ff73 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   db5f4:	ab18      	add	r3, sp, #96	; 0x60
   db5f6:	462a      	mov	r2, r5
   db5f8:	4639      	mov	r1, r7
   db5fa:	4630      	mov	r0, r6
   db5fc:	f7fb fa82 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   db600:	6863      	ldr	r3, [r4, #4]
   db602:	9302      	str	r3, [sp, #8]
  int32 input1_multiplier = op_params.input1_multiplier;
   db604:	68a3      	ldr	r3, [r4, #8]
   db606:	9303      	str	r3, [sp, #12]
  int input1_shift = op_params.input1_shift;
   db608:	68e3      	ldr	r3, [r4, #12]
   db60a:	9304      	str	r3, [sp, #16]
  int32 input2_offset = op_params.input2_offset;
   db60c:	6923      	ldr	r3, [r4, #16]
   db60e:	9305      	str	r3, [sp, #20]
  int32 input2_multiplier = op_params.input2_multiplier;
   db610:	6963      	ldr	r3, [r4, #20]
   db612:	9306      	str	r3, [sp, #24]
  int input2_shift = op_params.input2_shift;
   db614:	69a3      	ldr	r3, [r4, #24]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  int left_shift = op_params.left_shift;
   db616:	f8d4 a000 	ldr.w	sl, [r4]
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   db61a:	9307      	str	r3, [sp, #28]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   db61c:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   db61e:	9509      	str	r5, [sp, #36]	; 0x24
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   db620:	2100      	movs	r1, #0
   db622:	a80b      	add	r0, sp, #44	; 0x2c
   db624:	f7fa ff20 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   db628:	4284      	cmp	r4, r0
   db62a:	da59      	bge.n	db6e0 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11e>
   db62c:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   db62e:	af0b      	add	r7, sp, #44	; 0x2c
   db630:	2101      	movs	r1, #1
   db632:	4638      	mov	r0, r7
   db634:	f7fa ff18 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   db638:	4285      	cmp	r5, r0
   db63a:	da4f      	bge.n	db6dc <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x11a>
   db63c:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   db63e:	2102      	movs	r1, #2
   db640:	4638      	mov	r0, r7
   db642:	f7fa ff11 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   db646:	4286      	cmp	r6, r0
   db648:	da46      	bge.n	db6d8 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x116>
   db64a:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   db64e:	2103      	movs	r1, #3
   db650:	4638      	mov	r0, r7
   db652:	f7fa ff09 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   db656:	4580      	cmp	r8, r0
   db658:	da3c      	bge.n	db6d4 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x112>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   db65a:	f8cd 8000 	str.w	r8, [sp]
   db65e:	4633      	mov	r3, r6
   db660:	462a      	mov	r2, r5
   db662:	4621      	mov	r1, r4
   db664:	9809      	ldr	r0, [sp, #36]	; 0x24
   db666:	f7fb f815 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
   db66a:	9b08      	ldr	r3, [sp, #32]
   db66c:	f913 9000 	ldrsb.w	r9, [r3, r0]
   db670:	9b02      	ldr	r3, [sp, #8]
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   db672:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
   db676:	4499      	add	r9, r3
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
   db678:	462a      	mov	r2, r5
   db67a:	4633      	mov	r3, r6
   db67c:	4621      	mov	r1, r4
   db67e:	a818      	add	r0, sp, #96	; 0x60
   db680:	f7fb f808 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db684:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   db686:	9a04      	ldr	r2, [sp, #16]
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db688:	f913 b000 	ldrsb.w	fp, [r3, r0]
   db68c:	9b05      	ldr	r3, [sp, #20]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   db68e:	9903      	ldr	r1, [sp, #12]
   db690:	fa09 f00a 	lsl.w	r0, r9, sl
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db694:	449b      	add	fp, r3
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   db696:	f7fa ffab 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          const int32 input1_val =
              input1_offset + input1_data[SubscriptToIndex(desc1, b, y, x, c)];
          const int32 input2_val =
              input2_offset + input2_data[SubscriptToIndex(desc2, b, y, x, c)];
          const int32 shifted_input1_val = input1_val * (1 << left_shift);
          const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db69a:	fa0b fb0a 	lsl.w	fp, fp, sl
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
   db69e:	4681      	mov	r9, r0
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   db6a0:	9a07      	ldr	r2, [sp, #28]
   db6a2:	9906      	ldr	r1, [sp, #24]
   db6a4:	4658      	mov	r0, fp
   db6a6:	f7fa ffa3 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
          output_data[Offset(output_shape, b, y, x, c)] =
   db6aa:	f8cd 8000 	str.w	r8, [sp]
          const int32 scaled_input1_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input1_val, input1_multiplier, input1_shift);
          const int32 scaled_input2_val =
              MultiplyByQuantizedMultiplierSmallerThanOneExp(
                  shifted_input2_val, input2_multiplier, input2_shift);
   db6ae:	4683      	mov	fp, r0
          output_data[Offset(output_shape, b, y, x, c)] =
   db6b0:	4633      	mov	r3, r6
   db6b2:	462a      	mov	r2, r5
   db6b4:	4621      	mov	r1, r4
   db6b6:	4638      	mov	r0, r7
   db6b8:	f7fa ff3b 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   db6bc:	45d9      	cmp	r9, fp
   db6be:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   db6c0:	bfcc      	ite	gt
   db6c2:	f04f 0900 	movgt.w	r9, #0
   db6c6:	f04f 0901 	movle.w	r9, #1
   db6ca:	f803 9000 	strb.w	r9, [r3, r0]
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   db6ce:	f108 0801 	add.w	r8, r8, #1
   db6d2:	e7bc      	b.n	db64e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x8c>
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   db6d4:	3601      	adds	r6, #1
   db6d6:	e7b2      	b.n	db63e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x7c>
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   db6d8:	3501      	adds	r5, #1
   db6da:	e7a8      	b.n	db62e <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x6c>
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   db6dc:	3401      	adds	r4, #1
   db6de:	e79f      	b.n	db620 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb+0x5e>
    const RuntimeShape& unextended_output_shape, bool* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   db6e0:	a80b      	add	r0, sp, #44	; 0x2c
   db6e2:	f7fa feb6 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
              F(scaled_input1_val, scaled_input2_val);
        }
      }
    }
  }
}
   db6e6:	b021      	add	sp, #132	; 0x84
   db6e8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000db6ec <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus LessEqualEval(TfLiteContext* context, TfLiteNode* node) {
   db6ec:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   db6f0:	680a      	ldr	r2, [r1, #0]
   db6f2:	f8d0 9008 	ldr.w	r9, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   db6f6:	6895      	ldr	r5, [r2, #8]
   db6f8:	4682      	mov	sl, r0
   db6fa:	6850      	ldr	r0, [r2, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   db6fc:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   db6fe:	2338      	movs	r3, #56	; 0x38
   db700:	fb03 f800 	mul.w	r8, r3, r0
   db704:	fb03 9505 	mla	r5, r3, r5, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   db708:	6854      	ldr	r4, [r2, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   db70a:	eb09 0608 	add.w	r6, r9, r8
   db70e:	b0ab      	sub	sp, #172	; 0xac
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   db710:	4629      	mov	r1, r5
   db712:	4630      	mov	r0, r6
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   db714:	fb03 9404 	mla	r4, r3, r4, r9
   db718:	f008 fb32 	bl	e3d80 <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
  switch (input1->type) {
   db71c:	f819 2008 	ldrb.w	r2, [r9, r8]

TfLiteStatus LessEqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   db720:	f080 0001 	eor.w	r0, r0, #1
  switch (input1->type) {
   db724:	1e53      	subs	r3, r2, #1

TfLiteStatus LessEqualEval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  bool requires_broadcast = !HaveSameShapes(input1, input2);
   db726:	b2c7      	uxtb	r7, r0
  switch (input1->type) {
   db728:	2b08      	cmp	r3, #8
   db72a:	f200 8225 	bhi.w	dbb78 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x48c>
   db72e:	e8df f013 	tbh	[pc, r3, lsl #1]
   db732:	0009      	.short	0x0009
   db734:	00f00057 	.word	0x00f00057
   db738:	022300a1 	.word	0x022300a1
   db73c:	02230223 	.word	0x02230223
   db740:	01840223 	.word	0x01840223
   db744:	f10d 0874 	add.w	r8, sp, #116	; 0x74
    case kTfLiteFloat32:
      TF_LITE_COMPARISON(float, LessEqual, requires_broadcast);
   db748:	4631      	mov	r1, r6
   db74a:	b1cf      	cbz	r7, db780 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x94>
   db74c:	a813      	add	r0, sp, #76	; 0x4c
   db74e:	f7fb f930 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db752:	4629      	mov	r1, r5
   db754:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db756:	6876      	ldr	r6, [r6, #4]
   db758:	f7fb f92b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db75c:	b105      	cbz	r5, db760 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x74>
   db75e:	686d      	ldr	r5, [r5, #4]
   db760:	4621      	mov	r1, r4
   db762:	4640      	mov	r0, r8
   db764:	f7fb f925 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db768:	b104      	cbz	r4, db76c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x80>
   db76a:	6864      	ldr	r4, [r4, #4]
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
TFLITE_COMPARISON_OP(LessEqual);
   db76c:	9402      	str	r4, [sp, #8]
   db76e:	e88d 0120 	stmia.w	sp, {r5, r8}
   db772:	ab18      	add	r3, sp, #96	; 0x60
   db774:	4632      	mov	r2, r6
   db776:	a913      	add	r1, sp, #76	; 0x4c
   db778:	a822      	add	r0, sp, #136	; 0x88
   db77a:	f7fd fb10 	bl	d8d9e <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIfXadL_ZNS0_11LessEqualFnIfEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   db77e:	e19e      	b.n	dbabe <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   db780:	a818      	add	r0, sp, #96	; 0x60
   db782:	f7fb f916 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db786:	4629      	mov	r1, r5
   db788:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db78a:	6876      	ldr	r6, [r6, #4]
   db78c:	f7fb f911 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db790:	b105      	cbz	r5, db794 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0xa8>
   db792:	686d      	ldr	r5, [r5, #4]
   db794:	4621      	mov	r1, r4
   db796:	a822      	add	r0, sp, #136	; 0x88
   db798:	f7fb f90b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db79c:	b104      	cbz	r4, db7a0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0xb4>
   db79e:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db7a0:	4641      	mov	r1, r8
   db7a2:	aa22      	add	r2, sp, #136	; 0x88
   db7a4:	a818      	add	r0, sp, #96	; 0x60
   db7a6:	f7fa fee6 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   db7aa:	3c01      	subs	r4, #1
   db7ac:	4633      	mov	r3, r6
   db7ae:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   db7b0:	2600      	movs	r6, #0
   db7b2:	2700      	movs	r7, #0
   db7b4:	4286      	cmp	r6, r0
   db7b6:	eb77 0201 	sbcs.w	r2, r7, r1
   db7ba:	f280 81e4 	bge.w	dbb86 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   db7be:	ecb3 7a01 	vldmia	r3!, {s14}
   db7c2:	ecf5 7a01 	vldmia	r5!, {s15}
   db7c6:	eeb4 7ae7 	vcmpe.f32	s14, s15
   db7ca:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   db7ce:	bf94      	ite	ls
   db7d0:	2201      	movls	r2, #1
   db7d2:	2200      	movhi	r2, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db7d4:	3601      	adds	r6, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   db7d6:	f804 2f01 	strb.w	r2, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db7da:	f147 0700 	adc.w	r7, r7, #0
   db7de:	e7e9      	b.n	db7b4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0xc8>
   db7e0:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, LessEqual, requires_broadcast);
   db7e4:	4631      	mov	r1, r6
   db7e6:	b1cf      	cbz	r7, db81c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x130>
   db7e8:	a813      	add	r0, sp, #76	; 0x4c
   db7ea:	f7fb f8e2 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db7ee:	4629      	mov	r1, r5
   db7f0:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db7f2:	6876      	ldr	r6, [r6, #4]
   db7f4:	f7fb f8dd 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db7f8:	b105      	cbz	r5, db7fc <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x110>
   db7fa:	686d      	ldr	r5, [r5, #4]
   db7fc:	4621      	mov	r1, r4
   db7fe:	4640      	mov	r0, r8
   db800:	f7fb f8d7 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db804:	b104      	cbz	r4, db808 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x11c>
   db806:	6864      	ldr	r4, [r4, #4]
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
TFLITE_COMPARISON_OP(LessEqual);
   db808:	9402      	str	r4, [sp, #8]
   db80a:	e88d 0120 	stmia.w	sp, {r5, r8}
   db80e:	ab18      	add	r3, sp, #96	; 0x60
   db810:	4632      	mov	r2, r6
   db812:	a913      	add	r1, sp, #76	; 0x4c
   db814:	a822      	add	r0, sp, #136	; 0x88
   db816:	f7fd fb35 	bl	d8e84 <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIlXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   db81a:	e150      	b.n	dbabe <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   db81c:	a818      	add	r0, sp, #96	; 0x60
   db81e:	f7fb f8c8 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db822:	4629      	mov	r1, r5
   db824:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db826:	6876      	ldr	r6, [r6, #4]
   db828:	f7fb f8c3 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db82c:	b105      	cbz	r5, db830 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x144>
   db82e:	686d      	ldr	r5, [r5, #4]
   db830:	4621      	mov	r1, r4
   db832:	a822      	add	r0, sp, #136	; 0x88
   db834:	f7fb f8bd 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db838:	b104      	cbz	r4, db83c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x150>
   db83a:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db83c:	aa22      	add	r2, sp, #136	; 0x88
   db83e:	4641      	mov	r1, r8
   db840:	a818      	add	r0, sp, #96	; 0x60
   db842:	f7fa fe98 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   db846:	3c01      	subs	r4, #1
   db848:	17c1      	asrs	r1, r0, #31
  for (int64_t i = 0; i < flatsize; ++i) {
   db84a:	2200      	movs	r2, #0
   db84c:	2300      	movs	r3, #0
   db84e:	4282      	cmp	r2, r0
   db850:	eb73 0701 	sbcs.w	r7, r3, r1
   db854:	f280 8197 	bge.w	dbb86 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   db858:	f856 7022 	ldr.w	r7, [r6, r2, lsl #2]
   db85c:	f855 e022 	ldr.w	lr, [r5, r2, lsl #2]
   db860:	4577      	cmp	r7, lr
   db862:	bfcc      	ite	gt
   db864:	2700      	movgt	r7, #0
   db866:	2701      	movle	r7, #1
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db868:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   db86a:	f804 7f01 	strb.w	r7, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db86e:	f143 0300 	adc.w	r3, r3, #0
   db872:	e7ec      	b.n	db84e <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x162>
   db874:	f10d 0874 	add.w	r8, sp, #116	; 0x74
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, LessEqual, requires_broadcast);
   db878:	4631      	mov	r1, r6
   db87a:	b1cf      	cbz	r7, db8b0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x1c4>
   db87c:	a813      	add	r0, sp, #76	; 0x4c
   db87e:	f7fb f898 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db882:	4629      	mov	r1, r5
   db884:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db886:	6876      	ldr	r6, [r6, #4]
   db888:	f7fb f893 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db88c:	b105      	cbz	r5, db890 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x1a4>
   db88e:	686d      	ldr	r5, [r5, #4]
   db890:	4621      	mov	r1, r4
   db892:	4640      	mov	r0, r8
   db894:	f7fb f88d 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db898:	b104      	cbz	r4, db89c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x1b0>
   db89a:	6864      	ldr	r4, [r4, #4]
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
TFLITE_COMPARISON_OP(LessEqual);
   db89c:	9402      	str	r4, [sp, #8]
   db89e:	e88d 0120 	stmia.w	sp, {r5, r8}
   db8a2:	ab18      	add	r3, sp, #96	; 0x60
   db8a4:	4632      	mov	r2, r6
   db8a6:	a913      	add	r1, sp, #76	; 0x4c
   db8a8:	a822      	add	r0, sp, #136	; 0x88
   db8aa:	f7fd fb57 	bl	d8f5c <_ZN6tflite13reference_ops29BroadcastComparison4DSlowImplIxXadL_ZNS0_11LessEqualFnIxEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   db8ae:	e106      	b.n	dbabe <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   db8b0:	a818      	add	r0, sp, #96	; 0x60
   db8b2:	f7fb f87e 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db8b6:	4629      	mov	r1, r5
   db8b8:	4640      	mov	r0, r8
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db8ba:	6876      	ldr	r6, [r6, #4]
   db8bc:	f7fb f879 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db8c0:	b105      	cbz	r5, db8c4 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x1d8>
   db8c2:	686d      	ldr	r5, [r5, #4]
   db8c4:	4621      	mov	r1, r4
   db8c6:	a822      	add	r0, sp, #136	; 0x88
   db8c8:	f7fb f873 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db8cc:	b104      	cbz	r4, db8d0 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x1e4>
   db8ce:	6864      	ldr	r4, [r4, #4]
inline void ComparisonImpl(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db8d0:	aa22      	add	r2, sp, #136	; 0x88
   db8d2:	4641      	mov	r1, r8
   db8d4:	a818      	add	r0, sp, #96	; 0x60
   db8d6:	f7fa fe4e 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   db8da:	3d08      	subs	r5, #8
   db8dc:	17c1      	asrs	r1, r0, #31
   db8de:	f1a6 0e08 	sub.w	lr, r6, #8
   db8e2:	3c01      	subs	r4, #1
  for (int64_t i = 0; i < flatsize; ++i) {
   db8e4:	2200      	movs	r2, #0
   db8e6:	2300      	movs	r3, #0
   db8e8:	4282      	cmp	r2, r0
   db8ea:	eb73 0601 	sbcs.w	r6, r3, r1
   db8ee:	f280 814a 	bge.w	dbb86 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x49a>
    output_data[i] = F(input1_data[i], input2_data[i]);
   db8f2:	e9fe 6702 	ldrd	r6, r7, [lr, #8]!
   db8f6:	e9f5 ab02 	ldrd	sl, fp, [r5, #8]!
   db8fa:	45b2      	cmp	sl, r6
   db8fc:	eb7b 0607 	sbcs.w	r6, fp, r7
   db900:	bfac      	ite	ge
   db902:	2601      	movge	r6, #1
   db904:	2600      	movlt	r6, #0
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db906:	3201      	adds	r2, #1
    output_data[i] = F(input1_data[i], input2_data[i]);
   db908:	f804 6f01 	strb.w	r6, [r4, #1]!
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db90c:	f143 0300 	adc.w	r3, r3, #0
   db910:	e7ea      	b.n	db8e8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x1fc>
TF_LITE_QUANTIZE_COMPARISON(Equal);
TF_LITE_QUANTIZE_COMPARISON(NotEqual);
TF_LITE_QUANTIZE_COMPARISON(Greater);
TF_LITE_QUANTIZE_COMPARISON(GreaterEqual);
TF_LITE_QUANTIZE_COMPARISON(Less);
TF_LITE_QUANTIZE_COMPARISON(LessEqual);
   db912:	6933      	ldr	r3, [r6, #16]
   db914:	68f0      	ldr	r0, [r6, #12]
   db916:	f1c3 0900 	rsb	r9, r3, #0
   db91a:	692b      	ldr	r3, [r5, #16]
   db91c:	f1c3 0800 	rsb	r8, r3, #0
   db920:	f00b fb54 	bl	e6fcc <__aeabi_f2d>
   db924:	ec41 0b10 	vmov	d0, r0, r1
   db928:	a910      	add	r1, sp, #64	; 0x40
   db92a:	a80f      	add	r0, sp, #60	; 0x3c
   db92c:	f008 fa9c 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   db930:	68e8      	ldr	r0, [r5, #12]
   db932:	f00b fb4b 	bl	e6fcc <__aeabi_f2d>
   db936:	ec41 0b10 	vmov	d0, r0, r1
   db93a:	a912      	add	r1, sp, #72	; 0x48
   db93c:	a811      	add	r0, sp, #68	; 0x44
   db93e:	f008 fa93 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   db942:	2308      	movs	r3, #8
   db944:	9322      	str	r3, [sp, #136]	; 0x88
   db946:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   db948:	9324      	str	r3, [sp, #144]	; 0x90
   db94a:	9b10      	ldr	r3, [sp, #64]	; 0x40
   db94c:	9325      	str	r3, [sp, #148]	; 0x94
   db94e:	9b11      	ldr	r3, [sp, #68]	; 0x44
   db950:	9327      	str	r3, [sp, #156]	; 0x9c
   db952:	9b12      	ldr	r3, [sp, #72]	; 0x48
   db954:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   db958:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   db95c:	9328      	str	r3, [sp, #160]	; 0xa0
   db95e:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   db962:	4631      	mov	r1, r6
   db964:	a813      	add	r0, sp, #76	; 0x4c
   db966:	b1bf      	cbz	r7, db998 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x2ac>
   db968:	f7fb f823 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db96c:	4629      	mov	r1, r5
   db96e:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db970:	6876      	ldr	r6, [r6, #4]
   db972:	f7fb f81e 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db976:	4621      	mov	r1, r4
   db978:	4640      	mov	r0, r8
   db97a:	686d      	ldr	r5, [r5, #4]
   db97c:	f7fb f819 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db980:	b104      	cbz	r4, db984 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x298>
   db982:	6864      	ldr	r4, [r4, #4]
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
TFLITE_COMPARISON_OP(LessEqual);
   db984:	9402      	str	r4, [sp, #8]
   db986:	e88d 0120 	stmia.w	sp, {r5, r8}
   db98a:	ab18      	add	r3, sp, #96	; 0x60
   db98c:	4632      	mov	r2, r6
   db98e:	a913      	add	r1, sp, #76	; 0x4c
   db990:	a822      	add	r0, sp, #136	; 0x88
   db992:	f7ff fd81 	bl	db498 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIhXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   db996:	e092      	b.n	dbabe <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d2>
   db998:	f7fb f80b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   db99c:	6873      	ldr	r3, [r6, #4]
   db99e:	9305      	str	r3, [sp, #20]
   db9a0:	4629      	mov	r1, r5
   db9a2:	a818      	add	r0, sp, #96	; 0x60
   db9a4:	f7fb f805 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   db9a8:	4621      	mov	r1, r4
   db9aa:	4640      	mov	r0, r8
   db9ac:	f8d5 b004 	ldr.w	fp, [r5, #4]
   db9b0:	f7fa ffff 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   db9b4:	b104      	cbz	r4, db9b8 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x2cc>
   db9b6:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   db9b8:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   db9ba:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   db9bc:	9b24      	ldr	r3, [sp, #144]	; 0x90
   db9be:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   db9c0:	9b25      	ldr	r3, [sp, #148]	; 0x94
   db9c2:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   db9c4:	9b26      	ldr	r3, [sp, #152]	; 0x98
   db9c6:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   db9c8:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   db9ca:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db9cc:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   db9ce:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   db9d0:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   db9d2:	a918      	add	r1, sp, #96	; 0x60
   db9d4:	a813      	add	r0, sp, #76	; 0x4c
   db9d6:	f7fa fdce 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   db9da:	4602      	mov	r2, r0
   db9dc:	17c3      	asrs	r3, r0, #31
   db9de:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   db9e2:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   db9e4:	f04f 0800 	mov.w	r8, #0
   db9e8:	f04f 0900 	mov.w	r9, #0
   db9ec:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   db9f0:	4590      	cmp	r8, r2
   db9f2:	eb79 0303 	sbcs.w	r3, r9, r3
   db9f6:	f280 80b4 	bge.w	dbb62 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x476>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   db9fa:	f81b 5008 	ldrb.w	r5, [fp, r8]
   db9fe:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dba00:	9a08      	ldr	r2, [sp, #32]
   dba02:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dba04:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dba06:	9b05      	ldr	r3, [sp, #20]
   dba08:	f813 0008 	ldrb.w	r0, [r3, r8]
   dba0c:	9b06      	ldr	r3, [sp, #24]
   dba0e:	4418      	add	r0, r3
   dba10:	40b8      	lsls	r0, r7
   dba12:	f7fa fded 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dba16:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dba18:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   dba1a:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   dba1c:	990a      	ldr	r1, [sp, #40]	; 0x28
   dba1e:	4628      	mov	r0, r5
   dba20:	f7fa fde6 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   dba24:	4582      	cmp	sl, r0
   dba26:	bfcc      	ite	gt
   dba28:	2000      	movgt	r0, #0
   dba2a:	2001      	movle	r0, #1
   dba2c:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   dba30:	f118 0801 	adds.w	r8, r8, #1
   dba34:	f149 0900 	adc.w	r9, r9, #0
   dba38:	e7d8      	b.n	db9ec <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x300>
   dba3a:	6933      	ldr	r3, [r6, #16]
   dba3c:	68f0      	ldr	r0, [r6, #12]
   dba3e:	f1c3 0900 	rsb	r9, r3, #0
   dba42:	692b      	ldr	r3, [r5, #16]
   dba44:	f1c3 0800 	rsb	r8, r3, #0
   dba48:	f00b fac0 	bl	e6fcc <__aeabi_f2d>
   dba4c:	ec41 0b10 	vmov	d0, r0, r1
   dba50:	a910      	add	r1, sp, #64	; 0x40
   dba52:	a80f      	add	r0, sp, #60	; 0x3c
   dba54:	f008 fa08 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   dba58:	68e8      	ldr	r0, [r5, #12]
   dba5a:	f00b fab7 	bl	e6fcc <__aeabi_f2d>
   dba5e:	ec41 0b10 	vmov	d0, r0, r1
   dba62:	a912      	add	r1, sp, #72	; 0x48
   dba64:	a811      	add	r0, sp, #68	; 0x44
   dba66:	f008 f9ff 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
   dba6a:	2308      	movs	r3, #8
   dba6c:	9322      	str	r3, [sp, #136]	; 0x88
   dba6e:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   dba70:	9324      	str	r3, [sp, #144]	; 0x90
   dba72:	9b10      	ldr	r3, [sp, #64]	; 0x40
   dba74:	9325      	str	r3, [sp, #148]	; 0x94
   dba76:	9b11      	ldr	r3, [sp, #68]	; 0x44
   dba78:	9327      	str	r3, [sp, #156]	; 0x9c
   dba7a:	9b12      	ldr	r3, [sp, #72]	; 0x48
   dba7c:	f8cd 8098 	str.w	r8, [sp, #152]	; 0x98
   dba80:	f8cd 908c 	str.w	r9, [sp, #140]	; 0x8c
   dba84:	9328      	str	r3, [sp, #160]	; 0xa0
   dba86:	f10d 0874 	add.w	r8, sp, #116	; 0x74
   dba8a:	4631      	mov	r1, r6
   dba8c:	a813      	add	r0, sp, #76	; 0x4c
   dba8e:	b1c7      	cbz	r7, dbac2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3d6>
   dba90:	f7fa ff8f 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dba94:	4629      	mov	r1, r5
   dba96:	a818      	add	r0, sp, #96	; 0x60
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dba98:	6876      	ldr	r6, [r6, #4]
   dba9a:	f7fa ff8a 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dba9e:	4621      	mov	r1, r4
   dbaa0:	4640      	mov	r0, r8
   dbaa2:	686d      	ldr	r5, [r5, #4]
   dbaa4:	f7fa ff85 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dbaa8:	b104      	cbz	r4, dbaac <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3c0>
   dbaaa:	6864      	ldr	r4, [r4, #4]
TFLITE_COMPARISON_OP(Equal);
TFLITE_COMPARISON_OP(NotEqual);
TFLITE_COMPARISON_OP(Greater);
TFLITE_COMPARISON_OP(GreaterEqual);
TFLITE_COMPARISON_OP(Less);
TFLITE_COMPARISON_OP(LessEqual);
   dbaac:	9402      	str	r4, [sp, #8]
   dbaae:	e88d 0120 	stmia.w	sp, {r5, r8}
   dbab2:	ab18      	add	r3, sp, #96	; 0x60
   dbab4:	4632      	mov	r2, r6
   dbab6:	a913      	add	r1, sp, #76	; 0x4c
   dbab8:	a822      	add	r0, sp, #136	; 0x88
   dbaba:	f7ff fd82 	bl	db5c2 <_ZN6tflite13reference_ops36BroadcastComparison4DSlowWithScalingIaXadL_ZNS0_11LessEqualFnIlEEbT_S3_EEEEvRKNS_16ComparisonParamsERKNS_12RuntimeShapeEPKS3_S9_SB_S9_Pb>
   dbabe:	4640      	mov	r0, r8
   dbac0:	e050      	b.n	dbb64 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x478>
   dbac2:	f7fa ff76 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dbac6:	6873      	ldr	r3, [r6, #4]
   dbac8:	9305      	str	r3, [sp, #20]
   dbaca:	4629      	mov	r1, r5
   dbacc:	a818      	add	r0, sp, #96	; 0x60
   dbace:	f7fa ff70 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dbad2:	4621      	mov	r1, r4
   dbad4:	4640      	mov	r0, r8
   dbad6:	f8d5 b004 	ldr.w	fp, [r5, #4]
   dbada:	f7fa ff6a 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dbade:	b104      	cbz	r4, dbae2 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x3f6>
   dbae0:	6864      	ldr	r4, [r4, #4]
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
  int32 input1_offset = op_params.input1_offset;
   dbae2:	9b23      	ldr	r3, [sp, #140]	; 0x8c
   dbae4:	9306      	str	r3, [sp, #24]
  int32 input1_multiplier = op_params.input1_multiplier;
   dbae6:	9b24      	ldr	r3, [sp, #144]	; 0x90
   dbae8:	9307      	str	r3, [sp, #28]
  int input1_shift = op_params.input1_shift;
   dbaea:	9b25      	ldr	r3, [sp, #148]	; 0x94
   dbaec:	9308      	str	r3, [sp, #32]
  int32 input2_offset = op_params.input2_offset;
   dbaee:	9b26      	ldr	r3, [sp, #152]	; 0x98
   dbaf0:	9309      	str	r3, [sp, #36]	; 0x24
  int32 input2_multiplier = op_params.input2_multiplier;
   dbaf2:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   dbaf4:	930a      	str	r3, [sp, #40]	; 0x28
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   dbaf6:	aa1d      	add	r2, sp, #116	; 0x74
  int32 input1_offset = op_params.input1_offset;
  int32 input1_multiplier = op_params.input1_multiplier;
  int input1_shift = op_params.input1_shift;
  int32 input2_offset = op_params.input2_offset;
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;
   dbaf8:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   dbafa:	930b      	str	r3, [sp, #44]	; 0x2c

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
   dbafc:	a918      	add	r1, sp, #96	; 0x60
   dbafe:	a813      	add	r0, sp, #76	; 0x4c
   dbb00:	f7fa fd39 	bl	d6576 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_S2_>
   dbb04:	4602      	mov	r2, r0
   dbb06:	17c3      	asrs	r3, r0, #31
   dbb08:	e9cd 230c 	strd	r2, r3, [sp, #48]	; 0x30
template <typename T, ComparisonFn<int32> F>
inline void ComparisonWithScaling(
    const ComparisonParams& op_params, const RuntimeShape& input1_shape,
    const T* input1_data, const RuntimeShape& input2_shape,
    const T* input2_data, const RuntimeShape& output_shape, bool* output_data) {
  int left_shift = op_params.left_shift;
   dbb0c:	9f22      	ldr	r7, [sp, #136]	; 0x88
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   dbb0e:	f04f 0800 	mov.w	r8, #0
   dbb12:	f04f 0900 	mov.w	r9, #0
   dbb16:	e9dd 230c 	ldrd	r2, r3, [sp, #48]	; 0x30
   dbb1a:	4590      	cmp	r8, r2
   dbb1c:	eb79 0303 	sbcs.w	r3, r9, r3
   dbb20:	da1f      	bge.n	dbb62 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x476>
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dbb22:	f91b 5008 	ldrsb.w	r5, [fp, r8]
   dbb26:	9b09      	ldr	r3, [sp, #36]	; 0x24
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dbb28:	9a08      	ldr	r2, [sp, #32]
   dbb2a:	9907      	ldr	r1, [sp, #28]
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dbb2c:	441d      	add	r5, r3
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dbb2e:	9b05      	ldr	r3, [sp, #20]
   dbb30:	f913 0008 	ldrsb.w	r0, [r3, r8]
   dbb34:	9b06      	ldr	r3, [sp, #24]
   dbb36:	4418      	add	r0, r3
   dbb38:	40b8      	lsls	r0, r7
   dbb3a:	f7fa fd59 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
    const int32 input1_val = input1_offset + input1_data[i];
    const int32 input2_val = input2_offset + input2_data[i];
    const int32 shifted_input1_val = input1_val * (1 << left_shift);
    const int32 shifted_input2_val = input2_val * (1 << left_shift);
   dbb3e:	40bd      	lsls	r5, r7
    const int32 scaled_input1_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input1_val, input1_multiplier, input1_shift);
   dbb40:	4682      	mov	sl, r0
    const int32 scaled_input2_val =
        MultiplyByQuantizedMultiplierSmallerThanOneExp(
            shifted_input2_val, input2_multiplier, input2_shift);
   dbb42:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   dbb44:	990a      	ldr	r1, [sp, #40]	; 0x28
   dbb46:	4628      	mov	r0, r5
   dbb48:	f7fa fd52 	bl	d65f0 <_ZN6tflite46MultiplyByQuantizedMultiplierSmallerThanOneExpElli>
    output_data[i] = F(scaled_input1_val, scaled_input2_val);
   dbb4c:	4582      	cmp	sl, r0
   dbb4e:	bfcc      	ite	gt
   dbb50:	2000      	movgt	r0, #0
   dbb52:	2001      	movle	r0, #1
   dbb54:	f804 0008 	strb.w	r0, [r4, r8]
  int32 input2_multiplier = op_params.input2_multiplier;
  int input2_shift = op_params.input2_shift;

  const int64_t flatsize =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int64_t i = 0; i < flatsize; ++i) {
   dbb58:	f118 0801 	adds.w	r8, r8, #1
   dbb5c:	f149 0900 	adc.w	r9, r9, #0
   dbb60:	e7d9      	b.n	dbb16 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x42a>
   dbb62:	a81d      	add	r0, sp, #116	; 0x74
   dbb64:	f7fa fc75 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   dbb68:	a818      	add	r0, sp, #96	; 0x60
   dbb6a:	f7fa fc72 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   dbb6e:	a813      	add	r0, sp, #76	; 0x4c
   dbb70:	f7fa fc6f 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
   dbb74:	2000      	movs	r0, #0
   dbb76:	e00e      	b.n	dbb96 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x4aa>
                                     requires_broadcast);
      break;
    default:
      context->ReportError(context,
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
   dbb78:	4650      	mov	r0, sl
   dbb7a:	f8da 3014 	ldr.w	r3, [sl, #20]
   dbb7e:	4907      	ldr	r1, [pc, #28]	; (dbb9c <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x4b0>)
   dbb80:	4798      	blx	r3
      return kTfLiteError;
   dbb82:	2001      	movs	r0, #1
   dbb84:	e007      	b.n	dbb96 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x4aa>
      break;
    case kTfLiteInt32:
      TF_LITE_COMPARISON(int32_t, LessEqual, requires_broadcast);
      break;
    case kTfLiteInt64:
      TF_LITE_COMPARISON(int64_t, LessEqual, requires_broadcast);
   dbb86:	a822      	add	r0, sp, #136	; 0x88
   dbb88:	f7fa fc63 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   dbb8c:	4640      	mov	r0, r8
   dbb8e:	f7fa fc60 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   dbb92:	a818      	add	r0, sp, #96	; 0x60
   dbb94:	e7ec      	b.n	dbb70 <_ZN6tflite3ops5micro11comparisons12_GLOBAL__N_113LessEqualEvalEP13TfLiteContextP10TfLiteNode+0x484>
                           "Does not support type %d, requires float|int|uint8",
                           input1->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   dbb96:	b02b      	add	sp, #172	; 0xac
   dbb98:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dbb9c:	000e99bf 	.word	0x000e99bf

000dbba0 <_ZN6tflite3ops5micro4conv4InitEP13TfLiteContextPKcj>:
  return kTfLiteOk;
}

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
   dbba0:	2000      	movs	r0, #0
   dbba2:	4770      	bx	lr

000dbba4 <_ZN6tflite3ops5micro4conv4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
   dbba4:	4770      	bx	lr

000dbba6 <_ZN6tflite3ops5micro4conv7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   dbba6:	2000      	movs	r0, #0
   dbba8:	4770      	bx	lr

000dbbaa <_ZNK6tflite12RuntimeShape8FlatSizeEv>:
    BuildFrom<const std::initializer_list<int>>(init_list);
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
   dbbaa:	b510      	push	{r4, lr}

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   dbbac:	6801      	ldr	r1, [r0, #0]
   dbbae:	2904      	cmp	r1, #4
   dbbb0:	bfcc      	ite	gt
   dbbb2:	6843      	ldrgt	r3, [r0, #4]
   dbbb4:	1d03      	addle	r3, r0, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dbbb6:	2200      	movs	r2, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
   dbbb8:	2001      	movs	r0, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dbbba:	428a      	cmp	r2, r1
   dbbbc:	da04      	bge.n	dbbc8 <_ZNK6tflite12RuntimeShape8FlatSizeEv+0x1e>
      buffer_size *= dims_data[i];
   dbbbe:	f853 4022 	ldr.w	r4, [r3, r2, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dbbc2:	3201      	adds	r2, #1
      buffer_size *= dims_data[i];
   dbbc4:	4360      	muls	r0, r4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dbbc6:	e7f8      	b.n	dbbba <_ZNK6tflite12RuntimeShape8FlatSizeEv+0x10>
      buffer_size *= dims_data[i];
    }
    return buffer_size;
  }
   dbbc8:	bd10      	pop	{r4, pc}

000dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>:
  return MatchingArraySize(array1, index1, args...);
}

// Get common shape dim, DCHECKing that they all agree.
inline int MatchingDim(const RuntimeShape& shape1, int index1,
                       const RuntimeShape& shape2, int index2) {
   dbbca:	b570      	push	{r4, r5, r6, lr}
   dbbcc:	4615      	mov	r5, r2
   dbbce:	461e      	mov	r6, r3
  TFLITE_DCHECK_EQ(shape1.Dims(index1), shape2.Dims(index2));
   dbbd0:	f7fa fc4a 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dbbd4:	4631      	mov	r1, r6
   dbbd6:	4604      	mov	r4, r0
   dbbd8:	4628      	mov	r0, r5
   dbbda:	f7fa fc45 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dbbde:	4284      	cmp	r4, r0
   dbbe0:	d001      	beq.n	dbbe6 <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i+0x1c>
   dbbe2:	f008 fbb3 	bl	e434c <abort>
  return shape1.Dims(index1);
}
   dbbe6:	bd70      	pop	{r4, r5, r6, pc}

000dbbe8 <_ZN6tflite29MultiplyByQuantizedMultiplierElli>:
  return SaturatingRoundingDoublingHighMul(x * (1 << left_shift),
                                           quantized_multiplier);
}

inline int32 MultiplyByQuantizedMultiplier(int32 x, int32 quantized_multiplier,
                                           int shift) {
   dbbe8:	b570      	push	{r4, r5, r6, lr}
  using gemmlowp::RoundingDivideByPOT;
  using gemmlowp::SaturatingRoundingDoublingHighMul;
  int left_shift = shift > 0 ? shift : 0;
   dbbea:	ea22 74e2 	bic.w	r4, r2, r2, asr #31
  int right_shift = shift > 0 ? 0 : -shift;
   dbbee:	2a00      	cmp	r2, #0
  return RoundingDivideByPOT(SaturatingRoundingDoublingHighMul(
   dbbf0:	fa00 f004 	lsl.w	r0, r0, r4
inline int32 MultiplyByQuantizedMultiplier(int32 x, int32 quantized_multiplier,
                                           int shift) {
  using gemmlowp::RoundingDivideByPOT;
  using gemmlowp::SaturatingRoundingDoublingHighMul;
  int left_shift = shift > 0 ? shift : 0;
  int right_shift = shift > 0 ? 0 : -shift;
   dbbf4:	bfd4      	ite	le
   dbbf6:	4256      	negle	r6, r2
   dbbf8:	2600      	movgt	r6, #0
// This function implements the same computation as the ARMv7 NEON VQRDMULH
// instruction.
template <>
inline std::int32_t SaturatingRoundingDoublingHighMul(std::int32_t a,
                                                      std::int32_t b) {
  bool overflow = a == b && a == std::numeric_limits<std::int32_t>::min();
   dbbfa:	4288      	cmp	r0, r1
   dbbfc:	d104      	bne.n	dbc08 <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x20>
   dbbfe:	f101 4300 	add.w	r3, r1, #2147483648	; 0x80000000
   dbc02:	425a      	negs	r2, r3
   dbc04:	415a      	adcs	r2, r3
   dbc06:	e000      	b.n	dbc0a <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x22>
   dbc08:	2200      	movs	r2, #0
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
   dbc0a:	fb80 4501 	smull	r4, r5, r0, r1
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
   dbc0e:	2c00      	cmp	r4, #0
   dbc10:	f175 0300 	sbcs.w	r3, r5, #0
   dbc14:	4b18      	ldr	r3, [pc, #96]	; (dbc78 <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x90>)
   dbc16:	bfa8      	it	ge
   dbc18:	f04f 4380 	movge.w	r3, #1073741824	; 0x40000000
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
   dbc1c:	b97a      	cbnz	r2, dbc3e <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x56>
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
   dbc1e:	18e4      	adds	r4, r4, r3
   dbc20:	eb45 75e3 	adc.w	r5, r5, r3, asr #31
   dbc24:	2c00      	cmp	r4, #0
   dbc26:	f175 0300 	sbcs.w	r3, r5, #0
   dbc2a:	da04      	bge.n	dbc36 <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x4e>
   dbc2c:	f06f 4200 	mvn.w	r2, #2147483648	; 0x80000000
   dbc30:	2300      	movs	r3, #0
   dbc32:	18a4      	adds	r4, r4, r2
   dbc34:	415d      	adcs	r5, r3
   dbc36:	0fe0      	lsrs	r0, r4, #31
   dbc38:	ea40 0445 	orr.w	r4, r0, r5, lsl #1
   dbc3c:	e001      	b.n	dbc42 <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x5a>
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
   dbc3e:	f06f 4400 	mvn.w	r4, #2147483648	; 0x80000000
// Correctly-rounded-to-nearest division by a power-of-two.
// Also known as a rounding arithmetic right shift.
template <typename IntegerType>
inline IntegerType RoundingDivideByPOT(IntegerType x, int exponent) {
  assert(exponent >= 0);
  assert(exponent <= 31);
   dbc42:	2e1f      	cmp	r6, #31
   dbc44:	dd06      	ble.n	dbc54 <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x6c>
   dbc46:	4b0d      	ldr	r3, [pc, #52]	; (dbc7c <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x94>)
   dbc48:	4a0d      	ldr	r2, [pc, #52]	; (dbc80 <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x98>)
   dbc4a:	480e      	ldr	r0, [pc, #56]	; (dbc84 <_ZN6tflite29MultiplyByQuantizedMultiplierElli+0x9c>)
   dbc4c:	f240 1167 	movw	r1, #359	; 0x167
   dbc50:	f008 fb8c 	bl	e436c <__assert_func>
  const IntegerType mask = Dup<IntegerType>((1ll << exponent) - 1);
   dbc54:	4632      	mov	r2, r6
   dbc56:	2001      	movs	r0, #1
   dbc58:	2100      	movs	r1, #0
   dbc5a:	f00b f847 	bl	e6cec <__aeabi_llsl>
   dbc5e:	3801      	subs	r0, #1
  return RoundingDivideByPOT(SaturatingRoundingDoublingHighMul(
                                 x * (1 << left_shift), quantized_multiplier),
                             right_shift);
   dbc60:	ea00 0304 	and.w	r3, r0, r4
   dbc64:	1040      	asrs	r0, r0, #1
   dbc66:	eb00 70d4 	add.w	r0, r0, r4, lsr #31
   dbc6a:	4134      	asrs	r4, r6
}
   dbc6c:	4283      	cmp	r3, r0
   dbc6e:	bfd4      	ite	le
   dbc70:	4620      	movle	r0, r4
   dbc72:	1c60      	addgt	r0, r4, #1
   dbc74:	bd70      	pop	{r4, r5, r6, pc}
   dbc76:	bf00      	nop
   dbc78:	c0000001 	.word	0xc0000001
   dbc7c:	000e9701 	.word	0x000e9701
   dbc80:	000e9b66 	.word	0x000e9b66
   dbc84:	000e9662 	.word	0x000e9662

000dbc88 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_>:
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const float* input_data, const RuntimeShape& filter_shape,
                 const float* filter_data, const RuntimeShape& bias_shape,
                 const float* bias_data, const RuntimeShape& output_shape,
                 float* output_data, const RuntimeShape& im2col_shape,
                 float* im2col_data) {
   dbc88:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dbc8c:	ed2d 8b04 	vpush	{d8-d9}
   dbc90:	b09b      	sub	sp, #108	; 0x6c
   dbc92:	4699      	mov	r9, r3
  const int stride_width = params.stride_width;
   dbc94:	f9b0 300a 	ldrsh.w	r3, [r0, #10]
   dbc98:	930a      	str	r3, [sp, #40]	; 0x28
  const int stride_height = params.stride_height;
   dbc9a:	f9b0 300c 	ldrsh.w	r3, [r0, #12]
   dbc9e:	930b      	str	r3, [sp, #44]	; 0x2c
  const int dilation_width_factor = params.dilation_width_factor;
   dbca0:	f9b0 300e 	ldrsh.w	r3, [r0, #14]
   dbca4:	930c      	str	r3, [sp, #48]	; 0x30
  const int dilation_height_factor = params.dilation_height_factor;
   dbca6:	f9b0 3010 	ldrsh.w	r3, [r0, #16]
   dbcaa:	930d      	str	r3, [sp, #52]	; 0x34
  const int pad_width = params.padding_values.width;
   dbcac:	f9b0 3002 	ldrsh.w	r3, [r0, #2]
   dbcb0:	930e      	str	r3, [sp, #56]	; 0x38
  const int pad_height = params.padding_values.height;
   dbcb2:	f9b0 3004 	ldrsh.w	r3, [r0, #4]
   dbcb6:	930f      	str	r3, [sp, #60]	; 0x3c
  const float output_activation_min = params.float_activation_min;
  const float output_activation_max = params.float_activation_max;
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dbcb8:	680b      	ldr	r3, [r1, #0]
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const float* input_data, const RuntimeShape& filter_shape,
                 const float* filter_data, const RuntimeShape& bias_shape,
                 const float* bias_data, const RuntimeShape& output_shape,
                 float* output_data, const RuntimeShape& im2col_shape,
                 float* im2col_data) {
   dbcba:	9219      	str	r2, [sp, #100]	; 0x64
  const int dilation_height_factor = params.dilation_height_factor;
  const int pad_width = params.padding_values.width;
  const int pad_height = params.padding_values.height;
  const float output_activation_min = params.float_activation_min;
  const float output_activation_max = params.float_activation_max;
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dbcbc:	2b04      	cmp	r3, #4
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const float* input_data, const RuntimeShape& filter_shape,
                 const float* filter_data, const RuntimeShape& bias_shape,
                 const float* bias_data, const RuntimeShape& output_shape,
                 float* output_data, const RuntimeShape& im2col_shape,
                 float* im2col_data) {
   dbcbe:	468b      	mov	fp, r1
   dbcc0:	f8dd a0ac 	ldr.w	sl, [sp, #172]	; 0xac
  const int stride_height = params.stride_height;
  const int dilation_width_factor = params.dilation_width_factor;
  const int dilation_height_factor = params.dilation_height_factor;
  const int pad_width = params.padding_values.width;
  const int pad_height = params.padding_values.height;
  const float output_activation_min = params.float_activation_min;
   dbcc4:	edd0 8a0c 	vldr	s17, [r0, #48]	; 0x30
  const float output_activation_max = params.float_activation_max;
   dbcc8:	ed90 9a0d 	vldr	s18, [r0, #52]	; 0x34
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dbccc:	d001      	beq.n	dbcd2 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x4a>
   dbcce:	f008 fb3d 	bl	e434c <abort>
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
   dbcd2:	f8d9 3000 	ldr.w	r3, [r9]
   dbcd6:	2b04      	cmp	r3, #4
   dbcd8:	d1f9      	bne.n	dbcce <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x46>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   dbcda:	f8da 3000 	ldr.w	r3, [sl]
   dbcde:	2b04      	cmp	r3, #4
   dbce0:	d1f5      	bne.n	dbcce <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x46>

  (void)im2col_data;   // only used in optimized code.
  (void)im2col_shape;  // only used in optimized code.
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dbce2:	2300      	movs	r3, #0
   dbce4:	4619      	mov	r1, r3
   dbce6:	4652      	mov	r2, sl
   dbce8:	4658      	mov	r0, fp
   dbcea:	f7ff ff6e 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   dbcee:	2303      	movs	r3, #3
   dbcf0:	4619      	mov	r1, r3
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  (void)im2col_data;   // only used in optimized code.
  (void)im2col_shape;  // only used in optimized code.
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dbcf2:	9010      	str	r0, [sp, #64]	; 0x40
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   dbcf4:	464a      	mov	r2, r9
   dbcf6:	4658      	mov	r0, fp
   dbcf8:	f7ff ff67 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
   dbcfc:	2303      	movs	r3, #3
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  (void)im2col_data;   // only used in optimized code.
  (void)im2col_shape;  // only used in optimized code.
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   dbcfe:	9011      	str	r0, [sp, #68]	; 0x44
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
   dbd00:	4652      	mov	r2, sl
   dbd02:	2100      	movs	r1, #0
   dbd04:	4648      	mov	r0, r9
   dbd06:	f7ff ff60 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  if (bias_data) {
   dbd0a:	9b2a      	ldr	r3, [sp, #168]	; 0xa8

  (void)im2col_data;   // only used in optimized code.
  (void)im2col_shape;  // only used in optimized code.
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
   dbd0c:	9009      	str	r0, [sp, #36]	; 0x24
  if (bias_data) {
   dbd0e:	b12b      	cbz	r3, dbd1c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x94>
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
   dbd10:	9829      	ldr	r0, [sp, #164]	; 0xa4
   dbd12:	f7ff ff4a 	bl	dbbaa <_ZNK6tflite12RuntimeShape8FlatSizeEv>
   dbd16:	9b09      	ldr	r3, [sp, #36]	; 0x24
   dbd18:	4283      	cmp	r3, r0
   dbd1a:	d1d8      	bne.n	dbcce <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x46>
  }
  const int input_height = input_shape.Dims(1);
   dbd1c:	2101      	movs	r1, #1
   dbd1e:	4658      	mov	r0, fp
   dbd20:	f7fa fba2 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   dbd24:	2102      	movs	r1, #2
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
   dbd26:	9012      	str	r0, [sp, #72]	; 0x48
  const int input_width = input_shape.Dims(2);
   dbd28:	4658      	mov	r0, fp
   dbd2a:	f7fa fb9d 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_height = filter_shape.Dims(1);
   dbd2e:	2101      	movs	r1, #1
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   dbd30:	9013      	str	r0, [sp, #76]	; 0x4c
  const int filter_height = filter_shape.Dims(1);
   dbd32:	4648      	mov	r0, r9
   dbd34:	f7fa fb98 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_width = filter_shape.Dims(2);
   dbd38:	2102      	movs	r1, #2
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
   dbd3a:	9014      	str	r0, [sp, #80]	; 0x50
  const int filter_width = filter_shape.Dims(2);
   dbd3c:	4648      	mov	r0, r9
   dbd3e:	f7fa fb93 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   dbd42:	2101      	movs	r1, #1
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
   dbd44:	9015      	str	r0, [sp, #84]	; 0x54
  const int output_height = output_shape.Dims(1);
   dbd46:	4650      	mov	r0, sl
   dbd48:	f7fa fb8e 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   dbd4c:	2102      	movs	r1, #2
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   dbd4e:	9016      	str	r0, [sp, #88]	; 0x58
  const int output_width = output_shape.Dims(2);
   dbd50:	4650      	mov	r0, sl
   dbd52:	f7fa fb89 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  for (int batch = 0; batch < batches; ++batch) {
   dbd56:	2500      	movs	r5, #0
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   dbd58:	9017      	str	r0, [sp, #92]	; 0x5c
  for (int batch = 0; batch < batches; ++batch) {
   dbd5a:	9b10      	ldr	r3, [sp, #64]	; 0x40
   dbd5c:	429d      	cmp	r5, r3
   dbd5e:	f280 809c 	bge.w	dbe9a <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x212>
   dbd62:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   dbd64:	425b      	negs	r3, r3
   dbd66:	9308      	str	r3, [sp, #32]
   dbd68:	2300      	movs	r3, #0
   dbd6a:	9303      	str	r3, [sp, #12]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   dbd6c:	9b03      	ldr	r3, [sp, #12]
   dbd6e:	9a16      	ldr	r2, [sp, #88]	; 0x58
   dbd70:	4293      	cmp	r3, r2
   dbd72:	f280 8090 	bge.w	dbe96 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x20e>
   dbd76:	9b0e      	ldr	r3, [sp, #56]	; 0x38
   dbd78:	425b      	negs	r3, r3
   dbd7a:	9307      	str	r3, [sp, #28]
   dbd7c:	2300      	movs	r3, #0
   dbd7e:	9304      	str	r3, [sp, #16]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   dbd80:	9b04      	ldr	r3, [sp, #16]
   dbd82:	9a17      	ldr	r2, [sp, #92]	; 0x5c
   dbd84:	4293      	cmp	r3, r2
   dbd86:	da7e      	bge.n	dbe86 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x1fe>
   dbd88:	2400      	movs	r4, #0
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dbd8a:	9b09      	ldr	r3, [sp, #36]	; 0x24
   dbd8c:	429c      	cmp	r4, r3
   dbd8e:	da72      	bge.n	dbe76 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x1ee>
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          float total = 0.f;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   dbd90:	2300      	movs	r3, #0
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dbd92:	9e08      	ldr	r6, [sp, #32]
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          float total = 0.f;
   dbd94:	ed9f 8a43 	vldr	s16, [pc, #268]	; dbea4 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x21c>
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   dbd98:	9305      	str	r3, [sp, #20]
   dbd9a:	9b05      	ldr	r3, [sp, #20]
   dbd9c:	9a14      	ldr	r2, [sp, #80]	; 0x50
   dbd9e:	4293      	cmp	r3, r2
   dbda0:	da42      	bge.n	dbe28 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x1a0>
   dbda2:	2300      	movs	r3, #0
   dbda4:	9f07      	ldr	r7, [sp, #28]
   dbda6:	9306      	str	r3, [sp, #24]
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   dbda8:	9b06      	ldr	r3, [sp, #24]
   dbdaa:	9a15      	ldr	r2, [sp, #84]	; 0x54
   dbdac:	4293      	cmp	r3, r2
   dbdae:	da35      	bge.n	dbe1c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x194>
   dbdb0:	f04f 0800 	mov.w	r8, #0
              for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
   dbdb4:	9b11      	ldr	r3, [sp, #68]	; 0x44
   dbdb6:	4598      	cmp	r8, r3
   dbdb8:	da2a      	bge.n	dbe10 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x188>
                const int in_x = in_x_origin + dilation_width_factor * filter_x;
                const int in_y =
                    in_y_origin + dilation_height_factor * filter_y;
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
   dbdba:	2f00      	cmp	r7, #0
   dbdbc:	db25      	blt.n	dbe0a <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x182>
   dbdbe:	9b13      	ldr	r3, [sp, #76]	; 0x4c
   dbdc0:	42bb      	cmp	r3, r7
   dbdc2:	dd22      	ble.n	dbe0a <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x182>
   dbdc4:	2e00      	cmp	r6, #0
   dbdc6:	db20      	blt.n	dbe0a <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x182>
   dbdc8:	9b12      	ldr	r3, [sp, #72]	; 0x48
   dbdca:	42b3      	cmp	r3, r6
   dbdcc:	dd1d      	ble.n	dbe0a <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x182>
                    (in_y < input_height)) {
                  float input_value = input_data[Offset(
   dbdce:	463b      	mov	r3, r7
   dbdd0:	4632      	mov	r2, r6
   dbdd2:	4629      	mov	r1, r5
   dbdd4:	f8cd 8000 	str.w	r8, [sp]
   dbdd8:	4658      	mov	r0, fp
   dbdda:	f7fa fbaa 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                      input_shape, batch, in_y, in_x, in_channel)];
                  float filter_value =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
   dbdde:	9b06      	ldr	r3, [sp, #24]
                    in_y_origin + dilation_height_factor * filter_y;
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height)) {
                  float input_value = input_data[Offset(
   dbde0:	9018      	str	r0, [sp, #96]	; 0x60
                      input_shape, batch, in_y, in_x, in_channel)];
                  float filter_value =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
   dbde2:	9a05      	ldr	r2, [sp, #20]
   dbde4:	f8cd 8000 	str.w	r8, [sp]
   dbde8:	4621      	mov	r1, r4
   dbdea:	4648      	mov	r0, r9
   dbdec:	f7fa fba1 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height)) {
                  float input_value = input_data[Offset(
                      input_shape, batch, in_y, in_x, in_channel)];
   dbdf0:	9a18      	ldr	r2, [sp, #96]	; 0x60
   dbdf2:	9b19      	ldr	r3, [sp, #100]	; 0x64
   dbdf4:	eb03 0382 	add.w	r3, r3, r2, lsl #2
                  float filter_value =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
                                         filter_x, in_channel)];
   dbdf8:	9a28      	ldr	r2, [sp, #160]	; 0xa0
                  total += (input_value * filter_value);
   dbdfa:	ed93 7a00 	vldr	s14, [r3]
                    (in_y < input_height)) {
                  float input_value = input_data[Offset(
                      input_shape, batch, in_y, in_x, in_channel)];
                  float filter_value =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
                                         filter_x, in_channel)];
   dbdfe:	eb02 0080 	add.w	r0, r2, r0, lsl #2
                  total += (input_value * filter_value);
   dbe02:	edd0 7a00 	vldr	s15, [r0]
   dbe06:	eea7 8a27 	vfma.f32	s16, s14, s15
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          float total = 0.f;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
              for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
   dbe0a:	f108 0801 	add.w	r8, r8, #1
   dbe0e:	e7d1      	b.n	dbdb4 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x12c>
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          float total = 0.f;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   dbe10:	9b06      	ldr	r3, [sp, #24]
   dbe12:	3301      	adds	r3, #1
   dbe14:	9306      	str	r3, [sp, #24]
   dbe16:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   dbe18:	441f      	add	r7, r3
   dbe1a:	e7c5      	b.n	dbda8 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x120>
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          float total = 0.f;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   dbe1c:	9b05      	ldr	r3, [sp, #20]
   dbe1e:	3301      	adds	r3, #1
   dbe20:	9305      	str	r3, [sp, #20]
   dbe22:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   dbe24:	441e      	add	r6, r3
   dbe26:	e7b8      	b.n	dbd9a <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x112>
                }
              }
            }
          }
          float bias_value = 0.0f;
          if (bias_data) {
   dbe28:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
   dbe2a:	b123      	cbz	r3, dbe36 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x1ae>
            bias_value = bias_data[out_channel];
   dbe2c:	eb03 0384 	add.w	r3, r3, r4, lsl #2
   dbe30:	edd3 9a00 	vldr	s19, [r3]
   dbe34:	e001      	b.n	dbe3a <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x1b2>
                  total += (input_value * filter_value);
                }
              }
            }
          }
          float bias_value = 0.0f;
   dbe36:	eddf 9a1b 	vldr	s19, [pc, #108]	; dbea4 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x21c>
          if (bias_data) {
            bias_value = bias_data[out_channel];
          }
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
   dbe3a:	9400      	str	r4, [sp, #0]
   dbe3c:	9b04      	ldr	r3, [sp, #16]
   dbe3e:	9a03      	ldr	r2, [sp, #12]
   dbe40:	4629      	mov	r1, r5
   dbe42:	4650      	mov	r0, sl
   dbe44:	f7fa fb75 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              ActivationFunctionWithMinMax(total + bias_value,
   dbe48:	ee78 7a29 	vadd.f32	s15, s16, s19
          }
          float bias_value = 0.0f;
          if (bias_data) {
            bias_value = bias_data[out_channel];
          }
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
   dbe4c:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
	return __b;
      return __a;
   dbe4e:	eef4 8ae7 	vcmpe.f32	s17, s15
   dbe52:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   dbe56:	bfc8      	it	gt
   dbe58:	eef0 7a68 	vmovgt.f32	s15, s17
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   dbe5c:	eeb4 9a67 	vcmp.f32	s18, s15
   dbe60:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   dbe64:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   dbe68:	bf48      	it	mi
   dbe6a:	eef0 7a49 	vmovmi.f32	s15, s18
              ActivationFunctionWithMinMax(total + bias_value,
                                           output_activation_min,
                                           output_activation_max);
   dbe6e:	edc0 7a00 	vstr	s15, [r0]
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dbe72:	3401      	adds	r4, #1
   dbe74:	e789      	b.n	dbd8a <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0x102>
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   dbe76:	9b04      	ldr	r3, [sp, #16]
   dbe78:	9a0a      	ldr	r2, [sp, #40]	; 0x28
   dbe7a:	3301      	adds	r3, #1
   dbe7c:	9304      	str	r3, [sp, #16]
   dbe7e:	9b07      	ldr	r3, [sp, #28]
   dbe80:	4413      	add	r3, r2
   dbe82:	9307      	str	r3, [sp, #28]
   dbe84:	e77c      	b.n	dbd80 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0xf8>
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   dbe86:	9b03      	ldr	r3, [sp, #12]
   dbe88:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   dbe8a:	3301      	adds	r3, #1
   dbe8c:	9303      	str	r3, [sp, #12]
   dbe8e:	9b08      	ldr	r3, [sp, #32]
   dbe90:	4413      	add	r3, r2
   dbe92:	9308      	str	r3, [sp, #32]
   dbe94:	e76a      	b.n	dbd6c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0xe4>
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
   dbe96:	3501      	adds	r5, #1
   dbe98:	e75f      	b.n	dbd5a <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_+0xd2>
                                           output_activation_max);
        }
      }
    }
  }
}
   dbe9a:	b01b      	add	sp, #108	; 0x6c
   dbe9c:	ecbd 8b04 	vpop	{d8-d9}
   dbea0:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dbea4:	00000000 	.word	0x00000000

000dbea8 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv>:
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const uint8* input_data, const RuntimeShape& filter_shape,
                 const uint8* filter_data, const RuntimeShape& bias_shape,
                 const int32* bias_data, const RuntimeShape& output_shape,
                 uint8* output_data, const RuntimeShape& im2col_shape,
                 uint8* im2col_data, void* cpu_backend_context) {
   dbea8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dbeac:	b0a3      	sub	sp, #140	; 0x8c
   dbeae:	469a      	mov	sl, r3
  (void)cpu_backend_context;  // only used in optimized code.
  (void)im2col_data;   // only used in optimized code.
  (void)im2col_shape;  // only used in optimized code.
  const int stride_width = params.stride_width;
   dbeb0:	f9b0 300a 	ldrsh.w	r3, [r0, #10]
   dbeb4:	930d      	str	r3, [sp, #52]	; 0x34
  const int stride_height = params.stride_height;
   dbeb6:	f9b0 300c 	ldrsh.w	r3, [r0, #12]
   dbeba:	930e      	str	r3, [sp, #56]	; 0x38
  const int dilation_width_factor = params.dilation_width_factor;
   dbebc:	f9b0 300e 	ldrsh.w	r3, [r0, #14]
   dbec0:	930f      	str	r3, [sp, #60]	; 0x3c
  const int dilation_height_factor = params.dilation_height_factor;
   dbec2:	f9b0 3010 	ldrsh.w	r3, [r0, #16]
   dbec6:	9310      	str	r3, [sp, #64]	; 0x40
  const int pad_width = params.padding_values.width;
   dbec8:	f9b0 3002 	ldrsh.w	r3, [r0, #2]
   dbecc:	9311      	str	r3, [sp, #68]	; 0x44
  const int pad_height = params.padding_values.height;
   dbece:	f9b0 3004 	ldrsh.w	r3, [r0, #4]
   dbed2:	9312      	str	r3, [sp, #72]	; 0x48
  const int32 input_offset = params.input_offset;
   dbed4:	6943      	ldr	r3, [r0, #20]
   dbed6:	9313      	str	r3, [sp, #76]	; 0x4c
  const int32 filter_offset = params.weights_offset;
   dbed8:	6983      	ldr	r3, [r0, #24]
   dbeda:	9314      	str	r3, [sp, #80]	; 0x50
  const int32 output_offset = params.output_offset;
   dbedc:	69c3      	ldr	r3, [r0, #28]
   dbede:	9315      	str	r3, [sp, #84]	; 0x54
  const int32 output_multiplier = params.output_multiplier;
   dbee0:	6a03      	ldr	r3, [r0, #32]
   dbee2:	9316      	str	r3, [sp, #88]	; 0x58
  const int output_shift = params.output_shift;
   dbee4:	6a43      	ldr	r3, [r0, #36]	; 0x24
   dbee6:	9317      	str	r3, [sp, #92]	; 0x5c
  const int32 output_activation_min = params.quantized_activation_min;
   dbee8:	6a83      	ldr	r3, [r0, #40]	; 0x28
   dbeea:	930a      	str	r3, [sp, #40]	; 0x28
  const int32 output_activation_max = params.quantized_activation_max;
   dbeec:	6ac3      	ldr	r3, [r0, #44]	; 0x2c
   dbeee:	930b      	str	r3, [sp, #44]	; 0x2c
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const uint8* input_data, const RuntimeShape& filter_shape,
                 const uint8* filter_data, const RuntimeShape& bias_shape,
                 const int32* bias_data, const RuntimeShape& output_shape,
                 uint8* output_data, const RuntimeShape& im2col_shape,
                 uint8* im2col_data, void* cpu_backend_context) {
   dbef0:	9221      	str	r2, [sp, #132]	; 0x84
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   dbef2:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   dbef4:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const uint8* input_data, const RuntimeShape& filter_shape,
                 const uint8* filter_data, const RuntimeShape& bias_shape,
                 const int32* bias_data, const RuntimeShape& output_shape,
                 uint8* output_data, const RuntimeShape& im2col_shape,
                 uint8* im2col_data, void* cpu_backend_context) {
   dbef6:	f8dd 80bc 	ldr.w	r8, [sp, #188]	; 0xbc
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   dbefa:	4293      	cmp	r3, r2
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const uint8* input_data, const RuntimeShape& filter_shape,
                 const uint8* filter_data, const RuntimeShape& bias_shape,
                 const int32* bias_data, const RuntimeShape& output_shape,
                 uint8* output_data, const RuntimeShape& im2col_shape,
                 uint8* im2col_data, void* cpu_backend_context) {
   dbefc:	4689      	mov	r9, r1
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   dbefe:	dd01      	ble.n	dbf04 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x5c>
   dbf00:	f008 fa24 	bl	e434c <abort>

  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dbf04:	680b      	ldr	r3, [r1, #0]
   dbf06:	2b04      	cmp	r3, #4
   dbf08:	d1fa      	bne.n	dbf00 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x58>
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
   dbf0a:	f8da 3000 	ldr.w	r3, [sl]
   dbf0e:	2b04      	cmp	r3, #4
   dbf10:	d1f6      	bne.n	dbf00 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x58>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   dbf12:	f8d8 3000 	ldr.w	r3, [r8]
   dbf16:	2b04      	cmp	r3, #4
   dbf18:	d1f2      	bne.n	dbf00 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x58>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dbf1a:	2300      	movs	r3, #0
   dbf1c:	4619      	mov	r1, r3
   dbf1e:	4642      	mov	r2, r8
   dbf20:	4648      	mov	r0, r9
   dbf22:	f7ff fe52 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   dbf26:	2303      	movs	r3, #3
   dbf28:	4619      	mov	r1, r3
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);

  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dbf2a:	9018      	str	r0, [sp, #96]	; 0x60
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   dbf2c:	4652      	mov	r2, sl
   dbf2e:	4648      	mov	r0, r9
   dbf30:	f7ff fe4b 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
   dbf34:	2303      	movs	r3, #3

  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   dbf36:	9019      	str	r0, [sp, #100]	; 0x64
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
   dbf38:	4642      	mov	r2, r8
   dbf3a:	2100      	movs	r1, #0
   dbf3c:	4650      	mov	r0, sl
   dbf3e:	f7ff fe44 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  if (bias_data) {
   dbf42:	9b2e      	ldr	r3, [sp, #184]	; 0xb8
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
   dbf44:	900c      	str	r0, [sp, #48]	; 0x30
  if (bias_data) {
   dbf46:	b12b      	cbz	r3, dbf54 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0xac>
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
   dbf48:	982d      	ldr	r0, [sp, #180]	; 0xb4
   dbf4a:	f7ff fe2e 	bl	dbbaa <_ZNK6tflite12RuntimeShape8FlatSizeEv>
   dbf4e:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   dbf50:	4283      	cmp	r3, r0
   dbf52:	d1d5      	bne.n	dbf00 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x58>
  }
  const int input_height = input_shape.Dims(1);
   dbf54:	2101      	movs	r1, #1
   dbf56:	4648      	mov	r0, r9
   dbf58:	f7fa fa86 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   dbf5c:	2102      	movs	r1, #2
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
   dbf5e:	901a      	str	r0, [sp, #104]	; 0x68
  const int input_width = input_shape.Dims(2);
   dbf60:	4648      	mov	r0, r9
   dbf62:	f7fa fa81 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_height = filter_shape.Dims(1);
   dbf66:	2101      	movs	r1, #1
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   dbf68:	901b      	str	r0, [sp, #108]	; 0x6c
  const int filter_height = filter_shape.Dims(1);
   dbf6a:	4650      	mov	r0, sl
   dbf6c:	f7fa fa7c 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_width = filter_shape.Dims(2);
   dbf70:	2102      	movs	r1, #2
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
   dbf72:	901c      	str	r0, [sp, #112]	; 0x70
  const int filter_width = filter_shape.Dims(2);
   dbf74:	4650      	mov	r0, sl
   dbf76:	f7fa fa77 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   dbf7a:	2101      	movs	r1, #1
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
   dbf7c:	901d      	str	r0, [sp, #116]	; 0x74
  const int output_height = output_shape.Dims(1);
   dbf7e:	4640      	mov	r0, r8
   dbf80:	f7fa fa72 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   dbf84:	2102      	movs	r1, #2
  }
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   dbf86:	901e      	str	r0, [sp, #120]	; 0x78
  const int output_width = output_shape.Dims(2);
   dbf88:	4640      	mov	r0, r8
   dbf8a:	f7fa fa6d 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  for (int batch = 0; batch < batches; ++batch) {
   dbf8e:	f04f 0b00 	mov.w	fp, #0
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   dbf92:	901f      	str	r0, [sp, #124]	; 0x7c
  for (int batch = 0; batch < batches; ++batch) {
   dbf94:	9b18      	ldr	r3, [sp, #96]	; 0x60
   dbf96:	459b      	cmp	fp, r3
   dbf98:	f280 8093 	bge.w	dc0c2 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x21a>
   dbf9c:	9b12      	ldr	r3, [sp, #72]	; 0x48
   dbf9e:	425b      	negs	r3, r3
   dbfa0:	9309      	str	r3, [sp, #36]	; 0x24
   dbfa2:	2300      	movs	r3, #0
   dbfa4:	9304      	str	r3, [sp, #16]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   dbfa6:	9b04      	ldr	r3, [sp, #16]
   dbfa8:	9a1e      	ldr	r2, [sp, #120]	; 0x78
   dbfaa:	4293      	cmp	r3, r2
   dbfac:	f280 8086 	bge.w	dc0bc <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x214>
   dbfb0:	9b11      	ldr	r3, [sp, #68]	; 0x44
   dbfb2:	425b      	negs	r3, r3
   dbfb4:	9308      	str	r3, [sp, #32]
   dbfb6:	2300      	movs	r3, #0
   dbfb8:	9305      	str	r3, [sp, #20]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   dbfba:	9b05      	ldr	r3, [sp, #20]
   dbfbc:	9a1f      	ldr	r2, [sp, #124]	; 0x7c
   dbfbe:	4293      	cmp	r3, r2
   dbfc0:	da74      	bge.n	dc0ac <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x204>
   dbfc2:	2400      	movs	r4, #0
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dbfc4:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   dbfc6:	429c      	cmp	r4, r3
   dbfc8:	da68      	bge.n	dc09c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x1f4>
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
   dbfca:	2500      	movs	r5, #0
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dbfcc:	9e09      	ldr	r6, [sp, #36]	; 0x24
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   dbfce:	9506      	str	r5, [sp, #24]
   dbfd0:	9b06      	ldr	r3, [sp, #24]
   dbfd2:	9a1c      	ldr	r2, [sp, #112]	; 0x70
   dbfd4:	4293      	cmp	r3, r2
   dbfd6:	da41      	bge.n	dc05c <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x1b4>
   dbfd8:	2300      	movs	r3, #0
   dbfda:	9f08      	ldr	r7, [sp, #32]
   dbfdc:	9307      	str	r3, [sp, #28]
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   dbfde:	9b07      	ldr	r3, [sp, #28]
   dbfe0:	9a1d      	ldr	r2, [sp, #116]	; 0x74
   dbfe2:	4293      	cmp	r3, r2
   dbfe4:	da34      	bge.n	dc050 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x1a8>
   dbfe6:	2300      	movs	r3, #0
   dbfe8:	9303      	str	r3, [sp, #12]
              for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
   dbfea:	9b03      	ldr	r3, [sp, #12]
   dbfec:	9a19      	ldr	r2, [sp, #100]	; 0x64
   dbfee:	4293      	cmp	r3, r2
   dbff0:	da28      	bge.n	dc044 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x19c>
                const int in_x = in_x_origin + dilation_width_factor * filter_x;
                const int in_y =
                    in_y_origin + dilation_height_factor * filter_y;
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
   dbff2:	2f00      	cmp	r7, #0
   dbff4:	db23      	blt.n	dc03e <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x196>
   dbff6:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
   dbff8:	42bb      	cmp	r3, r7
   dbffa:	dd20      	ble.n	dc03e <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x196>
   dbffc:	2e00      	cmp	r6, #0
   dbffe:	db1e      	blt.n	dc03e <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x196>
   dc000:	9b1a      	ldr	r3, [sp, #104]	; 0x68
   dc002:	42b3      	cmp	r3, r6
   dc004:	dd1b      	ble.n	dc03e <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x196>
                    (in_y < input_height)) {
                  int32 input_val = input_data[Offset(input_shape, batch, in_y,
   dc006:	9b03      	ldr	r3, [sp, #12]
   dc008:	9300      	str	r3, [sp, #0]
   dc00a:	4632      	mov	r2, r6
   dc00c:	463b      	mov	r3, r7
   dc00e:	4659      	mov	r1, fp
   dc010:	4648      	mov	r0, r9
   dc012:	f7fa fa8e 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                                                      in_x, in_channel)];
                  int32 filter_val =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
   dc016:	9b03      	ldr	r3, [sp, #12]
                    in_y_origin + dilation_height_factor * filter_y;
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height)) {
                  int32 input_val = input_data[Offset(input_shape, batch, in_y,
   dc018:	9020      	str	r0, [sp, #128]	; 0x80
                                                      in_x, in_channel)];
                  int32 filter_val =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
   dc01a:	9300      	str	r3, [sp, #0]
   dc01c:	9a06      	ldr	r2, [sp, #24]
   dc01e:	9b07      	ldr	r3, [sp, #28]
   dc020:	4621      	mov	r1, r4
   dc022:	4650      	mov	r0, sl
   dc024:	f7fa fa85 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                                         filter_x, in_channel)];
                  acc +=
                      (filter_val + filter_offset) * (input_val + input_offset);
   dc028:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   dc02a:	9a14      	ldr	r2, [sp, #80]	; 0x50
   dc02c:	5c1b      	ldrb	r3, [r3, r0]
   dc02e:	9920      	ldr	r1, [sp, #128]	; 0x80
   dc030:	4413      	add	r3, r2
   dc032:	9a21      	ldr	r2, [sp, #132]	; 0x84
   dc034:	5c52      	ldrb	r2, [r2, r1]
   dc036:	9913      	ldr	r1, [sp, #76]	; 0x4c
   dc038:	440a      	add	r2, r1
   dc03a:	fb02 5503 	mla	r5, r2, r3, r5
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
              for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
   dc03e:	9b03      	ldr	r3, [sp, #12]
   dc040:	3301      	adds	r3, #1
   dc042:	e7d1      	b.n	dbfe8 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x140>
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   dc044:	9b07      	ldr	r3, [sp, #28]
   dc046:	3301      	adds	r3, #1
   dc048:	9307      	str	r3, [sp, #28]
   dc04a:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   dc04c:	441f      	add	r7, r3
   dc04e:	e7c6      	b.n	dbfde <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x136>
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   dc050:	9b06      	ldr	r3, [sp, #24]
   dc052:	3301      	adds	r3, #1
   dc054:	9306      	str	r3, [sp, #24]
   dc056:	9b10      	ldr	r3, [sp, #64]	; 0x40
   dc058:	441e      	add	r6, r3
   dc05a:	e7b9      	b.n	dbfd0 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x128>
                      (filter_val + filter_offset) * (input_val + input_offset);
                }
              }
            }
          }
          if (bias_data) {
   dc05c:	9b2e      	ldr	r3, [sp, #184]	; 0xb8
   dc05e:	b113      	cbz	r3, dc066 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x1be>
            acc += bias_data[out_channel];
   dc060:	f853 3024 	ldr.w	r3, [r3, r4, lsl #2]
   dc064:	441d      	add	r5, r3
          }
          acc = MultiplyByQuantizedMultiplier(acc, output_multiplier,
   dc066:	9a17      	ldr	r2, [sp, #92]	; 0x5c
   dc068:	9916      	ldr	r1, [sp, #88]	; 0x58
   dc06a:	4628      	mov	r0, r5
   dc06c:	f7ff fdbc 	bl	dbbe8 <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
                                              output_shift);
          acc += output_offset;
   dc070:	9b15      	ldr	r3, [sp, #84]	; 0x54
          acc = std::max(acc, output_activation_min);
          acc = std::min(acc, output_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
   dc072:	9400      	str	r4, [sp, #0]
          if (bias_data) {
            acc += bias_data[out_channel];
          }
          acc = MultiplyByQuantizedMultiplier(acc, output_multiplier,
                                              output_shift);
          acc += output_offset;
   dc074:	4418      	add	r0, r3
   dc076:	9b0a      	ldr	r3, [sp, #40]	; 0x28
          acc = std::max(acc, output_activation_min);
          acc = std::min(acc, output_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
   dc078:	9a04      	ldr	r2, [sp, #16]
   dc07a:	4283      	cmp	r3, r0
   dc07c:	bfb8      	it	lt
   dc07e:	4603      	movlt	r3, r0
   dc080:	461d      	mov	r5, r3
   dc082:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   dc084:	429d      	cmp	r5, r3
   dc086:	bfa8      	it	ge
   dc088:	461d      	movge	r5, r3
   dc08a:	4659      	mov	r1, fp
   dc08c:	9b05      	ldr	r3, [sp, #20]
   dc08e:	4640      	mov	r0, r8
   dc090:	f7fa fa4f 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<uint8>(acc);
   dc094:	9b30      	ldr	r3, [sp, #192]	; 0xc0
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dc096:	3401      	adds	r4, #1
                                              output_shift);
          acc += output_offset;
          acc = std::max(acc, output_activation_min);
          acc = std::min(acc, output_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
              static_cast<uint8>(acc);
   dc098:	541d      	strb	r5, [r3, r0]
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dc09a:	e793      	b.n	dbfc4 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x11c>
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   dc09c:	9b05      	ldr	r3, [sp, #20]
   dc09e:	9a0d      	ldr	r2, [sp, #52]	; 0x34
   dc0a0:	3301      	adds	r3, #1
   dc0a2:	9305      	str	r3, [sp, #20]
   dc0a4:	9b08      	ldr	r3, [sp, #32]
   dc0a6:	4413      	add	r3, r2
   dc0a8:	9308      	str	r3, [sp, #32]
   dc0aa:	e786      	b.n	dbfba <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0x112>
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   dc0ac:	9b04      	ldr	r3, [sp, #16]
   dc0ae:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   dc0b0:	3301      	adds	r3, #1
   dc0b2:	9304      	str	r3, [sp, #16]
   dc0b4:	9b09      	ldr	r3, [sp, #36]	; 0x24
   dc0b6:	4413      	add	r3, r2
   dc0b8:	9309      	str	r3, [sp, #36]	; 0x24
   dc0ba:	e774      	b.n	dbfa6 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0xfe>
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
   dc0bc:	f10b 0b01 	add.w	fp, fp, #1
   dc0c0:	e768      	b.n	dbf94 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv+0xec>
              static_cast<uint8>(acc);
        }
      }
    }
  }
}
   dc0c2:	b023      	add	sp, #140	; 0x8c
   dc0c4:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dc0c8 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa>:
    const ConvParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
   dc0c8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dc0cc:	b09d      	sub	sp, #116	; 0x74
   dc0ce:	4699      	mov	r9, r3
  // Get parameters.
  const int32 input_offset = params.input_offset;  // r = s(q - Z)
   dc0d0:	6943      	ldr	r3, [r0, #20]
   dc0d2:	9309      	str	r3, [sp, #36]	; 0x24
  const int stride_width = params.stride_width;
   dc0d4:	f9b0 300a 	ldrsh.w	r3, [r0, #10]
   dc0d8:	930a      	str	r3, [sp, #40]	; 0x28
  const int stride_height = params.stride_height;
   dc0da:	f9b0 300c 	ldrsh.w	r3, [r0, #12]
   dc0de:	930b      	str	r3, [sp, #44]	; 0x2c
  const int dilation_width_factor = params.dilation_width_factor;
   dc0e0:	f9b0 300e 	ldrsh.w	r3, [r0, #14]
   dc0e4:	930c      	str	r3, [sp, #48]	; 0x30
  const int dilation_height_factor = params.dilation_height_factor;
   dc0e6:	f9b0 3010 	ldrsh.w	r3, [r0, #16]
   dc0ea:	930d      	str	r3, [sp, #52]	; 0x34
  const int pad_width = params.padding_values.width;
   dc0ec:	f9b0 3002 	ldrsh.w	r3, [r0, #2]
   dc0f0:	930e      	str	r3, [sp, #56]	; 0x38
  const int pad_height = params.padding_values.height;
   dc0f2:	f9b0 3004 	ldrsh.w	r3, [r0, #4]
   dc0f6:	930f      	str	r3, [sp, #60]	; 0x3c
  const int32 output_offset = params.output_offset;
   dc0f8:	69c3      	ldr	r3, [r0, #28]
   dc0fa:	9310      	str	r3, [sp, #64]	; 0x40
  const int32 output_activation_min = std::numeric_limits<int8_t>::min();
  const int32 output_activation_max = std::numeric_limits<int8_t>::max();

  // Sanity check.
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dc0fc:	f8d9 3000 	ldr.w	r3, [r9]
    const ConvParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
   dc100:	911a      	str	r1, [sp, #104]	; 0x68
  const int32 output_activation_min = std::numeric_limits<int8_t>::min();
  const int32 output_activation_max = std::numeric_limits<int8_t>::max();

  // Sanity check.
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dc102:	2b04      	cmp	r3, #4
    const ConvParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
   dc104:	921b      	str	r2, [sp, #108]	; 0x6c
   dc106:	f8dd b09c 	ldr.w	fp, [sp, #156]	; 0x9c
  const int32 output_activation_min = std::numeric_limits<int8_t>::min();
  const int32 output_activation_max = std::numeric_limits<int8_t>::max();

  // Sanity check.
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dc10a:	d001      	beq.n	dc110 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x48>
   dc10c:	f008 f91e 	bl	e434c <abort>
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
   dc110:	f8db 3000 	ldr.w	r3, [fp]
   dc114:	2b04      	cmp	r3, #4
   dc116:	d1f9      	bne.n	dc10c <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x44>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   dc118:	9b2b      	ldr	r3, [sp, #172]	; 0xac
   dc11a:	681b      	ldr	r3, [r3, #0]
   dc11c:	2b04      	cmp	r3, #4
   dc11e:	d1f5      	bne.n	dc10c <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x44>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dc120:	2300      	movs	r3, #0
   dc122:	4619      	mov	r1, r3
   dc124:	9a2b      	ldr	r2, [sp, #172]	; 0xac
   dc126:	4648      	mov	r0, r9
   dc128:	f7ff fd4f 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   dc12c:	2303      	movs	r3, #3
   dc12e:	4619      	mov	r1, r3
  // Sanity check.
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dc130:	9011      	str	r0, [sp, #68]	; 0x44
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   dc132:	465a      	mov	r2, fp
   dc134:	4648      	mov	r0, r9
   dc136:	f7ff fd48 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
   dc13a:	2303      	movs	r3, #3
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   dc13c:	9012      	str	r0, [sp, #72]	; 0x48
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
   dc13e:	9a2b      	ldr	r2, [sp, #172]	; 0xac
   dc140:	2100      	movs	r1, #0
   dc142:	4658      	mov	r0, fp
   dc144:	f7ff fd41 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  if (bias_data) {
   dc148:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
   dc14a:	9008      	str	r0, [sp, #32]
  if (bias_data) {
   dc14c:	b12b      	cbz	r3, dc15a <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x92>
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
   dc14e:	9829      	ldr	r0, [sp, #164]	; 0xa4
   dc150:	f7ff fd2b 	bl	dbbaa <_ZNK6tflite12RuntimeShape8FlatSizeEv>
   dc154:	9b08      	ldr	r3, [sp, #32]
   dc156:	4283      	cmp	r3, r0
   dc158:	d1d8      	bne.n	dc10c <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x44>
  }

  // Check dimensions of the tensors.
  const int input_height = input_shape.Dims(1);
   dc15a:	2101      	movs	r1, #1
   dc15c:	4648      	mov	r0, r9
   dc15e:	f7fa f983 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   dc162:	2102      	movs	r1, #2
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }

  // Check dimensions of the tensors.
  const int input_height = input_shape.Dims(1);
   dc164:	9013      	str	r0, [sp, #76]	; 0x4c
  const int input_width = input_shape.Dims(2);
   dc166:	4648      	mov	r0, r9
   dc168:	f7fa f97e 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_height = filter_shape.Dims(1);
   dc16c:	2101      	movs	r1, #1
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }

  // Check dimensions of the tensors.
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   dc16e:	9014      	str	r0, [sp, #80]	; 0x50
  const int filter_height = filter_shape.Dims(1);
   dc170:	4658      	mov	r0, fp
   dc172:	f7fa f979 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_width = filter_shape.Dims(2);
   dc176:	2102      	movs	r1, #2
  }

  // Check dimensions of the tensors.
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
   dc178:	9015      	str	r0, [sp, #84]	; 0x54
  const int filter_width = filter_shape.Dims(2);
   dc17a:	4658      	mov	r0, fp
   dc17c:	f7fa f974 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   dc180:	2101      	movs	r1, #1

  // Check dimensions of the tensors.
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
   dc182:	9016      	str	r0, [sp, #88]	; 0x58
  const int output_height = output_shape.Dims(1);
   dc184:	982b      	ldr	r0, [sp, #172]	; 0xac
   dc186:	f7fa f96f 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   dc18a:	2102      	movs	r1, #2
  // Check dimensions of the tensors.
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   dc18c:	9017      	str	r0, [sp, #92]	; 0x5c
  const int output_width = output_shape.Dims(2);
   dc18e:	982b      	ldr	r0, [sp, #172]	; 0xac
   dc190:	f7fa f96a 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  for (int batch = 0; batch < batches; ++batch) {
   dc194:	f04f 0a00 	mov.w	sl, #0
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   dc198:	9018      	str	r0, [sp, #96]	; 0x60
  for (int batch = 0; batch < batches; ++batch) {
   dc19a:	9b11      	ldr	r3, [sp, #68]	; 0x44
   dc19c:	459a      	cmp	sl, r3
   dc19e:	f280 8091 	bge.w	dc2c4 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1fc>
   dc1a2:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   dc1a4:	425b      	negs	r3, r3
   dc1a6:	9307      	str	r3, [sp, #28]
   dc1a8:	2300      	movs	r3, #0
   dc1aa:	9302      	str	r3, [sp, #8]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   dc1ac:	9b02      	ldr	r3, [sp, #8]
   dc1ae:	9a17      	ldr	r2, [sp, #92]	; 0x5c
   dc1b0:	4293      	cmp	r3, r2
   dc1b2:	da6b      	bge.n	dc28c <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1c4>
   dc1b4:	9b0e      	ldr	r3, [sp, #56]	; 0x38
   dc1b6:	425b      	negs	r3, r3
   dc1b8:	9306      	str	r3, [sp, #24]
   dc1ba:	2300      	movs	r3, #0
   dc1bc:	9303      	str	r3, [sp, #12]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   dc1be:	9b03      	ldr	r3, [sp, #12]
   dc1c0:	9a18      	ldr	r2, [sp, #96]	; 0x60
   dc1c2:	4293      	cmp	r3, r2
   dc1c4:	da5a      	bge.n	dc27c <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1b4>
   dc1c6:	2400      	movs	r4, #0
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dc1c8:	9b08      	ldr	r3, [sp, #32]
   dc1ca:	429c      	cmp	r4, r3
   dc1cc:	da4e      	bge.n	dc26c <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1a4>
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
   dc1ce:	2500      	movs	r5, #0
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dc1d0:	9f07      	ldr	r7, [sp, #28]
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   dc1d2:	9504      	str	r5, [sp, #16]
   dc1d4:	9b04      	ldr	r3, [sp, #16]
   dc1d6:	9a15      	ldr	r2, [sp, #84]	; 0x54
   dc1d8:	4293      	cmp	r3, r2
   dc1da:	da24      	bge.n	dc226 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x15e>
   dc1dc:	2300      	movs	r3, #0
   dc1de:	f8dd 8018 	ldr.w	r8, [sp, #24]
   dc1e2:	9305      	str	r3, [sp, #20]
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   dc1e4:	9b05      	ldr	r3, [sp, #20]
   dc1e6:	9a16      	ldr	r2, [sp, #88]	; 0x58
   dc1e8:	4293      	cmp	r3, r2
   dc1ea:	da16      	bge.n	dc21a <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x152>
   dc1ec:	2600      	movs	r6, #0
              for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
   dc1ee:	9b12      	ldr	r3, [sp, #72]	; 0x48
   dc1f0:	429e      	cmp	r6, r3
   dc1f2:	da0c      	bge.n	dc20e <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x146>
                const int in_x = in_x_origin + dilation_width_factor * filter_x;
                const int in_y =
                    in_y_origin + dilation_height_factor * filter_y;
                // Zero padding by omitting the areas outside the image.
                const bool is_point_inside_image =
                    (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
   dc1f4:	f1b8 0f00 	cmp.w	r8, #0
   dc1f8:	db07      	blt.n	dc20a <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x142>
   dc1fa:	9b14      	ldr	r3, [sp, #80]	; 0x50
   dc1fc:	4543      	cmp	r3, r8
   dc1fe:	dd04      	ble.n	dc20a <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x142>
   dc200:	2f00      	cmp	r7, #0
   dc202:	db02      	blt.n	dc20a <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x142>
   dc204:	9b13      	ldr	r3, [sp, #76]	; 0x4c
   dc206:	42bb      	cmp	r3, r7
   dc208:	dc43      	bgt.n	dc292 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1ca>
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
              for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
   dc20a:	3601      	adds	r6, #1
   dc20c:	e7ef      	b.n	dc1ee <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x126>
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   dc20e:	9b05      	ldr	r3, [sp, #20]
   dc210:	3301      	adds	r3, #1
   dc212:	9305      	str	r3, [sp, #20]
   dc214:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   dc216:	4498      	add	r8, r3
   dc218:	e7e4      	b.n	dc1e4 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x11c>
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
          const int in_x_origin = (out_x * stride_width) - pad_width;
          const int in_y_origin = (out_y * stride_height) - pad_height;
          int32 acc = 0;
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   dc21a:	9b04      	ldr	r3, [sp, #16]
   dc21c:	3301      	adds	r3, #1
   dc21e:	9304      	str	r3, [sp, #16]
   dc220:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   dc222:	441f      	add	r7, r3
   dc224:	e7d6      	b.n	dc1d4 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x10c>
                }
              }
            }
          }

          if (bias_data) {
   dc226:	9b2a      	ldr	r3, [sp, #168]	; 0xa8
   dc228:	b113      	cbz	r3, dc230 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x168>
            acc += bias_data[out_channel];
   dc22a:	f853 3024 	ldr.w	r3, [r3, r4, lsl #2]
   dc22e:	441d      	add	r5, r3
          }
          acc = MultiplyByQuantizedMultiplier(
   dc230:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
   dc232:	f853 2024 	ldr.w	r2, [r3, r4, lsl #2]
   dc236:	9b1a      	ldr	r3, [sp, #104]	; 0x68
   dc238:	4628      	mov	r0, r5
   dc23a:	f853 1024 	ldr.w	r1, [r3, r4, lsl #2]
   dc23e:	f7ff fcd3 	bl	dbbe8 <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
              acc, output_multiplier[out_channel], output_shift[out_channel]);
          acc += output_offset;
   dc242:	9b10      	ldr	r3, [sp, #64]	; 0x40
          acc = std::max(acc, output_activation_min);
          acc = std::min(acc, output_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
   dc244:	9400      	str	r4, [sp, #0]
   dc246:	f06f 057f 	mvn.w	r5, #127	; 0x7f
          if (bias_data) {
            acc += bias_data[out_channel];
          }
          acc = MultiplyByQuantizedMultiplier(
              acc, output_multiplier[out_channel], output_shift[out_channel]);
          acc += output_offset;
   dc24a:	4418      	add	r0, r3
   dc24c:	4285      	cmp	r5, r0
   dc24e:	bfb8      	it	lt
   dc250:	4605      	movlt	r5, r0
          acc = std::max(acc, output_activation_min);
          acc = std::min(acc, output_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, out_channel)] =
   dc252:	9b03      	ldr	r3, [sp, #12]
   dc254:	9a02      	ldr	r2, [sp, #8]
   dc256:	982b      	ldr	r0, [sp, #172]	; 0xac
   dc258:	4651      	mov	r1, sl
   dc25a:	f7fa f96a 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<int8_t>(acc);
   dc25e:	2d7f      	cmp	r5, #127	; 0x7f
   dc260:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   dc262:	bfa8      	it	ge
   dc264:	257f      	movge	r5, #127	; 0x7f
   dc266:	541d      	strb	r5, [r3, r0]
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
   dc268:	3401      	adds	r4, #1
   dc26a:	e7ad      	b.n	dc1c8 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x100>
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   dc26c:	9b03      	ldr	r3, [sp, #12]
   dc26e:	9a0a      	ldr	r2, [sp, #40]	; 0x28
   dc270:	3301      	adds	r3, #1
   dc272:	9303      	str	r3, [sp, #12]
   dc274:	9b06      	ldr	r3, [sp, #24]
   dc276:	4413      	add	r3, r2
   dc278:	9306      	str	r3, [sp, #24]
   dc27a:	e7a0      	b.n	dc1be <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0xf6>
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   dc27c:	9b02      	ldr	r3, [sp, #8]
   dc27e:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   dc280:	3301      	adds	r3, #1
   dc282:	9302      	str	r3, [sp, #8]
   dc284:	9b07      	ldr	r3, [sp, #28]
   dc286:	4413      	add	r3, r2
   dc288:	9307      	str	r3, [sp, #28]
   dc28a:	e78f      	b.n	dc1ac <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0xe4>
  const int input_width = input_shape.Dims(2);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  for (int batch = 0; batch < batches; ++batch) {
   dc28c:	f10a 0a01 	add.w	sl, sl, #1
   dc290:	e783      	b.n	dc19a <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0xd2>
                // Zero padding by omitting the areas outside the image.
                const bool is_point_inside_image =
                    (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height);
                if (is_point_inside_image) {
                  int32 input_val = input_data[Offset(input_shape, batch, in_y,
   dc292:	4643      	mov	r3, r8
   dc294:	463a      	mov	r2, r7
   dc296:	4651      	mov	r1, sl
   dc298:	9600      	str	r6, [sp, #0]
   dc29a:	4648      	mov	r0, r9
   dc29c:	f7fa f949 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                                                      in_x, in_channel)];
                  int32 filter_val =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
   dc2a0:	9b05      	ldr	r3, [sp, #20]
                // Zero padding by omitting the areas outside the image.
                const bool is_point_inside_image =
                    (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height);
                if (is_point_inside_image) {
                  int32 input_val = input_data[Offset(input_shape, batch, in_y,
   dc2a2:	9019      	str	r0, [sp, #100]	; 0x64
                                                      in_x, in_channel)];
                  int32 filter_val =
                      filter_data[Offset(filter_shape, out_channel, filter_y,
   dc2a4:	9a04      	ldr	r2, [sp, #16]
   dc2a6:	9600      	str	r6, [sp, #0]
   dc2a8:	4621      	mov	r1, r4
   dc2aa:	4658      	mov	r0, fp
   dc2ac:	f7fa f941 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                  // long as the filter size (filter_y * filter_x * in_channel)
                  // does not exceed 2^16, which is the case in all the models
                  // we have seen so far.
                  // TODO(jianlijianli): Add a check to make sure the
                  // accumulator depth is smaller than 2^16.
                  acc += filter_val * (input_val + input_offset);
   dc2b0:	9a19      	ldr	r2, [sp, #100]	; 0x64
   dc2b2:	9b26      	ldr	r3, [sp, #152]	; 0x98
   dc2b4:	569b      	ldrsb	r3, [r3, r2]
   dc2b6:	9a09      	ldr	r2, [sp, #36]	; 0x24
   dc2b8:	4413      	add	r3, r2
   dc2ba:	9a28      	ldr	r2, [sp, #160]	; 0xa0
   dc2bc:	5612      	ldrsb	r2, [r2, r0]
   dc2be:	fb02 5503 	mla	r5, r2, r3, r5
   dc2c2:	e7a2      	b.n	dc20a <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x142>
              static_cast<int8_t>(acc);
        }
      }
    }
  }
}
   dc2c4:	b01d      	add	sp, #116	; 0x74
   dc2c6:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dc2ca <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>:

// Matching GetWindowedOutputSize in TensorFlow.
inline int ComputeOutSize(TfLitePadding padding, int image_size,
                          int filter_size, int stride, int dilation_rate = 1) {
  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;
  switch (padding) {
   dc2ca:	2801      	cmp	r0, #1
   dc2cc:	d008      	beq.n	dc2e0 <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii+0x16>
   dc2ce:	2802      	cmp	r0, #2
   dc2d0:	d10b      	bne.n	dc2ea <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii+0x20>
    case kTfLitePaddingSame:
      return (image_size + stride - 1) / stride;
    case kTfLitePaddingValid:
      return (image_size + stride - effective_filter_size) / stride;
   dc2d2:	9800      	ldr	r0, [sp, #0]
   dc2d4:	3a01      	subs	r2, #1
   dc2d6:	4350      	muls	r0, r2
   dc2d8:	4419      	add	r1, r3
   dc2da:	3001      	adds	r0, #1
   dc2dc:	1a09      	subs	r1, r1, r0
   dc2de:	e001      	b.n	dc2e4 <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii+0x1a>
inline int ComputeOutSize(TfLitePadding padding, int image_size,
                          int filter_size, int stride, int dilation_rate = 1) {
  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;
  switch (padding) {
    case kTfLitePaddingSame:
      return (image_size + stride - 1) / stride;
   dc2e0:	4419      	add	r1, r3
   dc2e2:	3901      	subs	r1, #1
    case kTfLitePaddingValid:
      return (image_size + stride - effective_filter_size) / stride;
   dc2e4:	fb91 f0f3 	sdiv	r0, r1, r3
   dc2e8:	4770      	bx	lr
    default:
      return 0;
   dc2ea:	2000      	movs	r0, #0
  }
}
   dc2ec:	4770      	bx	lr
	...

000dc2f0 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE>:

TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, int width, int height,
                             int filter_width, int filter_height, int out_width,
                             int out_height, const TfLiteType data_type,
                             OpData* data) {
   dc2f0:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dc2f4:	469a      	mov	sl, r3
  bool has_bias = node->inputs->size == 3;
   dc2f6:	680b      	ldr	r3, [r1, #0]
  // Check number of inputs/outputs
  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
   dc2f8:	681b      	ldr	r3, [r3, #0]

TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, int width, int height,
                             int filter_width, int filter_height, int out_width,
                             int out_height, const TfLiteType data_type,
                             OpData* data) {
   dc2fa:	b08d      	sub	sp, #52	; 0x34
  bool has_bias = node->inputs->size == 3;
  // Check number of inputs/outputs
  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
   dc2fc:	3b02      	subs	r3, #2
   dc2fe:	2b01      	cmp	r3, #1

TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, int width, int height,
                             int filter_width, int filter_height, int out_width,
                             int out_height, const TfLiteType data_type,
                             OpData* data) {
   dc300:	4680      	mov	r8, r0
   dc302:	4689      	mov	r9, r1
   dc304:	4616      	mov	r6, r2
   dc306:	9c1c      	ldr	r4, [sp, #112]	; 0x70
  bool has_bias = node->inputs->size == 3;
  // Check number of inputs/outputs
  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
   dc308:	d908      	bls.n	dc31c <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x2c>
   dc30a:	4b49      	ldr	r3, [pc, #292]	; (dc430 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x140>)
   dc30c:	9300      	str	r3, [sp, #0]
   dc30e:	6944      	ldr	r4, [r0, #20]
   dc310:	4a48      	ldr	r2, [pc, #288]	; (dc434 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x144>)
   dc312:	4949      	ldr	r1, [pc, #292]	; (dc438 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x148>)
   dc314:	234f      	movs	r3, #79	; 0x4f
   dc316:	47a0      	blx	r4
   dc318:	2001      	movs	r0, #1
   dc31a:	e085      	b.n	dc428 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x138>
  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);
   dc31c:	684b      	ldr	r3, [r1, #4]
   dc31e:	681b      	ldr	r3, [r3, #0]
   dc320:	2b01      	cmp	r3, #1
   dc322:	d00c      	beq.n	dc33e <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x4e>
   dc324:	9302      	str	r3, [sp, #8]
   dc326:	4b45      	ldr	r3, [pc, #276]	; (dc43c <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x14c>)
   dc328:	9301      	str	r3, [sp, #4]
   dc32a:	2401      	movs	r4, #1
   dc32c:	4b44      	ldr	r3, [pc, #272]	; (dc440 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x150>)
   dc32e:	9300      	str	r3, [sp, #0]
   dc330:	9403      	str	r4, [sp, #12]
   dc332:	6945      	ldr	r5, [r0, #20]
   dc334:	4a3f      	ldr	r2, [pc, #252]	; (dc434 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x144>)
   dc336:	4943      	ldr	r1, [pc, #268]	; (dc444 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x154>)
   dc338:	2350      	movs	r3, #80	; 0x50
   dc33a:	47a8      	blx	r5
   dc33c:	e7ec      	b.n	dc318 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x28>

  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
   dc33e:	f892 b000 	ldrb.w	fp, [r2]
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
   dc342:	6893      	ldr	r3, [r2, #8]
   dc344:	68d5      	ldr	r5, [r2, #12]
   dc346:	6917      	ldr	r7, [r2, #16]
   dc348:	9309      	str	r3, [sp, #36]	; 0x24

inline TfLitePaddingValues ComputePaddingHeightWidth(
    int stride_height, int stride_width, int dilation_rate_height,
    int dilation_rate_width, int in_height, int in_width, int filter_height,
    int filter_width, TfLitePadding padding, int* out_height, int* out_width) {
  *out_width = ComputeOutSize(padding, in_width, filter_width, stride_width,
   dc34a:	4651      	mov	r1, sl
   dc34c:	6853      	ldr	r3, [r2, #4]
   dc34e:	9500      	str	r5, [sp, #0]
   dc350:	9a17      	ldr	r2, [sp, #92]	; 0x5c
   dc352:	930a      	str	r3, [sp, #40]	; 0x28
   dc354:	4658      	mov	r0, fp
   dc356:	f7ff ffb8 	bl	dc2ca <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>
                              dilation_rate_width);
  *out_height = ComputeOutSize(padding, in_height, filter_height, stride_height,
   dc35a:	9700      	str	r7, [sp, #0]

inline TfLitePaddingValues ComputePaddingHeightWidth(
    int stride_height, int stride_width, int dilation_rate_height,
    int dilation_rate_width, int in_height, int in_width, int filter_height,
    int filter_width, TfLitePadding padding, int* out_height, int* out_width) {
  *out_width = ComputeOutSize(padding, in_width, filter_width, stride_width,
   dc35c:	900b      	str	r0, [sp, #44]	; 0x2c
                              dilation_rate_width);
  *out_height = ComputeOutSize(padding, in_height, filter_height, stride_height,
   dc35e:	9b09      	ldr	r3, [sp, #36]	; 0x24
   dc360:	9a18      	ldr	r2, [sp, #96]	; 0x60
   dc362:	9916      	ldr	r1, [sp, #88]	; 0x58
   dc364:	4658      	mov	r0, fp
   dc366:	f7ff ffb0 	bl	dc2ca <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>
inline int ComputePaddingWithOffset(int stride, int dilation_rate, int in_size,
                                    int filter_size, int out_size,
                                    int* offset) {
  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;
  int total_padding =
      ((out_size - 1) * stride + effective_filter_size - in_size);
   dc36a:	9b18      	ldr	r3, [sp, #96]	; 0x60
   dc36c:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   dc36e:	3b01      	subs	r3, #1
   dc370:	435f      	muls	r7, r3
   dc372:	9b09      	ldr	r3, [sp, #36]	; 0x24
   dc374:	3701      	adds	r7, #1
   dc376:	3801      	subs	r0, #1
   dc378:	fb03 7000 	mla	r0, r3, r0, r7
   dc37c:	9b16      	ldr	r3, [sp, #88]	; 0x58
   dc37e:	1ac0      	subs	r0, r0, r3
   dc380:	9b17      	ldr	r3, [sp, #92]	; 0x5c
   dc382:	3b01      	subs	r3, #1
   dc384:	436b      	muls	r3, r5
   dc386:	1e55      	subs	r5, r2, #1
   dc388:	9a0a      	ldr	r2, [sp, #40]	; 0x28
   dc38a:	3301      	adds	r3, #1
   dc38c:	fb02 3505 	mla	r5, r2, r5, r3
   dc390:	ebca 0a05 	rsb	sl, sl, r5
  total_padding = total_padding > 0 ? total_padding : 0;
   dc394:	ea2a 7aea 	bic.w	sl, sl, sl, asr #31
   dc398:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
   dc39c:	ea4f 036a 	mov.w	r3, sl, asr #1
   dc3a0:	6023      	str	r3, [r4, #0]
   dc3a2:	1043      	asrs	r3, r0, #1
   dc3a4:	6063      	str	r3, [r4, #4]

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training.
  if (data_type != kTfLiteFloat32) {
   dc3a6:	f89d 306c 	ldrb.w	r3, [sp, #108]	; 0x6c
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
   dc3aa:	f00a 0501 	and.w	r5, sl, #1
   dc3ae:	f000 0001 	and.w	r0, r0, #1

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training.
  if (data_type != kTfLiteFloat32) {
   dc3b2:	2b01      	cmp	r3, #1
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
   dc3b4:	60a5      	str	r5, [r4, #8]
   dc3b6:	60e0      	str	r0, [r4, #12]

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training.
  if (data_type != kTfLiteFloat32) {
   dc3b8:	d035      	beq.n	dc426 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x136>
   dc3ba:	f8d9 5000 	ldr.w	r5, [r9]
   dc3be:	f8d8 0008 	ldr.w	r0, [r8, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dc3c2:	6869      	ldr	r1, [r5, #4]
   dc3c4:	68aa      	ldr	r2, [r5, #8]
}

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
   dc3c6:	68ed      	ldr	r5, [r5, #12]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dc3c8:	2338      	movs	r3, #56	; 0x38

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
   dc3ca:	1c6f      	adds	r7, r5, #1
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dc3cc:	fb03 0101 	mla	r1, r3, r1, r0
   dc3d0:	fb03 0202 	mla	r2, r3, r2, r0
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dc3d4:	bf18      	it	ne
   dc3d6:	fb03 0305 	mlane	r3, r3, r5, r0
    const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
    const TfLiteTensor* bias =
        GetOptionalInputTensor(context, node, kBiasTensor);
    TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
   dc3da:	f504 758c 	add.w	r5, r4, #280	; 0x118
   dc3de:	9507      	str	r5, [sp, #28]
   dc3e0:	f104 0518 	add.w	r5, r4, #24
   dc3e4:	9506      	str	r5, [sp, #24]
   dc3e6:	f504 7507 	add.w	r5, r4, #540	; 0x21c
   dc3ea:	9505      	str	r5, [sp, #20]
   dc3ec:	f504 7506 	add.w	r5, r4, #536	; 0x218
   dc3f0:	9504      	str	r5, [sp, #16]
   dc3f2:	f104 0514 	add.w	r5, r4, #20
   dc3f6:	f104 0410 	add.w	r4, r4, #16
   dc3fa:	9402      	str	r4, [sp, #8]
   dc3fc:	f106 0614 	add.w	r6, r6, #20
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dc400:	f8d9 4004 	ldr.w	r4, [r9, #4]
   dc404:	9503      	str	r5, [sp, #12]
   dc406:	9601      	str	r6, [sp, #4]
   dc408:	6864      	ldr	r4, [r4, #4]
   dc40a:	f04f 0538 	mov.w	r5, #56	; 0x38
   dc40e:	fb05 0004 	mla	r0, r5, r4, r0
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
  }
  return nullptr;
   dc412:	bf08      	it	eq
   dc414:	2300      	moveq	r3, #0
   dc416:	9000      	str	r0, [sp, #0]
   dc418:	4640      	mov	r0, r8
   dc41a:	f007 fbb7 	bl	e3b8c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_>
                             int filter_width, int filter_height, int out_width,
                             int out_height, const TfLiteType data_type,
                             OpData* data) {
  bool has_bias = node->inputs->size == 3;
  // Check number of inputs/outputs
  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
   dc41e:	3000      	adds	r0, #0
   dc420:	bf18      	it	ne
   dc422:	2001      	movne	r0, #1
   dc424:	e000      	b.n	dc428 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE+0x138>
        &data->output_multiplier, &data->output_shift,
        &data->output_activation_min, &data->output_activation_max,
        data->per_channel_output_multiplier,
        reinterpret_cast<int*>(data->per_channel_output_shift)));
  }
  return kTfLiteOk;
   dc426:	2000      	movs	r0, #0
}
   dc428:	b00d      	add	sp, #52	; 0x34
   dc42a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dc42e:	bf00      	nop
   dc430:	000e9aaf 	.word	0x000e9aaf
   dc434:	000e99f2 	.word	0x000e99f2
   dc438:	000e9a98 	.word	0x000e9a98
   dc43c:	000eb295 	.word	0x000eb295
   dc440:	000e9ad3 	.word	0x000e9ad3
   dc444:	000e98c8 	.word	0x000e98c8

000dc448 <_ZN6tflite3ops5micro4conv13EvalQuantizedEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_>:

void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
   dc448:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dc44c:	b0b1      	sub	sp, #196	; 0xc4
  int32_t output_activation_min;
  int32_t output_activation_max;
};

inline PaddingType RuntimePaddingType(TfLitePadding padding) {
  switch (padding) {
   dc44e:	7810      	ldrb	r0, [r2, #0]

void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
   dc450:	f8dd 80e8 	ldr.w	r8, [sp, #232]	; 0xe8
   dc454:	9f3b      	ldr	r7, [sp, #236]	; 0xec
  const int32_t input_offset = -input->params.zero_point;
   dc456:	f8d8 1010 	ldr.w	r1, [r8, #16]

void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
   dc45a:	9e3f      	ldr	r6, [sp, #252]	; 0xfc
   dc45c:	9c3c      	ldr	r4, [sp, #240]	; 0xf0
   dc45e:	9d3d      	ldr	r5, [sp, #244]	; 0xf4
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;
   dc460:	f8d6 c010 	ldr.w	ip, [r6, #16]
void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
   dc464:	f1c1 0e00 	rsb	lr, r1, #0
  const int32_t filter_offset = -filter->params.zero_point;
   dc468:	6939      	ldr	r1, [r7, #16]
  int32_t output_activation_min;
  int32_t output_activation_max;
};

inline PaddingType RuntimePaddingType(TfLitePadding padding) {
  switch (padding) {
   dc46a:	2801      	cmp	r0, #1
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
   dc46c:	f1c1 0100 	rsb	r1, r1, #0
  int32_t output_activation_min;
  int32_t output_activation_max;
};

inline PaddingType RuntimePaddingType(TfLitePadding padding) {
  switch (padding) {
   dc470:	d003      	beq.n	dc47a <_ZN6tflite3ops5micro4conv13EvalQuantizedEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x32>
      return PaddingType::kSame;
    case TfLitePadding::kTfLitePaddingValid:
      return PaddingType::kValid;
    case TfLitePadding::kTfLitePaddingUnknown:
    default:
      return PaddingType::kNone;
   dc472:	2802      	cmp	r0, #2
   dc474:	bf0c      	ite	eq
   dc476:	2002      	moveq	r0, #2
   dc478:	2000      	movne	r0, #0
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(params->padding);
   dc47a:	f88d 0088 	strb.w	r0, [sp, #136]	; 0x88
  op_params.padding_values.width = data->padding.width;
   dc47e:	6818      	ldr	r0, [r3, #0]
   dc480:	f8ad 008a 	strh.w	r0, [sp, #138]	; 0x8a
  op_params.padding_values.height = data->padding.height;
   dc484:	6858      	ldr	r0, [r3, #4]
   dc486:	f8ad 008c 	strh.w	r0, [sp, #140]	; 0x8c
  op_params.stride_width = params->stride_width;
   dc48a:	6850      	ldr	r0, [r2, #4]
   dc48c:	f8ad 0092 	strh.w	r0, [sp, #146]	; 0x92
  op_params.stride_height = params->stride_height;
   dc490:	6890      	ldr	r0, [r2, #8]
   dc492:	f8ad 0094 	strh.w	r0, [sp, #148]	; 0x94
  op_params.dilation_width_factor = params->dilation_width_factor;
   dc496:	68d0      	ldr	r0, [r2, #12]
  op_params.dilation_height_factor = params->dilation_height_factor;
   dc498:	6912      	ldr	r2, [r2, #16]
   dc49a:	f8ad 2098 	strh.w	r2, [sp, #152]	; 0x98
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
   dc49e:	691a      	ldr	r2, [r3, #16]
   dc4a0:	922a      	str	r2, [sp, #168]	; 0xa8
  op_params.output_shift = -data->output_shift;
   dc4a2:	695a      	ldr	r2, [r3, #20]
  op_params.padding_type = RuntimePaddingType(params->padding);
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
   dc4a4:	f8ad 0096 	strh.w	r0, [sp, #150]	; 0x96
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
   dc4a8:	4252      	negs	r2, r2
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
   dc4aa:	9128      	str	r1, [sp, #160]	; 0xa0
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
   dc4ac:	922b      	str	r2, [sp, #172]	; 0xac
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
   dc4ae:	4641      	mov	r1, r8
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
   dc4b0:	f8d3 2218 	ldr.w	r2, [r3, #536]	; 0x218
  op_params.quantized_activation_max = data->output_activation_max;
   dc4b4:	f8d3 321c 	ldr.w	r3, [r3, #540]	; 0x21c
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = input_offset;
   dc4b8:	f8cd e09c 	str.w	lr, [sp, #156]	; 0x9c
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
   dc4bc:	a809      	add	r0, sp, #36	; 0x24
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
   dc4be:	f8cd c0a4 	str.w	ip, [sp, #164]	; 0xa4
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
   dc4c2:	922c      	str	r2, [sp, #176]	; 0xb0
  op_params.quantized_activation_max = data->output_activation_max;
   dc4c4:	932d      	str	r3, [sp, #180]	; 0xb4
  reference_ops::Conv(op_params, GetTensorShape(input),
   dc4c6:	f7fa fa74 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                      GetTensorData<uint8_t>(input), GetTensorShape(filter),
   dc4ca:	4639      	mov	r1, r7
   dc4cc:	a80e      	add	r0, sp, #56	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dc4ce:	f8d8 8004 	ldr.w	r8, [r8, #4]
   dc4d2:	f7fa fa6e 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dc4d6:	f8d7 9004 	ldr.w	r9, [r7, #4]
                      GetTensorData<uint8_t>(filter), GetTensorShape(bias),
   dc4da:	af13      	add	r7, sp, #76	; 0x4c
   dc4dc:	4621      	mov	r1, r4
   dc4de:	4638      	mov	r0, r7
   dc4e0:	f7fa fa67 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dc4e4:	b114      	cbz	r4, dc4ec <_ZN6tflite3ops5micro4conv13EvalQuantizedEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xa4>
   dc4e6:	f8d4 a004 	ldr.w	sl, [r4, #4]
   dc4ea:	e000      	b.n	dc4ee <_ZN6tflite3ops5micro4conv13EvalQuantizedEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xa6>
   dc4ec:	46a2      	mov	sl, r4
                      GetTensorData<int32_t>(bias), GetTensorShape(output),
   dc4ee:	ac18      	add	r4, sp, #96	; 0x60
   dc4f0:	4631      	mov	r1, r6
   dc4f2:	4620      	mov	r0, r4
   dc4f4:	f7fa fa5d 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dc4f8:	f8d6 b004 	ldr.w	fp, [r6, #4]
                      GetTensorData<uint8_t>(output), GetTensorShape(im2col),
   dc4fc:	ae1d      	add	r6, sp, #116	; 0x74
   dc4fe:	4629      	mov	r1, r5
   dc500:	4630      	mov	r0, r6
   dc502:	f7fa fa56 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dc506:	b105      	cbz	r5, dc50a <_ZN6tflite3ops5micro4conv13EvalQuantizedEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xc2>
   dc508:	686d      	ldr	r5, [r5, #4]
                      GetTensorData<uint8_t>(im2col), nullptr);
   dc50a:	9506      	str	r5, [sp, #24]
   dc50c:	2300      	movs	r3, #0
   dc50e:	4642      	mov	r2, r8
   dc510:	a909      	add	r1, sp, #36	; 0x24
   dc512:	9307      	str	r3, [sp, #28]
   dc514:	a822      	add	r0, sp, #136	; 0x88
   dc516:	ab0e      	add	r3, sp, #56	; 0x38
   dc518:	9605      	str	r6, [sp, #20]
   dc51a:	f8cd b010 	str.w	fp, [sp, #16]
   dc51e:	9403      	str	r4, [sp, #12]
   dc520:	f8cd a008 	str.w	sl, [sp, #8]
   dc524:	9701      	str	r7, [sp, #4]
   dc526:	f8cd 9000 	str.w	r9, [sp]
   dc52a:	f7ff fcbd 	bl	dbea8 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_PhS6_SB_Pv>
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<uint8_t>(input), GetTensorShape(filter),
                      GetTensorData<uint8_t>(filter), GetTensorShape(bias),
                      GetTensorData<int32_t>(bias), GetTensorShape(output),
                      GetTensorData<uint8_t>(output), GetTensorShape(im2col),
   dc52e:	4630      	mov	r0, r6
   dc530:	f7f9 ff8f 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<uint8_t>(input), GetTensorShape(filter),
                      GetTensorData<uint8_t>(filter), GetTensorShape(bias),
                      GetTensorData<int32_t>(bias), GetTensorShape(output),
   dc534:	4620      	mov	r0, r4
   dc536:	f7f9 ff8c 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<uint8_t>(input), GetTensorShape(filter),
                      GetTensorData<uint8_t>(filter), GetTensorShape(bias),
   dc53a:	4638      	mov	r0, r7
   dc53c:	f7f9 ff89 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<uint8_t>(input), GetTensorShape(filter),
   dc540:	a80e      	add	r0, sp, #56	; 0x38
   dc542:	f7f9 ff86 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  reference_ops::Conv(op_params, GetTensorShape(input),
   dc546:	a809      	add	r0, sp, #36	; 0x24
   dc548:	f7f9 ff83 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                      GetTensorData<uint8_t>(input), GetTensorShape(filter),
                      GetTensorData<uint8_t>(filter), GetTensorShape(bias),
                      GetTensorData<int32_t>(bias), GetTensorShape(output),
                      GetTensorData<uint8_t>(output), GetTensorShape(im2col),
                      GetTensorData<uint8_t>(im2col), nullptr);
}
   dc54c:	b031      	add	sp, #196	; 0xc4
   dc54e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dc552 <_ZN6tflite3ops5micro4conv23EvalQuantizedPerChannelEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_>:
void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, OpData* data,
                             const TfLiteTensor* input,
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output,
                             TfLiteTensor* im2col) {
   dc552:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dc556:	b0ad      	sub	sp, #180	; 0xb4
   dc558:	ac37      	add	r4, sp, #220	; 0xdc
   dc55a:	9f36      	ldr	r7, [sp, #216]	; 0xd8
   dc55c:	e894 0430 	ldmia.w	r4, {r4, r5, sl}
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
   dc560:	6939      	ldr	r1, [r7, #16]
   dc562:	4249      	negs	r1, r1
   dc564:	9123      	str	r1, [sp, #140]	; 0x8c
  op_params.output_offset = output->params.zero_point;
   dc566:	f8da 1010 	ldr.w	r1, [sl, #16]
   dc56a:	9125      	str	r1, [sp, #148]	; 0x94
  op_params.stride_height = params->stride_height;
   dc56c:	6891      	ldr	r1, [r2, #8]
   dc56e:	f8ad 1084 	strh.w	r1, [sp, #132]	; 0x84
  op_params.stride_width = params->stride_width;
   dc572:	6851      	ldr	r1, [r2, #4]
   dc574:	f8ad 1082 	strh.w	r1, [sp, #130]	; 0x82
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
   dc578:	4699      	mov	r9, r3
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.dilation_height_factor = params->dilation_height_factor;
   dc57a:	6911      	ldr	r1, [r2, #16]
  op_params.dilation_width_factor = params->dilation_width_factor;
   dc57c:	68d2      	ldr	r2, [r2, #12]
   dc57e:	f8ad 2086 	strh.w	r2, [sp, #134]	; 0x86
  op_params.padding_values.height = data->padding.height;
   dc582:	685a      	ldr	r2, [r3, #4]
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.dilation_height_factor = params->dilation_height_factor;
   dc584:	f8ad 1088 	strh.w	r1, [sp, #136]	; 0x88
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
   dc588:	f8ad 207c 	strh.w	r2, [sp, #124]	; 0x7c
  op_params.padding_values.width = data->padding.width;

  reference_integer_ops::ConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
   dc58c:	4639      	mov	r1, r7
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
   dc58e:	f859 2b18 	ldr.w	r2, [r9], #24
   dc592:	f8ad 207a 	strh.w	r2, [sp, #122]	; 0x7a

  reference_integer_ops::ConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
   dc596:	a80a      	add	r0, sp, #40	; 0x28
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;

  reference_integer_ops::ConvPerChannel(
   dc598:	f503 768c 	add.w	r6, r3, #280	; 0x118
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
   dc59c:	f7fa fa09 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dc5a0:	f8d7 b004 	ldr.w	fp, [r7, #4]
      GetTensorData<int8>(input), GetTensorShape(filter),
   dc5a4:	af0f      	add	r7, sp, #60	; 0x3c
   dc5a6:	4621      	mov	r1, r4
   dc5a8:	4638      	mov	r0, r7
   dc5aa:	f7fa fa02 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dc5ae:	b104      	cbz	r4, dc5b2 <_ZN6tflite3ops5micro4conv23EvalQuantizedPerChannelEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_+0x60>
   dc5b0:	6864      	ldr	r4, [r4, #4]
      GetTensorData<int8>(filter), GetTensorShape(bias),
   dc5b2:	f10d 0850 	add.w	r8, sp, #80	; 0x50
   dc5b6:	4629      	mov	r1, r5
   dc5b8:	4640      	mov	r0, r8
   dc5ba:	f7fa f9fa 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dc5be:	b10d      	cbz	r5, dc5c4 <_ZN6tflite3ops5micro4conv23EvalQuantizedPerChannelEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_+0x72>
   dc5c0:	686b      	ldr	r3, [r5, #4]
   dc5c2:	e000      	b.n	dc5c6 <_ZN6tflite3ops5micro4conv23EvalQuantizedPerChannelEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_+0x74>
   dc5c4:	462b      	mov	r3, r5
      GetTensorData<int32>(bias), GetTensorShape(output),
   dc5c6:	ad19      	add	r5, sp, #100	; 0x64
   dc5c8:	4651      	mov	r1, sl
   dc5ca:	4628      	mov	r0, r5
   dc5cc:	9309      	str	r3, [sp, #36]	; 0x24
   dc5ce:	f7fa f9f0 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorData<int8>(output));
   dc5d2:	f8da 2004 	ldr.w	r2, [sl, #4]
   dc5d6:	9b09      	ldr	r3, [sp, #36]	; 0x24
   dc5d8:	9206      	str	r2, [sp, #24]
   dc5da:	4649      	mov	r1, r9
   dc5dc:	4632      	mov	r2, r6
   dc5de:	9304      	str	r3, [sp, #16]
   dc5e0:	a81e      	add	r0, sp, #120	; 0x78
   dc5e2:	ab0a      	add	r3, sp, #40	; 0x28
   dc5e4:	9505      	str	r5, [sp, #20]
   dc5e6:	f8cd 800c 	str.w	r8, [sp, #12]
   dc5ea:	9402      	str	r4, [sp, #8]
   dc5ec:	9701      	str	r7, [sp, #4]
   dc5ee:	f8cd b000 	str.w	fp, [sp]
   dc5f2:	f7ff fd69 	bl	dc0c8 <_ZN6tflite21reference_integer_ops14ConvPerChannelERKNS_10ConvParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa>
  reference_integer_ops::ConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
      GetTensorData<int8>(filter), GetTensorShape(bias),
      GetTensorData<int32>(bias), GetTensorShape(output),
   dc5f6:	4628      	mov	r0, r5
   dc5f8:	f7f9 ff2b 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>

  reference_integer_ops::ConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
      GetTensorData<int8>(filter), GetTensorShape(bias),
   dc5fc:	4640      	mov	r0, r8
   dc5fe:	f7f9 ff28 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.padding_values.width = data->padding.width;

  reference_integer_ops::ConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
   dc602:	4638      	mov	r0, r7
   dc604:	f7f9 ff25 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;

  reference_integer_ops::ConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
   dc608:	a80a      	add	r0, sp, #40	; 0x28
   dc60a:	f7f9 ff22 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
      GetTensorData<int8>(input), GetTensorShape(filter),
      GetTensorData<int8>(filter), GetTensorShape(bias),
      GetTensorData<int32>(bias), GetTensorShape(output),
      GetTensorData<int8>(output));
}
   dc60e:	b02d      	add	sp, #180	; 0xb4
   dc610:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dc614 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_>:

void EvalFloat(TfLiteContext* context, TfLiteNode* node,
               TfLiteConvParams* params, OpData* data,
               const TfLiteTensor* input, const TfLiteTensor* filter,
               const TfLiteTensor* bias, TfLiteTensor* im2col,
               TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
   dc614:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
   dc618:	7d11      	ldrb	r1, [r2, #20]

void EvalFloat(TfLiteContext* context, TfLiteNode* node,
               TfLiteConvParams* params, OpData* data,
               const TfLiteTensor* input, const TfLiteTensor* filter,
               const TfLiteTensor* bias, TfLiteTensor* im2col,
               TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
   dc61a:	b0b1      	sub	sp, #196	; 0xc4
   dc61c:	ac3a      	add	r4, sp, #232	; 0xe8
// Calculates the useful range of an activation layer given its activation
// tensor.a
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
   dc61e:	2901      	cmp	r1, #1
   dc620:	e894 0170 	ldmia.w	r4, {r4, r5, r6, r8}
   dc624:	9f3f      	ldr	r7, [sp, #252]	; 0xfc
   dc626:	d011      	beq.n	dc64c <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x38>
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
   dc628:	2903      	cmp	r1, #3
   dc62a:	d012      	beq.n	dc652 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x3e>
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
    *activation_max = std::numeric_limits<T>::max();
   dc62c:	ed9f 7a3f 	vldr	s14, [pc, #252]	; dc72c <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x118>
   dc630:	eddf 6a3f 	vldr	s13, [pc, #252]	; dc730 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x11c>
   dc634:	2902      	cmp	r1, #2
   dc636:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   dc63a:	bf18      	it	ne
   dc63c:	eef0 7a47 	vmovne.f32	s15, s14
   dc640:	eebf 7a00 	vmov.f32	s14, #240	; 0xbf800000 -1.0
   dc644:	bf18      	it	ne
   dc646:	eeb0 7a66 	vmovne.f32	s14, s13
   dc64a:	e006      	b.n	dc65a <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x46>
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
   dc64c:	eddf 7a37 	vldr	s15, [pc, #220]	; dc72c <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x118>
   dc650:	e001      	b.n	dc656 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x42>
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
    *activation_max = 6;
   dc652:	eef1 7a08 	vmov.f32	s15, #24	; 0x40c00000  6.0
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
   dc656:	ed9f 7a37 	vldr	s14, [pc, #220]	; dc734 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x120>
  int32_t output_activation_min;
  int32_t output_activation_max;
};

inline PaddingType RuntimePaddingType(TfLitePadding padding) {
  switch (padding) {
   dc65a:	7811      	ldrb	r1, [r2, #0]
   dc65c:	2901      	cmp	r1, #1
   dc65e:	d003      	beq.n	dc668 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x54>
      return PaddingType::kSame;
    case TfLitePadding::kTfLitePaddingValid:
      return PaddingType::kValid;
    case TfLitePadding::kTfLitePaddingUnknown:
    default:
      return PaddingType::kNone;
   dc660:	2902      	cmp	r1, #2
   dc662:	bf0c      	ite	eq
   dc664:	2102      	moveq	r1, #2
   dc666:	2100      	movne	r1, #0
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(params->padding);
   dc668:	f88d 1088 	strb.w	r1, [sp, #136]	; 0x88
  op_params.padding_values.width = data->padding.width;
   dc66c:	6819      	ldr	r1, [r3, #0]
  op_params.padding_values.height = data->padding.height;
   dc66e:	685b      	ldr	r3, [r3, #4]
   dc670:	f8ad 308c 	strh.w	r3, [sp, #140]	; 0x8c
  op_params.stride_width = params->stride_width;
   dc674:	6853      	ldr	r3, [r2, #4]
   dc676:	f8ad 3092 	strh.w	r3, [sp, #146]	; 0x92
  op_params.stride_height = params->stride_height;
   dc67a:	6893      	ldr	r3, [r2, #8]
   dc67c:	f8ad 3094 	strh.w	r3, [sp, #148]	; 0x94
  op_params.dilation_width_factor = params->dilation_width_factor;
   dc680:	68d3      	ldr	r3, [r2, #12]
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(params->padding);
  op_params.padding_values.width = data->padding.width;
   dc682:	f8ad 108a 	strh.w	r1, [sp, #138]	; 0x8a
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
   dc686:	f8ad 3096 	strh.w	r3, [sp, #150]	; 0x96
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  reference_ops::Conv(op_params, GetTensorShape(input),
   dc68a:	4621      	mov	r1, r4
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
   dc68c:	6913      	ldr	r3, [r2, #16]
   dc68e:	f8ad 3098 	strh.w	r3, [sp, #152]	; 0x98
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  reference_ops::Conv(op_params, GetTensorShape(input),
   dc692:	a809      	add	r0, sp, #36	; 0x24
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
   dc694:	ed8d 7a2e 	vstr	s14, [sp, #184]	; 0xb8
  op_params.float_activation_max = output_activation_max;
   dc698:	edcd 7a2f 	vstr	s15, [sp, #188]	; 0xbc

  reference_ops::Conv(op_params, GetTensorShape(input),
   dc69c:	f7fa f989 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dc6a0:	b104      	cbz	r4, dc6a4 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x90>
   dc6a2:	6864      	ldr	r4, [r4, #4]
                      GetTensorData<float>(input), GetTensorShape(filter),
   dc6a4:	4629      	mov	r1, r5
   dc6a6:	a80e      	add	r0, sp, #56	; 0x38
   dc6a8:	f7fa f983 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dc6ac:	b105      	cbz	r5, dc6b0 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0x9c>
   dc6ae:	686d      	ldr	r5, [r5, #4]
                      GetTensorData<float>(filter), GetTensorShape(bias),
   dc6b0:	f10d 094c 	add.w	r9, sp, #76	; 0x4c
   dc6b4:	4631      	mov	r1, r6
   dc6b6:	4648      	mov	r0, r9
   dc6b8:	f7fa f97b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dc6bc:	b106      	cbz	r6, dc6c0 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xac>
   dc6be:	6876      	ldr	r6, [r6, #4]
                      GetTensorData<float>(bias), GetTensorShape(output),
   dc6c0:	f10d 0a60 	add.w	sl, sp, #96	; 0x60
   dc6c4:	4639      	mov	r1, r7
   dc6c6:	4650      	mov	r0, sl
   dc6c8:	f7fa f973 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dc6cc:	b107      	cbz	r7, dc6d0 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xbc>
   dc6ce:	687f      	ldr	r7, [r7, #4]
                      GetTensorData<float>(output), GetTensorShape(im2col),
   dc6d0:	f10d 0b74 	add.w	fp, sp, #116	; 0x74
   dc6d4:	4641      	mov	r1, r8
   dc6d6:	4658      	mov	r0, fp
   dc6d8:	f7fa f96b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dc6dc:	f1b8 0f00 	cmp.w	r8, #0
   dc6e0:	d002      	beq.n	dc6e8 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xd4>
   dc6e2:	f8d8 3004 	ldr.w	r3, [r8, #4]
   dc6e6:	e000      	b.n	dc6ea <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_+0xd6>
   dc6e8:	4643      	mov	r3, r8
                      GetTensorData<float>(im2col));
   dc6ea:	4622      	mov	r2, r4
   dc6ec:	a909      	add	r1, sp, #36	; 0x24
   dc6ee:	9306      	str	r3, [sp, #24]
   dc6f0:	a822      	add	r0, sp, #136	; 0x88
   dc6f2:	ab0e      	add	r3, sp, #56	; 0x38
   dc6f4:	f8cd b014 	str.w	fp, [sp, #20]
   dc6f8:	9704      	str	r7, [sp, #16]
   dc6fa:	f8cd a00c 	str.w	sl, [sp, #12]
   dc6fe:	9602      	str	r6, [sp, #8]
   dc700:	e88d 0220 	stmia.w	sp, {r5, r9}
   dc704:	f7ff fac0 	bl	dbc88 <_ZN6tflite13reference_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_>

  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<float>(input), GetTensorShape(filter),
                      GetTensorData<float>(filter), GetTensorShape(bias),
                      GetTensorData<float>(bias), GetTensorShape(output),
                      GetTensorData<float>(output), GetTensorShape(im2col),
   dc708:	4658      	mov	r0, fp
   dc70a:	f7f9 fea2 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.float_activation_max = output_activation_max;

  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<float>(input), GetTensorShape(filter),
                      GetTensorData<float>(filter), GetTensorShape(bias),
                      GetTensorData<float>(bias), GetTensorShape(output),
   dc70e:	4650      	mov	r0, sl
   dc710:	f7f9 fe9f 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<float>(input), GetTensorShape(filter),
                      GetTensorData<float>(filter), GetTensorShape(bias),
   dc714:	4648      	mov	r0, r9
   dc716:	f7f9 fe9c 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  reference_ops::Conv(op_params, GetTensorShape(input),
                      GetTensorData<float>(input), GetTensorShape(filter),
   dc71a:	a80e      	add	r0, sp, #56	; 0x38
   dc71c:	f7f9 fe99 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  reference_ops::Conv(op_params, GetTensorShape(input),
   dc720:	a809      	add	r0, sp, #36	; 0x24
   dc722:	f7f9 fe96 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                      GetTensorData<float>(input), GetTensorShape(filter),
                      GetTensorData<float>(filter), GetTensorShape(bias),
                      GetTensorData<float>(bias), GetTensorShape(output),
                      GetTensorData<float>(output), GetTensorShape(im2col),
                      GetTensorData<float>(im2col));
}
   dc726:	b031      	add	sp, #196	; 0xc4
   dc728:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dc72c:	7f7fffff 	.word	0x7f7fffff
   dc730:	ff7fffff 	.word	0xff7fffff
   dc734:	00000000 	.word	0x00000000

000dc738 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dc738:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dc73c:	680a      	ldr	r2, [r1, #0]
   dc73e:	6887      	ldr	r7, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dc740:	6896      	ldr	r6, [r2, #8]
   dc742:	4688      	mov	r8, r1
   dc744:	6851      	ldr	r1, [r2, #4]
}

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
   dc746:	68d2      	ldr	r2, [r2, #12]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dc748:	2338      	movs	r3, #56	; 0x38
   dc74a:	fb03 fa01 	mul.w	sl, r3, r1
   dc74e:	f5ad 7d15 	sub.w	sp, sp, #596	; 0x254
   dc752:	eb07 010a 	add.w	r1, r7, sl
   dc756:	4605      	mov	r5, r0
  int filter_height = filter->dims->data[1];
  int output_width = output->dims->data[2];
  int output_height = output->dims->data[1];

  OpData data;
  if (input->type != kTfLiteFloat32) {
   dc758:	f817 000a 	ldrb.w	r0, [r7, sl]
   dc75c:	910b      	str	r1, [sp, #44]	; 0x2c

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
   dc75e:	1c51      	adds	r1, r2, #1
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dc760:	fb03 7606 	mla	r6, r3, r6, r7
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dc764:	bf14      	ite	ne
   dc766:	fb03 7302 	mlane	r3, r3, r2, r7
  }
  return nullptr;
   dc76a:	2300      	moveq	r3, #0
   dc76c:	2801      	cmp	r0, #1
   dc76e:	930a      	str	r3, [sp, #40]	; 0x28
   dc770:	d024      	beq.n	dc7bc <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x84>
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
   dc772:	f896 4030 	ldrb.w	r4, [r6, #48]	; 0x30
   dc776:	2c01      	cmp	r4, #1
   dc778:	d00e      	beq.n	dc798 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x60>
   dc77a:	4b47      	ldr	r3, [pc, #284]	; (dc898 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x160>)
   dc77c:	9301      	str	r3, [sp, #4]
   dc77e:	2601      	movs	r6, #1
   dc780:	4b46      	ldr	r3, [pc, #280]	; (dc89c <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x164>)
   dc782:	9402      	str	r4, [sp, #8]
   dc784:	9300      	str	r3, [sp, #0]
   dc786:	696c      	ldr	r4, [r5, #20]
   dc788:	9603      	str	r6, [sp, #12]
   dc78a:	23dd      	movs	r3, #221	; 0xdd
   dc78c:	4a44      	ldr	r2, [pc, #272]	; (dc8a0 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x168>)
   dc78e:	4945      	ldr	r1, [pc, #276]	; (dc8a4 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x16c>)
   dc790:	4628      	mov	r0, r5
   dc792:	47a0      	blx	r4
   dc794:	4634      	mov	r4, r6
   dc796:	e079      	b.n	dc88c <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x154>
                      kTfLiteAffineQuantization);

    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
   dc798:	6b73      	ldr	r3, [r6, #52]	; 0x34
    TF_LITE_ENSURE(context, affine_quantization);
   dc79a:	b923      	cbnz	r3, dc7a6 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x6e>
   dc79c:	4b42      	ldr	r3, [pc, #264]	; (dc8a8 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x170>)
   dc79e:	9300      	str	r3, [sp, #0]
   dc7a0:	696e      	ldr	r6, [r5, #20]
   dc7a2:	23e2      	movs	r3, #226	; 0xe2
   dc7a4:	e005      	b.n	dc7b2 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x7a>
    TF_LITE_ENSURE(context, affine_quantization->scale);
   dc7a6:	681b      	ldr	r3, [r3, #0]
   dc7a8:	b943      	cbnz	r3, dc7bc <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x84>
   dc7aa:	4b40      	ldr	r3, [pc, #256]	; (dc8ac <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x174>)
   dc7ac:	696e      	ldr	r6, [r5, #20]
   dc7ae:	9300      	str	r3, [sp, #0]
   dc7b0:	23e3      	movs	r3, #227	; 0xe3
   dc7b2:	4a3b      	ldr	r2, [pc, #236]	; (dc8a0 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x168>)
   dc7b4:	493e      	ldr	r1, [pc, #248]	; (dc8b0 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x178>)
   dc7b6:	4628      	mov	r0, r5
   dc7b8:	47b0      	blx	r6
   dc7ba:	e067      	b.n	dc88c <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x154>
                      GetTensorData<float>(output), GetTensorShape(im2col),
                      GetTensorData<float>(im2col));
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
   dc7bc:	f8d8 3014 	ldr.w	r3, [r8, #20]
   dc7c0:	9309      	str	r3, [sp, #36]	; 0x24
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dc7c2:	f8d8 3004 	ldr.w	r3, [r8, #4]
  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);

  int input_width = input->dims->data[2];
  int input_height = input->dims->data[1];
  int filter_width = filter->dims->data[2];
   dc7c6:	68b2      	ldr	r2, [r6, #8]
   dc7c8:	685b      	ldr	r3, [r3, #4]
   dc7ca:	f04f 0938 	mov.w	r9, #56	; 0x38
   dc7ce:	fb09 7903 	mla	r9, r9, r3, r7
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);

  int input_width = input->dims->data[2];
   dc7d2:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
  int input_height = input->dims->data[1];
  int filter_width = filter->dims->data[2];
  int filter_height = filter->dims->data[1];
  int output_width = output->dims->data[2];
   dc7d4:	f8d9 1008 	ldr.w	r1, [r9, #8]
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);

  int input_width = input->dims->data[2];
   dc7d8:	689b      	ldr	r3, [r3, #8]
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
  }

  TF_LITE_ENSURE_STATUS(CalculateOpData(
   dc7da:	9005      	str	r0, [sp, #20]
   dc7dc:	f10d 0b30 	add.w	fp, sp, #48	; 0x30
   dc7e0:	f8cd b018 	str.w	fp, [sp, #24]
   dc7e4:	6888      	ldr	r0, [r1, #8]
   dc7e6:	9004      	str	r0, [sp, #16]
   dc7e8:	68c9      	ldr	r1, [r1, #12]
   dc7ea:	9103      	str	r1, [sp, #12]
   dc7ec:	6891      	ldr	r1, [r2, #8]
   dc7ee:	9102      	str	r1, [sp, #8]
   dc7f0:	68d2      	ldr	r2, [r2, #12]
   dc7f2:	9201      	str	r2, [sp, #4]
   dc7f4:	689a      	ldr	r2, [r3, #8]
   dc7f6:	9200      	str	r2, [sp, #0]
   dc7f8:	68db      	ldr	r3, [r3, #12]
   dc7fa:	9a09      	ldr	r2, [sp, #36]	; 0x24
   dc7fc:	4641      	mov	r1, r8
   dc7fe:	4628      	mov	r0, r5
   dc800:	f7ff fd76 	bl	dc2f0 <_ZN6tflite3ops5micro4conv15CalculateOpDataEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsiiiiii10TfLiteTypePNS2_6OpDataE>
   dc804:	4604      	mov	r4, r0
   dc806:	2800      	cmp	r0, #0
   dc808:	d13f      	bne.n	dc88a <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x152>
      context, node, params, input_width, input_height, filter_width,
      filter_height, output_width, output_height, input->type, &data));

  switch (input->type) {  // Already know in/out types are same.
   dc80a:	f817 000a 	ldrb.w	r0, [r7, sl]
   dc80e:	2803      	cmp	r0, #3
   dc810:	d022      	beq.n	dc858 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x120>
   dc812:	2809      	cmp	r0, #9
   dc814:	d011      	beq.n	dc83a <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x102>
   dc816:	2801      	cmp	r0, #1
   dc818:	d12e      	bne.n	dc878 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x140>
    case kTfLiteFloat32:
      EvalFloat(context, node, params, &data, input, filter, bias, nullptr,
                nullptr, output);
   dc81a:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   dc81c:	9302      	str	r3, [sp, #8]
   dc81e:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   dc820:	9300      	str	r3, [sp, #0]
   dc822:	f8cd 9014 	str.w	r9, [sp, #20]
   dc826:	9404      	str	r4, [sp, #16]
   dc828:	9403      	str	r4, [sp, #12]
   dc82a:	9601      	str	r6, [sp, #4]
   dc82c:	465b      	mov	r3, fp
   dc82e:	9a09      	ldr	r2, [sp, #36]	; 0x24
   dc830:	4641      	mov	r1, r8
   dc832:	4628      	mov	r0, r5
   dc834:	f7ff feee 	bl	dc614 <_ZN6tflite3ops5micro4conv9EvalFloatEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_>
      break;
   dc838:	e028      	b.n	dc88c <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x154>
    case kTfLiteInt8:
      EvalQuantizedPerChannel(context, node, params, &data, input, filter, bias,
                              output, nullptr);
   dc83a:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   dc83c:	9302      	str	r3, [sp, #8]
   dc83e:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   dc840:	9300      	str	r3, [sp, #0]
   dc842:	9404      	str	r4, [sp, #16]
   dc844:	f8cd 900c 	str.w	r9, [sp, #12]
   dc848:	9601      	str	r6, [sp, #4]
   dc84a:	465b      	mov	r3, fp
   dc84c:	9a09      	ldr	r2, [sp, #36]	; 0x24
   dc84e:	4641      	mov	r1, r8
   dc850:	4628      	mov	r0, r5
   dc852:	f7ff fe7e 	bl	dc552 <_ZN6tflite3ops5micro4conv23EvalQuantizedPerChannelEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_>
      break;
   dc856:	e019      	b.n	dc88c <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x154>
    case kTfLiteUInt8:
      EvalQuantized(context, node, params, &data, input, filter, bias, nullptr,
                    nullptr, output);
   dc858:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   dc85a:	9302      	str	r3, [sp, #8]
   dc85c:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   dc85e:	9300      	str	r3, [sp, #0]
   dc860:	f8cd 9014 	str.w	r9, [sp, #20]
   dc864:	9404      	str	r4, [sp, #16]
   dc866:	9403      	str	r4, [sp, #12]
   dc868:	9601      	str	r6, [sp, #4]
   dc86a:	465b      	mov	r3, fp
   dc86c:	9a09      	ldr	r2, [sp, #36]	; 0x24
   dc86e:	4641      	mov	r1, r8
   dc870:	4628      	mov	r0, r5
   dc872:	f7ff fde9 	bl	dc448 <_ZN6tflite3ops5micro4conv13EvalQuantizedEP13TfLiteContextP10TfLiteNodeP16TfLiteConvParamsPNS2_6OpDataEPK12TfLiteTensorSD_SD_PSB_SE_SE_>
      break;
   dc876:	e009      	b.n	dc88c <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x154>
    default:
      context->ReportError(context, "Type %s (%d) not supported.",
   dc878:	696c      	ldr	r4, [r5, #20]
   dc87a:	f7f7 fc4f 	bl	d411c <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type), input->type);
   dc87e:	f817 300a 	ldrb.w	r3, [r7, sl]
   dc882:	490c      	ldr	r1, [pc, #48]	; (dc8b4 <_ZN6tflite3ops5micro4conv4EvalEP13TfLiteContextP10TfLiteNode+0x17c>)
   dc884:	4602      	mov	r2, r0
   dc886:	4628      	mov	r0, r5
   dc888:	47a0      	blx	r4
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
  }

  TF_LITE_ENSURE_STATUS(CalculateOpData(
   dc88a:	2401      	movs	r4, #1
      context->ReportError(context, "Type %s (%d) not supported.",
                           TfLiteTypeGetName(input->type), input->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   dc88c:	4620      	mov	r0, r4
   dc88e:	f50d 7d15 	add.w	sp, sp, #596	; 0x254
   dc892:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dc896:	bf00      	nop
   dc898:	000e9ae7 	.word	0x000e9ae7
   dc89c:	000e9b01 	.word	0x000e9b01
   dc8a0:	000e99f2 	.word	0x000e99f2
   dc8a4:	000e98c8 	.word	0x000e98c8
   dc8a8:	000e9b1b 	.word	0x000e9b1b
   dc8ac:	000e9b2f 	.word	0x000e9b2f
   dc8b0:	000e9a98 	.word	0x000e9a98
   dc8b4:	000e9b4a 	.word	0x000e9b4a

000dc8b8 <_ZN6tflite3ops5micro16Register_CONV_2DEv>:

TfLiteRegistration* Register_CONV_2D() {
  static TfLiteRegistration r = {conv::Init, conv::Free, conv::Prepare,
                                 conv::Eval};
  return &r;
}
   dc8b8:	4800      	ldr	r0, [pc, #0]	; (dc8bc <_ZN6tflite3ops5micro16Register_CONV_2DEv+0x4>)
   dc8ba:	4770      	bx	lr
   dc8bc:	2003bfbc 	.word	0x2003bfbc

000dc8c0 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode>:
namespace tflite {
namespace ops {
namespace micro {
namespace dequantize {

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   dc8c0:	b5f0      	push	{r4, r5, r6, r7, lr}
   dc8c2:	680b      	ldr	r3, [r1, #0]
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
   dc8c4:	681e      	ldr	r6, [r3, #0]
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   dc8c6:	2e01      	cmp	r6, #1
namespace tflite {
namespace ops {
namespace micro {
namespace dequantize {

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   dc8c8:	b085      	sub	sp, #20
   dc8ca:	4605      	mov	r5, r0
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   dc8cc:	d00c      	beq.n	dc8e8 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x28>
   dc8ce:	4b20      	ldr	r3, [pc, #128]	; (dc950 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x90>)
   dc8d0:	9301      	str	r3, [sp, #4]
   dc8d2:	2401      	movs	r4, #1
   dc8d4:	4b1f      	ldr	r3, [pc, #124]	; (dc954 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x94>)
   dc8d6:	9300      	str	r3, [sp, #0]
   dc8d8:	9403      	str	r4, [sp, #12]
   dc8da:	9602      	str	r6, [sp, #8]
   dc8dc:	6945      	ldr	r5, [r0, #20]
   dc8de:	4a1e      	ldr	r2, [pc, #120]	; (dc958 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x98>)
   dc8e0:	491e      	ldr	r1, [pc, #120]	; (dc95c <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x9c>)
   dc8e2:	231d      	movs	r3, #29
   dc8e4:	47a8      	blx	r5
   dc8e6:	e02d      	b.n	dc944 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x84>
   dc8e8:	684f      	ldr	r7, [r1, #4]
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
   dc8ea:	683c      	ldr	r4, [r7, #0]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
   dc8ec:	2c01      	cmp	r4, #1
   dc8ee:	d00b      	beq.n	dc908 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x48>
   dc8f0:	4b17      	ldr	r3, [pc, #92]	; (dc950 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x90>)
   dc8f2:	9301      	str	r3, [sp, #4]
   dc8f4:	4b1a      	ldr	r3, [pc, #104]	; (dc960 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0xa0>)
   dc8f6:	9300      	str	r3, [sp, #0]
   dc8f8:	9603      	str	r6, [sp, #12]
   dc8fa:	9402      	str	r4, [sp, #8]
   dc8fc:	6944      	ldr	r4, [r0, #20]
   dc8fe:	4a16      	ldr	r2, [pc, #88]	; (dc958 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x98>)
   dc900:	4916      	ldr	r1, [pc, #88]	; (dc95c <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x9c>)
   dc902:	231e      	movs	r3, #30
   dc904:	47a0      	blx	r4
   dc906:	e01d      	b.n	dc944 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x84>

  // TODO(b/140515557): Add cached dequant to improve hybrid model performance.
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  TF_LITE_ENSURE(context,
   dc908:	685a      	ldr	r2, [r3, #4]
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);

  // TODO(b/140515557): Add cached dequant to improve hybrid model performance.
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
   dc90a:	6881      	ldr	r1, [r0, #8]
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  TF_LITE_ENSURE(context,
   dc90c:	2338      	movs	r3, #56	; 0x38
   dc90e:	435a      	muls	r2, r3
   dc910:	5c8a      	ldrb	r2, [r1, r2]
   dc912:	2a03      	cmp	r2, #3
   dc914:	d009      	beq.n	dc92a <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x6a>
   dc916:	2a09      	cmp	r2, #9
   dc918:	d007      	beq.n	dc92a <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x6a>
   dc91a:	4b12      	ldr	r3, [pc, #72]	; (dc964 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0xa4>)
   dc91c:	9300      	str	r3, [sp, #0]
   dc91e:	6945      	ldr	r5, [r0, #20]
   dc920:	4a0d      	ldr	r2, [pc, #52]	; (dc958 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x98>)
   dc922:	4911      	ldr	r1, [pc, #68]	; (dc968 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0xa8>)
   dc924:	2325      	movs	r3, #37	; 0x25
   dc926:	47a8      	blx	r5
   dc928:	e00c      	b.n	dc944 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x84>
                 input->type == kTfLiteUInt8 || input->type == kTfLiteInt8);
  TF_LITE_ENSURE(context, output->type == kTfLiteFloat32);
   dc92a:	687a      	ldr	r2, [r7, #4]
   dc92c:	4353      	muls	r3, r2
   dc92e:	5ccb      	ldrb	r3, [r1, r3]
   dc930:	2b01      	cmp	r3, #1
   dc932:	d009      	beq.n	dc948 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x88>
   dc934:	4b0d      	ldr	r3, [pc, #52]	; (dc96c <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0xac>)
   dc936:	9300      	str	r3, [sp, #0]
   dc938:	696c      	ldr	r4, [r5, #20]
   dc93a:	4a07      	ldr	r2, [pc, #28]	; (dc958 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x98>)
   dc93c:	490a      	ldr	r1, [pc, #40]	; (dc968 <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0xa8>)
   dc93e:	2326      	movs	r3, #38	; 0x26
   dc940:	4628      	mov	r0, r5
   dc942:	47a0      	blx	r4
   dc944:	2001      	movs	r0, #1
   dc946:	e000      	b.n	dc94a <_ZN6tflite3ops5micro10dequantize7PrepareEP13TfLiteContextP10TfLiteNode+0x8a>

  return kTfLiteOk;
   dc948:	2000      	movs	r0, #0
}
   dc94a:	b005      	add	sp, #20
   dc94c:	bdf0      	pop	{r4, r5, r6, r7, pc}
   dc94e:	bf00      	nop
   dc950:	000eb295 	.word	0x000eb295
   dc954:	000e98e2 	.word	0x000e98e2
   dc958:	000e9bc0 	.word	0x000e9bc0
   dc95c:	000e98c8 	.word	0x000e98c8
   dc960:	000e98f2 	.word	0x000e98f2
   dc964:	000e9c6c 	.word	0x000e9c6c
   dc968:	000e9a98 	.word	0x000e9a98
   dc96c:	000e9ca6 	.word	0x000e9ca6

000dc970 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_>:
}

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
   dc970:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
   dc974:	6806      	ldr	r6, [r0, #0]
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   dc976:	680b      	ldr	r3, [r1, #0]
   dc978:	429e      	cmp	r6, r3
}

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
   dc97a:	4604      	mov	r4, r0
   dc97c:	460f      	mov	r7, r1
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   dc97e:	d101      	bne.n	dc984 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_+0x14>
   dc980:	2500      	movs	r5, #0
   dc982:	e00d      	b.n	dc9a0 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_+0x30>
   dc984:	f007 fce2 	bl	e434c <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   dc988:	4629      	mov	r1, r5
   dc98a:	4620      	mov	r0, r4
   dc98c:	f7f9 fd6c 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dc990:	4629      	mov	r1, r5
   dc992:	4680      	mov	r8, r0
   dc994:	4638      	mov	r0, r7
   dc996:	f7f9 fd67 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dc99a:	4580      	cmp	r8, r0
   dc99c:	d1f2      	bne.n	dc984 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_+0x14>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   dc99e:	3501      	adds	r5, #1
   dc9a0:	42b5      	cmp	r5, r6
   dc9a2:	dbf1      	blt.n	dc988 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_+0x18>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   dc9a4:	2e04      	cmp	r6, #4
   dc9a6:	bfcc      	ite	gt
   dc9a8:	6864      	ldrgt	r4, [r4, #4]
   dc9aa:	3404      	addle	r4, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dc9ac:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
   dc9ae:	2001      	movs	r0, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dc9b0:	429e      	cmp	r6, r3
   dc9b2:	dd04      	ble.n	dc9be <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_+0x4e>
      buffer_size *= dims_data[i];
   dc9b4:	f854 2023 	ldr.w	r2, [r4, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dc9b8:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
   dc9ba:	4350      	muls	r0, r2
   dc9bc:	e7f8      	b.n	dc9b0 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_+0x40>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
  }
  return shape.FlatSize();
}
   dc9be:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
	...

000dc9c4 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dc9c4:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
   dc9c8:	680b      	ldr	r3, [r1, #0]
   dc9ca:	f8d0 b008 	ldr.w	fp, [r0, #8]
   dc9ce:	685a      	ldr	r2, [r3, #4]
   dc9d0:	2338      	movs	r3, #56	; 0x38
   dc9d2:	fb03 f802 	mul.w	r8, r3, r2
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
   dc9d6:	684a      	ldr	r2, [r1, #4]

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
   dc9d8:	eb0b 0508 	add.w	r5, fp, r8
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
   dc9dc:	6854      	ldr	r4, [r2, #4]

  tflite::DequantizationParams op_params;
  op_params.zero_point = input->params.zero_point;
   dc9de:	f8d5 9010 	ldr.w	r9, [r5, #16]
  TF_LITE_ENSURE(context, output->type == kTfLiteFloat32);

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dc9e2:	b08b      	sub	sp, #44	; 0x2c
   dc9e4:	4682      	mov	sl, r0
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  tflite::DequantizationParams op_params;
  op_params.zero_point = input->params.zero_point;
  op_params.scale = input->params.scale;
   dc9e6:	68e8      	ldr	r0, [r5, #12]
  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
   dc9e8:	fb03 b404 	mla	r4, r3, r4, fp

  tflite::DequantizationParams op_params;
  op_params.zero_point = input->params.zero_point;
  op_params.scale = input->params.scale;
   dc9ec:	f00a faee 	bl	e6fcc <__aeabi_f2d>
   dc9f0:	4606      	mov	r6, r0
  switch (input->type) {
   dc9f2:	f81b 0008 	ldrb.w	r0, [fp, r8]
   dc9f6:	2803      	cmp	r0, #3
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  tflite::DequantizationParams op_params;
  op_params.zero_point = input->params.zero_point;
  op_params.scale = input->params.scale;
   dc9f8:	460f      	mov	r7, r1
  switch (input->type) {
   dc9fa:	d002      	beq.n	dca02 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0x3e>
   dc9fc:	2809      	cmp	r0, #9
   dc9fe:	d025      	beq.n	dca4c <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0x88>
   dca00:	e051      	b.n	dcaa6 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0xe2>
    case kTfLiteUInt8:
      reference_ops::Dequantize(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   dca02:	4629      	mov	r1, r5
   dca04:	4668      	mov	r0, sp
   dca06:	f7f9 ffd4 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
          GetTensorShape(output), GetTensorData<float>(output));
   dca0a:	4621      	mov	r1, r4
   dca0c:	a805      	add	r0, sp, #20
   dca0e:	f8d5 8004 	ldr.w	r8, [r5, #4]
   dca12:	f7f9 ffce 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dca16:	b104      	cbz	r4, dca1a <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0x56>
   dca18:	6864      	ldr	r4, [r4, #4]
inline void Dequantize(const tflite::DequantizationParams& op_params,
                       const RuntimeShape& input_shape, const T* input_data,
                       const RuntimeShape& output_shape, float* output_data) {
  int32 zero_point = op_params.zero_point;
  const double scale = op_params.scale;
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
   dca1a:	a905      	add	r1, sp, #20
   dca1c:	4668      	mov	r0, sp
   dca1e:	f7ff ffa7 	bl	dc970 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_>
   dca22:	4645      	mov	r5, r8
   dca24:	4682      	mov	sl, r0

  for (int i = 0; i < flat_size; i++) {
   dca26:	ebc8 0305 	rsb	r3, r8, r5
   dca2a:	4553      	cmp	r3, sl
   dca2c:	da33      	bge.n	dca96 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0xd2>
    const int32 val = input_data[i];
   dca2e:	f815 0b01 	ldrb.w	r0, [r5], #1
    const float result = static_cast<float>(scale * (val - zero_point));
    output_data[i] = result;
   dca32:	ebc9 0000 	rsb	r0, r9, r0
   dca36:	f00a fab7 	bl	e6fa8 <__aeabi_i2d>
   dca3a:	4632      	mov	r2, r6
   dca3c:	463b      	mov	r3, r7
   dca3e:	f00a fb19 	bl	e7074 <__aeabi_dmul>
   dca42:	f00a fdf9 	bl	e7638 <__aeabi_d2f>
   dca46:	f844 0b04 	str.w	r0, [r4], #4
   dca4a:	e7ec      	b.n	dca26 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0x62>
      break;
    case kTfLiteInt8:
      reference_ops::Dequantize(
          op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
   dca4c:	4629      	mov	r1, r5
   dca4e:	4668      	mov	r0, sp
   dca50:	f7f9 ffaf 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
          GetTensorShape(output), GetTensorData<float>(output));
   dca54:	4621      	mov	r1, r4
   dca56:	a805      	add	r0, sp, #20
   dca58:	f8d5 8004 	ldr.w	r8, [r5, #4]
   dca5c:	f7f9 ffa9 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dca60:	b104      	cbz	r4, dca64 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0xa0>
   dca62:	6864      	ldr	r4, [r4, #4]
inline void Dequantize(const tflite::DequantizationParams& op_params,
                       const RuntimeShape& input_shape, const T* input_data,
                       const RuntimeShape& output_shape, float* output_data) {
  int32 zero_point = op_params.zero_point;
  const double scale = op_params.scale;
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
   dca64:	a905      	add	r1, sp, #20
   dca66:	4668      	mov	r0, sp
   dca68:	f7ff ff82 	bl	dc970 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_>
   dca6c:	4645      	mov	r5, r8
   dca6e:	4682      	mov	sl, r0

  for (int i = 0; i < flat_size; i++) {
   dca70:	ebc8 0305 	rsb	r3, r8, r5
   dca74:	4553      	cmp	r3, sl
   dca76:	da0e      	bge.n	dca96 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0xd2>
    const int32 val = input_data[i];
   dca78:	f915 0b01 	ldrsb.w	r0, [r5], #1
    const float result = static_cast<float>(scale * (val - zero_point));
    output_data[i] = result;
   dca7c:	ebc9 0000 	rsb	r0, r9, r0
   dca80:	f00a fa92 	bl	e6fa8 <__aeabi_i2d>
   dca84:	4632      	mov	r2, r6
   dca86:	463b      	mov	r3, r7
   dca88:	f00a faf4 	bl	e7074 <__aeabi_dmul>
   dca8c:	f00a fdd4 	bl	e7638 <__aeabi_d2f>
   dca90:	f844 0b04 	str.w	r0, [r4], #4
   dca94:	e7ec      	b.n	dca70 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0xac>
   dca96:	a805      	add	r0, sp, #20
   dca98:	f7f9 fcdb 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(output), GetTensorData<float>(output));
      break;
    case kTfLiteInt8:
      reference_ops::Dequantize(
          op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
   dca9c:	4668      	mov	r0, sp
   dca9e:	f7f9 fcd8 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(context, "Type %s (%d) not supported.",
                           TfLiteTypeGetName(input->type), input->type);
      return kTfLiteError;
  }

  return kTfLiteOk;
   dcaa2:	2000      	movs	r0, #0
      break;
    case kTfLiteInt8:
      reference_ops::Dequantize(
          op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
          GetTensorShape(output), GetTensorData<float>(output));
      break;
   dcaa4:	e00a      	b.n	dcabc <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0xf8>
    default:
      context->ReportError(context, "Type %s (%d) not supported.",
   dcaa6:	f8da 4014 	ldr.w	r4, [sl, #20]
   dcaaa:	f7f7 fb37 	bl	d411c <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type), input->type);
   dcaae:	f81b 3008 	ldrb.w	r3, [fp, r8]
   dcab2:	4904      	ldr	r1, [pc, #16]	; (dcac4 <_ZN6tflite3ops5micro10dequantize4EvalEP13TfLiteContextP10TfLiteNode+0x100>)
   dcab4:	4602      	mov	r2, r0
   dcab6:	4650      	mov	r0, sl
   dcab8:	47a0      	blx	r4
      return kTfLiteError;
   dcaba:	2001      	movs	r0, #1
  }

  return kTfLiteOk;
}
   dcabc:	b00b      	add	sp, #44	; 0x2c
   dcabe:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dcac2:	bf00      	nop
   dcac4:	000e9b4a 	.word	0x000e9b4a

000dcac8 <_ZN6tflite3ops5micro19Register_DEQUANTIZEEv>:

TfLiteRegistration* Register_DEQUANTIZE() {
  static TfLiteRegistration r = {nullptr, nullptr, dequantize::Prepare,
                                 dequantize::Eval};
  return &r;
}
   dcac8:	4800      	ldr	r0, [pc, #0]	; (dcacc <_ZN6tflite3ops5micro19Register_DEQUANTIZEEv+0x4>)
   dcaca:	4770      	bx	lr
   dcacc:	2003bfdc 	.word	0x2003bfdc

000dcad0 <_ZSt3absf>:
#endif

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  abs(float __x)
  { return __builtin_fabsf(__x); }
   dcad0:	eeb0 0ac0 	vabs.f32	s0, s0
   dcad4:	4770      	bx	lr

000dcad6 <_ZZN6tflite3ops5micro11elementwise12_GLOBAL__N_110SquareEvalEP13TfLiteContextP10TfLiteNodeENUlfE_4_FUNEf>:
TfLiteStatus RsqrtEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalNumeric(context, node, [](float f) { return 1.f / std::sqrt(f); });
}

TfLiteStatus SquareEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalNumeric(context, node, [](float f) { return f * f; });
   dcad6:	ee20 0a00 	vmul.f32	s0, s0, s0
   dcada:	4770      	bx	lr

000dcadc <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode>:
  return type == kTfLiteBool;
}

typedef bool (*IsSupportedType)(TfLiteType);
template <IsSupportedType>
TfLiteStatus GenericPrepare(TfLiteContext* context, TfLiteNode* node) {
   dcadc:	e92d 41ff 	stmdb	sp!, {r0, r1, r2, r3, r4, r5, r6, r7, r8, lr}
   dcae0:	680b      	ldr	r3, [r1, #0]
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
   dcae2:	681e      	ldr	r6, [r3, #0]
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   dcae4:	2e01      	cmp	r6, #1
  return type == kTfLiteBool;
}

typedef bool (*IsSupportedType)(TfLiteType);
template <IsSupportedType>
TfLiteStatus GenericPrepare(TfLiteContext* context, TfLiteNode* node) {
   dcae6:	4605      	mov	r5, r0
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   dcae8:	d009      	beq.n	dcafe <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x22>
   dcaea:	4b21      	ldr	r3, [pc, #132]	; (dcb70 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x94>)
   dcaec:	9301      	str	r3, [sp, #4]
   dcaee:	2401      	movs	r4, #1
   dcaf0:	4b20      	ldr	r3, [pc, #128]	; (dcb74 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x98>)
   dcaf2:	9300      	str	r3, [sp, #0]
   dcaf4:	9403      	str	r4, [sp, #12]
   dcaf6:	9602      	str	r6, [sp, #8]
   dcaf8:	6945      	ldr	r5, [r0, #20]
   dcafa:	2327      	movs	r3, #39	; 0x27
   dcafc:	e022      	b.n	dcb44 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x68>
   dcafe:	6849      	ldr	r1, [r1, #4]
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
   dcb00:	680c      	ldr	r4, [r1, #0]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
   dcb02:	2c01      	cmp	r4, #1
   dcb04:	d00c      	beq.n	dcb20 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x44>
   dcb06:	4b1a      	ldr	r3, [pc, #104]	; (dcb70 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x94>)
   dcb08:	9301      	str	r3, [sp, #4]
   dcb0a:	4b1b      	ldr	r3, [pc, #108]	; (dcb78 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x9c>)
   dcb0c:	9300      	str	r3, [sp, #0]
   dcb0e:	9603      	str	r6, [sp, #12]
   dcb10:	9402      	str	r4, [sp, #8]
   dcb12:	6944      	ldr	r4, [r0, #20]
   dcb14:	4a19      	ldr	r2, [pc, #100]	; (dcb7c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa0>)
   dcb16:	491a      	ldr	r1, [pc, #104]	; (dcb80 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa4>)
   dcb18:	2328      	movs	r3, #40	; 0x28
   dcb1a:	47a0      	blx	r4
   dcb1c:	4630      	mov	r0, r6
   dcb1e:	e023      	b.n	dcb68 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x8c>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dcb20:	685e      	ldr	r6, [r3, #4]
  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, output->type);
   dcb22:	6849      	ldr	r1, [r1, #4]
   dcb24:	6887      	ldr	r7, [r0, #8]
   dcb26:	2238      	movs	r2, #56	; 0x38
   dcb28:	4356      	muls	r6, r2
   dcb2a:	434a      	muls	r2, r1
   dcb2c:	5dbb      	ldrb	r3, [r7, r6]
   dcb2e:	5cba      	ldrb	r2, [r7, r2]
   dcb30:	4293      	cmp	r3, r2
   dcb32:	d00b      	beq.n	dcb4c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x70>
   dcb34:	9302      	str	r3, [sp, #8]
   dcb36:	4b13      	ldr	r3, [pc, #76]	; (dcb84 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa8>)
   dcb38:	9301      	str	r3, [sp, #4]
   dcb3a:	4b13      	ldr	r3, [pc, #76]	; (dcb88 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xac>)
   dcb3c:	9300      	str	r3, [sp, #0]
   dcb3e:	9203      	str	r2, [sp, #12]
   dcb40:	6945      	ldr	r5, [r0, #20]
   dcb42:	232b      	movs	r3, #43	; 0x2b
   dcb44:	4a0d      	ldr	r2, [pc, #52]	; (dcb7c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa0>)
   dcb46:	490e      	ldr	r1, [pc, #56]	; (dcb80 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa4>)
   dcb48:	47a8      	blx	r5
   dcb4a:	e00a      	b.n	dcb62 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x86>
  if (!IsSupportedType(input->type)) {
   dcb4c:	b95b      	cbnz	r3, dcb66 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x8a>
    context->ReportError(context, "Input data type %s (%d) is not supported.",
   dcb4e:	f8d0 8014 	ldr.w	r8, [r0, #20]
   dcb52:	4618      	mov	r0, r3
   dcb54:	f7f7 fae2 	bl	d411c <TfLiteTypeGetName>
   dcb58:	5dbb      	ldrb	r3, [r7, r6]
   dcb5a:	490c      	ldr	r1, [pc, #48]	; (dcb8c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xb0>)
   dcb5c:	4602      	mov	r2, r0
   dcb5e:	4628      	mov	r0, r5
   dcb60:	47c0      	blx	r8
                         TfLiteTypeGetName(input->type), input->type);
    return kTfLiteError;
   dcb62:	4620      	mov	r0, r4
   dcb64:	e000      	b.n	dcb68 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x8c>
  }
  return kTfLiteOk;
   dcb66:	2000      	movs	r0, #0
}
   dcb68:	b004      	add	sp, #16
   dcb6a:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   dcb6e:	bf00      	nop
   dcb70:	000eb295 	.word	0x000eb295
   dcb74:	000e98e2 	.word	0x000e98e2
   dcb78:	000e98f2 	.word	0x000e98f2
   dcb7c:	000e9cc5 	.word	0x000e9cc5
   dcb80:	000e98c8 	.word	0x000e98c8
   dcb84:	000e990f 	.word	0x000e990f
   dcb88:	000e9903 	.word	0x000e9903
   dcb8c:	000e9d72 	.word	0x000e9d72

000dcb90 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsNumericSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode>:
   dcb90:	f7ff bfa4 	b.w	dcadc <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114GenericPrepareIXadL_ZNS3_22IsLogicalSupportedTypeE10TfLiteTypeEEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode>

000dcb94 <_ZSt3sinf>:
  using ::sin;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  sin(float __x)
  { return __builtin_sinf(__x); }
   dcb94:	f008 bcfc 	b.w	e5590 <sinf>

000dcb98 <_ZSt3cosf>:
  using ::cos;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  cos(float __x)
  { return __builtin_cosf(__x); }
   dcb98:	f008 bbe2 	b.w	e5360 <cosf>

000dcb9c <_ZSt3logf>:
  using ::log;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  log(float __x)
  { return __builtin_logf(__x); }
   dcb9c:	f008 be4a 	b.w	e5834 <logf>

000dcba0 <_ZSt4sqrtf>:
  using ::sqrt;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  sqrt(float __x)
  { return __builtin_sqrtf(__x); }
   dcba0:	f008 bec4 	b.w	e592c <sqrtf>

000dcba4 <_ZZN6tflite3ops5micro11elementwise12_GLOBAL__N_19RsqrtEvalEP13TfLiteContextP10TfLiteNodeENUlfE_4_FUNEf>:
TfLiteStatus SqrtEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalNumeric(context, node, std::sqrt);
}

TfLiteStatus RsqrtEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalNumeric(context, node, [](float f) { return 1.f / std::sqrt(f); });
   dcba4:	b508      	push	{r3, lr}
   dcba6:	f008 fec1 	bl	e592c <sqrtf>
   dcbaa:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   dcbae:	ee87 0a80 	vdiv.f32	s0, s15, s0
   dcbb2:	bd08      	pop	{r3, pc}

000dcbb4 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>:
  }
  return kTfLiteOk;
}

template <typename T>
inline TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node,
   dcbb4:	e92d 4dff 	stmdb	sp!, {r0, r1, r2, r3, r4, r5, r6, r7, r8, sl, fp, lr}
   dcbb8:	4690      	mov	r8, r2
   dcbba:	680a      	ldr	r2, [r1, #0]
   dcbbc:	6883      	ldr	r3, [r0, #8]
   dcbbe:	6854      	ldr	r4, [r2, #4]
   dcbc0:	2238      	movs	r2, #56	; 0x38
   dcbc2:	4362      	muls	r2, r4
   dcbc4:	189c      	adds	r4, r3, r2
                             T func(T), TfLiteType expected_type) {
  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
   dcbc6:	5c9a      	ldrb	r2, [r3, r2]
   dcbc8:	2a01      	cmp	r2, #1
   dcbca:	d00d      	beq.n	dcbe8 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x34>
   dcbcc:	4b20      	ldr	r3, [pc, #128]	; (dcc50 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x9c>)
   dcbce:	9301      	str	r3, [sp, #4]
   dcbd0:	2401      	movs	r4, #1
   dcbd2:	4b20      	ldr	r3, [pc, #128]	; (dcc54 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0xa0>)
   dcbd4:	9202      	str	r2, [sp, #8]
   dcbd6:	9300      	str	r3, [sp, #0]
   dcbd8:	9403      	str	r4, [sp, #12]
   dcbda:	6945      	ldr	r5, [r0, #20]
   dcbdc:	4a1e      	ldr	r2, [pc, #120]	; (dcc58 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0xa4>)
   dcbde:	491f      	ldr	r1, [pc, #124]	; (dcc5c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0xa8>)
   dcbe0:	2339      	movs	r3, #57	; 0x39
   dcbe2:	47a8      	blx	r5
   dcbe4:	4620      	mov	r0, r4
   dcbe6:	e030      	b.n	dcc4a <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x96>
   dcbe8:	68a0      	ldr	r0, [r4, #8]
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
   dcbea:	f8d0 e000 	ldr.w	lr, [r0]
   dcbee:	2200      	movs	r2, #0
inline int NumIntermediates(const TfLiteNode* node) {
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
   dcbf0:	2601      	movs	r6, #1
   dcbf2:	2700      	movs	r7, #0
  for (int i = 0; i < dims->size; ++i) {
   dcbf4:	4596      	cmp	lr, r2
   dcbf6:	dd0c      	ble.n	dcc12 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x5e>
    count *= dims->data[i];
   dcbf8:	f850 cf04 	ldr.w	ip, [r0, #4]!
   dcbfc:	ea4f 7bec 	mov.w	fp, ip, asr #31
   dcc00:	fb06 f50b 	mul.w	r5, r6, fp
   dcc04:	fb0c 5507 	mla	r5, ip, r7, r5
   dcc08:	fba6 670c 	umull	r6, r7, r6, ip
   dcc0c:	442f      	add	r7, r5
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
   dcc0e:	3201      	adds	r2, #1
   dcc10:	e7f0      	b.n	dcbf4 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x40>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dcc12:	684a      	ldr	r2, [r1, #4]
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dcc14:	6865      	ldr	r5, [r4, #4]
   dcc16:	6851      	ldr	r1, [r2, #4]
   dcc18:	2238      	movs	r2, #56	; 0x38
   dcc1a:	fb02 3301 	mla	r3, r2, r1, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dcc1e:	b103      	cbz	r3, dcc22 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x6e>
   dcc20:	685b      	ldr	r3, [r3, #4]
   dcc22:	461c      	mov	r4, r3
  const int64_t num_elements = NumElements(input);
  const T* in_data = GetTensorData<T>(input);
  T* out_data = GetTensorData<T>(output);
  for (int64_t i = 0; i < num_elements; ++i) {
   dcc24:	f04f 0a00 	mov.w	sl, #0
   dcc28:	f04f 0b00 	mov.w	fp, #0
   dcc2c:	45b2      	cmp	sl, r6
   dcc2e:	eb7b 0307 	sbcs.w	r3, fp, r7
   dcc32:	da09      	bge.n	dcc48 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x94>
    out_data[i] = func(in_data[i]);
   dcc34:	ecb5 0a01 	vldmia	r5!, {s0}
   dcc38:	47c0      	blx	r8
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
  const int64_t num_elements = NumElements(input);
  const T* in_data = GetTensorData<T>(input);
  T* out_data = GetTensorData<T>(output);
  for (int64_t i = 0; i < num_elements; ++i) {
   dcc3a:	f11a 0a01 	adds.w	sl, sl, #1
    out_data[i] = func(in_data[i]);
   dcc3e:	eca4 0a01 	vstmia	r4!, {s0}
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
  const int64_t num_elements = NumElements(input);
  const T* in_data = GetTensorData<T>(input);
  T* out_data = GetTensorData<T>(output);
  for (int64_t i = 0; i < num_elements; ++i) {
   dcc42:	f14b 0b00 	adc.w	fp, fp, #0
   dcc46:	e7f1      	b.n	dcc2c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13+0x78>
    out_data[i] = func(in_data[i]);
  }
  return kTfLiteOk;
   dcc48:	2000      	movs	r0, #0
}
   dcc4a:	b004      	add	sp, #16
   dcc4c:	e8bd 8df0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, sl, fp, pc}
   dcc50:	000e9d9c 	.word	0x000e9d9c
   dcc54:	000e9903 	.word	0x000e9903
   dcc58:	000e9cc5 	.word	0x000e9cc5
   dcc5c:	000e98c8 	.word	0x000e98c8

000dcc60 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_110SquareEvalEP13TfLiteContextP10TfLiteNode>:

inline TfLiteStatus EvalNumeric(TfLiteContext* context, TfLiteNode* node,
                                float float_func(float)) {
  return EvalImpl<float>(context, node, float_func, kTfLiteFloat32);
   dcc60:	4a01      	ldr	r2, [pc, #4]	; (dcc68 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_110SquareEvalEP13TfLiteContextP10TfLiteNode+0x8>)
   dcc62:	f7ff bfa7 	b.w	dcbb4 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
   dcc66:	bf00      	nop
   dcc68:	000dcad7 	.word	0x000dcad7

000dcc6c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17AbsEvalEP13TfLiteContextP10TfLiteNode>:
   dcc6c:	4a01      	ldr	r2, [pc, #4]	; (dcc74 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17AbsEvalEP13TfLiteContextP10TfLiteNode+0x8>)
   dcc6e:	f7ff bfa1 	b.w	dcbb4 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
   dcc72:	bf00      	nop
   dcc74:	000dcad1 	.word	0x000dcad1

000dcc78 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17SinEvalEP13TfLiteContextP10TfLiteNode>:
   dcc78:	4a01      	ldr	r2, [pc, #4]	; (dcc80 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17SinEvalEP13TfLiteContextP10TfLiteNode+0x8>)
   dcc7a:	f7ff bf9b 	b.w	dcbb4 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
   dcc7e:	bf00      	nop
   dcc80:	000dcb95 	.word	0x000dcb95

000dcc84 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17CosEvalEP13TfLiteContextP10TfLiteNode>:
   dcc84:	4a01      	ldr	r2, [pc, #4]	; (dcc8c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17CosEvalEP13TfLiteContextP10TfLiteNode+0x8>)
   dcc86:	f7ff bf95 	b.w	dcbb4 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
   dcc8a:	bf00      	nop
   dcc8c:	000dcb99 	.word	0x000dcb99

000dcc90 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_19RsqrtEvalEP13TfLiteContextP10TfLiteNode>:
   dcc90:	4a01      	ldr	r2, [pc, #4]	; (dcc98 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_19RsqrtEvalEP13TfLiteContextP10TfLiteNode+0x8>)
   dcc92:	f7ff bf8f 	b.w	dcbb4 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
   dcc96:	bf00      	nop
   dcc98:	000dcba5 	.word	0x000dcba5

000dcc9c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17LogEvalEP13TfLiteContextP10TfLiteNode>:
   dcc9c:	4a01      	ldr	r2, [pc, #4]	; (dcca4 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_17LogEvalEP13TfLiteContextP10TfLiteNode+0x8>)
   dcc9e:	f7ff bf89 	b.w	dcbb4 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
   dcca2:	bf00      	nop
   dcca4:	000dcb9d 	.word	0x000dcb9d

000dcca8 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18SqrtEvalEP13TfLiteContextP10TfLiteNode>:
   dcca8:	4a01      	ldr	r2, [pc, #4]	; (dccb0 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18SqrtEvalEP13TfLiteContextP10TfLiteNode+0x8>)
   dccaa:	f7ff bf83 	b.w	dcbb4 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_18EvalImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePFT_SA_E10TfLiteType.constprop.13>
   dccae:	bf00      	nop
   dccb0:	000dcba1 	.word	0x000dcba1

000dccb4 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus SquareEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalNumeric(context, node, [](float f) { return f * f; });
}

TfLiteStatus LogicalNotEval(TfLiteContext* context, TfLiteNode* node) {
   dccb4:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dccb8:	680a      	ldr	r2, [r1, #0]
   dccba:	6883      	ldr	r3, [r0, #8]
   dccbc:	460f      	mov	r7, r1
   dccbe:	6851      	ldr	r1, [r2, #4]
   dccc0:	2238      	movs	r2, #56	; 0x38
   dccc2:	434a      	muls	r2, r1
   dccc4:	189e      	adds	r6, r3, r2
template <typename T>
inline TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node,
                             T func(T), TfLiteType expected_type) {
  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
   dccc6:	5c9a      	ldrb	r2, [r3, r2]
   dccc8:	2a06      	cmp	r2, #6

TfLiteStatus SquareEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalNumeric(context, node, [](float f) { return f * f; });
}

TfLiteStatus LogicalNotEval(TfLiteContext* context, TfLiteNode* node) {
   dccca:	b085      	sub	sp, #20
template <typename T>
inline TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node,
                             T func(T), TfLiteType expected_type) {
  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
   dcccc:	d00d      	beq.n	dccea <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x36>
   dccce:	2306      	movs	r3, #6
   dccd0:	9303      	str	r3, [sp, #12]
   dccd2:	4b1f      	ldr	r3, [pc, #124]	; (dcd50 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x9c>)
   dccd4:	9301      	str	r3, [sp, #4]
   dccd6:	4b1f      	ldr	r3, [pc, #124]	; (dcd54 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0xa0>)
   dccd8:	9202      	str	r2, [sp, #8]
   dccda:	9300      	str	r3, [sp, #0]
   dccdc:	6944      	ldr	r4, [r0, #20]
   dccde:	4a1e      	ldr	r2, [pc, #120]	; (dcd58 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0xa4>)
   dcce0:	491e      	ldr	r1, [pc, #120]	; (dcd5c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0xa8>)
   dcce2:	2339      	movs	r3, #57	; 0x39
   dcce4:	47a0      	blx	r4
   dcce6:	2001      	movs	r0, #1
   dcce8:	e02f      	b.n	dcd4a <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x96>
   dccea:	68b2      	ldr	r2, [r6, #8]
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
   dccec:	f8d2 c000 	ldr.w	ip, [r2]
   dccf0:	2400      	movs	r4, #0
inline int NumIntermediates(const TfLiteNode* node) {
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
   dccf2:	2001      	movs	r0, #1
   dccf4:	2100      	movs	r1, #0
  for (int i = 0; i < dims->size; ++i) {
   dccf6:	45a4      	cmp	ip, r4
   dccf8:	dd0c      	ble.n	dcd14 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x60>
    count *= dims->data[i];
   dccfa:	f852 ef04 	ldr.w	lr, [r2, #4]!
   dccfe:	ea4f 79ee 	mov.w	r9, lr, asr #31
   dcd02:	fb00 f509 	mul.w	r5, r0, r9
   dcd06:	fb0e 5501 	mla	r5, lr, r1, r5
   dcd0a:	fba0 010e 	umull	r0, r1, r0, lr
   dcd0e:	4429      	add	r1, r5
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
   dcd10:	3401      	adds	r4, #1
   dcd12:	e7f0      	b.n	dccf6 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x42>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dcd14:	687a      	ldr	r2, [r7, #4]
   dcd16:	6852      	ldr	r2, [r2, #4]
   dcd18:	2438      	movs	r4, #56	; 0x38
   dcd1a:	fb04 3302 	mla	r3, r4, r2, r3
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dcd1e:	6872      	ldr	r2, [r6, #4]

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dcd20:	b103      	cbz	r3, dcd24 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x70>
   dcd22:	685b      	ldr	r3, [r3, #4]
   dcd24:	3a01      	subs	r2, #1
   dcd26:	3b01      	subs	r3, #1
  const int64_t num_elements = NumElements(input);
  const T* in_data = GetTensorData<T>(input);
  T* out_data = GetTensorData<T>(output);
  for (int64_t i = 0; i < num_elements; ++i) {
   dcd28:	2400      	movs	r4, #0
   dcd2a:	2500      	movs	r5, #0
   dcd2c:	4284      	cmp	r4, r0
   dcd2e:	eb75 0601 	sbcs.w	r6, r5, r1
   dcd32:	da09      	bge.n	dcd48 <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x94>
    out_data[i] = func(in_data[i]);
   dcd34:	f812 6f01 	ldrb.w	r6, [r2, #1]!
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
  const int64_t num_elements = NumElements(input);
  const T* in_data = GetTensorData<T>(input);
  T* out_data = GetTensorData<T>(output);
  for (int64_t i = 0; i < num_elements; ++i) {
   dcd38:	3401      	adds	r4, #1
    out_data[i] = func(in_data[i]);
   dcd3a:	f086 0601 	eor.w	r6, r6, #1
   dcd3e:	f803 6f01 	strb.w	r6, [r3, #1]!
  TfLiteTensor* output = GetOutput(context, node, 0);
  TF_LITE_ENSURE_EQ(context, input->type, expected_type);
  const int64_t num_elements = NumElements(input);
  const T* in_data = GetTensorData<T>(input);
  T* out_data = GetTensorData<T>(output);
  for (int64_t i = 0; i < num_elements; ++i) {
   dcd42:	f145 0500 	adc.w	r5, r5, #0
   dcd46:	e7f1      	b.n	dcd2c <_ZN6tflite3ops5micro11elementwise12_GLOBAL__N_114LogicalNotEvalEP13TfLiteContextP10TfLiteNode+0x78>
    out_data[i] = func(in_data[i]);
  }
  return kTfLiteOk;
   dcd48:	2000      	movs	r0, #0
  return EvalNumeric(context, node, [](float f) { return f * f; });
}

TfLiteStatus LogicalNotEval(TfLiteContext* context, TfLiteNode* node) {
  return EvalLogical(context, node, [](bool v) { return !v; });
}
   dcd4a:	b005      	add	sp, #20
   dcd4c:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}
   dcd50:	000e9d9c 	.word	0x000e9d9c
   dcd54:	000e9903 	.word	0x000e9903
   dcd58:	000e9cc5 	.word	0x000e9cc5
   dcd5c:	000e98c8 	.word	0x000e98c8

000dcd60 <_ZN6tflite3ops5micro12Register_ABSEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::AbsEval};
  return &r;
}
   dcd60:	4800      	ldr	r0, [pc, #0]	; (dcd64 <_ZN6tflite3ops5micro12Register_ABSEv+0x4>)
   dcd62:	4770      	bx	lr
   dcd64:	2003c0dc 	.word	0x2003c0dc

000dcd68 <_ZN6tflite3ops5micro12Register_SINEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::SinEval};
  return &r;
}
   dcd68:	4800      	ldr	r0, [pc, #0]	; (dcd6c <_ZN6tflite3ops5micro12Register_SINEv+0x4>)
   dcd6a:	4770      	bx	lr
   dcd6c:	2003c0bc 	.word	0x2003c0bc

000dcd70 <_ZN6tflite3ops5micro12Register_COSEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::CosEval};
  return &r;
}
   dcd70:	4800      	ldr	r0, [pc, #0]	; (dcd74 <_ZN6tflite3ops5micro12Register_COSEv+0x4>)
   dcd72:	4770      	bx	lr
   dcd74:	2003c03c 	.word	0x2003c03c

000dcd78 <_ZN6tflite3ops5micro12Register_LOGEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::LogEval};
  return &r;
}
   dcd78:	4800      	ldr	r0, [pc, #0]	; (dcd7c <_ZN6tflite3ops5micro12Register_LOGEv+0x4>)
   dcd7a:	4770      	bx	lr
   dcd7c:	2003bffc 	.word	0x2003bffc

000dcd80 <_ZN6tflite3ops5micro13Register_SQRTEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::SqrtEval};
  return &r;
}
   dcd80:	4800      	ldr	r0, [pc, #0]	; (dcd84 <_ZN6tflite3ops5micro13Register_SQRTEv+0x4>)
   dcd82:	4770      	bx	lr
   dcd84:	2003c09c 	.word	0x2003c09c

000dcd88 <_ZN6tflite3ops5micro14Register_RSQRTEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::RsqrtEval};
  return &r;
}
   dcd88:	4800      	ldr	r0, [pc, #0]	; (dcd8c <_ZN6tflite3ops5micro14Register_RSQRTEv+0x4>)
   dcd8a:	4770      	bx	lr
   dcd8c:	2003c07c 	.word	0x2003c07c

000dcd90 <_ZN6tflite3ops5micro15Register_SQUAREEv>:
  static TfLiteRegistration r = {
      /* init */ nullptr, /* free */ nullptr,
      elementwise::GenericPrepare<elementwise::IsNumericSupportedType>,
      elementwise::SquareEval};
  return &r;
}
   dcd90:	4800      	ldr	r0, [pc, #0]	; (dcd94 <_ZN6tflite3ops5micro15Register_SQUAREEv+0x4>)
   dcd92:	4770      	bx	lr
   dcd94:	2003c01c 	.word	0x2003c01c

000dcd98 <_ZN6tflite3ops5micro20Register_LOGICAL_NOTEv>:
  static TfLiteRegistration r = {
      /*init=*/nullptr, /*free=*/nullptr,
      elementwise::GenericPrepare<elementwise::IsLogicalSupportedType>,
      elementwise::LogicalNotEval};
  return &r;
}
   dcd98:	4800      	ldr	r0, [pc, #0]	; (dcd9c <_ZN6tflite3ops5micro20Register_LOGICAL_NOTEv+0x4>)
   dcd9a:	4770      	bx	lr
   dcd9c:	2003c05c 	.word	0x2003c05c

000dcda0 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode>:
namespace floor {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dcda0:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dcda4:	680b      	ldr	r3, [r1, #0]
   dcda6:	6882      	ldr	r2, [r0, #8]
   dcda8:	685b      	ldr	r3, [r3, #4]
   dcdaa:	2438      	movs	r4, #56	; 0x38
   dcdac:	4363      	muls	r3, r4
   dcdae:	18d5      	adds	r5, r2, r3
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
   dcdb0:	5cd3      	ldrb	r3, [r2, r3]
   dcdb2:	2b01      	cmp	r3, #1
namespace floor {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dcdb4:	b08e      	sub	sp, #56	; 0x38
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
   dcdb6:	d00d      	beq.n	dcdd4 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x34>
   dcdb8:	9302      	str	r3, [sp, #8]
   dcdba:	4b2b      	ldr	r3, [pc, #172]	; (dce68 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xc8>)
   dcdbc:	9301      	str	r3, [sp, #4]
   dcdbe:	2401      	movs	r4, #1
   dcdc0:	4b2a      	ldr	r3, [pc, #168]	; (dce6c <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xcc>)
   dcdc2:	9300      	str	r3, [sp, #0]
   dcdc4:	9403      	str	r4, [sp, #12]
   dcdc6:	6945      	ldr	r5, [r0, #20]
   dcdc8:	4a29      	ldr	r2, [pc, #164]	; (dce70 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xd0>)
   dcdca:	492a      	ldr	r1, [pc, #168]	; (dce74 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xd4>)
   dcdcc:	231f      	movs	r3, #31
   dcdce:	47a8      	blx	r5
   dcdd0:	4620      	mov	r0, r4
   dcdd2:	e046      	b.n	dce62 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xc2>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dcdd4:	684b      	ldr	r3, [r1, #4]
   dcdd6:	685b      	ldr	r3, [r3, #4]
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  reference_ops::Floor(GetTensorShape(input), GetTensorData<float>(input),
   dcdd8:	4629      	mov	r1, r5
   dcdda:	fb04 2403 	mla	r4, r4, r3, r2
   dcdde:	a804      	add	r0, sp, #16
   dcde0:	f7f9 fde7 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                       GetTensorShape(output), GetTensorData<float>(output));
   dcde4:	4621      	mov	r1, r4
   dcde6:	a809      	add	r0, sp, #36	; 0x24
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dcde8:	686e      	ldr	r6, [r5, #4]
   dcdea:	f7f9 fde2 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dcdee:	b104      	cbz	r4, dcdf2 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x52>
   dcdf0:	6864      	ldr	r4, [r4, #4]
   dcdf2:	9f04      	ldr	r7, [sp, #16]

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   dcdf4:	9b09      	ldr	r3, [sp, #36]	; 0x24
   dcdf6:	429f      	cmp	r7, r3
   dcdf8:	d101      	bne.n	dcdfe <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x5e>
   dcdfa:	2500      	movs	r5, #0
   dcdfc:	e00d      	b.n	dce1a <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x7a>
   dcdfe:	f007 faa5 	bl	e434c <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   dce02:	4629      	mov	r1, r5
   dce04:	a804      	add	r0, sp, #16
   dce06:	f7f9 fb2f 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dce0a:	4629      	mov	r1, r5
   dce0c:	4680      	mov	r8, r0
   dce0e:	a809      	add	r0, sp, #36	; 0x24
   dce10:	f7f9 fb2a 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dce14:	4580      	cmp	r8, r0
   dce16:	d1f2      	bne.n	dcdfe <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x5e>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   dce18:	3501      	adds	r5, #1
   dce1a:	42af      	cmp	r7, r5
   dce1c:	dcf1      	bgt.n	dce02 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x62>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   dce1e:	2f04      	cmp	r7, #4
   dce20:	bfcc      	ite	gt
   dce22:	9a05      	ldrgt	r2, [sp, #20]
   dce24:	aa05      	addle	r2, sp, #20
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dce26:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
   dce28:	f04f 0801 	mov.w	r8, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dce2c:	429f      	cmp	r7, r3
   dce2e:	dd05      	ble.n	dce3c <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x9c>
      buffer_size *= dims_data[i];
   dce30:	f852 1023 	ldr.w	r1, [r2, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dce34:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
   dce36:	fb01 f808 	mul.w	r8, r1, r8
   dce3a:	e7f7      	b.n	dce2c <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0x8c>
   dce3c:	4635      	mov	r5, r6
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dce3e:	2600      	movs	r6, #0

inline void Floor(const RuntimeShape& input_shape, const float* input_data,
                  const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; i++) {
   dce40:	4546      	cmp	r6, r8
   dce42:	da07      	bge.n	dce54 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xb4>
  using ::floor;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  floor(float __x)
  { return __builtin_floorf(__x); }
   dce44:	ecb5 0a01 	vldmia	r5!, {s0}
   dce48:	f008 fad2 	bl	e53f0 <floorf>
   dce4c:	3601      	adds	r6, #1
    int offset = i;
    output_data[offset] = std::floor(input_data[offset]);
   dce4e:	eca4 0a01 	vstmia	r4!, {s0}
   dce52:	e7f5      	b.n	dce40 <_ZN6tflite3ops5micro5floor4EvalEP13TfLiteContextP10TfLiteNode+0xa0>
   dce54:	a809      	add	r0, sp, #36	; 0x24
   dce56:	f7f9 fafc 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  reference_ops::Floor(GetTensorShape(input), GetTensorData<float>(input),
   dce5a:	a804      	add	r0, sp, #16
   dce5c:	f7f9 faf9 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                       GetTensorShape(output), GetTensorData<float>(output));
  return kTfLiteOk;
   dce60:	2000      	movs	r0, #0
}
   dce62:	b00e      	add	sp, #56	; 0x38
   dce64:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   dce68:	000ea137 	.word	0x000ea137
   dce6c:	000e9903 	.word	0x000e9903
   dce70:	000e9daa 	.word	0x000e9daa
   dce74:	000e98c8 	.word	0x000e98c8

000dce78 <_ZN6tflite3ops5micro14Register_FLOOREv>:
TfLiteRegistration* Register_FLOOR() {
  static TfLiteRegistration r = {/*init=*/nullptr,
                                 /*free=*/nullptr, /*prepare=*/nullptr,
                                 floor::Eval};
  return &r;
}
   dce78:	4800      	ldr	r0, [pc, #0]	; (dce7c <_ZN6tflite3ops5micro14Register_FLOOREv+0x4>)
   dce7a:	4770      	bx	lr
   dce7c:	2003c0fc 	.word	0x2003c0fc

000dce80 <_ZN6tflite3ops5micro15fully_connected4InitEP13TfLiteContextPKcj>:

}  // namespace

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
   dce80:	2000      	movs	r0, #0
   dce82:	4770      	bx	lr

000dce84 <_ZN6tflite3ops5micro15fully_connected4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
   dce84:	4770      	bx	lr

000dce86 <_ZN6tflite3ops5micro15fully_connected7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   dce86:	2000      	movs	r0, #0
   dce88:	4770      	bx	lr

000dce8a <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi>:
}

// Data is required to be contiguous, and so many operators can use either the
// full array flat size or the flat size with one dimension skipped (commonly
// the depth).
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
   dce8a:	b538      	push	{r3, r4, r5, lr}
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
   dce8c:	2900      	cmp	r1, #0
   dce8e:	6804      	ldr	r4, [r0, #0]
   dce90:	db01      	blt.n	dce96 <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi+0xc>
   dce92:	42a1      	cmp	r1, r4
   dce94:	db01      	blt.n	dce9a <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi+0x10>
   dce96:	f007 fa59 	bl	e434c <abort>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   dce9a:	2c04      	cmp	r4, #4
   dce9c:	bfcc      	ite	gt
   dce9e:	6843      	ldrgt	r3, [r0, #4]
   dcea0:	1d03      	addle	r3, r0, #4
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
  const auto* dims_data = shape.DimsData();
  int flat_size = 1;
  for (int i = 0; i < dims_count; ++i) {
   dcea2:	2200      	movs	r2, #0
// the depth).
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
  const auto* dims_data = shape.DimsData();
  int flat_size = 1;
   dcea4:	2001      	movs	r0, #1
  for (int i = 0; i < dims_count; ++i) {
   dcea6:	42a2      	cmp	r2, r4
   dcea8:	da07      	bge.n	dceba <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi+0x30>
    flat_size *= (i == skip_dim) ? 1 : dims_data[i];
   dceaa:	428a      	cmp	r2, r1
   dceac:	bf14      	ite	ne
   dceae:	f853 5022 	ldrne.w	r5, [r3, r2, lsl #2]
   dceb2:	2501      	moveq	r5, #1
   dceb4:	4368      	muls	r0, r5
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
  const auto* dims_data = shape.DimsData();
  int flat_size = 1;
  for (int i = 0; i < dims_count; ++i) {
   dceb6:	3201      	adds	r2, #1
   dceb8:	e7f5      	b.n	dcea6 <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi+0x1c>
    flat_size *= (i == skip_dim) ? 1 : dims_data[i];
  }
  return flat_size;
}
   dceba:	bd38      	pop	{r3, r4, r5, pc}

000dcebc <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph>:
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    uint8* output_data) {
   dcebc:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dcec0:	b08b      	sub	sp, #44	; 0x2c
   dcec2:	461d      	mov	r5, r3
  const int32 input_offset = params.input_offset;
   dcec4:	6803      	ldr	r3, [r0, #0]
   dcec6:	9302      	str	r3, [sp, #8]
  const int32 filter_offset = params.weights_offset;
   dcec8:	6843      	ldr	r3, [r0, #4]
   dceca:	9303      	str	r3, [sp, #12]
  const int32 output_offset = params.output_offset;
   dcecc:	6883      	ldr	r3, [r0, #8]
   dcece:	9304      	str	r3, [sp, #16]
   dced0:	682e      	ldr	r6, [r5, #0]
  const int32 output_multiplier = params.output_multiplier;
   dced2:	68c3      	ldr	r3, [r0, #12]
   dced4:	9305      	str	r3, [sp, #20]
  const int output_shift = params.output_shift;
   dced6:	6903      	ldr	r3, [r0, #16]
   dced8:	9306      	str	r3, [sp, #24]
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;
  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);
   dceda:	2e01      	cmp	r6, #1
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
   dcedc:	6943      	ldr	r3, [r0, #20]
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    uint8* output_data) {
   dcede:	9f17      	ldr	r7, [sp, #92]	; 0x5c
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
   dcee0:	9301      	str	r3, [sp, #4]
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    uint8* output_data) {
   dcee2:	4614      	mov	r4, r2
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;
   dcee4:	f8d0 b018 	ldr.w	fp, [r0, #24]
  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);
   dcee8:	dc01      	bgt.n	dceee <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0x32>
   dceea:	f007 fa2f 	bl	e434c <abort>
   dceee:	683b      	ldr	r3, [r7, #0]
  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);
   dcef0:	2b00      	cmp	r3, #0
   dcef2:	ddfa      	ble.n	dceea <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0x2e>

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   dcef4:	9a01      	ldr	r2, [sp, #4]
   dcef6:	455a      	cmp	r2, fp
   dcef8:	dcf7      	bgt.n	dceea <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0x2e>
  // but the current --variable_batch hack consists in overwriting the 3rd
  // dimension with the runtime batch size, as we don't keep track for each
  // array of which dimension is the batch dimension in it.
  const int output_dim_count = output_shape.DimensionsCount();
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
   dcefa:	f103 38ff 	add.w	r8, r3, #4294967295	; 0xffffffff
   dcefe:	4641      	mov	r1, r8
   dcf00:	4638      	mov	r0, r7
   dcf02:	f7ff ffc2 	bl	dce8a <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi>
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
   dcf06:	4643      	mov	r3, r8
   dcf08:	463a      	mov	r2, r7
   dcf0a:	1eb1      	subs	r1, r6, #2
  // but the current --variable_batch hack consists in overwriting the 3rd
  // dimension with the runtime batch size, as we don't keep track for each
  // array of which dimension is the batch dimension in it.
  const int output_dim_count = output_shape.DimensionsCount();
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
   dcf0c:	9007      	str	r0, [sp, #28]
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
   dcf0e:	4628      	mov	r0, r5
   dcf10:	f7fe fe5b 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
   dcf14:	1e71      	subs	r1, r6, #1
  // array of which dimension is the batch dimension in it.
  const int output_dim_count = output_shape.DimensionsCount();
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
   dcf16:	4682      	mov	sl, r0
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
   dcf18:	4628      	mov	r0, r5
   dcf1a:	f7f9 faa5 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dcf1e:	f8dd 8060 	ldr.w	r8, [sp, #96]	; 0x60
   dcf22:	4606      	mov	r6, r0
   dcf24:	4267      	negs	r7, r4
  for (int b = 0; b < batches; ++b) {
   dcf26:	f04f 0900 	mov.w	r9, #0
   dcf2a:	9b07      	ldr	r3, [sp, #28]
   dcf2c:	4599      	cmp	r9, r3
   dcf2e:	da39      	bge.n	dcfa4 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0xe8>
   dcf30:	9b14      	ldr	r3, [sp, #80]	; 0x50
   dcf32:	2500      	movs	r5, #0
    for (int out_c = 0; out_c < output_depth; ++out_c) {
   dcf34:	4555      	cmp	r5, sl
   dcf36:	da2f      	bge.n	dcf98 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0xdc>
   dcf38:	469c      	mov	ip, r3
   dcf3a:	46a6      	mov	lr, r4
      int32 acc = 0;
   dcf3c:	2000      	movs	r0, #0
      for (int d = 0; d < accum_depth; ++d) {
   dcf3e:	eb0e 0207 	add.w	r2, lr, r7
   dcf42:	4296      	cmp	r6, r2
   dcf44:	dd0f      	ble.n	dcf66 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0xaa>
        int32 input_val = input_data[b * accum_depth + d];
   dcf46:	f81e 2b01 	ldrb.w	r2, [lr], #1
   dcf4a:	9208      	str	r2, [sp, #32]
        int32 filter_val = filter_data[out_c * accum_depth + d];
        acc += (filter_val + filter_offset) * (input_val + input_offset);
   dcf4c:	9903      	ldr	r1, [sp, #12]
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
      int32 acc = 0;
      for (int d = 0; d < accum_depth; ++d) {
        int32 input_val = input_data[b * accum_depth + d];
        int32 filter_val = filter_data[out_c * accum_depth + d];
   dcf4e:	f81c 2b01 	ldrb.w	r2, [ip], #1
        acc += (filter_val + filter_offset) * (input_val + input_offset);
   dcf52:	440a      	add	r2, r1
   dcf54:	9209      	str	r2, [sp, #36]	; 0x24
   dcf56:	9902      	ldr	r1, [sp, #8]
   dcf58:	9a08      	ldr	r2, [sp, #32]
   dcf5a:	440a      	add	r2, r1
   dcf5c:	4611      	mov	r1, r2
   dcf5e:	9a09      	ldr	r2, [sp, #36]	; 0x24
   dcf60:	fb01 0002 	mla	r0, r1, r2, r0
                                       output_shape, output_dim_count - 1);
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
      int32 acc = 0;
      for (int d = 0; d < accum_depth; ++d) {
   dcf64:	e7eb      	b.n	dcf3e <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0x82>
        int32 input_val = input_data[b * accum_depth + d];
        int32 filter_val = filter_data[out_c * accum_depth + d];
        acc += (filter_val + filter_offset) * (input_val + input_offset);
      }
      if (bias_data) {
   dcf66:	9a16      	ldr	r2, [sp, #88]	; 0x58
   dcf68:	b112      	cbz	r2, dcf70 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0xb4>
        acc += bias_data[out_c];
   dcf6a:	f852 2025 	ldr.w	r2, [r2, r5, lsl #2]
   dcf6e:	4410      	add	r0, r2
      }
      acc = MultiplyByQuantizedMultiplier(acc, output_multiplier, output_shift);
   dcf70:	9a06      	ldr	r2, [sp, #24]
   dcf72:	9905      	ldr	r1, [sp, #20]
   dcf74:	9308      	str	r3, [sp, #32]
   dcf76:	f7fe fe37 	bl	dbbe8 <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
      acc += output_offset;
   dcf7a:	9b04      	ldr	r3, [sp, #16]
   dcf7c:	4418      	add	r0, r3
   dcf7e:	9b01      	ldr	r3, [sp, #4]
   dcf80:	4298      	cmp	r0, r3
   dcf82:	bfb8      	it	lt
   dcf84:	4618      	movlt	r0, r3
      acc = std::max(acc, output_activation_min);
      acc = std::min(acc, output_activation_max);
      output_data[out_c + output_depth * b] = static_cast<uint8>(acc);
   dcf86:	4558      	cmp	r0, fp
   dcf88:	9b08      	ldr	r3, [sp, #32]
   dcf8a:	bfa8      	it	ge
   dcf8c:	4658      	movge	r0, fp
   dcf8e:	f808 0005 	strb.w	r0, [r8, r5]
   dcf92:	4433      	add	r3, r6
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
   dcf94:	3501      	adds	r5, #1
   dcf96:	e7cd      	b.n	dcf34 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0x78>
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
   dcf98:	f109 0901 	add.w	r9, r9, #1
   dcf9c:	44d0      	add	r8, sl
   dcf9e:	4434      	add	r4, r6
   dcfa0:	1bbf      	subs	r7, r7, r6
   dcfa2:	e7c2      	b.n	dcf2a <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph+0x6e>
      acc = std::max(acc, output_activation_min);
      acc = std::min(acc, output_activation_max);
      output_data[out_c + output_depth * b] = static_cast<uint8>(acc);
    }
  }
}
   dcfa4:	b00b      	add	sp, #44	; 0x2c
   dcfa6:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dcfaa <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps>:
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int16* output_data) {
   dcfaa:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dcfae:	b089      	sub	sp, #36	; 0x24
   dcfb0:	461e      	mov	r6, r3
  const int32 input_offset = params.input_offset;
   dcfb2:	6803      	ldr	r3, [r0, #0]
   dcfb4:	9301      	str	r3, [sp, #4]
  const int32 filter_offset = params.weights_offset;
   dcfb6:	6843      	ldr	r3, [r0, #4]
   dcfb8:	9302      	str	r3, [sp, #8]
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
   dcfba:	68c3      	ldr	r3, [r0, #12]
   dcfbc:	9303      	str	r3, [sp, #12]
  const int output_shift = params.output_shift;
   dcfbe:	6903      	ldr	r3, [r0, #16]
   dcfc0:	9304      	str	r3, [sp, #16]
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;
   dcfc2:	f8d0 a018 	ldr.w	sl, [r0, #24]
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
   dcfc6:	6943      	ldr	r3, [r0, #20]
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int16* output_data) {
   dcfc8:	f8dd 8054 	ldr.w	r8, [sp, #84]	; 0x54
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
   dcfcc:	6885      	ldr	r5, [r0, #8]
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
   dcfce:	9300      	str	r3, [sp, #0]
  const int32 output_activation_max = params.quantized_activation_max;

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   dcfd0:	4553      	cmp	r3, sl
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const uint8* input_data, const RuntimeShape& filter_shape,
    const uint8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int16* output_data) {
   dcfd2:	4614      	mov	r4, r2
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   dcfd4:	dd01      	ble.n	dcfda <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0x30>
   dcfd6:	f007 f9b9 	bl	e434c <abort>
  TFLITE_DCHECK_EQ(output_offset, 0);
   dcfda:	2d00      	cmp	r5, #0
   dcfdc:	d1fb      	bne.n	dcfd6 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0x2c>
  // but the current --variable_batch hack consists in overwriting the 3rd
  // dimension with the runtime batch size, as we don't keep track for each
  // array of which dimension is the batch dimension in it.
  const int output_dim_count = output_shape.DimensionsCount();
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
   dcfde:	f8d8 3000 	ldr.w	r3, [r8]
   dcfe2:	6837      	ldr	r7, [r6, #0]
   dcfe4:	f103 39ff 	add.w	r9, r3, #4294967295	; 0xffffffff
   dcfe8:	4649      	mov	r1, r9
   dcfea:	4640      	mov	r0, r8
   dcfec:	f7ff ff4d 	bl	dce8a <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi>
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
   dcff0:	464b      	mov	r3, r9
   dcff2:	4642      	mov	r2, r8
   dcff4:	1eb9      	subs	r1, r7, #2
  // but the current --variable_batch hack consists in overwriting the 3rd
  // dimension with the runtime batch size, as we don't keep track for each
  // array of which dimension is the batch dimension in it.
  const int output_dim_count = output_shape.DimensionsCount();
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
   dcff6:	9005      	str	r0, [sp, #20]
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
   dcff8:	4630      	mov	r0, r6
   dcffa:	f7fe fde6 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
   dcffe:	1e79      	subs	r1, r7, #1
  // array of which dimension is the batch dimension in it.
  const int output_dim_count = output_shape.DimensionsCount();
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
   dd000:	4683      	mov	fp, r0
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
   dd002:	4630      	mov	r0, r6
   dd004:	f7f9 fa30 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dd008:	ea4f 034b 	mov.w	r3, fp, lsl #1
   dd00c:	f8dd 9058 	ldr.w	r9, [sp, #88]	; 0x58
   dd010:	9306      	str	r3, [sp, #24]
   dd012:	4607      	mov	r7, r0
   dd014:	f1c4 0800 	rsb	r8, r4, #0
  for (int b = 0; b < batches; ++b) {
   dd018:	9b05      	ldr	r3, [sp, #20]
   dd01a:	429d      	cmp	r5, r3
   dd01c:	da33      	bge.n	dd086 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0xdc>
   dd01e:	9b12      	ldr	r3, [sp, #72]	; 0x48
   dd020:	2600      	movs	r6, #0
    for (int out_c = 0; out_c < output_depth; ++out_c) {
   dd022:	455e      	cmp	r6, fp
   dd024:	da28      	bge.n	dd078 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0xce>
      // Internal accumulation.
      // Initialize accumulator with the bias-value.
      int32 accum = bias_data[out_c];
   dd026:	9a14      	ldr	r2, [sp, #80]	; 0x50
   dd028:	f852 0026 	ldr.w	r0, [r2, r6, lsl #2]
   dd02c:	469c      	mov	ip, r3
   dd02e:	46a6      	mov	lr, r4
      // Accumulation loop.
      for (int d = 0; d < accum_depth; ++d) {
   dd030:	eb08 020e 	add.w	r2, r8, lr
   dd034:	4297      	cmp	r7, r2
   dd036:	dd0d      	ble.n	dd054 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0xaa>
        int16 input_val = input_data[b * accum_depth + d] + input_offset;
   dd038:	9901      	ldr	r1, [sp, #4]
   dd03a:	f81e 2b01 	ldrb.w	r2, [lr], #1
   dd03e:	440a      	add	r2, r1
   dd040:	9207      	str	r2, [sp, #28]
        int16 filter_val = filter_data[out_c * accum_depth + d] + filter_offset;
   dd042:	f81c 1b01 	ldrb.w	r1, [ip], #1
   dd046:	9a02      	ldr	r2, [sp, #8]
   dd048:	4411      	add	r1, r2
        accum += filter_val * input_val;
   dd04a:	f8bd 201c 	ldrh.w	r2, [sp, #28]
   dd04e:	fb11 0002 	smlabb	r0, r1, r2, r0
    for (int out_c = 0; out_c < output_depth; ++out_c) {
      // Internal accumulation.
      // Initialize accumulator with the bias-value.
      int32 accum = bias_data[out_c];
      // Accumulation loop.
      for (int d = 0; d < accum_depth; ++d) {
   dd052:	e7ed      	b.n	dd030 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0x86>
      // Down-scale the final int32 accumulator to the scale used by our
      // (16-bit, typically 3 integer bits) fixed-point format. The quantized
      // multiplier and shift here have been pre-computed offline
      // (e.g. by toco).
      accum =
          MultiplyByQuantizedMultiplier(accum, output_multiplier, output_shift);
   dd054:	9a04      	ldr	r2, [sp, #16]
   dd056:	9903      	ldr	r1, [sp, #12]
   dd058:	9307      	str	r3, [sp, #28]
   dd05a:	f7fe fdc5 	bl	dbbe8 <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
   dd05e:	9b00      	ldr	r3, [sp, #0]
   dd060:	4298      	cmp	r0, r3
   dd062:	bfb8      	it	lt
   dd064:	4618      	movlt	r0, r3
      // Saturate, cast to int16, and store to output array.
      accum = std::max(accum, output_activation_min - output_offset);
      accum = std::min(accum, output_activation_max - output_offset);
      accum += output_offset;
      output_data[out_c + output_depth * b] = accum;
   dd066:	4550      	cmp	r0, sl
   dd068:	9b07      	ldr	r3, [sp, #28]
   dd06a:	bfa8      	it	ge
   dd06c:	4650      	movge	r0, sl
   dd06e:	f829 0016 	strh.w	r0, [r9, r6, lsl #1]
   dd072:	443b      	add	r3, r7
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
   dd074:	3601      	adds	r6, #1
   dd076:	e7d4      	b.n	dd022 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0x78>
   dd078:	9b06      	ldr	r3, [sp, #24]
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,
                                       output_shape, output_dim_count - 1);
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
   dd07a:	3501      	adds	r5, #1
   dd07c:	4499      	add	r9, r3
   dd07e:	443c      	add	r4, r7
   dd080:	ebc7 0808 	rsb	r8, r7, r8
   dd084:	e7c8      	b.n	dd018 <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps+0x6e>
      accum = std::min(accum, output_activation_max - output_offset);
      accum += output_offset;
      output_data[out_c + output_depth * b] = accum;
    }
  }
}
   dd086:	b009      	add	sp, #36	; 0x24
   dd088:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dd08c <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa>:
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const int8_t* input_data, const RuntimeShape& filter_shape,
    const int8_t* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8_t* output_data) {
   dd08c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dd090:	b08b      	sub	sp, #44	; 0x2c
   dd092:	461d      	mov	r5, r3
  const int32 input_offset = params.input_offset;
   dd094:	6803      	ldr	r3, [r0, #0]
   dd096:	9302      	str	r3, [sp, #8]
  const int32 filter_offset = params.weights_offset;
   dd098:	6843      	ldr	r3, [r0, #4]
   dd09a:	9303      	str	r3, [sp, #12]
  const int32 output_offset = params.output_offset;
   dd09c:	6883      	ldr	r3, [r0, #8]
   dd09e:	9304      	str	r3, [sp, #16]
   dd0a0:	682e      	ldr	r6, [r5, #0]
  const int32 output_multiplier = params.output_multiplier;
   dd0a2:	68c3      	ldr	r3, [r0, #12]
   dd0a4:	9305      	str	r3, [sp, #20]
  const int output_shift = params.output_shift;
   dd0a6:	6903      	ldr	r3, [r0, #16]
   dd0a8:	9306      	str	r3, [sp, #24]
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;
  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);
   dd0aa:	2e01      	cmp	r6, #1
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
   dd0ac:	6943      	ldr	r3, [r0, #20]
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const int8_t* input_data, const RuntimeShape& filter_shape,
    const int8_t* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8_t* output_data) {
   dd0ae:	9f17      	ldr	r7, [sp, #92]	; 0x5c
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
   dd0b0:	9301      	str	r3, [sp, #4]
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const int8_t* input_data, const RuntimeShape& filter_shape,
    const int8_t* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8_t* output_data) {
   dd0b2:	4614      	mov	r4, r2
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;
   dd0b4:	f8d0 b018 	ldr.w	fp, [r0, #24]
  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);
   dd0b8:	dc01      	bgt.n	dd0be <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x32>
   dd0ba:	f007 f947 	bl	e434c <abort>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 2);
   dd0be:	683b      	ldr	r3, [r7, #0]
   dd0c0:	2b02      	cmp	r3, #2
   dd0c2:	d1fa      	bne.n	dd0ba <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x2e>

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   dd0c4:	9b01      	ldr	r3, [sp, #4]
   dd0c6:	455b      	cmp	r3, fp
   dd0c8:	dcf7      	bgt.n	dd0ba <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x2e>
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = output_shape.Dims(0);
   dd0ca:	2100      	movs	r1, #0
   dd0cc:	4638      	mov	r0, r7
   dd0ce:	f7f9 f9cb 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_depth = output_shape.Dims(1);
   dd0d2:	2101      	movs	r1, #1
  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 2);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = output_shape.Dims(0);
   dd0d4:	9007      	str	r0, [sp, #28]
  const int output_depth = output_shape.Dims(1);
   dd0d6:	4638      	mov	r0, r7
   dd0d8:	f7f9 f9c6 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  TFLITE_DCHECK_LE(output_depth, filter_shape.Dims(filter_dim_count - 2));
   dd0dc:	1eb1      	subs	r1, r6, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 2);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = output_shape.Dims(0);
  const int output_depth = output_shape.Dims(1);
   dd0de:	4681      	mov	r9, r0
  TFLITE_DCHECK_LE(output_depth, filter_shape.Dims(filter_dim_count - 2));
   dd0e0:	4628      	mov	r0, r5
   dd0e2:	f7f9 f9c1 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dd0e6:	4581      	cmp	r9, r0
   dd0e8:	dce7      	bgt.n	dd0ba <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x2e>
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
   dd0ea:	1e71      	subs	r1, r6, #1
   dd0ec:	4628      	mov	r0, r5
   dd0ee:	f7f9 f9bb 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dd0f2:	f8dd 8060 	ldr.w	r8, [sp, #96]	; 0x60
   dd0f6:	4606      	mov	r6, r0
   dd0f8:	4267      	negs	r7, r4
  for (int b = 0; b < batches; ++b) {
   dd0fa:	f04f 0a00 	mov.w	sl, #0
   dd0fe:	9b07      	ldr	r3, [sp, #28]
   dd100:	459a      	cmp	sl, r3
   dd102:	da39      	bge.n	dd178 <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0xec>
   dd104:	9b14      	ldr	r3, [sp, #80]	; 0x50
   dd106:	2500      	movs	r5, #0
    for (int out_c = 0; out_c < output_depth; ++out_c) {
   dd108:	454d      	cmp	r5, r9
   dd10a:	da2f      	bge.n	dd16c <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0xe0>
   dd10c:	469c      	mov	ip, r3
   dd10e:	46a6      	mov	lr, r4
      int32 acc = 0;
   dd110:	2000      	movs	r0, #0
      for (int d = 0; d < accum_depth; ++d) {
   dd112:	eb07 020e 	add.w	r2, r7, lr
   dd116:	4296      	cmp	r6, r2
   dd118:	dd0f      	ble.n	dd13a <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0xae>
        int32 input_val = input_data[b * accum_depth + d];
   dd11a:	f91e 2b01 	ldrsb.w	r2, [lr], #1
   dd11e:	9208      	str	r2, [sp, #32]
        int32 filter_val = filter_data[out_c * accum_depth + d];
        acc += (filter_val + filter_offset) * (input_val + input_offset);
   dd120:	9903      	ldr	r1, [sp, #12]
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
      int32 acc = 0;
      for (int d = 0; d < accum_depth; ++d) {
        int32 input_val = input_data[b * accum_depth + d];
        int32 filter_val = filter_data[out_c * accum_depth + d];
   dd122:	f91c 2b01 	ldrsb.w	r2, [ip], #1
        acc += (filter_val + filter_offset) * (input_val + input_offset);
   dd126:	440a      	add	r2, r1
   dd128:	9209      	str	r2, [sp, #36]	; 0x24
   dd12a:	9902      	ldr	r1, [sp, #8]
   dd12c:	9a08      	ldr	r2, [sp, #32]
   dd12e:	440a      	add	r2, r1
   dd130:	4611      	mov	r1, r2
   dd132:	9a09      	ldr	r2, [sp, #36]	; 0x24
   dd134:	fb01 0002 	mla	r0, r1, r2, r0
  TFLITE_DCHECK_LE(output_depth, filter_shape.Dims(filter_dim_count - 2));
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
      int32 acc = 0;
      for (int d = 0; d < accum_depth; ++d) {
   dd138:	e7eb      	b.n	dd112 <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x86>
        int32 input_val = input_data[b * accum_depth + d];
        int32 filter_val = filter_data[out_c * accum_depth + d];
        acc += (filter_val + filter_offset) * (input_val + input_offset);
      }
      if (bias_data) {
   dd13a:	9a16      	ldr	r2, [sp, #88]	; 0x58
   dd13c:	b112      	cbz	r2, dd144 <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0xb8>
        acc += bias_data[out_c];
   dd13e:	f852 2025 	ldr.w	r2, [r2, r5, lsl #2]
   dd142:	4410      	add	r0, r2
      }
      acc = MultiplyByQuantizedMultiplier(acc, output_multiplier, output_shift);
   dd144:	9a06      	ldr	r2, [sp, #24]
   dd146:	9905      	ldr	r1, [sp, #20]
   dd148:	9308      	str	r3, [sp, #32]
   dd14a:	f7fe fd4d 	bl	dbbe8 <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
      acc += output_offset;
   dd14e:	9b04      	ldr	r3, [sp, #16]
   dd150:	4418      	add	r0, r3
   dd152:	9b01      	ldr	r3, [sp, #4]
   dd154:	4298      	cmp	r0, r3
   dd156:	bfb8      	it	lt
   dd158:	4618      	movlt	r0, r3
      acc = std::max(acc, output_activation_min);
      acc = std::min(acc, output_activation_max);
      output_data[out_c + output_depth * b] = static_cast<int8_t>(acc);
   dd15a:	4558      	cmp	r0, fp
   dd15c:	9b08      	ldr	r3, [sp, #32]
   dd15e:	bfa8      	it	ge
   dd160:	4658      	movge	r0, fp
   dd162:	f808 0005 	strb.w	r0, [r8, r5]
   dd166:	4433      	add	r3, r6
  const int batches = output_shape.Dims(0);
  const int output_depth = output_shape.Dims(1);
  TFLITE_DCHECK_LE(output_depth, filter_shape.Dims(filter_dim_count - 2));
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
   dd168:	3501      	adds	r5, #1
   dd16a:	e7cd      	b.n	dd108 <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x7c>
  const int filter_dim_count = filter_shape.DimensionsCount();
  const int batches = output_shape.Dims(0);
  const int output_depth = output_shape.Dims(1);
  TFLITE_DCHECK_LE(output_depth, filter_shape.Dims(filter_dim_count - 2));
  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
  for (int b = 0; b < batches; ++b) {
   dd16c:	f10a 0a01 	add.w	sl, sl, #1
   dd170:	44c8      	add	r8, r9
   dd172:	4434      	add	r4, r6
   dd174:	1bbf      	subs	r7, r7, r6
   dd176:	e7c2      	b.n	dd0fe <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa+0x72>
      acc = std::max(acc, output_activation_min);
      acc = std::min(acc, output_activation_max);
      output_data[out_c + output_depth * b] = static_cast<int8_t>(acc);
    }
  }
}
   dd178:	b00b      	add	sp, #44	; 0x2c
   dd17a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

000dd180 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode>:
      GetTensorShape(bias), GetTensorData<float>(bias), GetTensorShape(output),
      GetTensorData<float>(output));
  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dd180:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dd184:	ed2d 8b02 	vpush	{d8}
   dd188:	680c      	ldr	r4, [r1, #0]
  auto* params =
      reinterpret_cast<TfLiteFullyConnectedParams*>(node->builtin_data);
   dd18a:	694b      	ldr	r3, [r1, #20]
   dd18c:	f8d0 9008 	ldr.w	r9, [r0, #8]
      GetTensorShape(bias), GetTensorData<float>(bias), GetTensorShape(output),
      GetTensorData<float>(output));
  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dd190:	b0af      	sub	sp, #188	; 0xbc
   dd192:	4680      	mov	r8, r0
  auto* params =
      reinterpret_cast<TfLiteFullyConnectedParams*>(node->builtin_data);
   dd194:	9307      	str	r3, [sp, #28]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dd196:	68a0      	ldr	r0, [r4, #8]
   dd198:	6863      	ldr	r3, [r4, #4]
}

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
   dd19a:	68e4      	ldr	r4, [r4, #12]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dd19c:	2238      	movs	r2, #56	; 0x38
   dd19e:	fb02 fa00 	mul.w	sl, r2, r0

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
   dd1a2:	1c60      	adds	r0, r4, #1
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dd1a4:	fb02 f303 	mul.w	r3, r2, r3
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dd1a8:	bf18      	it	ne
   dd1aa:	fb02 9404 	mlane	r4, r2, r4, r9
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dd1ae:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dd1b0:	eb09 0703 	add.w	r7, r9, r3
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dd1b4:	6852      	ldr	r2, [r2, #4]
                             TfLiteType data_type, const TfLiteTensor* input,
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output,
                             OpData* data) {
  TfLiteStatus status = kTfLiteOk;
  if (data_type != kTfLiteFloat32) {
   dd1b6:	f819 3003 	ldrb.w	r3, [r9, r3]
   dd1ba:	f04f 0b38 	mov.w	fp, #56	; 0x38
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
  }
  return nullptr;
   dd1be:	bf08      	it	eq
   dd1c0:	2400      	moveq	r4, #0
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dd1c2:	fb0b fb02 	mul.w	fp, fp, r2
   dd1c6:	2b01      	cmp	r3, #1
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dd1c8:	eb09 060a 	add.w	r6, r9, sl
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dd1cc:	eb09 050b 	add.w	r5, r9, fp
   dd1d0:	d021      	beq.n	dd216 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x96>
    double real_multiplier = 0.0;
   dd1d2:	ab2e      	add	r3, sp, #184	; 0xb8
   dd1d4:	2000      	movs	r0, #0
   dd1d6:	2100      	movs	r1, #0
   dd1d8:	e963 010a 	strd	r0, r1, [r3, #-40]!	; 0x28
    TF_LITE_ENSURE_STATUS(GetQuantizedConvolutionMultipler(
   dd1dc:	4632      	mov	r2, r6
   dd1de:	9301      	str	r3, [sp, #4]
   dd1e0:	9500      	str	r5, [sp, #0]
   dd1e2:	4623      	mov	r3, r4
   dd1e4:	4639      	mov	r1, r7
   dd1e6:	4640      	mov	r0, r8
   dd1e8:	f006 fc36 	bl	e3a58 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd>
   dd1ec:	2800      	cmp	r0, #0
   dd1ee:	d132      	bne.n	dd256 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0xd6>
        context, input, filter, bias, output, &real_multiplier));
    int exponent;
    QuantizeMultiplier(real_multiplier, &data->output_multiplier, &exponent);
   dd1f0:	a91f      	add	r1, sp, #124	; 0x7c
   dd1f2:	a80b      	add	r0, sp, #44	; 0x2c
   dd1f4:	ed9d 0b24 	vldr	d0, [sp, #144]	; 0x90
   dd1f8:	f006 fdcc 	bl	e3d94 <_ZN6tflite18QuantizeMultiplierEdPlPi>
    data->output_shift = -exponent;
   dd1fc:	9b1f      	ldr	r3, [sp, #124]	; 0x7c
   dd1fe:	425b      	negs	r3, r3
   dd200:	930c      	str	r3, [sp, #48]	; 0x30
    TF_LITE_ENSURE_STATUS(CalculateActivationRangeQuantized(
   dd202:	9b07      	ldr	r3, [sp, #28]
   dd204:	7819      	ldrb	r1, [r3, #0]
   dd206:	ab0e      	add	r3, sp, #56	; 0x38
   dd208:	9300      	str	r3, [sp, #0]
   dd20a:	462a      	mov	r2, r5
   dd20c:	ab0d      	add	r3, sp, #52	; 0x34
   dd20e:	4640      	mov	r0, r8
   dd210:	f006 fc80 	bl	e3b14 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_>
   dd214:	b9f8      	cbnz	r0, dd256 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0xd6>
  OpData local_data_object;
  OpData* data = &local_data_object;
  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, data_type, input,
                                        filter, bias, output, data));

  switch (filter->type) {  // Already know in/out types are same.
   dd216:	f819 200a 	ldrb.w	r2, [r9, sl]
   dd21a:	2a03      	cmp	r2, #3
   dd21c:	d176      	bne.n	dd30c <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x18c>
                           TfLiteFullyConnectedParams* params, OpData* data,
                           const TfLiteTensor* input,
                           const TfLiteTensor* filter, const TfLiteTensor* bias,
                           TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
   dd21e:	6933      	ldr	r3, [r6, #16]
  const int32_t output_offset = output->params.zero_point;

  tflite::FullyConnectedParams op_params;
  op_params.input_offset = input_offset;
   dd220:	693a      	ldr	r2, [r7, #16]
                           const TfLiteTensor* input,
                           const TfLiteTensor* filter, const TfLiteTensor* bias,
                           TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;
   dd222:	6929      	ldr	r1, [r5, #16]

  tflite::FullyConnectedParams op_params;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
   dd224:	9126      	str	r1, [sp, #152]	; 0x98
                           TfLiteFullyConnectedParams* params, OpData* data,
                           const TfLiteTensor* input,
                           const TfLiteTensor* filter, const TfLiteTensor* bias,
                           TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
   dd226:	425b      	negs	r3, r3
  const int32_t output_offset = output->params.zero_point;

  tflite::FullyConnectedParams op_params;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
   dd228:	9325      	str	r3, [sp, #148]	; 0x94
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
   dd22a:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   dd22c:	9327      	str	r3, [sp, #156]	; 0x9c
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;
   dd22e:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   dd230:	425b      	negs	r3, r3
   dd232:	9328      	str	r3, [sp, #160]	; 0xa0
  op_params.quantized_activation_min = data->output_activation_min;
   dd234:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   dd236:	9329      	str	r3, [sp, #164]	; 0xa4
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;

  tflite::FullyConnectedParams op_params;
  op_params.input_offset = input_offset;
   dd238:	4252      	negs	r2, r2
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
   dd23a:	9b0e      	ldr	r3, [sp, #56]	; 0x38
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;

  tflite::FullyConnectedParams op_params;
  op_params.input_offset = input_offset;
   dd23c:	9224      	str	r2, [sp, #144]	; 0x90
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
   dd23e:	932a      	str	r3, [sp, #168]	; 0xa8
  reference_ops::FullyConnected(                                       \
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input), \
      GetTensorShape(filter), GetTensorData<uint8_t>(filter),          \
      GetTensorShape(bias), GetTensorData<int32_t>(bias),              \
      GetTensorShape(output), GetTensorData<output_data_type>(output))
  switch (output->type) {
   dd240:	f819 300b 	ldrb.w	r3, [r9, fp]
   dd244:	2b03      	cmp	r3, #3
   dd246:	d008      	beq.n	dd25a <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0xda>
   dd248:	2b07      	cmp	r3, #7
   dd24a:	d02c      	beq.n	dd2a6 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x126>
      TF_LITE_FULLY_CONNECTED(int16_t);
      break;
    default:
      context->ReportError(
          context,
          "Quantized FullyConnected expects output data type uint8 or int16");
   dd24c:	f8d8 3014 	ldr.w	r3, [r8, #20]
   dd250:	499d      	ldr	r1, [pc, #628]	; (dd4c8 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x348>)
   dd252:	4640      	mov	r0, r8
   dd254:	4798      	blx	r3
                           output);

    default:
      context->ReportError(context, "Type %d not currently supported.",
                           filter->type);
      return kTfLiteError;
   dd256:	2001      	movs	r0, #1
   dd258:	e131      	b.n	dd4be <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x33e>
      GetTensorShape(filter), GetTensorData<uint8_t>(filter),          \
      GetTensorShape(bias), GetTensorData<int32_t>(bias),              \
      GetTensorShape(output), GetTensorData<output_data_type>(output))
  switch (output->type) {
    case kTfLiteUInt8:
      TF_LITE_FULLY_CONNECTED(uint8_t);
   dd25a:	4639      	mov	r1, r7
   dd25c:	a810      	add	r0, sp, #64	; 0x40
   dd25e:	f7f9 fba8 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd262:	4631      	mov	r1, r6
   dd264:	a815      	add	r0, sp, #84	; 0x54
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dd266:	f8d7 8004 	ldr.w	r8, [r7, #4]
   dd26a:	f7f9 fba2 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd26e:	f8d6 9004 	ldr.w	r9, [r6, #4]
   dd272:	ae1a      	add	r6, sp, #104	; 0x68
   dd274:	4621      	mov	r1, r4
   dd276:	4630      	mov	r0, r6
   dd278:	f7f9 fb9b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd27c:	b104      	cbz	r4, dd280 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x100>
   dd27e:	6864      	ldr	r4, [r4, #4]
   dd280:	af1f      	add	r7, sp, #124	; 0x7c
   dd282:	4629      	mov	r1, r5
   dd284:	4638      	mov	r0, r7
   dd286:	f7f9 fb94 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd28a:	686b      	ldr	r3, [r5, #4]
   dd28c:	9304      	str	r3, [sp, #16]
   dd28e:	9703      	str	r7, [sp, #12]
   dd290:	9402      	str	r4, [sp, #8]
   dd292:	9601      	str	r6, [sp, #4]
   dd294:	f8cd 9000 	str.w	r9, [sp]
   dd298:	ab15      	add	r3, sp, #84	; 0x54
   dd29a:	4642      	mov	r2, r8
   dd29c:	a910      	add	r1, sp, #64	; 0x40
   dd29e:	a824      	add	r0, sp, #144	; 0x90
   dd2a0:	f7ff fe0c 	bl	dcebc <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ph>
   dd2a4:	e024      	b.n	dd2f0 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x170>
      break;
    case kTfLiteInt16:
      TF_LITE_FULLY_CONNECTED(int16_t);
   dd2a6:	4639      	mov	r1, r7
   dd2a8:	a810      	add	r0, sp, #64	; 0x40
   dd2aa:	f7f9 fb82 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd2ae:	4631      	mov	r1, r6
   dd2b0:	a815      	add	r0, sp, #84	; 0x54
   dd2b2:	f8d7 8004 	ldr.w	r8, [r7, #4]
   dd2b6:	f7f9 fb7c 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd2ba:	f8d6 9004 	ldr.w	r9, [r6, #4]
   dd2be:	ae1a      	add	r6, sp, #104	; 0x68
   dd2c0:	4621      	mov	r1, r4
   dd2c2:	4630      	mov	r0, r6
   dd2c4:	f7f9 fb75 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd2c8:	b104      	cbz	r4, dd2cc <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x14c>
   dd2ca:	6864      	ldr	r4, [r4, #4]
   dd2cc:	af1f      	add	r7, sp, #124	; 0x7c
   dd2ce:	4629      	mov	r1, r5
   dd2d0:	4638      	mov	r0, r7
   dd2d2:	f7f9 fb6e 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd2d6:	686b      	ldr	r3, [r5, #4]
   dd2d8:	9304      	str	r3, [sp, #16]
   dd2da:	9703      	str	r7, [sp, #12]
   dd2dc:	9402      	str	r4, [sp, #8]
   dd2de:	9601      	str	r6, [sp, #4]
   dd2e0:	f8cd 9000 	str.w	r9, [sp]
   dd2e4:	ab15      	add	r3, sp, #84	; 0x54
   dd2e6:	4642      	mov	r2, r8
   dd2e8:	a910      	add	r1, sp, #64	; 0x40
   dd2ea:	a824      	add	r0, sp, #144	; 0x90
   dd2ec:	f7ff fe5d 	bl	dcfaa <_ZN6tflite13reference_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_PKlS6_Ps>
   dd2f0:	4638      	mov	r0, r7
   dd2f2:	f7f9 f8ae 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   dd2f6:	4630      	mov	r0, r6
   dd2f8:	f7f9 f8ab 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   dd2fc:	a815      	add	r0, sp, #84	; 0x54
   dd2fe:	f7f9 f8a8 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   dd302:	a810      	add	r0, sp, #64	; 0x40
   dd304:	f7f9 f8a5 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
          context,
          "Quantized FullyConnected expects output data type uint8 or int16");
      return kTfLiteError;
  }

  return kTfLiteOk;
   dd308:	2000      	movs	r0, #0
   dd30a:	e0d8      	b.n	dd4be <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x33e>
  OpData local_data_object;
  OpData* data = &local_data_object;
  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, data_type, input,
                                        filter, bias, output, data));

  switch (filter->type) {  // Already know in/out types are same.
   dd30c:	2a09      	cmp	r2, #9
   dd30e:	d136      	bne.n	dd37e <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x1fe>
                               TfLiteFullyConnectedParams* params, OpData* data,
                               const TfLiteTensor* input,
                               const TfLiteTensor* filter,
                               const TfLiteTensor* bias, TfLiteTensor* output) {
  FullyConnectedParams op_params;
  op_params.input_offset = -input->params.zero_point;
   dd310:	693b      	ldr	r3, [r7, #16]
   dd312:	425b      	negs	r3, r3
   dd314:	9324      	str	r3, [sp, #144]	; 0x90
  op_params.weights_offset = -filter->params.zero_point;
   dd316:	6933      	ldr	r3, [r6, #16]
   dd318:	425b      	negs	r3, r3
   dd31a:	9325      	str	r3, [sp, #148]	; 0x94
  op_params.output_offset = output->params.zero_point;
   dd31c:	692b      	ldr	r3, [r5, #16]
   dd31e:	9326      	str	r3, [sp, #152]	; 0x98
  op_params.output_multiplier = data->output_multiplier;
   dd320:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   dd322:	9327      	str	r3, [sp, #156]	; 0x9c
  // TODO(b/138810107): Figure out whether output shift should be inverted
  op_params.output_shift = -data->output_shift;
   dd324:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   dd326:	425b      	negs	r3, r3
   dd328:	9328      	str	r3, [sp, #160]	; 0xa0
  op_params.quantized_activation_min = data->output_activation_min;
   dd32a:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   dd32c:	9329      	str	r3, [sp, #164]	; 0xa4
  op_params.quantized_activation_max = data->output_activation_max;

  reference_integer_ops::FullyConnected(
      op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
   dd32e:	4639      	mov	r1, r7
  op_params.output_offset = output->params.zero_point;
  op_params.output_multiplier = data->output_multiplier;
  // TODO(b/138810107): Figure out whether output shift should be inverted
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
   dd330:	9b0e      	ldr	r3, [sp, #56]	; 0x38
   dd332:	932a      	str	r3, [sp, #168]	; 0xa8

  reference_integer_ops::FullyConnected(
      op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
   dd334:	a810      	add	r0, sp, #64	; 0x40
   dd336:	f7f9 fb3c 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(filter), GetTensorData<int8_t>(filter),
   dd33a:	4631      	mov	r1, r6
   dd33c:	a815      	add	r0, sp, #84	; 0x54
   dd33e:	f8d7 8004 	ldr.w	r8, [r7, #4]
   dd342:	f7f9 fb36 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd346:	f8d6 9004 	ldr.w	r9, [r6, #4]
      GetTensorShape(bias), GetTensorData<int32_t>(bias),
   dd34a:	ae1a      	add	r6, sp, #104	; 0x68
   dd34c:	4621      	mov	r1, r4
   dd34e:	4630      	mov	r0, r6
   dd350:	f7f9 fb2f 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd354:	b104      	cbz	r4, dd358 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x1d8>
   dd356:	6864      	ldr	r4, [r4, #4]
      GetTensorShape(output), GetTensorData<int8_t>(output));
   dd358:	af1f      	add	r7, sp, #124	; 0x7c
   dd35a:	4629      	mov	r1, r5
   dd35c:	4638      	mov	r0, r7
   dd35e:	f7f9 fb28 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd362:	686b      	ldr	r3, [r5, #4]
   dd364:	9304      	str	r3, [sp, #16]
   dd366:	9703      	str	r7, [sp, #12]
   dd368:	9402      	str	r4, [sp, #8]
   dd36a:	9601      	str	r6, [sp, #4]
   dd36c:	f8cd 9000 	str.w	r9, [sp]
   dd370:	ab15      	add	r3, sp, #84	; 0x54
   dd372:	4642      	mov	r2, r8
   dd374:	a910      	add	r1, sp, #64	; 0x40
   dd376:	a824      	add	r0, sp, #144	; 0x90
   dd378:	f7ff fe88 	bl	dd08c <_ZN6tflite21reference_integer_ops14FullyConnectedERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PKlS6_Pa>
   dd37c:	e7b8      	b.n	dd2f0 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x170>
  OpData local_data_object;
  OpData* data = &local_data_object;
  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, data_type, input,
                                        filter, bias, output, data));

  switch (filter->type) {  // Already know in/out types are same.
   dd37e:	2a01      	cmp	r2, #1
   dd380:	f040 8097 	bne.w	dd4b2 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x332>
   dd384:	9b07      	ldr	r3, [sp, #28]
   dd386:	781b      	ldrb	r3, [r3, #0]
// Calculates the useful range of an activation layer given its activation
// tensor.a
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
   dd388:	2b01      	cmp	r3, #1
   dd38a:	d011      	beq.n	dd3b0 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x230>
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
   dd38c:	2b03      	cmp	r3, #3
   dd38e:	d012      	beq.n	dd3b6 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x236>
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
    *activation_max = std::numeric_limits<T>::max();
   dd390:	ed9f 8a4e 	vldr	s16, [pc, #312]	; dd4cc <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x34c>
   dd394:	eddf 8a4e 	vldr	s17, [pc, #312]	; dd4d0 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x350>
   dd398:	2b02      	cmp	r3, #2
   dd39a:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   dd39e:	bf08      	it	eq
   dd3a0:	eeb0 8a67 	vmoveq.f32	s16, s15
   dd3a4:	eeff 7a00 	vmov.f32	s15, #240	; 0xbf800000 -1.0
   dd3a8:	bf08      	it	eq
   dd3aa:	eef0 8a67 	vmoveq.f32	s17, s15
   dd3ae:	e006      	b.n	dd3be <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x23e>
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
   dd3b0:	ed9f 8a46 	vldr	s16, [pc, #280]	; dd4cc <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x34c>
   dd3b4:	e001      	b.n	dd3ba <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x23a>
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
    *activation_max = 6;
   dd3b6:	eeb1 8a08 	vmov.f32	s16, #24	; 0x40c00000  6.0
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
   dd3ba:	eddf 8a46 	vldr	s17, [pc, #280]	; dd4d4 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x354>
                           &output_activation_max);
  tflite::FullyConnectedParams op_params;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  tflite::reference_ops::FullyConnected(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
   dd3be:	4639      	mov	r1, r7
   dd3c0:	a815      	add	r0, sp, #84	; 0x54
   dd3c2:	f7f9 faf6 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(filter), GetTensorData<float>(filter),
   dd3c6:	4631      	mov	r1, r6
   dd3c8:	a81a      	add	r0, sp, #104	; 0x68
   dd3ca:	687f      	ldr	r7, [r7, #4]
   dd3cc:	f7f9 faf1 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd3d0:	6873      	ldr	r3, [r6, #4]
   dd3d2:	9308      	str	r3, [sp, #32]
      GetTensorShape(bias), GetTensorData<float>(bias), GetTensorShape(output),
   dd3d4:	4621      	mov	r1, r4
   dd3d6:	a81f      	add	r0, sp, #124	; 0x7c
   dd3d8:	f7f9 faeb 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd3dc:	b104      	cbz	r4, dd3e0 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x260>
   dd3de:	6864      	ldr	r4, [r4, #4]
   dd3e0:	4629      	mov	r1, r5
   dd3e2:	a824      	add	r0, sp, #144	; 0x90
   dd3e4:	f7f9 fae5 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dd3e8:	b105      	cbz	r5, dd3ec <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x26c>
   dd3ea:	686d      	ldr	r5, [r5, #4]
  // but the current --variable_batch hack consists in overwriting the 3rd
  // dimension with the runtime batch size, as we don't keep track for each
  // array of which dimension is the batch dimension in it.
  const int output_dims_count = output_shape.DimensionsCount();
  const int weights_dims_count = weights_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);
   dd3ec:	9e24      	ldr	r6, [sp, #144]	; 0x90
   dd3ee:	f8dd 9068 	ldr.w	r9, [sp, #104]	; 0x68
   dd3f2:	3e01      	subs	r6, #1
   dd3f4:	4631      	mov	r1, r6
   dd3f6:	a824      	add	r0, sp, #144	; 0x90
   dd3f8:	f7ff fd47 	bl	dce8a <_ZN6tflite15FlatSizeSkipDimERKNS_12RuntimeShapeEi>
  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,
                                       output_shape, output_dims_count - 1);
   dd3fc:	4633      	mov	r3, r6
   dd3fe:	aa24      	add	r2, sp, #144	; 0x90
  // but the current --variable_batch hack consists in overwriting the 3rd
  // dimension with the runtime batch size, as we don't keep track for each
  // array of which dimension is the batch dimension in it.
  const int output_dims_count = output_shape.DimensionsCount();
  const int weights_dims_count = weights_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);
   dd400:	9009      	str	r0, [sp, #36]	; 0x24
  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,
                                       output_shape, output_dims_count - 1);
   dd402:	f1a9 0102 	sub.w	r1, r9, #2
   dd406:	a81a      	add	r0, sp, #104	; 0x68
   dd408:	f7fe fbdf 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);
   dd40c:	f109 31ff 	add.w	r1, r9, #4294967295	; 0xffffffff
  // array of which dimension is the batch dimension in it.
  const int output_dims_count = output_shape.DimensionsCount();
  const int weights_dims_count = weights_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);
  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,
                                       output_shape, output_dims_count - 1);
   dd410:	4680      	mov	r8, r0
  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);
   dd412:	a81a      	add	r0, sp, #104	; 0x68
   dd414:	f7f9 f828 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dd418:	ea4f 0388 	mov.w	r3, r8, lsl #2
   dd41c:	9307      	str	r3, [sp, #28]
   dd41e:	ea4f 0c80 	mov.w	ip, r0, lsl #2
   dd422:	463b      	mov	r3, r7
  for (int b = 0; b < batches; ++b) {
   dd424:	2200      	movs	r2, #0
   dd426:	9909      	ldr	r1, [sp, #36]	; 0x24
   dd428:	4291      	cmp	r1, r2
   dd42a:	dd37      	ble.n	dd49c <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x31c>
   dd42c:	9908      	ldr	r1, [sp, #32]
   dd42e:	4626      	mov	r6, r4
   dd430:	46a9      	mov	r9, r5
   dd432:	2700      	movs	r7, #0
    for (int out_c = 0; out_c < output_depth; ++out_c) {
   dd434:	45b8      	cmp	r8, r7
   dd436:	dd2c      	ble.n	dd492 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x312>
   dd438:	eddf 7a26 	vldr	s15, [pc, #152]	; dd4d4 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x354>
   dd43c:	468a      	mov	sl, r1
   dd43e:	469b      	mov	fp, r3
   dd440:	f04f 0e00 	mov.w	lr, #0
      float total = 0.f;
      for (int d = 0; d < accum_depth; ++d) {
   dd444:	4570      	cmp	r0, lr
   dd446:	dd08      	ble.n	dd45a <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x2da>
        total += input_data[b * accum_depth + d] *
   dd448:	ecfb 6a01 	vldmia	fp!, {s13}
   dd44c:	ecba 7a01 	vldmia	sl!, {s14}
                                       output_shape, output_dims_count - 1);
  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
      float total = 0.f;
      for (int d = 0; d < accum_depth; ++d) {
   dd450:	f10e 0e01 	add.w	lr, lr, #1
        total += input_data[b * accum_depth + d] *
                 weights_data[out_c * accum_depth + d];
   dd454:	eee6 7a87 	vfma.f32	s15, s13, s14
   dd458:	e7f4      	b.n	dd444 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x2c4>
      }
      float bias_value = 0.0f;
      if (bias_data) {
   dd45a:	b114      	cbz	r4, dd462 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x2e2>
        bias_value = bias_data[out_c];
   dd45c:	ed96 7a00 	vldr	s14, [r6]
   dd460:	e001      	b.n	dd466 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x2e6>
      float total = 0.f;
      for (int d = 0; d < accum_depth; ++d) {
        total += input_data[b * accum_depth + d] *
                 weights_data[out_c * accum_depth + d];
      }
      float bias_value = 0.0f;
   dd462:	ed9f 7a1c 	vldr	s14, [pc, #112]	; dd4d4 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x354>
      if (bias_data) {
        bias_value = bias_data[out_c];
      }
      output_data[out_c + output_depth * b] = ActivationFunctionWithMinMax(
   dd466:	ee77 7a87 	vadd.f32	s15, s15, s14
  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);
  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,
                                       output_shape, output_dims_count - 1);
  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);
  for (int b = 0; b < batches; ++b) {
    for (int out_c = 0; out_c < output_depth; ++out_c) {
   dd46a:	3701      	adds	r7, #1
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
	return __b;
      return __a;
   dd46c:	eef4 7ae8 	vcmpe.f32	s15, s17
   dd470:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   dd474:	bf48      	it	mi
   dd476:	eef0 7a68 	vmovmi.f32	s15, s17
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   dd47a:	eef4 7a48 	vcmp.f32	s15, s16
   dd47e:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   dd482:	bfc8      	it	gt
   dd484:	eef0 7a48 	vmovgt.f32	s15, s16
      float bias_value = 0.0f;
      if (bias_data) {
        bias_value = bias_data[out_c];
      }
      output_data[out_c + output_depth * b] = ActivationFunctionWithMinMax(
          total + bias_value, output_activation_min, output_activation_max);
   dd488:	ece9 7a01 	vstmia	r9!, {s15}
   dd48c:	3604      	adds	r6, #4
   dd48e:	4461      	add	r1, ip
   dd490:	e7d0      	b.n	dd434 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x2b4>
   dd492:	9907      	ldr	r1, [sp, #28]
  const int weights_dims_count = weights_shape.DimensionsCount();
  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);
  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,
                                       output_shape, output_dims_count - 1);
  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);
  for (int b = 0; b < batches; ++b) {
   dd494:	3201      	adds	r2, #1
   dd496:	440d      	add	r5, r1
   dd498:	4463      	add	r3, ip
   dd49a:	e7c4      	b.n	dd426 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x2a6>
   dd49c:	a824      	add	r0, sp, #144	; 0x90
   dd49e:	f7f8 ffd8 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   dd4a2:	a81f      	add	r0, sp, #124	; 0x7c
   dd4a4:	f7f8 ffd5 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  tflite::FullyConnectedParams op_params;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  tflite::reference_ops::FullyConnected(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
      GetTensorShape(filter), GetTensorData<float>(filter),
   dd4a8:	a81a      	add	r0, sp, #104	; 0x68
   dd4aa:	f7f8 ffd2 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                           &output_activation_max);
  tflite::FullyConnectedParams op_params;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  tflite::reference_ops::FullyConnected(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
   dd4ae:	a815      	add	r0, sp, #84	; 0x54
   dd4b0:	e728      	b.n	dd304 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x184>
      return EvalQuantized(context, node, params, data, input, filter, bias,
                           output);

    default:
      context->ReportError(context, "Type %d not currently supported.",
                           filter->type);
   dd4b2:	f8d8 3014 	ldr.w	r3, [r8, #20]
   dd4b6:	4908      	ldr	r1, [pc, #32]	; (dd4d8 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0x358>)
   dd4b8:	4640      	mov	r0, r8
   dd4ba:	4798      	blx	r3
   dd4bc:	e6cb      	b.n	dd256 <_ZN6tflite3ops5micro15fully_connected4EvalEP13TfLiteContextP10TfLiteNode+0xd6>
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   dd4be:	b02f      	add	sp, #188	; 0xbc
   dd4c0:	ecbd 8b02 	vpop	{d8}
   dd4c4:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dd4c8:	000e9e51 	.word	0x000e9e51
   dd4cc:	7f7fffff 	.word	0x7f7fffff
   dd4d0:	ff7fffff 	.word	0xff7fffff
   dd4d4:	00000000 	.word	0x00000000
   dd4d8:	000e9e92 	.word	0x000e9e92

000dd4dc <_ZN6tflite3ops5micro24Register_FULLY_CONNECTEDEv>:
TfLiteRegistration* Register_FULLY_CONNECTED() {
  static TfLiteRegistration r = {fully_connected::Init, fully_connected::Free,
                                 fully_connected::Prepare,
                                 fully_connected::Eval};
  return &r;
}
   dd4dc:	4800      	ldr	r0, [pc, #0]	; (dd4e0 <_ZN6tflite3ops5micro24Register_FULLY_CONNECTEDEv+0x4>)
   dd4de:	4770      	bx	lr
   dd4e0:	2003c11c 	.word	0x2003c11c

000dd4e4 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_19LogicalOrEbb>:
  }

  return kTfLiteOk;
}

bool LogicalOr(bool x, bool y) { return x || y; }
   dd4e4:	2800      	cmp	r0, #0
   dd4e6:	bf0c      	ite	eq
   dd4e8:	4608      	moveq	r0, r1
   dd4ea:	2001      	movne	r0, #1
   dd4ec:	4770      	bx	lr

000dd4ee <_ZN6tflite3ops5micro7logical12_GLOBAL__N_110LogicalAndEbb>:

TfLiteStatus LogicalOrEval(TfLiteContext* context, TfLiteNode* node) {
  return LogicalImpl(context, node, LogicalOr);
}

bool LogicalAnd(bool x, bool y) { return x && y; }
   dd4ee:	2800      	cmp	r0, #0
   dd4f0:	bf14      	ite	ne
   dd4f2:	4608      	movne	r0, r1
   dd4f4:	2000      	moveq	r0, #0
   dd4f6:	4770      	bx	lr

000dd4f8 <_ZN6tflite3ops5micro19Register_LOGICAL_OREv>:
  // Init, Free, Prepare, Eval are satisfying the Interface required by
  // TfLiteRegistration.
  static TfLiteRegistration r = {/* init */ nullptr, /* free */ nullptr,
                                 /* prepare */ nullptr, logical::LogicalOrEval};
  return &r;
}
   dd4f8:	4800      	ldr	r0, [pc, #0]	; (dd4fc <_ZN6tflite3ops5micro19Register_LOGICAL_OREv+0x4>)
   dd4fa:	4770      	bx	lr
   dd4fc:	2003c13c 	.word	0x2003c13c

000dd500 <_ZN6tflite3ops5micro20Register_LOGICAL_ANDEv>:
  // TfLiteRegistration.
  static TfLiteRegistration r = {/* init */ nullptr, /* free */ nullptr,
                                 /* prepare */ nullptr,
                                 logical::LogicalAndEval};
  return &r;
}
   dd500:	4800      	ldr	r0, [pc, #0]	; (dd504 <_ZN6tflite3ops5micro20Register_LOGICAL_ANDEv+0x4>)
   dd502:	4770      	bx	lr
   dd504:	2003c15c 	.word	0x2003c15c

000dd508 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E>:
}

// R: Result type. T1: Input 1 type. T2: Input 2 type.
// TODO(renjieliu): Refactor other binary functions to use this one.
template <typename R, typename T1, typename T2>
inline void BinaryFunction(const RuntimeShape& input1_shape,
   dd508:	e92d 4ff8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dd50c:	4699      	mov	r9, r3
   dd50e:	6807      	ldr	r7, [r0, #0]
}

inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0,
                            const RuntimeShape& check_shape_1) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   dd510:	6813      	ldr	r3, [r2, #0]
   dd512:	9d0a      	ldr	r5, [sp, #40]	; 0x28
   dd514:	429f      	cmp	r7, r3
   dd516:	4604      	mov	r4, r0
   dd518:	4688      	mov	r8, r1
   dd51a:	4616      	mov	r6, r2
   dd51c:	d102      	bne.n	dd524 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1c>
   dd51e:	f04f 0a00 	mov.w	sl, #0
   dd522:	e00e      	b.n	dd542 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x3a>
   dd524:	f006 ff12 	bl	e434c <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   dd528:	4651      	mov	r1, sl
   dd52a:	4620      	mov	r0, r4
   dd52c:	f7f8 ff9c 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dd530:	4651      	mov	r1, sl
   dd532:	4683      	mov	fp, r0
   dd534:	4630      	mov	r0, r6
   dd536:	f7f8 ff97 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dd53a:	4583      	cmp	fp, r0
   dd53c:	d1f2      	bne.n	dd524 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1c>
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0,
                            const RuntimeShape& check_shape_1) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   dd53e:	f10a 0a01 	add.w	sl, sl, #1
   dd542:	4557      	cmp	r7, sl
   dd544:	dcf0      	bgt.n	dd528 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x20>

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   dd546:	682b      	ldr	r3, [r5, #0]
   dd548:	429f      	cmp	r7, r3
   dd54a:	d1eb      	bne.n	dd524 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1c>
   dd54c:	f04f 0a00 	mov.w	sl, #0
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   dd550:	4557      	cmp	r7, sl
   dd552:	dd0d      	ble.n	dd570 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x68>
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   dd554:	4651      	mov	r1, sl
   dd556:	4620      	mov	r0, r4
   dd558:	f7f8 ff86 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dd55c:	4651      	mov	r1, sl
   dd55e:	4606      	mov	r6, r0
   dd560:	4628      	mov	r0, r5
   dd562:	f7f8 ff81 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dd566:	4286      	cmp	r6, r0
   dd568:	d1dc      	bne.n	dd524 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1c>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   dd56a:	f10a 0a01 	add.w	sl, sl, #1
   dd56e:	e7ef      	b.n	dd550 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x48>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   dd570:	2f04      	cmp	r7, #4
   dd572:	bfcc      	ite	gt
   dd574:	6864      	ldrgt	r4, [r4, #4]
   dd576:	3404      	addle	r4, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dd578:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
   dd57a:	f04f 0a01 	mov.w	sl, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dd57e:	429f      	cmp	r7, r3
   dd580:	dc01      	bgt.n	dd586 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x7e>
   dd582:	2400      	movs	r4, #0
   dd584:	e005      	b.n	dd592 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x8a>
      buffer_size *= dims_data[i];
   dd586:	f854 2023 	ldr.w	r2, [r4, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dd58a:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
   dd58c:	fb02 fa0a 	mul.w	sl, r2, sl
   dd590:	e7f5      	b.n	dd57e <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x76>
                           const T2* input2_data,
                           const RuntimeShape& output_shape, R* output_data,
                           R (*func)(T1, T2)) {
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int i = 0; i < flat_size; ++i) {
   dd592:	4554      	cmp	r4, sl
   dd594:	da09      	bge.n	dd5aa <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0xa2>
    output_data[i] = func(input1_data[i], input2_data[i]);
   dd596:	f819 1004 	ldrb.w	r1, [r9, r4]
   dd59a:	f818 0004 	ldrb.w	r0, [r8, r4]
   dd59e:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   dd5a0:	4798      	blx	r3
   dd5a2:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   dd5a4:	5518      	strb	r0, [r3, r4]
                           const T2* input2_data,
                           const RuntimeShape& output_shape, R* output_data,
                           R (*func)(T1, T2)) {
  const int flat_size =
      MatchingFlatSize(input1_shape, input2_shape, output_shape);
  for (int i = 0; i < flat_size; ++i) {
   dd5a6:	3401      	adds	r4, #1
   dd5a8:	e7f3      	b.n	dd592 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x8a>
   dd5aa:	e8bd 8ff8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dd5ae <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E>:
//
// Also appears to duplicte MinimumMaximum.
//
// R: Result type. T1: Input 1 type. T2: Input 2 type.
template <typename R, typename T1, typename T2>
inline void BroadcastBinaryFunction4DSlow(
   dd5ae:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dd5b2:	469a      	mov	sl, r3
    const RuntimeShape& unextended_input1_shape, const T1* input1_data,
    const RuntimeShape& unextended_input2_shape, const T2* input2_data,
    const RuntimeShape& unextended_output_shape, R* output_data,
    R (*func)(T1, T2)) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   dd5b4:	6803      	ldr	r3, [r0, #0]
//
// Also appears to duplicte MinimumMaximum.
//
// R: Result type. T1: Input 1 type. T2: Input 2 type.
template <typename R, typename T1, typename T2>
inline void BroadcastBinaryFunction4DSlow(
   dd5b6:	b0a5      	sub	sp, #148	; 0x94
    const RuntimeShape& unextended_input1_shape, const T1* input1_data,
    const RuntimeShape& unextended_input2_shape, const T2* input2_data,
    const RuntimeShape& unextended_output_shape, R* output_data,
    R (*func)(T1, T2)) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   dd5b8:	2b04      	cmp	r3, #4
//
// Also appears to duplicte MinimumMaximum.
//
// R: Result type. T1: Input 1 type. T2: Input 2 type.
template <typename R, typename T1, typename T2>
inline void BroadcastBinaryFunction4DSlow(
   dd5ba:	4614      	mov	r4, r2
   dd5bc:	4605      	mov	r5, r0
   dd5be:	9103      	str	r1, [sp, #12]
   dd5c0:	9a2e      	ldr	r2, [sp, #184]	; 0xb8
    const RuntimeShape& unextended_input1_shape, const T1* input1_data,
    const RuntimeShape& unextended_input2_shape, const T2* input2_data,
    const RuntimeShape& unextended_output_shape, R* output_data,
    R (*func)(T1, T2)) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   dd5c2:	dd01      	ble.n	dd5c8 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1a>
   dd5c4:	f006 fec2 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   dd5c8:	6823      	ldr	r3, [r4, #0]
   dd5ca:	2b04      	cmp	r3, #4
   dd5cc:	dcfa      	bgt.n	dd5c4 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   dd5ce:	6813      	ldr	r3, [r2, #0]
   dd5d0:	2b04      	cmp	r3, #4
   dd5d2:	dcf7      	bgt.n	dd5c4 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
   dd5d4:	2301      	movs	r3, #1
   dd5d6:	2104      	movs	r1, #4
   dd5d8:	a805      	add	r0, sp, #20
   dd5da:	f7f8 ff7e 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   dd5de:	462a      	mov	r2, r5
   dd5e0:	2301      	movs	r3, #1
   dd5e2:	2104      	movs	r1, #4
   dd5e4:	a80a      	add	r0, sp, #40	; 0x28
   dd5e6:	f7f8 ff78 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   dd5ea:	4622      	mov	r2, r4
   dd5ec:	2301      	movs	r3, #1
   dd5ee:	2104      	movs	r1, #4
   dd5f0:	a80f      	add	r0, sp, #60	; 0x3c
   dd5f2:	f7f8 ff72 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   dd5f6:	f10d 0970 	add.w	r9, sp, #112	; 0x70
  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
   dd5fa:	f04f 0b01 	mov.w	fp, #1
   dd5fe:	f10d 0890 	add.w	r8, sp, #144	; 0x90

  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
   dd602:	465e      	mov	r6, fp
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
   dd604:	2403      	movs	r4, #3
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
   dd606:	ad14      	add	r5, sp, #80	; 0x50
   dd608:	464f      	mov	r7, r9
   dd60a:	4621      	mov	r1, r4
   dd60c:	a80a      	add	r0, sp, #40	; 0x28
   dd60e:	f7f8 ff2b 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
   dd612:	4621      	mov	r1, r4

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
   dd614:	f845 0024 	str.w	r0, [r5, r4, lsl #2]
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
   dd618:	a80a      	add	r0, sp, #40	; 0x28
  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
   dd61a:	f849 6d04 	str.w	r6, [r9, #-4]!
    desc0_stride *= extended_input0_shape.Dims(i);
   dd61e:	f7f8 ff23 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
   dd622:	4621      	mov	r1, r4
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
   dd624:	4346      	muls	r6, r0
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
   dd626:	a80f      	add	r0, sp, #60	; 0x3c
   dd628:	f7f8 ff1e 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dd62c:	ab1c      	add	r3, sp, #112	; 0x70
    desc1_out->strides[i] = desc1_stride;
    desc1_stride *= extended_input1_shape.Dims(i);
   dd62e:	4621      	mov	r1, r4
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
   dd630:	f843 0024 	str.w	r0, [r3, r4, lsl #2]
    desc1_out->strides[i] = desc1_stride;
   dd634:	f848 bd04 	str.w	fp, [r8, #-4]!
    desc1_stride *= extended_input1_shape.Dims(i);
   dd638:	a80f      	add	r0, sp, #60	; 0x3c
   dd63a:	f7f8 ff15 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
   dd63e:	f114 34ff 	adds.w	r4, r4, #4294967295	; 0xffffffff
    desc0_out->extents[i] = extended_input0_shape.Dims(i);
    desc0_out->strides[i] = desc0_stride;
    desc0_stride *= extended_input0_shape.Dims(i);
    desc1_out->extents[i] = extended_input1_shape.Dims(i);
    desc1_out->strides[i] = desc1_stride;
    desc1_stride *= extended_input1_shape.Dims(i);
   dd642:	fb00 fb0b 	mul.w	fp, r0, fp
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);

  // Copy dims to desc, calculating strides.
  int desc0_stride = 1;
  int desc1_stride = 1;
  for (int i = N - 1; i >= 0; --i) {
   dd646:	d2e0      	bcs.n	dd60a <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x5c>
   dd648:	2400      	movs	r4, #0
      if (extent0 == 1) {
        desc0_out->strides[i] = 0;
        desc0_out->extents[i] = extent1;
      } else {
        TFLITE_DCHECK_EQ(extent1, 1);
        desc1_out->strides[i] = 0;
   dd64a:	46a0      	mov	r8, r4

  // Walk over each dimension. If the extents are equal do nothing.
  // Otherwise, set the desc with extent 1 to have extent equal to the other and
  // stride 0.
  for (int i = 0; i < N; ++i) {
    const int extent0 = extended_input0_shape.Dims(i);
   dd64c:	4621      	mov	r1, r4
   dd64e:	a80a      	add	r0, sp, #40	; 0x28
   dd650:	f7f8 ff0a 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
    const int extent1 = extended_input1_shape.Dims(i);
   dd654:	4621      	mov	r1, r4

  // Walk over each dimension. If the extents are equal do nothing.
  // Otherwise, set the desc with extent 1 to have extent equal to the other and
  // stride 0.
  for (int i = 0; i < N; ++i) {
    const int extent0 = extended_input0_shape.Dims(i);
   dd656:	4606      	mov	r6, r0
    const int extent1 = extended_input1_shape.Dims(i);
   dd658:	a80f      	add	r0, sp, #60	; 0x3c
   dd65a:	f7f8 ff05 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
    if (extent0 != extent1) {
   dd65e:	4286      	cmp	r6, r0
   dd660:	d010      	beq.n	dd684 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0xd6>
      if (extent0 == 1) {
   dd662:	2e01      	cmp	r6, #1
   dd664:	ea4f 0384 	mov.w	r3, r4, lsl #2
   dd668:	d105      	bne.n	dd676 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0xc8>
        desc0_out->strides[i] = 0;
   dd66a:	442b      	add	r3, r5
   dd66c:	f8c3 8010 	str.w	r8, [r3, #16]
        desc0_out->extents[i] = extent1;
   dd670:	f845 0024 	str.w	r0, [r5, r4, lsl #2]
   dd674:	e006      	b.n	dd684 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0xd6>
      } else {
        TFLITE_DCHECK_EQ(extent1, 1);
   dd676:	2801      	cmp	r0, #1
   dd678:	d1a4      	bne.n	dd5c4 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
        desc1_out->strides[i] = 0;
   dd67a:	443b      	add	r3, r7
   dd67c:	f8c3 8010 	str.w	r8, [r3, #16]
        desc1_out->extents[i] = extent0;
   dd680:	f847 6024 	str.w	r6, [r7, r4, lsl #2]
  }

  // Walk over each dimension. If the extents are equal do nothing.
  // Otherwise, set the desc with extent 1 to have extent equal to the other and
  // stride 0.
  for (int i = 0; i < N; ++i) {
   dd684:	3401      	adds	r4, #1
   dd686:	2c04      	cmp	r4, #4
   dd688:	d1e0      	bne.n	dd64c <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x9e>
    NdArrayDesc<N>* desc0_out, NdArrayDesc<N>* desc1_out) {
  TFLITE_DCHECK(desc0_out != nullptr);
  TFLITE_DCHECK(desc1_out != nullptr);

  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
  auto extended_input1_shape = RuntimeShape::ExtendedShape(N, input1_shape);
   dd68a:	a80f      	add	r0, sp, #60	; 0x3c
   dd68c:	f7f8 fee1 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
    const RuntimeShape& input0_shape, const RuntimeShape& input1_shape,
    NdArrayDesc<N>* desc0_out, NdArrayDesc<N>* desc1_out) {
  TFLITE_DCHECK(desc0_out != nullptr);
  TFLITE_DCHECK(desc1_out != nullptr);

  auto extended_input0_shape = RuntimeShape::ExtendedShape(N, input0_shape);
   dd690:	a80a      	add	r0, sp, #40	; 0x28
   dd692:	f7f8 fede 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   dd696:	2400      	movs	r4, #0
   dd698:	2100      	movs	r1, #0
   dd69a:	a805      	add	r0, sp, #20
   dd69c:	f7f8 fee4 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dd6a0:	4284      	cmp	r4, r0
   dd6a2:	da5d      	bge.n	dd760 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1b2>
   dd6a4:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   dd6a6:	f10d 0914 	add.w	r9, sp, #20
   dd6aa:	2101      	movs	r1, #1
   dd6ac:	4648      	mov	r0, r9
   dd6ae:	f7f8 fedb 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dd6b2:	4285      	cmp	r5, r0
   dd6b4:	da52      	bge.n	dd75c <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1ae>
   dd6b6:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   dd6b8:	2102      	movs	r1, #2
   dd6ba:	4648      	mov	r0, r9
   dd6bc:	f7f8 fed4 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dd6c0:	4286      	cmp	r6, r0
   dd6c2:	da49      	bge.n	dd758 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1aa>
   dd6c4:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dd6c6:	2103      	movs	r1, #3
   dd6c8:	4648      	mov	r0, r9
   dd6ca:	f7f8 fecd 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dd6ce:	4287      	cmp	r7, r0
   dd6d0:	da40      	bge.n	dd754 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x1a6>
  }
  return offset;
}

inline int Offset(const RuntimeShape& shape, int i0, int i1, int i2, int i3) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), 4);
   dd6d2:	9b05      	ldr	r3, [sp, #20]
   dd6d4:	2b04      	cmp	r3, #4
   dd6d6:	f47f af75 	bne.w	dd5c4 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  const int* dims_data = reinterpret_cast<const int*>(shape.DimsDataUpTo4D());
  TFLITE_DCHECK(i0 >= 0 && i0 < dims_data[0]);
   dd6da:	2c00      	cmp	r4, #0
   dd6dc:	f6ff af72 	blt.w	dd5c4 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
   dd6e0:	9b06      	ldr	r3, [sp, #24]
   dd6e2:	429c      	cmp	r4, r3
   dd6e4:	f6bf af6e 	bge.w	dd5c4 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  TFLITE_DCHECK(i1 >= 0 && i1 < dims_data[1]);
   dd6e8:	2d00      	cmp	r5, #0
   dd6ea:	f6ff af6b 	blt.w	dd5c4 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
   dd6ee:	9b07      	ldr	r3, [sp, #28]
   dd6f0:	429d      	cmp	r5, r3
   dd6f2:	f6bf af67 	bge.w	dd5c4 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  TFLITE_DCHECK(i2 >= 0 && i2 < dims_data[2]);
   dd6f6:	2e00      	cmp	r6, #0
   dd6f8:	f6ff af64 	blt.w	dd5c4 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
   dd6fc:	9908      	ldr	r1, [sp, #32]
   dd6fe:	428e      	cmp	r6, r1
   dd700:	f6bf af60 	bge.w	dd5c4 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  TFLITE_DCHECK(i3 >= 0 && i3 < dims_data[3]);
   dd704:	2f00      	cmp	r7, #0
   dd706:	f6ff af5d 	blt.w	dd5c4 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
   dd70a:	9a09      	ldr	r2, [sp, #36]	; 0x24
   dd70c:	4297      	cmp	r7, r2
   dd70e:	f6bf af59 	bge.w	dd5c4 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x16>
  return ((i0 * dims_data[1] + i1) * dims_data[2] + i2) * dims_data[3] + i3;
   dd712:	fb03 5304 	mla	r3, r3, r4, r5
   dd716:	fb01 6303 	mla	r3, r1, r3, r6
   dd71a:	fb02 7803 	mla	r8, r2, r3, r7
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   dd71e:	9700      	str	r7, [sp, #0]
   dd720:	4633      	mov	r3, r6
   dd722:	462a      	mov	r2, r5
   dd724:	4621      	mov	r1, r4
   dd726:	a814      	add	r0, sp, #80	; 0x50
   dd728:	f7f8 ffb4 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   dd72c:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   dd72e:	4683      	mov	fp, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   dd730:	4633      	mov	r3, r6
   dd732:	462a      	mov	r2, r5
   dd734:	4621      	mov	r1, r4
   dd736:	a81c      	add	r0, sp, #112	; 0x70
   dd738:	f7f8 ffac 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = func(in1_val, in2_val);
   dd73c:	9b03      	ldr	r3, [sp, #12]
   dd73e:	f81a 1000 	ldrb.w	r1, [sl, r0]
   dd742:	f813 000b 	ldrb.w	r0, [r3, fp]
   dd746:	9b30      	ldr	r3, [sp, #192]	; 0xc0
   dd748:	4798      	blx	r3
   dd74a:	9b2f      	ldr	r3, [sp, #188]	; 0xbc
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dd74c:	3701      	adds	r7, #1
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = func(in1_val, in2_val);
   dd74e:	f803 0008 	strb.w	r0, [r3, r8]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dd752:	e7b8      	b.n	dd6c6 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x118>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   dd754:	3601      	adds	r6, #1
   dd756:	e7af      	b.n	dd6b8 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0x10a>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   dd758:	3501      	adds	r5, #1
   dd75a:	e7a4      	b.n	dd6a6 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0xf8>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   dd75c:	3401      	adds	r4, #1
   dd75e:	e79b      	b.n	dd698 <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E+0xea>
    R (*func)(T1, T2)) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   dd760:	a805      	add	r0, sp, #20
   dd762:	f7f8 fe76 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = func(in1_val, in2_val);
        }
      }
    }
  }
}
   dd766:	b025      	add	sp, #148	; 0x94
   dd768:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dd76c <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE>:
constexpr int kInputTensor1 = 0;
constexpr int kInputTensor2 = 1;
constexpr int kOutputTensor = 0;

TfLiteStatus LogicalImpl(TfLiteContext* context, TfLiteNode* node,
                         bool (*func)(bool, bool)) {
   dd76c:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
   dd770:	680b      	ldr	r3, [r1, #0]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dd772:	685c      	ldr	r4, [r3, #4]
   dd774:	689d      	ldr	r5, [r3, #8]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dd776:	684b      	ldr	r3, [r1, #4]
   dd778:	4617      	mov	r7, r2
   dd77a:	6882      	ldr	r2, [r0, #8]
   dd77c:	6859      	ldr	r1, [r3, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dd77e:	2038      	movs	r0, #56	; 0x38
   dd780:	fb00 2404 	mla	r4, r0, r4, r2
   dd784:	fb00 2505 	mla	r5, r0, r5, r2
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dd788:	fb00 2801 	mla	r8, r0, r1, r2
   dd78c:	b094      	sub	sp, #80	; 0x50
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  if (HaveSameShapes(input1, input2)) {
   dd78e:	4629      	mov	r1, r5
   dd790:	4620      	mov	r0, r4
   dd792:	f006 faf5 	bl	e3d80 <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>
   dd796:	ae0f      	add	r6, sp, #60	; 0x3c
    reference_ops::BinaryFunction<bool, bool, bool>(
        GetTensorShape(input1), GetTensorData<bool>(input1),
   dd798:	4621      	mov	r1, r4
                         bool (*func)(bool, bool)) {
  const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);
  const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  if (HaveSameShapes(input1, input2)) {
   dd79a:	b1f8      	cbz	r0, dd7dc <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x70>
    reference_ops::BinaryFunction<bool, bool, bool>(
        GetTensorShape(input1), GetTensorData<bool>(input1),
   dd79c:	a805      	add	r0, sp, #20
   dd79e:	f7f9 f908 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dd7a2:	b104      	cbz	r4, dd7a6 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x3a>
   dd7a4:	6864      	ldr	r4, [r4, #4]
        GetTensorShape(input2), GetTensorData<bool>(input2),
   dd7a6:	4629      	mov	r1, r5
   dd7a8:	a80a      	add	r0, sp, #40	; 0x28
   dd7aa:	f7f9 f902 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd7ae:	b105      	cbz	r5, dd7b2 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x46>
   dd7b0:	686d      	ldr	r5, [r5, #4]
        GetTensorShape(output), GetTensorData<bool>(output), func);
   dd7b2:	4641      	mov	r1, r8
   dd7b4:	4630      	mov	r0, r6
   dd7b6:	f7f9 f8fc 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dd7ba:	f1b8 0f00 	cmp.w	r8, #0
   dd7be:	d002      	beq.n	dd7c6 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x5a>
   dd7c0:	f8d8 2004 	ldr.w	r2, [r8, #4]
   dd7c4:	e000      	b.n	dd7c8 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x5c>
   dd7c6:	4642      	mov	r2, r8
   dd7c8:	9201      	str	r2, [sp, #4]
   dd7ca:	9702      	str	r7, [sp, #8]
   dd7cc:	9600      	str	r6, [sp, #0]
   dd7ce:	462b      	mov	r3, r5
   dd7d0:	aa0a      	add	r2, sp, #40	; 0x28
   dd7d2:	4621      	mov	r1, r4
   dd7d4:	a805      	add	r0, sp, #20
   dd7d6:	f7ff fe97 	bl	dd508 <_ZN6tflite13reference_ops14BinaryFunctionIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E>
   dd7da:	e01e      	b.n	dd81a <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0xae>
  } else {
    reference_ops::BroadcastBinaryFunction4DSlow<bool, bool, bool>(
        GetTensorShape(input1), GetTensorData<bool>(input1),
   dd7dc:	a805      	add	r0, sp, #20
   dd7de:	f7f9 f8e8 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dd7e2:	b104      	cbz	r4, dd7e6 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x7a>
   dd7e4:	6864      	ldr	r4, [r4, #4]
        GetTensorShape(input2), GetTensorData<bool>(input2),
   dd7e6:	4629      	mov	r1, r5
   dd7e8:	a80a      	add	r0, sp, #40	; 0x28
   dd7ea:	f7f9 f8e2 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   dd7ee:	b105      	cbz	r5, dd7f2 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x86>
   dd7f0:	686d      	ldr	r5, [r5, #4]
        GetTensorShape(output), GetTensorData<bool>(output), func);
   dd7f2:	4641      	mov	r1, r8
   dd7f4:	4630      	mov	r0, r6
   dd7f6:	f7f9 f8dc 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dd7fa:	f1b8 0f00 	cmp.w	r8, #0
   dd7fe:	d002      	beq.n	dd806 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x9a>
   dd800:	f8d8 1004 	ldr.w	r1, [r8, #4]
   dd804:	e000      	b.n	dd808 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE+0x9c>
   dd806:	4641      	mov	r1, r8
   dd808:	9101      	str	r1, [sp, #4]
   dd80a:	9702      	str	r7, [sp, #8]
   dd80c:	9600      	str	r6, [sp, #0]
   dd80e:	462b      	mov	r3, r5
   dd810:	aa0a      	add	r2, sp, #40	; 0x28
   dd812:	4621      	mov	r1, r4
   dd814:	a805      	add	r0, sp, #20
   dd816:	f7ff feca 	bl	dd5ae <_ZN6tflite13reference_ops29BroadcastBinaryFunction4DSlowIbbbEEvRKNS_12RuntimeShapeEPKT0_S4_PKT1_S4_PT_PFSB_S5_S8_E>
   dd81a:	4630      	mov	r0, r6
   dd81c:	f7f8 fe19 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
        GetTensorShape(input2), GetTensorData<bool>(input2),
        GetTensorShape(output), GetTensorData<bool>(output), func);
  } else {
    reference_ops::BroadcastBinaryFunction4DSlow<bool, bool, bool>(
        GetTensorShape(input1), GetTensorData<bool>(input1),
        GetTensorShape(input2), GetTensorData<bool>(input2),
   dd820:	a80a      	add	r0, sp, #40	; 0x28
   dd822:	f7f8 fe16 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
        GetTensorShape(input1), GetTensorData<bool>(input1),
        GetTensorShape(input2), GetTensorData<bool>(input2),
        GetTensorShape(output), GetTensorData<bool>(output), func);
  } else {
    reference_ops::BroadcastBinaryFunction4DSlow<bool, bool, bool>(
        GetTensorShape(input1), GetTensorData<bool>(input1),
   dd826:	a805      	add	r0, sp, #20
   dd828:	f7f8 fe13 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
        GetTensorShape(input2), GetTensorData<bool>(input2),
        GetTensorShape(output), GetTensorData<bool>(output), func);
  }

  return kTfLiteOk;
}
   dd82c:	2000      	movs	r0, #0
   dd82e:	b014      	add	sp, #80	; 0x50
   dd830:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}

000dd834 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_113LogicalOrEvalEP13TfLiteContextP10TfLiteNode>:

bool LogicalOr(bool x, bool y) { return x || y; }

TfLiteStatus LogicalOrEval(TfLiteContext* context, TfLiteNode* node) {
  return LogicalImpl(context, node, LogicalOr);
   dd834:	4a01      	ldr	r2, [pc, #4]	; (dd83c <_ZN6tflite3ops5micro7logical12_GLOBAL__N_113LogicalOrEvalEP13TfLiteContextP10TfLiteNode+0x8>)
   dd836:	f7ff bf99 	b.w	dd76c <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE>
   dd83a:	bf00      	nop
   dd83c:	000dd4e5 	.word	0x000dd4e5

000dd840 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_114LogicalAndEvalEP13TfLiteContextP10TfLiteNode>:
}

bool LogicalAnd(bool x, bool y) { return x && y; }

TfLiteStatus LogicalAndEval(TfLiteContext* context, TfLiteNode* node) {
  return LogicalImpl(context, node, LogicalAnd);
   dd840:	4a01      	ldr	r2, [pc, #4]	; (dd848 <_ZN6tflite3ops5micro7logical12_GLOBAL__N_114LogicalAndEvalEP13TfLiteContextP10TfLiteNode+0x8>)
   dd842:	f7ff bf93 	b.w	dd76c <_ZN6tflite3ops5micro7logical12_GLOBAL__N_111LogicalImplEP13TfLiteContextP10TfLiteNodePFbbbE>
   dd846:	bf00      	nop
   dd848:	000dd4ef 	.word	0x000dd4ef

000dd84c <_ZN6tflite3ops5micro11activations7PrepareEP13TfLiteContextP10TfLiteNode>:
constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   dd84c:	2000      	movs	r0, #0
   dd84e:	4770      	bx	lr

000dd850 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf>:

namespace tflite {
namespace reference_ops {

inline void Logistic(const RuntimeShape& input_shape, const float* input_data,
                     const RuntimeShape& output_shape, float* output_data) {
   dd850:	e92d 47f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, lr}
   dd854:	ed2d 8b02 	vpush	{d8}
   dd858:	461e      	mov	r6, r3
   dd85a:	f8d0 8000 	ldr.w	r8, [r0]

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   dd85e:	6813      	ldr	r3, [r2, #0]
   dd860:	4598      	cmp	r8, r3
   dd862:	4604      	mov	r4, r0
   dd864:	460f      	mov	r7, r1
   dd866:	4691      	mov	r9, r2
   dd868:	d101      	bne.n	dd86e <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x1e>
   dd86a:	2500      	movs	r5, #0
   dd86c:	e00d      	b.n	dd88a <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x3a>
   dd86e:	f006 fd6d 	bl	e434c <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   dd872:	4629      	mov	r1, r5
   dd874:	4620      	mov	r0, r4
   dd876:	f7f8 fdf7 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dd87a:	4629      	mov	r1, r5
   dd87c:	4682      	mov	sl, r0
   dd87e:	4648      	mov	r0, r9
   dd880:	f7f8 fdf2 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dd884:	4582      	cmp	sl, r0
   dd886:	d1f2      	bne.n	dd86e <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x1e>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   dd888:	3501      	adds	r5, #1
   dd88a:	45a8      	cmp	r8, r5
   dd88c:	dcf1      	bgt.n	dd872 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x22>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   dd88e:	f1b8 0f04 	cmp.w	r8, #4
   dd892:	bfcc      	ite	gt
   dd894:	6864      	ldrgt	r4, [r4, #4]
   dd896:	3404      	addle	r4, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dd898:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
   dd89a:	f04f 0901 	mov.w	r9, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dd89e:	4598      	cmp	r8, r3
   dd8a0:	dd05      	ble.n	dd8ae <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x5e>
      buffer_size *= dims_data[i];
   dd8a2:	f854 2023 	ldr.w	r2, [r4, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dd8a6:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
   dd8a8:	fb02 f909 	mul.w	r9, r2, r9
   dd8ac:	e7f7      	b.n	dd89e <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x4e>
   dd8ae:	4634      	mov	r4, r6
   dd8b0:	463d      	mov	r5, r7
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dd8b2:	2600      	movs	r6, #0
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; i++) {
    float val = input_data[i];
    float result = 1.f / (1.f + std::exp(-val));
    output_data[i] = result;
   dd8b4:	eeb7 8a00 	vmov.f32	s16, #112	; 0x3f800000  1.0

inline void Logistic(const RuntimeShape& input_shape, const float* input_data,
                     const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; i++) {
   dd8b8:	454e      	cmp	r6, r9
   dd8ba:	da0d      	bge.n	dd8d8 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x88>
    float val = input_data[i];
   dd8bc:	ecb5 0a01 	vldmia	r5!, {s0}
  using ::exp;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  exp(float __x)
  { return __builtin_expf(__x); }
   dd8c0:	eeb1 0a40 	vneg.f32	s0, s0
   dd8c4:	f007 ff34 	bl	e5730 <expf>
    float result = 1.f / (1.f + std::exp(-val));
    output_data[i] = result;
   dd8c8:	ee30 0a08 	vadd.f32	s0, s0, s16

inline void Logistic(const RuntimeShape& input_shape, const float* input_data,
                     const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; i++) {
   dd8cc:	3601      	adds	r6, #1
    float val = input_data[i];
    float result = 1.f / (1.f + std::exp(-val));
    output_data[i] = result;
   dd8ce:	eec8 7a00 	vdiv.f32	s15, s16, s0
   dd8d2:	ece4 7a01 	vstmia	r4!, {s15}

inline void Logistic(const RuntimeShape& input_shape, const float* input_data,
                     const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; i++) {
   dd8d6:	e7ef      	b.n	dd8b8 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf+0x68>
    float val = input_data[i];
    float result = 1.f / (1.f + std::exp(-val));
    output_data[i] = result;
  }
}
   dd8d8:	ecbd 8b02 	vpop	{d8}
   dd8dc:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}

000dd8e0 <_ZN6tflite3ops5micro11activations4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dd8e0:	b570      	push	{r4, r5, r6, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dd8e2:	680a      	ldr	r2, [r1, #0]
   dd8e4:	6883      	ldr	r3, [r0, #8]
   dd8e6:	6852      	ldr	r2, [r2, #4]
   dd8e8:	2438      	movs	r4, #56	; 0x38
   dd8ea:	4362      	muls	r2, r4
   dd8ec:	4605      	mov	r5, r0
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (input->type) {
   dd8ee:	5c98      	ldrb	r0, [r3, r2]
   dd8f0:	2801      	cmp	r0, #1

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dd8f2:	b08a      	sub	sp, #40	; 0x28
   dd8f4:	eb03 0602 	add.w	r6, r3, r2
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (input->type) {
   dd8f8:	d11d      	bne.n	dd936 <_ZN6tflite3ops5micro11activations4EvalEP13TfLiteContextP10TfLiteNode+0x56>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dd8fa:	684a      	ldr	r2, [r1, #4]
   dd8fc:	6852      	ldr	r2, [r2, #4]
    case kTfLiteFloat32: {
      reference_ops::Logistic(
          GetTensorShape(input), GetTensorData<float>(input),
   dd8fe:	4631      	mov	r1, r6
   dd900:	fb04 3402 	mla	r4, r4, r2, r3
   dd904:	4668      	mov	r0, sp
   dd906:	f7f9 f854 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
          GetTensorShape(output), GetTensorData<float>(output));
   dd90a:	4621      	mov	r1, r4
   dd90c:	a805      	add	r0, sp, #20
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dd90e:	6875      	ldr	r5, [r6, #4]
   dd910:	f7f9 f84f 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dd914:	b10c      	cbz	r4, dd91a <_ZN6tflite3ops5micro11activations4EvalEP13TfLiteContextP10TfLiteNode+0x3a>
   dd916:	6863      	ldr	r3, [r4, #4]
   dd918:	e000      	b.n	dd91c <_ZN6tflite3ops5micro11activations4EvalEP13TfLiteContextP10TfLiteNode+0x3c>
   dd91a:	4623      	mov	r3, r4
   dd91c:	aa05      	add	r2, sp, #20
   dd91e:	4629      	mov	r1, r5
   dd920:	4668      	mov	r0, sp
   dd922:	f7ff ff95 	bl	dd850 <_ZN6tflite13reference_ops8LogisticERKNS_12RuntimeShapeEPKfS3_Pf>
   dd926:	a805      	add	r0, sp, #20
   dd928:	f7f8 fd93 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (input->type) {
    case kTfLiteFloat32: {
      reference_ops::Logistic(
          GetTensorShape(input), GetTensorData<float>(input),
   dd92c:	4668      	mov	r0, sp
   dd92e:	f7f8 fd90 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
          GetTensorShape(output), GetTensorData<float>(output));
      return kTfLiteOk;
   dd932:	2000      	movs	r0, #0
   dd934:	e007      	b.n	dd946 <_ZN6tflite3ops5micro11activations4EvalEP13TfLiteContextP10TfLiteNode+0x66>
    }
    default: {
      // TODO(b/141211002): Also support other data types once we have supported
      // temporary tensors in TFLM.
      context->ReportError(context,
   dd936:	696c      	ldr	r4, [r5, #20]
   dd938:	f7f6 fbf0 	bl	d411c <TfLiteTypeGetName>
                           "Only float32 is supported currently, got %s",
                           TfLiteTypeGetName(input->type));
   dd93c:	4903      	ldr	r1, [pc, #12]	; (dd94c <_ZN6tflite3ops5micro11activations4EvalEP13TfLiteContextP10TfLiteNode+0x6c>)
   dd93e:	4602      	mov	r2, r0
   dd940:	4628      	mov	r0, r5
   dd942:	47a0      	blx	r4
      return kTfLiteError;
   dd944:	2001      	movs	r0, #1
    }
  }
}
   dd946:	b00a      	add	sp, #40	; 0x28
   dd948:	bd70      	pop	{r4, r5, r6, pc}
   dd94a:	bf00      	nop
   dd94c:	000e9eb3 	.word	0x000e9eb3

000dd950 <_ZN6tflite3ops5micro17Register_LOGISTICEv>:
TfLiteRegistration* Register_LOGISTIC() {
  static TfLiteRegistration r = {/*init=*/nullptr,
                                 /*free=*/nullptr, activations::Prepare,
                                 activations::Eval};
  return &r;
}
   dd950:	4800      	ldr	r0, [pc, #0]	; (dd954 <_ZN6tflite3ops5micro17Register_LOGISTICEv+0x4>)
   dd952:	4770      	bx	lr
   dd954:	2003c17c 	.word	0x2003c17c

000dd958 <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MaximumOp2opIfEET_S6_S6_>:
};

struct MaximumOp {
  template <typename data_type>
  static data_type op(data_type el1, data_type el2) {
    return el1 > el2 ? el1 : el2;
   dd958:	eeb4 0ae0 	vcmpe.f32	s0, s1
   dd95c:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
  }
   dd960:	bfd8      	it	le
   dd962:	eeb0 0a60 	vmovle.f32	s0, s1
   dd966:	4770      	bx	lr

000dd968 <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MaximumOp2opIhEET_S6_S6_>:
   dd968:	4288      	cmp	r0, r1
   dd96a:	bf38      	it	cc
   dd96c:	4608      	movcc	r0, r1
   dd96e:	4770      	bx	lr

000dd970 <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MaximumOp2opIaEET_S6_S6_>:
   dd970:	4288      	cmp	r0, r1
   dd972:	bfb8      	it	lt
   dd974:	4608      	movlt	r0, r1
   dd976:	4770      	bx	lr

000dd978 <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MaximumOp2opIlEET_S6_S6_>:
   dd978:	4288      	cmp	r0, r1
   dd97a:	bfb8      	it	lt
   dd97c:	4608      	movlt	r0, r1
   dd97e:	4770      	bx	lr

000dd980 <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MaximumOp2opIxEET_S6_S6_>:
  TfLiteTensor* output;
};

struct MaximumOp {
  template <typename data_type>
  static data_type op(data_type el1, data_type el2) {
   dd980:	b500      	push	{lr}
    return el1 > el2 ? el1 : el2;
   dd982:	4290      	cmp	r0, r2
   dd984:	eb71 0e03 	sbcs.w	lr, r1, r3
   dd988:	bfbc      	itt	lt
   dd98a:	4610      	movlt	r0, r2
   dd98c:	4619      	movlt	r1, r3
  }
   dd98e:	f85d fb04 	ldr.w	pc, [sp], #4

000dd992 <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MinimumOp2opIfEET_S6_S6_>:
};

struct MinimumOp {
  template <typename data_type>
  static data_type op(data_type el1, data_type el2) {
    return el1 < el2 ? el1 : el2;
   dd992:	eeb4 0ae0 	vcmpe.f32	s0, s1
   dd996:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
  }
   dd99a:	bf58      	it	pl
   dd99c:	eeb0 0a60 	vmovpl.f32	s0, s1
   dd9a0:	4770      	bx	lr

000dd9a2 <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MinimumOp2opIhEET_S6_S6_>:
   dd9a2:	4288      	cmp	r0, r1
   dd9a4:	bf28      	it	cs
   dd9a6:	4608      	movcs	r0, r1
   dd9a8:	4770      	bx	lr

000dd9aa <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MinimumOp2opIaEET_S6_S6_>:
   dd9aa:	4288      	cmp	r0, r1
   dd9ac:	bfa8      	it	ge
   dd9ae:	4608      	movge	r0, r1
   dd9b0:	4770      	bx	lr

000dd9b2 <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MinimumOp2opIlEET_S6_S6_>:
   dd9b2:	4288      	cmp	r0, r1
   dd9b4:	bfa8      	it	ge
   dd9b6:	4608      	movge	r0, r1
   dd9b8:	4770      	bx	lr

000dd9ba <_ZN6tflite3ops5micro15maximum_minimum12_GLOBAL__N_19MinimumOp2opIxEET_S6_S6_>:
  }
};

struct MinimumOp {
  template <typename data_type>
  static data_type op(data_type el1, data_type el2) {
   dd9ba:	b500      	push	{lr}
    return el1 < el2 ? el1 : el2;
   dd9bc:	4282      	cmp	r2, r0
   dd9be:	eb73 0e01 	sbcs.w	lr, r3, r1
   dd9c2:	bfbc      	itt	lt
   dd9c4:	4610      	movlt	r0, r2
   dd9c6:	4619      	movlt	r1, r3
  }
   dd9c8:	f85d fb04 	ldr.w	pc, [sp], #4

000dd9cc <_ZN6tflite3ops5micro16Register_MAXIMUMEv>:
      /* free */ nullptr,
      /* prepare */ nullptr,
      maximum_minimum::Eval<maximum_minimum::kReference,
                            maximum_minimum::MaximumOp>};
  return &r;
}
   dd9cc:	4800      	ldr	r0, [pc, #0]	; (dd9d0 <_ZN6tflite3ops5micro16Register_MAXIMUMEv+0x4>)
   dd9ce:	4770      	bx	lr
   dd9d0:	2003c1bc 	.word	0x2003c1bc

000dd9d4 <_ZN6tflite3ops5micro16Register_MINIMUMEv>:
      /* free */ nullptr,
      /* prepare */ nullptr,
      maximum_minimum::Eval<maximum_minimum::kReference,
                            maximum_minimum::MinimumOp>};
  return &r;
}
   dd9d4:	4800      	ldr	r0, [pc, #0]	; (dd9d8 <_ZN6tflite3ops5micro16Register_MINIMUMEv+0x4>)
   dd9d6:	4770      	bx	lr
   dd9d8:	2003c19c 	.word	0x2003c19c

000dd9dc <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>:

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   dd9dc:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dd9e0:	469b      	mov	fp, r3
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   dd9e2:	6803      	ldr	r3, [r0, #0]

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   dd9e4:	b09b      	sub	sp, #108	; 0x6c
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   dd9e6:	2b04      	cmp	r3, #4

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   dd9e8:	4616      	mov	r6, r2
   dd9ea:	4604      	mov	r4, r0
   dd9ec:	9102      	str	r1, [sp, #8]
   dd9ee:	9a24      	ldr	r2, [sp, #144]	; 0x90
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   dd9f0:	dd01      	ble.n	dd9f6 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x1a>
   dd9f2:	f006 fcab 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   dd9f6:	6833      	ldr	r3, [r6, #0]
   dd9f8:	2b04      	cmp	r3, #4
   dd9fa:	dcfa      	bgt.n	dd9f2 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   dd9fc:	6813      	ldr	r3, [r2, #0]
   dd9fe:	2b04      	cmp	r3, #4
   dda00:	dcf7      	bgt.n	dd9f2 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
   dda02:	2301      	movs	r3, #1
   dda04:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   dda06:	ad0a      	add	r5, sp, #40	; 0x28
   dda08:	a805      	add	r0, sp, #20
   dda0a:	f7f8 fd66 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   dda0e:	4620      	mov	r0, r4
   dda10:	ab12      	add	r3, sp, #72	; 0x48
   dda12:	462a      	mov	r2, r5
   dda14:	4631      	mov	r1, r6
   dda16:	f7f9 f875 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   dda1a:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   dda1c:	9503      	str	r5, [sp, #12]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   dda1e:	2100      	movs	r1, #0
   dda20:	a805      	add	r0, sp, #20
   dda22:	f7f8 fd21 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dda26:	4284      	cmp	r4, r0
   dda28:	da49      	bge.n	ddabe <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xe2>
   dda2a:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   dda2c:	af05      	add	r7, sp, #20
   dda2e:	2101      	movs	r1, #1
   dda30:	4638      	mov	r0, r7
   dda32:	f7f8 fd19 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dda36:	4285      	cmp	r5, r0
   dda38:	da3f      	bge.n	ddaba <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xde>
   dda3a:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   dda3c:	2102      	movs	r1, #2
   dda3e:	4638      	mov	r0, r7
   dda40:	f7f8 fd12 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dda44:	4286      	cmp	r6, r0
   dda46:	da36      	bge.n	ddab6 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xda>
   dda48:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dda4c:	2103      	movs	r1, #3
   dda4e:	4638      	mov	r0, r7
   dda50:	f7f8 fd0a 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dda54:	4580      	cmp	r8, r0
   dda56:	da2c      	bge.n	ddab2 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd6>
          auto out_idx = Offset(output_shape, b, y, x, c);
   dda58:	f8cd 8000 	str.w	r8, [sp]
   dda5c:	4633      	mov	r3, r6
   dda5e:	462a      	mov	r2, r5
   dda60:	4621      	mov	r1, r4
   dda62:	4638      	mov	r0, r7
   dda64:	f7f8 fd65 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   dda68:	f8cd 8000 	str.w	r8, [sp]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
   dda6c:	4681      	mov	r9, r0
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   dda6e:	4633      	mov	r3, r6
   dda70:	462a      	mov	r2, r5
   dda72:	4621      	mov	r1, r4
   dda74:	9803      	ldr	r0, [sp, #12]
   dda76:	f7f8 fe0d 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   dda7a:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   dda7e:	4682      	mov	sl, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   dda80:	4633      	mov	r3, r6
   dda82:	462a      	mov	r2, r5
   dda84:	4621      	mov	r1, r4
   dda86:	a812      	add	r0, sp, #72	; 0x48
   dda88:	f7f8 fe04 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   dda8c:	9b25      	ldr	r3, [sp, #148]	; 0x94
   dda8e:	eb03 0989 	add.w	r9, r3, r9, lsl #2
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
   dda92:	9b02      	ldr	r3, [sp, #8]
          auto in2_val = input2_data[in2_idx];
   dda94:	eb0b 0080 	add.w	r0, fp, r0, lsl #2
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
   dda98:	eb03 0a8a 	add.w	sl, r3, sl, lsl #2
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   dda9c:	edd0 0a00 	vldr	s1, [r0]
   ddaa0:	ed9a 0a00 	vldr	s0, [sl]
   ddaa4:	9b26      	ldr	r3, [sp, #152]	; 0x98
   ddaa6:	4798      	blx	r3
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddaa8:	f108 0801 	add.w	r8, r8, #1
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   ddaac:	ed89 0a00 	vstr	s0, [r9]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddab0:	e7cc      	b.n	dda4c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x70>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   ddab2:	3601      	adds	r6, #1
   ddab4:	e7c2      	b.n	dda3c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x60>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   ddab6:	3501      	adds	r5, #1
   ddab8:	e7b8      	b.n	dda2c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x50>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddaba:	3401      	adds	r4, #1
   ddabc:	e7af      	b.n	dda1e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x42>
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   ddabe:	a805      	add	r0, sp, #20
   ddac0:	f7f8 fcc7 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = op(in1_val, in2_val);
        }
      }
    }
  }
}
   ddac4:	b01b      	add	sp, #108	; 0x6c
   ddac6:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000ddaca <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>:

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddaca:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   ddace:	4699      	mov	r9, r3
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddad0:	6803      	ldr	r3, [r0, #0]

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddad2:	b09b      	sub	sp, #108	; 0x6c
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddad4:	2b04      	cmp	r3, #4

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddad6:	4616      	mov	r6, r2
   ddad8:	4604      	mov	r4, r0
   ddada:	9102      	str	r1, [sp, #8]
   ddadc:	9a24      	ldr	r2, [sp, #144]	; 0x90
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddade:	dd01      	ble.n	ddae4 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x1a>
   ddae0:	f006 fc34 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   ddae4:	6833      	ldr	r3, [r6, #0]
   ddae6:	2b04      	cmp	r3, #4
   ddae8:	dcfa      	bgt.n	ddae0 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   ddaea:	6813      	ldr	r3, [r2, #0]
   ddaec:	2b04      	cmp	r3, #4
   ddaee:	dcf7      	bgt.n	ddae0 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
   ddaf0:	2301      	movs	r3, #1
   ddaf2:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   ddaf4:	ad0a      	add	r5, sp, #40	; 0x28
   ddaf6:	a805      	add	r0, sp, #20
   ddaf8:	f7f8 fcef 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   ddafc:	4620      	mov	r0, r4
   ddafe:	ab12      	add	r3, sp, #72	; 0x48
   ddb00:	462a      	mov	r2, r5
   ddb02:	4631      	mov	r1, r6
   ddb04:	f7f8 fffe 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddb08:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddb0a:	9503      	str	r5, [sp, #12]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddb0c:	2100      	movs	r1, #0
   ddb0e:	a805      	add	r0, sp, #20
   ddb10:	f7f8 fcaa 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   ddb14:	4284      	cmp	r4, r0
   ddb16:	da43      	bge.n	ddba0 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd6>
   ddb18:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   ddb1a:	af05      	add	r7, sp, #20
   ddb1c:	2101      	movs	r1, #1
   ddb1e:	4638      	mov	r0, r7
   ddb20:	f7f8 fca2 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   ddb24:	4285      	cmp	r5, r0
   ddb26:	da39      	bge.n	ddb9c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd2>
   ddb28:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   ddb2a:	2102      	movs	r1, #2
   ddb2c:	4638      	mov	r0, r7
   ddb2e:	f7f8 fc9b 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   ddb32:	4286      	cmp	r6, r0
   ddb34:	da30      	bge.n	ddb98 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xce>
   ddb36:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddb3a:	2103      	movs	r1, #3
   ddb3c:	4638      	mov	r0, r7
   ddb3e:	f7f8 fc93 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   ddb42:	4580      	cmp	r8, r0
   ddb44:	da26      	bge.n	ddb94 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xca>
          auto out_idx = Offset(output_shape, b, y, x, c);
   ddb46:	f8cd 8000 	str.w	r8, [sp]
   ddb4a:	4633      	mov	r3, r6
   ddb4c:	462a      	mov	r2, r5
   ddb4e:	4621      	mov	r1, r4
   ddb50:	4638      	mov	r0, r7
   ddb52:	f7f8 fcee 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddb56:	f8cd 8000 	str.w	r8, [sp]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
   ddb5a:	4682      	mov	sl, r0
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddb5c:	4633      	mov	r3, r6
   ddb5e:	462a      	mov	r2, r5
   ddb60:	4621      	mov	r1, r4
   ddb62:	9803      	ldr	r0, [sp, #12]
   ddb64:	f7f8 fd96 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   ddb68:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddb6c:	4683      	mov	fp, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   ddb6e:	4633      	mov	r3, r6
   ddb70:	462a      	mov	r2, r5
   ddb72:	4621      	mov	r1, r4
   ddb74:	a812      	add	r0, sp, #72	; 0x48
   ddb76:	f7f8 fd8d 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   ddb7a:	9b02      	ldr	r3, [sp, #8]
   ddb7c:	f819 1000 	ldrb.w	r1, [r9, r0]
   ddb80:	f813 000b 	ldrb.w	r0, [r3, fp]
   ddb84:	9b26      	ldr	r3, [sp, #152]	; 0x98
   ddb86:	4798      	blx	r3
   ddb88:	9b25      	ldr	r3, [sp, #148]	; 0x94
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddb8a:	f108 0801 	add.w	r8, r8, #1
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   ddb8e:	f803 000a 	strb.w	r0, [r3, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddb92:	e7d2      	b.n	ddb3a <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x70>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   ddb94:	3601      	adds	r6, #1
   ddb96:	e7c8      	b.n	ddb2a <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x60>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   ddb98:	3501      	adds	r5, #1
   ddb9a:	e7be      	b.n	ddb1a <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x50>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddb9c:	3401      	adds	r4, #1
   ddb9e:	e7b5      	b.n	ddb0c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x42>
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   ddba0:	a805      	add	r0, sp, #20
   ddba2:	f7f8 fc56 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = op(in1_val, in2_val);
        }
      }
    }
  }
}
   ddba6:	b01b      	add	sp, #108	; 0x6c
   ddba8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000ddbac <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>:

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddbac:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   ddbb0:	4699      	mov	r9, r3
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddbb2:	6803      	ldr	r3, [r0, #0]

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddbb4:	b09b      	sub	sp, #108	; 0x6c
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddbb6:	2b04      	cmp	r3, #4

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddbb8:	4616      	mov	r6, r2
   ddbba:	4604      	mov	r4, r0
   ddbbc:	9102      	str	r1, [sp, #8]
   ddbbe:	9a24      	ldr	r2, [sp, #144]	; 0x90
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddbc0:	dd01      	ble.n	ddbc6 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x1a>
   ddbc2:	f006 fbc3 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   ddbc6:	6833      	ldr	r3, [r6, #0]
   ddbc8:	2b04      	cmp	r3, #4
   ddbca:	dcfa      	bgt.n	ddbc2 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   ddbcc:	6813      	ldr	r3, [r2, #0]
   ddbce:	2b04      	cmp	r3, #4
   ddbd0:	dcf7      	bgt.n	ddbc2 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
   ddbd2:	2301      	movs	r3, #1
   ddbd4:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   ddbd6:	ad0a      	add	r5, sp, #40	; 0x28
   ddbd8:	a805      	add	r0, sp, #20
   ddbda:	f7f8 fc7e 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   ddbde:	4620      	mov	r0, r4
   ddbe0:	ab12      	add	r3, sp, #72	; 0x48
   ddbe2:	462a      	mov	r2, r5
   ddbe4:	4631      	mov	r1, r6
   ddbe6:	f7f8 ff8d 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddbea:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddbec:	9503      	str	r5, [sp, #12]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddbee:	2100      	movs	r1, #0
   ddbf0:	a805      	add	r0, sp, #20
   ddbf2:	f7f8 fc39 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   ddbf6:	4284      	cmp	r4, r0
   ddbf8:	da43      	bge.n	ddc82 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd6>
   ddbfa:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   ddbfc:	af05      	add	r7, sp, #20
   ddbfe:	2101      	movs	r1, #1
   ddc00:	4638      	mov	r0, r7
   ddc02:	f7f8 fc31 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   ddc06:	4285      	cmp	r5, r0
   ddc08:	da39      	bge.n	ddc7e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd2>
   ddc0a:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   ddc0c:	2102      	movs	r1, #2
   ddc0e:	4638      	mov	r0, r7
   ddc10:	f7f8 fc2a 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   ddc14:	4286      	cmp	r6, r0
   ddc16:	da30      	bge.n	ddc7a <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xce>
   ddc18:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddc1c:	2103      	movs	r1, #3
   ddc1e:	4638      	mov	r0, r7
   ddc20:	f7f8 fc22 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   ddc24:	4580      	cmp	r8, r0
   ddc26:	da26      	bge.n	ddc76 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xca>
          auto out_idx = Offset(output_shape, b, y, x, c);
   ddc28:	f8cd 8000 	str.w	r8, [sp]
   ddc2c:	4633      	mov	r3, r6
   ddc2e:	462a      	mov	r2, r5
   ddc30:	4621      	mov	r1, r4
   ddc32:	4638      	mov	r0, r7
   ddc34:	f7f8 fc7d 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddc38:	f8cd 8000 	str.w	r8, [sp]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
   ddc3c:	4682      	mov	sl, r0
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddc3e:	4633      	mov	r3, r6
   ddc40:	462a      	mov	r2, r5
   ddc42:	4621      	mov	r1, r4
   ddc44:	9803      	ldr	r0, [sp, #12]
   ddc46:	f7f8 fd25 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   ddc4a:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddc4e:	4683      	mov	fp, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   ddc50:	4633      	mov	r3, r6
   ddc52:	462a      	mov	r2, r5
   ddc54:	4621      	mov	r1, r4
   ddc56:	a812      	add	r0, sp, #72	; 0x48
   ddc58:	f7f8 fd1c 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   ddc5c:	9b02      	ldr	r3, [sp, #8]
   ddc5e:	f919 1000 	ldrsb.w	r1, [r9, r0]
   ddc62:	f913 000b 	ldrsb.w	r0, [r3, fp]
   ddc66:	9b26      	ldr	r3, [sp, #152]	; 0x98
   ddc68:	4798      	blx	r3
   ddc6a:	9b25      	ldr	r3, [sp, #148]	; 0x94
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddc6c:	f108 0801 	add.w	r8, r8, #1
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   ddc70:	f803 000a 	strb.w	r0, [r3, sl]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddc74:	e7d2      	b.n	ddc1c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x70>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   ddc76:	3601      	adds	r6, #1
   ddc78:	e7c8      	b.n	ddc0c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x60>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   ddc7a:	3501      	adds	r5, #1
   ddc7c:	e7be      	b.n	ddbfc <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x50>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddc7e:	3401      	adds	r4, #1
   ddc80:	e7b5      	b.n	ddbee <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x42>
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   ddc82:	a805      	add	r0, sp, #20
   ddc84:	f7f8 fbe5 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = op(in1_val, in2_val);
        }
      }
    }
  }
}
   ddc88:	b01b      	add	sp, #108	; 0x6c
   ddc8a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000ddc8e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>:

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddc8e:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   ddc92:	4699      	mov	r9, r3
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddc94:	6803      	ldr	r3, [r0, #0]

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddc96:	b09b      	sub	sp, #108	; 0x6c
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddc98:	2b04      	cmp	r3, #4

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddc9a:	4616      	mov	r6, r2
   ddc9c:	4604      	mov	r4, r0
   ddc9e:	9102      	str	r1, [sp, #8]
   ddca0:	9a24      	ldr	r2, [sp, #144]	; 0x90
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddca2:	dd01      	ble.n	ddca8 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x1a>
   ddca4:	f006 fb52 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   ddca8:	6833      	ldr	r3, [r6, #0]
   ddcaa:	2b04      	cmp	r3, #4
   ddcac:	dcfa      	bgt.n	ddca4 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   ddcae:	6813      	ldr	r3, [r2, #0]
   ddcb0:	2b04      	cmp	r3, #4
   ddcb2:	dcf7      	bgt.n	ddca4 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
   ddcb4:	2301      	movs	r3, #1
   ddcb6:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   ddcb8:	ad0a      	add	r5, sp, #40	; 0x28
   ddcba:	a805      	add	r0, sp, #20
   ddcbc:	f7f8 fc0d 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   ddcc0:	4620      	mov	r0, r4
   ddcc2:	ab12      	add	r3, sp, #72	; 0x48
   ddcc4:	462a      	mov	r2, r5
   ddcc6:	4631      	mov	r1, r6
   ddcc8:	f7f8 ff1c 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddccc:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddcce:	9503      	str	r5, [sp, #12]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddcd0:	2100      	movs	r1, #0
   ddcd2:	a805      	add	r0, sp, #20
   ddcd4:	f7f8 fbc8 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   ddcd8:	4284      	cmp	r4, r0
   ddcda:	da43      	bge.n	ddd64 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd6>
   ddcdc:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   ddcde:	af05      	add	r7, sp, #20
   ddce0:	2101      	movs	r1, #1
   ddce2:	4638      	mov	r0, r7
   ddce4:	f7f8 fbc0 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   ddce8:	4285      	cmp	r5, r0
   ddcea:	da39      	bge.n	ddd60 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd2>
   ddcec:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   ddcee:	2102      	movs	r1, #2
   ddcf0:	4638      	mov	r0, r7
   ddcf2:	f7f8 fbb9 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   ddcf6:	4286      	cmp	r6, r0
   ddcf8:	da30      	bge.n	ddd5c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xce>
   ddcfa:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddcfe:	2103      	movs	r1, #3
   ddd00:	4638      	mov	r0, r7
   ddd02:	f7f8 fbb1 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   ddd06:	4580      	cmp	r8, r0
   ddd08:	da26      	bge.n	ddd58 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xca>
          auto out_idx = Offset(output_shape, b, y, x, c);
   ddd0a:	f8cd 8000 	str.w	r8, [sp]
   ddd0e:	4633      	mov	r3, r6
   ddd10:	462a      	mov	r2, r5
   ddd12:	4621      	mov	r1, r4
   ddd14:	4638      	mov	r0, r7
   ddd16:	f7f8 fc0c 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddd1a:	f8cd 8000 	str.w	r8, [sp]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
   ddd1e:	4682      	mov	sl, r0
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddd20:	4633      	mov	r3, r6
   ddd22:	462a      	mov	r2, r5
   ddd24:	4621      	mov	r1, r4
   ddd26:	9803      	ldr	r0, [sp, #12]
   ddd28:	f7f8 fcb4 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   ddd2c:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   ddd30:	4683      	mov	fp, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   ddd32:	4633      	mov	r3, r6
   ddd34:	462a      	mov	r2, r5
   ddd36:	4621      	mov	r1, r4
   ddd38:	a812      	add	r0, sp, #72	; 0x48
   ddd3a:	f7f8 fcab 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   ddd3e:	9b02      	ldr	r3, [sp, #8]
   ddd40:	f859 1020 	ldr.w	r1, [r9, r0, lsl #2]
   ddd44:	f853 002b 	ldr.w	r0, [r3, fp, lsl #2]
   ddd48:	9b26      	ldr	r3, [sp, #152]	; 0x98
   ddd4a:	4798      	blx	r3
   ddd4c:	9b25      	ldr	r3, [sp, #148]	; 0x94
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddd4e:	f108 0801 	add.w	r8, r8, #1
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   ddd52:	f843 002a 	str.w	r0, [r3, sl, lsl #2]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddd56:	e7d2      	b.n	ddcfe <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x70>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   ddd58:	3601      	adds	r6, #1
   ddd5a:	e7c8      	b.n	ddcee <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x60>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   ddd5c:	3501      	adds	r5, #1
   ddd5e:	e7be      	b.n	ddcde <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x50>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   ddd60:	3401      	adds	r4, #1
   ddd62:	e7b5      	b.n	ddcd0 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x42>
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   ddd64:	a805      	add	r0, sp, #20
   ddd66:	f7f8 fb74 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = op(in1_val, in2_val);
        }
      }
    }
  }
}
   ddd6a:	b01b      	add	sp, #108	; 0x6c
   ddd6c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000ddd70 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>:

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddd70:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   ddd74:	469b      	mov	fp, r3
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddd76:	6803      	ldr	r3, [r0, #0]

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddd78:	b09d      	sub	sp, #116	; 0x74
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddd7a:	2b04      	cmp	r3, #4

namespace tflite {
namespace reference_ops {

template <typename T, typename Op>
void MaximumMinimumBroadcast4DSlow(const RuntimeShape& unextended_input1_shape,
   ddd7c:	4616      	mov	r6, r2
   ddd7e:	4604      	mov	r4, r0
   ddd80:	9104      	str	r1, [sp, #16]
   ddd82:	9a26      	ldr	r2, [sp, #152]	; 0x98
                                   const T* input1_data,
                                   const RuntimeShape& unextended_input2_shape,
                                   const T* input2_data,
                                   const RuntimeShape& unextended_output_shape,
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   ddd84:	dd01      	ble.n	ddd8a <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x1a>
   ddd86:	f006 fae1 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   ddd8a:	6833      	ldr	r3, [r6, #0]
   ddd8c:	2b04      	cmp	r3, #4
   ddd8e:	dcfa      	bgt.n	ddd86 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   ddd90:	6813      	ldr	r3, [r2, #0]
   ddd92:	2b04      	cmp	r3, #4
   ddd94:	dcf7      	bgt.n	ddd86 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x16>
   ddd96:	2301      	movs	r3, #1
   ddd98:	2104      	movs	r1, #4
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
   ddd9a:	ad0c      	add	r5, sp, #48	; 0x30
   ddd9c:	a807      	add	r0, sp, #28
   ddd9e:	f7f8 fb9c 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   ddda2:	4620      	mov	r0, r4
   ddda4:	ab14      	add	r3, sp, #80	; 0x50
   ddda6:	462a      	mov	r2, r5
   ddda8:	4631      	mov	r1, r6
   dddaa:	f7f8 feab 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   dddae:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   dddb0:	9505      	str	r5, [sp, #20]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   dddb2:	2100      	movs	r1, #0
   dddb4:	a807      	add	r0, sp, #28
   dddb6:	f7f8 fb57 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dddba:	4284      	cmp	r4, r0
   dddbc:	da4a      	bge.n	dde54 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xe4>
   dddbe:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   dddc0:	af07      	add	r7, sp, #28
   dddc2:	2101      	movs	r1, #1
   dddc4:	4638      	mov	r0, r7
   dddc6:	f7f8 fb4f 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dddca:	4285      	cmp	r5, r0
   dddcc:	da40      	bge.n	dde50 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xe0>
   dddce:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   dddd0:	9703      	str	r7, [sp, #12]
   dddd2:	2102      	movs	r1, #2
   dddd4:	9803      	ldr	r0, [sp, #12]
   dddd6:	f7f8 fb47 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dddda:	4286      	cmp	r6, r0
   ddddc:	da36      	bge.n	dde4c <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xdc>
   dddde:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   ddde2:	2103      	movs	r1, #3
   ddde4:	9803      	ldr	r0, [sp, #12]
   ddde6:	f7f8 fb3f 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dddea:	4580      	cmp	r8, r0
   dddec:	da2c      	bge.n	dde48 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0xd8>
          auto out_idx = Offset(output_shape, b, y, x, c);
   dddee:	f8cd 8000 	str.w	r8, [sp]
   dddf2:	4633      	mov	r3, r6
   dddf4:	462a      	mov	r2, r5
   dddf6:	4621      	mov	r1, r4
   dddf8:	9803      	ldr	r0, [sp, #12]
   dddfa:	f7f8 fb9a 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   dddfe:	f8cd 8000 	str.w	r8, [sp]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
   dde02:	4681      	mov	r9, r0
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   dde04:	4633      	mov	r3, r6
   dde06:	462a      	mov	r2, r5
   dde08:	4621      	mov	r1, r4
   dde0a:	9805      	ldr	r0, [sp, #20]
   dde0c:	f7f8 fc42 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   dde10:	f8cd 8000 	str.w	r8, [sp]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   dde14:	4682      	mov	sl, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   dde16:	4633      	mov	r3, r6
   dde18:	462a      	mov	r2, r5
   dde1a:	4621      	mov	r1, r4
   dde1c:	a814      	add	r0, sp, #80	; 0x50
   dde1e:	f7f8 fc39 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   dde22:	9b27      	ldr	r3, [sp, #156]	; 0x9c
   dde24:	9f28      	ldr	r7, [sp, #160]	; 0xa0
   dde26:	eb03 09c9 	add.w	r9, r3, r9, lsl #3
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
   dde2a:	9b04      	ldr	r3, [sp, #16]
          auto in2_val = input2_data[in2_idx];
   dde2c:	eb0b 00c0 	add.w	r0, fp, r0, lsl #3
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
   dde30:	eb03 0aca 	add.w	sl, r3, sl, lsl #3
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   dde34:	e9d0 2300 	ldrd	r2, r3, [r0]
   dde38:	e9da 0100 	ldrd	r0, r1, [sl]
   dde3c:	47b8      	blx	r7
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dde3e:	f108 0801 	add.w	r8, r8, #1
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = op(in1_val, in2_val);
   dde42:	e9c9 0100 	strd	r0, r1, [r9]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   dde46:	e7cc      	b.n	ddde2 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x72>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   dde48:	3601      	adds	r6, #1
   dde4a:	e7c2      	b.n	dddd2 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x62>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   dde4c:	3501      	adds	r5, #1
   dde4e:	e7b7      	b.n	dddc0 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x50>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   dde50:	3401      	adds	r4, #1
   dde52:	e7ae      	b.n	dddb2 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_+0x42>
                                   T* output_data, Op op) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   dde54:	a807      	add	r0, sp, #28
   dde56:	f7f8 fafc 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = op(in1_val, in2_val);
        }
      }
    }
  }
}
   dde5a:	b01d      	add	sp, #116	; 0x74
   dde5c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dde60 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode>:
      GetTensorData<data_type>(op_context.output),
      op_type::template op<data_type>);
}

template <KernelType kernel_type, typename OpType>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dde60:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
   dde64:	680b      	ldr	r3, [r1, #0]
   dde66:	f8d0 8008 	ldr.w	r8, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dde6a:	685c      	ldr	r4, [r3, #4]
   dde6c:	689d      	ldr	r5, [r3, #8]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dde6e:	684b      	ldr	r3, [r1, #4]
   dde70:	685f      	ldr	r7, [r3, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dde72:	2238      	movs	r2, #56	; 0x38
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dde74:	4357      	muls	r7, r2
   dde76:	4681      	mov	r9, r0
  OpContext op_context(context, node);

  if (kernel_type == kReference) {
    switch (op_context.output->type) {
   dde78:	f818 0007 	ldrb.w	r0, [r8, r7]
   dde7c:	1e43      	subs	r3, r0, #1
      GetTensorData<data_type>(op_context.output),
      op_type::template op<data_type>);
}

template <KernelType kernel_type, typename OpType>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dde7e:	b095      	sub	sp, #84	; 0x54
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dde80:	fb02 8404 	mla	r4, r2, r4, r8
   dde84:	fb02 8505 	mla	r5, r2, r5, r8
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dde88:	eb08 0607 	add.w	r6, r8, r7
  OpContext op_context(context, node);

  if (kernel_type == kReference) {
    switch (op_context.output->type) {
   dde8c:	2b08      	cmp	r3, #8
   dde8e:	f200 80a2 	bhi.w	ddfd6 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x176>
   dde92:	e8df f003 	tbb	[pc, r3]
   dde96:	5c05      	.short	0x5c05
   dde98:	a0a07922 	.word	0xa0a07922
   dde9c:	a0a0      	.short	0xa0a0
   dde9e:	3f          	.byte	0x3f
   dde9f:	00          	.byte	0x00
}  // namespace

template <typename data_type, typename op_type>
void TFLiteOperation(TfLiteContext* context, TfLiteNode* node,
                     const OpContext& op_context) {
  reference_ops::MaximumMinimumBroadcast4DSlow(
   ddea0:	4621      	mov	r1, r4
   ddea2:	a80f      	add	r0, sp, #60	; 0x3c
   ddea4:	f7f8 fd85 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   ddea8:	b104      	cbz	r4, ddeac <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x4c>
   ddeaa:	6864      	ldr	r4, [r4, #4]
   ddeac:	4629      	mov	r1, r5
   ddeae:	a80a      	add	r0, sp, #40	; 0x28
   ddeb0:	f7f8 fd7f 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddeb4:	b105      	cbz	r5, ddeb8 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x58>
   ddeb6:	686d      	ldr	r5, [r5, #4]
   ddeb8:	af05      	add	r7, sp, #20
   ddeba:	4631      	mov	r1, r6
   ddebc:	4638      	mov	r0, r7
   ddebe:	f7f8 fd78 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddec2:	4b4c      	ldr	r3, [pc, #304]	; (ddff4 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x194>)
   ddec4:	9302      	str	r3, [sp, #8]
   ddec6:	6873      	ldr	r3, [r6, #4]
   ddec8:	9301      	str	r3, [sp, #4]
   ddeca:	9700      	str	r7, [sp, #0]
   ddecc:	462b      	mov	r3, r5
   ddece:	aa0a      	add	r2, sp, #40	; 0x28
   dded0:	4621      	mov	r1, r4
   dded2:	a80f      	add	r0, sp, #60	; 0x3c
   dded4:	f7ff fd82 	bl	dd9dc <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   dded8:	e072      	b.n	ddfc0 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
   ddeda:	4621      	mov	r1, r4
   ddedc:	a80f      	add	r0, sp, #60	; 0x3c
   ddede:	f7f8 fd68 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddee2:	b104      	cbz	r4, ddee6 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x86>
   ddee4:	6864      	ldr	r4, [r4, #4]
   ddee6:	4629      	mov	r1, r5
   ddee8:	a80a      	add	r0, sp, #40	; 0x28
   ddeea:	f7f8 fd62 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddeee:	b105      	cbz	r5, ddef2 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x92>
   ddef0:	686d      	ldr	r5, [r5, #4]
   ddef2:	af05      	add	r7, sp, #20
   ddef4:	4631      	mov	r1, r6
   ddef6:	4638      	mov	r0, r7
   ddef8:	f7f8 fd5b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddefc:	4b3e      	ldr	r3, [pc, #248]	; (ddff8 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x198>)
   ddefe:	9302      	str	r3, [sp, #8]
   ddf00:	6873      	ldr	r3, [r6, #4]
   ddf02:	9301      	str	r3, [sp, #4]
   ddf04:	9700      	str	r7, [sp, #0]
   ddf06:	462b      	mov	r3, r5
   ddf08:	aa0a      	add	r2, sp, #40	; 0x28
   ddf0a:	4621      	mov	r1, r4
   ddf0c:	a80f      	add	r0, sp, #60	; 0x3c
   ddf0e:	f7ff fddc 	bl	ddaca <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   ddf12:	e055      	b.n	ddfc0 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
   ddf14:	4621      	mov	r1, r4
   ddf16:	a80f      	add	r0, sp, #60	; 0x3c
   ddf18:	f7f8 fd4b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddf1c:	b104      	cbz	r4, ddf20 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xc0>
   ddf1e:	6864      	ldr	r4, [r4, #4]
   ddf20:	4629      	mov	r1, r5
   ddf22:	a80a      	add	r0, sp, #40	; 0x28
   ddf24:	f7f8 fd45 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddf28:	b105      	cbz	r5, ddf2c <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xcc>
   ddf2a:	686d      	ldr	r5, [r5, #4]
   ddf2c:	af05      	add	r7, sp, #20
   ddf2e:	4631      	mov	r1, r6
   ddf30:	4638      	mov	r0, r7
   ddf32:	f7f8 fd3e 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddf36:	4b31      	ldr	r3, [pc, #196]	; (ddffc <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x19c>)
   ddf38:	9302      	str	r3, [sp, #8]
   ddf3a:	6873      	ldr	r3, [r6, #4]
   ddf3c:	9301      	str	r3, [sp, #4]
   ddf3e:	9700      	str	r7, [sp, #0]
   ddf40:	462b      	mov	r3, r5
   ddf42:	aa0a      	add	r2, sp, #40	; 0x28
   ddf44:	4621      	mov	r1, r4
   ddf46:	a80f      	add	r0, sp, #60	; 0x3c
   ddf48:	f7ff fe30 	bl	ddbac <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   ddf4c:	e038      	b.n	ddfc0 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
   ddf4e:	4621      	mov	r1, r4
   ddf50:	a80f      	add	r0, sp, #60	; 0x3c
   ddf52:	f7f8 fd2e 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddf56:	b104      	cbz	r4, ddf5a <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xfa>
   ddf58:	6864      	ldr	r4, [r4, #4]
   ddf5a:	4629      	mov	r1, r5
   ddf5c:	a80a      	add	r0, sp, #40	; 0x28
   ddf5e:	f7f8 fd28 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddf62:	b105      	cbz	r5, ddf66 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x106>
   ddf64:	686d      	ldr	r5, [r5, #4]
   ddf66:	af05      	add	r7, sp, #20
   ddf68:	4631      	mov	r1, r6
   ddf6a:	4638      	mov	r0, r7
   ddf6c:	f7f8 fd21 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddf70:	4b23      	ldr	r3, [pc, #140]	; (de000 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x1a0>)
   ddf72:	9302      	str	r3, [sp, #8]
   ddf74:	6873      	ldr	r3, [r6, #4]
   ddf76:	9301      	str	r3, [sp, #4]
   ddf78:	9700      	str	r7, [sp, #0]
   ddf7a:	462b      	mov	r3, r5
   ddf7c:	aa0a      	add	r2, sp, #40	; 0x28
   ddf7e:	4621      	mov	r1, r4
   ddf80:	a80f      	add	r0, sp, #60	; 0x3c
   ddf82:	f7ff fe84 	bl	ddc8e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   ddf86:	e01b      	b.n	ddfc0 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
   ddf88:	4621      	mov	r1, r4
   ddf8a:	a80f      	add	r0, sp, #60	; 0x3c
   ddf8c:	f7f8 fd11 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddf90:	b104      	cbz	r4, ddf94 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x134>
   ddf92:	6864      	ldr	r4, [r4, #4]
   ddf94:	4629      	mov	r1, r5
   ddf96:	a80a      	add	r0, sp, #40	; 0x28
   ddf98:	f7f8 fd0b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddf9c:	b105      	cbz	r5, ddfa0 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x140>
   ddf9e:	686d      	ldr	r5, [r5, #4]
   ddfa0:	af05      	add	r7, sp, #20
   ddfa2:	4631      	mov	r1, r6
   ddfa4:	4638      	mov	r0, r7
   ddfa6:	f7f8 fd04 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   ddfaa:	4b16      	ldr	r3, [pc, #88]	; (de004 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x1a4>)
   ddfac:	9302      	str	r3, [sp, #8]
   ddfae:	6873      	ldr	r3, [r6, #4]
   ddfb0:	9301      	str	r3, [sp, #4]
   ddfb2:	9700      	str	r7, [sp, #0]
   ddfb4:	462b      	mov	r3, r5
   ddfb6:	aa0a      	add	r2, sp, #40	; 0x28
   ddfb8:	4621      	mov	r1, r4
   ddfba:	a80f      	add	r0, sp, #60	; 0x3c
   ddfbc:	f7ff fed8 	bl	ddd70 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   ddfc0:	4638      	mov	r0, r7
   ddfc2:	f7f8 fa46 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   ddfc6:	a80a      	add	r0, sp, #40	; 0x28
   ddfc8:	f7f8 fa43 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   ddfcc:	a80f      	add	r0, sp, #60	; 0x3c
   ddfce:	f7f8 fa40 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  } else {
    context->ReportError(context,
                         "Kernel type not supported by Maximum/Minimum.");
    return kTfLiteError;
  }
  return kTfLiteOk;
   ddfd2:	2000      	movs	r0, #0
   ddfd4:	e00a      	b.n	ddfec <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x18c>
        break;
      case kTfLiteInt64:
        TFLiteOperation<int64_t, OpType>(context, node, op_context);
        break;
      default:
        context->ReportError(
   ddfd6:	f8d9 4014 	ldr.w	r4, [r9, #20]
   ddfda:	f7f6 f89f 	bl	d411c <TfLiteTypeGetName>
   ddfde:	f818 3007 	ldrb.w	r3, [r8, r7]
   ddfe2:	4909      	ldr	r1, [pc, #36]	; (de008 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MinimumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x1a8>)
   ddfe4:	4602      	mov	r2, r0
   ddfe6:	4648      	mov	r0, r9
   ddfe8:	47a0      	blx	r4
            context, "Type %s (%d) is not supported by Maximum/Minimum.",
            TfLiteTypeGetName(op_context.output->type),
            op_context.output->type);
        return kTfLiteError;
   ddfea:	2001      	movs	r0, #1
    context->ReportError(context,
                         "Kernel type not supported by Maximum/Minimum.");
    return kTfLiteError;
  }
  return kTfLiteOk;
}
   ddfec:	b015      	add	sp, #84	; 0x54
   ddfee:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}
   ddff2:	bf00      	nop
   ddff4:	000dd993 	.word	0x000dd993
   ddff8:	000dd9a3 	.word	0x000dd9a3
   ddffc:	000dd9ab 	.word	0x000dd9ab
   de000:	000dd9b3 	.word	0x000dd9b3
   de004:	000dd9bb 	.word	0x000dd9bb
   de008:	000e9edf 	.word	0x000e9edf

000de00c <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode>:
      GetTensorData<data_type>(op_context.output),
      op_type::template op<data_type>);
}

template <KernelType kernel_type, typename OpType>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   de00c:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
   de010:	680b      	ldr	r3, [r1, #0]
   de012:	f8d0 8008 	ldr.w	r8, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   de016:	685c      	ldr	r4, [r3, #4]
   de018:	689d      	ldr	r5, [r3, #8]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   de01a:	684b      	ldr	r3, [r1, #4]
   de01c:	685f      	ldr	r7, [r3, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   de01e:	2238      	movs	r2, #56	; 0x38
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   de020:	4357      	muls	r7, r2
   de022:	4681      	mov	r9, r0
  OpContext op_context(context, node);

  if (kernel_type == kReference) {
    switch (op_context.output->type) {
   de024:	f818 0007 	ldrb.w	r0, [r8, r7]
   de028:	1e43      	subs	r3, r0, #1
      GetTensorData<data_type>(op_context.output),
      op_type::template op<data_type>);
}

template <KernelType kernel_type, typename OpType>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   de02a:	b095      	sub	sp, #84	; 0x54
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   de02c:	fb02 8404 	mla	r4, r2, r4, r8
   de030:	fb02 8505 	mla	r5, r2, r5, r8
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   de034:	eb08 0607 	add.w	r6, r8, r7
  OpContext op_context(context, node);

  if (kernel_type == kReference) {
    switch (op_context.output->type) {
   de038:	2b08      	cmp	r3, #8
   de03a:	f200 80a2 	bhi.w	de182 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x176>
   de03e:	e8df f003 	tbb	[pc, r3]
   de042:	5c05      	.short	0x5c05
   de044:	a0a07922 	.word	0xa0a07922
   de048:	a0a0      	.short	0xa0a0
   de04a:	3f          	.byte	0x3f
   de04b:	00          	.byte	0x00
}  // namespace

template <typename data_type, typename op_type>
void TFLiteOperation(TfLiteContext* context, TfLiteNode* node,
                     const OpContext& op_context) {
  reference_ops::MaximumMinimumBroadcast4DSlow(
   de04c:	4621      	mov	r1, r4
   de04e:	a80f      	add	r0, sp, #60	; 0x3c
   de050:	f7f8 fcaf 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de054:	b104      	cbz	r4, de058 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x4c>
   de056:	6864      	ldr	r4, [r4, #4]
   de058:	4629      	mov	r1, r5
   de05a:	a80a      	add	r0, sp, #40	; 0x28
   de05c:	f7f8 fca9 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de060:	b105      	cbz	r5, de064 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x58>
   de062:	686d      	ldr	r5, [r5, #4]
   de064:	af05      	add	r7, sp, #20
   de066:	4631      	mov	r1, r6
   de068:	4638      	mov	r0, r7
   de06a:	f7f8 fca2 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de06e:	4b4c      	ldr	r3, [pc, #304]	; (de1a0 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x194>)
   de070:	9302      	str	r3, [sp, #8]
   de072:	6873      	ldr	r3, [r6, #4]
   de074:	9301      	str	r3, [sp, #4]
   de076:	9700      	str	r7, [sp, #0]
   de078:	462b      	mov	r3, r5
   de07a:	aa0a      	add	r2, sp, #40	; 0x28
   de07c:	4621      	mov	r1, r4
   de07e:	a80f      	add	r0, sp, #60	; 0x3c
   de080:	f7ff fcac 	bl	dd9dc <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIfPFfffEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   de084:	e072      	b.n	de16c <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
   de086:	4621      	mov	r1, r4
   de088:	a80f      	add	r0, sp, #60	; 0x3c
   de08a:	f7f8 fc92 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de08e:	b104      	cbz	r4, de092 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x86>
   de090:	6864      	ldr	r4, [r4, #4]
   de092:	4629      	mov	r1, r5
   de094:	a80a      	add	r0, sp, #40	; 0x28
   de096:	f7f8 fc8c 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de09a:	b105      	cbz	r5, de09e <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x92>
   de09c:	686d      	ldr	r5, [r5, #4]
   de09e:	af05      	add	r7, sp, #20
   de0a0:	4631      	mov	r1, r6
   de0a2:	4638      	mov	r0, r7
   de0a4:	f7f8 fc85 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de0a8:	4b3e      	ldr	r3, [pc, #248]	; (de1a4 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x198>)
   de0aa:	9302      	str	r3, [sp, #8]
   de0ac:	6873      	ldr	r3, [r6, #4]
   de0ae:	9301      	str	r3, [sp, #4]
   de0b0:	9700      	str	r7, [sp, #0]
   de0b2:	462b      	mov	r3, r5
   de0b4:	aa0a      	add	r2, sp, #40	; 0x28
   de0b6:	4621      	mov	r1, r4
   de0b8:	a80f      	add	r0, sp, #60	; 0x3c
   de0ba:	f7ff fd06 	bl	ddaca <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIhPFhhhEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   de0be:	e055      	b.n	de16c <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
   de0c0:	4621      	mov	r1, r4
   de0c2:	a80f      	add	r0, sp, #60	; 0x3c
   de0c4:	f7f8 fc75 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de0c8:	b104      	cbz	r4, de0cc <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xc0>
   de0ca:	6864      	ldr	r4, [r4, #4]
   de0cc:	4629      	mov	r1, r5
   de0ce:	a80a      	add	r0, sp, #40	; 0x28
   de0d0:	f7f8 fc6f 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de0d4:	b105      	cbz	r5, de0d8 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xcc>
   de0d6:	686d      	ldr	r5, [r5, #4]
   de0d8:	af05      	add	r7, sp, #20
   de0da:	4631      	mov	r1, r6
   de0dc:	4638      	mov	r0, r7
   de0de:	f7f8 fc68 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de0e2:	4b31      	ldr	r3, [pc, #196]	; (de1a8 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x19c>)
   de0e4:	9302      	str	r3, [sp, #8]
   de0e6:	6873      	ldr	r3, [r6, #4]
   de0e8:	9301      	str	r3, [sp, #4]
   de0ea:	9700      	str	r7, [sp, #0]
   de0ec:	462b      	mov	r3, r5
   de0ee:	aa0a      	add	r2, sp, #40	; 0x28
   de0f0:	4621      	mov	r1, r4
   de0f2:	a80f      	add	r0, sp, #60	; 0x3c
   de0f4:	f7ff fd5a 	bl	ddbac <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIaPFaaaEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   de0f8:	e038      	b.n	de16c <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
   de0fa:	4621      	mov	r1, r4
   de0fc:	a80f      	add	r0, sp, #60	; 0x3c
   de0fe:	f7f8 fc58 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de102:	b104      	cbz	r4, de106 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xfa>
   de104:	6864      	ldr	r4, [r4, #4]
   de106:	4629      	mov	r1, r5
   de108:	a80a      	add	r0, sp, #40	; 0x28
   de10a:	f7f8 fc52 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de10e:	b105      	cbz	r5, de112 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x106>
   de110:	686d      	ldr	r5, [r5, #4]
   de112:	af05      	add	r7, sp, #20
   de114:	4631      	mov	r1, r6
   de116:	4638      	mov	r0, r7
   de118:	f7f8 fc4b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de11c:	4b23      	ldr	r3, [pc, #140]	; (de1ac <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x1a0>)
   de11e:	9302      	str	r3, [sp, #8]
   de120:	6873      	ldr	r3, [r6, #4]
   de122:	9301      	str	r3, [sp, #4]
   de124:	9700      	str	r7, [sp, #0]
   de126:	462b      	mov	r3, r5
   de128:	aa0a      	add	r2, sp, #40	; 0x28
   de12a:	4621      	mov	r1, r4
   de12c:	a80f      	add	r0, sp, #60	; 0x3c
   de12e:	f7ff fdae 	bl	ddc8e <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIlPFlllEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   de132:	e01b      	b.n	de16c <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x160>
   de134:	4621      	mov	r1, r4
   de136:	a80f      	add	r0, sp, #60	; 0x3c
   de138:	f7f8 fc3b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de13c:	b104      	cbz	r4, de140 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x134>
   de13e:	6864      	ldr	r4, [r4, #4]
   de140:	4629      	mov	r1, r5
   de142:	a80a      	add	r0, sp, #40	; 0x28
   de144:	f7f8 fc35 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de148:	b105      	cbz	r5, de14c <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x140>
   de14a:	686d      	ldr	r5, [r5, #4]
   de14c:	af05      	add	r7, sp, #20
   de14e:	4631      	mov	r1, r6
   de150:	4638      	mov	r0, r7
   de152:	f7f8 fc2e 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   de156:	4b16      	ldr	r3, [pc, #88]	; (de1b0 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x1a4>)
   de158:	9302      	str	r3, [sp, #8]
   de15a:	6873      	ldr	r3, [r6, #4]
   de15c:	9301      	str	r3, [sp, #4]
   de15e:	9700      	str	r7, [sp, #0]
   de160:	462b      	mov	r3, r5
   de162:	aa0a      	add	r2, sp, #40	; 0x28
   de164:	4621      	mov	r1, r4
   de166:	a80f      	add	r0, sp, #60	; 0x3c
   de168:	f7ff fe02 	bl	ddd70 <_ZN6tflite13reference_ops29MaximumMinimumBroadcast4DSlowIxPFxxxEEEvRKNS_12RuntimeShapeEPKT_S6_S9_S6_PS7_T0_>
   de16c:	4638      	mov	r0, r7
   de16e:	f7f8 f970 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   de172:	a80a      	add	r0, sp, #40	; 0x28
   de174:	f7f8 f96d 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   de178:	a80f      	add	r0, sp, #60	; 0x3c
   de17a:	f7f8 f96a 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  } else {
    context->ReportError(context,
                         "Kernel type not supported by Maximum/Minimum.");
    return kTfLiteError;
  }
  return kTfLiteOk;
   de17e:	2000      	movs	r0, #0
   de180:	e00a      	b.n	de198 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x18c>
        break;
      case kTfLiteInt64:
        TFLiteOperation<int64_t, OpType>(context, node, op_context);
        break;
      default:
        context->ReportError(
   de182:	f8d9 4014 	ldr.w	r4, [r9, #20]
   de186:	f7f5 ffc9 	bl	d411c <TfLiteTypeGetName>
   de18a:	f818 3007 	ldrb.w	r3, [r8, r7]
   de18e:	4909      	ldr	r1, [pc, #36]	; (de1b4 <_ZN6tflite3ops5micro15maximum_minimum4EvalILNS2_12_GLOBAL__N_110KernelTypeE0ENS4_9MaximumOpEEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x1a8>)
   de190:	4602      	mov	r2, r0
   de192:	4648      	mov	r0, r9
   de194:	47a0      	blx	r4
            context, "Type %s (%d) is not supported by Maximum/Minimum.",
            TfLiteTypeGetName(op_context.output->type),
            op_context.output->type);
        return kTfLiteError;
   de196:	2001      	movs	r0, #1
    context->ReportError(context,
                         "Kernel type not supported by Maximum/Minimum.");
    return kTfLiteError;
  }
  return kTfLiteOk;
}
   de198:	b015      	add	sp, #84	; 0x54
   de19a:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}
   de19e:	bf00      	nop
   de1a0:	000dd959 	.word	0x000dd959
   de1a4:	000dd969 	.word	0x000dd969
   de1a8:	000dd971 	.word	0x000dd971
   de1ac:	000dd979 	.word	0x000dd979
   de1b0:	000dd981 	.word	0x000dd981
   de1b4:	000e9edf 	.word	0x000e9edf

000de1b8 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode>:
namespace neg {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   de1b8:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   de1bc:	680b      	ldr	r3, [r1, #0]
   de1be:	6884      	ldr	r4, [r0, #8]
   de1c0:	685b      	ldr	r3, [r3, #4]
   de1c2:	2738      	movs	r7, #56	; 0x38
   de1c4:	437b      	muls	r3, r7
   de1c6:	b08a      	sub	sp, #40	; 0x28
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  switch (input->type) {
   de1c8:	5ce2      	ldrb	r2, [r4, r3]
   de1ca:	2a01      	cmp	r2, #1
   de1cc:	eb04 0603 	add.w	r6, r4, r3
   de1d0:	d145      	bne.n	de25e <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0xa6>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   de1d2:	684b      	ldr	r3, [r1, #4]
   de1d4:	685b      	ldr	r3, [r3, #4]
    // TODO(wangtz): handle for kTfLiteInt8
    case kTfLiteFloat32:
      reference_ops::Negate(GetTensorShape(input), GetTensorData<float>(input),
   de1d6:	4631      	mov	r1, r6
   de1d8:	fb07 4403 	mla	r4, r7, r3, r4
   de1dc:	4668      	mov	r0, sp
   de1de:	f7f8 fbe8 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                            GetTensorShape(output),
   de1e2:	4621      	mov	r1, r4
   de1e4:	a805      	add	r0, sp, #20
   de1e6:	6876      	ldr	r6, [r6, #4]
   de1e8:	f7f8 fbe3 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   de1ec:	b104      	cbz	r4, de1f0 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x38>
   de1ee:	6864      	ldr	r4, [r4, #4]
   de1f0:	9f00      	ldr	r7, [sp, #0]

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   de1f2:	9b05      	ldr	r3, [sp, #20]
   de1f4:	429f      	cmp	r7, r3
   de1f6:	d101      	bne.n	de1fc <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x44>
   de1f8:	2500      	movs	r5, #0
   de1fa:	e00d      	b.n	de218 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x60>
   de1fc:	f006 f8a6 	bl	e434c <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   de200:	4629      	mov	r1, r5
   de202:	4668      	mov	r0, sp
   de204:	f7f8 f930 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   de208:	4629      	mov	r1, r5
   de20a:	4680      	mov	r8, r0
   de20c:	a805      	add	r0, sp, #20
   de20e:	f7f8 f92b 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   de212:	4580      	cmp	r8, r0
   de214:	d1f2      	bne.n	de1fc <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x44>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   de216:	3501      	adds	r5, #1
   de218:	42af      	cmp	r7, r5
   de21a:	dcf1      	bgt.n	de200 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x48>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   de21c:	2f04      	cmp	r7, #4
   de21e:	bfcc      	ite	gt
   de220:	9a01      	ldrgt	r2, [sp, #4]
   de222:	aa01      	addle	r2, sp, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   de224:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
   de226:	2101      	movs	r1, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   de228:	429f      	cmp	r7, r3
   de22a:	dd04      	ble.n	de236 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x7e>
      buffer_size *= dims_data[i];
   de22c:	f852 0023 	ldr.w	r0, [r2, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   de230:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
   de232:	4341      	muls	r1, r0
   de234:	e7f8      	b.n	de228 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x70>
   de236:	4633      	mov	r3, r6
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   de238:	2200      	movs	r2, #0
template <typename T>
inline void Negate(const RuntimeShape& input_shape, const T* input_data,
                   const RuntimeShape& output_shape, T* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; ++i) {
   de23a:	428a      	cmp	r2, r1
   de23c:	da07      	bge.n	de24e <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x96>
    output_data[i] = -input_data[i];
   de23e:	ecf3 7a01 	vldmia	r3!, {s15}
   de242:	eef1 7a67 	vneg.f32	s15, s15
   de246:	ece4 7a01 	vstmia	r4!, {s15}
template <typename T>
inline void Negate(const RuntimeShape& input_shape, const T* input_data,
                   const RuntimeShape& output_shape, T* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);

  for (int i = 0; i < flat_size; ++i) {
   de24a:	3201      	adds	r2, #1
   de24c:	e7f5      	b.n	de23a <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0x82>
   de24e:	a805      	add	r0, sp, #20
   de250:	f7f8 f8ff 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  switch (input->type) {
    // TODO(wangtz): handle for kTfLiteInt8
    case kTfLiteFloat32:
      reference_ops::Negate(GetTensorShape(input), GetTensorData<float>(input),
   de254:	4668      	mov	r0, sp
   de256:	f7f8 f8fc 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
    default:
      context->ReportError(
          context, "Neg only currently supports float32, got %d.", input->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
   de25a:	2000      	movs	r0, #0
   de25c:	e003      	b.n	de266 <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0xae>
                            GetTensorShape(output),
                            GetTensorData<float>(output));
      break;
    default:
      context->ReportError(
          context, "Neg only currently supports float32, got %d.", input->type);
   de25e:	6943      	ldr	r3, [r0, #20]
   de260:	4902      	ldr	r1, [pc, #8]	; (de26c <_ZN6tflite3ops5micro3neg4EvalEP13TfLiteContextP10TfLiteNode+0xb4>)
   de262:	4798      	blx	r3
      return kTfLiteError;
   de264:	2001      	movs	r0, #1
  }
  return kTfLiteOk;
}
   de266:	b00a      	add	sp, #40	; 0x28
   de268:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   de26c:	000e9f11 	.word	0x000e9f11

000de270 <_ZN6tflite3ops5micro12Register_NEGEv>:

TfLiteRegistration* Register_NEG() {
  static TfLiteRegistration r = {/*init=*/nullptr, /*free=*/nullptr,
                                 /*prepare=*/nullptr, neg::Eval};
  return &r;
}
   de270:	4800      	ldr	r0, [pc, #0]	; (de274 <_ZN6tflite3ops5micro12Register_NEGEv+0x4>)
   de272:	4770      	bx	lr
   de274:	2003c1dc 	.word	0x2003c1dc

000de278 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_17PrepareEP13TfLiteContextP10TfLiteNode>:

constexpr int kOutputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   de278:	2000      	movs	r0, #0
   de27a:	4770      	bx	lr

000de27c <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode>:
  }

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   de27c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   de280:	684b      	ldr	r3, [r1, #4]
   de282:	6885      	ldr	r5, [r0, #8]
  const TfLitePackParams* data =
      reinterpret_cast<TfLitePackParams*>(node->builtin_data);
   de284:	694a      	ldr	r2, [r1, #20]
  }

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   de286:	b085      	sub	sp, #20
   de288:	9000      	str	r0, [sp, #0]
   de28a:	6858      	ldr	r0, [r3, #4]
   de28c:	2338      	movs	r3, #56	; 0x38
   de28e:	4358      	muls	r0, r3
   de290:	182b      	adds	r3, r5, r0
  const TfLitePackParams* data =
      reinterpret_cast<TfLitePackParams*>(node->builtin_data);

  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (output->type) {
   de292:	5c28      	ldrb	r0, [r5, r0]
   de294:	1e46      	subs	r6, r0, #1
   de296:	2e08      	cmp	r6, #8
   de298:	f200 821a 	bhi.w	de6d0 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x454>
   de29c:	e8df f016 	tbh	[pc, r6, lsl #1]
   de2a0:	013f0009 	.word	0x013f0009
   de2a4:	01aa0077 	.word	0x01aa0077
   de2a8:	02180218 	.word	0x02180218
   de2ac:	02180218 	.word	0x02180218
   de2b0:	00da      	.short	0x00da

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
   de2b2:	6808      	ldr	r0, [r1, #0]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de2b4:	689f      	ldr	r7, [r3, #8]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de2b6:	6840      	ldr	r0, [r0, #4]

  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (output->type) {
    case kTfLiteFloat32: {
      return PackImpl<float>(context, node, output, data->values_count,
   de2b8:	f8d2 8000 	ldr.w	r8, [r2]
                             data->axis);
   de2bc:	6852      	ldr	r2, [r2, #4]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de2be:	f8d7 e000 	ldr.w	lr, [r7]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de2c2:	2638      	movs	r6, #56	; 0x38
   de2c4:	fb06 5500 	mla	r5, r6, r0, r5
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
   de2c8:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de2ca:	68ad      	ldr	r5, [r5, #8]
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
    axis += dimensions;
   de2cc:	bfb8      	it	lt
   de2ce:	4472      	addlt	r2, lr
   de2d0:	46bc      	mov	ip, r7
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de2d2:	2000      	movs	r0, #0

  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
   de2d4:	2601      	movs	r6, #1
  for (int i = 0; i < axis; ++i) {
   de2d6:	4282      	cmp	r2, r0
   de2d8:	dd05      	ble.n	de2e6 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x6a>
    outer_size *= output_dims->data[i];
   de2da:	f85c 9f04 	ldr.w	r9, [ip, #4]!
  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de2de:	3001      	adds	r0, #1
    outer_size *= output_dims->data[i];
   de2e0:	fb09 f606 	mul.w	r6, r9, r6
   de2e4:	e7f7      	b.n	de2d6 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
   de2e6:	1c50      	adds	r0, r2, #1
  }
  int copy_size = 1;
   de2e8:	2201      	movs	r2, #1
  for (int i = axis + 1; i < dimensions; ++i) {
   de2ea:	4586      	cmp	lr, r0
   de2ec:	f100 0c01 	add.w	ip, r0, #1
   de2f0:	dd04      	ble.n	de2fc <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x80>
    copy_size *= output_dims->data[i];
   de2f2:	f857 002c 	ldr.w	r0, [r7, ip, lsl #2]
   de2f6:	4342      	muls	r2, r0
   de2f8:	4660      	mov	r0, ip
   de2fa:	e7f6      	b.n	de2ea <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x6e>
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de2fc:	f8d5 c000 	ldr.w	ip, [r5]
   de300:	4628      	mov	r0, r5
   de302:	2700      	movs	r7, #0
   de304:	2501      	movs	r5, #1
   de306:	45bc      	cmp	ip, r7
   de308:	dd05      	ble.n	de316 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x9a>
    input_size *= input_dims->data[i];
   de30a:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= output_dims->data[i];
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de30e:	3701      	adds	r7, #1
    input_size *= input_dims->data[i];
   de310:	fb0e f505 	mul.w	r5, lr, r5
   de314:	e7f7      	b.n	de306 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x8a>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);
   de316:	fb02 f006 	mul.w	r0, r2, r6
   de31a:	4285      	cmp	r5, r0
   de31c:	d001      	beq.n	de322 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xa6>
   de31e:	f006 f815 	bl	e434c <abort>
   de322:	685b      	ldr	r3, [r3, #4]
   de324:	9301      	str	r3, [sp, #4]
   de326:	fb02 f308 	mul.w	r3, r2, r8
   de32a:	009b      	lsls	r3, r3, #2
   de32c:	2000      	movs	r0, #0
   de32e:	ea4f 0982 	mov.w	r9, r2, lsl #2
   de332:	9302      	str	r3, [sp, #8]

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de334:	4605      	mov	r5, r0
   de336:	45a8      	cmp	r8, r5
   de338:	dc01      	bgt.n	de33e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc2>
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (output->type) {
    case kTfLiteFloat32: {
      return PackImpl<float>(context, node, output, data->values_count,
                             data->axis);
   de33a:	2000      	movs	r0, #0
   de33c:	e1d1      	b.n	de6e2 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x466>
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
   de33e:	680b      	ldr	r3, [r1, #0]
   de340:	eb03 0385 	add.w	r3, r3, r5, lsl #2
   de344:	2438      	movs	r4, #56	; 0x38
   de346:	685f      	ldr	r7, [r3, #4]
   de348:	9b00      	ldr	r3, [sp, #0]
   de34a:	689b      	ldr	r3, [r3, #8]
   de34c:	fb04 3307 	mla	r3, r4, r7, r3
   de350:	b103      	cbz	r3, de354 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xd8>
   de352:	685b      	ldr	r3, [r3, #4]
   de354:	2700      	movs	r7, #0
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de356:	46bc      	mov	ip, r7
   de358:	4566      	cmp	r6, ip
   de35a:	dd15      	ble.n	de388 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x10c>
   de35c:	9c01      	ldr	r4, [sp, #4]
   de35e:	eb00 0e07 	add.w	lr, r0, r7
   de362:	44a6      	add	lr, r4
   de364:	469b      	mov	fp, r3
      const T* input_ptr = input_data + copy_size * k;
      int loc = k * values_count * copy_size + i * copy_size;
      T* output_ptr = output_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   de366:	f04f 0a00 	mov.w	sl, #0
   de36a:	4552      	cmp	r2, sl
   de36c:	dd06      	ble.n	de37c <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x100>
   de36e:	ecfb 7a01 	vldmia	fp!, {s15}
   de372:	f10a 0a01 	add.w	sl, sl, #1
   de376:	ecee 7a01 	vstmia	lr!, {s15}
   de37a:	e7f6      	b.n	de36a <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xee>
   de37c:	9c02      	ldr	r4, [sp, #8]
  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de37e:	f10c 0c01 	add.w	ip, ip, #1
   de382:	444b      	add	r3, r9
   de384:	4427      	add	r7, r4
   de386:	e7e7      	b.n	de358 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xdc>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de388:	3501      	adds	r5, #1
   de38a:	4448      	add	r0, r9
   de38c:	e7d3      	b.n	de336 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xba>

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
   de38e:	6808      	ldr	r0, [r1, #0]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de390:	689f      	ldr	r7, [r3, #8]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de392:	6840      	ldr	r0, [r0, #4]
    case kTfLiteFloat32: {
      return PackImpl<float>(context, node, output, data->values_count,
                             data->axis);
    }
    case kTfLiteUInt8: {
      return PackImpl<uint8_t>(context, node, output, data->values_count,
   de394:	f8d2 9000 	ldr.w	r9, [r2]
                               data->axis);
   de398:	6852      	ldr	r2, [r2, #4]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de39a:	f8d7 e000 	ldr.w	lr, [r7]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de39e:	2638      	movs	r6, #56	; 0x38
   de3a0:	fb06 5500 	mla	r5, r6, r0, r5
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
   de3a4:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de3a6:	68ae      	ldr	r6, [r5, #8]
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
    axis += dimensions;
   de3a8:	bfb8      	it	lt
   de3aa:	4472      	addlt	r2, lr
   de3ac:	46bc      	mov	ip, r7
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de3ae:	2000      	movs	r0, #0

  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
   de3b0:	2501      	movs	r5, #1
  for (int i = 0; i < axis; ++i) {
   de3b2:	4282      	cmp	r2, r0
   de3b4:	dd05      	ble.n	de3c2 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x146>
    outer_size *= output_dims->data[i];
   de3b6:	f85c 8f04 	ldr.w	r8, [ip, #4]!
  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de3ba:	3001      	adds	r0, #1
    outer_size *= output_dims->data[i];
   de3bc:	fb08 f505 	mul.w	r5, r8, r5
   de3c0:	e7f7      	b.n	de3b2 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x136>
   de3c2:	1c50      	adds	r0, r2, #1
  }
  int copy_size = 1;
   de3c4:	2201      	movs	r2, #1
  for (int i = axis + 1; i < dimensions; ++i) {
   de3c6:	4586      	cmp	lr, r0
   de3c8:	f100 0c01 	add.w	ip, r0, #1
   de3cc:	dd04      	ble.n	de3d8 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x15c>
    copy_size *= output_dims->data[i];
   de3ce:	f857 002c 	ldr.w	r0, [r7, ip, lsl #2]
   de3d2:	4342      	muls	r2, r0
   de3d4:	4660      	mov	r0, ip
   de3d6:	e7f6      	b.n	de3c6 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x14a>
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de3d8:	f8d6 c000 	ldr.w	ip, [r6]
   de3dc:	4630      	mov	r0, r6
   de3de:	2700      	movs	r7, #0
   de3e0:	2601      	movs	r6, #1
   de3e2:	45bc      	cmp	ip, r7
   de3e4:	dd05      	ble.n	de3f2 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x176>
    input_size *= input_dims->data[i];
   de3e6:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= output_dims->data[i];
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de3ea:	3701      	adds	r7, #1
    input_size *= input_dims->data[i];
   de3ec:	fb0e f606 	mul.w	r6, lr, r6
   de3f0:	e7f7      	b.n	de3e2 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x166>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);
   de3f2:	fb02 f005 	mul.w	r0, r2, r5
   de3f6:	4286      	cmp	r6, r0
   de3f8:	d191      	bne.n	de31e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xa2>
   de3fa:	685e      	ldr	r6, [r3, #4]
   de3fc:	fb02 f309 	mul.w	r3, r2, r9
   de400:	9301      	str	r3, [sp, #4]

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de402:	2000      	movs	r0, #0
   de404:	4581      	cmp	r9, r0
   de406:	dd98      	ble.n	de33a <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xbe>
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
   de408:	680b      	ldr	r3, [r1, #0]
   de40a:	eb03 0380 	add.w	r3, r3, r0, lsl #2
   de40e:	2438      	movs	r4, #56	; 0x38
   de410:	685f      	ldr	r7, [r3, #4]
   de412:	9b00      	ldr	r3, [sp, #0]
   de414:	689b      	ldr	r3, [r3, #8]
   de416:	fb04 3307 	mla	r3, r4, r7, r3
   de41a:	b103      	cbz	r3, de41e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1a2>
   de41c:	685b      	ldr	r3, [r3, #4]
   de41e:	425f      	negs	r7, r3
   de420:	46b6      	mov	lr, r6
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de422:	f04f 0c00 	mov.w	ip, #0
   de426:	4565      	cmp	r5, ip
   de428:	dd11      	ble.n	de44e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1d2>
   de42a:	46f2      	mov	sl, lr
   de42c:	4698      	mov	r8, r3
      const T* input_ptr = input_data + copy_size * k;
      int loc = k * values_count * copy_size + i * copy_size;
      T* output_ptr = output_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   de42e:	eb07 0b08 	add.w	fp, r7, r8
   de432:	455a      	cmp	r2, fp
   de434:	dd04      	ble.n	de440 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1c4>
   de436:	f818 bb01 	ldrb.w	fp, [r8], #1
   de43a:	f80a bb01 	strb.w	fp, [sl], #1
   de43e:	e7f6      	b.n	de42e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1b2>
   de440:	9c01      	ldr	r4, [sp, #4]
  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de442:	f10c 0c01 	add.w	ip, ip, #1
   de446:	4413      	add	r3, r2
   de448:	44a6      	add	lr, r4
   de44a:	1abf      	subs	r7, r7, r2
   de44c:	e7eb      	b.n	de426 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1aa>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de44e:	3001      	adds	r0, #1
   de450:	4416      	add	r6, r2
   de452:	e7d7      	b.n	de404 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x188>

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
   de454:	6808      	ldr	r0, [r1, #0]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de456:	689f      	ldr	r7, [r3, #8]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de458:	6840      	ldr	r0, [r0, #4]
    case kTfLiteUInt8: {
      return PackImpl<uint8_t>(context, node, output, data->values_count,
                               data->axis);
    }
    case kTfLiteInt8: {
      return PackImpl<int8_t>(context, node, output, data->values_count,
   de45a:	f8d2 9000 	ldr.w	r9, [r2]
                              data->axis);
   de45e:	6852      	ldr	r2, [r2, #4]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de460:	f8d7 e000 	ldr.w	lr, [r7]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de464:	2638      	movs	r6, #56	; 0x38
   de466:	fb06 5500 	mla	r5, r6, r0, r5
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
   de46a:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de46c:	68ae      	ldr	r6, [r5, #8]
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
    axis += dimensions;
   de46e:	bfb8      	it	lt
   de470:	4472      	addlt	r2, lr
   de472:	46bc      	mov	ip, r7
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de474:	2000      	movs	r0, #0

  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
   de476:	2501      	movs	r5, #1
  for (int i = 0; i < axis; ++i) {
   de478:	4282      	cmp	r2, r0
   de47a:	dd05      	ble.n	de488 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x20c>
    outer_size *= output_dims->data[i];
   de47c:	f85c 8f04 	ldr.w	r8, [ip, #4]!
  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de480:	3001      	adds	r0, #1
    outer_size *= output_dims->data[i];
   de482:	fb08 f505 	mul.w	r5, r8, r5
   de486:	e7f7      	b.n	de478 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1fc>
   de488:	1c50      	adds	r0, r2, #1
  }
  int copy_size = 1;
   de48a:	2201      	movs	r2, #1
  for (int i = axis + 1; i < dimensions; ++i) {
   de48c:	4586      	cmp	lr, r0
   de48e:	f100 0c01 	add.w	ip, r0, #1
   de492:	dd04      	ble.n	de49e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x222>
    copy_size *= output_dims->data[i];
   de494:	f857 002c 	ldr.w	r0, [r7, ip, lsl #2]
   de498:	4342      	muls	r2, r0
   de49a:	4660      	mov	r0, ip
   de49c:	e7f6      	b.n	de48c <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x210>
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de49e:	f8d6 c000 	ldr.w	ip, [r6]
   de4a2:	4630      	mov	r0, r6
   de4a4:	2700      	movs	r7, #0
   de4a6:	2601      	movs	r6, #1
   de4a8:	45bc      	cmp	ip, r7
   de4aa:	dd05      	ble.n	de4b8 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x23c>
    input_size *= input_dims->data[i];
   de4ac:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= output_dims->data[i];
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de4b0:	3701      	adds	r7, #1
    input_size *= input_dims->data[i];
   de4b2:	fb0e f606 	mul.w	r6, lr, r6
   de4b6:	e7f7      	b.n	de4a8 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x22c>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);
   de4b8:	fb02 f005 	mul.w	r0, r2, r5
   de4bc:	4286      	cmp	r6, r0
   de4be:	f47f af2e 	bne.w	de31e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xa2>
   de4c2:	685e      	ldr	r6, [r3, #4]
   de4c4:	fb02 f309 	mul.w	r3, r2, r9
   de4c8:	9301      	str	r3, [sp, #4]

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de4ca:	2000      	movs	r0, #0
   de4cc:	4581      	cmp	r9, r0
   de4ce:	f77f af34 	ble.w	de33a <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xbe>
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
   de4d2:	680b      	ldr	r3, [r1, #0]
   de4d4:	eb03 0380 	add.w	r3, r3, r0, lsl #2
   de4d8:	2438      	movs	r4, #56	; 0x38
   de4da:	685f      	ldr	r7, [r3, #4]
   de4dc:	9b00      	ldr	r3, [sp, #0]
   de4de:	689b      	ldr	r3, [r3, #8]
   de4e0:	fb04 3307 	mla	r3, r4, r7, r3
   de4e4:	b103      	cbz	r3, de4e8 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x26c>
   de4e6:	685b      	ldr	r3, [r3, #4]
   de4e8:	425f      	negs	r7, r3
   de4ea:	46b6      	mov	lr, r6
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de4ec:	f04f 0c00 	mov.w	ip, #0
   de4f0:	4565      	cmp	r5, ip
   de4f2:	dd11      	ble.n	de518 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x29c>
   de4f4:	46f2      	mov	sl, lr
   de4f6:	4698      	mov	r8, r3
      const T* input_ptr = input_data + copy_size * k;
      int loc = k * values_count * copy_size + i * copy_size;
      T* output_ptr = output_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   de4f8:	eb08 0b07 	add.w	fp, r8, r7
   de4fc:	455a      	cmp	r2, fp
   de4fe:	dd04      	ble.n	de50a <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x28e>
   de500:	f918 bb01 	ldrsb.w	fp, [r8], #1
   de504:	f80a bb01 	strb.w	fp, [sl], #1
   de508:	e7f6      	b.n	de4f8 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x27c>
   de50a:	9c01      	ldr	r4, [sp, #4]
  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de50c:	f10c 0c01 	add.w	ip, ip, #1
   de510:	4413      	add	r3, r2
   de512:	44a6      	add	lr, r4
   de514:	1abf      	subs	r7, r7, r2
   de516:	e7eb      	b.n	de4f0 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x274>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de518:	3001      	adds	r0, #1
   de51a:	4416      	add	r6, r2
   de51c:	e7d6      	b.n	de4cc <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x250>

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
   de51e:	6808      	ldr	r0, [r1, #0]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de520:	689e      	ldr	r6, [r3, #8]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de522:	6840      	ldr	r0, [r0, #4]
    case kTfLiteInt8: {
      return PackImpl<int8_t>(context, node, output, data->values_count,
                              data->axis);
    }
    case kTfLiteInt32: {
      return PackImpl<int32_t>(context, node, output, data->values_count,
   de524:	f8d2 8000 	ldr.w	r8, [r2]
                               data->axis);
   de528:	6852      	ldr	r2, [r2, #4]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de52a:	f8d6 e000 	ldr.w	lr, [r6]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de52e:	2738      	movs	r7, #56	; 0x38
   de530:	fb07 5500 	mla	r5, r7, r0, r5
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
   de534:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de536:	68ad      	ldr	r5, [r5, #8]
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
    axis += dimensions;
   de538:	bfb8      	it	lt
   de53a:	4472      	addlt	r2, lr
   de53c:	46b4      	mov	ip, r6
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de53e:	2000      	movs	r0, #0

  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
   de540:	2701      	movs	r7, #1
  for (int i = 0; i < axis; ++i) {
   de542:	4282      	cmp	r2, r0
   de544:	dd05      	ble.n	de552 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2d6>
    outer_size *= output_dims->data[i];
   de546:	f85c 9f04 	ldr.w	r9, [ip, #4]!
  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de54a:	3001      	adds	r0, #1
    outer_size *= output_dims->data[i];
   de54c:	fb09 f707 	mul.w	r7, r9, r7
   de550:	e7f7      	b.n	de542 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2c6>
   de552:	1c50      	adds	r0, r2, #1
  }
  int copy_size = 1;
   de554:	2201      	movs	r2, #1
  for (int i = axis + 1; i < dimensions; ++i) {
   de556:	4586      	cmp	lr, r0
   de558:	f100 0c01 	add.w	ip, r0, #1
   de55c:	dd04      	ble.n	de568 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2ec>
    copy_size *= output_dims->data[i];
   de55e:	f856 002c 	ldr.w	r0, [r6, ip, lsl #2]
   de562:	4342      	muls	r2, r0
   de564:	4660      	mov	r0, ip
   de566:	e7f6      	b.n	de556 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2da>
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de568:	f8d5 c000 	ldr.w	ip, [r5]
   de56c:	4628      	mov	r0, r5
   de56e:	2600      	movs	r6, #0
   de570:	2501      	movs	r5, #1
   de572:	45b4      	cmp	ip, r6
   de574:	dd05      	ble.n	de582 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x306>
    input_size *= input_dims->data[i];
   de576:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= output_dims->data[i];
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de57a:	3601      	adds	r6, #1
    input_size *= input_dims->data[i];
   de57c:	fb0e f505 	mul.w	r5, lr, r5
   de580:	e7f7      	b.n	de572 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2f6>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);
   de582:	fb02 f007 	mul.w	r0, r2, r7
   de586:	4285      	cmp	r5, r0
   de588:	f47f aec9 	bne.w	de31e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xa2>
   de58c:	685b      	ldr	r3, [r3, #4]
   de58e:	9301      	str	r3, [sp, #4]
   de590:	fb02 f308 	mul.w	r3, r2, r8
   de594:	009b      	lsls	r3, r3, #2
   de596:	2500      	movs	r5, #0
   de598:	ea4f 0c82 	mov.w	ip, r2, lsl #2
   de59c:	9302      	str	r3, [sp, #8]

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de59e:	462e      	mov	r6, r5
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
   de5a0:	f04f 0938 	mov.w	r9, #56	; 0x38
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de5a4:	45b0      	cmp	r8, r6
   de5a6:	f77f aec8 	ble.w	de33a <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xbe>
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
   de5aa:	680b      	ldr	r3, [r1, #0]
   de5ac:	eb03 0386 	add.w	r3, r3, r6, lsl #2
   de5b0:	6858      	ldr	r0, [r3, #4]
   de5b2:	9b00      	ldr	r3, [sp, #0]
   de5b4:	689b      	ldr	r3, [r3, #8]
   de5b6:	fb09 3300 	mla	r3, r9, r0, r3
   de5ba:	b103      	cbz	r3, de5be <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x342>
   de5bc:	685b      	ldr	r3, [r3, #4]
   de5be:	f04f 0e00 	mov.w	lr, #0
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de5c2:	46f2      	mov	sl, lr
   de5c4:	4557      	cmp	r7, sl
   de5c6:	dd12      	ble.n	de5ee <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x372>
      const T* input_ptr = input_data + copy_size * k;
      int loc = k * values_count * copy_size + i * copy_size;
      T* output_ptr = output_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   de5c8:	9c01      	ldr	r4, [sp, #4]
   de5ca:	eb05 0b0e 	add.w	fp, r5, lr
  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de5ce:	2000      	movs	r0, #0
      const T* input_ptr = input_data + copy_size * k;
      int loc = k * values_count * copy_size + i * copy_size;
      T* output_ptr = output_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   de5d0:	44a3      	add	fp, r4
   de5d2:	4282      	cmp	r2, r0
   de5d4:	dd05      	ble.n	de5e2 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x366>
   de5d6:	f853 4020 	ldr.w	r4, [r3, r0, lsl #2]
   de5da:	f84b 4020 	str.w	r4, [fp, r0, lsl #2]
   de5de:	3001      	adds	r0, #1
   de5e0:	e7f7      	b.n	de5d2 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x356>
   de5e2:	9802      	ldr	r0, [sp, #8]
  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de5e4:	f10a 0a01 	add.w	sl, sl, #1
   de5e8:	4463      	add	r3, ip
   de5ea:	4486      	add	lr, r0
   de5ec:	e7ea      	b.n	de5c4 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x348>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de5ee:	3601      	adds	r6, #1
   de5f0:	4465      	add	r5, ip
   de5f2:	e7d7      	b.n	de5a4 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x328>
    case kTfLiteInt32: {
      return PackImpl<int32_t>(context, node, output, data->values_count,
                               data->axis);
    }
    case kTfLiteInt64: {
      return PackImpl<int64_t>(context, node, output, data->values_count,
   de5f4:	6810      	ldr	r0, [r2, #0]
   de5f6:	9001      	str	r0, [sp, #4]

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
   de5f8:	6808      	ldr	r0, [r1, #0]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de5fa:	689e      	ldr	r6, [r3, #8]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de5fc:	6840      	ldr	r0, [r0, #4]
      return PackImpl<int32_t>(context, node, output, data->values_count,
                               data->axis);
    }
    case kTfLiteInt64: {
      return PackImpl<int64_t>(context, node, output, data->values_count,
                               data->axis);
   de5fe:	6852      	ldr	r2, [r2, #4]
}

template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
   de600:	6837      	ldr	r7, [r6, #0]
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de602:	f04f 0e38 	mov.w	lr, #56	; 0x38
   de606:	fb0e 5500 	mla	r5, lr, r0, r5
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
   de60a:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                      TfLiteTensor* output, int values_count, int axis) {
  const int dimensions = output->dims->size;
  const TfLiteTensor* input0 = &context->tensors[node->inputs->data[0]];
  const TfLiteIntArray* input_dims = input0->dims;
   de60c:	68ad      	ldr	r5, [r5, #8]
  const TfLiteIntArray* output_dims = output->dims;

  if (axis < 0) {
    axis += dimensions;
   de60e:	bfb8      	it	lt
   de610:	19d2      	addlt	r2, r2, r7
   de612:	46b4      	mov	ip, r6
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de614:	2000      	movs	r0, #0

  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
   de616:	f04f 0e01 	mov.w	lr, #1
  for (int i = 0; i < axis; ++i) {
   de61a:	4282      	cmp	r2, r0
   de61c:	dd05      	ble.n	de62a <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3ae>
    outer_size *= output_dims->data[i];
   de61e:	f85c 8f04 	ldr.w	r8, [ip, #4]!
  if (axis < 0) {
    axis += dimensions;
  }

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   de622:	3001      	adds	r0, #1
    outer_size *= output_dims->data[i];
   de624:	fb08 fe0e 	mul.w	lr, r8, lr
   de628:	e7f7      	b.n	de61a <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x39e>
   de62a:	1c50      	adds	r0, r2, #1
  }
  int copy_size = 1;
   de62c:	2201      	movs	r2, #1
  for (int i = axis + 1; i < dimensions; ++i) {
   de62e:	4287      	cmp	r7, r0
   de630:	f100 0c01 	add.w	ip, r0, #1
   de634:	dd04      	ble.n	de640 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3c4>
    copy_size *= output_dims->data[i];
   de636:	f856 002c 	ldr.w	r0, [r6, ip, lsl #2]
   de63a:	4342      	muls	r2, r0
   de63c:	4660      	mov	r0, ip
   de63e:	e7f6      	b.n	de62e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3b2>
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de640:	f8d5 c000 	ldr.w	ip, [r5]
   de644:	4628      	mov	r0, r5
   de646:	2600      	movs	r6, #0
   de648:	2501      	movs	r5, #1
   de64a:	45b4      	cmp	ip, r6
   de64c:	dd04      	ble.n	de658 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3dc>
    input_size *= input_dims->data[i];
   de64e:	f850 7f04 	ldr.w	r7, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= output_dims->data[i];
  }
  int input_size = 1;
  for (int i = 0; i < input_dims->size; ++i) {
   de652:	3601      	adds	r6, #1
    input_size *= input_dims->data[i];
   de654:	437d      	muls	r5, r7
   de656:	e7f8      	b.n	de64a <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3ce>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);
   de658:	fb02 f00e 	mul.w	r0, r2, lr
   de65c:	4285      	cmp	r5, r0
   de65e:	f47f ae5e 	bne.w	de31e <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xa2>
   de662:	685b      	ldr	r3, [r3, #4]
   de664:	9302      	str	r3, [sp, #8]
   de666:	9b01      	ldr	r3, [sp, #4]
   de668:	4353      	muls	r3, r2
   de66a:	00db      	lsls	r3, r3, #3
   de66c:	2000      	movs	r0, #0
   de66e:	00d7      	lsls	r7, r2, #3
   de670:	9303      	str	r3, [sp, #12]

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de672:	4605      	mov	r5, r0
   de674:	9b01      	ldr	r3, [sp, #4]
   de676:	42ab      	cmp	r3, r5
   de678:	f77f ae5f 	ble.w	de33a <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xbe>
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
   de67c:	680b      	ldr	r3, [r1, #0]
   de67e:	eb03 0385 	add.w	r3, r3, r5, lsl #2
   de682:	2438      	movs	r4, #56	; 0x38
   de684:	685e      	ldr	r6, [r3, #4]
   de686:	9b00      	ldr	r3, [sp, #0]
   de688:	689b      	ldr	r3, [r3, #8]
   de68a:	fb04 3306 	mla	r3, r4, r6, r3
   de68e:	b103      	cbz	r3, de692 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x416>
   de690:	685b      	ldr	r3, [r3, #4]
   de692:	f04f 0c00 	mov.w	ip, #0
   de696:	461e      	mov	r6, r3
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de698:	46e1      	mov	r9, ip
   de69a:	45ce      	cmp	lr, r9
   de69c:	dd15      	ble.n	de6ca <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x44e>
   de69e:	9c02      	ldr	r4, [sp, #8]
   de6a0:	eb00 080c 	add.w	r8, r0, ip
   de6a4:	44a0      	add	r8, r4
   de6a6:	46b3      	mov	fp, r6
      const T* input_ptr = input_data + copy_size * k;
      int loc = k * values_count * copy_size + i * copy_size;
      T* output_ptr = output_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   de6a8:	f04f 0a00 	mov.w	sl, #0
   de6ac:	4552      	cmp	r2, sl
   de6ae:	dd06      	ble.n	de6be <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x442>
   de6b0:	e8fb 3402 	ldrd	r3, r4, [fp], #8
   de6b4:	f10a 0a01 	add.w	sl, sl, #1
   de6b8:	e8e8 3402 	strd	r3, r4, [r8], #8
   de6bc:	e7f6      	b.n	de6ac <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x430>
   de6be:	9c03      	ldr	r4, [sp, #12]
  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->inputs->data[i]];
    const T* input_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   de6c0:	f109 0901 	add.w	r9, r9, #1
   de6c4:	443e      	add	r6, r7
   de6c6:	44a4      	add	ip, r4
   de6c8:	e7e7      	b.n	de69a <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x41e>
  }
  TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);

  T* output_data = GetTensorData<T>(output);

  for (int i = 0; i < values_count; ++i) {
   de6ca:	3501      	adds	r5, #1
   de6cc:	4438      	add	r0, r7
   de6ce:	e7d1      	b.n	de674 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3f8>
    case kTfLiteInt64: {
      return PackImpl<int64_t>(context, node, output, data->values_count,
                               data->axis);
    }
    default: {
      context->ReportError(context, "Type '%s' is not supported by pack.",
   de6d0:	9b00      	ldr	r3, [sp, #0]
   de6d2:	695d      	ldr	r5, [r3, #20]
   de6d4:	f7f5 fd22 	bl	d411c <TfLiteTypeGetName>
                           TfLiteTypeGetName(output->type));
   de6d8:	4903      	ldr	r1, [pc, #12]	; (de6e8 <_ZN6tflite3ops5micro4pack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x46c>)
   de6da:	4602      	mov	r2, r0
   de6dc:	9800      	ldr	r0, [sp, #0]
   de6de:	47a8      	blx	r5
      return kTfLiteError;
   de6e0:	2001      	movs	r0, #1
    }
  }

  return kTfLiteOk;
}
   de6e2:	b005      	add	sp, #20
   de6e4:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   de6e8:	000e9f3e 	.word	0x000e9f3e

000de6ec <_ZN6tflite3ops5micro13Register_PACKEv>:
}  // namespace pack

TfLiteRegistration* Register_PACK() {
  static TfLiteRegistration r = {nullptr, nullptr, pack::Prepare, pack::Eval};
  return &r;
}
   de6ec:	4800      	ldr	r0, [pc, #0]	; (de6f0 <_ZN6tflite3ops5micro13Register_PACKEv+0x4>)
   de6ee:	4770      	bx	lr
   de6f0:	2003c1fc 	.word	0x2003c1fc

000de6f4 <_ZN6tflite3ops5micro7pooling4InitEP13TfLiteContextPKcj>:

}  // namespace

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
   de6f4:	2000      	movs	r0, #0
   de6f6:	4770      	bx	lr

000de6f8 <_ZN6tflite3ops5micro7pooling4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
   de6f8:	4770      	bx	lr

000de6fa <_ZN6tflite3ops5micro7pooling7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   de6fa:	2000      	movs	r0, #0
   de6fc:	4770      	bx	lr
	...

000de700 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf>:
namespace reference_ops {

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape,
                        const float* input_data,
                        const RuntimeShape& output_shape, float* output_data) {
   de700:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   de704:	ed2d 8b04 	vpush	{d8-d9}
   de708:	461d      	mov	r5, r3
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   de70a:	680b      	ldr	r3, [r1, #0]
namespace reference_ops {

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape,
                        const float* input_data,
                        const RuntimeShape& output_shape, float* output_data) {
   de70c:	b095      	sub	sp, #84	; 0x54
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   de70e:	2b04      	cmp	r3, #4
namespace reference_ops {

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape,
                        const float* input_data,
                        const RuntimeShape& output_shape, float* output_data) {
   de710:	4604      	mov	r4, r0
   de712:	468a      	mov	sl, r1
   de714:	9212      	str	r2, [sp, #72]	; 0x48
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   de716:	d001      	beq.n	de71c <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x1c>
   de718:	f005 fe18 	bl	e434c <abort>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   de71c:	682b      	ldr	r3, [r5, #0]
   de71e:	2b04      	cmp	r3, #4
   de720:	d1fa      	bne.n	de718 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x18>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   de722:	2300      	movs	r3, #0
   de724:	4619      	mov	r1, r3
   de726:	462a      	mov	r2, r5
   de728:	4650      	mov	r0, sl
   de72a:	f7fd fa4e 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   de72e:	2303      	movs	r3, #3
   de730:	4619      	mov	r1, r3
   de732:	462a      	mov	r2, r5
                        const RuntimeShape& input_shape,
                        const float* input_data,
                        const RuntimeShape& output_shape, float* output_data) {
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   de734:	9008      	str	r0, [sp, #32]
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   de736:	4650      	mov	r0, sl
   de738:	f7fd fa47 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
   de73c:	2101      	movs	r1, #1
                        const float* input_data,
                        const RuntimeShape& output_shape, float* output_data) {
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   de73e:	9009      	str	r0, [sp, #36]	; 0x24
  const int input_height = input_shape.Dims(1);
   de740:	4650      	mov	r0, sl
   de742:	f7f7 fe91 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   de746:	2102      	movs	r1, #2
                        const RuntimeShape& output_shape, float* output_data) {
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
   de748:	900a      	str	r0, [sp, #40]	; 0x28
  const int input_width = input_shape.Dims(2);
   de74a:	4650      	mov	r0, sl
   de74c:	f7f7 fe8c 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   de750:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   de752:	900b      	str	r0, [sp, #44]	; 0x2c
  const int output_height = output_shape.Dims(1);
   de754:	4628      	mov	r0, r5
   de756:	f7f7 fe87 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   de75a:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   de75c:	900c      	str	r0, [sp, #48]	; 0x30
  const int output_width = output_shape.Dims(2);
   de75e:	4628      	mov	r0, r5
   de760:	f7f7 fe82 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int stride_height = params.stride_height;
   de764:	68e3      	ldr	r3, [r4, #12]
   de766:	930f      	str	r3, [sp, #60]	; 0x3c
  const int stride_width = params.stride_width;
   de768:	6923      	ldr	r3, [r4, #16]
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   de76a:	900e      	str	r0, [sp, #56]	; 0x38
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
   de76c:	9310      	str	r3, [sp, #64]	; 0x40
  for (int batch = 0; batch < batches; ++batch) {
   de76e:	f04f 0b00 	mov.w	fp, #0
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              total +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
              filter_count++;
   de772:	eef7 9a00 	vmov.f32	s19, #112	; 0x3f800000  1.0
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
   de776:	9b08      	ldr	r3, [sp, #32]
   de778:	459b      	cmp	fp, r3
   de77a:	f280 8092 	bge.w	de8a2 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x1a2>
   de77e:	2300      	movs	r3, #0
   de780:	9304      	str	r3, [sp, #16]
   de782:	9302      	str	r3, [sp, #8]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   de784:	9b02      	ldr	r3, [sp, #8]
   de786:	9a0c      	ldr	r2, [sp, #48]	; 0x30
   de788:	4293      	cmp	r3, r2
   de78a:	f280 8087 	bge.w	de89c <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x19c>
   de78e:	2300      	movs	r3, #0
   de790:	9305      	str	r3, [sp, #20]
   de792:	9303      	str	r3, [sp, #12]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   de794:	9b03      	ldr	r3, [sp, #12]
   de796:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   de798:	4293      	cmp	r3, r2
   de79a:	da77      	bge.n	de88c <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x18c>
   de79c:	f04f 0900 	mov.w	r9, #0
        for (int channel = 0; channel < depth; ++channel) {
   de7a0:	9b09      	ldr	r3, [sp, #36]	; 0x24
   de7a2:	4599      	cmp	r9, r3
   de7a4:	da6a      	bge.n	de87c <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x17c>
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
   de7a6:	f9b4 7002 	ldrsh.w	r7, [r4, #2]
   de7aa:	9b05      	ldr	r3, [sp, #20]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
   de7ac:	f9b4 8004 	ldrsh.w	r8, [r4, #4]
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float total = 0.f;
          float filter_count = 0;
   de7b0:	ed9f 8a3e 	vldr	s16, [pc, #248]	; de8ac <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x1ac>
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
   de7b4:	1bdb      	subs	r3, r3, r7
   de7b6:	9306      	str	r3, [sp, #24]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
   de7b8:	9b04      	ldr	r3, [sp, #16]
   de7ba:	9a06      	ldr	r2, [sp, #24]
   de7bc:	ebc8 0803 	rsb	r8, r8, r3
   de7c0:	9b06      	ldr	r3, [sp, #24]
   de7c2:	425b      	negs	r3, r3
   de7c4:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
   de7c8:	9307      	str	r3, [sp, #28]
   de7ca:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   de7cc:	1a9a      	subs	r2, r3, r2
   de7ce:	69a3      	ldr	r3, [r4, #24]
   de7d0:	429a      	cmp	r2, r3
   de7d2:	bfa8      	it	ge
   de7d4:	461a      	movge	r2, r3
   de7d6:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   de7d8:	920d      	str	r2, [sp, #52]	; 0x34
   de7da:	6962      	ldr	r2, [r4, #20]
   de7dc:	ebc8 0303 	rsb	r3, r8, r3
   de7e0:	4293      	cmp	r3, r2
   de7e2:	f1c8 0600 	rsb	r6, r8, #0
   de7e6:	bfa8      	it	ge
   de7e8:	4613      	movge	r3, r2
   de7ea:	ea26 76e6 	bic.w	r6, r6, r6, asr #31
   de7ee:	9311      	str	r3, [sp, #68]	; 0x44
          const int filter_x_end =
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float total = 0.f;
   de7f0:	eef0 8a48 	vmov.f32	s17, s16
          float filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   de7f4:	9b11      	ldr	r3, [sp, #68]	; 0x44
   de7f6:	429e      	cmp	r6, r3
   de7f8:	da1c      	bge.n	de834 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x134>
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              total +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
   de7fa:	eb08 0306 	add.w	r3, r8, r6
   de7fe:	9f07      	ldr	r7, [sp, #28]
   de800:	9313      	str	r3, [sp, #76]	; 0x4c
              std::min(params.filter_height, input_height - in_y_origin);
          float total = 0.f;
          float filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   de802:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   de804:	429f      	cmp	r7, r3
   de806:	da13      	bge.n	de830 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x130>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              total +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
   de808:	9b06      	ldr	r3, [sp, #24]
   de80a:	f8cd 9000 	str.w	r9, [sp]
   de80e:	18fb      	adds	r3, r7, r3
   de810:	9a13      	ldr	r2, [sp, #76]	; 0x4c
   de812:	4659      	mov	r1, fp
   de814:	4650      	mov	r0, sl
   de816:	f7f7 fe8c 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   de81a:	9b12      	ldr	r3, [sp, #72]	; 0x48
   de81c:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   de820:	edd0 7a00 	vldr	s15, [r0]
              filter_count++;
   de824:	ee38 8a29 	vadd.f32	s16, s16, s19
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              total +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
   de828:	ee78 8aa7 	vadd.f32	s17, s17, s15
              std::min(params.filter_height, input_height - in_y_origin);
          float total = 0.f;
          float filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   de82c:	3701      	adds	r7, #1
   de82e:	e7e8      	b.n	de802 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x102>
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float total = 0.f;
          float filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   de830:	3601      	adds	r6, #1
   de832:	e7df      	b.n	de7f4 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0xf4>
              total +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
              filter_count++;
            }
          }
          const float average = total / filter_count;
   de834:	ee88 9a88 	vdiv.f32	s18, s17, s16
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
   de838:	f8cd 9000 	str.w	r9, [sp]
   de83c:	9b03      	ldr	r3, [sp, #12]
   de83e:	9a02      	ldr	r2, [sp, #8]
   de840:	4659      	mov	r1, fp
   de842:	4628      	mov	r0, r5
   de844:	f7f7 fe75 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   de848:	9b22      	ldr	r3, [sp, #136]	; 0x88
   de84a:	eb03 0080 	add.w	r0, r3, r0, lsl #2
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
   de84e:	f109 0901 	add.w	r9, r9, #1
              filter_count++;
            }
          }
          const float average = total / filter_count;
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
              ActivationFunctionWithMinMax(average, params.float_activation_min,
   de852:	edd4 7a09 	vldr	s15, [r4, #36]	; 0x24
                                           params.float_activation_max);
   de856:	ed94 7a0a 	vldr	s14, [r4, #40]	; 0x28
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
	return __b;
      return __a;
   de85a:	eeb4 9ae7 	vcmpe.f32	s18, s15
   de85e:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   de862:	bf58      	it	pl
   de864:	eef0 7a49 	vmovpl.f32	s15, s18
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   de868:	eeb4 7a67 	vcmp.f32	s14, s15
   de86c:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   de870:	bf48      	it	mi
   de872:	eef0 7a47 	vmovmi.f32	s15, s14
   de876:	edc0 7a00 	vstr	s15, [r0]
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
   de87a:	e791      	b.n	de7a0 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0xa0>
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   de87c:	9b03      	ldr	r3, [sp, #12]
   de87e:	9a10      	ldr	r2, [sp, #64]	; 0x40
   de880:	3301      	adds	r3, #1
   de882:	9303      	str	r3, [sp, #12]
   de884:	9b05      	ldr	r3, [sp, #20]
   de886:	4413      	add	r3, r2
   de888:	9305      	str	r3, [sp, #20]
   de88a:	e783      	b.n	de794 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x94>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   de88c:	9b02      	ldr	r3, [sp, #8]
   de88e:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
   de890:	3301      	adds	r3, #1
   de892:	9302      	str	r3, [sp, #8]
   de894:	9b04      	ldr	r3, [sp, #16]
   de896:	4413      	add	r3, r2
   de898:	9304      	str	r3, [sp, #16]
   de89a:	e773      	b.n	de784 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x84>
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
   de89c:	f10b 0b01 	add.w	fp, fp, #1
   de8a0:	e769      	b.n	de776 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x76>
                                           params.float_activation_max);
        }
      }
    }
  }
}
   de8a2:	b015      	add	sp, #84	; 0x54
   de8a4:	ecbd 8b04 	vpop	{d8-d9}
   de8a8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   de8ac:	00000000 	.word	0x00000000

000de8b0 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph>:

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape,
                        const uint8* input_data,
                        const RuntimeShape& output_shape, uint8* output_data) {
   de8b0:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   de8b4:	b097      	sub	sp, #92	; 0x5c
   de8b6:	461f      	mov	r7, r3
   de8b8:	9214      	str	r2, [sp, #80]	; 0x50
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
   de8ba:	6a03      	ldr	r3, [r0, #32]
   de8bc:	69c2      	ldr	r2, [r0, #28]
   de8be:	429a      	cmp	r2, r3
}

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape,
                        const uint8* input_data,
                        const RuntimeShape& output_shape, uint8* output_data) {
   de8c0:	4604      	mov	r4, r0
   de8c2:	460e      	mov	r6, r1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
   de8c4:	dd01      	ble.n	de8ca <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x1a>

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape,
                        const uint8* input_data,
                        const RuntimeShape& output_shape, uint8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
   de8c6:	f005 fd41 	bl	e434c <abort>
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   de8ca:	680b      	ldr	r3, [r1, #0]
   de8cc:	2b04      	cmp	r3, #4
   de8ce:	d1fa      	bne.n	de8c6 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x16>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   de8d0:	683b      	ldr	r3, [r7, #0]
   de8d2:	2b04      	cmp	r3, #4
   de8d4:	d1f7      	bne.n	de8c6 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x16>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   de8d6:	2300      	movs	r3, #0
   de8d8:	4619      	mov	r1, r3
   de8da:	463a      	mov	r2, r7
   de8dc:	4630      	mov	r0, r6
   de8de:	f7fd f974 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   de8e2:	2303      	movs	r3, #3
   de8e4:	4619      	mov	r1, r3
   de8e6:	463a      	mov	r2, r7
                        const RuntimeShape& output_shape, uint8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   de8e8:	900a      	str	r0, [sp, #40]	; 0x28
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   de8ea:	4630      	mov	r0, r6
   de8ec:	f7fd f96d 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
   de8f0:	2101      	movs	r1, #1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   de8f2:	900b      	str	r0, [sp, #44]	; 0x2c
  const int input_height = input_shape.Dims(1);
   de8f4:	4630      	mov	r0, r6
   de8f6:	f7f7 fdb7 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   de8fa:	2102      	movs	r1, #2
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
   de8fc:	900c      	str	r0, [sp, #48]	; 0x30
  const int input_width = input_shape.Dims(2);
   de8fe:	4630      	mov	r0, r6
   de900:	f7f7 fdb2 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   de904:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   de906:	900d      	str	r0, [sp, #52]	; 0x34
  const int output_height = output_shape.Dims(1);
   de908:	4638      	mov	r0, r7
   de90a:	f7f7 fdad 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   de90e:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   de910:	900e      	str	r0, [sp, #56]	; 0x38
  const int output_width = output_shape.Dims(2);
   de912:	4638      	mov	r0, r7
   de914:	f7f7 fda8 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int stride_height = params.stride_height;
   de918:	68e3      	ldr	r3, [r4, #12]
   de91a:	9310      	str	r3, [sp, #64]	; 0x40
  const int stride_width = params.stride_width;
   de91c:	6923      	ldr	r3, [r4, #16]
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   de91e:	900f      	str	r0, [sp, #60]	; 0x3c
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
   de920:	9311      	str	r3, [sp, #68]	; 0x44
  for (int batch = 0; batch < batches; ++batch) {
   de922:	f04f 0a00 	mov.w	sl, #0
   de926:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   de928:	459a      	cmp	sl, r3
   de92a:	f280 8088 	bge.w	dea3e <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x18e>
   de92e:	2300      	movs	r3, #0
   de930:	9305      	str	r3, [sp, #20]
   de932:	9303      	str	r3, [sp, #12]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   de934:	9b03      	ldr	r3, [sp, #12]
   de936:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   de938:	4293      	cmp	r3, r2
   de93a:	da7d      	bge.n	dea38 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x188>
   de93c:	2300      	movs	r3, #0
   de93e:	9306      	str	r3, [sp, #24]
   de940:	9304      	str	r3, [sp, #16]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   de942:	9b04      	ldr	r3, [sp, #16]
   de944:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
   de946:	4293      	cmp	r3, r2
   de948:	da6e      	bge.n	dea28 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x178>
   de94a:	2300      	movs	r3, #0
   de94c:	9302      	str	r3, [sp, #8]
        for (int channel = 0; channel < depth; ++channel) {
   de94e:	9b02      	ldr	r3, [sp, #8]
   de950:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   de952:	4293      	cmp	r3, r2
   de954:	da60      	bge.n	dea18 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x168>
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
   de956:	9a06      	ldr	r2, [sp, #24]
   de958:	f9b4 3002 	ldrsh.w	r3, [r4, #2]
   de95c:	1ad3      	subs	r3, r2, r3
   de95e:	9308      	str	r3, [sp, #32]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
   de960:	9a05      	ldr	r2, [sp, #20]
   de962:	f9b4 3004 	ldrsh.w	r3, [r4, #4]
   de966:	1ad3      	subs	r3, r2, r3
   de968:	9309      	str	r3, [sp, #36]	; 0x24
   de96a:	9b08      	ldr	r3, [sp, #32]
   de96c:	9a08      	ldr	r2, [sp, #32]
   de96e:	425b      	negs	r3, r3
   de970:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
   de974:	9307      	str	r3, [sp, #28]
   de976:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   de978:	1a9a      	subs	r2, r3, r2
   de97a:	69a3      	ldr	r3, [r4, #24]
   de97c:	429a      	cmp	r2, r3
   de97e:	bfa8      	it	ge
   de980:	461a      	movge	r2, r3
   de982:	9b09      	ldr	r3, [sp, #36]	; 0x24
   de984:	9213      	str	r2, [sp, #76]	; 0x4c
   de986:	f1c3 0800 	rsb	r8, r3, #0
   de98a:	9a09      	ldr	r2, [sp, #36]	; 0x24
   de98c:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   de98e:	1a9a      	subs	r2, r3, r2
   de990:	6963      	ldr	r3, [r4, #20]
   de992:	429a      	cmp	r2, r3
   de994:	bfa8      	it	ge
   de996:	461a      	movge	r2, r3
          const int filter_x_end =
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
   de998:	2500      	movs	r5, #0
   de99a:	ea28 78e8 	bic.w	r8, r8, r8, asr #31
   de99e:	9212      	str	r2, [sp, #72]	; 0x48
          int filter_count = 0;
   de9a0:	46ab      	mov	fp, r5
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   de9a2:	9b12      	ldr	r3, [sp, #72]	; 0x48
   de9a4:	4598      	cmp	r8, r3
   de9a6:	da1e      	bge.n	de9e6 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x136>
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
   de9a8:	9b09      	ldr	r3, [sp, #36]	; 0x24
   de9aa:	f8dd 901c 	ldr.w	r9, [sp, #28]
   de9ae:	4443      	add	r3, r8
   de9b0:	ebc9 0b0b 	rsb	fp, r9, fp
   de9b4:	9315      	str	r3, [sp, #84]	; 0x54
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   de9b6:	9a13      	ldr	r2, [sp, #76]	; 0x4c
   de9b8:	4591      	cmp	r9, r2
   de9ba:	eb0b 0309 	add.w	r3, fp, r9
   de9be:	da0e      	bge.n	de9de <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x12e>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
   de9c0:	9b02      	ldr	r3, [sp, #8]
   de9c2:	9300      	str	r3, [sp, #0]
   de9c4:	9b08      	ldr	r3, [sp, #32]
   de9c6:	9a15      	ldr	r2, [sp, #84]	; 0x54
   de9c8:	444b      	add	r3, r9
   de9ca:	4651      	mov	r1, sl
   de9cc:	4630      	mov	r0, r6
   de9ce:	f7f7 fdb0 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   de9d2:	9b14      	ldr	r3, [sp, #80]	; 0x50
   de9d4:	5c1b      	ldrb	r3, [r3, r0]
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   de9d6:	f109 0901 	add.w	r9, r9, #1
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
   de9da:	441d      	add	r5, r3
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   de9dc:	e7eb      	b.n	de9b6 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x106>
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   de9de:	f108 0801 	add.w	r8, r8, #1
   de9e2:	469b      	mov	fp, r3
   de9e4:	e7dd      	b.n	de9a2 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xf2>
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
              filter_count++;
            }
          }
          acc = (acc + filter_count / 2) / filter_count;
   de9e6:	eb05 056b 	add.w	r5, r5, fp, asr #1
   de9ea:	fb95 fbfb 	sdiv	fp, r5, fp
   de9ee:	69e5      	ldr	r5, [r4, #28]
   de9f0:	6a23      	ldr	r3, [r4, #32]
          acc = std::max(acc, params.quantized_activation_min);
          acc = std::min(acc, params.quantized_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
   de9f2:	9a03      	ldr	r2, [sp, #12]
   de9f4:	455d      	cmp	r5, fp
   de9f6:	bfb8      	it	lt
   de9f8:	465d      	movlt	r5, fp
   de9fa:	429d      	cmp	r5, r3
   de9fc:	bfa8      	it	ge
   de9fe:	461d      	movge	r5, r3
   dea00:	9b02      	ldr	r3, [sp, #8]
   dea02:	9300      	str	r3, [sp, #0]
   dea04:	4651      	mov	r1, sl
   dea06:	9b04      	ldr	r3, [sp, #16]
   dea08:	4638      	mov	r0, r7
   dea0a:	f7f7 fd92 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<uint8>(acc);
   dea0e:	9b20      	ldr	r3, [sp, #128]	; 0x80
   dea10:	541d      	strb	r5, [r3, r0]
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
   dea12:	9b02      	ldr	r3, [sp, #8]
   dea14:	3301      	adds	r3, #1
   dea16:	e799      	b.n	de94c <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x9c>
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   dea18:	9b04      	ldr	r3, [sp, #16]
   dea1a:	9a11      	ldr	r2, [sp, #68]	; 0x44
   dea1c:	3301      	adds	r3, #1
   dea1e:	9304      	str	r3, [sp, #16]
   dea20:	9b06      	ldr	r3, [sp, #24]
   dea22:	4413      	add	r3, r2
   dea24:	9306      	str	r3, [sp, #24]
   dea26:	e78c      	b.n	de942 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x92>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   dea28:	9b03      	ldr	r3, [sp, #12]
   dea2a:	9a10      	ldr	r2, [sp, #64]	; 0x40
   dea2c:	3301      	adds	r3, #1
   dea2e:	9303      	str	r3, [sp, #12]
   dea30:	9b05      	ldr	r3, [sp, #20]
   dea32:	4413      	add	r3, r2
   dea34:	9305      	str	r3, [sp, #20]
   dea36:	e77d      	b.n	de934 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x84>
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
   dea38:	f10a 0a01 	add.w	sl, sl, #1
   dea3c:	e773      	b.n	de926 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x76>
              static_cast<uint8>(acc);
        }
      }
    }
  }
}
   dea3e:	b017      	add	sp, #92	; 0x5c
   dea40:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dea44 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf>:
  }
}

inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const float* input_data, const RuntimeShape& output_shape,
                    float* output_data) {
   dea44:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dea48:	ed2d 8b02 	vpush	{d8}
   dea4c:	461e      	mov	r6, r3
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dea4e:	680b      	ldr	r3, [r1, #0]
  }
}

inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const float* input_data, const RuntimeShape& output_shape,
                    float* output_data) {
   dea50:	b097      	sub	sp, #92	; 0x5c
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dea52:	2b04      	cmp	r3, #4
  }
}

inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const float* input_data, const RuntimeShape& output_shape,
                    float* output_data) {
   dea54:	4604      	mov	r4, r0
   dea56:	460d      	mov	r5, r1
   dea58:	9212      	str	r2, [sp, #72]	; 0x48
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dea5a:	d001      	beq.n	dea60 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x1c>
   dea5c:	f005 fc76 	bl	e434c <abort>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   dea60:	6833      	ldr	r3, [r6, #0]
   dea62:	2b04      	cmp	r3, #4
   dea64:	d1fa      	bne.n	dea5c <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x18>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dea66:	2300      	movs	r3, #0
   dea68:	4619      	mov	r1, r3
   dea6a:	4632      	mov	r2, r6
   dea6c:	4628      	mov	r0, r5
   dea6e:	f7fd f8ac 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   dea72:	2303      	movs	r3, #3
   dea74:	4619      	mov	r1, r3
   dea76:	4632      	mov	r2, r6
inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const float* input_data, const RuntimeShape& output_shape,
                    float* output_data) {
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dea78:	9008      	str	r0, [sp, #32]
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   dea7a:	4628      	mov	r0, r5
   dea7c:	f7fd f8a5 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
   dea80:	2101      	movs	r1, #1
                    const float* input_data, const RuntimeShape& output_shape,
                    float* output_data) {
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   dea82:	9009      	str	r0, [sp, #36]	; 0x24
  const int input_height = input_shape.Dims(1);
   dea84:	4628      	mov	r0, r5
   dea86:	f7f7 fcef 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   dea8a:	2102      	movs	r1, #2
                    float* output_data) {
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
   dea8c:	900a      	str	r0, [sp, #40]	; 0x28
  const int input_width = input_shape.Dims(2);
   dea8e:	4628      	mov	r0, r5
   dea90:	f7f7 fcea 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   dea94:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   dea96:	900b      	str	r0, [sp, #44]	; 0x2c
  const int output_height = output_shape.Dims(1);
   dea98:	4630      	mov	r0, r6
   dea9a:	f7f7 fce5 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   dea9e:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   deaa0:	900c      	str	r0, [sp, #48]	; 0x30
  const int output_width = output_shape.Dims(2);
   deaa2:	4630      	mov	r0, r6
   deaa4:	f7f7 fce0 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int stride_height = params.stride_height;
   deaa8:	68e3      	ldr	r3, [r4, #12]
   deaaa:	930e      	str	r3, [sp, #56]	; 0x38
          const int filter_x_end =
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float max = std::numeric_limits<float>::lowest();
   deaac:	ed9f 8a54 	vldr	s16, [pc, #336]	; dec00 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x1bc>
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
   deab0:	6923      	ldr	r3, [r4, #16]
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   deab2:	900d      	str	r0, [sp, #52]	; 0x34
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
   deab4:	930f      	str	r3, [sp, #60]	; 0x3c
  for (int batch = 0; batch < batches; ++batch) {
   deab6:	f04f 0b00 	mov.w	fp, #0
   deaba:	9b08      	ldr	r3, [sp, #32]
   deabc:	459b      	cmp	fp, r3
   deabe:	f280 8099 	bge.w	debf4 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x1b0>
   deac2:	2300      	movs	r3, #0
   deac4:	9304      	str	r3, [sp, #16]
   deac6:	9302      	str	r3, [sp, #8]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   deac8:	9b02      	ldr	r3, [sp, #8]
   deaca:	9a0c      	ldr	r2, [sp, #48]	; 0x30
   deacc:	4293      	cmp	r3, r2
   deace:	f280 808e 	bge.w	debee <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x1aa>
   dead2:	2300      	movs	r3, #0
   dead4:	9305      	str	r3, [sp, #20]
   dead6:	9303      	str	r3, [sp, #12]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   dead8:	9b03      	ldr	r3, [sp, #12]
   deada:	9a0d      	ldr	r2, [sp, #52]	; 0x34
   deadc:	4293      	cmp	r3, r2
   deade:	da7e      	bge.n	debde <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x19a>
   deae0:	f04f 0800 	mov.w	r8, #0
        for (int channel = 0; channel < depth; ++channel) {
   deae4:	9b09      	ldr	r3, [sp, #36]	; 0x24
   deae6:	4598      	cmp	r8, r3
   deae8:	da71      	bge.n	debce <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x18a>
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
   deaea:	f9b4 9002 	ldrsh.w	r9, [r4, #2]
   deaee:	9b05      	ldr	r3, [sp, #20]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
   deaf0:	f9b4 a004 	ldrsh.w	sl, [r4, #4]
          const int filter_x_end =
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float max = std::numeric_limits<float>::lowest();
   deaf4:	ed8d 8a15 	vstr	s16, [sp, #84]	; 0x54
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
   deaf8:	ebc9 0303 	rsb	r3, r9, r3
   deafc:	9307      	str	r3, [sp, #28]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
   deafe:	9b04      	ldr	r3, [sp, #16]
   deb00:	9a07      	ldr	r2, [sp, #28]
   deb02:	ebca 0a03 	rsb	sl, sl, r3
   deb06:	9b07      	ldr	r3, [sp, #28]
   deb08:	425b      	negs	r3, r3
   deb0a:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
   deb0e:	9306      	str	r3, [sp, #24]
   deb10:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   deb12:	1a9a      	subs	r2, r3, r2
   deb14:	69a3      	ldr	r3, [r4, #24]
   deb16:	429a      	cmp	r2, r3
   deb18:	bfa8      	it	ge
   deb1a:	461a      	movge	r2, r3
   deb1c:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   deb1e:	9210      	str	r2, [sp, #64]	; 0x40
   deb20:	ebca 0203 	rsb	r2, sl, r3
   deb24:	6963      	ldr	r3, [r4, #20]
   deb26:	429a      	cmp	r2, r3
   deb28:	f1ca 0700 	rsb	r7, sl, #0
   deb2c:	bfa8      	it	ge
   deb2e:	461a      	movge	r2, r3
   deb30:	ea27 77e7 	bic.w	r7, r7, r7, asr #31
   deb34:	9211      	str	r2, [sp, #68]	; 0x44
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float max = std::numeric_limits<float>::lowest();
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   deb36:	9b11      	ldr	r3, [sp, #68]	; 0x44
   deb38:	429f      	cmp	r7, r3
   deb3a:	da24      	bge.n	deb86 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x142>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
   deb3c:	eb07 030a 	add.w	r3, r7, sl
   deb40:	f8dd 9018 	ldr.w	r9, [sp, #24]
   deb44:	9313      	str	r3, [sp, #76]	; 0x4c
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float max = std::numeric_limits<float>::lowest();
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   deb46:	9b10      	ldr	r3, [sp, #64]	; 0x40
   deb48:	4599      	cmp	r9, r3
   deb4a:	da1a      	bge.n	deb82 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x13e>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
   deb4c:	9b07      	ldr	r3, [sp, #28]
   deb4e:	f8cd 8000 	str.w	r8, [sp]
   deb52:	444b      	add	r3, r9
   deb54:	9a13      	ldr	r2, [sp, #76]	; 0x4c
   deb56:	4659      	mov	r1, fp
   deb58:	4628      	mov	r0, r5
   deb5a:	f7f7 fcea 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   deb5e:	9b12      	ldr	r3, [sp, #72]	; 0x48
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   deb60:	eddd 7a15 	vldr	s15, [sp, #84]	; 0x54
   deb64:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   deb68:	ed90 7a00 	vldr	s14, [r0]
   deb6c:	eeb4 7ae7 	vcmpe.f32	s14, s15
   deb70:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
	return __b;
      return __a;
   deb74:	bfd8      	it	le
   deb76:	a815      	addle	r0, sp, #84	; 0x54
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float max = std::numeric_limits<float>::lowest();
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   deb78:	f109 0901 	add.w	r9, r9, #1
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
   deb7c:	6803      	ldr	r3, [r0, #0]
   deb7e:	9315      	str	r3, [sp, #84]	; 0x54
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float max = std::numeric_limits<float>::lowest();
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   deb80:	e7e1      	b.n	deb46 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x102>
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          float max = std::numeric_limits<float>::lowest();
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   deb82:	3701      	adds	r7, #1
   deb84:	e7d7      	b.n	deb36 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0xf2>
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
            }
          }
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
   deb86:	f8cd 8000 	str.w	r8, [sp]
   deb8a:	9b03      	ldr	r3, [sp, #12]
   deb8c:	9a02      	ldr	r2, [sp, #8]
   deb8e:	4659      	mov	r1, fp
   deb90:	4630      	mov	r0, r6
   deb92:	f7f7 fcce 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              ActivationFunctionWithMinMax(max, params.float_activation_min,
   deb96:	edd4 7a09 	vldr	s15, [r4, #36]	; 0x24
   deb9a:	eddd 6a15 	vldr	s13, [sp, #84]	; 0x54
                                           params.float_activation_max);
   deb9e:	ed94 7a0a 	vldr	s14, [r4, #40]	; 0x28
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
            }
          }
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
   deba2:	9b22      	ldr	r3, [sp, #136]	; 0x88
   deba4:	eef4 6ae7 	vcmpe.f32	s13, s15
   deba8:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   debac:	bf58      	it	pl
   debae:	eef0 7a66 	vmovpl.f32	s15, s13
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   debb2:	eeb4 7a67 	vcmp.f32	s14, s15
   debb6:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   debba:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   debbe:	bf48      	it	mi
   debc0:	eef0 7a47 	vmovmi.f32	s15, s14
              ActivationFunctionWithMinMax(max, params.float_activation_min,
                                           params.float_activation_max);
   debc4:	edc0 7a00 	vstr	s15, [r0]
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
   debc8:	f108 0801 	add.w	r8, r8, #1
   debcc:	e78a      	b.n	deae4 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0xa0>
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   debce:	9b03      	ldr	r3, [sp, #12]
   debd0:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
   debd2:	3301      	adds	r3, #1
   debd4:	9303      	str	r3, [sp, #12]
   debd6:	9b05      	ldr	r3, [sp, #20]
   debd8:	4413      	add	r3, r2
   debda:	9305      	str	r3, [sp, #20]
   debdc:	e77c      	b.n	dead8 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x94>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   debde:	9b02      	ldr	r3, [sp, #8]
   debe0:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   debe2:	3301      	adds	r3, #1
   debe4:	9302      	str	r3, [sp, #8]
   debe6:	9b04      	ldr	r3, [sp, #16]
   debe8:	4413      	add	r3, r2
   debea:	9304      	str	r3, [sp, #16]
   debec:	e76c      	b.n	deac8 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x84>
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
   debee:	f10b 0b01 	add.w	fp, fp, #1
   debf2:	e762      	b.n	deaba <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x76>
                                           params.float_activation_max);
        }
      }
    }
  }
}
   debf4:	b017      	add	sp, #92	; 0x5c
   debf6:	ecbd 8b02 	vpop	{d8}
   debfa:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   debfe:	bf00      	nop
   dec00:	ff7fffff 	.word	0xff7fffff

000dec04 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph>:

inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const uint8* input_data, const RuntimeShape& output_shape,
                    uint8* output_data) {
   dec04:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dec08:	b097      	sub	sp, #92	; 0x5c
   dec0a:	461e      	mov	r6, r3
   dec0c:	9208      	str	r2, [sp, #32]
  TFLITE_DCHECK_LE(params.quantized_activation_min,
   dec0e:	6a03      	ldr	r3, [r0, #32]
   dec10:	69c2      	ldr	r2, [r0, #28]
                   params.quantized_activation_max);
   dec12:	429a      	cmp	r2, r3
  }
}

inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const uint8* input_data, const RuntimeShape& output_shape,
                    uint8* output_data) {
   dec14:	4604      	mov	r4, r0
   dec16:	460d      	mov	r5, r1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
   dec18:	dd01      	ble.n	dec1e <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x1a>
}

inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
                    const uint8* input_data, const RuntimeShape& output_shape,
                    uint8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
   dec1a:	f005 fb97 	bl	e434c <abort>
                   params.quantized_activation_max);
  TFLITE_DCHECK_GE(params.quantized_activation_min, 0);
   dec1e:	2a00      	cmp	r2, #0
   dec20:	dbfb      	blt.n	dec1a <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x16>
  TFLITE_DCHECK_LE(params.quantized_activation_max, 255);
   dec22:	2bff      	cmp	r3, #255	; 0xff
   dec24:	dcf9      	bgt.n	dec1a <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x16>
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dec26:	680b      	ldr	r3, [r1, #0]
   dec28:	2b04      	cmp	r3, #4
   dec2a:	d1f6      	bne.n	dec1a <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x16>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   dec2c:	6833      	ldr	r3, [r6, #0]
   dec2e:	2b04      	cmp	r3, #4
   dec30:	d1f3      	bne.n	dec1a <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x16>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dec32:	2300      	movs	r3, #0
   dec34:	4619      	mov	r1, r3
   dec36:	4632      	mov	r2, r6
   dec38:	4628      	mov	r0, r5
   dec3a:	f7fc ffc6 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   dec3e:	2303      	movs	r3, #3
   dec40:	4619      	mov	r1, r3
   dec42:	4632      	mov	r2, r6
                   params.quantized_activation_max);
  TFLITE_DCHECK_GE(params.quantized_activation_min, 0);
  TFLITE_DCHECK_LE(params.quantized_activation_max, 255);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dec44:	9009      	str	r0, [sp, #36]	; 0x24
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   dec46:	4628      	mov	r0, r5
   dec48:	f7fc ffbf 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
   dec4c:	2101      	movs	r1, #1
  TFLITE_DCHECK_GE(params.quantized_activation_min, 0);
  TFLITE_DCHECK_LE(params.quantized_activation_max, 255);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   dec4e:	900b      	str	r0, [sp, #44]	; 0x2c
  const int input_height = input_shape.Dims(1);
   dec50:	4628      	mov	r0, r5
   dec52:	f7f7 fc09 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   dec56:	2102      	movs	r1, #2
  TFLITE_DCHECK_LE(params.quantized_activation_max, 255);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
   dec58:	900c      	str	r0, [sp, #48]	; 0x30
  const int input_width = input_shape.Dims(2);
   dec5a:	4628      	mov	r0, r5
   dec5c:	f7f7 fc04 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   dec60:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   dec62:	900d      	str	r0, [sp, #52]	; 0x34
  const int output_height = output_shape.Dims(1);
   dec64:	4630      	mov	r0, r6
   dec66:	f7f7 fbff 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   dec6a:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   dec6c:	900e      	str	r0, [sp, #56]	; 0x38
  const int output_width = output_shape.Dims(2);
   dec6e:	4630      	mov	r0, r6
   dec70:	f7f7 fbfa 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int stride_height = params.stride_height;
   dec74:	68e3      	ldr	r3, [r4, #12]
   dec76:	9310      	str	r3, [sp, #64]	; 0x40
  const int stride_width = params.stride_width;
   dec78:	6923      	ldr	r3, [r4, #16]
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   dec7a:	900f      	str	r0, [sp, #60]	; 0x3c
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
   dec7c:	9311      	str	r3, [sp, #68]	; 0x44
  for (int batch = 0; batch < batches; ++batch) {
   dec7e:	f04f 0b00 	mov.w	fp, #0
   dec82:	9b09      	ldr	r3, [sp, #36]	; 0x24
   dec84:	459b      	cmp	fp, r3
   dec86:	f280 808d 	bge.w	deda4 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x1a0>
   dec8a:	2300      	movs	r3, #0
   dec8c:	9305      	str	r3, [sp, #20]
   dec8e:	9302      	str	r3, [sp, #8]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   dec90:	9b02      	ldr	r3, [sp, #8]
   dec92:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   dec94:	4293      	cmp	r3, r2
   dec96:	f280 8082 	bge.w	ded9e <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x19a>
   dec9a:	2300      	movs	r3, #0
   dec9c:	9304      	str	r3, [sp, #16]
   dec9e:	9303      	str	r3, [sp, #12]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   deca0:	9b03      	ldr	r3, [sp, #12]
   deca2:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
   deca4:	4293      	cmp	r3, r2
   deca6:	da72      	bge.n	ded8e <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x18a>
   deca8:	f04f 0800 	mov.w	r8, #0
        for (int channel = 0; channel < depth; ++channel) {
   decac:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   decae:	4598      	cmp	r8, r3
   decb0:	da65      	bge.n	ded7e <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x17a>
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
   decb2:	f9b4 9002 	ldrsh.w	r9, [r4, #2]
   decb6:	9b04      	ldr	r3, [sp, #16]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
   decb8:	f9b4 a004 	ldrsh.w	sl, [r4, #4]
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
   decbc:	ebc9 0303 	rsb	r3, r9, r3
   decc0:	9307      	str	r3, [sp, #28]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
   decc2:	9b05      	ldr	r3, [sp, #20]
   decc4:	9a07      	ldr	r2, [sp, #28]
   decc6:	ebca 0a03 	rsb	sl, sl, r3
   decca:	9b07      	ldr	r3, [sp, #28]
   deccc:	425b      	negs	r3, r3
   decce:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
   decd2:	9306      	str	r3, [sp, #24]
   decd4:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   decd6:	1a9a      	subs	r2, r3, r2
   decd8:	69a3      	ldr	r3, [r4, #24]
   decda:	429a      	cmp	r2, r3
   decdc:	bfa8      	it	ge
   decde:	461a      	movge	r2, r3
   dece0:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   dece2:	920a      	str	r2, [sp, #40]	; 0x28
   dece4:	ebca 0203 	rsb	r2, sl, r3
   dece8:	6963      	ldr	r3, [r4, #20]
   decea:	429a      	cmp	r2, r3
   decec:	bfa8      	it	ge
   decee:	461a      	movge	r2, r3
   decf0:	f1ca 0700 	rsb	r7, sl, #0
          const int filter_x_end =
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          uint8 max = 0;
   decf4:	f04f 0300 	mov.w	r3, #0
   decf8:	ea27 77e7 	bic.w	r7, r7, r7, asr #31
   decfc:	9212      	str	r2, [sp, #72]	; 0x48
   decfe:	f88d 3057 	strb.w	r3, [sp, #87]	; 0x57
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   ded02:	9b12      	ldr	r3, [sp, #72]	; 0x48
   ded04:	429f      	cmp	r7, r3
   ded06:	da22      	bge.n	ded4e <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x14a>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
   ded08:	eb0a 0307 	add.w	r3, sl, r7
   ded0c:	f8dd 9018 	ldr.w	r9, [sp, #24]
   ded10:	9313      	str	r3, [sp, #76]	; 0x4c
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          uint8 max = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   ded12:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   ded14:	4599      	cmp	r9, r3
   ded16:	da18      	bge.n	ded4a <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x146>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
   ded18:	9b07      	ldr	r3, [sp, #28]
   ded1a:	9a13      	ldr	r2, [sp, #76]	; 0x4c
   ded1c:	f8cd 8000 	str.w	r8, [sp]
   ded20:	444b      	add	r3, r9
   ded22:	4659      	mov	r1, fp
   ded24:	4628      	mov	r0, r5
   ded26:	f7f7 fc04 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   ded2a:	9a08      	ldr	r2, [sp, #32]
   ded2c:	9b08      	ldr	r3, [sp, #32]
   ded2e:	5c11      	ldrb	r1, [r2, r0]
   ded30:	f89d 2057 	ldrb.w	r2, [sp, #87]	; 0x57
   ded34:	4291      	cmp	r1, r2
   ded36:	4403      	add	r3, r0
	return __b;
      return __a;
   ded38:	bf98      	it	ls
   ded3a:	f10d 0357 	addls.w	r3, sp, #87	; 0x57
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          uint8 max = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   ded3e:	f109 0901 	add.w	r9, r9, #1
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              max = std::max(
                  max,
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
   ded42:	781b      	ldrb	r3, [r3, #0]
   ded44:	f88d 3057 	strb.w	r3, [sp, #87]	; 0x57
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          uint8 max = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   ded48:	e7e3      	b.n	ded12 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x10e>
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          uint8 max = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   ded4a:	3701      	adds	r7, #1
   ded4c:	e7d9      	b.n	ded02 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xfe>
   ded4e:	7f27      	ldrb	r7, [r4, #28]
   ded50:	f89d 3057 	ldrb.w	r3, [sp, #87]	; 0x57
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);
            }
          }
          max = std::max<uint8>(max, params.quantized_activation_min);
          max = std::min<uint8>(max, params.quantized_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
   ded54:	f8cd 8000 	str.w	r8, [sp]
   ded58:	429f      	cmp	r7, r3
   ded5a:	bf38      	it	cc
   ded5c:	461f      	movcc	r7, r3
   ded5e:	f894 3020 	ldrb.w	r3, [r4, #32]
   ded62:	9a02      	ldr	r2, [sp, #8]
   ded64:	429f      	cmp	r7, r3
   ded66:	bf28      	it	cs
   ded68:	461f      	movcs	r7, r3
   ded6a:	4659      	mov	r1, fp
   ded6c:	9b03      	ldr	r3, [sp, #12]
   ded6e:	4630      	mov	r0, r6
   ded70:	f7f7 fbdf 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<uint8>(max);
   ded74:	9b20      	ldr	r3, [sp, #128]	; 0x80
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
   ded76:	f108 0801 	add.w	r8, r8, #1
            }
          }
          max = std::max<uint8>(max, params.quantized_activation_min);
          max = std::min<uint8>(max, params.quantized_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
              static_cast<uint8>(max);
   ded7a:	541f      	strb	r7, [r3, r0]
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
   ded7c:	e796      	b.n	decac <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xa8>
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   ded7e:	9b03      	ldr	r3, [sp, #12]
   ded80:	9a11      	ldr	r2, [sp, #68]	; 0x44
   ded82:	3301      	adds	r3, #1
   ded84:	9303      	str	r3, [sp, #12]
   ded86:	9b04      	ldr	r3, [sp, #16]
   ded88:	4413      	add	r3, r2
   ded8a:	9304      	str	r3, [sp, #16]
   ded8c:	e788      	b.n	deca0 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x9c>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   ded8e:	9b02      	ldr	r3, [sp, #8]
   ded90:	9a10      	ldr	r2, [sp, #64]	; 0x40
   ded92:	3301      	adds	r3, #1
   ded94:	9302      	str	r3, [sp, #8]
   ded96:	9b05      	ldr	r3, [sp, #20]
   ded98:	4413      	add	r3, r2
   ded9a:	9305      	str	r3, [sp, #20]
   ded9c:	e778      	b.n	dec90 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x8c>
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
   ded9e:	f10b 0b01 	add.w	fp, fp, #1
   deda2:	e76e      	b.n	dec82 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x7e>
              static_cast<uint8>(max);
        }
      }
    }
  }
}
   deda4:	b017      	add	sp, #92	; 0x5c
   deda6:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000dedaa <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa>:
namespace tflite {
namespace reference_integer_ops {

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape, const int8* input_data,
                        const RuntimeShape& output_shape, int8* output_data) {
   dedaa:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dedae:	b097      	sub	sp, #92	; 0x5c
   dedb0:	461f      	mov	r7, r3
   dedb2:	9214      	str	r2, [sp, #80]	; 0x50
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
   dedb4:	6a03      	ldr	r3, [r0, #32]
   dedb6:	69c2      	ldr	r2, [r0, #28]
   dedb8:	429a      	cmp	r2, r3
namespace tflite {
namespace reference_integer_ops {

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape, const int8* input_data,
                        const RuntimeShape& output_shape, int8* output_data) {
   dedba:	4604      	mov	r4, r0
   dedbc:	4689      	mov	r9, r1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
   dedbe:	dd01      	ble.n	dedc4 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x1a>
namespace reference_integer_ops {

inline void AveragePool(const PoolParams& params,
                        const RuntimeShape& input_shape, const int8* input_data,
                        const RuntimeShape& output_shape, int8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
   dedc0:	f005 fac4 	bl	e434c <abort>
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   dedc4:	680b      	ldr	r3, [r1, #0]
   dedc6:	2b04      	cmp	r3, #4
   dedc8:	d1fa      	bne.n	dedc0 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x16>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   dedca:	683b      	ldr	r3, [r7, #0]
   dedcc:	2b04      	cmp	r3, #4
   dedce:	d1f7      	bne.n	dedc0 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x16>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dedd0:	2300      	movs	r3, #0
   dedd2:	4619      	mov	r1, r3
   dedd4:	463a      	mov	r2, r7
   dedd6:	4648      	mov	r0, r9
   dedd8:	f7fc fef7 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   deddc:	2303      	movs	r3, #3
   dedde:	4619      	mov	r1, r3
   dede0:	463a      	mov	r2, r7
                        const RuntimeShape& output_shape, int8* output_data) {
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   dede2:	900a      	str	r0, [sp, #40]	; 0x28
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   dede4:	4648      	mov	r0, r9
   dede6:	f7fc fef0 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
   dedea:	2101      	movs	r1, #1
  TFLITE_DCHECK_LE(params.quantized_activation_min,
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   dedec:	900b      	str	r0, [sp, #44]	; 0x2c
  const int input_height = input_shape.Dims(1);
   dedee:	4648      	mov	r0, r9
   dedf0:	f7f7 fb3a 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   dedf4:	2102      	movs	r1, #2
                   params.quantized_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
   dedf6:	900c      	str	r0, [sp, #48]	; 0x30
  const int input_width = input_shape.Dims(2);
   dedf8:	4648      	mov	r0, r9
   dedfa:	f7f7 fb35 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   dedfe:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   dee00:	900d      	str	r0, [sp, #52]	; 0x34
  const int output_height = output_shape.Dims(1);
   dee02:	4638      	mov	r0, r7
   dee04:	f7f7 fb30 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   dee08:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   dee0a:	900e      	str	r0, [sp, #56]	; 0x38
  const int output_width = output_shape.Dims(2);
   dee0c:	4638      	mov	r0, r7
   dee0e:	f7f7 fb2b 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int stride_height = params.stride_height;
   dee12:	68e3      	ldr	r3, [r4, #12]
   dee14:	9310      	str	r3, [sp, #64]	; 0x40
  const int stride_width = params.stride_width;
   dee16:	6923      	ldr	r3, [r4, #16]
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int depth = MatchingDim(input_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   dee18:	900f      	str	r0, [sp, #60]	; 0x3c
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
   dee1a:	9311      	str	r3, [sp, #68]	; 0x44
  for (int batch = 0; batch < batches; ++batch) {
   dee1c:	f04f 0a00 	mov.w	sl, #0
   dee20:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   dee22:	459a      	cmp	sl, r3
   dee24:	f280 808d 	bge.w	def42 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x198>
   dee28:	2300      	movs	r3, #0
   dee2a:	9305      	str	r3, [sp, #20]
   dee2c:	9303      	str	r3, [sp, #12]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   dee2e:	9b03      	ldr	r3, [sp, #12]
   dee30:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   dee32:	4293      	cmp	r3, r2
   dee34:	f280 8082 	bge.w	def3c <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x192>
   dee38:	2300      	movs	r3, #0
   dee3a:	9306      	str	r3, [sp, #24]
   dee3c:	9304      	str	r3, [sp, #16]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   dee3e:	9b04      	ldr	r3, [sp, #16]
   dee40:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
   dee42:	4293      	cmp	r3, r2
   dee44:	da72      	bge.n	def2c <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x182>
   dee46:	2300      	movs	r3, #0
   dee48:	9302      	str	r3, [sp, #8]
        for (int channel = 0; channel < depth; ++channel) {
   dee4a:	9b02      	ldr	r3, [sp, #8]
   dee4c:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   dee4e:	4293      	cmp	r3, r2
   dee50:	da64      	bge.n	def1c <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x172>
          const int in_x_origin =
              (out_x * stride_width) - params.padding_values.width;
   dee52:	9a06      	ldr	r2, [sp, #24]
   dee54:	f9b4 3002 	ldrsh.w	r3, [r4, #2]
   dee58:	1ad3      	subs	r3, r2, r3
   dee5a:	9308      	str	r3, [sp, #32]
          const int in_y_origin =
              (out_y * stride_height) - params.padding_values.height;
   dee5c:	9a05      	ldr	r2, [sp, #20]
   dee5e:	f9b4 3004 	ldrsh.w	r3, [r4, #4]
   dee62:	1ad3      	subs	r3, r2, r3
   dee64:	9309      	str	r3, [sp, #36]	; 0x24
   dee66:	9b08      	ldr	r3, [sp, #32]
   dee68:	9a08      	ldr	r2, [sp, #32]
   dee6a:	425b      	negs	r3, r3
   dee6c:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
   dee70:	9307      	str	r3, [sp, #28]
   dee72:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   dee74:	1a9a      	subs	r2, r3, r2
   dee76:	69a3      	ldr	r3, [r4, #24]
   dee78:	429a      	cmp	r2, r3
   dee7a:	bfa8      	it	ge
   dee7c:	461a      	movge	r2, r3
   dee7e:	9b09      	ldr	r3, [sp, #36]	; 0x24
   dee80:	9213      	str	r2, [sp, #76]	; 0x4c
   dee82:	425e      	negs	r6, r3
   dee84:	9a09      	ldr	r2, [sp, #36]	; 0x24
   dee86:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   dee88:	1a9a      	subs	r2, r3, r2
   dee8a:	6963      	ldr	r3, [r4, #20]
   dee8c:	429a      	cmp	r2, r3
   dee8e:	bfa8      	it	ge
   dee90:	461a      	movge	r2, r3
          const int filter_x_end =
              std::min(params.filter_width, input_width - in_x_origin);
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
   dee92:	2500      	movs	r5, #0
   dee94:	ea26 76e6 	bic.w	r6, r6, r6, asr #31
   dee98:	9212      	str	r2, [sp, #72]	; 0x48
          int filter_count = 0;
   dee9a:	46ab      	mov	fp, r5
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   dee9c:	9b12      	ldr	r3, [sp, #72]	; 0x48
   dee9e:	429e      	cmp	r6, r3
   deea0:	da1d      	bge.n	deede <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x134>
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
   deea2:	9b09      	ldr	r3, [sp, #36]	; 0x24
   deea4:	f8dd 801c 	ldr.w	r8, [sp, #28]
   deea8:	4433      	add	r3, r6
   deeaa:	ebc8 0b0b 	rsb	fp, r8, fp
   deeae:	9315      	str	r3, [sp, #84]	; 0x54
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   deeb0:	9a13      	ldr	r2, [sp, #76]	; 0x4c
   deeb2:	4590      	cmp	r8, r2
   deeb4:	eb0b 0308 	add.w	r3, fp, r8
   deeb8:	da0e      	bge.n	deed8 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x12e>
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
   deeba:	9b02      	ldr	r3, [sp, #8]
   deebc:	9300      	str	r3, [sp, #0]
   deebe:	9b08      	ldr	r3, [sp, #32]
   deec0:	9a15      	ldr	r2, [sp, #84]	; 0x54
   deec2:	4443      	add	r3, r8
   deec4:	4651      	mov	r1, sl
   deec6:	4648      	mov	r0, r9
   deec8:	f7f7 fb33 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   deecc:	9b14      	ldr	r3, [sp, #80]	; 0x50
   deece:	561b      	ldrsb	r3, [r3, r0]
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   deed0:	f108 0801 	add.w	r8, r8, #1
                 ++filter_x) {
              const int in_x = in_x_origin + filter_x;
              const int in_y = in_y_origin + filter_y;
              acc +=
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
   deed4:	441d      	add	r5, r3
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
               ++filter_y) {
            for (int filter_x = filter_x_start; filter_x < filter_x_end;
   deed6:	e7eb      	b.n	deeb0 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x106>
          const int filter_y_start = std::max(0, -in_y_origin);
          const int filter_y_end =
              std::min(params.filter_height, input_height - in_y_origin);
          int32 acc = 0;
          int filter_count = 0;
          for (int filter_y = filter_y_start; filter_y < filter_y_end;
   deed8:	3601      	adds	r6, #1
   deeda:	469b      	mov	fp, r3
   deedc:	e7de      	b.n	dee9c <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0xf2>
              filter_count++;
            }
          }
          // Round to the closest integer value.
          acc = acc > 0 ? (acc + filter_count / 2) / filter_count
                        : (acc - filter_count / 2) / filter_count;
   deede:	2d00      	cmp	r5, #0
   deee0:	bfd7      	itett	le
   deee2:	2302      	movle	r3, #2
                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];
              filter_count++;
            }
          }
          // Round to the closest integer value.
          acc = acc > 0 ? (acc + filter_count / 2) / filter_count
   deee4:	eb05 056b 	addgt.w	r5, r5, fp, asr #1
                        : (acc - filter_count / 2) / filter_count;
   deee8:	fb9b f3f3 	sdivle	r3, fp, r3
   deeec:	1aed      	suble	r5, r5, r3
   deeee:	fb95 fbfb 	sdiv	fp, r5, fp
   deef2:	69e5      	ldr	r5, [r4, #28]
          acc = std::max(acc, params.quantized_activation_min);
          acc = std::min(acc, params.quantized_activation_max);
          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =
   deef4:	9b02      	ldr	r3, [sp, #8]
   deef6:	9300      	str	r3, [sp, #0]
   deef8:	45ab      	cmp	fp, r5
   deefa:	bfb8      	it	lt
   deefc:	46ab      	movlt	fp, r5
   deefe:	6a25      	ldr	r5, [r4, #32]
   def00:	9b04      	ldr	r3, [sp, #16]
   def02:	9a03      	ldr	r2, [sp, #12]
   def04:	455d      	cmp	r5, fp
   def06:	4651      	mov	r1, sl
   def08:	4638      	mov	r0, r7
   def0a:	bfa8      	it	ge
   def0c:	465d      	movge	r5, fp
   def0e:	f7f7 fb10 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
              static_cast<int8>(acc);
   def12:	9b20      	ldr	r3, [sp, #128]	; 0x80
   def14:	541d      	strb	r5, [r3, r0]
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int channel = 0; channel < depth; ++channel) {
   def16:	9b02      	ldr	r3, [sp, #8]
   def18:	3301      	adds	r3, #1
   def1a:	e795      	b.n	dee48 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x9e>
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   def1c:	9b04      	ldr	r3, [sp, #16]
   def1e:	9a11      	ldr	r2, [sp, #68]	; 0x44
   def20:	3301      	adds	r3, #1
   def22:	9304      	str	r3, [sp, #16]
   def24:	9b06      	ldr	r3, [sp, #24]
   def26:	4413      	add	r3, r2
   def28:	9306      	str	r3, [sp, #24]
   def2a:	e788      	b.n	dee3e <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x94>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   def2c:	9b03      	ldr	r3, [sp, #12]
   def2e:	9a10      	ldr	r2, [sp, #64]	; 0x40
   def30:	3301      	adds	r3, #1
   def32:	9303      	str	r3, [sp, #12]
   def34:	9b05      	ldr	r3, [sp, #20]
   def36:	4413      	add	r3, r2
   def38:	9305      	str	r3, [sp, #20]
   def3a:	e778      	b.n	dee2e <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x84>
  const int input_width = input_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  const int stride_height = params.stride_height;
  const int stride_width = params.stride_width;
  for (int batch = 0; batch < batches; ++batch) {
   def3c:	f10a 0a01 	add.w	sl, sl, #1
   def40:	e76e      	b.n	dee20 <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa+0x76>
              static_cast<int8>(acc);
        }
      }
    }
  }
}
   def42:	b017      	add	sp, #92	; 0x5c
   def44:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000def48 <_ZN6tflite3ops5micro7pooling12_GLOBAL__N_115CalculateOpDataEPK13TfLiteContextPK16TfLitePoolParamsPK12TfLiteTensorSC_PNS3_6OpDataE.isra.2>:

struct OpData {
  TfLitePaddingValues padding;
};

TfLiteStatus CalculateOpData(const TfLiteContext* context,
   def48:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   def4c:	688b      	ldr	r3, [r1, #8]
  int width = SizeOfDimension(input, 2);

  int out_height, out_width;

  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
   def4e:	6846      	ldr	r6, [r0, #4]

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
inline int SizeOfDimension(const TfLiteTensor* t, int dim) {
  return t->dims->data[dim];
   def50:	689f      	ldr	r7, [r3, #8]
   def52:	f8d3 b00c 	ldr.w	fp, [r3, #12]
      /*dilation_rate_height=*/1,
      /*dilation_rate_width=*/1, height, width, params->filter_height,
      params->filter_width, params->padding, &out_height, &out_width);
   def56:	f890 a000 	ldrb.w	sl, [r0]
   def5a:	68c3      	ldr	r3, [r0, #12]
  int width = SizeOfDimension(input, 2);

  int out_height, out_width;

  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
   def5c:	f8d0 9008 	ldr.w	r9, [r0, #8]
      /*dilation_rate_height=*/1,
      /*dilation_rate_width=*/1, height, width, params->filter_height,
   def60:	f8d0 8010 	ldr.w	r8, [r0, #16]

struct OpData {
  TfLitePaddingValues padding;
};

TfLiteStatus CalculateOpData(const TfLiteContext* context,
   def64:	b085      	sub	sp, #20

inline TfLitePaddingValues ComputePaddingHeightWidth(
    int stride_height, int stride_width, int dilation_rate_height,
    int dilation_rate_width, int in_height, int in_width, int filter_height,
    int filter_width, TfLitePadding padding, int* out_height, int* out_width) {
  *out_width = ComputeOutSize(padding, in_width, filter_width, stride_width,
   def66:	2401      	movs	r4, #1

  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      /*dilation_rate_height=*/1,
      /*dilation_rate_width=*/1, height, width, params->filter_height,
      params->filter_width, params->padding, &out_height, &out_width);
   def68:	9302      	str	r3, [sp, #8]

struct OpData {
  TfLitePaddingValues padding;
};

TfLiteStatus CalculateOpData(const TfLiteContext* context,
   def6a:	4615      	mov	r5, r2
   def6c:	9400      	str	r4, [sp, #0]
   def6e:	4633      	mov	r3, r6
   def70:	9a02      	ldr	r2, [sp, #8]
   def72:	4659      	mov	r1, fp
   def74:	4650      	mov	r0, sl
   def76:	f7fd f9a8 	bl	dc2ca <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>
                              dilation_rate_width);
  *out_height = ComputeOutSize(padding, in_height, filter_height, stride_height,
   def7a:	9400      	str	r4, [sp, #0]

inline TfLitePaddingValues ComputePaddingHeightWidth(
    int stride_height, int stride_width, int dilation_rate_height,
    int dilation_rate_width, int in_height, int in_width, int filter_height,
    int filter_width, TfLitePadding padding, int* out_height, int* out_width) {
  *out_width = ComputeOutSize(padding, in_width, filter_width, stride_width,
   def7c:	9003      	str	r0, [sp, #12]
                              dilation_rate_width);
  *out_height = ComputeOutSize(padding, in_height, filter_height, stride_height,
   def7e:	464b      	mov	r3, r9
   def80:	4642      	mov	r2, r8
   def82:	4639      	mov	r1, r7
   def84:	4650      	mov	r0, sl
   def86:	f7fd f9a0 	bl	dc2ca <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>
inline int ComputePaddingWithOffset(int stride, int dilation_rate, int in_size,
                                    int filter_size, int out_size,
                                    int* offset) {
  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;
  int total_padding =
      ((out_size - 1) * stride + effective_filter_size - in_size);
   def8a:	9b03      	ldr	r3, [sp, #12]
   def8c:	1e5c      	subs	r4, r3, #1
   def8e:	9b02      	ldr	r3, [sp, #8]
   def90:	3801      	subs	r0, #1
   def92:	fb06 3604 	mla	r6, r6, r4, r3
   def96:	fb09 8800 	mla	r8, r9, r0, r8
   def9a:	ebcb 0606 	rsb	r6, fp, r6
   def9e:	ebc7 0708 	rsb	r7, r7, r8
  total_padding = total_padding > 0 ? total_padding : 0;
   defa2:	ea26 76e6 	bic.w	r6, r6, r6, asr #31

  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      /*dilation_rate_height=*/1,
      /*dilation_rate_width=*/1, height, width, params->filter_height,
      params->filter_width, params->padding, &out_height, &out_width);
   defa6:	1073      	asrs	r3, r6, #1
   defa8:	ea27 77e7 	bic.w	r7, r7, r7, asr #31
   defac:	602b      	str	r3, [r5, #0]
   defae:	f006 0601 	and.w	r6, r6, #1
   defb2:	107b      	asrs	r3, r7, #1

  return kTfLiteOk;
}
   defb4:	2000      	movs	r0, #0

  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      /*dilation_rate_height=*/1,
      /*dilation_rate_width=*/1, height, width, params->filter_height,
      params->filter_width, params->padding, &out_height, &out_width);
   defb6:	f007 0701 	and.w	r7, r7, #1
   defba:	606b      	str	r3, [r5, #4]
   defbc:	60ae      	str	r6, [r5, #8]
   defbe:	60ef      	str	r7, [r5, #12]

  return kTfLiteOk;
}
   defc0:	b005      	add	sp, #20
   defc2:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

000defc8 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode>:
      return kTfLiteError;
  }
  return kTfLiteOk;
}

TfLiteStatus MaxEval(TfLiteContext* context, TfLiteNode* node) {
   defc8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   defcc:	680b      	ldr	r3, [r1, #0]
   defce:	f8d0 a008 	ldr.w	sl, [r0, #8]
   defd2:	685b      	ldr	r3, [r3, #4]
  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);
   defd4:	694d      	ldr	r5, [r1, #20]
   defd6:	2438      	movs	r4, #56	; 0x38
   defd8:	fb04 f803 	mul.w	r8, r4, r3
      return kTfLiteError;
  }
  return kTfLiteOk;
}

TfLiteStatus MaxEval(TfLiteContext* context, TfLiteNode* node) {
   defdc:	b09f      	sub	sp, #124	; 0x7c
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   defde:	684b      	ldr	r3, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   defe0:	eb0a 0708 	add.w	r7, sl, r8
   defe4:	4681      	mov	r9, r0
  OpData data;

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, input, output, &data));
   defe6:	aa05      	add	r2, sp, #20
   defe8:	4639      	mov	r1, r7
   defea:	4628      	mov	r0, r5
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   defec:	f8d3 b004 	ldr.w	fp, [r3, #4]
   deff0:	f7ff ffaa 	bl	def48 <_ZN6tflite3ops5micro7pooling12_GLOBAL__N_115CalculateOpDataEPK13TfLiteContextPK16TfLitePoolParamsPK12TfLiteTensorSC_PNS3_6OpDataE.isra.2>
   deff4:	4606      	mov	r6, r0
   deff6:	2800      	cmp	r0, #0
   deff8:	f040 8085 	bne.w	df106 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x13e>

  switch (input->type) {
   deffc:	f81a 0008 	ldrb.w	r0, [sl, r8]
   df000:	2801      	cmp	r0, #1
   df002:	fb04 a40b 	mla	r4, r4, fp, sl
   df006:	d002      	beq.n	df00e <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x46>
   df008:	2803      	cmp	r0, #3
   df00a:	d041      	beq.n	df090 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0xc8>
   df00c:	e073      	b.n	df0f6 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x12e>

void MaxEvalFloat(TfLiteContext* context, TfLiteNode* node,
                  TfLitePoolParams* params, OpData* data,
                  const TfLiteTensor* input, TfLiteTensor* output) {
  float activation_min, activation_max;
  CalculateActivationRange(params->activation, &activation_min,
   df00e:	7d2b      	ldrb	r3, [r5, #20]
// Calculates the useful range of an activation layer given its activation
// tensor.a
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
   df010:	2b01      	cmp	r3, #1
   df012:	d011      	beq.n	df038 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x70>
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
   df014:	2b03      	cmp	r3, #3
   df016:	d012      	beq.n	df03e <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x76>
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
    *activation_max = std::numeric_limits<T>::max();
   df018:	ed9f 7a3d 	vldr	s14, [pc, #244]	; df110 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x148>
   df01c:	eddf 6a3d 	vldr	s13, [pc, #244]	; df114 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x14c>
   df020:	2b02      	cmp	r3, #2
   df022:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   df026:	bf18      	it	ne
   df028:	eef0 7a47 	vmovne.f32	s15, s14
   df02c:	eebf 7a00 	vmov.f32	s14, #240	; 0xbf800000 -1.0
   df030:	bf18      	it	ne
   df032:	eeb0 7a66 	vmovne.f32	s14, s13
   df036:	e006      	b.n	df046 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x7e>
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
   df038:	eddf 7a35 	vldr	s15, [pc, #212]	; df110 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x148>
   df03c:	e001      	b.n	df042 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x7a>
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
    *activation_max = 6;
   df03e:	eef1 7a08 	vmov.f32	s15, #24	; 0x40c00000  6.0
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
   df042:	ed9f 7a35 	vldr	s14, [pc, #212]	; df118 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x150>
                           &activation_max);

  tflite::PoolParams op_params;
  op_params.stride_height = params->stride_height;
   df046:	68ab      	ldr	r3, [r5, #8]
   df048:	9316      	str	r3, [sp, #88]	; 0x58
  op_params.stride_width = params->stride_width;
   df04a:	686b      	ldr	r3, [r5, #4]
   df04c:	9317      	str	r3, [sp, #92]	; 0x5c
  op_params.filter_height = params->filter_height;
   df04e:	692b      	ldr	r3, [r5, #16]
   df050:	9318      	str	r3, [sp, #96]	; 0x60
  op_params.filter_width = params->filter_width;
   df052:	68eb      	ldr	r3, [r5, #12]
   df054:	9319      	str	r3, [sp, #100]	; 0x64
  op_params.padding_values.height = data->padding.height;
   df056:	9b06      	ldr	r3, [sp, #24]
   df058:	f8ad 3050 	strh.w	r3, [sp, #80]	; 0x50
  op_params.padding_values.width = data->padding.width;
  op_params.float_activation_min = activation_min;
  op_params.float_activation_max = activation_max;
  reference_ops::MaxPool(op_params, GetTensorShape(input),
   df05c:	4639      	mov	r1, r7
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.filter_height = params->filter_height;
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
   df05e:	9b05      	ldr	r3, [sp, #20]
   df060:	f8ad 304e 	strh.w	r3, [sp, #78]	; 0x4e
  op_params.float_activation_min = activation_min;
  op_params.float_activation_max = activation_max;
  reference_ops::MaxPool(op_params, GetTensorShape(input),
   df064:	a809      	add	r0, sp, #36	; 0x24
  op_params.stride_width = params->stride_width;
  op_params.filter_height = params->filter_height;
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.float_activation_min = activation_min;
   df066:	ed8d 7a1c 	vstr	s14, [sp, #112]	; 0x70
  op_params.float_activation_max = activation_max;
   df06a:	edcd 7a1d 	vstr	s15, [sp, #116]	; 0x74
  reference_ops::MaxPool(op_params, GetTensorShape(input),
   df06e:	f7f7 fca0 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                         GetTensorData<float>(input), GetTensorShape(output),
   df072:	4621      	mov	r1, r4
   df074:	a80e      	add	r0, sp, #56	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   df076:	687d      	ldr	r5, [r7, #4]
   df078:	f7f7 fc9b 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   df07c:	b104      	cbz	r4, df080 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0xb8>
   df07e:	6864      	ldr	r4, [r4, #4]
                         GetTensorData<float>(output));
   df080:	9400      	str	r4, [sp, #0]
   df082:	ab0e      	add	r3, sp, #56	; 0x38
   df084:	462a      	mov	r2, r5
   df086:	a909      	add	r1, sp, #36	; 0x24
   df088:	a813      	add	r0, sp, #76	; 0x4c
   df08a:	f7ff fcdb 	bl	dea44 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf>
   df08e:	e02b      	b.n	df0e8 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x120>
void MaxEvalQuantizedUInt8(TfLiteContext* context, TfLiteNode* node,
                           TfLitePoolParams* params, OpData* data,
                           const TfLiteTensor* input, TfLiteTensor* output) {
  int32_t activation_min, activation_max;
  CalculateActivationRangeUint8(params->activation, output, &activation_min,
                                &activation_max);
   df090:	7d28      	ldrb	r0, [r5, #20]
   df092:	ab04      	add	r3, sp, #16
   df094:	aa03      	add	r2, sp, #12
   df096:	4621      	mov	r1, r4
   df098:	f004 fd6c 	bl	e3b74 <_ZN6tflite29CalculateActivationRangeUint8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>

  tflite::PoolParams op_params;
  op_params.stride_height = params->stride_height;
   df09c:	68ab      	ldr	r3, [r5, #8]
   df09e:	9316      	str	r3, [sp, #88]	; 0x58
  op_params.stride_width = params->stride_width;
   df0a0:	686b      	ldr	r3, [r5, #4]
   df0a2:	9317      	str	r3, [sp, #92]	; 0x5c
  op_params.filter_height = params->filter_height;
   df0a4:	692b      	ldr	r3, [r5, #16]
   df0a6:	9318      	str	r3, [sp, #96]	; 0x60
  op_params.filter_width = params->filter_width;
   df0a8:	68eb      	ldr	r3, [r5, #12]
   df0aa:	9319      	str	r3, [sp, #100]	; 0x64
  op_params.padding_values.height = data->padding.height;
   df0ac:	9b06      	ldr	r3, [sp, #24]
   df0ae:	f8ad 3050 	strh.w	r3, [sp, #80]	; 0x50
  op_params.padding_values.width = data->padding.width;
   df0b2:	9b05      	ldr	r3, [sp, #20]
   df0b4:	f8ad 304e 	strh.w	r3, [sp, #78]	; 0x4e
  op_params.quantized_activation_min = activation_min;
   df0b8:	9b03      	ldr	r3, [sp, #12]
   df0ba:	931a      	str	r3, [sp, #104]	; 0x68
  op_params.quantized_activation_max = activation_max;
  reference_ops::MaxPool(op_params, GetTensorShape(input),
   df0bc:	4639      	mov	r1, r7
  op_params.filter_height = params->filter_height;
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
   df0be:	9b04      	ldr	r3, [sp, #16]
   df0c0:	931b      	str	r3, [sp, #108]	; 0x6c
  reference_ops::MaxPool(op_params, GetTensorShape(input),
   df0c2:	a809      	add	r0, sp, #36	; 0x24
   df0c4:	f7f7 fc75 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                         GetTensorData<uint8_t>(input), GetTensorShape(output),
   df0c8:	4621      	mov	r1, r4
   df0ca:	a80e      	add	r0, sp, #56	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   df0cc:	687d      	ldr	r5, [r7, #4]
   df0ce:	f7f7 fc70 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   df0d2:	b10c      	cbz	r4, df0d8 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x110>
   df0d4:	6863      	ldr	r3, [r4, #4]
   df0d6:	e000      	b.n	df0da <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x112>
   df0d8:	4633      	mov	r3, r6
                         GetTensorData<uint8_t>(output));
   df0da:	9300      	str	r3, [sp, #0]
   df0dc:	462a      	mov	r2, r5
   df0de:	ab0e      	add	r3, sp, #56	; 0x38
   df0e0:	a909      	add	r1, sp, #36	; 0x24
   df0e2:	a813      	add	r0, sp, #76	; 0x4c
   df0e4:	f7ff fd8e 	bl	dec04 <_ZN6tflite13reference_ops7MaxPoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph>
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
  reference_ops::MaxPool(op_params, GetTensorShape(input),
                         GetTensorData<uint8_t>(input), GetTensorShape(output),
   df0e8:	a80e      	add	r0, sp, #56	; 0x38
   df0ea:	f7f7 f9b2 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
  reference_ops::MaxPool(op_params, GetTensorShape(input),
   df0ee:	a809      	add	r0, sp, #36	; 0x24
   df0f0:	f7f7 f9af 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   df0f4:	e008      	b.n	df108 <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x140>
      break;
    case kTfLiteUInt8:
      MaxEvalQuantizedUInt8(context, node, params, &data, input, output);
      break;
    default:
      context->ReportError(context, "Type %s not currently supported.",
   df0f6:	f8d9 4014 	ldr.w	r4, [r9, #20]
   df0fa:	f7f5 f80f 	bl	d411c <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type));
   df0fe:	4907      	ldr	r1, [pc, #28]	; (df11c <_ZN6tflite3ops5micro7pooling7MaxEvalEP13TfLiteContextP10TfLiteNode+0x154>)
   df100:	4602      	mov	r2, r0
   df102:	4648      	mov	r0, r9
   df104:	47a0      	blx	r4
  OpData data;

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, input, output, &data));
   df106:	2601      	movs	r6, #1
      context->ReportError(context, "Type %s not currently supported.",
                           TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   df108:	4630      	mov	r0, r6
   df10a:	b01f      	add	sp, #124	; 0x7c
   df10c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   df110:	7f7fffff 	.word	0x7f7fffff
   df114:	ff7fffff 	.word	0xff7fffff
   df118:	00000000 	.word	0x00000000
   df11c:	000e9f62 	.word	0x000e9f62

000df120 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

TfLiteStatus AverageEval(TfLiteContext* context, TfLiteNode* node) {
   df120:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   df124:	680b      	ldr	r3, [r1, #0]
   df126:	f8d0 a008 	ldr.w	sl, [r0, #8]
   df12a:	685b      	ldr	r3, [r3, #4]
  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);
   df12c:	694d      	ldr	r5, [r1, #20]
   df12e:	2438      	movs	r4, #56	; 0x38
   df130:	fb04 f803 	mul.w	r8, r4, r3

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

TfLiteStatus AverageEval(TfLiteContext* context, TfLiteNode* node) {
   df134:	b09f      	sub	sp, #124	; 0x7c
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   df136:	684b      	ldr	r3, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   df138:	eb0a 0608 	add.w	r6, sl, r8
   df13c:	4681      	mov	r9, r0
  OpData data;

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, input, output, &data));
   df13e:	aa05      	add	r2, sp, #20
   df140:	4631      	mov	r1, r6
   df142:	4628      	mov	r0, r5
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   df144:	f8d3 b004 	ldr.w	fp, [r3, #4]
   df148:	f7ff fefe 	bl	def48 <_ZN6tflite3ops5micro7pooling12_GLOBAL__N_115CalculateOpDataEPK13TfLiteContextPK16TfLitePoolParamsPK12TfLiteTensorSC_PNS3_6OpDataE.isra.2>
   df14c:	4607      	mov	r7, r0
   df14e:	2800      	cmp	r0, #0
   df150:	f040 80b4 	bne.w	df2bc <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x19c>

  // Inputs and outputs share the same type, guarenteed by the converter.
  switch (input->type) {
   df154:	f81a 0008 	ldrb.w	r0, [sl, r8]
   df158:	2803      	cmp	r0, #3
   df15a:	fb04 a40b 	mla	r4, r4, fp, sl
   df15e:	d045      	beq.n	df1ec <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0xcc>
   df160:	2809      	cmp	r0, #9
   df162:	d070      	beq.n	df246 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x126>
   df164:	2801      	cmp	r0, #1
   df166:	f040 80a1 	bne.w	df2ac <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x18c>

void AverageEvalFloat(const TfLiteContext* context, const TfLiteNode* node,
                      const TfLitePoolParams* params, const OpData* data,
                      const TfLiteTensor* input, TfLiteTensor* output) {
  float activation_min, activation_max;
  CalculateActivationRange(params->activation, &activation_min,
   df16a:	7d2b      	ldrb	r3, [r5, #20]
// Calculates the useful range of an activation layer given its activation
// tensor.a
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
   df16c:	2b01      	cmp	r3, #1
   df16e:	d011      	beq.n	df194 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x74>
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
   df170:	2b03      	cmp	r3, #3
   df172:	d012      	beq.n	df19a <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x7a>
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
    *activation_max = std::numeric_limits<T>::max();
   df174:	ed9f 7a54 	vldr	s14, [pc, #336]	; df2c8 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x1a8>
   df178:	eddf 6a54 	vldr	s13, [pc, #336]	; df2cc <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x1ac>
   df17c:	2b02      	cmp	r3, #2
   df17e:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   df182:	bf18      	it	ne
   df184:	eef0 7a47 	vmovne.f32	s15, s14
   df188:	eebf 7a00 	vmov.f32	s14, #240	; 0xbf800000 -1.0
   df18c:	bf18      	it	ne
   df18e:	eeb0 7a66 	vmovne.f32	s14, s13
   df192:	e006      	b.n	df1a2 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x82>
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
   df194:	eddf 7a4c 	vldr	s15, [pc, #304]	; df2c8 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x1a8>
   df198:	e001      	b.n	df19e <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x7e>
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
    *activation_max = 6;
   df19a:	eef1 7a08 	vmov.f32	s15, #24	; 0x40c00000  6.0
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
   df19e:	ed9f 7a4c 	vldr	s14, [pc, #304]	; df2d0 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x1b0>
                           &activation_max);

  PoolParams op_params;
  op_params.stride_height = params->stride_height;
   df1a2:	68ab      	ldr	r3, [r5, #8]
   df1a4:	9316      	str	r3, [sp, #88]	; 0x58
  op_params.stride_width = params->stride_width;
   df1a6:	686b      	ldr	r3, [r5, #4]
   df1a8:	9317      	str	r3, [sp, #92]	; 0x5c
  op_params.filter_height = params->filter_height;
   df1aa:	692b      	ldr	r3, [r5, #16]
   df1ac:	9318      	str	r3, [sp, #96]	; 0x60
  op_params.filter_width = params->filter_width;
   df1ae:	68eb      	ldr	r3, [r5, #12]
   df1b0:	9319      	str	r3, [sp, #100]	; 0x64
  op_params.padding_values.height = data->padding.height;
   df1b2:	9b06      	ldr	r3, [sp, #24]
   df1b4:	f8ad 3050 	strh.w	r3, [sp, #80]	; 0x50
  op_params.padding_values.width = data->padding.width;
  op_params.float_activation_min = activation_min;
  op_params.float_activation_max = activation_max;
  reference_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
   df1b8:	4631      	mov	r1, r6
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.filter_height = params->filter_height;
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
   df1ba:	9b05      	ldr	r3, [sp, #20]
   df1bc:	f8ad 304e 	strh.w	r3, [sp, #78]	; 0x4e
  op_params.float_activation_min = activation_min;
  op_params.float_activation_max = activation_max;
  reference_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
   df1c0:	a809      	add	r0, sp, #36	; 0x24
  op_params.stride_width = params->stride_width;
  op_params.filter_height = params->filter_height;
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.float_activation_min = activation_min;
   df1c2:	ed8d 7a1c 	vstr	s14, [sp, #112]	; 0x70
  op_params.float_activation_max = activation_max;
   df1c6:	edcd 7a1d 	vstr	s15, [sp, #116]	; 0x74
  reference_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
   df1ca:	f7f7 fbf2 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(output), GetTensorData<float>(output));
   df1ce:	4621      	mov	r1, r4
   df1d0:	a80e      	add	r0, sp, #56	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   df1d2:	6875      	ldr	r5, [r6, #4]
   df1d4:	f7f7 fbed 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   df1d8:	b104      	cbz	r4, df1dc <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0xbc>
   df1da:	6864      	ldr	r4, [r4, #4]
   df1dc:	9400      	str	r4, [sp, #0]
   df1de:	ab0e      	add	r3, sp, #56	; 0x38
   df1e0:	462a      	mov	r2, r5
   df1e2:	a909      	add	r1, sp, #36	; 0x24
   df1e4:	a813      	add	r0, sp, #76	; 0x4c
   df1e6:	f7ff fa8b 	bl	de700 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKfS6_Pf>
   df1ea:	e058      	b.n	df29e <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x17e>
void AverageEvalUint8(const TfLiteContext* context, const TfLiteNode* node,
                      const TfLitePoolParams* params, const OpData* data,
                      const TfLiteTensor* input, TfLiteTensor* output) {
  int32_t activation_min, activation_max;
  CalculateActivationRangeUint8(params->activation, output, &activation_min,
                                &activation_max);
   df1ec:	7d28      	ldrb	r0, [r5, #20]
   df1ee:	ab04      	add	r3, sp, #16
   df1f0:	aa03      	add	r2, sp, #12
   df1f2:	4621      	mov	r1, r4
   df1f4:	f004 fcbe 	bl	e3b74 <_ZN6tflite29CalculateActivationRangeUint8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>

  PoolParams op_params;
  op_params.stride_height = params->stride_height;
   df1f8:	68ab      	ldr	r3, [r5, #8]
   df1fa:	9316      	str	r3, [sp, #88]	; 0x58
  op_params.stride_width = params->stride_width;
   df1fc:	686b      	ldr	r3, [r5, #4]
   df1fe:	9317      	str	r3, [sp, #92]	; 0x5c
  op_params.filter_height = params->filter_height;
   df200:	692b      	ldr	r3, [r5, #16]
   df202:	9318      	str	r3, [sp, #96]	; 0x60
  op_params.filter_width = params->filter_width;
   df204:	68eb      	ldr	r3, [r5, #12]
   df206:	9319      	str	r3, [sp, #100]	; 0x64
  op_params.padding_values.height = data->padding.height;
   df208:	9b06      	ldr	r3, [sp, #24]
   df20a:	f8ad 3050 	strh.w	r3, [sp, #80]	; 0x50
  op_params.padding_values.width = data->padding.width;
   df20e:	9b05      	ldr	r3, [sp, #20]
   df210:	f8ad 304e 	strh.w	r3, [sp, #78]	; 0x4e
  op_params.quantized_activation_min = activation_min;
   df214:	9b03      	ldr	r3, [sp, #12]
   df216:	931a      	str	r3, [sp, #104]	; 0x68
  op_params.quantized_activation_max = activation_max;
  reference_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   df218:	4631      	mov	r1, r6
  op_params.filter_height = params->filter_height;
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
   df21a:	9b04      	ldr	r3, [sp, #16]
   df21c:	931b      	str	r3, [sp, #108]	; 0x6c
  reference_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   df21e:	a809      	add	r0, sp, #36	; 0x24
   df220:	f7f7 fbc7 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(output), GetTensorData<uint8_t>(output));
   df224:	4621      	mov	r1, r4
   df226:	a80e      	add	r0, sp, #56	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   df228:	6875      	ldr	r5, [r6, #4]
   df22a:	f7f7 fbc2 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   df22e:	b10c      	cbz	r4, df234 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x114>
   df230:	6863      	ldr	r3, [r4, #4]
   df232:	e000      	b.n	df236 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x116>
   df234:	463b      	mov	r3, r7
   df236:	9300      	str	r3, [sp, #0]
   df238:	462a      	mov	r2, r5
   df23a:	ab0e      	add	r3, sp, #56	; 0x38
   df23c:	a909      	add	r1, sp, #36	; 0x24
   df23e:	a813      	add	r0, sp, #76	; 0x4c
   df240:	f7ff fb36 	bl	de8b0 <_ZN6tflite13reference_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKhS6_Ph>
   df244:	e02b      	b.n	df29e <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x17e>
void AverageEvalInt8(const TfLiteContext* context, const TfLiteNode* node,
                     const TfLitePoolParams* params, const OpData* data,
                     const TfLiteTensor* input, TfLiteTensor* output) {
  int32_t activation_min, activation_max;
  CalculateActivationRangeInt8(params->activation, output, &activation_min,
                               &activation_max);
   df246:	7d28      	ldrb	r0, [r5, #20]
   df248:	ab04      	add	r3, sp, #16
   df24a:	aa03      	add	r2, sp, #12
   df24c:	4621      	mov	r1, r4
   df24e:	f004 fd8b 	bl	e3d68 <_ZN6tflite28CalculateActivationRangeInt8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>

  PoolParams op_params;
  op_params.stride_height = params->stride_height;
   df252:	68ab      	ldr	r3, [r5, #8]
   df254:	9316      	str	r3, [sp, #88]	; 0x58
  op_params.stride_width = params->stride_width;
   df256:	686b      	ldr	r3, [r5, #4]
   df258:	9317      	str	r3, [sp, #92]	; 0x5c
  op_params.filter_height = params->filter_height;
   df25a:	692b      	ldr	r3, [r5, #16]
   df25c:	9318      	str	r3, [sp, #96]	; 0x60
  op_params.filter_width = params->filter_width;
   df25e:	68eb      	ldr	r3, [r5, #12]
   df260:	9319      	str	r3, [sp, #100]	; 0x64
  op_params.padding_values.height = data->padding.height;
   df262:	9b06      	ldr	r3, [sp, #24]
   df264:	f8ad 3050 	strh.w	r3, [sp, #80]	; 0x50
  op_params.padding_values.width = data->padding.width;
   df268:	9b05      	ldr	r3, [sp, #20]
   df26a:	f8ad 304e 	strh.w	r3, [sp, #78]	; 0x4e
  op_params.quantized_activation_min = activation_min;
   df26e:	9b03      	ldr	r3, [sp, #12]
   df270:	931a      	str	r3, [sp, #104]	; 0x68
  op_params.quantized_activation_max = activation_max;
  reference_integer_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
   df272:	4631      	mov	r1, r6
  op_params.filter_height = params->filter_height;
  op_params.filter_width = params->filter_width;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
   df274:	9b04      	ldr	r3, [sp, #16]
   df276:	931b      	str	r3, [sp, #108]	; 0x6c
  reference_integer_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
   df278:	a809      	add	r0, sp, #36	; 0x24
   df27a:	f7f7 fb9a 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(output), GetTensorData<int8_t>(output));
   df27e:	4621      	mov	r1, r4
   df280:	a80e      	add	r0, sp, #56	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   df282:	6875      	ldr	r5, [r6, #4]
   df284:	f7f7 fb95 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   df288:	b10c      	cbz	r4, df28e <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x16e>
   df28a:	6863      	ldr	r3, [r4, #4]
   df28c:	e000      	b.n	df290 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x170>
   df28e:	463b      	mov	r3, r7
   df290:	9300      	str	r3, [sp, #0]
   df292:	462a      	mov	r2, r5
   df294:	ab0e      	add	r3, sp, #56	; 0x38
   df296:	a909      	add	r1, sp, #36	; 0x24
   df298:	a813      	add	r0, sp, #76	; 0x4c
   df29a:	f7ff fd86 	bl	dedaa <_ZN6tflite21reference_integer_ops11AveragePoolERKNS_10PoolParamsERKNS_12RuntimeShapeEPKaS6_Pa>
   df29e:	a80e      	add	r0, sp, #56	; 0x38
   df2a0:	f7f7 f8d7 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = activation_min;
  op_params.quantized_activation_max = activation_max;
  reference_integer_ops::AveragePool(
      op_params, GetTensorShape(input), GetTensorData<int8_t>(input),
   df2a4:	a809      	add	r0, sp, #36	; 0x24
   df2a6:	f7f7 f8d4 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   df2aa:	e008      	b.n	df2be <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x19e>
      break;
    case kTfLiteInt8:
      AverageEvalInt8(context, node, params, &data, input, output);
      break;
    default:
      context->ReportError(context, "Input type %s is not currently supported",
   df2ac:	f8d9 4014 	ldr.w	r4, [r9, #20]
   df2b0:	f7f4 ff34 	bl	d411c <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type));
   df2b4:	4907      	ldr	r1, [pc, #28]	; (df2d4 <_ZN6tflite3ops5micro7pooling11AverageEvalEP13TfLiteContextP10TfLiteNode+0x1b4>)
   df2b6:	4602      	mov	r2, r0
   df2b8:	4648      	mov	r0, r9
   df2ba:	47a0      	blx	r4
  OpData data;

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, input, output, &data));
   df2bc:	2701      	movs	r7, #1
      context->ReportError(context, "Input type %s is not currently supported",
                           TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   df2be:	4638      	mov	r0, r7
   df2c0:	b01f      	add	sp, #124	; 0x7c
   df2c2:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   df2c6:	bf00      	nop
   df2c8:	7f7fffff 	.word	0x7f7fffff
   df2cc:	ff7fffff 	.word	0xff7fffff
   df2d0:	00000000 	.word	0x00000000
   df2d4:	000e9f83 	.word	0x000e9f83

000df2d8 <_ZN6tflite3ops5micro24Register_AVERAGE_POOL_2DEv>:
      pooling::Free,
      pooling::Prepare,
      pooling::AverageEval,
  };
  return &r;
}
   df2d8:	4800      	ldr	r0, [pc, #0]	; (df2dc <_ZN6tflite3ops5micro24Register_AVERAGE_POOL_2DEv+0x4>)
   df2da:	4770      	bx	lr
   df2dc:	2003c21c 	.word	0x2003c21c

000df2e0 <_ZN6tflite3ops5micro20Register_MAX_POOL_2DEv>:

TfLiteRegistration* Register_MAX_POOL_2D() {
  static TfLiteRegistration r = {pooling::Init, pooling::Free, pooling::Prepare,
                                 pooling::MaxEval};
  return &r;
}
   df2e0:	4800      	ldr	r0, [pc, #0]	; (df2e4 <_ZN6tflite3ops5micro20Register_MAX_POOL_2DEv+0x4>)
   df2e2:	4770      	bx	lr
   df2e4:	2003c23c 	.word	0x2003c23c

000df2e8 <_ZN6tflite3ops5micro11activations12PreluPrepareEP13TfLiteContextP10TfLiteNode>:
namespace micro {
namespace activations {

TfLiteStatus PreluPrepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   df2e8:	2000      	movs	r0, #0
   df2ea:	4770      	bx	lr

000df2ec <_ZN6tflite3ops5micro14Register_PRELUEv>:

TfLiteRegistration* Register_PRELU() {
  static TfLiteRegistration r = {nullptr, nullptr, activations::PreluPrepare,
                                 activations::PreluEval};
  return &r;
}
   df2ec:	4800      	ldr	r0, [pc, #0]	; (df2f0 <_ZN6tflite3ops5micro14Register_PRELUEv+0x4>)
   df2ee:	4770      	bx	lr
   df2f0:	2003c25c 	.word	0x2003c25c

000df2f4 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf>:
}

inline void BroadcastPrelu4DSlowFloat(
    const RuntimeShape& unextended_input1_shape, const float* input1_data,
    const RuntimeShape& unextended_input2_shape, const float* input2_data,
    const RuntimeShape& unextended_output_shape, float* output_data) {
   df2f4:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   df2f8:	469b      	mov	fp, r3
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   df2fa:	6803      	ldr	r3, [r0, #0]
}

inline void BroadcastPrelu4DSlowFloat(
    const RuntimeShape& unextended_input1_shape, const float* input1_data,
    const RuntimeShape& unextended_input2_shape, const float* input2_data,
    const RuntimeShape& unextended_output_shape, float* output_data) {
   df2fc:	b09b      	sub	sp, #108	; 0x6c
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   df2fe:	2b04      	cmp	r3, #4
}

inline void BroadcastPrelu4DSlowFloat(
    const RuntimeShape& unextended_input1_shape, const float* input1_data,
    const RuntimeShape& unextended_input2_shape, const float* input2_data,
    const RuntimeShape& unextended_output_shape, float* output_data) {
   df300:	4616      	mov	r6, r2
   df302:	4604      	mov	r4, r0
   df304:	468a      	mov	sl, r1
   df306:	9a24      	ldr	r2, [sp, #144]	; 0x90
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
   df308:	dd01      	ble.n	df30e <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x1a>
   df30a:	f005 f81f 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
   df30e:	6833      	ldr	r3, [r6, #0]
   df310:	2b04      	cmp	r3, #4
   df312:	dcfa      	bgt.n	df30a <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x16>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   df314:	6813      	ldr	r3, [r2, #0]
   df316:	2b04      	cmp	r3, #4
   df318:	dcf7      	bgt.n	df30a <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x16>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
   df31a:	2301      	movs	r3, #1
   df31c:	2104      	movs	r1, #4
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);
   df31e:	ad0a      	add	r5, sp, #40	; 0x28
   df320:	a805      	add	r0, sp, #20
   df322:	f7f7 f8da 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   df326:	4620      	mov	r0, r4
   df328:	ab12      	add	r3, sp, #72	; 0x48
   df32a:	462a      	mov	r2, r5
   df32c:	4631      	mov	r1, r6
   df32e:	f7f7 fbe9 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   df332:	2400      	movs	r4, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   df334:	9503      	str	r5, [sp, #12]
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   df336:	2100      	movs	r1, #0
   df338:	a805      	add	r0, sp, #20
   df33a:	f7f7 f895 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   df33e:	4284      	cmp	r4, r0
   df340:	da47      	bge.n	df3d2 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0xde>
   df342:	2500      	movs	r5, #0
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   df344:	2101      	movs	r1, #1
   df346:	a805      	add	r0, sp, #20
   df348:	f7f7 f88e 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   df34c:	4285      	cmp	r5, r0
   df34e:	da3e      	bge.n	df3ce <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0xda>
   df350:	2600      	movs	r6, #0
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   df352:	2102      	movs	r1, #2
   df354:	a805      	add	r0, sp, #20
   df356:	f7f7 f887 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   df35a:	4286      	cmp	r6, r0
   df35c:	da35      	bge.n	df3ca <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0xd6>
   df35e:	2700      	movs	r7, #0
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   df360:	2103      	movs	r1, #3
   df362:	a805      	add	r0, sp, #20
   df364:	f7f7 f880 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   df368:	4287      	cmp	r7, r0
   df36a:	da2c      	bge.n	df3c6 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0xd2>
          auto out_idx = Offset(output_shape, b, y, x, c);
   df36c:	9700      	str	r7, [sp, #0]
   df36e:	4633      	mov	r3, r6
   df370:	462a      	mov	r2, r5
   df372:	4621      	mov	r1, r4
   df374:	a805      	add	r0, sp, #20
   df376:	f7f7 f8dc 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   df37a:	9700      	str	r7, [sp, #0]

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
   df37c:	4680      	mov	r8, r0
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   df37e:	4633      	mov	r3, r6
   df380:	462a      	mov	r2, r5
   df382:	4621      	mov	r1, r4
   df384:	9803      	ldr	r0, [sp, #12]
   df386:	f7f7 f985 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   df38a:	9700      	str	r7, [sp, #0]
  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
   df38c:	4681      	mov	r9, r0
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
   df38e:	4633      	mov	r3, r6
   df390:	462a      	mov	r2, r5
   df392:	4621      	mov	r1, r4
   df394:	a812      	add	r0, sp, #72	; 0x48
   df396:	f7f7 f97d 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          auto in1_val = input1_data[in1_idx];
   df39a:	eb0a 0989 	add.w	r9, sl, r9, lsl #2
   df39e:	edd9 7a00 	vldr	s15, [r9]
          auto in2_val = input2_data[in2_idx];
          output_data[out_idx] = in1_val >= 0.0 ? in1_val : in1_val * in2_val;
   df3a2:	9b25      	ldr	r3, [sp, #148]	; 0x94
        for (int c = 0; c < output_shape.Dims(3); ++c) {
          auto out_idx = Offset(output_shape, b, y, x, c);
          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);
          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);
          auto in1_val = input1_data[in1_idx];
          auto in2_val = input2_data[in2_idx];
   df3a4:	eb0b 0080 	add.w	r0, fp, r0, lsl #2
   df3a8:	ed90 7a00 	vldr	s14, [r0]
          output_data[out_idx] = in1_val >= 0.0 ? in1_val : in1_val * in2_val;
   df3ac:	eef5 7ac0 	vcmpe.f32	s15, #0.0
   df3b0:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   df3b4:	bfb8      	it	lt
   df3b6:	ee67 7a87 	vmullt.f32	s15, s15, s14
   df3ba:	eb03 0888 	add.w	r8, r3, r8, lsl #2
   df3be:	edc8 7a00 	vstr	s15, [r8]
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
        for (int c = 0; c < output_shape.Dims(3); ++c) {
   df3c2:	3701      	adds	r7, #1
   df3c4:	e7cc      	b.n	df360 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x6c>
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
      for (int x = 0; x < output_shape.Dims(2); ++x) {
   df3c6:	3601      	adds	r6, #1
   df3c8:	e7c3      	b.n	df352 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x5e>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
    for (int y = 0; y < output_shape.Dims(1); ++y) {
   df3ca:	3501      	adds	r5, #1
   df3cc:	e7ba      	b.n	df344 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x50>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,
                                      unextended_input2_shape, &desc1, &desc2);

  for (int b = 0; b < output_shape.Dims(0); ++b) {
   df3ce:	3401      	adds	r4, #1
   df3d0:	e7b1      	b.n	df336 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf+0x42>
    const RuntimeShape& unextended_output_shape, float* output_data) {
  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   df3d2:	a805      	add	r0, sp, #20
   df3d4:	f7f7 f83d 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
          output_data[out_idx] = in1_val >= 0.0 ? in1_val : in1_val * in2_val;
        }
      }
    }
  }
}
   df3d8:	b01b      	add	sp, #108	; 0x6c
   df3da:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

000df3e0 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph>:
                                 const RuntimeShape& input_shape,
                                 const uint8* input_data,
                                 const RuntimeShape& alpha_shape,
                                 const uint8* alpha_data,
                                 const RuntimeShape& output_shape,
                                 uint8* output_data) {
   df3e0:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   df3e4:	461d      	mov	r5, r3
  TFLITE_DCHECK_LE(input_shape.DimensionsCount(), 4);
   df3e6:	680b      	ldr	r3, [r1, #0]
                                 const RuntimeShape& input_shape,
                                 const uint8* input_data,
                                 const RuntimeShape& alpha_shape,
                                 const uint8* alpha_data,
                                 const RuntimeShape& output_shape,
                                 uint8* output_data) {
   df3e8:	b09d      	sub	sp, #116	; 0x74
  TFLITE_DCHECK_LE(input_shape.DimensionsCount(), 4);
   df3ea:	2b04      	cmp	r3, #4
                                 const RuntimeShape& input_shape,
                                 const uint8* input_data,
                                 const RuntimeShape& alpha_shape,
                                 const uint8* alpha_data,
                                 const RuntimeShape& output_shape,
                                 uint8* output_data) {
   df3ec:	9204      	str	r2, [sp, #16]
   df3ee:	4681      	mov	r9, r0
   df3f0:	460c      	mov	r4, r1
   df3f2:	9a27      	ldr	r2, [sp, #156]	; 0x9c
  TFLITE_DCHECK_LE(input_shape.DimensionsCount(), 4);
   df3f4:	dd01      	ble.n	df3fa <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1a>
   df3f6:	f004 ffa9 	bl	e434c <abort>
  TFLITE_DCHECK_LE(alpha_shape.DimensionsCount(), 4);
   df3fa:	682b      	ldr	r3, [r5, #0]
   df3fc:	2b04      	cmp	r3, #4
   df3fe:	dcfa      	bgt.n	df3f6 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x16>
  TFLITE_DCHECK_LE(output_shape.DimensionsCount(), 4);
   df400:	6813      	ldr	r3, [r2, #0]
   df402:	2b04      	cmp	r3, #4
   df404:	dcf7      	bgt.n	df3f6 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x16>
   df406:	2301      	movs	r3, #1
   df408:	2104      	movs	r1, #4
   df40a:	a807      	add	r0, sp, #28
   df40c:	f7f7 f865 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  const RuntimeShape extended_output_shape =
      RuntimeShape::ExtendedShape(4, output_shape);
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input_shape, alpha_shape, &desc1, &desc2);
   df410:	4629      	mov	r1, r5
   df412:	ab14      	add	r3, sp, #80	; 0x50
   df414:	aa0c      	add	r2, sp, #48	; 0x30
   df416:	4620      	mov	r0, r4
   df418:	f7f7 fb74 	bl	d6b04 <_ZN6tflite35NdArrayDescsForElementwiseBroadcastILi4EEEvRKNS_12RuntimeShapeES3_PNS_11NdArrayDescIXT_EEES6_>

  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   df41c:	2500      	movs	r5, #0
   df41e:	2100      	movs	r1, #0
   df420:	a807      	add	r0, sp, #28
   df422:	f7f7 f821 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   df426:	4285      	cmp	r5, r0
   df428:	f280 80ac 	bge.w	df584 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1a4>
   df42c:	2600      	movs	r6, #0
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
   df42e:	2101      	movs	r1, #1
   df430:	a807      	add	r0, sp, #28
   df432:	f7f7 f819 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   df436:	4286      	cmp	r6, r0
   df438:	f280 80a2 	bge.w	df580 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1a0>
   df43c:	2700      	movs	r7, #0
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
   df43e:	f10d 0a1c 	add.w	sl, sp, #28
   df442:	2102      	movs	r1, #2
   df444:	4650      	mov	r0, sl
   df446:	f7f7 f80f 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   df44a:	4287      	cmp	r7, r0
   df44c:	f280 8096 	bge.w	df57c <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x19c>
   df450:	f04f 0800 	mov.w	r8, #0
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   df454:	2103      	movs	r1, #3
   df456:	4650      	mov	r0, sl
   df458:	f7f7 f806 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   df45c:	4580      	cmp	r8, r0
   df45e:	f280 808b 	bge.w	df578 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x198>
          int output_index = Offset(extended_output_shape, b, y, x, c);
   df462:	463b      	mov	r3, r7
   df464:	4632      	mov	r2, r6
   df466:	4629      	mov	r1, r5
   df468:	f8cd 8000 	str.w	r8, [sp]
   df46c:	4650      	mov	r0, sl
   df46e:	f7f7 f860 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          int input_index = SubscriptToIndex(desc1, b, y, x, c);
   df472:	463b      	mov	r3, r7

  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
          int output_index = Offset(extended_output_shape, b, y, x, c);
   df474:	4683      	mov	fp, r0
          int input_index = SubscriptToIndex(desc1, b, y, x, c);
   df476:	f8cd 8000 	str.w	r8, [sp]
   df47a:	4632      	mov	r2, r6
   df47c:	4629      	mov	r1, r5
   df47e:	a80c      	add	r0, sp, #48	; 0x30
   df480:	f7f7 f908 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
          const int32 input_value =
              params.input_offset + input_data[input_index];
   df484:	9b04      	ldr	r3, [sp, #16]
   df486:	f8d9 4000 	ldr.w	r4, [r9]
   df48a:	5c1b      	ldrb	r3, [r3, r0]
          if (input_value >= 0) {
   df48c:	191c      	adds	r4, r3, r4
   df48e:	d403      	bmi.n	df498 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0xb8>
            output_data[output_index] = input_data[input_index];
   df490:	9a28      	ldr	r2, [sp, #160]	; 0xa0
   df492:	f802 300b 	strb.w	r3, [r2, fp]
   df496:	e06c      	b.n	df572 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x192>
          } else {
            auto alpha_index = SubscriptToIndex(desc2, b, y, x, c);
   df498:	463b      	mov	r3, r7
   df49a:	4632      	mov	r2, r6
   df49c:	f8cd 8000 	str.w	r8, [sp]
   df4a0:	4629      	mov	r1, r5
   df4a2:	a814      	add	r0, sp, #80	; 0x50
   df4a4:	f7f7 f8f6 	bl	d6694 <_ZN6tflite16SubscriptToIndexERKNS_11NdArrayDescILi4EEEiiii>
            const int32 alpha_value =
                params.alpha_offset + alpha_data[alpha_index];
            const int32 unclamped_output =
                params.output_offset +
   df4a8:	f8d9 3008 	ldr.w	r3, [r9, #8]
   df4ac:	9303      	str	r3, [sp, #12]
                MultiplyByQuantizedMultiplierSmallerThanOneExp(
   df4ae:	9b26      	ldr	r3, [sp, #152]	; 0x98
                    input_value * alpha_value, params.output_multiplier,
                    params.output_shift);
   df4b0:	f8d9 e010 	ldr.w	lr, [r9, #16]
            auto alpha_index = SubscriptToIndex(desc2, b, y, x, c);
            const int32 alpha_value =
                params.alpha_offset + alpha_data[alpha_index];
            const int32 unclamped_output =
                params.output_offset +
                MultiplyByQuantizedMultiplierSmallerThanOneExp(
   df4b4:	5c1a      	ldrb	r2, [r3, r0]
   df4b6:	f8d9 3004 	ldr.w	r3, [r9, #4]
   df4ba:	441a      	add	r2, r3
                    input_value * alpha_value, params.output_multiplier,
   df4bc:	f8d9 300c 	ldr.w	r3, [r9, #12]
            auto alpha_index = SubscriptToIndex(desc2, b, y, x, c);
            const int32 alpha_value =
                params.alpha_offset + alpha_data[alpha_index];
            const int32 unclamped_output =
                params.output_offset +
                MultiplyByQuantizedMultiplierSmallerThanOneExp(
   df4c0:	4362      	muls	r2, r4
// This function implements the same computation as the ARMv7 NEON VQRDMULH
// instruction.
template <>
inline std::int32_t SaturatingRoundingDoublingHighMul(std::int32_t a,
                                                      std::int32_t b) {
  bool overflow = a == b && a == std::numeric_limits<std::int32_t>::min();
   df4c2:	429a      	cmp	r2, r3
   df4c4:	d104      	bne.n	df4d0 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0xf0>
   df4c6:	f102 4100 	add.w	r1, r2, #2147483648	; 0x80000000
   df4ca:	424c      	negs	r4, r1
   df4cc:	414c      	adcs	r4, r1
   df4ce:	e000      	b.n	df4d2 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0xf2>
   df4d0:	2400      	movs	r4, #0
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
   df4d2:	fb82 2303 	smull	r2, r3, r2, r3
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
   df4d6:	2a00      	cmp	r2, #0
   df4d8:	f173 0100 	sbcs.w	r1, r3, #0
   df4dc:	482c      	ldr	r0, [pc, #176]	; (df590 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1b0>)
   df4de:	bfa8      	it	ge
   df4e0:	f04f 4080 	movge.w	r0, #1073741824	; 0x40000000
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
   df4e4:	b994      	cbnz	r4, df50c <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x12c>
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
   df4e6:	1812      	adds	r2, r2, r0
   df4e8:	eb43 73e0 	adc.w	r3, r3, r0, asr #31
   df4ec:	2a00      	cmp	r2, #0
   df4ee:	f173 0100 	sbcs.w	r1, r3, #0
   df4f2:	da07      	bge.n	df504 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x124>
   df4f4:	f06f 4000 	mvn.w	r0, #2147483648	; 0x80000000
   df4f8:	1880      	adds	r0, r0, r2
   df4fa:	f04f 0100 	mov.w	r1, #0
   df4fe:	4159      	adcs	r1, r3
   df500:	4602      	mov	r2, r0
   df502:	460b      	mov	r3, r1
   df504:	0fd4      	lsrs	r4, r2, #31
   df506:	ea44 0443 	orr.w	r4, r4, r3, lsl #1
   df50a:	e001      	b.n	df510 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x130>
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
   df50c:	f06f 4400 	mvn.w	r4, #2147483648	; 0x80000000

inline int32 MultiplyByQuantizedMultiplierSmallerThanOneExp(
    int32 x, int32 quantized_multiplier, int left_shift) {
  using gemmlowp::RoundingDivideByPOT;
  using gemmlowp::SaturatingRoundingDoublingHighMul;
  return RoundingDivideByPOT(
   df510:	f1ce 0300 	rsb	r3, lr, #0

// Correctly-rounded-to-nearest division by a power-of-two.
// Also known as a rounding arithmetic right shift.
template <typename IntegerType>
inline IntegerType RoundingDivideByPOT(IntegerType x, int exponent) {
  assert(exponent >= 0);
   df514:	2b00      	cmp	r3, #0
   df516:	da04      	bge.n	df522 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x142>
   df518:	4b1e      	ldr	r3, [pc, #120]	; (df594 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1b4>)
   df51a:	4a1f      	ldr	r2, [pc, #124]	; (df598 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1b8>)
   df51c:	f44f 71b3 	mov.w	r1, #358	; 0x166
   df520:	e005      	b.n	df52e <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x14e>
  assert(exponent <= 31);
   df522:	2b1f      	cmp	r3, #31
   df524:	dd06      	ble.n	df534 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x154>
   df526:	4b1d      	ldr	r3, [pc, #116]	; (df59c <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1bc>)
   df528:	4a1b      	ldr	r2, [pc, #108]	; (df598 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1b8>)
   df52a:	f240 1167 	movw	r1, #359	; 0x167
   df52e:	481c      	ldr	r0, [pc, #112]	; (df5a0 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x1c0>)
   df530:	f004 ff1c 	bl	e436c <__assert_func>
  const IntegerType mask = Dup<IntegerType>((1ll << exponent) - 1);
   df534:	461a      	mov	r2, r3
   df536:	2001      	movs	r0, #1
   df538:	2100      	movs	r1, #0
   df53a:	9305      	str	r3, [sp, #20]
   df53c:	f007 fbd6 	bl	e6cec <__aeabi_llsl>
          } else {
            auto alpha_index = SubscriptToIndex(desc2, b, y, x, c);
            const int32 alpha_value =
                params.alpha_offset + alpha_data[alpha_index];
            const int32 unclamped_output =
                params.output_offset +
   df540:	9b05      	ldr	r3, [sp, #20]
   df542:	3801      	subs	r0, #1
   df544:	ea00 0204 	and.w	r2, r0, r4
   df548:	1040      	asrs	r0, r0, #1
   df54a:	fa44 f303 	asr.w	r3, r4, r3
   df54e:	eb00 70d4 	add.w	r0, r0, r4, lsr #31
   df552:	4282      	cmp	r2, r0
   df554:	bfd4      	ite	le
   df556:	461c      	movle	r4, r3
   df558:	1c5c      	addgt	r4, r3, #1
   df55a:	9b03      	ldr	r3, [sp, #12]
   df55c:	441c      	add	r4, r3
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   df55e:	2c00      	cmp	r4, #0
   df560:	dd03      	ble.n	df56a <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x18a>
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
   df562:	2cfe      	cmp	r4, #254	; 0xfe
   df564:	dd02      	ble.n	df56c <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x18c>
	return __b;
      return __a;
   df566:	24ff      	movs	r4, #255	; 0xff
   df568:	e000      	b.n	df56c <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x18c>
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
	return __b;
      return __a;
   df56a:	2400      	movs	r4, #0
                    params.output_shift);
            const int32 quantized_min = std::numeric_limits<uint8_t>::min();
            const int32 quantized_max = std::numeric_limits<uint8_t>::max();
            const int32 clamped_output = std::min(
                quantized_max, std::max(quantized_min, unclamped_output));
            output_data[output_index] = static_cast<uint8>(clamped_output);
   df56c:	9b28      	ldr	r3, [sp, #160]	; 0xa0
   df56e:	f803 400b 	strb.w	r4, [r3, fp]
  NdArrayDescsForElementwiseBroadcast(input_shape, alpha_shape, &desc1, &desc2);

  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
        for (int c = 0; c < extended_output_shape.Dims(3); ++c) {
   df572:	f108 0801 	add.w	r8, r8, #1
   df576:	e76d      	b.n	df454 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x74>
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input_shape, alpha_shape, &desc1, &desc2);

  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
      for (int x = 0; x < extended_output_shape.Dims(2); ++x) {
   df578:	3701      	adds	r7, #1
   df57a:	e760      	b.n	df43e <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x5e>
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input_shape, alpha_shape, &desc1, &desc2);

  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
    for (int y = 0; y < extended_output_shape.Dims(1); ++y) {
   df57c:	3601      	adds	r6, #1
   df57e:	e756      	b.n	df42e <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x4e>
      RuntimeShape::ExtendedShape(4, output_shape);
  NdArrayDesc<4> desc1;
  NdArrayDesc<4> desc2;
  NdArrayDescsForElementwiseBroadcast(input_shape, alpha_shape, &desc1, &desc2);

  for (int b = 0; b < extended_output_shape.Dims(0); ++b) {
   df580:	3501      	adds	r5, #1
   df582:	e74c      	b.n	df41e <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph+0x3e>
                                 uint8* output_data) {
  TFLITE_DCHECK_LE(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(alpha_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(output_shape.DimensionsCount(), 4);
  const RuntimeShape extended_output_shape =
      RuntimeShape::ExtendedShape(4, output_shape);
   df584:	a807      	add	r0, sp, #28
   df586:	f7f6 ff64 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
          }
        }
      }
    }
  }
}
   df58a:	b01d      	add	sp, #116	; 0x74
   df58c:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   df590:	c0000001 	.word	0xc0000001
   df594:	000e9654 	.word	0x000e9654
   df598:	000e9fe4 	.word	0x000e9fe4
   df59c:	000e9701 	.word	0x000e9701
   df5a0:	000e9662 	.word	0x000e9662

000df5a4 <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus PreluEval(TfLiteContext* context, TfLiteNode* node) {
   df5a4:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
   df5a8:	680b      	ldr	r3, [r1, #0]
   df5aa:	6887      	ldr	r7, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   df5ac:	689c      	ldr	r4, [r3, #8]
   df5ae:	4681      	mov	r9, r0
   df5b0:	6858      	ldr	r0, [r3, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   df5b2:	684b      	ldr	r3, [r1, #4]
   df5b4:	685b      	ldr	r3, [r3, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   df5b6:	2238      	movs	r2, #56	; 0x38
   df5b8:	b09b      	sub	sp, #108	; 0x6c
   df5ba:	fb02 f800 	mul.w	r8, r2, r0
   df5be:	fb02 7404 	mla	r4, r2, r4, r7
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   df5c2:	435a      	muls	r2, r3
  const TfLiteTensor* input = GetInput(context, node, 0);
  const TfLiteTensor* alpha = GetInput(context, node, 1);
  TfLiteTensor* output = GetOutput(context, node, 0);
  int32_t output_multiplier = 0;
   df5c4:	2300      	movs	r3, #0
   df5c6:	9304      	str	r3, [sp, #16]
  int output_shift = 0;
   df5c8:	9305      	str	r3, [sp, #20]
  if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt16) {
   df5ca:	5cbb      	ldrb	r3, [r7, r2]
   df5cc:	f003 03fb 	and.w	r3, r3, #251	; 0xfb
   df5d0:	2b03      	cmp	r3, #3
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   df5d2:	eb07 0608 	add.w	r6, r7, r8
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   df5d6:	eb07 0502 	add.w	r5, r7, r2
   df5da:	d113      	bne.n	df604 <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0x60>
    double real_multiplier =
        input->params.scale * alpha->params.scale / output->params.scale;
    QuantizeMultiplierSmallerThanOneExp(real_multiplier, &output_multiplier,
                                        &output_shift);
   df5dc:	ed94 7a03 	vldr	s14, [r4, #12]
   df5e0:	edd6 7a03 	vldr	s15, [r6, #12]
   df5e4:	ee67 7a87 	vmul.f32	s15, s15, s14
   df5e8:	ed95 7a03 	vldr	s14, [r5, #12]
   df5ec:	eec7 6a87 	vdiv.f32	s13, s15, s14
   df5f0:	ee16 0a90 	vmov	r0, s13
   df5f4:	f007 fcea 	bl	e6fcc <__aeabi_f2d>
   df5f8:	ec41 0b10 	vmov	d0, r0, r1
   df5fc:	a905      	add	r1, sp, #20
   df5fe:	a804      	add	r0, sp, #16
   df600:	f004 fc32 	bl	e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>
  }
  switch (input->type) {
   df604:	f817 0008 	ldrb.w	r0, [r7, r8]
   df608:	2801      	cmp	r0, #1
   df60a:	d028      	beq.n	df65e <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0xba>
   df60c:	2803      	cmp	r0, #3
   df60e:	d14a      	bne.n	df6a6 <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0x102>
          GetTensorShape(output), GetTensorData<float>(output));
      return kTfLiteOk;
    } break;
    case kTfLiteUInt8: {
      PreluParams op_params;
      op_params.input_offset = -input->params.zero_point;
   df610:	6933      	ldr	r3, [r6, #16]
   df612:	425b      	negs	r3, r3
   df614:	9306      	str	r3, [sp, #24]
      op_params.alpha_offset = -alpha->params.zero_point;
   df616:	6923      	ldr	r3, [r4, #16]
   df618:	425b      	negs	r3, r3
   df61a:	9307      	str	r3, [sp, #28]
      op_params.output_offset = output->params.zero_point;
   df61c:	692b      	ldr	r3, [r5, #16]
   df61e:	9308      	str	r3, [sp, #32]
      op_params.output_multiplier = output_multiplier;
   df620:	9b04      	ldr	r3, [sp, #16]
   df622:	9309      	str	r3, [sp, #36]	; 0x24
      op_params.output_shift = output_shift;
      reference_ops::BroadcastPrelu4DSlow(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   df624:	4631      	mov	r1, r6
      PreluParams op_params;
      op_params.input_offset = -input->params.zero_point;
      op_params.alpha_offset = -alpha->params.zero_point;
      op_params.output_offset = output->params.zero_point;
      op_params.output_multiplier = output_multiplier;
      op_params.output_shift = output_shift;
   df626:	9b05      	ldr	r3, [sp, #20]
   df628:	930a      	str	r3, [sp, #40]	; 0x28
      reference_ops::BroadcastPrelu4DSlow(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   df62a:	a80b      	add	r0, sp, #44	; 0x2c
   df62c:	f7f7 f9c1 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
          GetTensorShape(alpha), GetTensorData<uint8_t>(alpha),
   df630:	4621      	mov	r1, r4
   df632:	a810      	add	r0, sp, #64	; 0x40
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   df634:	6876      	ldr	r6, [r6, #4]
   df636:	f7f7 f9bc 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   df63a:	6867      	ldr	r7, [r4, #4]
          GetTensorShape(output), GetTensorData<uint8_t>(output));
   df63c:	ac15      	add	r4, sp, #84	; 0x54
   df63e:	4629      	mov	r1, r5
   df640:	4620      	mov	r0, r4
   df642:	f7f7 f9b6 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   df646:	686b      	ldr	r3, [r5, #4]
   df648:	9302      	str	r3, [sp, #8]
   df64a:	a806      	add	r0, sp, #24
   df64c:	9401      	str	r4, [sp, #4]
   df64e:	9700      	str	r7, [sp, #0]
   df650:	ab10      	add	r3, sp, #64	; 0x40
   df652:	4632      	mov	r2, r6
   df654:	a90b      	add	r1, sp, #44	; 0x2c
   df656:	f7ff fec3 	bl	df3e0 <_ZN6tflite13reference_ops20BroadcastPrelu4DSlowERKNS_11PreluParamsERKNS_12RuntimeShapeEPKhS6_S8_S6_Ph>
   df65a:	4620      	mov	r0, r4
   df65c:	e019      	b.n	df692 <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0xee>
                                        &output_shift);
  }
  switch (input->type) {
    case kTfLiteFloat32: {
      BroadcastPrelu4DSlowFloat(
          GetTensorShape(input), GetTensorData<float>(input),
   df65e:	4631      	mov	r1, r6
   df660:	a80b      	add	r0, sp, #44	; 0x2c
   df662:	f7f7 f9a6 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
          GetTensorShape(alpha), GetTensorData<float>(alpha),
   df666:	4621      	mov	r1, r4
   df668:	a810      	add	r0, sp, #64	; 0x40
   df66a:	6877      	ldr	r7, [r6, #4]
   df66c:	f7f7 f9a1 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   df670:	b104      	cbz	r4, df674 <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0xd0>
   df672:	6864      	ldr	r4, [r4, #4]
          GetTensorShape(output), GetTensorData<float>(output));
   df674:	ae15      	add	r6, sp, #84	; 0x54
   df676:	4629      	mov	r1, r5
   df678:	4630      	mov	r0, r6
   df67a:	f7f7 f99a 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   df67e:	686b      	ldr	r3, [r5, #4]
   df680:	9301      	str	r3, [sp, #4]
   df682:	a80b      	add	r0, sp, #44	; 0x2c
   df684:	9600      	str	r6, [sp, #0]
   df686:	4623      	mov	r3, r4
   df688:	aa10      	add	r2, sp, #64	; 0x40
   df68a:	4639      	mov	r1, r7
   df68c:	f7ff fe32 	bl	df2f4 <_ZN6tflite3ops5micro11activations25BroadcastPrelu4DSlowFloatERKNS_12RuntimeShapeEPKfS5_S7_S5_Pf>
   df690:	4630      	mov	r0, r6
      op_params.output_multiplier = output_multiplier;
      op_params.output_shift = output_shift;
      reference_ops::BroadcastPrelu4DSlow(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(alpha), GetTensorData<uint8_t>(alpha),
          GetTensorShape(output), GetTensorData<uint8_t>(output));
   df692:	f7f6 fede 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
      op_params.output_offset = output->params.zero_point;
      op_params.output_multiplier = output_multiplier;
      op_params.output_shift = output_shift;
      reference_ops::BroadcastPrelu4DSlow(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(alpha), GetTensorData<uint8_t>(alpha),
   df696:	a810      	add	r0, sp, #64	; 0x40
   df698:	f7f6 fedb 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
      op_params.alpha_offset = -alpha->params.zero_point;
      op_params.output_offset = output->params.zero_point;
      op_params.output_multiplier = output_multiplier;
      op_params.output_shift = output_shift;
      reference_ops::BroadcastPrelu4DSlow(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   df69c:	a80b      	add	r0, sp, #44	; 0x2c
   df69e:	f7f6 fed8 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
          GetTensorShape(alpha), GetTensorData<uint8_t>(alpha),
          GetTensorShape(output), GetTensorData<uint8_t>(output));
      return kTfLiteOk;
   df6a2:	2000      	movs	r0, #0
   df6a4:	e008      	b.n	df6b8 <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0x114>
    } break;
    default:
      context->ReportError(
   df6a6:	f8d9 4014 	ldr.w	r4, [r9, #20]
   df6aa:	f7f4 fd37 	bl	d411c <TfLiteTypeGetName>
          context, "Only float32 and uint8 are supported currently, got %d.",
          TfLiteTypeGetName(input->type));
   df6ae:	4904      	ldr	r1, [pc, #16]	; (df6c0 <_ZN6tflite3ops5micro11activations9PreluEvalEP13TfLiteContextP10TfLiteNode+0x11c>)
   df6b0:	4602      	mov	r2, r0
   df6b2:	4648      	mov	r0, r9
   df6b4:	47a0      	blx	r4
      return kTfLiteError;
   df6b6:	2001      	movs	r0, #1
  }
}
   df6b8:	b01b      	add	sp, #108	; 0x6c
   df6ba:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}
   df6be:	bf00      	nop
   df6c0:	000e9fac 	.word	0x000e9fac

000df6c4 <_ZN6tflite3ops5micro8quantize4InitEP13TfLiteContextPKcj>:
namespace micro {
namespace quantize {

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
   df6c4:	2000      	movs	r0, #0
   df6c6:	4770      	bx	lr

000df6c8 <_ZN6tflite3ops5micro8quantize4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
   df6c8:	4770      	bx	lr
	...

000df6cc <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   df6cc:	b5f0      	push	{r4, r5, r6, r7, lr}
   df6ce:	680f      	ldr	r7, [r1, #0]
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
   df6d0:	683c      	ldr	r4, [r7, #0]
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   df6d2:	2c01      	cmp	r4, #1
  return nullptr;
}

void Free(TfLiteContext* context, void* buffer) {}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   df6d4:	b085      	sub	sp, #20
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   df6d6:	d009      	beq.n	df6ec <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x20>
   df6d8:	4a32      	ldr	r2, [pc, #200]	; (df7a4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xd8>)
   df6da:	9201      	str	r2, [sp, #4]
   df6dc:	2501      	movs	r5, #1
   df6de:	4a32      	ldr	r2, [pc, #200]	; (df7a8 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xdc>)
   df6e0:	9503      	str	r5, [sp, #12]
   df6e2:	9402      	str	r4, [sp, #8]
   df6e4:	9200      	str	r2, [sp, #0]
   df6e6:	6944      	ldr	r4, [r0, #20]
   df6e8:	2322      	movs	r3, #34	; 0x22
   df6ea:	e021      	b.n	df730 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x64>
   df6ec:	684a      	ldr	r2, [r1, #4]
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
   df6ee:	6815      	ldr	r5, [r2, #0]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
   df6f0:	2d01      	cmp	r5, #1
   df6f2:	d00b      	beq.n	df70c <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x40>
   df6f4:	4a2b      	ldr	r2, [pc, #172]	; (df7a4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xd8>)
   df6f6:	9201      	str	r2, [sp, #4]
   df6f8:	4a2c      	ldr	r2, [pc, #176]	; (df7ac <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe0>)
   df6fa:	9200      	str	r2, [sp, #0]
   df6fc:	9403      	str	r4, [sp, #12]
   df6fe:	9502      	str	r5, [sp, #8]
   df700:	6945      	ldr	r5, [r0, #20]
   df702:	4a2b      	ldr	r2, [pc, #172]	; (df7b0 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe4>)
   df704:	492b      	ldr	r1, [pc, #172]	; (df7b4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe8>)
   df706:	2323      	movs	r3, #35	; 0x23
   df708:	47a8      	blx	r5
   df70a:	e046      	b.n	df79a <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xce>

  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
   df70c:	6852      	ldr	r2, [r2, #4]

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);

  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
   df70e:	6886      	ldr	r6, [r0, #8]
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
   df710:	2138      	movs	r1, #56	; 0x38
   df712:	434a      	muls	r2, r1
   df714:	eb06 0e02 	add.w	lr, r6, r2

  // TODO(b/128934713): Add support for fixed-point per-channel quantization.
  // Currently this only support affine per-layer quantization.
  TF_LITE_ENSURE_EQ(context, output->quantization.type,
   df718:	f89e 4030 	ldrb.w	r4, [lr, #48]	; 0x30
   df71c:	2c01      	cmp	r4, #1
   df71e:	d00c      	beq.n	df73a <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x6e>
   df720:	4a25      	ldr	r2, [pc, #148]	; (df7b8 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xec>)
   df722:	9201      	str	r2, [sp, #4]
   df724:	4a25      	ldr	r2, [pc, #148]	; (df7bc <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xf0>)
   df726:	9503      	str	r5, [sp, #12]
   df728:	9402      	str	r4, [sp, #8]
   df72a:	9200      	str	r2, [sp, #0]
   df72c:	6944      	ldr	r4, [r0, #20]
   df72e:	232b      	movs	r3, #43	; 0x2b
   df730:	4a1f      	ldr	r2, [pc, #124]	; (df7b0 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe4>)
   df732:	4920      	ldr	r1, [pc, #128]	; (df7b4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe8>)
   df734:	47a0      	blx	r4
   df736:	4628      	mov	r0, r5
   df738:	e032      	b.n	df7a0 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xd4>
                    kTfLiteAffineQuantization);
  const auto* affine_quantization =
      reinterpret_cast<TfLiteAffineQuantization*>(output->quantization.params);
   df73a:	f8de 5034 	ldr.w	r5, [lr, #52]	; 0x34
  TF_LITE_ENSURE(context, affine_quantization);
   df73e:	b925      	cbnz	r5, df74a <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x7e>
   df740:	4a1f      	ldr	r2, [pc, #124]	; (df7c0 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xf4>)
   df742:	9200      	str	r2, [sp, #0]
   df744:	6945      	ldr	r5, [r0, #20]
   df746:	232e      	movs	r3, #46	; 0x2e
   df748:	e024      	b.n	df794 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xc8>
  TF_LITE_ENSURE(context, affine_quantization->scale);
   df74a:	682d      	ldr	r5, [r5, #0]
   df74c:	b925      	cbnz	r5, df758 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x8c>
   df74e:	4a1d      	ldr	r2, [pc, #116]	; (df7c4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xf8>)
   df750:	9200      	str	r2, [sp, #0]
   df752:	6945      	ldr	r5, [r0, #20]
   df754:	232f      	movs	r3, #47	; 0x2f
   df756:	e01d      	b.n	df794 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xc8>
  TF_LITE_ENSURE(context, affine_quantization->scale->size == 1);
   df758:	682d      	ldr	r5, [r5, #0]
   df75a:	2d01      	cmp	r5, #1
   df75c:	d004      	beq.n	df768 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x9c>
   df75e:	4a1a      	ldr	r2, [pc, #104]	; (df7c8 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xfc>)
   df760:	9200      	str	r2, [sp, #0]
   df762:	6945      	ldr	r5, [r0, #20]
   df764:	2330      	movs	r3, #48	; 0x30
   df766:	e015      	b.n	df794 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xc8>

  TF_LITE_ENSURE(context, input->type == kTfLiteFloat32);
   df768:	687c      	ldr	r4, [r7, #4]
   df76a:	4361      	muls	r1, r4
   df76c:	5c74      	ldrb	r4, [r6, r1]
   df76e:	2c01      	cmp	r4, #1
   df770:	d007      	beq.n	df782 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xb6>
   df772:	4a16      	ldr	r2, [pc, #88]	; (df7cc <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x100>)
   df774:	9200      	str	r2, [sp, #0]
   df776:	6944      	ldr	r4, [r0, #20]
   df778:	4a0d      	ldr	r2, [pc, #52]	; (df7b0 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe4>)
   df77a:	4915      	ldr	r1, [pc, #84]	; (df7d0 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x104>)
   df77c:	2332      	movs	r3, #50	; 0x32
   df77e:	47a0      	blx	r4
   df780:	e7d9      	b.n	df736 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x6a>
  TF_LITE_ENSURE(context,
   df782:	5cb2      	ldrb	r2, [r6, r2]
   df784:	2a03      	cmp	r2, #3
   df786:	d00a      	beq.n	df79e <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xd2>
   df788:	2a09      	cmp	r2, #9
   df78a:	d008      	beq.n	df79e <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xd2>
   df78c:	4a11      	ldr	r2, [pc, #68]	; (df7d4 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x108>)
   df78e:	9200      	str	r2, [sp, #0]
   df790:	6945      	ldr	r5, [r0, #20]
   df792:	2334      	movs	r3, #52	; 0x34
   df794:	4a06      	ldr	r2, [pc, #24]	; (df7b0 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xe4>)
   df796:	490e      	ldr	r1, [pc, #56]	; (df7d0 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0x104>)
   df798:	47a8      	blx	r5
   df79a:	4620      	mov	r0, r4
   df79c:	e000      	b.n	df7a0 <_ZN6tflite3ops5micro8quantize7PrepareEP13TfLiteContextP10TfLiteNode+0xd4>
                 output->type == kTfLiteUInt8 || output->type == kTfLiteInt8);

  return kTfLiteOk;
   df79e:	2000      	movs	r0, #0
}
   df7a0:	b005      	add	sp, #20
   df7a2:	bdf0      	pop	{r4, r5, r6, r7, pc}
   df7a4:	000eb295 	.word	0x000eb295
   df7a8:	000e98e2 	.word	0x000e98e2
   df7ac:	000e98f2 	.word	0x000e98f2
   df7b0:	000ea03e 	.word	0x000ea03e
   df7b4:	000e98c8 	.word	0x000e98c8
   df7b8:	000e9ae7 	.word	0x000e9ae7
   df7bc:	000ea0e8 	.word	0x000ea0e8
   df7c0:	000e9b1b 	.word	0x000e9b1b
   df7c4:	000e9b2f 	.word	0x000e9b2f
   df7c8:	000ea102 	.word	0x000ea102
   df7cc:	000ea128 	.word	0x000ea128
   df7d0:	000e9a98 	.word	0x000e9a98
   df7d4:	000ea146 	.word	0x000ea146

000df7d8 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   df7d8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
   df7dc:	680b      	ldr	r3, [r1, #0]
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
   df7de:	6849      	ldr	r1, [r1, #4]

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
   df7e0:	f8d0 8008 	ldr.w	r8, [r0, #8]
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
   df7e4:	684d      	ldr	r5, [r1, #4]

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
   df7e6:	685b      	ldr	r3, [r3, #4]
   df7e8:	2238      	movs	r2, #56	; 0x38
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];
   df7ea:	4355      	muls	r5, r2
   df7ec:	eb08 0a05 	add.w	sl, r8, r5
                 output->type == kTfLiteUInt8 || output->type == kTfLiteInt8);

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   df7f0:	b08d      	sub	sp, #52	; 0x34
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
   df7f2:	4353      	muls	r3, r2
                 output->type == kTfLiteUInt8 || output->type == kTfLiteInt8);

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   df7f4:	4683      	mov	fp, r0
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  tflite::QuantizationParams op_params;
  op_params.zero_point = output->params.zero_point;
  op_params.scale = output->params.scale;
   df7f6:	f8da 000c 	ldr.w	r0, [sl, #12]

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
   df7fa:	9301      	str	r3, [sp, #4]
   df7fc:	eb08 0403 	add.w	r4, r8, r3
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  tflite::QuantizationParams op_params;
  op_params.zero_point = output->params.zero_point;
  op_params.scale = output->params.scale;
   df800:	f007 fbe4 	bl	e6fcc <__aeabi_f2d>
  switch (output->type) {
   df804:	f818 2005 	ldrb.w	r2, [r8, r5]
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  tflite::QuantizationParams op_params;
  op_params.zero_point = output->params.zero_point;
   df808:	f8da 9010 	ldr.w	r9, [sl, #16]
  op_params.scale = output->params.scale;
  switch (output->type) {
   df80c:	2a03      	cmp	r2, #3
  TfLiteTensor* input = &context->tensors[node->inputs->data[0]];
  TfLiteTensor* output = &context->tensors[node->outputs->data[0]];

  tflite::QuantizationParams op_params;
  op_params.zero_point = output->params.zero_point;
  op_params.scale = output->params.scale;
   df80e:	4606      	mov	r6, r0
   df810:	460f      	mov	r7, r1
  switch (output->type) {
   df812:	d035      	beq.n	df880 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0xa8>
   df814:	2a09      	cmp	r2, #9
   df816:	9b01      	ldr	r3, [sp, #4]
   df818:	d16b      	bne.n	df8f2 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x11a>
    case kTfLiteInt8:
      reference_ops::AffineQuantize(
          op_params, GetTensorShape(input), GetTensorData<float>(input),
   df81a:	4621      	mov	r1, r4
   df81c:	a802      	add	r0, sp, #8
   df81e:	f7f7 f8c8 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   df822:	b104      	cbz	r4, df826 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x4e>
   df824:	6864      	ldr	r4, [r4, #4]
          GetTensorShape(output), GetTensorData<int8_t>(output));
   df826:	4651      	mov	r1, sl
   df828:	a807      	add	r0, sp, #28
   df82a:	f7f7 f8c2 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                           const RuntimeShape& input_shape,
                           const float* input_data,
                           const RuntimeShape& output_shape, T* output_data) {
  const int32 zero_point = op_params.zero_point;
  const double scale = static_cast<double>(op_params.scale);
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
   df82e:	a907      	add	r1, sp, #28
   df830:	a802      	add	r0, sp, #8
   df832:	f7fd f89d 	bl	dc970 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_>
   df836:	f8da a004 	ldr.w	sl, [sl, #4]
   df83a:	4680      	mov	r8, r0
   df83c:	4655      	mov	r5, sl
  static constexpr int32 min_val = std::numeric_limits<T>::min();
  static constexpr int32 max_val = std::numeric_limits<T>::max();

  for (int i = 0; i < flat_size; i++) {
   df83e:	ebca 0305 	rsb	r3, sl, r5
   df842:	4598      	cmp	r8, r3
   df844:	dd4d      	ble.n	df8e2 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x10a>
}
inline double TfLiteRound(const double x) { return ::round(x); }
#else
template <class T>
inline T TfLiteRound(const T x) {
  return std::round(x);
   df846:	f854 0b04 	ldr.w	r0, [r4], #4
   df84a:	f007 fbbf 	bl	e6fcc <__aeabi_f2d>
   df84e:	4632      	mov	r2, r6
   df850:	463b      	mov	r3, r7
   df852:	f007 fd39 	bl	e72c8 <__aeabi_ddiv>
   df856:	ec41 0b10 	vmov	d0, r0, r1
   df85a:	f005 fce3 	bl	e5224 <round>
    const float val = input_data[i];
    int32 unclamped = static_cast<int32>(TfLiteRound(val / scale)) + zero_point;
   df85e:	ec51 0b10 	vmov	r0, r1, d0
   df862:	f007 fea1 	bl	e75a8 <__aeabi_d2iz>
   df866:	4448      	add	r0, r9
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   df868:	f110 0f80 	cmn.w	r0, #128	; 0x80
   df86c:	db03      	blt.n	df876 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x9e>
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   df86e:	287f      	cmp	r0, #127	; 0x7f
   df870:	bfa8      	it	ge
   df872:	207f      	movge	r0, #127	; 0x7f
   df874:	e001      	b.n	df87a <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0xa2>
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
	return __b;
   df876:	f06f 007f 	mvn.w	r0, #127	; 0x7f
    int32 clamped = std::min(std::max(unclamped, min_val), max_val);
    output_data[i] = clamped;
   df87a:	f805 0b01 	strb.w	r0, [r5], #1
   df87e:	e7de      	b.n	df83e <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x66>
      break;
    case kTfLiteUInt8:
      reference_ops::AffineQuantize(
          op_params, GetTensorShape(input), GetTensorData<float>(input),
   df880:	4621      	mov	r1, r4
   df882:	a802      	add	r0, sp, #8
   df884:	f7f7 f895 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   df888:	b104      	cbz	r4, df88c <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0xb4>
   df88a:	6864      	ldr	r4, [r4, #4]
          GetTensorShape(output), GetTensorData<uint8_t>(output));
   df88c:	4651      	mov	r1, sl
   df88e:	a807      	add	r0, sp, #28
   df890:	f7f7 f88f 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
                           const RuntimeShape& input_shape,
                           const float* input_data,
                           const RuntimeShape& output_shape, T* output_data) {
  const int32 zero_point = op_params.zero_point;
  const double scale = static_cast<double>(op_params.scale);
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
   df894:	a907      	add	r1, sp, #28
   df896:	a802      	add	r0, sp, #8
   df898:	f7fd f86a 	bl	dc970 <_ZN6tflite16MatchingFlatSizeERKNS_12RuntimeShapeES2_>
   df89c:	f8da 8004 	ldr.w	r8, [sl, #4]
   df8a0:	4605      	mov	r5, r0
   df8a2:	46c2      	mov	sl, r8
  static constexpr int32 min_val = std::numeric_limits<T>::min();
  static constexpr int32 max_val = std::numeric_limits<T>::max();

  for (int i = 0; i < flat_size; i++) {
   df8a4:	ebc8 030a 	rsb	r3, r8, sl
   df8a8:	429d      	cmp	r5, r3
   df8aa:	dd1a      	ble.n	df8e2 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x10a>
   df8ac:	f854 0b04 	ldr.w	r0, [r4], #4
   df8b0:	f007 fb8c 	bl	e6fcc <__aeabi_f2d>
   df8b4:	4632      	mov	r2, r6
   df8b6:	463b      	mov	r3, r7
   df8b8:	f007 fd06 	bl	e72c8 <__aeabi_ddiv>
   df8bc:	ec41 0b10 	vmov	d0, r0, r1
   df8c0:	f005 fcb0 	bl	e5224 <round>
    const float val = input_data[i];
    int32 unclamped = static_cast<int32>(TfLiteRound(val / scale)) + zero_point;
   df8c4:	ec51 0b10 	vmov	r0, r1, d0
   df8c8:	f007 fe6e 	bl	e75a8 <__aeabi_d2iz>
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   df8cc:	eb10 0009 	adds.w	r0, r0, r9
   df8d0:	d403      	bmi.n	df8da <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x102>
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   df8d2:	28ff      	cmp	r0, #255	; 0xff
   df8d4:	bfa8      	it	ge
   df8d6:	20ff      	movge	r0, #255	; 0xff
   df8d8:	e000      	b.n	df8dc <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x104>
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
	return __b;
   df8da:	2000      	movs	r0, #0
    int32 clamped = std::min(std::max(unclamped, min_val), max_val);
    output_data[i] = clamped;
   df8dc:	f80a 0b01 	strb.w	r0, [sl], #1
   df8e0:	e7e0      	b.n	df8a4 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0xcc>
   df8e2:	a807      	add	r0, sp, #28
   df8e4:	f7f6 fdb5 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
          op_params, GetTensorShape(input), GetTensorData<float>(input),
          GetTensorShape(output), GetTensorData<int8_t>(output));
      break;
    case kTfLiteUInt8:
      reference_ops::AffineQuantize(
          op_params, GetTensorShape(input), GetTensorData<float>(input),
   df8e8:	a802      	add	r0, sp, #8
   df8ea:	f7f6 fdb2 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
      context->ReportError(context, "Output type %s (%d) not supported",
                           TfLiteTypeGetName(input->type), output->type);
      return kTfLiteError;
  }

  return kTfLiteOk;
   df8ee:	2000      	movs	r0, #0
      break;
    case kTfLiteUInt8:
      reference_ops::AffineQuantize(
          op_params, GetTensorShape(input), GetTensorData<float>(input),
          GetTensorShape(output), GetTensorData<uint8_t>(output));
      break;
   df8f0:	e00c      	b.n	df90c <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x134>
    default:
      context->ReportError(context, "Output type %s (%d) not supported",
   df8f2:	f818 0003 	ldrb.w	r0, [r8, r3]
   df8f6:	f8db 4014 	ldr.w	r4, [fp, #20]
   df8fa:	f7f4 fc0f 	bl	d411c <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type), output->type);
   df8fe:	f818 3005 	ldrb.w	r3, [r8, r5]
   df902:	4904      	ldr	r1, [pc, #16]	; (df914 <_ZN6tflite3ops5micro8quantize4EvalEP13TfLiteContextP10TfLiteNode+0x13c>)
   df904:	4602      	mov	r2, r0
   df906:	4658      	mov	r0, fp
   df908:	47a0      	blx	r4
      return kTfLiteError;
   df90a:	2001      	movs	r0, #1
  }

  return kTfLiteOk;
}
   df90c:	b00d      	add	sp, #52	; 0x34
   df90e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   df912:	bf00      	nop
   df914:	000ea182 	.word	0x000ea182

000df918 <_ZN6tflite3ops5micro17Register_QUANTIZEEv>:
// quantized output, in int8 or uint8 format.
TfLiteRegistration* Register_QUANTIZE() {
  static TfLiteRegistration r = {quantize::Init, quantize::Free,
                                 quantize::Prepare, quantize::Eval};
  return &r;
}
   df918:	4800      	ldr	r0, [pc, #0]	; (df91c <_ZN6tflite3ops5micro17Register_QUANTIZEEv+0x4>)
   df91a:	4770      	bx	lr
   df91c:	2003c27c 	.word	0x2003c27c

000df920 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode>:
  TF_LITE_ENSURE_EQ(context, input->type, output->type);
  TF_LITE_ENSURE_EQ(context, num_input_elements, num_output_elements);
  return kTfLiteOk;
}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   df920:	b530      	push	{r4, r5, lr}
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
   df922:	680b      	ldr	r3, [r1, #0]
  TF_LITE_ENSURE(context, NumInputs(node) == 1 || NumInputs(node) == 2);
   df924:	681b      	ldr	r3, [r3, #0]
   df926:	3b01      	subs	r3, #1
   df928:	2b01      	cmp	r3, #1
  TF_LITE_ENSURE_EQ(context, input->type, output->type);
  TF_LITE_ENSURE_EQ(context, num_input_elements, num_output_elements);
  return kTfLiteOk;
}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   df92a:	b085      	sub	sp, #20
   df92c:	4602      	mov	r2, r0
  TF_LITE_ENSURE(context, NumInputs(node) == 1 || NumInputs(node) == 2);
   df92e:	d813      	bhi.n	df958 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x38>
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
   df930:	684b      	ldr	r3, [r1, #4]
   df932:	681b      	ldr	r3, [r3, #0]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
   df934:	2b01      	cmp	r3, #1
   df936:	d00d      	beq.n	df954 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x34>
   df938:	9302      	str	r3, [sp, #8]
   df93a:	4b0c      	ldr	r3, [pc, #48]	; (df96c <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x4c>)
   df93c:	9301      	str	r3, [sp, #4]
   df93e:	2401      	movs	r4, #1
   df940:	4b0b      	ldr	r3, [pc, #44]	; (df970 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x50>)
   df942:	9300      	str	r3, [sp, #0]
   df944:	9403      	str	r4, [sp, #12]
   df946:	6955      	ldr	r5, [r2, #20]
   df948:	490a      	ldr	r1, [pc, #40]	; (df974 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x54>)
   df94a:	4a0b      	ldr	r2, [pc, #44]	; (df978 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x58>)
   df94c:	2348      	movs	r3, #72	; 0x48
   df94e:	47a8      	blx	r5
   df950:	4620      	mov	r0, r4
   df952:	e009      	b.n	df968 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x48>
  return kTfLiteOk;
   df954:	2000      	movs	r0, #0
   df956:	e007      	b.n	df968 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x48>
  TF_LITE_ENSURE_EQ(context, num_input_elements, num_output_elements);
  return kTfLiteOk;
}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  TF_LITE_ENSURE(context, NumInputs(node) == 1 || NumInputs(node) == 2);
   df958:	4b08      	ldr	r3, [pc, #32]	; (df97c <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x5c>)
   df95a:	9300      	str	r3, [sp, #0]
   df95c:	6944      	ldr	r4, [r0, #20]
   df95e:	4a06      	ldr	r2, [pc, #24]	; (df978 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x58>)
   df960:	4907      	ldr	r1, [pc, #28]	; (df980 <_ZN6tflite3ops5micro7reshape7PrepareEP13TfLiteContextP10TfLiteNode+0x60>)
   df962:	2347      	movs	r3, #71	; 0x47
   df964:	47a0      	blx	r4
   df966:	2001      	movs	r0, #1
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
  return kTfLiteOk;
}
   df968:	b005      	add	sp, #20
   df96a:	bd30      	pop	{r4, r5, pc}
   df96c:	000eb295 	.word	0x000eb295
   df970:	000e98f2 	.word	0x000e98f2
   df974:	000e98c8 	.word	0x000e98c8
   df978:	000ea1a4 	.word	0x000ea1a4
   df97c:	000ea24d 	.word	0x000ea24d
   df980:	000e9a98 	.word	0x000e9a98

000df984 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode>:

constexpr int kInputTensor = 0;
constexpr int kShapeTensor = 1;
constexpr int kOutputTensor = 0;

TfLiteStatus ReshapeOutput(TfLiteContext* context, TfLiteNode* node) {
   df984:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   df988:	f8d1 c000 	ldr.w	ip, [r1]
   df98c:	6886      	ldr	r6, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   df98e:	f8dc 7004 	ldr.w	r7, [ip, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   df992:	6849      	ldr	r1, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   df994:	2338      	movs	r3, #56	; 0x38
   df996:	435f      	muls	r7, r3
   df998:	19f2      	adds	r2, r6, r7
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   df99a:	684d      	ldr	r5, [r1, #4]
   df99c:	6891      	ldr	r1, [r2, #8]
   df99e:	435d      	muls	r5, r3
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
   df9a0:	f8d1 b000 	ldr.w	fp, [r1]
   df9a4:	b085      	sub	sp, #20
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   df9a6:	eb06 0a05 	add.w	sl, r6, r5
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
   df9aa:	2400      	movs	r4, #0
inline int NumIntermediates(const TfLiteNode* node) {
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
   df9ac:	2201      	movs	r2, #1
   df9ae:	2300      	movs	r3, #0
  for (int i = 0; i < dims->size; ++i) {
   df9b0:	45a3      	cmp	fp, r4
   df9b2:	dd0c      	ble.n	df9ce <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x4a>
    count *= dims->data[i];
   df9b4:	f851 ef04 	ldr.w	lr, [r1, #4]!
   df9b8:	ea4f 79ee 	mov.w	r9, lr, asr #31
   df9bc:	fb02 f809 	mul.w	r8, r2, r9
   df9c0:	fb0e 8803 	mla	r8, lr, r3, r8
   df9c4:	fba2 230e 	umull	r2, r3, r2, lr
   df9c8:	4443      	add	r3, r8
  return node->intermediates->size;
}

inline int64_t NumElements(const TfLiteIntArray* dims) {
  int64_t count = 1;
  for (int i = 0; i < dims->size; ++i) {
   df9ca:	3401      	adds	r4, #1
   df9cc:	e7f0      	b.n	df9b0 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x2c>
  // of output elements in the same as the number of input elements.
  int num_input_elements = NumElements(input);
  TfLiteIntArray* output_shape = output->dims;

  if (NumInputs(node) == 1 &&  // Legacy scalar supported with params.
      output_shape->size == 1 && output_shape->data[0] == 0) {
   df9ce:	f8dc 3000 	ldr.w	r3, [ip]
  // Tensorflow's Reshape allows one of the shape components to have the
  // special -1 value, meaning it will be calculated automatically based on the
  // input. Here we calculate what that dimension should be so that the number
  // of output elements in the same as the number of input elements.
  int num_input_elements = NumElements(input);
  TfLiteIntArray* output_shape = output->dims;
   df9d2:	f8da 4008 	ldr.w	r4, [sl, #8]

  if (NumInputs(node) == 1 &&  // Legacy scalar supported with params.
      output_shape->size == 1 && output_shape->data[0] == 0) {
   df9d6:	2b01      	cmp	r3, #1
   df9d8:	d105      	bne.n	df9e6 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x62>
  // input. Here we calculate what that dimension should be so that the number
  // of output elements in the same as the number of input elements.
  int num_input_elements = NumElements(input);
  TfLiteIntArray* output_shape = output->dims;

  if (NumInputs(node) == 1 &&  // Legacy scalar supported with params.
   df9da:	6823      	ldr	r3, [r4, #0]
   df9dc:	2b01      	cmp	r3, #1
   df9de:	d102      	bne.n	df9e6 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x62>
      output_shape->size == 1 && output_shape->data[0] == 0) {
   df9e0:	6863      	ldr	r3, [r4, #4]
   df9e2:	2b00      	cmp	r3, #0
   df9e4:	d04c      	beq.n	dfa80 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xfc>
    output_shape->size = 0;
  }

  int num_output_elements = 1;
  int stretch_dim = -1;
  for (int i = 0; i < output_shape->size; ++i) {
   df9e6:	f8d4 9000 	ldr.w	r9, [r4]
   df9ea:	46a0      	mov	r8, r4
   df9ec:	f04f 31ff 	mov.w	r1, #4294967295	; 0xffffffff
   df9f0:	2301      	movs	r3, #1
   df9f2:	f04f 0e00 	mov.w	lr, #0
   df9f6:	45ce      	cmp	lr, r9
   df9f8:	da18      	bge.n	dfa2c <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xa8>
    int value = output_shape->data[i];
   df9fa:	f858 cf04 	ldr.w	ip, [r8, #4]!
    if (value == -1) {
   df9fe:	f1bc 3fff 	cmp.w	ip, #4294967295	; 0xffffffff
   dfa02:	d10c      	bne.n	dfa1e <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x9a>
      TF_LITE_ENSURE_EQ(context, stretch_dim, -1);
   dfa04:	f1b1 3fff 	cmp.w	r1, #4294967295	; 0xffffffff
   dfa08:	d00c      	beq.n	dfa24 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xa0>
   dfa0a:	4b20      	ldr	r3, [pc, #128]	; (dfa8c <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x108>)
   dfa0c:	9301      	str	r3, [sp, #4]
   dfa0e:	4b20      	ldr	r3, [pc, #128]	; (dfa90 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x10c>)
   dfa10:	9300      	str	r3, [sp, #0]
   dfa12:	f8cd c00c 	str.w	ip, [sp, #12]
   dfa16:	9102      	str	r1, [sp, #8]
   dfa18:	6944      	ldr	r4, [r0, #20]
   dfa1a:	2336      	movs	r3, #54	; 0x36
   dfa1c:	e029      	b.n	dfa72 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xee>
      stretch_dim = i;
    } else {
      num_output_elements *= value;
   dfa1e:	fb0c f303 	mul.w	r3, ip, r3
   dfa22:	e000      	b.n	dfa26 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xa2>
   dfa24:	4671      	mov	r1, lr
    output_shape->size = 0;
  }

  int num_output_elements = 1;
  int stretch_dim = -1;
  for (int i = 0; i < output_shape->size; ++i) {
   dfa26:	f10e 0e01 	add.w	lr, lr, #1
   dfa2a:	e7e4      	b.n	df9f6 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x72>
      stretch_dim = i;
    } else {
      num_output_elements *= value;
    }
  }
  if (stretch_dim != -1) {
   dfa2c:	f1b1 3fff 	cmp.w	r1, #4294967295	; 0xffffffff
    output_shape->data[stretch_dim] = num_input_elements / num_output_elements;
   dfa30:	bf1e      	ittt	ne
   dfa32:	eb04 0181 	addne.w	r1, r4, r1, lsl #2
   dfa36:	fb92 fef3 	sdivne	lr, r2, r3
   dfa3a:	f8c1 e004 	strne.w	lr, [r1, #4]
    num_output_elements *= output_shape->data[stretch_dim];
  }

  TF_LITE_ENSURE_EQ(context, input->type, output->type);
   dfa3e:	5df1      	ldrb	r1, [r6, r7]
   dfa40:	5d74      	ldrb	r4, [r6, r5]
      num_output_elements *= value;
    }
  }
  if (stretch_dim != -1) {
    output_shape->data[stretch_dim] = num_input_elements / num_output_elements;
    num_output_elements *= output_shape->data[stretch_dim];
   dfa42:	bf18      	it	ne
   dfa44:	fb0e f303 	mulne.w	r3, lr, r3
  }

  TF_LITE_ENSURE_EQ(context, input->type, output->type);
   dfa48:	42a1      	cmp	r1, r4
   dfa4a:	d008      	beq.n	dfa5e <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xda>
   dfa4c:	4b11      	ldr	r3, [pc, #68]	; (dfa94 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x110>)
   dfa4e:	9301      	str	r3, [sp, #4]
   dfa50:	4b11      	ldr	r3, [pc, #68]	; (dfa98 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x114>)
   dfa52:	9300      	str	r3, [sp, #0]
   dfa54:	9403      	str	r4, [sp, #12]
   dfa56:	9102      	str	r1, [sp, #8]
   dfa58:	6944      	ldr	r4, [r0, #20]
   dfa5a:	2341      	movs	r3, #65	; 0x41
   dfa5c:	e009      	b.n	dfa72 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xee>
  TF_LITE_ENSURE_EQ(context, num_input_elements, num_output_elements);
   dfa5e:	429a      	cmp	r2, r3
   dfa60:	d00c      	beq.n	dfa7c <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0xf8>
   dfa62:	9303      	str	r3, [sp, #12]
   dfa64:	4b0d      	ldr	r3, [pc, #52]	; (dfa9c <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x118>)
   dfa66:	9301      	str	r3, [sp, #4]
   dfa68:	4b0d      	ldr	r3, [pc, #52]	; (dfaa0 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x11c>)
   dfa6a:	9300      	str	r3, [sp, #0]
   dfa6c:	9202      	str	r2, [sp, #8]
   dfa6e:	6944      	ldr	r4, [r0, #20]
   dfa70:	2342      	movs	r3, #66	; 0x42
   dfa72:	4a0c      	ldr	r2, [pc, #48]	; (dfaa4 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x120>)
   dfa74:	490c      	ldr	r1, [pc, #48]	; (dfaa8 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x124>)
   dfa76:	47a0      	blx	r4
   dfa78:	2001      	movs	r0, #1
   dfa7a:	e003      	b.n	dfa84 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x100>
  return kTfLiteOk;
   dfa7c:	2000      	movs	r0, #0
   dfa7e:	e001      	b.n	dfa84 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x100>
  if (NumInputs(node) == 1 &&  // Legacy scalar supported with params.
      output_shape->size == 1 && output_shape->data[0] == 0) {
    // Legacy tflite models use a shape parameter of [0] to indicate scalars,
    // so adjust accordingly. TODO(b/111614235): Allow zero-sized buffers during
    // toco conversion.
    output_shape->size = 0;
   dfa80:	6023      	str	r3, [r4, #0]
   dfa82:	e7b0      	b.n	df9e6 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode+0x62>
  }

  TF_LITE_ENSURE_EQ(context, input->type, output->type);
  TF_LITE_ENSURE_EQ(context, num_input_elements, num_output_elements);
  return kTfLiteOk;
}
   dfa84:	b005      	add	sp, #20
   dfa86:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dfa8a:	bf00      	nop
   dfa8c:	000ea27a 	.word	0x000ea27a
   dfa90:	000ea27d 	.word	0x000ea27d
   dfa94:	000e990f 	.word	0x000e990f
   dfa98:	000e9903 	.word	0x000e9903
   dfa9c:	000ea289 	.word	0x000ea289
   dfaa0:	000ea29d 	.word	0x000ea29d
   dfaa4:	000ea1a4 	.word	0x000ea1a4
   dfaa8:	000e98c8 	.word	0x000e98c8

000dfaac <_ZN6tflite3ops5micro7reshape4EvalEP13TfLiteContextP10TfLiteNode>:
  TF_LITE_ENSURE(context, NumInputs(node) == 1 || NumInputs(node) == 2);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dfaac:	b570      	push	{r4, r5, r6, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dfaae:	680a      	ldr	r2, [r1, #0]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dfab0:	684b      	ldr	r3, [r1, #4]
   dfab2:	6886      	ldr	r6, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dfab4:	6854      	ldr	r4, [r2, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dfab6:	685d      	ldr	r5, [r3, #4]
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  if (ReshapeOutput(context, node) != kTfLiteOk) {
   dfab8:	f7ff ff64 	bl	df984 <_ZN6tflite3ops5micro7reshape13ReshapeOutputEP13TfLiteContextP10TfLiteNode>
   dfabc:	b970      	cbnz	r0, dfadc <_ZN6tflite3ops5micro7reshape4EvalEP13TfLiteContextP10TfLiteNode+0x30>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dfabe:	2338      	movs	r3, #56	; 0x38
   dfac0:	fb03 6204 	mla	r2, r3, r4, r6
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dfac4:	fb03 6505 	mla	r5, r3, r5, r6
   dfac8:	4603      	mov	r3, r0
    return kTfLiteError;
  }

  for (int i = 0; i < input->bytes; ++i) {
   dfaca:	6991      	ldr	r1, [r2, #24]
   dfacc:	4299      	cmp	r1, r3
   dface:	d906      	bls.n	dfade <_ZN6tflite3ops5micro7reshape4EvalEP13TfLiteContextP10TfLiteNode+0x32>
    output->data.raw[i] = input->data.raw[i];
   dfad0:	6851      	ldr	r1, [r2, #4]
   dfad2:	5ccc      	ldrb	r4, [r1, r3]
   dfad4:	6869      	ldr	r1, [r5, #4]
   dfad6:	54cc      	strb	r4, [r1, r3]
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  if (ReshapeOutput(context, node) != kTfLiteOk) {
    return kTfLiteError;
  }

  for (int i = 0; i < input->bytes; ++i) {
   dfad8:	3301      	adds	r3, #1
   dfada:	e7f6      	b.n	dfaca <_ZN6tflite3ops5micro7reshape4EvalEP13TfLiteContextP10TfLiteNode+0x1e>

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  if (ReshapeOutput(context, node) != kTfLiteOk) {
    return kTfLiteError;
   dfadc:	2001      	movs	r0, #1

  for (int i = 0; i < input->bytes; ++i) {
    output->data.raw[i] = input->data.raw[i];
  }
  return kTfLiteOk;
}
   dfade:	bd70      	pop	{r4, r5, r6, pc}

000dfae0 <_ZN6tflite3ops5micro16Register_RESHAPEEv>:

TfLiteRegistration* Register_RESHAPE() {
  static TfLiteRegistration r = {nullptr, nullptr, reshape::Prepare,
                                 reshape::Eval};
  return &r;
}
   dfae0:	4800      	ldr	r0, [pc, #0]	; (dfae4 <_ZN6tflite3ops5micro16Register_RESHAPEEv+0x4>)
   dfae2:	4770      	bx	lr
   dfae4:	2003c29c 	.word	0x2003c29c

000dfae8 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode>:
namespace round {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   dfae8:	b5f0      	push	{r4, r5, r6, r7, lr}
   dfaea:	680b      	ldr	r3, [r1, #0]
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
   dfaec:	681e      	ldr	r6, [r3, #0]
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   dfaee:	2e01      	cmp	r6, #1
namespace round {

constexpr int kInputTensor = 0;
constexpr int kOutputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   dfaf0:	b085      	sub	sp, #20
   dfaf2:	4605      	mov	r5, r0
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
   dfaf4:	d009      	beq.n	dfb0a <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x22>
   dfaf6:	4b3b      	ldr	r3, [pc, #236]	; (dfbe4 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xfc>)
   dfaf8:	9301      	str	r3, [sp, #4]
   dfafa:	2401      	movs	r4, #1
   dfafc:	4b3a      	ldr	r3, [pc, #232]	; (dfbe8 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x100>)
   dfafe:	9300      	str	r3, [sp, #0]
   dfb00:	9403      	str	r4, [sp, #12]
   dfb02:	9602      	str	r6, [sp, #8]
   dfb04:	6945      	ldr	r5, [r0, #20]
   dfb06:	2321      	movs	r3, #33	; 0x21
   dfb08:	e01e      	b.n	dfb48 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
   dfb0a:	f8d1 e004 	ldr.w	lr, [r1, #4]
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
   dfb0e:	f8de 4000 	ldr.w	r4, [lr]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
   dfb12:	2c01      	cmp	r4, #1
   dfb14:	d008      	beq.n	dfb28 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x40>
   dfb16:	4b33      	ldr	r3, [pc, #204]	; (dfbe4 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xfc>)
   dfb18:	9301      	str	r3, [sp, #4]
   dfb1a:	4b34      	ldr	r3, [pc, #208]	; (dfbec <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x104>)
   dfb1c:	9300      	str	r3, [sp, #0]
   dfb1e:	9603      	str	r6, [sp, #12]
   dfb20:	9402      	str	r4, [sp, #8]
   dfb22:	6944      	ldr	r4, [r0, #20]
   dfb24:	2322      	movs	r3, #34	; 0x22
   dfb26:	e022      	b.n	dfb6e <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x86>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dfb28:	6859      	ldr	r1, [r3, #4]
   dfb2a:	6882      	ldr	r2, [r0, #8]
   dfb2c:	2338      	movs	r3, #56	; 0x38
   dfb2e:	4359      	muls	r1, r3
   dfb30:	1857      	adds	r7, r2, r1
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
   dfb32:	5c56      	ldrb	r6, [r2, r1]
   dfb34:	2e01      	cmp	r6, #1
   dfb36:	d00b      	beq.n	dfb50 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x68>
   dfb38:	4b2d      	ldr	r3, [pc, #180]	; (dfbf0 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x108>)
   dfb3a:	9301      	str	r3, [sp, #4]
   dfb3c:	4b2d      	ldr	r3, [pc, #180]	; (dfbf4 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x10c>)
   dfb3e:	9300      	str	r3, [sp, #0]
   dfb40:	9403      	str	r4, [sp, #12]
   dfb42:	9602      	str	r6, [sp, #8]
   dfb44:	6945      	ldr	r5, [r0, #20]
   dfb46:	2323      	movs	r3, #35	; 0x23
   dfb48:	4a2b      	ldr	r2, [pc, #172]	; (dfbf8 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x110>)
   dfb4a:	492c      	ldr	r1, [pc, #176]	; (dfbfc <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x114>)
   dfb4c:	47a8      	blx	r5
   dfb4e:	e042      	b.n	dfbd6 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xee>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dfb50:	f8de 1004 	ldr.w	r1, [lr, #4]
   dfb54:	434b      	muls	r3, r1
   dfb56:	18d1      	adds	r1, r2, r3
  TF_LITE_ENSURE_EQ(context, output->type, input->type);
   dfb58:	5cd4      	ldrb	r4, [r2, r3]
   dfb5a:	2c01      	cmp	r4, #1
   dfb5c:	d00a      	beq.n	dfb74 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x8c>
   dfb5e:	4b25      	ldr	r3, [pc, #148]	; (dfbf4 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x10c>)
   dfb60:	9301      	str	r3, [sp, #4]
   dfb62:	4b27      	ldr	r3, [pc, #156]	; (dfc00 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x118>)
   dfb64:	9300      	str	r3, [sp, #0]
   dfb66:	9603      	str	r6, [sp, #12]
   dfb68:	9402      	str	r4, [sp, #8]
   dfb6a:	6944      	ldr	r4, [r0, #20]
   dfb6c:	2324      	movs	r3, #36	; 0x24
   dfb6e:	4a22      	ldr	r2, [pc, #136]	; (dfbf8 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x110>)
   dfb70:	4922      	ldr	r1, [pc, #136]	; (dfbfc <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x114>)
   dfb72:	e02f      	b.n	dfbd4 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xec>
  TF_LITE_ENSURE_EQ(context, output->bytes, input->bytes);
   dfb74:	698b      	ldr	r3, [r1, #24]
   dfb76:	69ba      	ldr	r2, [r7, #24]
   dfb78:	4293      	cmp	r3, r2
   dfb7a:	d008      	beq.n	dfb8e <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xa6>
   dfb7c:	9302      	str	r3, [sp, #8]
   dfb7e:	4b21      	ldr	r3, [pc, #132]	; (dfc04 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x11c>)
   dfb80:	9301      	str	r3, [sp, #4]
   dfb82:	4b21      	ldr	r3, [pc, #132]	; (dfc08 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x120>)
   dfb84:	9300      	str	r3, [sp, #0]
   dfb86:	9203      	str	r2, [sp, #12]
   dfb88:	6945      	ldr	r5, [r0, #20]
   dfb8a:	2325      	movs	r3, #37	; 0x25
   dfb8c:	e7dc      	b.n	dfb48 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
  TF_LITE_ENSURE_EQ(context, output->dims->size, input->dims->size);
   dfb8e:	688b      	ldr	r3, [r1, #8]
   dfb90:	68ba      	ldr	r2, [r7, #8]
   dfb92:	681e      	ldr	r6, [r3, #0]
   dfb94:	6811      	ldr	r1, [r2, #0]
   dfb96:	428e      	cmp	r6, r1
   dfb98:	d008      	beq.n	dfbac <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xc4>
   dfb9a:	4b1c      	ldr	r3, [pc, #112]	; (dfc0c <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x124>)
   dfb9c:	9301      	str	r3, [sp, #4]
   dfb9e:	4b1c      	ldr	r3, [pc, #112]	; (dfc10 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x128>)
   dfba0:	9300      	str	r3, [sp, #0]
   dfba2:	9103      	str	r1, [sp, #12]
   dfba4:	9602      	str	r6, [sp, #8]
   dfba6:	6945      	ldr	r5, [r0, #20]
   dfba8:	2326      	movs	r3, #38	; 0x26
   dfbaa:	e7cd      	b.n	dfb48 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
   dfbac:	2100      	movs	r1, #0
  for (int i = 0; i < output->dims->size; ++i) {
   dfbae:	42b1      	cmp	r1, r6
   dfbb0:	da15      	bge.n	dfbde <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xf6>
    TF_LITE_ENSURE_EQ(context, output->dims->data[i], input->dims->data[i]);
   dfbb2:	f853 0f04 	ldr.w	r0, [r3, #4]!
   dfbb6:	f852 4f04 	ldr.w	r4, [r2, #4]!
   dfbba:	42a0      	cmp	r0, r4
   dfbbc:	d00d      	beq.n	dfbda <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xf2>
   dfbbe:	4b15      	ldr	r3, [pc, #84]	; (dfc14 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x12c>)
   dfbc0:	9301      	str	r3, [sp, #4]
   dfbc2:	4b15      	ldr	r3, [pc, #84]	; (dfc18 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x130>)
   dfbc4:	9002      	str	r0, [sp, #8]
   dfbc6:	9300      	str	r3, [sp, #0]
   dfbc8:	9403      	str	r4, [sp, #12]
   dfbca:	696c      	ldr	r4, [r5, #20]
   dfbcc:	4a0a      	ldr	r2, [pc, #40]	; (dfbf8 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x110>)
   dfbce:	490b      	ldr	r1, [pc, #44]	; (dfbfc <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0x114>)
   dfbd0:	2328      	movs	r3, #40	; 0x28
   dfbd2:	4628      	mov	r0, r5
   dfbd4:	47a0      	blx	r4
   dfbd6:	2001      	movs	r0, #1
   dfbd8:	e002      	b.n	dfbe0 <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xf8>
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
  TF_LITE_ENSURE_EQ(context, output->type, input->type);
  TF_LITE_ENSURE_EQ(context, output->bytes, input->bytes);
  TF_LITE_ENSURE_EQ(context, output->dims->size, input->dims->size);
  for (int i = 0; i < output->dims->size; ++i) {
   dfbda:	3101      	adds	r1, #1
   dfbdc:	e7e7      	b.n	dfbae <_ZN6tflite3ops5micro5round7PrepareEP13TfLiteContextP10TfLiteNode+0xc6>
    TF_LITE_ENSURE_EQ(context, output->dims->data[i], input->dims->data[i]);
  }
  return kTfLiteOk;
   dfbde:	2000      	movs	r0, #0
}
   dfbe0:	b005      	add	sp, #20
   dfbe2:	bdf0      	pop	{r4, r5, r6, r7, pc}
   dfbe4:	000eb295 	.word	0x000eb295
   dfbe8:	000e98e2 	.word	0x000e98e2
   dfbec:	000e98f2 	.word	0x000e98f2
   dfbf0:	000ea137 	.word	0x000ea137
   dfbf4:	000e9903 	.word	0x000e9903
   dfbf8:	000ea2b0 	.word	0x000ea2b0
   dfbfc:	000e98c8 	.word	0x000e98c8
   dfc00:	000e990f 	.word	0x000e990f
   dfc04:	000e991c 	.word	0x000e991c
   dfc08:	000e9929 	.word	0x000e9929
   dfc0c:	000e9937 	.word	0x000e9937
   dfc10:	000e9949 	.word	0x000e9949
   dfc14:	000e995c 	.word	0x000e995c
   dfc18:	000e9971 	.word	0x000e9971

000dfc1c <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf>:
    return floor_val = floor_val + 1.0f;
  }
}

inline void Round(const RuntimeShape& input_shape, const float* input_data,
                  const RuntimeShape& output_shape, float* output_data) {
   dfc1c:	e92d 47f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, lr}
   dfc20:	ed2d 8b04 	vpush	{d8-d9}
   dfc24:	461e      	mov	r6, r3
   dfc26:	f8d0 8000 	ldr.w	r8, [r0]

// Flat size calculation, checking that dimensions match with one or more other
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
   dfc2a:	6813      	ldr	r3, [r2, #0]
   dfc2c:	4598      	cmp	r8, r3
   dfc2e:	4604      	mov	r4, r0
   dfc30:	460f      	mov	r7, r1
   dfc32:	4691      	mov	r9, r2
   dfc34:	d101      	bne.n	dfc3a <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x1e>
   dfc36:	2500      	movs	r5, #0
   dfc38:	e00d      	b.n	dfc56 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x3a>
   dfc3a:	f004 fb87 	bl	e434c <abort>
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   dfc3e:	4629      	mov	r1, r5
   dfc40:	4620      	mov	r0, r4
   dfc42:	f7f6 fc11 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dfc46:	4629      	mov	r1, r5
   dfc48:	4682      	mov	sl, r0
   dfc4a:	4648      	mov	r0, r9
   dfc4c:	f7f6 fc0c 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dfc50:	4582      	cmp	sl, r0
   dfc52:	d1f2      	bne.n	dfc3a <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x1e>
// arrays.
inline int MatchingFlatSize(const RuntimeShape& shape,
                            const RuntimeShape& check_shape_0) {
  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   dfc54:	3501      	adds	r5, #1
   dfc56:	45a8      	cmp	r8, r5
   dfc58:	dcf1      	bgt.n	dfc3e <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x22>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   dfc5a:	f1b8 0f04 	cmp.w	r8, #4
   dfc5e:	bfcc      	ite	gt
   dfc60:	6864      	ldrgt	r4, [r4, #4]
   dfc62:	3404      	addle	r4, #4
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dfc64:	2300      	movs	r3, #0
  }

  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
   dfc66:	f04f 0901 	mov.w	r9, #1
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dfc6a:	4598      	cmp	r8, r3
   dfc6c:	dd05      	ble.n	dfc7a <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x5e>
      buffer_size *= dims_data[i];
   dfc6e:	f854 2023 	ldr.w	r2, [r4, r3, lsl #2]
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dfc72:	3301      	adds	r3, #1
      buffer_size *= dims_data[i];
   dfc74:	fb02 f909 	mul.w	r9, r2, r9
   dfc78:	e7f7      	b.n	dfc6a <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x4e>
   dfc7a:	4634      	mov	r4, r6
   dfc7c:	463d      	mov	r5, r7
  // Returns the total count of elements, that is the size when flattened into a
  // vector.
  inline int FlatSize() const {
    int buffer_size = 1;
    const int* dims_data = reinterpret_cast<const int*>(DimsData());
    for (int i = 0; i < size_; i++) {
   dfc7e:	2600      	movs	r6, #0
namespace reference_ops {

inline float RoundToNearest(float value) {
  auto floor_val = std::floor(value);
  auto diff = value - floor_val;
  if ((diff < 0.5f) ||
   dfc80:	eef6 8a00 	vmov.f32	s17, #96	; 0x3f000000  0.5
      ((diff == 0.5f) && (static_cast<int>(floor_val) % 2 == 0))) {
    return floor_val;
  } else {
    return floor_val = floor_val + 1.0f;
   dfc84:	eeb7 9a00 	vmov.f32	s18, #112	; 0x3f800000  1.0
}

inline void Round(const RuntimeShape& input_shape, const float* input_data,
                  const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
  for (int i = 0; i < flat_size; ++i) {
   dfc88:	454e      	cmp	r6, r9
   dfc8a:	da1d      	bge.n	dfcc8 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0xac>
    // Note that this implementation matches that of tensorFlow tf.round
    // and corresponds to the bankers rounding method.
    // cfenv (for fesetround) is not yet supported universally on Android, so
    // using a work around.
    output_data[i] = RoundToNearest(input_data[i]);
   dfc8c:	ecb5 8a01 	vldmia	r5!, {s16}
  using ::floor;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  floor(float __x)
  { return __builtin_floorf(__x); }
   dfc90:	eeb0 0a48 	vmov.f32	s0, s16
   dfc94:	f005 fbac 	bl	e53f0 <floorf>

namespace reference_ops {

inline float RoundToNearest(float value) {
  auto floor_val = std::floor(value);
  auto diff = value - floor_val;
   dfc98:	ee38 8a40 	vsub.f32	s16, s16, s0
  if ((diff < 0.5f) ||
   dfc9c:	eeb4 8ae8 	vcmpe.f32	s16, s17
   dfca0:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   dfca4:	d40c      	bmi.n	dfcc0 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0xa4>
   dfca6:	eeb4 8a68 	vcmp.f32	s16, s17
   dfcaa:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   dfcae:	d105      	bne.n	dfcbc <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0xa0>
      ((diff == 0.5f) && (static_cast<int>(floor_val) % 2 == 0))) {
   dfcb0:	eefd 7ac0 	vcvt.s32.f32	s15, s0
   dfcb4:	ee17 3a90 	vmov	r3, s15
   dfcb8:	07db      	lsls	r3, r3, #31
   dfcba:	d501      	bpl.n	dfcc0 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0xa4>
    return floor_val;
  } else {
    return floor_val = floor_val + 1.0f;
   dfcbc:	ee30 0a09 	vadd.f32	s0, s0, s18
  for (int i = 0; i < flat_size; ++i) {
    // Note that this implementation matches that of tensorFlow tf.round
    // and corresponds to the bankers rounding method.
    // cfenv (for fesetround) is not yet supported universally on Android, so
    // using a work around.
    output_data[i] = RoundToNearest(input_data[i]);
   dfcc0:	eca4 0a01 	vstmia	r4!, {s0}
}

inline void Round(const RuntimeShape& input_shape, const float* input_data,
                  const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
  for (int i = 0; i < flat_size; ++i) {
   dfcc4:	3601      	adds	r6, #1
   dfcc6:	e7df      	b.n	dfc88 <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf+0x6c>
    // and corresponds to the bankers rounding method.
    // cfenv (for fesetround) is not yet supported universally on Android, so
    // using a work around.
    output_data[i] = RoundToNearest(input_data[i]);
  }
}
   dfcc8:	ecbd 8b04 	vpop	{d8-d9}
   dfccc:	e8bd 87f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, pc}

000dfcd0 <_ZN6tflite3ops5micro5round4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   dfcd0:	b530      	push	{r4, r5, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dfcd2:	680a      	ldr	r2, [r1, #0]
   dfcd4:	6883      	ldr	r3, [r0, #8]
   dfcd6:	6854      	ldr	r4, [r2, #4]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dfcd8:	684a      	ldr	r2, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dfcda:	2538      	movs	r5, #56	; 0x38
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dfcdc:	6852      	ldr	r2, [r2, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   dfcde:	fb05 3404 	mla	r4, r5, r4, r3
   dfce2:	b08b      	sub	sp, #44	; 0x2c
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   dfce4:	fb05 3502 	mla	r5, r5, r2, r3
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
}

inline RuntimeShape GetTensorShape(const TfLiteTensor* tensor) {
  if (tensor == nullptr) {
   dfce8:	b90c      	cbnz	r4, dfcee <_ZN6tflite3ops5micro5round4EvalEP13TfLiteContextP10TfLiteNode+0x1e>
  // larger shapes are separately allocated.
  static constexpr int kMaxSmallSize = 4;

  RuntimeShape& operator=(RuntimeShape const&) = delete;

  RuntimeShape() : size_(0) {}
   dfcea:	9400      	str	r4, [sp, #0]
   dfcec:	e008      	b.n	dfd00 <_ZN6tflite3ops5micro5round4EvalEP13TfLiteContextP10TfLiteNode+0x30>
    return RuntimeShape();
  }

  TfLiteIntArray* dims = tensor->dims;
  const int dims_size = dims->size;
   dfcee:	68a2      	ldr	r2, [r4, #8]
    for (int i = 0; i < shape_size; ++i) {
      SetDim(i, value);
    }
  }

  RuntimeShape(int dimensions_count, const int32* dims_data) : size_(0) {
   dfcf0:	2300      	movs	r3, #0
   dfcf2:	f852 1b04 	ldr.w	r1, [r2], #4
   dfcf6:	9300      	str	r3, [sp, #0]
    ReplaceWith(dimensions_count, dims_data);
   dfcf8:	4668      	mov	r0, sp
   dfcfa:	f7f8 f849 	bl	d7d90 <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   dfcfe:	6864      	ldr	r4, [r4, #4]
}

inline RuntimeShape GetTensorShape(const TfLiteTensor* tensor) {
  if (tensor == nullptr) {
   dfd00:	b915      	cbnz	r5, dfd08 <_ZN6tflite3ops5micro5round4EvalEP13TfLiteContextP10TfLiteNode+0x38>
  // larger shapes are separately allocated.
  static constexpr int kMaxSmallSize = 4;

  RuntimeShape& operator=(RuntimeShape const&) = delete;

  RuntimeShape() : size_(0) {}
   dfd02:	9505      	str	r5, [sp, #20]

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dfd04:	462b      	mov	r3, r5
   dfd06:	e008      	b.n	dfd1a <_ZN6tflite3ops5micro5round4EvalEP13TfLiteContextP10TfLiteNode+0x4a>
  if (tensor == nullptr) {
    return RuntimeShape();
  }

  TfLiteIntArray* dims = tensor->dims;
  const int dims_size = dims->size;
   dfd08:	68aa      	ldr	r2, [r5, #8]
    for (int i = 0; i < shape_size; ++i) {
      SetDim(i, value);
    }
  }

  RuntimeShape(int dimensions_count, const int32* dims_data) : size_(0) {
   dfd0a:	2300      	movs	r3, #0
   dfd0c:	f852 1b04 	ldr.w	r1, [r2], #4
   dfd10:	9305      	str	r3, [sp, #20]
    ReplaceWith(dimensions_count, dims_data);
   dfd12:	a805      	add	r0, sp, #20
   dfd14:	f7f8 f83c 	bl	d7d90 <_ZN6tflite12RuntimeShape11ReplaceWithEiPKl>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   dfd18:	686b      	ldr	r3, [r5, #4]
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  reference_ops::Round(GetTensorShape(input), GetTensorData<float>(input),
                       GetTensorShape(output), GetTensorData<float>(output));
   dfd1a:	aa05      	add	r2, sp, #20
   dfd1c:	4621      	mov	r1, r4
   dfd1e:	4668      	mov	r0, sp
   dfd20:	f7ff ff7c 	bl	dfc1c <_ZN6tflite13reference_ops5RoundERKNS_12RuntimeShapeEPKfS3_Pf>
   dfd24:	a805      	add	r0, sp, #20
   dfd26:	f7f6 fb94 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  reference_ops::Round(GetTensorShape(input), GetTensorData<float>(input),
   dfd2a:	4668      	mov	r0, sp
   dfd2c:	f7f6 fb91 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                       GetTensorShape(output), GetTensorData<float>(output));

  return kTfLiteOk;
}
   dfd30:	2000      	movs	r0, #0
   dfd32:	b00b      	add	sp, #44	; 0x2c
   dfd34:	bd30      	pop	{r4, r5, pc}
	...

000dfd38 <_ZN6tflite3ops5micro14Register_ROUNDEv>:

TfLiteRegistration* Register_ROUND() {
  static TfLiteRegistration r = {/*init=*/nullptr,
                                 /*free=*/nullptr, round::Prepare, round::Eval};
  return &r;
}
   dfd38:	4800      	ldr	r0, [pc, #0]	; (dfd3c <_ZN6tflite3ops5micro14Register_ROUNDEv+0x4>)
   dfd3a:	4770      	bx	lr
   dfd3c:	2003c2bc 	.word	0x2003c2bc

000dfd40 <_ZN6tflite3ops5micro11activations4InitEP13TfLiteContextPKcj>:

}  // namespace

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
   dfd40:	2000      	movs	r0, #0
   dfd42:	4770      	bx	lr

000dfd44 <_ZN6tflite3ops5micro11activations4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
   dfd44:	4770      	bx	lr

000dfd46 <_ZN6tflite3ops5micro11activations14SoftmaxPrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus SoftmaxPrepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   dfd46:	2000      	movs	r0, #0
   dfd48:	4770      	bx	lr
	...

000dfd4c <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>:
// This function implements the same computation as the ARMv7 NEON VQRDMULH
// instruction.
template <>
inline std::int32_t SaturatingRoundingDoublingHighMul(std::int32_t a,
                                                      std::int32_t b) {
  bool overflow = a == b && a == std::numeric_limits<std::int32_t>::min();
   dfd4c:	4288      	cmp	r0, r1

// This function implements the same computation as the ARMv7 NEON VQRDMULH
// instruction.
template <>
inline std::int32_t SaturatingRoundingDoublingHighMul(std::int32_t a,
                                                      std::int32_t b) {
   dfd4e:	b510      	push	{r4, lr}
  bool overflow = a == b && a == std::numeric_limits<std::int32_t>::min();
   dfd50:	d104      	bne.n	dfd5c <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_+0x10>
   dfd52:	f100 4300 	add.w	r3, r0, #2147483648	; 0x80000000
   dfd56:	425c      	negs	r4, r3
   dfd58:	415c      	adcs	r4, r3
   dfd5a:	e000      	b.n	dfd5e <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_+0x12>
   dfd5c:	2400      	movs	r4, #0
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
   dfd5e:	fb80 2301 	smull	r2, r3, r0, r1
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
   dfd62:	2a00      	cmp	r2, #0
   dfd64:	f173 0100 	sbcs.w	r1, r3, #0
   dfd68:	490b      	ldr	r1, [pc, #44]	; (dfd98 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_+0x4c>)
   dfd6a:	bfa8      	it	ge
   dfd6c:	f04f 4180 	movge.w	r1, #1073741824	; 0x40000000
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
   dfd70:	b97c      	cbnz	r4, dfd92 <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_+0x46>
  std::int64_t a_64(a);
  std::int64_t b_64(b);
  std::int64_t ab_64 = a_64 * b_64;
  std::int32_t nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
  std::int32_t ab_x2_high32 =
      static_cast<std::int32_t>((ab_64 + nudge) / (1ll << 31));
   dfd72:	1852      	adds	r2, r2, r1
   dfd74:	eb43 73e1 	adc.w	r3, r3, r1, asr #31
   dfd78:	2a00      	cmp	r2, #0
   dfd7a:	f173 0100 	sbcs.w	r1, r3, #0
   dfd7e:	da04      	bge.n	dfd8a <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_+0x3e>
   dfd80:	f06f 4000 	mvn.w	r0, #2147483648	; 0x80000000
   dfd84:	2100      	movs	r1, #0
   dfd86:	1812      	adds	r2, r2, r0
   dfd88:	414b      	adcs	r3, r1
   dfd8a:	0fd0      	lsrs	r0, r2, #31
   dfd8c:	ea40 0043 	orr.w	r0, r0, r3, lsl #1
   dfd90:	bd10      	pop	{r4, pc}
  return overflow ? std::numeric_limits<std::int32_t>::max() : ab_x2_high32;
   dfd92:	f06f 4000 	mvn.w	r0, #2147483648	; 0x80000000
}
   dfd96:	bd10      	pop	{r4, pc}
   dfd98:	c0000001 	.word	0xc0000001

000dfd9c <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_>:
  return flat_size;
}

// A combination of MatchingFlatSize() and FlatSizeSkipDim().
inline int MatchingFlatSizeSkipDim(const RuntimeShape& shape, int skip_dim,
                                   const RuntimeShape& check_shape_0) {
   dfd9c:	e92d 43f8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, lr}
   dfda0:	6806      	ldr	r6, [r0, #0]
   dfda2:	4604      	mov	r4, r0
   dfda4:	460f      	mov	r7, r1
   dfda6:	4690      	mov	r8, r2
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   dfda8:	2500      	movs	r5, #0
   dfdaa:	42b5      	cmp	r5, r6
   dfdac:	da10      	bge.n	dfdd0 <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x34>
    if (i != skip_dim) {
   dfdae:	42bd      	cmp	r5, r7
   dfdb0:	d00c      	beq.n	dfdcc <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x30>
      TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
   dfdb2:	4629      	mov	r1, r5
   dfdb4:	4620      	mov	r0, r4
   dfdb6:	f7f6 fb57 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dfdba:	4629      	mov	r1, r5
   dfdbc:	4681      	mov	r9, r0
   dfdbe:	4640      	mov	r0, r8
   dfdc0:	f7f6 fb52 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dfdc4:	4581      	cmp	r9, r0
   dfdc6:	d001      	beq.n	dfdcc <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x30>
   dfdc8:	f004 fac0 	bl	e434c <abort>

// A combination of MatchingFlatSize() and FlatSizeSkipDim().
inline int MatchingFlatSizeSkipDim(const RuntimeShape& shape, int skip_dim,
                                   const RuntimeShape& check_shape_0) {
  const int dims_count = shape.DimensionsCount();
  for (int i = 0; i < dims_count; ++i) {
   dfdcc:	3501      	adds	r5, #1
   dfdce:	e7ec      	b.n	dfdaa <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0xe>
// Data is required to be contiguous, and so many operators can use either the
// full array flat size or the flat size with one dimension skipped (commonly
// the depth).
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
   dfdd0:	2f00      	cmp	r7, #0
   dfdd2:	dbf9      	blt.n	dfdc8 <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x2c>
   dfdd4:	42b7      	cmp	r7, r6
   dfdd6:	daf7      	bge.n	dfdc8 <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x2c>

  inline int32* DimsData() {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
  }
  inline const int32* DimsData() const {
    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;
   dfdd8:	2e04      	cmp	r6, #4
   dfdda:	bfcc      	ite	gt
   dfddc:	6864      	ldrgt	r4, [r4, #4]
   dfdde:	3404      	addle	r4, #4
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
  const auto* dims_data = shape.DimsData();
  int flat_size = 1;
  for (int i = 0; i < dims_count; ++i) {
   dfde0:	2300      	movs	r3, #0
// the depth).
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
  const auto* dims_data = shape.DimsData();
  int flat_size = 1;
   dfde2:	2001      	movs	r0, #1
  for (int i = 0; i < dims_count; ++i) {
   dfde4:	429e      	cmp	r6, r3
   dfde6:	dd07      	ble.n	dfdf8 <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x5c>
    flat_size *= (i == skip_dim) ? 1 : dims_data[i];
   dfde8:	429f      	cmp	r7, r3
   dfdea:	bf14      	ite	ne
   dfdec:	f854 2023 	ldrne.w	r2, [r4, r3, lsl #2]
   dfdf0:	2201      	moveq	r2, #1
   dfdf2:	4350      	muls	r0, r2
inline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {
  const int dims_count = shape.DimensionsCount();
  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);
  const auto* dims_data = shape.DimsData();
  int flat_size = 1;
  for (int i = 0; i < dims_count; ++i) {
   dfdf4:	3301      	adds	r3, #1
   dfdf6:	e7f5      	b.n	dfde4 <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_+0x48>
    if (i != skip_dim) {
      TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));
    }
  }
  return FlatSizeSkipDim(shape, skip_dim);
}
   dfdf8:	e8bd 83f8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, pc}

000dfdfc <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf>:
namespace tflite {
namespace reference_ops {

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const float* input_data,
                    const RuntimeShape& output_shape, float* output_data) {
   dfdfc:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dfe00:	ed2d 8b02 	vpush	{d8}
  const int trailing_dim = input_shape.DimensionsCount() - 1;
   dfe04:	680e      	ldr	r6, [r1, #0]
namespace tflite {
namespace reference_ops {

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const float* input_data,
                    const RuntimeShape& output_shape, float* output_data) {
   dfe06:	b089      	sub	sp, #36	; 0x24
   dfe08:	460d      	mov	r5, r1
  const int trailing_dim = input_shape.DimensionsCount() - 1;
   dfe0a:	3e01      	subs	r6, #1
namespace tflite {
namespace reference_ops {

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const float* input_data,
                    const RuntimeShape& output_shape, float* output_data) {
   dfe0c:	9002      	str	r0, [sp, #8]
  const int trailing_dim = input_shape.DimensionsCount() - 1;
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
   dfe0e:	4631      	mov	r1, r6
   dfe10:	4628      	mov	r0, r5
namespace tflite {
namespace reference_ops {

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const float* input_data,
                    const RuntimeShape& output_shape, float* output_data) {
   dfe12:	4614      	mov	r4, r2
  const int trailing_dim = input_shape.DimensionsCount() - 1;
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
   dfe14:	461a      	mov	r2, r3
namespace tflite {
namespace reference_ops {

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const float* input_data,
                    const RuntimeShape& output_shape, float* output_data) {
   dfe16:	461f      	mov	r7, r3
  const int trailing_dim = input_shape.DimensionsCount() - 1;
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
   dfe18:	f7ff ffc0 	bl	dfd9c <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_>
}

// Get common shape dim, DCHECKing that they all agree.
inline int MatchingDim(const RuntimeShape& shape1, int index1,
                       const RuntimeShape& shape2, int index2) {
  TFLITE_DCHECK_EQ(shape1.Dims(index1), shape2.Dims(index2));
   dfe1c:	4631      	mov	r1, r6
   dfe1e:	9003      	str	r0, [sp, #12]
   dfe20:	4628      	mov	r0, r5
   dfe22:	f7f6 fb21 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dfe26:	4631      	mov	r1, r6
   dfe28:	4605      	mov	r5, r0
   dfe2a:	4638      	mov	r0, r7
   dfe2c:	f7f6 fb1c 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   dfe30:	4285      	cmp	r5, r0
   dfe32:	d001      	beq.n	dfe38 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x3c>
   dfe34:	f004 fa8a 	bl	e434c <abort>
   dfe38:	00ab      	lsls	r3, r5, #2
   dfe3a:	9e14      	ldr	r6, [sp, #80]	; 0x50

  for (int i = 0; i < outer_size; ++i) {
    // Find max element value which we'll use to ensure numerical stability
    // taking advantage of the following equality:
    // exp(x[i])/sum(exp(x[i])) == exp(x[i]+C)/sum(exp(x[i]+C))
    float max = std::numeric_limits<float>::lowest();
   dfe3c:	ed9f 8a3f 	vldr	s16, [pc, #252]	; dff3c <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x140>
   dfe40:	9301      	str	r3, [sp, #4]
   dfe42:	2700      	movs	r7, #0
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
  const int depth =
      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);

  for (int i = 0; i < outer_size; ++i) {
   dfe44:	9b03      	ldr	r3, [sp, #12]
   dfe46:	429f      	cmp	r7, r3
   dfe48:	da72      	bge.n	dff30 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x134>
    // Find max element value which we'll use to ensure numerical stability
    // taking advantage of the following equality:
    // exp(x[i])/sum(exp(x[i])) == exp(x[i]+C)/sum(exp(x[i]+C))
    float max = std::numeric_limits<float>::lowest();
   dfe4a:	ed8d 8a07 	vstr	s16, [sp, #28]
   dfe4e:	4621      	mov	r1, r4
    for (int c = 0; c < depth; ++c) {
   dfe50:	2200      	movs	r2, #0
   dfe52:	42aa      	cmp	r2, r5
   dfe54:	da0f      	bge.n	dfe76 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x7a>
      max = std::max(max, input_data[i * depth + c]);
   dfe56:	460b      	mov	r3, r1
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   dfe58:	ed93 7a00 	vldr	s14, [r3]
   dfe5c:	eddd 7a07 	vldr	s15, [sp, #28]
   dfe60:	eeb4 7ae7 	vcmpe.f32	s14, s15
   dfe64:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
	return __b;
      return __a;
   dfe68:	bfd8      	it	le
   dfe6a:	ab07      	addle	r3, sp, #28
   dfe6c:	3104      	adds	r1, #4
   dfe6e:	681b      	ldr	r3, [r3, #0]
   dfe70:	9307      	str	r3, [sp, #28]
  for (int i = 0; i < outer_size; ++i) {
    // Find max element value which we'll use to ensure numerical stability
    // taking advantage of the following equality:
    // exp(x[i])/sum(exp(x[i])) == exp(x[i]+C)/sum(exp(x[i]+C))
    float max = std::numeric_limits<float>::lowest();
    for (int c = 0; c < depth; ++c) {
   dfe72:	3201      	adds	r2, #1
   dfe74:	e7ed      	b.n	dfe52 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x56>
   dfe76:	46a2      	mov	sl, r4
   dfe78:	f04f 0800 	mov.w	r8, #0
   dfe7c:	f04f 0900 	mov.w	r9, #0
      max = std::max(max, input_data[i * depth + c]);
    }

    // Compute sum.
    float sum = 0.f;
    for (int c = 0; c < depth; ++c) {
   dfe80:	45a8      	cmp	r8, r5
   dfe82:	da23      	bge.n	dfecc <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0xd0>
      sum += std::exp((input_data[i * depth + c] - max) * params.beta);
   dfe84:	ecba 7a01 	vldmia	sl!, {s14}
   dfe88:	eddd 7a07 	vldr	s15, [sp, #28]
   dfe8c:	ee77 7a67 	vsub.f32	s15, s14, s15
      max = std::max(max, input_data[i * depth + c]);
    }

    // Compute sum.
    float sum = 0.f;
    for (int c = 0; c < depth; ++c) {
   dfe90:	f108 0801 	add.w	r8, r8, #1
      sum += std::exp((input_data[i * depth + c] - max) * params.beta);
   dfe94:	ee17 0a90 	vmov	r0, s15
   dfe98:	f007 f898 	bl	e6fcc <__aeabi_f2d>
   dfe9c:	9b02      	ldr	r3, [sp, #8]
   dfe9e:	e9d3 2300 	ldrd	r2, r3, [r3]
   dfea2:	f007 f8e7 	bl	e7074 <__aeabi_dmul>
   dfea6:	ec41 0b10 	vmov	d0, r0, r1
   dfeaa:	f005 fbbd 	bl	e5628 <exp>
   dfeae:	ec53 2b10 	vmov	r2, r3, d0
   dfeb2:	4648      	mov	r0, r9
   dfeb4:	e9cd 2304 	strd	r2, r3, [sp, #16]
   dfeb8:	f007 f888 	bl	e6fcc <__aeabi_f2d>
   dfebc:	e9dd 2304 	ldrd	r2, r3, [sp, #16]
   dfec0:	f006 ff26 	bl	e6d10 <__adddf3>
   dfec4:	f007 fbb8 	bl	e7638 <__aeabi_d2f>
   dfec8:	4681      	mov	r9, r0
      max = std::max(max, input_data[i * depth + c]);
    }

    // Compute sum.
    float sum = 0.f;
    for (int c = 0; c < depth; ++c) {
   dfeca:	e7d9      	b.n	dfe80 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x84>
   dfecc:	46b2      	mov	sl, r6
   dfece:	46a3      	mov	fp, r4
   dfed0:	f04f 0800 	mov.w	r8, #0
      sum += std::exp((input_data[i * depth + c] - max) * params.beta);
    }

    // Compute result.
    for (int c = 0; c < depth; ++c) {
   dfed4:	45a8      	cmp	r8, r5
   dfed6:	da26      	bge.n	dff26 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x12a>
      output_data[i * depth + c] =
          std::exp((input_data[i * depth + c] - max) * params.beta) / sum;
   dfed8:	ecbb 7a01 	vldmia	fp!, {s14}
   dfedc:	eddd 7a07 	vldr	s15, [sp, #28]
   dfee0:	ee77 7a67 	vsub.f32	s15, s14, s15
    for (int c = 0; c < depth; ++c) {
      sum += std::exp((input_data[i * depth + c] - max) * params.beta);
    }

    // Compute result.
    for (int c = 0; c < depth; ++c) {
   dfee4:	f108 0801 	add.w	r8, r8, #1
      output_data[i * depth + c] =
          std::exp((input_data[i * depth + c] - max) * params.beta) / sum;
   dfee8:	ee17 0a90 	vmov	r0, s15
   dfeec:	f007 f86e 	bl	e6fcc <__aeabi_f2d>
   dfef0:	9b02      	ldr	r3, [sp, #8]
   dfef2:	e9d3 2300 	ldrd	r2, r3, [r3]
   dfef6:	f007 f8bd 	bl	e7074 <__aeabi_dmul>
   dfefa:	ec41 0b10 	vmov	d0, r0, r1
   dfefe:	f005 fb93 	bl	e5628 <exp>
   dff02:	4648      	mov	r0, r9
   dff04:	ed8d 0b04 	vstr	d0, [sp, #16]
   dff08:	f007 f860 	bl	e6fcc <__aeabi_f2d>
   dff0c:	ed9d 0b04 	vldr	d0, [sp, #16]
   dff10:	4602      	mov	r2, r0
   dff12:	460b      	mov	r3, r1
   dff14:	ec51 0b10 	vmov	r0, r1, d0
   dff18:	f007 f9d6 	bl	e72c8 <__aeabi_ddiv>
   dff1c:	f007 fb8c 	bl	e7638 <__aeabi_d2f>
   dff20:	f84a 0b04 	str.w	r0, [sl], #4
    for (int c = 0; c < depth; ++c) {
      sum += std::exp((input_data[i * depth + c] - max) * params.beta);
    }

    // Compute result.
    for (int c = 0; c < depth; ++c) {
   dff24:	e7d6      	b.n	dfed4 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0xd8>
   dff26:	9b01      	ldr	r3, [sp, #4]
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
  const int depth =
      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);

  for (int i = 0; i < outer_size; ++i) {
   dff28:	3701      	adds	r7, #1
   dff2a:	441c      	add	r4, r3
   dff2c:	441e      	add	r6, r3
   dff2e:	e789      	b.n	dfe44 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf+0x48>
    for (int c = 0; c < depth; ++c) {
      output_data[i * depth + c] =
          std::exp((input_data[i * depth + c] - max) * params.beta) / sum;
    }
  }
}
   dff30:	b009      	add	sp, #36	; 0x24
   dff32:	ecbd 8b02 	vpop	{d8}
   dff36:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dff3a:	bf00      	nop
   dff3c:	ff7fffff 	.word	0xff7fffff

000dff40 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf>:
  }
}

// Performs softmax along the input of size (input_size * batch_size).
inline void Softmax(const float* in, const int input_size, const int batch_size,
                    const float beta, float* out) {
   dff40:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   dff44:	ed2d 8b04 	vpush	{d8-d9}
   dff48:	4605      	mov	r5, r0
   dff4a:	b085      	sub	sp, #20
   dff4c:	460e      	mov	r6, r1
   dff4e:	4693      	mov	fp, r2
   dff50:	eeb0 9a40 	vmov.f32	s18, s0
   dff54:	461c      	mov	r4, r3
    for (int i = 0; i < input_size; i++) {
      out[i] *= reciprocal_sum_exp;
    }

    // Advance in and out pointers for the next batch.
    in += input_size;
   dff56:	ea4f 0a81 	mov.w	sl, r1, lsl #2
inline void Softmax(const float* in, const int input_size, const int batch_size,
                    const float beta, float* out) {
  //  TF_LITE_ASSERT(input_size > 0);

  // For each batch
  for (int b = 0; b < batch_size; b++) {
   dff5a:	2700      	movs	r7, #0
      out[i] = std::exp((in[i] - max_coeff) * beta);
      exp_sum += out[i];
    }

    // Divide by the sum of exps.
    float reciprocal_sum_exp = 1.f / exp_sum;
   dff5c:	eef7 9a00 	vmov.f32	s19, #112	; 0x3f800000  1.0
inline void Softmax(const float* in, const int input_size, const int batch_size,
                    const float beta, float* out) {
  //  TF_LITE_ASSERT(input_size > 0);

  // For each batch
  for (int b = 0; b < batch_size; b++) {
   dff60:	455f      	cmp	r7, fp
   dff62:	da3e      	bge.n	dffe2 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0xa2>
    // Find the max coeff.
    float max_coeff = in[0];
   dff64:	462b      	mov	r3, r5
   dff66:	ecb3 8a01 	vldmia	r3!, {s16}
    for (int i = 1; i < input_size; i++) {
   dff6a:	2101      	movs	r1, #1
   dff6c:	42b1      	cmp	r1, r6
   dff6e:	da0a      	bge.n	dff86 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x46>
      if (in[i] > max_coeff) max_coeff = in[i];
   dff70:	ecf3 7a01 	vldmia	r3!, {s15}
   dff74:	eeb4 8a67 	vcmp.f32	s16, s15
   dff78:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   dff7c:	bf48      	it	mi
   dff7e:	eeb0 8a67 	vmovmi.f32	s16, s15

  // For each batch
  for (int b = 0; b < batch_size; b++) {
    // Find the max coeff.
    float max_coeff = in[0];
    for (int i = 1; i < input_size; i++) {
   dff82:	3101      	adds	r1, #1
   dff84:	e7f2      	b.n	dff6c <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x2c>
   dff86:	eddf 8a19 	vldr	s17, [pc, #100]	; dffec <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0xac>
   dff8a:	462a      	mov	r2, r5
   dff8c:	46a0      	mov	r8, r4
   dff8e:	4623      	mov	r3, r4
   dff90:	f04f 0900 	mov.w	r9, #0
      if (in[i] > max_coeff) max_coeff = in[i];
    }

    // Compute the normalized sum of exps.
    float exp_sum = 0.0;
    for (int i = 0; i < input_size; i++) {
   dff94:	45b1      	cmp	r9, r6
   dff96:	9302      	str	r3, [sp, #8]
   dff98:	da12      	bge.n	dffc0 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x80>
      out[i] = std::exp((in[i] - max_coeff) * beta);
   dff9a:	ecb2 0a01 	vldmia	r2!, {s0}
  using ::exp;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  exp(float __x)
  { return __builtin_expf(__x); }
   dff9e:	ee30 0a48 	vsub.f32	s0, s0, s16
   dffa2:	9201      	str	r2, [sp, #4]
   dffa4:	ee20 0a09 	vmul.f32	s0, s0, s18
   dffa8:	9203      	str	r2, [sp, #12]
   dffaa:	f005 fbc1 	bl	e5730 <expf>
   dffae:	9b02      	ldr	r3, [sp, #8]
      if (in[i] > max_coeff) max_coeff = in[i];
    }

    // Compute the normalized sum of exps.
    float exp_sum = 0.0;
    for (int i = 0; i < input_size; i++) {
   dffb0:	9a01      	ldr	r2, [sp, #4]
      out[i] = std::exp((in[i] - max_coeff) * beta);
   dffb2:	eca3 0a01 	vstmia	r3!, {s0}
      exp_sum += out[i];
   dffb6:	ee78 8a80 	vadd.f32	s17, s17, s0
      if (in[i] > max_coeff) max_coeff = in[i];
    }

    // Compute the normalized sum of exps.
    float exp_sum = 0.0;
    for (int i = 0; i < input_size; i++) {
   dffba:	f109 0901 	add.w	r9, r9, #1
   dffbe:	e7e9      	b.n	dff94 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x54>
      out[i] = std::exp((in[i] - max_coeff) * beta);
      exp_sum += out[i];
    }

    // Divide by the sum of exps.
    float reciprocal_sum_exp = 1.f / exp_sum;
   dffc0:	ee89 7aa8 	vdiv.f32	s14, s19, s17
    for (int i = 0; i < input_size; i++) {
   dffc4:	2300      	movs	r3, #0
   dffc6:	42b3      	cmp	r3, r6
   dffc8:	da07      	bge.n	dffda <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x9a>
   dffca:	3301      	adds	r3, #1
      out[i] *= reciprocal_sum_exp;
   dffcc:	edd8 7a00 	vldr	s15, [r8]
   dffd0:	ee67 7a87 	vmul.f32	s15, s15, s14
   dffd4:	ece8 7a01 	vstmia	r8!, {s15}
      exp_sum += out[i];
    }

    // Divide by the sum of exps.
    float reciprocal_sum_exp = 1.f / exp_sum;
    for (int i = 0; i < input_size; i++) {
   dffd8:	e7f5      	b.n	dffc6 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x86>
      out[i] *= reciprocal_sum_exp;
    }

    // Advance in and out pointers for the next batch.
    in += input_size;
   dffda:	4455      	add	r5, sl
    out += input_size;
   dffdc:	4454      	add	r4, sl
inline void Softmax(const float* in, const int input_size, const int batch_size,
                    const float beta, float* out) {
  //  TF_LITE_ASSERT(input_size > 0);

  // For each batch
  for (int b = 0; b < batch_size; b++) {
   dffde:	3701      	adds	r7, #1
   dffe0:	e7be      	b.n	dff60 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf+0x20>

    // Advance in and out pointers for the next batch.
    in += input_size;
    out += input_size;
  }
}
   dffe2:	b005      	add	sp, #20
   dffe4:	ecbd 8b04 	vpop	{d8-d9}
   dffe8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   dffec:	00000000 	.word	0x00000000

000dfff0 <_ZN6tflite3ops5micro11activations14Softmax1DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams>:

// Takes a 1D tensor and performs softmax along it.
void Softmax1DFloat(const TfLiteTensor* input, TfLiteTensor* output,
                    TfLiteSoftmaxParams* params) {
   dfff0:	b510      	push	{r4, lr}
  const int input_size = input->dims->data[0];
   dfff2:	6884      	ldr	r4, [r0, #8]
  tflite::reference_ops::Softmax(input->data.f, input_size, 1, params->beta,
                                 output->data.f);
   dfff4:	684b      	ldr	r3, [r1, #4]
   dfff6:	ed92 0a00 	vldr	s0, [r2]
   dfffa:	6861      	ldr	r1, [r4, #4]
   dfffc:	6840      	ldr	r0, [r0, #4]
   dfffe:	2201      	movs	r2, #1
   e0000:	f7ff ff9e 	bl	dff40 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf>
   e0004:	bd10      	pop	{r4, pc}

000e0006 <_ZN6tflite3ops5micro11activations14Softmax2DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams>:
}

// Takes a 2D tensor and perform softmax along the last dimension.
void Softmax2DFloat(const TfLiteTensor* input, TfLiteTensor* output,
                    TfLiteSoftmaxParams* params) {
   e0006:	b510      	push	{r4, lr}
  const int batch_size = input->dims->data[0];
   e0008:	6884      	ldr	r4, [r0, #8]
  const int input_size = input->dims->data[1];
  tflite::reference_ops::Softmax(input->data.f, input_size, batch_size,
                                 params->beta, output->data.f);
   e000a:	684b      	ldr	r3, [r1, #4]
   e000c:	ed92 0a00 	vldr	s0, [r2]
   e0010:	68a1      	ldr	r1, [r4, #8]
   e0012:	6862      	ldr	r2, [r4, #4]
   e0014:	6840      	ldr	r0, [r0, #4]
   e0016:	f7ff ff93 	bl	dff40 <_ZN6tflite13reference_ops7SoftmaxEPKfiifPf>
   e001a:	bd10      	pop	{r4, pc}

000e001c <_ZN6tflite3ops5micro11activations14Softmax4DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams>:
                                 GetTensorData<uint8_t>(output));
}

// Takes a 4D tensor and perform softmax along the forth dimension.
void Softmax4DFloat(const TfLiteTensor* input, TfLiteTensor* output,
                    TfLiteSoftmaxParams* params) {
   e001c:	b530      	push	{r4, r5, lr}
   e001e:	4604      	mov	r4, r0
   e0020:	b097      	sub	sp, #92	; 0x5c
  SoftmaxParams op_params;
  op_params.beta = params->beta;
   e0022:	6810      	ldr	r0, [r2, #0]
                                 GetTensorData<uint8_t>(output));
}

// Takes a 4D tensor and perform softmax along the forth dimension.
void Softmax4DFloat(const TfLiteTensor* input, TfLiteTensor* output,
                    TfLiteSoftmaxParams* params) {
   e0024:	460d      	mov	r5, r1
  SoftmaxParams op_params;
  op_params.beta = params->beta;
   e0026:	f006 ffd1 	bl	e6fcc <__aeabi_f2d>
   e002a:	e9cd 010c 	strd	r0, r1, [sp, #48]	; 0x30
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
   e002e:	4621      	mov	r1, r4
   e0030:	a802      	add	r0, sp, #8
   e0032:	f7f6 fcbe 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e0036:	b104      	cbz	r4, e003a <_ZN6tflite3ops5micro11activations14Softmax4DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams+0x1e>
   e0038:	6864      	ldr	r4, [r4, #4]
      GetTensorShape(output), GetTensorData<float>(output));
   e003a:	4629      	mov	r1, r5
   e003c:	a807      	add	r0, sp, #28
   e003e:	f7f6 fcb8 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e0042:	b105      	cbz	r5, e0046 <_ZN6tflite3ops5micro11activations14Softmax4DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams+0x2a>
   e0044:	686d      	ldr	r5, [r5, #4]
   e0046:	9500      	str	r5, [sp, #0]
   e0048:	ab07      	add	r3, sp, #28
   e004a:	4622      	mov	r2, r4
   e004c:	a902      	add	r1, sp, #8
   e004e:	a80c      	add	r0, sp, #48	; 0x30
   e0050:	f7ff fed4 	bl	dfdfc <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKfS6_Pf>
   e0054:	a807      	add	r0, sp, #28
   e0056:	f7f6 f9fc 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
void Softmax4DFloat(const TfLiteTensor* input, TfLiteTensor* output,
                    TfLiteSoftmaxParams* params) {
  SoftmaxParams op_params;
  op_params.beta = params->beta;
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
   e005a:	a802      	add	r0, sp, #8
   e005c:	f7f6 f9f9 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
      GetTensorShape(output), GetTensorData<float>(output));
}
   e0060:	b017      	add	sp, #92	; 0x5c
   e0062:	bd30      	pop	{r4, r5, pc}

000e0064 <_ZN6tflite3ops5micro16Register_SOFTMAXEv>:
TfLiteRegistration* Register_SOFTMAX() {
  static TfLiteRegistration r = {activations::Init, activations::Free,
                                 activations::SoftmaxPrepare,
                                 activations::SoftmaxEval};
  return &r;
}
   e0064:	4800      	ldr	r0, [pc, #0]	; (e0068 <_ZN6tflite3ops5micro16Register_SOFTMAXEv+0x4>)
   e0066:	4770      	bx	lr
   e0068:	2003c2dc 	.word	0x2003c2dc

000e006c <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i>:
}

// Correctly-rounded-to-nearest division by a power-of-two.
// Also known as a rounding arithmetic right shift.
template <typename IntegerType>
inline IntegerType RoundingDivideByPOT(IntegerType x, int exponent) {
   e006c:	b538      	push	{r3, r4, r5, lr}
  assert(exponent >= 0);
   e006e:	1e0d      	subs	r5, r1, #0
}

// Correctly-rounded-to-nearest division by a power-of-two.
// Also known as a rounding arithmetic right shift.
template <typename IntegerType>
inline IntegerType RoundingDivideByPOT(IntegerType x, int exponent) {
   e0070:	4604      	mov	r4, r0
  assert(exponent >= 0);
   e0072:	da04      	bge.n	e007e <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x12>
   e0074:	4b0f      	ldr	r3, [pc, #60]	; (e00b4 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x48>)
   e0076:	4a10      	ldr	r2, [pc, #64]	; (e00b8 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x4c>)
   e0078:	f44f 71b3 	mov.w	r1, #358	; 0x166
   e007c:	e005      	b.n	e008a <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x1e>
  assert(exponent <= 31);
   e007e:	2d1f      	cmp	r5, #31
   e0080:	dd06      	ble.n	e0090 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x24>
   e0082:	4b0e      	ldr	r3, [pc, #56]	; (e00bc <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x50>)
   e0084:	4a0c      	ldr	r2, [pc, #48]	; (e00b8 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x4c>)
   e0086:	f240 1167 	movw	r1, #359	; 0x167
   e008a:	480d      	ldr	r0, [pc, #52]	; (e00c0 <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i+0x54>)
   e008c:	f004 f96e 	bl	e436c <__assert_func>
  const IntegerType mask = Dup<IntegerType>((1ll << exponent) - 1);
   e0090:	462a      	mov	r2, r5
   e0092:	2001      	movs	r0, #1
   e0094:	2100      	movs	r1, #0
   e0096:	f006 fe29 	bl	e6cec <__aeabi_llsl>
   e009a:	3801      	subs	r0, #1
  const IntegerType one = Dup<IntegerType>(1);
  const IntegerType remainder = BitAnd(x, mask);
  const IntegerType threshold =
      Add(ShiftRight(mask, 1), BitAnd(MaskIfLessThan(x, zero), one));
  return Add(ShiftRight(x, exponent),
             BitAnd(MaskIfGreaterThan(remainder, threshold), one));
   e009c:	1043      	asrs	r3, r0, #1
   e009e:	ea00 0204 	and.w	r2, r0, r4
   e00a2:	eb03 73d4 	add.w	r3, r3, r4, lsr #31
   e00a6:	fa44 f005 	asr.w	r0, r4, r5
}
   e00aa:	429a      	cmp	r2, r3
   e00ac:	bfc8      	it	gt
   e00ae:	3001      	addgt	r0, #1
   e00b0:	bd38      	pop	{r3, r4, r5, pc}
   e00b2:	bf00      	nop
   e00b4:	000e9654 	.word	0x000e9654
   e00b8:	000ea4e0 	.word	0x000ea4e0
   e00bc:	000e9701 	.word	0x000e9701
   e00c0:	000e9662 	.word	0x000e9662

000e00c4 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>:
// A FixedPoint multiplication is just a
// SaturatingRoundingDoublingHighMul operation on the underlying
// raw integer values. The IntegerBits simply add up, as is obvious
// from the fact that the range is [-2^IntegerBits, 2^IntegerBits).
template <typename tRawType, int tIntegerBits_a, int tIntegerBits_b>
FixedPoint<tRawType, tIntegerBits_a + tIntegerBits_b> operator*(
   e00c4:	b508      	push	{r3, lr}
    FixedPoint<tRawType, tIntegerBits_a> a,
    FixedPoint<tRawType, tIntegerBits_b> b) {
  FixedPoint<tRawType, tIntegerBits_a + tIntegerBits_b> c;
  c.raw() = SaturatingRoundingDoublingHighMul(a.raw(), b.raw());
   e00c6:	f7ff fe41 	bl	dfd4c <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>
  return c;
}
   e00ca:	bd08      	pop	{r3, pc}

000e00cc <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>:
// fixed-point value, regardless of the actual Scalar type. This allows
// writing generic code that applies just as well to the 32-bit and 16-bit
// cases. In the 16-bit case, the raw integer value is internally
// rounding-shifted by 16 bits to the right.
template <typename FixedPointType>
inline typename FixedPointType::ScalarRawType RescaleConstantInitializer(
   e00cc:	b508      	push	{r3, lr}
    std::int32_t int32_value) {
  typedef typename FixedPointType::ScalarRawType ScalarRawType;
  static constexpr int ScalarTypeBits = 8 * sizeof(ScalarRawType);
  return static_cast<ScalarRawType>(
      RoundingDivideByPOT<std::int32_t>(int32_value, 32 - ScalarTypeBits));
   e00ce:	2100      	movs	r1, #0
   e00d0:	f7ff ffcc 	bl	e006c <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i>
}
   e00d4:	bd08      	pop	{r3, pc}
	...

000e00d8 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_>:

// Implementation of logistic function.

// Returns 1 / (1 + x) for x in (0, 1).
template <typename tRawType>
FixedPoint<tRawType, 0> one_over_one_plus_x_for_x_in_0_1(
   e00d8:	e92d 4ff8 	stmdb	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, lr}

template <>
inline std::int32_t RoundingHalfSum(std::int32_t a, std::int32_t b) {
  std::int64_t a64 = a;
  std::int64_t b64 = b;
  std::int64_t sum = a64 + b64;
   e00dc:	f06f 4600 	mvn.w	r6, #2147483648	; 0x80000000
   e00e0:	1833      	adds	r3, r6, r0
   e00e2:	f04f 0700 	mov.w	r7, #0
   e00e6:	eb47 74e0 	adc.w	r4, r7, r0, asr #31
   e00ea:	4618      	mov	r0, r3
  std::int64_t sign = sum >= 0 ? 1 : -1;
   e00ec:	1c63      	adds	r3, r4, #1
   e00ee:	bf05      	ittet	eq
   e00f0:	f1b0 3fff 	cmpeq.w	r0, #4294967295	; 0xffffffff
   e00f4:	4602      	moveq	r2, r0
   e00f6:	2201      	movne	r2, #1
   e00f8:	4623      	moveq	r3, r4
   e00fa:	bf18      	it	ne
   e00fc:	2300      	movne	r3, #0
  return static_cast<std::int32_t>((sum + sign) / 2);
   e00fe:	1886      	adds	r6, r0, r2
   e0100:	eb44 0703 	adc.w	r7, r4, r3
   e0104:	0ffb      	lsrs	r3, r7, #31
   e0106:	18f6      	adds	r6, r6, r3
  F0 half_denominator = RoundingHalfSum(a, F0::One());
  // Newton-Raphson division
  // https://en.wikipedia.org/wiki/Division_algorithm#Newton.E2.80.93Raphson_division
  // Refer to that page for the logic behind the 48/17 and 32/17 constants.
  const F2 constant_48_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, 1515870810, 48.0 / 17.0);
   e0108:	f04f 305a 	mov.w	r0, #1515870810	; 0x5a5a5a5a
inline std::int32_t RoundingHalfSum(std::int32_t a, std::int32_t b) {
  std::int64_t a64 = a;
  std::int64_t b64 = b;
  std::int64_t sum = a64 + b64;
  std::int64_t sign = sum >= 0 ? 1 : -1;
  return static_cast<std::int32_t>((sum + sign) / 2);
   e010c:	f147 0700 	adc.w	r7, r7, #0
  F0 half_denominator = RoundingHalfSum(a, F0::One());
  // Newton-Raphson division
  // https://en.wikipedia.org/wiki/Division_algorithm#Newton.E2.80.93Raphson_division
  // Refer to that page for the logic behind the 48/17 and 32/17 constants.
  const F2 constant_48_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, 1515870810, 48.0 / 17.0);
   e0110:	f7ff ffdc 	bl	e00cc <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
   e0114:	4604      	mov	r4, r0
  const F2 constant_neg_32_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, -1010580540, -32.0 / 17.0);
   e0116:	4840      	ldr	r0, [pc, #256]	; (e0218 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x140>)
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e0118:	f8df b100 	ldr.w	fp, [pc, #256]	; e021c <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x144>
  // https://en.wikipedia.org/wiki/Division_algorithm#Newton.E2.80.93Raphson_division
  // Refer to that page for the logic behind the 48/17 and 32/17 constants.
  const F2 constant_48_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, 1515870810, 48.0 / 17.0);
  const F2 constant_neg_32_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, -1010580540, -32.0 / 17.0);
   e011c:	f7ff ffd6 	bl	e00cc <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
inline std::int32_t RoundingHalfSum(std::int32_t a, std::int32_t b) {
  std::int64_t a64 = a;
  std::int64_t b64 = b;
  std::int64_t sum = a64 + b64;
  std::int64_t sign = sum >= 0 ? 1 : -1;
  return static_cast<std::int32_t>((sum + sign) / 2);
   e0120:	107f      	asrs	r7, r7, #1
   e0122:	ea4f 0636 	mov.w	r6, r6, rrx
template <typename tRawType, int tIntegerBits_a, int tIntegerBits_b>
FixedPoint<tRawType, tIntegerBits_a + tIntegerBits_b> operator*(
    FixedPoint<tRawType, tIntegerBits_a> a,
    FixedPoint<tRawType, tIntegerBits_b> b) {
  FixedPoint<tRawType, tIntegerBits_a + tIntegerBits_b> c;
  c.raw() = SaturatingRoundingDoublingHighMul(a.raw(), b.raw());
   e0126:	4601      	mov	r1, r0
   e0128:	4630      	mov	r0, r6
   e012a:	f7ff fe0f 	bl	dfd4c <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>
inline std::int32_t RoundingHalfSum(std::int32_t a, std::int32_t b) {
  std::int64_t a64 = a;
  std::int64_t b64 = b;
  std::int64_t sum = a64 + b64;
  std::int64_t sign = sum >= 0 ? 1 : -1;
  return static_cast<std::int32_t>((sum + sign) / 2);
   e012e:	46b2      	mov	sl, r6
}

// Integer addition. Not saturating. Overflow is undefined behavior.
template <typename tIntegerType>
tIntegerType Add(tIntegerType a, tIntegerType b) {
  return a + b;
   e0130:	4404      	add	r4, r0
   e0132:	2503      	movs	r5, #3
  const auto min = std::numeric_limits<tIntegerType>::min();
  const auto max = std::numeric_limits<tIntegerType>::max();
  return wide_shifted < min
             ? min
             : wide_shifted > max ? max
                                  : static_cast<tIntegerType>(wide_shifted);
   e0134:	f06f 4600 	mvn.w	r6, #2147483648	; 0x80000000
   e0138:	2700      	movs	r7, #0
template <typename tRawType, int tIntegerBits_a, int tIntegerBits_b>
FixedPoint<tRawType, tIntegerBits_a + tIntegerBits_b> operator*(
    FixedPoint<tRawType, tIntegerBits_a> a,
    FixedPoint<tRawType, tIntegerBits_b> b) {
  FixedPoint<tRawType, tIntegerBits_a + tIntegerBits_b> c;
  c.raw() = SaturatingRoundingDoublingHighMul(a.raw(), b.raw());
   e013a:	4621      	mov	r1, r4
   e013c:	4650      	mov	r0, sl
   e013e:	f7ff fe05 	bl	dfd4c <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>
   e0142:	f1c0 5100 	rsb	r1, r0, #536870912	; 0x20000000
   e0146:	4620      	mov	r0, r4
   e0148:	f7ff fe00 	bl	dfd4c <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e014c:	f1b0 5f00 	cmp.w	r0, #536870912	; 0x20000000
   e0150:	da07      	bge.n	e0162 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x8a>
   e0152:	4558      	cmp	r0, fp
   e0154:	f04f 31ff 	mov.w	r1, #4294967295	; 0xffffffff
   e0158:	f04f 0e00 	mov.w	lr, #0
   e015c:	bfa8      	it	ge
   e015e:	2100      	movge	r1, #0
   e0160:	e002      	b.n	e0168 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x90>
   e0162:	f04f 3eff 	mov.w	lr, #4294967295	; 0xffffffff
   e0166:	2100      	movs	r1, #0
//
// tIntegerType may be int32 or any narrower signed type.
template <typename tIntegerType>
tIntegerType ShiftLeft(tIntegerType a, int offset) {
  const std::int64_t wide_a = static_cast<std::int64_t>(a);
  const std::int64_t wide_shifted = wide_a * (1 << offset);
   e0168:	17c3      	asrs	r3, r0, #31
   e016a:	ea4f 0983 	mov.w	r9, r3, lsl #2
   e016e:	ea4f 0880 	mov.w	r8, r0, lsl #2
   e0172:	ea49 7990 	orr.w	r9, r9, r0, lsr #30
  const auto min = std::numeric_limits<tIntegerType>::min();
  const auto max = std::numeric_limits<tIntegerType>::max();
  return wide_shifted < min
             ? min
             : wide_shifted > max ? max
                                  : static_cast<tIntegerType>(wide_shifted);
   e0176:	f1b8 4f00 	cmp.w	r8, #2147483648	; 0x80000000
   e017a:	f179 33ff 	sbcs.w	r3, r9, #4294967295	; 0xffffffff
   e017e:	db07      	blt.n	e0190 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0xb8>
   e0180:	4546      	cmp	r6, r8
   e0182:	eb77 0309 	sbcs.w	r3, r7, r9
   e0186:	bfac      	ite	ge
   e0188:	4643      	movge	r3, r8
   e018a:	f06f 4300 	mvnlt.w	r3, #2147483648	; 0x80000000
   e018e:	e001      	b.n	e0194 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0xbc>
   e0190:	f04f 4300 	mov.w	r3, #2147483648	; 0x80000000
}

// Integer addition. Not saturating. Overflow is undefined behavior.
template <typename tIntegerType>
tIntegerType Add(tIntegerType a, tIntegerType b) {
  return a + b;
   e0194:	ea6f 020e 	mvn.w	r2, lr
   e0198:	4013      	ands	r3, r2
   e019a:	f02e 4e00 	bic.w	lr, lr, #2147483648	; 0x80000000
   e019e:	ea83 0e0e 	eor.w	lr, r3, lr
   e01a2:	43cb      	mvns	r3, r1
   e01a4:	ea0e 0e03 	and.w	lr, lr, r3
   e01a8:	f001 4100 	and.w	r1, r1, #2147483648	; 0x80000000
   e01ac:	ea8e 0101 	eor.w	r1, lr, r1
  const F2 constant_48_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, 1515870810, 48.0 / 17.0);
  const F2 constant_neg_32_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, -1010580540, -32.0 / 17.0);
  F2 x = constant_48_over_17 + half_denominator * constant_neg_32_over_17;
  for (int i = 0; i < 3; i++) {
   e01b0:	3d01      	subs	r5, #1
}

// Integer addition. Not saturating. Overflow is undefined behavior.
template <typename tIntegerType>
tIntegerType Add(tIntegerType a, tIntegerType b) {
  return a + b;
   e01b2:	440c      	add	r4, r1
  const F2 constant_48_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, 1515870810, 48.0 / 17.0);
  const F2 constant_neg_32_over_17 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F2, -1010580540, -32.0 / 17.0);
  F2 x = constant_48_over_17 + half_denominator * constant_neg_32_over_17;
  for (int i = 0; i < 3; i++) {
   e01b4:	d1c1      	bne.n	e013a <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x62>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e01b6:	f1b4 4f80 	cmp.w	r4, #1073741824	; 0x40000000
   e01ba:	da07      	bge.n	e01cc <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0xf4>
   e01bc:	f1b4 4f40 	cmp.w	r4, #3221225472	; 0xc0000000
   e01c0:	f04f 30ff 	mov.w	r0, #4294967295	; 0xffffffff
   e01c4:	4629      	mov	r1, r5
   e01c6:	bfc8      	it	gt
   e01c8:	2000      	movgt	r0, #0
   e01ca:	e002      	b.n	e01d2 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0xfa>
   e01cc:	f04f 31ff 	mov.w	r1, #4294967295	; 0xffffffff
   e01d0:	4628      	mov	r0, r5
//
// tIntegerType may be int32 or any narrower signed type.
template <typename tIntegerType>
tIntegerType ShiftLeft(tIntegerType a, int offset) {
  const std::int64_t wide_a = static_cast<std::int64_t>(a);
  const std::int64_t wide_shifted = wide_a * (1 << offset);
   e01d2:	1922      	adds	r2, r4, r4
   e01d4:	ea4f 73e4 	mov.w	r3, r4, asr #31
   e01d8:	415b      	adcs	r3, r3
  const auto min = std::numeric_limits<tIntegerType>::min();
  const auto max = std::numeric_limits<tIntegerType>::max();
  return wide_shifted < min
             ? min
             : wide_shifted > max ? max
                                  : static_cast<tIntegerType>(wide_shifted);
   e01da:	f1b2 4f00 	cmp.w	r2, #2147483648	; 0x80000000
   e01de:	f173 34ff 	sbcs.w	r4, r3, #4294967295	; 0xffffffff
   e01e2:	db0a      	blt.n	e01fa <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x122>
   e01e4:	f06f 4400 	mvn.w	r4, #2147483648	; 0x80000000
   e01e8:	4294      	cmp	r4, r2
   e01ea:	f04f 0500 	mov.w	r5, #0
   e01ee:	eb75 0403 	sbcs.w	r4, r5, r3
   e01f2:	bfb8      	it	lt
   e01f4:	f06f 4200 	mvnlt.w	r2, #2147483648	; 0x80000000
   e01f8:	e001      	b.n	e01fe <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_+0x126>
   e01fa:	f04f 4200 	mov.w	r2, #2147483648	; 0x80000000
   e01fe:	ea22 0201 	bic.w	r2, r2, r1
   e0202:	f021 4100 	bic.w	r1, r1, #2147483648	; 0x80000000
   e0206:	404a      	eors	r2, r1
   e0208:	ea22 0200 	bic.w	r2, r2, r0
   e020c:	f000 4000 	and.w	r0, r0, #2147483648	; 0x80000000
    F2 one_minus_half_denominator_times_x =
        F2::One() - half_denominator_times_x;
    x = x + Rescale<2>(x * one_minus_half_denominator_times_x);
  }
  return Rescale<0>(ExactMulByPot<-1>(x));
}
   e0210:	4050      	eors	r0, r2
   e0212:	e8bd 8ff8 	ldmia.w	sp!, {r3, r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e0216:	bf00      	nop
   e0218:	c3c3c3c4 	.word	0xc3c3c3c4
   e021c:	e0000001 	.word	0xe0000001

000e0220 <_ZN8gemmlowp55exp_on_interval_between_negative_one_quarter_and_0_exclIlEENS_10FixedPointIT_Li0EEES3_>:

// Implementation of exponential function.

// Returns exp(x) for x in [-1/4, 0).
template <typename tRawType>
FixedPoint<tRawType, 0> exp_on_interval_between_negative_one_quarter_and_0_excl(
   e0220:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
   e0224:	4604      	mov	r4, r0
    FixedPoint<tRawType, 0> a) {
  typedef FixedPoint<tRawType, 0> F;
  const F constant_term =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F, 1895147668, std::exp(-1.0 / 8.0));
   e0226:	4814      	ldr	r0, [pc, #80]	; (e0278 <_ZN8gemmlowp55exp_on_interval_between_negative_one_quarter_and_0_exclIlEENS_10FixedPointIT_Li0EEES3_+0x58>)
   e0228:	f7ff ff50 	bl	e00cc <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
   e022c:	4606      	mov	r6, r0
  const F constant_1_over_3 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F, 715827883, 1.0 / 3.0);
   e022e:	4813      	ldr	r0, [pc, #76]	; (e027c <_ZN8gemmlowp55exp_on_interval_between_negative_one_quarter_and_0_exclIlEENS_10FixedPointIT_Li0EEES3_+0x5c>)
   e0230:	f7ff ff4c 	bl	e00cc <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
}

// Integer addition. Not saturating. Overflow is undefined behavior.
template <typename tIntegerType>
tIntegerType Add(tIntegerType a, tIntegerType b) {
  return a + b;
   e0234:	f104 5480 	add.w	r4, r4, #268435456	; 0x10000000
    FixedPoint<tRawType, 0> a) {
  typedef FixedPoint<tRawType, 0> F;
  const F constant_term =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F, 1895147668, std::exp(-1.0 / 8.0));
  const F constant_1_over_3 =
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F, 715827883, 1.0 / 3.0);
   e0238:	4680      	mov	r8, r0
  // We're evaluating a Taylor expansion around -1/8, so we do the change of
  // variable: x = a + 1/8.
  // In fixed-point with 0 integer bits, 1/8 is represented by 1 << 28.
  F x = a + F::template ConstantPOT<-3>();
  F x2 = x * x;
   e023a:	4621      	mov	r1, r4
   e023c:	4620      	mov	r0, r4
   e023e:	f7ff ff41 	bl	e00c4 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
  F x3 = x2 * x;
   e0242:	4621      	mov	r1, r4
      GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(F, 715827883, 1.0 / 3.0);
  // We're evaluating a Taylor expansion around -1/8, so we do the change of
  // variable: x = a + 1/8.
  // In fixed-point with 0 integer bits, 1/8 is represented by 1 << 28.
  F x = a + F::template ConstantPOT<-3>();
  F x2 = x * x;
   e0244:	4605      	mov	r5, r0
  F x3 = x2 * x;
   e0246:	f7ff ff3d 	bl	e00c4 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
  F x4 = x2 * x2;
   e024a:	4629      	mov	r1, r5
  // We're evaluating a Taylor expansion around -1/8, so we do the change of
  // variable: x = a + 1/8.
  // In fixed-point with 0 integer bits, 1/8 is represented by 1 << 28.
  F x = a + F::template ConstantPOT<-3>();
  F x2 = x * x;
  F x3 = x2 * x;
   e024c:	4607      	mov	r7, r0
  F x4 = x2 * x2;
   e024e:	4628      	mov	r0, r5
   e0250:	f7ff ff38 	bl	e00c4 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
};

template <int Exponent, typename IntegerType>
struct ImplSaturatingRoundingMultiplyByPOT<Exponent, IntegerType, -1> {
  static IntegerType eval(IntegerType x) {
    return RoundingDivideByPOT<IntegerType>(x, -Exponent);
   e0254:	2102      	movs	r1, #2
   e0256:	f7ff ff09 	bl	e006c <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i>
  F x2 = x * x;
  F x3 = x2 * x;
  F x4 = x2 * x2;
  F x4_over_4 = SaturatingRoundingMultiplyByPOT<-2>(x4);
  F x4_over_24_plus_x3_over_6_plus_x2_over_2 =
      SaturatingRoundingMultiplyByPOT<-1>(
   e025a:	4641      	mov	r1, r8
   e025c:	4438      	add	r0, r7
   e025e:	f7ff ff31 	bl	e00c4 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
};

template <int Exponent, typename IntegerType>
struct ImplSaturatingRoundingMultiplyByPOT<Exponent, IntegerType, -1> {
  static IntegerType eval(IntegerType x) {
    return RoundingDivideByPOT<IntegerType>(x, -Exponent);
   e0262:	2101      	movs	r1, #1
   e0264:	4428      	add	r0, r5
   e0266:	f7ff ff01 	bl	e006c <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i>
  F x4 = x2 * x2;
  F x4_over_4 = SaturatingRoundingMultiplyByPOT<-2>(x4);
  F x4_over_24_plus_x3_over_6_plus_x2_over_2 =
      SaturatingRoundingMultiplyByPOT<-1>(
          ((x4_over_4 + x3) * constant_1_over_3) + x2);
  return AddSaturatingIf16Bit(
   e026a:	1821      	adds	r1, r4, r0
   e026c:	4630      	mov	r0, r6
   e026e:	f7ff ff29 	bl	e00c4 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
      constant_term,
      constant_term * (x + x4_over_24_plus_x3_over_6_plus_x2_over_2));
}
   e0272:	4430      	add	r0, r6
   e0274:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   e0278:	70f5a894 	.word	0x70f5a894
   e027c:	2aaaaaab 	.word	0x2aaaaaab

000e0280 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE>:

// Returns exp(x) for x < 0.
template <typename tRawType, int tIntegerBits>
FixedPoint<tRawType, 0> exp_on_negative_values(
   e0280:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
  return a * b;
}

template <typename tIntegerType>
tIntegerType Sub(tIntegerType a, tIntegerType b) {
  return a - b;
   e0282:	f040 467f 	orr.w	r6, r0, #4278190080	; 0xff000000
      constant_term * (x + x4_over_24_plus_x3_over_6_plus_x2_over_2));
}

// Returns exp(x) for x < 0.
template <typename tRawType, int tIntegerBits>
FixedPoint<tRawType, 0> exp_on_negative_values(
   e0286:	4607      	mov	r7, r0
  static constexpr int kIntegerBits = InputF::kIntegerBits;
  const InputF kOneQuarter = InputF::template ConstantPOT<-2>();
  InputF mask = kOneQuarter - InputF::FromScalarRaw(1);
  InputF a_mod_quarter_minus_one_quarter = (a & mask) - kOneQuarter;
  ResultF result = exp_on_interval_between_negative_one_quarter_and_0_excl(
      Rescale<0>(a_mod_quarter_minus_one_quarter));
   e0288:	0170      	lsls	r0, r6, #5
   e028a:	f7ff ffc9 	bl	e0220 <_ZN8gemmlowp55exp_on_interval_between_negative_one_quarter_and_0_exclIlEENS_10FixedPointIT_Li0EEES3_>
   e028e:	4605      	mov	r5, r0
    result = SelectUsingMask(                                               \
        MaskIfNonZero(BitAnd(remainder, Dup<tRawType>(1 << kShiftAmount))), \
        result * kMultiplier, result);                                      \
  }

  GEMMLOWP_EXP_BARREL_SHIFTER(-2, 1672461947);
   e0290:	4833      	ldr	r0, [pc, #204]	; (e0360 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE+0xe0>)
   e0292:	f7ff ff1b 	bl	e00cc <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
   e0296:	4601      	mov	r1, r0
   e0298:	4628      	mov	r0, r5
   e029a:	f7ff ff13 	bl	e00c4 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
  return a * b;
}

template <typename tIntegerType>
tIntegerType Sub(tIntegerType a, tIntegerType b) {
  return a - b;
   e029e:	1bf6      	subs	r6, r6, r7
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e02a0:	f346 6400 	sbfx	r4, r6, #24, #1
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e02a4:	4020      	ands	r0, r4
   e02a6:	ea25 0504 	bic.w	r5, r5, r4
   e02aa:	ea80 0405 	eor.w	r4, r0, r5
        MaskIfNonZero(BitAnd(remainder, Dup<tRawType>(1 << kShiftAmount))), \
        result * kMultiplier, result);                                      \
  }

  GEMMLOWP_EXP_BARREL_SHIFTER(-2, 1672461947);
  GEMMLOWP_EXP_BARREL_SHIFTER(-1, 1302514674);
   e02ae:	482d      	ldr	r0, [pc, #180]	; (e0364 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE+0xe4>)
   e02b0:	f7ff ff0c 	bl	e00cc <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
   e02b4:	4601      	mov	r1, r0
   e02b6:	4620      	mov	r0, r4
   e02b8:	f7ff ff04 	bl	e00c4 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e02bc:	f346 6540 	sbfx	r5, r6, #25, #1
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e02c0:	4028      	ands	r0, r5
}

// Plain bit-wise AND
template <typename tIntegerType>
tIntegerType BitAnd(tIntegerType a, tIntegerType b) {
  return a & b;
   e02c2:	ea24 0405 	bic.w	r4, r4, r5
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e02c6:	4044      	eors	r4, r0
        result * kMultiplier, result);                                      \
  }

  GEMMLOWP_EXP_BARREL_SHIFTER(-2, 1672461947);
  GEMMLOWP_EXP_BARREL_SHIFTER(-1, 1302514674);
  GEMMLOWP_EXP_BARREL_SHIFTER(+0, 790015084);
   e02c8:	4827      	ldr	r0, [pc, #156]	; (e0368 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE+0xe8>)
   e02ca:	f7ff feff 	bl	e00cc <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
   e02ce:	4601      	mov	r1, r0
   e02d0:	4620      	mov	r0, r4
   e02d2:	f7ff fef7 	bl	e00c4 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e02d6:	f346 6580 	sbfx	r5, r6, #26, #1
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e02da:	4028      	ands	r0, r5
}

// Plain bit-wise AND
template <typename tIntegerType>
tIntegerType BitAnd(tIntegerType a, tIntegerType b) {
  return a & b;
   e02dc:	ea24 0405 	bic.w	r4, r4, r5
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e02e0:	4044      	eors	r4, r0
  }

  GEMMLOWP_EXP_BARREL_SHIFTER(-2, 1672461947);
  GEMMLOWP_EXP_BARREL_SHIFTER(-1, 1302514674);
  GEMMLOWP_EXP_BARREL_SHIFTER(+0, 790015084);
  GEMMLOWP_EXP_BARREL_SHIFTER(+1, 290630308);
   e02e2:	4822      	ldr	r0, [pc, #136]	; (e036c <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE+0xec>)
   e02e4:	f7ff fef2 	bl	e00cc <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
   e02e8:	4601      	mov	r1, r0
   e02ea:	4620      	mov	r0, r4
   e02ec:	f7ff feea 	bl	e00c4 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e02f0:	f346 65c0 	sbfx	r5, r6, #27, #1
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e02f4:	4028      	ands	r0, r5
}

// Plain bit-wise AND
template <typename tIntegerType>
tIntegerType BitAnd(tIntegerType a, tIntegerType b) {
  return a & b;
   e02f6:	ea24 0405 	bic.w	r4, r4, r5
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e02fa:	4044      	eors	r4, r0

  GEMMLOWP_EXP_BARREL_SHIFTER(-2, 1672461947);
  GEMMLOWP_EXP_BARREL_SHIFTER(-1, 1302514674);
  GEMMLOWP_EXP_BARREL_SHIFTER(+0, 790015084);
  GEMMLOWP_EXP_BARREL_SHIFTER(+1, 290630308);
  GEMMLOWP_EXP_BARREL_SHIFTER(+2, 39332535);
   e02fc:	481c      	ldr	r0, [pc, #112]	; (e0370 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE+0xf0>)
   e02fe:	f7ff fee5 	bl	e00cc <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
   e0302:	4601      	mov	r1, r0
   e0304:	4620      	mov	r0, r4
   e0306:	f7ff fedd 	bl	e00c4 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e030a:	f346 7500 	sbfx	r5, r6, #28, #1
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e030e:	4028      	ands	r0, r5
}

// Plain bit-wise AND
template <typename tIntegerType>
tIntegerType BitAnd(tIntegerType a, tIntegerType b) {
  return a & b;
   e0310:	ea24 0405 	bic.w	r4, r4, r5
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e0314:	4044      	eors	r4, r0
  GEMMLOWP_EXP_BARREL_SHIFTER(-2, 1672461947);
  GEMMLOWP_EXP_BARREL_SHIFTER(-1, 1302514674);
  GEMMLOWP_EXP_BARREL_SHIFTER(+0, 790015084);
  GEMMLOWP_EXP_BARREL_SHIFTER(+1, 290630308);
  GEMMLOWP_EXP_BARREL_SHIFTER(+2, 39332535);
  GEMMLOWP_EXP_BARREL_SHIFTER(+3, 720401);
   e0316:	4817      	ldr	r0, [pc, #92]	; (e0374 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE+0xf4>)
   e0318:	f7ff fed8 	bl	e00cc <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
   e031c:	4601      	mov	r1, r0
   e031e:	4620      	mov	r0, r4
   e0320:	f7ff fed0 	bl	e00c4 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e0324:	f346 7540 	sbfx	r5, r6, #29, #1
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e0328:	4028      	ands	r0, r5
}

// Plain bit-wise AND
template <typename tIntegerType>
tIntegerType BitAnd(tIntegerType a, tIntegerType b) {
  return a & b;
   e032a:	ea24 0405 	bic.w	r4, r4, r5
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e032e:	4044      	eors	r4, r0
  GEMMLOWP_EXP_BARREL_SHIFTER(-1, 1302514674);
  GEMMLOWP_EXP_BARREL_SHIFTER(+0, 790015084);
  GEMMLOWP_EXP_BARREL_SHIFTER(+1, 290630308);
  GEMMLOWP_EXP_BARREL_SHIFTER(+2, 39332535);
  GEMMLOWP_EXP_BARREL_SHIFTER(+3, 720401);
  GEMMLOWP_EXP_BARREL_SHIFTER(+4, 242);
   e0330:	20f2      	movs	r0, #242	; 0xf2
   e0332:	f7ff fecb 	bl	e00cc <_ZN8gemmlowp26RescaleConstantInitializerINS_10FixedPointIlLi2EEEEENT_13ScalarRawTypeEl>
   e0336:	4601      	mov	r1, r0
   e0338:	4620      	mov	r0, r4
   e033a:	f7ff fec3 	bl	e00c4 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e033e:	fab7 f787 	clz	r7, r7
   e0342:	f346 7680 	sbfx	r6, r6, #30, #1
   e0346:	097f      	lsrs	r7, r7, #5
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e0348:	4030      	ands	r0, r6
// For each input scalar, the corresponding bits of the result are set if the
// input scalar is non-zero.
template <typename tIntegerType>
tIntegerType MaskIfNonZero(tIntegerType a) {
  static constexpr tIntegerType zero = 0;
  return a ? BitNot(zero) : zero;
   e034a:	427f      	negs	r7, r7
}

// Plain bit-wise XOR
template <typename tIntegerType>
tIntegerType BitXor(tIntegerType a, tIntegerType b) {
  return a ^ b;
   e034c:	ea24 0406 	bic.w	r4, r4, r6
   e0350:	4044      	eors	r4, r0
        GEMMLOWP_CHECKED_FIXEDPOINT_CONSTANT(InputF, -(1 << clampB), -32.0);
    result = SelectUsingMask(MaskIfLessThan(a, clamp), ResultF::Zero(), result);
  }

  result = SelectUsingMask(MaskIfZero(a), ResultF::One(), result);
  return result;
   e0352:	43f8      	mvns	r0, r7
   e0354:	4004      	ands	r4, r0
   e0356:	f027 4000 	bic.w	r0, r7, #2147483648	; 0x80000000
}
   e035a:	4060      	eors	r0, r4
   e035c:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
   e035e:	bf00      	nop
   e0360:	63afbe7b 	.word	0x63afbe7b
   e0364:	4da2cbf2 	.word	0x4da2cbf2
   e0368:	2f16ac6c 	.word	0x2f16ac6c
   e036c:	1152aaa4 	.word	0x1152aaa4
   e0370:	02582ab7 	.word	0x02582ab7
   e0374:	000afe11 	.word	0x000afe11

000e0378 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph>:
  }
}

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const uint8* input_data,
                    const RuntimeShape& output_shape, uint8* output_data) {
   e0378:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e037c:	b08d      	sub	sp, #52	; 0x34
  using FixedPointScaledDiff =
      gemmlowp::FixedPoint<int32, kScaledDiffIntegerBits>;
  using FixedPointAccum = gemmlowp::FixedPoint<int32, kAccumulationIntegerBits>;
  using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;

  const int trailing_dim = input_shape.DimensionsCount() - 1;
   e037e:	680d      	ldr	r5, [r1, #0]
  }
}

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const uint8* input_data,
                    const RuntimeShape& output_shape, uint8* output_data) {
   e0380:	461e      	mov	r6, r3
  const int32 input_beta_multiplier = params.input_multiplier;
   e0382:	6883      	ldr	r3, [r0, #8]
   e0384:	9301      	str	r3, [sp, #4]
  }
}

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const uint8* input_data,
                    const RuntimeShape& output_shape, uint8* output_data) {
   e0386:	460c      	mov	r4, r1
  const int32 input_beta_multiplier = params.input_multiplier;
  const int32 input_beta_left_shift = params.input_left_shift;
   e0388:	68c3      	ldr	r3, [r0, #12]
   e038a:	9302      	str	r3, [sp, #8]
  using FixedPointScaledDiff =
      gemmlowp::FixedPoint<int32, kScaledDiffIntegerBits>;
  using FixedPointAccum = gemmlowp::FixedPoint<int32, kAccumulationIntegerBits>;
  using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;

  const int trailing_dim = input_shape.DimensionsCount() - 1;
   e038c:	3d01      	subs	r5, #1
inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const uint8* input_data,
                    const RuntimeShape& output_shape, uint8* output_data) {
  const int32 input_beta_multiplier = params.input_multiplier;
  const int32 input_beta_left_shift = params.input_left_shift;
  const int diff_min = params.diff_min;
   e038e:	6983      	ldr	r3, [r0, #24]
   e0390:	9303      	str	r3, [sp, #12]
  using FixedPointAccum = gemmlowp::FixedPoint<int32, kAccumulationIntegerBits>;
  using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;

  const int trailing_dim = input_shape.DimensionsCount() - 1;
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
   e0392:	4629      	mov	r1, r5
   e0394:	4620      	mov	r0, r4
  }
}

inline void Softmax(const SoftmaxParams& params,
                    const RuntimeShape& input_shape, const uint8* input_data,
                    const RuntimeShape& output_shape, uint8* output_data) {
   e0396:	4692      	mov	sl, r2
  using FixedPointAccum = gemmlowp::FixedPoint<int32, kAccumulationIntegerBits>;
  using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;

  const int trailing_dim = input_shape.DimensionsCount() - 1;
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
   e0398:	4632      	mov	r2, r6
   e039a:	f7ff fcff 	bl	dfd9c <_ZN6tflite23MatchingFlatSizeSkipDimERKNS_12RuntimeShapeEiS2_>
   e039e:	4629      	mov	r1, r5
   e03a0:	9004      	str	r0, [sp, #16]
   e03a2:	4620      	mov	r0, r4
   e03a4:	f7f6 f860 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   e03a8:	4629      	mov	r1, r5
   e03aa:	4604      	mov	r4, r0
   e03ac:	4630      	mov	r0, r6
   e03ae:	f7f6 f85b 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   e03b2:	4284      	cmp	r4, r0
   e03b4:	d001      	beq.n	e03ba <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x42>
   e03b6:	f003 ffc9 	bl	e434c <abort>
   e03ba:	f8dd 9058 	ldr.w	r9, [sp, #88]	; 0x58
   e03be:	4656      	mov	r6, sl
   e03c0:	f1ca 0500 	rsb	r5, sl, #0
   e03c4:	2700      	movs	r7, #0
  const int depth =
      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);

  for (int i = 0; i < outer_size; ++i) {
   e03c6:	9b04      	ldr	r3, [sp, #16]
   e03c8:	429f      	cmp	r7, r3
   e03ca:	da7d      	bge.n	e04c8 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x150>
    uint8 max_in_row = 0;
   e03cc:	f04f 0300 	mov.w	r3, #0
   e03d0:	f88d 3023 	strb.w	r3, [sp, #35]	; 0x23
   e03d4:	4632      	mov	r2, r6
    for (int c = 0; c < depth; ++c) {
   e03d6:	1953      	adds	r3, r2, r5
   e03d8:	42a3      	cmp	r3, r4
   e03da:	da0c      	bge.n	e03f6 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x7e>
      max_in_row = std::max(max_in_row, input_data[i * depth + c]);
   e03dc:	4613      	mov	r3, r2
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   e03de:	f89d 1023 	ldrb.w	r1, [sp, #35]	; 0x23
   e03e2:	7818      	ldrb	r0, [r3, #0]
   e03e4:	4288      	cmp	r0, r1
	return __b;
      return __a;
   e03e6:	bf98      	it	ls
   e03e8:	f10d 0323 	addls.w	r3, sp, #35	; 0x23
   e03ec:	3201      	adds	r2, #1
   e03ee:	781b      	ldrb	r3, [r3, #0]
   e03f0:	f88d 3023 	strb.w	r3, [sp, #35]	; 0x23
  const int depth =
      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);

  for (int i = 0; i < outer_size; ++i) {
    uint8 max_in_row = 0;
    for (int c = 0; c < depth; ++c) {
   e03f4:	e7ef      	b.n	e03d6 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x5e>
   e03f6:	46b0      	mov	r8, r6
   e03f8:	f04f 0b00 	mov.w	fp, #0
      max_in_row = std::max(max_in_row, input_data[i * depth + c]);
    }

    FixedPointAccum sum_of_exps = FixedPointAccum::Zero();
    for (int c = 0; c < depth; ++c) {
   e03fc:	eb08 0305 	add.w	r3, r8, r5
   e0400:	42a3      	cmp	r3, r4
   e0402:	da13      	bge.n	e042c <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xb4>
      int32 input_diff =
          static_cast<int32>(input_data[i * depth + c]) - max_in_row;
   e0404:	f818 3b01 	ldrb.w	r3, [r8], #1
   e0408:	f89d 0023 	ldrb.w	r0, [sp, #35]	; 0x23
   e040c:	1a18      	subs	r0, r3, r0
      if (input_diff >= diff_min) {
   e040e:	9b03      	ldr	r3, [sp, #12]
   e0410:	4283      	cmp	r3, r0
   e0412:	dcf3      	bgt.n	e03fc <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x84>

inline int32 MultiplyByQuantizedMultiplierGreaterThanOne(
    int32 x, int32 quantized_multiplier, int left_shift) {
  using gemmlowp::SaturatingRoundingDoublingHighMul;
  return SaturatingRoundingDoublingHighMul(x * (1 << left_shift),
                                           quantized_multiplier);
   e0414:	9b02      	ldr	r3, [sp, #8]
   e0416:	9901      	ldr	r1, [sp, #4]
   e0418:	4098      	lsls	r0, r3
   e041a:	f7ff fc97 	bl	dfd4c <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>
            MultiplyByQuantizedMultiplierGreaterThanOne(
                input_diff, input_beta_multiplier, input_beta_left_shift);
        const FixedPointScaledDiff scaled_diff_f8 =
            FixedPointScaledDiff::FromRaw(input_diff_rescaled);
        sum_of_exps = sum_of_exps + gemmlowp::Rescale<kAccumulationIntegerBits>(
                                        exp_on_negative_values(scaled_diff_f8));
   e041e:	f7ff ff2f 	bl	e0280 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE>
};

template <int Exponent, typename IntegerType>
struct ImplSaturatingRoundingMultiplyByPOT<Exponent, IntegerType, -1> {
  static IntegerType eval(IntegerType x) {
    return RoundingDivideByPOT<IntegerType>(x, -Exponent);
   e0422:	210c      	movs	r1, #12
   e0424:	f7ff fe22 	bl	e006c <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i>
}

// Integer addition. Not saturating. Overflow is undefined behavior.
template <typename tIntegerType>
tIntegerType Add(tIntegerType a, tIntegerType b) {
  return a + b;
   e0428:	4483      	add	fp, r0
    for (int c = 0; c < depth; ++c) {
      max_in_row = std::max(max_in_row, input_data[i * depth + c]);
    }

    FixedPointAccum sum_of_exps = FixedPointAccum::Zero();
    for (int c = 0; c < depth; ++c) {
   e042a:	e7e7      	b.n	e03fc <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x84>
int CountLeadingZeros(T integer_input) {
  static_assert(std::is_unsigned<T>::value,
                "Only unsigned integer types handled.");
#if defined(__GNUC__)
  return integer_input ? __builtin_clz(integer_input)
                       : std::numeric_limits<T>::digits;
   e042c:	f1bb 0f00 	cmp.w	fp, #0
   e0430:	d002      	beq.n	e0438 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xc0>
   e0432:	fabb f88b 	clz	r8, fp
   e0436:	e001      	b.n	e043c <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xc4>
   e0438:	f04f 0820 	mov.w	r8, #32
   e043c:	fa0b f008 	lsl.w	r0, fp, r8
      static_cast<int32>((static_cast<uint32>(x) << headroom_plus_one) -
                         (static_cast<uint32>(1) << 31));

  gemmlowp::FixedPoint<int32, 0> shifted_scale =
      gemmlowp::one_over_one_plus_x_for_x_in_0_1(
          gemmlowp::FixedPoint<int32, 0>::FromRaw(shifted_sum_minus_one));
   e0440:	f100 4000 	add.w	r0, r0, #2147483648	; 0x80000000
   e0444:	f7ff fe48 	bl	e00d8 <_ZN8gemmlowp32one_over_one_plus_x_for_x_in_0_1IlEENS_10FixedPointIT_Li0EEES3_>

    int num_bits_over_unit;
    FixedPoint0 shifted_scale = FixedPoint0::FromRaw(GetReciprocal(
        sum_of_exps.raw(), kAccumulationIntegerBits, &num_bits_over_unit));

    for (int c = 0; c < depth; ++c) {
   e0448:	9916      	ldr	r1, [sp, #88]	; 0x58
   e044a:	1a69      	subs	r1, r5, r1
   e044c:	4451      	add	r1, sl
   e044e:	4683      	mov	fp, r0
      }
    }

    int num_bits_over_unit;
    FixedPoint0 shifted_scale = FixedPoint0::FromRaw(GetReciprocal(
        sum_of_exps.raw(), kAccumulationIntegerBits, &num_bits_over_unit));
   e0450:	4632      	mov	r2, r6
   e0452:	464b      	mov	r3, r9

    for (int c = 0; c < depth; ++c) {
   e0454:	9105      	str	r1, [sp, #20]
                input_diff, input_beta_multiplier, input_beta_left_shift);
        const FixedPointScaledDiff scaled_diff_f8 =
            FixedPointScaledDiff::FromRaw(input_diff_rescaled);

        FixedPoint0 exp_in_0 = exp_on_negative_values(scaled_diff_f8);
        int32 unsat_output = gemmlowp::RoundingDivideByPOT(
   e0456:	f1c8 0823 	rsb	r8, r8, #35	; 0x23

    int num_bits_over_unit;
    FixedPoint0 shifted_scale = FixedPoint0::FromRaw(GetReciprocal(
        sum_of_exps.raw(), kAccumulationIntegerBits, &num_bits_over_unit));

    for (int c = 0; c < depth; ++c) {
   e045a:	9905      	ldr	r1, [sp, #20]
   e045c:	4419      	add	r1, r3
   e045e:	428c      	cmp	r4, r1
   e0460:	dd2d      	ble.n	e04be <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x146>
      int32 input_diff =
          static_cast<int32>(input_data[i * depth + c]) - max_in_row;
   e0462:	f812 1b01 	ldrb.w	r1, [r2], #1
   e0466:	f89d 0023 	ldrb.w	r0, [sp, #35]	; 0x23
   e046a:	1a08      	subs	r0, r1, r0
      if (input_diff >= diff_min) {
   e046c:	9903      	ldr	r1, [sp, #12]
   e046e:	4281      	cmp	r1, r0
   e0470:	dc20      	bgt.n	e04b4 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x13c>
   e0472:	9306      	str	r3, [sp, #24]

inline int32 MultiplyByQuantizedMultiplierGreaterThanOne(
    int32 x, int32 quantized_multiplier, int left_shift) {
  using gemmlowp::SaturatingRoundingDoublingHighMul;
  return SaturatingRoundingDoublingHighMul(x * (1 << left_shift),
                                           quantized_multiplier);
   e0474:	9b02      	ldr	r3, [sp, #8]
   e0476:	9901      	ldr	r1, [sp, #4]
   e0478:	9207      	str	r2, [sp, #28]
   e047a:	4098      	lsls	r0, r3
   e047c:	f7ff fc66 	bl	dfd4c <_ZN8gemmlowp33SaturatingRoundingDoublingHighMulIlEET_S1_S1_>
            MultiplyByQuantizedMultiplierGreaterThanOne(
                input_diff, input_beta_multiplier, input_beta_left_shift);
        const FixedPointScaledDiff scaled_diff_f8 =
            FixedPointScaledDiff::FromRaw(input_diff_rescaled);

        FixedPoint0 exp_in_0 = exp_on_negative_values(scaled_diff_f8);
   e0480:	f7ff fefe 	bl	e0280 <_ZN8gemmlowp22exp_on_negative_valuesIlLi5EEENS_10FixedPointIT_Li0EEENS1_IS2_XT0_EEE>
        int32 unsat_output = gemmlowp::RoundingDivideByPOT(
            (shifted_scale * exp_in_0).raw(), num_bits_over_unit + 31 - 8);
   e0484:	4601      	mov	r1, r0
   e0486:	4658      	mov	r0, fp
   e0488:	f7ff fe1c 	bl	e00c4 <_ZN8gemmlowpmlIlLi0ELi0EEENS_10FixedPointIT_XplT0_T1_EEENS1_IS2_XT0_EEENS1_IS2_XT1_EEE>
                input_diff, input_beta_multiplier, input_beta_left_shift);
        const FixedPointScaledDiff scaled_diff_f8 =
            FixedPointScaledDiff::FromRaw(input_diff_rescaled);

        FixedPoint0 exp_in_0 = exp_on_negative_values(scaled_diff_f8);
        int32 unsat_output = gemmlowp::RoundingDivideByPOT(
   e048c:	4641      	mov	r1, r8
   e048e:	f7ff fded 	bl	e006c <_ZN8gemmlowp19RoundingDivideByPOTIlEET_S1_i>
            (shifted_scale * exp_in_0).raw(), num_bits_over_unit + 31 - 8);

        output_data[i * depth + c] = static_cast<uint8>(
            std::max(std::min(unsat_output, static_cast<int32>(255)),
   e0492:	21ff      	movs	r1, #255	; 0xff
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
   e0494:	4288      	cmp	r0, r1
   e0496:	910a      	str	r1, [sp, #40]	; 0x28
	return __b;
      return __a;
   e0498:	bfd4      	ite	le
   e049a:	a909      	addle	r1, sp, #36	; 0x24
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   e049c:	a90a      	addgt	r1, sp, #40	; 0x28
        const FixedPointScaledDiff scaled_diff_f8 =
            FixedPointScaledDiff::FromRaw(input_diff_rescaled);

        FixedPoint0 exp_in_0 = exp_on_negative_values(scaled_diff_f8);
        int32 unsat_output = gemmlowp::RoundingDivideByPOT(
            (shifted_scale * exp_in_0).raw(), num_bits_over_unit + 31 - 8);
   e049e:	9009      	str	r0, [sp, #36]	; 0x24

        output_data[i * depth + c] = static_cast<uint8>(
            std::max(std::min(unsat_output, static_cast<int32>(255)),
                     static_cast<int32>(0)));
   e04a0:	2000      	movs	r0, #0
   e04a2:	900b      	str	r0, [sp, #44]	; 0x2c
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   e04a4:	6808      	ldr	r0, [r1, #0]
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
   e04a6:	9b06      	ldr	r3, [sp, #24]
   e04a8:	9a07      	ldr	r2, [sp, #28]
    max(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return  __a < __b ? __b : __a;
      if (__a < __b)
   e04aa:	2800      	cmp	r0, #0
	return __b;
   e04ac:	bfb8      	it	lt
   e04ae:	a90b      	addlt	r1, sp, #44	; 0x2c
   e04b0:	6809      	ldr	r1, [r1, #0]
   e04b2:	e001      	b.n	e04b8 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x140>

      } else {
        output_data[i * depth + c] = 0;
   e04b4:	f04f 0100 	mov.w	r1, #0
   e04b8:	7019      	strb	r1, [r3, #0]
   e04ba:	3301      	adds	r3, #1

    int num_bits_over_unit;
    FixedPoint0 shifted_scale = FixedPoint0::FromRaw(GetReciprocal(
        sum_of_exps.raw(), kAccumulationIntegerBits, &num_bits_over_unit));

    for (int c = 0; c < depth; ++c) {
   e04bc:	e7cd      	b.n	e045a <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0xe2>
  const int outer_size =
      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
  const int depth =
      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);

  for (int i = 0; i < outer_size; ++i) {
   e04be:	3701      	adds	r7, #1
   e04c0:	44a1      	add	r9, r4
   e04c2:	4426      	add	r6, r4
   e04c4:	1b2d      	subs	r5, r5, r4
   e04c6:	e77e      	b.n	e03c6 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph+0x4e>
      } else {
        output_data[i * depth + c] = 0;
      }
    }
  }
}
   e04c8:	b00d      	add	sp, #52	; 0x34
   e04ca:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

000e04d0 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode>:
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
      GetTensorShape(output), GetTensorData<uint8_t>(output));
}

TfLiteStatus SoftmaxEval(TfLiteContext* context, TfLiteNode* node) {
   e04d0:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e04d4:	680b      	ldr	r3, [r1, #0]
   e04d6:	f8d0 8008 	ldr.w	r8, [r0, #8]
   e04da:	685a      	ldr	r2, [r3, #4]
  auto* params = reinterpret_cast<TfLiteSoftmaxParams*>(node->builtin_data);
   e04dc:	694f      	ldr	r7, [r1, #20]
   e04de:	2338      	movs	r3, #56	; 0x38
   e04e0:	fb03 f902 	mul.w	r9, r3, r2
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   e04e4:	684a      	ldr	r2, [r1, #4]
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
      GetTensorShape(output), GetTensorData<uint8_t>(output));
}

TfLiteStatus SoftmaxEval(TfLiteContext* context, TfLiteNode* node) {
   e04e6:	b09f      	sub	sp, #124	; 0x7c
   e04e8:	6854      	ldr	r4, [r2, #4]
   e04ea:	4605      	mov	r5, r0
  auto* params = reinterpret_cast<TfLiteSoftmaxParams*>(node->builtin_data);

  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);

  OpData local_data_object;
   e04ec:	2210      	movs	r2, #16
   e04ee:	2100      	movs	r1, #0
   e04f0:	a806      	add	r0, sp, #24
   e04f2:	fb03 8404 	mla	r4, r3, r4, r8
   e04f6:	f007 f967 	bl	e77c8 <memset>
TfLiteStatus CalculateSoftmaxOpData(TfLiteContext* context,
                                    const TfLiteTensor* input,
                                    TfLiteTensor* output,
                                    const TfLiteSoftmaxParams* params,
                                    OpData* data) {
  if (input->type == kTfLiteUInt8) {
   e04fa:	f818 3009 	ldrb.w	r3, [r8, r9]
   e04fe:	2b03      	cmp	r3, #3
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e0500:	eb08 0609 	add.w	r6, r8, r9
   e0504:	d13f      	bne.n	e0586 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0xb6>
    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);
   e0506:	6923      	ldr	r3, [r4, #16]
   e0508:	b16b      	cbz	r3, e0526 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x56>
   e050a:	9302      	str	r3, [sp, #8]
   e050c:	4b67      	ldr	r3, [pc, #412]	; (e06ac <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1dc>)
   e050e:	9301      	str	r3, [sp, #4]
   e0510:	2200      	movs	r2, #0
   e0512:	4b67      	ldr	r3, [pc, #412]	; (e06b0 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1e0>)
   e0514:	9203      	str	r2, [sp, #12]
   e0516:	9300      	str	r3, [sp, #0]
   e0518:	696c      	ldr	r4, [r5, #20]
   e051a:	4a66      	ldr	r2, [pc, #408]	; (e06b4 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1e4>)
   e051c:	4966      	ldr	r1, [pc, #408]	; (e06b8 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1e8>)
   e051e:	232c      	movs	r3, #44	; 0x2c
   e0520:	4628      	mov	r0, r5
   e0522:	47a0      	blx	r4
   e0524:	e0bd      	b.n	e06a2 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1d2>
    TF_LITE_ENSURE(context, output->params.scale == 1.f / 256);
   e0526:	ed94 7a03 	vldr	s14, [r4, #12]
   e052a:	eddf 7a64 	vldr	s15, [pc, #400]	; e06bc <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1ec>
   e052e:	eeb4 7a67 	vcmp.f32	s14, s15
   e0532:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e0536:	d008      	beq.n	e054a <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x7a>
   e0538:	4b61      	ldr	r3, [pc, #388]	; (e06c0 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1f0>)
   e053a:	9300      	str	r3, [sp, #0]
   e053c:	696c      	ldr	r4, [r5, #20]
   e053e:	4a5d      	ldr	r2, [pc, #372]	; (e06b4 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1e4>)
   e0540:	4960      	ldr	r1, [pc, #384]	; (e06c4 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1f4>)
   e0542:	232d      	movs	r3, #45	; 0x2d
   e0544:	4628      	mov	r0, r5
   e0546:	47a0      	blx	r4
   e0548:	e0ab      	b.n	e06a2 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1d2>

    static const int kScaledDiffIntegerBits = 5;

    tflite::PreprocessSoftmaxScaling(
        params->beta, input->params.scale, kScaledDiffIntegerBits,
        &data->input_multiplier, &data->input_left_shift);
   e054a:	68f0      	ldr	r0, [r6, #12]
   e054c:	f006 fd3e 	bl	e6fcc <__aeabi_f2d>
   e0550:	e9cd 0104 	strd	r0, r1, [sp, #16]
   e0554:	6838      	ldr	r0, [r7, #0]
   e0556:	f006 fd39 	bl	e6fcc <__aeabi_f2d>
   e055a:	ed9d 1b04 	vldr	d1, [sp, #16]
   e055e:	ec41 0b10 	vmov	d0, r0, r1
   e0562:	aa07      	add	r2, sp, #28
   e0564:	a906      	add	r1, sp, #24
   e0566:	2005      	movs	r0, #5
   e0568:	f003 fcaa 	bl	e3ec0 <_ZN6tflite24PreprocessSoftmaxScalingEddiPlPi>
    data->diff_min = -1.0 * tflite::CalculateInputRadius(
   e056c:	221f      	movs	r2, #31
   e056e:	9907      	ldr	r1, [sp, #28]
   e0570:	2005      	movs	r0, #5
   e0572:	f003 fcdd 	bl	e3f30 <_ZN6tflite20CalculateInputRadiusEiii>
                                kScaledDiffIntegerBits, data->input_left_shift);
   e0576:	f006 fd17 	bl	e6fa8 <__aeabi_i2d>
   e057a:	f101 4300 	add.w	r3, r1, #2147483648	; 0x80000000
   e057e:	4619      	mov	r1, r3
   e0580:	f007 f812 	bl	e75a8 <__aeabi_d2iz>
   e0584:	9009      	str	r0, [sp, #36]	; 0x24
  TF_LITE_ENSURE_STATUS(
      CalculateSoftmaxOpData(context, input, output, params, data));

  // TODO(ahentz): consider an implementation that works for many (all?)
  // dimensions.
  switch (input->type) {
   e0586:	f818 8009 	ldrb.w	r8, [r8, r9]
   e058a:	f1b8 0f01 	cmp.w	r8, #1
   e058e:	d11f      	bne.n	e05d0 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x100>
#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/c_api_internal.h"

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
   e0590:	68b3      	ldr	r3, [r6, #8]
   e0592:	681a      	ldr	r2, [r3, #0]
    case kTfLiteFloat32: {
      if (NumDimensions(input) == 1) {
   e0594:	2a01      	cmp	r2, #1
   e0596:	d105      	bne.n	e05a4 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0xd4>
        Softmax1DFloat(input, output, params);
   e0598:	463a      	mov	r2, r7
   e059a:	4621      	mov	r1, r4
   e059c:	4630      	mov	r0, r6
   e059e:	f7ff fd27 	bl	dfff0 <_ZN6tflite3ops5micro11activations14Softmax1DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams>
   e05a2:	e042      	b.n	e062a <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x15a>
        return kTfLiteOk;
      }
      if (NumDimensions(input) == 2) {
   e05a4:	2a02      	cmp	r2, #2
   e05a6:	d105      	bne.n	e05b4 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0xe4>
        Softmax2DFloat(input, output, params);
   e05a8:	463a      	mov	r2, r7
   e05aa:	4621      	mov	r1, r4
   e05ac:	4630      	mov	r0, r6
   e05ae:	f7ff fd2a 	bl	e0006 <_ZN6tflite3ops5micro11activations14Softmax2DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams>
   e05b2:	e03a      	b.n	e062a <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x15a>
        return kTfLiteOk;
      }
      if (NumDimensions(input) == 4) {
   e05b4:	2a04      	cmp	r2, #4
   e05b6:	d105      	bne.n	e05c4 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0xf4>
        Softmax4DFloat(input, output, params);
   e05b8:	463a      	mov	r2, r7
   e05ba:	4621      	mov	r1, r4
   e05bc:	4630      	mov	r0, r6
   e05be:	f7ff fd2d 	bl	e001c <_ZN6tflite3ops5micro11activations14Softmax4DFloatEPK12TfLiteTensorPS3_P19TfLiteSoftmaxParams>
   e05c2:	e032      	b.n	e062a <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x15a>
        return kTfLiteOk;
      }
      context->ReportError(
          context, "Only 1D, 2D and 4D tensors supported currently, got %dD.",
          NumDimensions(input));
   e05c4:	4628      	mov	r0, r5
   e05c6:	696b      	ldr	r3, [r5, #20]
   e05c8:	493f      	ldr	r1, [pc, #252]	; (e06c8 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1f8>)
   e05ca:	4798      	blx	r3
      return kTfLiteError;
   e05cc:	4640      	mov	r0, r8
   e05ce:	e069      	b.n	e06a4 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1d4>
  TF_LITE_ENSURE_STATUS(
      CalculateSoftmaxOpData(context, input, output, params, data));

  // TODO(ahentz): consider an implementation that works for many (all?)
  // dimensions.
  switch (input->type) {
   e05d0:	f1b8 0f03 	cmp.w	r8, #3
   e05d4:	d160      	bne.n	e0698 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1c8>
   e05d6:	68b3      	ldr	r3, [r6, #8]
   e05d8:	681f      	ldr	r7, [r3, #0]
          context, "Only 1D, 2D and 4D tensors supported currently, got %dD.",
          NumDimensions(input));
      return kTfLiteError;
    }
    case kTfLiteUInt8: {
      if (NumDimensions(input) == 1) {
   e05da:	2f01      	cmp	r7, #1
   e05dc:	d127      	bne.n	e062e <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x15e>
  // TODO(ahentz): this is arguably a dirty trick. Since the implementation
  // always traverses the last dimension of a 4D tensor, we will pretend our 1D
  // tensor is 4D in a special way. We will convert a (Y) shape into a (1,
  // 1, 1, Y) shape.
  const int input_size = input->dims->data[0];
  const int32_t shape_data[4] = {1, 1, 1, input_size};
   e05de:	ad0a      	add	r5, sp, #40	; 0x28
                        TfLiteSoftmaxParams* params, OpData* data) {
  // TODO(ahentz): this is arguably a dirty trick. Since the implementation
  // always traverses the last dimension of a 4D tensor, we will pretend our 1D
  // tensor is 4D in a special way. We will convert a (Y) shape into a (1,
  // 1, 1, Y) shape.
  const int input_size = input->dims->data[0];
   e05e0:	f8d3 8004 	ldr.w	r8, [r3, #4]
  const int32_t shape_data[4] = {1, 1, 1, input_size};
   e05e4:	2210      	movs	r2, #16
   e05e6:	2100      	movs	r1, #0
   e05e8:	4628      	mov	r0, r5
   e05ea:	f007 f8ed 	bl	e77c8 <memset>
   e05ee:	970a      	str	r7, [sp, #40]	; 0x28
   e05f0:	970b      	str	r7, [sp, #44]	; 0x2c
   e05f2:	970c      	str	r7, [sp, #48]	; 0x30
   e05f4:	f8cd 8034 	str.w	r8, [sp, #52]	; 0x34
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
#endif  // TF_LITE_STATIC_MEMORY
    }
    size_ = dimensions_count;
   e05f8:	2304      	movs	r3, #4
   e05fa:	930f      	str	r3, [sp, #60]	; 0x3c
  }

  inline void ReplaceWith(int dimensions_count, const int32* dims_data) {
    Resize(dimensions_count);
    int32* dst_dims = DimsData();
    std::memcpy(dst_dims, dims_data, dimensions_count * sizeof(int32));
   e05fc:	e895 000f 	ldmia.w	r5, {r0, r1, r2, r3}
   e0600:	af10      	add	r7, sp, #64	; 0x40
   e0602:	e887 000f 	stmia.w	r7, {r0, r1, r2, r3}
  const int batch_size = input->dims->data[0];
  const int input_size = input->dims->data[1];
  const int32_t shape_data[4] = {batch_size, 1, 1, input_size};
  RuntimeShape shape(4, shape_data);
  SoftmaxParams op_params;
  op_params.input_multiplier = data->input_multiplier;
   e0606:	9b06      	ldr	r3, [sp, #24]
   e0608:	9316      	str	r3, [sp, #88]	; 0x58
  op_params.input_left_shift = data->input_left_shift;
   e060a:	9b07      	ldr	r3, [sp, #28]
   e060c:	9317      	str	r3, [sp, #92]	; 0x5c
  op_params.diff_min = data->diff_min;
   e060e:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e0610:	931a      	str	r3, [sp, #104]	; 0x68
   e0612:	b104      	cbz	r4, e0616 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x146>
   e0614:	6864      	ldr	r4, [r4, #4]
  tflite::reference_ops::Softmax(op_params, shape,
                                 GetTensorData<uint8_t>(input), shape,
                                 GetTensorData<uint8_t>(output));
   e0616:	9400      	str	r4, [sp, #0]
   e0618:	ab0f      	add	r3, sp, #60	; 0x3c
   e061a:	a814      	add	r0, sp, #80	; 0x50
   e061c:	6872      	ldr	r2, [r6, #4]
   e061e:	4619      	mov	r1, r3
   e0620:	f7ff feaa 	bl	e0378 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph>
  // tensor is 4D in a special way. We will convert a (X, Y) shape into a (X,
  // 1, 1, Y) shape.
  const int batch_size = input->dims->data[0];
  const int input_size = input->dims->data[1];
  const int32_t shape_data[4] = {batch_size, 1, 1, input_size};
  RuntimeShape shape(4, shape_data);
   e0624:	a80f      	add	r0, sp, #60	; 0x3c
  SoftmaxParams op_params;
  op_params.input_multiplier = data->input_multiplier;
  op_params.input_left_shift = data->input_left_shift;
  op_params.diff_min = data->diff_min;
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   e0626:	f7f5 ff14 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
        Softmax2DQuantized(input, output, params, data);
        return kTfLiteOk;
      }
      if (NumDimensions(input) == 4) {
        Softmax4DQuantized(input, output, params, data);
        return kTfLiteOk;
   e062a:	2000      	movs	r0, #0
   e062c:	e03a      	b.n	e06a4 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1d4>
    case kTfLiteUInt8: {
      if (NumDimensions(input) == 1) {
        Softmax1DQuantized(input, output, params, data);
        return kTfLiteOk;
      }
      if (NumDimensions(input) == 2) {
   e062e:	2f02      	cmp	r7, #2
   e0630:	d10f      	bne.n	e0652 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x182>
  // always traverses the last dimension of a 4D tensor, we will pretend our 2D
  // tensor is 4D in a special way. We will convert a (X, Y) shape into a (X,
  // 1, 1, Y) shape.
  const int batch_size = input->dims->data[0];
  const int input_size = input->dims->data[1];
  const int32_t shape_data[4] = {batch_size, 1, 1, input_size};
   e0632:	ad0a      	add	r5, sp, #40	; 0x28
   e0634:	2210      	movs	r2, #16
   e0636:	2100      	movs	r1, #0
   e0638:	4628      	mov	r0, r5
                        TfLiteSoftmaxParams* params, OpData* data) {
  // TODO(ahentz): this is arguably a dirty trick. Since the implementation
  // always traverses the last dimension of a 4D tensor, we will pretend our 2D
  // tensor is 4D in a special way. We will convert a (X, Y) shape into a (X,
  // 1, 1, Y) shape.
  const int batch_size = input->dims->data[0];
   e063a:	f8d3 8004 	ldr.w	r8, [r3, #4]
  const int input_size = input->dims->data[1];
   e063e:	689f      	ldr	r7, [r3, #8]
  const int32_t shape_data[4] = {batch_size, 1, 1, input_size};
   e0640:	f007 f8c2 	bl	e77c8 <memset>
   e0644:	2301      	movs	r3, #1
   e0646:	930b      	str	r3, [sp, #44]	; 0x2c
   e0648:	930c      	str	r3, [sp, #48]	; 0x30
   e064a:	f8cd 8028 	str.w	r8, [sp, #40]	; 0x28
   e064e:	970d      	str	r7, [sp, #52]	; 0x34
   e0650:	e7d2      	b.n	e05f8 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x128>
      }
      if (NumDimensions(input) == 2) {
        Softmax2DQuantized(input, output, params, data);
        return kTfLiteOk;
      }
      if (NumDimensions(input) == 4) {
   e0652:	2f04      	cmp	r7, #4
   e0654:	d11c      	bne.n	e0690 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1c0>
}

void Softmax4DQuantized(const TfLiteTensor* input, TfLiteTensor* output,
                        TfLiteSoftmaxParams* params, OpData* data) {
  SoftmaxParams op_params;
  op_params.input_multiplier = data->input_multiplier;
   e0656:	9b06      	ldr	r3, [sp, #24]
   e0658:	9316      	str	r3, [sp, #88]	; 0x58
  op_params.input_left_shift = data->input_left_shift;
   e065a:	9b07      	ldr	r3, [sp, #28]
   e065c:	9317      	str	r3, [sp, #92]	; 0x5c
  op_params.diff_min = data->diff_min;
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   e065e:	4631      	mov	r1, r6
void Softmax4DQuantized(const TfLiteTensor* input, TfLiteTensor* output,
                        TfLiteSoftmaxParams* params, OpData* data) {
  SoftmaxParams op_params;
  op_params.input_multiplier = data->input_multiplier;
  op_params.input_left_shift = data->input_left_shift;
  op_params.diff_min = data->diff_min;
   e0660:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e0662:	931a      	str	r3, [sp, #104]	; 0x68
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   e0664:	a80a      	add	r0, sp, #40	; 0x28
   e0666:	f7f6 f9a4 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(output), GetTensorData<uint8_t>(output));
   e066a:	4621      	mov	r1, r4
   e066c:	a80f      	add	r0, sp, #60	; 0x3c
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e066e:	6875      	ldr	r5, [r6, #4]
   e0670:	f7f6 f99f 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e0674:	b104      	cbz	r4, e0678 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1a8>
   e0676:	6864      	ldr	r4, [r4, #4]
   e0678:	9400      	str	r4, [sp, #0]
   e067a:	ab0f      	add	r3, sp, #60	; 0x3c
   e067c:	462a      	mov	r2, r5
   e067e:	a90a      	add	r1, sp, #40	; 0x28
   e0680:	a814      	add	r0, sp, #80	; 0x50
   e0682:	f7ff fe79 	bl	e0378 <_ZN6tflite13reference_ops7SoftmaxERKNS_13SoftmaxParamsERKNS_12RuntimeShapeEPKhS6_Ph>
   e0686:	a80f      	add	r0, sp, #60	; 0x3c
   e0688:	f7f5 fee3 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  SoftmaxParams op_params;
  op_params.input_multiplier = data->input_multiplier;
  op_params.input_left_shift = data->input_left_shift;
  op_params.diff_min = data->diff_min;
  tflite::reference_ops::Softmax(
      op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   e068c:	a80a      	add	r0, sp, #40	; 0x28
   e068e:	e7ca      	b.n	e0626 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x156>
        Softmax4DQuantized(input, output, params, data);
        return kTfLiteOk;
      }
      context->ReportError(
          context, "Only 2D and 4D tensors supported currently, got %dD.",
          NumDimensions(input));
   e0690:	696b      	ldr	r3, [r5, #20]
   e0692:	490e      	ldr	r1, [pc, #56]	; (e06cc <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1fc>)
   e0694:	463a      	mov	r2, r7
   e0696:	e002      	b.n	e069e <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x1ce>
      return kTfLiteError;
    }
    default:
      context->ReportError(
          context, "Only float32 and uint8_t supported currently, got %d.",
          input->type);
   e0698:	696b      	ldr	r3, [r5, #20]
   e069a:	490d      	ldr	r1, [pc, #52]	; (e06d0 <_ZN6tflite3ops5micro11activations11SoftmaxEvalEP13TfLiteContextP10TfLiteNode+0x200>)
   e069c:	4642      	mov	r2, r8
   e069e:	4628      	mov	r0, r5
   e06a0:	4798      	blx	r3
  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);

  OpData local_data_object;
  OpData* data = &local_data_object;
  TF_LITE_ENSURE_STATUS(
   e06a2:	2001      	movs	r0, #1
      context->ReportError(
          context, "Only float32 and uint8_t supported currently, got %d.",
          input->type);
      return kTfLiteError;
  }
}
   e06a4:	b01f      	add	sp, #124	; 0x7c
   e06a6:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}
   e06aa:	bf00      	nop
   e06ac:	000eb18d 	.word	0x000eb18d
   e06b0:	000ea400 	.word	0x000ea400
   e06b4:	000ea357 	.word	0x000ea357
   e06b8:	000e98c8 	.word	0x000e98c8
   e06bc:	3b800000 	.word	0x3b800000
   e06c0:	000ea41a 	.word	0x000ea41a
   e06c4:	000e9a98 	.word	0x000e9a98
   e06c8:	000ea43c 	.word	0x000ea43c
   e06cc:	000ea475 	.word	0x000ea475
   e06d0:	000ea4aa 	.word	0x000ea4aa

000e06d4 <_ZN6tflite3ops5micro5split7PrepareEP13TfLiteContextP10TfLiteNode>:
namespace micro {
namespace split {

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   e06d4:	2000      	movs	r0, #0
   e06d6:	4770      	bx	lr

000e06d8 <_ZN6tflite3ops5micro14Register_SPLITEv>:
}  // namespace split

TfLiteRegistration* Register_SPLIT() {
  static TfLiteRegistration r = {nullptr, nullptr, split::Prepare, split::Eval};
  return &r;
}
   e06d8:	4800      	ldr	r0, [pc, #0]	; (e06dc <_ZN6tflite3ops5micro14Register_SPLITEv+0x4>)
   e06da:	4770      	bx	lr
   e06dc:	2003c2fc 	.word	0x2003c2fc

000e06e0 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>:
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e06e0:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e06e4:	684c      	ldr	r4, [r1, #4]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
   e06e6:	6896      	ldr	r6, [r2, #8]
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e06e8:	6865      	ldr	r5, [r4, #4]

  const int split_dimensions = input_dims->size;
   e06ea:	f8d6 c000 	ldr.w	ip, [r6]
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
   e06ee:	6827      	ldr	r7, [r4, #0]
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e06f0:	6884      	ldr	r4, [r0, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e06f2:	2b00      	cmp	r3, #0
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e06f4:	f04f 0e38 	mov.w	lr, #56	; 0x38
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e06f8:	b085      	sub	sp, #20
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e06fa:	fb0e 4405 	mla	r4, lr, r5, r4

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e06fe:	bfb8      	it	lt
   e0700:	4463      	addlt	r3, ip

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e0702:	4563      	cmp	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e0704:	9102      	str	r1, [sp, #8]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0706:	68a4      	ldr	r4, [r4, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e0708:	db01      	blt.n	e070e <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2e>
   e070a:	f003 fe1f 	bl	e434c <abort>
  TFLITE_DCHECK_EQ(output_dims->size, split_dimensions);
   e070e:	6825      	ldr	r5, [r4, #0]
   e0710:	45ac      	cmp	ip, r5
   e0712:	d1fa      	bne.n	e070a <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e0714:	009d      	lsls	r5, r3, #2
   e0716:	eb04 0e05 	add.w	lr, r4, r5

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
   e071a:	4435      	add	r5, r6
   e071c:	f8de 4004 	ldr.w	r4, [lr, #4]
   e0720:	686d      	ldr	r5, [r5, #4]
   e0722:	437c      	muls	r4, r7
   e0724:	42ac      	cmp	r4, r5
   e0726:	d1f0      	bne.n	e070a <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e0728:	2401      	movs	r4, #1
   e072a:	2500      	movs	r5, #0
   e072c:	e9cd 4500 	strd	r4, r5, [sp]
   e0730:	46b2      	mov	sl, r6
   e0732:	f04f 0800 	mov.w	r8, #0
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e0736:	4598      	cmp	r8, r3
   e0738:	da14      	bge.n	e0764 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x84>
    outer_size *= input_dims->data[i];
   e073a:	f85a 9f04 	ldr.w	r9, [sl, #4]!
   e073e:	9900      	ldr	r1, [sp, #0]
   e0740:	464c      	mov	r4, r9
   e0742:	17e5      	asrs	r5, r4, #31
   e0744:	fb01 f405 	mul.w	r4, r1, r5
   e0748:	9901      	ldr	r1, [sp, #4]
   e074a:	fb09 4b01 	mla	fp, r9, r1, r4
   e074e:	9900      	ldr	r1, [sp, #0]
   e0750:	fba1 4509 	umull	r4, r5, r1, r9
   e0754:	e9cd 4500 	strd	r4, r5, [sp]
   e0758:	9901      	ldr	r1, [sp, #4]
   e075a:	4459      	add	r1, fp
   e075c:	9101      	str	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e075e:	f108 0801 	add.w	r8, r8, #1
   e0762:	e7e8      	b.n	e0736 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x56>
   e0764:	3301      	adds	r3, #1
   e0766:	f10c 0c01 	add.w	ip, ip, #1
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
   e076a:	2401      	movs	r4, #1
   e076c:	2500      	movs	r5, #0
   e076e:	3301      	adds	r3, #1
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e0770:	4563      	cmp	r3, ip
   e0772:	d00b      	beq.n	e078c <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xac>
    base_inner_size *= input_dims->data[i];
   e0774:	f856 a023 	ldr.w	sl, [r6, r3, lsl #2]
   e0778:	ea4f 79ea 	mov.w	r9, sl, asr #31
   e077c:	fb04 f809 	mul.w	r8, r4, r9
   e0780:	fb0a 8805 	mla	r8, sl, r5, r8
   e0784:	fba4 450a 	umull	r4, r5, r4, sl
   e0788:	4445      	add	r5, r8
  for (int i = 0; i < axis; ++i) {
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e078a:	e7f0      	b.n	e076e <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x8e>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e078c:	f8d2 c004 	ldr.w	ip, [r2, #4]
   e0790:	f04f 0800 	mov.w	r8, #0
   e0794:	f04f 0900 	mov.w	r9, #0
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e0798:	e9dd 2300 	ldrd	r2, r3, [sp]
   e079c:	4590      	cmp	r8, r2
   e079e:	eb79 0303 	sbcs.w	r3, r9, r3
   e07a2:	f8cd 800c 	str.w	r8, [sp, #12]
   e07a6:	da2a      	bge.n	e07fe <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x11e>
   e07a8:	2600      	movs	r6, #0
    for (int i = 0; i < output_count; ++i) {
   e07aa:	42be      	cmp	r6, r7
   e07ac:	da22      	bge.n	e07f4 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x114>
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e07ae:	9b02      	ldr	r3, [sp, #8]
   e07b0:	685b      	ldr	r3, [r3, #4]
   e07b2:	eb03 0386 	add.w	r3, r3, r6, lsl #2
   e07b6:	2138      	movs	r1, #56	; 0x38
   e07b8:	685a      	ldr	r2, [r3, #4]
   e07ba:	6883      	ldr	r3, [r0, #8]
   e07bc:	fb01 3302 	mla	r3, r1, r2, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e07c0:	b103      	cbz	r3, e07c4 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xe4>
   e07c2:	685b      	ldr	r3, [r3, #4]
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
   e07c4:	f8de 2004 	ldr.w	r2, [lr, #4]
   e07c8:	9903      	ldr	r1, [sp, #12]
   e07ca:	4362      	muls	r2, r4
   e07cc:	fb02 fb01 	mul.w	fp, r2, r1
   e07d0:	eb03 038b 	add.w	r3, r3, fp, lsl #2
   e07d4:	46e2      	mov	sl, ip
      T* output_ptr = output_data + k * copy_size;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e07d6:	f04f 0b00 	mov.w	fp, #0
   e07da:	4593      	cmp	fp, r2
   e07dc:	da06      	bge.n	e07ec <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x10c>
   e07de:	ecfa 7a01 	vldmia	sl!, {s15}
   e07e2:	f10b 0b01 	add.w	fp, fp, #1
   e07e6:	ece3 7a01 	vstmia	r3!, {s15}
   e07ea:	e7f6      	b.n	e07da <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xfa>
      input_ptr += copy_size;
   e07ec:	eb0c 0c82 	add.w	ip, ip, r2, lsl #2
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
   e07f0:	3601      	adds	r6, #1
   e07f2:	e7da      	b.n	e07aa <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xca>
   e07f4:	f118 0801 	adds.w	r8, r8, #1
   e07f8:	f149 0900 	adc.w	r9, r9, #0
  for (int i = axis + 1; i < split_dimensions; ++i) {
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e07fc:	e7cc      	b.n	e0798 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xb8>
      input_ptr += copy_size;
    }
  }

  return kTfLiteOk;
}
   e07fe:	2000      	movs	r0, #0
   e0800:	b005      	add	sp, #20
   e0802:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000e0806 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>:
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e0806:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e080a:	684c      	ldr	r4, [r1, #4]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
   e080c:	6896      	ldr	r6, [r2, #8]
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e080e:	6865      	ldr	r5, [r4, #4]

  const int split_dimensions = input_dims->size;
   e0810:	f8d6 c000 	ldr.w	ip, [r6]
   e0814:	6827      	ldr	r7, [r4, #0]
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0816:	6884      	ldr	r4, [r0, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e0818:	2b00      	cmp	r3, #0
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e081a:	f04f 0e38 	mov.w	lr, #56	; 0x38
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e081e:	b085      	sub	sp, #20
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0820:	fb0e 4405 	mla	r4, lr, r5, r4

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e0824:	bfb8      	it	lt
   e0826:	4463      	addlt	r3, ip

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e0828:	4563      	cmp	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e082a:	9102      	str	r1, [sp, #8]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e082c:	68a4      	ldr	r4, [r4, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e082e:	db01      	blt.n	e0834 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2e>
   e0830:	f003 fd8c 	bl	e434c <abort>
  TFLITE_DCHECK_EQ(output_dims->size, split_dimensions);
   e0834:	6825      	ldr	r5, [r4, #0]
   e0836:	45ac      	cmp	ip, r5
   e0838:	d1fa      	bne.n	e0830 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e083a:	009d      	lsls	r5, r3, #2
   e083c:	eb04 0e05 	add.w	lr, r4, r5

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
   e0840:	4435      	add	r5, r6
   e0842:	f8de 4004 	ldr.w	r4, [lr, #4]
   e0846:	686d      	ldr	r5, [r5, #4]
   e0848:	437c      	muls	r4, r7
   e084a:	42ac      	cmp	r4, r5
   e084c:	d1f0      	bne.n	e0830 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e084e:	2401      	movs	r4, #1
   e0850:	2500      	movs	r5, #0
   e0852:	e9cd 4500 	strd	r4, r5, [sp]
   e0856:	46b2      	mov	sl, r6
   e0858:	f04f 0800 	mov.w	r8, #0
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e085c:	4598      	cmp	r8, r3
   e085e:	da14      	bge.n	e088a <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x84>
    outer_size *= input_dims->data[i];
   e0860:	f85a 9f04 	ldr.w	r9, [sl, #4]!
   e0864:	9900      	ldr	r1, [sp, #0]
   e0866:	464c      	mov	r4, r9
   e0868:	17e5      	asrs	r5, r4, #31
   e086a:	fb01 f405 	mul.w	r4, r1, r5
   e086e:	9901      	ldr	r1, [sp, #4]
   e0870:	fb09 4b01 	mla	fp, r9, r1, r4
   e0874:	9900      	ldr	r1, [sp, #0]
   e0876:	fba1 4509 	umull	r4, r5, r1, r9
   e087a:	e9cd 4500 	strd	r4, r5, [sp]
   e087e:	9901      	ldr	r1, [sp, #4]
   e0880:	4459      	add	r1, fp
   e0882:	9101      	str	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e0884:	f108 0801 	add.w	r8, r8, #1
   e0888:	e7e8      	b.n	e085c <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x56>
   e088a:	3301      	adds	r3, #1
   e088c:	f10c 0c01 	add.w	ip, ip, #1
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
   e0890:	2401      	movs	r4, #1
   e0892:	2500      	movs	r5, #0
   e0894:	3301      	adds	r3, #1
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e0896:	4563      	cmp	r3, ip
   e0898:	d00b      	beq.n	e08b2 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xac>
    base_inner_size *= input_dims->data[i];
   e089a:	f856 a023 	ldr.w	sl, [r6, r3, lsl #2]
   e089e:	ea4f 79ea 	mov.w	r9, sl, asr #31
   e08a2:	fb04 f809 	mul.w	r8, r4, r9
   e08a6:	fb0a 8805 	mla	r8, sl, r5, r8
   e08aa:	fba4 450a 	umull	r4, r5, r4, sl
   e08ae:	4445      	add	r5, r8
  for (int i = 0; i < axis; ++i) {
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e08b0:	e7f0      	b.n	e0894 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x8e>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e08b2:	6856      	ldr	r6, [r2, #4]
   e08b4:	f04f 0800 	mov.w	r8, #0
   e08b8:	f04f 0900 	mov.w	r9, #0
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e08bc:	e9dd 2300 	ldrd	r2, r3, [sp]
   e08c0:	4590      	cmp	r8, r2
   e08c2:	eb79 0303 	sbcs.w	r3, r9, r3
   e08c6:	f8cd 800c 	str.w	r8, [sp, #12]
   e08ca:	da27      	bge.n	e091c <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x116>
   e08cc:	f04f 0c00 	mov.w	ip, #0
    for (int i = 0; i < output_count; ++i) {
   e08d0:	45bc      	cmp	ip, r7
   e08d2:	da1e      	bge.n	e0912 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x10c>
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e08d4:	9b02      	ldr	r3, [sp, #8]
   e08d6:	685b      	ldr	r3, [r3, #4]
   e08d8:	eb03 038c 	add.w	r3, r3, ip, lsl #2
   e08dc:	2138      	movs	r1, #56	; 0x38
   e08de:	685a      	ldr	r2, [r3, #4]
   e08e0:	6883      	ldr	r3, [r0, #8]
   e08e2:	fb01 3302 	mla	r3, r1, r2, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e08e6:	b103      	cbz	r3, e08ea <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xe4>
   e08e8:	685b      	ldr	r3, [r3, #4]
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
   e08ea:	f8de 2004 	ldr.w	r2, [lr, #4]
   e08ee:	9903      	ldr	r1, [sp, #12]
   e08f0:	4362      	muls	r2, r4
   e08f2:	fb02 3301 	mla	r3, r2, r1, r3
      T* output_ptr = output_data + k * copy_size;
   e08f6:	46b2      	mov	sl, r6
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e08f8:	ebc6 0b0a 	rsb	fp, r6, sl
   e08fc:	4593      	cmp	fp, r2
   e08fe:	da04      	bge.n	e090a <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x104>
   e0900:	f81a bb01 	ldrb.w	fp, [sl], #1
   e0904:	f803 bb01 	strb.w	fp, [r3], #1
   e0908:	e7f6      	b.n	e08f8 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xf2>
      input_ptr += copy_size;
   e090a:	4416      	add	r6, r2
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
   e090c:	f10c 0c01 	add.w	ip, ip, #1
   e0910:	e7de      	b.n	e08d0 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xca>
   e0912:	f118 0801 	adds.w	r8, r8, #1
   e0916:	f149 0900 	adc.w	r9, r9, #0
  for (int i = axis + 1; i < split_dimensions; ++i) {
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e091a:	e7cf      	b.n	e08bc <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xb6>
      input_ptr += copy_size;
    }
  }

  return kTfLiteOk;
}
   e091c:	2000      	movs	r0, #0
   e091e:	b005      	add	sp, #20
   e0920:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000e0924 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>:
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e0924:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e0928:	684c      	ldr	r4, [r1, #4]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
   e092a:	6896      	ldr	r6, [r2, #8]
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e092c:	6865      	ldr	r5, [r4, #4]

  const int split_dimensions = input_dims->size;
   e092e:	f8d6 c000 	ldr.w	ip, [r6]
   e0932:	6827      	ldr	r7, [r4, #0]
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0934:	6884      	ldr	r4, [r0, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e0936:	2b00      	cmp	r3, #0
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0938:	f04f 0e38 	mov.w	lr, #56	; 0x38
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e093c:	b085      	sub	sp, #20
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e093e:	fb0e 4405 	mla	r4, lr, r5, r4

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e0942:	bfb8      	it	lt
   e0944:	4463      	addlt	r3, ip

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e0946:	4563      	cmp	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e0948:	9102      	str	r1, [sp, #8]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e094a:	68a4      	ldr	r4, [r4, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e094c:	db01      	blt.n	e0952 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2e>
   e094e:	f003 fcfd 	bl	e434c <abort>
  TFLITE_DCHECK_EQ(output_dims->size, split_dimensions);
   e0952:	6825      	ldr	r5, [r4, #0]
   e0954:	45ac      	cmp	ip, r5
   e0956:	d1fa      	bne.n	e094e <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e0958:	009d      	lsls	r5, r3, #2
   e095a:	eb04 0e05 	add.w	lr, r4, r5

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
   e095e:	4435      	add	r5, r6
   e0960:	f8de 4004 	ldr.w	r4, [lr, #4]
   e0964:	686d      	ldr	r5, [r5, #4]
   e0966:	437c      	muls	r4, r7
   e0968:	42ac      	cmp	r4, r5
   e096a:	d1f0      	bne.n	e094e <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e096c:	2401      	movs	r4, #1
   e096e:	2500      	movs	r5, #0
   e0970:	e9cd 4500 	strd	r4, r5, [sp]
   e0974:	46b2      	mov	sl, r6
   e0976:	f04f 0800 	mov.w	r8, #0
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e097a:	4598      	cmp	r8, r3
   e097c:	da14      	bge.n	e09a8 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x84>
    outer_size *= input_dims->data[i];
   e097e:	f85a 9f04 	ldr.w	r9, [sl, #4]!
   e0982:	9900      	ldr	r1, [sp, #0]
   e0984:	464c      	mov	r4, r9
   e0986:	17e5      	asrs	r5, r4, #31
   e0988:	fb01 f405 	mul.w	r4, r1, r5
   e098c:	9901      	ldr	r1, [sp, #4]
   e098e:	fb09 4b01 	mla	fp, r9, r1, r4
   e0992:	9900      	ldr	r1, [sp, #0]
   e0994:	fba1 4509 	umull	r4, r5, r1, r9
   e0998:	e9cd 4500 	strd	r4, r5, [sp]
   e099c:	9901      	ldr	r1, [sp, #4]
   e099e:	4459      	add	r1, fp
   e09a0:	9101      	str	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e09a2:	f108 0801 	add.w	r8, r8, #1
   e09a6:	e7e8      	b.n	e097a <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x56>
   e09a8:	3301      	adds	r3, #1
   e09aa:	f10c 0c01 	add.w	ip, ip, #1
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
   e09ae:	2401      	movs	r4, #1
   e09b0:	2500      	movs	r5, #0
   e09b2:	3301      	adds	r3, #1
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e09b4:	4563      	cmp	r3, ip
   e09b6:	d00b      	beq.n	e09d0 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xac>
    base_inner_size *= input_dims->data[i];
   e09b8:	f856 a023 	ldr.w	sl, [r6, r3, lsl #2]
   e09bc:	ea4f 79ea 	mov.w	r9, sl, asr #31
   e09c0:	fb04 f809 	mul.w	r8, r4, r9
   e09c4:	fb0a 8805 	mla	r8, sl, r5, r8
   e09c8:	fba4 450a 	umull	r4, r5, r4, sl
   e09cc:	4445      	add	r5, r8
  for (int i = 0; i < axis; ++i) {
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e09ce:	e7f0      	b.n	e09b2 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x8e>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e09d0:	6856      	ldr	r6, [r2, #4]
   e09d2:	f04f 0800 	mov.w	r8, #0
   e09d6:	f04f 0900 	mov.w	r9, #0
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e09da:	e9dd 2300 	ldrd	r2, r3, [sp]
   e09de:	4590      	cmp	r8, r2
   e09e0:	eb79 0303 	sbcs.w	r3, r9, r3
   e09e4:	f8cd 800c 	str.w	r8, [sp, #12]
   e09e8:	da27      	bge.n	e0a3a <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x116>
   e09ea:	f04f 0c00 	mov.w	ip, #0
    for (int i = 0; i < output_count; ++i) {
   e09ee:	45bc      	cmp	ip, r7
   e09f0:	da1e      	bge.n	e0a30 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x10c>
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e09f2:	9b02      	ldr	r3, [sp, #8]
   e09f4:	685b      	ldr	r3, [r3, #4]
   e09f6:	eb03 038c 	add.w	r3, r3, ip, lsl #2
   e09fa:	2138      	movs	r1, #56	; 0x38
   e09fc:	685a      	ldr	r2, [r3, #4]
   e09fe:	6883      	ldr	r3, [r0, #8]
   e0a00:	fb01 3302 	mla	r3, r1, r2, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e0a04:	b103      	cbz	r3, e0a08 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xe4>
   e0a06:	685b      	ldr	r3, [r3, #4]
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
   e0a08:	f8de 2004 	ldr.w	r2, [lr, #4]
   e0a0c:	9903      	ldr	r1, [sp, #12]
   e0a0e:	4362      	muls	r2, r4
   e0a10:	fb02 3301 	mla	r3, r2, r1, r3
      T* output_ptr = output_data + k * copy_size;
   e0a14:	46b2      	mov	sl, r6
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e0a16:	ebc6 0b0a 	rsb	fp, r6, sl
   e0a1a:	4593      	cmp	fp, r2
   e0a1c:	da04      	bge.n	e0a28 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x104>
   e0a1e:	f91a bb01 	ldrsb.w	fp, [sl], #1
   e0a22:	f803 bb01 	strb.w	fp, [r3], #1
   e0a26:	e7f6      	b.n	e0a16 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xf2>
      input_ptr += copy_size;
   e0a28:	4416      	add	r6, r2
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
   e0a2a:	f10c 0c01 	add.w	ip, ip, #1
   e0a2e:	e7de      	b.n	e09ee <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xca>
   e0a30:	f118 0801 	adds.w	r8, r8, #1
   e0a34:	f149 0900 	adc.w	r9, r9, #0
  for (int i = axis + 1; i < split_dimensions; ++i) {
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e0a38:	e7cf      	b.n	e09da <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xb6>
      input_ptr += copy_size;
    }
  }

  return kTfLiteOk;
}
   e0a3a:	2000      	movs	r0, #0
   e0a3c:	b005      	add	sp, #20
   e0a3e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000e0a42 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>:
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e0a42:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e0a46:	684c      	ldr	r4, [r1, #4]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
   e0a48:	6896      	ldr	r6, [r2, #8]
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0a4a:	6865      	ldr	r5, [r4, #4]

  const int split_dimensions = input_dims->size;
   e0a4c:	f8d6 c000 	ldr.w	ip, [r6]
   e0a50:	6827      	ldr	r7, [r4, #0]
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0a52:	6884      	ldr	r4, [r0, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e0a54:	2b00      	cmp	r3, #0
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0a56:	f04f 0e38 	mov.w	lr, #56	; 0x38
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e0a5a:	b085      	sub	sp, #20
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0a5c:	fb0e 4405 	mla	r4, lr, r5, r4

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e0a60:	bfb8      	it	lt
   e0a62:	4463      	addlt	r3, ip

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e0a64:	4563      	cmp	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e0a66:	9102      	str	r1, [sp, #8]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0a68:	68a4      	ldr	r4, [r4, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e0a6a:	db01      	blt.n	e0a70 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2e>
   e0a6c:	f003 fc6e 	bl	e434c <abort>
  TFLITE_DCHECK_EQ(output_dims->size, split_dimensions);
   e0a70:	6825      	ldr	r5, [r4, #0]
   e0a72:	45ac      	cmp	ip, r5
   e0a74:	d1fa      	bne.n	e0a6c <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e0a76:	009d      	lsls	r5, r3, #2
   e0a78:	eb04 0e05 	add.w	lr, r4, r5

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
   e0a7c:	4435      	add	r5, r6
   e0a7e:	f8de 4004 	ldr.w	r4, [lr, #4]
   e0a82:	686d      	ldr	r5, [r5, #4]
   e0a84:	437c      	muls	r4, r7
   e0a86:	42ac      	cmp	r4, r5
   e0a88:	d1f0      	bne.n	e0a6c <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e0a8a:	2401      	movs	r4, #1
   e0a8c:	2500      	movs	r5, #0
   e0a8e:	e9cd 4500 	strd	r4, r5, [sp]
   e0a92:	46b2      	mov	sl, r6
   e0a94:	f04f 0800 	mov.w	r8, #0
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e0a98:	4598      	cmp	r8, r3
   e0a9a:	da14      	bge.n	e0ac6 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x84>
    outer_size *= input_dims->data[i];
   e0a9c:	f85a 9f04 	ldr.w	r9, [sl, #4]!
   e0aa0:	9900      	ldr	r1, [sp, #0]
   e0aa2:	464c      	mov	r4, r9
   e0aa4:	17e5      	asrs	r5, r4, #31
   e0aa6:	fb01 f405 	mul.w	r4, r1, r5
   e0aaa:	9901      	ldr	r1, [sp, #4]
   e0aac:	fb09 4b01 	mla	fp, r9, r1, r4
   e0ab0:	9900      	ldr	r1, [sp, #0]
   e0ab2:	fba1 4509 	umull	r4, r5, r1, r9
   e0ab6:	e9cd 4500 	strd	r4, r5, [sp]
   e0aba:	9901      	ldr	r1, [sp, #4]
   e0abc:	4459      	add	r1, fp
   e0abe:	9101      	str	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e0ac0:	f108 0801 	add.w	r8, r8, #1
   e0ac4:	e7e8      	b.n	e0a98 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x56>
   e0ac6:	3301      	adds	r3, #1
   e0ac8:	f10c 0c01 	add.w	ip, ip, #1
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
   e0acc:	2401      	movs	r4, #1
   e0ace:	2500      	movs	r5, #0
   e0ad0:	3301      	adds	r3, #1
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e0ad2:	4563      	cmp	r3, ip
   e0ad4:	d00b      	beq.n	e0aee <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xac>
    base_inner_size *= input_dims->data[i];
   e0ad6:	f856 a023 	ldr.w	sl, [r6, r3, lsl #2]
   e0ada:	ea4f 79ea 	mov.w	r9, sl, asr #31
   e0ade:	fb04 f809 	mul.w	r8, r4, r9
   e0ae2:	fb0a 8805 	mla	r8, sl, r5, r8
   e0ae6:	fba4 450a 	umull	r4, r5, r4, sl
   e0aea:	4445      	add	r5, r8
  for (int i = 0; i < axis; ++i) {
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e0aec:	e7f0      	b.n	e0ad0 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x8e>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e0aee:	f8d2 c004 	ldr.w	ip, [r2, #4]
   e0af2:	f04f 0800 	mov.w	r8, #0
   e0af6:	f04f 0900 	mov.w	r9, #0
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e0afa:	e9dd 2300 	ldrd	r2, r3, [sp]
   e0afe:	4590      	cmp	r8, r2
   e0b00:	eb79 0303 	sbcs.w	r3, r9, r3
   e0b04:	f8cd 800c 	str.w	r8, [sp, #12]
   e0b08:	da29      	bge.n	e0b5e <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x11c>
   e0b0a:	2600      	movs	r6, #0
    for (int i = 0; i < output_count; ++i) {
   e0b0c:	42be      	cmp	r6, r7
   e0b0e:	da21      	bge.n	e0b54 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x112>
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e0b10:	9b02      	ldr	r3, [sp, #8]
   e0b12:	685b      	ldr	r3, [r3, #4]
   e0b14:	eb03 0386 	add.w	r3, r3, r6, lsl #2
   e0b18:	2138      	movs	r1, #56	; 0x38
   e0b1a:	685a      	ldr	r2, [r3, #4]
   e0b1c:	6883      	ldr	r3, [r0, #8]
   e0b1e:	fb01 3302 	mla	r3, r1, r2, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e0b22:	b103      	cbz	r3, e0b26 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xe4>
   e0b24:	685b      	ldr	r3, [r3, #4]
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
   e0b26:	f8de 2004 	ldr.w	r2, [lr, #4]
      T* output_ptr = output_data + k * copy_size;
   e0b2a:	9903      	ldr	r1, [sp, #12]
  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
   e0b2c:	4362      	muls	r2, r4
      T* output_ptr = output_data + k * copy_size;
   e0b2e:	fb02 fb01 	mul.w	fp, r2, r1
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e0b32:	f04f 0a00 	mov.w	sl, #0
   e0b36:	eb03 034b 	add.w	r3, r3, fp, lsl #1
   e0b3a:	4592      	cmp	sl, r2
   e0b3c:	da06      	bge.n	e0b4c <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x10a>
   e0b3e:	f93c b01a 	ldrsh.w	fp, [ip, sl, lsl #1]
   e0b42:	f823 b01a 	strh.w	fp, [r3, sl, lsl #1]
   e0b46:	f10a 0a01 	add.w	sl, sl, #1
   e0b4a:	e7f6      	b.n	e0b3a <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xf8>
      input_ptr += copy_size;
   e0b4c:	eb0c 0c42 	add.w	ip, ip, r2, lsl #1
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
   e0b50:	3601      	adds	r6, #1
   e0b52:	e7db      	b.n	e0b0c <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xca>
   e0b54:	f118 0801 	adds.w	r8, r8, #1
   e0b58:	f149 0900 	adc.w	r9, r9, #0
  for (int i = axis + 1; i < split_dimensions; ++i) {
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e0b5c:	e7cd      	b.n	e0afa <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xb8>
      input_ptr += copy_size;
    }
  }

  return kTfLiteOk;
}
   e0b5e:	2000      	movs	r0, #0
   e0b60:	b005      	add	sp, #20
   e0b62:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000e0b66 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>:
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e0b66:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e0b6a:	684c      	ldr	r4, [r1, #4]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
   e0b6c:	6896      	ldr	r6, [r2, #8]
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0b6e:	6865      	ldr	r5, [r4, #4]

  const int split_dimensions = input_dims->size;
   e0b70:	f8d6 c000 	ldr.w	ip, [r6]
   e0b74:	6827      	ldr	r7, [r4, #0]
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0b76:	6884      	ldr	r4, [r0, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e0b78:	2b00      	cmp	r3, #0
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0b7a:	f04f 0e38 	mov.w	lr, #56	; 0x38
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e0b7e:	b085      	sub	sp, #20
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0b80:	fb0e 4405 	mla	r4, lr, r5, r4

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;
   e0b84:	bfb8      	it	lt
   e0b86:	4463      	addlt	r3, ip

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e0b88:	4563      	cmp	r3, ip
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}

template <typename T>
TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   e0b8a:	9102      	str	r1, [sp, #8]
                       const TfLiteTensor* input, int axis_value) {
  const int output_count = NumOutputs(node);
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* output_dims = output0->dims;
   e0b8c:	68a4      	ldr	r4, [r4, #8]

  const int split_dimensions = input_dims->size;
  int axis = axis_value < 0 ? axis_value + split_dimensions : axis_value;

  TFLITE_DCHECK_LT(axis, split_dimensions);
   e0b8e:	db01      	blt.n	e0b94 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2e>
   e0b90:	f003 fbdc 	bl	e434c <abort>
  TFLITE_DCHECK_EQ(output_dims->size, split_dimensions);
   e0b94:	6825      	ldr	r5, [r4, #0]
   e0b96:	45ac      	cmp	ip, r5
   e0b98:	d1fa      	bne.n	e0b90 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e0b9a:	009d      	lsls	r5, r3, #2
   e0b9c:	eb04 0e05 	add.w	lr, r4, r5

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
   e0ba0:	4435      	add	r5, r6
   e0ba2:	f8de 4004 	ldr.w	r4, [lr, #4]
   e0ba6:	686d      	ldr	r5, [r5, #4]
   e0ba8:	437c      	muls	r4, r7
   e0baa:	42ac      	cmp	r4, r5
   e0bac:	d1f0      	bne.n	e0b90 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x2a>
   e0bae:	2401      	movs	r4, #1
   e0bb0:	2500      	movs	r5, #0
   e0bb2:	e9cd 4500 	strd	r4, r5, [sp]
   e0bb6:	46b2      	mov	sl, r6
   e0bb8:	f04f 0800 	mov.w	r8, #0
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e0bbc:	4598      	cmp	r8, r3
   e0bbe:	da14      	bge.n	e0bea <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x84>
    outer_size *= input_dims->data[i];
   e0bc0:	f85a 9f04 	ldr.w	r9, [sl, #4]!
   e0bc4:	9900      	ldr	r1, [sp, #0]
   e0bc6:	464c      	mov	r4, r9
   e0bc8:	17e5      	asrs	r5, r4, #31
   e0bca:	fb01 f405 	mul.w	r4, r1, r5
   e0bce:	9901      	ldr	r1, [sp, #4]
   e0bd0:	fb09 4b01 	mla	fp, r9, r1, r4
   e0bd4:	9900      	ldr	r1, [sp, #0]
   e0bd6:	fba1 4509 	umull	r4, r5, r1, r9
   e0bda:	e9cd 4500 	strd	r4, r5, [sp]
   e0bde:	9901      	ldr	r1, [sp, #4]
   e0be0:	4459      	add	r1, fp
   e0be2:	9101      	str	r1, [sp, #4]

  int64_t split_size = output_dims->data[axis] * output_count;

  TFLITE_DCHECK_EQ(split_size, input_dims->data[axis]);
  int64_t outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e0be4:	f108 0801 	add.w	r8, r8, #1
   e0be8:	e7e8      	b.n	e0bbc <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x56>
   e0bea:	3301      	adds	r3, #1
   e0bec:	f10c 0c01 	add.w	ip, ip, #1
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
   e0bf0:	2401      	movs	r4, #1
   e0bf2:	2500      	movs	r5, #0
   e0bf4:	3301      	adds	r3, #1
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e0bf6:	4563      	cmp	r3, ip
   e0bf8:	d00b      	beq.n	e0c12 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xac>
    base_inner_size *= input_dims->data[i];
   e0bfa:	f856 a023 	ldr.w	sl, [r6, r3, lsl #2]
   e0bfe:	ea4f 79ea 	mov.w	r9, sl, asr #31
   e0c02:	fb04 f809 	mul.w	r8, r4, r9
   e0c06:	fb0a 8805 	mla	r8, sl, r5, r8
   e0c0a:	fba4 450a 	umull	r4, r5, r4, sl
   e0c0e:	4445      	add	r5, r8
  for (int i = 0; i < axis; ++i) {
    outer_size *= input_dims->data[i];
  }

  int64_t base_inner_size = 1;
  for (int i = axis + 1; i < split_dimensions; ++i) {
   e0c10:	e7f0      	b.n	e0bf4 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x8e>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e0c12:	f8d2 c004 	ldr.w	ip, [r2, #4]
   e0c16:	f04f 0800 	mov.w	r8, #0
   e0c1a:	f04f 0900 	mov.w	r9, #0
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e0c1e:	e9dd 2300 	ldrd	r2, r3, [sp]
   e0c22:	4590      	cmp	r8, r2
   e0c24:	eb79 0303 	sbcs.w	r3, r9, r3
   e0c28:	f8cd 800c 	str.w	r8, [sp, #12]
   e0c2c:	da29      	bge.n	e0c82 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x11c>
   e0c2e:	2600      	movs	r6, #0
    for (int i = 0; i < output_count; ++i) {
   e0c30:	42be      	cmp	r6, r7
   e0c32:	da21      	bge.n	e0c78 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x112>
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e0c34:	9b02      	ldr	r3, [sp, #8]
   e0c36:	685b      	ldr	r3, [r3, #4]
   e0c38:	eb03 0386 	add.w	r3, r3, r6, lsl #2
   e0c3c:	2138      	movs	r1, #56	; 0x38
   e0c3e:	685a      	ldr	r2, [r3, #4]
   e0c40:	6883      	ldr	r3, [r0, #8]
   e0c42:	fb01 3302 	mla	r3, r1, r2, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e0c46:	b103      	cbz	r3, e0c4a <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xe4>
   e0c48:	685b      	ldr	r3, [r3, #4]
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
   e0c4a:	f8de 2004 	ldr.w	r2, [lr, #4]
      T* output_ptr = output_data + k * copy_size;
   e0c4e:	9903      	ldr	r1, [sp, #12]
  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
      TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
      T* output_data = GetTensorData<T>(t);
      const int copy_size = output_dims->data[axis] * base_inner_size;
   e0c50:	4362      	muls	r2, r4
      T* output_ptr = output_data + k * copy_size;
   e0c52:	fb02 fb01 	mul.w	fp, r2, r1
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e0c56:	f04f 0a00 	mov.w	sl, #0
   e0c5a:	eb03 038b 	add.w	r3, r3, fp, lsl #2
   e0c5e:	4592      	cmp	sl, r2
   e0c60:	da06      	bge.n	e0c70 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0x10a>
   e0c62:	f85c 102a 	ldr.w	r1, [ip, sl, lsl #2]
   e0c66:	f843 102a 	str.w	r1, [r3, sl, lsl #2]
   e0c6a:	f10a 0a01 	add.w	sl, sl, #1
   e0c6e:	e7f6      	b.n	e0c5e <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xf8>
      input_ptr += copy_size;
   e0c70:	eb0c 0c82 	add.w	ip, ip, r2, lsl #2
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
    for (int i = 0; i < output_count; ++i) {
   e0c74:	3601      	adds	r6, #1
   e0c76:	e7db      	b.n	e0c30 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xca>
   e0c78:	f118 0801 	adds.w	r8, r8, #1
   e0c7c:	f149 0900 	adc.w	r9, r9, #0
  for (int i = axis + 1; i < split_dimensions; ++i) {
    base_inner_size *= input_dims->data[i];
  }

  const T* input_ptr = GetTensorData<T>(input);
  for (int k = 0; k < outer_size; ++k) {
   e0c80:	e7cd      	b.n	e0c1e <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori+0xb8>
      input_ptr += copy_size;
    }
  }

  return kTfLiteOk;
}
   e0c82:	2000      	movs	r0, #0
   e0c84:	b005      	add	sp, #20
   e0c86:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

000e0c8c <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e0c8c:	e92d 41f3 	stmdb	sp!, {r0, r1, r4, r5, r6, r7, r8, lr}
   e0c90:	680a      	ldr	r2, [r1, #0]
   e0c92:	f8d0 e008 	ldr.w	lr, [r0, #8]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e0c96:	6857      	ldr	r7, [r2, #4]
   e0c98:	2338      	movs	r3, #56	; 0x38
   e0c9a:	fb03 e707 	mla	r7, r3, r7, lr
   e0c9e:	4605      	mov	r5, r0
  const TfLiteTensor* input = GetInput(context, node, 1);

  // Dynamic output tensors are needed if axis tensor is not constant.
  // But Micro doesn't support dynamic memeory allocation, so we only support
  // constant axis tensor for now.
  TF_LITE_ENSURE_MSG(context, IsConstantTensor(axis),
   e0ca0:	f897 8014 	ldrb.w	r8, [r7, #20]
   e0ca4:	f1b8 0f01 	cmp.w	r8, #1
   e0ca8:	d003      	beq.n	e0cb2 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0x26>
   e0caa:	6943      	ldr	r3, [r0, #20]
   e0cac:	4926      	ldr	r1, [pc, #152]	; (e0d48 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xbc>)
   e0cae:	4798      	blx	r3
   e0cb0:	e046      	b.n	e0d40 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb4>
   e0cb2:	6896      	ldr	r6, [r2, #8]
   e0cb4:	435e      	muls	r6, r3
                     "Non constant axis tensor not supported");

  int axis_value = GetTensorData<int32_t>(axis)[0];
   e0cb6:	687b      	ldr	r3, [r7, #4]
   e0cb8:	681b      	ldr	r3, [r3, #0]
   e0cba:	eb0e 0206 	add.w	r2, lr, r6
  if (axis_value < 0) {
   e0cbe:	2b00      	cmp	r3, #0
   e0cc0:	6897      	ldr	r7, [r2, #8]
   e0cc2:	da0a      	bge.n	e0cda <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0x4e>
    axis_value += NumDimensions(input);
  }

  TF_LITE_ENSURE(context, axis_value >= 0);
   e0cc4:	683c      	ldr	r4, [r7, #0]
   e0cc6:	191b      	adds	r3, r3, r4
   e0cc8:	d507      	bpl.n	e0cda <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0x4e>
   e0cca:	4b20      	ldr	r3, [pc, #128]	; (e0d4c <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xc0>)
   e0ccc:	9300      	str	r3, [sp, #0]
   e0cce:	6945      	ldr	r5, [r0, #20]
   e0cd0:	4a1f      	ldr	r2, [pc, #124]	; (e0d50 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xc4>)
   e0cd2:	4920      	ldr	r1, [pc, #128]	; (e0d54 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xc8>)
   e0cd4:	2357      	movs	r3, #87	; 0x57
   e0cd6:	47a8      	blx	r5
   e0cd8:	e032      	b.n	e0d40 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb4>
  TF_LITE_ENSURE(context, axis_value < NumDimensions(input));
   e0cda:	6838      	ldr	r0, [r7, #0]
   e0cdc:	4283      	cmp	r3, r0
   e0cde:	db08      	blt.n	e0cf2 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0x66>
   e0ce0:	4b1d      	ldr	r3, [pc, #116]	; (e0d58 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xcc>)
   e0ce2:	9300      	str	r3, [sp, #0]
   e0ce4:	696c      	ldr	r4, [r5, #20]
   e0ce6:	4a1a      	ldr	r2, [pc, #104]	; (e0d50 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xc4>)
   e0ce8:	491a      	ldr	r1, [pc, #104]	; (e0d54 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xc8>)
   e0cea:	2358      	movs	r3, #88	; 0x58
   e0cec:	4628      	mov	r0, r5
   e0cee:	47a0      	blx	r4
   e0cf0:	e026      	b.n	e0d40 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb4>

  switch (input->type) {
   e0cf2:	f81e 0006 	ldrb.w	r0, [lr, r6]
   e0cf6:	1e44      	subs	r4, r0, #1
   e0cf8:	2c08      	cmp	r4, #8
   e0cfa:	d81a      	bhi.n	e0d32 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xa6>
   e0cfc:	e8df f004 	tbb	[pc, r4]
   e0d00:	19091505 	.word	0x19091505
   e0d04:	19111919 	.word	0x19111919
   e0d08:	0d          	.byte	0x0d
   e0d09:	00          	.byte	0x00
    case kTfLiteFloat32: {
      return SplitImpl<float>(context, node, input, axis_value);
   e0d0a:	4628      	mov	r0, r5
   e0d0c:	f7ff fce8 	bl	e06e0 <_ZN6tflite3ops5micro5split9SplitImplIfEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>
   e0d10:	e017      	b.n	e0d42 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb6>
    }
    case kTfLiteUInt8: {
      return SplitImpl<uint8_t>(context, node, input, axis_value);
   e0d12:	4628      	mov	r0, r5
   e0d14:	f7ff fd77 	bl	e0806 <_ZN6tflite3ops5micro5split9SplitImplIhEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>
   e0d18:	e013      	b.n	e0d42 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb6>
    }
    case kTfLiteInt8: {
      return SplitImpl<int8_t>(context, node, input, axis_value);
   e0d1a:	4628      	mov	r0, r5
   e0d1c:	f7ff fe02 	bl	e0924 <_ZN6tflite3ops5micro5split9SplitImplIaEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>
   e0d20:	e00f      	b.n	e0d42 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb6>
    }
    case kTfLiteInt16: {
      return SplitImpl<int16_t>(context, node, input, axis_value);
   e0d22:	4628      	mov	r0, r5
   e0d24:	f7ff fe8d 	bl	e0a42 <_ZN6tflite3ops5micro5split9SplitImplIsEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>
   e0d28:	e00b      	b.n	e0d42 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb6>
    }
    case kTfLiteInt32: {
      return SplitImpl<int32_t>(context, node, input, axis_value);
   e0d2a:	4628      	mov	r0, r5
   e0d2c:	f7ff ff1b 	bl	e0b66 <_ZN6tflite3ops5micro5split9SplitImplIlEE12TfLiteStatusP13TfLiteContextP10TfLiteNodePK12TfLiteTensori>
   e0d30:	e007      	b.n	e0d42 <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xb6>
    }
    default:
      context->ReportError(context, "Type %s currently not supported.",
   e0d32:	696c      	ldr	r4, [r5, #20]
   e0d34:	f7f3 f9f2 	bl	d411c <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type));
   e0d38:	4908      	ldr	r1, [pc, #32]	; (e0d5c <_ZN6tflite3ops5micro5split4EvalEP13TfLiteContextP10TfLiteNode+0xd0>)
   e0d3a:	4602      	mov	r2, r0
   e0d3c:	4628      	mov	r0, r5
   e0d3e:	47a0      	blx	r4
      return kTfLiteError;
   e0d40:	2001      	movs	r0, #1
  }
#undef TF_LITE_SPLIT

  return kTfLiteOk;
}
   e0d42:	b002      	add	sp, #8
   e0d44:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   e0d48:	000ea53a 	.word	0x000ea53a
   e0d4c:	000ea6af 	.word	0x000ea6af
   e0d50:	000ea608 	.word	0x000ea608
   e0d54:	000e9a98 	.word	0x000e9a98
   e0d58:	000ea6bf 	.word	0x000ea6bf
   e0d5c:	000ea6e1 	.word	0x000ea6e1

000e0d60 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi>:
}

inline void StridedSlicePadIndices(tflite::StridedSliceParams* p,
                                   int dim_count) {
  // Add indices and mask bits to fully include extra dimensions
  TFLITE_CHECK_LE(dim_count, 4);
   e0d60:	2904      	cmp	r1, #4
  if (v < lo) return lo;
  return v;
}

inline void StridedSlicePadIndices(tflite::StridedSliceParams* p,
                                   int dim_count) {
   e0d62:	b570      	push	{r4, r5, r6, lr}
  // Add indices and mask bits to fully include extra dimensions
  TFLITE_CHECK_LE(dim_count, 4);
   e0d64:	dd01      	ble.n	e0d6a <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0xa>
   e0d66:	f003 faf1 	bl	e434c <abort>
  TFLITE_CHECK_GE(dim_count, p->start_indices_count);
   e0d6a:	f990 3000 	ldrsb.w	r3, [r0]
   e0d6e:	428b      	cmp	r3, r1
   e0d70:	dcf9      	bgt.n	e0d66 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x6>
  TFLITE_CHECK_EQ(p->start_indices_count, p->stop_indices_count);
   e0d72:	f990 200a 	ldrsb.w	r2, [r0, #10]
   e0d76:	429a      	cmp	r2, r3
   e0d78:	d1f5      	bne.n	e0d66 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x6>
  TFLITE_CHECK_EQ(p->stop_indices_count, p->strides_count);
   e0d7a:	f990 3014 	ldrsb.w	r3, [r0, #20]
   e0d7e:	4293      	cmp	r3, r2
   e0d80:	d1f1      	bne.n	e0d66 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x6>

  const int pad_count = dim_count - p->start_indices_count;

  // Pad indices at start, so move arrays by pad_count.
  for (int i = p->start_indices_count - 1; i >= 0; --i) {
   e0d82:	1e5d      	subs	r5, r3, #1
   e0d84:	eb00 0443 	add.w	r4, r0, r3, lsl #1
   e0d88:	eb00 0241 	add.w	r2, r0, r1, lsl #1
   e0d8c:	1acb      	subs	r3, r1, r3
   e0d8e:	2d00      	cmp	r5, #0
   e0d90:	db0c      	blt.n	e0dac <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x4c>
    p->strides[i + pad_count] = p->strides[i];
   e0d92:	f9b4 6014 	ldrsh.w	r6, [r4, #20]
   e0d96:	8296      	strh	r6, [r2, #20]
    p->start_indices[i + pad_count] = p->start_indices[i];
   e0d98:	f9b4 6000 	ldrsh.w	r6, [r4]
   e0d9c:	8016      	strh	r6, [r2, #0]
    p->stop_indices[i + pad_count] = p->stop_indices[i];
   e0d9e:	f9b4 600a 	ldrsh.w	r6, [r4, #10]
   e0da2:	8156      	strh	r6, [r2, #10]
  TFLITE_CHECK_EQ(p->stop_indices_count, p->strides_count);

  const int pad_count = dim_count - p->start_indices_count;

  // Pad indices at start, so move arrays by pad_count.
  for (int i = p->start_indices_count - 1; i >= 0; --i) {
   e0da4:	3d01      	subs	r5, #1
   e0da6:	3c02      	subs	r4, #2
   e0da8:	3a02      	subs	r2, #2
   e0daa:	e7f0      	b.n	e0d8e <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x2e>
   e0dac:	2400      	movs	r4, #0
   e0dae:	4602      	mov	r2, r0
    p->strides[i + pad_count] = p->strides[i];
    p->start_indices[i + pad_count] = p->start_indices[i];
    p->stop_indices[i + pad_count] = p->stop_indices[i];
  }
  for (int i = 0; i < pad_count; ++i) {
    p->start_indices[i] = 0;
   e0db0:	4626      	mov	r6, r4
    p->stop_indices[i] = 1;
   e0db2:	2501      	movs	r5, #1
  for (int i = p->start_indices_count - 1; i >= 0; --i) {
    p->strides[i + pad_count] = p->strides[i];
    p->start_indices[i + pad_count] = p->start_indices[i];
    p->stop_indices[i + pad_count] = p->stop_indices[i];
  }
  for (int i = 0; i < pad_count; ++i) {
   e0db4:	429c      	cmp	r4, r3
   e0db6:	f102 0202 	add.w	r2, r2, #2
   e0dba:	da04      	bge.n	e0dc6 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x66>
    p->start_indices[i] = 0;
   e0dbc:	8016      	strh	r6, [r2, #0]
    p->stop_indices[i] = 1;
   e0dbe:	8155      	strh	r5, [r2, #10]
    p->strides[i] = 1;
   e0dc0:	8295      	strh	r5, [r2, #20]
  for (int i = p->start_indices_count - 1; i >= 0; --i) {
    p->strides[i + pad_count] = p->strides[i];
    p->start_indices[i + pad_count] = p->start_indices[i];
    p->stop_indices[i + pad_count] = p->stop_indices[i];
  }
  for (int i = 0; i < pad_count; ++i) {
   e0dc2:	3401      	adds	r4, #1
   e0dc4:	e7f6      	b.n	e0db4 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi+0x54>
    p->stop_indices[i] = 1;
    p->strides[i] = 1;
  }

  // Pad masks with 0s or 1s as required.
  p->shrink_axis_mask <<= pad_count;
   e0dc6:	f9b0 2026 	ldrsh.w	r2, [r0, #38]	; 0x26
   e0dca:	409a      	lsls	r2, r3
   e0dcc:	84c2      	strh	r2, [r0, #38]	; 0x26
  p->ellipsis_mask <<= pad_count;
   e0dce:	f9b0 2020 	ldrsh.w	r2, [r0, #32]
   e0dd2:	409a      	lsls	r2, r3
   e0dd4:	8402      	strh	r2, [r0, #32]
  p->new_axis_mask <<= pad_count;
   e0dd6:	f9b0 2024 	ldrsh.w	r2, [r0, #36]	; 0x24
  p->begin_mask <<= pad_count;
  p->end_mask <<= pad_count;
  p->begin_mask |= (1 << pad_count) - 1;
   e0dda:	2401      	movs	r4, #1
  }

  // Pad masks with 0s or 1s as required.
  p->shrink_axis_mask <<= pad_count;
  p->ellipsis_mask <<= pad_count;
  p->new_axis_mask <<= pad_count;
   e0ddc:	409a      	lsls	r2, r3
  p->begin_mask <<= pad_count;
  p->end_mask <<= pad_count;
  p->begin_mask |= (1 << pad_count) - 1;
   e0dde:	409c      	lsls	r4, r3
  }

  // Pad masks with 0s or 1s as required.
  p->shrink_axis_mask <<= pad_count;
  p->ellipsis_mask <<= pad_count;
  p->new_axis_mask <<= pad_count;
   e0de0:	8482      	strh	r2, [r0, #36]	; 0x24
  p->begin_mask <<= pad_count;
  p->end_mask <<= pad_count;
   e0de2:	f9b0 2022 	ldrsh.w	r2, [r0, #34]	; 0x22
  p->begin_mask |= (1 << pad_count) - 1;
   e0de6:	3c01      	subs	r4, #1
  // Pad masks with 0s or 1s as required.
  p->shrink_axis_mask <<= pad_count;
  p->ellipsis_mask <<= pad_count;
  p->new_axis_mask <<= pad_count;
  p->begin_mask <<= pad_count;
  p->end_mask <<= pad_count;
   e0de8:	fa02 f503 	lsl.w	r5, r2, r3
  p->begin_mask |= (1 << pad_count) - 1;
   e0dec:	b222      	sxth	r2, r4
   e0dee:	f9b0 401e 	ldrsh.w	r4, [r0, #30]
   e0df2:	fa04 f303 	lsl.w	r3, r4, r3
  p->end_mask |= (1 << pad_count) - 1;

  p->start_indices_count = dim_count;
   e0df6:	b249      	sxtb	r1, r1
  p->shrink_axis_mask <<= pad_count;
  p->ellipsis_mask <<= pad_count;
  p->new_axis_mask <<= pad_count;
  p->begin_mask <<= pad_count;
  p->end_mask <<= pad_count;
  p->begin_mask |= (1 << pad_count) - 1;
   e0df8:	4313      	orrs	r3, r2
  p->end_mask |= (1 << pad_count) - 1;
   e0dfa:	432a      	orrs	r2, r5
  p->shrink_axis_mask <<= pad_count;
  p->ellipsis_mask <<= pad_count;
  p->new_axis_mask <<= pad_count;
  p->begin_mask <<= pad_count;
  p->end_mask <<= pad_count;
  p->begin_mask |= (1 << pad_count) - 1;
   e0dfc:	83c3      	strh	r3, [r0, #30]
  p->end_mask |= (1 << pad_count) - 1;
   e0dfe:	8442      	strh	r2, [r0, #34]	; 0x22

  p->start_indices_count = dim_count;
   e0e00:	7001      	strb	r1, [r0, #0]
  p->stop_indices_count = dim_count;
   e0e02:	7281      	strb	r1, [r0, #10]
  p->strides_count = dim_count;
   e0e04:	7501      	strb	r1, [r0, #20]
   e0e06:	bd70      	pop	{r4, r5, r6, pc}

000e0e08 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>:

// Return the index for the first element along that axis. This index will be a
// positive integer between [0, axis_size - 1] that can be used to index
// directly into the data.
inline int StartForAxis(const tflite::StridedSliceParams& params,
                        const RuntimeShape& input_shape, int axis) {
   e0e08:	b510      	push	{r4, lr}
   e0e0a:	4603      	mov	r3, r0
   e0e0c:	4608      	mov	r0, r1
  const auto begin_mask = params.begin_mask;
  const auto* start_indices = params.start_indices;
  const auto* strides = params.strides;
  // Begin with the specified index.
  int start = start_indices[axis];
   e0e0e:	eb03 0142 	add.w	r1, r3, r2, lsl #1

  // begin_mask override
  if (begin_mask & 1 << axis) {
   e0e12:	f9b3 301e 	ldrsh.w	r3, [r3, #30]
                        const RuntimeShape& input_shape, int axis) {
  const auto begin_mask = params.begin_mask;
  const auto* start_indices = params.start_indices;
  const auto* strides = params.strides;
  // Begin with the specified index.
  int start = start_indices[axis];
   e0e16:	f9b1 4002 	ldrsh.w	r4, [r1, #2]

  // begin_mask override
  if (begin_mask & 1 << axis) {
   e0e1a:	4113      	asrs	r3, r2
   e0e1c:	07db      	lsls	r3, r3, #31
   e0e1e:	d507      	bpl.n	e0e30 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi+0x28>
    if (strides[axis] > 0) {
   e0e20:	f9b1 3016 	ldrsh.w	r3, [r1, #22]
   e0e24:	2b00      	cmp	r3, #0
      // clamped below (Note: We could have set them to 0 and axis_size-1, but
      // use lowest() and max() to maintain symmetry with StopForAxis())
      start = std::numeric_limits<int>::lowest();
    } else {
      // Backward iteration - use the last element.
      start = std::numeric_limits<int>::max();
   e0e26:	bfcc      	ite	gt
   e0e28:	f04f 4400 	movgt.w	r4, #2147483648	; 0x80000000
   e0e2c:	f06f 4400 	mvnle.w	r4, #2147483648	; 0x80000000
    }
  }

  // Handle negative indices
  int axis_size = input_shape.Dims(axis);
   e0e30:	4611      	mov	r1, r2
   e0e32:	f7f5 fb19 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  if (start < 0) {
   e0e36:	2c00      	cmp	r4, #0
    start += axis_size;
   e0e38:	bfb8      	it	lt
   e0e3a:	1824      	addlt	r4, r4, r0
namespace tflite {
namespace strided_slice {

// Use until std::clamp() is available from C++17.
inline int Clamp(const int v, const int lo, const int hi) {
  TFLITE_DCHECK(!(hi < lo));
   e0e3c:	3801      	subs	r0, #1
   e0e3e:	d501      	bpl.n	e0e44 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi+0x3c>
   e0e40:	f003 fa84 	bl	e434c <abort>
  if (hi < v) return hi;
   e0e44:	4284      	cmp	r4, r0
   e0e46:	bfd8      	it	le
   e0e48:	ea24 70e4 	bicle.w	r0, r4, r4, asr #31

  // Clamping
  start = Clamp(start, 0, axis_size - 1);

  return start;
}
   e0e4c:	bd10      	pop	{r4, pc}

000e0e4e <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>:
// element. ie. So if you were iterating through all elements of a 1D array of
// size 4, this function would return 4 as the stop, because it is one past the
// "real" indices of 0, 1, 2 & 3.
inline int StopForAxis(const tflite::StridedSliceParams& params,
                       const RuntimeShape& input_shape, int axis,
                       int start_for_axis) {
   e0e4e:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
   e0e50:	4615      	mov	r5, r2
   e0e52:	460f      	mov	r7, r1
  const auto* stop_indices = params.stop_indices;
  const auto* strides = params.strides;

  // Begin with the specified index
  const bool shrink_axis = shrink_axis_mask & (1 << axis);
  int stop = stop_indices[axis];
   e0e54:	eb00 0145 	add.w	r1, r0, r5, lsl #1
// size 4, this function would return 4 as the stop, because it is one past the
// "real" indices of 0, 1, 2 & 3.
inline int StopForAxis(const tflite::StridedSliceParams& params,
                       const RuntimeShape& input_shape, int axis,
                       int start_for_axis) {
  const auto end_mask = params.end_mask;
   e0e58:	f9b0 2022 	ldrsh.w	r2, [r0, #34]	; 0x22
  const auto* stop_indices = params.stop_indices;
  const auto* strides = params.strides;

  // Begin with the specified index
  const bool shrink_axis = shrink_axis_mask & (1 << axis);
  int stop = stop_indices[axis];
   e0e5c:	f9b1 400c 	ldrsh.w	r4, [r1, #12]

  // When shrinking an axis, the end position does not matter (and can be
  // incorrect when negative indexing is used, see Issue #19260). Always use
  // start_for_axis + 1 to generate a length 1 slice, since start_for_axis has
  // already been adjusted for negative indices.
  if (shrink_axis) {
   e0e60:	f9b0 1026 	ldrsh.w	r1, [r0, #38]	; 0x26
   e0e64:	4129      	asrs	r1, r5
   e0e66:	07c9      	lsls	r1, r1, #31
    stop = start_for_axis + 1;
   e0e68:	bf48      	it	mi
   e0e6a:	1c5c      	addmi	r4, r3, #1
  }

  // end_mask override
  if (end_mask & (1 << axis)) {
   e0e6c:	fa42 f305 	asr.w	r3, r2, r5
   e0e70:	07da      	lsls	r2, r3, #31
                       const RuntimeShape& input_shape, int axis,
                       int start_for_axis) {
  const auto end_mask = params.end_mask;
  const auto shrink_axis_mask = params.shrink_axis_mask;
  const auto* stop_indices = params.stop_indices;
  const auto* strides = params.strides;
   e0e72:	f100 0616 	add.w	r6, r0, #22
  if (shrink_axis) {
    stop = start_for_axis + 1;
  }

  // end_mask override
  if (end_mask & (1 << axis)) {
   e0e76:	d507      	bpl.n	e0e88 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii+0x3a>
    if (strides[axis] > 0) {
   e0e78:	f936 3015 	ldrsh.w	r3, [r6, r5, lsl #1]
      // Forward iteration - use the last element. These values will get
      // clamped below
      stop = std::numeric_limits<int>::max();
    } else {
      // Backward iteration - use the first element.
      stop = std::numeric_limits<int>::lowest();
   e0e7c:	2b00      	cmp	r3, #0
   e0e7e:	bfcc      	ite	gt
   e0e80:	f06f 4400 	mvngt.w	r4, #2147483648	; 0x80000000
   e0e84:	f04f 4400 	movle.w	r4, #2147483648	; 0x80000000
    }
  }

  // Handle negative indices
  const int axis_size = input_shape.Dims(axis);
   e0e88:	4629      	mov	r1, r5
   e0e8a:	4638      	mov	r0, r7
   e0e8c:	f7f5 faec 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  }

  // Clamping
  // Because the end index points one past the last element, we need slightly
  // different clamping ranges depending on the direction.
  if (strides[axis] > 0) {
   e0e90:	f936 3015 	ldrsh.w	r3, [r6, r5, lsl #1]
    }
  }

  // Handle negative indices
  const int axis_size = input_shape.Dims(axis);
  if (stop < 0) {
   e0e94:	2c00      	cmp	r4, #0
    stop += axis_size;
   e0e96:	bfb8      	it	lt
   e0e98:	1824      	addlt	r4, r4, r0
  }

  // Clamping
  // Because the end index points one past the last element, we need slightly
  // different clamping ranges depending on the direction.
  if (strides[axis] > 0) {
   e0e9a:	2b00      	cmp	r3, #0
   e0e9c:	dd08      	ble.n	e0eb0 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii+0x62>
namespace tflite {
namespace strided_slice {

// Use until std::clamp() is available from C++17.
inline int Clamp(const int v, const int lo, const int hi) {
  TFLITE_DCHECK(!(hi < lo));
   e0e9e:	2800      	cmp	r0, #0
   e0ea0:	da01      	bge.n	e0ea6 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii+0x58>
   e0ea2:	f003 fa53 	bl	e434c <abort>
  if (hi < v) return hi;
   e0ea6:	4284      	cmp	r4, r0
   e0ea8:	dc09      	bgt.n	e0ebe <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii+0x70>
   e0eaa:	ea24 70e4 	bic.w	r0, r4, r4, asr #31
   e0eae:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
  if (strides[axis] > 0) {
    // Forward iteration
    stop = Clamp(stop, 0, axis_size);
  } else {
    // Backward iteration
    stop = Clamp(stop, -1, axis_size - 1);
   e0eb0:	3801      	subs	r0, #1
namespace tflite {
namespace strided_slice {

// Use until std::clamp() is available from C++17.
inline int Clamp(const int v, const int lo, const int hi) {
  TFLITE_DCHECK(!(hi < lo));
   e0eb2:	1c43      	adds	r3, r0, #1
   e0eb4:	dbf5      	blt.n	e0ea2 <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii+0x54>
  if (hi < v) return hi;
   e0eb6:	4284      	cmp	r4, r0
   e0eb8:	bfd8      	it	le
   e0eba:	ea44 70e4 	orrle.w	r0, r4, r4, asr #31
    // Backward iteration
    stop = Clamp(stop, -1, axis_size - 1);
  }

  return stop;
}
   e0ebe:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}

000e0ec0 <_ZN6tflite3ops5micro13strided_slice19StridedSliceContextC1EP13TfLiteContextP10TfLiteNode>:
constexpr int kEndTensor = 2;
constexpr int kStridesTensor = 3;
constexpr int kOutputTensor = 0;

struct StridedSliceContext {
  StridedSliceContext(TfLiteContext* context, TfLiteNode* node) {
   e0ec0:	b5f0      	push	{r4, r5, r6, r7, lr}
    params = reinterpret_cast<TfLiteStridedSliceParams*>(node->builtin_data);
   e0ec2:	6954      	ldr	r4, [r2, #20]
   e0ec4:	6004      	str	r4, [r0, #0]
   e0ec6:	6814      	ldr	r4, [r2, #0]
   e0ec8:	688d      	ldr	r5, [r1, #8]
   e0eca:	6866      	ldr	r6, [r4, #4]
   e0ecc:	2438      	movs	r4, #56	; 0x38
   e0ece:	fb04 5506 	mla	r5, r4, r6, r5
    input = GetInput(context, node, kInputTensor);
   e0ed2:	6045      	str	r5, [r0, #4]
   e0ed4:	6816      	ldr	r6, [r2, #0]
    begin = GetInput(context, node, kBeginTensor);
   e0ed6:	688f      	ldr	r7, [r1, #8]
   e0ed8:	68b6      	ldr	r6, [r6, #8]
   e0eda:	fb04 7606 	mla	r6, r4, r6, r7
   e0ede:	6086      	str	r6, [r0, #8]
   e0ee0:	6816      	ldr	r6, [r2, #0]
    end = GetInput(context, node, kEndTensor);
   e0ee2:	688f      	ldr	r7, [r1, #8]
   e0ee4:	68f6      	ldr	r6, [r6, #12]
   e0ee6:	fb04 7606 	mla	r6, r4, r6, r7
   e0eea:	60c6      	str	r6, [r0, #12]
   e0eec:	6816      	ldr	r6, [r2, #0]
    strides = GetInput(context, node, kStridesTensor);
   e0eee:	688f      	ldr	r7, [r1, #8]
   e0ef0:	6936      	ldr	r6, [r6, #16]
   e0ef2:	fb04 7606 	mla	r6, r4, r6, r7
   e0ef6:	6106      	str	r6, [r0, #16]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   e0ef8:	6852      	ldr	r2, [r2, #4]
    output = GetOutput(context, node, kOutputTensor);
   e0efa:	6889      	ldr	r1, [r1, #8]
   e0efc:	6852      	ldr	r2, [r2, #4]
   e0efe:	fb04 1402 	mla	r4, r4, r2, r1
   e0f02:	6144      	str	r4, [r0, #20]
#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/c_api_internal.h"

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
   e0f04:	68aa      	ldr	r2, [r5, #8]
   e0f06:	6812      	ldr	r2, [r2, #0]
    dims = NumDimensions(input);
   e0f08:	6182      	str	r2, [r0, #24]
  }
   e0f0a:	bdf0      	pop	{r4, r5, r6, r7, pc}

000e0f0c <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE>:
// This Op only supports 1-4D cases and since we use the reference 4D
// implementation, the 1-3D tensors are mapped to 4D.
const int kMaxDim = 4;

tflite::StridedSliceParams BuildStridedSliceParams(
    StridedSliceContext* op_context) {
   e0f0c:	b570      	push	{r4, r5, r6, lr}
  tflite::StridedSliceParams op_params;
  op_params.start_indices_count = op_context->dims;
   e0f0e:	698e      	ldr	r6, [r1, #24]
  op_params.stop_indices_count = op_context->dims;
  op_params.strides_count = op_context->dims;
   e0f10:	4603      	mov	r3, r0
const int kMaxDim = 4;

tflite::StridedSliceParams BuildStridedSliceParams(
    StridedSliceContext* op_context) {
  tflite::StridedSliceParams op_params;
  op_params.start_indices_count = op_context->dims;
   e0f12:	b272      	sxtb	r2, r6
  op_params.stop_indices_count = op_context->dims;
  op_params.strides_count = op_context->dims;
   e0f14:	f803 2f14 	strb.w	r2, [r3, #20]!
const int kMaxDim = 4;

tflite::StridedSliceParams BuildStridedSliceParams(
    StridedSliceContext* op_context) {
  tflite::StridedSliceParams op_params;
  op_params.start_indices_count = op_context->dims;
   e0f18:	7002      	strb	r2, [r0, #0]
  op_params.stop_indices_count = op_context->dims;
   e0f1a:	7282      	strb	r2, [r0, #10]
  op_params.strides_count = op_context->dims;

  for (int i = 0; i < op_context->dims; ++i) {
   e0f1c:	2400      	movs	r4, #0
   e0f1e:	42b4      	cmp	r4, r6
   e0f20:	da15      	bge.n	e0f4e <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE+0x42>
    op_params.start_indices[i] = GetTensorData<int32_t>(op_context->begin)[i];
   e0f22:	688a      	ldr	r2, [r1, #8]
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e0f24:	b102      	cbz	r2, e0f28 <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE+0x1c>
   e0f26:	6852      	ldr	r2, [r2, #4]
   e0f28:	f852 2024 	ldr.w	r2, [r2, r4, lsl #2]
   e0f2c:	f823 2c12 	strh.w	r2, [r3, #-18]
    op_params.stop_indices[i] = GetTensorData<int32_t>(op_context->end)[i];
   e0f30:	68ca      	ldr	r2, [r1, #12]
   e0f32:	00a5      	lsls	r5, r4, #2
   e0f34:	b102      	cbz	r2, e0f38 <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE+0x2c>
   e0f36:	6852      	ldr	r2, [r2, #4]
   e0f38:	5952      	ldr	r2, [r2, r5]
   e0f3a:	f823 2c08 	strh.w	r2, [r3, #-8]
    op_params.strides[i] = GetTensorData<int32_t>(op_context->strides)[i];
   e0f3e:	690a      	ldr	r2, [r1, #16]
   e0f40:	b102      	cbz	r2, e0f44 <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE+0x38>
   e0f42:	6852      	ldr	r2, [r2, #4]
   e0f44:	5952      	ldr	r2, [r2, r5]
   e0f46:	f823 2f02 	strh.w	r2, [r3, #2]!
  tflite::StridedSliceParams op_params;
  op_params.start_indices_count = op_context->dims;
  op_params.stop_indices_count = op_context->dims;
  op_params.strides_count = op_context->dims;

  for (int i = 0; i < op_context->dims; ++i) {
   e0f4a:	3401      	adds	r4, #1
   e0f4c:	e7e7      	b.n	e0f1e <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE+0x12>
    op_params.start_indices[i] = GetTensorData<int32_t>(op_context->begin)[i];
    op_params.stop_indices[i] = GetTensorData<int32_t>(op_context->end)[i];
    op_params.strides[i] = GetTensorData<int32_t>(op_context->strides)[i];
  }

  op_params.begin_mask = op_context->params->begin_mask;
   e0f4e:	680b      	ldr	r3, [r1, #0]
   e0f50:	681a      	ldr	r2, [r3, #0]
  op_params.ellipsis_mask = 0;
  op_params.end_mask = op_context->params->end_mask;
   e0f52:	6859      	ldr	r1, [r3, #4]
    op_params.start_indices[i] = GetTensorData<int32_t>(op_context->begin)[i];
    op_params.stop_indices[i] = GetTensorData<int32_t>(op_context->end)[i];
    op_params.strides[i] = GetTensorData<int32_t>(op_context->strides)[i];
  }

  op_params.begin_mask = op_context->params->begin_mask;
   e0f54:	83c2      	strh	r2, [r0, #30]
  op_params.ellipsis_mask = 0;
  op_params.end_mask = op_context->params->end_mask;
  op_params.new_axis_mask = 0;
  op_params.shrink_axis_mask = op_context->params->shrink_axis_mask;
   e0f56:	691b      	ldr	r3, [r3, #16]
    op_params.strides[i] = GetTensorData<int32_t>(op_context->strides)[i];
  }

  op_params.begin_mask = op_context->params->begin_mask;
  op_params.ellipsis_mask = 0;
  op_params.end_mask = op_context->params->end_mask;
   e0f58:	8441      	strh	r1, [r0, #34]	; 0x22
    op_params.stop_indices[i] = GetTensorData<int32_t>(op_context->end)[i];
    op_params.strides[i] = GetTensorData<int32_t>(op_context->strides)[i];
  }

  op_params.begin_mask = op_context->params->begin_mask;
  op_params.ellipsis_mask = 0;
   e0f5a:	2200      	movs	r2, #0
   e0f5c:	8402      	strh	r2, [r0, #32]
  op_params.end_mask = op_context->params->end_mask;
  op_params.new_axis_mask = 0;
   e0f5e:	8482      	strh	r2, [r0, #36]	; 0x24
  op_params.shrink_axis_mask = op_context->params->shrink_axis_mask;
   e0f60:	84c3      	strh	r3, [r0, #38]	; 0x26
  return op_params;
}
   e0f62:	bd70      	pop	{r4, r5, r6, pc}

000e0f64 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE>:

// Processes the indexing tensors (begin, end and strides) to resize the
// output tensor. This function is callable from both Prepare() and Eval() as
// long as the caller ensures the indexing tensors are present.
TfLiteStatus CheckOutputSize(TfLiteContext* context,
                             StridedSliceContext* op_context) {
   e0f64:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e0f68:	ed2d 8b02 	vpush	{d8}
   e0f6c:	460f      	mov	r7, r1
   e0f6e:	b095      	sub	sp, #84	; 0x54
  using ::tflite::strided_slice::StartForAxis;
  using ::tflite::strided_slice::StopForAxis;
  TfLiteIntArray* output_shape = op_context->output->dims;
   e0f70:	694b      	ldr	r3, [r1, #20]

// Processes the indexing tensors (begin, end and strides) to resize the
// output tensor. This function is callable from both Prepare() and Eval() as
// long as the caller ensures the indexing tensors are present.
TfLiteStatus CheckOutputSize(TfLiteContext* context,
                             StridedSliceContext* op_context) {
   e0f72:	4605      	mov	r5, r0
  using ::tflite::strided_slice::StartForAxis;
  using ::tflite::strided_slice::StopForAxis;
  TfLiteIntArray* output_shape = op_context->output->dims;
  int shape_size = 0;
  auto op_params = BuildStridedSliceParams(op_context);
   e0f74:	a80a      	add	r0, sp, #40	; 0x28
// long as the caller ensures the indexing tensors are present.
TfLiteStatus CheckOutputSize(TfLiteContext* context,
                             StridedSliceContext* op_context) {
  using ::tflite::strided_slice::StartForAxis;
  using ::tflite::strided_slice::StopForAxis;
  TfLiteIntArray* output_shape = op_context->output->dims;
   e0f76:	f8d3 8008 	ldr.w	r8, [r3, #8]
  int shape_size = 0;
  auto op_params = BuildStridedSliceParams(op_context);
   e0f7a:	f7ff ffc7 	bl	e0f0c <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE>
  auto input_shape = GetTensorShape(op_context->input);
   e0f7e:	6879      	ldr	r1, [r7, #4]
   e0f80:	a805      	add	r0, sp, #20
   e0f82:	f7f5 fd16 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
TfLiteStatus CheckOutputSize(TfLiteContext* context,
                             StridedSliceContext* op_context) {
  using ::tflite::strided_slice::StartForAxis;
  using ::tflite::strided_slice::StopForAxis;
  TfLiteIntArray* output_shape = op_context->output->dims;
  int shape_size = 0;
   e0f86:	2600      	movs	r6, #0
  auto op_params = BuildStridedSliceParams(op_context);
  auto input_shape = GetTensorShape(op_context->input);
  for (int idx = 0; idx < op_context->dims; ++idx) {
   e0f88:	f8d7 9018 	ldr.w	r9, [r7, #24]
   e0f8c:	4634      	mov	r4, r6
   e0f8e:	454c      	cmp	r4, r9
   e0f90:	da4b      	bge.n	e102a <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xc6>
    int32_t stride = GetTensorData<int32_t>(op_context->strides)[idx];
   e0f92:	693b      	ldr	r3, [r7, #16]
   e0f94:	b103      	cbz	r3, e0f98 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0x34>
   e0f96:	685b      	ldr	r3, [r3, #4]
   e0f98:	f853 3024 	ldr.w	r3, [r3, r4, lsl #2]
   e0f9c:	ee08 3a10 	vmov	s16, r3
    TF_LITE_ENSURE_MSG(context, stride != 0, "stride value has to be non-zero");
   e0fa0:	b923      	cbnz	r3, e0fac <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0x48>
   e0fa2:	696b      	ldr	r3, [r5, #20]
   e0fa4:	492c      	ldr	r1, [pc, #176]	; (e1058 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xf4>)
   e0fa6:	4628      	mov	r0, r5
   e0fa8:	4798      	blx	r3
   e0faa:	e039      	b.n	e1020 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xbc>
    int32_t begin = StartForAxis(op_params, input_shape, idx);
   e0fac:	4622      	mov	r2, r4
   e0fae:	a905      	add	r1, sp, #20
   e0fb0:	a80a      	add	r0, sp, #40	; 0x28
   e0fb2:	f7ff ff29 	bl	e0e08 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
    int32_t end = StopForAxis(op_params, input_shape, idx, begin);
   e0fb6:	4622      	mov	r2, r4
  auto op_params = BuildStridedSliceParams(op_context);
  auto input_shape = GetTensorShape(op_context->input);
  for (int idx = 0; idx < op_context->dims; ++idx) {
    int32_t stride = GetTensorData<int32_t>(op_context->strides)[idx];
    TF_LITE_ENSURE_MSG(context, stride != 0, "stride value has to be non-zero");
    int32_t begin = StartForAxis(op_params, input_shape, idx);
   e0fb8:	4682      	mov	sl, r0
    int32_t end = StopForAxis(op_params, input_shape, idx, begin);
   e0fba:	4603      	mov	r3, r0
   e0fbc:	a905      	add	r1, sp, #20
   e0fbe:	a80a      	add	r0, sp, #40	; 0x28
   e0fc0:	f7ff ff45 	bl	e0e4e <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>

    // When shrinking an axis, the end position does not matter (and can be
    // incorrect when negative indexing is used, see Issue #19260). Always use
    // begin + 1 to generate a length 1 slice, since begin has
    // already been adjusted for negative indices by StartForAxis.
    const bool shrink_axis = op_context->params->shrink_axis_mask & (1 << idx);
   e0fc4:	683b      	ldr	r3, [r7, #0]
   e0fc6:	691a      	ldr	r2, [r3, #16]
   e0fc8:	4122      	asrs	r2, r4
    if (shrink_axis) {
   e0fca:	f012 0b01 	ands.w	fp, r2, #1
      end = begin + 1;
   e0fce:	bf18      	it	ne
   e0fd0:	f10a 0001 	addne.w	r0, sl, #1
  using ::ceil;

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR float
  ceil(float __x)
  { return __builtin_ceilf(__x); }
   e0fd4:	ebca 0000 	rsb	r0, sl, r0
   e0fd8:	ee07 0a90 	vmov	s15, r0
   e0fdc:	eeb8 0ac8 	vcvt.f32.s32	s0, s16
   e0fe0:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   e0fe4:	ee87 0a80 	vdiv.f32	s0, s15, s0
   e0fe8:	f004 f974 	bl	e52d4 <ceilf>
    }

    // This is valid for both positive and negative strides
    int32_t dim_shape = std::ceil((end - begin) / static_cast<float>(stride));
    dim_shape = dim_shape < 0 ? 0 : dim_shape;
    if (!shrink_axis) {
   e0fec:	f1bb 0f00 	cmp.w	fp, #0
   e0ff0:	d119      	bne.n	e1026 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xc2>
      TF_LITE_ENSURE_EQ(context, output_shape->data[shape_size], dim_shape);
   e0ff2:	eb08 0286 	add.w	r2, r8, r6, lsl #2
   e0ff6:	6852      	ldr	r2, [r2, #4]
    if (shrink_axis) {
      end = begin + 1;
    }

    // This is valid for both positive and negative strides
    int32_t dim_shape = std::ceil((end - begin) / static_cast<float>(stride));
   e0ff8:	eebd 0ac0 	vcvt.s32.f32	s0, s0
    dim_shape = dim_shape < 0 ? 0 : dim_shape;
   e0ffc:	ee10 3a10 	vmov	r3, s0
   e1000:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
    if (!shrink_axis) {
      TF_LITE_ENSURE_EQ(context, output_shape->data[shape_size], dim_shape);
   e1004:	4293      	cmp	r3, r2
   e1006:	d00d      	beq.n	e1024 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xc0>
   e1008:	9303      	str	r3, [sp, #12]
   e100a:	4b14      	ldr	r3, [pc, #80]	; (e105c <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xf8>)
   e100c:	9301      	str	r3, [sp, #4]
   e100e:	696c      	ldr	r4, [r5, #20]
   e1010:	4b13      	ldr	r3, [pc, #76]	; (e1060 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xfc>)
   e1012:	9300      	str	r3, [sp, #0]
   e1014:	9202      	str	r2, [sp, #8]
   e1016:	2373      	movs	r3, #115	; 0x73
   e1018:	4a12      	ldr	r2, [pc, #72]	; (e1064 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0x100>)
   e101a:	4913      	ldr	r1, [pc, #76]	; (e1068 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0x104>)
   e101c:	4628      	mov	r0, r5
   e101e:	47a0      	blx	r4
   e1020:	2401      	movs	r4, #1
   e1022:	e010      	b.n	e1046 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xe2>
      shape_size++;
   e1024:	3601      	adds	r6, #1
  using ::tflite::strided_slice::StopForAxis;
  TfLiteIntArray* output_shape = op_context->output->dims;
  int shape_size = 0;
  auto op_params = BuildStridedSliceParams(op_context);
  auto input_shape = GetTensorShape(op_context->input);
  for (int idx = 0; idx < op_context->dims; ++idx) {
   e1026:	3401      	adds	r4, #1
   e1028:	e7b1      	b.n	e0f8e <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0x2a>
    if (!shrink_axis) {
      TF_LITE_ENSURE_EQ(context, output_shape->data[shape_size], dim_shape);
      shape_size++;
    }
  }
  TF_LITE_ENSURE_EQ(context, output_shape->size, shape_size);
   e102a:	f8d8 3000 	ldr.w	r3, [r8]
   e102e:	42b3      	cmp	r3, r6
   e1030:	d008      	beq.n	e1044 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xe0>
   e1032:	9302      	str	r3, [sp, #8]
   e1034:	4b0d      	ldr	r3, [pc, #52]	; (e106c <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0x108>)
   e1036:	9301      	str	r3, [sp, #4]
   e1038:	4b0d      	ldr	r3, [pc, #52]	; (e1070 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0x10c>)
   e103a:	9300      	str	r3, [sp, #0]
   e103c:	9603      	str	r6, [sp, #12]
   e103e:	696c      	ldr	r4, [r5, #20]
   e1040:	2377      	movs	r3, #119	; 0x77
   e1042:	e7e9      	b.n	e1018 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE+0xb4>
  return kTfLiteOk;
   e1044:	2400      	movs	r4, #0
  using ::tflite::strided_slice::StartForAxis;
  using ::tflite::strided_slice::StopForAxis;
  TfLiteIntArray* output_shape = op_context->output->dims;
  int shape_size = 0;
  auto op_params = BuildStridedSliceParams(op_context);
  auto input_shape = GetTensorShape(op_context->input);
   e1046:	a805      	add	r0, sp, #20
   e1048:	f7f5 fa03 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
      shape_size++;
    }
  }
  TF_LITE_ENSURE_EQ(context, output_shape->size, shape_size);
  return kTfLiteOk;
}
   e104c:	4620      	mov	r0, r4
   e104e:	b015      	add	sp, #84	; 0x54
   e1050:	ecbd 8b02 	vpop	{d8}
   e1054:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e1058:	000ea702 	.word	0x000ea702
   e105c:	000ea880 	.word	0x000ea880
   e1060:	000ea88a 	.word	0x000ea88a
   e1064:	000ea7d1 	.word	0x000ea7d1
   e1068:	000e98c8 	.word	0x000e98c8
   e106c:	000ea8a9 	.word	0x000ea8a9
   e1070:	000ea8b4 	.word	0x000ea8b4

000e1074 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   e1074:	b570      	push	{r4, r5, r6, lr}
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
   e1076:	680b      	ldr	r3, [r1, #0]
   e1078:	681b      	ldr	r3, [r3, #0]
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 4);
   e107a:	2b04      	cmp	r3, #4
  }
  TF_LITE_ENSURE_EQ(context, output_shape->size, shape_size);
  return kTfLiteOk;
}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   e107c:	b08c      	sub	sp, #48	; 0x30
   e107e:	4605      	mov	r5, r0
   e1080:	460a      	mov	r2, r1
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 4);
   e1082:	d00d      	beq.n	e10a0 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x2c>
   e1084:	9302      	str	r3, [sp, #8]
   e1086:	4b16      	ldr	r3, [pc, #88]	; (e10e0 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x6c>)
   e1088:	9301      	str	r3, [sp, #4]
   e108a:	2204      	movs	r2, #4
   e108c:	4b15      	ldr	r3, [pc, #84]	; (e10e4 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x70>)
   e108e:	9300      	str	r3, [sp, #0]
   e1090:	9203      	str	r2, [sp, #12]
   e1092:	6944      	ldr	r4, [r0, #20]
   e1094:	237c      	movs	r3, #124	; 0x7c
   e1096:	4a14      	ldr	r2, [pc, #80]	; (e10e8 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x74>)
   e1098:	4914      	ldr	r1, [pc, #80]	; (e10ec <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x78>)
   e109a:	47a0      	blx	r4
   e109c:	2001      	movs	r0, #1
   e109e:	e01d      	b.n	e10dc <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x68>
inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }
   e10a0:	684b      	ldr	r3, [r1, #4]
   e10a2:	681c      	ldr	r4, [r3, #0]
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
   e10a4:	2c01      	cmp	r4, #1
   e10a6:	d009      	beq.n	e10bc <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x48>
   e10a8:	4b11      	ldr	r3, [pc, #68]	; (e10f0 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x7c>)
   e10aa:	9301      	str	r3, [sp, #4]
   e10ac:	2601      	movs	r6, #1
   e10ae:	4b11      	ldr	r3, [pc, #68]	; (e10f4 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x80>)
   e10b0:	9300      	str	r3, [sp, #0]
   e10b2:	9603      	str	r6, [sp, #12]
   e10b4:	9402      	str	r4, [sp, #8]
   e10b6:	6944      	ldr	r4, [r0, #20]
   e10b8:	237d      	movs	r3, #125	; 0x7d
   e10ba:	e7ec      	b.n	e1096 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x22>
  StridedSliceContext op_context(context, node);
   e10bc:	4601      	mov	r1, r0
   e10be:	a805      	add	r0, sp, #20
   e10c0:	f7ff fefe 	bl	e0ec0 <_ZN6tflite3ops5micro13strided_slice19StridedSliceContextC1EP13TfLiteContextP10TfLiteNode>
  TF_LITE_ENSURE_MSG(context, op_context.dims <= kMaxDim,
   e10c4:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e10c6:	2b04      	cmp	r3, #4
   e10c8:	dd04      	ble.n	e10d4 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x60>
   e10ca:	696b      	ldr	r3, [r5, #20]
   e10cc:	490a      	ldr	r1, [pc, #40]	; (e10f8 <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x84>)
   e10ce:	4628      	mov	r0, r5
   e10d0:	4798      	blx	r3
   e10d2:	e7e3      	b.n	e109c <_ZN6tflite3ops5micro13strided_slice7PrepareEP13TfLiteContextP10TfLiteNode+0x28>
                     "input dim should not exceed 4");
  return CheckOutputSize(context, &op_context);
   e10d4:	a905      	add	r1, sp, #20
   e10d6:	4628      	mov	r0, r5
   e10d8:	f7ff ff44 	bl	e0f64 <_ZN6tflite3ops5micro13strided_slice15CheckOutputSizeEP13TfLiteContextPNS2_19StridedSliceContextE>
}
   e10dc:	b00c      	add	sp, #48	; 0x30
   e10de:	bd70      	pop	{r4, r5, r6, pc}
   e10e0:	000ea992 	.word	0x000ea992
   e10e4:	000e98e2 	.word	0x000e98e2
   e10e8:	000ea7d1 	.word	0x000ea7d1
   e10ec:	000e98c8 	.word	0x000e98c8
   e10f0:	000eb295 	.word	0x000eb295
   e10f4:	000e98f2 	.word	0x000e98f2
   e10f8:	000ea8c7 	.word	0x000ea8c7

000e10fc <_ZN6tflite3ops5micro22Register_STRIDED_SLICEEv>:
TfLiteRegistration* Register_STRIDED_SLICE() {
  static TfLiteRegistration r = {
      nullptr, nullptr, strided_slice::Prepare,
      strided_slice::Eval<strided_slice::kReference>};
  return &r;
}
   e10fc:	4800      	ldr	r0, [pc, #0]	; (e1100 <_ZN6tflite3ops5micro22Register_STRIDED_SLICEEv+0x4>)
   e10fe:	4770      	bx	lr
   e1100:	2003c31c 	.word	0x2003c31c

000e1104 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_>:

namespace tflite {

namespace reference_ops {
template <typename T>
inline void StridedSlice(const tflite::StridedSliceParams& op_params,
   e1104:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e1108:	b0a1      	sub	sp, #132	; 0x84
   e110a:	461e      	mov	r6, r3
   e110c:	460f      	mov	r7, r1
   e110e:	920a      	str	r2, [sp, #40]	; 0x28
                         const RuntimeShape& unextended_input_shape,
                         const T* input_data,
                         const RuntimeShape& unextended_output_shape,
                         T* output_data) {
  // Note that the output_shape is not used herein.
  tflite::StridedSliceParams params_copy = op_params;
   e1110:	4603      	mov	r3, r0
   e1112:	ac16      	add	r4, sp, #88	; 0x58
   e1114:	f100 0528 	add.w	r5, r0, #40	; 0x28
   e1118:	6818      	ldr	r0, [r3, #0]
   e111a:	6859      	ldr	r1, [r3, #4]
   e111c:	4622      	mov	r2, r4
   e111e:	c203      	stmia	r2!, {r0, r1}
   e1120:	3308      	adds	r3, #8
   e1122:	42ab      	cmp	r3, r5
   e1124:	4614      	mov	r4, r2
   e1126:	d1f7      	bne.n	e1118 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x14>

  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
   e1128:	683b      	ldr	r3, [r7, #0]
   e112a:	2b04      	cmp	r3, #4
   e112c:	dd01      	ble.n	e1132 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x2e>
   e112e:	f003 f90d 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   e1132:	6833      	ldr	r3, [r6, #0]
   e1134:	2b04      	cmp	r3, #4
   e1136:	dcfa      	bgt.n	e112e <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x2a>
  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be
  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their
  // inputs should already be 4-D, so this function should not be needed.
  inline static RuntimeShape ExtendedShape(int new_shape_size,
                                           const RuntimeShape& shape) {
    return RuntimeShape(new_shape_size, shape, 1);
   e1138:	ad0c      	add	r5, sp, #48	; 0x30
   e113a:	2301      	movs	r3, #1
   e113c:	463a      	mov	r2, r7
   e113e:	2104      	movs	r1, #4
   e1140:	4628      	mov	r0, r5
   e1142:	f7f5 f9ca 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   e1146:	2301      	movs	r3, #1
   e1148:	4632      	mov	r2, r6
   e114a:	2104      	movs	r1, #4
   e114c:	a811      	add	r0, sp, #68	; 0x44
   e114e:	f7f5 f9c4 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  // Reverse and pad to 4 dimensions because that is what the runtime code
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);
   e1152:	2104      	movs	r1, #4
   e1154:	a816      	add	r0, sp, #88	; 0x58
   e1156:	f7ff fe03 	bl	e0d60 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi>

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
   e115a:	2200      	movs	r2, #0
   e115c:	4629      	mov	r1, r5
   e115e:	a816      	add	r0, sp, #88	; 0x58
   e1160:	f7ff fe52 	bl	e0e08 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
   e1164:	2200      	movs	r2, #0
   e1166:	4603      	mov	r3, r0
   e1168:	4629      	mov	r1, r5

  // Reverse and pad to 4 dimensions because that is what the runtime code
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
   e116a:	4604      	mov	r4, r0
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
   e116c:	a816      	add	r0, sp, #88	; 0x58
   e116e:	f7ff fe6e 	bl	e0e4e <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
   e1172:	2201      	movs	r2, #1
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
   e1174:	9003      	str	r0, [sp, #12]
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
   e1176:	4629      	mov	r1, r5
   e1178:	a816      	add	r0, sp, #88	; 0x58
   e117a:	f7ff fe45 	bl	e0e08 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
   e117e:	2201      	movs	r2, #1
   e1180:	4603      	mov	r3, r0
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
   e1182:	9004      	str	r0, [sp, #16]
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
   e1184:	4629      	mov	r1, r5
   e1186:	a816      	add	r0, sp, #88	; 0x58
   e1188:	f7ff fe61 	bl	e0e4e <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
   e118c:	2202      	movs	r2, #2
  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
   e118e:	9005      	str	r0, [sp, #20]
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
   e1190:	4629      	mov	r1, r5
   e1192:	a816      	add	r0, sp, #88	; 0x58
   e1194:	f7ff fe38 	bl	e0e08 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
   e1198:	2202      	movs	r2, #2
   e119a:	4603      	mov	r3, r0
   e119c:	4629      	mov	r1, r5
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
   e119e:	4680      	mov	r8, r0
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
   e11a0:	a816      	add	r0, sp, #88	; 0x58
   e11a2:	f7ff fe54 	bl	e0e4e <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
   e11a6:	2203      	movs	r2, #3
   e11a8:	4629      	mov	r1, r5
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
   e11aa:	4681      	mov	r9, r0
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
   e11ac:	a816      	add	r0, sp, #88	; 0x58
   e11ae:	f7ff fe2b 	bl	e0e08 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
   e11b2:	2203      	movs	r2, #3
   e11b4:	4603      	mov	r3, r0
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
   e11b6:	4682      	mov	sl, r0
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
   e11b8:	4629      	mov	r1, r5
   e11ba:	a816      	add	r0, sp, #88	; 0x58
   e11bc:	f7ff fe47 	bl	e0e4e <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
   e11c0:	f9bd 306e 	ldrsh.w	r3, [sp, #110]	; 0x6e
   e11c4:	9306      	str	r3, [sp, #24]
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
   e11c6:	f9bd 3070 	ldrsh.w	r3, [sp, #112]	; 0x70
   e11ca:	9307      	str	r3, [sp, #28]
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
   e11cc:	f9bd 3072 	ldrsh.w	r3, [sp, #114]	; 0x72
   e11d0:	9308      	str	r3, [sp, #32]
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e11d2:	f9bd 3074 	ldrsh.w	r3, [sp, #116]	; 0x74
   e11d6:	9309      	str	r3, [sp, #36]	; 0x24
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
   e11d8:	4683      	mov	fp, r0
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
   e11da:	950b      	str	r5, [sp, #44]	; 0x2c

inline bool LoopCondition(int index, int stop, int stride) {
  // True when we have reached the end of an axis and should loop.
  return stride > 0 ? index >= stop : index <= stop;
   e11dc:	9b06      	ldr	r3, [sp, #24]
   e11de:	2b00      	cmp	r3, #0
   e11e0:	9b03      	ldr	r3, [sp, #12]
   e11e2:	dd04      	ble.n	e11ee <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xea>
   e11e4:	429c      	cmp	r4, r3
   e11e6:	bfb4      	ite	lt
   e11e8:	2300      	movlt	r3, #0
   e11ea:	2301      	movge	r3, #1
   e11ec:	e003      	b.n	e11f6 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xf2>
   e11ee:	429c      	cmp	r4, r3
   e11f0:	bfcc      	ite	gt
   e11f2:	2300      	movgt	r3, #0
   e11f4:	2301      	movle	r3, #1
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
   e11f6:	2b00      	cmp	r3, #0
   e11f8:	d146      	bne.n	e1288 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x184>
   e11fa:	9d04      	ldr	r5, [sp, #16]
   e11fc:	9b07      	ldr	r3, [sp, #28]
   e11fe:	2b00      	cmp	r3, #0
   e1200:	9b05      	ldr	r3, [sp, #20]
   e1202:	dd04      	ble.n	e120e <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x10a>
   e1204:	429d      	cmp	r5, r3
   e1206:	bfb4      	ite	lt
   e1208:	2300      	movlt	r3, #0
   e120a:	2301      	movge	r3, #1
   e120c:	e003      	b.n	e1216 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x112>
   e120e:	429d      	cmp	r5, r3
   e1210:	bfcc      	ite	gt
   e1212:	2300      	movgt	r3, #0
   e1214:	2301      	movle	r3, #1
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
   e1216:	2b00      	cmp	r3, #0
   e1218:	d133      	bne.n	e1282 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x17e>
   e121a:	4646      	mov	r6, r8
   e121c:	9b08      	ldr	r3, [sp, #32]
   e121e:	2b00      	cmp	r3, #0
   e1220:	dd04      	ble.n	e122c <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x128>
   e1222:	454e      	cmp	r6, r9
   e1224:	bfb4      	ite	lt
   e1226:	2300      	movlt	r3, #0
   e1228:	2301      	movge	r3, #1
   e122a:	e003      	b.n	e1234 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x130>
   e122c:	454e      	cmp	r6, r9
   e122e:	bfcc      	ite	gt
   e1230:	2300      	movgt	r3, #0
   e1232:	2301      	movle	r3, #1
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
   e1234:	bb13      	cbnz	r3, e127c <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x178>
   e1236:	4657      	mov	r7, sl
   e1238:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e123a:	2b00      	cmp	r3, #0
   e123c:	dd04      	ble.n	e1248 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x144>
   e123e:	455f      	cmp	r7, fp
   e1240:	bfb4      	ite	lt
   e1242:	2300      	movlt	r3, #0
   e1244:	2301      	movge	r3, #1
   e1246:	e003      	b.n	e1250 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x14c>
   e1248:	455f      	cmp	r7, fp
   e124a:	bfcc      	ite	gt
   e124c:	2300      	movgt	r3, #0
   e124e:	2301      	movle	r3, #1
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e1250:	b98b      	cbnz	r3, e1276 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x172>
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
   e1252:	9700      	str	r7, [sp, #0]
   e1254:	4633      	mov	r3, r6
   e1256:	462a      	mov	r2, r5
   e1258:	4621      	mov	r1, r4
   e125a:	980b      	ldr	r0, [sp, #44]	; 0x2c
   e125c:	f7f5 f969 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   e1260:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e1262:	9a2a      	ldr	r2, [sp, #168]	; 0xa8
   e1264:	eb03 0080 	add.w	r0, r3, r0, lsl #2
   e1268:	6803      	ldr	r3, [r0, #0]
   e126a:	f842 3b04 	str.w	r3, [r2], #4
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e126e:	9b09      	ldr	r3, [sp, #36]	; 0x24
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
   e1270:	922a      	str	r2, [sp, #168]	; 0xa8
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e1272:	441f      	add	r7, r3
   e1274:	e7e0      	b.n	e1238 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x134>
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
   e1276:	9b08      	ldr	r3, [sp, #32]
   e1278:	441e      	add	r6, r3
   e127a:	e7cf      	b.n	e121c <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x118>

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
   e127c:	9b07      	ldr	r3, [sp, #28]
   e127e:	441d      	add	r5, r3
   e1280:	e7bc      	b.n	e11fc <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xf8>
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
   e1282:	9b06      	ldr	r3, [sp, #24]
   e1284:	441c      	add	r4, r3
   e1286:	e7a9      	b.n	e11dc <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xd8>
  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape input_shape =
      RuntimeShape::ExtendedShape(4, unextended_input_shape);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   e1288:	a811      	add	r0, sp, #68	; 0x44
   e128a:	f7f5 f8e2 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  tflite::StridedSliceParams params_copy = op_params;

  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape input_shape =
      RuntimeShape::ExtendedShape(4, unextended_input_shape);
   e128e:	a80c      	add	r0, sp, #48	; 0x30
   e1290:	f7f5 f8df 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
        }
      }
    }
  }
}
   e1294:	b021      	add	sp, #132	; 0x84
   e1296:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000e129a <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_>:

namespace tflite {

namespace reference_ops {
template <typename T>
inline void StridedSlice(const tflite::StridedSliceParams& op_params,
   e129a:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e129e:	b0a1      	sub	sp, #132	; 0x84
   e12a0:	461e      	mov	r6, r3
   e12a2:	460f      	mov	r7, r1
   e12a4:	920a      	str	r2, [sp, #40]	; 0x28
                         const RuntimeShape& unextended_input_shape,
                         const T* input_data,
                         const RuntimeShape& unextended_output_shape,
                         T* output_data) {
  // Note that the output_shape is not used herein.
  tflite::StridedSliceParams params_copy = op_params;
   e12a6:	4603      	mov	r3, r0
   e12a8:	ac16      	add	r4, sp, #88	; 0x58
   e12aa:	f100 0528 	add.w	r5, r0, #40	; 0x28
   e12ae:	6818      	ldr	r0, [r3, #0]
   e12b0:	6859      	ldr	r1, [r3, #4]
   e12b2:	4622      	mov	r2, r4
   e12b4:	c203      	stmia	r2!, {r0, r1}
   e12b6:	3308      	adds	r3, #8
   e12b8:	42ab      	cmp	r3, r5
   e12ba:	4614      	mov	r4, r2
   e12bc:	d1f7      	bne.n	e12ae <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x14>

  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
   e12be:	683b      	ldr	r3, [r7, #0]
   e12c0:	2b04      	cmp	r3, #4
   e12c2:	dd01      	ble.n	e12c8 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x2e>
   e12c4:	f003 f842 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   e12c8:	6833      	ldr	r3, [r6, #0]
   e12ca:	2b04      	cmp	r3, #4
   e12cc:	dcfa      	bgt.n	e12c4 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x2a>
   e12ce:	ad0c      	add	r5, sp, #48	; 0x30
   e12d0:	2301      	movs	r3, #1
   e12d2:	463a      	mov	r2, r7
   e12d4:	2104      	movs	r1, #4
   e12d6:	4628      	mov	r0, r5
   e12d8:	f7f5 f8ff 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   e12dc:	2301      	movs	r3, #1
   e12de:	4632      	mov	r2, r6
   e12e0:	2104      	movs	r1, #4
   e12e2:	a811      	add	r0, sp, #68	; 0x44
   e12e4:	f7f5 f8f9 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  // Reverse and pad to 4 dimensions because that is what the runtime code
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);
   e12e8:	2104      	movs	r1, #4
   e12ea:	a816      	add	r0, sp, #88	; 0x58
   e12ec:	f7ff fd38 	bl	e0d60 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi>

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
   e12f0:	2200      	movs	r2, #0
   e12f2:	4629      	mov	r1, r5
   e12f4:	a816      	add	r0, sp, #88	; 0x58
   e12f6:	f7ff fd87 	bl	e0e08 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
   e12fa:	2200      	movs	r2, #0
   e12fc:	4603      	mov	r3, r0
   e12fe:	4629      	mov	r1, r5

  // Reverse and pad to 4 dimensions because that is what the runtime code
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
   e1300:	4604      	mov	r4, r0
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
   e1302:	a816      	add	r0, sp, #88	; 0x58
   e1304:	f7ff fda3 	bl	e0e4e <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
   e1308:	2201      	movs	r2, #1
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
   e130a:	9003      	str	r0, [sp, #12]
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
   e130c:	4629      	mov	r1, r5
   e130e:	a816      	add	r0, sp, #88	; 0x58
   e1310:	f7ff fd7a 	bl	e0e08 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
   e1314:	2201      	movs	r2, #1
   e1316:	4603      	mov	r3, r0
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
   e1318:	9004      	str	r0, [sp, #16]
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
   e131a:	4629      	mov	r1, r5
   e131c:	a816      	add	r0, sp, #88	; 0x58
   e131e:	f7ff fd96 	bl	e0e4e <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
   e1322:	2202      	movs	r2, #2
  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
   e1324:	9005      	str	r0, [sp, #20]
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
   e1326:	4629      	mov	r1, r5
   e1328:	a816      	add	r0, sp, #88	; 0x58
   e132a:	f7ff fd6d 	bl	e0e08 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
   e132e:	2202      	movs	r2, #2
   e1330:	4603      	mov	r3, r0
   e1332:	4629      	mov	r1, r5
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
   e1334:	4680      	mov	r8, r0
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
   e1336:	a816      	add	r0, sp, #88	; 0x58
   e1338:	f7ff fd89 	bl	e0e4e <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
   e133c:	2203      	movs	r2, #3
   e133e:	4629      	mov	r1, r5
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
   e1340:	4681      	mov	r9, r0
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
   e1342:	a816      	add	r0, sp, #88	; 0x58
   e1344:	f7ff fd60 	bl	e0e08 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
   e1348:	2203      	movs	r2, #3
   e134a:	4603      	mov	r3, r0
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
   e134c:	4682      	mov	sl, r0
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
   e134e:	4629      	mov	r1, r5
   e1350:	a816      	add	r0, sp, #88	; 0x58
   e1352:	f7ff fd7c 	bl	e0e4e <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
   e1356:	f9bd 306e 	ldrsh.w	r3, [sp, #110]	; 0x6e
   e135a:	9306      	str	r3, [sp, #24]
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
   e135c:	f9bd 3070 	ldrsh.w	r3, [sp, #112]	; 0x70
   e1360:	9307      	str	r3, [sp, #28]
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
   e1362:	f9bd 3072 	ldrsh.w	r3, [sp, #114]	; 0x72
   e1366:	9308      	str	r3, [sp, #32]
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e1368:	f9bd 3074 	ldrsh.w	r3, [sp, #116]	; 0x74
   e136c:	9309      	str	r3, [sp, #36]	; 0x24
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
   e136e:	4683      	mov	fp, r0
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
   e1370:	950b      	str	r5, [sp, #44]	; 0x2c
   e1372:	9b06      	ldr	r3, [sp, #24]
   e1374:	2b00      	cmp	r3, #0
   e1376:	9b03      	ldr	r3, [sp, #12]
   e1378:	dd04      	ble.n	e1384 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xea>
   e137a:	429c      	cmp	r4, r3
   e137c:	bfb4      	ite	lt
   e137e:	2300      	movlt	r3, #0
   e1380:	2301      	movge	r3, #1
   e1382:	e003      	b.n	e138c <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xf2>
   e1384:	429c      	cmp	r4, r3
   e1386:	bfcc      	ite	gt
   e1388:	2300      	movgt	r3, #0
   e138a:	2301      	movle	r3, #1
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
   e138c:	2b00      	cmp	r3, #0
   e138e:	d144      	bne.n	e141a <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x180>
   e1390:	9d04      	ldr	r5, [sp, #16]
   e1392:	9b07      	ldr	r3, [sp, #28]
   e1394:	2b00      	cmp	r3, #0
   e1396:	9b05      	ldr	r3, [sp, #20]
   e1398:	dd04      	ble.n	e13a4 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x10a>
   e139a:	429d      	cmp	r5, r3
   e139c:	bfb4      	ite	lt
   e139e:	2300      	movlt	r3, #0
   e13a0:	2301      	movge	r3, #1
   e13a2:	e003      	b.n	e13ac <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x112>
   e13a4:	429d      	cmp	r5, r3
   e13a6:	bfcc      	ite	gt
   e13a8:	2300      	movgt	r3, #0
   e13aa:	2301      	movle	r3, #1
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
   e13ac:	2b00      	cmp	r3, #0
   e13ae:	d131      	bne.n	e1414 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x17a>
   e13b0:	4646      	mov	r6, r8
   e13b2:	9b08      	ldr	r3, [sp, #32]
   e13b4:	2b00      	cmp	r3, #0
   e13b6:	dd04      	ble.n	e13c2 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x128>
   e13b8:	454e      	cmp	r6, r9
   e13ba:	bfb4      	ite	lt
   e13bc:	2300      	movlt	r3, #0
   e13be:	2301      	movge	r3, #1
   e13c0:	e003      	b.n	e13ca <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x130>
   e13c2:	454e      	cmp	r6, r9
   e13c4:	bfcc      	ite	gt
   e13c6:	2300      	movgt	r3, #0
   e13c8:	2301      	movle	r3, #1
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
   e13ca:	bb03      	cbnz	r3, e140e <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x174>
   e13cc:	4657      	mov	r7, sl
   e13ce:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e13d0:	2b00      	cmp	r3, #0
   e13d2:	dd04      	ble.n	e13de <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x144>
   e13d4:	455f      	cmp	r7, fp
   e13d6:	bfb4      	ite	lt
   e13d8:	2300      	movlt	r3, #0
   e13da:	2301      	movge	r3, #1
   e13dc:	e003      	b.n	e13e6 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x14c>
   e13de:	455f      	cmp	r7, fp
   e13e0:	bfcc      	ite	gt
   e13e2:	2300      	movgt	r3, #0
   e13e4:	2301      	movle	r3, #1
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e13e6:	b97b      	cbnz	r3, e1408 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x16e>
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
   e13e8:	9700      	str	r7, [sp, #0]
   e13ea:	4633      	mov	r3, r6
   e13ec:	462a      	mov	r2, r5
   e13ee:	4621      	mov	r1, r4
   e13f0:	980b      	ldr	r0, [sp, #44]	; 0x2c
   e13f2:	f7f5 f89e 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   e13f6:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e13f8:	9a2a      	ldr	r2, [sp, #168]	; 0xa8
   e13fa:	5c1b      	ldrb	r3, [r3, r0]
   e13fc:	f802 3b01 	strb.w	r3, [r2], #1
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e1400:	9b09      	ldr	r3, [sp, #36]	; 0x24
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
   e1402:	922a      	str	r2, [sp, #168]	; 0xa8
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e1404:	441f      	add	r7, r3
   e1406:	e7e2      	b.n	e13ce <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x134>
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
   e1408:	9b08      	ldr	r3, [sp, #32]
   e140a:	441e      	add	r6, r3
   e140c:	e7d1      	b.n	e13b2 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x118>

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
   e140e:	9b07      	ldr	r3, [sp, #28]
   e1410:	441d      	add	r5, r3
   e1412:	e7be      	b.n	e1392 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xf8>
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
   e1414:	9b06      	ldr	r3, [sp, #24]
   e1416:	441c      	add	r4, r3
   e1418:	e7ab      	b.n	e1372 <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xd8>
  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape input_shape =
      RuntimeShape::ExtendedShape(4, unextended_input_shape);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   e141a:	a811      	add	r0, sp, #68	; 0x44
   e141c:	f7f5 f819 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  tflite::StridedSliceParams params_copy = op_params;

  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape input_shape =
      RuntimeShape::ExtendedShape(4, unextended_input_shape);
   e1420:	a80c      	add	r0, sp, #48	; 0x30
   e1422:	f7f5 f816 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
        }
      }
    }
  }
}
   e1426:	b021      	add	sp, #132	; 0x84
   e1428:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000e142c <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_>:

namespace tflite {

namespace reference_ops {
template <typename T>
inline void StridedSlice(const tflite::StridedSliceParams& op_params,
   e142c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e1430:	b0a1      	sub	sp, #132	; 0x84
   e1432:	461e      	mov	r6, r3
   e1434:	460f      	mov	r7, r1
   e1436:	920a      	str	r2, [sp, #40]	; 0x28
                         const RuntimeShape& unextended_input_shape,
                         const T* input_data,
                         const RuntimeShape& unextended_output_shape,
                         T* output_data) {
  // Note that the output_shape is not used herein.
  tflite::StridedSliceParams params_copy = op_params;
   e1438:	4603      	mov	r3, r0
   e143a:	ac16      	add	r4, sp, #88	; 0x58
   e143c:	f100 0528 	add.w	r5, r0, #40	; 0x28
   e1440:	6818      	ldr	r0, [r3, #0]
   e1442:	6859      	ldr	r1, [r3, #4]
   e1444:	4622      	mov	r2, r4
   e1446:	c203      	stmia	r2!, {r0, r1}
   e1448:	3308      	adds	r3, #8
   e144a:	42ab      	cmp	r3, r5
   e144c:	4614      	mov	r4, r2
   e144e:	d1f7      	bne.n	e1440 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x14>

  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
   e1450:	683b      	ldr	r3, [r7, #0]
   e1452:	2b04      	cmp	r3, #4
   e1454:	dd01      	ble.n	e145a <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x2e>
   e1456:	f002 ff79 	bl	e434c <abort>
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
   e145a:	6833      	ldr	r3, [r6, #0]
   e145c:	2b04      	cmp	r3, #4
   e145e:	dcfa      	bgt.n	e1456 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x2a>
   e1460:	ad0c      	add	r5, sp, #48	; 0x30
   e1462:	2301      	movs	r3, #1
   e1464:	463a      	mov	r2, r7
   e1466:	2104      	movs	r1, #4
   e1468:	4628      	mov	r0, r5
   e146a:	f7f5 f836 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
   e146e:	2301      	movs	r3, #1
   e1470:	4632      	mov	r2, r6
   e1472:	2104      	movs	r1, #4
   e1474:	a811      	add	r0, sp, #68	; 0x44
   e1476:	f7f5 f830 	bl	d64da <_ZN6tflite12RuntimeShapeC1EiRKS0_i>
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);

  // Reverse and pad to 4 dimensions because that is what the runtime code
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);
   e147a:	2104      	movs	r1, #4
   e147c:	a816      	add	r0, sp, #88	; 0x58
   e147e:	f7ff fc6f 	bl	e0d60 <_ZN6tflite13strided_slice22StridedSlicePadIndicesEPNS_18StridedSliceParamsEi>

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
   e1482:	2200      	movs	r2, #0
   e1484:	4629      	mov	r1, r5
   e1486:	a816      	add	r0, sp, #88	; 0x58
   e1488:	f7ff fcbe 	bl	e0e08 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
   e148c:	2200      	movs	r2, #0
   e148e:	4603      	mov	r3, r0
   e1490:	4629      	mov	r1, r5

  // Reverse and pad to 4 dimensions because that is what the runtime code
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
   e1492:	4604      	mov	r4, r0
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
   e1494:	a816      	add	r0, sp, #88	; 0x58
   e1496:	f7ff fcda 	bl	e0e4e <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
   e149a:	2201      	movs	r2, #1
  // requires (ie. all shapes must be 4D and are given backwards).
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
   e149c:	9003      	str	r0, [sp, #12]
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
   e149e:	4629      	mov	r1, r5
   e14a0:	a816      	add	r0, sp, #88	; 0x58
   e14a2:	f7ff fcb1 	bl	e0e08 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
   e14a6:	2201      	movs	r2, #1
   e14a8:	4603      	mov	r3, r0
  strided_slice::StridedSlicePadIndices(&params_copy, 4);

  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
   e14aa:	9004      	str	r0, [sp, #16]
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
   e14ac:	4629      	mov	r1, r5
   e14ae:	a816      	add	r0, sp, #88	; 0x58
   e14b0:	f7ff fccd 	bl	e0e4e <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
   e14b4:	2202      	movs	r2, #2
  const int start_b = strided_slice::StartForAxis(params_copy, input_shape, 0);
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
   e14b6:	9005      	str	r0, [sp, #20]
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
   e14b8:	4629      	mov	r1, r5
   e14ba:	a816      	add	r0, sp, #88	; 0x58
   e14bc:	f7ff fca4 	bl	e0e08 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
   e14c0:	2202      	movs	r2, #2
   e14c2:	4603      	mov	r3, r0
   e14c4:	4629      	mov	r1, r5
  const int stop_b =
      strided_slice::StopForAxis(params_copy, input_shape, 0, start_b);
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
   e14c6:	4680      	mov	r8, r0
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
   e14c8:	a816      	add	r0, sp, #88	; 0x58
   e14ca:	f7ff fcc0 	bl	e0e4e <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
   e14ce:	2203      	movs	r2, #3
   e14d0:	4629      	mov	r1, r5
  const int start_h = strided_slice::StartForAxis(params_copy, input_shape, 1);
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
   e14d2:	4681      	mov	r9, r0
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
   e14d4:	a816      	add	r0, sp, #88	; 0x58
   e14d6:	f7ff fc97 	bl	e0e08 <_ZN6tflite13strided_slice12StartForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEi>
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
   e14da:	2203      	movs	r2, #3
   e14dc:	4603      	mov	r3, r0
  const int stop_h =
      strided_slice::StopForAxis(params_copy, input_shape, 1, start_h);
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
   e14de:	4682      	mov	sl, r0
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
   e14e0:	4629      	mov	r1, r5
   e14e2:	a816      	add	r0, sp, #88	; 0x58
   e14e4:	f7ff fcb3 	bl	e0e4e <_ZN6tflite13strided_slice11StopForAxisERKNS_18StridedSliceParamsERKNS_12RuntimeShapeEii>

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
   e14e8:	f9bd 306e 	ldrsh.w	r3, [sp, #110]	; 0x6e
   e14ec:	9306      	str	r3, [sp, #24]
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
   e14ee:	f9bd 3070 	ldrsh.w	r3, [sp, #112]	; 0x70
   e14f2:	9307      	str	r3, [sp, #28]
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
   e14f4:	f9bd 3072 	ldrsh.w	r3, [sp, #114]	; 0x72
   e14f8:	9308      	str	r3, [sp, #32]
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e14fa:	f9bd 3074 	ldrsh.w	r3, [sp, #116]	; 0x74
   e14fe:	9309      	str	r3, [sp, #36]	; 0x24
  const int start_w = strided_slice::StartForAxis(params_copy, input_shape, 2);
  const int stop_w =
      strided_slice::StopForAxis(params_copy, input_shape, 2, start_w);
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);
   e1500:	4683      	mov	fp, r0
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
   e1502:	950b      	str	r5, [sp, #44]	; 0x2c
   e1504:	9b06      	ldr	r3, [sp, #24]
   e1506:	2b00      	cmp	r3, #0
   e1508:	9b03      	ldr	r3, [sp, #12]
   e150a:	dd04      	ble.n	e1516 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xea>
   e150c:	429c      	cmp	r4, r3
   e150e:	bfb4      	ite	lt
   e1510:	2300      	movlt	r3, #0
   e1512:	2301      	movge	r3, #1
   e1514:	e003      	b.n	e151e <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xf2>
   e1516:	429c      	cmp	r4, r3
   e1518:	bfcc      	ite	gt
   e151a:	2300      	movgt	r3, #0
   e151c:	2301      	movle	r3, #1
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
   e151e:	2b00      	cmp	r3, #0
   e1520:	d144      	bne.n	e15ac <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x180>
   e1522:	9d04      	ldr	r5, [sp, #16]
   e1524:	9b07      	ldr	r3, [sp, #28]
   e1526:	2b00      	cmp	r3, #0
   e1528:	9b05      	ldr	r3, [sp, #20]
   e152a:	dd04      	ble.n	e1536 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x10a>
   e152c:	429d      	cmp	r5, r3
   e152e:	bfb4      	ite	lt
   e1530:	2300      	movlt	r3, #0
   e1532:	2301      	movge	r3, #1
   e1534:	e003      	b.n	e153e <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x112>
   e1536:	429d      	cmp	r5, r3
   e1538:	bfcc      	ite	gt
   e153a:	2300      	movgt	r3, #0
   e153c:	2301      	movle	r3, #1
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
   e153e:	2b00      	cmp	r3, #0
   e1540:	d131      	bne.n	e15a6 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x17a>
   e1542:	4646      	mov	r6, r8
   e1544:	9b08      	ldr	r3, [sp, #32]
   e1546:	2b00      	cmp	r3, #0
   e1548:	dd04      	ble.n	e1554 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x128>
   e154a:	454e      	cmp	r6, r9
   e154c:	bfb4      	ite	lt
   e154e:	2300      	movlt	r3, #0
   e1550:	2301      	movge	r3, #1
   e1552:	e003      	b.n	e155c <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x130>
   e1554:	454e      	cmp	r6, r9
   e1556:	bfcc      	ite	gt
   e1558:	2300      	movgt	r3, #0
   e155a:	2301      	movle	r3, #1
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
   e155c:	bb03      	cbnz	r3, e15a0 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x174>
   e155e:	4657      	mov	r7, sl
   e1560:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e1562:	2b00      	cmp	r3, #0
   e1564:	dd04      	ble.n	e1570 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x144>
   e1566:	455f      	cmp	r7, fp
   e1568:	bfb4      	ite	lt
   e156a:	2300      	movlt	r3, #0
   e156c:	2301      	movge	r3, #1
   e156e:	e003      	b.n	e1578 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x14c>
   e1570:	455f      	cmp	r7, fp
   e1572:	bfcc      	ite	gt
   e1574:	2300      	movgt	r3, #0
   e1576:	2301      	movle	r3, #1
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e1578:	b97b      	cbnz	r3, e159a <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x16e>
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
   e157a:	9700      	str	r7, [sp, #0]
   e157c:	4633      	mov	r3, r6
   e157e:	462a      	mov	r2, r5
   e1580:	4621      	mov	r1, r4
   e1582:	980b      	ldr	r0, [sp, #44]	; 0x2c
   e1584:	f7f4 ffd5 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   e1588:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e158a:	9a2a      	ldr	r2, [sp, #168]	; 0xa8
   e158c:	561b      	ldrsb	r3, [r3, r0]
   e158e:	f802 3b01 	strb.w	r3, [r2], #1
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e1592:	9b09      	ldr	r3, [sp, #36]	; 0x24
                 in_d, stop_d, params_copy.strides[3]);
             in_d += params_copy.strides[3]) {
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
   e1594:	922a      	str	r2, [sp, #168]	; 0xa8
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
           !strided_slice::LoopCondition(in_w, stop_w, params_copy.strides[2]);
           in_w += params_copy.strides[2]) {
        for (int in_d = start_d; !strided_slice::LoopCondition(
   e1596:	441f      	add	r7, r3
   e1598:	e7e2      	b.n	e1560 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x134>
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
         !strided_slice::LoopCondition(in_h, stop_h, params_copy.strides[1]);
         in_h += params_copy.strides[1]) {
      for (int in_w = start_w;
   e159a:	9b08      	ldr	r3, [sp, #32]
   e159c:	441e      	add	r6, r3
   e159e:	e7d1      	b.n	e1544 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0x118>

  T* out_ptr = output_data;
  for (int in_b = start_b;
       !strided_slice::LoopCondition(in_b, stop_b, params_copy.strides[0]);
       in_b += params_copy.strides[0]) {
    for (int in_h = start_h;
   e15a0:	9b07      	ldr	r3, [sp, #28]
   e15a2:	441d      	add	r5, r3
   e15a4:	e7be      	b.n	e1524 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xf8>
  const int start_d = strided_slice::StartForAxis(params_copy, input_shape, 3);
  const int stop_d =
      strided_slice::StopForAxis(params_copy, input_shape, 3, start_d);

  T* out_ptr = output_data;
  for (int in_b = start_b;
   e15a6:	9b06      	ldr	r3, [sp, #24]
   e15a8:	441c      	add	r4, r3
   e15aa:	e7ab      	b.n	e1504 <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_+0xd8>
  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape input_shape =
      RuntimeShape::ExtendedShape(4, unextended_input_shape);
  const RuntimeShape output_shape =
      RuntimeShape::ExtendedShape(4, unextended_output_shape);
   e15ac:	a811      	add	r0, sp, #68	; 0x44
   e15ae:	f7f4 ff50 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  tflite::StridedSliceParams params_copy = op_params;

  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);
  const RuntimeShape input_shape =
      RuntimeShape::ExtendedShape(4, unextended_input_shape);
   e15b2:	a80c      	add	r0, sp, #48	; 0x30
   e15b4:	f7f4 ff4d 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
          *out_ptr++ = input_data[Offset(input_shape, in_b, in_h, in_w, in_d)];
        }
      }
    }
  }
}
   e15b8:	b021      	add	sp, #132	; 0x84
   e15ba:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

000e15c0 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode>:
                     "input dim should not exceed 4");
  return CheckOutputSize(context, &op_context);
}

template <KernelType kernel_type>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e15c0:	b510      	push	{r4, lr}
   e15c2:	b09e      	sub	sp, #120	; 0x78
  StridedSliceContext op_context(context, node);
   e15c4:	460a      	mov	r2, r1
                     "input dim should not exceed 4");
  return CheckOutputSize(context, &op_context);
}

template <KernelType kernel_type>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e15c6:	4604      	mov	r4, r0
  StridedSliceContext op_context(context, node);
   e15c8:	4601      	mov	r1, r0
   e15ca:	a80d      	add	r0, sp, #52	; 0x34
   e15cc:	f7ff fc78 	bl	e0ec0 <_ZN6tflite3ops5micro13strided_slice19StridedSliceContextC1EP13TfLiteContextP10TfLiteNode>
  auto op_params = BuildStridedSliceParams(&op_context);
   e15d0:	a90d      	add	r1, sp, #52	; 0x34
   e15d2:	a814      	add	r0, sp, #80	; 0x50
   e15d4:	f7ff fc9a 	bl	e0f0c <_ZN6tflite3ops5micro13strided_slice23BuildStridedSliceParamsEPNS2_19StridedSliceContextE>
  kernel_type::StridedSlice(op_params, GetTensorShape(op_context.input), \
                            GetTensorData<data_type>(op_context.input),  \
                            GetTensorShape(op_context.output),           \
                            GetTensorData<data_type>(op_context.output))

  switch (op_context.input->type) {
   e15d8:	990e      	ldr	r1, [sp, #56]	; 0x38
   e15da:	780a      	ldrb	r2, [r1, #0]
   e15dc:	2a03      	cmp	r2, #3
   e15de:	d01a      	beq.n	e1616 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x56>
   e15e0:	2a09      	cmp	r2, #9
   e15e2:	d036      	beq.n	e1652 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x92>
   e15e4:	2a01      	cmp	r2, #1
   e15e6:	d14b      	bne.n	e1680 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xc0>
    case kTfLiteFloat32:
      if (kernel_type == kReference) {
        TF_LITE_STRIDED_SLICE(reference_ops, float);
   e15e8:	a803      	add	r0, sp, #12
   e15ea:	f7f5 f9e2 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e15ee:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   e15f0:	b10a      	cbz	r2, e15f6 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x36>
   e15f2:	6854      	ldr	r4, [r2, #4]
   e15f4:	e000      	b.n	e15f8 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x38>
   e15f6:	4614      	mov	r4, r2
   e15f8:	9912      	ldr	r1, [sp, #72]	; 0x48
   e15fa:	a808      	add	r0, sp, #32
   e15fc:	f7f5 f9d9 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e1600:	9b12      	ldr	r3, [sp, #72]	; 0x48

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e1602:	b103      	cbz	r3, e1606 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x46>
   e1604:	685b      	ldr	r3, [r3, #4]
   e1606:	9300      	str	r3, [sp, #0]
   e1608:	4622      	mov	r2, r4
   e160a:	ab08      	add	r3, sp, #32
   e160c:	a903      	add	r1, sp, #12
   e160e:	a814      	add	r0, sp, #80	; 0x50
   e1610:	f7ff fd78 	bl	e1104 <_ZN6tflite13reference_ops12StridedSliceIfEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_>
   e1614:	e015      	b.n	e1642 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x82>
      }
      break;
    case kTfLiteUInt8:
      if (kernel_type == kReference) {
        TF_LITE_STRIDED_SLICE(reference_ops, uint8_t);
   e1616:	a803      	add	r0, sp, #12
   e1618:	f7f5 f9cb 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e161c:	9a0e      	ldr	r2, [sp, #56]	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e161e:	b10a      	cbz	r2, e1624 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x64>
   e1620:	6854      	ldr	r4, [r2, #4]
   e1622:	e000      	b.n	e1626 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x66>
   e1624:	4614      	mov	r4, r2
   e1626:	9912      	ldr	r1, [sp, #72]	; 0x48
   e1628:	a808      	add	r0, sp, #32
   e162a:	f7f5 f9c2 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e162e:	9b12      	ldr	r3, [sp, #72]	; 0x48

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e1630:	b103      	cbz	r3, e1634 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x74>
   e1632:	685b      	ldr	r3, [r3, #4]
   e1634:	9300      	str	r3, [sp, #0]
   e1636:	4622      	mov	r2, r4
   e1638:	ab08      	add	r3, sp, #32
   e163a:	a903      	add	r1, sp, #12
   e163c:	a814      	add	r0, sp, #80	; 0x50
   e163e:	f7ff fe2c 	bl	e129a <_ZN6tflite13reference_ops12StridedSliceIhEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_>
   e1642:	a808      	add	r0, sp, #32
   e1644:	f7f4 ff05 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   e1648:	a803      	add	r0, sp, #12
   e164a:	f7f4 ff02 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
                           "by StridedSlice.",
                           op_context.input->type);
      return kTfLiteError;
  }
#undef TF_LITE_STRIDED_SLICE
  return kTfLiteOk;
   e164e:	2000      	movs	r0, #0
      break;
    case kTfLiteUInt8:
      if (kernel_type == kReference) {
        TF_LITE_STRIDED_SLICE(reference_ops, uint8_t);
      }
      break;
   e1650:	e01b      	b.n	e168a <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xca>
    case kTfLiteInt8:
      if (kernel_type == kReference) {
        TF_LITE_STRIDED_SLICE(reference_ops, int8_t);
   e1652:	a803      	add	r0, sp, #12
   e1654:	f7f5 f9ad 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e1658:	9a0e      	ldr	r2, [sp, #56]	; 0x38
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e165a:	b10a      	cbz	r2, e1660 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa0>
   e165c:	6854      	ldr	r4, [r2, #4]
   e165e:	e000      	b.n	e1662 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xa2>
   e1660:	4614      	mov	r4, r2
   e1662:	9912      	ldr	r1, [sp, #72]	; 0x48
   e1664:	a808      	add	r0, sp, #32
   e1666:	f7f5 f9a4 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e166a:	9b12      	ldr	r3, [sp, #72]	; 0x48

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e166c:	b103      	cbz	r3, e1670 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xb0>
   e166e:	685b      	ldr	r3, [r3, #4]
   e1670:	9300      	str	r3, [sp, #0]
   e1672:	4622      	mov	r2, r4
   e1674:	ab08      	add	r3, sp, #32
   e1676:	a903      	add	r1, sp, #12
   e1678:	a814      	add	r0, sp, #80	; 0x50
   e167a:	f7ff fed7 	bl	e142c <_ZN6tflite13reference_ops12StridedSliceIaEEvRKNS_18StridedSliceParamsERKNS_12RuntimeShapeEPKT_S7_PS8_>
   e167e:	e7e0      	b.n	e1642 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0x82>
      }
      break;
    default:
      context->ReportError(context,
   e1680:	4620      	mov	r0, r4
   e1682:	6963      	ldr	r3, [r4, #20]
   e1684:	4902      	ldr	r1, [pc, #8]	; (e1690 <_ZN6tflite3ops5micro13strided_slice4EvalILNS2_10KernelTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+0xd0>)
   e1686:	4798      	blx	r3
                           "Type %d is currently not supported "
                           "by StridedSlice.",
                           op_context.input->type);
      return kTfLiteError;
   e1688:	2001      	movs	r0, #1
  }
#undef TF_LITE_STRIDED_SLICE
  return kTfLiteOk;
}
   e168a:	b01e      	add	sp, #120	; 0x78
   e168c:	bd10      	pop	{r4, pc}
   e168e:	bf00      	nop
   e1690:	000ea994 	.word	0x000ea994

000e1694 <_ZN6tflite3ops5micro4svdf4InitEP13TfLiteContextPKcj>:
// Output tensor.
constexpr int kOutputTensor = 0;

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
   e1694:	2000      	movs	r0, #0
   e1696:	4770      	bx	lr

000e1698 <_ZN6tflite3ops5micro4svdf4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
   e1698:	4770      	bx	lr
	...

000e169c <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_>:

static inline void ApplyTimeWeightsBiasAndActivation(
    int batch_size, int memory_size, int num_filters, int num_units, int rank,
    const TfLiteTensor* weights_time, const TfLiteTensor* bias,
    TfLiteFusedActivation activation, TfLiteTensor* activation_state,
    TfLiteTensor* scratch, TfLiteTensor* output) {
   e169c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e16a0:	b087      	sub	sp, #28
   e16a2:	4693      	mov	fp, r2
   e16a4:	9101      	str	r1, [sp, #4]
   e16a6:	fb01 f10b 	mul.w	r1, r1, fp
   e16aa:	0089      	lsls	r1, r1, #2
   e16ac:	9103      	str	r1, [sp, #12]
   e16ae:	ea4f 018b 	mov.w	r1, fp, lsl #2
   e16b2:	9104      	str	r1, [sp, #16]
   e16b4:	9901      	ldr	r1, [sp, #4]
   e16b6:	f89d 204c 	ldrb.w	r2, [sp, #76]	; 0x4c
   e16ba:	9205      	str	r2, [sp, #20]
   e16bc:	2500      	movs	r5, #0
   e16be:	ea21 76e1 	bic.w	r6, r1, r1, asr #31
   e16c2:	9a16      	ldr	r2, [sp, #88]	; 0x58
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;
    const float* vector1_ptr = GetTensorData<float>(weights_time);
    const float* vector2_ptr =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int i = 0; i < num_filters; ++i) {
      *scratch_ptr_batch = 0.f;
   e16c4:	ed9f 7a77 	vldr	s14, [pc, #476]	; e18a4 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x208>

static inline void ApplyTimeWeightsBiasAndActivation(
    int batch_size, int memory_size, int num_filters, int num_units, int rank,
    const TfLiteTensor* weights_time, const TfLiteTensor* bias,
    TfLiteFusedActivation activation, TfLiteTensor* activation_state,
    TfLiteTensor* scratch, TfLiteTensor* output) {
   e16c8:	9300      	str	r3, [sp, #0]
   e16ca:	00b6      	lsls	r6, r6, #2
   e16cc:	46ae      	mov	lr, r5
  // Compute matmul(activation_state, weights_time).
  // The rightmost column is used to save temporary output (with the size of
  // num_filters). This is achieved by starting at
  // GetTensorData<float>(activation_state), and having the stride equal to
  // memory_size.
  for (int b = 0; b < batch_size; ++b) {
   e16ce:	46ac      	mov	ip, r5
   e16d0:	4584      	cmp	ip, r0
   e16d2:	da38      	bge.n	e1746 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xaa>
   e16d4:	9915      	ldr	r1, [sp, #84]	; 0x54
   e16d6:	b109      	cbz	r1, e16dc <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x40>
   e16d8:	6849      	ldr	r1, [r1, #4]
   e16da:	e000      	b.n	e16de <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x42>
   e16dc:	9915      	ldr	r1, [sp, #84]	; 0x54
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e16de:	9c11      	ldr	r4, [sp, #68]	; 0x44
    // Perform batched vector dot product:
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;
   e16e0:	4429      	add	r1, r5
   e16e2:	b10c      	cbz	r4, e16e8 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x4c>
   e16e4:	6867      	ldr	r7, [r4, #4]
   e16e6:	e000      	b.n	e16ea <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x4e>
   e16e8:	9f11      	ldr	r7, [sp, #68]	; 0x44

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e16ea:	9c14      	ldr	r4, [sp, #80]	; 0x50
   e16ec:	b10c      	cbz	r4, e16f2 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x56>
   e16ee:	6864      	ldr	r4, [r4, #4]
   e16f0:	e000      	b.n	e16f4 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x58>
   e16f2:	9c14      	ldr	r4, [sp, #80]	; 0x50
    const float* vector1_ptr = GetTensorData<float>(weights_time);
    const float* vector2_ptr =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
   e16f4:	4474      	add	r4, lr
    for (int i = 0; i < num_filters; ++i) {
   e16f6:	f04f 0800 	mov.w	r8, #0
   e16fa:	45d8      	cmp	r8, fp
   e16fc:	da1c      	bge.n	e1738 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x9c>
      *scratch_ptr_batch = 0.f;
   e16fe:	eca1 7a01 	vstmia	r1!, {s14}
   e1702:	46a2      	mov	sl, r4
   e1704:	9702      	str	r7, [sp, #8]
      for (int j = 0; j < memory_size; ++j) {
   e1706:	f04f 0900 	mov.w	r9, #0
   e170a:	9b01      	ldr	r3, [sp, #4]
   e170c:	4599      	cmp	r9, r3
   e170e:	da0e      	bge.n	e172e <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x92>
        *scratch_ptr_batch += *vector1_ptr++ * *vector2_ptr++;
   e1710:	9b02      	ldr	r3, [sp, #8]
   e1712:	ecfa 6a01 	vldmia	sl!, {s13}
   e1716:	ecb3 6a01 	vldmia	r3!, {s12}
   e171a:	ed51 7a01 	vldr	s15, [r1, #-4]
   e171e:	9302      	str	r3, [sp, #8]
   e1720:	eee6 7a26 	vfma.f32	s15, s12, s13
    const float* vector1_ptr = GetTensorData<float>(weights_time);
    const float* vector2_ptr =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int i = 0; i < num_filters; ++i) {
      *scratch_ptr_batch = 0.f;
      for (int j = 0; j < memory_size; ++j) {
   e1724:	f109 0901 	add.w	r9, r9, #1
        *scratch_ptr_batch += *vector1_ptr++ * *vector2_ptr++;
   e1728:	ed41 7a01 	vstr	s15, [r1, #-4]
    const float* vector1_ptr = GetTensorData<float>(weights_time);
    const float* vector2_ptr =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int i = 0; i < num_filters; ++i) {
      *scratch_ptr_batch = 0.f;
      for (int j = 0; j < memory_size; ++j) {
   e172c:	e7ed      	b.n	e170a <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x6e>
   e172e:	4437      	add	r7, r6
   e1730:	4434      	add	r4, r6
    // Perform batched vector dot product:
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;
    const float* vector1_ptr = GetTensorData<float>(weights_time);
    const float* vector2_ptr =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int i = 0; i < num_filters; ++i) {
   e1732:	f108 0801 	add.w	r8, r8, #1
   e1736:	e7e0      	b.n	e16fa <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x5e>
   e1738:	9b03      	ldr	r3, [sp, #12]
   e173a:	449e      	add	lr, r3
   e173c:	9b04      	ldr	r3, [sp, #16]
  // Compute matmul(activation_state, weights_time).
  // The rightmost column is used to save temporary output (with the size of
  // num_filters). This is achieved by starting at
  // GetTensorData<float>(activation_state), and having the stride equal to
  // memory_size.
  for (int b = 0; b < batch_size; ++b) {
   e173e:	f10c 0c01 	add.w	ip, ip, #1
   e1742:	441d      	add	r5, r3
   e1744:	e7c4      	b.n	e16d0 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x34>
      scratch_ptr_batch++;
    }
  }

  // Initialize output with bias if provided.
  if (bias) {
   e1746:	9b12      	ldr	r3, [sp, #72]	; 0x48
   e1748:	b333      	cbz	r3, e1798 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xfc>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e174a:	f8d3 c004 	ldr.w	ip, [r3, #4]

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e174e:	b10a      	cbz	r2, e1754 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xb8>
   e1750:	6851      	ldr	r1, [r2, #4]
   e1752:	e000      	b.n	e1756 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xba>
   e1754:	4611      	mov	r1, r2
   e1756:	9b00      	ldr	r3, [sp, #0]
    // TODO(kreeger): doc me - VectorBatchVectorAssign
    const float* bias_data = GetTensorData<float>(bias);
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size; ++i) {
   e1758:	2400      	movs	r4, #0
   e175a:	ea4f 0e83 	mov.w	lr, r3, lsl #2
   e175e:	4284      	cmp	r4, r0
   e1760:	db0b      	blt.n	e177a <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xde>
   e1762:	9b00      	ldr	r3, [sp, #0]
   e1764:	ea4f 0983 	mov.w	r9, r3, lsl #2
   e1768:	9b10      	ldr	r3, [sp, #64]	; 0x40
        *output_ptr++ = *bias_ptr++;
      }
    }
  } else {
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size * num_units; ++i) {
   e176a:	2500      	movs	r5, #0
   e176c:	ea23 7ee3 	bic.w	lr, r3, r3, asr #31
   e1770:	ea4f 0e8e 	mov.w	lr, lr, lsl #2
   e1774:	462e      	mov	r6, r5
   e1776:	462f      	mov	r7, r5
   e1778:	e021      	b.n	e17be <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x122>
   e177a:	460e      	mov	r6, r1
    // TODO(kreeger): doc me - VectorBatchVectorAssign
    const float* bias_data = GetTensorData<float>(bias);
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size; ++i) {
      float* output_ptr = output_data + i * num_units;
      const float* bias_ptr = bias_data;
   e177c:	4667      	mov	r7, ip
      for (int j = 0; j < num_units; ++j) {
   e177e:	2500      	movs	r5, #0
   e1780:	9b00      	ldr	r3, [sp, #0]
   e1782:	429d      	cmp	r5, r3
   e1784:	da05      	bge.n	e1792 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xf6>
        *output_ptr++ = *bias_ptr++;
   e1786:	f857 8b04 	ldr.w	r8, [r7], #4
   e178a:	f846 8b04 	str.w	r8, [r6], #4
    const float* bias_data = GetTensorData<float>(bias);
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size; ++i) {
      float* output_ptr = output_data + i * num_units;
      const float* bias_ptr = bias_data;
      for (int j = 0; j < num_units; ++j) {
   e178e:	3501      	adds	r5, #1
   e1790:	e7f6      	b.n	e1780 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xe4>
  // Initialize output with bias if provided.
  if (bias) {
    // TODO(kreeger): doc me - VectorBatchVectorAssign
    const float* bias_data = GetTensorData<float>(bias);
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size; ++i) {
   e1792:	3401      	adds	r4, #1
   e1794:	4471      	add	r1, lr
   e1796:	e7e2      	b.n	e175e <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xc2>
   e1798:	b10a      	cbz	r2, e179e <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x102>
   e179a:	6854      	ldr	r4, [r2, #4]
   e179c:	e000      	b.n	e17a0 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x104>
   e179e:	4614      	mov	r4, r2
        *output_ptr++ = *bias_ptr++;
      }
    }
  } else {
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size * num_units; ++i) {
   e17a0:	9b00      	ldr	r3, [sp, #0]
   e17a2:	2100      	movs	r1, #0
   e17a4:	fb03 f500 	mul.w	r5, r3, r0
      *output_data++ = 0.0f;
   e17a8:	2600      	movs	r6, #0
        *output_ptr++ = *bias_ptr++;
      }
    }
  } else {
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size * num_units; ++i) {
   e17aa:	42a9      	cmp	r1, r5
   e17ac:	dad9      	bge.n	e1762 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0xc6>
      *output_data++ = 0.0f;
   e17ae:	f844 6b04 	str.w	r6, [r4], #4
        *output_ptr++ = *bias_ptr++;
      }
    }
  } else {
    float* output_data = GetTensorData<float>(output);
    for (int i = 0; i < batch_size * num_units; ++i) {
   e17b2:	3101      	adds	r1, #1
   e17b4:	e7f9      	b.n	e17aa <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x10e>
   e17b6:	9b04      	ldr	r3, [sp, #16]
      *output_data++ = 0.0f;
    }
  }

  // Reduction sum.
  for (int b = 0; b < batch_size; ++b) {
   e17b8:	3701      	adds	r7, #1
   e17ba:	441e      	add	r6, r3
   e17bc:	444d      	add	r5, r9
   e17be:	4287      	cmp	r7, r0
   e17c0:	da25      	bge.n	e180e <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x172>
   e17c2:	b10a      	cbz	r2, e17c8 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x12c>
   e17c4:	6851      	ldr	r1, [r2, #4]
   e17c6:	e000      	b.n	e17ca <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x12e>
   e17c8:	4611      	mov	r1, r2
   e17ca:	9b15      	ldr	r3, [sp, #84]	; 0x54
   e17cc:	b10b      	cbz	r3, e17d2 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x136>
   e17ce:	685c      	ldr	r4, [r3, #4]
   e17d0:	e000      	b.n	e17d4 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x138>
   e17d2:	9c15      	ldr	r4, [sp, #84]	; 0x54
    float* output_ptr_batch = GetTensorData<float>(output) + b * num_units;
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;
   e17d4:	4434      	add	r4, r6
   e17d6:	4429      	add	r1, r5

    // Reduction sum vector
    const float* input_vector_ptr = scratch_ptr_batch;
    for (int i = 0; i < num_units; ++i) {
   e17d8:	f04f 0c00 	mov.w	ip, #0
   e17dc:	9b00      	ldr	r3, [sp, #0]
   e17de:	459c      	cmp	ip, r3
   e17e0:	dae9      	bge.n	e17b6 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x11a>
   e17e2:	46a2      	mov	sl, r4
   e17e4:	f04f 0800 	mov.w	r8, #0
      for (int j = 0; j < rank; j++) {
   e17e8:	9b10      	ldr	r3, [sp, #64]	; 0x40
   e17ea:	4598      	cmp	r8, r3
   e17ec:	da0a      	bge.n	e1804 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x168>
        output_ptr_batch[i] += *input_vector_ptr++;
   e17ee:	edd1 7a00 	vldr	s15, [r1]
   e17f2:	ecba 7a01 	vldmia	sl!, {s14}
   e17f6:	ee77 7a87 	vadd.f32	s15, s15, s14
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;

    // Reduction sum vector
    const float* input_vector_ptr = scratch_ptr_batch;
    for (int i = 0; i < num_units; ++i) {
      for (int j = 0; j < rank; j++) {
   e17fa:	f108 0801 	add.w	r8, r8, #1
        output_ptr_batch[i] += *input_vector_ptr++;
   e17fe:	edc1 7a00 	vstr	s15, [r1]
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;

    // Reduction sum vector
    const float* input_vector_ptr = scratch_ptr_batch;
    for (int i = 0; i < num_units; ++i) {
      for (int j = 0; j < rank; j++) {
   e1802:	e7f1      	b.n	e17e8 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x14c>
   e1804:	4474      	add	r4, lr
    float* output_ptr_batch = GetTensorData<float>(output) + b * num_units;
    float* scratch_ptr_batch = GetTensorData<float>(scratch) + b * num_filters;

    // Reduction sum vector
    const float* input_vector_ptr = scratch_ptr_batch;
    for (int i = 0; i < num_units; ++i) {
   e1806:	f10c 0c01 	add.w	ip, ip, #1
   e180a:	3104      	adds	r1, #4
   e180c:	e7e6      	b.n	e17dc <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x140>
   e180e:	2400      	movs	r4, #0
inline float ActivationValFloat(TfLiteFusedActivation act, float a) {
  switch (act) {
    case kTfLiteActNone:
      return a;
    case kTfLiteActRelu:
      return a < 0.f ? 0.f : a;
   e1810:	ed9f 7a24 	vldr	s14, [pc, #144]	; e18a4 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x208>
   e1814:	4625      	mov	r5, r4
      }
    }
  }

  // Apply activation.
  for (int b = 0; b < batch_size; ++b) {
   e1816:	4285      	cmp	r5, r0
   e1818:	da20      	bge.n	e185c <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1c0>
   e181a:	b10a      	cbz	r2, e1820 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x184>
   e181c:	6851      	ldr	r1, [r2, #4]
   e181e:	e000      	b.n	e1822 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x186>
   e1820:	4611      	mov	r1, r2
    float* output_ptr_batch = GetTensorData<float>(output) + b * num_units;
   e1822:	4421      	add	r1, r4
    for (int i = 0; i < num_units; ++i) {
   e1824:	2600      	movs	r6, #0
   e1826:	9b00      	ldr	r3, [sp, #0]
   e1828:	429e      	cmp	r6, r3
   e182a:	da14      	bge.n	e1856 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1ba>
namespace ops {
namespace micro {

// Returns the floating point value for a fused activation:
inline float ActivationValFloat(TfLiteFusedActivation act, float a) {
  switch (act) {
   e182c:	9b05      	ldr	r3, [sp, #20]
      *output_ptr_batch = ActivationValFloat(activation, *output_ptr_batch);
   e182e:	edd1 7a00 	vldr	s15, [r1]
   e1832:	b163      	cbz	r3, e184e <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1b2>
   e1834:	2b01      	cmp	r3, #1
   e1836:	d107      	bne.n	e1848 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1ac>
    case kTfLiteActNone:
      return a;
    case kTfLiteActRelu:
      return a < 0.f ? 0.f : a;
   e1838:	eef5 7a40 	vcmp.f32	s15, #0.0
   e183c:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e1840:	bf48      	it	mi
   e1842:	eef0 7a47 	vmovmi.f32	s15, s14
   e1846:	e002      	b.n	e184e <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1b2>
    default:
      // TODO(kreeger): Implement more activations.
      exit(1);
   e1848:	2001      	movs	r0, #1
   e184a:	f005 ff8f 	bl	e776c <exit>
   e184e:	ece1 7a01 	vstmia	r1!, {s15}
  }

  // Apply activation.
  for (int b = 0; b < batch_size; ++b) {
    float* output_ptr_batch = GetTensorData<float>(output) + b * num_units;
    for (int i = 0; i < num_units; ++i) {
   e1852:	3601      	adds	r6, #1
   e1854:	e7e7      	b.n	e1826 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x18a>
      }
    }
  }

  // Apply activation.
  for (int b = 0; b < batch_size; ++b) {
   e1856:	3501      	adds	r5, #1
   e1858:	444c      	add	r4, r9
   e185a:	e7dc      	b.n	e1816 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x17a>
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int f = 0; f < num_filters; ++f) {
      // Shift the vector left:
      float* batch_ptr = state_ptr_batch;
      float* batch_start = state_ptr_batch + 1;
      float* batch_end = state_ptr_batch + memory_size;
   e185c:	9b01      	ldr	r3, [sp, #4]
      while (batch_start != batch_end) {
        *batch_ptr++ = *batch_start++;
      }
      state_ptr_batch[memory_size - 1] = 0.0f;
   e185e:	2200      	movs	r2, #0
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int f = 0; f < num_filters; ++f) {
      // Shift the vector left:
      float* batch_ptr = state_ptr_batch;
      float* batch_start = state_ptr_batch + 1;
      float* batch_end = state_ptr_batch + memory_size;
   e1860:	0099      	lsls	r1, r3, #2
      while (batch_start != batch_end) {
        *batch_ptr++ = *batch_start++;
      }
      state_ptr_batch[memory_size - 1] = 0.0f;
   e1862:	4615      	mov	r5, r2
   e1864:	2700      	movs	r7, #0
    }
  }

  // Left shift the activation_state to make room for next cycle's activation.
  // TODO(alanchiao): explore collapsing this into a single loop.
  for (int b = 0; b < batch_size; ++b) {
   e1866:	4285      	cmp	r5, r0
   e1868:	da19      	bge.n	e189e <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x202>
   e186a:	9b14      	ldr	r3, [sp, #80]	; 0x50
   e186c:	b10b      	cbz	r3, e1872 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1d6>
   e186e:	685b      	ldr	r3, [r3, #4]
   e1870:	e000      	b.n	e1874 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1d8>
   e1872:	9b14      	ldr	r3, [sp, #80]	; 0x50
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
   e1874:	4413      	add	r3, r2
    for (int f = 0; f < num_filters; ++f) {
   e1876:	2600      	movs	r6, #0
   e1878:	455e      	cmp	r6, fp
   e187a:	da0c      	bge.n	e1896 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1fa>
      // Shift the vector left:
      float* batch_ptr = state_ptr_batch;
      float* batch_start = state_ptr_batch + 1;
   e187c:	1d1c      	adds	r4, r3, #4
      float* batch_end = state_ptr_batch + memory_size;
   e187e:	440b      	add	r3, r1
      while (batch_start != batch_end) {
   e1880:	429c      	cmp	r4, r3
   e1882:	d004      	beq.n	e188e <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1f2>
        *batch_ptr++ = *batch_start++;
   e1884:	f854 eb04 	ldr.w	lr, [r4], #4
   e1888:	f844 ec08 	str.w	lr, [r4, #-8]
    for (int f = 0; f < num_filters; ++f) {
      // Shift the vector left:
      float* batch_ptr = state_ptr_batch;
      float* batch_start = state_ptr_batch + 1;
      float* batch_end = state_ptr_batch + memory_size;
      while (batch_start != batch_end) {
   e188c:	e7f8      	b.n	e1880 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1e4>
        *batch_ptr++ = *batch_start++;
      }
      state_ptr_batch[memory_size - 1] = 0.0f;
   e188e:	f843 7c04 	str.w	r7, [r3, #-4]
  // Left shift the activation_state to make room for next cycle's activation.
  // TODO(alanchiao): explore collapsing this into a single loop.
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int f = 0; f < num_filters; ++f) {
   e1892:	3601      	adds	r6, #1
   e1894:	e7f0      	b.n	e1878 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1dc>
   e1896:	9b03      	ldr	r3, [sp, #12]
    }
  }

  // Left shift the activation_state to make room for next cycle's activation.
  // TODO(alanchiao): explore collapsing this into a single loop.
  for (int b = 0; b < batch_size; ++b) {
   e1898:	3501      	adds	r5, #1
   e189a:	441a      	add	r2, r3
   e189c:	e7e3      	b.n	e1866 <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_+0x1ca>
      }
      state_ptr_batch[memory_size - 1] = 0.0f;
      state_ptr_batch += memory_size;
    }
  }
}
   e189e:	b007      	add	sp, #28
   e18a0:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e18a4:	00000000 	.word	0x00000000

000e18a8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode>:
  return nullptr;
}

void Free(TfLiteContext* context, void* buffer) {}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   e18a8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  // [4] = Activation State (variable),
  //         {2, batch_size, memory_size * num_filters}
  // TODO(kreeger): Use input tensor as variable until scratch tensor allocation
  // has been implemented (cl/263032056)
  // TF_LITE_ENSURE_EQ(context, node->inputs->size, 5);
  TF_LITE_ENSURE_EQ(context, node->inputs->size, 6);
   e18ac:	680d      	ldr	r5, [r1, #0]
   e18ae:	682b      	ldr	r3, [r5, #0]
   e18b0:	2b06      	cmp	r3, #6
  return nullptr;
}

void Free(TfLiteContext* context, void* buffer) {}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   e18b2:	b087      	sub	sp, #28
   e18b4:	4607      	mov	r7, r0
   e18b6:	4689      	mov	r9, r1
  // [4] = Activation State (variable),
  //         {2, batch_size, memory_size * num_filters}
  // TODO(kreeger): Use input tensor as variable until scratch tensor allocation
  // has been implemented (cl/263032056)
  // TF_LITE_ENSURE_EQ(context, node->inputs->size, 5);
  TF_LITE_ENSURE_EQ(context, node->inputs->size, 6);
   e18b8:	d00e      	beq.n	e18d8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x30>
   e18ba:	9302      	str	r3, [sp, #8]
   e18bc:	4b9f      	ldr	r3, [pc, #636]	; (e1b3c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x294>)
   e18be:	9301      	str	r3, [sp, #4]
   e18c0:	2206      	movs	r2, #6
   e18c2:	4b9f      	ldr	r3, [pc, #636]	; (e1b40 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x298>)
   e18c4:	9203      	str	r2, [sp, #12]
   e18c6:	9300      	str	r3, [sp, #0]
   e18c8:	6944      	ldr	r4, [r0, #20]
   e18ca:	4a9e      	ldr	r2, [pc, #632]	; (e1b44 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x29c>)
   e18cc:	499e      	ldr	r1, [pc, #632]	; (e1b48 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2a0>)
   e18ce:	f44f 739f 	mov.w	r3, #318	; 0x13e
   e18d2:	47a0      	blx	r4
   e18d4:	2001      	movs	r0, #1
   e18d6:	e297      	b.n	e1e08 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x560>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e18d8:	68a9      	ldr	r1, [r5, #8]
   e18da:	6883      	ldr	r3, [r0, #8]
   e18dc:	686c      	ldr	r4, [r5, #4]
   e18de:	2238      	movs	r2, #56	; 0x38
   e18e0:	4351      	muls	r1, r2
   e18e2:	1858      	adds	r0, r3, r1
   e18e4:	9105      	str	r1, [sp, #20]
}

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
   e18e6:	6929      	ldr	r1, [r5, #16]

  // Define input constants based on input tensor definition above:
  const int rank = params->rank;
  const int input_size = input->dims->data[1];
  const int batch_size = input->dims->data[0];
  const int num_filters = weights_feature->dims->data[0];
   e18e8:	6880      	ldr	r0, [r0, #8]
  if (use_tensor) {
   e18ea:	f1b1 3fff 	cmp.w	r1, #4294967295	; 0xffffffff
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e18ee:	fb02 f404 	mul.w	r4, r2, r4
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e18f2:	bf18      	it	ne
   e18f4:	fb02 3e01 	mlane	lr, r2, r1, r3
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);
  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];

  // Define input constants based on input tensor definition above:
  const int rank = params->rank;
   e18f8:	f8d9 2014 	ldr.w	r2, [r9, #20]
  const int input_size = input->dims->data[1];
  const int batch_size = input->dims->data[0];
  const int num_filters = weights_feature->dims->data[0];
   e18fc:	6841      	ldr	r1, [r0, #4]
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);
  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];

  // Define input constants based on input tensor definition above:
  const int rank = params->rank;
   e18fe:	6812      	ldr	r2, [r2, #0]
  const int input_size = input->dims->data[1];
  const int batch_size = input->dims->data[0];
  const int num_filters = weights_feature->dims->data[0];
  TF_LITE_ENSURE_EQ(context, num_filters % rank, 0);
   e1900:	fb91 fbf2 	sdiv	fp, r1, r2
   e1904:	fb02 121b 	mls	r2, r2, fp, r1
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e1908:	eb03 0604 	add.w	r6, r3, r4
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
  }
  return nullptr;
   e190c:	bf08      	it	eq
   e190e:	f04f 0e00 	moveq.w	lr, #0
   e1912:	b152      	cbz	r2, e192a <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x82>
   e1914:	2300      	movs	r3, #0
   e1916:	9303      	str	r3, [sp, #12]
   e1918:	4b8c      	ldr	r3, [pc, #560]	; (e1b4c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2a4>)
   e191a:	9301      	str	r3, [sp, #4]
   e191c:	4b8c      	ldr	r3, [pc, #560]	; (e1b50 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2a8>)
   e191e:	9300      	str	r3, [sp, #0]
   e1920:	9202      	str	r2, [sp, #8]
   e1922:	697c      	ldr	r4, [r7, #20]
   e1924:	f240 134d 	movw	r3, #333	; 0x14d
   e1928:	e21e      	b.n	e1d68 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
  const int num_units = num_filters / rank;
  const int memory_size = weights_time->dims->data[1];

  // Validate Input Tensor:
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
   e192a:	5d1c      	ldrb	r4, [r3, r4]
   e192c:	2c01      	cmp	r4, #1
   e192e:	d00a      	beq.n	e1946 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x9e>
   e1930:	4b88      	ldr	r3, [pc, #544]	; (e1b54 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2ac>)
   e1932:	9301      	str	r3, [sp, #4]
   e1934:	2501      	movs	r5, #1
   e1936:	4b88      	ldr	r3, [pc, #544]	; (e1b58 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b0>)
   e1938:	9300      	str	r3, [sp, #0]
   e193a:	9503      	str	r5, [sp, #12]
   e193c:	9402      	str	r4, [sp, #8]
   e193e:	697c      	ldr	r4, [r7, #20]
   e1940:	f44f 73a9 	mov.w	r3, #338	; 0x152
   e1944:	e210      	b.n	e1d68 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];

  // Define input constants based on input tensor definition above:
  const int rank = params->rank;
  const int input_size = input->dims->data[1];
   e1946:	68b6      	ldr	r6, [r6, #8]
#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/c_api_internal.h"

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
   e1948:	6832      	ldr	r2, [r6, #0]
  const int num_units = num_filters / rank;
  const int memory_size = weights_time->dims->data[1];

  // Validate Input Tensor:
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 2);
   e194a:	2a02      	cmp	r2, #2
   e194c:	d00a      	beq.n	e1964 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0xbc>
   e194e:	2302      	movs	r3, #2
   e1950:	9303      	str	r3, [sp, #12]
   e1952:	4b82      	ldr	r3, [pc, #520]	; (e1b5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b4>)
   e1954:	9301      	str	r3, [sp, #4]
   e1956:	4b82      	ldr	r3, [pc, #520]	; (e1b60 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b8>)
   e1958:	9300      	str	r3, [sp, #0]
   e195a:	9202      	str	r2, [sp, #8]
   e195c:	697d      	ldr	r5, [r7, #20]
   e195e:	f240 1353 	movw	r3, #339	; 0x153
   e1962:	e22f      	b.n	e1dc4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
   e1964:	f8d0 8000 	ldr.w	r8, [r0]

  // Validate Weights Feature Input Tensor:
  TF_LITE_ENSURE_EQ(context, NumDimensions(weights_feature), 2);
   e1968:	f1b8 0f02 	cmp.w	r8, #2
   e196c:	d00a      	beq.n	e1984 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0xdc>
   e196e:	4b7b      	ldr	r3, [pc, #492]	; (e1b5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b4>)
   e1970:	9301      	str	r3, [sp, #4]
   e1972:	4b7c      	ldr	r3, [pc, #496]	; (e1b64 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2bc>)
   e1974:	9300      	str	r3, [sp, #0]
   e1976:	9203      	str	r2, [sp, #12]
   e1978:	f8cd 8008 	str.w	r8, [sp, #8]
   e197c:	697d      	ldr	r5, [r7, #20]
   e197e:	f44f 73ab 	mov.w	r3, #342	; 0x156
   e1982:	e21f      	b.n	e1dc4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];

  // Define input constants based on input tensor definition above:
  const int rank = params->rank;
  const int input_size = input->dims->data[1];
   e1984:	f8d6 c008 	ldr.w	ip, [r6, #8]
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 2);

  // Validate Weights Feature Input Tensor:
  TF_LITE_ENSURE_EQ(context, NumDimensions(weights_feature), 2);
  TF_LITE_ENSURE_EQ(context, weights_feature->dims->data[1], input_size);
   e1988:	6882      	ldr	r2, [r0, #8]
   e198a:	4594      	cmp	ip, r2
   e198c:	d00a      	beq.n	e19a4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0xfc>
   e198e:	4b76      	ldr	r3, [pc, #472]	; (e1b68 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2c0>)
   e1990:	9301      	str	r3, [sp, #4]
   e1992:	4b76      	ldr	r3, [pc, #472]	; (e1b6c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2c4>)
   e1994:	9300      	str	r3, [sp, #0]
   e1996:	f8cd c00c 	str.w	ip, [sp, #12]
   e199a:	9202      	str	r2, [sp, #8]
   e199c:	697d      	ldr	r5, [r7, #20]
   e199e:	f240 1357 	movw	r3, #343	; 0x157
   e19a2:	e20f      	b.n	e1dc4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e19a4:	68ea      	ldr	r2, [r5, #12]
   e19a6:	f04f 0c38 	mov.w	ip, #56	; 0x38
   e19aa:	fb0c fc02 	mul.w	ip, ip, r2
   e19ae:	eb03 020c 	add.w	r2, r3, ip
   e19b2:	9204      	str	r2, [sp, #16]
  const int input_size = input->dims->data[1];
  const int batch_size = input->dims->data[0];
  const int num_filters = weights_feature->dims->data[0];
  TF_LITE_ENSURE_EQ(context, num_filters % rank, 0);
  const int num_units = num_filters / rank;
  const int memory_size = weights_time->dims->data[1];
   e19b4:	6890      	ldr	r0, [r2, #8]
#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/c_api_internal.h"

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
   e19b6:	6802      	ldr	r2, [r0, #0]
  // Validate Weights Feature Input Tensor:
  TF_LITE_ENSURE_EQ(context, NumDimensions(weights_feature), 2);
  TF_LITE_ENSURE_EQ(context, weights_feature->dims->data[1], input_size);

  // Validate Weights Time Input Tensor:
  TF_LITE_ENSURE_EQ(context, NumDimensions(weights_time), 2);
   e19b8:	2a02      	cmp	r2, #2
   e19ba:	d00a      	beq.n	e19d2 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x12a>
   e19bc:	4b67      	ldr	r3, [pc, #412]	; (e1b5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b4>)
   e19be:	9301      	str	r3, [sp, #4]
   e19c0:	4b6b      	ldr	r3, [pc, #428]	; (e1b70 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2c8>)
   e19c2:	9300      	str	r3, [sp, #0]
   e19c4:	f8cd 800c 	str.w	r8, [sp, #12]
   e19c8:	9202      	str	r2, [sp, #8]
   e19ca:	697d      	ldr	r5, [r7, #20]
   e19cc:	f44f 73ad 	mov.w	r3, #346	; 0x15a
   e19d0:	e1f8      	b.n	e1dc4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  TF_LITE_ENSURE_EQ(context, weights_time->dims->data[0], num_filters);
   e19d2:	6842      	ldr	r2, [r0, #4]
   e19d4:	4291      	cmp	r1, r2
   e19d6:	d009      	beq.n	e19ec <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x144>
   e19d8:	4b66      	ldr	r3, [pc, #408]	; (e1b74 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2cc>)
   e19da:	9301      	str	r3, [sp, #4]
   e19dc:	4b66      	ldr	r3, [pc, #408]	; (e1b78 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2d0>)
   e19de:	9300      	str	r3, [sp, #0]
   e19e0:	9103      	str	r1, [sp, #12]
   e19e2:	9202      	str	r2, [sp, #8]
   e19e4:	697d      	ldr	r5, [r7, #20]
   e19e6:	f240 135b 	movw	r3, #347	; 0x15b
   e19ea:	e1eb      	b.n	e1dc4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  TF_LITE_ENSURE_EQ(context, weights_time->dims->data[1], memory_size);

  // Validate Optional Bias Input Tensor:
  if (bias) {
   e19ec:	f1be 0f00 	cmp.w	lr, #0
   e19f0:	d01e      	beq.n	e1a30 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x188>
    TF_LITE_ENSURE_EQ(context, bias->dims->data[0], num_units);
   e19f2:	f8de 2008 	ldr.w	r2, [lr, #8]
   e19f6:	6852      	ldr	r2, [r2, #4]
   e19f8:	4593      	cmp	fp, r2
   e19fa:	d00a      	beq.n	e1a12 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x16a>
   e19fc:	4b5f      	ldr	r3, [pc, #380]	; (e1b7c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2d4>)
   e19fe:	9301      	str	r3, [sp, #4]
   e1a00:	4b5f      	ldr	r3, [pc, #380]	; (e1b80 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2d8>)
   e1a02:	9300      	str	r3, [sp, #0]
   e1a04:	f8cd b00c 	str.w	fp, [sp, #12]
   e1a08:	9202      	str	r2, [sp, #8]
   e1a0a:	697d      	ldr	r5, [r7, #20]
   e1a0c:	f44f 73b0 	mov.w	r3, #352	; 0x160
   e1a10:	e1d8      	b.n	e1dc4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
    TF_LITE_ENSURE_EQ(context, bias->type, kTfLiteFloat32);
   e1a12:	f89e 2000 	ldrb.w	r2, [lr]
   e1a16:	2a01      	cmp	r2, #1
   e1a18:	d00a      	beq.n	e1a30 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x188>
   e1a1a:	4b4e      	ldr	r3, [pc, #312]	; (e1b54 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2ac>)
   e1a1c:	9301      	str	r3, [sp, #4]
   e1a1e:	2401      	movs	r4, #1
   e1a20:	4b58      	ldr	r3, [pc, #352]	; (e1b84 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2dc>)
   e1a22:	9300      	str	r3, [sp, #0]
   e1a24:	9403      	str	r4, [sp, #12]
   e1a26:	9202      	str	r2, [sp, #8]
   e1a28:	697d      	ldr	r5, [r7, #20]
   e1a2a:	f240 1361 	movw	r3, #353	; 0x161
   e1a2e:	e1c9      	b.n	e1dc4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
      GetInput(context, node, kWeightsFeatureTensor);
  const TfLiteTensor* weights_time =
      GetInput(context, node, kWeightsTimeTensor);
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);
  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];
   e1a30:	696a      	ldr	r2, [r5, #20]
   e1a32:	f04f 0a38 	mov.w	sl, #56	; 0x38
   e1a36:	fb0a f202 	mul.w	r2, sl, r2
   e1a3a:	eb03 0e02 	add.w	lr, r3, r2
    TF_LITE_ENSURE_EQ(context, bias->dims->data[0], num_units);
    TF_LITE_ENSURE_EQ(context, bias->type, kTfLiteFloat32);
  }

  // Validate Activation State Input Tensor:
  TF_LITE_ENSURE_EQ(context, activation_state->type, kTfLiteFloat32);
   e1a3e:	5c9c      	ldrb	r4, [r3, r2]
   e1a40:	2c01      	cmp	r4, #1
   e1a42:	d00a      	beq.n	e1a5a <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x1b2>
   e1a44:	4b43      	ldr	r3, [pc, #268]	; (e1b54 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2ac>)
   e1a46:	9301      	str	r3, [sp, #4]
   e1a48:	2501      	movs	r5, #1
   e1a4a:	4b4f      	ldr	r3, [pc, #316]	; (e1b88 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2e0>)
   e1a4c:	9300      	str	r3, [sp, #0]
   e1a4e:	9503      	str	r5, [sp, #12]
   e1a50:	9402      	str	r4, [sp, #8]
   e1a52:	697c      	ldr	r4, [r7, #20]
   e1a54:	f240 1365 	movw	r3, #357	; 0x165
   e1a58:	e186      	b.n	e1d68 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
   e1a5a:	f8de e008 	ldr.w	lr, [lr, #8]
   e1a5e:	f8de 2000 	ldr.w	r2, [lr]
  TF_LITE_ENSURE_EQ(context, NumDimensions(activation_state), 2);
   e1a62:	2a02      	cmp	r2, #2
   e1a64:	d00a      	beq.n	e1a7c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x1d4>
   e1a66:	2302      	movs	r3, #2
   e1a68:	9303      	str	r3, [sp, #12]
   e1a6a:	4b3c      	ldr	r3, [pc, #240]	; (e1b5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b4>)
   e1a6c:	9301      	str	r3, [sp, #4]
   e1a6e:	4b47      	ldr	r3, [pc, #284]	; (e1b8c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2e4>)
   e1a70:	9300      	str	r3, [sp, #0]
   e1a72:	9202      	str	r2, [sp, #8]
   e1a74:	697d      	ldr	r5, [r7, #20]
   e1a76:	f44f 73b3 	mov.w	r3, #358	; 0x166
   e1a7a:	e1a3      	b.n	e1dc4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];

  // Define input constants based on input tensor definition above:
  const int rank = params->rank;
  const int input_size = input->dims->data[1];
  const int batch_size = input->dims->data[0];
   e1a7c:	f8d6 8004 	ldr.w	r8, [r6, #4]
  }

  // Validate Activation State Input Tensor:
  TF_LITE_ENSURE_EQ(context, activation_state->type, kTfLiteFloat32);
  TF_LITE_ENSURE_EQ(context, NumDimensions(activation_state), 2);
  TF_LITE_ENSURE_EQ(context, activation_state->dims->data[0], batch_size);
   e1a80:	f8de 6004 	ldr.w	r6, [lr, #4]
   e1a84:	45b0      	cmp	r8, r6
   e1a86:	d00a      	beq.n	e1a9e <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x1f6>
   e1a88:	4b41      	ldr	r3, [pc, #260]	; (e1b90 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2e8>)
   e1a8a:	9301      	str	r3, [sp, #4]
   e1a8c:	4b41      	ldr	r3, [pc, #260]	; (e1b94 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2ec>)
   e1a8e:	9300      	str	r3, [sp, #0]
   e1a90:	f8cd 800c 	str.w	r8, [sp, #12]
   e1a94:	9602      	str	r6, [sp, #8]
   e1a96:	697d      	ldr	r5, [r7, #20]
   e1a98:	f240 1367 	movw	r3, #359	; 0x167
   e1a9c:	e192      	b.n	e1dc4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  const int input_size = input->dims->data[1];
  const int batch_size = input->dims->data[0];
  const int num_filters = weights_feature->dims->data[0];
  TF_LITE_ENSURE_EQ(context, num_filters % rank, 0);
  const int num_units = num_filters / rank;
  const int memory_size = weights_time->dims->data[1];
   e1a9e:	6880      	ldr	r0, [r0, #8]

  // Validate Activation State Input Tensor:
  TF_LITE_ENSURE_EQ(context, activation_state->type, kTfLiteFloat32);
  TF_LITE_ENSURE_EQ(context, NumDimensions(activation_state), 2);
  TF_LITE_ENSURE_EQ(context, activation_state->dims->data[0], batch_size);
  TF_LITE_ENSURE_EQ(context, activation_state->dims->data[1],
   e1aa0:	f8de 6008 	ldr.w	r6, [lr, #8]
   e1aa4:	fb00 fe01 	mul.w	lr, r0, r1
   e1aa8:	4576      	cmp	r6, lr
   e1aaa:	d00a      	beq.n	e1ac2 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x21a>
   e1aac:	4b3a      	ldr	r3, [pc, #232]	; (e1b98 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2f0>)
   e1aae:	9301      	str	r3, [sp, #4]
   e1ab0:	4b3a      	ldr	r3, [pc, #232]	; (e1b9c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2f4>)
   e1ab2:	9300      	str	r3, [sp, #0]
   e1ab4:	f8cd e00c 	str.w	lr, [sp, #12]
   e1ab8:	9602      	str	r6, [sp, #8]
   e1aba:	697d      	ldr	r5, [r7, #20]
   e1abc:	f240 1369 	movw	r3, #361	; 0x169
   e1ac0:	e180      	b.n	e1dc4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  //       ApplyTimeWeightsBiasAndActivation():
  //         float, {2, batch_size, num_filters}
  // TODO(kreeger): Use input tensor as variable until scratch tensor allocation
  // has been implemented (cl/263032056)
  // TfLiteTensor* scratch_tensor = GetTemporary(context, node, 0);
  TfLiteTensor* scratch_tensor = &context->tensors[node->inputs->data[5]];
   e1ac2:	69ad      	ldr	r5, [r5, #24]
   e1ac4:	fb0a fa05 	mul.w	sl, sl, r5
   e1ac8:	eb03 060a 	add.w	r6, r3, sl

  TF_LITE_ENSURE_EQ(context, scratch_tensor->type, kTfLiteFloat32);
   e1acc:	f813 500a 	ldrb.w	r5, [r3, sl]
   e1ad0:	2d01      	cmp	r5, #1
   e1ad2:	d009      	beq.n	e1ae8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x240>
   e1ad4:	4b1f      	ldr	r3, [pc, #124]	; (e1b54 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2ac>)
   e1ad6:	9301      	str	r3, [sp, #4]
   e1ad8:	4b31      	ldr	r3, [pc, #196]	; (e1ba0 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2f8>)
   e1ada:	9300      	str	r3, [sp, #0]
   e1adc:	9403      	str	r4, [sp, #12]
   e1ade:	9502      	str	r5, [sp, #8]
   e1ae0:	697d      	ldr	r5, [r7, #20]
   e1ae2:	f44f 73ba 	mov.w	r3, #372	; 0x174
   e1ae6:	e16d      	b.n	e1dc4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
   e1ae8:	68b4      	ldr	r4, [r6, #8]
   e1aea:	6826      	ldr	r6, [r4, #0]
  TF_LITE_ENSURE_EQ(context, NumDimensions(scratch_tensor), 2);
   e1aec:	2e02      	cmp	r6, #2
   e1aee:	d009      	beq.n	e1b04 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x25c>
   e1af0:	4b1a      	ldr	r3, [pc, #104]	; (e1b5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2b4>)
   e1af2:	9301      	str	r3, [sp, #4]
   e1af4:	4b2b      	ldr	r3, [pc, #172]	; (e1ba4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2fc>)
   e1af6:	9300      	str	r3, [sp, #0]
   e1af8:	9203      	str	r2, [sp, #12]
   e1afa:	9602      	str	r6, [sp, #8]
   e1afc:	697c      	ldr	r4, [r7, #20]
   e1afe:	f240 1375 	movw	r3, #373	; 0x175
   e1b02:	e131      	b.n	e1d68 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
  TF_LITE_ENSURE_EQ(context, scratch_tensor->dims->data[0], batch_size);
   e1b04:	6862      	ldr	r2, [r4, #4]
   e1b06:	4590      	cmp	r8, r2
   e1b08:	d00a      	beq.n	e1b20 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x278>
   e1b0a:	4b21      	ldr	r3, [pc, #132]	; (e1b90 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2e8>)
   e1b0c:	9301      	str	r3, [sp, #4]
   e1b0e:	4b26      	ldr	r3, [pc, #152]	; (e1ba8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x300>)
   e1b10:	9300      	str	r3, [sp, #0]
   e1b12:	f8cd 800c 	str.w	r8, [sp, #12]
   e1b16:	9202      	str	r2, [sp, #8]
   e1b18:	697c      	ldr	r4, [r7, #20]
   e1b1a:	f44f 73bb 	mov.w	r3, #374	; 0x176
   e1b1e:	e123      	b.n	e1d68 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
  TF_LITE_ENSURE_EQ(context, scratch_tensor->dims->data[1], num_filters);
   e1b20:	68a2      	ldr	r2, [r4, #8]
   e1b22:	4291      	cmp	r1, r2
   e1b24:	d044      	beq.n	e1bb0 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x308>
   e1b26:	4b13      	ldr	r3, [pc, #76]	; (e1b74 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2cc>)
   e1b28:	9301      	str	r3, [sp, #4]
   e1b2a:	4b20      	ldr	r3, [pc, #128]	; (e1bac <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x304>)
   e1b2c:	9300      	str	r3, [sp, #0]
   e1b2e:	9103      	str	r1, [sp, #12]
   e1b30:	9202      	str	r2, [sp, #8]
   e1b32:	697c      	ldr	r4, [r7, #20]
   e1b34:	f240 1377 	movw	r3, #375	; 0x177
   e1b38:	e116      	b.n	e1d68 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
   e1b3a:	bf00      	nop
   e1b3c:	000e78c0 	.word	0x000e78c0
   e1b40:	000eaa6e 	.word	0x000eaa6e
   e1b44:	000ea9c8 	.word	0x000ea9c8
   e1b48:	000e98c8 	.word	0x000e98c8
   e1b4c:	000eb18d 	.word	0x000eb18d
   e1b50:	000eaa81 	.word	0x000eaa81
   e1b54:	000ea137 	.word	0x000ea137
   e1b58:	000e9903 	.word	0x000e9903
   e1b5c:	000ea278 	.word	0x000ea278
   e1b60:	000ea6cc 	.word	0x000ea6cc
   e1b64:	000eaa94 	.word	0x000eaa94
   e1b68:	000eaab3 	.word	0x000eaab3
   e1b6c:	000eaabe 	.word	0x000eaabe
   e1b70:	000eaadd 	.word	0x000eaadd
   e1b74:	000eab92 	.word	0x000eab92
   e1b78:	000eadd7 	.word	0x000eadd7
   e1b7c:	000eaaf9 	.word	0x000eaaf9
   e1b80:	000eab03 	.word	0x000eab03
   e1b84:	000eab17 	.word	0x000eab17
   e1b88:	000eab22 	.word	0x000eab22
   e1b8c:	000eab39 	.word	0x000eab39
   e1b90:	000eab59 	.word	0x000eab59
   e1b94:	000eab64 	.word	0x000eab64
   e1b98:	000eab84 	.word	0x000eab84
   e1b9c:	000eab9e 	.word	0x000eab9e
   e1ba0:	000eabbe 	.word	0x000eabbe
   e1ba4:	000eabd3 	.word	0x000eabd3
   e1ba8:	000eabf1 	.word	0x000eabf1
   e1bac:	000eac0f 	.word	0x000eac0f
   e1bb0:	9a05      	ldr	r2, [sp, #20]
   e1bb2:	5c9c      	ldrb	r4, [r3, r2]
}

// Determines whether it is a hybrid op - one that has float inputs and
// quantized weights.
inline bool IsHybridOp(const TfLiteTensor* input, const TfLiteTensor* weight) {
  return ((weight->type == kTfLiteUInt8 || weight->type == kTfLiteInt8) &&
   e1bb4:	2c03      	cmp	r4, #3
   e1bb6:	d002      	beq.n	e1bbe <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x316>
   e1bb8:	2c09      	cmp	r4, #9
   e1bba:	f040 810a 	bne.w	e1dd2 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x52a>
  // TODO(kreeger): Handle full quant svdf b/139435798
  if (is_hybrid_op) {
    // Validate Input Tensor dtypes:
    TF_LITE_ENSURE(context, weights_feature->type == kTfLiteUInt8 ||
                                weights_feature->type == kTfLiteInt8);
    TF_LITE_ENSURE(context, weights_time->type == kTfLiteUInt8 ||
   e1bbe:	f813 200c 	ldrb.w	r2, [r3, ip]
   e1bc2:	2a03      	cmp	r2, #3
   e1bc4:	d007      	beq.n	e1bd6 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x32e>
   e1bc6:	2a09      	cmp	r2, #9
   e1bc8:	d005      	beq.n	e1bd6 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x32e>
   e1bca:	4b91      	ldr	r3, [pc, #580]	; (e1e10 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x568>)
   e1bcc:	9300      	str	r3, [sp, #0]
   e1bce:	697c      	ldr	r4, [r7, #20]
   e1bd0:	f240 1381 	movw	r3, #385	; 0x181
   e1bd4:	e026      	b.n	e1c24 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x37c>
    // Validate Scratch Tensors:
    // [0] = (shared - see above for usage)
    // [1] = Input Quantized, int8_t/uint8_t, {2, batch_size, input_size}
    // [2] = Scaling Factors, float, {1, batch_size}
    // [3] = Float Weights Time, float, {2, num_filters, memory_size}
    TF_LITE_ENSURE_EQ(context, node->temporaries->size, 4);
   e1bd6:	f8d9 500c 	ldr.w	r5, [r9, #12]
   e1bda:	682a      	ldr	r2, [r5, #0]
   e1bdc:	2a04      	cmp	r2, #4
   e1bde:	d00a      	beq.n	e1bf6 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x34e>
   e1be0:	2304      	movs	r3, #4
   e1be2:	9303      	str	r3, [sp, #12]
   e1be4:	4b8b      	ldr	r3, [pc, #556]	; (e1e14 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x56c>)
   e1be6:	9301      	str	r3, [sp, #4]
   e1be8:	4b8b      	ldr	r3, [pc, #556]	; (e1e18 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x570>)
   e1bea:	9300      	str	r3, [sp, #0]
   e1bec:	9202      	str	r2, [sp, #8]
   e1bee:	697c      	ldr	r4, [r7, #20]
   e1bf0:	f44f 73c4 	mov.w	r3, #392	; 0x188
   e1bf4:	e0b8      	b.n	e1d68 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
}
inline TfLiteTensor* GetTemporary(TfLiteContext* context, TfLiteNode* node,
                                  int index) {
  return &context->tensors[flatbuffers::EndianScalar(
      node->temporaries->data[index])];
   e1bf6:	68ae      	ldr	r6, [r5, #8]
   e1bf8:	68ec      	ldr	r4, [r5, #12]
   e1bfa:	692d      	ldr	r5, [r5, #16]
   e1bfc:	2238      	movs	r2, #56	; 0x38
   e1bfe:	4356      	muls	r6, r2
   e1c00:	4354      	muls	r4, r2
   e1c02:	436a      	muls	r2, r5
    TfLiteTensor* scratch_input_quantized = GetTemporary(context, node, 1);
    TfLiteTensor* scratch_scaling_factors = GetTemporary(context, node, 2);
    TfLiteTensor* scratch_float_weights_time = GetTemporary(context, node, 3);

    // Validate Input Quantized Scratch Tensor:
    TF_LITE_ENSURE(context, scratch_input_quantized->type == kTfLiteUInt8 ||
   e1c04:	5d9d      	ldrb	r5, [r3, r6]
   e1c06:	2d03      	cmp	r5, #3
   e1c08:	eb03 0a06 	add.w	sl, r3, r6
   e1c0c:	eb03 0c04 	add.w	ip, r3, r4
   e1c10:	eb03 0e02 	add.w	lr, r3, r2
   e1c14:	d00b      	beq.n	e1c2e <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x386>
   e1c16:	2d09      	cmp	r5, #9
   e1c18:	d009      	beq.n	e1c2e <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x386>
   e1c1a:	4b80      	ldr	r3, [pc, #512]	; (e1e1c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x574>)
   e1c1c:	9300      	str	r3, [sp, #0]
   e1c1e:	697c      	ldr	r4, [r7, #20]
   e1c20:	f240 138f 	movw	r3, #399	; 0x18f
   e1c24:	4a7e      	ldr	r2, [pc, #504]	; (e1e20 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x578>)
   e1c26:	497f      	ldr	r1, [pc, #508]	; (e1e24 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x57c>)
   e1c28:	4638      	mov	r0, r7
   e1c2a:	47a0      	blx	r4
   e1c2c:	e652      	b.n	e18d4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2c>
                                scratch_input_quantized->type == kTfLiteInt8);
    TF_LITE_ENSURE_EQ(context, scratch_input_quantized->dims->data[0],
   e1c2e:	f8da 5008 	ldr.w	r5, [sl, #8]
   e1c32:	686d      	ldr	r5, [r5, #4]
   e1c34:	45a8      	cmp	r8, r5
   e1c36:	d00a      	beq.n	e1c4e <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x3a6>
   e1c38:	4b7b      	ldr	r3, [pc, #492]	; (e1e28 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x580>)
   e1c3a:	9301      	str	r3, [sp, #4]
   e1c3c:	4b7b      	ldr	r3, [pc, #492]	; (e1e2c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x584>)
   e1c3e:	9300      	str	r3, [sp, #0]
   e1c40:	f8cd 800c 	str.w	r8, [sp, #12]
   e1c44:	9502      	str	r5, [sp, #8]
   e1c46:	697c      	ldr	r4, [r7, #20]
   e1c48:	f240 1391 	movw	r3, #401	; 0x191
   e1c4c:	e08c      	b.n	e1d68 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
                      batch_size);

    // Validate Scaling Factors Scratch Tensor:
    TF_LITE_ENSURE_EQ(context, scratch_scaling_factors->type, kTfLiteFloat32);
   e1c4e:	5d1e      	ldrb	r6, [r3, r4]
   e1c50:	2e01      	cmp	r6, #1
   e1c52:	d00a      	beq.n	e1c6a <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x3c2>
   e1c54:	4b76      	ldr	r3, [pc, #472]	; (e1e30 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x588>)
   e1c56:	9301      	str	r3, [sp, #4]
   e1c58:	2401      	movs	r4, #1
   e1c5a:	4b76      	ldr	r3, [pc, #472]	; (e1e34 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x58c>)
   e1c5c:	9300      	str	r3, [sp, #0]
   e1c5e:	9403      	str	r4, [sp, #12]
   e1c60:	9602      	str	r6, [sp, #8]
   e1c62:	697d      	ldr	r5, [r7, #20]
   e1c64:	f44f 73ca 	mov.w	r3, #404	; 0x194
   e1c68:	e0ac      	b.n	e1dc4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
   e1c6a:	f8dc 4008 	ldr.w	r4, [ip, #8]
#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/c_api_internal.h"

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
   e1c6e:	6825      	ldr	r5, [r4, #0]
    TF_LITE_ENSURE_EQ(context, NumDimensions(scratch_scaling_factors), 1);
   e1c70:	2d01      	cmp	r5, #1
   e1c72:	d009      	beq.n	e1c88 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x3e0>
   e1c74:	4b70      	ldr	r3, [pc, #448]	; (e1e38 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x590>)
   e1c76:	9301      	str	r3, [sp, #4]
   e1c78:	4b70      	ldr	r3, [pc, #448]	; (e1e3c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x594>)
   e1c7a:	9300      	str	r3, [sp, #0]
   e1c7c:	9603      	str	r6, [sp, #12]
   e1c7e:	9502      	str	r5, [sp, #8]
   e1c80:	697c      	ldr	r4, [r7, #20]
   e1c82:	f240 1395 	movw	r3, #405	; 0x195
   e1c86:	e06f      	b.n	e1d68 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
    TF_LITE_ENSURE_EQ(context, scratch_scaling_factors->dims->data[0],
   e1c88:	6864      	ldr	r4, [r4, #4]
   e1c8a:	45a0      	cmp	r8, r4
   e1c8c:	d00a      	beq.n	e1ca4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x3fc>
   e1c8e:	4b66      	ldr	r3, [pc, #408]	; (e1e28 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x580>)
   e1c90:	9301      	str	r3, [sp, #4]
   e1c92:	4b6b      	ldr	r3, [pc, #428]	; (e1e40 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x598>)
   e1c94:	9300      	str	r3, [sp, #0]
   e1c96:	f8cd 800c 	str.w	r8, [sp, #12]
   e1c9a:	9402      	str	r4, [sp, #8]
   e1c9c:	697c      	ldr	r4, [r7, #20]
   e1c9e:	f240 1397 	movw	r3, #407	; 0x197
   e1ca2:	e061      	b.n	e1d68 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
                      batch_size);

    // Validate Float Weights Time Scratch Tensor:
    TF_LITE_ENSURE_EQ(context, scratch_float_weights_time->type,
   e1ca4:	5c9c      	ldrb	r4, [r3, r2]
   e1ca6:	2c01      	cmp	r4, #1
   e1ca8:	d009      	beq.n	e1cbe <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x416>
   e1caa:	4b61      	ldr	r3, [pc, #388]	; (e1e30 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x588>)
   e1cac:	9301      	str	r3, [sp, #4]
   e1cae:	4b65      	ldr	r3, [pc, #404]	; (e1e44 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x59c>)
   e1cb0:	9300      	str	r3, [sp, #0]
   e1cb2:	9503      	str	r5, [sp, #12]
   e1cb4:	9402      	str	r4, [sp, #8]
   e1cb6:	697c      	ldr	r4, [r7, #20]
   e1cb8:	f240 139b 	movw	r3, #411	; 0x19b
   e1cbc:	e054      	b.n	e1d68 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
   e1cbe:	f8de 3008 	ldr.w	r3, [lr, #8]
   e1cc2:	681a      	ldr	r2, [r3, #0]
                      kTfLiteFloat32);
    TF_LITE_ENSURE_EQ(context, NumDimensions(scratch_float_weights_time), 2);
   e1cc4:	2a02      	cmp	r2, #2
   e1cc6:	d00a      	beq.n	e1cde <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x436>
   e1cc8:	2302      	movs	r3, #2
   e1cca:	9303      	str	r3, [sp, #12]
   e1ccc:	4b5e      	ldr	r3, [pc, #376]	; (e1e48 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5a0>)
   e1cce:	9301      	str	r3, [sp, #4]
   e1cd0:	4b5e      	ldr	r3, [pc, #376]	; (e1e4c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5a4>)
   e1cd2:	9300      	str	r3, [sp, #0]
   e1cd4:	9202      	str	r2, [sp, #8]
   e1cd6:	697d      	ldr	r5, [r7, #20]
   e1cd8:	f44f 73ce 	mov.w	r3, #412	; 0x19c
   e1cdc:	e072      	b.n	e1dc4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
    TF_LITE_ENSURE_EQ(context, scratch_float_weights_time->dims->data[0],
   e1cde:	685a      	ldr	r2, [r3, #4]
   e1ce0:	4291      	cmp	r1, r2
   e1ce2:	d009      	beq.n	e1cf8 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x450>
   e1ce4:	4b5a      	ldr	r3, [pc, #360]	; (e1e50 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5a8>)
   e1ce6:	9301      	str	r3, [sp, #4]
   e1ce8:	4b5a      	ldr	r3, [pc, #360]	; (e1e54 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5ac>)
   e1cea:	9300      	str	r3, [sp, #0]
   e1cec:	9103      	str	r1, [sp, #12]
   e1cee:	9202      	str	r2, [sp, #8]
   e1cf0:	697d      	ldr	r5, [r7, #20]
   e1cf2:	f44f 73cf 	mov.w	r3, #414	; 0x19e
   e1cf6:	e065      	b.n	e1dc4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
                      num_filters);
    TF_LITE_ENSURE_EQ(context, scratch_float_weights_time->dims->data[1],
   e1cf8:	689b      	ldr	r3, [r3, #8]
   e1cfa:	4298      	cmp	r0, r3
   e1cfc:	d009      	beq.n	e1d12 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x46a>
   e1cfe:	9302      	str	r3, [sp, #8]
   e1d00:	4b55      	ldr	r3, [pc, #340]	; (e1e58 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5b0>)
   e1d02:	9301      	str	r3, [sp, #4]
   e1d04:	4b55      	ldr	r3, [pc, #340]	; (e1e5c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5b4>)
   e1d06:	9300      	str	r3, [sp, #0]
   e1d08:	9003      	str	r0, [sp, #12]
   e1d0a:	697d      	ldr	r5, [r7, #20]
   e1d0c:	f44f 73d0 	mov.w	r3, #416	; 0x1a0
   e1d10:	e058      	b.n	e1dc4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
      weights_time_ptr = GetTensorData<int8_t>(weights_time);
    }
    SymmetricDequantize(weights_time_ptr,
                        NumElements(scratch_float_weights_time),
                        weights_time->params.scale,
                        GetTensorData<float>(scratch_float_weights_time));
   e1d12:	9b04      	ldr	r3, [sp, #16]
   e1d14:	f8de 2004 	ldr.w	r2, [lr, #4]
   e1d18:	ed93 0a03 	vldr	s0, [r3, #12]
   e1d1c:	4341      	muls	r1, r0
   e1d1e:	6858      	ldr	r0, [r3, #4]
   e1d20:	f7f4 fb54 	bl	d63cc <_ZN6tflite19SymmetricDequantizeEPKaifPf>
    // TF_LITE_ENSURE_EQ(context, node->temporaries->size, 1);
  }

  // Validate Tensor Output:
  // [0] = float, {2, batch_size, num_units}
  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);
   e1d24:	f8d9 3004 	ldr.w	r3, [r9, #4]
   e1d28:	681d      	ldr	r5, [r3, #0]
   e1d2a:	2d01      	cmp	r5, #1
   e1d2c:	d00a      	beq.n	e1d44 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x49c>
   e1d2e:	4b42      	ldr	r3, [pc, #264]	; (e1e38 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x590>)
   e1d30:	9301      	str	r3, [sp, #4]
   e1d32:	2401      	movs	r4, #1
   e1d34:	4b4a      	ldr	r3, [pc, #296]	; (e1e60 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5b8>)
   e1d36:	9300      	str	r3, [sp, #0]
   e1d38:	9403      	str	r4, [sp, #12]
   e1d3a:	9502      	str	r5, [sp, #8]
   e1d3c:	697d      	ldr	r5, [r7, #20]
   e1d3e:	f44f 73e0 	mov.w	r3, #448	; 0x1c0
   e1d42:	e03f      	b.n	e1dc4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   e1d44:	685a      	ldr	r2, [r3, #4]
   e1d46:	2338      	movs	r3, #56	; 0x38
   e1d48:	4353      	muls	r3, r2
   e1d4a:	68ba      	ldr	r2, [r7, #8]
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  TF_LITE_ENSURE_EQ(context, output->type, kTfLiteFloat32);
   e1d4c:	5cd4      	ldrb	r4, [r2, r3]
   e1d4e:	2c01      	cmp	r4, #1
   e1d50:	eb02 0103 	add.w	r1, r2, r3
   e1d54:	d00c      	beq.n	e1d70 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c8>
   e1d56:	4b36      	ldr	r3, [pc, #216]	; (e1e30 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x588>)
   e1d58:	9301      	str	r3, [sp, #4]
   e1d5a:	4b42      	ldr	r3, [pc, #264]	; (e1e64 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5bc>)
   e1d5c:	9300      	str	r3, [sp, #0]
   e1d5e:	9503      	str	r5, [sp, #12]
   e1d60:	9402      	str	r4, [sp, #8]
   e1d62:	697c      	ldr	r4, [r7, #20]
   e1d64:	f44f 73e1 	mov.w	r3, #450	; 0x1c2
   e1d68:	4a2d      	ldr	r2, [pc, #180]	; (e1e20 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x578>)
   e1d6a:	493f      	ldr	r1, [pc, #252]	; (e1e68 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5c0>)
   e1d6c:	4638      	mov	r0, r7
   e1d6e:	e5b0      	b.n	e18d2 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2a>
   e1d70:	688b      	ldr	r3, [r1, #8]
#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/c_api_internal.h"

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
   e1d72:	681a      	ldr	r2, [r3, #0]
  TF_LITE_ENSURE_EQ(context, NumDimensions(output), 2);
   e1d74:	2a02      	cmp	r2, #2
   e1d76:	d00a      	beq.n	e1d8e <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4e6>
   e1d78:	2302      	movs	r3, #2
   e1d7a:	9303      	str	r3, [sp, #12]
   e1d7c:	4b32      	ldr	r3, [pc, #200]	; (e1e48 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5a0>)
   e1d7e:	9301      	str	r3, [sp, #4]
   e1d80:	4b3a      	ldr	r3, [pc, #232]	; (e1e6c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5c4>)
   e1d82:	9300      	str	r3, [sp, #0]
   e1d84:	9202      	str	r2, [sp, #8]
   e1d86:	697d      	ldr	r5, [r7, #20]
   e1d88:	f240 13c3 	movw	r3, #451	; 0x1c3
   e1d8c:	e01a      	b.n	e1dc4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  TF_LITE_ENSURE_EQ(context, output->dims->data[0], batch_size);
   e1d8e:	685a      	ldr	r2, [r3, #4]
   e1d90:	4590      	cmp	r8, r2
   e1d92:	d00a      	beq.n	e1daa <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x502>
   e1d94:	4b24      	ldr	r3, [pc, #144]	; (e1e28 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x580>)
   e1d96:	9301      	str	r3, [sp, #4]
   e1d98:	4b35      	ldr	r3, [pc, #212]	; (e1e70 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5c8>)
   e1d9a:	9300      	str	r3, [sp, #0]
   e1d9c:	f8cd 800c 	str.w	r8, [sp, #12]
   e1da0:	9202      	str	r2, [sp, #8]
   e1da2:	697d      	ldr	r5, [r7, #20]
   e1da4:	f44f 73e2 	mov.w	r3, #452	; 0x1c4
   e1da8:	e00c      	b.n	e1dc4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  TF_LITE_ENSURE_EQ(context, output->dims->data[1], num_units);
   e1daa:	689b      	ldr	r3, [r3, #8]
   e1dac:	459b      	cmp	fp, r3
   e1dae:	d00e      	beq.n	e1dce <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x526>
   e1db0:	9302      	str	r3, [sp, #8]
   e1db2:	4b30      	ldr	r3, [pc, #192]	; (e1e74 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5cc>)
   e1db4:	9301      	str	r3, [sp, #4]
   e1db6:	4b30      	ldr	r3, [pc, #192]	; (e1e78 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5d0>)
   e1db8:	9300      	str	r3, [sp, #0]
   e1dba:	f8cd b00c 	str.w	fp, [sp, #12]
   e1dbe:	697d      	ldr	r5, [r7, #20]
   e1dc0:	f240 13c5 	movw	r3, #453	; 0x1c5
   e1dc4:	4a16      	ldr	r2, [pc, #88]	; (e1e20 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x578>)
   e1dc6:	4928      	ldr	r1, [pc, #160]	; (e1e68 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5c0>)
   e1dc8:	4638      	mov	r0, r7
   e1dca:	47a8      	blx	r5
   e1dcc:	e582      	b.n	e18d4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x2c>

  return kTfLiteOk;
   e1dce:	2000      	movs	r0, #0
   e1dd0:	e01a      	b.n	e1e08 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x560>
                        NumElements(scratch_float_weights_time),
                        weights_time->params.scale,
                        GetTensorData<float>(scratch_float_weights_time));
  } else {
    // Validate Input Tensor dtypes:
    TF_LITE_ENSURE_EQ(context, weights_feature->type, kTfLiteFloat32);
   e1dd2:	2c01      	cmp	r4, #1
   e1dd4:	d00a      	beq.n	e1dec <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x544>
   e1dd6:	4b16      	ldr	r3, [pc, #88]	; (e1e30 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x588>)
   e1dd8:	9301      	str	r3, [sp, #4]
   e1dda:	2501      	movs	r5, #1
   e1ddc:	4b27      	ldr	r3, [pc, #156]	; (e1e7c <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5d4>)
   e1dde:	9300      	str	r3, [sp, #0]
   e1de0:	9503      	str	r5, [sp, #12]
   e1de2:	9402      	str	r4, [sp, #8]
   e1de4:	697c      	ldr	r4, [r7, #20]
   e1de6:	f44f 73da 	mov.w	r3, #436	; 0x1b4
   e1dea:	e7bd      	b.n	e1d68 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x4c0>
    TF_LITE_ENSURE_EQ(context, weights_time->type, kTfLiteFloat32);
   e1dec:	f813 300c 	ldrb.w	r3, [r3, ip]
   e1df0:	2b01      	cmp	r3, #1
   e1df2:	d097      	beq.n	e1d24 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x47c>
   e1df4:	9302      	str	r3, [sp, #8]
   e1df6:	4b0e      	ldr	r3, [pc, #56]	; (e1e30 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x588>)
   e1df8:	9301      	str	r3, [sp, #4]
   e1dfa:	4b21      	ldr	r3, [pc, #132]	; (e1e80 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x5d8>)
   e1dfc:	9300      	str	r3, [sp, #0]
   e1dfe:	9403      	str	r4, [sp, #12]
   e1e00:	697d      	ldr	r5, [r7, #20]
   e1e02:	f240 13b5 	movw	r3, #437	; 0x1b5
   e1e06:	e7dd      	b.n	e1dc4 <_ZN6tflite3ops5micro4svdf7PrepareEP13TfLiteContextP10TfLiteNode+0x51c>
  TF_LITE_ENSURE_EQ(context, NumDimensions(output), 2);
  TF_LITE_ENSURE_EQ(context, output->dims->data[0], batch_size);
  TF_LITE_ENSURE_EQ(context, output->dims->data[1], num_units);

  return kTfLiteOk;
}
   e1e08:	b007      	add	sp, #28
   e1e0a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e1e0e:	bf00      	nop
   e1e10:	000eac2d 	.word	0x000eac2d
   e1e14:	000ea992 	.word	0x000ea992
   e1e18:	000eac75 	.word	0x000eac75
   e1e1c:	000eac8d 	.word	0x000eac8d
   e1e20:	000ea9c8 	.word	0x000ea9c8
   e1e24:	000e9a98 	.word	0x000e9a98
   e1e28:	000eab59 	.word	0x000eab59
   e1e2c:	000eaceb 	.word	0x000eaceb
   e1e30:	000ea137 	.word	0x000ea137
   e1e34:	000ead12 	.word	0x000ead12
   e1e38:	000eb295 	.word	0x000eb295
   e1e3c:	000ead30 	.word	0x000ead30
   e1e40:	000ead57 	.word	0x000ead57
   e1e44:	000ead7e 	.word	0x000ead7e
   e1e48:	000ea278 	.word	0x000ea278
   e1e4c:	000ead9f 	.word	0x000ead9f
   e1e50:	000eab92 	.word	0x000eab92
   e1e54:	000eadc9 	.word	0x000eadc9
   e1e58:	000eadf3 	.word	0x000eadf3
   e1e5c:	000eadff 	.word	0x000eadff
   e1e60:	000e9ad3 	.word	0x000e9ad3
   e1e64:	000e990f 	.word	0x000e990f
   e1e68:	000e98c8 	.word	0x000e98c8
   e1e6c:	000eae3f 	.word	0x000eae3f
   e1e70:	000eae55 	.word	0x000eae55
   e1e74:	000eaaf9 	.word	0x000eaaf9
   e1e78:	000eae6b 	.word	0x000eae6b
   e1e7c:	000eae29 	.word	0x000eae29
   e1e80:	000ead8c 	.word	0x000ead8c

000e1e84 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e1e84:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e1e88:	ed2d 8b02 	vpush	{d8}
   e1e8c:	b097      	sub	sp, #92	; 0x5c
   e1e8e:	680a      	ldr	r2, [r1, #0]
  auto* params = reinterpret_cast<TfLiteSVDFParams*>(node->builtin_data);
   e1e90:	694b      	ldr	r3, [r1, #20]
   e1e92:	9309      	str	r3, [sp, #36]	; 0x24
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e1e94:	6893      	ldr	r3, [r2, #8]
   e1e96:	6855      	ldr	r5, [r2, #4]
   e1e98:	f04f 0c38 	mov.w	ip, #56	; 0x38
   e1e9c:	fb0c fe03 	mul.w	lr, ip, r3
}

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
   e1ea0:	6913      	ldr	r3, [r2, #16]
  TF_LITE_ENSURE_EQ(context, output->dims->data[1], num_units);

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e1ea2:	4607      	mov	r7, r0
   e1ea4:	6880      	ldr	r0, [r0, #8]
  if (use_tensor) {
   e1ea6:	1c5c      	adds	r4, r3, #1
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e1ea8:	bf14      	ite	ne
   e1eaa:	fb0c 0303 	mlane	r3, ip, r3, r0
  }
  return nullptr;
   e1eae:	2300      	moveq	r3, #0
   e1eb0:	930b      	str	r3, [sp, #44]	; 0x2c
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);

  // TODO(kreeger): Use input tensor as variable until scratch tensor allocation
  // has been implemented (cl/263032056)
  // TfLiteTensor* scratch = GetTemporary(context, node, /*index=*/0);
  TfLiteTensor* scratch = &context->tensors[node->inputs->data[5]];
   e1eb2:	6993      	ldr	r3, [r2, #24]

  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];
   e1eb4:	6954      	ldr	r4, [r2, #20]
  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);

  // TODO(kreeger): Use input tensor as variable until scratch tensor allocation
  // has been implemented (cl/263032056)
  // TfLiteTensor* scratch = GetTemporary(context, node, /*index=*/0);
  TfLiteTensor* scratch = &context->tensors[node->inputs->data[5]];
   e1eb6:	fb0c 0303 	mla	r3, ip, r3, r0
   e1eba:	930e      	str	r3, [sp, #56]	; 0x38

  TfLiteTensor* activation_state =
      &context->tensors[node->inputs->data[kInputActivationStateTensor]];
   e1ebc:	fb0c 0304 	mla	r3, ip, r4, r0
   e1ec0:	9308      	str	r3, [sp, #32]
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   e1ec2:	684b      	ldr	r3, [r1, #4]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e1ec4:	eb00 060e 	add.w	r6, r0, lr
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   e1ec8:	685b      	ldr	r3, [r3, #4]
  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

  switch (weights_feature->type) {
   e1eca:	f810 e00e 	ldrb.w	lr, [r0, lr]
   e1ece:	fb0c 0303 	mla	r3, ip, r3, r0
   e1ed2:	f1be 0f03 	cmp.w	lr, #3
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e1ed6:	fb0c 0505 	mla	r5, ip, r5, r0
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   e1eda:	930d      	str	r3, [sp, #52]	; 0x34
   e1edc:	f000 809f 	beq.w	e201e <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x19a>
   e1ee0:	f1be 0f09 	cmp.w	lr, #9
   e1ee4:	f000 809b 	beq.w	e201e <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x19a>
   e1ee8:	f1be 0f01 	cmp.w	lr, #1
   e1eec:	f040 816d 	bne.w	e21ca <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x346>
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e1ef0:	68d3      	ldr	r3, [r2, #12]
   e1ef2:	2238      	movs	r2, #56	; 0x38
   e1ef4:	fb02 0303 	mla	r3, r2, r3, r0
   e1ef8:	930c      	str	r3, [sp, #48]	; 0x30
                          const TfLiteTensor* weights_time,
                          const TfLiteTensor* bias,
                          const TfLiteSVDFParams* params, TfLiteTensor* scratch,
                          TfLiteTensor* activation_state,
                          TfLiteTensor* output) {
  const int rank = params->rank;
   e1efa:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e1efc:	681b      	ldr	r3, [r3, #0]
   e1efe:	930f      	str	r3, [sp, #60]	; 0x3c
  const int batch_size = input->dims->data[0];
   e1f00:	68ab      	ldr	r3, [r5, #8]
   e1f02:	f8d3 b004 	ldr.w	fp, [r3, #4]
  const int input_size = input->dims->data[1];
   e1f06:	689b      	ldr	r3, [r3, #8]
   e1f08:	930a      	str	r3, [sp, #40]	; 0x28
  const int num_filters = weights_feature->dims->data[0];
   e1f0a:	68b3      	ldr	r3, [r6, #8]
   e1f0c:	685a      	ldr	r2, [r3, #4]
  const int num_units = num_filters / rank;
   e1f0e:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   e1f10:	fb92 f3f3 	sdiv	r3, r2, r3
   e1f14:	9310      	str	r3, [sp, #64]	; 0x40
  const int memory_size = weights_time->dims->data[1];
   e1f16:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   e1f18:	689b      	ldr	r3, [r3, #8]
   e1f1a:	f8d3 a008 	ldr.w	sl, [r3, #8]
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0f;
   e1f1e:	f10a 4380 	add.w	r3, sl, #1073741824	; 0x40000000
   e1f22:	3b01      	subs	r3, #1
   e1f24:	009b      	lsls	r3, r3, #2
   e1f26:	fb0a f702 	mul.w	r7, sl, r2
   e1f2a:	00bf      	lsls	r7, r7, #2
   e1f2c:	f103 0804 	add.w	r8, r3, #4
   e1f30:	4618      	mov	r0, r3
  const int memory_size = weights_time->dims->data[1];

  // Clear the activation (activation_state's leftmost column).
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in leftmost column and make sure it passes.
  for (int b = 0; b < batch_size; ++b) {
   e1f32:	f04f 0e00 	mov.w	lr, #0
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0f;
   e1f36:	f04f 0900 	mov.w	r9, #0
  const int memory_size = weights_time->dims->data[1];

  // Clear the activation (activation_state's leftmost column).
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in leftmost column and make sure it passes.
  for (int b = 0; b < batch_size; ++b) {
   e1f3a:	45f3      	cmp	fp, lr
   e1f3c:	dd13      	ble.n	e1f66 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xe2>
   e1f3e:	9908      	ldr	r1, [sp, #32]
   e1f40:	b109      	cbz	r1, e1f46 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xc2>
   e1f42:	6849      	ldr	r1, [r1, #4]
   e1f44:	e000      	b.n	e1f48 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xc4>
   e1f46:	9908      	ldr	r1, [sp, #32]
   e1f48:	4401      	add	r1, r0
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
   e1f4a:	f04f 0c00 	mov.w	ip, #0
   e1f4e:	4562      	cmp	r2, ip
   e1f50:	dd05      	ble.n	e1f5e <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xda>
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0f;
   e1f52:	f8c1 9000 	str.w	r9, [r1]
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in leftmost column and make sure it passes.
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
   e1f56:	f10c 0c01 	add.w	ip, ip, #1
   e1f5a:	4441      	add	r1, r8
   e1f5c:	e7f7      	b.n	e1f4e <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xca>
  const int memory_size = weights_time->dims->data[1];

  // Clear the activation (activation_state's leftmost column).
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in leftmost column and make sure it passes.
  for (int b = 0; b < batch_size; ++b) {
   e1f5e:	f10e 0e01 	add.w	lr, lr, #1
   e1f62:	4438      	add	r0, r7
   e1f64:	e7e9      	b.n	e1f3a <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xb6>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e1f66:	6871      	ldr	r1, [r6, #4]
   e1f68:	9111      	str	r1, [sp, #68]	; 0x44
   e1f6a:	6869      	ldr	r1, [r5, #4]
   e1f6c:	9112      	str	r1, [sp, #72]	; 0x48

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e1f6e:	9908      	ldr	r1, [sp, #32]
   e1f70:	b109      	cbz	r1, e1f76 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xf2>
   e1f72:	6849      	ldr	r1, [r1, #4]
   e1f74:	e000      	b.n	e1f78 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0xf4>
   e1f76:	9908      	ldr	r1, [sp, #32]
   e1f78:	980a      	ldr	r0, [sp, #40]	; 0x28
   e1f7a:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
   e1f7e:	0080      	lsls	r0, r0, #2
  // stride equal to memory_size.

  // Perform batched matrix vector multiply accumulate operation:
  const float* matrix = GetTensorData<float>(weights_feature);
  const float* vector = GetTensorData<float>(input);
  float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
   e1f80:	4419      	add	r1, r3
   e1f82:	ea22 77e2 	bic.w	r7, r2, r2, asr #31
      const float* vector_in_batch = vector + i * input_size;
      for (int k = 0; k < input_size; ++k) {
        dot_prod += *matrix_ptr++ * *vector_in_batch++;
      }
      *result_in_batch += dot_prod;
      result_in_batch += memory_size;
   e1f86:	3304      	adds	r3, #4
   e1f88:	9015      	str	r0, [sp, #84]	; 0x54
   e1f8a:	fb03 f007 	mul.w	r0, r3, r7
   e1f8e:	9014      	str	r0, [sp, #80]	; 0x50
   e1f90:	2000      	movs	r0, #0
  // Perform batched matrix vector multiply accumulate operation:
  const float* matrix = GetTensorData<float>(weights_feature);
  const float* vector = GetTensorData<float>(input);
  float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
  float* result_in_batch = result;
  for (int i = 0; i < batch_size; ++i) {
   e1f92:	4606      	mov	r6, r0
   e1f94:	45b3      	cmp	fp, r6
   e1f96:	dd2f      	ble.n	e1ff8 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x174>
    const float* matrix_ptr = matrix;
    for (int j = 0; j < num_filters; ++j) {
      float dot_prod = 0.0f;
      const float* vector_in_batch = vector + i * input_size;
   e1f98:	9d12      	ldr	r5, [sp, #72]	; 0x48
   e1f9a:	f8dd c044 	ldr.w	ip, [sp, #68]	; 0x44
   e1f9e:	eb05 0580 	add.w	r5, r5, r0, lsl #2
   e1fa2:	9513      	str	r5, [sp, #76]	; 0x4c
   e1fa4:	f04f 0e00 	mov.w	lr, #0
   e1fa8:	460d      	mov	r5, r1
  const float* vector = GetTensorData<float>(input);
  float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
  float* result_in_batch = result;
  for (int i = 0; i < batch_size; ++i) {
    const float* matrix_ptr = matrix;
    for (int j = 0; j < num_filters; ++j) {
   e1faa:	4572      	cmp	r2, lr
   e1fac:	dd1e      	ble.n	e1fec <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x168>
   e1fae:	f8dd 904c 	ldr.w	r9, [sp, #76]	; 0x4c
   e1fb2:	ed9f 7a8d 	vldr	s14, [pc, #564]	; e21e8 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x364>
   e1fb6:	4667      	mov	r7, ip
   e1fb8:	f04f 0800 	mov.w	r8, #0
      float dot_prod = 0.0f;
      const float* vector_in_batch = vector + i * input_size;
      for (int k = 0; k < input_size; ++k) {
   e1fbc:	9c0a      	ldr	r4, [sp, #40]	; 0x28
   e1fbe:	4544      	cmp	r4, r8
   e1fc0:	dd08      	ble.n	e1fd4 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x150>
        dot_prod += *matrix_ptr++ * *vector_in_batch++;
   e1fc2:	ecf7 6a01 	vldmia	r7!, {s13}
   e1fc6:	ecf9 7a01 	vldmia	r9!, {s15}
  for (int i = 0; i < batch_size; ++i) {
    const float* matrix_ptr = matrix;
    for (int j = 0; j < num_filters; ++j) {
      float dot_prod = 0.0f;
      const float* vector_in_batch = vector + i * input_size;
      for (int k = 0; k < input_size; ++k) {
   e1fca:	f108 0801 	add.w	r8, r8, #1
        dot_prod += *matrix_ptr++ * *vector_in_batch++;
   e1fce:	eea6 7aa7 	vfma.f32	s14, s13, s15
   e1fd2:	e7f3      	b.n	e1fbc <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x138>
      }
      *result_in_batch += dot_prod;
   e1fd4:	edd5 7a00 	vldr	s15, [r5]
   e1fd8:	9c15      	ldr	r4, [sp, #84]	; 0x54
   e1fda:	ee77 7a87 	vadd.f32	s15, s15, s14
   e1fde:	44a4      	add	ip, r4
   e1fe0:	edc5 7a00 	vstr	s15, [r5]
  const float* vector = GetTensorData<float>(input);
  float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
  float* result_in_batch = result;
  for (int i = 0; i < batch_size; ++i) {
    const float* matrix_ptr = matrix;
    for (int j = 0; j < num_filters; ++j) {
   e1fe4:	f10e 0e01 	add.w	lr, lr, #1
      const float* vector_in_batch = vector + i * input_size;
      for (int k = 0; k < input_size; ++k) {
        dot_prod += *matrix_ptr++ * *vector_in_batch++;
      }
      *result_in_batch += dot_prod;
      result_in_batch += memory_size;
   e1fe8:	441d      	add	r5, r3
   e1fea:	e7de      	b.n	e1faa <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x126>
   e1fec:	9c14      	ldr	r4, [sp, #80]	; 0x50
   e1fee:	4421      	add	r1, r4
   e1ff0:	9c0a      	ldr	r4, [sp, #40]	; 0x28
  // Perform batched matrix vector multiply accumulate operation:
  const float* matrix = GetTensorData<float>(weights_feature);
  const float* vector = GetTensorData<float>(input);
  float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
  float* result_in_batch = result;
  for (int i = 0; i < batch_size; ++i) {
   e1ff2:	3601      	adds	r6, #1
   e1ff4:	4420      	add	r0, r4
   e1ff6:	e7cd      	b.n	e1f94 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x110>
    }
  }

  ApplyTimeWeightsBiasAndActivation(
      batch_size, memory_size, num_filters, num_units, rank, weights_time, bias,
      params->activation, activation_state, scratch, output);
   e1ff8:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   e1ffa:	9306      	str	r3, [sp, #24]
   e1ffc:	9b0e      	ldr	r3, [sp, #56]	; 0x38
   e1ffe:	9305      	str	r3, [sp, #20]
   e2000:	9b08      	ldr	r3, [sp, #32]
   e2002:	9304      	str	r3, [sp, #16]
   e2004:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e2006:	791b      	ldrb	r3, [r3, #4]
   e2008:	9303      	str	r3, [sp, #12]
   e200a:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e200c:	9302      	str	r3, [sp, #8]
   e200e:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   e2010:	9301      	str	r3, [sp, #4]
   e2012:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   e2014:	9300      	str	r3, [sp, #0]
   e2016:	4651      	mov	r1, sl
   e2018:	9b10      	ldr	r3, [sp, #64]	; 0x40
   e201a:	4658      	mov	r0, fp
   e201c:	e0d1      	b.n	e21c2 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x33e>
   e201e:	68cf      	ldr	r7, [r1, #12]
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
}
inline TfLiteTensor* GetTemporary(TfLiteContext* context, TfLiteNode* node,
                                  int index) {
  return &context->tensors[flatbuffers::EndianScalar(
      node->temporaries->data[index])];
   e2020:	68ba      	ldr	r2, [r7, #8]
   e2022:	68fb      	ldr	r3, [r7, #12]
   e2024:	693f      	ldr	r7, [r7, #16]
   e2026:	2138      	movs	r1, #56	; 0x38
   e2028:	fb01 0202 	mla	r2, r1, r2, r0
   e202c:	fb01 0303 	mla	r3, r1, r3, r0
   e2030:	fb01 0107 	mla	r1, r1, r7, r0
   e2034:	9110      	str	r1, [sp, #64]	; 0x40
    const TfLiteTensor* weights_feature, const TfLiteTensor* weights_time,
    const TfLiteTensor* bias, const TfLiteSVDFParams* params,
    TfLiteTensor* scratch, TfLiteTensor* scaling_factors,
    TfLiteTensor* input_quantized, TfLiteTensor* activation_state,
    TfLiteTensor* output) {
  const int rank = params->rank;
   e2036:	9909      	ldr	r1, [sp, #36]	; 0x24
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e2038:	6868      	ldr	r0, [r5, #4]
   e203a:	6809      	ldr	r1, [r1, #0]
   e203c:	910f      	str	r1, [sp, #60]	; 0x3c
  const int batch_size = input->dims->data[0];
   e203e:	68a9      	ldr	r1, [r5, #8]
   e2040:	f8d1 a004 	ldr.w	sl, [r1, #4]
  const int input_size = input->dims->data[1];
   e2044:	6889      	ldr	r1, [r1, #8]
   e2046:	910a      	str	r1, [sp, #40]	; 0x28
  const int num_filters = weights_feature->dims->data[0];
   e2048:	68b1      	ldr	r1, [r6, #8]
   e204a:	f8d1 b004 	ldr.w	fp, [r1, #4]
  const int num_units = num_filters / rank;
   e204e:	990f      	ldr	r1, [sp, #60]	; 0x3c
   e2050:	fb9b f1f1 	sdiv	r1, fp, r1
   e2054:	9111      	str	r1, [sp, #68]	; 0x44
  const int memory_size = weights_time->dims->data[1];
   e2056:	9910      	ldr	r1, [sp, #64]	; 0x40
   e2058:	6889      	ldr	r1, [r1, #8]
   e205a:	6889      	ldr	r1, [r1, #8]
   e205c:	910c      	str	r1, [sp, #48]	; 0x30
   e205e:	6871      	ldr	r1, [r6, #4]
   e2060:	9114      	str	r1, [sp, #80]	; 0x50

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e2062:	b112      	cbz	r2, e206a <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x1e6>
   e2064:	f8d2 9004 	ldr.w	r9, [r2, #4]
   e2068:	e000      	b.n	e206c <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x1e8>
   e206a:	4691      	mov	r9, r2
   e206c:	b10b      	cbz	r3, e2072 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x1ee>
   e206e:	685f      	ldr	r7, [r3, #4]
   e2070:	e000      	b.n	e2074 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x1f0>
   e2072:	461f      	mov	r7, r3
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0;
   e2074:	9b0c      	ldr	r3, [sp, #48]	; 0x30

  // Initialize the pointer to storage for scaling factors.
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors);

  // Initialize the weights scale.
  const float weights_feature_scale = weights_feature->params.scale;
   e2076:	ed96 8a03 	vldr	s16, [r6, #12]
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0;
   e207a:	eddf 7a5b 	vldr	s15, [pc, #364]	; e21e8 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x364>
   e207e:	f103 4280 	add.w	r2, r3, #1073741824	; 0x40000000
   e2082:	3a01      	subs	r2, #1
   e2084:	0096      	lsls	r6, r2, #2
   e2086:	fb03 f10b 	mul.w	r1, r3, fp
   e208a:	0089      	lsls	r1, r1, #2
   e208c:	f106 0804 	add.w	r8, r6, #4
   e2090:	4632      	mov	r2, r6

  // Clear the activation (activation_state's leftmost column).
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in the leftmost column and make sure it passes.
  // TODO(kreeger): Use a port of tensor_utils when ready (b/140272187).
  for (int b = 0; b < batch_size; ++b) {
   e2092:	f04f 0e00 	mov.w	lr, #0
   e2096:	45f2      	cmp	sl, lr
   e2098:	dd13      	ble.n	e20c2 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x23e>
   e209a:	9b08      	ldr	r3, [sp, #32]
   e209c:	b10b      	cbz	r3, e20a2 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x21e>
   e209e:	685b      	ldr	r3, [r3, #4]
   e20a0:	e000      	b.n	e20a4 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x220>
   e20a2:	9b08      	ldr	r3, [sp, #32]
   e20a4:	4413      	add	r3, r2
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
   e20a6:	f04f 0c00 	mov.w	ip, #0
   e20aa:	45e3      	cmp	fp, ip
   e20ac:	dd05      	ble.n	e20ba <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x236>
      float* state_ptr = state_ptr_batch + c * memory_size;
      state_ptr[memory_size - 1] = 0.0;
   e20ae:	edc3 7a00 	vstr	s15, [r3]
  // values in the leftmost column and make sure it passes.
  // TODO(kreeger): Use a port of tensor_utils when ready (b/140272187).
  for (int b = 0; b < batch_size; ++b) {
    float* state_ptr_batch =
        GetTensorData<float>(activation_state) + b * memory_size * num_filters;
    for (int c = 0; c < num_filters; ++c) {
   e20b2:	f10c 0c01 	add.w	ip, ip, #1
   e20b6:	4443      	add	r3, r8
   e20b8:	e7f7      	b.n	e20aa <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x226>

  // Clear the activation (activation_state's leftmost column).
  // TODO(ghodrat): Add a test which initialize activation_state with invalid
  // values in the leftmost column and make sure it passes.
  // TODO(kreeger): Use a port of tensor_utils when ready (b/140272187).
  for (int b = 0; b < batch_size; ++b) {
   e20ba:	f10e 0e01 	add.w	lr, lr, #1
   e20be:	440a      	add	r2, r1
   e20c0:	e7e9      	b.n	e2096 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x212>
    }
  }

  // Determine if input pointer batch is a zero based vector:
  bool is_zero_vector = true;
  for (int i = 0; i < batch_size * input_size && is_zero_vector; ++i) {
   e20c2:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e20c4:	4601      	mov	r1, r0
   e20c6:	fb03 fe0a 	mul.w	lr, r3, sl
   e20ca:	2200      	movs	r2, #0
   e20cc:	2301      	movs	r3, #1
   e20ce:	4596      	cmp	lr, r2
   e20d0:	dd0b      	ble.n	e20ea <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x266>
   e20d2:	b163      	cbz	r3, e20ee <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x26a>
    if (input_ptr_batch[i] != 0.0f) {
   e20d4:	ecf1 7a01 	vldmia	r1!, {s15}
   e20d8:	eef5 7a40 	vcmp.f32	s15, #0.0
   e20dc:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e20e0:	bf0c      	ite	eq
   e20e2:	2301      	moveq	r3, #1
   e20e4:	2300      	movne	r3, #0
    }
  }

  // Determine if input pointer batch is a zero based vector:
  bool is_zero_vector = true;
  for (int i = 0; i < batch_size * input_size && is_zero_vector; ++i) {
   e20e6:	3201      	adds	r2, #1
   e20e8:	e7f1      	b.n	e20ce <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x24a>
    if (input_ptr_batch[i] != 0.0f) {
      is_zero_vector = false;
    }
  }

  if (!is_zero_vector) {
   e20ea:	2b00      	cmp	r3, #0
   e20ec:	d156      	bne.n	e219c <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x318>
    SignedSymmetricPerChannelQuantize(input_ptr_batch, input->dims, 0,
                                      quantized_input_ptr_batch,
                                      scaling_factors_ptr);
   e20ee:	9700      	str	r7, [sp, #0]
   e20f0:	464b      	mov	r3, r9
   e20f2:	2200      	movs	r2, #0
   e20f4:	68a9      	ldr	r1, [r5, #8]
   e20f6:	f7f4 f8d1 	bl	d629c <_ZN6tflite33SignedSymmetricPerChannelQuantizeEPKfP14TfLiteIntArrayiPaPf>
   e20fa:	463b      	mov	r3, r7
   e20fc:	4639      	mov	r1, r7

    // Quantize input from float to int8.
    for (int b = 0; b < batch_size; ++b) {
   e20fe:	2200      	movs	r2, #0
   e2100:	4592      	cmp	sl, r2
   e2102:	dd07      	ble.n	e2114 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x290>
      scaling_factors_ptr[b] *= weights_feature_scale;
   e2104:	edd1 7a00 	vldr	s15, [r1]
   e2108:	ee67 7a88 	vmul.f32	s15, s15, s16
    SignedSymmetricPerChannelQuantize(input_ptr_batch, input->dims, 0,
                                      quantized_input_ptr_batch,
                                      scaling_factors_ptr);

    // Quantize input from float to int8.
    for (int b = 0; b < batch_size; ++b) {
   e210c:	3201      	adds	r2, #1
      scaling_factors_ptr[b] *= weights_feature_scale;
   e210e:	ece1 7a01 	vstmia	r1!, {s15}
   e2112:	e7f5      	b.n	e2100 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x27c>
   e2114:	9a08      	ldr	r2, [sp, #32]
   e2116:	b10a      	cbz	r2, e211c <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x298>
   e2118:	6851      	ldr	r1, [r2, #4]
   e211a:	e000      	b.n	e211e <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x29a>
   e211c:	9908      	ldr	r1, [sp, #32]
         ++i, quantized_input_ptr_batch += input_size) {
      const float batch_scaling_factor = scaling_factors_ptr[i];

      // Get the address of the first row:
      const int8_t* row_ptr = weights_feature_ptr;
      for (int j = 0; j < num_filters; ++j, result += memory_size) {
   e211e:	1d32      	adds	r2, r6, #4
   e2120:	ea2b 7eeb 	bic.w	lr, fp, fp, asr #31
   e2124:	fb02 f00e 	mul.w	r0, r2, lr
   e2128:	9013      	str	r0, [sp, #76]	; 0x4c
   e212a:	980a      	ldr	r0, [sp, #40]	; 0x28
   e212c:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
    // Compute conv1d(inputs, weights_feature).
    // The rightmost column of activation_state is used to save the current
    // cycle activation. This is achieved by starting at
    // GetTensorData<float>(activation_state)[memory_size - 1] and having the
    // stride equal to memory_size. (Matrix batch vector multiply accumulate)
    float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
   e2130:	4431      	add	r1, r6
    for (int i = 0; i < batch_size;
   e2132:	2700      	movs	r7, #0
   e2134:	9015      	str	r0, [sp, #84]	; 0x54
   e2136:	45ba      	cmp	sl, r7
   e2138:	dd30      	ble.n	e219c <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x318>
         ++i, quantized_input_ptr_batch += input_size) {
      const float batch_scaling_factor = scaling_factors_ptr[i];
   e213a:	ecf3 6a01 	vldmia	r3!, {s13}

      // Get the address of the first row:
      const int8_t* row_ptr = weights_feature_ptr;
   e213e:	f8dd 8050 	ldr.w	r8, [sp, #80]	; 0x50
    // GetTensorData<float>(activation_state)[memory_size - 1] and having the
    // stride equal to memory_size. (Matrix batch vector multiply accumulate)
    float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
    for (int i = 0; i < batch_size;
         ++i, quantized_input_ptr_batch += input_size) {
      const float batch_scaling_factor = scaling_factors_ptr[i];
   e2142:	460e      	mov	r6, r1

      // Get the address of the first row:
      const int8_t* row_ptr = weights_feature_ptr;
      for (int j = 0; j < num_filters; ++j, result += memory_size) {
   e2144:	f04f 0c00 	mov.w	ip, #0
   e2148:	45e3      	cmp	fp, ip
   e214a:	dd21      	ble.n	e2190 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x30c>
   e214c:	2000      	movs	r0, #0
   e214e:	4605      	mov	r5, r0
        // Initialize the dot product sum for the row to 0.
        int32_t dotprod = 0;
        for (int k = 0; k < input_size; ++k, ++row_ptr) {
   e2150:	9c0a      	ldr	r4, [sp, #40]	; 0x28
   e2152:	4284      	cmp	r4, r0
   e2154:	dd0c      	ble.n	e2170 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x2ec>
          dotprod += (*row_ptr) * (quantized_input_ptr_batch[k]);
   e2156:	f918 4000 	ldrsb.w	r4, [r8, r0]
   e215a:	46a6      	mov	lr, r4
   e215c:	f919 4000 	ldrsb.w	r4, [r9, r0]
   e2160:	9412      	str	r4, [sp, #72]	; 0x48
   e2162:	4674      	mov	r4, lr
   e2164:	f8bd e048 	ldrh.w	lr, [sp, #72]	; 0x48
      // Get the address of the first row:
      const int8_t* row_ptr = weights_feature_ptr;
      for (int j = 0; j < num_filters; ++j, result += memory_size) {
        // Initialize the dot product sum for the row to 0.
        int32_t dotprod = 0;
        for (int k = 0; k < input_size; ++k, ++row_ptr) {
   e2168:	3001      	adds	r0, #1
          dotprod += (*row_ptr) * (quantized_input_ptr_batch[k]);
   e216a:	fb14 550e 	smlabb	r5, r4, lr, r5
   e216e:	e7ef      	b.n	e2150 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x2cc>
        }
        *result += dotprod * batch_scaling_factor;
   e2170:	ee07 5a90 	vmov	s15, r5
   e2174:	ed96 7a00 	vldr	s14, [r6]
   e2178:	9815      	ldr	r0, [sp, #84]	; 0x54
   e217a:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   e217e:	4480      	add	r8, r0
   e2180:	eea6 7aa7 	vfma.f32	s14, s13, s15
         ++i, quantized_input_ptr_batch += input_size) {
      const float batch_scaling_factor = scaling_factors_ptr[i];

      // Get the address of the first row:
      const int8_t* row_ptr = weights_feature_ptr;
      for (int j = 0; j < num_filters; ++j, result += memory_size) {
   e2184:	f10c 0c01 	add.w	ip, ip, #1
        // Initialize the dot product sum for the row to 0.
        int32_t dotprod = 0;
        for (int k = 0; k < input_size; ++k, ++row_ptr) {
          dotprod += (*row_ptr) * (quantized_input_ptr_batch[k]);
        }
        *result += dotprod * batch_scaling_factor;
   e2188:	ed86 7a00 	vstr	s14, [r6]
         ++i, quantized_input_ptr_batch += input_size) {
      const float batch_scaling_factor = scaling_factors_ptr[i];

      // Get the address of the first row:
      const int8_t* row_ptr = weights_feature_ptr;
      for (int j = 0; j < num_filters; ++j, result += memory_size) {
   e218c:	4416      	add	r6, r2
   e218e:	e7db      	b.n	e2148 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x2c4>
   e2190:	9813      	ldr	r0, [sp, #76]	; 0x4c
   e2192:	4401      	add	r1, r0
   e2194:	980a      	ldr	r0, [sp, #40]	; 0x28
    // The rightmost column of activation_state is used to save the current
    // cycle activation. This is achieved by starting at
    // GetTensorData<float>(activation_state)[memory_size - 1] and having the
    // stride equal to memory_size. (Matrix batch vector multiply accumulate)
    float* result = &GetTensorData<float>(activation_state)[memory_size - 1];
    for (int i = 0; i < batch_size;
   e2196:	3701      	adds	r7, #1
   e2198:	4481      	add	r9, r0
   e219a:	e7cc      	b.n	e2136 <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x2b2>

  // TODO(alanchiao): can optimize hybrid case ~5% by unrolling loop in applying
  // time weights so that the inner loop multiplies eight elements at a time.
  ApplyTimeWeightsBiasAndActivation(
      batch_size, memory_size, num_filters, num_units, rank, weights_time, bias,
      params->activation, activation_state, scratch, output);
   e219c:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   e219e:	9306      	str	r3, [sp, #24]
   e21a0:	9b0e      	ldr	r3, [sp, #56]	; 0x38
   e21a2:	9305      	str	r3, [sp, #20]
   e21a4:	9b08      	ldr	r3, [sp, #32]
   e21a6:	9304      	str	r3, [sp, #16]
   e21a8:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e21aa:	990c      	ldr	r1, [sp, #48]	; 0x30
   e21ac:	791b      	ldrb	r3, [r3, #4]
   e21ae:	9303      	str	r3, [sp, #12]
   e21b0:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e21b2:	9302      	str	r3, [sp, #8]
   e21b4:	9b10      	ldr	r3, [sp, #64]	; 0x40
   e21b6:	9301      	str	r3, [sp, #4]
   e21b8:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   e21ba:	9300      	str	r3, [sp, #0]
   e21bc:	9b11      	ldr	r3, [sp, #68]	; 0x44
   e21be:	465a      	mov	r2, fp
   e21c0:	4650      	mov	r0, sl
   e21c2:	f7ff fa6b 	bl	e169c <_ZN6tflite3ops5micro4svdf12_GLOBAL__N_1L33ApplyTimeWeightsBiasAndActivationEiiiiiPK12TfLiteTensorS6_21TfLiteFusedActivationPS4_S8_S8_>
      TfLiteTensor* scratch_float_weights_time = GetTemporary(context, node, 3);
      EvalHybridSVDF(context, node, input, weights_feature,
                     scratch_float_weights_time, bias, params, scratch,
                     scratch_scaling_factors, scratch_input_quantized,
                     activation_state, output);
      return kTfLiteOk;
   e21c6:	2000      	movs	r0, #0
   e21c8:	e008      	b.n	e21dc <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x358>
      break;
    }

    default:
      // TODO(kreeger): Handle this case for full quant svdf b/139435798
      context->ReportError(context, "Type %s not currently supported.",
   e21ca:	4670      	mov	r0, lr
   e21cc:	697c      	ldr	r4, [r7, #20]
   e21ce:	f7f1 ffa5 	bl	d411c <TfLiteTypeGetName>
                           TfLiteTypeGetName(weights_feature->type));
   e21d2:	4906      	ldr	r1, [pc, #24]	; (e21ec <_ZN6tflite3ops5micro4svdf4EvalEP13TfLiteContextP10TfLiteNode+0x368>)
   e21d4:	4602      	mov	r2, r0
   e21d6:	4638      	mov	r0, r7
   e21d8:	47a0      	blx	r4
      return kTfLiteError;
   e21da:	2001      	movs	r0, #1
  }
  return kTfLiteOk;
}
   e21dc:	b017      	add	sp, #92	; 0x5c
   e21de:	ecbd 8b02 	vpop	{d8}
   e21e2:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e21e6:	bf00      	nop
   e21e8:	00000000 	.word	0x00000000
   e21ec:	000e9f62 	.word	0x000e9f62

000e21f0 <_ZN6tflite3ops5micro13Register_SVDFEv>:

TfLiteRegistration* Register_SVDF() {
  static TfLiteRegistration r = {svdf::Init, svdf::Free, svdf::Prepare,
                                 svdf::Eval};
  return &r;
}
   e21f0:	4800      	ldr	r0, [pc, #0]	; (e21f4 <_ZN6tflite3ops5micro13Register_SVDFEv+0x4>)
   e21f2:	4770      	bx	lr
   e21f4:	2003c33c 	.word	0x2003c33c

000e21f8 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_17PrepareEP13TfLiteContextP10TfLiteNode>:

constexpr int kInputTensor = 0;

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   e21f8:	2000      	movs	r0, #0
   e21fa:	4770      	bx	lr

000e21fc <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode>:
  }

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e21fc:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e2200:	680b      	ldr	r3, [r1, #0]
   e2202:	6885      	ldr	r5, [r0, #8]
  TfLiteUnpackParams* data =
      reinterpret_cast<TfLiteUnpackParams*>(node->builtin_data);
   e2204:	694a      	ldr	r2, [r1, #20]
  }

  return kTfLiteOk;
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e2206:	b085      	sub	sp, #20
   e2208:	9001      	str	r0, [sp, #4]
   e220a:	6858      	ldr	r0, [r3, #4]
   e220c:	2338      	movs	r3, #56	; 0x38
   e220e:	4358      	muls	r0, r3
   e2210:	182b      	adds	r3, r5, r0
  TfLiteUnpackParams* data =
      reinterpret_cast<TfLiteUnpackParams*>(node->builtin_data);

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);

  switch (input->type) {
   e2212:	5c28      	ldrb	r0, [r5, r0]
   e2214:	1e46      	subs	r6, r0, #1
   e2216:	2e08      	cmp	r6, #8
   e2218:	f200 81c3 	bhi.w	e25a2 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3a6>
   e221c:	e8df f016 	tbh	[pc, r6, lsl #1]
   e2220:	007a0009 	.word	0x007a0009
   e2224:	01c100e7 	.word	0x01c100e7
   e2228:	01c101c1 	.word	0x01c101c1
   e222c:	01c101c1 	.word	0x01c101c1
   e2230:	0154      	.short	0x0154
}

template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
   e2232:	6848      	ldr	r0, [r1, #4]
  const TfLiteIntArray* input_dims = input->dims;
   e2234:	f8d3 c008 	ldr.w	ip, [r3, #8]
  const TfLiteIntArray* output_dims = output0->dims;
   e2238:	6840      	ldr	r0, [r0, #4]

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);

  switch (input->type) {
    case kTfLiteFloat32: {
      return UnpackImpl<float>(context, node, input, data->num, data->axis);
   e223a:	f8d2 8000 	ldr.w	r8, [r2]
   e223e:	6852      	ldr	r2, [r2, #4]
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
   e2240:	2638      	movs	r6, #56	; 0x38
   e2242:	fb06 5500 	mla	r5, r6, r0, r5
  const int dimensions = input_dims->size;

  if (axis < 0) {
   e2246:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
   e2248:	68af      	ldr	r7, [r5, #8]
  const int dimensions = input_dims->size;
   e224a:	f8dc 5000 	ldr.w	r5, [ip]

  if (axis < 0) {
    axis += NumDimensions(input);
   e224e:	bfb8      	it	lt
   e2250:	1952      	addlt	r2, r2, r5
  }

  TFLITE_DCHECK_LT(axis, dimensions);
   e2252:	4295      	cmp	r5, r2
   e2254:	dc01      	bgt.n	e225a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5e>
   e2256:	f002 f879 	bl	e434c <abort>
   e225a:	46e6      	mov	lr, ip
   e225c:	2000      	movs	r0, #0
   e225e:	2601      	movs	r6, #1

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e2260:	4282      	cmp	r2, r0
   e2262:	dd05      	ble.n	e2270 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x74>
    outer_size *= input_dims->data[i];
   e2264:	f85e 9f04 	ldr.w	r9, [lr, #4]!
  }

  TFLITE_DCHECK_LT(axis, dimensions);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e2268:	3001      	adds	r0, #1
    outer_size *= input_dims->data[i];
   e226a:	fb09 f606 	mul.w	r6, r9, r6
   e226e:	e7f7      	b.n	e2260 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x64>
   e2270:	1c50      	adds	r0, r2, #1
   e2272:	f105 0e01 	add.w	lr, r5, #1
  }
  int copy_size = 1;
   e2276:	2201      	movs	r2, #1
   e2278:	3001      	adds	r0, #1
  for (int i = axis + 1; i < dimensions; ++i) {
   e227a:	4570      	cmp	r0, lr
   e227c:	d003      	beq.n	e2286 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x8a>
    copy_size *= input_dims->data[i];
   e227e:	f85c 5020 	ldr.w	r5, [ip, r0, lsl #2]
   e2282:	436a      	muls	r2, r5
   e2284:	e7f8      	b.n	e2278 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x7c>
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
   e2286:	f8d7 c000 	ldr.w	ip, [r7]
   e228a:	4638      	mov	r0, r7
   e228c:	2501      	movs	r5, #1
   e228e:	2700      	movs	r7, #0
   e2290:	45bc      	cmp	ip, r7
   e2292:	dd05      	ble.n	e22a0 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xa4>
    output_size *= output_dims->data[i];
   e2294:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= input_dims->data[i];
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
   e2298:	3701      	adds	r7, #1
    output_size *= output_dims->data[i];
   e229a:	fb0e f505 	mul.w	r5, lr, r5
   e229e:	e7f7      	b.n	e2290 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x94>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);
   e22a0:	fb02 f006 	mul.w	r0, r2, r6
   e22a4:	4285      	cmp	r5, r0
   e22a6:	d1d6      	bne.n	e2256 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e22a8:	685b      	ldr	r3, [r3, #4]
   e22aa:	9302      	str	r3, [sp, #8]
   e22ac:	fb02 f308 	mul.w	r3, r2, r8
   e22b0:	009b      	lsls	r3, r3, #2
   e22b2:	2000      	movs	r0, #0
   e22b4:	ea4f 0982 	mov.w	r9, r2, lsl #2
   e22b8:	9303      	str	r3, [sp, #12]

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
   e22ba:	4605      	mov	r5, r0
   e22bc:	45a8      	cmp	r8, r5
   e22be:	dc01      	bgt.n	e22c4 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc8>

  const TfLiteTensor* input = GetInput(context, node, kInputTensor);

  switch (input->type) {
    case kTfLiteFloat32: {
      return UnpackImpl<float>(context, node, input, data->num, data->axis);
   e22c0:	2000      	movs	r0, #0
   e22c2:	e177      	b.n	e25b4 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3b8>
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e22c4:	684b      	ldr	r3, [r1, #4]
   e22c6:	eb03 0385 	add.w	r3, r3, r5, lsl #2
   e22ca:	2438      	movs	r4, #56	; 0x38
   e22cc:	685f      	ldr	r7, [r3, #4]
   e22ce:	9b01      	ldr	r3, [sp, #4]
   e22d0:	689b      	ldr	r3, [r3, #8]
   e22d2:	fb04 3307 	mla	r3, r4, r7, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e22d6:	b103      	cbz	r3, e22da <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xde>
   e22d8:	685b      	ldr	r3, [r3, #4]
   e22da:	2700      	movs	r7, #0
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   e22dc:	46bc      	mov	ip, r7
   e22de:	4566      	cmp	r6, ip
   e22e0:	dd15      	ble.n	e230e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x112>
   e22e2:	9c02      	ldr	r4, [sp, #8]
   e22e4:	eb00 0e07 	add.w	lr, r0, r7
   e22e8:	44a6      	add	lr, r4
   e22ea:	469b      	mov	fp, r3
      T* output_ptr = output_data + copy_size * k;
      int loc = k * output_count * copy_size + i * copy_size;
      const T* input_ptr = input_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e22ec:	f04f 0a00 	mov.w	sl, #0
   e22f0:	4552      	cmp	r2, sl
   e22f2:	dd06      	ble.n	e2302 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x106>
   e22f4:	ecfe 7a01 	vldmia	lr!, {s15}
   e22f8:	f10a 0a01 	add.w	sl, sl, #1
   e22fc:	eceb 7a01 	vstmia	fp!, {s15}
   e2300:	e7f6      	b.n	e22f0 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xf4>
   e2302:	9c03      	ldr	r4, [sp, #12]
  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   e2304:	f10c 0c01 	add.w	ip, ip, #1
   e2308:	4427      	add	r7, r4
   e230a:	444b      	add	r3, r9
   e230c:	e7e7      	b.n	e22de <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xe2>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
   e230e:	3501      	adds	r5, #1
   e2310:	4448      	add	r0, r9
   e2312:	e7d3      	b.n	e22bc <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc0>
}

template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
   e2314:	6848      	ldr	r0, [r1, #4]
  const TfLiteIntArray* input_dims = input->dims;
   e2316:	f8d3 c008 	ldr.w	ip, [r3, #8]
  const TfLiteIntArray* output_dims = output0->dims;
   e231a:	6840      	ldr	r0, [r0, #4]
  switch (input->type) {
    case kTfLiteFloat32: {
      return UnpackImpl<float>(context, node, input, data->num, data->axis);
    }
    case kTfLiteInt32: {
      return UnpackImpl<int32_t>(context, node, input, data->num, data->axis);
   e231c:	f8d2 8000 	ldr.w	r8, [r2]
   e2320:	6852      	ldr	r2, [r2, #4]
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
   e2322:	2638      	movs	r6, #56	; 0x38
   e2324:	fb06 5500 	mla	r5, r6, r0, r5
  const int dimensions = input_dims->size;

  if (axis < 0) {
   e2328:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
   e232a:	68ae      	ldr	r6, [r5, #8]
  const int dimensions = input_dims->size;
   e232c:	f8dc 5000 	ldr.w	r5, [ip]

  if (axis < 0) {
    axis += NumDimensions(input);
   e2330:	bfb8      	it	lt
   e2332:	1952      	addlt	r2, r2, r5
  }

  TFLITE_DCHECK_LT(axis, dimensions);
   e2334:	4295      	cmp	r5, r2
   e2336:	dd8e      	ble.n	e2256 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
   e2338:	46e6      	mov	lr, ip
   e233a:	2000      	movs	r0, #0
   e233c:	2701      	movs	r7, #1

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e233e:	4282      	cmp	r2, r0
   e2340:	dd05      	ble.n	e234e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x152>
    outer_size *= input_dims->data[i];
   e2342:	f85e 9f04 	ldr.w	r9, [lr, #4]!
  }

  TFLITE_DCHECK_LT(axis, dimensions);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e2346:	3001      	adds	r0, #1
    outer_size *= input_dims->data[i];
   e2348:	fb09 f707 	mul.w	r7, r9, r7
   e234c:	e7f7      	b.n	e233e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x142>
   e234e:	1c50      	adds	r0, r2, #1
   e2350:	f105 0e01 	add.w	lr, r5, #1
  }
  int copy_size = 1;
   e2354:	2201      	movs	r2, #1
   e2356:	3001      	adds	r0, #1
  for (int i = axis + 1; i < dimensions; ++i) {
   e2358:	4570      	cmp	r0, lr
   e235a:	d003      	beq.n	e2364 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x168>
    copy_size *= input_dims->data[i];
   e235c:	f85c 5020 	ldr.w	r5, [ip, r0, lsl #2]
   e2360:	436a      	muls	r2, r5
   e2362:	e7f8      	b.n	e2356 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x15a>
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
   e2364:	f8d6 c000 	ldr.w	ip, [r6]
   e2368:	4630      	mov	r0, r6
   e236a:	2501      	movs	r5, #1
   e236c:	2600      	movs	r6, #0
   e236e:	45b4      	cmp	ip, r6
   e2370:	dd05      	ble.n	e237e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x182>
    output_size *= output_dims->data[i];
   e2372:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= input_dims->data[i];
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
   e2376:	3601      	adds	r6, #1
    output_size *= output_dims->data[i];
   e2378:	fb0e f505 	mul.w	r5, lr, r5
   e237c:	e7f7      	b.n	e236e <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x172>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);
   e237e:	fb02 f007 	mul.w	r0, r2, r7
   e2382:	4285      	cmp	r5, r0
   e2384:	f47f af67 	bne.w	e2256 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e2388:	685b      	ldr	r3, [r3, #4]
   e238a:	9302      	str	r3, [sp, #8]
   e238c:	fb02 f308 	mul.w	r3, r2, r8
   e2390:	009b      	lsls	r3, r3, #2
   e2392:	2500      	movs	r5, #0
   e2394:	ea4f 0c82 	mov.w	ip, r2, lsl #2
   e2398:	9303      	str	r3, [sp, #12]

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
   e239a:	462e      	mov	r6, r5
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e239c:	f04f 0938 	mov.w	r9, #56	; 0x38
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
   e23a0:	45b0      	cmp	r8, r6
   e23a2:	dd8d      	ble.n	e22c0 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc4>
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e23a4:	684b      	ldr	r3, [r1, #4]
   e23a6:	eb03 0386 	add.w	r3, r3, r6, lsl #2
   e23aa:	6858      	ldr	r0, [r3, #4]
   e23ac:	9b01      	ldr	r3, [sp, #4]
   e23ae:	689b      	ldr	r3, [r3, #8]
   e23b0:	fb09 3300 	mla	r3, r9, r0, r3

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e23b4:	b103      	cbz	r3, e23b8 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1bc>
   e23b6:	685b      	ldr	r3, [r3, #4]
   e23b8:	f04f 0e00 	mov.w	lr, #0
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   e23bc:	46f2      	mov	sl, lr
   e23be:	4557      	cmp	r7, sl
   e23c0:	dd12      	ble.n	e23e8 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1ec>
      T* output_ptr = output_data + copy_size * k;
      int loc = k * output_count * copy_size + i * copy_size;
      const T* input_ptr = input_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e23c2:	9c02      	ldr	r4, [sp, #8]
   e23c4:	eb05 0b0e 	add.w	fp, r5, lr
  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   e23c8:	2000      	movs	r0, #0
      T* output_ptr = output_data + copy_size * k;
      int loc = k * output_count * copy_size + i * copy_size;
      const T* input_ptr = input_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e23ca:	44a3      	add	fp, r4
   e23cc:	4282      	cmp	r2, r0
   e23ce:	dd05      	ble.n	e23dc <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1e0>
   e23d0:	f85b 4020 	ldr.w	r4, [fp, r0, lsl #2]
   e23d4:	f843 4020 	str.w	r4, [r3, r0, lsl #2]
   e23d8:	3001      	adds	r0, #1
   e23da:	e7f7      	b.n	e23cc <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1d0>
   e23dc:	9803      	ldr	r0, [sp, #12]
  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   e23de:	f10a 0a01 	add.w	sl, sl, #1
   e23e2:	4486      	add	lr, r0
   e23e4:	4463      	add	r3, ip
   e23e6:	e7ea      	b.n	e23be <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1c2>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
   e23e8:	3601      	adds	r6, #1
   e23ea:	4465      	add	r5, ip
   e23ec:	e7d8      	b.n	e23a0 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x1a4>
    }
    case kTfLiteInt32: {
      return UnpackImpl<int32_t>(context, node, input, data->num, data->axis);
    }
    case kTfLiteUInt8: {
      return UnpackImpl<uint8_t>(context, node, input, data->num, data->axis);
   e23ee:	6810      	ldr	r0, [r2, #0]
   e23f0:	9002      	str	r0, [sp, #8]
}

template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
   e23f2:	6848      	ldr	r0, [r1, #4]
  const TfLiteIntArray* input_dims = input->dims;
   e23f4:	f8d3 c008 	ldr.w	ip, [r3, #8]
  const TfLiteIntArray* output_dims = output0->dims;
   e23f8:	6840      	ldr	r0, [r0, #4]
    }
    case kTfLiteInt32: {
      return UnpackImpl<int32_t>(context, node, input, data->num, data->axis);
    }
    case kTfLiteUInt8: {
      return UnpackImpl<uint8_t>(context, node, input, data->num, data->axis);
   e23fa:	6852      	ldr	r2, [r2, #4]
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
   e23fc:	2638      	movs	r6, #56	; 0x38
   e23fe:	fb06 5500 	mla	r5, r6, r0, r5
  const int dimensions = input_dims->size;

  if (axis < 0) {
   e2402:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
   e2404:	68af      	ldr	r7, [r5, #8]
  const int dimensions = input_dims->size;
   e2406:	f8dc 5000 	ldr.w	r5, [ip]

  if (axis < 0) {
    axis += NumDimensions(input);
   e240a:	bfb8      	it	lt
   e240c:	1952      	addlt	r2, r2, r5
  }

  TFLITE_DCHECK_LT(axis, dimensions);
   e240e:	4295      	cmp	r5, r2
   e2410:	f77f af21 	ble.w	e2256 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
   e2414:	46e6      	mov	lr, ip
   e2416:	2000      	movs	r0, #0
   e2418:	2601      	movs	r6, #1

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e241a:	4282      	cmp	r2, r0
   e241c:	dd05      	ble.n	e242a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x22e>
    outer_size *= input_dims->data[i];
   e241e:	f85e 8f04 	ldr.w	r8, [lr, #4]!
  }

  TFLITE_DCHECK_LT(axis, dimensions);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e2422:	3001      	adds	r0, #1
    outer_size *= input_dims->data[i];
   e2424:	fb08 f606 	mul.w	r6, r8, r6
   e2428:	e7f7      	b.n	e241a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x21e>
   e242a:	1c50      	adds	r0, r2, #1
   e242c:	f105 0e01 	add.w	lr, r5, #1
  }
  int copy_size = 1;
   e2430:	2201      	movs	r2, #1
   e2432:	3001      	adds	r0, #1
  for (int i = axis + 1; i < dimensions; ++i) {
   e2434:	4570      	cmp	r0, lr
   e2436:	d003      	beq.n	e2440 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x244>
    copy_size *= input_dims->data[i];
   e2438:	f85c 5020 	ldr.w	r5, [ip, r0, lsl #2]
   e243c:	436a      	muls	r2, r5
   e243e:	e7f8      	b.n	e2432 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x236>
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
   e2440:	f8d7 c000 	ldr.w	ip, [r7]
   e2444:	4638      	mov	r0, r7
   e2446:	2501      	movs	r5, #1
   e2448:	2700      	movs	r7, #0
   e244a:	45bc      	cmp	ip, r7
   e244c:	dd05      	ble.n	e245a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x25e>
    output_size *= output_dims->data[i];
   e244e:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= input_dims->data[i];
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
   e2452:	3701      	adds	r7, #1
    output_size *= output_dims->data[i];
   e2454:	fb0e f505 	mul.w	r5, lr, r5
   e2458:	e7f7      	b.n	e244a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x24e>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);
   e245a:	fb02 f006 	mul.w	r0, r2, r6
   e245e:	4285      	cmp	r5, r0
   e2460:	f47f aef9 	bne.w	e2256 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
   e2464:	6858      	ldr	r0, [r3, #4]
   e2466:	9b02      	ldr	r3, [sp, #8]
   e2468:	4247      	negs	r7, r0
   e246a:	fb02 f903 	mul.w	r9, r2, r3

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
   e246e:	2500      	movs	r5, #0
   e2470:	9b02      	ldr	r3, [sp, #8]
   e2472:	42ab      	cmp	r3, r5
   e2474:	f77f af24 	ble.w	e22c0 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc4>
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e2478:	684b      	ldr	r3, [r1, #4]
   e247a:	9c01      	ldr	r4, [sp, #4]
   e247c:	eb03 0385 	add.w	r3, r3, r5, lsl #2
   e2480:	68a4      	ldr	r4, [r4, #8]
   e2482:	685b      	ldr	r3, [r3, #4]
   e2484:	f04f 0e38 	mov.w	lr, #56	; 0x38
   e2488:	fb0e 4303 	mla	r3, lr, r3, r4
   e248c:	b103      	cbz	r3, e2490 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x294>
   e248e:	685b      	ldr	r3, [r3, #4]
   e2490:	46be      	mov	lr, r7
   e2492:	4684      	mov	ip, r0
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   e2494:	f04f 0800 	mov.w	r8, #0
   e2498:	4546      	cmp	r6, r8
   e249a:	dd11      	ble.n	e24c0 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2c4>
   e249c:	461c      	mov	r4, r3
   e249e:	46e2      	mov	sl, ip
      T* output_ptr = output_data + copy_size * k;
      int loc = k * output_count * copy_size + i * copy_size;
      const T* input_ptr = input_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e24a0:	eb0a 0b0e 	add.w	fp, sl, lr
   e24a4:	455a      	cmp	r2, fp
   e24a6:	dd04      	ble.n	e24b2 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2b6>
   e24a8:	f81a bb01 	ldrb.w	fp, [sl], #1
   e24ac:	f804 bb01 	strb.w	fp, [r4], #1
   e24b0:	e7f6      	b.n	e24a0 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2a4>
  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   e24b2:	f108 0801 	add.w	r8, r8, #1
   e24b6:	44cc      	add	ip, r9
   e24b8:	4413      	add	r3, r2
   e24ba:	ebc9 0e0e 	rsb	lr, r9, lr
   e24be:	e7eb      	b.n	e2498 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x29c>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
   e24c0:	3501      	adds	r5, #1
   e24c2:	4410      	add	r0, r2
   e24c4:	1abf      	subs	r7, r7, r2
   e24c6:	e7d3      	b.n	e2470 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x274>
    }
    case kTfLiteUInt8: {
      return UnpackImpl<uint8_t>(context, node, input, data->num, data->axis);
    }
    case kTfLiteInt8: {
      return UnpackImpl<int8_t>(context, node, input, data->num, data->axis);
   e24c8:	6810      	ldr	r0, [r2, #0]
   e24ca:	9002      	str	r0, [sp, #8]
}

template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
   e24cc:	6848      	ldr	r0, [r1, #4]
  const TfLiteIntArray* input_dims = input->dims;
   e24ce:	f8d3 c008 	ldr.w	ip, [r3, #8]
  const TfLiteIntArray* output_dims = output0->dims;
   e24d2:	6840      	ldr	r0, [r0, #4]
    }
    case kTfLiteUInt8: {
      return UnpackImpl<uint8_t>(context, node, input, data->num, data->axis);
    }
    case kTfLiteInt8: {
      return UnpackImpl<int8_t>(context, node, input, data->num, data->axis);
   e24d4:	6852      	ldr	r2, [r2, #4]
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
   e24d6:	2638      	movs	r6, #56	; 0x38
   e24d8:	fb06 5500 	mla	r5, r6, r0, r5
  const int dimensions = input_dims->size;

  if (axis < 0) {
   e24dc:	2a00      	cmp	r2, #0
template <typename T>
TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteTensor* input, int output_count, int axis) {
  const TfLiteTensor* output0 = &context->tensors[node->outputs->data[0]];
  const TfLiteIntArray* input_dims = input->dims;
  const TfLiteIntArray* output_dims = output0->dims;
   e24de:	68af      	ldr	r7, [r5, #8]
  const int dimensions = input_dims->size;
   e24e0:	f8dc 5000 	ldr.w	r5, [ip]

  if (axis < 0) {
    axis += NumDimensions(input);
   e24e4:	bfb8      	it	lt
   e24e6:	1952      	addlt	r2, r2, r5
  }

  TFLITE_DCHECK_LT(axis, dimensions);
   e24e8:	4295      	cmp	r5, r2
   e24ea:	f77f aeb4 	ble.w	e2256 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
   e24ee:	46e6      	mov	lr, ip
   e24f0:	2000      	movs	r0, #0
   e24f2:	2601      	movs	r6, #1

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e24f4:	4282      	cmp	r2, r0
   e24f6:	dd05      	ble.n	e2504 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x308>
    outer_size *= input_dims->data[i];
   e24f8:	f85e 8f04 	ldr.w	r8, [lr, #4]!
  }

  TFLITE_DCHECK_LT(axis, dimensions);

  int outer_size = 1;
  for (int i = 0; i < axis; ++i) {
   e24fc:	3001      	adds	r0, #1
    outer_size *= input_dims->data[i];
   e24fe:	fb08 f606 	mul.w	r6, r8, r6
   e2502:	e7f7      	b.n	e24f4 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x2f8>
   e2504:	1c50      	adds	r0, r2, #1
   e2506:	f105 0e01 	add.w	lr, r5, #1
  }
  int copy_size = 1;
   e250a:	2201      	movs	r2, #1
   e250c:	3001      	adds	r0, #1
  for (int i = axis + 1; i < dimensions; ++i) {
   e250e:	4570      	cmp	r0, lr
   e2510:	d003      	beq.n	e251a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x31e>
    copy_size *= input_dims->data[i];
   e2512:	f85c 5020 	ldr.w	r5, [ip, r0, lsl #2]
   e2516:	436a      	muls	r2, r5
   e2518:	e7f8      	b.n	e250c <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x310>
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
   e251a:	f8d7 c000 	ldr.w	ip, [r7]
   e251e:	4638      	mov	r0, r7
   e2520:	2501      	movs	r5, #1
   e2522:	2700      	movs	r7, #0
   e2524:	45bc      	cmp	ip, r7
   e2526:	dd05      	ble.n	e2534 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x338>
    output_size *= output_dims->data[i];
   e2528:	f850 ef04 	ldr.w	lr, [r0, #4]!
  int copy_size = 1;
  for (int i = axis + 1; i < dimensions; ++i) {
    copy_size *= input_dims->data[i];
  }
  int output_size = 1;
  for (int i = 0; i < output_dims->size; ++i) {
   e252c:	3701      	adds	r7, #1
    output_size *= output_dims->data[i];
   e252e:	fb0e f505 	mul.w	r5, lr, r5
   e2532:	e7f7      	b.n	e2524 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x328>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);
   e2534:	fb02 f006 	mul.w	r0, r2, r6
   e2538:	4285      	cmp	r5, r0
   e253a:	f47f ae8c 	bne.w	e2256 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x5a>
   e253e:	6858      	ldr	r0, [r3, #4]
   e2540:	9b02      	ldr	r3, [sp, #8]
   e2542:	4247      	negs	r7, r0
   e2544:	fb02 f903 	mul.w	r9, r2, r3

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
   e2548:	2500      	movs	r5, #0
   e254a:	9b02      	ldr	r3, [sp, #8]
   e254c:	42ab      	cmp	r3, r5
   e254e:	f77f aeb7 	ble.w	e22c0 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0xc4>
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
   e2552:	684b      	ldr	r3, [r1, #4]
   e2554:	9c01      	ldr	r4, [sp, #4]
   e2556:	eb03 0385 	add.w	r3, r3, r5, lsl #2
   e255a:	68a4      	ldr	r4, [r4, #8]
   e255c:	685b      	ldr	r3, [r3, #4]
   e255e:	f04f 0e38 	mov.w	lr, #56	; 0x38
   e2562:	fb0e 4303 	mla	r3, lr, r3, r4
   e2566:	b103      	cbz	r3, e256a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x36e>
   e2568:	685b      	ldr	r3, [r3, #4]
   e256a:	46be      	mov	lr, r7
   e256c:	4684      	mov	ip, r0
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   e256e:	f04f 0800 	mov.w	r8, #0
   e2572:	4546      	cmp	r6, r8
   e2574:	dd11      	ble.n	e259a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x39e>
   e2576:	461c      	mov	r4, r3
   e2578:	46e2      	mov	sl, ip
      T* output_ptr = output_data + copy_size * k;
      int loc = k * output_count * copy_size + i * copy_size;
      const T* input_ptr = input_data + loc;
      for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
   e257a:	eb0a 0b0e 	add.w	fp, sl, lr
   e257e:	455a      	cmp	r2, fp
   e2580:	dd04      	ble.n	e258c <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x390>
   e2582:	f91a bb01 	ldrsb.w	fp, [sl], #1
   e2586:	f804 bb01 	strb.w	fp, [r4], #1
   e258a:	e7f6      	b.n	e257a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x37e>
  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
    TfLiteTensor* t = &context->tensors[node->outputs->data[i]];
    T* output_data = GetTensorData<T>(t);
    for (int k = 0; k < outer_size; ++k) {
   e258c:	f108 0801 	add.w	r8, r8, #1
   e2590:	44cc      	add	ip, r9
   e2592:	4413      	add	r3, r2
   e2594:	ebc9 0e0e 	rsb	lr, r9, lr
   e2598:	e7eb      	b.n	e2572 <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x376>
  }
  TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);

  const T* input_data = GetTensorData<T>(input);

  for (int i = 0; i < output_count; ++i) {
   e259a:	3501      	adds	r5, #1
   e259c:	4410      	add	r0, r2
   e259e:	1abf      	subs	r7, r7, r2
   e25a0:	e7d3      	b.n	e254a <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x34e>
    }
    case kTfLiteInt8: {
      return UnpackImpl<int8_t>(context, node, input, data->num, data->axis);
    }
    default: {
      context->ReportError(context, "Type '%s' is not supported by unpack.",
   e25a2:	9b01      	ldr	r3, [sp, #4]
   e25a4:	695d      	ldr	r5, [r3, #20]
   e25a6:	f7f1 fdb9 	bl	d411c <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type));
   e25aa:	4904      	ldr	r1, [pc, #16]	; (e25bc <_ZN6tflite3ops5micro6unpack12_GLOBAL__N_14EvalEP13TfLiteContextP10TfLiteNode+0x3c0>)
   e25ac:	4602      	mov	r2, r0
   e25ae:	9801      	ldr	r0, [sp, #4]
   e25b0:	47a8      	blx	r5
      return kTfLiteError;
   e25b2:	2001      	movs	r0, #1
    }
  }

  return kTfLiteOk;
}
   e25b4:	b005      	add	sp, #20
   e25b6:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e25ba:	bf00      	nop
   e25bc:	000eae81 	.word	0x000eae81

000e25c0 <_ZN6tflite3ops5micro15Register_UNPACKEv>:

TfLiteRegistration* Register_UNPACK() {
  static TfLiteRegistration r = {nullptr, nullptr, unpack::Prepare,
                                 unpack::Eval};
  return &r;
}
   e25c0:	4800      	ldr	r0, [pc, #0]	; (e25c4 <_ZN6tflite3ops5micro15Register_UNPACKEv+0x4>)
   e25c2:	4770      	bx	lr
   e25c4:	2003c35c 	.word	0x2003c35c

000e25c8 <_ZN6tflite3ops5micro14depthwise_conv4InitEP13TfLiteContextPKcj>:

}  // namespace

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  return nullptr;
}
   e25c8:	2000      	movs	r0, #0
   e25ca:	4770      	bx	lr

000e25cc <_ZN6tflite3ops5micro14depthwise_conv4FreeEP13TfLiteContextPv>:

void Free(TfLiteContext* context, void* buffer) {}
   e25cc:	4770      	bx	lr

000e25ce <_ZN6tflite3ops5micro14depthwise_conv7PrepareEP13TfLiteContextP10TfLiteNode>:

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return kTfLiteOk;
}
   e25ce:	2000      	movs	r0, #0
   e25d0:	4770      	bx	lr

000e25d2 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa>:
    const DepthwiseParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
   e25d2:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e25d6:	b0a5      	sub	sp, #148	; 0x94
   e25d8:	469b      	mov	fp, r3
  // Get parameters.
  // TODO(b/141565753): Re-introduce ScopedProfilingLabel on Micro.
  const int stride_width = params.stride_width;
   e25da:	f9b0 300a 	ldrsh.w	r3, [r0, #10]
   e25de:	930f      	str	r3, [sp, #60]	; 0x3c
  const int stride_height = params.stride_height;
   e25e0:	f9b0 300c 	ldrsh.w	r3, [r0, #12]
   e25e4:	9310      	str	r3, [sp, #64]	; 0x40
  const int dilation_width_factor = params.dilation_width_factor;
   e25e6:	f9b0 300e 	ldrsh.w	r3, [r0, #14]
   e25ea:	9311      	str	r3, [sp, #68]	; 0x44
  const int dilation_height_factor = params.dilation_height_factor;
   e25ec:	f9b0 3010 	ldrsh.w	r3, [r0, #16]
   e25f0:	9312      	str	r3, [sp, #72]	; 0x48
  const int pad_width = params.padding_values.width;
   e25f2:	f9b0 3002 	ldrsh.w	r3, [r0, #2]
   e25f6:	9313      	str	r3, [sp, #76]	; 0x4c
  const int pad_height = params.padding_values.height;
   e25f8:	f9b0 3004 	ldrsh.w	r3, [r0, #4]
   e25fc:	9314      	str	r3, [sp, #80]	; 0x50
  const int depth_multiplier = params.depth_multiplier;
   e25fe:	f9b0 3012 	ldrsh.w	r3, [r0, #18]
   e2602:	9308      	str	r3, [sp, #32]
  const int32 input_offset = params.input_offset;
   e2604:	6943      	ldr	r3, [r0, #20]
   e2606:	9315      	str	r3, [sp, #84]	; 0x54
  const int32 output_offset = params.output_offset;
   e2608:	69c3      	ldr	r3, [r0, #28]
   e260a:	9316      	str	r3, [sp, #88]	; 0x58
  const int32 output_activation_min = params.quantized_activation_min;
   e260c:	6a83      	ldr	r3, [r0, #40]	; 0x28
   e260e:	930b      	str	r3, [sp, #44]	; 0x2c
  const int32 output_activation_max = params.quantized_activation_max;
   e2610:	6ac3      	ldr	r3, [r0, #44]	; 0x2c
   e2612:	930c      	str	r3, [sp, #48]	; 0x30

  // Check dimensions of the tensors.
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e2614:	f8db 3000 	ldr.w	r3, [fp]
    const DepthwiseParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
   e2618:	9122      	str	r1, [sp, #136]	; 0x88
  const int32 output_offset = params.output_offset;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;

  // Check dimensions of the tensors.
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e261a:	2b04      	cmp	r3, #4
    const DepthwiseParams& params, const int32* output_multiplier,
    const int32* output_shift, const RuntimeShape& input_shape,
    const int8* input_data, const RuntimeShape& filter_shape,
    const int8* filter_data, const RuntimeShape& bias_shape,
    const int32* bias_data, const RuntimeShape& output_shape,
    int8* output_data) {
   e261c:	9223      	str	r2, [sp, #140]	; 0x8c
   e261e:	f8dd a0cc 	ldr.w	sl, [sp, #204]	; 0xcc
  const int32 output_offset = params.output_offset;
  const int32 output_activation_min = params.quantized_activation_min;
  const int32 output_activation_max = params.quantized_activation_max;

  // Check dimensions of the tensors.
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e2622:	d001      	beq.n	e2628 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x56>
   e2624:	f001 fe92 	bl	e434c <abort>
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
   e2628:	9b2f      	ldr	r3, [sp, #188]	; 0xbc
   e262a:	681b      	ldr	r3, [r3, #0]
   e262c:	2b04      	cmp	r3, #4
   e262e:	d1f9      	bne.n	e2624 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x52>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   e2630:	f8da 3000 	ldr.w	r3, [sl]
   e2634:	2b04      	cmp	r3, #4
   e2636:	d1f5      	bne.n	e2624 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x52>

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   e2638:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e263a:	9a0c      	ldr	r2, [sp, #48]	; 0x30
   e263c:	4293      	cmp	r3, r2
   e263e:	dcf1      	bgt.n	e2624 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x52>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   e2640:	2300      	movs	r3, #0
   e2642:	4619      	mov	r1, r3
   e2644:	4652      	mov	r2, sl
   e2646:	4658      	mov	r0, fp
   e2648:	f7f9 fabf 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e264c:	2303      	movs	r3, #3
   e264e:	4619      	mov	r1, r3
   e2650:	4652      	mov	r2, sl
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   e2652:	9017      	str	r0, [sp, #92]	; 0x5c
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e2654:	982f      	ldr	r0, [sp, #188]	; 0xbc
   e2656:	f7f9 fab8 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
   e265a:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e265c:	4604      	mov	r4, r0
  const int input_height = input_shape.Dims(1);
   e265e:	4658      	mov	r0, fp
   e2660:	f7f3 ff02 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   e2664:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
   e2666:	9018      	str	r0, [sp, #96]	; 0x60
  const int input_width = input_shape.Dims(2);
   e2668:	4658      	mov	r0, fp
   e266a:	f7f3 fefd 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_depth = input_shape.Dims(3);
   e266e:	2103      	movs	r1, #3

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   e2670:	9019      	str	r0, [sp, #100]	; 0x64
  const int input_depth = input_shape.Dims(3);
   e2672:	4658      	mov	r0, fp
   e2674:	f7f3 fef8 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_height = filter_shape.Dims(1);
   e2678:	2101      	movs	r1, #1
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
   e267a:	900d      	str	r0, [sp, #52]	; 0x34
  const int filter_height = filter_shape.Dims(1);
   e267c:	982f      	ldr	r0, [sp, #188]	; 0xbc
   e267e:	f7f3 fef3 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_width = filter_shape.Dims(2);
   e2682:	2102      	movs	r1, #2
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
   e2684:	901a      	str	r0, [sp, #104]	; 0x68
  const int filter_width = filter_shape.Dims(2);
   e2686:	982f      	ldr	r0, [sp, #188]	; 0xbc
   e2688:	f7f3 feee 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   e268c:	2101      	movs	r1, #1
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
   e268e:	901b      	str	r0, [sp, #108]	; 0x6c
  const int output_height = output_shape.Dims(1);
   e2690:	4650      	mov	r0, sl
   e2692:	f7f3 fee9 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   e2696:	2102      	movs	r1, #2
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   e2698:	901c      	str	r0, [sp, #112]	; 0x70
  const int output_width = output_shape.Dims(2);
   e269a:	4650      	mov	r0, sl
   e269c:	f7f3 fee4 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
   e26a0:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   e26a2:	9a08      	ldr	r2, [sp, #32]
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   e26a4:	901d      	str	r0, [sp, #116]	; 0x74
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
   e26a6:	4353      	muls	r3, r2
   e26a8:	429c      	cmp	r4, r3
   e26aa:	d1bb      	bne.n	e2624 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x52>
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
   e26ac:	9831      	ldr	r0, [sp, #196]	; 0xc4
   e26ae:	f7f9 fa7c 	bl	dbbaa <_ZNK6tflite12RuntimeShape8FlatSizeEv>
   e26b2:	4284      	cmp	r4, r0
   e26b4:	d1b6      	bne.n	e2624 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x52>
   e26b6:	f04f 0900 	mov.w	r9, #0

  for (int batch = 0; batch < batches; ++batch) {
   e26ba:	9b17      	ldr	r3, [sp, #92]	; 0x5c
   e26bc:	4599      	cmp	r9, r3
   e26be:	f280 80ab 	bge.w	e2818 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x246>
   e26c2:	9b14      	ldr	r3, [sp, #80]	; 0x50
   e26c4:	425b      	negs	r3, r3
   e26c6:	9309      	str	r3, [sp, #36]	; 0x24
   e26c8:	2300      	movs	r3, #0
   e26ca:	9303      	str	r3, [sp, #12]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   e26cc:	9b03      	ldr	r3, [sp, #12]
   e26ce:	9a1c      	ldr	r2, [sp, #112]	; 0x70
   e26d0:	4293      	cmp	r3, r2
   e26d2:	f280 8083 	bge.w	e27dc <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x20a>
   e26d6:	9b13      	ldr	r3, [sp, #76]	; 0x4c
   e26d8:	425b      	negs	r3, r3
   e26da:	930a      	str	r3, [sp, #40]	; 0x28
   e26dc:	2300      	movs	r3, #0
   e26de:	9304      	str	r3, [sp, #16]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   e26e0:	9b04      	ldr	r3, [sp, #16]
   e26e2:	9a1d      	ldr	r2, [sp, #116]	; 0x74
   e26e4:	4293      	cmp	r3, r2
   e26e6:	da71      	bge.n	e27cc <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1fa>
   e26e8:	f04f 0800 	mov.w	r8, #0
   e26ec:	f8cd 8014 	str.w	r8, [sp, #20]
        for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
   e26f0:	9b05      	ldr	r3, [sp, #20]
   e26f2:	9a0d      	ldr	r2, [sp, #52]	; 0x34
   e26f4:	4293      	cmp	r3, r2
   e26f6:	da61      	bge.n	e27bc <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1ea>
   e26f8:	9a32      	ldr	r2, [sp, #200]	; 0xc8
   e26fa:	ea4f 0388 	mov.w	r3, r8, lsl #2
   e26fe:	441a      	add	r2, r3
   e2700:	9221      	str	r2, [sp, #132]	; 0x84
   e2702:	9a22      	ldr	r2, [sp, #136]	; 0x88
   e2704:	441a      	add	r2, r3
   e2706:	9220      	str	r2, [sp, #128]	; 0x80
   e2708:	9a23      	ldr	r2, [sp, #140]	; 0x8c
   e270a:	18d3      	adds	r3, r2, r3
   e270c:	931f      	str	r3, [sp, #124]	; 0x7c
   e270e:	2400      	movs	r4, #0
          for (int m = 0; m < depth_multiplier; ++m) {
   e2710:	9b08      	ldr	r3, [sp, #32]
   e2712:	429c      	cmp	r4, r3
   e2714:	da4c      	bge.n	e27b0 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x1de>
   e2716:	eb08 0304 	add.w	r3, r8, r4
            const int output_channel = m + in_channel * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            int32 acc = 0;
   e271a:	2500      	movs	r5, #0

  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
          for (int m = 0; m < depth_multiplier; ++m) {
   e271c:	9e09      	ldr	r6, [sp, #36]	; 0x24
   e271e:	930e      	str	r3, [sp, #56]	; 0x38
            const int output_channel = m + in_channel * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            int32 acc = 0;
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   e2720:	9506      	str	r5, [sp, #24]
   e2722:	9b06      	ldr	r3, [sp, #24]
   e2724:	9a1a      	ldr	r2, [sp, #104]	; 0x68
   e2726:	4293      	cmp	r3, r2
   e2728:	da1c      	bge.n	e2764 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x192>
   e272a:	2300      	movs	r3, #0
   e272c:	9f0a      	ldr	r7, [sp, #40]	; 0x28
   e272e:	9307      	str	r3, [sp, #28]
              for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   e2730:	9b07      	ldr	r3, [sp, #28]
   e2732:	9a1b      	ldr	r2, [sp, #108]	; 0x6c
   e2734:	4293      	cmp	r3, r2
   e2736:	da0f      	bge.n	e2758 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x186>
                const int in_x = in_x_origin + dilation_width_factor * filter_x;
                const int in_y =
                    in_y_origin + dilation_height_factor * filter_y;
                // Zero padding by omitting the areas outside the image.
                const bool is_point_inside_image =
                    (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
   e2738:	2f00      	cmp	r7, #0
   e273a:	db07      	blt.n	e274c <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x17a>
   e273c:	9b19      	ldr	r3, [sp, #100]	; 0x64
   e273e:	42bb      	cmp	r3, r7
   e2740:	dd04      	ble.n	e274c <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x17a>
   e2742:	2e00      	cmp	r6, #0
   e2744:	db02      	blt.n	e274c <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x17a>
   e2746:	9b18      	ldr	r3, [sp, #96]	; 0x60
   e2748:	42b3      	cmp	r3, r6
   e274a:	dc4a      	bgt.n	e27e2 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x210>
            const int output_channel = m + in_channel * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            int32 acc = 0;
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
              for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   e274c:	9b07      	ldr	r3, [sp, #28]
   e274e:	3301      	adds	r3, #1
   e2750:	9307      	str	r3, [sp, #28]
   e2752:	9b11      	ldr	r3, [sp, #68]	; 0x44
   e2754:	441f      	add	r7, r3
   e2756:	e7eb      	b.n	e2730 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x15e>
          for (int m = 0; m < depth_multiplier; ++m) {
            const int output_channel = m + in_channel * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            int32 acc = 0;
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   e2758:	9b06      	ldr	r3, [sp, #24]
   e275a:	3301      	adds	r3, #1
   e275c:	9306      	str	r3, [sp, #24]
   e275e:	9b12      	ldr	r3, [sp, #72]	; 0x48
   e2760:	441e      	add	r6, r3
   e2762:	e7de      	b.n	e2722 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x150>
                  // accumulator depth is smaller than 2^16.
                  acc += filter_val * (input_val + input_offset);
                }
              }
            }
            if (bias_data) {
   e2764:	9b32      	ldr	r3, [sp, #200]	; 0xc8
   e2766:	b11b      	cbz	r3, e2770 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x19e>
              acc += bias_data[output_channel];
   e2768:	9b21      	ldr	r3, [sp, #132]	; 0x84
   e276a:	f853 3024 	ldr.w	r3, [r3, r4, lsl #2]
   e276e:	441d      	add	r5, r3
            }
            acc = MultiplyByQuantizedMultiplier(
   e2770:	9b1f      	ldr	r3, [sp, #124]	; 0x7c
   e2772:	f853 2024 	ldr.w	r2, [r3, r4, lsl #2]
   e2776:	9b20      	ldr	r3, [sp, #128]	; 0x80
   e2778:	4628      	mov	r0, r5
   e277a:	f853 1024 	ldr.w	r1, [r3, r4, lsl #2]
   e277e:	f7f9 fa33 	bl	dbbe8 <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
                acc, output_multiplier[output_channel],
                output_shift[output_channel]);
            acc += output_offset;
   e2782:	9b16      	ldr	r3, [sp, #88]	; 0x58
            acc = std::max(acc, output_activation_min);
            acc = std::min(acc, output_activation_max);
            output_data[Offset(output_shape, batch, out_y, out_x,
   e2784:	9a03      	ldr	r2, [sp, #12]
              acc += bias_data[output_channel];
            }
            acc = MultiplyByQuantizedMultiplier(
                acc, output_multiplier[output_channel],
                output_shift[output_channel]);
            acc += output_offset;
   e2786:	4418      	add	r0, r3
   e2788:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e278a:	4283      	cmp	r3, r0
   e278c:	bfb8      	it	lt
   e278e:	4603      	movlt	r3, r0
   e2790:	461d      	mov	r5, r3
   e2792:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   e2794:	429d      	cmp	r5, r3
   e2796:	bfa8      	it	ge
   e2798:	461d      	movge	r5, r3
            acc = std::max(acc, output_activation_min);
            acc = std::min(acc, output_activation_max);
            output_data[Offset(output_shape, batch, out_y, out_x,
   e279a:	9b0e      	ldr	r3, [sp, #56]	; 0x38
   e279c:	9300      	str	r3, [sp, #0]
   e279e:	4649      	mov	r1, r9
   e27a0:	9b04      	ldr	r3, [sp, #16]
   e27a2:	4650      	mov	r0, sl
   e27a4:	f7f3 fec5 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                               output_channel)] = static_cast<int8_t>(acc);
   e27a8:	9b34      	ldr	r3, [sp, #208]	; 0xd0

  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
          for (int m = 0; m < depth_multiplier; ++m) {
   e27aa:	3401      	adds	r4, #1
                output_shift[output_channel]);
            acc += output_offset;
            acc = std::max(acc, output_activation_min);
            acc = std::min(acc, output_activation_max);
            output_data[Offset(output_shape, batch, out_y, out_x,
                               output_channel)] = static_cast<int8_t>(acc);
   e27ac:	541d      	strb	r5, [r3, r0]

  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
          for (int m = 0; m < depth_multiplier; ++m) {
   e27ae:	e7af      	b.n	e2710 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x13e>
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int in_channel = 0; in_channel < input_depth; ++in_channel) {
   e27b0:	9b05      	ldr	r3, [sp, #20]
   e27b2:	3301      	adds	r3, #1
   e27b4:	9305      	str	r3, [sp, #20]
   e27b6:	9b08      	ldr	r3, [sp, #32]
   e27b8:	4498      	add	r8, r3
   e27ba:	e799      	b.n	e26f0 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x11e>
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   e27bc:	9b04      	ldr	r3, [sp, #16]
   e27be:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
   e27c0:	3301      	adds	r3, #1
   e27c2:	9304      	str	r3, [sp, #16]
   e27c4:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e27c6:	4413      	add	r3, r2
   e27c8:	930a      	str	r3, [sp, #40]	; 0x28
   e27ca:	e789      	b.n	e26e0 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x10e>
  const int output_width = output_shape.Dims(2);
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   e27cc:	9b03      	ldr	r3, [sp, #12]
   e27ce:	9a10      	ldr	r2, [sp, #64]	; 0x40
   e27d0:	3301      	adds	r3, #1
   e27d2:	9303      	str	r3, [sp, #12]
   e27d4:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e27d6:	4413      	add	r3, r2
   e27d8:	9309      	str	r3, [sp, #36]	; 0x24
   e27da:	e777      	b.n	e26cc <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0xfa>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int batch = 0; batch < batches; ++batch) {
   e27dc:	f109 0901 	add.w	r9, r9, #1
   e27e0:	e76b      	b.n	e26ba <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0xe8>
                // Zero padding by omitting the areas outside the image.
                const bool is_point_inside_image =
                    (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height);
                if (is_point_inside_image) {
                  int32 input_val = input_data[Offset(input_shape, batch, in_y,
   e27e2:	9b05      	ldr	r3, [sp, #20]
   e27e4:	9300      	str	r3, [sp, #0]
   e27e6:	4632      	mov	r2, r6
   e27e8:	463b      	mov	r3, r7
   e27ea:	4649      	mov	r1, r9
   e27ec:	4658      	mov	r0, fp
   e27ee:	f7f3 fea0 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                                                      in_x, in_channel)];
                  int32 filter_val = filter_data[Offset(
   e27f2:	9b0e      	ldr	r3, [sp, #56]	; 0x38
                // Zero padding by omitting the areas outside the image.
                const bool is_point_inside_image =
                    (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height);
                if (is_point_inside_image) {
                  int32 input_val = input_data[Offset(input_shape, batch, in_y,
   e27f4:	901e      	str	r0, [sp, #120]	; 0x78
                                                      in_x, in_channel)];
                  int32 filter_val = filter_data[Offset(
   e27f6:	9300      	str	r3, [sp, #0]
   e27f8:	9a06      	ldr	r2, [sp, #24]
   e27fa:	9b07      	ldr	r3, [sp, #28]
   e27fc:	982f      	ldr	r0, [sp, #188]	; 0xbc
   e27fe:	2100      	movs	r1, #0
   e2800:	f7f3 fe97 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                  // long as the filter size (filter_y * filter_x * in_channel)
                  // does not exceed 2^16, which is the case in all the models
                  // we have seen so far.
                  // TODO(jianlijianli): Add a check to make sure the
                  // accumulator depth is smaller than 2^16.
                  acc += filter_val * (input_val + input_offset);
   e2804:	9a1e      	ldr	r2, [sp, #120]	; 0x78
   e2806:	9b2e      	ldr	r3, [sp, #184]	; 0xb8
   e2808:	569b      	ldrsb	r3, [r3, r2]
   e280a:	9a15      	ldr	r2, [sp, #84]	; 0x54
   e280c:	4413      	add	r3, r2
   e280e:	9a30      	ldr	r2, [sp, #192]	; 0xc0
   e2810:	5612      	ldrsb	r2, [r2, r0]
   e2812:	fb02 5503 	mla	r5, r2, r3, r5
   e2816:	e799      	b.n	e274c <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa+0x17a>
          }
        }
      }
    }
  }
}
   e2818:	b025      	add	sp, #148	; 0x94
   e281a:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

000e2820 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf>:
inline void DepthwiseConv(
    const DepthwiseParams& params, const RuntimeShape& input_shape,
    const float* input_data, const RuntimeShape& filter_shape,
    const float* filter_data, const RuntimeShape& bias_shape,
    const float* bias_data, const RuntimeShape& output_shape,
    float* output_data) {
   e2820:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e2824:	ed2d 8b04 	vpush	{d8-d9}
   e2828:	b09d      	sub	sp, #116	; 0x74
   e282a:	4698      	mov	r8, r3
  const int stride_width = params.stride_width;
   e282c:	f9b0 300a 	ldrsh.w	r3, [r0, #10]
   e2830:	930d      	str	r3, [sp, #52]	; 0x34
  const int stride_height = params.stride_height;
   e2832:	f9b0 300c 	ldrsh.w	r3, [r0, #12]
   e2836:	930e      	str	r3, [sp, #56]	; 0x38
  const int dilation_width_factor = params.dilation_width_factor;
   e2838:	f9b0 300e 	ldrsh.w	r3, [r0, #14]
   e283c:	930f      	str	r3, [sp, #60]	; 0x3c
  const int dilation_height_factor = params.dilation_height_factor;
   e283e:	f9b0 3010 	ldrsh.w	r3, [r0, #16]
   e2842:	9310      	str	r3, [sp, #64]	; 0x40
  const int pad_width = params.padding_values.width;
   e2844:	f9b0 3002 	ldrsh.w	r3, [r0, #2]
   e2848:	9311      	str	r3, [sp, #68]	; 0x44
  const int pad_height = params.padding_values.height;
   e284a:	f9b0 3004 	ldrsh.w	r3, [r0, #4]
   e284e:	9312      	str	r3, [sp, #72]	; 0x48
  const int depth_multiplier = params.depth_multiplier;
   e2850:	f9b0 3012 	ldrsh.w	r3, [r0, #18]
   e2854:	9307      	str	r3, [sp, #28]
  const float output_activation_min = params.float_activation_min;
  const float output_activation_max = params.float_activation_max;
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e2856:	680b      	ldr	r3, [r1, #0]
inline void DepthwiseConv(
    const DepthwiseParams& params, const RuntimeShape& input_shape,
    const float* input_data, const RuntimeShape& filter_shape,
    const float* filter_data, const RuntimeShape& bias_shape,
    const float* bias_data, const RuntimeShape& output_shape,
    float* output_data) {
   e2858:	921b      	str	r2, [sp, #108]	; 0x6c
  const int pad_width = params.padding_values.width;
  const int pad_height = params.padding_values.height;
  const int depth_multiplier = params.depth_multiplier;
  const float output_activation_min = params.float_activation_min;
  const float output_activation_max = params.float_activation_max;
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e285a:	2b04      	cmp	r3, #4
inline void DepthwiseConv(
    const DepthwiseParams& params, const RuntimeShape& input_shape,
    const float* input_data, const RuntimeShape& filter_shape,
    const float* filter_data, const RuntimeShape& bias_shape,
    const float* bias_data, const RuntimeShape& output_shape,
    float* output_data) {
   e285c:	460f      	mov	r7, r1
   e285e:	f8dd 90b4 	ldr.w	r9, [sp, #180]	; 0xb4
  const int dilation_width_factor = params.dilation_width_factor;
  const int dilation_height_factor = params.dilation_height_factor;
  const int pad_width = params.padding_values.width;
  const int pad_height = params.padding_values.height;
  const int depth_multiplier = params.depth_multiplier;
  const float output_activation_min = params.float_activation_min;
   e2862:	edd0 8a0c 	vldr	s17, [r0, #48]	; 0x30
  const float output_activation_max = params.float_activation_max;
   e2866:	ed90 9a0d 	vldr	s18, [r0, #52]	; 0x34
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e286a:	d001      	beq.n	e2870 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x50>
   e286c:	f001 fd6e 	bl	e434c <abort>
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
   e2870:	f8d8 3000 	ldr.w	r3, [r8]
   e2874:	2b04      	cmp	r3, #4
   e2876:	d1f9      	bne.n	e286c <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x4c>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   e2878:	f8d9 3000 	ldr.w	r3, [r9]
   e287c:	2b04      	cmp	r3, #4
   e287e:	d1f5      	bne.n	e286c <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x4c>

  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   e2880:	2300      	movs	r3, #0
   e2882:	4619      	mov	r1, r3
   e2884:	464a      	mov	r2, r9
   e2886:	4638      	mov	r0, r7
   e2888:	f7f9 f99f 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e288c:	2303      	movs	r3, #3
   e288e:	4619      	mov	r1, r3
   e2890:	464a      	mov	r2, r9
  const float output_activation_max = params.float_activation_max;
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   e2892:	9013      	str	r0, [sp, #76]	; 0x4c
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e2894:	4640      	mov	r0, r8
   e2896:	f7f9 f998 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
   e289a:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e289c:	4604      	mov	r4, r0
  const int input_height = input_shape.Dims(1);
   e289e:	4638      	mov	r0, r7
   e28a0:	f7f3 fde2 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   e28a4:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
   e28a6:	9014      	str	r0, [sp, #80]	; 0x50
  const int input_width = input_shape.Dims(2);
   e28a8:	4638      	mov	r0, r7
   e28aa:	f7f3 fddd 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_depth = input_shape.Dims(3);
   e28ae:	2103      	movs	r1, #3
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   e28b0:	9015      	str	r0, [sp, #84]	; 0x54
  const int input_depth = input_shape.Dims(3);
   e28b2:	4638      	mov	r0, r7
   e28b4:	f7f3 fdd8 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_height = filter_shape.Dims(1);
   e28b8:	2101      	movs	r1, #1

  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
   e28ba:	900b      	str	r0, [sp, #44]	; 0x2c
  const int filter_height = filter_shape.Dims(1);
   e28bc:	4640      	mov	r0, r8
   e28be:	f7f3 fdd3 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_width = filter_shape.Dims(2);
   e28c2:	2102      	movs	r1, #2
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
   e28c4:	9016      	str	r0, [sp, #88]	; 0x58
  const int filter_width = filter_shape.Dims(2);
   e28c6:	4640      	mov	r0, r8
   e28c8:	f7f3 fdce 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   e28cc:	2101      	movs	r1, #1
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
   e28ce:	9017      	str	r0, [sp, #92]	; 0x5c
  const int output_height = output_shape.Dims(1);
   e28d0:	4648      	mov	r0, r9
   e28d2:	f7f3 fdc9 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   e28d6:	2102      	movs	r1, #2
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   e28d8:	9018      	str	r0, [sp, #96]	; 0x60
  const int output_width = output_shape.Dims(2);
   e28da:	4648      	mov	r0, r9
   e28dc:	f7f3 fdc4 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
   e28e0:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e28e2:	9a07      	ldr	r2, [sp, #28]
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   e28e4:	9019      	str	r0, [sp, #100]	; 0x64
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
   e28e6:	4353      	muls	r3, r2
   e28e8:	429c      	cmp	r4, r3
   e28ea:	d1bf      	bne.n	e286c <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x4c>
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
   e28ec:	982b      	ldr	r0, [sp, #172]	; 0xac
   e28ee:	f7f9 f95c 	bl	dbbaa <_ZNK6tflite12RuntimeShape8FlatSizeEv>
   e28f2:	4284      	cmp	r4, r0
   e28f4:	d1ba      	bne.n	e286c <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x4c>
   e28f6:	f04f 0b00 	mov.w	fp, #0

  for (int b = 0; b < batches; ++b) {
   e28fa:	9b13      	ldr	r3, [sp, #76]	; 0x4c
   e28fc:	459b      	cmp	fp, r3
   e28fe:	f280 80ad 	bge.w	e2a5c <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x23c>
   e2902:	9b12      	ldr	r3, [sp, #72]	; 0x48
   e2904:	425b      	negs	r3, r3
   e2906:	9309      	str	r3, [sp, #36]	; 0x24
   e2908:	2300      	movs	r3, #0
   e290a:	9302      	str	r3, [sp, #8]
    for (int out_y = 0; out_y < output_height; ++out_y) {
   e290c:	9b02      	ldr	r3, [sp, #8]
   e290e:	9a18      	ldr	r2, [sp, #96]	; 0x60
   e2910:	4293      	cmp	r3, r2
   e2912:	f280 80a0 	bge.w	e2a56 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x236>
   e2916:	9b11      	ldr	r3, [sp, #68]	; 0x44
   e2918:	425b      	negs	r3, r3
   e291a:	9308      	str	r3, [sp, #32]
   e291c:	2300      	movs	r3, #0
   e291e:	9303      	str	r3, [sp, #12]
      for (int out_x = 0; out_x < output_width; ++out_x) {
   e2920:	9b03      	ldr	r3, [sp, #12]
   e2922:	9a19      	ldr	r2, [sp, #100]	; 0x64
   e2924:	4293      	cmp	r3, r2
   e2926:	f280 808e 	bge.w	e2a46 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x226>
   e292a:	2400      	movs	r4, #0
   e292c:	9404      	str	r4, [sp, #16]
        for (int ic = 0; ic < input_depth; ++ic) {
   e292e:	9b04      	ldr	r3, [sp, #16]
   e2930:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   e2932:	4293      	cmp	r3, r2
   e2934:	da7f      	bge.n	e2a36 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x216>
   e2936:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   e2938:	eb03 0384 	add.w	r3, r3, r4, lsl #2
   e293c:	930a      	str	r3, [sp, #40]	; 0x28
   e293e:	2300      	movs	r3, #0
   e2940:	9305      	str	r3, [sp, #20]
          for (int m = 0; m < depth_multiplier; m++) {
   e2942:	9b05      	ldr	r3, [sp, #20]
   e2944:	9a07      	ldr	r2, [sp, #28]
   e2946:	4293      	cmp	r3, r2
   e2948:	da6f      	bge.n	e2a2a <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x20a>
   e294a:	4423      	add	r3, r4
   e294c:	930c      	str	r3, [sp, #48]	; 0x30
   e294e:	9d09      	ldr	r5, [sp, #36]	; 0x24
            const int oc = m + ic * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            float total = 0.f;
   e2950:	ed9f 8a45 	vldr	s16, [pc, #276]	; e2a68 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x248>
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   e2954:	2300      	movs	r3, #0
   e2956:	9306      	str	r3, [sp, #24]
   e2958:	9b06      	ldr	r3, [sp, #24]
   e295a:	9a16      	ldr	r2, [sp, #88]	; 0x58
   e295c:	4293      	cmp	r3, r2
   e295e:	da38      	bge.n	e29d2 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x1b2>
   e2960:	9e08      	ldr	r6, [sp, #32]
   e2962:	f04f 0a00 	mov.w	sl, #0
              for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   e2966:	9b17      	ldr	r3, [sp, #92]	; 0x5c
   e2968:	459a      	cmp	sl, r3
   e296a:	da2c      	bge.n	e29c6 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x1a6>
                const int in_x = in_x_origin + dilation_width_factor * filter_x;
                const int in_y =
                    in_y_origin + dilation_height_factor * filter_y;
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
   e296c:	2e00      	cmp	r6, #0
   e296e:	db25      	blt.n	e29bc <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x19c>
   e2970:	9b15      	ldr	r3, [sp, #84]	; 0x54
   e2972:	42b3      	cmp	r3, r6
   e2974:	dd22      	ble.n	e29bc <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x19c>
   e2976:	2d00      	cmp	r5, #0
   e2978:	db20      	blt.n	e29bc <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x19c>
   e297a:	9b14      	ldr	r3, [sp, #80]	; 0x50
   e297c:	42ab      	cmp	r3, r5
   e297e:	dd1d      	ble.n	e29bc <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x19c>
                    (in_y < input_height)) {
                  float input_value =
                      input_data[Offset(input_shape, b, in_y, in_x, ic)];
   e2980:	9b04      	ldr	r3, [sp, #16]
   e2982:	9300      	str	r3, [sp, #0]
   e2984:	462a      	mov	r2, r5
   e2986:	4633      	mov	r3, r6
   e2988:	4659      	mov	r1, fp
   e298a:	4638      	mov	r0, r7
   e298c:	f7f3 fdd1 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                  float filter_value = filter_data[Offset(
   e2990:	9b0c      	ldr	r3, [sp, #48]	; 0x30
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height)) {
                  float input_value =
                      input_data[Offset(input_shape, b, in_y, in_x, ic)];
   e2992:	901a      	str	r0, [sp, #104]	; 0x68
                  float filter_value = filter_data[Offset(
   e2994:	9300      	str	r3, [sp, #0]
   e2996:	9a06      	ldr	r2, [sp, #24]
   e2998:	4653      	mov	r3, sl
   e299a:	2100      	movs	r1, #0
   e299c:	4640      	mov	r0, r8
   e299e:	f7f3 fdc8 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                // If the location is outside the bounds of the input image,
                // use zero as a default value.
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height)) {
                  float input_value =
                      input_data[Offset(input_shape, b, in_y, in_x, ic)];
   e29a2:	9a1a      	ldr	r2, [sp, #104]	; 0x68
   e29a4:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
   e29a6:	eb03 0382 	add.w	r3, r3, r2, lsl #2
                  float filter_value = filter_data[Offset(
                      filter_shape, 0, filter_y, filter_x, oc)];
   e29aa:	9a2a      	ldr	r2, [sp, #168]	; 0xa8
                  total += (input_value * filter_value);
   e29ac:	ed93 7a00 	vldr	s14, [r3]
                if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                    (in_y < input_height)) {
                  float input_value =
                      input_data[Offset(input_shape, b, in_y, in_x, ic)];
                  float filter_value = filter_data[Offset(
                      filter_shape, 0, filter_y, filter_x, oc)];
   e29b0:	eb02 0080 	add.w	r0, r2, r0, lsl #2
                  total += (input_value * filter_value);
   e29b4:	edd0 7a00 	vldr	s15, [r0]
   e29b8:	eea7 8a27 	vfma.f32	s16, s14, s15
   e29bc:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
            const int oc = m + ic * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            float total = 0.f;
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
              for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   e29be:	f10a 0a01 	add.w	sl, sl, #1
   e29c2:	441e      	add	r6, r3
   e29c4:	e7cf      	b.n	e2966 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x146>
          for (int m = 0; m < depth_multiplier; m++) {
            const int oc = m + ic * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            float total = 0.f;
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   e29c6:	9b06      	ldr	r3, [sp, #24]
   e29c8:	3301      	adds	r3, #1
   e29ca:	9306      	str	r3, [sp, #24]
   e29cc:	9b10      	ldr	r3, [sp, #64]	; 0x40
   e29ce:	441d      	add	r5, r3
   e29d0:	e7c2      	b.n	e2958 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x138>
                  total += (input_value * filter_value);
                }
              }
            }
            float bias_value = 0.0f;
            if (bias_data) {
   e29d2:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   e29d4:	b11b      	cbz	r3, e29de <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x1be>
              bias_value = bias_data[oc];
   e29d6:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e29d8:	edd3 9a00 	vldr	s19, [r3]
   e29dc:	e001      	b.n	e29e2 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x1c2>
                      filter_shape, 0, filter_y, filter_x, oc)];
                  total += (input_value * filter_value);
                }
              }
            }
            float bias_value = 0.0f;
   e29de:	eddf 9a22 	vldr	s19, [pc, #136]	; e2a68 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x248>
            if (bias_data) {
              bias_value = bias_data[oc];
            }
            output_data[Offset(output_shape, b, out_y, out_x, oc)] =
   e29e2:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   e29e4:	9300      	str	r3, [sp, #0]
   e29e6:	9a02      	ldr	r2, [sp, #8]
   e29e8:	9b03      	ldr	r3, [sp, #12]
   e29ea:	4659      	mov	r1, fp
   e29ec:	4648      	mov	r0, r9
   e29ee:	f7f3 fda0 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   e29f2:	9b2e      	ldr	r3, [sp, #184]	; 0xb8
                ActivationFunctionWithMinMax(total + bias_value,
   e29f4:	ee78 7a29 	vadd.f32	s15, s16, s19
            }
            float bias_value = 0.0f;
            if (bias_data) {
              bias_value = bias_data[oc];
            }
            output_data[Offset(output_shape, b, out_y, out_x, oc)] =
   e29f8:	eb03 0080 	add.w	r0, r3, r0, lsl #2
      return __a;
   e29fc:	eef4 8ae7 	vcmpe.f32	s17, s15

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int ic = 0; ic < input_depth; ++ic) {
          for (int m = 0; m < depth_multiplier; m++) {
   e2a00:	9b05      	ldr	r3, [sp, #20]
   e2a02:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e2a06:	bfc8      	it	gt
   e2a08:	eef0 7a68 	vmovgt.f32	s15, s17
   e2a0c:	3301      	adds	r3, #1
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
	return __b;
   e2a0e:	eeb4 9a67 	vcmp.f32	s18, s15
   e2a12:	9305      	str	r3, [sp, #20]
   e2a14:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e2a16:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e2a1a:	bf48      	it	mi
   e2a1c:	eef0 7a49 	vmovmi.f32	s15, s18
   e2a20:	3304      	adds	r3, #4
              bias_value = bias_data[oc];
            }
            output_data[Offset(output_shape, b, out_y, out_x, oc)] =
                ActivationFunctionWithMinMax(total + bias_value,
                                             output_activation_min,
                                             output_activation_max);
   e2a22:	edc0 7a00 	vstr	s15, [r0]
   e2a26:	930a      	str	r3, [sp, #40]	; 0x28

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int ic = 0; ic < input_depth; ++ic) {
          for (int m = 0; m < depth_multiplier; m++) {
   e2a28:	e78b      	b.n	e2942 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x122>
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int ic = 0; ic < input_depth; ++ic) {
   e2a2a:	9b04      	ldr	r3, [sp, #16]
   e2a2c:	3301      	adds	r3, #1
   e2a2e:	9304      	str	r3, [sp, #16]
   e2a30:	9b07      	ldr	r3, [sp, #28]
   e2a32:	441c      	add	r4, r3
   e2a34:	e77b      	b.n	e292e <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x10e>
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   e2a36:	9b03      	ldr	r3, [sp, #12]
   e2a38:	9a0d      	ldr	r2, [sp, #52]	; 0x34
   e2a3a:	3301      	adds	r3, #1
   e2a3c:	9303      	str	r3, [sp, #12]
   e2a3e:	9b08      	ldr	r3, [sp, #32]
   e2a40:	4413      	add	r3, r2
   e2a42:	9308      	str	r3, [sp, #32]
   e2a44:	e76c      	b.n	e2920 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0x100>
  const int output_width = output_shape.Dims(2);
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   e2a46:	9b02      	ldr	r3, [sp, #8]
   e2a48:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   e2a4a:	3301      	adds	r3, #1
   e2a4c:	9302      	str	r3, [sp, #8]
   e2a4e:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e2a50:	4413      	add	r3, r2
   e2a52:	9309      	str	r3, [sp, #36]	; 0x24
   e2a54:	e75a      	b.n	e290c <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0xec>
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

  for (int b = 0; b < batches; ++b) {
   e2a56:	f10b 0b01 	add.w	fp, fp, #1
   e2a5a:	e74e      	b.n	e28fa <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf+0xda>
          }
        }
      }
    }
  }
}
   e2a5c:	b01d      	add	sp, #116	; 0x74
   e2a5e:	ecbd 8b04 	vpop	{d8-d9}
   e2a62:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e2a66:	bf00      	nop
   e2a68:	00000000 	.word	0x00000000

000e2a6c <_ZN6tflite3ops5micro26Register_DEPTHWISE_CONV_2DEv>:

TfLiteRegistration* Register_DEPTHWISE_CONV_2D() {
  static TfLiteRegistration r = {depthwise_conv::Init, depthwise_conv::Free,
                                 depthwise_conv::Prepare, depthwise_conv::Eval};
  return &r;
}
   e2a6c:	4800      	ldr	r0, [pc, #0]	; (e2a70 <_ZN6tflite3ops5micro26Register_DEPTHWISE_CONV_2DEv+0x4>)
   e2a6e:	4770      	bx	lr
   e2a70:	2003c37c 	.word	0x2003c37c

000e2a74 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph>:
         right_shift;
}

template <DepthwiseConvOutputRounding output_rounding>
struct DepthwiseConvBasicKernel {
  static inline void Run(const DepthwiseParams& params,
   e2a74:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e2a78:	b0a5      	sub	sp, #148	; 0x94
   e2a7a:	469a      	mov	sl, r3
                         const uint8* input_data,
                         const RuntimeShape& filter_shape,
                         const uint8* filter_data,
                         const RuntimeShape& bias_shape, const int32* bias_data,
                         const RuntimeShape& output_shape, uint8* output_data) {
    const int stride_width = params.stride_width;
   e2a7c:	f9b0 300a 	ldrsh.w	r3, [r0, #10]
   e2a80:	930f      	str	r3, [sp, #60]	; 0x3c
    const int stride_height = params.stride_height;
   e2a82:	f9b0 300c 	ldrsh.w	r3, [r0, #12]
   e2a86:	9310      	str	r3, [sp, #64]	; 0x40
    const int dilation_width_factor = params.dilation_width_factor;
   e2a88:	f9b0 300e 	ldrsh.w	r3, [r0, #14]
   e2a8c:	9311      	str	r3, [sp, #68]	; 0x44
    const int dilation_height_factor = params.dilation_height_factor;
   e2a8e:	f9b0 3010 	ldrsh.w	r3, [r0, #16]
   e2a92:	9312      	str	r3, [sp, #72]	; 0x48
    const int pad_width = params.padding_values.width;
   e2a94:	f9b0 3002 	ldrsh.w	r3, [r0, #2]
   e2a98:	9313      	str	r3, [sp, #76]	; 0x4c
    const int pad_height = params.padding_values.height;
   e2a9a:	f9b0 3004 	ldrsh.w	r3, [r0, #4]
   e2a9e:	9314      	str	r3, [sp, #80]	; 0x50
    const int depth_multiplier = params.depth_multiplier;
   e2aa0:	f9b0 3012 	ldrsh.w	r3, [r0, #18]
   e2aa4:	9308      	str	r3, [sp, #32]
    const int32 output_activation_min = params.quantized_activation_min;
   e2aa6:	6a83      	ldr	r3, [r0, #40]	; 0x28
   e2aa8:	930b      	str	r3, [sp, #44]	; 0x2c
    const int32 output_activation_max = params.quantized_activation_max;
   e2aaa:	6ac3      	ldr	r3, [r0, #44]	; 0x2c
   e2aac:	930c      	str	r3, [sp, #48]	; 0x30
    const int32 input_offset = params.input_offset;
   e2aae:	6943      	ldr	r3, [r0, #20]
   e2ab0:	9315      	str	r3, [sp, #84]	; 0x54
    const int32 filter_offset = params.weights_offset;
   e2ab2:	6983      	ldr	r3, [r0, #24]
   e2ab4:	9316      	str	r3, [sp, #88]	; 0x58
    const int32 output_offset = params.output_offset;
   e2ab6:	69c3      	ldr	r3, [r0, #28]
   e2ab8:	9317      	str	r3, [sp, #92]	; 0x5c
    const int32 output_multiplier = params.output_multiplier;
   e2aba:	6a03      	ldr	r3, [r0, #32]
   e2abc:	9318      	str	r3, [sp, #96]	; 0x60
    const int output_shift = params.output_shift;
   e2abe:	6a43      	ldr	r3, [r0, #36]	; 0x24
   e2ac0:	9319      	str	r3, [sp, #100]	; 0x64
    TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e2ac2:	680b      	ldr	r3, [r1, #0]
         right_shift;
}

template <DepthwiseConvOutputRounding output_rounding>
struct DepthwiseConvBasicKernel {
  static inline void Run(const DepthwiseParams& params,
   e2ac4:	9223      	str	r2, [sp, #140]	; 0x8c
    const int32 input_offset = params.input_offset;
    const int32 filter_offset = params.weights_offset;
    const int32 output_offset = params.output_offset;
    const int32 output_multiplier = params.output_multiplier;
    const int output_shift = params.output_shift;
    TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e2ac6:	2b04      	cmp	r3, #4
         right_shift;
}

template <DepthwiseConvOutputRounding output_rounding>
struct DepthwiseConvBasicKernel {
  static inline void Run(const DepthwiseParams& params,
   e2ac8:	4689      	mov	r9, r1
    const int32 input_offset = params.input_offset;
    const int32 filter_offset = params.weights_offset;
    const int32 output_offset = params.output_offset;
    const int32 output_multiplier = params.output_multiplier;
    const int output_shift = params.output_shift;
    TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e2aca:	d001      	beq.n	e2ad0 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x5c>
   e2acc:	f001 fc3e 	bl	e434c <abort>
    TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
   e2ad0:	f8da 3000 	ldr.w	r3, [sl]
   e2ad4:	2b04      	cmp	r3, #4
   e2ad6:	d1f9      	bne.n	e2acc <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x58>
    TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   e2ad8:	9b31      	ldr	r3, [sp, #196]	; 0xc4
   e2ada:	681b      	ldr	r3, [r3, #0]
   e2adc:	2b04      	cmp	r3, #4
   e2ade:	d1f5      	bne.n	e2acc <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x58>

    TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   e2ae0:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e2ae2:	9a0c      	ldr	r2, [sp, #48]	; 0x30
   e2ae4:	4293      	cmp	r3, r2
   e2ae6:	dcf1      	bgt.n	e2acc <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x58>
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   e2ae8:	2300      	movs	r3, #0
   e2aea:	4619      	mov	r1, r3
   e2aec:	9a31      	ldr	r2, [sp, #196]	; 0xc4
   e2aee:	4648      	mov	r0, r9
   e2af0:	f7f9 f86b 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e2af4:	2303      	movs	r3, #3
   e2af6:	4619      	mov	r1, r3
   e2af8:	9a31      	ldr	r2, [sp, #196]	; 0xc4
    TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
    TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
    TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

    TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   e2afa:	901a      	str	r0, [sp, #104]	; 0x68
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e2afc:	4650      	mov	r0, sl
   e2afe:	f7f9 f864 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
    const int input_height = input_shape.Dims(1);
   e2b02:	2101      	movs	r1, #1
    TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
    TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

    TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e2b04:	4604      	mov	r4, r0
    const int input_height = input_shape.Dims(1);
   e2b06:	4648      	mov	r0, r9
   e2b08:	f7f3 fcae 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
    const int input_width = input_shape.Dims(2);
   e2b0c:	2102      	movs	r1, #2
    TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

    TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
    const int input_height = input_shape.Dims(1);
   e2b0e:	901b      	str	r0, [sp, #108]	; 0x6c
    const int input_width = input_shape.Dims(2);
   e2b10:	4648      	mov	r0, r9
   e2b12:	f7f3 fca9 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
    const int input_depth = input_shape.Dims(3);
   e2b16:	2103      	movs	r1, #3

    TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
    const int input_height = input_shape.Dims(1);
    const int input_width = input_shape.Dims(2);
   e2b18:	901c      	str	r0, [sp, #112]	; 0x70
    const int input_depth = input_shape.Dims(3);
   e2b1a:	4648      	mov	r0, r9
   e2b1c:	f7f3 fca4 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
    const int filter_height = filter_shape.Dims(1);
   e2b20:	2101      	movs	r1, #1
    TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
    const int input_height = input_shape.Dims(1);
    const int input_width = input_shape.Dims(2);
    const int input_depth = input_shape.Dims(3);
   e2b22:	900d      	str	r0, [sp, #52]	; 0x34
    const int filter_height = filter_shape.Dims(1);
   e2b24:	4650      	mov	r0, sl
   e2b26:	f7f3 fc9f 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
    const int filter_width = filter_shape.Dims(2);
   e2b2a:	2102      	movs	r1, #2
    const int batches = MatchingDim(input_shape, 0, output_shape, 0);
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
    const int input_height = input_shape.Dims(1);
    const int input_width = input_shape.Dims(2);
    const int input_depth = input_shape.Dims(3);
    const int filter_height = filter_shape.Dims(1);
   e2b2c:	901d      	str	r0, [sp, #116]	; 0x74
    const int filter_width = filter_shape.Dims(2);
   e2b2e:	4650      	mov	r0, sl
   e2b30:	f7f3 fc9a 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
    const int output_height = output_shape.Dims(1);
   e2b34:	2101      	movs	r1, #1
    const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
    const int input_height = input_shape.Dims(1);
    const int input_width = input_shape.Dims(2);
    const int input_depth = input_shape.Dims(3);
    const int filter_height = filter_shape.Dims(1);
    const int filter_width = filter_shape.Dims(2);
   e2b36:	901e      	str	r0, [sp, #120]	; 0x78
    const int output_height = output_shape.Dims(1);
   e2b38:	9831      	ldr	r0, [sp, #196]	; 0xc4
   e2b3a:	f7f3 fc95 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
    const int output_width = output_shape.Dims(2);
   e2b3e:	2102      	movs	r1, #2
    const int input_height = input_shape.Dims(1);
    const int input_width = input_shape.Dims(2);
    const int input_depth = input_shape.Dims(3);
    const int filter_height = filter_shape.Dims(1);
    const int filter_width = filter_shape.Dims(2);
    const int output_height = output_shape.Dims(1);
   e2b40:	901f      	str	r0, [sp, #124]	; 0x7c
    const int output_width = output_shape.Dims(2);
   e2b42:	9831      	ldr	r0, [sp, #196]	; 0xc4
   e2b44:	f7f3 fc90 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
    TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
   e2b48:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   e2b4a:	9a08      	ldr	r2, [sp, #32]
    const int input_width = input_shape.Dims(2);
    const int input_depth = input_shape.Dims(3);
    const int filter_height = filter_shape.Dims(1);
    const int filter_width = filter_shape.Dims(2);
    const int output_height = output_shape.Dims(1);
    const int output_width = output_shape.Dims(2);
   e2b4c:	9020      	str	r0, [sp, #128]	; 0x80
    TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
   e2b4e:	4353      	muls	r3, r2
   e2b50:	429c      	cmp	r4, r3
   e2b52:	d1bb      	bne.n	e2acc <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x58>
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
   e2b54:	982f      	ldr	r0, [sp, #188]	; 0xbc
   e2b56:	f7f9 f828 	bl	dbbaa <_ZNK6tflite12RuntimeShape8FlatSizeEv>
   e2b5a:	4284      	cmp	r4, r0
   e2b5c:	d1b6      	bne.n	e2acc <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x58>
   e2b5e:	f04f 0b00 	mov.w	fp, #0

    for (int b = 0; b < batches; ++b) {
   e2b62:	9b1a      	ldr	r3, [sp, #104]	; 0x68
   e2b64:	459b      	cmp	fp, r3
   e2b66:	f280 80a1 	bge.w	e2cac <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x238>
   e2b6a:	9b14      	ldr	r3, [sp, #80]	; 0x50
   e2b6c:	425b      	negs	r3, r3
   e2b6e:	9309      	str	r3, [sp, #36]	; 0x24
   e2b70:	2300      	movs	r3, #0
   e2b72:	9303      	str	r3, [sp, #12]
      for (int out_y = 0; out_y < output_height; ++out_y) {
   e2b74:	9b03      	ldr	r3, [sp, #12]
   e2b76:	9a1f      	ldr	r2, [sp, #124]	; 0x7c
   e2b78:	4293      	cmp	r3, r2
   e2b7a:	f280 8094 	bge.w	e2ca6 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x232>
   e2b7e:	9b13      	ldr	r3, [sp, #76]	; 0x4c
   e2b80:	425b      	negs	r3, r3
   e2b82:	930a      	str	r3, [sp, #40]	; 0x28
   e2b84:	2300      	movs	r3, #0
   e2b86:	9304      	str	r3, [sp, #16]
        for (int out_x = 0; out_x < output_width; ++out_x) {
   e2b88:	9b04      	ldr	r3, [sp, #16]
   e2b8a:	9a20      	ldr	r2, [sp, #128]	; 0x80
   e2b8c:	4293      	cmp	r3, r2
   e2b8e:	f280 8082 	bge.w	e2c96 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x222>
   e2b92:	2500      	movs	r5, #0
   e2b94:	9505      	str	r5, [sp, #20]
          for (int ic = 0; ic < input_depth; ++ic) {
   e2b96:	9b05      	ldr	r3, [sp, #20]
   e2b98:	9a0d      	ldr	r2, [sp, #52]	; 0x34
   e2b9a:	4293      	cmp	r3, r2
   e2b9c:	da73      	bge.n	e2c86 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x212>
   e2b9e:	9b30      	ldr	r3, [sp, #192]	; 0xc0
   e2ba0:	eb03 0385 	add.w	r3, r3, r5, lsl #2
   e2ba4:	9322      	str	r3, [sp, #136]	; 0x88
   e2ba6:	2600      	movs	r6, #0
            for (int m = 0; m < depth_multiplier; m++) {
   e2ba8:	9b08      	ldr	r3, [sp, #32]
   e2baa:	429e      	cmp	r6, r3
   e2bac:	da65      	bge.n	e2c7a <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x206>
   e2bae:	1973      	adds	r3, r6, r5
              const int oc = m + ic * depth_multiplier;
              const int in_x_origin = (out_x * stride_width) - pad_width;
              const int in_y_origin = (out_y * stride_height) - pad_height;
              int32 acc = 0;
   e2bb0:	2400      	movs	r4, #0

    for (int b = 0; b < batches; ++b) {
      for (int out_y = 0; out_y < output_height; ++out_y) {
        for (int out_x = 0; out_x < output_width; ++out_x) {
          for (int ic = 0; ic < input_depth; ++ic) {
            for (int m = 0; m < depth_multiplier; m++) {
   e2bb2:	9f09      	ldr	r7, [sp, #36]	; 0x24
   e2bb4:	930e      	str	r3, [sp, #56]	; 0x38
              const int oc = m + ic * depth_multiplier;
              const int in_x_origin = (out_x * stride_width) - pad_width;
              const int in_y_origin = (out_y * stride_height) - pad_height;
              int32 acc = 0;
              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   e2bb6:	9406      	str	r4, [sp, #24]
   e2bb8:	9b06      	ldr	r3, [sp, #24]
   e2bba:	9a1d      	ldr	r2, [sp, #116]	; 0x74
   e2bbc:	4293      	cmp	r3, r2
   e2bbe:	da3a      	bge.n	e2c36 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1c2>
   e2bc0:	2300      	movs	r3, #0
   e2bc2:	f8dd 8028 	ldr.w	r8, [sp, #40]	; 0x28
   e2bc6:	9307      	str	r3, [sp, #28]
                for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   e2bc8:	9b07      	ldr	r3, [sp, #28]
   e2bca:	9a1e      	ldr	r2, [sp, #120]	; 0x78
   e2bcc:	4293      	cmp	r3, r2
   e2bce:	da2c      	bge.n	e2c2a <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1b6>
                      in_x_origin + dilation_width_factor * filter_x;
                  const int in_y =
                      in_y_origin + dilation_height_factor * filter_y;
                  // If the location is outside the bounds of the input image,
                  // use zero as a default value.
                  if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
   e2bd0:	f1b8 0f00 	cmp.w	r8, #0
   e2bd4:	db23      	blt.n	e2c1e <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1aa>
   e2bd6:	9b1c      	ldr	r3, [sp, #112]	; 0x70
   e2bd8:	4543      	cmp	r3, r8
   e2bda:	dd20      	ble.n	e2c1e <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1aa>
   e2bdc:	2f00      	cmp	r7, #0
   e2bde:	db1e      	blt.n	e2c1e <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1aa>
   e2be0:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
   e2be2:	42bb      	cmp	r3, r7
   e2be4:	dd1b      	ble.n	e2c1e <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1aa>
                      (in_y < input_height)) {
                    int32 input_val =
                        input_data[Offset(input_shape, b, in_y, in_x, ic)];
   e2be6:	9b05      	ldr	r3, [sp, #20]
   e2be8:	9300      	str	r3, [sp, #0]
   e2bea:	463a      	mov	r2, r7
   e2bec:	4643      	mov	r3, r8
   e2bee:	4659      	mov	r1, fp
   e2bf0:	4648      	mov	r0, r9
   e2bf2:	f7f3 fc9e 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                    int32 filter_val = filter_data[Offset(
   e2bf6:	9b0e      	ldr	r3, [sp, #56]	; 0x38
                  // If the location is outside the bounds of the input image,
                  // use zero as a default value.
                  if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                      (in_y < input_height)) {
                    int32 input_val =
                        input_data[Offset(input_shape, b, in_y, in_x, ic)];
   e2bf8:	9021      	str	r0, [sp, #132]	; 0x84
                    int32 filter_val = filter_data[Offset(
   e2bfa:	9300      	str	r3, [sp, #0]
   e2bfc:	9a06      	ldr	r2, [sp, #24]
   e2bfe:	9b07      	ldr	r3, [sp, #28]
   e2c00:	2100      	movs	r1, #0
   e2c02:	4650      	mov	r0, sl
   e2c04:	f7f3 fc95 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                        filter_shape, 0, filter_y, filter_x, oc)];
                    acc += (filter_val + filter_offset) *
   e2c08:	9b2e      	ldr	r3, [sp, #184]	; 0xb8
   e2c0a:	9a16      	ldr	r2, [sp, #88]	; 0x58
   e2c0c:	5c1b      	ldrb	r3, [r3, r0]
   e2c0e:	9921      	ldr	r1, [sp, #132]	; 0x84
   e2c10:	4413      	add	r3, r2
   e2c12:	9a23      	ldr	r2, [sp, #140]	; 0x8c
   e2c14:	5c52      	ldrb	r2, [r2, r1]
   e2c16:	9915      	ldr	r1, [sp, #84]	; 0x54
   e2c18:	440a      	add	r2, r1
   e2c1a:	fb02 4403 	mla	r4, r2, r3, r4
              const int oc = m + ic * depth_multiplier;
              const int in_x_origin = (out_x * stride_width) - pad_width;
              const int in_y_origin = (out_y * stride_height) - pad_height;
              int32 acc = 0;
              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
                for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   e2c1e:	9b07      	ldr	r3, [sp, #28]
   e2c20:	3301      	adds	r3, #1
   e2c22:	9307      	str	r3, [sp, #28]
   e2c24:	9b11      	ldr	r3, [sp, #68]	; 0x44
   e2c26:	4498      	add	r8, r3
   e2c28:	e7ce      	b.n	e2bc8 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x154>
            for (int m = 0; m < depth_multiplier; m++) {
              const int oc = m + ic * depth_multiplier;
              const int in_x_origin = (out_x * stride_width) - pad_width;
              const int in_y_origin = (out_y * stride_height) - pad_height;
              int32 acc = 0;
              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   e2c2a:	9b06      	ldr	r3, [sp, #24]
   e2c2c:	3301      	adds	r3, #1
   e2c2e:	9306      	str	r3, [sp, #24]
   e2c30:	9b12      	ldr	r3, [sp, #72]	; 0x48
   e2c32:	441f      	add	r7, r3
   e2c34:	e7c0      	b.n	e2bb8 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x144>
                    acc += (filter_val + filter_offset) *
                           (input_val + input_offset);
                  }
                }
              }
              if (bias_data) {
   e2c36:	9b30      	ldr	r3, [sp, #192]	; 0xc0
   e2c38:	b11b      	cbz	r3, e2c42 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x1ce>
                acc += bias_data[oc];
   e2c3a:	9b22      	ldr	r3, [sp, #136]	; 0x88
   e2c3c:	f853 3026 	ldr.w	r3, [r3, r6, lsl #2]
   e2c40:	441c      	add	r4, r3
}

template <>
inline int32 DepthwiseConvRound<DepthwiseConvOutputRounding::kAwayFromZero>(
    int32 x, int32 quantized_multiplier, int shift) {
  return MultiplyByQuantizedMultiplier(x, quantized_multiplier, shift);
   e2c42:	9a19      	ldr	r2, [sp, #100]	; 0x64
   e2c44:	9918      	ldr	r1, [sp, #96]	; 0x60
   e2c46:	4620      	mov	r0, r4
   e2c48:	f7f8 ffce 	bl	dbbe8 <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
              if (bias_data) {
                acc += bias_data[oc];
              }
              acc = DepthwiseConvRound<output_rounding>(acc, output_multiplier,
                                                        output_shift);
              acc += output_offset;
   e2c4c:	9b17      	ldr	r3, [sp, #92]	; 0x5c
              acc = std::max(acc, output_activation_min);
              acc = std::min(acc, output_activation_max);
              output_data[Offset(output_shape, b, out_y, out_x, oc)] =
   e2c4e:	9a03      	ldr	r2, [sp, #12]
              if (bias_data) {
                acc += bias_data[oc];
              }
              acc = DepthwiseConvRound<output_rounding>(acc, output_multiplier,
                                                        output_shift);
              acc += output_offset;
   e2c50:	4418      	add	r0, r3
   e2c52:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e2c54:	4283      	cmp	r3, r0
   e2c56:	bfb8      	it	lt
   e2c58:	4603      	movlt	r3, r0
   e2c5a:	461c      	mov	r4, r3
   e2c5c:	9b0c      	ldr	r3, [sp, #48]	; 0x30
              acc = std::max(acc, output_activation_min);
              acc = std::min(acc, output_activation_max);
              output_data[Offset(output_shape, b, out_y, out_x, oc)] =
   e2c5e:	9831      	ldr	r0, [sp, #196]	; 0xc4
   e2c60:	429c      	cmp	r4, r3
   e2c62:	bfa8      	it	ge
   e2c64:	461c      	movge	r4, r3
   e2c66:	9b0e      	ldr	r3, [sp, #56]	; 0x38
   e2c68:	9300      	str	r3, [sp, #0]
   e2c6a:	4659      	mov	r1, fp
   e2c6c:	9b04      	ldr	r3, [sp, #16]
   e2c6e:	f7f3 fc60 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   e2c72:	9b32      	ldr	r3, [sp, #200]	; 0xc8

    for (int b = 0; b < batches; ++b) {
      for (int out_y = 0; out_y < output_height; ++out_y) {
        for (int out_x = 0; out_x < output_width; ++out_x) {
          for (int ic = 0; ic < input_depth; ++ic) {
            for (int m = 0; m < depth_multiplier; m++) {
   e2c74:	3601      	adds	r6, #1
              acc = DepthwiseConvRound<output_rounding>(acc, output_multiplier,
                                                        output_shift);
              acc += output_offset;
              acc = std::max(acc, output_activation_min);
              acc = std::min(acc, output_activation_max);
              output_data[Offset(output_shape, b, out_y, out_x, oc)] =
   e2c76:	541c      	strb	r4, [r3, r0]

    for (int b = 0; b < batches; ++b) {
      for (int out_y = 0; out_y < output_height; ++out_y) {
        for (int out_x = 0; out_x < output_width; ++out_x) {
          for (int ic = 0; ic < input_depth; ++ic) {
            for (int m = 0; m < depth_multiplier; m++) {
   e2c78:	e796      	b.n	e2ba8 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x134>
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

    for (int b = 0; b < batches; ++b) {
      for (int out_y = 0; out_y < output_height; ++out_y) {
        for (int out_x = 0; out_x < output_width; ++out_x) {
          for (int ic = 0; ic < input_depth; ++ic) {
   e2c7a:	9b05      	ldr	r3, [sp, #20]
   e2c7c:	3301      	adds	r3, #1
   e2c7e:	9305      	str	r3, [sp, #20]
   e2c80:	9b08      	ldr	r3, [sp, #32]
   e2c82:	441d      	add	r5, r3
   e2c84:	e787      	b.n	e2b96 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x122>
    TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

    for (int b = 0; b < batches; ++b) {
      for (int out_y = 0; out_y < output_height; ++out_y) {
        for (int out_x = 0; out_x < output_width; ++out_x) {
   e2c86:	9b04      	ldr	r3, [sp, #16]
   e2c88:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
   e2c8a:	3301      	adds	r3, #1
   e2c8c:	9304      	str	r3, [sp, #16]
   e2c8e:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e2c90:	4413      	add	r3, r2
   e2c92:	930a      	str	r3, [sp, #40]	; 0x28
   e2c94:	e778      	b.n	e2b88 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x114>
    const int output_width = output_shape.Dims(2);
    TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

    for (int b = 0; b < batches; ++b) {
      for (int out_y = 0; out_y < output_height; ++out_y) {
   e2c96:	9b03      	ldr	r3, [sp, #12]
   e2c98:	9a10      	ldr	r2, [sp, #64]	; 0x40
   e2c9a:	3301      	adds	r3, #1
   e2c9c:	9303      	str	r3, [sp, #12]
   e2c9e:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e2ca0:	4413      	add	r3, r2
   e2ca2:	9309      	str	r3, [sp, #36]	; 0x24
   e2ca4:	e766      	b.n	e2b74 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0x100>
    const int output_height = output_shape.Dims(1);
    const int output_width = output_shape.Dims(2);
    TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);

    for (int b = 0; b < batches; ++b) {
   e2ca6:	f10b 0b01 	add.w	fp, fp, #1
   e2caa:	e75a      	b.n	e2b62 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph+0xee>
            }
          }
        }
      }
    }
  }
   e2cac:	b025      	add	sp, #148	; 0x94
   e2cae:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
	...

000e2cb4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode>:
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
        GetTensorShape(output), GetTensorData<uint8_t>(output));
  }
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e2cb4:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e2cb8:	f5ad 7d69 	sub.w	sp, sp, #932	; 0x3a4
   e2cbc:	684b      	ldr	r3, [r1, #4]
   e2cbe:	f8d0 a008 	ldr.w	sl, [r0, #8]
   e2cc2:	930c      	str	r3, [sp, #48]	; 0x30
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   e2cc4:	685b      	ldr	r3, [r3, #4]
   e2cc6:	f8d1 b000 	ldr.w	fp, [r1]
  auto* params =
      reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);
   e2cca:	694d      	ldr	r5, [r1, #20]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e2ccc:	f8db 7008 	ldr.w	r7, [fp, #8]
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
        GetTensorShape(output), GetTensorData<uint8_t>(output));
  }
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e2cd0:	910a      	str	r1, [sp, #40]	; 0x28
  return (tensor->is_variable) ? tensor : nullptr;
}
inline TfLiteTensor* GetOutput(TfLiteContext* context, TfLiteNode* node,
                               int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];
   e2cd2:	2238      	movs	r2, #56	; 0x38
   e2cd4:	fb02 a303 	mla	r3, r2, r3, sl
   e2cd8:	9309      	str	r3, [sp, #36]	; 0x24
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e2cda:	f8db 3004 	ldr.w	r3, [fp, #4]
   e2cde:	4353      	muls	r3, r2
   e2ce0:	930b      	str	r3, [sp, #44]	; 0x2c
   e2ce2:	eb0a 0903 	add.w	r9, sl, r3
}
inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,
                                            TfLiteNode* node, int index) {
  return &context->tensors[node->intermediates->data[index]];
}
inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }
   e2ce6:	f8db 3000 	ldr.w	r3, [fp]

  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
  const TfLiteTensor* bias =
      (NumInputs(node) == 3) ? GetInput(context, node, kBiasTensor) : nullptr;
   e2cea:	2b03      	cmp	r3, #3
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e2cec:	bf08      	it	eq
   e2cee:	f8db 400c 	ldreq.w	r4, [fp, #12]
   e2cf2:	fb02 a707 	mla	r7, r2, r7, sl
   e2cf6:	bf08      	it	eq
   e2cf8:	fb02 a404 	mlaeq	r4, r2, r4, sl

  const TfLiteType data_type = input->type;
   e2cfc:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   e2cfe:	f81a 2002 	ldrb.w	r2, [sl, r2]
   e2d02:	920d      	str	r2, [sp, #52]	; 0x34
   e2d04:	f8d9 2008 	ldr.w	r2, [r9, #8]

namespace tflite {

inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }
inline int SizeOfDimension(const TfLiteTensor* t, int dim) {
  return t->dims->data[dim];
   e2d08:	68d1      	ldr	r1, [r2, #12]
   e2d0a:	6892      	ldr	r2, [r2, #8]
   e2d0c:	9213      	str	r2, [sp, #76]	; 0x4c
   e2d0e:	68ba      	ldr	r2, [r7, #8]
   e2d10:	9114      	str	r1, [sp, #80]	; 0x50
   e2d12:	68d1      	ldr	r1, [r2, #12]
   e2d14:	6892      	ldr	r2, [r2, #8]
   e2d16:	9211      	str	r2, [sp, #68]	; 0x44
  int width = SizeOfDimension(input, 2);
  int height = SizeOfDimension(input, 1);
  int filter_width = SizeOfDimension(filter, 2);
  int filter_height = SizeOfDimension(filter, 1);
  int out_width = ComputeOutSize(params->padding, width, filter_width,
   e2d18:	782a      	ldrb	r2, [r5, #0]
   e2d1a:	920e      	str	r2, [sp, #56]	; 0x38
   e2d1c:	686a      	ldr	r2, [r5, #4]
   e2d1e:	920f      	str	r2, [sp, #60]	; 0x3c
                                 params->stride_width);
  int out_height = ComputeOutSize(params->padding, height, filter_height,
   e2d20:	68aa      	ldr	r2, [r5, #8]
   e2d22:	9112      	str	r1, [sp, #72]	; 0x48
   e2d24:	9210      	str	r2, [sp, #64]	; 0x40
                                  params->stride_height);
  OpData data;
  if (input->type != kTfLiteFloat32) {
   e2d26:	9a0d      	ldr	r2, [sp, #52]	; 0x34

  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
  const TfLiteTensor* input = GetInput(context, node, kInputTensor);
  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
  const TfLiteTensor* bias =
      (NumInputs(node) == 3) ? GetInput(context, node, kBiasTensor) : nullptr;
   e2d28:	bf18      	it	ne
   e2d2a:	2400      	movne	r4, #0
  int out_width = ComputeOutSize(params->padding, width, filter_width,
                                 params->stride_width);
  int out_height = ComputeOutSize(params->padding, height, filter_height,
                                  params->stride_height);
  OpData data;
  if (input->type != kTfLiteFloat32) {
   e2d2c:	2a01      	cmp	r2, #1
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
        GetTensorShape(output), GetTensorData<uint8_t>(output));
  }
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   e2d2e:	4606      	mov	r6, r0
  int out_width = ComputeOutSize(params->padding, width, filter_width,
                                 params->stride_width);
  int out_height = ComputeOutSize(params->padding, height, filter_height,
                                  params->stride_height);
  OpData data;
  if (input->type != kTfLiteFloat32) {
   e2d30:	d02b      	beq.n	e2d8a <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0xd6>
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
   e2d32:	f897 8030 	ldrb.w	r8, [r7, #48]	; 0x30
   e2d36:	f1b8 0f01 	cmp.w	r8, #1
   e2d3a:	d010      	beq.n	e2d5e <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0xaa>
   e2d3c:	4baa      	ldr	r3, [pc, #680]	; (e2fe8 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x334>)
   e2d3e:	9301      	str	r3, [sp, #4]
   e2d40:	2401      	movs	r4, #1
   e2d42:	4baa      	ldr	r3, [pc, #680]	; (e2fec <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x338>)
   e2d44:	9300      	str	r3, [sp, #0]
   e2d46:	9403      	str	r4, [sp, #12]
   e2d48:	f8cd 8008 	str.w	r8, [sp, #8]
   e2d4c:	6945      	ldr	r5, [r0, #20]
   e2d4e:	4aa8      	ldr	r2, [pc, #672]	; (e2ff0 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x33c>)
   e2d50:	49a8      	ldr	r1, [pc, #672]	; (e2ff4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x340>)
   e2d52:	f240 13cd 	movw	r3, #461	; 0x1cd
   e2d56:	47a8      	blx	r5
   e2d58:	4620      	mov	r0, r4
   e2d5a:	f000 bc8e 	b.w	e367a <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x9c6>
                      kTfLiteAffineQuantization);

    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
   e2d5e:	6b7a      	ldr	r2, [r7, #52]	; 0x34
    TF_LITE_ENSURE(context, affine_quantization);
   e2d60:	b92a      	cbnz	r2, e2d6e <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0xba>
   e2d62:	4ba5      	ldr	r3, [pc, #660]	; (e2ff8 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x344>)
   e2d64:	9300      	str	r3, [sp, #0]
   e2d66:	6944      	ldr	r4, [r0, #20]
   e2d68:	f44f 73e9 	mov.w	r3, #466	; 0x1d2
   e2d6c:	e006      	b.n	e2d7c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0xc8>
    TF_LITE_ENSURE(context, affine_quantization->scale);
   e2d6e:	6812      	ldr	r2, [r2, #0]
   e2d70:	b95a      	cbnz	r2, e2d8a <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0xd6>
   e2d72:	4ba2      	ldr	r3, [pc, #648]	; (e2ffc <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x348>)
   e2d74:	9300      	str	r3, [sp, #0]
   e2d76:	6944      	ldr	r4, [r0, #20]
   e2d78:	f240 13d3 	movw	r3, #467	; 0x1d3
   e2d7c:	4630      	mov	r0, r6
   e2d7e:	4a9c      	ldr	r2, [pc, #624]	; (e2ff0 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x33c>)
   e2d80:	499f      	ldr	r1, [pc, #636]	; (e3000 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x34c>)
   e2d82:	47a0      	blx	r4
   e2d84:	4640      	mov	r0, r8
   e2d86:	f000 bc78 	b.w	e367a <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x9c6>
                             int height, int filter_width, int filter_height,
                             int out_width, int out_height,
                             const TfLiteType data_type, OpData* data) {
  bool has_bias = node->inputs->size == 3;
  // Check number of inputs/outputs
  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
   e2d8a:	3b02      	subs	r3, #2
   e2d8c:	2b01      	cmp	r3, #1
   e2d8e:	d908      	bls.n	e2da2 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0xee>
   e2d90:	4b9c      	ldr	r3, [pc, #624]	; (e3004 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x350>)
   e2d92:	9300      	str	r3, [sp, #0]
   e2d94:	6974      	ldr	r4, [r6, #20]
   e2d96:	4a96      	ldr	r2, [pc, #600]	; (e2ff0 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x33c>)
   e2d98:	4999      	ldr	r1, [pc, #612]	; (e3000 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x34c>)
   e2d9a:	2343      	movs	r3, #67	; 0x43
   e2d9c:	4630      	mov	r0, r6
   e2d9e:	47a0      	blx	r4
   e2da0:	e2b8      	b.n	e3314 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x660>
  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);
   e2da2:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   e2da4:	681b      	ldr	r3, [r3, #0]
   e2da6:	2b01      	cmp	r3, #1
   e2da8:	d00d      	beq.n	e2dc6 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x112>
   e2daa:	9302      	str	r3, [sp, #8]
   e2dac:	4b96      	ldr	r3, [pc, #600]	; (e3008 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x354>)
   e2dae:	9301      	str	r3, [sp, #4]
   e2db0:	2201      	movs	r2, #1
   e2db2:	4b96      	ldr	r3, [pc, #600]	; (e300c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x358>)
   e2db4:	9203      	str	r2, [sp, #12]
   e2db6:	9300      	str	r3, [sp, #0]
   e2db8:	6974      	ldr	r4, [r6, #20]
   e2dba:	4a8d      	ldr	r2, [pc, #564]	; (e2ff0 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x33c>)
   e2dbc:	498d      	ldr	r1, [pc, #564]	; (e2ff4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x340>)
   e2dbe:	2344      	movs	r3, #68	; 0x44
   e2dc0:	4630      	mov	r0, r6
   e2dc2:	47a0      	blx	r4
   e2dc4:	e2a6      	b.n	e3314 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x660>
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
   e2dc6:	696b      	ldr	r3, [r5, #20]
   e2dc8:	f8d5 8018 	ldr.w	r8, [r5, #24]
   e2dcc:	9315      	str	r3, [sp, #84]	; 0x54

inline TfLitePaddingValues ComputePaddingHeightWidth(
    int stride_height, int stride_width, int dilation_rate_height,
    int dilation_rate_width, int in_height, int in_width, int filter_height,
    int filter_width, TfLitePadding padding, int* out_height, int* out_width) {
  *out_width = ComputeOutSize(padding, in_width, filter_width, stride_width,
   e2dce:	9300      	str	r3, [sp, #0]
   e2dd0:	9a12      	ldr	r2, [sp, #72]	; 0x48
   e2dd2:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   e2dd4:	9914      	ldr	r1, [sp, #80]	; 0x50
   e2dd6:	980e      	ldr	r0, [sp, #56]	; 0x38
   e2dd8:	f7f9 fa77 	bl	dc2ca <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>
                              dilation_rate_width);
  *out_height = ComputeOutSize(padding, in_height, filter_height, stride_height,
   e2ddc:	f8cd 8000 	str.w	r8, [sp]

inline TfLitePaddingValues ComputePaddingHeightWidth(
    int stride_height, int stride_width, int dilation_rate_height,
    int dilation_rate_width, int in_height, int in_width, int filter_height,
    int filter_width, TfLitePadding padding, int* out_height, int* out_width) {
  *out_width = ComputeOutSize(padding, in_width, filter_width, stride_width,
   e2de0:	9016      	str	r0, [sp, #88]	; 0x58
                              dilation_rate_width);
  *out_height = ComputeOutSize(padding, in_height, filter_height, stride_height,
   e2de2:	9b10      	ldr	r3, [sp, #64]	; 0x40
   e2de4:	9a11      	ldr	r2, [sp, #68]	; 0x44
   e2de6:	9913      	ldr	r1, [sp, #76]	; 0x4c
   e2de8:	980e      	ldr	r0, [sp, #56]	; 0x38
   e2dea:	f7f9 fa6e 	bl	dc2ca <_ZN6tflite14ComputeOutSizeE13TfLitePaddingiiii>
inline int ComputePaddingWithOffset(int stride, int dilation_rate, int in_size,
                                    int filter_size, int out_size,
                                    int* offset) {
  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;
  int total_padding =
      ((out_size - 1) * stride + effective_filter_size - in_size);
   e2dee:	9b11      	ldr	r3, [sp, #68]	; 0x44
   e2df0:	9a15      	ldr	r2, [sp, #84]	; 0x54
   e2df2:	990f      	ldr	r1, [sp, #60]	; 0x3c
   e2df4:	3b01      	subs	r3, #1
   e2df6:	fb08 f803 	mul.w	r8, r8, r3
   e2dfa:	9b10      	ldr	r3, [sp, #64]	; 0x40
   e2dfc:	f108 0801 	add.w	r8, r8, #1
   e2e00:	3801      	subs	r0, #1
   e2e02:	fb03 8000 	mla	r0, r3, r0, r8
   e2e06:	9b13      	ldr	r3, [sp, #76]	; 0x4c
   e2e08:	1ac0      	subs	r0, r0, r3
   e2e0a:	9b12      	ldr	r3, [sp, #72]	; 0x48
   e2e0c:	3b01      	subs	r3, #1
   e2e0e:	4353      	muls	r3, r2
   e2e10:	9a16      	ldr	r2, [sp, #88]	; 0x58
   e2e12:	3301      	adds	r3, #1
   e2e14:	3a01      	subs	r2, #1
   e2e16:	fb01 3302 	mla	r3, r1, r2, r3
   e2e1a:	9a14      	ldr	r2, [sp, #80]	; 0x50
   e2e1c:	1a9b      	subs	r3, r3, r2
  total_padding = total_padding > 0 ? total_padding : 0;
   e2e1e:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
   e2e22:	105a      	asrs	r2, r3, #1
   e2e24:	f003 0301 	and.w	r3, r3, #1
   e2e28:	9362      	str	r3, [sp, #392]	; 0x188

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training.
  if (data_type != kTfLiteFloat32) {
   e2e2a:	9b0d      	ldr	r3, [sp, #52]	; 0x34
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
   e2e2c:	9260      	str	r2, [sp, #384]	; 0x180
   e2e2e:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
   e2e32:	1042      	asrs	r2, r0, #1

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training.
  if (data_type != kTfLiteFloat32) {
   e2e34:	2b01      	cmp	r3, #1
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
   e2e36:	f000 0001 	and.w	r0, r0, #1
   e2e3a:	9261      	str	r2, [sp, #388]	; 0x184
   e2e3c:	9063      	str	r0, [sp, #396]	; 0x18c

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training.
  if (data_type != kTfLiteFloat32) {
   e2e3e:	d02d      	beq.n	e2e9c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x1e8>
}

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
   e2e40:	f8db 000c 	ldr.w	r0, [fp, #12]
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e2e44:	f8db 1004 	ldr.w	r1, [fp, #4]
   e2e48:	f8db 2008 	ldr.w	r2, [fp, #8]
   e2e4c:	2338      	movs	r3, #56	; 0x38

inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
   e2e4e:	f1b0 3fff 	cmp.w	r0, #4294967295	; 0xffffffff
  return t->dims->data[dim];
}
inline const TfLiteTensor* GetInput(TfLiteContext* context, TfLiteNode* node,
                                    int index) {
  return &context
              ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e2e52:	fb03 a101 	mla	r1, r3, r1, sl
   e2e56:	fb03 a202 	mla	r2, r3, r2, sl
                                                  const TfLiteNode* node,
                                                  int index) {
  const bool use_tensor = node->inputs->data[index] != kOptionalTensor;
  if (use_tensor) {
    return &context
                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];
   e2e5a:	bf18      	it	ne
   e2e5c:	fb03 a300 	mlane	r3, r3, r0, sl
    const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);
    const TfLiteTensor* bias =
        GetOptionalInputTensor(context, node, kBiasTensor);
    TfLiteTensor* output = GetOutput(context, node, kOutputTensor);

    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
   e2e60:	a8a6      	add	r0, sp, #664	; 0x298
   e2e62:	9007      	str	r0, [sp, #28]
   e2e64:	a866      	add	r0, sp, #408	; 0x198
   e2e66:	9006      	str	r0, [sp, #24]
   e2e68:	a8e7      	add	r0, sp, #924	; 0x39c
   e2e6a:	9005      	str	r0, [sp, #20]
   e2e6c:	a8e6      	add	r0, sp, #920	; 0x398
   e2e6e:	9004      	str	r0, [sp, #16]
   e2e70:	a865      	add	r0, sp, #404	; 0x194
   e2e72:	9003      	str	r0, [sp, #12]
   e2e74:	a864      	add	r0, sp, #400	; 0x190
   e2e76:	9002      	str	r0, [sp, #8]
   e2e78:	f105 0010 	add.w	r0, r5, #16
   e2e7c:	9001      	str	r0, [sp, #4]
   e2e7e:	980c      	ldr	r0, [sp, #48]	; 0x30
   e2e80:	6840      	ldr	r0, [r0, #4]
   e2e82:	f04f 0e38 	mov.w	lr, #56	; 0x38
   e2e86:	fb0e a000 	mla	r0, lr, r0, sl
  }
  return nullptr;
   e2e8a:	bf08      	it	eq
   e2e8c:	2300      	moveq	r3, #0
   e2e8e:	9000      	str	r0, [sp, #0]
   e2e90:	4630      	mov	r0, r6
   e2e92:	f000 fe7b 	bl	e3b8c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_>
   e2e96:	2800      	cmp	r0, #0
   e2e98:	f040 823c 	bne.w	e3314 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x660>
                                        filter_width, filter_height, out_width,
                                        out_height, data_type, &data));

  // TODO(aselle): Consider whether float conv and quantized conv should be
  // separate ops to avoid dispatch overhead here.
  switch (input->type) {  // Already know in/out types are same.
   e2e9c:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e2e9e:	f81a 8003 	ldrb.w	r8, [sl, r3]
   e2ea2:	f1b8 0f03 	cmp.w	r8, #3
   e2ea6:	f040 80bb 	bne.w	e3020 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x36c>

void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteDepthwiseConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
   e2eaa:	f8d9 3010 	ldr.w	r3, [r9, #16]
  const int32_t output_offset = output->params.zero_point;

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
   e2eae:	9960      	ldr	r1, [sp, #384]	; 0x180
void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteDepthwiseConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
   e2eb0:	693a      	ldr	r2, [r7, #16]

void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteDepthwiseConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
   e2eb2:	f1c3 0a00 	rsb	sl, r3, #0
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;
   e2eb6:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e2eb8:	6918      	ldr	r0, [r3, #16]

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
   e2eba:	f8ad 114a 	strh.w	r1, [sp, #330]	; 0x14a
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
   e2ebe:	2301      	movs	r3, #1
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
   e2ec0:	9961      	ldr	r1, [sp, #388]	; 0x184
  const int32_t filter_offset = -filter->params.zero_point;
  const int32_t output_offset = output->params.zero_point;

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
   e2ec2:	f88d 3148 	strb.w	r3, [sp, #328]	; 0x148
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
   e2ec6:	f8ad 114c 	strh.w	r1, [sp, #332]	; 0x14c
  op_params.stride_width = params->stride_width;
   e2eca:	6869      	ldr	r1, [r5, #4]
   e2ecc:	f8ad 1152 	strh.w	r1, [sp, #338]	; 0x152
  op_params.stride_height = params->stride_height;
   e2ed0:	68a9      	ldr	r1, [r5, #8]
   e2ed2:	f8ad 1154 	strh.w	r1, [sp, #340]	; 0x154
  op_params.dilation_width_factor = 1;
   e2ed6:	f8ad 3156 	strh.w	r3, [sp, #342]	; 0x156
  op_params.dilation_height_factor = 1;
   e2eda:	f8ad 3158 	strh.w	r3, [sp, #344]	; 0x158
void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteDepthwiseConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* output) {
  const int32_t input_offset = -input->params.zero_point;
  const int32_t filter_offset = -filter->params.zero_point;
   e2ede:	4252      	negs	r2, r2
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
  op_params.depth_multiplier = params->depth_multiplier;
   e2ee0:	68e9      	ldr	r1, [r5, #12]
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
   e2ee2:	9258      	str	r2, [sp, #352]	; 0x160
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
   e2ee4:	9a64      	ldr	r2, [sp, #400]	; 0x190
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
  op_params.depth_multiplier = params->depth_multiplier;
   e2ee6:	f8ad 115a 	strh.w	r1, [sp, #346]	; 0x15a
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
   e2eea:	925a      	str	r2, [sp, #360]	; 0x168
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.quantized_activation_min = data->output_activation_min;
   e2eec:	99e6      	ldr	r1, [sp, #920]	; 0x398
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;
   e2eee:	9a65      	ldr	r2, [sp, #404]	; 0x194
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.quantized_activation_min = data->output_activation_min;
   e2ef0:	915c      	str	r1, [sp, #368]	; 0x170
  op_params.quantized_activation_max = data->output_activation_max;
   e2ef2:	99e7      	ldr	r1, [sp, #924]	; 0x39c
   e2ef4:	915d      	str	r1, [sp, #372]	; 0x174
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;
   e2ef6:	4252      	negs	r2, r2
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
   e2ef8:	9059      	str	r0, [sp, #356]	; 0x164
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;

  // Figure out if we can use the optimized path for this set of parameters.
  const int filter_width = GetTensorShape(filter).Dims(2);
   e2efa:	4639      	mov	r1, r7
   e2efc:	a84d      	add	r0, sp, #308	; 0x134
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;
   e2efe:	925b      	str	r2, [sp, #364]	; 0x16c
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
   e2f00:	930b      	str	r3, [sp, #44]	; 0x2c
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  op_params.input_offset = input_offset;
   e2f02:	f8cd a15c 	str.w	sl, [sp, #348]	; 0x15c
  op_params.output_multiplier = data->output_multiplier;
  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.
  op_params.output_shift = -data->output_shift;

  // Figure out if we can use the optimized path for this set of parameters.
  const int filter_width = GetTensorShape(filter).Dims(2);
   e2f06:	f7f3 fd54 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e2f0a:	2102      	movs	r1, #2
   e2f0c:	a84d      	add	r0, sp, #308	; 0x134
   e2f0e:	f7f3 faab 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   e2f12:	4605      	mov	r5, r0
   e2f14:	a84d      	add	r0, sp, #308	; 0x134
   e2f16:	f7f3 fa9c 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  const int input_depth = GetTensorShape(input).Dims(3);
   e2f1a:	4649      	mov	r1, r9
   e2f1c:	a84d      	add	r0, sp, #308	; 0x134
   e2f1e:	f7f3 fd48 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e2f22:	4641      	mov	r1, r8
   e2f24:	a84d      	add	r0, sp, #308	; 0x134
   e2f26:	f7f3 fa9f 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   e2f2a:	4683      	mov	fp, r0
   e2f2c:	a84d      	add	r0, sp, #308	; 0x134
   e2f2e:	f7f3 fa90 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  const int output_depth = GetTensorShape(filter).Dims(3);
   e2f32:	4639      	mov	r1, r7
   e2f34:	a84d      	add	r0, sp, #308	; 0x134
   e2f36:	f7f3 fd3c 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e2f3a:	4641      	mov	r1, r8
   e2f3c:	a84d      	add	r0, sp, #308	; 0x134
   e2f3e:	f7f3 fa93 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   e2f42:	4680      	mov	r8, r0
   e2f44:	a84d      	add	r0, sp, #308	; 0x134
   e2f46:	f7f3 fa84 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  const int filter_height = GetTensorShape(filter).Dims(1);
   e2f4a:	4639      	mov	r1, r7
   e2f4c:	a84d      	add	r0, sp, #308	; 0x134
   e2f4e:	f7f3 fd30 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e2f52:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e2f54:	a84d      	add	r0, sp, #308	; 0x134
   e2f56:	4619      	mov	r1, r3
   e2f58:	f7f3 fa86 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
   e2f5c:	900b      	str	r0, [sp, #44]	; 0x2c
   e2f5e:	a84d      	add	r0, sp, #308	; 0x134
   e2f60:	f7f3 fa77 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  const int needed_size =
      output_depth * filter_width * filter_height * input_depth;
   e2f64:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e2f66:	fb08 f805 	mul.w	r8, r8, r5
   e2f6a:	fb03 f308 	mul.w	r3, r3, r8
  bool use_optimized_path = false;
  if ((filter_width == 8) && (input_offset == 0) && (input_depth == 1) &&
   e2f6e:	2d08      	cmp	r5, #8
  const int filter_width = GetTensorShape(filter).Dims(2);
  const int input_depth = GetTensorShape(input).Dims(3);
  const int output_depth = GetTensorShape(filter).Dims(3);
  const int filter_height = GetTensorShape(filter).Dims(1);
  const int needed_size =
      output_depth * filter_width * filter_height * input_depth;
   e2f70:	fb0b f303 	mul.w	r3, fp, r3
   e2f74:	ad3e      	add	r5, sp, #248	; 0xf8
   e2f76:	f50d 7886 	add.w	r8, sp, #268	; 0x10c
  bool use_optimized_path = false;
  if ((filter_width == 8) && (input_offset == 0) && (input_depth == 1) &&
   e2f7a:	f040 8351 	bne.w	e3620 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x96c>
   e2f7e:	f1ba 0f00 	cmp.w	sl, #0
   e2f82:	f040 834d 	bne.w	e3620 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x96c>
   e2f86:	f1bb 0f01 	cmp.w	fp, #1
   e2f8a:	f040 8349 	bne.w	e3620 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x96c>
   e2f8e:	f5b3 6f80 	cmp.w	r3, #1024	; 0x400
   e2f92:	f300 8345 	bgt.w	e3620 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x96c>
    // with an allocation mechanism available through the context API.
    // Use the address of the node as a proxy for its identity, since we need
    // to ensure the weight values are consistent between calls, and there's
    // no easy way to do that quickly other than relying on the identity of
    // the owning node.
    static TfLiteNode* initialized_node_address = node;
   e2f96:	f8df b07c 	ldr.w	fp, [pc, #124]	; e3014 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x360>
   e2f9a:	f8df a07c 	ldr.w	sl, [pc, #124]	; e3018 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x364>
   e2f9e:	f8db 3000 	ldr.w	r3, [fp]
   e2fa2:	f013 0f01 	tst.w	r3, #1
   e2fa6:	d109      	bne.n	e2fbc <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x308>
   e2fa8:	4658      	mov	r0, fp
   e2faa:	f7f1 f889 	bl	d40c0 <__cxa_guard_acquire>
   e2fae:	b128      	cbz	r0, e2fbc <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x308>
   e2fb0:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e2fb2:	f8ca 3000 	str.w	r3, [sl]
   e2fb6:	4658      	mov	r0, fp
   e2fb8:	f7f1 f887 	bl	d40ca <__cxa_guard_release>
    if (initialized_node_address == node) {
   e2fbc:	f8da 3000 	ldr.w	r3, [sl]
   e2fc0:	9a0a      	ldr	r2, [sp, #40]	; 0x28
   e2fc2:	429a      	cmp	r2, r3
   e2fc4:	f000 808b 	beq.w	e30de <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x42a>
      use_optimized_path = true;
    } else {
      static bool has_warned = false;
      if (!has_warned) {
   e2fc8:	f8df a050 	ldr.w	sl, [pc, #80]	; e301c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x368>
   e2fcc:	f89a 3000 	ldrb.w	r3, [sl]
   e2fd0:	2b00      	cmp	r3, #0
   e2fd2:	f040 8325 	bne.w	e3620 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x96c>
        context->ReportError(
            context,
            "Multiple depthwise conv ops match optimization parameters, but "
            "only the first will use the fast path, because there's only one "
            "RAM cache available");
   e2fd6:	6973      	ldr	r3, [r6, #20]
   e2fd8:	490d      	ldr	r1, [pc, #52]	; (e3010 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x35c>)
   e2fda:	4630      	mov	r0, r6
   e2fdc:	4798      	blx	r3
        has_warned = true;
   e2fde:	2301      	movs	r3, #1
   e2fe0:	f88a 3000 	strb.w	r3, [sl]
   e2fe4:	e31c      	b.n	e3620 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x96c>
   e2fe6:	bf00      	nop
   e2fe8:	000e9ae7 	.word	0x000e9ae7
   e2fec:	000e9b01 	.word	0x000e9b01
   e2ff0:	000eaea7 	.word	0x000eaea7
   e2ff4:	000e98c8 	.word	0x000e98c8
   e2ff8:	000e9b1b 	.word	0x000e9b1b
   e2ffc:	000e9b2f 	.word	0x000e9b2f
   e3000:	000e9a98 	.word	0x000e9a98
   e3004:	000e9aaf 	.word	0x000e9aaf
   e3008:	000eb295 	.word	0x000eb295
   e300c:	000e9ad3 	.word	0x000e9ad3
   e3010:	000eaf6a 	.word	0x000eaf6a
   e3014:	2003e4cc 	.word	0x2003e4cc
   e3018:	2003e4c4 	.word	0x2003e4c4
   e301c:	2003e4c8 	.word	0x2003e4c8
                                        filter_width, filter_height, out_width,
                                        out_height, data_type, &data));

  // TODO(aselle): Consider whether float conv and quantized conv should be
  // separate ops to avoid dispatch overhead here.
  switch (input->type) {  // Already know in/out types are same.
   e3020:	f1b8 0f09 	cmp.w	r8, #9
   e3024:	f040 8101 	bne.w	e322a <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x576>
                             TfLiteDepthwiseConvParams* params, OpData* data,
                             const TfLiteTensor* input,
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output) {
  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
   e3028:	2301      	movs	r3, #1
   e302a:	f88d 3148 	strb.w	r3, [sp, #328]	; 0x148
  op_params.padding_values.width = data->padding.width;
   e302e:	9b60      	ldr	r3, [sp, #384]	; 0x180
   e3030:	f8ad 314a 	strh.w	r3, [sp, #330]	; 0x14a
  op_params.padding_values.height = data->padding.height;
   e3034:	9b61      	ldr	r3, [sp, #388]	; 0x184
   e3036:	f8ad 314c 	strh.w	r3, [sp, #332]	; 0x14c
  op_params.stride_width = params->stride_width;
   e303a:	686b      	ldr	r3, [r5, #4]
   e303c:	f8ad 3152 	strh.w	r3, [sp, #338]	; 0x152
  op_params.stride_height = params->stride_height;
   e3040:	68ab      	ldr	r3, [r5, #8]
   e3042:	f8ad 3154 	strh.w	r3, [sp, #340]	; 0x154
  op_params.dilation_width_factor = params->dilation_width_factor;
   e3046:	696b      	ldr	r3, [r5, #20]
   e3048:	f8ad 3156 	strh.w	r3, [sp, #342]	; 0x156
  op_params.dilation_height_factor = params->dilation_height_factor;
   e304c:	69ab      	ldr	r3, [r5, #24]
   e304e:	f8ad 3158 	strh.w	r3, [sp, #344]	; 0x158
  op_params.depth_multiplier = params->depth_multiplier;
   e3052:	68eb      	ldr	r3, [r5, #12]
   e3054:	f8ad 315a 	strh.w	r3, [sp, #346]	; 0x15a
  op_params.input_offset = -input->params.zero_point;
   e3058:	f8d9 3010 	ldr.w	r3, [r9, #16]
   e305c:	425b      	negs	r3, r3
   e305e:	9357      	str	r3, [sp, #348]	; 0x15c
  op_params.weights_offset = 0;
   e3060:	2300      	movs	r3, #0
   e3062:	9358      	str	r3, [sp, #352]	; 0x160
  op_params.output_offset = output->params.zero_point;
   e3064:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e3066:	691b      	ldr	r3, [r3, #16]
   e3068:	9359      	str	r3, [sp, #356]	; 0x164
  // TODO(b/130439627): Use calculated value for clamping.
  op_params.quantized_activation_min = std::numeric_limits<int8_t>::min();
   e306a:	f06f 037f 	mvn.w	r3, #127	; 0x7f
  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
   e306e:	4649      	mov	r1, r9
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.input_offset = -input->params.zero_point;
  op_params.weights_offset = 0;
  op_params.output_offset = output->params.zero_point;
  // TODO(b/130439627): Use calculated value for clamping.
  op_params.quantized_activation_min = std::numeric_limits<int8_t>::min();
   e3070:	935c      	str	r3, [sp, #368]	; 0x170
  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
   e3072:	a83e      	add	r0, sp, #248	; 0xf8
  op_params.input_offset = -input->params.zero_point;
  op_params.weights_offset = 0;
  op_params.output_offset = output->params.zero_point;
  // TODO(b/130439627): Use calculated value for clamping.
  op_params.quantized_activation_min = std::numeric_limits<int8_t>::min();
  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();
   e3074:	237f      	movs	r3, #127	; 0x7f

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
   e3076:	ae43      	add	r6, sp, #268	; 0x10c
  op_params.input_offset = -input->params.zero_point;
  op_params.weights_offset = 0;
  op_params.output_offset = output->params.zero_point;
  // TODO(b/130439627): Use calculated value for clamping.
  op_params.quantized_activation_min = std::numeric_limits<int8_t>::min();
  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();
   e3078:	935d      	str	r3, [sp, #372]	; 0x174

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
   e307a:	f7f3 fc9a 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorData<int8>(input), GetTensorShape(filter),
   e307e:	4639      	mov	r1, r7
   e3080:	4630      	mov	r0, r6
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e3082:	f8d9 9004 	ldr.w	r9, [r9, #4]
   e3086:	f7f3 fc94 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e308a:	f8d7 a004 	ldr.w	sl, [r7, #4]
      GetTensorData<int8>(filter), GetTensorShape(bias),
   e308e:	af48      	add	r7, sp, #288	; 0x120
   e3090:	4621      	mov	r1, r4
   e3092:	4638      	mov	r0, r7
   e3094:	f7f3 fc8d 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e3098:	b104      	cbz	r4, e309c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x3e8>
   e309a:	6864      	ldr	r4, [r4, #4]
      GetTensorData<int32>(bias), GetTensorShape(output),
   e309c:	9909      	ldr	r1, [sp, #36]	; 0x24
   e309e:	ad4d      	add	r5, sp, #308	; 0x134
   e30a0:	4628      	mov	r0, r5
   e30a2:	f7f3 fc86 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorData<int8>(output));
   e30a6:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e30a8:	685b      	ldr	r3, [r3, #4]
   e30aa:	9306      	str	r3, [sp, #24]
   e30ac:	aaa6      	add	r2, sp, #664	; 0x298
   e30ae:	ab3e      	add	r3, sp, #248	; 0xf8
   e30b0:	a966      	add	r1, sp, #408	; 0x198
   e30b2:	a852      	add	r0, sp, #328	; 0x148
   e30b4:	9505      	str	r5, [sp, #20]
   e30b6:	9404      	str	r4, [sp, #16]
   e30b8:	9703      	str	r7, [sp, #12]
   e30ba:	f8cd a008 	str.w	sl, [sp, #8]
   e30be:	9601      	str	r6, [sp, #4]
   e30c0:	f8cd 9000 	str.w	r9, [sp]
   e30c4:	f7ff fa85 	bl	e25d2 <_ZN6tflite21reference_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKlS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_Pa>
  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
      GetTensorData<int8>(filter), GetTensorShape(bias),
      GetTensorData<int32>(bias), GetTensorShape(output),
   e30c8:	4628      	mov	r0, r5
   e30ca:	f7f3 f9c2 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
      GetTensorData<int8>(filter), GetTensorShape(bias),
   e30ce:	4638      	mov	r0, r7
   e30d0:	f7f3 f9bf 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
      GetTensorData<int8>(input), GetTensorShape(filter),
   e30d4:	4630      	mov	r0, r6
   e30d6:	f7f3 f9bc 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.quantized_activation_min = std::numeric_limits<int8_t>::min();
  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier,
      data->per_channel_output_shift, GetTensorShape(input),
   e30da:	a83e      	add	r0, sp, #248	; 0xf8
   e30dc:	e0a1      	b.n	e3222 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x56e>
      }
    }
  }
  if (use_optimized_path) {
    DepthwiseConvOptimizedForFilterWidthEight(
        context, op_params, GetTensorShape(input),
   e30de:	4649      	mov	r1, r9
   e30e0:	a848      	add	r0, sp, #288	; 0x120
   e30e2:	f7f3 fc66 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e30e6:	f8d9 3004 	ldr.w	r3, [r9, #4]
   e30ea:	931a      	str	r3, [sp, #104]	; 0x68
        GetTensorData<uint8_t>(input), GetTensorShape(filter),
   e30ec:	4639      	mov	r1, r7
   e30ee:	4640      	mov	r0, r8
   e30f0:	f7f3 fc5f 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e30f4:	687b      	ldr	r3, [r7, #4]
   e30f6:	931b      	str	r3, [sp, #108]	; 0x6c
        GetTensorData<uint8_t>(filter), GetTensorShape(bias),
   e30f8:	4621      	mov	r1, r4
   e30fa:	4628      	mov	r0, r5
   e30fc:	f7f3 fc59 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e3100:	2c00      	cmp	r4, #0
   e3102:	f000 8287 	beq.w	e3614 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x960>
   e3106:	6863      	ldr	r3, [r4, #4]
   e3108:	9316      	str	r3, [sp, #88]	; 0x58
        GetTensorData<int32_t>(bias), GetTensorShape(output),
   e310a:	9909      	ldr	r1, [sp, #36]	; 0x24
   e310c:	a839      	add	r0, sp, #228	; 0xe4
   e310e:	f7f3 fc50 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e3112:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e3114:	685b      	ldr	r3, [r3, #4]
   e3116:	9326      	str	r3, [sp, #152]	; 0x98
    TfLiteContext* context, const DepthwiseParams& params,
    const RuntimeShape& input_shape, const uint8* input_data,
    const RuntimeShape& filter_shape, const uint8* filter_data,
    const RuntimeShape& bias_shape, const int32* bias_data,
    const RuntimeShape& output_shape, uint8* output_data) {
  const int stride_width = params.stride_width;
   e3118:	f9bd 3152 	ldrsh.w	r3, [sp, #338]	; 0x152
   e311c:	9319      	str	r3, [sp, #100]	; 0x64
  const int stride_height = params.stride_height;
   e311e:	f9bd 3154 	ldrsh.w	r3, [sp, #340]	; 0x154
   e3122:	931c      	str	r3, [sp, #112]	; 0x70
  const int pad_width = params.padding_values.width;
   e3124:	f9bd 314a 	ldrsh.w	r3, [sp, #330]	; 0x14a
   e3128:	931d      	str	r3, [sp, #116]	; 0x74
  const int pad_height = params.padding_values.height;
   e312a:	f9bd 314c 	ldrsh.w	r3, [sp, #332]	; 0x14c
   e312e:	931e      	str	r3, [sp, #120]	; 0x78
  const int depth_multiplier = params.depth_multiplier;
   e3130:	f9bd 315a 	ldrsh.w	r3, [sp, #346]	; 0x15a
   e3134:	9317      	str	r3, [sp, #92]	; 0x5c
  const int32 output_activation_min = params.quantized_activation_min;
   e3136:	9b5c      	ldr	r3, [sp, #368]	; 0x170
   e3138:	931f      	str	r3, [sp, #124]	; 0x7c
  const int32 output_activation_max = params.quantized_activation_max;
   e313a:	9b5d      	ldr	r3, [sp, #372]	; 0x174
   e313c:	9320      	str	r3, [sp, #128]	; 0x80
  const int32 input_offset = params.input_offset;
   e313e:	9b57      	ldr	r3, [sp, #348]	; 0x15c
   e3140:	9327      	str	r3, [sp, #156]	; 0x9c
  const int32 filter_offset = params.weights_offset;
   e3142:	9b58      	ldr	r3, [sp, #352]	; 0x160
   e3144:	9321      	str	r3, [sp, #132]	; 0x84
  const int32 output_offset = params.output_offset;
   e3146:	9b59      	ldr	r3, [sp, #356]	; 0x164
   e3148:	9328      	str	r3, [sp, #160]	; 0xa0
  const int32 output_multiplier = params.output_multiplier;
   e314a:	9b5a      	ldr	r3, [sp, #360]	; 0x168
   e314c:	9329      	str	r3, [sp, #164]	; 0xa4
  const int output_shift = params.output_shift;
   e314e:	9b5b      	ldr	r3, [sp, #364]	; 0x16c
   e3150:	932a      	str	r3, [sp, #168]	; 0xa8
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e3152:	9b48      	ldr	r3, [sp, #288]	; 0x120
   e3154:	2b04      	cmp	r3, #4
   e3156:	f040 80df 	bne.w	e3318 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x664>
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
   e315a:	9b43      	ldr	r3, [sp, #268]	; 0x10c
   e315c:	2b04      	cmp	r3, #4
   e315e:	f040 80db 	bne.w	e3318 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x664>
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
   e3162:	9c39      	ldr	r4, [sp, #228]	; 0xe4
   e3164:	2c04      	cmp	r4, #4
   e3166:	f040 80d7 	bne.w	e3318 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x664>

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
   e316a:	9b1f      	ldr	r3, [sp, #124]	; 0x7c
   e316c:	9a20      	ldr	r2, [sp, #128]	; 0x80
   e316e:	4293      	cmp	r3, r2
   e3170:	f300 80d2 	bgt.w	e3318 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x664>
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   e3174:	2300      	movs	r3, #0
   e3176:	4619      	mov	r1, r3
   e3178:	aa39      	add	r2, sp, #228	; 0xe4
   e317a:	a848      	add	r0, sp, #288	; 0x120
   e317c:	f7f8 fd25 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e3180:	2303      	movs	r3, #3
   e3182:	4619      	mov	r1, r3
   e3184:	aa39      	add	r2, sp, #228	; 0xe4
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   e3186:	902b      	str	r0, [sp, #172]	; 0xac
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e3188:	a843      	add	r0, sp, #268	; 0x10c
   e318a:	f7f8 fd1e 	bl	dbbca <_ZN6tflite11MatchingDimERKNS_12RuntimeShapeEiS2_i>
  const int input_height = input_shape.Dims(1);
   e318e:	2101      	movs	r1, #1
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
   e3190:	4682      	mov	sl, r0
  const int input_height = input_shape.Dims(1);
   e3192:	a848      	add	r0, sp, #288	; 0x120
   e3194:	f7f3 f968 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_width = input_shape.Dims(2);
   e3198:	2102      	movs	r1, #2
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
   e319a:	9018      	str	r0, [sp, #96]	; 0x60
  const int input_width = input_shape.Dims(2);
   e319c:	a848      	add	r0, sp, #288	; 0x120
   e319e:	f7f3 f963 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int input_depth = input_shape.Dims(3);
   e31a2:	2103      	movs	r1, #3

  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
   e31a4:	900e      	str	r0, [sp, #56]	; 0x38
  const int input_depth = input_shape.Dims(3);
   e31a6:	a848      	add	r0, sp, #288	; 0x120
   e31a8:	f7f3 f95e 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_height = filter_shape.Dims(1);
   e31ac:	2101      	movs	r1, #1
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
   e31ae:	900a      	str	r0, [sp, #40]	; 0x28
  const int filter_height = filter_shape.Dims(1);
   e31b0:	a843      	add	r0, sp, #268	; 0x10c
   e31b2:	f7f3 f959 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int filter_width = filter_shape.Dims(2);
   e31b6:	2102      	movs	r1, #2
  const int batches = MatchingDim(input_shape, 0, output_shape, 0);
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
   e31b8:	9009      	str	r0, [sp, #36]	; 0x24
  const int filter_width = filter_shape.Dims(2);
   e31ba:	a843      	add	r0, sp, #268	; 0x10c
   e31bc:	f7f3 f954 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_height = output_shape.Dims(1);
   e31c0:	2101      	movs	r1, #1
  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
   e31c2:	4683      	mov	fp, r0
  const int output_height = output_shape.Dims(1);
   e31c4:	a839      	add	r0, sp, #228	; 0xe4
   e31c6:	f7f3 f94f 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  const int output_width = output_shape.Dims(2);
   e31ca:	2102      	movs	r1, #2
  const int input_height = input_shape.Dims(1);
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
   e31cc:	902c      	str	r0, [sp, #176]	; 0xb0
  const int output_width = output_shape.Dims(2);
   e31ce:	a839      	add	r0, sp, #228	; 0xe4
   e31d0:	f7f3 f94a 	bl	d6468 <_ZNK6tflite12RuntimeShape4DimsEi>
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
   e31d4:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e31d6:	9a17      	ldr	r2, [sp, #92]	; 0x5c
  const int input_width = input_shape.Dims(2);
  const int input_depth = input_shape.Dims(3);
  const int filter_height = filter_shape.Dims(1);
  const int filter_width = filter_shape.Dims(2);
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);
   e31d8:	902d      	str	r0, [sp, #180]	; 0xb4
  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
   e31da:	4353      	muls	r3, r2
   e31dc:	459a      	cmp	sl, r3
   e31de:	f040 809b 	bne.w	e3318 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x664>
  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
   e31e2:	a83e      	add	r0, sp, #248	; 0xf8
   e31e4:	f7f8 fce1 	bl	dbbaa <_ZNK6tflite12RuntimeShape8FlatSizeEv>
   e31e8:	4582      	cmp	sl, r0
   e31ea:	f040 8095 	bne.w	e3318 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x664>

  static int16_t reshaped_filter_data[kReshapedFilterDataSize];
  const int needed_size =
      output_depth * filter_width * filter_height * input_depth;
   e31ee:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e31f0:	fb0b f20a 	mul.w	r2, fp, sl
   e31f4:	435a      	muls	r2, r3
   e31f6:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e31f8:	435a      	muls	r2, r3
  if (needed_size > kReshapedFilterDataSize) {
   e31fa:	f5b2 6f80 	cmp.w	r2, #1024	; 0x400
   e31fe:	f340 808d 	ble.w	e331c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x668>
    context->ReportError(
        context,
        "Size too large for reshaped weight buffer (%d needed, %d available)",
        needed_size, kReshapedFilterDataSize);
   e3202:	6974      	ldr	r4, [r6, #20]
   e3204:	4962      	ldr	r1, [pc, #392]	; (e3390 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6dc>)
   e3206:	f44f 6380 	mov.w	r3, #1024	; 0x400
   e320a:	4630      	mov	r0, r6
   e320c:	47a0      	blx	r4
  if (use_optimized_path) {
    DepthwiseConvOptimizedForFilterWidthEight(
        context, op_params, GetTensorShape(input),
        GetTensorData<uint8_t>(input), GetTensorShape(filter),
        GetTensorData<uint8_t>(filter), GetTensorShape(bias),
        GetTensorData<int32_t>(bias), GetTensorShape(output),
   e320e:	a839      	add	r0, sp, #228	; 0xe4
   e3210:	f7f3 f91f 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  }
  if (use_optimized_path) {
    DepthwiseConvOptimizedForFilterWidthEight(
        context, op_params, GetTensorShape(input),
        GetTensorData<uint8_t>(input), GetTensorShape(filter),
        GetTensorData<uint8_t>(filter), GetTensorShape(bias),
   e3214:	a83e      	add	r0, sp, #248	; 0xf8
   e3216:	f7f3 f91c 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
    }
  }
  if (use_optimized_path) {
    DepthwiseConvOptimizedForFilterWidthEight(
        context, op_params, GetTensorShape(input),
        GetTensorData<uint8_t>(input), GetTensorShape(filter),
   e321a:	a843      	add	r0, sp, #268	; 0x10c
   e321c:	f7f3 f919 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
      }
    }
  }
  if (use_optimized_path) {
    DepthwiseConvOptimizedForFilterWidthEight(
        context, op_params, GetTensorShape(input),
   e3220:	a848      	add	r0, sp, #288	; 0x120
        GetTensorData<uint8_t>(filter), GetTensorShape(bias),
        GetTensorData<int32_t>(bias), GetTensorShape(output),
        GetTensorData<uint8_t>(output));
  } else {
    tflite::reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   e3222:	f7f3 f916 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
    default:
      context->ReportError(context, "Type %s (%d) not supported.",
                           TfLiteTypeGetName(input->type), input->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
   e3226:	2000      	movs	r0, #0
   e3228:	e227      	b.n	e367a <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x9c6>
                                        filter_width, filter_height, out_width,
                                        out_height, data_type, &data));

  // TODO(aselle): Consider whether float conv and quantized conv should be
  // separate ops to avoid dispatch overhead here.
  switch (input->type) {  // Already know in/out types are same.
   e322a:	f1b8 0f01 	cmp.w	r8, #1
   e322e:	d166      	bne.n	e32fe <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x64a>
void EvalFloat(TfLiteContext* context, TfLiteNode* node,
               TfLiteDepthwiseConvParams* params, OpData* data,
               const TfLiteTensor* input, const TfLiteTensor* filter,
               const TfLiteTensor* bias, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
   e3230:	7c2b      	ldrb	r3, [r5, #16]
// Calculates the useful range of an activation layer given its activation
// tensor.a
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
   e3232:	2b01      	cmp	r3, #1
   e3234:	d011      	beq.n	e325a <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x5a6>
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
   e3236:	2b03      	cmp	r3, #3
   e3238:	d012      	beq.n	e3260 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x5ac>
  } else if (activation == kTfLiteActRelu1) {
    *activation_min = -1;
    *activation_max = 1;
  } else {
    *activation_min = std::numeric_limits<T>::lowest();
    *activation_max = std::numeric_limits<T>::max();
   e323a:	ed9f 7a56 	vldr	s14, [pc, #344]	; e3394 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6e0>
   e323e:	eddf 6a56 	vldr	s13, [pc, #344]	; e3398 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6e4>
   e3242:	2b02      	cmp	r3, #2
   e3244:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   e3248:	bf18      	it	ne
   e324a:	eef0 7a47 	vmovne.f32	s15, s14
   e324e:	eebf 7a00 	vmov.f32	s14, #240	; 0xbf800000 -1.0
   e3252:	bf18      	it	ne
   e3254:	eeb0 7a66 	vmovne.f32	s14, s13
   e3258:	e006      	b.n	e3268 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x5b4>
template <typename T>
void CalculateActivationRange(TfLiteFusedActivation activation,
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
   e325a:	eddf 7a4e 	vldr	s15, [pc, #312]	; e3394 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6e0>
   e325e:	e001      	b.n	e3264 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x5b0>
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
    *activation_max = 6;
   e3260:	eef1 7a08 	vmov.f32	s15, #24	; 0x40c00000  6.0
                              T* activation_min, T* activation_max) {
  if (activation == kTfLiteActRelu) {
    *activation_min = 0;
    *activation_max = std::numeric_limits<T>::max();
  } else if (activation == kTfLiteActRelu6) {
    *activation_min = 0;
   e3264:	ed9f 7a4d 	vldr	s14, [pc, #308]	; e339c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6e8>
                           &output_activation_max);

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
   e3268:	9a60      	ldr	r2, [sp, #384]	; 0x180
   e326a:	f8ad 214a 	strh.w	r2, [sp, #330]	; 0x14a
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
   e326e:	2301      	movs	r3, #1
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
   e3270:	9a61      	ldr	r2, [sp, #388]	; 0x184
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  tflite::DepthwiseParams op_params;
  // Padding type is ignored, but still set.
  op_params.padding_type = PaddingType::kSame;
   e3272:	f88d 3148 	strb.w	r3, [sp, #328]	; 0x148
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
   e3276:	f8ad 214c 	strh.w	r2, [sp, #332]	; 0x14c
  op_params.stride_width = params->stride_width;
   e327a:	686a      	ldr	r2, [r5, #4]
   e327c:	f8ad 2152 	strh.w	r2, [sp, #338]	; 0x152
  op_params.stride_height = params->stride_height;
   e3280:	68aa      	ldr	r2, [r5, #8]
   e3282:	f8ad 2154 	strh.w	r2, [sp, #340]	; 0x154
  op_params.dilation_width_factor = 1;
   e3286:	f8ad 3156 	strh.w	r3, [sp, #342]	; 0x156
  op_params.dilation_height_factor = 1;
   e328a:	f8ad 3158 	strh.w	r3, [sp, #344]	; 0x158
  op_params.depth_multiplier = params->depth_multiplier;
   e328e:	68eb      	ldr	r3, [r5, #12]
   e3290:	f8ad 315a 	strh.w	r3, [sp, #346]	; 0x15a
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  tflite::reference_ops::DepthwiseConv(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
   e3294:	4649      	mov	r1, r9
   e3296:	a83e      	add	r0, sp, #248	; 0xf8
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = 1;
  op_params.dilation_height_factor = 1;
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.float_activation_min = output_activation_min;
   e3298:	ed8d 7a5e 	vstr	s14, [sp, #376]	; 0x178
  op_params.float_activation_max = output_activation_max;
   e329c:	edcd 7a5f 	vstr	s15, [sp, #380]	; 0x17c

  tflite::reference_ops::DepthwiseConv(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
      GetTensorShape(filter), GetTensorData<float>(filter),
      GetTensorShape(bias), GetTensorData<float>(bias), GetTensorShape(output),
   e32a0:	ad48      	add	r5, sp, #288	; 0x120
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  tflite::reference_ops::DepthwiseConv(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
   e32a2:	f7f3 fb86 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(filter), GetTensorData<float>(filter),
   e32a6:	4639      	mov	r1, r7
   e32a8:	a843      	add	r0, sp, #268	; 0x10c
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e32aa:	f8d9 8004 	ldr.w	r8, [r9, #4]
   e32ae:	f7f3 fb80 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
      GetTensorShape(bias), GetTensorData<float>(bias), GetTensorShape(output),
   e32b2:	4621      	mov	r1, r4
   e32b4:	4628      	mov	r0, r5
   e32b6:	f8d7 9004 	ldr.w	r9, [r7, #4]
   e32ba:	f7f3 fb7a 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e32be:	b104      	cbz	r4, e32c2 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x60e>
   e32c0:	6864      	ldr	r4, [r4, #4]
   e32c2:	9909      	ldr	r1, [sp, #36]	; 0x24
   e32c4:	af4d      	add	r7, sp, #308	; 0x134
   e32c6:	4638      	mov	r0, r7
   e32c8:	f7f3 fb73 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>

namespace tflite {

template <typename T>
inline T* GetTensorData(TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<T*>(tensor->data.raw) : nullptr;
   e32cc:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e32ce:	b10b      	cbz	r3, e32d4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x620>
   e32d0:	685e      	ldr	r6, [r3, #4]
   e32d2:	e000      	b.n	e32d6 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x622>
   e32d4:	9e09      	ldr	r6, [sp, #36]	; 0x24
      GetTensorData<float>(output));
   e32d6:	9604      	str	r6, [sp, #16]
   e32d8:	ab43      	add	r3, sp, #268	; 0x10c
   e32da:	4642      	mov	r2, r8
   e32dc:	a93e      	add	r1, sp, #248	; 0xf8
   e32de:	a852      	add	r0, sp, #328	; 0x148
   e32e0:	9703      	str	r7, [sp, #12]
   e32e2:	9402      	str	r4, [sp, #8]
   e32e4:	9501      	str	r5, [sp, #4]
   e32e6:	f8cd 9000 	str.w	r9, [sp]
   e32ea:	f7ff fa99 	bl	e2820 <_ZN6tflite13reference_ops13DepthwiseConvERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_Pf>
  op_params.float_activation_max = output_activation_max;

  tflite::reference_ops::DepthwiseConv(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
      GetTensorShape(filter), GetTensorData<float>(filter),
      GetTensorShape(bias), GetTensorData<float>(bias), GetTensorShape(output),
   e32ee:	4638      	mov	r0, r7
   e32f0:	f7f3 f8af 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   e32f4:	4628      	mov	r0, r5
   e32f6:	f7f3 f8ac 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  tflite::reference_ops::DepthwiseConv(
      op_params, GetTensorShape(input), GetTensorData<float>(input),
      GetTensorShape(filter), GetTensorData<float>(filter),
   e32fa:	a843      	add	r0, sp, #268	; 0x10c
   e32fc:	e6eb      	b.n	e30d6 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x422>
      break;
    case kTfLiteUInt8:
      EvalQuantized(context, node, params, &data, input, filter, bias, output);
      break;
    default:
      context->ReportError(context, "Type %s (%d) not supported.",
   e32fe:	4640      	mov	r0, r8
   e3300:	6974      	ldr	r4, [r6, #20]
   e3302:	f7f0 ff0b 	bl	d411c <TfLiteTypeGetName>
                           TfLiteTypeGetName(input->type), input->type);
   e3306:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e3308:	4925      	ldr	r1, [pc, #148]	; (e33a0 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6ec>)
   e330a:	f81a 3003 	ldrb.w	r3, [sl, r3]
   e330e:	4602      	mov	r2, r0
   e3310:	4630      	mov	r0, r6
   e3312:	47a0      	blx	r4
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
  }

  TF_LITE_ENSURE_STATUS(CalculateOpData(context, node, params, width, height,
   e3314:	2001      	movs	r0, #1
   e3316:	e1b0      	b.n	e367a <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x9c6>
  const int32 input_offset = params.input_offset;
  const int32 filter_offset = params.weights_offset;
  const int32 output_offset = params.output_offset;
  const int32 output_multiplier = params.output_multiplier;
  const int output_shift = params.output_shift;
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
   e3318:	f001 f818 	bl	e434c <abort>
    return;
  }

  RuntimeShape reshaped_filter_shape;
  reshaped_filter_shape.BuildFrom(
      {1, output_depth, filter_height, filter_width});
   e331c:	2301      	movs	r3, #1
   e331e:	9335      	str	r3, [sp, #212]	; 0xd4
    const int dimensions_count =
        std::distance(src_iterable.begin(), src_iterable.end());
    Resize(dimensions_count);
    int32* data = DimsData();
    for (auto it : src_iterable) {
      *data = it;
   e3320:	934e      	str	r3, [sp, #312]	; 0x138

  // If this is the first time through, repack the weights into a cached buffer
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
   e3322:	4b20      	ldr	r3, [pc, #128]	; (e33a4 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6f0>)
      TFLITE_CHECK(false && "No shape resizing supported on this platform");
#else   // TF_LITE_STATIC_MEMORY
      delete[] dims_pointer_;
#endif  // TF_LITE_STATIC_MEMORY
    }
    size_ = dimensions_count;
   e3324:	944d      	str	r4, [sp, #308]	; 0x134
   e3326:	781c      	ldrb	r4, [r3, #0]
    return;
  }

  RuntimeShape reshaped_filter_shape;
  reshaped_filter_shape.BuildFrom(
      {1, output_depth, filter_height, filter_width});
   e3328:	9a09      	ldr	r2, [sp, #36]	; 0x24
   e332a:	f8cd a0d8 	str.w	sl, [sp, #216]	; 0xd8
   e332e:	9237      	str	r2, [sp, #220]	; 0xdc
   e3330:	f8cd b0e0 	str.w	fp, [sp, #224]	; 0xe0
    const int dimensions_count =
        std::distance(src_iterable.begin(), src_iterable.end());
    Resize(dimensions_count);
    int32* data = DimsData();
    for (auto it : src_iterable) {
      *data = it;
   e3334:	f8cd a13c 	str.w	sl, [sp, #316]	; 0x13c
   e3338:	9250      	str	r2, [sp, #320]	; 0x140
   e333a:	f8cd b144 	str.w	fp, [sp, #324]	; 0x144

  // If this is the first time through, repack the weights into a cached buffer
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
   e333e:	2c00      	cmp	r4, #0
   e3340:	d137      	bne.n	e33b2 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6fe>
              filter_data + Offset(filter_shape, 0, filter_y, filter_x, oc);
          int16_t* reshaped_filter =
              reshaped_filter_data +
              Offset(reshaped_filter_shape, 0, oc, filter_y, filter_x);
          *reshaped_filter =
              static_cast<int16_t>(*current_filter) + filter_offset;
   e3342:	4f19      	ldr	r7, [pc, #100]	; (e33a8 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6f4>)

  // If this is the first time through, repack the weights into a cached buffer
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
    for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   e3344:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e3346:	42a3      	cmp	r3, r4
   e3348:	dd30      	ble.n	e33ac <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6f8>
   e334a:	2500      	movs	r5, #0
      for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   e334c:	45ab      	cmp	fp, r5
   e334e:	dd1c      	ble.n	e338a <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6d6>
   e3350:	2600      	movs	r6, #0
        for (int oc = 0; oc < output_depth; ++oc) {
   e3352:	45b2      	cmp	sl, r6
   e3354:	dd17      	ble.n	e3386 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x6d2>
          const uint8* current_filter =
              filter_data + Offset(filter_shape, 0, filter_y, filter_x, oc);
   e3356:	9600      	str	r6, [sp, #0]
   e3358:	462b      	mov	r3, r5
   e335a:	4622      	mov	r2, r4
   e335c:	2100      	movs	r1, #0
   e335e:	a843      	add	r0, sp, #268	; 0x10c
   e3360:	f7f3 f8e7 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          int16_t* reshaped_filter =
              reshaped_filter_data +
              Offset(reshaped_filter_shape, 0, oc, filter_y, filter_x);
   e3364:	4632      	mov	r2, r6
  if (!is_reshaped_filter_initialized) {
    for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
      for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
        for (int oc = 0; oc < output_depth; ++oc) {
          const uint8* current_filter =
              filter_data + Offset(filter_shape, 0, filter_y, filter_x, oc);
   e3366:	4680      	mov	r8, r0
          int16_t* reshaped_filter =
              reshaped_filter_data +
              Offset(reshaped_filter_shape, 0, oc, filter_y, filter_x);
   e3368:	4623      	mov	r3, r4
   e336a:	9500      	str	r5, [sp, #0]
   e336c:	2100      	movs	r1, #0
   e336e:	a84d      	add	r0, sp, #308	; 0x134
   e3370:	f7f3 f8df 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
          *reshaped_filter =
              static_cast<int16_t>(*current_filter) + filter_offset;
   e3374:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
   e3376:	9a21      	ldr	r2, [sp, #132]	; 0x84
   e3378:	f813 3008 	ldrb.w	r3, [r3, r8]
   e337c:	4413      	add	r3, r2
   e337e:	f827 3010 	strh.w	r3, [r7, r0, lsl #1]
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
    for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
      for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
        for (int oc = 0; oc < output_depth; ++oc) {
   e3382:	3601      	adds	r6, #1
   e3384:	e7e5      	b.n	e3352 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x69e>
  // If this is the first time through, repack the weights into a cached buffer
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
    for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
      for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   e3386:	3501      	adds	r5, #1
   e3388:	e7e0      	b.n	e334c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x698>

  // If this is the first time through, repack the weights into a cached buffer
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
    for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
   e338a:	3401      	adds	r4, #1
   e338c:	e7da      	b.n	e3344 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x690>
   e338e:	bf00      	nop
   e3390:	000eaffd 	.word	0x000eaffd
   e3394:	7f7fffff 	.word	0x7f7fffff
   e3398:	ff7fffff 	.word	0xff7fffff
   e339c:	00000000 	.word	0x00000000
   e33a0:	000e9b4a 	.word	0x000e9b4a
   e33a4:	2003dcc2 	.word	0x2003dcc2
   e33a8:	2003dcc4 	.word	0x2003dcc4
          *reshaped_filter =
              static_cast<int16_t>(*current_filter) + filter_offset;
        }
      }
    }
    is_reshaped_filter_initialized = true;
   e33ac:	4b9a      	ldr	r3, [pc, #616]	; (e3618 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x964>)
   e33ae:	2201      	movs	r2, #1
   e33b0:	701a      	strb	r2, [r3, #0]
  // If this is the first time through, repack the weights into a cached buffer
  // so that they can be accessed sequentially.
  static bool is_reshaped_filter_initialized = false;
  if (!is_reshaped_filter_initialized) {
    for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
      for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
   e33b2:	2300      	movs	r3, #0
   e33b4:	930b      	str	r3, [sp, #44]	; 0x2c
                    Offset(filter_shape, 0, filter_y, filter_x_start, oc);
                for (int filter_x = filter_x_start; filter_x < filter_x_end;
                     ++filter_x) {
                  int32 input_val = *current_input;
                  current_input += input_depth;
                  int32 filter_val = *current_filter;
   e33b6:	f1ca 0300 	rsb	r3, sl, #0
   e33ba:	9333      	str	r3, [sp, #204]	; 0xcc
      }
    }
    is_reshaped_filter_initialized = true;
  }

  for (int b = 0; b < batches; ++b) {
   e33bc:	9b2b      	ldr	r3, [sp, #172]	; 0xac
   e33be:	9a0b      	ldr	r2, [sp, #44]	; 0x2c
   e33c0:	4293      	cmp	r3, r2
   e33c2:	f340 8123 	ble.w	e360c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x958>
   e33c6:	9b1e      	ldr	r3, [sp, #120]	; 0x78
   e33c8:	9a18      	ldr	r2, [sp, #96]	; 0x60
   e33ca:	4413      	add	r3, r2
   e33cc:	9313      	str	r3, [sp, #76]	; 0x4c
   e33ce:	9b1e      	ldr	r3, [sp, #120]	; 0x78
   e33d0:	425b      	negs	r3, r3
   e33d2:	930c      	str	r3, [sp, #48]	; 0x30
   e33d4:	2300      	movs	r3, #0
   e33d6:	930f      	str	r3, [sp, #60]	; 0x3c
    for (int out_y = 0; out_y < output_height; ++out_y) {
   e33d8:	9b2c      	ldr	r3, [sp, #176]	; 0xb0
   e33da:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
   e33dc:	4293      	cmp	r3, r2
   e33de:	f340 8111 	ble.w	e3604 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x950>
   e33e2:	9a0c      	ldr	r2, [sp, #48]	; 0x30
   e33e4:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e33e6:	9818      	ldr	r0, [sp, #96]	; 0x60
   e33e8:	9909      	ldr	r1, [sp, #36]	; 0x24
   e33ea:	4413      	add	r3, r2
   e33ec:	9a13      	ldr	r2, [sp, #76]	; 0x4c
   e33ee:	4298      	cmp	r0, r3
   e33f0:	bfc8      	it	gt
   e33f2:	460a      	movgt	r2, r1
   e33f4:	9b1d      	ldr	r3, [sp, #116]	; 0x74
   e33f6:	922e      	str	r2, [sp, #184]	; 0xb8
   e33f8:	ebc3 030b 	rsb	r3, r3, fp
   e33fc:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   e33fe:	930d      	str	r3, [sp, #52]	; 0x34
   e3400:	9b1d      	ldr	r3, [sp, #116]	; 0x74
   e3402:	4413      	add	r3, r2
   e3404:	9314      	str	r3, [sp, #80]	; 0x50
   e3406:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   e3408:	9a0c      	ldr	r2, [sp, #48]	; 0x30
   e340a:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
   e340e:	9331      	str	r3, [sp, #196]	; 0xc4
   e3410:	9b13      	ldr	r3, [sp, #76]	; 0x4c
   e3412:	2a00      	cmp	r2, #0
   e3414:	eba3 0300 	sub.w	r3, r3, r0
   e3418:	bfa8      	it	ge
   e341a:	2300      	movge	r3, #0
   e341c:	9324      	str	r3, [sp, #144]	; 0x90
   e341e:	2300      	movs	r3, #0
   e3420:	9310      	str	r3, [sp, #64]	; 0x40
      for (int out_x = 0; out_x < output_width; ++out_x) {
   e3422:	9b2d      	ldr	r3, [sp, #180]	; 0xb4
   e3424:	9a10      	ldr	r2, [sp, #64]	; 0x40
   e3426:	4293      	cmp	r3, r2
   e3428:	f340 80e1 	ble.w	e35ee <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x93a>
   e342c:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   e342e:	9a0e      	ldr	r2, [sp, #56]	; 0x38
   e3430:	990d      	ldr	r1, [sp, #52]	; 0x34
   e3432:	ebcb 0303 	rsb	r3, fp, r3
   e3436:	9325      	str	r3, [sp, #148]	; 0x94
   e3438:	9b14      	ldr	r3, [sp, #80]	; 0x50
   e343a:	1a9b      	subs	r3, r3, r2
   e343c:	9330      	str	r3, [sp, #192]	; 0xc0
   e343e:	9b14      	ldr	r3, [sp, #80]	; 0x50
   e3440:	f04f 0800 	mov.w	r8, #0
   e3444:	428a      	cmp	r2, r1
   e3446:	bfc8      	it	gt
   e3448:	465b      	movgt	r3, fp
   e344a:	932f      	str	r3, [sp, #188]	; 0xbc
   e344c:	f8cd 8044 	str.w	r8, [sp, #68]	; 0x44
        for (int ic = 0; ic < input_depth; ++ic) {
   e3450:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e3452:	9a11      	ldr	r2, [sp, #68]	; 0x44
   e3454:	4293      	cmp	r3, r2
   e3456:	f340 80bf 	ble.w	e35d8 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x924>
   e345a:	9b16      	ldr	r3, [sp, #88]	; 0x58
   e345c:	eb03 0388 	add.w	r3, r3, r8, lsl #2
   e3460:	9332      	str	r3, [sp, #200]	; 0xc8
   e3462:	f04f 0900 	mov.w	r9, #0
          for (int m = 0; m < depth_multiplier; m++) {
   e3466:	9b17      	ldr	r3, [sp, #92]	; 0x5c
   e3468:	454b      	cmp	r3, r9
   e346a:	f340 80af 	ble.w	e35cc <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x918>
   e346e:	eb09 0308 	add.w	r3, r9, r8
   e3472:	9315      	str	r3, [sp, #84]	; 0x54
            }
            int in_y = in_y_start;
            int in_x_start = in_x_origin;
            int filter_x_start = 0;
            bool is_out_of_x_bounds = false;
            if (in_x_origin < 0) {
   e3474:	9b25      	ldr	r3, [sp, #148]	; 0x94
              is_out_of_x_bounds = true;
            }
            int filter_x_end = filter_width;
            if ((in_x_origin + filter_width) >= input_width) {
              filter_x_end -= (in_x_origin + filter_width) - input_width;
              is_out_of_x_bounds = true;
   e3476:	9a0d      	ldr	r2, [sp, #52]	; 0x34
            }
            for (int filter_y = filter_y_start; filter_y < filter_y_end;
   e3478:	9d24      	ldr	r5, [sp, #144]	; 0x90
            }
            int in_y = in_y_start;
            int in_x_start = in_x_origin;
            int filter_x_start = 0;
            bool is_out_of_x_bounds = false;
            if (in_x_origin < 0) {
   e347a:	2b00      	cmp	r3, #0
              in_x_start = 0;
              filter_x_start = 0 - in_x_origin;
   e347c:	bfbd      	ittte	lt
   e347e:	9b30      	ldrlt	r3, [sp, #192]	; 0xc0
   e3480:	9312      	strlt	r3, [sp, #72]	; 0x48
            int in_y = in_y_start;
            int in_x_start = in_x_origin;
            int filter_x_start = 0;
            bool is_out_of_x_bounds = false;
            if (in_x_origin < 0) {
              in_x_start = 0;
   e3482:	2300      	movlt	r3, #0
            }
            int in_y = in_y_start;
            int in_x_start = in_x_origin;
            int filter_x_start = 0;
            bool is_out_of_x_bounds = false;
            if (in_x_origin < 0) {
   e3484:	9322      	strge	r3, [sp, #136]	; 0x88
              in_x_start = 0;
   e3486:	bfb8      	it	lt
   e3488:	9322      	strlt	r3, [sp, #136]	; 0x88
              is_out_of_x_bounds = true;
            }
            int filter_x_end = filter_width;
            if ((in_x_origin + filter_width) >= input_width) {
              filter_x_end -= (in_x_origin + filter_width) - input_width;
              is_out_of_x_bounds = true;
   e348a:	9b0e      	ldr	r3, [sp, #56]	; 0x38
              filter_y_end -= (in_y_origin + filter_height) - input_height;
            }
            int in_y = in_y_start;
            int in_x_start = in_x_origin;
            int filter_x_start = 0;
            bool is_out_of_x_bounds = false;
   e348c:	bfaa      	itet	ge
   e348e:	2600      	movge	r6, #0
            if (in_x_origin < 0) {
              in_x_start = 0;
              filter_x_start = 0 - in_x_origin;
              is_out_of_x_bounds = true;
   e3490:	2601      	movlt	r6, #1
            if ((in_y_origin + filter_height) >= input_height) {
              filter_y_end -= (in_y_origin + filter_height) - input_height;
            }
            int in_y = in_y_start;
            int in_x_start = in_x_origin;
            int filter_x_start = 0;
   e3492:	9612      	strge	r6, [sp, #72]	; 0x48
        for (int ic = 0; ic < input_depth; ++ic) {
          for (int m = 0; m < depth_multiplier; m++) {
            const int oc = m + ic * depth_multiplier;
            const int in_x_origin = (out_x * stride_width) - pad_width;
            const int in_y_origin = (out_y * stride_height) - pad_height;
            int32 acc = 0;
   e3494:	2400      	movs	r4, #0
              is_out_of_x_bounds = true;
            }
            int filter_x_end = filter_width;
            if ((in_x_origin + filter_width) >= input_width) {
              filter_x_end -= (in_x_origin + filter_width) - input_width;
              is_out_of_x_bounds = true;
   e3496:	4293      	cmp	r3, r2
   e3498:	bfd8      	it	le
   e349a:	2601      	movle	r6, #1
   e349c:	9b31      	ldr	r3, [sp, #196]	; 0xc4
   e349e:	9a24      	ldr	r2, [sp, #144]	; 0x90
   e34a0:	1a9a      	subs	r2, r3, r2
            }
            for (int filter_y = filter_y_start; filter_y < filter_y_end;
   e34a2:	9b2e      	ldr	r3, [sp, #184]	; 0xb8
   e34a4:	42ab      	cmp	r3, r5
   e34a6:	442a      	add	r2, r5
   e34a8:	dd6d      	ble.n	e3586 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x8d2>
                 ++filter_y, ++in_y) {
              const uint8* current_input =
                  input_data + Offset(input_shape, b, in_y, in_x_start, ic);
   e34aa:	9b11      	ldr	r3, [sp, #68]	; 0x44
   e34ac:	9300      	str	r3, [sp, #0]
   e34ae:	990b      	ldr	r1, [sp, #44]	; 0x2c
   e34b0:	9b22      	ldr	r3, [sp, #136]	; 0x88
   e34b2:	a848      	add	r0, sp, #288	; 0x120
   e34b4:	f7f3 f83d 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   e34b8:	9b1a      	ldr	r3, [sp, #104]	; 0x68
   e34ba:	9023      	str	r0, [sp, #140]	; 0x8c
              if ((filter_width == 8) && !is_out_of_x_bounds) {
   e34bc:	f1bb 0f08 	cmp.w	fp, #8
              is_out_of_x_bounds = true;
            }
            for (int filter_y = filter_y_start; filter_y < filter_y_end;
                 ++filter_y, ++in_y) {
              const uint8* current_input =
                  input_data + Offset(input_shape, b, in_y, in_x_start, ic);
   e34c0:	eb03 0700 	add.w	r7, r3, r0
              if ((filter_width == 8) && !is_out_of_x_bounds) {
   e34c4:	d13c      	bne.n	e3540 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x88c>
   e34c6:	2e00      	cmp	r6, #0
   e34c8:	d13a      	bne.n	e3540 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x88c>
                int16* current_filter =
                    reshaped_filter_data + Offset(reshaped_filter_shape, 0, oc,
   e34ca:	9b12      	ldr	r3, [sp, #72]	; 0x48
   e34cc:	9300      	str	r3, [sp, #0]
   e34ce:	9a15      	ldr	r2, [sp, #84]	; 0x54
   e34d0:	462b      	mov	r3, r5
   e34d2:	4631      	mov	r1, r6
   e34d4:	a84d      	add	r0, sp, #308	; 0x134
   e34d6:	f7f3 f82c 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                                                  filter_y, filter_x_start);
   e34da:	4b50      	ldr	r3, [pc, #320]	; (e361c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x968>)
                const uint32_t input_vals0 =
                    *reinterpret_cast<const uint32_t*>(current_input);
   e34dc:	9a23      	ldr	r2, [sp, #140]	; 0x8c
              const uint8* current_input =
                  input_data + Offset(input_shape, b, in_y, in_x_start, ic);
              if ((filter_width == 8) && !is_out_of_x_bounds) {
                int16* current_filter =
                    reshaped_filter_data + Offset(reshaped_filter_shape, 0, oc,
                                                  filter_y, filter_x_start);
   e34de:	eb03 0040 	add.w	r0, r3, r0, lsl #1
                const uint32_t input_vals0 =
                    *reinterpret_cast<const uint32_t*>(current_input);
   e34e2:	9b1a      	ldr	r3, [sp, #104]	; 0x68
                current_input += 4;
                const int32_t filter_vals0 =
                    *reinterpret_cast<const int32_t*>(current_filter);
   e34e4:	f8d0 c000 	ldr.w	ip, [r0]
              if ((filter_width == 8) && !is_out_of_x_bounds) {
                int16* current_filter =
                    reshaped_filter_data + Offset(reshaped_filter_shape, 0, oc,
                                                  filter_y, filter_x_start);
                const uint32_t input_vals0 =
                    *reinterpret_cast<const uint32_t*>(current_input);
   e34e8:	5899      	ldr	r1, [r3, r2]
                const int32_t filter_vals0 =
                    *reinterpret_cast<const int32_t*>(current_filter);
                current_filter += 2;
                const uint8 input_val0 = input_vals0 & 0xff;
                const int16 filter_val0 = filter_vals0 & 0xffff;
                acc += filter_val0 * input_val0;
   e34ea:	fa0f f28c 	sxth.w	r2, ip
   e34ee:	b2cb      	uxtb	r3, r1
   e34f0:	fb02 4203 	mla	r2, r2, r3, r4
                const uint8 input_val1 = (input_vals0 >> 8) & 0xff;
                const int16 filter_val1 = (filter_vals0 >> 16) & 0xffff;
                acc += filter_val1 * input_val1;
   e34f4:	ea4f 4e2c 	mov.w	lr, ip, asr #16
   e34f8:	f3c1 2307 	ubfx	r3, r1, #8, #8
   e34fc:	fb0e 2e03 	mla	lr, lr, r3, r2

                const int32_t filter_vals1 =
                    *reinterpret_cast<const int32_t*>(current_filter);
   e3500:	6843      	ldr	r3, [r0, #4]
                current_filter += 2;
                const uint8 input_val2 = (input_vals0 >> 16) & 0xff;
                const int16 filter_val2 = filter_vals1 & 0xffff;
                acc += filter_val2 * input_val2;
   e3502:	f3c1 4c07 	ubfx	ip, r1, #16, #8
   e3506:	b21a      	sxth	r2, r3
                const uint8 input_val3 = (input_vals0 >> 24) & 0xff;
                const int16 filter_val3 = (filter_vals1 >> 16) & 0xffff;
                acc += filter_val3 * input_val3;
   e3508:	0e09      	lsrs	r1, r1, #24
   e350a:	141b      	asrs	r3, r3, #16
                const int32_t filter_vals1 =
                    *reinterpret_cast<const int32_t*>(current_filter);
                current_filter += 2;
                const uint8 input_val2 = (input_vals0 >> 16) & 0xff;
                const int16 filter_val2 = filter_vals1 & 0xffff;
                acc += filter_val2 * input_val2;
   e350c:	fb02 e20c 	mla	r2, r2, ip, lr
                const uint8 input_val3 = (input_vals0 >> 24) & 0xff;
                const int16 filter_val3 = (filter_vals1 >> 16) & 0xffff;
                acc += filter_val3 * input_val3;
   e3510:	fb01 2203 	mla	r2, r1, r3, r2

                const uint32_t input_vals1 =
                    *reinterpret_cast<const uint32_t*>(current_input);
   e3514:	687b      	ldr	r3, [r7, #4]
                const int32_t filter_vals2 =
                    *reinterpret_cast<const int32_t*>(current_filter);
   e3516:	6881      	ldr	r1, [r0, #8]
                current_filter += 2;
                const uint8 input_val4 = input_vals1 & 0xff;
                const int16 filter_val4 = filter_vals2 & 0xffff;
                acc += filter_val4 * input_val4;
   e3518:	b2dc      	uxtb	r4, r3
   e351a:	b20f      	sxth	r7, r1
   e351c:	fb07 2204 	mla	r2, r7, r4, r2
                const uint8 input_val5 = (input_vals1 >> 8) & 0xff;
                const int16 filter_val5 = (filter_vals2 >> 16) & 0xffff;
                acc += filter_val5 * input_val5;
   e3520:	1409      	asrs	r1, r1, #16
   e3522:	f3c3 2707 	ubfx	r7, r3, #8, #8
   e3526:	fb01 2207 	mla	r2, r1, r7, r2

                const int32_t filter_vals3 =
                    *reinterpret_cast<const int32_t*>(current_filter);
   e352a:	68c1      	ldr	r1, [r0, #12]
                const uint8 input_val6 = (input_vals1 >> 16) & 0xff;
                const int16 filter_val6 = filter_vals3 & 0xffff;
                acc += filter_val6 * input_val6;
   e352c:	f3c3 4007 	ubfx	r0, r3, #16, #8
   e3530:	b20c      	sxth	r4, r1
   e3532:	fb04 2400 	mla	r4, r4, r0, r2
                const uint8 input_val7 = (input_vals1 >> 24) & 0xff;
                const int16 filter_val7 = (filter_vals3 >> 16) & 0xffff;
                acc += filter_val7 * input_val7;
   e3536:	1409      	asrs	r1, r1, #16
   e3538:	0e1b      	lsrs	r3, r3, #24
   e353a:	fb03 4401 	mla	r4, r3, r1, r4
   e353e:	e020      	b.n	e3582 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x8ce>
              } else {
                const uint8* current_filter =
                    filter_data +
                    Offset(filter_shape, 0, filter_y, filter_x_start, oc);
   e3540:	9b15      	ldr	r3, [sp, #84]	; 0x54
   e3542:	9300      	str	r3, [sp, #0]
   e3544:	462a      	mov	r2, r5
   e3546:	9b12      	ldr	r3, [sp, #72]	; 0x48
   e3548:	2100      	movs	r1, #0
   e354a:	a843      	add	r0, sp, #268	; 0x10c
   e354c:	f7f2 fff1 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
   e3550:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
                for (int filter_x = filter_x_start; filter_x < filter_x_end;
                     ++filter_x) {
                  int32 input_val = *current_input;
   e3552:	9a0a      	ldr	r2, [sp, #40]	; 0x28
   e3554:	4418      	add	r0, r3
                acc += filter_val7 * input_val7;
              } else {
                const uint8* current_filter =
                    filter_data +
                    Offset(filter_shape, 0, filter_y, filter_x_start, oc);
                for (int filter_x = filter_x_start; filter_x < filter_x_end;
   e3556:	9b12      	ldr	r3, [sp, #72]	; 0x48
                     ++filter_x) {
                  int32 input_val = *current_input;
   e3558:	f1c2 0e00 	rsb	lr, r2, #0
   e355c:	9a0a      	ldr	r2, [sp, #40]	; 0x28
   e355e:	4417      	add	r7, r2
                acc += filter_val7 * input_val7;
              } else {
                const uint8* current_filter =
                    filter_data +
                    Offset(filter_shape, 0, filter_y, filter_x_start, oc);
                for (int filter_x = filter_x_start; filter_x < filter_x_end;
   e3560:	9a2f      	ldr	r2, [sp, #188]	; 0xbc
   e3562:	429a      	cmp	r2, r3
   e3564:	4450      	add	r0, sl
   e3566:	dd0c      	ble.n	e3582 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x8ce>
                  int32 input_val = *current_input;
                  current_input += input_depth;
                  int32 filter_val = *current_filter;
                  current_filter += output_depth;
                  acc +=
                      (filter_val + filter_offset) * (input_val + input_offset);
   e3568:	9a33      	ldr	r2, [sp, #204]	; 0xcc
   e356a:	9921      	ldr	r1, [sp, #132]	; 0x84
   e356c:	5c82      	ldrb	r2, [r0, r2]
   e356e:	eb02 0c01 	add.w	ip, r2, r1
   e3572:	f817 100e 	ldrb.w	r1, [r7, lr]
   e3576:	9a27      	ldr	r2, [sp, #156]	; 0x9c
   e3578:	4411      	add	r1, r2
   e357a:	fb01 440c 	mla	r4, r1, ip, r4
                acc += filter_val7 * input_val7;
              } else {
                const uint8* current_filter =
                    filter_data +
                    Offset(filter_shape, 0, filter_y, filter_x_start, oc);
                for (int filter_x = filter_x_start; filter_x < filter_x_end;
   e357e:	3301      	adds	r3, #1
   e3580:	e7ec      	b.n	e355c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x8a8>
            int filter_x_end = filter_width;
            if ((in_x_origin + filter_width) >= input_width) {
              filter_x_end -= (in_x_origin + filter_width) - input_width;
              is_out_of_x_bounds = true;
            }
            for (int filter_y = filter_y_start; filter_y < filter_y_end;
   e3582:	3501      	adds	r5, #1
   e3584:	e78a      	b.n	e349c <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x7e8>
                  acc +=
                      (filter_val + filter_offset) * (input_val + input_offset);
                }
              }
            }
            if (bias_data) {
   e3586:	9b16      	ldr	r3, [sp, #88]	; 0x58
   e3588:	b11b      	cbz	r3, e3592 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x8de>
              acc += bias_data[oc];
   e358a:	9b32      	ldr	r3, [sp, #200]	; 0xc8
   e358c:	f853 3029 	ldr.w	r3, [r3, r9, lsl #2]
   e3590:	441c      	add	r4, r3
}

template <>
inline int32 DepthwiseConvRound<DepthwiseConvOutputRounding::kAwayFromZero>(
    int32 x, int32 quantized_multiplier, int shift) {
  return MultiplyByQuantizedMultiplier(x, quantized_multiplier, shift);
   e3592:	9a2a      	ldr	r2, [sp, #168]	; 0xa8
   e3594:	9929      	ldr	r1, [sp, #164]	; 0xa4
   e3596:	4620      	mov	r0, r4
   e3598:	f7f8 fb26 	bl	dbbe8 <_ZN6tflite29MultiplyByQuantizedMultiplierElli>
            }
            acc = reference_ops::depthwise_conv::DepthwiseConvRound<
                DepthwiseConvOutputRounding::kAwayFromZero>(
                acc, output_multiplier, output_shift);
            acc += output_offset;
   e359c:	9b28      	ldr	r3, [sp, #160]	; 0xa0
            acc = std::max(acc, output_activation_min);
            acc = std::min(acc, output_activation_max);
            output_data[Offset(output_shape, b, out_y, out_x, oc)] =
   e359e:	9a0f      	ldr	r2, [sp, #60]	; 0x3c
   e35a0:	990b      	ldr	r1, [sp, #44]	; 0x2c
              acc += bias_data[oc];
            }
            acc = reference_ops::depthwise_conv::DepthwiseConvRound<
                DepthwiseConvOutputRounding::kAwayFromZero>(
                acc, output_multiplier, output_shift);
            acc += output_offset;
   e35a2:	4418      	add	r0, r3
   e35a4:	9b1f      	ldr	r3, [sp, #124]	; 0x7c
   e35a6:	4283      	cmp	r3, r0
   e35a8:	bfb8      	it	lt
   e35aa:	4603      	movlt	r3, r0
   e35ac:	461c      	mov	r4, r3
   e35ae:	9b20      	ldr	r3, [sp, #128]	; 0x80
   e35b0:	429c      	cmp	r4, r3
   e35b2:	bfa8      	it	ge
   e35b4:	461c      	movge	r4, r3
            acc = std::max(acc, output_activation_min);
            acc = std::min(acc, output_activation_max);
            output_data[Offset(output_shape, b, out_y, out_x, oc)] =
   e35b6:	9b15      	ldr	r3, [sp, #84]	; 0x54
   e35b8:	9300      	str	r3, [sp, #0]
   e35ba:	a839      	add	r0, sp, #228	; 0xe4
   e35bc:	9b10      	ldr	r3, [sp, #64]	; 0x40
   e35be:	f7f2 ffb8 	bl	d6532 <_ZN6tflite6OffsetERKNS_12RuntimeShapeEiiii>
                static_cast<uint8>(acc);
   e35c2:	9b26      	ldr	r3, [sp, #152]	; 0x98

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int ic = 0; ic < input_depth; ++ic) {
          for (int m = 0; m < depth_multiplier; m++) {
   e35c4:	f109 0901 	add.w	r9, r9, #1
                acc, output_multiplier, output_shift);
            acc += output_offset;
            acc = std::max(acc, output_activation_min);
            acc = std::min(acc, output_activation_max);
            output_data[Offset(output_shape, b, out_y, out_x, oc)] =
                static_cast<uint8>(acc);
   e35c8:	541c      	strb	r4, [r3, r0]
   e35ca:	e74c      	b.n	e3466 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x7b2>
  }

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
        for (int ic = 0; ic < input_depth; ++ic) {
   e35cc:	9b11      	ldr	r3, [sp, #68]	; 0x44
   e35ce:	3301      	adds	r3, #1
   e35d0:	9311      	str	r3, [sp, #68]	; 0x44
   e35d2:	9b17      	ldr	r3, [sp, #92]	; 0x5c
   e35d4:	4498      	add	r8, r3
   e35d6:	e73b      	b.n	e3450 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x79c>
    is_reshaped_filter_initialized = true;
  }

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      for (int out_x = 0; out_x < output_width; ++out_x) {
   e35d8:	9b10      	ldr	r3, [sp, #64]	; 0x40
   e35da:	9a19      	ldr	r2, [sp, #100]	; 0x64
   e35dc:	3301      	adds	r3, #1
   e35de:	9310      	str	r3, [sp, #64]	; 0x40
   e35e0:	9b0d      	ldr	r3, [sp, #52]	; 0x34
   e35e2:	4413      	add	r3, r2
   e35e4:	930d      	str	r3, [sp, #52]	; 0x34
   e35e6:	9b14      	ldr	r3, [sp, #80]	; 0x50
   e35e8:	1a9b      	subs	r3, r3, r2
   e35ea:	9314      	str	r3, [sp, #80]	; 0x50
   e35ec:	e719      	b.n	e3422 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x76e>
    }
    is_reshaped_filter_initialized = true;
  }

  for (int b = 0; b < batches; ++b) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
   e35ee:	9b0f      	ldr	r3, [sp, #60]	; 0x3c
   e35f0:	9a1c      	ldr	r2, [sp, #112]	; 0x70
   e35f2:	3301      	adds	r3, #1
   e35f4:	930f      	str	r3, [sp, #60]	; 0x3c
   e35f6:	9b13      	ldr	r3, [sp, #76]	; 0x4c
   e35f8:	1a9b      	subs	r3, r3, r2
   e35fa:	9313      	str	r3, [sp, #76]	; 0x4c
   e35fc:	9b0c      	ldr	r3, [sp, #48]	; 0x30
   e35fe:	4413      	add	r3, r2
   e3600:	930c      	str	r3, [sp, #48]	; 0x30
   e3602:	e6e9      	b.n	e33d8 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x724>
      }
    }
    is_reshaped_filter_initialized = true;
  }

  for (int b = 0; b < batches; ++b) {
   e3604:	9b0b      	ldr	r3, [sp, #44]	; 0x2c
   e3606:	3301      	adds	r3, #1
   e3608:	930b      	str	r3, [sp, #44]	; 0x2c
   e360a:	e6d7      	b.n	e33bc <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x708>
        "Size too large for reshaped weight buffer (%d needed, %d available)",
        needed_size, kReshapedFilterDataSize);
    return;
  }

  RuntimeShape reshaped_filter_shape;
   e360c:	a84d      	add	r0, sp, #308	; 0x134
   e360e:	f7f2 ff20 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
   e3612:	e5fc      	b.n	e320e <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x55a>
}

template <typename T>
inline const T* GetTensorData(const TfLiteTensor* tensor) {
  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)
                           : nullptr;
   e3614:	9416      	str	r4, [sp, #88]	; 0x58
   e3616:	e578      	b.n	e310a <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x456>
   e3618:	2003dcc2 	.word	0x2003dcc2
   e361c:	2003dcc4 	.word	0x2003dcc4
        GetTensorData<uint8_t>(filter), GetTensorShape(bias),
        GetTensorData<int32_t>(bias), GetTensorShape(output),
        GetTensorData<uint8_t>(output));
  } else {
    tflite::reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   e3620:	4649      	mov	r1, r9
   e3622:	a84d      	add	r0, sp, #308	; 0x134
   e3624:	f7f3 f9c5 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
        GetTensorShape(filter), GetTensorData<uint8_t>(filter),
   e3628:	4639      	mov	r1, r7
   e362a:	a848      	add	r0, sp, #288	; 0x120
   e362c:	f8d9 9004 	ldr.w	r9, [r9, #4]
   e3630:	f7f3 f9bf 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
   e3634:	4621      	mov	r1, r4
   e3636:	4640      	mov	r0, r8
   e3638:	687f      	ldr	r7, [r7, #4]
   e363a:	f7f3 f9ba 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
   e363e:	b104      	cbz	r4, e3642 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x98e>
   e3640:	6864      	ldr	r4, [r4, #4]
        GetTensorShape(output), GetTensorData<uint8_t>(output));
   e3642:	9909      	ldr	r1, [sp, #36]	; 0x24
   e3644:	4628      	mov	r0, r5
   e3646:	f7f3 f9b4 	bl	d69b2 <_ZN6tflite14GetTensorShapeEPK12TfLiteTensor>
  return depthwise_conv::DepthwiseConvBasicKernel<
      DepthwiseConvOutputRounding::kAwayFromZero>::Run(params, input_shape,
                                                       input_data, filter_shape,
                                                       filter_data, bias_shape,
                                                       bias_data, output_shape,
                                                       output_data);
   e364a:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e364c:	685b      	ldr	r3, [r3, #4]
   e364e:	9304      	str	r3, [sp, #16]
   e3650:	464a      	mov	r2, r9
   e3652:	ab48      	add	r3, sp, #288	; 0x120
   e3654:	a94d      	add	r1, sp, #308	; 0x134
   e3656:	a852      	add	r0, sp, #328	; 0x148
   e3658:	9503      	str	r5, [sp, #12]
   e365a:	9402      	str	r4, [sp, #8]
   e365c:	e88d 0180 	stmia.w	sp, {r7, r8}
   e3660:	f7ff fa08 	bl	e2a74 <_ZN6tflite13reference_ops14depthwise_conv24DepthwiseConvBasicKernelILNS_27DepthwiseConvOutputRoundingE1EE3RunERKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKhSA_SC_SA_PKlSA_Ph>
   e3664:	4628      	mov	r0, r5
   e3666:	f7f2 fef4 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
        GetTensorData<uint8_t>(output));
  } else {
    tflite::reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
        GetTensorShape(filter), GetTensorData<uint8_t>(filter),
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
   e366a:	4640      	mov	r0, r8
   e366c:	f7f2 fef1 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
        GetTensorData<int32_t>(bias), GetTensorShape(output),
        GetTensorData<uint8_t>(output));
  } else {
    tflite::reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
        GetTensorShape(filter), GetTensorData<uint8_t>(filter),
   e3670:	a848      	add	r0, sp, #288	; 0x120
   e3672:	f7f2 feee 	bl	d6452 <_ZN6tflite12RuntimeShapeD1Ev>
        GetTensorData<uint8_t>(filter), GetTensorShape(bias),
        GetTensorData<int32_t>(bias), GetTensorShape(output),
        GetTensorData<uint8_t>(output));
  } else {
    tflite::reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
   e3676:	a84d      	add	r0, sp, #308	; 0x134
   e3678:	e5d3      	b.n	e3222 <_ZN6tflite3ops5micro14depthwise_conv4EvalEP13TfLiteContextP10TfLiteNode+0x56e>
      context->ReportError(context, "Type %s (%d) not supported.",
                           TfLiteTypeGetName(input->type), input->type);
      return kTfLiteError;
  }
  return kTfLiteOk;
}
   e367a:	f50d 7d69 	add.w	sp, sp, #932	; 0x3a4
   e367e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e3682:	bf00      	nop

000e3684 <_ZN6tflite19GreedyMemoryPlannerD1Ev>:
  buffer_offsets_ = reinterpret_cast<int*>(next_free);
}

GreedyMemoryPlanner::~GreedyMemoryPlanner() {
  // We don't own the scratch buffer, so don't deallocate anything.
}
   e3684:	4770      	bx	lr

000e3686 <_ZN6tflite19GreedyMemoryPlanner14GetBufferCountEv>:
    line[kLineWidth] = 0;
    error_reporter->Report("%s", line);
  }
}

int GreedyMemoryPlanner::GetBufferCount() { return buffer_count_; }
   e3686:	6880      	ldr	r0, [r0, #8]
   e3688:	4770      	bx	lr

000e368a <_ZN6tflite19GreedyMemoryPlannerD0Ev>:
  next_free += sizeof(ListEntry) * max_buffer_count_;

  buffer_offsets_ = reinterpret_cast<int*>(next_free);
}

GreedyMemoryPlanner::~GreedyMemoryPlanner() {
   e368a:	b510      	push	{r4, lr}
  // We don't own the scratch buffer, so don't deallocate anything.
}
   e368c:	2128      	movs	r1, #40	; 0x28
  next_free += sizeof(ListEntry) * max_buffer_count_;

  buffer_offsets_ = reinterpret_cast<int*>(next_free);
}

GreedyMemoryPlanner::~GreedyMemoryPlanner() {
   e368e:	4604      	mov	r4, r0
  // We don't own the scratch buffer, so don't deallocate anything.
}
   e3690:	f001 fbb5 	bl	e4dfe <_ZdlPvj>
   e3694:	4620      	mov	r0, r4
   e3696:	bd10      	pop	{r4, pc}

000e3698 <_ZN6tflite19GreedyMemoryPlanner9AddBufferEPNS_13ErrorReporterEiii>:

TfLiteStatus GreedyMemoryPlanner::AddBuffer(
    tflite::ErrorReporter* error_reporter, int size, int first_time_used,
    int last_time_used) {
   e3698:	b570      	push	{r4, r5, r6, lr}
   e369a:	4616      	mov	r6, r2
  if (buffer_count_ >= max_buffer_count_) {
   e369c:	6884      	ldr	r4, [r0, #8]
   e369e:	6842      	ldr	r2, [r0, #4]
   e36a0:	4294      	cmp	r4, r2
  // We don't own the scratch buffer, so don't deallocate anything.
}

TfLiteStatus GreedyMemoryPlanner::AddBuffer(
    tflite::ErrorReporter* error_reporter, int size, int first_time_used,
    int last_time_used) {
   e36a2:	460d      	mov	r5, r1
  if (buffer_count_ >= max_buffer_count_) {
   e36a4:	db05      	blt.n	e36b2 <_ZN6tflite19GreedyMemoryPlanner9AddBufferEPNS_13ErrorReporterEiii+0x1a>
    error_reporter->Report("Too many buffers (max is %d)", max_buffer_count_);
   e36a6:	490b      	ldr	r1, [pc, #44]	; (e36d4 <_ZN6tflite19GreedyMemoryPlanner9AddBufferEPNS_13ErrorReporterEiii+0x3c>)
   e36a8:	4628      	mov	r0, r5
   e36aa:	f7f0 fe93 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
    return kTfLiteError;
   e36ae:	2001      	movs	r0, #1
   e36b0:	bd70      	pop	{r4, r5, r6, pc}
  }
  BufferRequirements* current = &requirements_[buffer_count_];
   e36b2:	68c5      	ldr	r5, [r0, #12]
   e36b4:	210c      	movs	r1, #12
   e36b6:	4361      	muls	r1, r4
   e36b8:	186c      	adds	r4, r5, r1
  current->size = size;
   e36ba:	506e      	str	r6, [r5, r1]
  current->first_time_used = first_time_used;
   e36bc:	6063      	str	r3, [r4, #4]
  current->last_time_used = last_time_used;
   e36be:	9b04      	ldr	r3, [sp, #16]
   e36c0:	60a3      	str	r3, [r4, #8]
  ++buffer_count_;
   e36c2:	6883      	ldr	r3, [r0, #8]
   e36c4:	3301      	adds	r3, #1
   e36c6:	6083      	str	r3, [r0, #8]
  need_to_calculate_offsets_ = true;
   e36c8:	2301      	movs	r3, #1
   e36ca:	f880 3024 	strb.w	r3, [r0, #36]	; 0x24
  return kTfLiteOk;
   e36ce:	2000      	movs	r0, #0
}
   e36d0:	bd70      	pop	{r4, r5, r6, pc}
   e36d2:	bf00      	nop
   e36d4:	000eb041 	.word	0x000eb041

000e36d8 <_ZN6tflite18ReverseSortInPlaceEPiS0_i>:
namespace tflite {

// Simple stable in-place sort function. Not time-efficient for large arrays.
// Would normally be in an anonymous namespace to keep it private, but we want
// to be able to test it externally.
void ReverseSortInPlace(int* values, int* ids, int size) {
   e36d8:	b5f0      	push	{r4, r5, r6, r7, lr}
   e36da:	4696      	mov	lr, r2
   e36dc:	4604      	mov	r4, r0
   e36de:	460b      	mov	r3, r1
  bool any_swapped;
  do {
    any_swapped = false;
    for (int i = 1; i < size; ++i) {
   e36e0:	2501      	movs	r5, #1
// Would normally be in an anonymous namespace to keep it private, but we want
// to be able to test it externally.
void ReverseSortInPlace(int* values, int* ids, int size) {
  bool any_swapped;
  do {
    any_swapped = false;
   e36e2:	2600      	movs	r6, #0
    for (int i = 1; i < size; ++i) {
   e36e4:	4575      	cmp	r5, lr
   e36e6:	da0f      	bge.n	e3708 <_ZN6tflite18ReverseSortInPlaceEPiS0_i+0x30>
      if (values[i - 1] < values[i]) {
   e36e8:	6827      	ldr	r7, [r4, #0]
   e36ea:	f854 2f04 	ldr.w	r2, [r4, #4]!
   e36ee:	4297      	cmp	r7, r2
   e36f0:	da07      	bge.n	e3702 <_ZN6tflite18ReverseSortInPlaceEPiS0_i+0x2a>
        const int value_temp = values[i - 1];
        values[i - 1] = values[i];
   e36f2:	f844 2c04 	str.w	r2, [r4, #-4]
        values[i] = value_temp;
   e36f6:	6027      	str	r7, [r4, #0]
        const int id_temp = ids[i - 1];
        ids[i - 1] = ids[i];
   e36f8:	e893 0044 	ldmia.w	r3, {r2, r6}
   e36fc:	601e      	str	r6, [r3, #0]
        ids[i] = id_temp;
   e36fe:	605a      	str	r2, [r3, #4]
        any_swapped = true;
   e3700:	2601      	movs	r6, #1
// to be able to test it externally.
void ReverseSortInPlace(int* values, int* ids, int size) {
  bool any_swapped;
  do {
    any_swapped = false;
    for (int i = 1; i < size; ++i) {
   e3702:	3501      	adds	r5, #1
   e3704:	3304      	adds	r3, #4
   e3706:	e7ed      	b.n	e36e4 <_ZN6tflite18ReverseSortInPlaceEPiS0_i+0xc>
// Simple stable in-place sort function. Not time-efficient for large arrays.
// Would normally be in an anonymous namespace to keep it private, but we want
// to be able to test it externally.
void ReverseSortInPlace(int* values, int* ids, int size) {
  bool any_swapped;
  do {
   e3708:	2e00      	cmp	r6, #0
   e370a:	d1e7      	bne.n	e36dc <_ZN6tflite18ReverseSortInPlaceEPiS0_i+0x4>
        ids[i] = id_temp;
        any_swapped = true;
      }
    }
  } while (any_swapped);
}
   e370c:	bdf0      	pop	{r4, r5, r6, r7, pc}
	...

000e3710 <_ZN6tflite19GreedyMemoryPlannerC1EPhi>:

GreedyMemoryPlanner::GreedyMemoryPlanner(unsigned char* scratch_buffer,
                                         int scratch_buffer_size)
    : buffer_count_(0), need_to_calculate_offsets_(true) {
   e3710:	4b0c      	ldr	r3, [pc, #48]	; (e3744 <_ZN6tflite19GreedyMemoryPlannerC1EPhi+0x34>)
   e3712:	6003      	str	r3, [r0, #0]
   e3714:	2300      	movs	r3, #0
   e3716:	6083      	str	r3, [r0, #8]
   e3718:	2301      	movs	r3, #1
   e371a:	f880 3024 	strb.w	r3, [r0, #36]	; 0x24
                              sizeof(int) +  // buffer_sizes_sorted_by_size_
                              sizeof(int) +  // buffer_ids_sorted_by_size_
                              sizeof(ListEntry) +  // buffers_sorted_by_offset_
                              sizeof(int);         // buffer_offsets_;
  // Allocate the arrays we need within the scratch buffer arena.
  max_buffer_count_ = scratch_buffer_size / per_buffer_size;
   e371e:	2324      	movs	r3, #36	; 0x24
   e3720:	fb92 f2f3 	sdiv	r2, r2, r3

  unsigned char* next_free = scratch_buffer;
  requirements_ = reinterpret_cast<BufferRequirements*>(next_free);
  next_free += sizeof(BufferRequirements) * max_buffer_count_;
   e3724:	230c      	movs	r3, #12
   e3726:	4353      	muls	r3, r2
      }
    }
  } while (any_swapped);
}

GreedyMemoryPlanner::GreedyMemoryPlanner(unsigned char* scratch_buffer,
   e3728:	b510      	push	{r4, lr}
                              sizeof(int) +  // buffer_sizes_sorted_by_size_
                              sizeof(int) +  // buffer_ids_sorted_by_size_
                              sizeof(ListEntry) +  // buffers_sorted_by_offset_
                              sizeof(int);         // buffer_offsets_;
  // Allocate the arrays we need within the scratch buffer arena.
  max_buffer_count_ = scratch_buffer_size / per_buffer_size;
   e372a:	6042      	str	r2, [r0, #4]

  unsigned char* next_free = scratch_buffer;
  requirements_ = reinterpret_cast<BufferRequirements*>(next_free);
   e372c:	60c1      	str	r1, [r0, #12]
  next_free += sizeof(BufferRequirements) * max_buffer_count_;

  buffer_sizes_sorted_by_size_ = reinterpret_cast<int*>(next_free);
  next_free += sizeof(int) * max_buffer_count_;
   e372e:	0092      	lsls	r2, r2, #2
  // Allocate the arrays we need within the scratch buffer arena.
  max_buffer_count_ = scratch_buffer_size / per_buffer_size;

  unsigned char* next_free = scratch_buffer;
  requirements_ = reinterpret_cast<BufferRequirements*>(next_free);
  next_free += sizeof(BufferRequirements) * max_buffer_count_;
   e3730:	4419      	add	r1, r3

  buffer_sizes_sorted_by_size_ = reinterpret_cast<int*>(next_free);
   e3732:	6101      	str	r1, [r0, #16]
  next_free += sizeof(int) * max_buffer_count_;
   e3734:	4411      	add	r1, r2

  buffer_ids_sorted_by_size_ = reinterpret_cast<int*>(next_free);
  next_free += sizeof(int) * max_buffer_count_;
   e3736:	440a      	add	r2, r1

  buffers_sorted_by_offset_ = reinterpret_cast<ListEntry*>(next_free);
   e3738:	6182      	str	r2, [r0, #24]
  next_free += sizeof(ListEntry) * max_buffer_count_;

  buffer_offsets_ = reinterpret_cast<int*>(next_free);
   e373a:	441a      	add	r2, r3
  next_free += sizeof(BufferRequirements) * max_buffer_count_;

  buffer_sizes_sorted_by_size_ = reinterpret_cast<int*>(next_free);
  next_free += sizeof(int) * max_buffer_count_;

  buffer_ids_sorted_by_size_ = reinterpret_cast<int*>(next_free);
   e373c:	6141      	str	r1, [r0, #20]
  next_free += sizeof(int) * max_buffer_count_;

  buffers_sorted_by_offset_ = reinterpret_cast<ListEntry*>(next_free);
  next_free += sizeof(ListEntry) * max_buffer_count_;

  buffer_offsets_ = reinterpret_cast<int*>(next_free);
   e373e:	6202      	str	r2, [r0, #32]
}
   e3740:	bd10      	pop	{r4, pc}
   e3742:	bf00      	nop
   e3744:	000eb0c4 	.word	0x000eb0c4

000e3748 <_ZNK6tflite19GreedyMemoryPlanner22DoesEntryOverlapInTimeEPKNS0_9ListEntryEii>:
  return kTfLiteOk;
}

bool GreedyMemoryPlanner::DoesEntryOverlapInTime(
    const GreedyMemoryPlanner::ListEntry* entry, const int first_time_used,
    const int last_time_used) const {
   e3748:	b510      	push	{r4, lr}
  const BufferRequirements* entry_requirements =
      &requirements_[entry->requirements_index];
   e374a:	684c      	ldr	r4, [r1, #4]
   e374c:	68c1      	ldr	r1, [r0, #12]
   e374e:	200c      	movs	r0, #12
   e3750:	fb00 1104 	mla	r1, r0, r4, r1
  if (entry_requirements->first_time_used > last_time_used) {
   e3754:	6848      	ldr	r0, [r1, #4]
   e3756:	4298      	cmp	r0, r3
   e3758:	dc05      	bgt.n	e3766 <_ZNK6tflite19GreedyMemoryPlanner22DoesEntryOverlapInTimeEPKNS0_9ListEntryEii+0x1e>
    return false;
  }
  if (first_time_used > entry_requirements->last_time_used) {
   e375a:	6888      	ldr	r0, [r1, #8]
   e375c:	4290      	cmp	r0, r2
   e375e:	bfb4      	ite	lt
   e3760:	2000      	movlt	r0, #0
   e3762:	2001      	movge	r0, #1
   e3764:	bd10      	pop	{r4, pc}
    const GreedyMemoryPlanner::ListEntry* entry, const int first_time_used,
    const int last_time_used) const {
  const BufferRequirements* entry_requirements =
      &requirements_[entry->requirements_index];
  if (entry_requirements->first_time_used > last_time_used) {
    return false;
   e3766:	2000      	movs	r0, #0
  }
  if (first_time_used > entry_requirements->last_time_used) {
    return false;
  }
  return true;
}
   e3768:	bd10      	pop	{r4, pc}

000e376a <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii>:

GreedyMemoryPlanner::ListEntry*
GreedyMemoryPlanner::NextSimultaneouslyActiveBuffer(
    const GreedyMemoryPlanner::ListEntry* start, const int first_time_used,
    const int last_time_used) {
   e376a:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
   e376e:	4605      	mov	r5, r0
   e3770:	4616      	mov	r6, r2
   e3772:	461f      	mov	r7, r3
  ListEntry* result = nullptr;
  ListEntry* candidate_next_entry;
  if (start == nullptr) {
   e3774:	b919      	cbnz	r1, e377e <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0x14>
    candidate_next_entry = &buffers_sorted_by_offset_[0];
   e3776:	6984      	ldr	r4, [r0, #24]
    }
    if (candidate_next_entry->next_entry_index == -1) {
      break;
    }
    candidate_next_entry =
        &buffers_sorted_by_offset_[candidate_next_entry->next_entry_index];
   e3778:	f04f 080c 	mov.w	r8, #12
   e377c:	e00d      	b.n	e379a <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0x30>
  ListEntry* result = nullptr;
  ListEntry* candidate_next_entry;
  if (start == nullptr) {
    candidate_next_entry = &buffers_sorted_by_offset_[0];
  } else {
    if (start->next_entry_index == -1) {
   e377e:	688b      	ldr	r3, [r1, #8]
   e3780:	1c59      	adds	r1, r3, #1
   e3782:	d013      	beq.n	e37ac <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0x42>
      return nullptr;
    }
    candidate_next_entry = &buffers_sorted_by_offset_[start->next_entry_index];
   e3784:	6982      	ldr	r2, [r0, #24]
   e3786:	240c      	movs	r4, #12
   e3788:	fb04 2403 	mla	r4, r4, r3, r2
   e378c:	e7f4      	b.n	e3778 <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0xe>
    if (DoesEntryOverlapInTime(candidate_next_entry, first_time_used,
                               last_time_used)) {
      result = candidate_next_entry;
      break;
    }
    if (candidate_next_entry->next_entry_index == -1) {
   e378e:	68a3      	ldr	r3, [r4, #8]
   e3790:	1c5a      	adds	r2, r3, #1
   e3792:	d00f      	beq.n	e37b4 <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0x4a>
      break;
    }
    candidate_next_entry =
        &buffers_sorted_by_offset_[candidate_next_entry->next_entry_index];
   e3794:	69ac      	ldr	r4, [r5, #24]
   e3796:	fb08 4403 	mla	r4, r8, r3, r4
      return nullptr;
    }
    candidate_next_entry = &buffers_sorted_by_offset_[start->next_entry_index];
  }
  do {
    if (DoesEntryOverlapInTime(candidate_next_entry, first_time_used,
   e379a:	463b      	mov	r3, r7
   e379c:	4632      	mov	r2, r6
   e379e:	4621      	mov	r1, r4
   e37a0:	4628      	mov	r0, r5
   e37a2:	f7ff ffd1 	bl	e3748 <_ZNK6tflite19GreedyMemoryPlanner22DoesEntryOverlapInTimeEPKNS0_9ListEntryEii>
   e37a6:	2800      	cmp	r0, #0
   e37a8:	d0f1      	beq.n	e378e <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0x24>
   e37aa:	e002      	b.n	e37b2 <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii+0x48>
  ListEntry* candidate_next_entry;
  if (start == nullptr) {
    candidate_next_entry = &buffers_sorted_by_offset_[0];
  } else {
    if (start->next_entry_index == -1) {
      return nullptr;
   e37ac:	2000      	movs	r0, #0
   e37ae:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   e37b2:	4620      	mov	r0, r4
    }
    candidate_next_entry =
        &buffers_sorted_by_offset_[candidate_next_entry->next_entry_index];
  } while (true);
  return result;
}
   e37b4:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}

000e37b8 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv>:

void GreedyMemoryPlanner::CalculateOffsetsIfNeeded() {
   e37b8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  if (!need_to_calculate_offsets_ || (buffer_count_ == 0)) {
   e37bc:	f890 3024 	ldrb.w	r3, [r0, #36]	; 0x24
        &buffers_sorted_by_offset_[candidate_next_entry->next_entry_index];
  } while (true);
  return result;
}

void GreedyMemoryPlanner::CalculateOffsetsIfNeeded() {
   e37c0:	b085      	sub	sp, #20
   e37c2:	4604      	mov	r4, r0
  if (!need_to_calculate_offsets_ || (buffer_count_ == 0)) {
   e37c4:	2b00      	cmp	r3, #0
   e37c6:	f000 8089 	beq.w	e38dc <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x124>
   e37ca:	6883      	ldr	r3, [r0, #8]
   e37cc:	2b00      	cmp	r3, #0
   e37ce:	f000 8085 	beq.w	e38dc <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x124>
    return;
  }
  need_to_calculate_offsets_ = false;
   e37d2:	2300      	movs	r3, #0
   e37d4:	f880 3024 	strb.w	r3, [r0, #36]	; 0x24
  // This helps find a more compact layout. Intuitively, you can think
  // about putting the large buffers in place first, and then the
  // smaller buffers can fit in the gaps, rather than fragmenting the
  // gaps with small buffers at the beginning.
  for (int i = 0; i < buffer_count_; ++i) {
    buffer_sizes_sorted_by_size_[i] = requirements_[i].size;
   e37d8:	250c      	movs	r5, #12
    buffer_ids_sorted_by_size_[i] = i;
    buffer_offsets_[i] = -1;
   e37da:	f04f 31ff 	mov.w	r1, #4294967295	; 0xffffffff
  // Start off by ordering the buffers in descending order of size.
  // This helps find a more compact layout. Intuitively, you can think
  // about putting the large buffers in place first, and then the
  // smaller buffers can fit in the gaps, rather than fragmenting the
  // gaps with small buffers at the beginning.
  for (int i = 0; i < buffer_count_; ++i) {
   e37de:	68a2      	ldr	r2, [r4, #8]
   e37e0:	429a      	cmp	r2, r3
   e37e2:	dd0e      	ble.n	e3802 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x4a>
    buffer_sizes_sorted_by_size_[i] = requirements_[i].size;
   e37e4:	68e0      	ldr	r0, [r4, #12]
   e37e6:	fb05 f203 	mul.w	r2, r5, r3
   e37ea:	5880      	ldr	r0, [r0, r2]
   e37ec:	6922      	ldr	r2, [r4, #16]
   e37ee:	f842 0023 	str.w	r0, [r2, r3, lsl #2]
    buffer_ids_sorted_by_size_[i] = i;
   e37f2:	6962      	ldr	r2, [r4, #20]
   e37f4:	f842 3023 	str.w	r3, [r2, r3, lsl #2]
    buffer_offsets_[i] = -1;
   e37f8:	6a22      	ldr	r2, [r4, #32]
   e37fa:	f842 1023 	str.w	r1, [r2, r3, lsl #2]
  // Start off by ordering the buffers in descending order of size.
  // This helps find a more compact layout. Intuitively, you can think
  // about putting the large buffers in place first, and then the
  // smaller buffers can fit in the gaps, rather than fragmenting the
  // gaps with small buffers at the beginning.
  for (int i = 0; i < buffer_count_; ++i) {
   e37fe:	3301      	adds	r3, #1
   e3800:	e7ed      	b.n	e37de <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x26>
    buffer_offsets_[i] = -1;
  }
  // This sorting algorithm is naive, and may end up taking a very long time
  // with hundreds of buffers.
  ReverseSortInPlace(buffer_sizes_sorted_by_size_, buffer_ids_sorted_by_size_,
                     buffer_count_);
   e3802:	6961      	ldr	r1, [r4, #20]
   e3804:	6920      	ldr	r0, [r4, #16]
   e3806:	f7ff ff67 	bl	e36d8 <_ZN6tflite18ReverseSortInPlaceEPiS0_i>

  // Put the largest buffer at offset zero to start the process.
  ListEntry* first_entry = &buffers_sorted_by_offset_[0];
   e380a:	f8d4 8018 	ldr.w	r8, [r4, #24]
  first_entry->offset = 0;
   e380e:	2300      	movs	r3, #0
   e3810:	f8c8 3000 	str.w	r3, [r8]
  first_entry->requirements_index = buffer_ids_sorted_by_size_[0];
   e3814:	6962      	ldr	r2, [r4, #20]
   e3816:	6812      	ldr	r2, [r2, #0]
   e3818:	f8c8 2004 	str.w	r2, [r8, #4]
  first_entry->next_entry_index = -1;
   e381c:	f04f 32ff 	mov.w	r2, #4294967295	; 0xffffffff
   e3820:	f8c8 2008 	str.w	r2, [r8, #8]
  next_free_entry_ = 1;
  buffer_offsets_[buffer_ids_sorted_by_size_[0]] = 0;
   e3824:	6962      	ldr	r2, [r4, #20]
  // Put the largest buffer at offset zero to start the process.
  ListEntry* first_entry = &buffers_sorted_by_offset_[0];
  first_entry->offset = 0;
  first_entry->requirements_index = buffer_ids_sorted_by_size_[0];
  first_entry->next_entry_index = -1;
  next_free_entry_ = 1;
   e3826:	2501      	movs	r5, #1
   e3828:	61e5      	str	r5, [r4, #28]
  buffer_offsets_[buffer_ids_sorted_by_size_[0]] = 0;
   e382a:	6811      	ldr	r1, [r2, #0]
   e382c:	6a22      	ldr	r2, [r4, #32]
   e382e:	f842 3021 	str.w	r3, [r2, r1, lsl #2]
  // Work through the rest of the buffers to find a good gap to place each one.
  for (int i = 1; i < buffer_count_; ++i) {
    // The id is the order the buffer was originally added by the client.
    const int buffer_id = buffer_ids_sorted_by_size_[i];
    // Look at what size and time range the buffer needs to be active.
    BufferRequirements* wanted_requirements = &requirements_[buffer_id];
   e3832:	f04f 090c 	mov.w	r9, #12
  first_entry->next_entry_index = -1;
  next_free_entry_ = 1;
  buffer_offsets_[buffer_ids_sorted_by_size_[0]] = 0;

  // Work through the rest of the buffers to find a good gap to place each one.
  for (int i = 1; i < buffer_count_; ++i) {
   e3836:	68a3      	ldr	r3, [r4, #8]
   e3838:	42ab      	cmp	r3, r5
   e383a:	dd4f      	ble.n	e38dc <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x124>
    // The id is the order the buffer was originally added by the client.
    const int buffer_id = buffer_ids_sorted_by_size_[i];
   e383c:	6963      	ldr	r3, [r4, #20]
    // Look at what size and time range the buffer needs to be active.
    BufferRequirements* wanted_requirements = &requirements_[buffer_id];
   e383e:	f8d4 b00c 	ldr.w	fp, [r4, #12]
  buffer_offsets_[buffer_ids_sorted_by_size_[0]] = 0;

  // Work through the rest of the buffers to find a good gap to place each one.
  for (int i = 1; i < buffer_count_; ++i) {
    // The id is the order the buffer was originally added by the client.
    const int buffer_id = buffer_ids_sorted_by_size_[i];
   e3842:	f853 a025 	ldr.w	sl, [r3, r5, lsl #2]
    // Look at what size and time range the buffer needs to be active.
    BufferRequirements* wanted_requirements = &requirements_[buffer_id];
   e3846:	fb09 f20a 	mul.w	r2, r9, sl
   e384a:	eb0b 0302 	add.w	r3, fp, r2
    const int wanted_size = wanted_requirements->size;
   e384e:	f85b 2002 	ldr.w	r2, [fp, r2]
   e3852:	9201      	str	r2, [sp, #4]
    // buffers are stored in the order of their starting position in the arena
    // so that it's easy to find the next buffer in memory, and so the gap.
    // The candidate_entry variable holds the buffer that we're considering
    // placing the current buffer after.
    ListEntry* prior_entry = nullptr;
    int candidate_offset = 0;
   e3854:	2600      	movs	r6, #0
    // The id is the order the buffer was originally added by the client.
    const int buffer_id = buffer_ids_sorted_by_size_[i];
    // Look at what size and time range the buffer needs to be active.
    BufferRequirements* wanted_requirements = &requirements_[buffer_id];
    const int wanted_size = wanted_requirements->size;
    const int wanted_first_time_used = wanted_requirements->first_time_used;
   e3856:	685a      	ldr	r2, [r3, #4]
    const int wanted_last_time_used = wanted_requirements->last_time_used;
   e3858:	689b      	ldr	r3, [r3, #8]
    // The id is the order the buffer was originally added by the client.
    const int buffer_id = buffer_ids_sorted_by_size_[i];
    // Look at what size and time range the buffer needs to be active.
    BufferRequirements* wanted_requirements = &requirements_[buffer_id];
    const int wanted_size = wanted_requirements->size;
    const int wanted_first_time_used = wanted_requirements->first_time_used;
   e385a:	9202      	str	r2, [sp, #8]
    const int wanted_last_time_used = wanted_requirements->last_time_used;
   e385c:	9303      	str	r3, [sp, #12]
    // Find the first buffer that's active in our time range. All placed
    // buffers are stored in the order of their starting position in the arena
    // so that it's easy to find the next buffer in memory, and so the gap.
    // The candidate_entry variable holds the buffer that we're considering
    // placing the current buffer after.
    ListEntry* prior_entry = nullptr;
   e385e:	4637      	mov	r7, r6
    int candidate_offset = 0;
    // Loop through the offset-ordered list of buffers, looking for gaps.
    while (true) {
      // Find out what the next active buffer is.
      ListEntry* next_entry = NextSimultaneouslyActiveBuffer(
          prior_entry, wanted_first_time_used, wanted_last_time_used);
   e3860:	9b03      	ldr	r3, [sp, #12]
   e3862:	9a02      	ldr	r2, [sp, #8]
   e3864:	4639      	mov	r1, r7
   e3866:	4620      	mov	r0, r4
   e3868:	f7ff ff7f 	bl	e376a <_ZN6tflite19GreedyMemoryPlanner30NextSimultaneouslyActiveBufferEPKNS0_9ListEntryEii>

      if (prior_entry) {
   e386c:	b14f      	cbz	r7, e3882 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0xca>
        BufferRequirements* candidate_requirements =
            &requirements_[prior_entry->requirements_index];
        const int prior_entry_offset =
            prior_entry->offset + candidate_requirements->size;
   e386e:	687b      	ldr	r3, [r7, #4]
   e3870:	fb09 f303 	mul.w	r3, r9, r3
   e3874:	f85b 2003 	ldr.w	r2, [fp, r3]
   e3878:	683b      	ldr	r3, [r7, #0]
   e387a:	4413      	add	r3, r2
   e387c:	429e      	cmp	r6, r3
   e387e:	bfb8      	it	lt
   e3880:	461e      	movlt	r6, r3
        if (prior_entry_offset > candidate_offset) {
          candidate_offset = prior_entry_offset;
        }
      }
      if (next_entry == nullptr) {
   e3882:	b978      	cbnz	r0, e38a4 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0xec>
    }
    // At this point, we've either found a gap (possibly at the end of the
    // list) and want to place the buffer there, or there are no other active
    // buffers in this time range and so we can put it at offset zero.
    // Record the buffer's offset in our plan.
    buffer_offsets_[buffer_id] = candidate_offset;
   e3884:	6a23      	ldr	r3, [r4, #32]
   e3886:	f843 602a 	str.w	r6, [r3, sl, lsl #2]
    // Add the newly-placed buffer to our offset-ordered list, so that
    // subsequent passes can fit in their buffers around it.
    ListEntry* new_entry = &buffers_sorted_by_offset_[next_free_entry_];
   e388a:	69e3      	ldr	r3, [r4, #28]
   e388c:	69a2      	ldr	r2, [r4, #24]
   e388e:	fb09 f303 	mul.w	r3, r9, r3
   e3892:	18d7      	adds	r7, r2, r3
    new_entry->offset = candidate_offset;
   e3894:	50d6      	str	r6, [r2, r3]
    new_entry->requirements_index = buffer_id;
   e3896:	f8c7 a004 	str.w	sl, [r7, #4]
    const int new_entry_index = next_free_entry_;
   e389a:	69e0      	ldr	r0, [r4, #28]
    ++next_free_entry_;
   e389c:	1c43      	adds	r3, r0, #1
   e389e:	61e3      	str	r3, [r4, #28]
   e38a0:	4643      	mov	r3, r8
   e38a2:	e011      	b.n	e38c8 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x110>
        // here.
        break;
      }
      // Find out how much space there is between us and the next buffer.
      const int gap = next_entry->offset - candidate_offset;
      if (gap >= wanted_size) {
   e38a4:	6803      	ldr	r3, [r0, #0]
   e38a6:	9a01      	ldr	r2, [sp, #4]
   e38a8:	1b9b      	subs	r3, r3, r6
   e38aa:	429a      	cmp	r2, r3
   e38ac:	4607      	mov	r7, r0
   e38ae:	dcd7      	bgt.n	e3860 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0xa8>
   e38b0:	e7e8      	b.n	e3884 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0xcc>
        // We're at the end of the list, so just add the new entry here.
        current_entry->next_entry_index = new_entry_index;
        new_entry->next_entry_index = -1;
        break;
      }
      ListEntry* next_entry = &buffers_sorted_by_offset_[next_entry_index];
   e38b2:	fb09 f102 	mul.w	r1, r9, r2
   e38b6:	f8d4 e018 	ldr.w	lr, [r4, #24]
   e38ba:	eb0e 0c01 	add.w	ip, lr, r1
      if (next_entry->offset > candidate_offset) {
   e38be:	f85e 1001 	ldr.w	r1, [lr, r1]
   e38c2:	428e      	cmp	r6, r1
   e38c4:	db06      	blt.n	e38d4 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x11c>
   e38c6:	4663      	mov	r3, ip
    ++next_free_entry_;
    ListEntry* current_entry = first_entry;
    // Make sure that we insert the buffer at the correct place in the ordered
    // list.
    while (true) {
      const int next_entry_index = current_entry->next_entry_index;
   e38c8:	689a      	ldr	r2, [r3, #8]
      if (next_entry_index == -1) {
   e38ca:	1c51      	adds	r1, r2, #1
   e38cc:	d1f1      	bne.n	e38b2 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0xfa>
        // We're at the end of the list, so just add the new entry here.
        current_entry->next_entry_index = new_entry_index;
   e38ce:	6098      	str	r0, [r3, #8]
        new_entry->next_entry_index = -1;
   e38d0:	60ba      	str	r2, [r7, #8]
   e38d2:	e001      	b.n	e38d8 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x120>
      }
      ListEntry* next_entry = &buffers_sorted_by_offset_[next_entry_index];
      if (next_entry->offset > candidate_offset) {
        // We're at the right spot to do an insertion and retain the sorting
        // order, so place the new entry here.
        new_entry->next_entry_index = current_entry->next_entry_index;
   e38d4:	60ba      	str	r2, [r7, #8]
        current_entry->next_entry_index = new_entry_index;
   e38d6:	6098      	str	r0, [r3, #8]
  first_entry->next_entry_index = -1;
  next_free_entry_ = 1;
  buffer_offsets_[buffer_ids_sorted_by_size_[0]] = 0;

  // Work through the rest of the buffers to find a good gap to place each one.
  for (int i = 1; i < buffer_count_; ++i) {
   e38d8:	3501      	adds	r5, #1
   e38da:	e7ac      	b.n	e3836 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv+0x7e>
        break;
      }
      current_entry = next_entry;
    }
  }
}
   e38dc:	b005      	add	sp, #20
   e38de:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}

000e38e2 <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv>:

int GreedyMemoryPlanner::GetMaximumMemorySize() {
   e38e2:	b570      	push	{r4, r5, r6, lr}
   e38e4:	4604      	mov	r4, r0
  CalculateOffsetsIfNeeded();
   e38e6:	f7ff ff67 	bl	e37b8 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv>
  if (buffer_count_ == 0) {
   e38ea:	68a0      	ldr	r0, [r4, #8]
   e38ec:	b198      	cbz	r0, e3916 <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv+0x34>
    return 0;
  }
  ListEntry* entry = &buffers_sorted_by_offset_[0];
   e38ee:	69a1      	ldr	r1, [r4, #24]
  int max_size = 0;
   e38f0:	2000      	movs	r0, #0
int GreedyMemoryPlanner::GetMaximumMemorySize() {
  CalculateOffsetsIfNeeded();
  if (buffer_count_ == 0) {
    return 0;
  }
  ListEntry* entry = &buffers_sorted_by_offset_[0];
   e38f2:	460b      	mov	r3, r1
  int max_size = 0;
  while (entry) {
    BufferRequirements* requirements =
        &requirements_[entry->requirements_index];
    const int current_size = entry->offset + requirements->size;
   e38f4:	250c      	movs	r5, #12
  if (buffer_count_ == 0) {
    return 0;
  }
  ListEntry* entry = &buffers_sorted_by_offset_[0];
  int max_size = 0;
  while (entry) {
   e38f6:	b173      	cbz	r3, e3916 <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv+0x34>
    BufferRequirements* requirements =
        &requirements_[entry->requirements_index];
    const int current_size = entry->offset + requirements->size;
   e38f8:	685a      	ldr	r2, [r3, #4]
   e38fa:	68e6      	ldr	r6, [r4, #12]
   e38fc:	436a      	muls	r2, r5
   e38fe:	58b6      	ldr	r6, [r6, r2]
   e3900:	681a      	ldr	r2, [r3, #0]
    if (current_size > max_size) {
      max_size = current_size;
    }
    if (entry->next_entry_index == -1) {
   e3902:	689b      	ldr	r3, [r3, #8]
  ListEntry* entry = &buffers_sorted_by_offset_[0];
  int max_size = 0;
  while (entry) {
    BufferRequirements* requirements =
        &requirements_[entry->requirements_index];
    const int current_size = entry->offset + requirements->size;
   e3904:	4432      	add	r2, r6
   e3906:	4290      	cmp	r0, r2
   e3908:	bfb8      	it	lt
   e390a:	4610      	movlt	r0, r2
    if (current_size > max_size) {
      max_size = current_size;
    }
    if (entry->next_entry_index == -1) {
   e390c:	1c5a      	adds	r2, r3, #1
   e390e:	d002      	beq.n	e3916 <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv+0x34>
      break;
    }
    entry = &buffers_sorted_by_offset_[entry->next_entry_index];
   e3910:	fb05 1303 	mla	r3, r5, r3, r1
   e3914:	e7ef      	b.n	e38f6 <_ZN6tflite19GreedyMemoryPlanner20GetMaximumMemorySizeEv+0x14>
  }
  return max_size;
}
   e3916:	bd70      	pop	{r4, r5, r6, pc}

000e3918 <_ZN6tflite19GreedyMemoryPlanner18GetOffsetForBufferEPNS_13ErrorReporterEiPi>:
}

int GreedyMemoryPlanner::GetBufferCount() { return buffer_count_; }

TfLiteStatus GreedyMemoryPlanner::GetOffsetForBuffer(
    tflite::ErrorReporter* error_reporter, int buffer_index, int* offset) {
   e3918:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
   e391a:	4614      	mov	r4, r2
   e391c:	4605      	mov	r5, r0
   e391e:	460f      	mov	r7, r1
   e3920:	461e      	mov	r6, r3
  CalculateOffsetsIfNeeded();
   e3922:	f7ff ff49 	bl	e37b8 <_ZN6tflite19GreedyMemoryPlanner24CalculateOffsetsIfNeededEv>
  if ((buffer_index < 0) || (buffer_index >= buffer_count_)) {
   e3926:	2c00      	cmp	r4, #0
   e3928:	db02      	blt.n	e3930 <_ZN6tflite19GreedyMemoryPlanner18GetOffsetForBufferEPNS_13ErrorReporterEiPi+0x18>
   e392a:	68ab      	ldr	r3, [r5, #8]
   e392c:	429c      	cmp	r4, r3
   e392e:	db07      	blt.n	e3940 <_ZN6tflite19GreedyMemoryPlanner18GetOffsetForBufferEPNS_13ErrorReporterEiPi+0x28>
    error_reporter->Report("buffer index %d is outside range 0 to %d",
                           buffer_index, buffer_count_);
   e3930:	68ab      	ldr	r3, [r5, #8]
   e3932:	4906      	ldr	r1, [pc, #24]	; (e394c <_ZN6tflite19GreedyMemoryPlanner18GetOffsetForBufferEPNS_13ErrorReporterEiPi+0x34>)
   e3934:	4622      	mov	r2, r4
   e3936:	4638      	mov	r0, r7
   e3938:	f7f0 fd4c 	bl	d43d4 <_ZN6tflite13ErrorReporter6ReportEPKcz>
   e393c:	2001      	movs	r0, #1
   e393e:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
    return kTfLiteError;
  }
  *offset = buffer_offsets_[buffer_index];
   e3940:	6a2b      	ldr	r3, [r5, #32]
   e3942:	f853 3024 	ldr.w	r3, [r3, r4, lsl #2]
   e3946:	6033      	str	r3, [r6, #0]
  return kTfLiteOk;
   e3948:	2000      	movs	r0, #0
}
   e394a:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
   e394c:	000eb05e 	.word	0x000eb05e

000e3950 <_ZZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_ENKUlfE_clEf>:
                                           TfLiteTensor* output,
                                           int32_t* act_min, int32_t* act_max) {
  const auto scale = output->params.scale;
  const auto zero_point = output->params.zero_point;

  auto quantize = [scale, zero_point](float f) {
   e3950:	b510      	push	{r4, lr}
    { return __builtin_rint(__x); }

#ifndef __CORRECT_ISO_CPP11_MATH_H_PROTO
  constexpr float
  round(float __x)
  { return __builtin_roundf(__x); }
   e3952:	edd0 7a00 	vldr	s15, [r0]
   e3956:	ee80 0a27 	vdiv.f32	s0, s0, s15
   e395a:	4604      	mov	r4, r0
   e395c:	f001 fde8 	bl	e5530 <roundf>
    return zero_point + static_cast<int32_t>(TfLiteRound(f / scale));
  };
   e3960:	6863      	ldr	r3, [r4, #4]
   e3962:	eefd 7ac0 	vcvt.s32.f32	s15, s0
   e3966:	ee17 0a90 	vmov	r0, s15
   e396a:	4418      	add	r0, r3
   e396c:	bd10      	pop	{r4, pc}
	...

000e3970 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_>:

namespace {
void CalculateActivationRangeQuantizedImpl(TfLiteFusedActivation activation,
                                           int32_t qmin, int32_t qmax,
                                           TfLiteTensor* output,
                                           int32_t* act_min, int32_t* act_max) {
   e3970:	b5f7      	push	{r0, r1, r2, r4, r5, r6, r7, lr}

  auto quantize = [scale, zero_point](float f) {
    return zero_point + static_cast<int32_t>(TfLiteRound(f / scale));
  };

  if (activation == kTfLiteActRelu) {
   e3972:	2801      	cmp	r0, #1

namespace {
void CalculateActivationRangeQuantizedImpl(TfLiteFusedActivation activation,
                                           int32_t qmin, int32_t qmax,
                                           TfLiteTensor* output,
                                           int32_t* act_min, int32_t* act_max) {
   e3974:	4615      	mov	r5, r2
  const auto scale = output->params.scale;
  const auto zero_point = output->params.zero_point;
   e3976:	691a      	ldr	r2, [r3, #16]

  auto quantize = [scale, zero_point](float f) {
    return zero_point + static_cast<int32_t>(TfLiteRound(f / scale));
  };
   e3978:	68db      	ldr	r3, [r3, #12]

namespace {
void CalculateActivationRangeQuantizedImpl(TfLiteFusedActivation activation,
                                           int32_t qmin, int32_t qmax,
                                           TfLiteTensor* output,
                                           int32_t* act_min, int32_t* act_max) {
   e397a:	9e08      	ldr	r6, [sp, #32]
   e397c:	9c09      	ldr	r4, [sp, #36]	; 0x24
  const auto scale = output->params.scale;
  const auto zero_point = output->params.zero_point;

  auto quantize = [scale, zero_point](float f) {
    return zero_point + static_cast<int32_t>(TfLiteRound(f / scale));
  };
   e397e:	9300      	str	r3, [sp, #0]

namespace {
void CalculateActivationRangeQuantizedImpl(TfLiteFusedActivation activation,
                                           int32_t qmin, int32_t qmax,
                                           TfLiteTensor* output,
                                           int32_t* act_min, int32_t* act_max) {
   e3980:	460f      	mov	r7, r1
  const auto scale = output->params.scale;
  const auto zero_point = output->params.zero_point;

  auto quantize = [scale, zero_point](float f) {
    return zero_point + static_cast<int32_t>(TfLiteRound(f / scale));
  };
   e3982:	9201      	str	r2, [sp, #4]

  if (activation == kTfLiteActRelu) {
   e3984:	d109      	bne.n	e399a <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x2a>
    *act_min = std::max(qmin, quantize(0.0));
   e3986:	ed9f 0a18 	vldr	s0, [pc, #96]	; e39e8 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x78>
   e398a:	4668      	mov	r0, sp
   e398c:	f7ff ffe0 	bl	e3950 <_ZZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_ENKUlfE_clEf>
   e3990:	42b8      	cmp	r0, r7
   e3992:	bfac      	ite	ge
   e3994:	6030      	strge	r0, [r6, #0]
   e3996:	6037      	strlt	r7, [r6, #0]
   e3998:	e023      	b.n	e39e2 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x72>
    *act_max = qmax;
  } else if (activation == kTfLiteActRelu6) {
   e399a:	2803      	cmp	r0, #3
   e399c:	d10b      	bne.n	e39b6 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x46>
    *act_min = std::max(qmin, quantize(0.0));
   e399e:	ed9f 0a12 	vldr	s0, [pc, #72]	; e39e8 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x78>
   e39a2:	4668      	mov	r0, sp
   e39a4:	f7ff ffd4 	bl	e3950 <_ZZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_ENKUlfE_clEf>
    *act_max = std::min(qmax, quantize(6.0));
   e39a8:	eeb1 0a08 	vmov.f32	s0, #24	; 0x40c00000  6.0

  if (activation == kTfLiteActRelu) {
    *act_min = std::max(qmin, quantize(0.0));
    *act_max = qmax;
  } else if (activation == kTfLiteActRelu6) {
    *act_min = std::max(qmin, quantize(0.0));
   e39ac:	42b8      	cmp	r0, r7
   e39ae:	bfac      	ite	ge
   e39b0:	6030      	strge	r0, [r6, #0]
   e39b2:	6037      	strlt	r7, [r6, #0]
   e39b4:	e00c      	b.n	e39d0 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x60>
    *act_max = std::min(qmax, quantize(6.0));
  } else if (activation == kTfLiteActRelu1) {
   e39b6:	2802      	cmp	r0, #2
   e39b8:	d112      	bne.n	e39e0 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x70>
    *act_min = std::max(qmin, quantize(-1.0));
   e39ba:	eebf 0a00 	vmov.f32	s0, #240	; 0xbf800000 -1.0
   e39be:	4668      	mov	r0, sp
   e39c0:	f7ff ffc6 	bl	e3950 <_ZZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_ENKUlfE_clEf>
    *act_max = std::min(qmax, quantize(1.0));
   e39c4:	eeb7 0a00 	vmov.f32	s0, #112	; 0x3f800000  1.0
    *act_max = qmax;
  } else if (activation == kTfLiteActRelu6) {
    *act_min = std::max(qmin, quantize(0.0));
    *act_max = std::min(qmax, quantize(6.0));
  } else if (activation == kTfLiteActRelu1) {
    *act_min = std::max(qmin, quantize(-1.0));
   e39c8:	42b8      	cmp	r0, r7
   e39ca:	bfac      	ite	ge
   e39cc:	6030      	strge	r0, [r6, #0]
   e39ce:	6037      	strlt	r7, [r6, #0]
    *act_max = std::min(qmax, quantize(1.0));
   e39d0:	4668      	mov	r0, sp
   e39d2:	f7ff ffbd 	bl	e3950 <_ZZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_ENKUlfE_clEf>
   e39d6:	4285      	cmp	r5, r0
   e39d8:	bfd4      	ite	le
   e39da:	6025      	strle	r5, [r4, #0]
   e39dc:	6020      	strgt	r0, [r4, #0]
   e39de:	e001      	b.n	e39e4 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_+0x74>
  } else {
    *act_min = qmin;
   e39e0:	6031      	str	r1, [r6, #0]
    *act_max = qmax;
   e39e2:	6025      	str	r5, [r4, #0]
  }
}
   e39e4:	b003      	add	sp, #12
   e39e6:	bdf0      	pop	{r4, r5, r6, r7, pc}
   e39e8:	00000000 	.word	0x00000000

000e39ec <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd>:

TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              TfLiteTensor* output,
                                              double* multiplier) {
   e39ec:	b5f7      	push	{r0, r1, r2, r4, r5, r6, r7, lr}
  const double input_product_scale = input->params.scale * filter->params.scale;
   e39ee:	edd2 7a03 	vldr	s15, [r2, #12]
   e39f2:	ed91 7a03 	vldr	s14, [r1, #12]
   e39f6:	ee67 7a27 	vmul.f32	s15, s14, s15

TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              TfLiteTensor* output,
                                              double* multiplier) {
   e39fa:	4604      	mov	r4, r0
  const double input_product_scale = input->params.scale * filter->params.scale;
   e39fc:	ee17 0a90 	vmov	r0, s15

TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              TfLiteTensor* output,
                                              double* multiplier) {
   e3a00:	461d      	mov	r5, r3
  const double input_product_scale = input->params.scale * filter->params.scale;
   e3a02:	f003 fae3 	bl	e6fcc <__aeabi_f2d>
  TF_LITE_ENSURE(context, input_product_scale >= 0);
   e3a06:	2200      	movs	r2, #0
   e3a08:	2300      	movs	r3, #0
TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              TfLiteTensor* output,
                                              double* multiplier) {
  const double input_product_scale = input->params.scale * filter->params.scale;
   e3a0a:	4606      	mov	r6, r0
   e3a0c:	460f      	mov	r7, r1
  TF_LITE_ENSURE(context, input_product_scale >= 0);
   e3a0e:	f003 fdb7 	bl	e7580 <__aeabi_dcmpge>
   e3a12:	b948      	cbnz	r0, e3a28 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd+0x3c>
   e3a14:	4b0c      	ldr	r3, [pc, #48]	; (e3a48 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd+0x5c>)
   e3a16:	9300      	str	r3, [sp, #0]
   e3a18:	4620      	mov	r0, r4
   e3a1a:	6965      	ldr	r5, [r4, #20]
   e3a1c:	4a0b      	ldr	r2, [pc, #44]	; (e3a4c <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd+0x60>)
   e3a1e:	490c      	ldr	r1, [pc, #48]	; (e3a50 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd+0x64>)
   e3a20:	2376      	movs	r3, #118	; 0x76
   e3a22:	47a8      	blx	r5
   e3a24:	2001      	movs	r0, #1
   e3a26:	e00c      	b.n	e3a42 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd+0x56>
  *multiplier = input_product_scale / output->params.scale;
   e3a28:	68e8      	ldr	r0, [r5, #12]
   e3a2a:	f003 facf 	bl	e6fcc <__aeabi_f2d>
   e3a2e:	460b      	mov	r3, r1
   e3a30:	4602      	mov	r2, r0
   e3a32:	4639      	mov	r1, r7
   e3a34:	4630      	mov	r0, r6
   e3a36:	f003 fc47 	bl	e72c8 <__aeabi_ddiv>
   e3a3a:	9b08      	ldr	r3, [sp, #32]
   e3a3c:	e9c3 0100 	strd	r0, r1, [r3]

  return kTfLiteOk;
   e3a40:	2000      	movs	r0, #0
}
   e3a42:	b003      	add	sp, #12
   e3a44:	bdf0      	pop	{r4, r5, r6, r7, pc}
   e3a46:	bf00      	nop
   e3a48:	000eb176 	.word	0x000eb176
   e3a4c:	000eb0dc 	.word	0x000eb0dc
   e3a50:	000e9a98 	.word	0x000e9a98
   e3a54:	00000000 	.word	0x00000000

000e3a58 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd>:
TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              const TfLiteTensor* bias,
                                              TfLiteTensor* output,
                                              double* multiplier) {
   e3a58:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  const double input_product_scale = input->params.scale * filter->params.scale;
   e3a5c:	ed91 7a03 	vldr	s14, [r1, #12]
   e3a60:	edd2 7a03 	vldr	s15, [r2, #12]
TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              const TfLiteTensor* bias,
                                              TfLiteTensor* output,
                                              double* multiplier) {
   e3a64:	b087      	sub	sp, #28
  const double input_product_scale = input->params.scale * filter->params.scale;
   e3a66:	ee67 7a27 	vmul.f32	s15, s14, s15
TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              const TfLiteTensor* bias,
                                              TfLiteTensor* output,
                                              double* multiplier) {
   e3a6a:	4604      	mov	r4, r0
   e3a6c:	461e      	mov	r6, r3
  const double input_product_scale = input->params.scale * filter->params.scale;
   e3a6e:	ee17 0a90 	vmov	r0, s15
TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              const TfLiteTensor* bias,
                                              TfLiteTensor* output,
                                              double* multiplier) {
   e3a72:	9b11      	ldr	r3, [sp, #68]	; 0x44
   e3a74:	9305      	str	r3, [sp, #20]
   e3a76:	460d      	mov	r5, r1
   e3a78:	4690      	mov	r8, r2
  const double input_product_scale = input->params.scale * filter->params.scale;
   e3a7a:	f003 faa7 	bl	e6fcc <__aeabi_f2d>
TfLiteStatus GetQuantizedConvolutionMultipler(TfLiteContext* context,
                                              const TfLiteTensor* input,
                                              const TfLiteTensor* filter,
                                              const TfLiteTensor* bias,
                                              TfLiteTensor* output,
                                              double* multiplier) {
   e3a7e:	f8dd 9040 	ldr.w	r9, [sp, #64]	; 0x40
  const double input_product_scale = input->params.scale * filter->params.scale;
   e3a82:	4682      	mov	sl, r0
   e3a84:	468b      	mov	fp, r1
  // TODO(ahentz): The following conditions must be guaranteed by the training
  // pipeline.
  if (bias) {
   e3a86:	b316      	cbz	r6, e3ace <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0x76>
    const double bias_scale = bias->params.scale;
   e3a88:	68f0      	ldr	r0, [r6, #12]
   e3a8a:	f003 fa9f 	bl	e6fcc <__aeabi_f2d>
   e3a8e:	e9cd 0102 	strd	r0, r1, [sp, #8]
_GLIBCXX_BEGIN_NAMESPACE_VERSION

#ifndef __CORRECT_ISO_CPP_MATH_H_PROTO
  inline _GLIBCXX_CONSTEXPR double
  abs(double __x)
  { return __builtin_fabs(__x); }
   e3a92:	4602      	mov	r2, r0
   e3a94:	460b      	mov	r3, r1
   e3a96:	4650      	mov	r0, sl
   e3a98:	4659      	mov	r1, fp
   e3a9a:	f003 f937 	bl	e6d0c <__aeabi_dsub>
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
   e3a9e:	e9dd 2302 	ldrd	r2, r3, [sp, #8]
   e3aa2:	4606      	mov	r6, r0
   e3aa4:	f021 4700 	bic.w	r7, r1, #2147483648	; 0x80000000
   e3aa8:	4650      	mov	r0, sl
   e3aaa:	4659      	mov	r1, fp
   e3aac:	f003 fd72 	bl	e7594 <__aeabi_dcmpgt>
   e3ab0:	b108      	cbz	r0, e3ab6 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0x5e>
	return __b;
   e3ab2:	e9dd ab02 	ldrd	sl, fp, [sp, #8]
    TF_LITE_ENSURE(context,
   e3ab6:	a315      	add	r3, pc, #84	; (adr r3, e3b0c <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0xb4>)
   e3ab8:	e9d3 2300 	ldrd	r2, r3, [r3]
   e3abc:	4650      	mov	r0, sl
   e3abe:	4659      	mov	r1, fp
   e3ac0:	f003 fad8 	bl	e7074 <__aeabi_dmul>
   e3ac4:	4632      	mov	r2, r6
   e3ac6:	463b      	mov	r3, r7
   e3ac8:	f003 fd5a 	bl	e7580 <__aeabi_dcmpge>
   e3acc:	b150      	cbz	r0, e3ae4 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0x8c>
                   std::abs(input_product_scale - bias_scale) <=
                       1e-6 * std::min(input_product_scale, bias_scale));
  }
  return GetQuantizedConvolutionMultipler(context, input, filter, output,
                                          multiplier);
   e3ace:	9b05      	ldr	r3, [sp, #20]
   e3ad0:	9310      	str	r3, [sp, #64]	; 0x40
   e3ad2:	4642      	mov	r2, r8
   e3ad4:	464b      	mov	r3, r9
   e3ad6:	4629      	mov	r1, r5
   e3ad8:	4620      	mov	r0, r4
}
   e3ada:	b007      	add	sp, #28
   e3adc:	e8bd 4ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
    TF_LITE_ENSURE(context,
                   std::abs(input_product_scale - bias_scale) <=
                       1e-6 * std::min(input_product_scale, bias_scale));
  }
  return GetQuantizedConvolutionMultipler(context, input, filter, output,
                                          multiplier);
   e3ae0:	f7ff bf84 	b.w	e39ec <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_PS2_Pd>
  const double input_product_scale = input->params.scale * filter->params.scale;
  // TODO(ahentz): The following conditions must be guaranteed by the training
  // pipeline.
  if (bias) {
    const double bias_scale = bias->params.scale;
    TF_LITE_ENSURE(context,
   e3ae4:	4b06      	ldr	r3, [pc, #24]	; (e3b00 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0xa8>)
   e3ae6:	9300      	str	r3, [sp, #0]
   e3ae8:	4620      	mov	r0, r4
   e3aea:	6965      	ldr	r5, [r4, #20]
   e3aec:	4a05      	ldr	r2, [pc, #20]	; (e3b04 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0xac>)
   e3aee:	4906      	ldr	r1, [pc, #24]	; (e3b08 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd+0xb0>)
   e3af0:	236a      	movs	r3, #106	; 0x6a
   e3af2:	47a8      	blx	r5
                   std::abs(input_product_scale - bias_scale) <=
                       1e-6 * std::min(input_product_scale, bias_scale));
  }
  return GetQuantizedConvolutionMultipler(context, input, filter, output,
                                          multiplier);
}
   e3af4:	2001      	movs	r0, #1
   e3af6:	b007      	add	sp, #28
   e3af8:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e3afc:	f3af 8000 	nop.w
   e3b00:	000eb18f 	.word	0x000eb18f
   e3b04:	000eb0dc 	.word	0x000eb0dc
   e3b08:	000e9a98 	.word	0x000e9a98
   e3b0c:	a0b5ed8d 	.word	0xa0b5ed8d
   e3b10:	3eb0c6f7 	.word	0x3eb0c6f7

000e3b14 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_>:

TfLiteStatus CalculateActivationRangeQuantized(TfLiteContext* context,
                                               TfLiteFusedActivation activation,
                                               TfLiteTensor* output,
                                               int32_t* act_min,
                                               int32_t* act_max) {
   e3b14:	b573      	push	{r0, r1, r4, r5, r6, lr}
   e3b16:	460d      	mov	r5, r1
  int32_t qmin = 0;
  int32_t qmax = 0;
  if (output->type == kTfLiteUInt8) {
   e3b18:	7811      	ldrb	r1, [r2, #0]
   e3b1a:	2903      	cmp	r1, #3

TfLiteStatus CalculateActivationRangeQuantized(TfLiteContext* context,
                                               TfLiteFusedActivation activation,
                                               TfLiteTensor* output,
                                               int32_t* act_min,
                                               int32_t* act_max) {
   e3b1c:	4614      	mov	r4, r2
  int32_t qmin = 0;
  int32_t qmax = 0;
  if (output->type == kTfLiteUInt8) {
   e3b1e:	d00c      	beq.n	e3b3a <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x26>
    qmin = std::numeric_limits<uint8_t>::min();
    qmax = std::numeric_limits<uint8_t>::max();
  } else if (output->type == kTfLiteInt8) {
   e3b20:	2909      	cmp	r1, #9
   e3b22:	d00d      	beq.n	e3b40 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x2c>
    qmin = std::numeric_limits<int8_t>::min();
    qmax = std::numeric_limits<int8_t>::max();
  } else if (output->type == kTfLiteInt16) {
   e3b24:	2907      	cmp	r1, #7
   e3b26:	d00f      	beq.n	e3b48 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x34>
    qmin = std::numeric_limits<int16_t>::min();
    qmax = std::numeric_limits<int16_t>::max();
  } else {
    TF_LITE_ENSURE(context, false);
   e3b28:	4b0e      	ldr	r3, [pc, #56]	; (e3b64 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x50>)
   e3b2a:	9300      	str	r3, [sp, #0]
   e3b2c:	6944      	ldr	r4, [r0, #20]
   e3b2e:	4a0e      	ldr	r2, [pc, #56]	; (e3b68 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x54>)
   e3b30:	490e      	ldr	r1, [pc, #56]	; (e3b6c <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x58>)
   e3b32:	23a9      	movs	r3, #169	; 0xa9
   e3b34:	47a0      	blx	r4
   e3b36:	2001      	movs	r0, #1
   e3b38:	e011      	b.n	e3b5e <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x4a>
                                               int32_t* act_max) {
  int32_t qmin = 0;
  int32_t qmax = 0;
  if (output->type == kTfLiteUInt8) {
    qmin = std::numeric_limits<uint8_t>::min();
    qmax = std::numeric_limits<uint8_t>::max();
   e3b3a:	22ff      	movs	r2, #255	; 0xff
                                               int32_t* act_min,
                                               int32_t* act_max) {
  int32_t qmin = 0;
  int32_t qmax = 0;
  if (output->type == kTfLiteUInt8) {
    qmin = std::numeric_limits<uint8_t>::min();
   e3b3c:	2100      	movs	r1, #0
   e3b3e:	e006      	b.n	e3b4e <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x3a>
    qmax = std::numeric_limits<uint8_t>::max();
  } else if (output->type == kTfLiteInt8) {
    qmin = std::numeric_limits<int8_t>::min();
    qmax = std::numeric_limits<int8_t>::max();
   e3b40:	227f      	movs	r2, #127	; 0x7f
  int32_t qmax = 0;
  if (output->type == kTfLiteUInt8) {
    qmin = std::numeric_limits<uint8_t>::min();
    qmax = std::numeric_limits<uint8_t>::max();
  } else if (output->type == kTfLiteInt8) {
    qmin = std::numeric_limits<int8_t>::min();
   e3b42:	f06f 017f 	mvn.w	r1, #127	; 0x7f
   e3b46:	e002      	b.n	e3b4e <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x3a>
    qmax = std::numeric_limits<int8_t>::max();
  } else if (output->type == kTfLiteInt16) {
    qmin = std::numeric_limits<int16_t>::min();
   e3b48:	4909      	ldr	r1, [pc, #36]	; (e3b70 <_ZN6tflite33CalculateActivationRangeQuantizedEP13TfLiteContext21TfLiteFusedActivationP12TfLiteTensorPlS5_+0x5c>)
    qmax = std::numeric_limits<int16_t>::max();
   e3b4a:	f647 72ff 	movw	r2, #32767	; 0x7fff
  } else {
    TF_LITE_ENSURE(context, false);
  }

  CalculateActivationRangeQuantizedImpl(activation, qmin, qmax, output, act_min,
                                        act_max);
   e3b4e:	9806      	ldr	r0, [sp, #24]
   e3b50:	9001      	str	r0, [sp, #4]
   e3b52:	9300      	str	r3, [sp, #0]
   e3b54:	4628      	mov	r0, r5
   e3b56:	4623      	mov	r3, r4
   e3b58:	f7ff ff0a 	bl	e3970 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_>
  return kTfLiteOk;
   e3b5c:	2000      	movs	r0, #0
}
   e3b5e:	b002      	add	sp, #8
   e3b60:	bd70      	pop	{r4, r5, r6, pc}
   e3b62:	bf00      	nop
   e3b64:	000eb1ee 	.word	0x000eb1ee
   e3b68:	000eb0dc 	.word	0x000eb0dc
   e3b6c:	000e9a98 	.word	0x000e9a98
   e3b70:	ffff8000 	.word	0xffff8000

000e3b74 <_ZN6tflite29CalculateActivationRangeUint8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>:

void CalculateActivationRangeUint8(TfLiteFusedActivation activation,
                                   TfLiteTensor* output, int32_t* act_min,
                                   int32_t* act_max) {
   e3b74:	b507      	push	{r0, r1, r2, lr}
  const int32_t qmin = std::numeric_limits<uint8_t>::min();
  const int32_t qmax = std::numeric_limits<uint8_t>::max();

  CalculateActivationRangeQuantizedImpl(activation, qmin, qmax, output, act_min,
                                        act_max);
   e3b76:	e88d 000c 	stmia.w	sp, {r2, r3}
   e3b7a:	460b      	mov	r3, r1
   e3b7c:	22ff      	movs	r2, #255	; 0xff
   e3b7e:	2100      	movs	r1, #0
   e3b80:	f7ff fef6 	bl	e3970 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_>
}
   e3b84:	b003      	add	sp, #12
   e3b86:	f85d fb04 	ldr.w	pc, [sp], #4
	...

000e3b8c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_>:
TfLiteStatus PopulateConvolutionQuantizationParams(
    TfLiteContext* context, const TfLiteTensor* input,
    const TfLiteTensor* filter, const TfLiteTensor* bias, TfLiteTensor* output,
    const TfLiteFusedActivation& activation, int32_t* multiplier, int* shift,
    int32_t* output_activation_min, int32_t* output_activation_max,
    int32_t* per_channel_multiplier, int* per_channel_shift) {
   e3b8c:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
  TF_LITE_ENSURE_EQ(context, input->quantization.type,
   e3b90:	f891 8030 	ldrb.w	r8, [r1, #48]	; 0x30
TfLiteStatus PopulateConvolutionQuantizationParams(
    TfLiteContext* context, const TfLiteTensor* input,
    const TfLiteTensor* filter, const TfLiteTensor* bias, TfLiteTensor* output,
    const TfLiteFusedActivation& activation, int32_t* multiplier, int* shift,
    int32_t* output_activation_min, int32_t* output_activation_max,
    int32_t* per_channel_multiplier, int* per_channel_shift) {
   e3b94:	b08d      	sub	sp, #52	; 0x34
  TF_LITE_ENSURE_EQ(context, input->quantization.type,
   e3b96:	f1b8 0f01 	cmp.w	r8, #1
TfLiteStatus PopulateConvolutionQuantizationParams(
    TfLiteContext* context, const TfLiteTensor* input,
    const TfLiteTensor* filter, const TfLiteTensor* bias, TfLiteTensor* output,
    const TfLiteFusedActivation& activation, int32_t* multiplier, int* shift,
    int32_t* output_activation_min, int32_t* output_activation_max,
    int32_t* per_channel_multiplier, int* per_channel_shift) {
   e3b9a:	4604      	mov	r4, r0
   e3b9c:	460e      	mov	r6, r1
   e3b9e:	4617      	mov	r7, r2
   e3ba0:	9307      	str	r3, [sp, #28]
  TF_LITE_ENSURE_EQ(context, input->quantization.type,
   e3ba2:	d00a      	beq.n	e3bba <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x2e>
   e3ba4:	4b63      	ldr	r3, [pc, #396]	; (e3d34 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1a8>)
   e3ba6:	9301      	str	r3, [sp, #4]
   e3ba8:	2501      	movs	r5, #1
   e3baa:	4b63      	ldr	r3, [pc, #396]	; (e3d38 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1ac>)
   e3bac:	9300      	str	r3, [sp, #0]
   e3bae:	9503      	str	r5, [sp, #12]
   e3bb0:	f8cd 8008 	str.w	r8, [sp, #8]
   e3bb4:	6944      	ldr	r4, [r0, #20]
   e3bb6:	2321      	movs	r3, #33	; 0x21
   e3bb8:	e033      	b.n	e3c22 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x96>
                    kTfLiteAffineQuantization);
  TF_LITE_ENSURE_EQ(context, filter->quantization.type,
   e3bba:	f892 5030 	ldrb.w	r5, [r2, #48]	; 0x30
   e3bbe:	2d01      	cmp	r5, #1
   e3bc0:	d00d      	beq.n	e3bde <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x52>
   e3bc2:	4b5c      	ldr	r3, [pc, #368]	; (e3d34 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1a8>)
   e3bc4:	9301      	str	r3, [sp, #4]
   e3bc6:	4b5d      	ldr	r3, [pc, #372]	; (e3d3c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1b0>)
   e3bc8:	9502      	str	r5, [sp, #8]
   e3bca:	9300      	str	r3, [sp, #0]
   e3bcc:	f8cd 800c 	str.w	r8, [sp, #12]
   e3bd0:	6944      	ldr	r4, [r0, #20]
   e3bd2:	4a5b      	ldr	r2, [pc, #364]	; (e3d40 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1b4>)
   e3bd4:	495b      	ldr	r1, [pc, #364]	; (e3d44 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1b8>)
   e3bd6:	2323      	movs	r3, #35	; 0x23
   e3bd8:	47a0      	blx	r4
   e3bda:	4645      	mov	r5, r8
   e3bdc:	e0a6      	b.n	e3d2c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1a0>
  // TF_LITE_ENSURE_EQ(context, bias->quantization.type,
  // kTfLiteAffineQuantization);

  // Check data type.
  const auto* affine_quantization =
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);
   e3bde:	6b51      	ldr	r1, [r2, #52]	; 0x34
  TF_LITE_ENSURE(context, affine_quantization);
   e3be0:	b921      	cbnz	r1, e3bec <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x60>
   e3be2:	4b59      	ldr	r3, [pc, #356]	; (e3d48 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1bc>)
   e3be4:	9300      	str	r3, [sp, #0]
   e3be6:	6944      	ldr	r4, [r0, #20]
   e3be8:	232d      	movs	r3, #45	; 0x2d
   e3bea:	e005      	b.n	e3bf8 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x6c>
  TF_LITE_ENSURE(context, affine_quantization->scale);
   e3bec:	680b      	ldr	r3, [r1, #0]
   e3bee:	b93b      	cbnz	r3, e3c00 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x74>
   e3bf0:	4b56      	ldr	r3, [pc, #344]	; (e3d4c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1c0>)
   e3bf2:	9300      	str	r3, [sp, #0]
   e3bf4:	6944      	ldr	r4, [r0, #20]
   e3bf6:	232e      	movs	r3, #46	; 0x2e
   e3bf8:	4a51      	ldr	r2, [pc, #324]	; (e3d40 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1b4>)
   e3bfa:	4955      	ldr	r1, [pc, #340]	; (e3d50 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1c4>)
   e3bfc:	47a0      	blx	r4
   e3bfe:	e095      	b.n	e3d2c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1a0>
  const bool is_per_channel = affine_quantization->scale->size > 1;
   e3c00:	f8d3 b000 	ldr.w	fp, [r3]
  if (is_per_channel) {
   e3c04:	f1bb 0f01 	cmp.w	fp, #1
   e3c08:	dd2f      	ble.n	e3c6a <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0xde>
    //  Currently only Int8 is supported for per channel quantization.
    TF_LITE_ENSURE_EQ(context, input->type, kTfLiteInt8);
   e3c0a:	7832      	ldrb	r2, [r6, #0]
   e3c0c:	2a09      	cmp	r2, #9
   e3c0e:	d00c      	beq.n	e3c2a <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x9e>
   e3c10:	2309      	movs	r3, #9
   e3c12:	9303      	str	r3, [sp, #12]
   e3c14:	4b4f      	ldr	r3, [pc, #316]	; (e3d54 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1c8>)
   e3c16:	9301      	str	r3, [sp, #4]
   e3c18:	4b4f      	ldr	r3, [pc, #316]	; (e3d58 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1cc>)
   e3c1a:	9300      	str	r3, [sp, #0]
   e3c1c:	9202      	str	r2, [sp, #8]
   e3c1e:	6944      	ldr	r4, [r0, #20]
   e3c20:	2332      	movs	r3, #50	; 0x32
   e3c22:	4a47      	ldr	r2, [pc, #284]	; (e3d40 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1b4>)
   e3c24:	4947      	ldr	r1, [pc, #284]	; (e3d44 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1b8>)
   e3c26:	47a0      	blx	r4
   e3c28:	e080      	b.n	e3d2c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1a0>
    TF_LITE_ENSURE_EQ(context, filter->type, kTfLiteInt8);
   e3c2a:	f897 e000 	ldrb.w	lr, [r7]
   e3c2e:	f1be 0f09 	cmp.w	lr, #9
   e3c32:	d009      	beq.n	e3c48 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0xbc>
   e3c34:	4b47      	ldr	r3, [pc, #284]	; (e3d54 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1c8>)
   e3c36:	9301      	str	r3, [sp, #4]
   e3c38:	4b48      	ldr	r3, [pc, #288]	; (e3d5c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1d0>)
   e3c3a:	9300      	str	r3, [sp, #0]
   e3c3c:	9203      	str	r2, [sp, #12]
   e3c3e:	f8cd e008 	str.w	lr, [sp, #8]
   e3c42:	6944      	ldr	r4, [r0, #20]
   e3c44:	2333      	movs	r3, #51	; 0x33
   e3c46:	e7ec      	b.n	e3c22 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x96>
    TF_LITE_ENSURE_EQ(
   e3c48:	68ba      	ldr	r2, [r7, #8]
   e3c4a:	6889      	ldr	r1, [r1, #8]
   e3c4c:	eb02 0281 	add.w	r2, r2, r1, lsl #2
   e3c50:	6852      	ldr	r2, [r2, #4]
   e3c52:	4593      	cmp	fp, r2
   e3c54:	d009      	beq.n	e3c6a <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0xde>
   e3c56:	4b42      	ldr	r3, [pc, #264]	; (e3d60 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1d4>)
   e3c58:	9301      	str	r3, [sp, #4]
   e3c5a:	4b42      	ldr	r3, [pc, #264]	; (e3d64 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1d8>)
   e3c5c:	9300      	str	r3, [sp, #0]
   e3c5e:	9203      	str	r2, [sp, #12]
   e3c60:	f8cd b008 	str.w	fp, [sp, #8]
   e3c64:	6944      	ldr	r4, [r0, #20]
   e3c66:	2336      	movs	r3, #54	; 0x36
   e3c68:	e7db      	b.n	e3c22 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x96>
        filter->dims->data[affine_quantization->quantized_dimension]);
  }

  // Populate multiplier and shift using affine quantization.
  const int num_channels = affine_quantization->scale->size;
  const float input_scale = input->params.scale;
   e3c6a:	edd6 7a03 	vldr	s15, [r6, #12]
  const float output_scale = output->params.scale;
   e3c6e:	9a16      	ldr	r2, [sp, #88]	; 0x58
        filter->dims->data[affine_quantization->quantized_dimension]);
  }

  // Populate multiplier and shift using affine quantization.
  const int num_channels = affine_quantization->scale->size;
  const float input_scale = input->params.scale;
   e3c70:	edcd 7a05 	vstr	s15, [sp, #20]
  const float output_scale = output->params.scale;
   e3c74:	edd2 7a03 	vldr	s15, [r2, #12]
  const float* filter_scales = affine_quantization->scale->data;
   e3c78:	1d1d      	adds	r5, r3, #4
  }

  // Populate multiplier and shift using affine quantization.
  const int num_channels = affine_quantization->scale->size;
  const float input_scale = input->params.scale;
  const float output_scale = output->params.scale;
   e3c7a:	edcd 7a06 	vstr	s15, [sp, #24]
  const float* filter_scales = affine_quantization->scale->data;
  for (int i = 0; i < num_channels; ++i) {
   e3c7e:	f04f 0a00 	mov.w	sl, #0
   e3c82:	45da      	cmp	sl, fp
   e3c84:	da2a      	bge.n	e3cdc <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x150>
    const double effective_output_scale = static_cast<double>(input_scale) *
                                          filter_scale /
                                          static_cast<double>(output_scale);
    int32_t significand;
    int shift;
    QuantizeMultiplier(effective_output_scale, &significand, &shift);
   e3c86:	f855 0b04 	ldr.w	r0, [r5], #4
   e3c8a:	f003 f99f 	bl	e6fcc <__aeabi_f2d>
   e3c8e:	4680      	mov	r8, r0
   e3c90:	9805      	ldr	r0, [sp, #20]
   e3c92:	4689      	mov	r9, r1
   e3c94:	f003 f99a 	bl	e6fcc <__aeabi_f2d>
   e3c98:	4602      	mov	r2, r0
   e3c9a:	460b      	mov	r3, r1
   e3c9c:	4640      	mov	r0, r8
   e3c9e:	4649      	mov	r1, r9
   e3ca0:	f003 f9e8 	bl	e7074 <__aeabi_dmul>
   e3ca4:	4680      	mov	r8, r0
   e3ca6:	9806      	ldr	r0, [sp, #24]
   e3ca8:	4689      	mov	r9, r1
   e3caa:	f003 f98f 	bl	e6fcc <__aeabi_f2d>
   e3cae:	4602      	mov	r2, r0
   e3cb0:	460b      	mov	r3, r1
   e3cb2:	4640      	mov	r0, r8
   e3cb4:	4649      	mov	r1, r9
   e3cb6:	f003 fb07 	bl	e72c8 <__aeabi_ddiv>
   e3cba:	ec41 0b10 	vmov	d0, r0, r1
   e3cbe:	a90a      	add	r1, sp, #40	; 0x28
   e3cc0:	a809      	add	r0, sp, #36	; 0x24
   e3cc2:	f000 f867 	bl	e3d94 <_ZN6tflite18QuantizeMultiplierEdPlPi>
    per_channel_multiplier[i] = significand;
   e3cc6:	9a1c      	ldr	r2, [sp, #112]	; 0x70
   e3cc8:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e3cca:	f842 302a 	str.w	r3, [r2, sl, lsl #2]
    per_channel_shift[i] = shift;
   e3cce:	9a1d      	ldr	r2, [sp, #116]	; 0x74
   e3cd0:	9b0a      	ldr	r3, [sp, #40]	; 0x28
   e3cd2:	f842 302a 	str.w	r3, [r2, sl, lsl #2]
  // Populate multiplier and shift using affine quantization.
  const int num_channels = affine_quantization->scale->size;
  const float input_scale = input->params.scale;
  const float output_scale = output->params.scale;
  const float* filter_scales = affine_quantization->scale->data;
  for (int i = 0; i < num_channels; ++i) {
   e3cd6:	f10a 0a01 	add.w	sl, sl, #1
   e3cda:	e7d2      	b.n	e3c82 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0xf6>
  }

  // Populate scalar quantization parameters.
  // This check on legacy quantization parameters is kept only for backward
  // compatibility.
  if (input->type == kTfLiteUInt8) {
   e3cdc:	7833      	ldrb	r3, [r6, #0]
   e3cde:	2b03      	cmp	r3, #3
   e3ce0:	d123      	bne.n	e3d2a <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x19e>
    // Check bias scale == input scale * filter scale.
    double real_multiplier = 0.0;
   e3ce2:	ab0c      	add	r3, sp, #48	; 0x30
   e3ce4:	2000      	movs	r0, #0
   e3ce6:	2100      	movs	r1, #0
   e3ce8:	e963 0102 	strd	r0, r1, [r3, #-8]!
    TF_LITE_ENSURE_STATUS(GetQuantizedConvolutionMultipler(
   e3cec:	9301      	str	r3, [sp, #4]
   e3cee:	9b16      	ldr	r3, [sp, #88]	; 0x58
   e3cf0:	9300      	str	r3, [sp, #0]
   e3cf2:	463a      	mov	r2, r7
   e3cf4:	9b07      	ldr	r3, [sp, #28]
   e3cf6:	4631      	mov	r1, r6
   e3cf8:	4620      	mov	r0, r4
   e3cfa:	f7ff fead 	bl	e3a58 <_ZN6tflite32GetQuantizedConvolutionMultiplerEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_Pd>
   e3cfe:	4605      	mov	r5, r0
   e3d00:	b108      	cbz	r0, e3d06 <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x17a>
   e3d02:	2501      	movs	r5, #1
   e3d04:	e012      	b.n	e3d2c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1a0>
        context, input, filter, bias, output, &real_multiplier));
    int exponent;

    // Populate quantization parameteters with multiplier and shift.
    QuantizeMultiplier(real_multiplier, multiplier, &exponent);
   e3d06:	a909      	add	r1, sp, #36	; 0x24
   e3d08:	9818      	ldr	r0, [sp, #96]	; 0x60
   e3d0a:	ed9d 0b0a 	vldr	d0, [sp, #40]	; 0x28
   e3d0e:	f000 f841 	bl	e3d94 <_ZN6tflite18QuantizeMultiplierEdPlPi>
    *shift = -exponent;
   e3d12:	9b09      	ldr	r3, [sp, #36]	; 0x24
   e3d14:	9a19      	ldr	r2, [sp, #100]	; 0x64
    CalculateActivationRangeUint8(activation, output, output_activation_min,
                                  output_activation_max);
   e3d16:	9817      	ldr	r0, [sp, #92]	; 0x5c
   e3d18:	9916      	ldr	r1, [sp, #88]	; 0x58
        context, input, filter, bias, output, &real_multiplier));
    int exponent;

    // Populate quantization parameteters with multiplier and shift.
    QuantizeMultiplier(real_multiplier, multiplier, &exponent);
    *shift = -exponent;
   e3d1a:	425b      	negs	r3, r3
   e3d1c:	6013      	str	r3, [r2, #0]
    CalculateActivationRangeUint8(activation, output, output_activation_min,
                                  output_activation_max);
   e3d1e:	7800      	ldrb	r0, [r0, #0]
   e3d20:	9b1b      	ldr	r3, [sp, #108]	; 0x6c
   e3d22:	9a1a      	ldr	r2, [sp, #104]	; 0x68
   e3d24:	f7ff ff26 	bl	e3b74 <_ZN6tflite29CalculateActivationRangeUint8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>
   e3d28:	e000      	b.n	e3d2c <_ZN6tflite37PopulateConvolutionQuantizationParamsEP13TfLiteContextPK12TfLiteTensorS4_S4_PS2_RK21TfLiteFusedActivationPlPiS9_S9_S9_SA_+0x1a0>
  }
  return kTfLiteOk;
   e3d2a:	2500      	movs	r5, #0
}
   e3d2c:	4628      	mov	r0, r5
   e3d2e:	b00d      	add	sp, #52	; 0x34
   e3d30:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e3d34:	000e9ae7 	.word	0x000e9ae7
   e3d38:	000eb1f4 	.word	0x000eb1f4
   e3d3c:	000e9b01 	.word	0x000e9b01
   e3d40:	000eb0dc 	.word	0x000eb0dc
   e3d44:	000e98c8 	.word	0x000e98c8
   e3d48:	000e9b1b 	.word	0x000e9b1b
   e3d4c:	000e9b2f 	.word	0x000e9b2f
   e3d50:	000e9a98 	.word	0x000e9a98
   e3d54:	000eacdf 	.word	0x000eacdf
   e3d58:	000e9903 	.word	0x000e9903
   e3d5c:	000eb20d 	.word	0x000eb20d
   e3d60:	000eb21a 	.word	0x000eb21a
   e3d64:	000eb257 	.word	0x000eb257

000e3d68 <_ZN6tflite28CalculateActivationRangeInt8E21TfLiteFusedActivationP12TfLiteTensorPlS3_>:
                                        act_max);
}

void CalculateActivationRangeInt8(TfLiteFusedActivation activation,
                                  TfLiteTensor* output, int32_t* act_min,
                                  int32_t* act_max) {
   e3d68:	b507      	push	{r0, r1, r2, lr}
  const int32_t qmin = std::numeric_limits<int8_t>::min();
  const int32_t qmax = std::numeric_limits<int8_t>::max();

  CalculateActivationRangeQuantizedImpl(activation, qmin, qmax, output, act_min,
                                        act_max);
   e3d6a:	e88d 000c 	stmia.w	sp, {r2, r3}
   e3d6e:	460b      	mov	r3, r1
   e3d70:	227f      	movs	r2, #127	; 0x7f
   e3d72:	f06f 017f 	mvn.w	r1, #127	; 0x7f
   e3d76:	f7ff fdfb 	bl	e3970 <_ZN6tflite12_GLOBAL__N_137CalculateActivationRangeQuantizedImplE21TfLiteFusedActivationllP12TfLiteTensorPlS4_>
}
   e3d7a:	b003      	add	sp, #12
   e3d7c:	f85d fb04 	ldr.w	pc, [sp], #4

000e3d80 <_ZN6tflite14HaveSameShapesEPK12TfLiteTensorS2_>:

bool HaveSameShapes(const TfLiteTensor* input1, const TfLiteTensor* input2) {
   e3d80:	b508      	push	{r3, lr}
  return TfLiteIntArrayEqual(input1->dims, input2->dims);
   e3d82:	6889      	ldr	r1, [r1, #8]
   e3d84:	6880      	ldr	r0, [r0, #8]
   e3d86:	f7f0 f9bb 	bl	d4100 <TfLiteIntArrayEqual>
}
   e3d8a:	3000      	adds	r0, #0
   e3d8c:	bf18      	it	ne
   e3d8e:	2001      	movne	r0, #1
   e3d90:	bd08      	pop	{r3, pc}
	...

000e3d94 <_ZN6tflite18QuantizeMultiplierEdPlPi>:
constexpr uint32_t kFractionRoundingMask = 0x003fffff;
constexpr uint32_t kFractionRoundingThreshold = 0x00200000;
}  // namespace

void QuantizeMultiplier(double double_multiplier, int32_t* quantized_multiplier,
                        int* shift) {
   e3d94:	b538      	push	{r3, r4, r5, lr}
  if (double_multiplier == 0.) {
   e3d96:	2200      	movs	r2, #0
constexpr uint32_t kFractionRoundingMask = 0x003fffff;
constexpr uint32_t kFractionRoundingThreshold = 0x00200000;
}  // namespace

void QuantizeMultiplier(double double_multiplier, int32_t* quantized_multiplier,
                        int* shift) {
   e3d98:	ed2d 8b02 	vpush	{d8}
   e3d9c:	eeb0 8a40 	vmov.f32	s16, s0
   e3da0:	eef0 8a60 	vmov.f32	s17, s1
   e3da4:	4605      	mov	r5, r0
   e3da6:	460c      	mov	r4, r1
  if (double_multiplier == 0.) {
   e3da8:	2300      	movs	r3, #0
   e3daa:	ec51 0b10 	vmov	r0, r1, d0
   e3dae:	f003 fbc9 	bl	e7544 <__aeabi_dcmpeq>
   e3db2:	b118      	cbz	r0, e3dbc <_ZN6tflite18QuantizeMultiplierEdPlPi+0x28>
    *quantized_multiplier = 0;
   e3db4:	2300      	movs	r3, #0
   e3db6:	602b      	str	r3, [r5, #0]
    *shift = 0;
   e3db8:	6023      	str	r3, [r4, #0]
   e3dba:	e02d      	b.n	e3e18 <_ZN6tflite18QuantizeMultiplierEdPlPi+0x84>
  // example on microcontrollers) then use an alternative implementation
  // that only requires integer and bitwise operations. To enable this, you
  // need to set the define during the build process for your platform.
  int64_t q_fixed = IntegerFrExp(double_multiplier, shift);
#else   // TFLITE_EMULATE_FLOAT
  const double q = std::frexp(double_multiplier, shift);
   e3dbc:	4620      	mov	r0, r4
   e3dbe:	eeb0 0a48 	vmov.f32	s0, s16
   e3dc2:	eef0 0a68 	vmov.f32	s1, s17
   e3dc6:	f001 f9f7 	bl	e51b8 <frexp>
   e3dca:	2200      	movs	r2, #0
   e3dcc:	4b14      	ldr	r3, [pc, #80]	; (e3e20 <_ZN6tflite18QuantizeMultiplierEdPlPi+0x8c>)
   e3dce:	ec51 0b10 	vmov	r0, r1, d0
   e3dd2:	f003 f94f 	bl	e7074 <__aeabi_dmul>
   e3dd6:	ec41 0b10 	vmov	d0, r0, r1
   e3dda:	f001 fa23 	bl	e5224 <round>
  auto q_fixed = static_cast<int64_t>(TfLiteRound(q * (1ll << 31)));
   e3dde:	ec51 0b10 	vmov	r0, r1, d0
   e3de2:	f003 fc79 	bl	e76d8 <__aeabi_d2lz>
#endif  // TFLITE_EMULATE_FLOAT
  TFLITE_CHECK(q_fixed <= (1ll << 31));
   e3de6:	f04f 4200 	mov.w	r2, #2147483648	; 0x80000000
   e3dea:	2300      	movs	r3, #0
   e3dec:	4282      	cmp	r2, r0
   e3dee:	418b      	sbcs	r3, r1
   e3df0:	da01      	bge.n	e3df6 <_ZN6tflite18QuantizeMultiplierEdPlPi+0x62>
   e3df2:	f000 faab 	bl	e434c <abort>
  if (q_fixed == (1ll << 31)) {
   e3df6:	2900      	cmp	r1, #0
   e3df8:	bf01      	itttt	eq
   e3dfa:	f1b0 4f00 	cmpeq.w	r0, #2147483648	; 0x80000000
    q_fixed /= 2;
    ++*shift;
   e3dfe:	6823      	ldreq	r3, [r4, #0]
   e3e00:	3301      	addeq	r3, #1
   e3e02:	6023      	streq	r3, [r4, #0]
  // that we're effectively flushing tiny double_multiplier's to zero.
  // We could conceivably handle values in the range (roughly) [32, 63]
  // as 'denormals' i.e. (shift==0, q_fixed < 2^30). In that point of view
  // the present handling is just doing 'flush denormals to zero'. We could
  // reconsider and actually generate nonzero denormals if a need arises.
  if (*shift < -31) {
   e3e04:	6823      	ldr	r3, [r4, #0]
  const double q = std::frexp(double_multiplier, shift);
  auto q_fixed = static_cast<int64_t>(TfLiteRound(q * (1ll << 31)));
#endif  // TFLITE_EMULATE_FLOAT
  TFLITE_CHECK(q_fixed <= (1ll << 31));
  if (q_fixed == (1ll << 31)) {
    q_fixed /= 2;
   e3e06:	bf08      	it	eq
   e3e08:	f04f 4080 	moveq.w	r0, #1073741824	; 0x40000000
  // that we're effectively flushing tiny double_multiplier's to zero.
  // We could conceivably handle values in the range (roughly) [32, 63]
  // as 'denormals' i.e. (shift==0, q_fixed < 2^30). In that point of view
  // the present handling is just doing 'flush denormals to zero'. We could
  // reconsider and actually generate nonzero denormals if a need arises.
  if (*shift < -31) {
   e3e0c:	331f      	adds	r3, #31
    *shift = 0;
   e3e0e:	bfbe      	ittt	lt
   e3e10:	2300      	movlt	r3, #0
    q_fixed = 0;
   e3e12:	2000      	movlt	r0, #0
  // We could conceivably handle values in the range (roughly) [32, 63]
  // as 'denormals' i.e. (shift==0, q_fixed < 2^30). In that point of view
  // the present handling is just doing 'flush denormals to zero'. We could
  // reconsider and actually generate nonzero denormals if a need arises.
  if (*shift < -31) {
    *shift = 0;
   e3e14:	6023      	strlt	r3, [r4, #0]
    q_fixed = 0;
  }
  *quantized_multiplier = static_cast<int32_t>(q_fixed);
   e3e16:	6028      	str	r0, [r5, #0]
}
   e3e18:	ecbd 8b02 	vpop	{d8}
   e3e1c:	bd38      	pop	{r3, r4, r5, pc}
   e3e1e:	bf00      	nop
   e3e20:	41e00000 	.word	0x41e00000

000e3e24 <_ZN6tflite32QuantizeMultiplierGreaterThanOneEdPlPi>:

void QuantizeMultiplierGreaterThanOne(double double_multiplier,
                                      int32_t* quantized_multiplier,
                                      int* left_shift) {
   e3e24:	b538      	push	{r3, r4, r5, lr}
  TFLITE_CHECK_GT(double_multiplier, 1.);
   e3e26:	2200      	movs	r2, #0
  *quantized_multiplier = static_cast<int32_t>(q_fixed);
}

void QuantizeMultiplierGreaterThanOne(double double_multiplier,
                                      int32_t* quantized_multiplier,
                                      int* left_shift) {
   e3e28:	ed2d 8b02 	vpush	{d8}
   e3e2c:	eeb0 8a40 	vmov.f32	s16, s0
   e3e30:	eef0 8a60 	vmov.f32	s17, s1
   e3e34:	4605      	mov	r5, r0
   e3e36:	460c      	mov	r4, r1
  TFLITE_CHECK_GT(double_multiplier, 1.);
   e3e38:	4b0a      	ldr	r3, [pc, #40]	; (e3e64 <_ZN6tflite32QuantizeMultiplierGreaterThanOneEdPlPi+0x40>)
   e3e3a:	ec51 0b10 	vmov	r0, r1, d0
   e3e3e:	f003 fba9 	bl	e7594 <__aeabi_dcmpgt>
   e3e42:	b908      	cbnz	r0, e3e48 <_ZN6tflite32QuantizeMultiplierGreaterThanOneEdPlPi+0x24>
   e3e44:	f000 fa82 	bl	e434c <abort>
  QuantizeMultiplier(double_multiplier, quantized_multiplier, left_shift);
   e3e48:	4621      	mov	r1, r4
   e3e4a:	4628      	mov	r0, r5
   e3e4c:	eeb0 0a48 	vmov.f32	s0, s16
   e3e50:	eef0 0a68 	vmov.f32	s1, s17
   e3e54:	f7ff ff9e 	bl	e3d94 <_ZN6tflite18QuantizeMultiplierEdPlPi>
  TFLITE_CHECK_GE(*left_shift, 0);
   e3e58:	6823      	ldr	r3, [r4, #0]
   e3e5a:	2b00      	cmp	r3, #0
   e3e5c:	dbf2      	blt.n	e3e44 <_ZN6tflite32QuantizeMultiplierGreaterThanOneEdPlPi+0x20>
}
   e3e5e:	ecbd 8b02 	vpop	{d8}
   e3e62:	bd38      	pop	{r3, r4, r5, pc}
   e3e64:	3ff00000 	.word	0x3ff00000

000e3e68 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi>:

void QuantizeMultiplierSmallerThanOneExp(double double_multiplier,
                                         int32_t* quantized_multiplier,
                                         int* left_shift) {
   e3e68:	b530      	push	{r4, r5, lr}
  TFLITE_CHECK_LT(double_multiplier, 1.);
   e3e6a:	2200      	movs	r2, #0
  TFLITE_CHECK_GE(*left_shift, 0);
}

void QuantizeMultiplierSmallerThanOneExp(double double_multiplier,
                                         int32_t* quantized_multiplier,
                                         int* left_shift) {
   e3e6c:	b085      	sub	sp, #20
   e3e6e:	4605      	mov	r5, r0
   e3e70:	460c      	mov	r4, r1
  TFLITE_CHECK_LT(double_multiplier, 1.);
   e3e72:	4b11      	ldr	r3, [pc, #68]	; (e3eb8 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi+0x50>)
   e3e74:	ec51 0b10 	vmov	r0, r1, d0
   e3e78:	ed8d 0b00 	vstr	d0, [sp]
   e3e7c:	f003 fb6c 	bl	e7558 <__aeabi_dcmplt>
   e3e80:	ed9d 0b00 	vldr	d0, [sp]
   e3e84:	b908      	cbnz	r0, e3e8a <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi+0x22>
   e3e86:	f000 fa61 	bl	e434c <abort>
  TFLITE_CHECK_GT(double_multiplier, 0.);
   e3e8a:	2200      	movs	r2, #0
   e3e8c:	2300      	movs	r3, #0
   e3e8e:	ec51 0b10 	vmov	r0, r1, d0
   e3e92:	ed8d 0b00 	vstr	d0, [sp]
   e3e96:	f003 fb7d 	bl	e7594 <__aeabi_dcmpgt>
   e3e9a:	2800      	cmp	r0, #0
   e3e9c:	d0f3      	beq.n	e3e86 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi+0x1e>
  int shift;
  QuantizeMultiplier(double_multiplier, quantized_multiplier, &shift);
   e3e9e:	a903      	add	r1, sp, #12
   e3ea0:	4628      	mov	r0, r5
   e3ea2:	ed9d 0b00 	vldr	d0, [sp]
   e3ea6:	f7ff ff75 	bl	e3d94 <_ZN6tflite18QuantizeMultiplierEdPlPi>
  TFLITE_CHECK_LE(shift, 0);
   e3eaa:	9b03      	ldr	r3, [sp, #12]
   e3eac:	2b00      	cmp	r3, #0
   e3eae:	dcea      	bgt.n	e3e86 <_ZN6tflite35QuantizeMultiplierSmallerThanOneExpEdPlPi+0x1e>
  *left_shift = shift;
   e3eb0:	6023      	str	r3, [r4, #0]
}
   e3eb2:	b005      	add	sp, #20
   e3eb4:	bd30      	pop	{r4, r5, pc}
   e3eb6:	bf00      	nop
   e3eb8:	3ff00000 	.word	0x3ff00000
   e3ebc:	00000000 	.word	0x00000000

000e3ec0 <_ZN6tflite24PreprocessSoftmaxScalingEddiPlPi>:
  }
}

void PreprocessSoftmaxScaling(double beta, double input_scale,
                              int input_integer_bits,
                              int32_t* quantized_multiplier, int* left_shift) {
   e3ec0:	b5f0      	push	{r4, r5, r6, r7, lr}
  if (IntegerDoubleCompare(input_beta_real_multiplier, (1ll << 31) - 1.0) > 0) {
    input_beta_real_multiplier = (1ll << 31) - 1.0;
  }
#else   // TFLITE_EMULATE_FLOAT
  const double input_beta_real_multiplier = std::min(
      beta * input_scale * (1 << (31 - input_integer_bits)), (1ll << 31) - 1.0);
   e3ec2:	2301      	movs	r3, #1
   e3ec4:	f1c0 001f 	rsb	r0, r0, #31
  }
}

void PreprocessSoftmaxScaling(double beta, double input_scale,
                              int input_integer_bits,
                              int32_t* quantized_multiplier, int* left_shift) {
   e3ec8:	b085      	sub	sp, #20
  if (IntegerDoubleCompare(input_beta_real_multiplier, (1ll << 31) - 1.0) > 0) {
    input_beta_real_multiplier = (1ll << 31) - 1.0;
  }
#else   // TFLITE_EMULATE_FLOAT
  const double input_beta_real_multiplier = std::min(
      beta * input_scale * (1 << (31 - input_integer_bits)), (1ll << 31) - 1.0);
   e3eca:	fa03 f000 	lsl.w	r0, r3, r0
  }
}

void PreprocessSoftmaxScaling(double beta, double input_scale,
                              int input_integer_bits,
                              int32_t* quantized_multiplier, int* left_shift) {
   e3ece:	ed8d 0b02 	vstr	d0, [sp, #8]
   e3ed2:	ed8d 1b00 	vstr	d1, [sp]
   e3ed6:	4615      	mov	r5, r2
   e3ed8:	460c      	mov	r4, r1
  if (IntegerDoubleCompare(input_beta_real_multiplier, (1ll << 31) - 1.0) > 0) {
    input_beta_real_multiplier = (1ll << 31) - 1.0;
  }
#else   // TFLITE_EMULATE_FLOAT
  const double input_beta_real_multiplier = std::min(
      beta * input_scale * (1 << (31 - input_integer_bits)), (1ll << 31) - 1.0);
   e3eda:	f003 f865 	bl	e6fa8 <__aeabi_i2d>
   e3ede:	ed9d 1b00 	vldr	d1, [sp]
   e3ee2:	ed9d 0b02 	vldr	d0, [sp, #8]
   e3ee6:	ec53 2b11 	vmov	r2, r3, d1
   e3eea:	4606      	mov	r6, r0
   e3eec:	460f      	mov	r7, r1
   e3eee:	ec51 0b10 	vmov	r0, r1, d0
   e3ef2:	f003 f8bf 	bl	e7074 <__aeabi_dmul>
   e3ef6:	4602      	mov	r2, r0
   e3ef8:	460b      	mov	r3, r1
   e3efa:	4630      	mov	r0, r6
   e3efc:	4639      	mov	r1, r7
   e3efe:	f003 f8b9 	bl	e7074 <__aeabi_dmul>
    min(const _Tp& __a, const _Tp& __b)
    {
      // concept requirements
      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
      //return __b < __a ? __b : __a;
      if (__b < __a)
   e3f02:	a309      	add	r3, pc, #36	; (adr r3, e3f28 <_ZN6tflite24PreprocessSoftmaxScalingEddiPlPi+0x68>)
   e3f04:	e9d3 2300 	ldrd	r2, r3, [r3]
   e3f08:	e9cd 0100 	strd	r0, r1, [sp]
   e3f0c:	f003 fb42 	bl	e7594 <__aeabi_dcmpgt>
   e3f10:	ed9d 0b00 	vldr	d0, [sp]
   e3f14:	b108      	cbz	r0, e3f1a <_ZN6tflite24PreprocessSoftmaxScalingEddiPlPi+0x5a>
	return __b;
   e3f16:	ed9f 0b04 	vldr	d0, [pc, #16]	; e3f28 <_ZN6tflite24PreprocessSoftmaxScalingEddiPlPi+0x68>
#endif  // TFLITE_EMULATE_FLOAT

  QuantizeMultiplierGreaterThanOne(input_beta_real_multiplier,
                                   quantized_multiplier, left_shift);
   e3f1a:	4629      	mov	r1, r5
   e3f1c:	4620      	mov	r0, r4
}
   e3f1e:	b005      	add	sp, #20
   e3f20:	e8bd 40f0 	ldmia.w	sp!, {r4, r5, r6, r7, lr}
  const double input_beta_real_multiplier = std::min(
      beta * input_scale * (1 << (31 - input_integer_bits)), (1ll << 31) - 1.0);
#endif  // TFLITE_EMULATE_FLOAT

  QuantizeMultiplierGreaterThanOne(input_beta_real_multiplier,
                                   quantized_multiplier, left_shift);
   e3f24:	f7ff bf7e 	b.w	e3e24 <_ZN6tflite32QuantizeMultiplierGreaterThanOneEdPlPi>
   e3f28:	ffc00000 	.word	0xffc00000
   e3f2c:	41dfffff 	.word	0x41dfffff

000e3f30 <_ZN6tflite20CalculateInputRadiusEiii>:
                                              reverse_scaling_divisor,
                                              reverse_scaling_left_shift);
}

int CalculateInputRadius(int input_integer_bits, int input_left_shift,
                         int total_signed_bits) {
   e3f30:	e92d 4370 	stmdb	sp!, {r4, r5, r6, r8, r9, lr}
   e3f34:	4604      	mov	r4, r0
      (1ll << (total_signed_bits - input_integer_bits)) /
      (1ll << input_left_shift);
  // Tighten bound using floor.  Suppose that we could use the exact value.
  // After scaling the difference, the result would be at the maximum.  Thus we
  // must ensure that our value has lower magnitude.
  return static_cast<int>(std::floor(max_input_rescaled));
   e3f36:	2001      	movs	r0, #1
   e3f38:	40a0      	lsls	r0, r4
   e3f3a:	3801      	subs	r0, #1
                                              reverse_scaling_divisor,
                                              reverse_scaling_left_shift);
}

int CalculateInputRadius(int input_integer_bits, int input_left_shift,
                         int total_signed_bits) {
   e3f3c:	460e      	mov	r6, r1
   e3f3e:	4615      	mov	r5, r2
      (1ll << (total_signed_bits - input_integer_bits)) /
      (1ll << input_left_shift);
  // Tighten bound using floor.  Suppose that we could use the exact value.
  // After scaling the difference, the result would be at the maximum.  Thus we
  // must ensure that our value has lower magnitude.
  return static_cast<int>(std::floor(max_input_rescaled));
   e3f40:	f003 f832 	bl	e6fa8 <__aeabi_i2d>
   e3f44:	1b2a      	subs	r2, r5, r4
   e3f46:	4680      	mov	r8, r0
   e3f48:	4689      	mov	r9, r1
   e3f4a:	2001      	movs	r0, #1
   e3f4c:	2100      	movs	r1, #0
   e3f4e:	f002 fecd 	bl	e6cec <__aeabi_llsl>
   e3f52:	f003 f861 	bl	e7018 <__aeabi_l2d>
   e3f56:	460b      	mov	r3, r1
   e3f58:	4602      	mov	r2, r0
   e3f5a:	4649      	mov	r1, r9
   e3f5c:	4640      	mov	r0, r8
   e3f5e:	f003 f889 	bl	e7074 <__aeabi_dmul>
   e3f62:	4632      	mov	r2, r6
   e3f64:	4604      	mov	r4, r0
   e3f66:	460d      	mov	r5, r1
   e3f68:	2001      	movs	r0, #1
   e3f6a:	2100      	movs	r1, #0
   e3f6c:	f002 febe 	bl	e6cec <__aeabi_llsl>
   e3f70:	f003 f852 	bl	e7018 <__aeabi_l2d>
   e3f74:	4602      	mov	r2, r0
   e3f76:	460b      	mov	r3, r1
   e3f78:	4620      	mov	r0, r4
   e3f7a:	4629      	mov	r1, r5
   e3f7c:	f003 f9a4 	bl	e72c8 <__aeabi_ddiv>
   e3f80:	ec41 0b10 	vmov	d0, r0, r1
   e3f84:	f001 f888 	bl	e5098 <floor>
   e3f88:	ec51 0b10 	vmov	r0, r1, d0
   e3f8c:	f003 fb0c 	bl	e75a8 <__aeabi_d2iz>
#endif  // TFLITE_EMULATE_FLOAT
}
   e3f90:	e8bd 8370 	ldmia.w	sp!, {r4, r5, r6, r8, r9, pc}

000e3f94 <os_thread_is_current>:
DYNALIB_BEGIN(hal_concurrent)

#if PLATFORM_THREADING
DYNALIB_FN(0, hal_concurrent, __gthread_equal, bool(__gthread_t, __gthread_t))
DYNALIB_FN(1, hal_concurrent, os_thread_create, os_result_t(os_thread_t*, const char*, os_thread_prio_t, os_thread_fn_t, void*, size_t))
DYNALIB_FN(2, hal_concurrent, os_thread_is_current, bool(os_thread_t))
   e3f94:	b508      	push	{r3, lr}
   e3f96:	4b02      	ldr	r3, [pc, #8]	; (e3fa0 <os_thread_is_current+0xc>)
   e3f98:	681b      	ldr	r3, [r3, #0]
   e3f9a:	689b      	ldr	r3, [r3, #8]
   e3f9c:	9301      	str	r3, [sp, #4]
   e3f9e:	bd08      	pop	{r3, pc}
   e3fa0:	00030248 	.word	0x00030248

000e3fa4 <os_thread_join>:
DYNALIB_FN(3, hal_concurrent, os_thread_yield, os_result_t(void))
DYNALIB_FN(4, hal_concurrent, os_thread_join, os_result_t(os_thread_t))
   e3fa4:	b508      	push	{r3, lr}
   e3fa6:	4b02      	ldr	r3, [pc, #8]	; (e3fb0 <os_thread_join+0xc>)
   e3fa8:	681b      	ldr	r3, [r3, #0]
   e3faa:	691b      	ldr	r3, [r3, #16]
   e3fac:	9301      	str	r3, [sp, #4]
   e3fae:	bd08      	pop	{r3, pc}
   e3fb0:	00030248 	.word	0x00030248

000e3fb4 <os_thread_cleanup>:
DYNALIB_FN(5, hal_concurrent, os_thread_cleanup, os_result_t(os_thread_t))
   e3fb4:	b508      	push	{r3, lr}
   e3fb6:	4b02      	ldr	r3, [pc, #8]	; (e3fc0 <os_thread_cleanup+0xc>)
   e3fb8:	681b      	ldr	r3, [r3, #0]
   e3fba:	695b      	ldr	r3, [r3, #20]
   e3fbc:	9301      	str	r3, [sp, #4]
   e3fbe:	bd08      	pop	{r3, pc}
   e3fc0:	00030248 	.word	0x00030248

000e3fc4 <os_mutex_create>:
DYNALIB_FN(8, hal_concurrent, os_timer_create, int(os_timer_t*, unsigned, void(*)(os_timer_t), void*, bool, void*))
DYNALIB_FN(9, hal_concurrent, os_timer_destroy, int(os_timer_t, void*))
DYNALIB_FN(10, hal_concurrent, os_timer_get_id, int(os_timer_t, void**))
DYNALIB_FN(11, hal_concurrent, os_timer_change, int(os_timer_t, os_timer_change_t, bool, unsigned, unsigned, void*))

DYNALIB_FN(12, hal_concurrent, os_mutex_create, int(os_mutex_t*))
   e3fc4:	b508      	push	{r3, lr}
   e3fc6:	4b02      	ldr	r3, [pc, #8]	; (e3fd0 <os_mutex_create+0xc>)
   e3fc8:	681b      	ldr	r3, [r3, #0]
   e3fca:	6b1b      	ldr	r3, [r3, #48]	; 0x30
   e3fcc:	9301      	str	r3, [sp, #4]
   e3fce:	bd08      	pop	{r3, pc}
   e3fd0:	00030248 	.word	0x00030248

000e3fd4 <os_mutex_recursive_create>:
DYNALIB_FN(13, hal_concurrent, os_mutex_destroy, int(os_mutex_t))
DYNALIB_FN(14, hal_concurrent, os_mutex_lock, int(os_mutex_t))
DYNALIB_FN(15, hal_concurrent, os_mutex_trylock, int(os_mutex_t))
DYNALIB_FN(16, hal_concurrent, os_mutex_unlock, int(os_mutex_t))

DYNALIB_FN(17, hal_concurrent, os_mutex_recursive_create, int(os_mutex_recursive_t*))
   e3fd4:	b508      	push	{r3, lr}
   e3fd6:	4b02      	ldr	r3, [pc, #8]	; (e3fe0 <os_mutex_recursive_create+0xc>)
   e3fd8:	681b      	ldr	r3, [r3, #0]
   e3fda:	6c5b      	ldr	r3, [r3, #68]	; 0x44
   e3fdc:	9301      	str	r3, [sp, #4]
   e3fde:	bd08      	pop	{r3, pc}
   e3fe0:	00030248 	.word	0x00030248

000e3fe4 <HAL_RNG_GetRandomNumber>:

DYNALIB_BEGIN(hal)

#if PLATFORM_ID > 3
DYNALIB_FN(0, hal, HAL_RNG_Configuration, void(void))
DYNALIB_FN(1, hal, HAL_RNG_GetRandomNumber, uint32_t(void))
   e3fe4:	b508      	push	{r3, lr}
   e3fe6:	4b02      	ldr	r3, [pc, #8]	; (e3ff0 <HAL_RNG_GetRandomNumber+0xc>)
   e3fe8:	681b      	ldr	r3, [r3, #0]
   e3fea:	685b      	ldr	r3, [r3, #4]
   e3fec:	9301      	str	r3, [sp, #4]
   e3fee:	bd08      	pop	{r3, pc}
   e3ff0:	00030218 	.word	0x00030218

000e3ff4 <HAL_Delay_Microseconds>:
#else
#define BASE_IDX 0
#endif

DYNALIB_FN(BASE_IDX + 0, hal, HAL_Delay_Milliseconds, void(uint32_t))
DYNALIB_FN(BASE_IDX + 1, hal, HAL_Delay_Microseconds, void(uint32_t))
   e3ff4:	b508      	push	{r3, lr}
   e3ff6:	4b02      	ldr	r3, [pc, #8]	; (e4000 <HAL_Delay_Microseconds+0xc>)
   e3ff8:	681b      	ldr	r3, [r3, #0]
   e3ffa:	68db      	ldr	r3, [r3, #12]
   e3ffc:	9301      	str	r3, [sp, #4]
   e3ffe:	bd08      	pop	{r3, pc}
   e4000:	00030218 	.word	0x00030218

000e4004 <HAL_Timer_Get_Milli_Seconds>:
DYNALIB_FN(BASE_IDX + 2, hal, HAL_Timer_Get_Micro_Seconds, system_tick_t(void))
DYNALIB_FN(BASE_IDX + 3, hal, HAL_Timer_Get_Milli_Seconds, system_tick_t(void))
   e4004:	b508      	push	{r3, lr}
   e4006:	4b02      	ldr	r3, [pc, #8]	; (e4010 <HAL_Timer_Get_Milli_Seconds+0xc>)
   e4008:	681b      	ldr	r3, [r3, #0]
   e400a:	695b      	ldr	r3, [r3, #20]
   e400c:	9301      	str	r3, [sp, #4]
   e400e:	bd08      	pop	{r3, pc}
   e4010:	00030218 	.word	0x00030218

000e4014 <HAL_Pin_Map>:
// New HAL functions must be added to the end of this list.
// GNINRAW

DYNALIB_BEGIN(hal_gpio)

DYNALIB_FN(0, hal_gpio, HAL_Pin_Map, Hal_Pin_Info*(void))
   e4014:	b508      	push	{r3, lr}
   e4016:	4b02      	ldr	r3, [pc, #8]	; (e4020 <HAL_Pin_Map+0xc>)
   e4018:	681b      	ldr	r3, [r3, #0]
   e401a:	681b      	ldr	r3, [r3, #0]
   e401c:	9301      	str	r3, [sp, #4]
   e401e:	bd08      	pop	{r3, pc}
   e4020:	0003022c 	.word	0x0003022c

000e4024 <HAL_Validate_Pin_Function>:
DYNALIB_FN(1, hal_gpio, HAL_Validate_Pin_Function, PinFunction(pin_t, PinFunction))
   e4024:	b508      	push	{r3, lr}
   e4026:	4b02      	ldr	r3, [pc, #8]	; (e4030 <HAL_Validate_Pin_Function+0xc>)
   e4028:	681b      	ldr	r3, [r3, #0]
   e402a:	685b      	ldr	r3, [r3, #4]
   e402c:	9301      	str	r3, [sp, #4]
   e402e:	bd08      	pop	{r3, pc}
   e4030:	0003022c 	.word	0x0003022c

000e4034 <HAL_Pin_Mode>:
DYNALIB_FN(2, hal_gpio, HAL_Pin_Mode, void(pin_t, PinMode))
   e4034:	b508      	push	{r3, lr}
   e4036:	4b02      	ldr	r3, [pc, #8]	; (e4040 <HAL_Pin_Mode+0xc>)
   e4038:	681b      	ldr	r3, [r3, #0]
   e403a:	689b      	ldr	r3, [r3, #8]
   e403c:	9301      	str	r3, [sp, #4]
   e403e:	bd08      	pop	{r3, pc}
   e4040:	0003022c 	.word	0x0003022c

000e4044 <HAL_Get_Pin_Mode>:
DYNALIB_FN(3, hal_gpio, HAL_Get_Pin_Mode, PinMode(pin_t))
   e4044:	b508      	push	{r3, lr}
   e4046:	4b02      	ldr	r3, [pc, #8]	; (e4050 <HAL_Get_Pin_Mode+0xc>)
   e4048:	681b      	ldr	r3, [r3, #0]
   e404a:	68db      	ldr	r3, [r3, #12]
   e404c:	9301      	str	r3, [sp, #4]
   e404e:	bd08      	pop	{r3, pc}
   e4050:	0003022c 	.word	0x0003022c

000e4054 <HAL_DAC_Write>:
DYNALIB_FN(6, hal_gpio, HAL_Interrupts_Attach, int(uint16_t, HAL_InterruptHandler, void*, InterruptMode, HAL_InterruptExtraConfiguration*))
DYNALIB_FN(7, hal_gpio, HAL_Interrupts_Detach, int(uint16_t))
DYNALIB_FN(8, hal_gpio, HAL_Interrupts_Enable_All, void(void))
DYNALIB_FN(9, hal_gpio, HAL_Interrupts_Disable_All, void(void))

DYNALIB_FN(10, hal_gpio, HAL_DAC_Write, void(pin_t, uint16_t))
   e4054:	b508      	push	{r3, lr}
   e4056:	4b02      	ldr	r3, [pc, #8]	; (e4060 <HAL_DAC_Write+0xc>)
   e4058:	681b      	ldr	r3, [r3, #0]
   e405a:	6a9b      	ldr	r3, [r3, #40]	; 0x28
   e405c:	9301      	str	r3, [sp, #4]
   e405e:	bd08      	pop	{r3, pc}
   e4060:	0003022c 	.word	0x0003022c

000e4064 <HAL_PWM_Write_Ext>:
DYNALIB_FN(25, hal_gpio, HAL_DAC_Get_Resolution, uint8_t(pin_t))
DYNALIB_FN(26, hal_gpio, HAL_DAC_Set_Resolution, void(pin_t, uint8_t))
DYNALIB_FN(27, hal_gpio, HAL_DAC_Enable_Buffer, void(pin_t pin, uint8_t state))
DYNALIB_FN(28, hal_gpio, HAL_PWM_Get_Resolution, uint8_t(uint16_t))
DYNALIB_FN(29, hal_gpio, HAL_PWM_Set_Resolution, void(uint16_t, uint8_t))
DYNALIB_FN(30, hal_gpio, HAL_PWM_Write_Ext, void(uint16_t, uint32_t))
   e4064:	b508      	push	{r3, lr}
   e4066:	4b02      	ldr	r3, [pc, #8]	; (e4070 <HAL_PWM_Write_Ext+0xc>)
   e4068:	681b      	ldr	r3, [r3, #0]
   e406a:	6f9b      	ldr	r3, [r3, #120]	; 0x78
   e406c:	9301      	str	r3, [sp, #4]
   e406e:	bd08      	pop	{r3, pc}
   e4070:	0003022c 	.word	0x0003022c

000e4074 <HAL_I2C_Write_Data>:
DYNALIB_FN(BASE_IDX + 3, hal_i2c, HAL_I2C_Begin, void(HAL_I2C_Interface, I2C_Mode, uint8_t, void*))
DYNALIB_FN(BASE_IDX + 4, hal_i2c, HAL_I2C_End, void(HAL_I2C_Interface, void*))
DYNALIB_FN(BASE_IDX + 5, hal_i2c, HAL_I2C_Request_Data, uint32_t(HAL_I2C_Interface, uint8_t, uint8_t, uint8_t, void*))
DYNALIB_FN(BASE_IDX + 6, hal_i2c, HAL_I2C_Begin_Transmission, void(HAL_I2C_Interface, uint8_t, void*))
DYNALIB_FN(BASE_IDX + 7, hal_i2c, HAL_I2C_End_Transmission, uint8_t(HAL_I2C_Interface, uint8_t, void*))
DYNALIB_FN(BASE_IDX + 8, hal_i2c, HAL_I2C_Write_Data, uint32_t(HAL_I2C_Interface, uint8_t, void*))
   e4074:	b508      	push	{r3, lr}
   e4076:	4b02      	ldr	r3, [pc, #8]	; (e4080 <HAL_I2C_Write_Data+0xc>)
   e4078:	681b      	ldr	r3, [r3, #0]
   e407a:	6a1b      	ldr	r3, [r3, #32]
   e407c:	9301      	str	r3, [sp, #4]
   e407e:	bd08      	pop	{r3, pc}
   e4080:	00030228 	.word	0x00030228

000e4084 <HAL_I2C_Available_Data>:
DYNALIB_FN(BASE_IDX + 9, hal_i2c, HAL_I2C_Available_Data, int32_t(HAL_I2C_Interface, void*))
   e4084:	b508      	push	{r3, lr}
   e4086:	4b02      	ldr	r3, [pc, #8]	; (e4090 <HAL_I2C_Available_Data+0xc>)
   e4088:	681b      	ldr	r3, [r3, #0]
   e408a:	6a5b      	ldr	r3, [r3, #36]	; 0x24
   e408c:	9301      	str	r3, [sp, #4]
   e408e:	bd08      	pop	{r3, pc}
   e4090:	00030228 	.word	0x00030228

000e4094 <HAL_I2C_Read_Data>:
DYNALIB_FN(BASE_IDX + 10, hal_i2c, HAL_I2C_Read_Data, int32_t(HAL_I2C_Interface, void*))
   e4094:	b508      	push	{r3, lr}
   e4096:	4b02      	ldr	r3, [pc, #8]	; (e40a0 <HAL_I2C_Read_Data+0xc>)
   e4098:	681b      	ldr	r3, [r3, #0]
   e409a:	6a9b      	ldr	r3, [r3, #40]	; 0x28
   e409c:	9301      	str	r3, [sp, #4]
   e409e:	bd08      	pop	{r3, pc}
   e40a0:	00030228 	.word	0x00030228

000e40a4 <HAL_I2C_Peek_Data>:
DYNALIB_FN(BASE_IDX + 11, hal_i2c, HAL_I2C_Peek_Data, int32_t(HAL_I2C_Interface, void*))
   e40a4:	b508      	push	{r3, lr}
   e40a6:	4b02      	ldr	r3, [pc, #8]	; (e40b0 <HAL_I2C_Peek_Data+0xc>)
   e40a8:	681b      	ldr	r3, [r3, #0]
   e40aa:	6adb      	ldr	r3, [r3, #44]	; 0x2c
   e40ac:	9301      	str	r3, [sp, #4]
   e40ae:	bd08      	pop	{r3, pc}
   e40b0:	00030228 	.word	0x00030228

000e40b4 <HAL_I2C_Flush_Data>:
DYNALIB_FN(BASE_IDX + 12, hal_i2c, HAL_I2C_Flush_Data, void(HAL_I2C_Interface, void*))
   e40b4:	b508      	push	{r3, lr}
   e40b6:	4b02      	ldr	r3, [pc, #8]	; (e40c0 <HAL_I2C_Flush_Data+0xc>)
   e40b8:	681b      	ldr	r3, [r3, #0]
   e40ba:	6b1b      	ldr	r3, [r3, #48]	; 0x30
   e40bc:	9301      	str	r3, [sp, #4]
   e40be:	bd08      	pop	{r3, pc}
   e40c0:	00030228 	.word	0x00030228

000e40c4 <HAL_I2C_Is_Enabled>:
DYNALIB_FN(BASE_IDX + 13, hal_i2c, HAL_I2C_Is_Enabled, bool(HAL_I2C_Interface, void*))
   e40c4:	b508      	push	{r3, lr}
   e40c6:	4b02      	ldr	r3, [pc, #8]	; (e40d0 <HAL_I2C_Is_Enabled+0xc>)
   e40c8:	681b      	ldr	r3, [r3, #0]
   e40ca:	6b5b      	ldr	r3, [r3, #52]	; 0x34
   e40cc:	9301      	str	r3, [sp, #4]
   e40ce:	bd08      	pop	{r3, pc}
   e40d0:	00030228 	.word	0x00030228

000e40d4 <HAL_I2C_Init>:
DYNALIB_FN(BASE_IDX + 14, hal_i2c, HAL_I2C_Set_Callback_On_Receive, void(HAL_I2C_Interface, void(*)(int), void*))
DYNALIB_FN(BASE_IDX + 15, hal_i2c, HAL_I2C_Set_Callback_On_Request, void(HAL_I2C_Interface, void(*)(void), void*))
DYNALIB_FN(BASE_IDX + 16, hal_i2c, HAL_I2C_Init, void(HAL_I2C_Interface, void*))
   e40d4:	b508      	push	{r3, lr}
   e40d6:	4b02      	ldr	r3, [pc, #8]	; (e40e0 <HAL_I2C_Init+0xc>)
   e40d8:	681b      	ldr	r3, [r3, #0]
   e40da:	6c1b      	ldr	r3, [r3, #64]	; 0x40
   e40dc:	9301      	str	r3, [sp, #4]
   e40de:	bd08      	pop	{r3, pc}
   e40e0:	00030228 	.word	0x00030228

000e40e4 <inet_inet_ntop>:
DYNALIB_FN(0, hal_inet, inet_inet_addr, in_addr_t(const char*))
DYNALIB_FN(1, hal_inet, inet_inet_aton, int(const char*, struct in_addr*))
DYNALIB_FN(2, hal_inet, inet_inet_network, in_addr_t(const char*))
DYNALIB_FN(3, hal_inet, inet_inet_ntoa, char*(struct in_addr))
DYNALIB_FN(4, hal_inet, inet_inet_ntoa_r, char*(struct in_addr, char*, socklen_t))
DYNALIB_FN(5, hal_inet, inet_inet_ntop, const char*(int, const void*, char*, socklen_t))
   e40e4:	b508      	push	{r3, lr}
   e40e6:	4b02      	ldr	r3, [pc, #8]	; (e40f0 <inet_inet_ntop+0xc>)
   e40e8:	681b      	ldr	r3, [r3, #0]
   e40ea:	695b      	ldr	r3, [r3, #20]
   e40ec:	9301      	str	r3, [sp, #4]
   e40ee:	bd08      	pop	{r3, pc}
   e40f0:	00030264 	.word	0x00030264

000e40f4 <netdb_freeaddrinfo>:

DYNALIB_BEGIN(hal_netdb)

DYNALIB_FN(0, hal_netdb, netdb_gethostbyname, struct hostent*(const char*))
DYNALIB_FN(1, hal_netdb, netdb_gethostbyname_r, int(const char*, struct hostent*, char*, size_t, struct hostent**, int*))
DYNALIB_FN(2, hal_netdb, netdb_freeaddrinfo, void(struct addrinfo*))
   e40f4:	b508      	push	{r3, lr}
   e40f6:	4b02      	ldr	r3, [pc, #8]	; (e4100 <netdb_freeaddrinfo+0xc>)
   e40f8:	681b      	ldr	r3, [r3, #0]
   e40fa:	689b      	ldr	r3, [r3, #8]
   e40fc:	9301      	str	r3, [sp, #4]
   e40fe:	bd08      	pop	{r3, pc}
   e4100:	00030268 	.word	0x00030268

000e4104 <netdb_getaddrinfo>:
DYNALIB_FN(3, hal_netdb, netdb_getaddrinfo, int(const char*, const char*, const struct addrinfo*, struct addrinfo**))
   e4104:	b508      	push	{r3, lr}
   e4106:	4b02      	ldr	r3, [pc, #8]	; (e4110 <netdb_getaddrinfo+0xc>)
   e4108:	681b      	ldr	r3, [r3, #0]
   e410a:	68db      	ldr	r3, [r3, #12]
   e410c:	9301      	str	r3, [sp, #4]
   e410e:	bd08      	pop	{r3, pc}
   e4110:	00030268 	.word	0x00030268

000e4114 <HAL_SPI_Init>:
DYNALIB_FN(2, hal_spi, HAL_SPI_Set_Bit_Order, void(HAL_SPI_Interface, uint8_t))
DYNALIB_FN(3, hal_spi, HAL_SPI_Set_Data_Mode, void(HAL_SPI_Interface, uint8_t))
DYNALIB_FN(4, hal_spi, HAL_SPI_Set_Clock_Divider, void(HAL_SPI_Interface, uint8_t))
DYNALIB_FN(5, hal_spi, HAL_SPI_Send_Receive_Data, uint16_t(HAL_SPI_Interface, uint16_t))
DYNALIB_FN(6, hal_spi, HAL_SPI_Is_Enabled_Old, bool(void))
DYNALIB_FN(7, hal_spi, HAL_SPI_Init, void(HAL_SPI_Interface))
   e4114:	b508      	push	{r3, lr}
   e4116:	4b02      	ldr	r3, [pc, #8]	; (e4120 <HAL_SPI_Init+0xc>)
   e4118:	681b      	ldr	r3, [r3, #0]
   e411a:	69db      	ldr	r3, [r3, #28]
   e411c:	9301      	str	r3, [sp, #4]
   e411e:	bd08      	pop	{r3, pc}
   e4120:	00030230 	.word	0x00030230

000e4124 <HAL_SPI_Is_Enabled>:
DYNALIB_FN(8, hal_spi, HAL_SPI_Is_Enabled, bool(HAL_SPI_Interface))
   e4124:	b508      	push	{r3, lr}
   e4126:	4b02      	ldr	r3, [pc, #8]	; (e4130 <HAL_SPI_Is_Enabled+0xc>)
   e4128:	681b      	ldr	r3, [r3, #0]
   e412a:	6a1b      	ldr	r3, [r3, #32]
   e412c:	9301      	str	r3, [sp, #4]
   e412e:	bd08      	pop	{r3, pc}
   e4130:	00030230 	.word	0x00030230

000e4134 <HAL_USART_Init>:
#define BASE_IDX 6 // Base index for all subsequent functions
#else
#define BASE_IDX 0
#endif

DYNALIB_FN(BASE_IDX + 0, hal_usart, HAL_USART_Init, void(HAL_USART_Serial, Ring_Buffer*, Ring_Buffer*))
   e4134:	b508      	push	{r3, lr}
   e4136:	4b02      	ldr	r3, [pc, #8]	; (e4140 <HAL_USART_Init+0xc>)
   e4138:	681b      	ldr	r3, [r3, #0]
   e413a:	681b      	ldr	r3, [r3, #0]
   e413c:	9301      	str	r3, [sp, #4]
   e413e:	bd08      	pop	{r3, pc}
   e4140:	0003023c 	.word	0x0003023c

000e4144 <HAL_USART_Write_Data>:
DYNALIB_FN(BASE_IDX + 1, hal_usart, HAL_USART_Begin, void(HAL_USART_Serial, uint32_t))
DYNALIB_FN(BASE_IDX + 2, hal_usart, HAL_USART_End, void(HAL_USART_Serial))
DYNALIB_FN(BASE_IDX + 3, hal_usart, HAL_USART_Write_Data, uint32_t(HAL_USART_Serial, uint8_t))
   e4144:	b508      	push	{r3, lr}
   e4146:	4b02      	ldr	r3, [pc, #8]	; (e4150 <HAL_USART_Write_Data+0xc>)
   e4148:	681b      	ldr	r3, [r3, #0]
   e414a:	68db      	ldr	r3, [r3, #12]
   e414c:	9301      	str	r3, [sp, #4]
   e414e:	bd08      	pop	{r3, pc}
   e4150:	0003023c 	.word	0x0003023c

000e4154 <HAL_USART_Available_Data>:
DYNALIB_FN(BASE_IDX + 4, hal_usart, HAL_USART_Available_Data, int32_t(HAL_USART_Serial))
   e4154:	b508      	push	{r3, lr}
   e4156:	4b02      	ldr	r3, [pc, #8]	; (e4160 <HAL_USART_Available_Data+0xc>)
   e4158:	681b      	ldr	r3, [r3, #0]
   e415a:	691b      	ldr	r3, [r3, #16]
   e415c:	9301      	str	r3, [sp, #4]
   e415e:	bd08      	pop	{r3, pc}
   e4160:	0003023c 	.word	0x0003023c

000e4164 <HAL_USART_Read_Data>:
DYNALIB_FN(BASE_IDX + 5, hal_usart, HAL_USART_Read_Data, int32_t(HAL_USART_Serial))
   e4164:	b508      	push	{r3, lr}
   e4166:	4b02      	ldr	r3, [pc, #8]	; (e4170 <HAL_USART_Read_Data+0xc>)
   e4168:	681b      	ldr	r3, [r3, #0]
   e416a:	695b      	ldr	r3, [r3, #20]
   e416c:	9301      	str	r3, [sp, #4]
   e416e:	bd08      	pop	{r3, pc}
   e4170:	0003023c 	.word	0x0003023c

000e4174 <HAL_USART_Peek_Data>:
DYNALIB_FN(BASE_IDX + 6, hal_usart, HAL_USART_Peek_Data, int32_t(HAL_USART_Serial))
   e4174:	b508      	push	{r3, lr}
   e4176:	4b02      	ldr	r3, [pc, #8]	; (e4180 <HAL_USART_Peek_Data+0xc>)
   e4178:	681b      	ldr	r3, [r3, #0]
   e417a:	699b      	ldr	r3, [r3, #24]
   e417c:	9301      	str	r3, [sp, #4]
   e417e:	bd08      	pop	{r3, pc}
   e4180:	0003023c 	.word	0x0003023c

000e4184 <HAL_USART_Flush_Data>:
DYNALIB_FN(BASE_IDX + 7, hal_usart, HAL_USART_Flush_Data, void(HAL_USART_Serial))
   e4184:	b508      	push	{r3, lr}
   e4186:	4b02      	ldr	r3, [pc, #8]	; (e4190 <HAL_USART_Flush_Data+0xc>)
   e4188:	681b      	ldr	r3, [r3, #0]
   e418a:	69db      	ldr	r3, [r3, #28]
   e418c:	9301      	str	r3, [sp, #4]
   e418e:	bd08      	pop	{r3, pc}
   e4190:	0003023c 	.word	0x0003023c

000e4194 <HAL_USART_Is_Enabled>:
DYNALIB_FN(BASE_IDX + 8, hal_usart, HAL_USART_Is_Enabled, bool(HAL_USART_Serial))
   e4194:	b508      	push	{r3, lr}
   e4196:	4b02      	ldr	r3, [pc, #8]	; (e41a0 <HAL_USART_Is_Enabled+0xc>)
   e4198:	681b      	ldr	r3, [r3, #0]
   e419a:	6a1b      	ldr	r3, [r3, #32]
   e419c:	9301      	str	r3, [sp, #4]
   e419e:	bd08      	pop	{r3, pc}
   e41a0:	0003023c 	.word	0x0003023c

000e41a4 <HAL_USART_Available_Data_For_Write>:
DYNALIB_FN(BASE_IDX + 9, hal_usart, HAL_USART_Half_Duplex, void(HAL_USART_Serial, bool))
DYNALIB_FN(BASE_IDX + 10, hal_usart, HAL_USART_Available_Data_For_Write, int32_t(HAL_USART_Serial))
   e41a4:	b508      	push	{r3, lr}
   e41a6:	4b02      	ldr	r3, [pc, #8]	; (e41b0 <HAL_USART_Available_Data_For_Write+0xc>)
   e41a8:	681b      	ldr	r3, [r3, #0]
   e41aa:	6a9b      	ldr	r3, [r3, #40]	; 0x28
   e41ac:	9301      	str	r3, [sp, #4]
   e41ae:	bd08      	pop	{r3, pc}
   e41b0:	0003023c 	.word	0x0003023c

000e41b4 <HAL_USB_USART_Init>:
#endif

DYNALIB_BEGIN(hal_usb)

#ifdef USB_CDC_ENABLE
DYNALIB_FN(0, hal_usb, HAL_USB_USART_Init, void(HAL_USB_USART_Serial, const HAL_USB_USART_Config*))
   e41b4:	b508      	push	{r3, lr}
   e41b6:	4b02      	ldr	r3, [pc, #8]	; (e41c0 <HAL_USB_USART_Init+0xc>)
   e41b8:	681b      	ldr	r3, [r3, #0]
   e41ba:	681b      	ldr	r3, [r3, #0]
   e41bc:	9301      	str	r3, [sp, #4]
   e41be:	bd08      	pop	{r3, pc}
   e41c0:	0003024c 	.word	0x0003024c

000e41c4 <HAL_USB_USART_Begin>:
DYNALIB_FN(1, hal_usb, HAL_USB_USART_Begin, void(HAL_USB_USART_Serial, uint32_t, void *))
   e41c4:	b508      	push	{r3, lr}
   e41c6:	4b02      	ldr	r3, [pc, #8]	; (e41d0 <HAL_USB_USART_Begin+0xc>)
   e41c8:	681b      	ldr	r3, [r3, #0]
   e41ca:	685b      	ldr	r3, [r3, #4]
   e41cc:	9301      	str	r3, [sp, #4]
   e41ce:	bd08      	pop	{r3, pc}
   e41d0:	0003024c 	.word	0x0003024c

000e41d4 <HAL_USB_USART_Available_Data>:
DYNALIB_FN(2, hal_usb, HAL_USB_USART_End, void(HAL_USB_USART_Serial))
DYNALIB_FN(3, hal_usb, HAL_USB_USART_Baud_Rate, unsigned int(HAL_USB_USART_Serial))
DYNALIB_FN(4, hal_usb, HAL_USB_USART_Available_Data, int32_t(HAL_USB_USART_Serial))
   e41d4:	b508      	push	{r3, lr}
   e41d6:	4b02      	ldr	r3, [pc, #8]	; (e41e0 <HAL_USB_USART_Available_Data+0xc>)
   e41d8:	681b      	ldr	r3, [r3, #0]
   e41da:	691b      	ldr	r3, [r3, #16]
   e41dc:	9301      	str	r3, [sp, #4]
   e41de:	bd08      	pop	{r3, pc}
   e41e0:	0003024c 	.word	0x0003024c

000e41e4 <HAL_USB_USART_Available_Data_For_Write>:
DYNALIB_FN(5, hal_usb, HAL_USB_USART_Available_Data_For_Write, int32_t(HAL_USB_USART_Serial))
   e41e4:	b508      	push	{r3, lr}
   e41e6:	4b02      	ldr	r3, [pc, #8]	; (e41f0 <HAL_USB_USART_Available_Data_For_Write+0xc>)
   e41e8:	681b      	ldr	r3, [r3, #0]
   e41ea:	695b      	ldr	r3, [r3, #20]
   e41ec:	9301      	str	r3, [sp, #4]
   e41ee:	bd08      	pop	{r3, pc}
   e41f0:	0003024c 	.word	0x0003024c

000e41f4 <HAL_USB_USART_Receive_Data>:
DYNALIB_FN(6, hal_usb, HAL_USB_USART_Receive_Data, int32_t(HAL_USB_USART_Serial, uint8_t))
   e41f4:	b508      	push	{r3, lr}
   e41f6:	4b02      	ldr	r3, [pc, #8]	; (e4200 <HAL_USB_USART_Receive_Data+0xc>)
   e41f8:	681b      	ldr	r3, [r3, #0]
   e41fa:	699b      	ldr	r3, [r3, #24]
   e41fc:	9301      	str	r3, [sp, #4]
   e41fe:	bd08      	pop	{r3, pc}
   e4200:	0003024c 	.word	0x0003024c

000e4204 <HAL_USB_USART_Send_Data>:
DYNALIB_FN(7, hal_usb, HAL_USB_USART_Send_Data, int32_t(HAL_USB_USART_Serial, uint8_t))
   e4204:	b508      	push	{r3, lr}
   e4206:	4b02      	ldr	r3, [pc, #8]	; (e4210 <HAL_USB_USART_Send_Data+0xc>)
   e4208:	681b      	ldr	r3, [r3, #0]
   e420a:	69db      	ldr	r3, [r3, #28]
   e420c:	9301      	str	r3, [sp, #4]
   e420e:	bd08      	pop	{r3, pc}
   e4210:	0003024c 	.word	0x0003024c

000e4214 <HAL_USB_USART_Flush_Data>:
DYNALIB_FN(8, hal_usb, HAL_USB_USART_Flush_Data, void(HAL_USB_USART_Serial))
   e4214:	b508      	push	{r3, lr}
   e4216:	4b02      	ldr	r3, [pc, #8]	; (e4220 <HAL_USB_USART_Flush_Data+0xc>)
   e4218:	681b      	ldr	r3, [r3, #0]
   e421a:	6a1b      	ldr	r3, [r3, #32]
   e421c:	9301      	str	r3, [sp, #4]
   e421e:	bd08      	pop	{r3, pc}
   e4220:	0003024c 	.word	0x0003024c

000e4224 <panic_>:
DYNALIB_FN(9, services, LED_Toggle, void(Led_TypeDef))
DYNALIB_FN(10, services, LED_Fade, void(Led_TypeDef))
DYNALIB_FN(11, services, Get_LED_Brightness, uint8_t(void))

DYNALIB_FN(12, services, set_logger_output, void(debug_output_fn, LoggerOutputLevel)) // Deprecated
DYNALIB_FN(13, services, panic_, void(ePanicCode, void*, void(*)(uint32_t)))
   e4224:	b508      	push	{r3, lr}
   e4226:	4b02      	ldr	r3, [pc, #8]	; (e4230 <panic_+0xc>)
   e4228:	681b      	ldr	r3, [r3, #0]
   e422a:	6b5b      	ldr	r3, [r3, #52]	; 0x34
   e422c:	9301      	str	r3, [sp, #4]
   e422e:	bd08      	pop	{r3, pc}
   e4230:	00030260 	.word	0x00030260

000e4234 <set_system_mode>:
#endif

DYNALIB_BEGIN(system)

DYNALIB_FN(0, system, system_mode, System_Mode_TypeDef(void))
DYNALIB_FN(1, system, set_system_mode, void(System_Mode_TypeDef))
   e4234:	b508      	push	{r3, lr}
   e4236:	4b02      	ldr	r3, [pc, #8]	; (e4240 <set_system_mode+0xc>)
   e4238:	681b      	ldr	r3, [r3, #0]
   e423a:	685b      	ldr	r3, [r3, #4]
   e423c:	9301      	str	r3, [sp, #4]
   e423e:	bd08      	pop	{r3, pc}
   e4240:	00030220 	.word	0x00030220

000e4244 <system_thread_set_state>:
DYNALIB_FN(6, system, system_sleep, int(Spark_Sleep_TypeDef, long, uint32_t, void*))
DYNALIB_FN(7, system, system_sleep_pin, int(uint16_t, uint16_t, long, uint32_t, void*))
DYNALIB_FN(8, system, system_subscribe_event, int(system_event_t, system_event_handler_t*, void*))
DYNALIB_FN(9, system, system_unsubscribe_event, void(system_event_t, system_event_handler_t*, void*))
DYNALIB_FN(10, system, system_button_pushed_duration, uint16_t(uint8_t, void*))
DYNALIB_FN(11, system, system_thread_set_state, void(spark::feature::State, void*))
   e4244:	b508      	push	{r3, lr}
   e4246:	4b02      	ldr	r3, [pc, #8]	; (e4250 <system_thread_set_state+0xc>)
   e4248:	681b      	ldr	r3, [r3, #0]
   e424a:	6adb      	ldr	r3, [r3, #44]	; 0x2c
   e424c:	9301      	str	r3, [sp, #4]
   e424e:	bd08      	pop	{r3, pc}
   e4250:	00030220 	.word	0x00030220

000e4254 <system_ctrl_set_app_request_handler>:
DYNALIB_FN(BASE_IDX + 6, system, led_pattern_period, uint16_t(int, int, void*))
DYNALIB_FN(BASE_IDX + 7, system, system_set_tester_handlers, int(system_tester_handlers_t*, void*))
DYNALIB_FN(BASE_IDX + 8, system, system_format_diag_data, int(const uint16_t*, size_t, unsigned, appender_fn, void*, void*))

// Control requests
DYNALIB_FN(BASE_IDX + 9, system, system_ctrl_set_app_request_handler, int(ctrl_request_handler_fn, void*))
   e4254:	b508      	push	{r3, lr}
   e4256:	4b03      	ldr	r3, [pc, #12]	; (e4264 <system_ctrl_set_app_request_handler+0x10>)
   e4258:	681b      	ldr	r3, [r3, #0]
   e425a:	f8d3 3088 	ldr.w	r3, [r3, #136]	; 0x88
   e425e:	9301      	str	r3, [sp, #4]
   e4260:	bd08      	pop	{r3, pc}
   e4262:	0000      	.short	0x0000
   e4264:	00030220 	.word	0x00030220

000e4268 <system_ctrl_set_result>:
DYNALIB_FN(BASE_IDX + 10, system, system_ctrl_alloc_reply_data, int(ctrl_request*, size_t, void*))
DYNALIB_FN(BASE_IDX + 11, system, system_ctrl_free_request_data, void(ctrl_request*, void*))
DYNALIB_FN(BASE_IDX + 12, system, system_ctrl_set_result, void(ctrl_request*, int, ctrl_completion_handler_fn, void*, void*))
   e4268:	b508      	push	{r3, lr}
   e426a:	4b03      	ldr	r3, [pc, #12]	; (e4278 <system_ctrl_set_result+0x10>)
   e426c:	681b      	ldr	r3, [r3, #0]
   e426e:	f8d3 3094 	ldr.w	r3, [r3, #148]	; 0x94
   e4272:	9301      	str	r3, [sp, #4]
   e4274:	bd08      	pop	{r3, pc}
   e4276:	0000      	.short	0x0000
   e4278:	00030220 	.word	0x00030220

000e427c <spark_set_random_seed_from_cloud_handler>:
DYNALIB_FN(10, system_cloud, spark_unsubscribe, void(void*))
DYNALIB_FN(11, system_cloud, spark_sync_time, bool(void*))
DYNALIB_FN(12, system_cloud, spark_sync_time_pending, bool(void*))
DYNALIB_FN(13, system_cloud, spark_sync_time_last, system_tick_t(time_t*, void*))
DYNALIB_FN(14, system_cloud, spark_set_connection_property, int(unsigned, unsigned, particle::protocol::connection_properties_t*, void*))
DYNALIB_FN(15, system_cloud, spark_set_random_seed_from_cloud_handler, int(void (*handler)(unsigned int), void*))
   e427c:	b508      	push	{r3, lr}
   e427e:	4b02      	ldr	r3, [pc, #8]	; (e4288 <spark_set_random_seed_from_cloud_handler+0xc>)
   e4280:	681b      	ldr	r3, [r3, #0]
   e4282:	6bdb      	ldr	r3, [r3, #60]	; 0x3c
   e4284:	9301      	str	r3, [sp, #4]
   e4286:	bd08      	pop	{r3, pc}
   e4288:	00030244 	.word	0x00030244

000e428c <network_connect>:
#endif

DYNALIB_BEGIN(system_net)

DYNALIB_FN(0, system_net, network_config, const void*(network_handle_t, uint32_t, void*))
DYNALIB_FN(1, system_net, network_connect, void(network_handle_t, uint32_t, uint32_t, void*))
   e428c:	b508      	push	{r3, lr}
   e428e:	4b02      	ldr	r3, [pc, #8]	; (e4298 <network_connect+0xc>)
   e4290:	681b      	ldr	r3, [r3, #0]
   e4292:	685b      	ldr	r3, [r3, #4]
   e4294:	9301      	str	r3, [sp, #4]
   e4296:	bd08      	pop	{r3, pc}
   e4298:	00030240 	.word	0x00030240

000e429c <network_connecting>:
DYNALIB_FN(2, system_net, network_connecting, bool(network_handle_t, uint32_t, void*))
   e429c:	b508      	push	{r3, lr}
   e429e:	4b02      	ldr	r3, [pc, #8]	; (e42a8 <network_connecting+0xc>)
   e42a0:	681b      	ldr	r3, [r3, #0]
   e42a2:	689b      	ldr	r3, [r3, #8]
   e42a4:	9301      	str	r3, [sp, #4]
   e42a6:	bd08      	pop	{r3, pc}
   e42a8:	00030240 	.word	0x00030240

000e42ac <network_disconnect>:
DYNALIB_FN(3, system_net, network_disconnect, void(network_handle_t, uint32_t, void*))
   e42ac:	b508      	push	{r3, lr}
   e42ae:	4b02      	ldr	r3, [pc, #8]	; (e42b8 <network_disconnect+0xc>)
   e42b0:	681b      	ldr	r3, [r3, #0]
   e42b2:	68db      	ldr	r3, [r3, #12]
   e42b4:	9301      	str	r3, [sp, #4]
   e42b6:	bd08      	pop	{r3, pc}
   e42b8:	00030240 	.word	0x00030240

000e42bc <network_ready>:
DYNALIB_FN(4, system_net, network_ready, bool(network_handle_t, uint32_t, void*))
   e42bc:	b508      	push	{r3, lr}
   e42be:	4b02      	ldr	r3, [pc, #8]	; (e42c8 <network_ready+0xc>)
   e42c0:	681b      	ldr	r3, [r3, #0]
   e42c2:	691b      	ldr	r3, [r3, #16]
   e42c4:	9301      	str	r3, [sp, #4]
   e42c6:	bd08      	pop	{r3, pc}
   e42c8:	00030240 	.word	0x00030240

000e42cc <network_on>:
DYNALIB_FN(5, system_net, network_on, void(network_handle_t, uint32_t, uint32_t, void*))
   e42cc:	b508      	push	{r3, lr}
   e42ce:	4b02      	ldr	r3, [pc, #8]	; (e42d8 <network_on+0xc>)
   e42d0:	681b      	ldr	r3, [r3, #0]
   e42d2:	695b      	ldr	r3, [r3, #20]
   e42d4:	9301      	str	r3, [sp, #4]
   e42d6:	bd08      	pop	{r3, pc}
   e42d8:	00030240 	.word	0x00030240

000e42dc <network_off>:
DYNALIB_FN(6, system_net, network_off, void(network_handle_t, uint32_t, uint32_t, void*))
   e42dc:	b508      	push	{r3, lr}
   e42de:	4b02      	ldr	r3, [pc, #8]	; (e42e8 <network_off+0xc>)
   e42e0:	681b      	ldr	r3, [r3, #0]
   e42e2:	699b      	ldr	r3, [r3, #24]
   e42e4:	9301      	str	r3, [sp, #4]
   e42e6:	bd08      	pop	{r3, pc}
   e42e8:	00030240 	.word	0x00030240

000e42ec <network_listen>:
DYNALIB_FN(7, system_net, network_listen, void(network_handle_t, uint32_t, void*))
   e42ec:	b508      	push	{r3, lr}
   e42ee:	4b02      	ldr	r3, [pc, #8]	; (e42f8 <network_listen+0xc>)
   e42f0:	681b      	ldr	r3, [r3, #0]
   e42f2:	69db      	ldr	r3, [r3, #28]
   e42f4:	9301      	str	r3, [sp, #4]
   e42f6:	bd08      	pop	{r3, pc}
   e42f8:	00030240 	.word	0x00030240

000e42fc <network_listening>:
DYNALIB_FN(8, system_net, network_listening, bool(network_handle_t, uint32_t, void*))
   e42fc:	b508      	push	{r3, lr}
   e42fe:	4b02      	ldr	r3, [pc, #8]	; (e4308 <network_listening+0xc>)
   e4300:	681b      	ldr	r3, [r3, #0]
   e4302:	6a1b      	ldr	r3, [r3, #32]
   e4304:	9301      	str	r3, [sp, #4]
   e4306:	bd08      	pop	{r3, pc}
   e4308:	00030240 	.word	0x00030240

000e430c <network_set_listen_timeout>:
DYNALIB_FN(9, system_net, network_has_credentials, bool(network_handle_t, uint32_t, void*))
DYNALIB_FN(10, system_net, network_set_credentials, int(network_handle_t, uint32_t, NetworkCredentials*, void*))
DYNALIB_FN(11, system_net, network_clear_credentials, bool(network_handle_t, uint32_t, NetworkCredentials*, void*))
DYNALIB_FN(12, system_net, network_set_listen_timeout, void(network_handle_t, uint16_t, void*))
   e430c:	b508      	push	{r3, lr}
   e430e:	4b02      	ldr	r3, [pc, #8]	; (e4318 <network_set_listen_timeout+0xc>)
   e4310:	681b      	ldr	r3, [r3, #0]
   e4312:	6b1b      	ldr	r3, [r3, #48]	; 0x30
   e4314:	9301      	str	r3, [sp, #4]
   e4316:	bd08      	pop	{r3, pc}
   e4318:	00030240 	.word	0x00030240

000e431c <network_get_listen_timeout>:
DYNALIB_FN(13, system_net, network_get_listen_timeout, uint16_t(network_handle_t, uint32_t, void*))
   e431c:	b508      	push	{r3, lr}
   e431e:	4b02      	ldr	r3, [pc, #8]	; (e4328 <network_get_listen_timeout+0xc>)
   e4320:	681b      	ldr	r3, [r3, #0]
   e4322:	6b5b      	ldr	r3, [r3, #52]	; 0x34
   e4324:	9301      	str	r3, [sp, #4]
   e4326:	bd08      	pop	{r3, pc}
   e4328:	00030240 	.word	0x00030240

000e432c <malloc>:
#include <assert.h>
#endif

DYNALIB_BEGIN(rt)

DYNALIB_FN(0, rt, malloc, void*(size_t))
   e432c:	b508      	push	{r3, lr}
   e432e:	4b02      	ldr	r3, [pc, #8]	; (e4338 <malloc+0xc>)
   e4330:	681b      	ldr	r3, [r3, #0]
   e4332:	681b      	ldr	r3, [r3, #0]
   e4334:	9301      	str	r3, [sp, #4]
   e4336:	bd08      	pop	{r3, pc}
   e4338:	0003021c 	.word	0x0003021c

000e433c <free>:
DYNALIB_FN(1, rt, free, void(void*))
   e433c:	b508      	push	{r3, lr}
   e433e:	4b02      	ldr	r3, [pc, #8]	; (e4348 <free+0xc>)
   e4340:	681b      	ldr	r3, [r3, #0]
   e4342:	685b      	ldr	r3, [r3, #4]
   e4344:	9301      	str	r3, [sp, #4]
   e4346:	bd08      	pop	{r3, pc}
   e4348:	0003021c 	.word	0x0003021c

000e434c <abort>:
DYNALIB_FN(6, rt, siscanf, int(const char*, const char*, ...))
DYNALIB_FN(7, rt, snprintf, int(char*, size_t, const char*, ...))
DYNALIB_FN(8, rt, sniprintf, int(char*, size_t, const char*, ...))
DYNALIB_FN(9, rt, vsnprintf, int(char*, size_t, const char*, va_list))
DYNALIB_FN(10, rt, vsniprintf, int(char*, size_t, const char*, va_list))
DYNALIB_FN(11, rt, abort, void(void))
   e434c:	b508      	push	{r3, lr}
   e434e:	4b02      	ldr	r3, [pc, #8]	; (e4358 <abort+0xc>)
   e4350:	681b      	ldr	r3, [r3, #0]
   e4352:	6adb      	ldr	r3, [r3, #44]	; 0x2c
   e4354:	9301      	str	r3, [sp, #4]
   e4356:	bd08      	pop	{r3, pc}
   e4358:	0003021c 	.word	0x0003021c

000e435c <__errno>:
DYNALIB_FN(12, rt, _malloc_r, void*(struct _reent*, size_t))
DYNALIB_FN(13, rt, _free_r, void(struct _reent*, void*))
DYNALIB_FN(14, rt, _realloc_r, void*(struct _reent*, void*, size_t))
DYNALIB_FN(15, rt, __errno, int*())
   e435c:	b508      	push	{r3, lr}
   e435e:	4b02      	ldr	r3, [pc, #8]	; (e4368 <__errno+0xc>)
   e4360:	681b      	ldr	r3, [r3, #0]
   e4362:	6bdb      	ldr	r3, [r3, #60]	; 0x3c
   e4364:	9301      	str	r3, [sp, #4]
   e4366:	bd08      	pop	{r3, pc}
   e4368:	0003021c 	.word	0x0003021c

000e436c <__assert_func>:
// on Gen 2 platforms without breaking inter-module dependencies.
// RT is currently being imported into system-part1 from system-part2,
// which is the reverse direction.

#if defined(DYNALIB_IMPORT) && !defined(RT_DYNALIB_NO_DEPENDENCY_BREAKING_IMPORTS)
DYNALIB_FN(16, rt, __assert_func, void(const char*, int, const char*, const char*))
   e436c:	b508      	push	{r3, lr}
   e436e:	4b02      	ldr	r3, [pc, #8]	; (e4378 <__assert_func+0xc>)
   e4370:	681b      	ldr	r3, [r3, #0]
   e4372:	6c1b      	ldr	r3, [r3, #64]	; 0x40
   e4374:	9301      	str	r3, [sp, #4]
   e4376:	bd08      	pop	{r3, pc}
   e4378:	0003021c 	.word	0x0003021c

000e437c <_GLOBAL__sub_I__ZN8particle3ble13WiringBleLock6mutex_E>:
    /**
     * Creates a shared mutex.
     */
    RecursiveMutex(os_mutex_recursive_t handle) : handle_(handle) {}

    RecursiveMutex() : handle_(nullptr)
   e437c:	4802      	ldr	r0, [pc, #8]	; (e4388 <_GLOBAL__sub_I__ZN8particle3ble13WiringBleLock6mutex_E+0xc>)
   e437e:	2300      	movs	r3, #0
   e4380:	6003      	str	r3, [r0, #0]
    {
        os_mutex_recursive_create(&handle_);
   e4382:	f7ff be27 	b.w	e3fd4 <os_mutex_recursive_create>
   e4386:	bf00      	nop
   e4388:	2003e4d0 	.word	0x2003e4d0

000e438c <_ZNSt14_Function_baseD1Ev>:
	}
      };

    _Function_base() : _M_manager(nullptr) { }

    ~_Function_base()
   e438c:	b510      	push	{r4, lr}
    {
      if (_M_manager)
   e438e:	6883      	ldr	r3, [r0, #8]
	}
      };

    _Function_base() : _M_manager(nullptr) { }

    ~_Function_base()
   e4390:	4604      	mov	r4, r0
    {
      if (_M_manager)
   e4392:	b113      	cbz	r3, e439a <_ZNSt14_Function_baseD1Ev+0xe>
	_M_manager(_M_functor, _M_functor, __destroy_functor);
   e4394:	2203      	movs	r2, #3
   e4396:	4601      	mov	r1, r0
   e4398:	4798      	blx	r3
    }
   e439a:	4620      	mov	r0, r4
   e439c:	bd10      	pop	{r4, pc}

000e439e <_ZN5spark13EthernetClass9listeningEv>:
    uint16_t getListenTimeout(void) {
        return network_get_listen_timeout(*this, 0, NULL);
    }

    bool listening(void) {
        return network_listening(*this, 0, NULL);
   e439e:	2200      	movs	r2, #0
   e43a0:	4611      	mov	r1, r2
   e43a2:	6840      	ldr	r0, [r0, #4]
   e43a4:	f7ff bfaa 	b.w	e42fc <network_listening>

000e43a8 <_ZN5spark13EthernetClass16getListenTimeoutEv>:
    void setListenTimeout(uint16_t timeout) {
        network_set_listen_timeout(*this, timeout, NULL);
    }

    uint16_t getListenTimeout(void) {
        return network_get_listen_timeout(*this, 0, NULL);
   e43a8:	2200      	movs	r2, #0
   e43aa:	4611      	mov	r1, r2
   e43ac:	6840      	ldr	r0, [r0, #4]
   e43ae:	f7ff bfb5 	b.w	e431c <network_get_listen_timeout>

000e43b2 <_ZN5spark13EthernetClass16setListenTimeoutEt>:
    void listen(bool begin=true) {
        network_listen(*this, begin ? 0 : 1, NULL);
    }

    void setListenTimeout(uint16_t timeout) {
        network_set_listen_timeout(*this, timeout, NULL);
   e43b2:	2200      	movs	r2, #0
   e43b4:	6840      	ldr	r0, [r0, #4]
   e43b6:	f7ff bfa9 	b.w	e430c <network_set_listen_timeout>

000e43ba <_ZN5spark13EthernetClass6listenEb>:
    void disconnect() {
        network_disconnect(*this, NETWORK_DISCONNECT_REASON_USER, NULL);
    }

    void listen(bool begin=true) {
        network_listen(*this, begin ? 0 : 1, NULL);
   e43ba:	2200      	movs	r2, #0
   e43bc:	f081 0101 	eor.w	r1, r1, #1
   e43c0:	6840      	ldr	r0, [r0, #4]
   e43c2:	f7ff bf93 	b.w	e42ec <network_listen>

000e43c6 <_ZN5spark13EthernetClass3offEv>:
    void on() {
        network_on(*this, 0, 0, NULL);
    }

    void off() {
        network_off(*this, 0, 0, NULL);
   e43c6:	2300      	movs	r3, #0
   e43c8:	461a      	mov	r2, r3
   e43ca:	4619      	mov	r1, r3
   e43cc:	6840      	ldr	r0, [r0, #4]
   e43ce:	f7ff bf85 	b.w	e42dc <network_off>

000e43d2 <_ZN5spark13EthernetClass2onEv>:
    EthernetClass() :
            NetworkClass(NETWORK_INTERFACE_ETHERNET) {
    }

    void on() {
        network_on(*this, 0, 0, NULL);
   e43d2:	2300      	movs	r3, #0
   e43d4:	461a      	mov	r2, r3
   e43d6:	4619      	mov	r1, r3
   e43d8:	6840      	ldr	r0, [r0, #4]
   e43da:	f7ff bf77 	b.w	e42cc <network_on>

000e43de <_ZN5spark13EthernetClass5readyEv>:
    bool listening(void) {
        return network_listening(*this, 0, NULL);
    }

    bool ready() {
        return network_ready(*this, 0,  NULL);
   e43de:	2200      	movs	r2, #0
   e43e0:	4611      	mov	r1, r2
   e43e2:	6840      	ldr	r0, [r0, #4]
   e43e4:	f7ff bf6a 	b.w	e42bc <network_ready>

000e43e8 <_ZN5spark13EthernetClass10connectingEv>:
    void connect(unsigned flags=0) {
        network_connect(*this, flags, 0, NULL);
    }

    bool connecting(void) {
        return network_connecting(*this, 0, NULL);
   e43e8:	2200      	movs	r2, #0
   e43ea:	4611      	mov	r1, r2
   e43ec:	6840      	ldr	r0, [r0, #4]
   e43ee:	f7ff bf55 	b.w	e429c <network_connecting>

000e43f2 <_ZN5spark13EthernetClass10disconnectEv>:
    }

    void disconnect() {
        network_disconnect(*this, NETWORK_DISCONNECT_REASON_USER, NULL);
   e43f2:	2200      	movs	r2, #0
   e43f4:	2102      	movs	r1, #2
   e43f6:	6840      	ldr	r0, [r0, #4]
   e43f8:	f7ff bf58 	b.w	e42ac <network_disconnect>

000e43fc <_ZN5spark13EthernetClass7connectEj>:
    void off() {
        network_off(*this, 0, 0, NULL);
    }

    void connect(unsigned flags=0) {
        network_connect(*this, flags, 0, NULL);
   e43fc:	2300      	movs	r3, #0
   e43fe:	461a      	mov	r2, r3
   e4400:	6840      	ldr	r0, [r0, #4]
   e4402:	f7ff bf43 	b.w	e428c <network_connect>
	...

000e4408 <_GLOBAL__sub_I__ZN5spark8EthernetE>:
    static NetworkClass& from(network_interface_t nif);

    virtual IPAddress resolve(const char* name);

    explicit NetworkClass(network_interface_t iface)
            : iface_(iface) {
   e4408:	4b02      	ldr	r3, [pc, #8]	; (e4414 <_GLOBAL__sub_I__ZN5spark8EthernetE+0xc>)
   e440a:	2203      	movs	r2, #3
   e440c:	605a      	str	r2, [r3, #4]
    }

class EthernetClass : public NetworkClass {
public:
    EthernetClass() :
            NetworkClass(NETWORK_INTERFACE_ETHERNET) {
   e440e:	4a02      	ldr	r2, [pc, #8]	; (e4418 <_GLOBAL__sub_I__ZN5spark8EthernetE+0x10>)
   e4410:	601a      	str	r2, [r3, #0]
   e4412:	4770      	bx	lr
   e4414:	2003e4d4 	.word	0x2003e4d4
   e4418:	000eb2a0 	.word	0x000eb2a0

000e441c <_ZN7TwoWireD1Ev>:
private:
  HAL_I2C_Interface _i2c;

public:
  TwoWire(HAL_I2C_Interface i2c);
  virtual ~TwoWire() {};
   e441c:	4770      	bx	lr

000e441e <_ZN7TwoWire5writeEPKhj>:

// must be called in:
// slave tx event callback
// or after beginTransmission(address)
size_t TwoWire::write(const uint8_t *data, size_t quantity)
{
   e441e:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
   e4420:	4606      	mov	r6, r0
   e4422:	4615      	mov	r5, r2
   e4424:	460c      	mov	r4, r1
   e4426:	188f      	adds	r7, r1, r2
  // in master/slave transmitter mode
  for(size_t i = 0; i < quantity; ++i)
   e4428:	42bc      	cmp	r4, r7
   e442a:	d006      	beq.n	e443a <_ZN7TwoWire5writeEPKhj+0x1c>
  {
    write(data[i]);
   e442c:	6833      	ldr	r3, [r6, #0]
   e442e:	f814 1b01 	ldrb.w	r1, [r4], #1
   e4432:	689b      	ldr	r3, [r3, #8]
   e4434:	4630      	mov	r0, r6
   e4436:	4798      	blx	r3
// slave tx event callback
// or after beginTransmission(address)
size_t TwoWire::write(const uint8_t *data, size_t quantity)
{
  // in master/slave transmitter mode
  for(size_t i = 0; i < quantity; ++i)
   e4438:	e7f6      	b.n	e4428 <_ZN7TwoWire5writeEPKhj+0xa>
  {
    write(data[i]);
  }

  return quantity;
}
   e443a:	4628      	mov	r0, r5
   e443c:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}

000e443e <_ZN7TwoWire5writeEh>:
// must be called in:
// slave tx event callback
// or after beginTransmission(address)
size_t TwoWire::write(uint8_t data)
{
  return HAL_I2C_Write_Data(_i2c, data, NULL);
   e443e:	2200      	movs	r2, #0
   e4440:	7c00      	ldrb	r0, [r0, #16]
   e4442:	f7ff be17 	b.w	e4074 <HAL_I2C_Write_Data>

000e4446 <_ZN7TwoWire9availableEv>:
// must be called in:
// slave rx event callback
// or after requestFrom(address, numBytes)
int TwoWire::available(void)
{
  return HAL_I2C_Available_Data(_i2c, NULL);
   e4446:	2100      	movs	r1, #0
   e4448:	7c00      	ldrb	r0, [r0, #16]
   e444a:	f7ff be1b 	b.w	e4084 <HAL_I2C_Available_Data>

000e444e <_ZN7TwoWire4readEv>:
// must be called in:
// slave rx event callback
// or after requestFrom(address, numBytes)
int TwoWire::read(void)
{
  return HAL_I2C_Read_Data(_i2c, NULL);
   e444e:	2100      	movs	r1, #0
   e4450:	7c00      	ldrb	r0, [r0, #16]
   e4452:	f7ff be1f 	b.w	e4094 <HAL_I2C_Read_Data>

000e4456 <_ZN7TwoWire4peekEv>:
// must be called in:
// slave rx event callback
// or after requestFrom(address, numBytes)
int TwoWire::peek(void)
{
  return HAL_I2C_Peek_Data(_i2c, NULL);
   e4456:	2100      	movs	r1, #0
   e4458:	7c00      	ldrb	r0, [r0, #16]
   e445a:	f7ff be23 	b.w	e40a4 <HAL_I2C_Peek_Data>

000e445e <_ZN7TwoWire5flushEv>:
}

void TwoWire::flush(void)
{
  HAL_I2C_Flush_Data(_i2c, NULL);
   e445e:	2100      	movs	r1, #0
   e4460:	7c00      	ldrb	r0, [r0, #16]
   e4462:	f7ff be27 	b.w	e40b4 <HAL_I2C_Flush_Data>

000e4466 <_ZN7TwoWireD0Ev>:
   e4466:	b510      	push	{r4, lr}
   e4468:	2114      	movs	r1, #20
   e446a:	4604      	mov	r4, r0
   e446c:	f000 fcc7 	bl	e4dfe <_ZdlPvj>
   e4470:	4620      	mov	r0, r4
   e4472:	bd10      	pop	{r4, pc}

000e4474 <_ZN7TwoWireC1E17HAL_I2C_Interface>:
#include "i2c_hal.h"
#include "spark_wiring_thread.h"

// Constructors ////////////////////////////////////////////////////////////////

TwoWire::TwoWire(HAL_I2C_Interface i2c)
   e4474:	b510      	push	{r4, lr}
   e4476:	4604      	mov	r4, r0
    virtual int available() = 0;
    virtual int read() = 0;
    virtual int peek() = 0;
    virtual void flush() = 0;

    Stream() {_timeout=1000;}
   e4478:	f44f 737a 	mov.w	r3, #1000	; 0x3e8
   e447c:	4608      	mov	r0, r1
   e447e:	60a3      	str	r3, [r4, #8]
  protected:
    void setWriteError(int err = 1) { write_error = err; }
    size_t printf_impl(bool newline, const char* format, ...);

  public:
    Print() : write_error(0) {}
   e4480:	2100      	movs	r1, #0
   e4482:	4b04      	ldr	r3, [pc, #16]	; (e4494 <_ZN7TwoWireC1E17HAL_I2C_Interface+0x20>)
{
  _i2c = i2c;
   e4484:	7420      	strb	r0, [r4, #16]
   e4486:	6061      	str	r1, [r4, #4]
#include "i2c_hal.h"
#include "spark_wiring_thread.h"

// Constructors ////////////////////////////////////////////////////////////////

TwoWire::TwoWire(HAL_I2C_Interface i2c)
   e4488:	6023      	str	r3, [r4, #0]
{
  _i2c = i2c;
  HAL_I2C_Init(_i2c, NULL);
   e448a:	f7ff fe23 	bl	e40d4 <HAL_I2C_Init>

}
   e448e:	4620      	mov	r0, r4
   e4490:	bd10      	pop	{r4, pc}
   e4492:	bf00      	nop
   e4494:	000eb2d4 	.word	0x000eb2d4

000e4498 <_ZN7TwoWire9isEnabledEv>:
  HAL_I2C_Set_Callback_On_Request(_i2c, function, NULL);
}

bool TwoWire::isEnabled()
{
  return HAL_I2C_Is_Enabled(_i2c, NULL);
   e4498:	2100      	movs	r1, #0
   e449a:	7c00      	ldrb	r0, [r0, #16]
   e449c:	f7ff be12 	b.w	e40c4 <HAL_I2C_Is_Enabled>

000e44a0 <_ZN9IPAddressD1Ev>:
    IPAddress(uint8_t first_octet, uint8_t second_octet, uint8_t third_octet, uint8_t fourth_octet);
    IPAddress(uint32_t address);
    IPAddress(const uint8_t* address);
    IPAddress(const HAL_IPAddress& address);

    virtual ~IPAddress() {}
   e44a0:	4770      	bx	lr

000e44a2 <_ZN9IPAddressD0Ev>:
   e44a2:	b510      	push	{r4, lr}
   e44a4:	2118      	movs	r1, #24
   e44a6:	4604      	mov	r4, r0
   e44a8:	f000 fca9 	bl	e4dfe <_ZdlPvj>
   e44ac:	4620      	mov	r0, r4
   e44ae:	bd10      	pop	{r4, pc}

000e44b0 <_ZNK9IPAddress7printToER5Print>:
#endif // Wiring_IPv6
	return address.ipv4==that.address.ipv4;
}

size_t IPAddress::printTo(Print& p) const
{
   e44b0:	b5f0      	push	{r4, r5, r6, r7, lr}
#if Wiring_IPv6
#if HAL_USE_INET_HAL_POSIX
	if (address.v==6) {
   e44b2:	7d03      	ldrb	r3, [r0, #20]
   e44b4:	2b06      	cmp	r3, #6
#endif // Wiring_IPv6
	return address.ipv4==that.address.ipv4;
}

size_t IPAddress::printTo(Print& p) const
{
   e44b6:	b08d      	sub	sp, #52	; 0x34
   e44b8:	460e      	mov	r6, r1
   e44ba:	f100 0704 	add.w	r7, r0, #4
   e44be:	f04f 0400 	mov.w	r4, #0
#if Wiring_IPv6
#if HAL_USE_INET_HAL_POSIX
	if (address.v==6) {
   e44c2:	d002      	beq.n	e44ca <_ZNK9IPAddress7printToER5Print+0x1a>
   e44c4:	f100 0508 	add.w	r5, r0, #8
   e44c8:	e018      	b.n	e44fc <_ZNK9IPAddress7printToER5Print+0x4c>
		char buf[INET6_ADDRSTRLEN+1];
		buf[0] = 0;
   e44ca:	ad0c      	add	r5, sp, #48	; 0x30
		inet_inet_ntop(AF_INET6, address.ipv6, buf, sizeof(buf));
   e44cc:	4639      	mov	r1, r7
{
#if Wiring_IPv6
#if HAL_USE_INET_HAL_POSIX
	if (address.v==6) {
		char buf[INET6_ADDRSTRLEN+1];
		buf[0] = 0;
   e44ce:	f805 4d30 	strb.w	r4, [r5, #-48]!
		inet_inet_ntop(AF_INET6, address.ipv6, buf, sizeof(buf));
   e44d2:	232f      	movs	r3, #47	; 0x2f
   e44d4:	462a      	mov	r2, r5
   e44d6:	200a      	movs	r0, #10
   e44d8:	f7ff fe04 	bl	e40e4 <inet_inet_ntop>
    void clearWriteError() { setWriteError(0); }

    virtual size_t write(uint8_t) = 0;
    size_t write(const char *str) {
      if (str == NULL) return 0;
      return write((const uint8_t *)str, strlen(str));
   e44dc:	4628      	mov	r0, r5
   e44de:	f003 f9ad 	bl	e783c <strlen>
   e44e2:	6833      	ldr	r3, [r6, #0]
   e44e4:	4602      	mov	r2, r0
   e44e6:	68db      	ldr	r3, [r3, #12]
   e44e8:	4629      	mov	r1, r5
   e44ea:	4630      	mov	r0, r6
   e44ec:	4798      	blx	r3
   e44ee:	e00f      	b.n	e4510 <_ZNK9IPAddress7printToER5Print+0x60>
#endif // HAL_USE_INET_HAL_POSIX
#endif // Wiring_IPv6
    size_t n = 0;
    for (int i = 0; i < 4; i++)
    {
        if (n)
   e44f0:	b124      	cbz	r4, e44fc <_ZNK9IPAddress7printToER5Print+0x4c>
            n += p.print('.');
   e44f2:	212e      	movs	r1, #46	; 0x2e
   e44f4:	4630      	mov	r0, r6
   e44f6:	f000 f9cd 	bl	e4894 <_ZN5Print5printEc>
   e44fa:	4404      	add	r4, r0
        n += p.print((*this)[i], DEC);
   e44fc:	f815 1d01 	ldrb.w	r1, [r5, #-1]!
   e4500:	220a      	movs	r2, #10
   e4502:	4630      	mov	r0, r6
   e4504:	f000 f9f6 	bl	e48f4 <_ZN5Print5printEhi>
#else
#pragma message "HAL_USE_INET_HAL_POSIX is required for IPv6 support in IPAddress::printTo()"
#endif // HAL_USE_INET_HAL_POSIX
#endif // Wiring_IPv6
    size_t n = 0;
    for (int i = 0; i < 4; i++)
   e4508:	42bd      	cmp	r5, r7
    {
        if (n)
            n += p.print('.');
        n += p.print((*this)[i], DEC);
   e450a:	4404      	add	r4, r0
#else
#pragma message "HAL_USE_INET_HAL_POSIX is required for IPv6 support in IPAddress::printTo()"
#endif // HAL_USE_INET_HAL_POSIX
#endif // Wiring_IPv6
    size_t n = 0;
    for (int i = 0; i < 4; i++)
   e450c:	d1f0      	bne.n	e44f0 <_ZNK9IPAddress7printToER5Print+0x40>
    {
        if (n)
            n += p.print('.');
        n += p.print((*this)[i], DEC);
   e450e:	4620      	mov	r0, r4
    }
    return n;
}
   e4510:	b00d      	add	sp, #52	; 0x34
   e4512:	bdf0      	pop	{r4, r5, r6, r7, pc}

000e4514 <_ZN9IPAddressC1Ev>:

#if HAL_USE_INET_HAL_POSIX
#include <arpa/inet.h>
#endif // HAL_USE_INET_HAL_POSIX

IPAddress::IPAddress()
   e4514:	b510      	push	{r4, lr}
   e4516:	4b05      	ldr	r3, [pc, #20]	; (e452c <_ZN9IPAddressC1Ev+0x18>)
   e4518:	4604      	mov	r4, r0
        return address;
    }

    virtual size_t printTo(Print& p) const;

    void clear() { memset(&address, 0, sizeof (address)); }
   e451a:	2211      	movs	r2, #17
   e451c:	f840 3b04 	str.w	r3, [r0], #4
   e4520:	2100      	movs	r1, #0
   e4522:	f003 f951 	bl	e77c8 <memset>
{
    clear();
}
   e4526:	4620      	mov	r0, r4
   e4528:	bd10      	pop	{r4, pc}
   e452a:	bf00      	nop
   e452c:	000eb2fc 	.word	0x000eb2fc

000e4530 <_ZN9IPAddressC1ERK16_HAL_IPAddress_t>:

IPAddress::IPAddress(const HAL_IPAddress& address)
   e4530:	4603      	mov	r3, r0
   e4532:	4a07      	ldr	r2, [pc, #28]	; (e4550 <_ZN9IPAddressC1ERK16_HAL_IPAddress_t+0x20>)
   e4534:	b510      	push	{r4, lr}
   e4536:	f843 2b04 	str.w	r2, [r3], #4
{
    memcpy(&this->address, &address, sizeof(address));
   e453a:	f101 0210 	add.w	r2, r1, #16
   e453e:	f851 4b04 	ldr.w	r4, [r1], #4
   e4542:	f843 4b04 	str.w	r4, [r3], #4
   e4546:	4291      	cmp	r1, r2
   e4548:	d1f9      	bne.n	e453e <_ZN9IPAddressC1ERK16_HAL_IPAddress_t+0xe>
   e454a:	780a      	ldrb	r2, [r1, #0]
   e454c:	701a      	strb	r2, [r3, #0]
}
   e454e:	bd10      	pop	{r4, pc}
   e4550:	000eb2fc 	.word	0x000eb2fc

000e4554 <_ZN9IPAddress8set_ipv4Ehhhh>:
    return address.ipv4!=0;
#endif
}

void IPAddress::set_ipv4(uint8_t b0, uint8_t b1, uint8_t b2, uint8_t b3)
{
   e4554:	b510      	push	{r4, lr}
    address.ipv4 = b0<<24 | b1 << 16 | b2 << 8 | b3;
   e4556:	f89d 4008 	ldrb.w	r4, [sp, #8]
   e455a:	ea44 2303 	orr.w	r3, r4, r3, lsl #8
   e455e:	ea43 4202 	orr.w	r2, r3, r2, lsl #16
   e4562:	ea42 6101 	orr.w	r1, r2, r1, lsl #24
        return &address;
    }

    inline void setVersion(uint8_t version) {
#if HAL_IPv6
        address.v = version;
   e4566:	2304      	movs	r3, #4
   e4568:	6041      	str	r1, [r0, #4]
   e456a:	7503      	strb	r3, [r0, #20]
   e456c:	bd10      	pop	{r4, pc}

000e456e <_ZN9IPAddressaSEPKh>:
    setVersion(4);
}

IPAddress& IPAddress::operator=(const uint8_t* address)
{
   e456e:	b537      	push	{r0, r1, r2, r4, r5, lr}
    set_ipv4(address[0], address[1], address[2], address[3]);
   e4570:	780d      	ldrb	r5, [r1, #0]
   e4572:	788b      	ldrb	r3, [r1, #2]
   e4574:	784a      	ldrb	r2, [r1, #1]
   e4576:	78c9      	ldrb	r1, [r1, #3]
   e4578:	9100      	str	r1, [sp, #0]
   e457a:	4629      	mov	r1, r5
   e457c:	f7ff ffea 	bl	e4554 <_ZN9IPAddress8set_ipv4Ehhhh>
    return *this;
}
   e4580:	b003      	add	sp, #12
   e4582:	bd30      	pop	{r4, r5, pc}

000e4584 <_GLOBAL__sub_I__ZN5spark3LogE>:
    // This handler doesn't support direct logging
}

// spark::Logger
inline spark::Logger::Logger(const char *name) :
        name_(name) {
   e4584:	4b01      	ldr	r3, [pc, #4]	; (e458c <_GLOBAL__sub_I__ZN5spark3LogE+0x8>)
   e4586:	4a02      	ldr	r2, [pc, #8]	; (e4590 <_GLOBAL__sub_I__ZN5spark3LogE+0xc>)
   e4588:	601a      	str	r2, [r3, #0]
   e458a:	4770      	bx	lr
   e458c:	2003e4dc 	.word	0x2003e4dc
   e4590:	000eb376 	.word	0x000eb376

000e4594 <_ZN5spark9MeshClass9listeningEv>:
    uint16_t getListenTimeout(void) {
        return network_get_listen_timeout(*this, 0, NULL);
    }

    bool listening(void) {
        return network_listening(*this, 0, NULL);
   e4594:	2200      	movs	r2, #0
   e4596:	4611      	mov	r1, r2
   e4598:	6840      	ldr	r0, [r0, #4]
   e459a:	f7ff beaf 	b.w	e42fc <network_listening>

000e459e <_ZN5spark9MeshClass16getListenTimeoutEv>:
    void setListenTimeout(uint16_t timeout) {
        network_set_listen_timeout(*this, timeout, NULL);
    }

    uint16_t getListenTimeout(void) {
        return network_get_listen_timeout(*this, 0, NULL);
   e459e:	2200      	movs	r2, #0
   e45a0:	4611      	mov	r1, r2
   e45a2:	6840      	ldr	r0, [r0, #4]
   e45a4:	f7ff beba 	b.w	e431c <network_get_listen_timeout>

000e45a8 <_ZN5spark9MeshClass16setListenTimeoutEt>:
    void listen(bool begin=true) {
        network_listen(*this, begin ? 0 : 1, NULL);
    }

    void setListenTimeout(uint16_t timeout) {
        network_set_listen_timeout(*this, timeout, NULL);
   e45a8:	2200      	movs	r2, #0
   e45aa:	6840      	ldr	r0, [r0, #4]
   e45ac:	f7ff beae 	b.w	e430c <network_set_listen_timeout>

000e45b0 <_ZN5spark9MeshClass6listenEb>:
    void disconnect() {
        network_disconnect(*this, NETWORK_DISCONNECT_REASON_USER, NULL);
    }

    void listen(bool begin=true) {
        network_listen(*this, begin ? 0 : 1, NULL);
   e45b0:	2200      	movs	r2, #0
   e45b2:	f081 0101 	eor.w	r1, r1, #1
   e45b6:	6840      	ldr	r0, [r0, #4]
   e45b8:	f7ff be98 	b.w	e42ec <network_listen>

000e45bc <_ZN5spark9MeshClass3offEv>:
    void on() {
        network_on(*this, 0, 0, NULL);
    }

    void off() {
        network_off(*this, 1, 0, NULL);
   e45bc:	2300      	movs	r3, #0
   e45be:	461a      	mov	r2, r3
   e45c0:	2101      	movs	r1, #1
   e45c2:	6840      	ldr	r0, [r0, #4]
   e45c4:	f7ff be8a 	b.w	e42dc <network_off>

000e45c8 <_ZN5spark9MeshClass2onEv>:
    MeshClass() :
            NetworkClass(NETWORK_INTERFACE_MESH) {
    }

    void on() {
        network_on(*this, 0, 0, NULL);
   e45c8:	2300      	movs	r3, #0
   e45ca:	461a      	mov	r2, r3
   e45cc:	4619      	mov	r1, r3
   e45ce:	6840      	ldr	r0, [r0, #4]
   e45d0:	f7ff be7c 	b.w	e42cc <network_on>

000e45d4 <_ZN5spark9MeshClass5readyEv>:
    bool listening(void) {
        return network_listening(*this, 0, NULL);
    }

    bool ready() {
        return network_ready(*this, 0,  NULL);
   e45d4:	2200      	movs	r2, #0
   e45d6:	4611      	mov	r1, r2
   e45d8:	6840      	ldr	r0, [r0, #4]
   e45da:	f7ff be6f 	b.w	e42bc <network_ready>

000e45de <_ZN5spark9MeshClass10connectingEv>:
    void connect(unsigned flags=0) {
        network_connect(*this, flags, 0, NULL);
    }

    bool connecting(void) {
        return network_connecting(*this, 0, NULL);
   e45de:	2200      	movs	r2, #0
   e45e0:	4611      	mov	r1, r2
   e45e2:	6840      	ldr	r0, [r0, #4]
   e45e4:	f7ff be5a 	b.w	e429c <network_connecting>

000e45e8 <_ZN5spark9MeshClass10disconnectEv>:
    }

    void disconnect() {
        network_disconnect(*this, NETWORK_DISCONNECT_REASON_USER, NULL);
   e45e8:	2200      	movs	r2, #0
   e45ea:	2102      	movs	r1, #2
   e45ec:	6840      	ldr	r0, [r0, #4]
   e45ee:	f7ff be5d 	b.w	e42ac <network_disconnect>

000e45f2 <_ZN5spark9MeshClass7connectEj>:
    void off() {
        network_off(*this, 1, 0, NULL);
    }

    void connect(unsigned flags=0) {
        network_connect(*this, flags, 0, NULL);
   e45f2:	2300      	movs	r3, #0
   e45f4:	461a      	mov	r2, r3
   e45f6:	6840      	ldr	r0, [r0, #4]
   e45f8:	f7ff be48 	b.w	e428c <network_connect>

000e45fc <_ZNKSt14default_deleteIN6Thread4DataEEclEPS1_.isra.6>:
	       enable_if<is_convertible<_Up*, _Tp*>::value>::type>
        default_delete(const default_delete<_Up>&) noexcept { }

      /// Calls @c delete @p __ptr
      void
      operator()(_Tp* __ptr) const
   e45fc:	b538      	push	{r3, r4, r5, lr}
      {
	static_assert(!is_void<_Tp>::value,
		      "can't delete pointer to incomplete type");
	static_assert(sizeof(_Tp)>0,
		      "can't delete pointer to incomplete type");
	delete __ptr;
   e45fe:	4605      	mov	r5, r0
   e4600:	b188      	cbz	r0, e4626 <_ZNKSt14default_deleteIN6Thread4DataEEclEPS1_.isra.6+0x2a>

      /// Destructor, invokes the deleter if the stored pointer is not null.
      ~unique_ptr() noexcept
      {
	auto& __ptr = std::get<0>(_M_t);
	if (__ptr != nullptr)
   e4602:	6804      	ldr	r4, [r0, #0]
   e4604:	b14c      	cbz	r4, e461a <_ZNKSt14default_deleteIN6Thread4DataEEclEPS1_.isra.6+0x1e>

    _Function_base() : _M_manager(nullptr) { }

    ~_Function_base()
    {
      if (_M_manager)
   e4606:	68a3      	ldr	r3, [r4, #8]
   e4608:	b11b      	cbz	r3, e4612 <_ZNKSt14default_deleteIN6Thread4DataEEclEPS1_.isra.6+0x16>
	_M_manager(_M_functor, _M_functor, __destroy_functor);
   e460a:	2203      	movs	r2, #3
   e460c:	4621      	mov	r1, r4
   e460e:	4620      	mov	r0, r4
   e4610:	4798      	blx	r3
      {
	static_assert(!is_void<_Tp>::value,
		      "can't delete pointer to incomplete type");
	static_assert(sizeof(_Tp)>0,
		      "can't delete pointer to incomplete type");
	delete __ptr;
   e4612:	2110      	movs	r1, #16
   e4614:	4620      	mov	r0, r4
   e4616:	f000 fbf2 	bl	e4dfe <_ZdlPvj>
   e461a:	4628      	mov	r0, r5
   e461c:	2114      	movs	r1, #20
      }
   e461e:	e8bd 4038 	ldmia.w	sp!, {r3, r4, r5, lr}
      {
	static_assert(!is_void<_Tp>::value,
		      "can't delete pointer to incomplete type");
	static_assert(sizeof(_Tp)>0,
		      "can't delete pointer to incomplete type");
	delete __ptr;
   e4622:	f000 bbec 	b.w	e4dfe <_ZdlPvj>
   e4626:	bd38      	pop	{r3, r4, r5, pc}

000e4628 <_ZNKSt14default_deleteI3UDPEclEPS0_.isra.8.constprop.13>:
   e4628:	b110      	cbz	r0, e4630 <_ZNKSt14default_deleteI3UDPEclEPS0_.isra.8.constprop.13+0x8>
   e462a:	6803      	ldr	r3, [r0, #0]
   e462c:	685b      	ldr	r3, [r3, #4]
   e462e:	4718      	bx	r3
   e4630:	4770      	bx	lr

000e4632 <_ZN6ThreadD1Ev>:
    Thread(Thread&& thread)
        : d_(std::move(thread.d_))
    {
    }

    ~Thread()
   e4632:	b510      	push	{r4, lr}
      }

      /// Return the stored pointer.
      pointer
      get() const noexcept
      { return std::get<0>(_M_t); }
   e4634:	6803      	ldr	r3, [r0, #0]
   e4636:	4604      	mov	r4, r0
        dispose();
    }

    void dispose()
    {
        if (!isValid())
   e4638:	b1bb      	cbz	r3, e466a <_ZN6ThreadD1Ev+0x38>
        return isCurrent();
    }

    bool isCurrent() const
    {
        return isValid() && os_thread_is_current(d_->handle);
   e463a:	6858      	ldr	r0, [r3, #4]
   e463c:	f7ff fcaa 	bl	e3f94 <os_thread_is_current>
   e4640:	b978      	cbnz	r0, e4662 <_ZN6ThreadD1Ev+0x30>
   e4642:	6823      	ldr	r3, [r4, #0]

        // We shouldn't dispose of current thread
        if (isCurrent())
            return;

        if (!d_->exited) {
   e4644:	7c5a      	ldrb	r2, [r3, #17]
   e4646:	b912      	cbnz	r2, e464e <_ZN6ThreadD1Ev+0x1c>
        d_.reset();
    }

    bool join()
    {
        return isValid() && os_thread_join(d_->handle)==0;
   e4648:	6858      	ldr	r0, [r3, #4]
   e464a:	f7ff fcab 	bl	e3fa4 <os_thread_join>

        if (!d_->exited) {
            join();
        }

        os_thread_cleanup(d_->handle);
   e464e:	6823      	ldr	r3, [r4, #0]
   e4650:	6858      	ldr	r0, [r3, #4]
   e4652:	f7ff fcaf 	bl	e3fb4 <os_thread_cleanup>
#endif
    {
      // concept requirements
      __glibcxx_function_requires(_SGIAssignableConcept<_Tp>)

      _Tp __tmp = _GLIBCXX_MOVE(__a);
   e4656:	6820      	ldr	r0, [r4, #0]
      __a = _GLIBCXX_MOVE(__b);
   e4658:	2300      	movs	r3, #0
   e465a:	6023      	str	r3, [r4, #0]
      void
      reset(pointer __p = pointer()) noexcept
      {
	using std::swap;
	swap(std::get<0>(_M_t), __p);
	if (__p != pointer())
   e465c:	b128      	cbz	r0, e466a <_ZN6ThreadD1Ev+0x38>
	  get_deleter()(__p);
   e465e:	f7ff ffcd 	bl	e45fc <_ZNKSt14default_deleteIN6Thread4DataEEclEPS1_.isra.6>

      /// Destructor, invokes the deleter if the stored pointer is not null.
      ~unique_ptr() noexcept
      {
	auto& __ptr = std::get<0>(_M_t);
	if (__ptr != nullptr)
   e4662:	6820      	ldr	r0, [r4, #0]
   e4664:	b108      	cbz	r0, e466a <_ZN6ThreadD1Ev+0x38>
	  get_deleter()(__ptr);
   e4666:	f7ff ffc9 	bl	e45fc <_ZNKSt14default_deleteIN6Thread4DataEEclEPS1_.isra.6>
    }

    ~Thread()
    {
        dispose();
    }
   e466a:	4620      	mov	r0, r4
   e466c:	bd10      	pop	{r4, pc}
	...

000e4670 <_ZN5spark9MeshClassD1Ev>:
    RecursiveMutex mutex_;
    std::unique_ptr<uint8_t[]> buffer_;
    std::atomic_bool exit_;
};

class MeshClass : public NetworkClass, public MeshPublish {
   e4670:	b538      	push	{r3, r4, r5, lr}
   e4672:	4b0c      	ldr	r3, [pc, #48]	; (e46a4 <_ZN5spark9MeshClassD1Ev+0x34>)
   e4674:	6003      	str	r3, [r0, #0]
   e4676:	4604      	mov	r4, r0

      /// Destructor, invokes the deleter if the stored pointer is not null.
      ~unique_ptr()
      {
	auto& __ptr = std::get<0>(_M_t);
	if (__ptr != nullptr)
   e4678:	f8d0 01cc 	ldr.w	r0, [r0, #460]	; 0x1cc
   e467c:	b108      	cbz	r0, e4682 <_ZN5spark9MeshClassD1Ev+0x12>
      void
      operator()(_Tp* __ptr) const
      {
	static_assert(sizeof(_Tp)>0,
		      "can't delete pointer to incomplete type");
	delete [] __ptr;
   e467e:	f7ef fd12 	bl	d40a6 <_ZdaPv>

      /// Destructor, invokes the deleter if the stored pointer is not null.
      ~unique_ptr() noexcept
      {
	auto& __ptr = std::get<0>(_M_t);
	if (__ptr != nullptr)
   e4682:	f8d4 51c4 	ldr.w	r5, [r4, #452]	; 0x1c4
   e4686:	b135      	cbz	r5, e4696 <_ZN5spark9MeshClassD1Ev+0x26>
      {
	static_assert(!is_void<_Tp>::value,
		      "can't delete pointer to incomplete type");
	static_assert(sizeof(_Tp)>0,
		      "can't delete pointer to incomplete type");
	delete __ptr;
   e4688:	4628      	mov	r0, r5
   e468a:	f7ff ffd2 	bl	e4632 <_ZN6ThreadD1Ev>
   e468e:	2104      	movs	r1, #4
   e4690:	4628      	mov	r0, r5
   e4692:	f000 fbb4 	bl	e4dfe <_ZdlPvj>

      /// Destructor, invokes the deleter if the stored pointer is not null.
      ~unique_ptr() noexcept
      {
	auto& __ptr = std::get<0>(_M_t);
	if (__ptr != nullptr)
   e4696:	68a0      	ldr	r0, [r4, #8]
   e4698:	b108      	cbz	r0, e469e <_ZN5spark9MeshClassD1Ev+0x2e>
	  get_deleter()(__ptr);
   e469a:	f7ff ffc5 	bl	e4628 <_ZNKSt14default_deleteI3UDPEclEPS0_.isra.8.constprop.13>
   e469e:	4620      	mov	r0, r4
   e46a0:	bd38      	pop	{r3, r4, r5, pc}
   e46a2:	bf00      	nop
   e46a4:	000eb3e4 	.word	0x000eb3e4

000e46a8 <_GLOBAL__sub_I__ZN5spark11MeshPublish13Subscriptions20event_handler_existsEPKcPFvS3_S3_EPvN17SubscriptionScope4EnumES3_>:

    return addr;
}

MeshClass Mesh;
} // namespace spark
   e46a8:	b538      	push	{r3, r4, r5, lr}
   e46aa:	4c0e      	ldr	r4, [pc, #56]	; (e46e4 <_GLOBAL__sub_I__ZN5spark11MeshPublish13Subscriptions20event_handler_existsEPKcPFvS3_S3_EPvN17SubscriptionScope4EnumES3_+0x3c>)
   e46ac:	4b0e      	ldr	r3, [pc, #56]	; (e46e8 <_GLOBAL__sub_I__ZN5spark11MeshPublish13Subscriptions20event_handler_existsEPKcPFvS3_S3_EPvN17SubscriptionScope4EnumES3_+0x40>)
   e46ae:	6023      	str	r3, [r4, #0]

  template<std::size_t _Idx, typename _Head>
    struct _Head_base<_Idx, _Head, false>
    {
      constexpr _Head_base()
      : _M_head_impl() { }
   e46b0:	2500      	movs	r5, #0
   e46b2:	2302      	movs	r3, #2
     */
    RecursiveMutex(os_mutex_recursive_t handle) : handle_(handle) {}

    RecursiveMutex() : handle_(nullptr)
    {
        os_mutex_recursive_create(&handle_);
   e46b4:	f504 70e4 	add.w	r0, r4, #456	; 0x1c8
   e46b8:	6063      	str	r3, [r4, #4]
   e46ba:	60a5      	str	r5, [r4, #8]
   e46bc:	f8c4 51c4 	str.w	r5, [r4, #452]	; 0x1c4
    /**
     * Creates a shared mutex.
     */
    RecursiveMutex(os_mutex_recursive_t handle) : handle_(handle) {}

    RecursiveMutex() : handle_(nullptr)
   e46c0:	f8c4 51c8 	str.w	r5, [r4, #456]	; 0x1c8
    {
        os_mutex_recursive_create(&handle_);
   e46c4:	f7ff fc86 	bl	e3fd4 <os_mutex_recursive_create>
public:
    MeshClass() :
            NetworkClass(NETWORK_INTERFACE_MESH) {
   e46c8:	4b08      	ldr	r3, [pc, #32]	; (e46ec <_GLOBAL__sub_I__ZN5spark11MeshPublish13Subscriptions20event_handler_existsEPKcPFvS3_S3_EPvN17SubscriptionScope4EnumES3_+0x44>)
   e46ca:	f8c4 51cc 	str.w	r5, [r4, #460]	; 0x1cc
      __atomic_base(const __atomic_base&) = delete;
      __atomic_base& operator=(const __atomic_base&) = delete;
      __atomic_base& operator=(const __atomic_base&) volatile = delete;

      // Requires __int_type convertible to _M_i.
      constexpr __atomic_base(__int_type __i) noexcept : _M_i (__i) { }
   e46ce:	f884 51d0 	strb.w	r5, [r4, #464]	; 0x1d0
   e46d2:	6023      	str	r3, [r4, #0]
    }

    return addr;
}

MeshClass Mesh;
   e46d4:	4620      	mov	r0, r4
   e46d6:	4a06      	ldr	r2, [pc, #24]	; (e46f0 <_GLOBAL__sub_I__ZN5spark11MeshPublish13Subscriptions20event_handler_existsEPKcPFvS3_S3_EPvN17SubscriptionScope4EnumES3_+0x48>)
   e46d8:	4906      	ldr	r1, [pc, #24]	; (e46f4 <_GLOBAL__sub_I__ZN5spark11MeshPublish13Subscriptions20event_handler_existsEPKcPFvS3_S3_EPvN17SubscriptionScope4EnumES3_+0x4c>)
} // namespace spark
   e46da:	e8bd 4038 	ldmia.w	sp!, {r3, r4, r5, lr}
    }

    return addr;
}

MeshClass Mesh;
   e46de:	f000 bb89 	b.w	e4df4 <__aeabi_atexit>
   e46e2:	bf00      	nop
   e46e4:	2003e4e0 	.word	0x2003e4e0
   e46e8:	000eb418 	.word	0x000eb418
   e46ec:	000eb3e4 	.word	0x000eb3e4
   e46f0:	2003c408 	.word	0x2003c408
   e46f4:	000e4671 	.word	0x000e4671

000e46f8 <_ZN5spark12NetworkClass7connectEj>:
        return Network;
    }
}

void NetworkClass::connect(unsigned flags) {
    network_connect(*this, flags, 0, nullptr);
   e46f8:	2300      	movs	r3, #0
   e46fa:	461a      	mov	r2, r3
   e46fc:	6840      	ldr	r0, [r0, #4]
   e46fe:	f7ff bdc5 	b.w	e428c <network_connect>

000e4702 <_ZN5spark12NetworkClass10disconnectEv>:
}

void NetworkClass::disconnect() {
    network_disconnect(*this, NETWORK_DISCONNECT_REASON_USER, nullptr);
   e4702:	2200      	movs	r2, #0
   e4704:	2102      	movs	r1, #2
   e4706:	6840      	ldr	r0, [r0, #4]
   e4708:	f7ff bdd0 	b.w	e42ac <network_disconnect>

000e470c <_ZN5spark12NetworkClass10connectingEv>:
}

bool NetworkClass::connecting() {
    return network_connecting(*this, 0, nullptr);
   e470c:	2200      	movs	r2, #0
   e470e:	4611      	mov	r1, r2
   e4710:	6840      	ldr	r0, [r0, #4]
   e4712:	f7ff bdc3 	b.w	e429c <network_connecting>

000e4716 <_ZN5spark12NetworkClass5readyEv>:
}

bool NetworkClass::ready() {
    return network_ready(*this, 0, nullptr);
   e4716:	2200      	movs	r2, #0
   e4718:	4611      	mov	r1, r2
   e471a:	6840      	ldr	r0, [r0, #4]
   e471c:	f7ff bdce 	b.w	e42bc <network_ready>

000e4720 <_ZN5spark12NetworkClass2onEv>:
}

void NetworkClass::on() {
    network_on(*this, 0, 0, nullptr);
   e4720:	2300      	movs	r3, #0
   e4722:	461a      	mov	r2, r3
   e4724:	4619      	mov	r1, r3
   e4726:	6840      	ldr	r0, [r0, #4]
   e4728:	f7ff bdd0 	b.w	e42cc <network_on>

000e472c <_ZN5spark12NetworkClass3offEv>:
}

void NetworkClass::off() {
    network_off(*this, 0, 0, nullptr);
   e472c:	2300      	movs	r3, #0
   e472e:	461a      	mov	r2, r3
   e4730:	4619      	mov	r1, r3
   e4732:	6840      	ldr	r0, [r0, #4]
   e4734:	f7ff bdd2 	b.w	e42dc <network_off>

000e4738 <_ZN5spark12NetworkClass6listenEb>:
}

void NetworkClass::listen(bool begin) {
    network_listen(*this, begin ? 0 : 1, nullptr);
   e4738:	2200      	movs	r2, #0
   e473a:	f081 0101 	eor.w	r1, r1, #1
   e473e:	6840      	ldr	r0, [r0, #4]
   e4740:	f7ff bdd4 	b.w	e42ec <network_listen>

000e4744 <_ZN5spark12NetworkClass16setListenTimeoutEt>:
}

void NetworkClass::setListenTimeout(uint16_t timeout) {
    network_set_listen_timeout(*this, timeout, nullptr);
   e4744:	2200      	movs	r2, #0
   e4746:	6840      	ldr	r0, [r0, #4]
   e4748:	f7ff bde0 	b.w	e430c <network_set_listen_timeout>

000e474c <_ZN5spark12NetworkClass16getListenTimeoutEv>:
}

uint16_t NetworkClass::getListenTimeout() {
    return network_get_listen_timeout(*this, 0, nullptr);
   e474c:	2200      	movs	r2, #0
   e474e:	4611      	mov	r1, r2
   e4750:	6840      	ldr	r0, [r0, #4]
   e4752:	f7ff bde3 	b.w	e431c <network_get_listen_timeout>

000e4756 <_ZN5spark12NetworkClass9listeningEv>:
}

bool NetworkClass::listening() {
    return network_listening(*this, 0, nullptr);
   e4756:	2200      	movs	r2, #0
   e4758:	4611      	mov	r1, r2
   e475a:	6840      	ldr	r0, [r0, #4]
   e475c:	f7ff bdce 	b.w	e42fc <network_listening>

000e4760 <_ZN5spark12NetworkClass7resolveEPKc>:
}

IPAddress NetworkClass::resolve(const char* name) {
   e4760:	e92d 43f0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, lr}
    IPAddress addr;
#if HAL_USE_INET_HAL_POSIX
    struct addrinfo *ai = nullptr;
   e4764:	2400      	movs	r4, #0

bool NetworkClass::listening() {
    return network_listening(*this, 0, nullptr);
}

IPAddress NetworkClass::resolve(const char* name) {
   e4766:	b095      	sub	sp, #84	; 0x54
   e4768:	4616      	mov	r6, r2
   e476a:	460d      	mov	r5, r1
   e476c:	4607      	mov	r7, r0
    IPAddress addr;
   e476e:	f7ff fed1 	bl	e4514 <_ZN9IPAddressC1Ev>
#if HAL_USE_INET_HAL_POSIX
    struct addrinfo *ai = nullptr;
    struct addrinfo hints = {};
   e4772:	4621      	mov	r1, r4
   e4774:	2220      	movs	r2, #32
   e4776:	a80c      	add	r0, sp, #48	; 0x30
}

IPAddress NetworkClass::resolve(const char* name) {
    IPAddress addr;
#if HAL_USE_INET_HAL_POSIX
    struct addrinfo *ai = nullptr;
   e4778:	9400      	str	r4, [sp, #0]
    struct addrinfo hints = {};
   e477a:	f003 f825 	bl	e77c8 <memset>
    hints.ai_flags = AI_ADDRCONFIG;
   e477e:	2340      	movs	r3, #64	; 0x40
   e4780:	930c      	str	r3, [sp, #48]	; 0x30
    hints.ai_family = AF_UNSPEC;
    const int r = getaddrinfo(name, nullptr, &hints, &ai);
   e4782:	4621      	mov	r1, r4
   e4784:	466b      	mov	r3, sp
   e4786:	aa0c      	add	r2, sp, #48	; 0x30
   e4788:	4630      	mov	r0, r6
   e478a:	f7ff fcbb 	bl	e4104 <netdb_getaddrinfo>
    if (!r) {
   e478e:	4604      	mov	r4, r0
   e4790:	2800      	cmp	r0, #0
   e4792:	d144      	bne.n	e481e <_ZN5spark12NetworkClass7resolveEPKc+0xbe>
        bool ok = false;
        // This is not really needed if AI_ADDRCONFIG is properly supported
        bool ipv4 = network_ready(*this, NETWORK_READY_TYPE_IPV4, nullptr);
   e4794:	4602      	mov	r2, r0
   e4796:	2101      	movs	r1, #1
   e4798:	6868      	ldr	r0, [r5, #4]
   e479a:	f7ff fd8f 	bl	e42bc <network_ready>
        bool ipv6 = network_ready(*this, NETWORK_READY_TYPE_IPV6, nullptr);
   e479e:	2102      	movs	r1, #2
    hints.ai_family = AF_UNSPEC;
    const int r = getaddrinfo(name, nullptr, &hints, &ai);
    if (!r) {
        bool ok = false;
        // This is not really needed if AI_ADDRCONFIG is properly supported
        bool ipv4 = network_ready(*this, NETWORK_READY_TYPE_IPV4, nullptr);
   e47a0:	4680      	mov	r8, r0
        bool ipv6 = network_ready(*this, NETWORK_READY_TYPE_IPV6, nullptr);
   e47a2:	4622      	mov	r2, r4
   e47a4:	6868      	ldr	r0, [r5, #4]
   e47a6:	f7ff fd89 	bl	e42bc <network_ready>
        for (auto cur = ai; cur != nullptr && !ok; cur = cur->ai_next) {
   e47aa:	9e00      	ldr	r6, [sp, #0]
    const int r = getaddrinfo(name, nullptr, &hints, &ai);
    if (!r) {
        bool ok = false;
        // This is not really needed if AI_ADDRCONFIG is properly supported
        bool ipv4 = network_ready(*this, NETWORK_READY_TYPE_IPV4, nullptr);
        bool ipv6 = network_ready(*this, NETWORK_READY_TYPE_IPV6, nullptr);
   e47ac:	4681      	mov	r9, r0
    struct addrinfo hints = {};
    hints.ai_flags = AI_ADDRCONFIG;
    hints.ai_family = AF_UNSPEC;
    const int r = getaddrinfo(name, nullptr, &hints, &ai);
    if (!r) {
        bool ok = false;
   e47ae:	4621      	mov	r1, r4
        // This is not really needed if AI_ADDRCONFIG is properly supported
        bool ipv4 = network_ready(*this, NETWORK_READY_TYPE_IPV4, nullptr);
        bool ipv6 = network_ready(*this, NETWORK_READY_TYPE_IPV6, nullptr);
        for (auto cur = ai; cur != nullptr && !ok; cur = cur->ai_next) {
   e47b0:	2e00      	cmp	r6, #0
   e47b2:	d034      	beq.n	e481e <_ZN5spark12NetworkClass7resolveEPKc+0xbe>
   e47b4:	2900      	cmp	r1, #0
   e47b6:	d132      	bne.n	e481e <_ZN5spark12NetworkClass7resolveEPKc+0xbe>
            // NOTE: using only the first entry that matches the current state of IPv4/IPv6 connectivity
            switch (cur->ai_family) {
   e47b8:	6873      	ldr	r3, [r6, #4]
   e47ba:	2b02      	cmp	r3, #2
   e47bc:	d002      	beq.n	e47c4 <_ZN5spark12NetworkClass7resolveEPKc+0x64>
   e47be:	2b0a      	cmp	r3, #10
   e47c0:	d009      	beq.n	e47d6 <_ZN5spark12NetworkClass7resolveEPKc+0x76>
   e47c2:	e02a      	b.n	e481a <_ZN5spark12NetworkClass7resolveEPKc+0xba>
                case AF_INET: {
                    if (!ipv4) {
   e47c4:	f1b8 0f00 	cmp.w	r8, #0
   e47c8:	d027      	beq.n	e481a <_ZN5spark12NetworkClass7resolveEPKc+0xba>
                        continue;
                    }
                    // NOTE: HAL_IPAddress is little-endian
                    auto in = (struct sockaddr_in*)cur->ai_addr;
                    addr = (const uint8_t*)(&in->sin_addr.s_addr);
   e47ca:	6971      	ldr	r1, [r6, #20]
   e47cc:	4638      	mov	r0, r7
   e47ce:	3104      	adds	r1, #4
   e47d0:	f7ff fecd 	bl	e456e <_ZN9IPAddressaSEPKh>
   e47d4:	e020      	b.n	e4818 <_ZN5spark12NetworkClass7resolveEPKc+0xb8>
                    ok = true;
                    break;
                }
                case AF_INET6: {
                    if (!ipv6) {
   e47d6:	f1b9 0f00 	cmp.w	r9, #0
   e47da:	d01e      	beq.n	e481a <_ZN5spark12NetworkClass7resolveEPKc+0xba>
                        continue;
                    }
                    auto in6 = (struct sockaddr_in6*)cur->ai_addr;
   e47dc:	6974      	ldr	r4, [r6, #20]
                    HAL_IPAddress a = {};
   e47de:	2211      	movs	r2, #17
   e47e0:	a801      	add	r0, sp, #4
   e47e2:	f002 fff1 	bl	e77c8 <memset>
                    a.v = 6;
   e47e6:	2306      	movs	r3, #6
   e47e8:	f88d 3014 	strb.w	r3, [sp, #20]
                    memcpy(a.ipv6, in6->sin6_addr.s6_addr, sizeof(a.ipv6));
   e47ec:	ad01      	add	r5, sp, #4
   e47ee:	f104 0308 	add.w	r3, r4, #8
   e47f2:	3418      	adds	r4, #24
   e47f4:	6818      	ldr	r0, [r3, #0]
   e47f6:	6859      	ldr	r1, [r3, #4]
   e47f8:	462a      	mov	r2, r5
   e47fa:	c203      	stmia	r2!, {r0, r1}
   e47fc:	3308      	adds	r3, #8
   e47fe:	42a3      	cmp	r3, r4
   e4800:	4615      	mov	r5, r2
   e4802:	d1f7      	bne.n	e47f4 <_ZN5spark12NetworkClass7resolveEPKc+0x94>
                    addr = IPAddress(a);
   e4804:	a901      	add	r1, sp, #4
   e4806:	a806      	add	r0, sp, #24

/**
 * The IP address stored in host order.
 *
 */
class IPAddress : public Printable {
   e4808:	ad07      	add	r5, sp, #28
   e480a:	f7ff fe91 	bl	e4530 <_ZN9IPAddressC1ERK16_HAL_IPAddress_t>
   e480e:	cd0f      	ldmia	r5!, {r0, r1, r2, r3}
   e4810:	1d3c      	adds	r4, r7, #4
   e4812:	c40f      	stmia	r4!, {r0, r1, r2, r3}
   e4814:	682b      	ldr	r3, [r5, #0]
   e4816:	7023      	strb	r3, [r4, #0]
                    ok = true;
   e4818:	2101      	movs	r1, #1
    if (!r) {
        bool ok = false;
        // This is not really needed if AI_ADDRCONFIG is properly supported
        bool ipv4 = network_ready(*this, NETWORK_READY_TYPE_IPV4, nullptr);
        bool ipv6 = network_ready(*this, NETWORK_READY_TYPE_IPV6, nullptr);
        for (auto cur = ai; cur != nullptr && !ok; cur = cur->ai_next) {
   e481a:	69f6      	ldr	r6, [r6, #28]
   e481c:	e7c8      	b.n	e47b0 <_ZN5spark12NetworkClass7resolveEPKc+0x50>
                    break;
                }
            }
        }
    }
    freeaddrinfo(ai);
   e481e:	9800      	ldr	r0, [sp, #0]
   e4820:	f7ff fc68 	bl	e40f4 <netdb_freeaddrinfo>
    return Cellular.resolve(name);
#endif // Wiring_Cellular

#endif // HAL_USE_INET_HAL_POSIX
    return addr;
}
   e4824:	4638      	mov	r0, r7
   e4826:	b015      	add	sp, #84	; 0x54
   e4828:	e8bd 83f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, pc}

000e482c <_GLOBAL__sub_I__ZN5spark7NetworkE>:
   e482c:	4b02      	ldr	r3, [pc, #8]	; (e4838 <_GLOBAL__sub_I__ZN5spark7NetworkE+0xc>)
   e482e:	4a03      	ldr	r2, [pc, #12]	; (e483c <_GLOBAL__sub_I__ZN5spark7NetworkE+0x10>)
   e4830:	601a      	str	r2, [r3, #0]
   e4832:	2200      	movs	r2, #0
   e4834:	605a      	str	r2, [r3, #4]
   e4836:	4770      	bx	lr
   e4838:	2003e6b4 	.word	0x2003e6b4
   e483c:	000eb418 	.word	0x000eb418

000e4840 <_ZN5Print5writeEPKhj>:

// Public Methods //////////////////////////////////////////////////////////////

/* default implementation: may be overridden */
size_t Print::write(const uint8_t *buffer, size_t size)
{
   e4840:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
   e4842:	4606      	mov	r6, r0
   e4844:	460d      	mov	r5, r1
   e4846:	188f      	adds	r7, r1, r2
  size_t n = 0;
   e4848:	2400      	movs	r4, #0
  while (size--) {
   e484a:	42bd      	cmp	r5, r7
   e484c:	d00c      	beq.n	e4868 <_ZN5Print5writeEPKhj+0x28>
     int chunk = write(*buffer++);
   e484e:	6833      	ldr	r3, [r6, #0]
   e4850:	f815 1b01 	ldrb.w	r1, [r5], #1
   e4854:	689b      	ldr	r3, [r3, #8]
   e4856:	4630      	mov	r0, r6
   e4858:	4798      	blx	r3
     if (chunk>=0)
   e485a:	2800      	cmp	r0, #0
   e485c:	db01      	blt.n	e4862 <_ZN5Print5writeEPKhj+0x22>
         n += chunk;
   e485e:	4404      	add	r4, r0

/* default implementation: may be overridden */
size_t Print::write(const uint8_t *buffer, size_t size)
{
  size_t n = 0;
  while (size--) {
   e4860:	e7f3      	b.n	e484a <_ZN5Print5writeEPKhj+0xa>
     int chunk = write(*buffer++);
   e4862:	2c00      	cmp	r4, #0
   e4864:	bf08      	it	eq
   e4866:	4604      	moveq	r4, r0
             n = chunk;
         break;
     }
  }
  return n;
}
   e4868:	4620      	mov	r0, r4
   e486a:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}

000e486c <_ZN5Print5writeEPKc>:

    int getWriteError() { return write_error; }
    void clearWriteError() { setWriteError(0); }

    virtual size_t write(uint8_t) = 0;
    size_t write(const char *str) {
   e486c:	b570      	push	{r4, r5, r6, lr}
   e486e:	4605      	mov	r5, r0
      if (str == NULL) return 0;
   e4870:	460c      	mov	r4, r1
      return write((const uint8_t *)str, strlen(str));
    }
   e4872:	4608      	mov	r0, r1
    int getWriteError() { return write_error; }
    void clearWriteError() { setWriteError(0); }

    virtual size_t write(uint8_t) = 0;
    size_t write(const char *str) {
      if (str == NULL) return 0;
   e4874:	b149      	cbz	r1, e488a <_ZN5Print5writeEPKc+0x1e>
      return write((const uint8_t *)str, strlen(str));
   e4876:	f002 ffe1 	bl	e783c <strlen>
   e487a:	682b      	ldr	r3, [r5, #0]
   e487c:	4602      	mov	r2, r0
   e487e:	4621      	mov	r1, r4
   e4880:	4628      	mov	r0, r5
   e4882:	68db      	ldr	r3, [r3, #12]
    }
   e4884:	e8bd 4070 	ldmia.w	sp!, {r4, r5, r6, lr}
    void clearWriteError() { setWriteError(0); }

    virtual size_t write(uint8_t) = 0;
    size_t write(const char *str) {
      if (str == NULL) return 0;
      return write((const uint8_t *)str, strlen(str));
   e4888:	4718      	bx	r3
    }
   e488a:	bd70      	pop	{r4, r5, r6, pc}

000e488c <_ZN5Print5printEPKc>:
   e488c:	b508      	push	{r3, lr}
   e488e:	f7ff ffed 	bl	e486c <_ZN5Print5writeEPKc>
   e4892:	bd08      	pop	{r3, pc}

000e4894 <_ZN5Print5printEc>:
  return write(str);
}

size_t Print::print(char c)
{
  return write(c);
   e4894:	6803      	ldr	r3, [r0, #0]
   e4896:	689b      	ldr	r3, [r3, #8]
   e4898:	4718      	bx	r3

000e489a <_ZN5Print11printNumberEmh>:
  return println(reinterpret_cast<const char*>(str));
}

// Private Methods /////////////////////////////////////////////////////////////

size_t Print::printNumber(unsigned long n, uint8_t base) {
   e489a:	b530      	push	{r4, r5, lr}
   e489c:	b08b      	sub	sp, #44	; 0x2c
   e489e:	460b      	mov	r3, r1
  char buf[8 * sizeof(long) + 1]; // Assumes 8-bit chars plus zero byte.
  char *str = &buf[sizeof(buf) - 1];

  *str = '\0';
   e48a0:	2100      	movs	r1, #0
   e48a2:	f88d 1024 	strb.w	r1, [sp, #36]	; 0x24

  // prevent crash if called with base == 1
  if (base < 2) base = 10;
   e48a6:	2a01      	cmp	r2, #1
   e48a8:	bf98      	it	ls
   e48aa:	220a      	movls	r2, #10
   e48ac:	f10d 0423 	add.w	r4, sp, #35	; 0x23

  do {
    unsigned long m = n;
    n /= base;
   e48b0:	fbb3 f5f2 	udiv	r5, r3, r2
    char c = m - base * n;
   e48b4:	fb05 3312 	mls	r3, r5, r2, r3
   e48b8:	f003 03ff 	and.w	r3, r3, #255	; 0xff
    *--str = c < 10 ? c + '0' : c + 'A' - 10;
   e48bc:	2b09      	cmp	r3, #9
   e48be:	bf94      	ite	ls
   e48c0:	3330      	addls	r3, #48	; 0x30
   e48c2:	3337      	addhi	r3, #55	; 0x37
   e48c4:	b2db      	uxtb	r3, r3
   e48c6:	4621      	mov	r1, r4
   e48c8:	f804 3901 	strb.w	r3, [r4], #-1
   e48cc:	462b      	mov	r3, r5
  *str = '\0';

  // prevent crash if called with base == 1
  if (base < 2) base = 10;

  do {
   e48ce:	2d00      	cmp	r5, #0
   e48d0:	d1ee      	bne.n	e48b0 <_ZN5Print11printNumberEmh+0x16>
    n /= base;
    char c = m - base * n;
    *--str = c < 10 ? c + '0' : c + 'A' - 10;
  } while(n);

  return write(str);
   e48d2:	f7ff ffcb 	bl	e486c <_ZN5Print5writeEPKc>
}
   e48d6:	b00b      	add	sp, #44	; 0x2c
   e48d8:	bd30      	pop	{r4, r5, pc}

000e48da <_ZN5Print5printEmi>:
    return printNumber(n, base);
  }
}

size_t Print::print(unsigned long n, int base)
{
   e48da:	b410      	push	{r4}
  if (base == 0) return write(n);
   e48dc:	b92a      	cbnz	r2, e48ea <_ZN5Print5printEmi+0x10>
   e48de:	6803      	ldr	r3, [r0, #0]
  else return printNumber(n, base);
}
   e48e0:	f85d 4b04 	ldr.w	r4, [sp], #4
  }
}

size_t Print::print(unsigned long n, int base)
{
  if (base == 0) return write(n);
   e48e4:	689b      	ldr	r3, [r3, #8]
   e48e6:	b2c9      	uxtb	r1, r1
   e48e8:	4718      	bx	r3
  else return printNumber(n, base);
   e48ea:	b2d2      	uxtb	r2, r2
}
   e48ec:	f85d 4b04 	ldr.w	r4, [sp], #4
}

size_t Print::print(unsigned long n, int base)
{
  if (base == 0) return write(n);
  else return printNumber(n, base);
   e48f0:	f7ff bfd3 	b.w	e489a <_ZN5Print11printNumberEmh>

000e48f4 <_ZN5Print5printEhi>:
  return write(c);
}

size_t Print::print(unsigned char b, int base)
{
  return print((unsigned long) b, base);
   e48f4:	f7ff bff1 	b.w	e48da <_ZN5Print5printEmi>

000e48f8 <_ZN8RGBClassD1Ev>:
#include "rgbled.h"

typedef void (raw_rgb_change_handler_t)(uint8_t, uint8_t, uint8_t);
typedef std::function<raw_rgb_change_handler_t> wiring_rgb_change_handler_t;

class RGBClass {
   e48f8:	b510      	push	{r4, lr}
   e48fa:	4604      	mov	r4, r0
   *  @ingroup functors
   *
   *  Polymorphic function wrapper.
   */
  template<typename _Res, typename... _ArgTypes>
    class function<_Res(_ArgTypes...)>
   e48fc:	f7ff fd46 	bl	e438c <_ZNSt14_Function_baseD1Ev>
   e4900:	4620      	mov	r0, r4
   e4902:	bd10      	pop	{r4, pc}

000e4904 <_GLOBAL__sub_I_RGB>:
	{
	  _Base::_M_init_functor(__functor, std::__addressof(__f.get()));
	}
      };

    _Function_base() : _M_manager(nullptr) { }
   e4904:	4803      	ldr	r0, [pc, #12]	; (e4914 <_GLOBAL__sub_I_RGB+0x10>)
#include "spark_wiring_rgb.h"
#include "spark_wiring_interrupts.h"

#include "core_hal.h"

RGBClass RGB;
   e4906:	4a04      	ldr	r2, [pc, #16]	; (e4918 <_GLOBAL__sub_I_RGB+0x14>)
   e4908:	4904      	ldr	r1, [pc, #16]	; (e491c <_GLOBAL__sub_I_RGB+0x18>)
   e490a:	2300      	movs	r3, #0
   e490c:	6083      	str	r3, [r0, #8]
   e490e:	f000 ba71 	b.w	e4df4 <__aeabi_atexit>
   e4912:	bf00      	nop
   e4914:	2003e6bc 	.word	0x2003e6bc
   e4918:	2003c408 	.word	0x2003c408
   e491c:	000e48f9 	.word	0x000e48f9

000e4920 <_ZN8SPIClassD1Ev>:
  Mutex mutex_;
#endif

public:
  SPIClass(HAL_SPI_Interface spi);
  virtual ~SPIClass() {};
   e4920:	4770      	bx	lr

000e4922 <_ZN8SPIClassD0Ev>:
   e4922:	b510      	push	{r4, lr}
   e4924:	2110      	movs	r1, #16
   e4926:	4604      	mov	r4, r0
   e4928:	f000 fa69 	bl	e4dfe <_ZdlPvj>
   e492c:	4620      	mov	r0, r4
   e492e:	bd10      	pop	{r4, pc}

000e4930 <_ZN8SPIClassC1E17HAL_SPI_Interface>:
  if (!info->enabled || info->default_settings)
    return particle::__SPISettings();
  return particle::__SPISettings(info->clock, info->bit_order, info->data_mode);
}

SPIClass::SPIClass(HAL_SPI_Interface spi)
   e4930:	b570      	push	{r4, r5, r6, lr}
   e4932:	4b08      	ldr	r3, [pc, #32]	; (e4954 <_ZN8SPIClassC1E17HAL_SPI_Interface+0x24>)
   e4934:	6003      	str	r3, [r0, #0]
   e4936:	4604      	mov	r4, r0
    Mutex(os_mutex_t handle) : handle_(handle) {}

    /**
     * Creates a new mutex.
     */
    Mutex() : handle_(nullptr)
   e4938:	2500      	movs	r5, #0
   e493a:	460e      	mov	r6, r1
   e493c:	f840 5f0c 	str.w	r5, [r0, #12]!
    {
        os_mutex_create(&handle_);
   e4940:	f7ff fb40 	bl	e3fc4 <os_mutex_create>
{
  _spi = spi;
  HAL_SPI_Init(_spi);
   e4944:	4630      	mov	r0, r6
  return particle::__SPISettings(info->clock, info->bit_order, info->data_mode);
}

SPIClass::SPIClass(HAL_SPI_Interface spi)
{
  _spi = spi;
   e4946:	7126      	strb	r6, [r4, #4]
  HAL_SPI_Init(_spi);
   e4948:	f7ff fbe4 	bl	e4114 <HAL_SPI_Init>
  dividerReference = SPI_CLK_SYSTEM;     // 0 indicates the system clock
   e494c:	60a5      	str	r5, [r4, #8]
}
   e494e:	4620      	mov	r0, r4
   e4950:	bd70      	pop	{r4, r5, r6, pc}
   e4952:	bf00      	nop
   e4954:	000eb44c 	.word	0x000eb44c

000e4958 <_ZN8SPIClass9isEnabledEv>:
  //To Do
}

bool SPIClass::isEnabled()
{
  return HAL_SPI_Is_Enabled(_spi);
   e4958:	7900      	ldrb	r0, [r0, #4]
   e495a:	f7ff bbe3 	b.w	e4124 <HAL_SPI_Is_Enabled>
	...

000e4960 <_GLOBAL__sub_I_System>:
    WAKEUP_REASON_RTC = 2,
    WAKEUP_REASON_PIN_OR_RTC = 3
};

struct SleepResult {
    SleepResult() {}
   e4960:	4b04      	ldr	r3, [pc, #16]	; (e4974 <_GLOBAL__sub_I_System+0x14>)
   e4962:	2000      	movs	r0, #0
   e4964:	f64f 72ff 	movw	r2, #65535	; 0xffff
   e4968:	7018      	strb	r0, [r3, #0]
   e496a:	8058      	strh	r0, [r3, #2]
   e496c:	809a      	strh	r2, [r3, #4]

class SystemClass {
public:

    SystemClass(System_Mode_TypeDef mode = DEFAULT) {
        set_system_mode(mode);
   e496e:	f7ff bc61 	b.w	e4234 <set_system_mode>
   e4972:	bf00      	nop
   e4974:	2003e6cc 	.word	0x2003e6cc

000e4978 <_GLOBAL__sub_I_TIME_FORMAT_DEFAULT>:
            calendar_time_cache = Convert_UnixTime_To_CalendarTime(unix_time);
            unix_time_cache = unix_time;
    }
}

const char* TimeClass::format_spec = TIME_FORMAT_DEFAULT;
   e4978:	4b02      	ldr	r3, [pc, #8]	; (e4984 <_GLOBAL__sub_I_TIME_FORMAT_DEFAULT+0xc>)
   e497a:	681a      	ldr	r2, [r3, #0]
   e497c:	4b02      	ldr	r3, [pc, #8]	; (e4988 <_GLOBAL__sub_I_TIME_FORMAT_DEFAULT+0x10>)
   e497e:	601a      	str	r2, [r3, #0]
   e4980:	4770      	bx	lr
   e4982:	bf00      	nop
   e4984:	2003c39c 	.word	0x2003c39c
   e4988:	2003e6d4 	.word	0x2003e6d4

000e498c <_ZN11USARTSerialD1Ev>:
private:
  HAL_USART_Serial _serial;
  bool _blocking;
public:
  USARTSerial(HAL_USART_Serial serial, Ring_Buffer *rx_buffer, Ring_Buffer *tx_buffer);
  virtual ~USARTSerial() {};
   e498c:	4770      	bx	lr

000e498e <_ZN11USARTSerial14blockOnOverrunEb>:
    HAL_USART_Half_Duplex(_serial, Enable);
}

void USARTSerial::blockOnOverrun(bool block)
{
  _blocking = block;
   e498e:	7441      	strb	r1, [r0, #17]
   e4990:	4770      	bx	lr

000e4992 <_ZN11USARTSerial17availableForWriteEv>:
}


int USARTSerial::availableForWrite(void)
{
   e4992:	b508      	push	{r3, lr}
  return std::max(0, (int)HAL_USART_Available_Data_For_Write(_serial));
   e4994:	7c00      	ldrb	r0, [r0, #16]
   e4996:	f7ff fc05 	bl	e41a4 <HAL_USART_Available_Data_For_Write>
}
   e499a:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
   e499e:	bd08      	pop	{r3, pc}

000e49a0 <_ZN11USARTSerial9availableEv>:

int USARTSerial::available(void)
{
   e49a0:	b508      	push	{r3, lr}
  return std::max(0, (int)HAL_USART_Available_Data(_serial));
   e49a2:	7c00      	ldrb	r0, [r0, #16]
   e49a4:	f7ff fbd6 	bl	e4154 <HAL_USART_Available_Data>
}
   e49a8:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
   e49ac:	bd08      	pop	{r3, pc}

000e49ae <_ZN11USARTSerial4peekEv>:

int USARTSerial::peek(void)
{
   e49ae:	b508      	push	{r3, lr}
  return std::max(-1, (int)HAL_USART_Peek_Data(_serial));
   e49b0:	7c00      	ldrb	r0, [r0, #16]
   e49b2:	f7ff fbdf 	bl	e4174 <HAL_USART_Peek_Data>
}
   e49b6:	ea30 0020 	bics.w	r0, r0, r0, asr #32
   e49ba:	bf28      	it	cs
   e49bc:	f04f 30ff 	movcs.w	r0, #4294967295	; 0xffffffff
   e49c0:	bd08      	pop	{r3, pc}

000e49c2 <_ZN11USARTSerial4readEv>:

int USARTSerial::read(void)
{
   e49c2:	b508      	push	{r3, lr}
  return std::max(-1, (int)HAL_USART_Read_Data(_serial));
   e49c4:	7c00      	ldrb	r0, [r0, #16]
   e49c6:	f7ff fbcd 	bl	e4164 <HAL_USART_Read_Data>
}
   e49ca:	ea30 0020 	bics.w	r0, r0, r0, asr #32
   e49ce:	bf28      	it	cs
   e49d0:	f04f 30ff 	movcs.w	r0, #4294967295	; 0xffffffff
   e49d4:	bd08      	pop	{r3, pc}

000e49d6 <_ZN11USARTSerial5flushEv>:

void USARTSerial::flush()
{
  HAL_USART_Flush_Data(_serial);
   e49d6:	7c00      	ldrb	r0, [r0, #16]
   e49d8:	f7ff bbd4 	b.w	e4184 <HAL_USART_Flush_Data>

000e49dc <_ZN11USARTSerialD0Ev>:
   e49dc:	b510      	push	{r4, lr}
   e49de:	2114      	movs	r1, #20
   e49e0:	4604      	mov	r4, r0
   e49e2:	f000 fa0c 	bl	e4dfe <_ZdlPvj>
   e49e6:	4620      	mov	r0, r4
   e49e8:	bd10      	pop	{r4, pc}

000e49ea <_ZN11USARTSerial5writeEh>:
}

size_t USARTSerial::write(uint8_t c)
{
   e49ea:	b570      	push	{r4, r5, r6, lr}
  // attempt a write if blocking, or for non-blocking if there is room.
  if (_blocking || HAL_USART_Available_Data_For_Write(_serial) > 0) {
   e49ec:	7c45      	ldrb	r5, [r0, #17]
{
  HAL_USART_Flush_Data(_serial);
}

size_t USARTSerial::write(uint8_t c)
{
   e49ee:	4604      	mov	r4, r0
   e49f0:	460e      	mov	r6, r1
  // attempt a write if blocking, or for non-blocking if there is room.
  if (_blocking || HAL_USART_Available_Data_For_Write(_serial) > 0) {
   e49f2:	b925      	cbnz	r5, e49fe <_ZN11USARTSerial5writeEh+0x14>
   e49f4:	7c00      	ldrb	r0, [r0, #16]
   e49f6:	f7ff fbd5 	bl	e41a4 <HAL_USART_Available_Data_For_Write>
   e49fa:	2800      	cmp	r0, #0
   e49fc:	dd05      	ble.n	e4a0a <_ZN11USARTSerial5writeEh+0x20>
    // the HAL always blocks.
	  return HAL_USART_Write_Data(_serial, c);
   e49fe:	4631      	mov	r1, r6
   e4a00:	7c20      	ldrb	r0, [r4, #16]
  }
  return 0;
}
   e4a02:	e8bd 4070 	ldmia.w	sp!, {r4, r5, r6, lr}
size_t USARTSerial::write(uint8_t c)
{
  // attempt a write if blocking, or for non-blocking if there is room.
  if (_blocking || HAL_USART_Available_Data_For_Write(_serial) > 0) {
    // the HAL always blocks.
	  return HAL_USART_Write_Data(_serial, c);
   e4a06:	f7ff bb9d 	b.w	e4144 <HAL_USART_Write_Data>
  }
  return 0;
}
   e4a0a:	4628      	mov	r0, r5
   e4a0c:	bd70      	pop	{r4, r5, r6, pc}
	...

000e4a10 <_ZN11USARTSerialC1E16HAL_USART_SerialP11Ring_BufferS2_>:
#include "spark_wiring_constants.h"
#include "module_info.h"

// Constructors ////////////////////////////////////////////////////////////////

USARTSerial::USARTSerial(HAL_USART_Serial serial, Ring_Buffer *rx_buffer, Ring_Buffer *tx_buffer)
   e4a10:	b510      	push	{r4, lr}
   e4a12:	4604      	mov	r4, r0
   e4a14:	4608      	mov	r0, r1
   e4a16:	4611      	mov	r1, r2
  protected:
    void setWriteError(int err = 1) { write_error = err; }
    size_t printf_impl(bool newline, const char* format, ...);

  public:
    Print() : write_error(0) {}
   e4a18:	2200      	movs	r2, #0
   e4a1a:	6062      	str	r2, [r4, #4]
   e4a1c:	f44f 727a 	mov.w	r2, #1000	; 0x3e8
   e4a20:	60a2      	str	r2, [r4, #8]
   e4a22:	4a05      	ldr	r2, [pc, #20]	; (e4a38 <_ZN11USARTSerialC1E16HAL_USART_SerialP11Ring_BufferS2_+0x28>)
   e4a24:	6022      	str	r2, [r4, #0]
{
  _serial = serial;
  // Default is blocking mode
  _blocking = true;
   e4a26:	2201      	movs	r2, #1

// Constructors ////////////////////////////////////////////////////////////////

USARTSerial::USARTSerial(HAL_USART_Serial serial, Ring_Buffer *rx_buffer, Ring_Buffer *tx_buffer)
{
  _serial = serial;
   e4a28:	7420      	strb	r0, [r4, #16]
  // Default is blocking mode
  _blocking = true;
   e4a2a:	7462      	strb	r2, [r4, #17]
  HAL_USART_Init(serial, rx_buffer, tx_buffer);
   e4a2c:	461a      	mov	r2, r3
   e4a2e:	f7ff fb81 	bl	e4134 <HAL_USART_Init>
}
   e4a32:	4620      	mov	r0, r4
   e4a34:	bd10      	pop	{r4, pc}
   e4a36:	bf00      	nop
   e4a38:	000eb488 	.word	0x000eb488

000e4a3c <_ZN11USARTSerial9isEnabledEv>:
USARTSerial::operator bool() {
  return true;
}

bool USARTSerial::isEnabled() {
  return HAL_USART_Is_Enabled(_serial);
   e4a3c:	7c00      	ldrb	r0, [r0, #16]
   e4a3e:	f7ff bba9 	b.w	e4194 <HAL_USART_Is_Enabled>
	...

000e4a44 <_Z22__fetch_global_Serial1v>:
static Ring_Buffer* serial1_rx_buffer = NULL;
static Ring_Buffer* serial1_tx_buffer = NULL;
#endif

USARTSerial& __fetch_global_Serial1()
{
   e4a44:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
#if ((MODULE_FUNCTION == MOD_FUNC_USER_PART) || (MODULE_FUNCTION == MOD_FUNC_MONO_FIRMWARE))
	static USARTSerial serial1(HAL_USART_SERIAL1, &serial1_rx_buffer, &serial1_tx_buffer);
#else
  if (!serial1_rx_buffer) {
   e4a46:	4c19      	ldr	r4, [pc, #100]	; (e4aac <_Z22__fetch_global_Serial1v+0x68>)
   e4a48:	6825      	ldr	r5, [r4, #0]
   e4a4a:	b94d      	cbnz	r5, e4a60 <_Z22__fetch_global_Serial1v+0x1c>
    serial1_rx_buffer = new Ring_Buffer();
   e4a4c:	2084      	movs	r0, #132	; 0x84
   e4a4e:	f7ef fb24 	bl	d409a <_Znwj>
   e4a52:	4606      	mov	r6, r0
   e4a54:	b118      	cbz	r0, e4a5e <_Z22__fetch_global_Serial1v+0x1a>
   e4a56:	2284      	movs	r2, #132	; 0x84
   e4a58:	4629      	mov	r1, r5
   e4a5a:	f002 feb5 	bl	e77c8 <memset>
   e4a5e:	6026      	str	r6, [r4, #0]
  }
  if (!serial1_tx_buffer) {
   e4a60:	4d13      	ldr	r5, [pc, #76]	; (e4ab0 <_Z22__fetch_global_Serial1v+0x6c>)
   e4a62:	682e      	ldr	r6, [r5, #0]
   e4a64:	b94e      	cbnz	r6, e4a7a <_Z22__fetch_global_Serial1v+0x36>
    serial1_tx_buffer = new Ring_Buffer();
   e4a66:	2084      	movs	r0, #132	; 0x84
   e4a68:	f7ef fb17 	bl	d409a <_Znwj>
   e4a6c:	4607      	mov	r7, r0
   e4a6e:	b118      	cbz	r0, e4a78 <_Z22__fetch_global_Serial1v+0x34>
   e4a70:	2284      	movs	r2, #132	; 0x84
   e4a72:	4631      	mov	r1, r6
   e4a74:	f002 fea8 	bl	e77c8 <memset>
   e4a78:	602f      	str	r7, [r5, #0]
  }
  static USARTSerial serial1(HAL_USART_SERIAL1, serial1_rx_buffer, serial1_tx_buffer);
   e4a7a:	4f0e      	ldr	r7, [pc, #56]	; (e4ab4 <_Z22__fetch_global_Serial1v+0x70>)
   e4a7c:	6839      	ldr	r1, [r7, #0]
   e4a7e:	f011 0601 	ands.w	r6, r1, #1
   e4a82:	d111      	bne.n	e4aa8 <_Z22__fetch_global_Serial1v+0x64>
   e4a84:	4638      	mov	r0, r7
   e4a86:	f7ef fb1b 	bl	d40c0 <__cxa_guard_acquire>
   e4a8a:	b168      	cbz	r0, e4aa8 <_Z22__fetch_global_Serial1v+0x64>
   e4a8c:	6822      	ldr	r2, [r4, #0]
   e4a8e:	682b      	ldr	r3, [r5, #0]
   e4a90:	4809      	ldr	r0, [pc, #36]	; (e4ab8 <_Z22__fetch_global_Serial1v+0x74>)
   e4a92:	4631      	mov	r1, r6
   e4a94:	f7ff ffbc 	bl	e4a10 <_ZN11USARTSerialC1E16HAL_USART_SerialP11Ring_BufferS2_>
   e4a98:	4638      	mov	r0, r7
   e4a9a:	f7ef fb16 	bl	d40ca <__cxa_guard_release>
   e4a9e:	4a07      	ldr	r2, [pc, #28]	; (e4abc <_Z22__fetch_global_Serial1v+0x78>)
   e4aa0:	4907      	ldr	r1, [pc, #28]	; (e4ac0 <_Z22__fetch_global_Serial1v+0x7c>)
   e4aa2:	4805      	ldr	r0, [pc, #20]	; (e4ab8 <_Z22__fetch_global_Serial1v+0x74>)
   e4aa4:	f000 f9a6 	bl	e4df4 <__aeabi_atexit>
#endif
	return serial1;
}
   e4aa8:	4803      	ldr	r0, [pc, #12]	; (e4ab8 <_Z22__fetch_global_Serial1v+0x74>)
   e4aaa:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
   e4aac:	2003e6f4 	.word	0x2003e6f4
   e4ab0:	2003e6ec 	.word	0x2003e6ec
   e4ab4:	2003e6f0 	.word	0x2003e6f0
   e4ab8:	2003e6d8 	.word	0x2003e6d8
   e4abc:	2003c408 	.word	0x2003c408
   e4ac0:	000e498d 	.word	0x000e498d

000e4ac4 <_ZN9USBSerial14blockOnOverrunEb>:
  HAL_USB_USART_Flush_Data(_serial);
}

void USBSerial::blockOnOverrun(bool block)
{
  _blocking = block;
   e4ac4:	7441      	strb	r1, [r0, #17]
   e4ac6:	4770      	bx	lr

000e4ac8 <_ZN9USBSerialD1Ev>:
#include "usb_hal.h"
#include "system_task.h"
#include "spark_wiring_startup.h"
#include "concurrent_hal.h"

class USBSerial : public Stream
   e4ac8:	4770      	bx	lr

000e4aca <_ZN9USBSerial4readEv>:
}


// Read data from buffer
int USBSerial::read()
{
   e4aca:	b508      	push	{r3, lr}
	return std::max(-1, (int)HAL_USB_USART_Receive_Data(_serial, false));
   e4acc:	2100      	movs	r1, #0
   e4ace:	7c00      	ldrb	r0, [r0, #16]
   e4ad0:	f7ff fb90 	bl	e41f4 <HAL_USB_USART_Receive_Data>
}
   e4ad4:	ea30 0020 	bics.w	r0, r0, r0, asr #32
   e4ad8:	bf28      	it	cs
   e4ada:	f04f 30ff 	movcs.w	r0, #4294967295	; 0xffffffff
   e4ade:	bd08      	pop	{r3, pc}

000e4ae0 <_ZN9USBSerial4peekEv>:
{
  _blocking = block;
}

int USBSerial::peek()
{
   e4ae0:	b508      	push	{r3, lr}
	return std::max(-1, (int)HAL_USB_USART_Receive_Data(_serial, true));
   e4ae2:	2101      	movs	r1, #1
   e4ae4:	7c00      	ldrb	r0, [r0, #16]
   e4ae6:	f7ff fb85 	bl	e41f4 <HAL_USB_USART_Receive_Data>
}
   e4aea:	ea30 0020 	bics.w	r0, r0, r0, asr #32
   e4aee:	bf28      	it	cs
   e4af0:	f04f 30ff 	movcs.w	r0, #4294967295	; 0xffffffff
   e4af4:	bd08      	pop	{r3, pc}

000e4af6 <_ZN9USBSerial17availableForWriteEv>:
{
	return std::max(-1, (int)HAL_USB_USART_Receive_Data(_serial, false));
}

int USBSerial::availableForWrite()
{
   e4af6:	b508      	push	{r3, lr}
  return std::max(0, (int)HAL_USB_USART_Available_Data_For_Write(_serial));
   e4af8:	7c00      	ldrb	r0, [r0, #16]
   e4afa:	f7ff fb73 	bl	e41e4 <HAL_USB_USART_Available_Data_For_Write>
}
   e4afe:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
   e4b02:	bd08      	pop	{r3, pc}

000e4b04 <_ZN9USBSerial9availableEv>:

int USBSerial::available()
{
   e4b04:	b508      	push	{r3, lr}
	return std::max(0, (int)HAL_USB_USART_Available_Data(_serial));
   e4b06:	7c00      	ldrb	r0, [r0, #16]
   e4b08:	f7ff fb64 	bl	e41d4 <HAL_USB_USART_Available_Data>
}
   e4b0c:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
   e4b10:	bd08      	pop	{r3, pc}

000e4b12 <_ZN9USBSerial5flushEv>:
  return 0;
}

void USBSerial::flush()
{
  HAL_USB_USART_Flush_Data(_serial);
   e4b12:	7c00      	ldrb	r0, [r0, #16]
   e4b14:	f7ff bb7e 	b.w	e4214 <HAL_USB_USART_Flush_Data>

000e4b18 <_ZN9USBSerialD0Ev>:
   e4b18:	b510      	push	{r4, lr}
   e4b1a:	2114      	movs	r1, #20
   e4b1c:	4604      	mov	r4, r0
   e4b1e:	f000 f96e 	bl	e4dfe <_ZdlPvj>
   e4b22:	4620      	mov	r0, r4
   e4b24:	bd10      	pop	{r4, pc}

000e4b26 <_ZN9USBSerial5writeEh>:
{
	return std::max(0, (int)HAL_USB_USART_Available_Data(_serial));
}

size_t USBSerial::write(uint8_t byte)
{
   e4b26:	b538      	push	{r3, r4, r5, lr}
   e4b28:	4604      	mov	r4, r0
  if (HAL_USB_USART_Available_Data_For_Write(_serial) > 0 || _blocking) {
   e4b2a:	7c00      	ldrb	r0, [r0, #16]
{
	return std::max(0, (int)HAL_USB_USART_Available_Data(_serial));
}

size_t USBSerial::write(uint8_t byte)
{
   e4b2c:	460d      	mov	r5, r1
  if (HAL_USB_USART_Available_Data_For_Write(_serial) > 0 || _blocking) {
   e4b2e:	f7ff fb59 	bl	e41e4 <HAL_USB_USART_Available_Data_For_Write>
   e4b32:	2800      	cmp	r0, #0
   e4b34:	dc01      	bgt.n	e4b3a <_ZN9USBSerial5writeEh+0x14>
   e4b36:	7c60      	ldrb	r0, [r4, #17]
   e4b38:	b128      	cbz	r0, e4b46 <_ZN9USBSerial5writeEh+0x20>
    return std::max(0, (int)HAL_USB_USART_Send_Data(_serial, byte));
   e4b3a:	4629      	mov	r1, r5
   e4b3c:	7c20      	ldrb	r0, [r4, #16]
   e4b3e:	f7ff fb61 	bl	e4204 <HAL_USB_USART_Send_Data>
   e4b42:	ea20 70e0 	bic.w	r0, r0, r0, asr #31
  }
  return 0;
}
   e4b46:	bd38      	pop	{r3, r4, r5, pc}

000e4b48 <_ZN9USBSerialC1E20HAL_USB_USART_SerialRK20HAL_USB_USART_Config>:

  HAL_USB_USART_Config conf = acquireSerialBuffer();
  HAL_USB_USART_Init(_serial, &conf);
}

USBSerial::USBSerial(HAL_USB_USART_Serial serial, const HAL_USB_USART_Config& conf)
   e4b48:	b510      	push	{r4, lr}
   e4b4a:	4604      	mov	r4, r0
   e4b4c:	2300      	movs	r3, #0
   e4b4e:	6063      	str	r3, [r4, #4]
   e4b50:	f44f 737a 	mov.w	r3, #1000	; 0x3e8
   e4b54:	60a3      	str	r3, [r4, #8]
   e4b56:	4b05      	ldr	r3, [pc, #20]	; (e4b6c <_ZN9USBSerialC1E20HAL_USB_USART_SerialRK20HAL_USB_USART_Config+0x24>)
   e4b58:	6023      	str	r3, [r4, #0]
{
  _serial = serial;
  _blocking = true;
   e4b5a:	2301      	movs	r3, #1

  HAL_USB_USART_Config conf = acquireSerialBuffer();
  HAL_USB_USART_Init(_serial, &conf);
}

USBSerial::USBSerial(HAL_USB_USART_Serial serial, const HAL_USB_USART_Config& conf)
   e4b5c:	4608      	mov	r0, r1
{
  _serial = serial;
   e4b5e:	7421      	strb	r1, [r4, #16]
  _blocking = true;
   e4b60:	7463      	strb	r3, [r4, #17]

  HAL_USB_USART_Init(_serial, &conf);
   e4b62:	4611      	mov	r1, r2
   e4b64:	f7ff fb26 	bl	e41b4 <HAL_USB_USART_Init>
}
   e4b68:	4620      	mov	r0, r4
   e4b6a:	bd10      	pop	{r4, pc}
   e4b6c:	000eb4b8 	.word	0x000eb4b8

000e4b70 <_ZN9USBSerial5beginEl>:
// Public methods
//

void USBSerial::begin(long speed)
{
    HAL_USB_USART_Begin(_serial, speed, NULL);
   e4b70:	2200      	movs	r2, #0
   e4b72:	7c00      	ldrb	r0, [r0, #16]
   e4b74:	f7ff bb26 	b.w	e41c4 <HAL_USB_USART_Begin>

000e4b78 <_Z19acquireSerialBufferv>:

// Preinstantiate Objects //////////////////////////////////////////////////////
#ifdef SPARK_USB_SERIAL

HAL_USB_USART_Config __attribute__((weak)) acquireSerialBuffer()
{
   e4b78:	b510      	push	{r4, lr}
  HAL_USB_USART_Config conf = {0};
   e4b7a:	2214      	movs	r2, #20

// Preinstantiate Objects //////////////////////////////////////////////////////
#ifdef SPARK_USB_SERIAL

HAL_USB_USART_Config __attribute__((weak)) acquireSerialBuffer()
{
   e4b7c:	4604      	mov	r4, r0
  HAL_USB_USART_Config conf = {0};
   e4b7e:	2100      	movs	r1, #0
   e4b80:	f002 fe22 	bl	e77c8 <memset>
  conf.rx_buffer_size = USB_RX_BUFFER_SIZE;
  conf.tx_buffer_size = USB_TX_BUFFER_SIZE;
#endif

  return conf;
}
   e4b84:	4620      	mov	r0, r4
   e4b86:	bd10      	pop	{r4, pc}

000e4b88 <_Z16_fetch_usbserialv>:

USBSerial& _fetch_usbserial()
{
   e4b88:	b530      	push	{r4, r5, lr}
  HAL_USB_USART_Config conf = acquireSerialBuffer();
	static USBSerial _usbserial(HAL_USB_USART_SERIAL, conf);
   e4b8a:	4d0e      	ldr	r5, [pc, #56]	; (e4bc4 <_Z16_fetch_usbserialv+0x3c>)

  return conf;
}

USBSerial& _fetch_usbserial()
{
   e4b8c:	b087      	sub	sp, #28
  HAL_USB_USART_Config conf = acquireSerialBuffer();
   e4b8e:	a801      	add	r0, sp, #4
   e4b90:	f7ff fff2 	bl	e4b78 <_Z19acquireSerialBufferv>
	static USBSerial _usbserial(HAL_USB_USART_SERIAL, conf);
   e4b94:	6829      	ldr	r1, [r5, #0]
   e4b96:	f011 0401 	ands.w	r4, r1, #1
   e4b9a:	d110      	bne.n	e4bbe <_Z16_fetch_usbserialv+0x36>
   e4b9c:	4628      	mov	r0, r5
   e4b9e:	f7ef fa8f 	bl	d40c0 <__cxa_guard_acquire>
   e4ba2:	b160      	cbz	r0, e4bbe <_Z16_fetch_usbserialv+0x36>
   e4ba4:	aa01      	add	r2, sp, #4
   e4ba6:	4621      	mov	r1, r4
   e4ba8:	4807      	ldr	r0, [pc, #28]	; (e4bc8 <_Z16_fetch_usbserialv+0x40>)
   e4baa:	f7ff ffcd 	bl	e4b48 <_ZN9USBSerialC1E20HAL_USB_USART_SerialRK20HAL_USB_USART_Config>
   e4bae:	4628      	mov	r0, r5
   e4bb0:	f7ef fa8b 	bl	d40ca <__cxa_guard_release>
   e4bb4:	4a05      	ldr	r2, [pc, #20]	; (e4bcc <_Z16_fetch_usbserialv+0x44>)
   e4bb6:	4906      	ldr	r1, [pc, #24]	; (e4bd0 <_Z16_fetch_usbserialv+0x48>)
   e4bb8:	4803      	ldr	r0, [pc, #12]	; (e4bc8 <_Z16_fetch_usbserialv+0x40>)
   e4bba:	f000 f91b 	bl	e4df4 <__aeabi_atexit>
	return _usbserial;
}
   e4bbe:	4802      	ldr	r0, [pc, #8]	; (e4bc8 <_Z16_fetch_usbserialv+0x40>)
   e4bc0:	b007      	add	sp, #28
   e4bc2:	bd30      	pop	{r4, r5, pc}
   e4bc4:	2003e6f8 	.word	0x2003e6f8
   e4bc8:	2003e6fc 	.word	0x2003e6fc
   e4bcc:	2003c408 	.word	0x2003c408
   e4bd0:	000e4ac9 	.word	0x000e4ac9

000e4bd4 <serialEventRun>:

/**
 * Provides background processing of serial data.
 */
void serialEventRun()
{
   e4bd4:	b508      	push	{r3, lr}
    if (serialEvent && Serial.available()>0)
   e4bd6:	4b0f      	ldr	r3, [pc, #60]	; (e4c14 <serialEventRun+0x40>)
   e4bd8:	b133      	cbz	r3, e4be8 <serialEventRun+0x14>
   e4bda:	f7ff ffd5 	bl	e4b88 <_Z16_fetch_usbserialv>
   e4bde:	6803      	ldr	r3, [r0, #0]
   e4be0:	691b      	ldr	r3, [r3, #16]
   e4be2:	4798      	blx	r3
   e4be4:	2800      	cmp	r0, #0
   e4be6:	dc0d      	bgt.n	e4c04 <serialEventRun+0x30>
        serialEvent();

    if (serialEvent1 && Serial1.available()>0)
   e4be8:	4b0b      	ldr	r3, [pc, #44]	; (e4c18 <serialEventRun+0x44>)
   e4bea:	b133      	cbz	r3, e4bfa <serialEventRun+0x26>
   e4bec:	f7ff ff2a 	bl	e4a44 <_Z22__fetch_global_Serial1v>
   e4bf0:	6803      	ldr	r3, [r0, #0]
   e4bf2:	691b      	ldr	r3, [r3, #16]
   e4bf4:	4798      	blx	r3
   e4bf6:	2800      	cmp	r0, #0
   e4bf8:	dc07      	bgt.n	e4c0a <serialEventRun+0x36>
        serialEvent1();

#if Wiring_Serial2
    if (serialEventRun2) serialEventRun2();
   e4bfa:	4b08      	ldr	r3, [pc, #32]	; (e4c1c <serialEventRun+0x48>)
   e4bfc:	b143      	cbz	r3, e4c10 <serialEventRun+0x3c>
   e4bfe:	f3af 8000 	nop.w

#if Wiring_USBSerial1
    if (usbSerialEvent1 && USBSerial1.available()>0)
        usbSerialEvent1();
#endif
}
   e4c02:	bd08      	pop	{r3, pc}
 * Provides background processing of serial data.
 */
void serialEventRun()
{
    if (serialEvent && Serial.available()>0)
        serialEvent();
   e4c04:	f3af 8000 	nop.w
   e4c08:	e7ee      	b.n	e4be8 <serialEventRun+0x14>

    if (serialEvent1 && Serial1.available()>0)
        serialEvent1();
   e4c0a:	f3af 8000 	nop.w
   e4c0e:	e7f4      	b.n	e4bfa <serialEventRun+0x26>
   e4c10:	bd08      	pop	{r3, pc}
   e4c12:	bf00      	nop
	...

000e4c20 <_post_loop>:
#if Wiring_Serial5
void serialEvent5() __attribute__((weak));
#endif

void _post_loop()
{
   e4c20:	b508      	push	{r3, lr}
	serialEventRun();
   e4c22:	f7ff ffd7 	bl	e4bd4 <serialEventRun>
		return !timeout_fn;
	}

	static inline system_tick_t current_time()
	{
		return HAL_Timer_Get_Milli_Seconds();
   e4c26:	f7ff f9ed 	bl	e4004 <HAL_Timer_Get_Milli_Seconds>
	/**
	 * Lifesign that the application is still working normally.
	 */
	static void checkin()
	{
		last_checkin = current_time();
   e4c2a:	4b01      	ldr	r3, [pc, #4]	; (e4c30 <_post_loop+0x10>)
   e4c2c:	6018      	str	r0, [r3, #0]
   e4c2e:	bd08      	pop	{r3, pc}
   e4c30:	2003e714 	.word	0x2003e714

000e4c34 <_Z27ctrl_request_custom_handlerP12ctrl_request>:
bool __backup_ram_was_valid() { return false; }

#endif

// Default handler for CTRL_REQUEST_APP_CUSTOM requests
void __attribute((weak)) ctrl_request_custom_handler(ctrl_request* req) {
   e4c34:	b507      	push	{r0, r1, r2, lr}
    system_ctrl_set_result(req, SYSTEM_ERROR_NOT_SUPPORTED, nullptr, nullptr, nullptr);
   e4c36:	2300      	movs	r3, #0
   e4c38:	9300      	str	r3, [sp, #0]
   e4c3a:	461a      	mov	r2, r3
   e4c3c:	f06f 0177 	mvn.w	r1, #119	; 0x77
   e4c40:	f7ff fb12 	bl	e4268 <system_ctrl_set_result>
}
   e4c44:	b003      	add	sp, #12
   e4c46:	f85d fb04 	ldr.w	pc, [sp], #4
	...

000e4c4c <_ZL20ctrl_request_handlerP12ctrl_request>:
// Callback invoked to process a logging configuration request
void(*log_process_ctrl_request_callback)(ctrl_request* req) = nullptr;
#endif

// Application handler for control requests
static void ctrl_request_handler(ctrl_request* req) {
   e4c4c:	b507      	push	{r0, r1, r2, lr}
    switch (req->type) {
   e4c4e:	8843      	ldrh	r3, [r0, #2]
   e4c50:	2b0a      	cmp	r3, #10
   e4c52:	d008      	beq.n	e4c66 <_ZL20ctrl_request_handlerP12ctrl_request+0x1a>
   e4c54:	2b50      	cmp	r3, #80	; 0x50
   e4c56:	d109      	bne.n	e4c6c <_ZL20ctrl_request_handlerP12ctrl_request+0x20>
#if Wiring_LogConfig
    case CTRL_REQUEST_LOG_CONFIG: {
        if (log_process_ctrl_request_callback) {
   e4c58:	4b09      	ldr	r3, [pc, #36]	; (e4c80 <_ZL20ctrl_request_handlerP12ctrl_request+0x34>)
   e4c5a:	681b      	ldr	r3, [r3, #0]
   e4c5c:	b13b      	cbz	r3, e4c6e <_ZL20ctrl_request_handlerP12ctrl_request+0x22>
    }
    default:
        system_ctrl_set_result(req, SYSTEM_ERROR_NOT_SUPPORTED, nullptr, nullptr, nullptr);
        break;
    }
}
   e4c5e:	b003      	add	sp, #12
   e4c60:	f85d eb04 	ldr.w	lr, [sp], #4
static void ctrl_request_handler(ctrl_request* req) {
    switch (req->type) {
#if Wiring_LogConfig
    case CTRL_REQUEST_LOG_CONFIG: {
        if (log_process_ctrl_request_callback) {
            log_process_ctrl_request_callback(req);
   e4c64:	4718      	bx	r3
        }
        break;
    }
#endif
    case CTRL_REQUEST_APP_CUSTOM: {
        ctrl_request_custom_handler(req);
   e4c66:	f7ff ffe5 	bl	e4c34 <_Z27ctrl_request_custom_handlerP12ctrl_request>
        break;
   e4c6a:	e006      	b.n	e4c7a <_ZL20ctrl_request_handlerP12ctrl_request+0x2e>
    }
    default:
        system_ctrl_set_result(req, SYSTEM_ERROR_NOT_SUPPORTED, nullptr, nullptr, nullptr);
   e4c6c:	2300      	movs	r3, #0
   e4c6e:	9300      	str	r3, [sp, #0]
   e4c70:	461a      	mov	r2, r3
   e4c72:	f06f 0177 	mvn.w	r1, #119	; 0x77
   e4c76:	f7ff faf7 	bl	e4268 <system_ctrl_set_result>
        break;
    }
}
   e4c7a:	b003      	add	sp, #12
   e4c7c:	f85d fb04 	ldr.w	pc, [sp], #4
   e4c80:	2003e710 	.word	0x2003e710

000e4c84 <module_user_init_hook>:

void module_user_init_hook()
{
   e4c84:	b510      	push	{r4, lr}
    }
#endif

#if HAL_PLATFORM_RNG
    // Initialize the default stdlib PRNG using hardware RNG as a seed
    const uint32_t seed = HAL_RNG_GetRandomNumber();
   e4c86:	f7ff f9ad 	bl	e3fe4 <HAL_RNG_GetRandomNumber>
   e4c8a:	4604      	mov	r4, r0
    srand(seed);
   e4c8c:	f002 fda4 	bl	e77d8 <srand>

    // If the user defines random_seed_from_cloud, call it with a seed value
    // generated by a hardware RNG as well.
    if (random_seed_from_cloud) {
   e4c90:	4b07      	ldr	r3, [pc, #28]	; (e4cb0 <module_user_init_hook+0x2c>)
   e4c92:	b113      	cbz	r3, e4c9a <module_user_init_hook+0x16>
        random_seed_from_cloud(seed);
   e4c94:	4620      	mov	r0, r4
   e4c96:	f3af 8000 	nop.w
    }
#endif
    // Register the random_seed_from_cloud handler
    spark_set_random_seed_from_cloud_handler(&random_seed_from_cloud, nullptr);
   e4c9a:	2100      	movs	r1, #0
   e4c9c:	4804      	ldr	r0, [pc, #16]	; (e4cb0 <module_user_init_hook+0x2c>)
   e4c9e:	f7ff faed 	bl	e427c <spark_set_random_seed_from_cloud_handler>

    // Register application handler for the control requests
    system_ctrl_set_app_request_handler(ctrl_request_handler, nullptr);
   e4ca2:	2100      	movs	r1, #0
   e4ca4:	4803      	ldr	r0, [pc, #12]	; (e4cb4 <module_user_init_hook+0x30>)
}
   e4ca6:	e8bd 4010 	ldmia.w	sp!, {r4, lr}
#endif
    // Register the random_seed_from_cloud handler
    spark_set_random_seed_from_cloud_handler(&random_seed_from_cloud, nullptr);

    // Register application handler for the control requests
    system_ctrl_set_app_request_handler(ctrl_request_handler, nullptr);
   e4caa:	f7ff bad3 	b.w	e4254 <system_ctrl_set_app_request_handler>
   e4cae:	bf00      	nop
   e4cb0:	00000000 	.word	0x00000000
   e4cb4:	000e4c4d 	.word	0x000e4c4d

000e4cb8 <pinAvailable>:

/*
 * @brief Perform safety check on desired pin to see if it's already
 * being used.  Return 0 if used, otherwise return 1 if available.
 */
bool pinAvailable(uint16_t pin) {
   e4cb8:	b510      	push	{r4, lr}
   e4cba:	4604      	mov	r4, r0

  // SPI safety check
#ifndef SPARK_WIRING_NO_SPI
  if(SPI.isEnabled() == true && (pin == SCK || pin == MOSI || pin == MISO))
   e4cbc:	480f      	ldr	r0, [pc, #60]	; (e4cfc <pinAvailable+0x44>)
   e4cbe:	f7ff fe4b 	bl	e4958 <_ZN8SPIClass9isEnabledEv>
   e4cc2:	b128      	cbz	r0, e4cd0 <pinAvailable+0x18>
   e4cc4:	f1a4 030b 	sub.w	r3, r4, #11
   e4cc8:	2b02      	cmp	r3, #2
   e4cca:	d801      	bhi.n	e4cd0 <pinAvailable+0x18>
  {
    return 0; // 'pin' is used
   e4ccc:	2000      	movs	r0, #0
   e4cce:	bd10      	pop	{r4, pc}
  }
#endif
  // I2C safety check
#ifndef SPARK_WIRING_NO_I2C
  if(Wire.isEnabled() == true && (pin == SCL || pin == SDA))
   e4cd0:	f000 f84e 	bl	e4d70 <_Z19__fetch_global_Wirev>
   e4cd4:	f7ff fbe0 	bl	e4498 <_ZN7TwoWire9isEnabledEv>
   e4cd8:	b108      	cbz	r0, e4cde <pinAvailable+0x26>
   e4cda:	2c01      	cmp	r4, #1
   e4cdc:	d9f6      	bls.n	e4ccc <pinAvailable+0x14>
    return 0; // 'pin' is used
  }
#endif
#ifndef SPARK_WIRING_NO_USART_SERIAL
  // Serial1 safety check
  if(Serial1.isEnabled() == true && (pin == RX || pin == TX))
   e4cde:	f7ff feb1 	bl	e4a44 <_Z22__fetch_global_Serial1v>
   e4ce2:	f7ff feab 	bl	e4a3c <_ZN11USARTSerial9isEnabledEv>
   e4ce6:	b118      	cbz	r0, e4cf0 <pinAvailable+0x38>
   e4ce8:	f1a4 0309 	sub.w	r3, r4, #9
   e4cec:	2b01      	cmp	r3, #1
   e4cee:	d9ed      	bls.n	e4ccc <pinAvailable+0x14>
  {
    return 0; // 'pin' is used
  }
#endif

  if (pin >= TOTAL_PINS)
   e4cf0:	2c1e      	cmp	r4, #30
   e4cf2:	bf8c      	ite	hi
   e4cf4:	2000      	movhi	r0, #0
   e4cf6:	2001      	movls	r0, #1
    return 0;
  else
    return 1; // 'pin' is available
}
   e4cf8:	bd10      	pop	{r4, pc}
   e4cfa:	bf00      	nop
   e4cfc:	2003e730 	.word	0x2003e730

000e4d00 <pinMode>:
 * or INPUT_PULLDOWN
 */
void pinMode(uint16_t pin, PinMode setMode)
{

  if(pin >= TOTAL_PINS || setMode == PIN_MODE_NONE )
   e4d00:	281e      	cmp	r0, #30
/*
 * @brief Set the mode of the pin to OUTPUT, INPUT, INPUT_PULLUP,
 * or INPUT_PULLDOWN
 */
void pinMode(uint16_t pin, PinMode setMode)
{
   e4d02:	b538      	push	{r3, r4, r5, lr}
   e4d04:	4604      	mov	r4, r0
   e4d06:	460d      	mov	r5, r1

  if(pin >= TOTAL_PINS || setMode == PIN_MODE_NONE )
   e4d08:	d80a      	bhi.n	e4d20 <pinMode+0x20>
   e4d0a:	29ff      	cmp	r1, #255	; 0xff
   e4d0c:	d008      	beq.n	e4d20 <pinMode+0x20>
  {
    return;
  }

  // Safety check
  if( !pinAvailable(pin) ) {
   e4d0e:	f7ff ffd3 	bl	e4cb8 <pinAvailable>
   e4d12:	b128      	cbz	r0, e4d20 <pinMode+0x20>
    return;
  }

  HAL_Pin_Mode(pin, setMode);
   e4d14:	4629      	mov	r1, r5
   e4d16:	4620      	mov	r0, r4
}
   e4d18:	e8bd 4038 	ldmia.w	sp!, {r3, r4, r5, lr}
  // Safety check
  if( !pinAvailable(pin) ) {
    return;
  }

  HAL_Pin_Mode(pin, setMode);
   e4d1c:	f7ff b98a 	b.w	e4034 <HAL_Pin_Mode>
   e4d20:	bd38      	pop	{r3, r4, r5, pc}

000e4d22 <_Z11analogWritetm>:
/*
 * @brief Should take an integer 0-255 and create a 500Hz PWM signal with a duty cycle from 0-100%.
 * On Photon, DAC1 and DAC2 act as true analog outputs(values: 0 to 4095) using onchip DAC peripheral
 */
void analogWrite(pin_t pin, uint32_t value)
{
   e4d22:	b538      	push	{r3, r4, r5, lr}
   e4d24:	4604      	mov	r4, r0
   e4d26:	460d      	mov	r5, r1
    // Safety check
    if (!pinAvailable(pin))
   e4d28:	f7ff ffc6 	bl	e4cb8 <pinAvailable>
   e4d2c:	b1f0      	cbz	r0, e4d6c <_Z11analogWritetm+0x4a>
    {
        return;
    }

    if (HAL_Validate_Pin_Function(pin, PF_DAC) == PF_DAC)
   e4d2e:	2104      	movs	r1, #4
   e4d30:	4620      	mov	r0, r4
   e4d32:	f7ff f977 	bl	e4024 <HAL_Validate_Pin_Function>
   e4d36:	2804      	cmp	r0, #4
   e4d38:	d105      	bne.n	e4d46 <_Z11analogWritetm+0x24>
    {
        HAL_DAC_Write(pin, value);
   e4d3a:	b2a9      	uxth	r1, r5
   e4d3c:	4620      	mov	r0, r4
            return;
        }

        HAL_PWM_Write_Ext(pin, value);
    }
}
   e4d3e:	e8bd 4038 	ldmia.w	sp!, {r3, r4, r5, lr}
        return;
    }

    if (HAL_Validate_Pin_Function(pin, PF_DAC) == PF_DAC)
    {
        HAL_DAC_Write(pin, value);
   e4d42:	f7ff b987 	b.w	e4054 <HAL_DAC_Write>
    }
    else if (HAL_Validate_Pin_Function(pin, PF_TIMER) == PF_TIMER)
   e4d46:	2102      	movs	r1, #2
   e4d48:	4620      	mov	r0, r4
   e4d4a:	f7ff f96b 	bl	e4024 <HAL_Validate_Pin_Function>
   e4d4e:	2802      	cmp	r0, #2
   e4d50:	d10c      	bne.n	e4d6c <_Z11analogWritetm+0x4a>
    {
        PinMode mode = HAL_Get_Pin_Mode(pin);
   e4d52:	4620      	mov	r0, r4
   e4d54:	f7ff f976 	bl	e4044 <HAL_Get_Pin_Mode>

        if (mode != OUTPUT && mode != AF_OUTPUT_PUSHPULL)
   e4d58:	2801      	cmp	r0, #1
   e4d5a:	d001      	beq.n	e4d60 <_Z11analogWritetm+0x3e>
   e4d5c:	2804      	cmp	r0, #4
   e4d5e:	d105      	bne.n	e4d6c <_Z11analogWritetm+0x4a>
        {
            return;
        }

        HAL_PWM_Write_Ext(pin, value);
   e4d60:	4629      	mov	r1, r5
   e4d62:	4620      	mov	r0, r4
    }
}
   e4d64:	e8bd 4038 	ldmia.w	sp!, {r3, r4, r5, lr}
        if (mode != OUTPUT && mode != AF_OUTPUT_PUSHPULL)
        {
            return;
        }

        HAL_PWM_Write_Ext(pin, value);
   e4d68:	f7ff b97c 	b.w	e4064 <HAL_PWM_Write_Ext>
   e4d6c:	bd38      	pop	{r3, r4, r5, pc}
	...

000e4d70 <_Z19__fetch_global_Wirev>:
#include "i2c_hal.h"

#ifndef SPARK_WIRING_NO_I2C

TwoWire& __fetch_global_Wire()
{
   e4d70:	b538      	push	{r3, r4, r5, lr}
	static TwoWire wire(HAL_I2C_INTERFACE1);
   e4d72:	4d0b      	ldr	r5, [pc, #44]	; (e4da0 <_Z19__fetch_global_Wirev+0x30>)
   e4d74:	6829      	ldr	r1, [r5, #0]
   e4d76:	f011 0401 	ands.w	r4, r1, #1
   e4d7a:	d10f      	bne.n	e4d9c <_Z19__fetch_global_Wirev+0x2c>
   e4d7c:	4628      	mov	r0, r5
   e4d7e:	f7ef f99f 	bl	d40c0 <__cxa_guard_acquire>
   e4d82:	b158      	cbz	r0, e4d9c <_Z19__fetch_global_Wirev+0x2c>
   e4d84:	4621      	mov	r1, r4
   e4d86:	4807      	ldr	r0, [pc, #28]	; (e4da4 <_Z19__fetch_global_Wirev+0x34>)
   e4d88:	f7ff fb74 	bl	e4474 <_ZN7TwoWireC1E17HAL_I2C_Interface>
   e4d8c:	4628      	mov	r0, r5
   e4d8e:	f7ef f99c 	bl	d40ca <__cxa_guard_release>
   e4d92:	4a05      	ldr	r2, [pc, #20]	; (e4da8 <_Z19__fetch_global_Wirev+0x38>)
   e4d94:	4905      	ldr	r1, [pc, #20]	; (e4dac <_Z19__fetch_global_Wirev+0x3c>)
   e4d96:	4803      	ldr	r0, [pc, #12]	; (e4da4 <_Z19__fetch_global_Wirev+0x34>)
   e4d98:	f000 f82c 	bl	e4df4 <__aeabi_atexit>
	return wire;
}
   e4d9c:	4801      	ldr	r0, [pc, #4]	; (e4da4 <_Z19__fetch_global_Wirev+0x34>)
   e4d9e:	bd38      	pop	{r3, r4, r5, pc}
   e4da0:	2003e718 	.word	0x2003e718
   e4da4:	2003e71c 	.word	0x2003e71c
   e4da8:	2003c408 	.word	0x2003c408
   e4dac:	000e441d 	.word	0x000e441d

000e4db0 <_GLOBAL__sub_I_SPI>:
#ifndef SPARK_WIRING_NO_SPI

SPIClass SPI(HAL_SPI_INTERFACE1);

#if Wiring_SPI1
SPIClass SPI1(HAL_SPI_INTERFACE2);
   e4db0:	b570      	push	{r4, r5, r6, lr}
#include "core_hal.h"
#include "spark_macros.h"

#ifndef SPARK_WIRING_NO_SPI

SPIClass SPI(HAL_SPI_INTERFACE1);
   e4db2:	4c0c      	ldr	r4, [pc, #48]	; (e4de4 <_GLOBAL__sub_I_SPI+0x34>)
   e4db4:	4e0c      	ldr	r6, [pc, #48]	; (e4de8 <_GLOBAL__sub_I_SPI+0x38>)
   e4db6:	4d0d      	ldr	r5, [pc, #52]	; (e4dec <_GLOBAL__sub_I_SPI+0x3c>)
   e4db8:	2100      	movs	r1, #0
   e4dba:	4620      	mov	r0, r4
   e4dbc:	f7ff fdb8 	bl	e4930 <_ZN8SPIClassC1E17HAL_SPI_Interface>
   e4dc0:	4620      	mov	r0, r4

#if Wiring_SPI1
SPIClass SPI1(HAL_SPI_INTERFACE2);
   e4dc2:	4c0b      	ldr	r4, [pc, #44]	; (e4df0 <_GLOBAL__sub_I_SPI+0x40>)
#include "core_hal.h"
#include "spark_macros.h"

#ifndef SPARK_WIRING_NO_SPI

SPIClass SPI(HAL_SPI_INTERFACE1);
   e4dc4:	4632      	mov	r2, r6
   e4dc6:	4629      	mov	r1, r5
   e4dc8:	f000 f814 	bl	e4df4 <__aeabi_atexit>

#if Wiring_SPI1
SPIClass SPI1(HAL_SPI_INTERFACE2);
   e4dcc:	2101      	movs	r1, #1
   e4dce:	4620      	mov	r0, r4
   e4dd0:	f7ff fdae 	bl	e4930 <_ZN8SPIClassC1E17HAL_SPI_Interface>
   e4dd4:	4632      	mov	r2, r6
   e4dd6:	4629      	mov	r1, r5
   e4dd8:	4620      	mov	r0, r4
   e4dda:	e8bd 4070 	ldmia.w	sp!, {r4, r5, r6, lr}
   e4dde:	f000 b809 	b.w	e4df4 <__aeabi_atexit>
   e4de2:	bf00      	nop
   e4de4:	2003e730 	.word	0x2003e730
   e4de8:	2003c408 	.word	0x2003c408
   e4dec:	000e4921 	.word	0x000e4921
   e4df0:	2003e740 	.word	0x2003e740

000e4df4 <__aeabi_atexit>:
   e4df4:	460b      	mov	r3, r1
   e4df6:	4601      	mov	r1, r0
   e4df8:	4618      	mov	r0, r3
   e4dfa:	f002 bca9 	b.w	e7750 <__cxa_atexit>

000e4dfe <_ZdlPvj>:
   e4dfe:	f7ef b950 	b.w	d40a2 <_ZdlPv>
	...

000e4e04 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj>:
   e4e04:	4b24      	ldr	r3, [pc, #144]	; (e4e98 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0x94>)
   e4e06:	681a      	ldr	r2, [r3, #0]
   e4e08:	07d0      	lsls	r0, r2, #31
   e4e0a:	bf5c      	itt	pl
   e4e0c:	2201      	movpl	r2, #1
   e4e0e:	601a      	strpl	r2, [r3, #0]
   e4e10:	4b22      	ldr	r3, [pc, #136]	; (e4e9c <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0x98>)
   e4e12:	681a      	ldr	r2, [r3, #0]
   e4e14:	07d1      	lsls	r1, r2, #31
   e4e16:	bf5c      	itt	pl
   e4e18:	2201      	movpl	r2, #1
   e4e1a:	601a      	strpl	r2, [r3, #0]
   e4e1c:	4b20      	ldr	r3, [pc, #128]	; (e4ea0 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0x9c>)
   e4e1e:	681a      	ldr	r2, [r3, #0]
   e4e20:	07d2      	lsls	r2, r2, #31
   e4e22:	bf5c      	itt	pl
   e4e24:	2201      	movpl	r2, #1
   e4e26:	601a      	strpl	r2, [r3, #0]
   e4e28:	4b1e      	ldr	r3, [pc, #120]	; (e4ea4 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xa0>)
   e4e2a:	681a      	ldr	r2, [r3, #0]
   e4e2c:	07d0      	lsls	r0, r2, #31
   e4e2e:	bf5c      	itt	pl
   e4e30:	2201      	movpl	r2, #1
   e4e32:	601a      	strpl	r2, [r3, #0]
   e4e34:	4b1c      	ldr	r3, [pc, #112]	; (e4ea8 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xa4>)
   e4e36:	681a      	ldr	r2, [r3, #0]
   e4e38:	07d1      	lsls	r1, r2, #31
   e4e3a:	bf5c      	itt	pl
   e4e3c:	2201      	movpl	r2, #1
   e4e3e:	601a      	strpl	r2, [r3, #0]
   e4e40:	4b1a      	ldr	r3, [pc, #104]	; (e4eac <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xa8>)
   e4e42:	681a      	ldr	r2, [r3, #0]
   e4e44:	07d2      	lsls	r2, r2, #31
   e4e46:	bf5c      	itt	pl
   e4e48:	2201      	movpl	r2, #1
   e4e4a:	601a      	strpl	r2, [r3, #0]
   e4e4c:	4b18      	ldr	r3, [pc, #96]	; (e4eb0 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xac>)
   e4e4e:	681a      	ldr	r2, [r3, #0]
   e4e50:	07d0      	lsls	r0, r2, #31
   e4e52:	bf5c      	itt	pl
   e4e54:	2201      	movpl	r2, #1
   e4e56:	601a      	strpl	r2, [r3, #0]
   e4e58:	4b16      	ldr	r3, [pc, #88]	; (e4eb4 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xb0>)
   e4e5a:	681a      	ldr	r2, [r3, #0]
   e4e5c:	07d1      	lsls	r1, r2, #31
   e4e5e:	bf5c      	itt	pl
   e4e60:	2201      	movpl	r2, #1
   e4e62:	601a      	strpl	r2, [r3, #0]
   e4e64:	4b14      	ldr	r3, [pc, #80]	; (e4eb8 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xb4>)
   e4e66:	681a      	ldr	r2, [r3, #0]
   e4e68:	07d2      	lsls	r2, r2, #31
   e4e6a:	bf5c      	itt	pl
   e4e6c:	2201      	movpl	r2, #1
   e4e6e:	601a      	strpl	r2, [r3, #0]
   e4e70:	4b12      	ldr	r3, [pc, #72]	; (e4ebc <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xb8>)
   e4e72:	681a      	ldr	r2, [r3, #0]
   e4e74:	07d0      	lsls	r0, r2, #31
   e4e76:	bf5c      	itt	pl
   e4e78:	2201      	movpl	r2, #1
   e4e7a:	601a      	strpl	r2, [r3, #0]
   e4e7c:	4b10      	ldr	r3, [pc, #64]	; (e4ec0 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xbc>)
   e4e7e:	681a      	ldr	r2, [r3, #0]
   e4e80:	07d1      	lsls	r1, r2, #31
   e4e82:	bf5c      	itt	pl
   e4e84:	2201      	movpl	r2, #1
   e4e86:	601a      	strpl	r2, [r3, #0]
   e4e88:	4b0e      	ldr	r3, [pc, #56]	; (e4ec4 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKSsj+0xc0>)
   e4e8a:	681a      	ldr	r2, [r3, #0]
   e4e8c:	07d2      	lsls	r2, r2, #31
   e4e8e:	bf5c      	itt	pl
   e4e90:	2201      	movpl	r2, #1
   e4e92:	601a      	strpl	r2, [r3, #0]
   e4e94:	4770      	bx	lr
   e4e96:	bf00      	nop
   e4e98:	2003e77c 	.word	0x2003e77c
   e4e9c:	2003e778 	.word	0x2003e778
   e4ea0:	2003e774 	.word	0x2003e774
   e4ea4:	2003e770 	.word	0x2003e770
   e4ea8:	2003e76c 	.word	0x2003e76c
   e4eac:	2003e768 	.word	0x2003e768
   e4eb0:	2003e764 	.word	0x2003e764
   e4eb4:	2003e760 	.word	0x2003e760
   e4eb8:	2003e75c 	.word	0x2003e75c
   e4ebc:	2003e758 	.word	0x2003e758
   e4ec0:	2003e754 	.word	0x2003e754
   e4ec4:	2003e750 	.word	0x2003e750

000e4ec8 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj>:
   e4ec8:	4b18      	ldr	r3, [pc, #96]	; (e4f2c <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x64>)
   e4eca:	681a      	ldr	r2, [r3, #0]
   e4ecc:	07d1      	lsls	r1, r2, #31
   e4ece:	bf5c      	itt	pl
   e4ed0:	2201      	movpl	r2, #1
   e4ed2:	601a      	strpl	r2, [r3, #0]
   e4ed4:	4b16      	ldr	r3, [pc, #88]	; (e4f30 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x68>)
   e4ed6:	681a      	ldr	r2, [r3, #0]
   e4ed8:	07d2      	lsls	r2, r2, #31
   e4eda:	bf5c      	itt	pl
   e4edc:	2201      	movpl	r2, #1
   e4ede:	601a      	strpl	r2, [r3, #0]
   e4ee0:	4b14      	ldr	r3, [pc, #80]	; (e4f34 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x6c>)
   e4ee2:	681a      	ldr	r2, [r3, #0]
   e4ee4:	07d0      	lsls	r0, r2, #31
   e4ee6:	bf5c      	itt	pl
   e4ee8:	2201      	movpl	r2, #1
   e4eea:	601a      	strpl	r2, [r3, #0]
   e4eec:	4b12      	ldr	r3, [pc, #72]	; (e4f38 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x70>)
   e4eee:	681a      	ldr	r2, [r3, #0]
   e4ef0:	07d1      	lsls	r1, r2, #31
   e4ef2:	bf5c      	itt	pl
   e4ef4:	2201      	movpl	r2, #1
   e4ef6:	601a      	strpl	r2, [r3, #0]
   e4ef8:	4b10      	ldr	r3, [pc, #64]	; (e4f3c <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x74>)
   e4efa:	681a      	ldr	r2, [r3, #0]
   e4efc:	07d2      	lsls	r2, r2, #31
   e4efe:	bf5c      	itt	pl
   e4f00:	2201      	movpl	r2, #1
   e4f02:	601a      	strpl	r2, [r3, #0]
   e4f04:	4b0e      	ldr	r3, [pc, #56]	; (e4f40 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x78>)
   e4f06:	681a      	ldr	r2, [r3, #0]
   e4f08:	07d0      	lsls	r0, r2, #31
   e4f0a:	bf5c      	itt	pl
   e4f0c:	2201      	movpl	r2, #1
   e4f0e:	601a      	strpl	r2, [r3, #0]
   e4f10:	4b0c      	ldr	r3, [pc, #48]	; (e4f44 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x7c>)
   e4f12:	681a      	ldr	r2, [r3, #0]
   e4f14:	07d1      	lsls	r1, r2, #31
   e4f16:	bf5c      	itt	pl
   e4f18:	2201      	movpl	r2, #1
   e4f1a:	601a      	strpl	r2, [r3, #0]
   e4f1c:	4b0a      	ldr	r3, [pc, #40]	; (e4f48 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x80>)
   e4f1e:	681a      	ldr	r2, [r3, #0]
   e4f20:	07d2      	lsls	r2, r2, #31
   e4f22:	bf5c      	itt	pl
   e4f24:	2201      	movpl	r2, #1
   e4f26:	601a      	strpl	r2, [r3, #0]
   e4f28:	4770      	bx	lr
   e4f2a:	bf00      	nop
   e4f2c:	2003e79c 	.word	0x2003e79c
   e4f30:	2003e798 	.word	0x2003e798
   e4f34:	2003e794 	.word	0x2003e794
   e4f38:	2003e790 	.word	0x2003e790
   e4f3c:	2003e78c 	.word	0x2003e78c
   e4f40:	2003e788 	.word	0x2003e788
   e4f44:	2003e784 	.word	0x2003e784
   e4f48:	2003e780 	.word	0x2003e780

000e4f4c <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj>:
   e4f4c:	4b18      	ldr	r3, [pc, #96]	; (e4fb0 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x64>)
   e4f4e:	681a      	ldr	r2, [r3, #0]
   e4f50:	07d1      	lsls	r1, r2, #31
   e4f52:	bf5c      	itt	pl
   e4f54:	2201      	movpl	r2, #1
   e4f56:	601a      	strpl	r2, [r3, #0]
   e4f58:	4b16      	ldr	r3, [pc, #88]	; (e4fb4 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x68>)
   e4f5a:	681a      	ldr	r2, [r3, #0]
   e4f5c:	07d2      	lsls	r2, r2, #31
   e4f5e:	bf5c      	itt	pl
   e4f60:	2201      	movpl	r2, #1
   e4f62:	601a      	strpl	r2, [r3, #0]
   e4f64:	4b14      	ldr	r3, [pc, #80]	; (e4fb8 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x6c>)
   e4f66:	681a      	ldr	r2, [r3, #0]
   e4f68:	07d0      	lsls	r0, r2, #31
   e4f6a:	bf5c      	itt	pl
   e4f6c:	2201      	movpl	r2, #1
   e4f6e:	601a      	strpl	r2, [r3, #0]
   e4f70:	4b12      	ldr	r3, [pc, #72]	; (e4fbc <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x70>)
   e4f72:	681a      	ldr	r2, [r3, #0]
   e4f74:	07d1      	lsls	r1, r2, #31
   e4f76:	bf5c      	itt	pl
   e4f78:	2201      	movpl	r2, #1
   e4f7a:	601a      	strpl	r2, [r3, #0]
   e4f7c:	4b10      	ldr	r3, [pc, #64]	; (e4fc0 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x74>)
   e4f7e:	681a      	ldr	r2, [r3, #0]
   e4f80:	07d2      	lsls	r2, r2, #31
   e4f82:	bf5c      	itt	pl
   e4f84:	2201      	movpl	r2, #1
   e4f86:	601a      	strpl	r2, [r3, #0]
   e4f88:	4b0e      	ldr	r3, [pc, #56]	; (e4fc4 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x78>)
   e4f8a:	681a      	ldr	r2, [r3, #0]
   e4f8c:	07d0      	lsls	r0, r2, #31
   e4f8e:	bf5c      	itt	pl
   e4f90:	2201      	movpl	r2, #1
   e4f92:	601a      	strpl	r2, [r3, #0]
   e4f94:	4b0c      	ldr	r3, [pc, #48]	; (e4fc8 <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x7c>)
   e4f96:	681a      	ldr	r2, [r3, #0]
   e4f98:	07d1      	lsls	r1, r2, #31
   e4f9a:	bf5c      	itt	pl
   e4f9c:	2201      	movpl	r2, #1
   e4f9e:	601a      	strpl	r2, [r3, #0]
   e4fa0:	4b0a      	ldr	r3, [pc, #40]	; (e4fcc <_GLOBAL__sub_I__ZNSt12ctype_bynameIwEC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEj+0x80>)
   e4fa2:	681a      	ldr	r2, [r3, #0]
   e4fa4:	07d2      	lsls	r2, r2, #31
   e4fa6:	bf5c      	itt	pl
   e4fa8:	2201      	movpl	r2, #1
   e4faa:	601a      	strpl	r2, [r3, #0]
   e4fac:	4770      	bx	lr
   e4fae:	bf00      	nop
   e4fb0:	2003e7bc 	.word	0x2003e7bc
   e4fb4:	2003e7b8 	.word	0x2003e7b8
   e4fb8:	2003e7b4 	.word	0x2003e7b4
   e4fbc:	2003e7b0 	.word	0x2003e7b0
   e4fc0:	2003e7ac 	.word	0x2003e7ac
   e4fc4:	2003e7a8 	.word	0x2003e7a8
   e4fc8:	2003e7a4 	.word	0x2003e7a4
   e4fcc:	2003e7a0 	.word	0x2003e7a0

000e4fd0 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj>:
   e4fd0:	4b24      	ldr	r3, [pc, #144]	; (e5064 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0x94>)
   e4fd2:	681a      	ldr	r2, [r3, #0]
   e4fd4:	07d0      	lsls	r0, r2, #31
   e4fd6:	bf5c      	itt	pl
   e4fd8:	2201      	movpl	r2, #1
   e4fda:	601a      	strpl	r2, [r3, #0]
   e4fdc:	4b22      	ldr	r3, [pc, #136]	; (e5068 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0x98>)
   e4fde:	681a      	ldr	r2, [r3, #0]
   e4fe0:	07d1      	lsls	r1, r2, #31
   e4fe2:	bf5c      	itt	pl
   e4fe4:	2201      	movpl	r2, #1
   e4fe6:	601a      	strpl	r2, [r3, #0]
   e4fe8:	4b20      	ldr	r3, [pc, #128]	; (e506c <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0x9c>)
   e4fea:	681a      	ldr	r2, [r3, #0]
   e4fec:	07d2      	lsls	r2, r2, #31
   e4fee:	bf5c      	itt	pl
   e4ff0:	2201      	movpl	r2, #1
   e4ff2:	601a      	strpl	r2, [r3, #0]
   e4ff4:	4b1e      	ldr	r3, [pc, #120]	; (e5070 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xa0>)
   e4ff6:	681a      	ldr	r2, [r3, #0]
   e4ff8:	07d0      	lsls	r0, r2, #31
   e4ffa:	bf5c      	itt	pl
   e4ffc:	2201      	movpl	r2, #1
   e4ffe:	601a      	strpl	r2, [r3, #0]
   e5000:	4b1c      	ldr	r3, [pc, #112]	; (e5074 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xa4>)
   e5002:	681a      	ldr	r2, [r3, #0]
   e5004:	07d1      	lsls	r1, r2, #31
   e5006:	bf5c      	itt	pl
   e5008:	2201      	movpl	r2, #1
   e500a:	601a      	strpl	r2, [r3, #0]
   e500c:	4b1a      	ldr	r3, [pc, #104]	; (e5078 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xa8>)
   e500e:	681a      	ldr	r2, [r3, #0]
   e5010:	07d2      	lsls	r2, r2, #31
   e5012:	bf5c      	itt	pl
   e5014:	2201      	movpl	r2, #1
   e5016:	601a      	strpl	r2, [r3, #0]
   e5018:	4b18      	ldr	r3, [pc, #96]	; (e507c <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xac>)
   e501a:	681a      	ldr	r2, [r3, #0]
   e501c:	07d0      	lsls	r0, r2, #31
   e501e:	bf5c      	itt	pl
   e5020:	2201      	movpl	r2, #1
   e5022:	601a      	strpl	r2, [r3, #0]
   e5024:	4b16      	ldr	r3, [pc, #88]	; (e5080 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xb0>)
   e5026:	681a      	ldr	r2, [r3, #0]
   e5028:	07d1      	lsls	r1, r2, #31
   e502a:	bf5c      	itt	pl
   e502c:	2201      	movpl	r2, #1
   e502e:	601a      	strpl	r2, [r3, #0]
   e5030:	4b14      	ldr	r3, [pc, #80]	; (e5084 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xb4>)
   e5032:	681a      	ldr	r2, [r3, #0]
   e5034:	07d2      	lsls	r2, r2, #31
   e5036:	bf5c      	itt	pl
   e5038:	2201      	movpl	r2, #1
   e503a:	601a      	strpl	r2, [r3, #0]
   e503c:	4b12      	ldr	r3, [pc, #72]	; (e5088 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xb8>)
   e503e:	681a      	ldr	r2, [r3, #0]
   e5040:	07d0      	lsls	r0, r2, #31
   e5042:	bf5c      	itt	pl
   e5044:	2201      	movpl	r2, #1
   e5046:	601a      	strpl	r2, [r3, #0]
   e5048:	4b10      	ldr	r3, [pc, #64]	; (e508c <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xbc>)
   e504a:	681a      	ldr	r2, [r3, #0]
   e504c:	07d1      	lsls	r1, r2, #31
   e504e:	bf5c      	itt	pl
   e5050:	2201      	movpl	r2, #1
   e5052:	601a      	strpl	r2, [r3, #0]
   e5054:	4b0e      	ldr	r3, [pc, #56]	; (e5090 <_GLOBAL__sub_I__ZNSt12ctype_bynameIcEC2ERKSsj+0xc0>)
   e5056:	681a      	ldr	r2, [r3, #0]
   e5058:	07d2      	lsls	r2, r2, #31
   e505a:	bf5c      	itt	pl
   e505c:	2201      	movpl	r2, #1
   e505e:	601a      	strpl	r2, [r3, #0]
   e5060:	4770      	bx	lr
   e5062:	bf00      	nop
   e5064:	2003e7ec 	.word	0x2003e7ec
   e5068:	2003e7e8 	.word	0x2003e7e8
   e506c:	2003e7e4 	.word	0x2003e7e4
   e5070:	2003e7e0 	.word	0x2003e7e0
   e5074:	2003e7dc 	.word	0x2003e7dc
   e5078:	2003e7d8 	.word	0x2003e7d8
   e507c:	2003e7d4 	.word	0x2003e7d4
   e5080:	2003e7d0 	.word	0x2003e7d0
   e5084:	2003e7cc 	.word	0x2003e7cc
   e5088:	2003e7c8 	.word	0x2003e7c8
   e508c:	2003e7c4 	.word	0x2003e7c4
   e5090:	2003e7c0 	.word	0x2003e7c0
   e5094:	00000000 	.word	0x00000000

000e5098 <floor>:
   e5098:	ec51 0b10 	vmov	r0, r1, d0
   e509c:	f3c1 530a 	ubfx	r3, r1, #20, #11
   e50a0:	e92d 41f0 	stmdb	sp!, {r4, r5, r6, r7, r8, lr}
   e50a4:	f2a3 35ff 	subw	r5, r3, #1023	; 0x3ff
   e50a8:	2d13      	cmp	r5, #19
   e50aa:	460c      	mov	r4, r1
   e50ac:	460f      	mov	r7, r1
   e50ae:	ee10 6a10 	vmov	r6, s0
   e50b2:	dc1d      	bgt.n	e50f0 <floor+0x58>
   e50b4:	2d00      	cmp	r5, #0
   e50b6:	db43      	blt.n	e5140 <floor+0xa8>
   e50b8:	4b3d      	ldr	r3, [pc, #244]	; (e51b0 <floor+0x118>)
   e50ba:	fa43 f805 	asr.w	r8, r3, r5
   e50be:	ea01 0308 	and.w	r3, r1, r8
   e50c2:	4303      	orrs	r3, r0
   e50c4:	d019      	beq.n	e50fa <floor+0x62>
   e50c6:	a338      	add	r3, pc, #224	; (adr r3, e51a8 <floor+0x110>)
   e50c8:	e9d3 2300 	ldrd	r2, r3, [r3]
   e50cc:	f001 fe20 	bl	e6d10 <__adddf3>
   e50d0:	2200      	movs	r2, #0
   e50d2:	2300      	movs	r3, #0
   e50d4:	f002 fa5e 	bl	e7594 <__aeabi_dcmpgt>
   e50d8:	b120      	cbz	r0, e50e4 <floor+0x4c>
   e50da:	2c00      	cmp	r4, #0
   e50dc:	db49      	blt.n	e5172 <floor+0xda>
   e50de:	ea27 0408 	bic.w	r4, r7, r8
   e50e2:	2600      	movs	r6, #0
   e50e4:	4623      	mov	r3, r4
   e50e6:	4632      	mov	r2, r6
   e50e8:	ec43 2b10 	vmov	d0, r2, r3
   e50ec:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   e50f0:	2d33      	cmp	r5, #51	; 0x33
   e50f2:	dd06      	ble.n	e5102 <floor+0x6a>
   e50f4:	f5b5 6f80 	cmp.w	r5, #1024	; 0x400
   e50f8:	d032      	beq.n	e5160 <floor+0xc8>
   e50fa:	ec41 0b10 	vmov	d0, r0, r1
   e50fe:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   e5102:	f2a3 4313 	subw	r3, r3, #1043	; 0x413
   e5106:	f04f 38ff 	mov.w	r8, #4294967295	; 0xffffffff
   e510a:	fa28 f803 	lsr.w	r8, r8, r3
   e510e:	ea10 0f08 	tst.w	r0, r8
   e5112:	d0f2      	beq.n	e50fa <floor+0x62>
   e5114:	a324      	add	r3, pc, #144	; (adr r3, e51a8 <floor+0x110>)
   e5116:	e9d3 2300 	ldrd	r2, r3, [r3]
   e511a:	f001 fdf9 	bl	e6d10 <__adddf3>
   e511e:	2200      	movs	r2, #0
   e5120:	2300      	movs	r3, #0
   e5122:	f002 fa37 	bl	e7594 <__aeabi_dcmpgt>
   e5126:	2800      	cmp	r0, #0
   e5128:	d0dc      	beq.n	e50e4 <floor+0x4c>
   e512a:	2c00      	cmp	r4, #0
   e512c:	db27      	blt.n	e517e <floor+0xe6>
   e512e:	463c      	mov	r4, r7
   e5130:	ea26 0608 	bic.w	r6, r6, r8
   e5134:	4623      	mov	r3, r4
   e5136:	4632      	mov	r2, r6
   e5138:	ec43 2b10 	vmov	d0, r2, r3
   e513c:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   e5140:	a319      	add	r3, pc, #100	; (adr r3, e51a8 <floor+0x110>)
   e5142:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5146:	f001 fde3 	bl	e6d10 <__adddf3>
   e514a:	2200      	movs	r2, #0
   e514c:	2300      	movs	r3, #0
   e514e:	f002 fa21 	bl	e7594 <__aeabi_dcmpgt>
   e5152:	2800      	cmp	r0, #0
   e5154:	d0c6      	beq.n	e50e4 <floor+0x4c>
   e5156:	2c00      	cmp	r4, #0
   e5158:	db1c      	blt.n	e5194 <floor+0xfc>
   e515a:	2600      	movs	r6, #0
   e515c:	4634      	mov	r4, r6
   e515e:	e7c1      	b.n	e50e4 <floor+0x4c>
   e5160:	ee10 2a10 	vmov	r2, s0
   e5164:	460b      	mov	r3, r1
   e5166:	f001 fdd3 	bl	e6d10 <__adddf3>
   e516a:	ec41 0b10 	vmov	d0, r0, r1
   e516e:	e8bd 81f0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, pc}
   e5172:	f44f 1380 	mov.w	r3, #1048576	; 0x100000
   e5176:	fa43 f505 	asr.w	r5, r3, r5
   e517a:	442f      	add	r7, r5
   e517c:	e7af      	b.n	e50de <floor+0x46>
   e517e:	2d14      	cmp	r5, #20
   e5180:	d010      	beq.n	e51a4 <floor+0x10c>
   e5182:	2301      	movs	r3, #1
   e5184:	f1c5 0534 	rsb	r5, r5, #52	; 0x34
   e5188:	fa03 f505 	lsl.w	r5, r3, r5
   e518c:	19ae      	adds	r6, r5, r6
   e518e:	bf28      	it	cs
   e5190:	18ff      	addcs	r7, r7, r3
   e5192:	e7cc      	b.n	e512e <floor+0x96>
   e5194:	f024 4200 	bic.w	r2, r4, #2147483648	; 0x80000000
   e5198:	4b06      	ldr	r3, [pc, #24]	; (e51b4 <floor+0x11c>)
   e519a:	4332      	orrs	r2, r6
   e519c:	bf18      	it	ne
   e519e:	461c      	movne	r4, r3
   e51a0:	2600      	movs	r6, #0
   e51a2:	e79f      	b.n	e50e4 <floor+0x4c>
   e51a4:	3701      	adds	r7, #1
   e51a6:	e7c2      	b.n	e512e <floor+0x96>
   e51a8:	8800759c 	.word	0x8800759c
   e51ac:	7e37e43c 	.word	0x7e37e43c
   e51b0:	000fffff 	.word	0x000fffff
   e51b4:	bff00000 	.word	0xbff00000

000e51b8 <frexp>:
   e51b8:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
   e51ba:	ec53 2b10 	vmov	r2, r3, d0
   e51be:	4e17      	ldr	r6, [pc, #92]	; (e521c <frexp+0x64>)
   e51c0:	f023 4100 	bic.w	r1, r3, #2147483648	; 0x80000000
   e51c4:	2500      	movs	r5, #0
   e51c6:	42b1      	cmp	r1, r6
   e51c8:	4604      	mov	r4, r0
   e51ca:	6005      	str	r5, [r0, #0]
   e51cc:	dc23      	bgt.n	e5216 <frexp+0x5e>
   e51ce:	ea52 0601 	orrs.w	r6, r2, r1
   e51d2:	d020      	beq.n	e5216 <frexp+0x5e>
   e51d4:	f5b1 1f80 	cmp.w	r1, #1048576	; 0x100000
   e51d8:	4618      	mov	r0, r3
   e51da:	da0c      	bge.n	e51f6 <frexp+0x3e>
   e51dc:	4619      	mov	r1, r3
   e51de:	2200      	movs	r2, #0
   e51e0:	ee10 0a10 	vmov	r0, s0
   e51e4:	4b0e      	ldr	r3, [pc, #56]	; (e5220 <frexp+0x68>)
   e51e6:	f001 ff45 	bl	e7074 <__aeabi_dmul>
   e51ea:	f06f 0535 	mvn.w	r5, #53	; 0x35
   e51ee:	4602      	mov	r2, r0
   e51f0:	4608      	mov	r0, r1
   e51f2:	f021 4100 	bic.w	r1, r1, #2147483648	; 0x80000000
   e51f6:	f020 40ff 	bic.w	r0, r0, #2139095040	; 0x7f800000
   e51fa:	f420 00e0 	bic.w	r0, r0, #7340032	; 0x700000
   e51fe:	1509      	asrs	r1, r1, #20
   e5200:	f040 537f 	orr.w	r3, r0, #1069547520	; 0x3fc00000
   e5204:	f2a1 31fe 	subw	r1, r1, #1022	; 0x3fe
   e5208:	4429      	add	r1, r5
   e520a:	f443 1300 	orr.w	r3, r3, #2097152	; 0x200000
   e520e:	6021      	str	r1, [r4, #0]
   e5210:	ec43 2b10 	vmov	d0, r2, r3
   e5214:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
   e5216:	ec43 2b10 	vmov	d0, r2, r3
   e521a:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
   e521c:	7fefffff 	.word	0x7fefffff
   e5220:	43500000 	.word	0x43500000

000e5224 <round>:
   e5224:	b5f8      	push	{r3, r4, r5, r6, r7, lr}
   e5226:	ec53 2b10 	vmov	r2, r3, d0
   e522a:	f3c3 540a 	ubfx	r4, r3, #20, #11
   e522e:	f2a4 30ff 	subw	r0, r4, #1023	; 0x3ff
   e5232:	2813      	cmp	r0, #19
   e5234:	4619      	mov	r1, r3
   e5236:	ee10 7a10 	vmov	r7, s0
   e523a:	dc12      	bgt.n	e5262 <round+0x3e>
   e523c:	2800      	cmp	r0, #0
   e523e:	db32      	blt.n	e52a6 <round+0x82>
   e5240:	4e23      	ldr	r6, [pc, #140]	; (e52d0 <round+0xac>)
   e5242:	4106      	asrs	r6, r0
   e5244:	4233      	tst	r3, r6
   e5246:	461d      	mov	r5, r3
   e5248:	d02a      	beq.n	e52a0 <round+0x7c>
   e524a:	f44f 2100 	mov.w	r1, #524288	; 0x80000
   e524e:	4101      	asrs	r1, r0
   e5250:	4429      	add	r1, r5
   e5252:	ea21 0106 	bic.w	r1, r1, r6
   e5256:	2400      	movs	r4, #0
   e5258:	460b      	mov	r3, r1
   e525a:	4622      	mov	r2, r4
   e525c:	ec43 2b10 	vmov	d0, r2, r3
   e5260:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
   e5262:	2833      	cmp	r0, #51	; 0x33
   e5264:	dd05      	ble.n	e5272 <round+0x4e>
   e5266:	f5b0 6f80 	cmp.w	r0, #1024	; 0x400
   e526a:	d022      	beq.n	e52b2 <round+0x8e>
   e526c:	ec43 2b10 	vmov	d0, r2, r3
   e5270:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
   e5272:	f2a4 4413 	subw	r4, r4, #1043	; 0x413
   e5276:	f04f 35ff 	mov.w	r5, #4294967295	; 0xffffffff
   e527a:	fa25 f404 	lsr.w	r4, r5, r4
   e527e:	4222      	tst	r2, r4
   e5280:	d0f4      	beq.n	e526c <round+0x48>
   e5282:	2301      	movs	r3, #1
   e5284:	f1c0 0033 	rsb	r0, r0, #51	; 0x33
   e5288:	fa03 f000 	lsl.w	r0, r3, r0
   e528c:	19c0      	adds	r0, r0, r7
   e528e:	bf28      	it	cs
   e5290:	18c9      	addcs	r1, r1, r3
   e5292:	ea20 0404 	bic.w	r4, r0, r4
   e5296:	460b      	mov	r3, r1
   e5298:	4622      	mov	r2, r4
   e529a:	ec43 2b10 	vmov	d0, r2, r3
   e529e:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
   e52a0:	2a00      	cmp	r2, #0
   e52a2:	d1d2      	bne.n	e524a <round+0x26>
   e52a4:	e7e2      	b.n	e526c <round+0x48>
   e52a6:	3001      	adds	r0, #1
   e52a8:	f003 4100 	and.w	r1, r3, #2147483648	; 0x80000000
   e52ac:	d009      	beq.n	e52c2 <round+0x9e>
   e52ae:	2400      	movs	r4, #0
   e52b0:	e7d2      	b.n	e5258 <round+0x34>
   e52b2:	ee10 0a10 	vmov	r0, s0
   e52b6:	4619      	mov	r1, r3
   e52b8:	f001 fd2a 	bl	e6d10 <__adddf3>
   e52bc:	ec41 0b10 	vmov	d0, r0, r1
   e52c0:	bdf8      	pop	{r3, r4, r5, r6, r7, pc}
   e52c2:	f041 517f 	orr.w	r1, r1, #1069547520	; 0x3fc00000
   e52c6:	f441 1140 	orr.w	r1, r1, #3145728	; 0x300000
   e52ca:	2400      	movs	r4, #0
   e52cc:	e7c4      	b.n	e5258 <round+0x34>
   e52ce:	bf00      	nop
   e52d0:	000fffff 	.word	0x000fffff

000e52d4 <ceilf>:
   e52d4:	ee10 2a10 	vmov	r2, s0
   e52d8:	f022 4100 	bic.w	r1, r2, #2147483648	; 0x80000000
   e52dc:	0dcb      	lsrs	r3, r1, #23
   e52de:	3b7f      	subs	r3, #127	; 0x7f
   e52e0:	2b16      	cmp	r3, #22
   e52e2:	dc1c      	bgt.n	e531e <ceilf+0x4a>
   e52e4:	2b00      	cmp	r3, #0
   e52e6:	ee10 0a10 	vmov	r0, s0
   e52ea:	db21      	blt.n	e5330 <ceilf+0x5c>
   e52ec:	4919      	ldr	r1, [pc, #100]	; (e5354 <ceilf+0x80>)
   e52ee:	4119      	asrs	r1, r3
   e52f0:	420a      	tst	r2, r1
   e52f2:	d01c      	beq.n	e532e <ceilf+0x5a>
   e52f4:	eddf 7a18 	vldr	s15, [pc, #96]	; e5358 <ceilf+0x84>
   e52f8:	ee70 7a27 	vadd.f32	s15, s0, s15
   e52fc:	eef5 7ac0 	vcmpe.f32	s15, #0.0
   e5300:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5304:	dd13      	ble.n	e532e <ceilf+0x5a>
   e5306:	2a00      	cmp	r2, #0
   e5308:	dd04      	ble.n	e5314 <ceilf+0x40>
   e530a:	f44f 0200 	mov.w	r2, #8388608	; 0x800000
   e530e:	fa42 f303 	asr.w	r3, r2, r3
   e5312:	4418      	add	r0, r3
   e5314:	ea20 0301 	bic.w	r3, r0, r1
   e5318:	ee00 3a10 	vmov	s0, r3
   e531c:	4770      	bx	lr
   e531e:	f1b1 4fff 	cmp.w	r1, #2139095040	; 0x7f800000
   e5322:	d304      	bcc.n	e532e <ceilf+0x5a>
   e5324:	ee30 0a00 	vadd.f32	s0, s0, s0
   e5328:	4770      	bx	lr
   e532a:	ed9f 0a0c 	vldr	s0, [pc, #48]	; e535c <ceilf+0x88>
   e532e:	4770      	bx	lr
   e5330:	eddf 7a09 	vldr	s15, [pc, #36]	; e5358 <ceilf+0x84>
   e5334:	ee70 7a27 	vadd.f32	s15, s0, s15
   e5338:	eef5 7ac0 	vcmpe.f32	s15, #0.0
   e533c:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5340:	ddf5      	ble.n	e532e <ceilf+0x5a>
   e5342:	2a00      	cmp	r2, #0
   e5344:	dbf1      	blt.n	e532a <ceilf+0x56>
   e5346:	2900      	cmp	r1, #0
   e5348:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   e534c:	bf18      	it	ne
   e534e:	eeb0 0a67 	vmovne.f32	s0, s15
   e5352:	4770      	bx	lr
   e5354:	007fffff 	.word	0x007fffff
   e5358:	7149f2ca 	.word	0x7149f2ca
   e535c:	80000000 	.word	0x80000000

000e5360 <cosf>:
   e5360:	b500      	push	{lr}
   e5362:	ee10 3a10 	vmov	r3, s0
   e5366:	4a20      	ldr	r2, [pc, #128]	; (e53e8 <cosf+0x88>)
   e5368:	f023 4300 	bic.w	r3, r3, #2147483648	; 0x80000000
   e536c:	4293      	cmp	r3, r2
   e536e:	b083      	sub	sp, #12
   e5370:	dd19      	ble.n	e53a6 <cosf+0x46>
   e5372:	f1b3 4fff 	cmp.w	r3, #2139095040	; 0x7f800000
   e5376:	db04      	blt.n	e5382 <cosf+0x22>
   e5378:	ee30 0a40 	vsub.f32	s0, s0, s0
   e537c:	b003      	add	sp, #12
   e537e:	f85d fb04 	ldr.w	pc, [sp], #4
   e5382:	4668      	mov	r0, sp
   e5384:	f000 fe9e 	bl	e60c4 <__ieee754_rem_pio2f>
   e5388:	f000 0003 	and.w	r0, r0, #3
   e538c:	2801      	cmp	r0, #1
   e538e:	d01a      	beq.n	e53c6 <cosf+0x66>
   e5390:	2802      	cmp	r0, #2
   e5392:	d00f      	beq.n	e53b4 <cosf+0x54>
   e5394:	b300      	cbz	r0, e53d8 <cosf+0x78>
   e5396:	2001      	movs	r0, #1
   e5398:	eddd 0a01 	vldr	s1, [sp, #4]
   e539c:	ed9d 0a00 	vldr	s0, [sp]
   e53a0:	f001 fbc4 	bl	e6b2c <__kernel_sinf>
   e53a4:	e7ea      	b.n	e537c <cosf+0x1c>
   e53a6:	eddf 0a11 	vldr	s1, [pc, #68]	; e53ec <cosf+0x8c>
   e53aa:	f001 f829 	bl	e6400 <__kernel_cosf>
   e53ae:	b003      	add	sp, #12
   e53b0:	f85d fb04 	ldr.w	pc, [sp], #4
   e53b4:	eddd 0a01 	vldr	s1, [sp, #4]
   e53b8:	ed9d 0a00 	vldr	s0, [sp]
   e53bc:	f001 f820 	bl	e6400 <__kernel_cosf>
   e53c0:	eeb1 0a40 	vneg.f32	s0, s0
   e53c4:	e7da      	b.n	e537c <cosf+0x1c>
   e53c6:	eddd 0a01 	vldr	s1, [sp, #4]
   e53ca:	ed9d 0a00 	vldr	s0, [sp]
   e53ce:	f001 fbad 	bl	e6b2c <__kernel_sinf>
   e53d2:	eeb1 0a40 	vneg.f32	s0, s0
   e53d6:	e7d1      	b.n	e537c <cosf+0x1c>
   e53d8:	eddd 0a01 	vldr	s1, [sp, #4]
   e53dc:	ed9d 0a00 	vldr	s0, [sp]
   e53e0:	f001 f80e 	bl	e6400 <__kernel_cosf>
   e53e4:	e7ca      	b.n	e537c <cosf+0x1c>
   e53e6:	bf00      	nop
   e53e8:	3f490fd8 	.word	0x3f490fd8
   e53ec:	00000000 	.word	0x00000000

000e53f0 <floorf>:
   e53f0:	ee10 2a10 	vmov	r2, s0
   e53f4:	f022 4100 	bic.w	r1, r2, #2147483648	; 0x80000000
   e53f8:	0dcb      	lsrs	r3, r1, #23
   e53fa:	3b7f      	subs	r3, #127	; 0x7f
   e53fc:	2b16      	cmp	r3, #22
   e53fe:	dc17      	bgt.n	e5430 <floorf+0x40>
   e5400:	2b00      	cmp	r3, #0
   e5402:	ee10 0a10 	vmov	r0, s0
   e5406:	db19      	blt.n	e543c <floorf+0x4c>
   e5408:	491a      	ldr	r1, [pc, #104]	; (e5474 <floorf+0x84>)
   e540a:	4119      	asrs	r1, r3
   e540c:	420a      	tst	r2, r1
   e540e:	d022      	beq.n	e5456 <floorf+0x66>
   e5410:	eddf 7a19 	vldr	s15, [pc, #100]	; e5478 <floorf+0x88>
   e5414:	ee70 7a27 	vadd.f32	s15, s0, s15
   e5418:	eef5 7ac0 	vcmpe.f32	s15, #0.0
   e541c:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5420:	dd19      	ble.n	e5456 <floorf+0x66>
   e5422:	2a00      	cmp	r2, #0
   e5424:	db18      	blt.n	e5458 <floorf+0x68>
   e5426:	ea20 0301 	bic.w	r3, r0, r1
   e542a:	ee00 3a10 	vmov	s0, r3
   e542e:	4770      	bx	lr
   e5430:	f1b1 4fff 	cmp.w	r1, #2139095040	; 0x7f800000
   e5434:	d30f      	bcc.n	e5456 <floorf+0x66>
   e5436:	ee30 0a00 	vadd.f32	s0, s0, s0
   e543a:	4770      	bx	lr
   e543c:	eddf 7a0e 	vldr	s15, [pc, #56]	; e5478 <floorf+0x88>
   e5440:	ee70 7a27 	vadd.f32	s15, s0, s15
   e5444:	eef5 7ac0 	vcmpe.f32	s15, #0.0
   e5448:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e544c:	dd03      	ble.n	e5456 <floorf+0x66>
   e544e:	2a00      	cmp	r2, #0
   e5450:	db08      	blt.n	e5464 <floorf+0x74>
   e5452:	ed9f 0a0a 	vldr	s0, [pc, #40]	; e547c <floorf+0x8c>
   e5456:	4770      	bx	lr
   e5458:	f44f 0200 	mov.w	r2, #8388608	; 0x800000
   e545c:	fa42 f303 	asr.w	r3, r2, r3
   e5460:	4418      	add	r0, r3
   e5462:	e7e0      	b.n	e5426 <floorf+0x36>
   e5464:	2900      	cmp	r1, #0
   e5466:	eeff 7a00 	vmov.f32	s15, #240	; 0xbf800000 -1.0
   e546a:	bf18      	it	ne
   e546c:	eeb0 0a67 	vmovne.f32	s0, s15
   e5470:	4770      	bx	lr
   e5472:	bf00      	nop
   e5474:	007fffff 	.word	0x007fffff
   e5478:	7149f2ca 	.word	0x7149f2ca
   e547c:	00000000 	.word	0x00000000

000e5480 <fmaxf>:
   e5480:	b508      	push	{r3, lr}
   e5482:	ed2d 8b02 	vpush	{d8}
   e5486:	eeb0 8a60 	vmov.f32	s16, s1
   e548a:	eef0 8a40 	vmov.f32	s17, s0
   e548e:	f000 f833 	bl	e54f8 <__fpclassifyf>
   e5492:	b920      	cbnz	r0, e549e <fmaxf+0x1e>
   e5494:	eeb0 0a48 	vmov.f32	s0, s16
   e5498:	ecbd 8b02 	vpop	{d8}
   e549c:	bd08      	pop	{r3, pc}
   e549e:	eeb0 0a48 	vmov.f32	s0, s16
   e54a2:	f000 f829 	bl	e54f8 <__fpclassifyf>
   e54a6:	b120      	cbz	r0, e54b2 <fmaxf+0x32>
   e54a8:	eef4 8ac8 	vcmpe.f32	s17, s16
   e54ac:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e54b0:	ddf0      	ble.n	e5494 <fmaxf+0x14>
   e54b2:	eeb0 0a68 	vmov.f32	s0, s17
   e54b6:	ecbd 8b02 	vpop	{d8}
   e54ba:	bd08      	pop	{r3, pc}

000e54bc <fminf>:
   e54bc:	b508      	push	{r3, lr}
   e54be:	ed2d 8b02 	vpush	{d8}
   e54c2:	eeb0 8a60 	vmov.f32	s16, s1
   e54c6:	eef0 8a40 	vmov.f32	s17, s0
   e54ca:	f000 f815 	bl	e54f8 <__fpclassifyf>
   e54ce:	b920      	cbnz	r0, e54da <fminf+0x1e>
   e54d0:	eeb0 0a48 	vmov.f32	s0, s16
   e54d4:	ecbd 8b02 	vpop	{d8}
   e54d8:	bd08      	pop	{r3, pc}
   e54da:	eeb0 0a48 	vmov.f32	s0, s16
   e54de:	f000 f80b 	bl	e54f8 <__fpclassifyf>
   e54e2:	b120      	cbz	r0, e54ee <fminf+0x32>
   e54e4:	eef4 8ac8 	vcmpe.f32	s17, s16
   e54e8:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e54ec:	d5f0      	bpl.n	e54d0 <fminf+0x14>
   e54ee:	eeb0 0a68 	vmov.f32	s0, s17
   e54f2:	ecbd 8b02 	vpop	{d8}
   e54f6:	bd08      	pop	{r3, pc}

000e54f8 <__fpclassifyf>:
   e54f8:	ee10 3a10 	vmov	r3, s0
   e54fc:	f033 4000 	bics.w	r0, r3, #2147483648	; 0x80000000
   e5500:	d101      	bne.n	e5506 <__fpclassifyf+0xe>
   e5502:	2002      	movs	r0, #2
   e5504:	4770      	bx	lr
   e5506:	f5a0 0300 	sub.w	r3, r0, #8388608	; 0x800000
   e550a:	f1b3 4ffe 	cmp.w	r3, #2130706432	; 0x7f000000
   e550e:	d201      	bcs.n	e5514 <__fpclassifyf+0x1c>
   e5510:	2004      	movs	r0, #4
   e5512:	4770      	bx	lr
   e5514:	4b05      	ldr	r3, [pc, #20]	; (e552c <__fpclassifyf+0x34>)
   e5516:	1e42      	subs	r2, r0, #1
   e5518:	429a      	cmp	r2, r3
   e551a:	d801      	bhi.n	e5520 <__fpclassifyf+0x28>
   e551c:	2003      	movs	r0, #3
   e551e:	4770      	bx	lr
   e5520:	f1a0 40ff 	sub.w	r0, r0, #2139095040	; 0x7f800000
   e5524:	fab0 f080 	clz	r0, r0
   e5528:	0940      	lsrs	r0, r0, #5
   e552a:	4770      	bx	lr
   e552c:	007ffffe 	.word	0x007ffffe

000e5530 <roundf>:
   e5530:	b082      	sub	sp, #8
   e5532:	ed8d 0a01 	vstr	s0, [sp, #4]
   e5536:	9901      	ldr	r1, [sp, #4]
   e5538:	f3c1 53c7 	ubfx	r3, r1, #23, #8
   e553c:	3b7f      	subs	r3, #127	; 0x7f
   e553e:	2b16      	cmp	r3, #22
   e5540:	dc10      	bgt.n	e5564 <roundf+0x34>
   e5542:	2b00      	cmp	r3, #0
   e5544:	db1a      	blt.n	e557c <roundf+0x4c>
   e5546:	4a11      	ldr	r2, [pc, #68]	; (e558c <roundf+0x5c>)
   e5548:	fa42 f003 	asr.w	r0, r2, r3
   e554c:	4201      	tst	r1, r0
   e554e:	d00b      	beq.n	e5568 <roundf+0x38>
   e5550:	f44f 0280 	mov.w	r2, #4194304	; 0x400000
   e5554:	411a      	asrs	r2, r3
   e5556:	440a      	add	r2, r1
   e5558:	ea22 0200 	bic.w	r2, r2, r0
   e555c:	ee00 2a10 	vmov	s0, r2
   e5560:	b002      	add	sp, #8
   e5562:	4770      	bx	lr
   e5564:	2b80      	cmp	r3, #128	; 0x80
   e5566:	d003      	beq.n	e5570 <roundf+0x40>
   e5568:	ed9d 0a01 	vldr	s0, [sp, #4]
   e556c:	b002      	add	sp, #8
   e556e:	4770      	bx	lr
   e5570:	eddd 7a01 	vldr	s15, [sp, #4]
   e5574:	ee37 0aa7 	vadd.f32	s0, s15, s15
   e5578:	b002      	add	sp, #8
   e557a:	4770      	bx	lr
   e557c:	3301      	adds	r3, #1
   e557e:	f001 4200 	and.w	r2, r1, #2147483648	; 0x80000000
   e5582:	d1eb      	bne.n	e555c <roundf+0x2c>
   e5584:	f042 527e 	orr.w	r2, r2, #1065353216	; 0x3f800000
   e5588:	e7e8      	b.n	e555c <roundf+0x2c>
   e558a:	bf00      	nop
   e558c:	007fffff 	.word	0x007fffff

000e5590 <sinf>:
   e5590:	b500      	push	{lr}
   e5592:	ee10 3a10 	vmov	r3, s0
   e5596:	4a21      	ldr	r2, [pc, #132]	; (e561c <sinf+0x8c>)
   e5598:	f023 4300 	bic.w	r3, r3, #2147483648	; 0x80000000
   e559c:	4293      	cmp	r3, r2
   e559e:	b083      	sub	sp, #12
   e55a0:	dd1a      	ble.n	e55d8 <sinf+0x48>
   e55a2:	f1b3 4fff 	cmp.w	r3, #2139095040	; 0x7f800000
   e55a6:	db04      	blt.n	e55b2 <sinf+0x22>
   e55a8:	ee30 0a40 	vsub.f32	s0, s0, s0
   e55ac:	b003      	add	sp, #12
   e55ae:	f85d fb04 	ldr.w	pc, [sp], #4
   e55b2:	4668      	mov	r0, sp
   e55b4:	f000 fd86 	bl	e60c4 <__ieee754_rem_pio2f>
   e55b8:	f000 0003 	and.w	r0, r0, #3
   e55bc:	2801      	cmp	r0, #1
   e55be:	d01d      	beq.n	e55fc <sinf+0x6c>
   e55c0:	2802      	cmp	r0, #2
   e55c2:	d011      	beq.n	e55e8 <sinf+0x58>
   e55c4:	b308      	cbz	r0, e560a <sinf+0x7a>
   e55c6:	eddd 0a01 	vldr	s1, [sp, #4]
   e55ca:	ed9d 0a00 	vldr	s0, [sp]
   e55ce:	f000 ff17 	bl	e6400 <__kernel_cosf>
   e55d2:	eeb1 0a40 	vneg.f32	s0, s0
   e55d6:	e7e9      	b.n	e55ac <sinf+0x1c>
   e55d8:	2000      	movs	r0, #0
   e55da:	eddf 0a11 	vldr	s1, [pc, #68]	; e5620 <sinf+0x90>
   e55de:	f001 faa5 	bl	e6b2c <__kernel_sinf>
   e55e2:	b003      	add	sp, #12
   e55e4:	f85d fb04 	ldr.w	pc, [sp], #4
   e55e8:	2001      	movs	r0, #1
   e55ea:	eddd 0a01 	vldr	s1, [sp, #4]
   e55ee:	ed9d 0a00 	vldr	s0, [sp]
   e55f2:	f001 fa9b 	bl	e6b2c <__kernel_sinf>
   e55f6:	eeb1 0a40 	vneg.f32	s0, s0
   e55fa:	e7d7      	b.n	e55ac <sinf+0x1c>
   e55fc:	eddd 0a01 	vldr	s1, [sp, #4]
   e5600:	ed9d 0a00 	vldr	s0, [sp]
   e5604:	f000 fefc 	bl	e6400 <__kernel_cosf>
   e5608:	e7d0      	b.n	e55ac <sinf+0x1c>
   e560a:	2001      	movs	r0, #1
   e560c:	eddd 0a01 	vldr	s1, [sp, #4]
   e5610:	ed9d 0a00 	vldr	s0, [sp]
   e5614:	f001 fa8a 	bl	e6b2c <__kernel_sinf>
   e5618:	e7c8      	b.n	e55ac <sinf+0x1c>
   e561a:	bf00      	nop
   e561c:	3f490fd8 	.word	0x3f490fd8
	...

000e5628 <exp>:
   e5628:	b5f0      	push	{r4, r5, r6, r7, lr}
   e562a:	ed2d 8b04 	vpush	{d8-d9}
   e562e:	eeb0 9a40 	vmov.f32	s18, s0
   e5632:	eef0 9a60 	vmov.f32	s19, s1
   e5636:	4c3a      	ldr	r4, [pc, #232]	; (e5720 <exp+0xf8>)
   e5638:	b08b      	sub	sp, #44	; 0x2c
   e563a:	f000 f9cd 	bl	e59d8 <__ieee754_exp>
   e563e:	f994 3000 	ldrsb.w	r3, [r4]
   e5642:	eeb0 8a40 	vmov.f32	s16, s0
   e5646:	eef0 8a60 	vmov.f32	s17, s1
   e564a:	3301      	adds	r3, #1
   e564c:	d038      	beq.n	e56c0 <exp+0x98>
   e564e:	eeb0 0a49 	vmov.f32	s0, s18
   e5652:	eef0 0a69 	vmov.f32	s1, s19
   e5656:	f001 fab1 	bl	e6bbc <finite>
   e565a:	b388      	cbz	r0, e56c0 <exp+0x98>
   e565c:	a32c      	add	r3, pc, #176	; (adr r3, e5710 <exp+0xe8>)
   e565e:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5662:	ec51 0b19 	vmov	r0, r1, d9
   e5666:	f001 ff95 	bl	e7594 <__aeabi_dcmpgt>
   e566a:	4605      	mov	r5, r0
   e566c:	bb80      	cbnz	r0, e56d0 <exp+0xa8>
   e566e:	a32a      	add	r3, pc, #168	; (adr r3, e5718 <exp+0xf0>)
   e5670:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5674:	ec51 0b19 	vmov	r0, r1, d9
   e5678:	f001 ff6e 	bl	e7558 <__aeabi_dcmplt>
   e567c:	b300      	cbz	r0, e56c0 <exp+0x98>
   e567e:	f994 3000 	ldrsb.w	r3, [r4]
   e5682:	4a28      	ldr	r2, [pc, #160]	; (e5724 <exp+0xfc>)
   e5684:	9508      	str	r5, [sp, #32]
   e5686:	2600      	movs	r6, #0
   e5688:	2700      	movs	r7, #0
   e568a:	2104      	movs	r1, #4
   e568c:	2b02      	cmp	r3, #2
   e568e:	ed8d 9b04 	vstr	d9, [sp, #16]
   e5692:	ed8d 9b02 	vstr	d9, [sp, #8]
   e5696:	e9cd 6706 	strd	r6, r7, [sp, #24]
   e569a:	e88d 0006 	stmia.w	sp, {r1, r2}
   e569e:	d030      	beq.n	e5702 <exp+0xda>
   e56a0:	4668      	mov	r0, sp
   e56a2:	f001 fa93 	bl	e6bcc <matherr>
   e56a6:	b360      	cbz	r0, e5702 <exp+0xda>
   e56a8:	9b08      	ldr	r3, [sp, #32]
   e56aa:	b11b      	cbz	r3, e56b4 <exp+0x8c>
   e56ac:	f7fe fe56 	bl	e435c <__errno>
   e56b0:	9b08      	ldr	r3, [sp, #32]
   e56b2:	6003      	str	r3, [r0, #0]
   e56b4:	ed9d 0b06 	vldr	d0, [sp, #24]
   e56b8:	b00b      	add	sp, #44	; 0x2c
   e56ba:	ecbd 8b04 	vpop	{d8-d9}
   e56be:	bdf0      	pop	{r4, r5, r6, r7, pc}
   e56c0:	eeb0 0a48 	vmov.f32	s0, s16
   e56c4:	eef0 0a68 	vmov.f32	s1, s17
   e56c8:	b00b      	add	sp, #44	; 0x2c
   e56ca:	ecbd 8b04 	vpop	{d8-d9}
   e56ce:	bdf0      	pop	{r4, r5, r6, r7, pc}
   e56d0:	4a14      	ldr	r2, [pc, #80]	; (e5724 <exp+0xfc>)
   e56d2:	f994 3000 	ldrsb.w	r3, [r4]
   e56d6:	9201      	str	r2, [sp, #4]
   e56d8:	2103      	movs	r1, #3
   e56da:	2200      	movs	r2, #0
   e56dc:	ed8d 9b04 	vstr	d9, [sp, #16]
   e56e0:	ed8d 9b02 	vstr	d9, [sp, #8]
   e56e4:	9100      	str	r1, [sp, #0]
   e56e6:	9208      	str	r2, [sp, #32]
   e56e8:	b92b      	cbnz	r3, e56f6 <exp+0xce>
   e56ea:	4b0f      	ldr	r3, [pc, #60]	; (e5728 <exp+0x100>)
   e56ec:	f04f 4260 	mov.w	r2, #3758096384	; 0xe0000000
   e56f0:	e9cd 2306 	strd	r2, r3, [sp, #24]
   e56f4:	e7d4      	b.n	e56a0 <exp+0x78>
   e56f6:	490d      	ldr	r1, [pc, #52]	; (e572c <exp+0x104>)
   e56f8:	2000      	movs	r0, #0
   e56fa:	2b02      	cmp	r3, #2
   e56fc:	e9cd 0106 	strd	r0, r1, [sp, #24]
   e5700:	d1ce      	bne.n	e56a0 <exp+0x78>
   e5702:	f7fe fe2b 	bl	e435c <__errno>
   e5706:	2322      	movs	r3, #34	; 0x22
   e5708:	6003      	str	r3, [r0, #0]
   e570a:	e7cd      	b.n	e56a8 <exp+0x80>
   e570c:	f3af 8000 	nop.w
   e5710:	fefa39ef 	.word	0xfefa39ef
   e5714:	40862e42 	.word	0x40862e42
   e5718:	d52d3051 	.word	0xd52d3051
   e571c:	c0874910 	.word	0xc0874910
   e5720:	2003c3a0 	.word	0x2003c3a0
   e5724:	000eb4e0 	.word	0x000eb4e0
   e5728:	47efffff 	.word	0x47efffff
   e572c:	7ff00000 	.word	0x7ff00000

000e5730 <expf>:
   e5730:	b5d0      	push	{r4, r6, r7, lr}
   e5732:	ed2d 8b02 	vpush	{d8}
   e5736:	4c39      	ldr	r4, [pc, #228]	; (e581c <expf+0xec>)
   e5738:	b08a      	sub	sp, #40	; 0x28
   e573a:	eef0 8a40 	vmov.f32	s17, s0
   e573e:	f000 fadd 	bl	e5cfc <__ieee754_expf>
   e5742:	f994 3000 	ldrsb.w	r3, [r4]
   e5746:	3301      	adds	r3, #1
   e5748:	eeb0 8a40 	vmov.f32	s16, s0
   e574c:	d03e      	beq.n	e57cc <expf+0x9c>
   e574e:	eeb0 0a68 	vmov.f32	s0, s17
   e5752:	f001 fa4d 	bl	e6bf0 <finitef>
   e5756:	2800      	cmp	r0, #0
   e5758:	d038      	beq.n	e57cc <expf+0x9c>
   e575a:	eddf 7a31 	vldr	s15, [pc, #196]	; e5820 <expf+0xf0>
   e575e:	eef4 8ae7 	vcmpe.f32	s17, s15
   e5762:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5766:	dc37      	bgt.n	e57d8 <expf+0xa8>
   e5768:	eddf 7a2e 	vldr	s15, [pc, #184]	; e5824 <expf+0xf4>
   e576c:	eef4 8ae7 	vcmpe.f32	s17, s15
   e5770:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5774:	d52a      	bpl.n	e57cc <expf+0x9c>
   e5776:	2304      	movs	r3, #4
   e5778:	4a2b      	ldr	r2, [pc, #172]	; (e5828 <expf+0xf8>)
   e577a:	9300      	str	r3, [sp, #0]
   e577c:	ee18 0a90 	vmov	r0, s17
   e5780:	2300      	movs	r3, #0
   e5782:	9308      	str	r3, [sp, #32]
   e5784:	9201      	str	r2, [sp, #4]
   e5786:	f001 fc21 	bl	e6fcc <__aeabi_f2d>
   e578a:	f994 3000 	ldrsb.w	r3, [r4]
   e578e:	2600      	movs	r6, #0
   e5790:	2700      	movs	r7, #0
   e5792:	2b02      	cmp	r3, #2
   e5794:	e9cd 0104 	strd	r0, r1, [sp, #16]
   e5798:	e9cd 0102 	strd	r0, r1, [sp, #8]
   e579c:	e9cd 6706 	strd	r6, r7, [sp, #24]
   e57a0:	d037      	beq.n	e5812 <expf+0xe2>
   e57a2:	4668      	mov	r0, sp
   e57a4:	f001 fa12 	bl	e6bcc <matherr>
   e57a8:	2800      	cmp	r0, #0
   e57aa:	d032      	beq.n	e5812 <expf+0xe2>
   e57ac:	9b08      	ldr	r3, [sp, #32]
   e57ae:	b11b      	cbz	r3, e57b8 <expf+0x88>
   e57b0:	f7fe fdd4 	bl	e435c <__errno>
   e57b4:	9b08      	ldr	r3, [sp, #32]
   e57b6:	6003      	str	r3, [r0, #0]
   e57b8:	e9dd 0106 	ldrd	r0, r1, [sp, #24]
   e57bc:	f001 ff3c 	bl	e7638 <__aeabi_d2f>
   e57c0:	ee00 0a10 	vmov	s0, r0
   e57c4:	b00a      	add	sp, #40	; 0x28
   e57c6:	ecbd 8b02 	vpop	{d8}
   e57ca:	bdd0      	pop	{r4, r6, r7, pc}
   e57cc:	eeb0 0a48 	vmov.f32	s0, s16
   e57d0:	b00a      	add	sp, #40	; 0x28
   e57d2:	ecbd 8b02 	vpop	{d8}
   e57d6:	bdd0      	pop	{r4, r6, r7, pc}
   e57d8:	2303      	movs	r3, #3
   e57da:	4a13      	ldr	r2, [pc, #76]	; (e5828 <expf+0xf8>)
   e57dc:	9300      	str	r3, [sp, #0]
   e57de:	ee18 0a90 	vmov	r0, s17
   e57e2:	2300      	movs	r3, #0
   e57e4:	9308      	str	r3, [sp, #32]
   e57e6:	9201      	str	r2, [sp, #4]
   e57e8:	f001 fbf0 	bl	e6fcc <__aeabi_f2d>
   e57ec:	f994 3000 	ldrsb.w	r3, [r4]
   e57f0:	e9cd 0104 	strd	r0, r1, [sp, #16]
   e57f4:	e9cd 0102 	strd	r0, r1, [sp, #8]
   e57f8:	b92b      	cbnz	r3, e5806 <expf+0xd6>
   e57fa:	4b0c      	ldr	r3, [pc, #48]	; (e582c <expf+0xfc>)
   e57fc:	f04f 4260 	mov.w	r2, #3758096384	; 0xe0000000
   e5800:	e9cd 2306 	strd	r2, r3, [sp, #24]
   e5804:	e7cd      	b.n	e57a2 <expf+0x72>
   e5806:	490a      	ldr	r1, [pc, #40]	; (e5830 <expf+0x100>)
   e5808:	2000      	movs	r0, #0
   e580a:	2b02      	cmp	r3, #2
   e580c:	e9cd 0106 	strd	r0, r1, [sp, #24]
   e5810:	d1c7      	bne.n	e57a2 <expf+0x72>
   e5812:	f7fe fda3 	bl	e435c <__errno>
   e5816:	2322      	movs	r3, #34	; 0x22
   e5818:	6003      	str	r3, [r0, #0]
   e581a:	e7c7      	b.n	e57ac <expf+0x7c>
   e581c:	2003c3a0 	.word	0x2003c3a0
   e5820:	42b17180 	.word	0x42b17180
   e5824:	c2cff1b5 	.word	0xc2cff1b5
   e5828:	000eb4e4 	.word	0x000eb4e4
   e582c:	47efffff 	.word	0x47efffff
   e5830:	7ff00000 	.word	0x7ff00000

000e5834 <logf>:
   e5834:	b510      	push	{r4, lr}
   e5836:	ed2d 8b02 	vpush	{d8}
   e583a:	b08a      	sub	sp, #40	; 0x28
   e583c:	eeb0 8a40 	vmov.f32	s16, s0
   e5840:	f000 fb34 	bl	e5eac <__ieee754_logf>
   e5844:	4b34      	ldr	r3, [pc, #208]	; (e5918 <logf+0xe4>)
   e5846:	f993 4000 	ldrsb.w	r4, [r3]
   e584a:	1c63      	adds	r3, r4, #1
   e584c:	d009      	beq.n	e5862 <logf+0x2e>
   e584e:	eeb4 8a48 	vcmp.f32	s16, s16
   e5852:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5856:	d604      	bvs.n	e5862 <logf+0x2e>
   e5858:	eeb5 8ac0 	vcmpe.f32	s16, #0.0
   e585c:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5860:	dd03      	ble.n	e586a <logf+0x36>
   e5862:	b00a      	add	sp, #40	; 0x28
   e5864:	ecbd 8b02 	vpop	{d8}
   e5868:	bd10      	pop	{r4, pc}
   e586a:	4b2c      	ldr	r3, [pc, #176]	; (e591c <logf+0xe8>)
   e586c:	9301      	str	r3, [sp, #4]
   e586e:	ee18 0a10 	vmov	r0, s16
   e5872:	2300      	movs	r3, #0
   e5874:	9308      	str	r3, [sp, #32]
   e5876:	f001 fba9 	bl	e6fcc <__aeabi_f2d>
   e587a:	e9cd 0104 	strd	r0, r1, [sp, #16]
   e587e:	e9cd 0102 	strd	r0, r1, [sp, #8]
   e5882:	b9dc      	cbnz	r4, e58bc <logf+0x88>
   e5884:	4b26      	ldr	r3, [pc, #152]	; (e5920 <logf+0xec>)
   e5886:	eeb5 8a40 	vcmp.f32	s16, #0.0
   e588a:	f04f 4260 	mov.w	r2, #3758096384	; 0xe0000000
   e588e:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5892:	e9cd 2306 	strd	r2, r3, [sp, #24]
   e5896:	d136      	bne.n	e5906 <logf+0xd2>
   e5898:	2302      	movs	r3, #2
   e589a:	9300      	str	r3, [sp, #0]
   e589c:	4668      	mov	r0, sp
   e589e:	f001 f995 	bl	e6bcc <matherr>
   e58a2:	b1c0      	cbz	r0, e58d6 <logf+0xa2>
   e58a4:	9b08      	ldr	r3, [sp, #32]
   e58a6:	b9db      	cbnz	r3, e58e0 <logf+0xac>
   e58a8:	e9dd 0106 	ldrd	r0, r1, [sp, #24]
   e58ac:	f001 fec4 	bl	e7638 <__aeabi_d2f>
   e58b0:	ee00 0a10 	vmov	s0, r0
   e58b4:	b00a      	add	sp, #40	; 0x28
   e58b6:	ecbd 8b02 	vpop	{d8}
   e58ba:	bd10      	pop	{r4, pc}
   e58bc:	4b19      	ldr	r3, [pc, #100]	; (e5924 <logf+0xf0>)
   e58be:	eeb5 8a40 	vcmp.f32	s16, #0.0
   e58c2:	2200      	movs	r2, #0
   e58c4:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e58c8:	e9cd 2306 	strd	r2, r3, [sp, #24]
   e58cc:	d10d      	bne.n	e58ea <logf+0xb6>
   e58ce:	2302      	movs	r3, #2
   e58d0:	429c      	cmp	r4, r3
   e58d2:	9300      	str	r3, [sp, #0]
   e58d4:	d1e2      	bne.n	e589c <logf+0x68>
   e58d6:	f7fe fd41 	bl	e435c <__errno>
   e58da:	2322      	movs	r3, #34	; 0x22
   e58dc:	6003      	str	r3, [r0, #0]
   e58de:	e7e1      	b.n	e58a4 <logf+0x70>
   e58e0:	f7fe fd3c 	bl	e435c <__errno>
   e58e4:	9b08      	ldr	r3, [sp, #32]
   e58e6:	6003      	str	r3, [r0, #0]
   e58e8:	e7de      	b.n	e58a8 <logf+0x74>
   e58ea:	2301      	movs	r3, #1
   e58ec:	2c02      	cmp	r4, #2
   e58ee:	9300      	str	r3, [sp, #0]
   e58f0:	d10b      	bne.n	e590a <logf+0xd6>
   e58f2:	f7fe fd33 	bl	e435c <__errno>
   e58f6:	2321      	movs	r3, #33	; 0x21
   e58f8:	6003      	str	r3, [r0, #0]
   e58fa:	480b      	ldr	r0, [pc, #44]	; (e5928 <logf+0xf4>)
   e58fc:	f001 f968 	bl	e6bd0 <nan>
   e5900:	ed8d 0b06 	vstr	d0, [sp, #24]
   e5904:	e7ce      	b.n	e58a4 <logf+0x70>
   e5906:	2301      	movs	r3, #1
   e5908:	9300      	str	r3, [sp, #0]
   e590a:	4668      	mov	r0, sp
   e590c:	f001 f95e 	bl	e6bcc <matherr>
   e5910:	2800      	cmp	r0, #0
   e5912:	d1f2      	bne.n	e58fa <logf+0xc6>
   e5914:	e7ed      	b.n	e58f2 <logf+0xbe>
   e5916:	bf00      	nop
   e5918:	2003c3a0 	.word	0x2003c3a0
   e591c:	000eb4ec 	.word	0x000eb4ec
   e5920:	c7efffff 	.word	0xc7efffff
   e5924:	fff00000 	.word	0xfff00000
   e5928:	000eb4f0 	.word	0x000eb4f0

000e592c <sqrtf>:
   e592c:	b510      	push	{r4, lr}
   e592e:	ed2d 8b02 	vpush	{d8}
   e5932:	b08a      	sub	sp, #40	; 0x28
   e5934:	eeb0 8a40 	vmov.f32	s16, s0
   e5938:	f000 fd10 	bl	e635c <__ieee754_sqrtf>
   e593c:	4b24      	ldr	r3, [pc, #144]	; (e59d0 <sqrtf+0xa4>)
   e593e:	f993 4000 	ldrsb.w	r4, [r3]
   e5942:	1c63      	adds	r3, r4, #1
   e5944:	d009      	beq.n	e595a <sqrtf+0x2e>
   e5946:	eeb4 8a48 	vcmp.f32	s16, s16
   e594a:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e594e:	d604      	bvs.n	e595a <sqrtf+0x2e>
   e5950:	eeb5 8ac0 	vcmpe.f32	s16, #0.0
   e5954:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5958:	d403      	bmi.n	e5962 <sqrtf+0x36>
   e595a:	b00a      	add	sp, #40	; 0x28
   e595c:	ecbd 8b02 	vpop	{d8}
   e5960:	bd10      	pop	{r4, pc}
   e5962:	2301      	movs	r3, #1
   e5964:	4a1b      	ldr	r2, [pc, #108]	; (e59d4 <sqrtf+0xa8>)
   e5966:	9300      	str	r3, [sp, #0]
   e5968:	ee18 0a10 	vmov	r0, s16
   e596c:	2300      	movs	r3, #0
   e596e:	9201      	str	r2, [sp, #4]
   e5970:	9308      	str	r3, [sp, #32]
   e5972:	f001 fb2b 	bl	e6fcc <__aeabi_f2d>
   e5976:	2200      	movs	r2, #0
   e5978:	e9cd 0104 	strd	r0, r1, [sp, #16]
   e597c:	e9cd 0102 	strd	r0, r1, [sp, #8]
   e5980:	2300      	movs	r3, #0
   e5982:	b1bc      	cbz	r4, e59b4 <sqrtf+0x88>
   e5984:	4610      	mov	r0, r2
   e5986:	4619      	mov	r1, r3
   e5988:	f001 fc9e 	bl	e72c8 <__aeabi_ddiv>
   e598c:	2c02      	cmp	r4, #2
   e598e:	e9cd 0106 	strd	r0, r1, [sp, #24]
   e5992:	d111      	bne.n	e59b8 <sqrtf+0x8c>
   e5994:	f7fe fce2 	bl	e435c <__errno>
   e5998:	2321      	movs	r3, #33	; 0x21
   e599a:	6003      	str	r3, [r0, #0]
   e599c:	9b08      	ldr	r3, [sp, #32]
   e599e:	b98b      	cbnz	r3, e59c4 <sqrtf+0x98>
   e59a0:	e9dd 0106 	ldrd	r0, r1, [sp, #24]
   e59a4:	f001 fe48 	bl	e7638 <__aeabi_d2f>
   e59a8:	ee00 0a10 	vmov	s0, r0
   e59ac:	b00a      	add	sp, #40	; 0x28
   e59ae:	ecbd 8b02 	vpop	{d8}
   e59b2:	bd10      	pop	{r4, pc}
   e59b4:	e9cd 2306 	strd	r2, r3, [sp, #24]
   e59b8:	4668      	mov	r0, sp
   e59ba:	f001 f907 	bl	e6bcc <matherr>
   e59be:	2800      	cmp	r0, #0
   e59c0:	d1ec      	bne.n	e599c <sqrtf+0x70>
   e59c2:	e7e7      	b.n	e5994 <sqrtf+0x68>
   e59c4:	f7fe fcca 	bl	e435c <__errno>
   e59c8:	9b08      	ldr	r3, [sp, #32]
   e59ca:	6003      	str	r3, [r0, #0]
   e59cc:	e7e8      	b.n	e59a0 <sqrtf+0x74>
   e59ce:	bf00      	nop
   e59d0:	2003c3a0 	.word	0x2003c3a0
   e59d4:	000eb4f4 	.word	0x000eb4f4

000e59d8 <__ieee754_exp>:
   e59d8:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e59dc:	ec55 4b10 	vmov	r4, r5, d0
   e59e0:	49bd      	ldr	r1, [pc, #756]	; (e5cd8 <__ieee754_exp+0x300>)
   e59e2:	f025 4200 	bic.w	r2, r5, #2147483648	; 0x80000000
   e59e6:	428a      	cmp	r2, r1
   e59e8:	b083      	sub	sp, #12
   e59ea:	ea4f 77d5 	mov.w	r7, r5, lsr #31
   e59ee:	d90d      	bls.n	e5a0c <__ieee754_exp+0x34>
   e59f0:	49ba      	ldr	r1, [pc, #744]	; (e5cdc <__ieee754_exp+0x304>)
   e59f2:	428a      	cmp	r2, r1
   e59f4:	d92a      	bls.n	e5a4c <__ieee754_exp+0x74>
   e59f6:	f3c5 0313 	ubfx	r3, r5, #0, #20
   e59fa:	4323      	orrs	r3, r4
   e59fc:	f040 80fa 	bne.w	e5bf4 <__ieee754_exp+0x21c>
   e5a00:	b10f      	cbz	r7, e5a06 <__ieee754_exp+0x2e>
   e5a02:	ed9f 0b9d 	vldr	d0, [pc, #628]	; e5c78 <__ieee754_exp+0x2a0>
   e5a06:	b003      	add	sp, #12
   e5a08:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e5a0c:	4bb4      	ldr	r3, [pc, #720]	; (e5ce0 <__ieee754_exp+0x308>)
   e5a0e:	429a      	cmp	r2, r3
   e5a10:	f200 80d5 	bhi.w	e5bbe <__ieee754_exp+0x1e6>
   e5a14:	4bb3      	ldr	r3, [pc, #716]	; (e5ce4 <__ieee754_exp+0x30c>)
   e5a16:	429a      	cmp	r2, r3
   e5a18:	f200 80ea 	bhi.w	e5bf0 <__ieee754_exp+0x218>
   e5a1c:	a398      	add	r3, pc, #608	; (adr r3, e5c80 <__ieee754_exp+0x2a8>)
   e5a1e:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5a22:	ee10 0a10 	vmov	r0, s0
   e5a26:	4629      	mov	r1, r5
   e5a28:	f001 f972 	bl	e6d10 <__adddf3>
   e5a2c:	2200      	movs	r2, #0
   e5a2e:	4bae      	ldr	r3, [pc, #696]	; (e5ce8 <__ieee754_exp+0x310>)
   e5a30:	f001 fdb0 	bl	e7594 <__aeabi_dcmpgt>
   e5a34:	2800      	cmp	r0, #0
   e5a36:	f000 811c 	beq.w	e5c72 <__ieee754_exp+0x29a>
   e5a3a:	4620      	mov	r0, r4
   e5a3c:	4629      	mov	r1, r5
   e5a3e:	2200      	movs	r2, #0
   e5a40:	4ba9      	ldr	r3, [pc, #676]	; (e5ce8 <__ieee754_exp+0x310>)
   e5a42:	f001 f965 	bl	e6d10 <__adddf3>
   e5a46:	ec41 0b10 	vmov	d0, r0, r1
   e5a4a:	e7dc      	b.n	e5a06 <__ieee754_exp+0x2e>
   e5a4c:	a38e      	add	r3, pc, #568	; (adr r3, e5c88 <__ieee754_exp+0x2b0>)
   e5a4e:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5a52:	ee10 0a10 	vmov	r0, s0
   e5a56:	4629      	mov	r1, r5
   e5a58:	f001 fd9c 	bl	e7594 <__aeabi_dcmpgt>
   e5a5c:	2800      	cmp	r0, #0
   e5a5e:	f040 80d3 	bne.w	e5c08 <__ieee754_exp+0x230>
   e5a62:	a38b      	add	r3, pc, #556	; (adr r3, e5c90 <__ieee754_exp+0x2b8>)
   e5a64:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5a68:	4620      	mov	r0, r4
   e5a6a:	4629      	mov	r1, r5
   e5a6c:	f001 fd74 	bl	e7558 <__aeabi_dcmplt>
   e5a70:	2800      	cmp	r0, #0
   e5a72:	d1c6      	bne.n	e5a02 <__ieee754_exp+0x2a>
   e5a74:	4e9d      	ldr	r6, [pc, #628]	; (e5cec <__ieee754_exp+0x314>)
   e5a76:	a388      	add	r3, pc, #544	; (adr r3, e5c98 <__ieee754_exp+0x2c0>)
   e5a78:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5a7c:	eb06 06c7 	add.w	r6, r6, r7, lsl #3
   e5a80:	4620      	mov	r0, r4
   e5a82:	4629      	mov	r1, r5
   e5a84:	f001 faf6 	bl	e7074 <__aeabi_dmul>
   e5a88:	e9d6 2300 	ldrd	r2, r3, [r6]
   e5a8c:	f001 f940 	bl	e6d10 <__adddf3>
   e5a90:	f001 fd8a 	bl	e75a8 <__aeabi_d2iz>
   e5a94:	4606      	mov	r6, r0
   e5a96:	f001 fa87 	bl	e6fa8 <__aeabi_i2d>
   e5a9a:	a381      	add	r3, pc, #516	; (adr r3, e5ca0 <__ieee754_exp+0x2c8>)
   e5a9c:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5aa0:	4680      	mov	r8, r0
   e5aa2:	4689      	mov	r9, r1
   e5aa4:	f001 fae6 	bl	e7074 <__aeabi_dmul>
   e5aa8:	4602      	mov	r2, r0
   e5aaa:	460b      	mov	r3, r1
   e5aac:	4620      	mov	r0, r4
   e5aae:	4629      	mov	r1, r5
   e5ab0:	f001 f92c 	bl	e6d0c <__aeabi_dsub>
   e5ab4:	a37c      	add	r3, pc, #496	; (adr r3, e5ca8 <__ieee754_exp+0x2d0>)
   e5ab6:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5aba:	e9cd 0100 	strd	r0, r1, [sp]
   e5abe:	4640      	mov	r0, r8
   e5ac0:	4649      	mov	r1, r9
   e5ac2:	f001 fad7 	bl	e7074 <__aeabi_dmul>
   e5ac6:	4682      	mov	sl, r0
   e5ac8:	468b      	mov	fp, r1
   e5aca:	4652      	mov	r2, sl
   e5acc:	465b      	mov	r3, fp
   e5ace:	e9dd 0100 	ldrd	r0, r1, [sp]
   e5ad2:	f001 f91b 	bl	e6d0c <__aeabi_dsub>
   e5ad6:	4604      	mov	r4, r0
   e5ad8:	460d      	mov	r5, r1
   e5ada:	4622      	mov	r2, r4
   e5adc:	462b      	mov	r3, r5
   e5ade:	4620      	mov	r0, r4
   e5ae0:	4629      	mov	r1, r5
   e5ae2:	f001 fac7 	bl	e7074 <__aeabi_dmul>
   e5ae6:	a372      	add	r3, pc, #456	; (adr r3, e5cb0 <__ieee754_exp+0x2d8>)
   e5ae8:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5aec:	4680      	mov	r8, r0
   e5aee:	4689      	mov	r9, r1
   e5af0:	f001 fac0 	bl	e7074 <__aeabi_dmul>
   e5af4:	a370      	add	r3, pc, #448	; (adr r3, e5cb8 <__ieee754_exp+0x2e0>)
   e5af6:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5afa:	f001 f907 	bl	e6d0c <__aeabi_dsub>
   e5afe:	4642      	mov	r2, r8
   e5b00:	464b      	mov	r3, r9
   e5b02:	f001 fab7 	bl	e7074 <__aeabi_dmul>
   e5b06:	a36e      	add	r3, pc, #440	; (adr r3, e5cc0 <__ieee754_exp+0x2e8>)
   e5b08:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5b0c:	f001 f900 	bl	e6d10 <__adddf3>
   e5b10:	4642      	mov	r2, r8
   e5b12:	464b      	mov	r3, r9
   e5b14:	f001 faae 	bl	e7074 <__aeabi_dmul>
   e5b18:	a36b      	add	r3, pc, #428	; (adr r3, e5cc8 <__ieee754_exp+0x2f0>)
   e5b1a:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5b1e:	f001 f8f5 	bl	e6d0c <__aeabi_dsub>
   e5b22:	4642      	mov	r2, r8
   e5b24:	464b      	mov	r3, r9
   e5b26:	f001 faa5 	bl	e7074 <__aeabi_dmul>
   e5b2a:	a369      	add	r3, pc, #420	; (adr r3, e5cd0 <__ieee754_exp+0x2f8>)
   e5b2c:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5b30:	f001 f8ee 	bl	e6d10 <__adddf3>
   e5b34:	4642      	mov	r2, r8
   e5b36:	464b      	mov	r3, r9
   e5b38:	f001 fa9c 	bl	e7074 <__aeabi_dmul>
   e5b3c:	4602      	mov	r2, r0
   e5b3e:	460b      	mov	r3, r1
   e5b40:	4620      	mov	r0, r4
   e5b42:	4629      	mov	r1, r5
   e5b44:	f001 f8e2 	bl	e6d0c <__aeabi_dsub>
   e5b48:	4680      	mov	r8, r0
   e5b4a:	4689      	mov	r9, r1
   e5b4c:	2e00      	cmp	r6, #0
   e5b4e:	d065      	beq.n	e5c1c <__ieee754_exp+0x244>
   e5b50:	4620      	mov	r0, r4
   e5b52:	4629      	mov	r1, r5
   e5b54:	4642      	mov	r2, r8
   e5b56:	464b      	mov	r3, r9
   e5b58:	f001 fa8c 	bl	e7074 <__aeabi_dmul>
   e5b5c:	4642      	mov	r2, r8
   e5b5e:	4604      	mov	r4, r0
   e5b60:	460d      	mov	r5, r1
   e5b62:	464b      	mov	r3, r9
   e5b64:	2000      	movs	r0, #0
   e5b66:	f04f 4180 	mov.w	r1, #1073741824	; 0x40000000
   e5b6a:	f001 f8cf 	bl	e6d0c <__aeabi_dsub>
   e5b6e:	4602      	mov	r2, r0
   e5b70:	460b      	mov	r3, r1
   e5b72:	4620      	mov	r0, r4
   e5b74:	4629      	mov	r1, r5
   e5b76:	f001 fba7 	bl	e72c8 <__aeabi_ddiv>
   e5b7a:	4602      	mov	r2, r0
   e5b7c:	460b      	mov	r3, r1
   e5b7e:	4650      	mov	r0, sl
   e5b80:	4659      	mov	r1, fp
   e5b82:	f001 f8c3 	bl	e6d0c <__aeabi_dsub>
   e5b86:	e9dd 2300 	ldrd	r2, r3, [sp]
   e5b8a:	f001 f8bf 	bl	e6d0c <__aeabi_dsub>
   e5b8e:	460b      	mov	r3, r1
   e5b90:	4602      	mov	r2, r0
   e5b92:	4955      	ldr	r1, [pc, #340]	; (e5ce8 <__ieee754_exp+0x310>)
   e5b94:	2000      	movs	r0, #0
   e5b96:	f001 f8b9 	bl	e6d0c <__aeabi_dsub>
   e5b9a:	f46f 737f 	mvn.w	r3, #1020	; 0x3fc
   e5b9e:	429e      	cmp	r6, r3
   e5ba0:	da60      	bge.n	e5c64 <__ieee754_exp+0x28c>
   e5ba2:	f506 767a 	add.w	r6, r6, #1000	; 0x3e8
   e5ba6:	eb01 5106 	add.w	r1, r1, r6, lsl #20
   e5baa:	2200      	movs	r2, #0
   e5bac:	f04f 73b8 	mov.w	r3, #24117248	; 0x1700000
   e5bb0:	f001 fa60 	bl	e7074 <__aeabi_dmul>
   e5bb4:	ec41 0b10 	vmov	d0, r0, r1
   e5bb8:	b003      	add	sp, #12
   e5bba:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e5bbe:	4b4c      	ldr	r3, [pc, #304]	; (e5cf0 <__ieee754_exp+0x318>)
   e5bc0:	429a      	cmp	r2, r3
   e5bc2:	f63f af57 	bhi.w	e5a74 <__ieee754_exp+0x9c>
   e5bc6:	4b4b      	ldr	r3, [pc, #300]	; (e5cf4 <__ieee754_exp+0x31c>)
   e5bc8:	ea4f 08c7 	mov.w	r8, r7, lsl #3
   e5bcc:	4443      	add	r3, r8
   e5bce:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5bd2:	ee10 0a10 	vmov	r0, s0
   e5bd6:	4629      	mov	r1, r5
   e5bd8:	f001 f898 	bl	e6d0c <__aeabi_dsub>
   e5bdc:	4b46      	ldr	r3, [pc, #280]	; (e5cf8 <__ieee754_exp+0x320>)
   e5bde:	f1c7 0601 	rsb	r6, r7, #1
   e5be2:	4498      	add	r8, r3
   e5be4:	e9cd 0100 	strd	r0, r1, [sp]
   e5be8:	e9d8 ab00 	ldrd	sl, fp, [r8]
   e5bec:	1bf6      	subs	r6, r6, r7
   e5bee:	e76c      	b.n	e5aca <__ieee754_exp+0xf2>
   e5bf0:	2600      	movs	r6, #0
   e5bf2:	e772      	b.n	e5ada <__ieee754_exp+0x102>
   e5bf4:	ee10 2a10 	vmov	r2, s0
   e5bf8:	462b      	mov	r3, r5
   e5bfa:	4620      	mov	r0, r4
   e5bfc:	4629      	mov	r1, r5
   e5bfe:	f001 f887 	bl	e6d10 <__adddf3>
   e5c02:	ec41 0b10 	vmov	d0, r0, r1
   e5c06:	e6fe      	b.n	e5a06 <__ieee754_exp+0x2e>
   e5c08:	a31d      	add	r3, pc, #116	; (adr r3, e5c80 <__ieee754_exp+0x2a8>)
   e5c0a:	e9d3 2300 	ldrd	r2, r3, [r3]
   e5c0e:	4610      	mov	r0, r2
   e5c10:	4619      	mov	r1, r3
   e5c12:	f001 fa2f 	bl	e7074 <__aeabi_dmul>
   e5c16:	ec41 0b10 	vmov	d0, r0, r1
   e5c1a:	e6f4      	b.n	e5a06 <__ieee754_exp+0x2e>
   e5c1c:	4602      	mov	r2, r0
   e5c1e:	460b      	mov	r3, r1
   e5c20:	4620      	mov	r0, r4
   e5c22:	4629      	mov	r1, r5
   e5c24:	f001 fa26 	bl	e7074 <__aeabi_dmul>
   e5c28:	2200      	movs	r2, #0
   e5c2a:	4606      	mov	r6, r0
   e5c2c:	460f      	mov	r7, r1
   e5c2e:	f04f 4380 	mov.w	r3, #1073741824	; 0x40000000
   e5c32:	4640      	mov	r0, r8
   e5c34:	4649      	mov	r1, r9
   e5c36:	f001 f869 	bl	e6d0c <__aeabi_dsub>
   e5c3a:	4602      	mov	r2, r0
   e5c3c:	460b      	mov	r3, r1
   e5c3e:	4630      	mov	r0, r6
   e5c40:	4639      	mov	r1, r7
   e5c42:	f001 fb41 	bl	e72c8 <__aeabi_ddiv>
   e5c46:	4622      	mov	r2, r4
   e5c48:	462b      	mov	r3, r5
   e5c4a:	f001 f85f 	bl	e6d0c <__aeabi_dsub>
   e5c4e:	4602      	mov	r2, r0
   e5c50:	460b      	mov	r3, r1
   e5c52:	2000      	movs	r0, #0
   e5c54:	4924      	ldr	r1, [pc, #144]	; (e5ce8 <__ieee754_exp+0x310>)
   e5c56:	f001 f859 	bl	e6d0c <__aeabi_dsub>
   e5c5a:	ec41 0b10 	vmov	d0, r0, r1
   e5c5e:	b003      	add	sp, #12
   e5c60:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e5c64:	eb01 5106 	add.w	r1, r1, r6, lsl #20
   e5c68:	ec41 0b10 	vmov	d0, r0, r1
   e5c6c:	b003      	add	sp, #12
   e5c6e:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e5c72:	4606      	mov	r6, r0
   e5c74:	e731      	b.n	e5ada <__ieee754_exp+0x102>
   e5c76:	bf00      	nop
	...
   e5c80:	8800759c 	.word	0x8800759c
   e5c84:	7e37e43c 	.word	0x7e37e43c
   e5c88:	fefa39ef 	.word	0xfefa39ef
   e5c8c:	40862e42 	.word	0x40862e42
   e5c90:	d52d3051 	.word	0xd52d3051
   e5c94:	c0874910 	.word	0xc0874910
   e5c98:	652b82fe 	.word	0x652b82fe
   e5c9c:	3ff71547 	.word	0x3ff71547
   e5ca0:	fee00000 	.word	0xfee00000
   e5ca4:	3fe62e42 	.word	0x3fe62e42
   e5ca8:	35793c76 	.word	0x35793c76
   e5cac:	3dea39ef 	.word	0x3dea39ef
   e5cb0:	72bea4d0 	.word	0x72bea4d0
   e5cb4:	3e663769 	.word	0x3e663769
   e5cb8:	c5d26bf1 	.word	0xc5d26bf1
   e5cbc:	3ebbbd41 	.word	0x3ebbbd41
   e5cc0:	af25de2c 	.word	0xaf25de2c
   e5cc4:	3f11566a 	.word	0x3f11566a
   e5cc8:	16bebd93 	.word	0x16bebd93
   e5ccc:	3f66c16c 	.word	0x3f66c16c
   e5cd0:	5555553e 	.word	0x5555553e
   e5cd4:	3fc55555 	.word	0x3fc55555
   e5cd8:	40862e41 	.word	0x40862e41
   e5cdc:	7fefffff 	.word	0x7fefffff
   e5ce0:	3fd62e42 	.word	0x3fd62e42
   e5ce4:	3e2fffff 	.word	0x3e2fffff
   e5ce8:	3ff00000 	.word	0x3ff00000
   e5cec:	000eb500 	.word	0x000eb500
   e5cf0:	3ff0a2b1 	.word	0x3ff0a2b1
   e5cf4:	000eb520 	.word	0x000eb520
   e5cf8:	000eb510 	.word	0x000eb510

000e5cfc <__ieee754_expf>:
   e5cfc:	ee10 3a10 	vmov	r3, s0
   e5d00:	f023 4200 	bic.w	r2, r3, #2147483648	; 0x80000000
   e5d04:	f1b2 4fff 	cmp.w	r2, #2139095040	; 0x7f800000
   e5d08:	d856      	bhi.n	e5db8 <__ieee754_expf+0xbc>
   e5d0a:	ea4f 71d3 	mov.w	r1, r3, lsr #31
   e5d0e:	d056      	beq.n	e5dbe <__ieee754_expf+0xc2>
   e5d10:	4854      	ldr	r0, [pc, #336]	; (e5e64 <__ieee754_expf+0x168>)
   e5d12:	4283      	cmp	r3, r0
   e5d14:	dc73      	bgt.n	e5dfe <__ieee754_expf+0x102>
   e5d16:	2b00      	cmp	r3, #0
   e5d18:	db69      	blt.n	e5dee <__ieee754_expf+0xf2>
   e5d1a:	4b53      	ldr	r3, [pc, #332]	; (e5e68 <__ieee754_expf+0x16c>)
   e5d1c:	429a      	cmp	r2, r3
   e5d1e:	d955      	bls.n	e5dcc <__ieee754_expf+0xd0>
   e5d20:	4b52      	ldr	r3, [pc, #328]	; (e5e6c <__ieee754_expf+0x170>)
   e5d22:	429a      	cmp	r2, r3
   e5d24:	d87d      	bhi.n	e5e22 <__ieee754_expf+0x126>
   e5d26:	4852      	ldr	r0, [pc, #328]	; (e5e70 <__ieee754_expf+0x174>)
   e5d28:	4a52      	ldr	r2, [pc, #328]	; (e5e74 <__ieee754_expf+0x178>)
   e5d2a:	008b      	lsls	r3, r1, #2
   e5d2c:	4418      	add	r0, r3
   e5d2e:	ed90 7a00 	vldr	s14, [r0]
   e5d32:	441a      	add	r2, r3
   e5d34:	ee70 4a47 	vsub.f32	s9, s0, s14
   e5d38:	f1c1 0301 	rsb	r3, r1, #1
   e5d3c:	ed92 7a00 	vldr	s14, [r2]
   e5d40:	1a5b      	subs	r3, r3, r1
   e5d42:	ee34 0ac7 	vsub.f32	s0, s9, s14
   e5d46:	ee60 7a00 	vmul.f32	s15, s0, s0
   e5d4a:	ed9f 4a4b 	vldr	s8, [pc, #300]	; e5e78 <__ieee754_expf+0x17c>
   e5d4e:	ed9f 5a4b 	vldr	s10, [pc, #300]	; e5e7c <__ieee754_expf+0x180>
   e5d52:	eddf 5a4b 	vldr	s11, [pc, #300]	; e5e80 <__ieee754_expf+0x184>
   e5d56:	ed9f 6a4b 	vldr	s12, [pc, #300]	; e5e84 <__ieee754_expf+0x188>
   e5d5a:	eddf 6a4b 	vldr	s13, [pc, #300]	; e5e88 <__ieee754_expf+0x18c>
   e5d5e:	eea7 5a84 	vfma.f32	s10, s15, s8
   e5d62:	eee7 5a85 	vfma.f32	s11, s15, s10
   e5d66:	eea7 6aa5 	vfma.f32	s12, s15, s11
   e5d6a:	eee7 6a86 	vfma.f32	s13, s15, s12
   e5d6e:	eeb0 6a40 	vmov.f32	s12, s0
   e5d72:	eea7 6ae6 	vfms.f32	s12, s15, s13
   e5d76:	eef0 6a00 	vmov.f32	s13, #0	; 0x40000000  2.0
   e5d7a:	2b00      	cmp	r3, #0
   e5d7c:	d044      	beq.n	e5e08 <__ieee754_expf+0x10c>
   e5d7e:	ee20 0a06 	vmul.f32	s0, s0, s12
   e5d82:	ee76 7ac6 	vsub.f32	s15, s13, s12
   e5d86:	f113 0f7d 	cmn.w	r3, #125	; 0x7d
   e5d8a:	ee80 6a27 	vdiv.f32	s12, s0, s15
   e5d8e:	eef7 6a00 	vmov.f32	s13, #112	; 0x3f800000  1.0
   e5d92:	ee37 7a46 	vsub.f32	s14, s14, s12
   e5d96:	ee37 7a64 	vsub.f32	s14, s14, s9
   e5d9a:	ee36 0ac7 	vsub.f32	s0, s13, s14
   e5d9e:	da5a      	bge.n	e5e56 <__ieee754_expf+0x15a>
   e5da0:	ee10 2a10 	vmov	r2, s0
   e5da4:	3364      	adds	r3, #100	; 0x64
   e5da6:	eb02 53c3 	add.w	r3, r2, r3, lsl #23
   e5daa:	eddf 7a38 	vldr	s15, [pc, #224]	; e5e8c <__ieee754_expf+0x190>
   e5dae:	ee00 3a10 	vmov	s0, r3
   e5db2:	ee20 0a27 	vmul.f32	s0, s0, s15
   e5db6:	4770      	bx	lr
   e5db8:	ee30 0a00 	vadd.f32	s0, s0, s0
   e5dbc:	4770      	bx	lr
   e5dbe:	eddf 7a34 	vldr	s15, [pc, #208]	; e5e90 <__ieee754_expf+0x194>
   e5dc2:	2900      	cmp	r1, #0
   e5dc4:	bf18      	it	ne
   e5dc6:	eeb0 0a67 	vmovne.f32	s0, s15
   e5dca:	4770      	bx	lr
   e5dcc:	f1b2 5f46 	cmp.w	r2, #830472192	; 0x31800000
   e5dd0:	d213      	bcs.n	e5dfa <__ieee754_expf+0xfe>
   e5dd2:	eddf 7a30 	vldr	s15, [pc, #192]	; e5e94 <__ieee754_expf+0x198>
   e5dd6:	ee70 7a27 	vadd.f32	s15, s0, s15
   e5dda:	eef7 6a00 	vmov.f32	s13, #112	; 0x3f800000  1.0
   e5dde:	eef4 7ae6 	vcmpe.f32	s15, s13
   e5de2:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5de6:	dd08      	ble.n	e5dfa <__ieee754_expf+0xfe>
   e5de8:	ee30 0a26 	vadd.f32	s0, s0, s13
   e5dec:	4770      	bx	lr
   e5dee:	4b2a      	ldr	r3, [pc, #168]	; (e5e98 <__ieee754_expf+0x19c>)
   e5df0:	429a      	cmp	r2, r3
   e5df2:	d992      	bls.n	e5d1a <__ieee754_expf+0x1e>
   e5df4:	ed9f 0a26 	vldr	s0, [pc, #152]	; e5e90 <__ieee754_expf+0x194>
   e5df8:	4770      	bx	lr
   e5dfa:	2300      	movs	r3, #0
   e5dfc:	e7a3      	b.n	e5d46 <__ieee754_expf+0x4a>
   e5dfe:	ed9f 0a25 	vldr	s0, [pc, #148]	; e5e94 <__ieee754_expf+0x198>
   e5e02:	ee20 0a00 	vmul.f32	s0, s0, s0
   e5e06:	4770      	bx	lr
   e5e08:	ee60 7a06 	vmul.f32	s15, s0, s12
   e5e0c:	ee76 6a66 	vsub.f32	s13, s12, s13
   e5e10:	eeb7 6a00 	vmov.f32	s12, #112	; 0x3f800000  1.0
   e5e14:	ee87 7aa6 	vdiv.f32	s14, s15, s13
   e5e18:	ee37 0a40 	vsub.f32	s0, s14, s0
   e5e1c:	ee36 0a40 	vsub.f32	s0, s12, s0
   e5e20:	4770      	bx	lr
   e5e22:	4b1e      	ldr	r3, [pc, #120]	; (e5e9c <__ieee754_expf+0x1a0>)
   e5e24:	ed9f 6a1e 	vldr	s12, [pc, #120]	; e5ea0 <__ieee754_expf+0x1a4>
   e5e28:	eddf 6a1e 	vldr	s13, [pc, #120]	; e5ea4 <__ieee754_expf+0x1a8>
   e5e2c:	ed9f 7a1e 	vldr	s14, [pc, #120]	; e5ea8 <__ieee754_expf+0x1ac>
   e5e30:	eb03 0381 	add.w	r3, r3, r1, lsl #2
   e5e34:	edd3 7a00 	vldr	s15, [r3]
   e5e38:	eee0 7a06 	vfma.f32	s15, s0, s12
   e5e3c:	eef0 4a40 	vmov.f32	s9, s0
   e5e40:	eefd 7ae7 	vcvt.s32.f32	s15, s15
   e5e44:	ee17 3a90 	vmov	r3, s15
   e5e48:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   e5e4c:	eee7 4ae6 	vfms.f32	s9, s15, s13
   e5e50:	ee27 7a87 	vmul.f32	s14, s15, s14
   e5e54:	e775      	b.n	e5d42 <__ieee754_expf+0x46>
   e5e56:	ee10 2a10 	vmov	r2, s0
   e5e5a:	eb02 53c3 	add.w	r3, r2, r3, lsl #23
   e5e5e:	ee00 3a10 	vmov	s0, r3
   e5e62:	4770      	bx	lr
   e5e64:	42b17217 	.word	0x42b17217
   e5e68:	3eb17218 	.word	0x3eb17218
   e5e6c:	3f851591 	.word	0x3f851591
   e5e70:	000eb540 	.word	0x000eb540
   e5e74:	000eb538 	.word	0x000eb538
   e5e78:	3331bb4c 	.word	0x3331bb4c
   e5e7c:	b5ddea0e 	.word	0xb5ddea0e
   e5e80:	388ab355 	.word	0x388ab355
   e5e84:	bb360b61 	.word	0xbb360b61
   e5e88:	3e2aaaab 	.word	0x3e2aaaab
   e5e8c:	0d800000 	.word	0x0d800000
   e5e90:	00000000 	.word	0x00000000
   e5e94:	7149f2ca 	.word	0x7149f2ca
   e5e98:	42cff1b5 	.word	0x42cff1b5
   e5e9c:	000eb530 	.word	0x000eb530
   e5ea0:	3fb8aa3b 	.word	0x3fb8aa3b
   e5ea4:	3f317180 	.word	0x3f317180
   e5ea8:	3717f7d1 	.word	0x3717f7d1

000e5eac <__ieee754_logf>:
   e5eac:	b430      	push	{r4, r5}
   e5eae:	b082      	sub	sp, #8
   e5eb0:	ed8d 0a01 	vstr	s0, [sp, #4]
   e5eb4:	9b01      	ldr	r3, [sp, #4]
   e5eb6:	f023 4200 	bic.w	r2, r3, #2147483648	; 0x80000000
   e5eba:	b372      	cbz	r2, e5f1a <__ieee754_logf+0x6e>
   e5ebc:	2b00      	cmp	r3, #0
   e5ebe:	db40      	blt.n	e5f42 <__ieee754_logf+0x96>
   e5ec0:	f1b3 4fff 	cmp.w	r3, #2139095040	; 0x7f800000
   e5ec4:	da48      	bge.n	e5f58 <__ieee754_logf+0xac>
   e5ec6:	f5b3 0f00 	cmp.w	r3, #8388608	; 0x800000
   e5eca:	db2f      	blt.n	e5f2c <__ieee754_logf+0x80>
   e5ecc:	2200      	movs	r2, #0
   e5ece:	496e      	ldr	r1, [pc, #440]	; (e6088 <__ieee754_logf+0x1dc>)
   e5ed0:	f3c3 0516 	ubfx	r5, r3, #0, #23
   e5ed4:	4429      	add	r1, r5
   e5ed6:	f401 0100 	and.w	r1, r1, #8388608	; 0x800000
   e5eda:	15db      	asrs	r3, r3, #23
   e5edc:	3b7f      	subs	r3, #127	; 0x7f
   e5ede:	f081 547e 	eor.w	r4, r1, #1065353216	; 0x3f800000
   e5ee2:	4413      	add	r3, r2
   e5ee4:	f105 000f 	add.w	r0, r5, #15
   e5ee8:	ea44 0205 	orr.w	r2, r4, r5
   e5eec:	ee00 2a10 	vmov	s0, r2
   e5ef0:	f3c0 0216 	ubfx	r2, r0, #0, #23
   e5ef4:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   e5ef8:	2a0f      	cmp	r2, #15
   e5efa:	eb03 53d1 	add.w	r3, r3, r1, lsr #23
   e5efe:	ee70 7a67 	vsub.f32	s15, s0, s15
   e5f02:	dc30      	bgt.n	e5f66 <__ieee754_logf+0xba>
   e5f04:	eef5 7a40 	vcmp.f32	s15, #0.0
   e5f08:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e5f0c:	d16c      	bne.n	e5fe8 <__ieee754_logf+0x13c>
   e5f0e:	2b00      	cmp	r3, #0
   e5f10:	f040 8096 	bne.w	e6040 <__ieee754_logf+0x194>
   e5f14:	ed9f 0a5d 	vldr	s0, [pc, #372]	; e608c <__ieee754_logf+0x1e0>
   e5f18:	e005      	b.n	e5f26 <__ieee754_logf+0x7a>
   e5f1a:	ed9f 7a5d 	vldr	s14, [pc, #372]	; e6090 <__ieee754_logf+0x1e4>
   e5f1e:	eddf 7a5b 	vldr	s15, [pc, #364]	; e608c <__ieee754_logf+0x1e0>
   e5f22:	ee87 0a27 	vdiv.f32	s0, s14, s15
   e5f26:	b002      	add	sp, #8
   e5f28:	bc30      	pop	{r4, r5}
   e5f2a:	4770      	bx	lr
   e5f2c:	eddf 7a59 	vldr	s15, [pc, #356]	; e6094 <__ieee754_logf+0x1e8>
   e5f30:	ed9d 7a01 	vldr	s14, [sp, #4]
   e5f34:	ee67 7a27 	vmul.f32	s15, s14, s15
   e5f38:	f06f 0218 	mvn.w	r2, #24
   e5f3c:	ee17 3a90 	vmov	r3, s15
   e5f40:	e7c5      	b.n	e5ece <__ieee754_logf+0x22>
   e5f42:	eddd 7a01 	vldr	s15, [sp, #4]
   e5f46:	ee37 7ae7 	vsub.f32	s14, s15, s15
   e5f4a:	eddf 7a50 	vldr	s15, [pc, #320]	; e608c <__ieee754_logf+0x1e0>
   e5f4e:	ee87 0a27 	vdiv.f32	s0, s14, s15
   e5f52:	b002      	add	sp, #8
   e5f54:	bc30      	pop	{r4, r5}
   e5f56:	4770      	bx	lr
   e5f58:	eddd 7a01 	vldr	s15, [sp, #4]
   e5f5c:	ee37 0aa7 	vadd.f32	s0, s15, s15
   e5f60:	b002      	add	sp, #8
   e5f62:	bc30      	pop	{r4, r5}
   e5f64:	4770      	bx	lr
   e5f66:	eef0 6a00 	vmov.f32	s13, #0	; 0x40000000  2.0
   e5f6a:	ee77 6aa6 	vadd.f32	s13, s15, s13
   e5f6e:	ed9f 2a4a 	vldr	s4, [pc, #296]	; e6098 <__ieee754_logf+0x1ec>
   e5f72:	ed9f 4a4a 	vldr	s8, [pc, #296]	; e609c <__ieee754_logf+0x1f0>
   e5f76:	ed9f 5a4a 	vldr	s10, [pc, #296]	; e60a0 <__ieee754_logf+0x1f4>
   e5f7a:	eddf 2a4a 	vldr	s5, [pc, #296]	; e60a4 <__ieee754_logf+0x1f8>
   e5f7e:	eddf 4a4a 	vldr	s9, [pc, #296]	; e60a8 <__ieee754_logf+0x1fc>
   e5f82:	ed9f 7a4a 	vldr	s14, [pc, #296]	; e60ac <__ieee754_logf+0x200>
   e5f86:	ed9f 6a4a 	vldr	s12, [pc, #296]	; e60b0 <__ieee754_logf+0x204>
   e5f8a:	4a4a      	ldr	r2, [pc, #296]	; (e60b4 <__ieee754_logf+0x208>)
   e5f8c:	eec7 3aa6 	vdiv.f32	s7, s15, s13
   e5f90:	f5c5 1157 	rsb	r1, r5, #3522560	; 0x35c000
   e5f94:	442a      	add	r2, r5
   e5f96:	f501 7122 	add.w	r1, r1, #648	; 0x288
   e5f9a:	430a      	orrs	r2, r1
   e5f9c:	2a00      	cmp	r2, #0
   e5f9e:	ee06 3a90 	vmov	s13, r3
   e5fa2:	ee63 5aa3 	vmul.f32	s11, s7, s7
   e5fa6:	eeb8 3ae6 	vcvt.f32.s32	s6, s13
   e5faa:	ee65 6aa5 	vmul.f32	s13, s11, s11
   e5fae:	eea6 4a82 	vfma.f32	s8, s13, s4
   e5fb2:	eee6 4aa2 	vfma.f32	s9, s13, s5
   e5fb6:	eea6 5a84 	vfma.f32	s10, s13, s8
   e5fba:	eea6 6aa4 	vfma.f32	s12, s13, s9
   e5fbe:	eea6 7a85 	vfma.f32	s14, s13, s10
   e5fc2:	ee27 7a25 	vmul.f32	s14, s14, s11
   e5fc6:	eea6 7a86 	vfma.f32	s14, s13, s12
   e5fca:	dd46      	ble.n	e605a <__ieee754_logf+0x1ae>
   e5fcc:	eeb6 0a00 	vmov.f32	s0, #96	; 0x3f000000  0.5
   e5fd0:	ee27 0a80 	vmul.f32	s0, s15, s0
   e5fd4:	ee20 0a27 	vmul.f32	s0, s0, s15
   e5fd8:	bb0b      	cbnz	r3, e601e <__ieee754_logf+0x172>
   e5fda:	ee37 7a00 	vadd.f32	s14, s14, s0
   e5fde:	eea3 0ac7 	vfms.f32	s0, s7, s14
   e5fe2:	ee37 0ac0 	vsub.f32	s0, s15, s0
   e5fe6:	e79e      	b.n	e5f26 <__ieee754_logf+0x7a>
   e5fe8:	ed9f 7a33 	vldr	s14, [pc, #204]	; e60b8 <__ieee754_logf+0x20c>
   e5fec:	eeb6 0a00 	vmov.f32	s0, #96	; 0x3f000000  0.5
   e5ff0:	eea7 0ac7 	vfms.f32	s0, s15, s14
   e5ff4:	ee27 7aa7 	vmul.f32	s14, s15, s15
   e5ff8:	ee20 0a07 	vmul.f32	s0, s0, s14
   e5ffc:	2b00      	cmp	r3, #0
   e5ffe:	d0f0      	beq.n	e5fe2 <__ieee754_logf+0x136>
   e6000:	ee07 3a10 	vmov	s14, r3
   e6004:	ed9f 6a2d 	vldr	s12, [pc, #180]	; e60bc <__ieee754_logf+0x210>
   e6008:	eddf 6a2d 	vldr	s13, [pc, #180]	; e60c0 <__ieee754_logf+0x214>
   e600c:	eeb8 7ac7 	vcvt.f32.s32	s14, s14
   e6010:	eea7 0a46 	vfms.f32	s0, s14, s12
   e6014:	ee30 0a67 	vsub.f32	s0, s0, s15
   e6018:	ee97 0a26 	vfnms.f32	s0, s14, s13
   e601c:	e783      	b.n	e5f26 <__ieee754_logf+0x7a>
   e601e:	eddf 6a27 	vldr	s13, [pc, #156]	; e60bc <__ieee754_logf+0x210>
   e6022:	ed9f 6a27 	vldr	s12, [pc, #156]	; e60c0 <__ieee754_logf+0x214>
   e6026:	ee37 7a00 	vadd.f32	s14, s14, s0
   e602a:	ee63 6a26 	vmul.f32	s13, s6, s13
   e602e:	eee3 6a87 	vfma.f32	s13, s7, s14
   e6032:	ee30 0a66 	vsub.f32	s0, s0, s13
   e6036:	ee30 0a67 	vsub.f32	s0, s0, s15
   e603a:	ee93 0a06 	vfnms.f32	s0, s6, s12
   e603e:	e772      	b.n	e5f26 <__ieee754_logf+0x7a>
   e6040:	ee07 3a90 	vmov	s15, r3
   e6044:	ed9f 0a1d 	vldr	s0, [pc, #116]	; e60bc <__ieee754_logf+0x210>
   e6048:	ed9f 7a1d 	vldr	s14, [pc, #116]	; e60c0 <__ieee754_logf+0x214>
   e604c:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   e6050:	ee27 0a80 	vmul.f32	s0, s15, s0
   e6054:	eea7 0a87 	vfma.f32	s0, s15, s14
   e6058:	e765      	b.n	e5f26 <__ieee754_logf+0x7a>
   e605a:	b173      	cbz	r3, e607a <__ieee754_logf+0x1ce>
   e605c:	ed9f 0a17 	vldr	s0, [pc, #92]	; e60bc <__ieee754_logf+0x210>
   e6060:	eddf 6a17 	vldr	s13, [pc, #92]	; e60c0 <__ieee754_logf+0x214>
   e6064:	ee37 7ac7 	vsub.f32	s14, s15, s14
   e6068:	ee20 0a43 	vnmul.f32	s0, s0, s6
   e606c:	eea3 0a87 	vfma.f32	s0, s7, s14
   e6070:	ee30 0a67 	vsub.f32	s0, s0, s15
   e6074:	ee93 0a26 	vfnms.f32	s0, s6, s13
   e6078:	e755      	b.n	e5f26 <__ieee754_logf+0x7a>
   e607a:	ee37 7ac7 	vsub.f32	s14, s15, s14
   e607e:	eee3 7ac7 	vfms.f32	s15, s7, s14
   e6082:	eeb0 0a67 	vmov.f32	s0, s15
   e6086:	e74e      	b.n	e5f26 <__ieee754_logf+0x7a>
   e6088:	004afb20 	.word	0x004afb20
   e608c:	00000000 	.word	0x00000000
   e6090:	cc000000 	.word	0xcc000000
   e6094:	4c000000 	.word	0x4c000000
   e6098:	3e178897 	.word	0x3e178897
   e609c:	3e3a3325 	.word	0x3e3a3325
   e60a0:	3e924925 	.word	0x3e924925
   e60a4:	3e1cd04f 	.word	0x3e1cd04f
   e60a8:	3e638e29 	.word	0x3e638e29
   e60ac:	3f2aaaab 	.word	0x3f2aaaab
   e60b0:	3ecccccd 	.word	0x3ecccccd
   e60b4:	ffcf5c30 	.word	0xffcf5c30
   e60b8:	3eaaaaab 	.word	0x3eaaaaab
   e60bc:	3717f7d1 	.word	0x3717f7d1
   e60c0:	3f317180 	.word	0x3f317180

000e60c4 <__ieee754_rem_pio2f>:
   e60c4:	b570      	push	{r4, r5, r6, lr}
   e60c6:	ee10 3a10 	vmov	r3, s0
   e60ca:	4a96      	ldr	r2, [pc, #600]	; (e6324 <__ieee754_rem_pio2f+0x260>)
   e60cc:	f023 4400 	bic.w	r4, r3, #2147483648	; 0x80000000
   e60d0:	4294      	cmp	r4, r2
   e60d2:	b086      	sub	sp, #24
   e60d4:	dd5f      	ble.n	e6196 <__ieee754_rem_pio2f+0xd2>
   e60d6:	4a94      	ldr	r2, [pc, #592]	; (e6328 <__ieee754_rem_pio2f+0x264>)
   e60d8:	4294      	cmp	r4, r2
   e60da:	ee10 6a10 	vmov	r6, s0
   e60de:	dc1b      	bgt.n	e6118 <__ieee754_rem_pio2f+0x54>
   e60e0:	2b00      	cmp	r3, #0
   e60e2:	eddf 7a92 	vldr	s15, [pc, #584]	; e632c <__ieee754_rem_pio2f+0x268>
   e60e6:	4a92      	ldr	r2, [pc, #584]	; (e6330 <__ieee754_rem_pio2f+0x26c>)
   e60e8:	f024 040f 	bic.w	r4, r4, #15
   e60ec:	f340 80d5 	ble.w	e629a <__ieee754_rem_pio2f+0x1d6>
   e60f0:	4294      	cmp	r4, r2
   e60f2:	ee70 7a67 	vsub.f32	s15, s0, s15
   e60f6:	d05e      	beq.n	e61b6 <__ieee754_rem_pio2f+0xf2>
   e60f8:	ed9f 7a8e 	vldr	s14, [pc, #568]	; e6334 <__ieee754_rem_pio2f+0x270>
   e60fc:	ee77 6ac7 	vsub.f32	s13, s15, s14
   e6100:	2301      	movs	r3, #1
   e6102:	ee77 7ae6 	vsub.f32	s15, s15, s13
   e6106:	edc0 6a00 	vstr	s13, [r0]
   e610a:	ee77 7ac7 	vsub.f32	s15, s15, s14
   e610e:	edc0 7a01 	vstr	s15, [r0, #4]
   e6112:	4618      	mov	r0, r3
   e6114:	b006      	add	sp, #24
   e6116:	bd70      	pop	{r4, r5, r6, pc}
   e6118:	4a87      	ldr	r2, [pc, #540]	; (e6338 <__ieee754_rem_pio2f+0x274>)
   e611a:	4294      	cmp	r4, r2
   e611c:	4605      	mov	r5, r0
   e611e:	dd5c      	ble.n	e61da <__ieee754_rem_pio2f+0x116>
   e6120:	f1b4 4fff 	cmp.w	r4, #2139095040	; 0x7f800000
   e6124:	da3f      	bge.n	e61a6 <__ieee754_rem_pio2f+0xe2>
   e6126:	15e2      	asrs	r2, r4, #23
   e6128:	3a86      	subs	r2, #134	; 0x86
   e612a:	eba4 53c2 	sub.w	r3, r4, r2, lsl #23
   e612e:	ee07 3a10 	vmov	s14, r3
   e6132:	eefd 6ac7 	vcvt.s32.f32	s13, s14
   e6136:	eddf 7a81 	vldr	s15, [pc, #516]	; e633c <__ieee754_rem_pio2f+0x278>
   e613a:	eef8 6ae6 	vcvt.f32.s32	s13, s13
   e613e:	ee37 7a66 	vsub.f32	s14, s14, s13
   e6142:	edcd 6a03 	vstr	s13, [sp, #12]
   e6146:	ee27 7a27 	vmul.f32	s14, s14, s15
   e614a:	eefd 6ac7 	vcvt.s32.f32	s13, s14
   e614e:	eef8 6ae6 	vcvt.f32.s32	s13, s13
   e6152:	ee37 7a66 	vsub.f32	s14, s14, s13
   e6156:	edcd 6a04 	vstr	s13, [sp, #16]
   e615a:	ee67 7a27 	vmul.f32	s15, s14, s15
   e615e:	eef5 7a40 	vcmp.f32	s15, #0.0
   e6162:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e6166:	edcd 7a05 	vstr	s15, [sp, #20]
   e616a:	f040 80b7 	bne.w	e62dc <__ieee754_rem_pio2f+0x218>
   e616e:	eef5 6a40 	vcmp.f32	s13, #0.0
   e6172:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e6176:	bf0c      	ite	eq
   e6178:	2301      	moveq	r3, #1
   e617a:	2302      	movne	r3, #2
   e617c:	4970      	ldr	r1, [pc, #448]	; (e6340 <__ieee754_rem_pio2f+0x27c>)
   e617e:	9101      	str	r1, [sp, #4]
   e6180:	2102      	movs	r1, #2
   e6182:	9100      	str	r1, [sp, #0]
   e6184:	a803      	add	r0, sp, #12
   e6186:	4629      	mov	r1, r5
   e6188:	f000 f9bc 	bl	e6504 <__kernel_rem_pio2f>
   e618c:	2e00      	cmp	r6, #0
   e618e:	f2c0 8097 	blt.w	e62c0 <__ieee754_rem_pio2f+0x1fc>
   e6192:	4603      	mov	r3, r0
   e6194:	e004      	b.n	e61a0 <__ieee754_rem_pio2f+0xdc>
   e6196:	2200      	movs	r2, #0
   e6198:	ed80 0a00 	vstr	s0, [r0]
   e619c:	6042      	str	r2, [r0, #4]
   e619e:	2300      	movs	r3, #0
   e61a0:	4618      	mov	r0, r3
   e61a2:	b006      	add	sp, #24
   e61a4:	bd70      	pop	{r4, r5, r6, pc}
   e61a6:	ee70 7a40 	vsub.f32	s15, s0, s0
   e61aa:	2300      	movs	r3, #0
   e61ac:	edc0 7a01 	vstr	s15, [r0, #4]
   e61b0:	edc0 7a00 	vstr	s15, [r0]
   e61b4:	e7f4      	b.n	e61a0 <__ieee754_rem_pio2f+0xdc>
   e61b6:	eddf 6a63 	vldr	s13, [pc, #396]	; e6344 <__ieee754_rem_pio2f+0x280>
   e61ba:	ed9f 7a63 	vldr	s14, [pc, #396]	; e6348 <__ieee754_rem_pio2f+0x284>
   e61be:	ee77 7ae6 	vsub.f32	s15, s15, s13
   e61c2:	2301      	movs	r3, #1
   e61c4:	ee77 6ac7 	vsub.f32	s13, s15, s14
   e61c8:	ee77 7ae6 	vsub.f32	s15, s15, s13
   e61cc:	edc0 6a00 	vstr	s13, [r0]
   e61d0:	ee77 7ac7 	vsub.f32	s15, s15, s14
   e61d4:	edc0 7a01 	vstr	s15, [r0, #4]
   e61d8:	e7e2      	b.n	e61a0 <__ieee754_rem_pio2f+0xdc>
   e61da:	f000 fd01 	bl	e6be0 <fabsf>
   e61de:	eddf 6a5b 	vldr	s13, [pc, #364]	; e634c <__ieee754_rem_pio2f+0x288>
   e61e2:	eddf 5a52 	vldr	s11, [pc, #328]	; e632c <__ieee754_rem_pio2f+0x268>
   e61e6:	ed9f 7a53 	vldr	s14, [pc, #332]	; e6334 <__ieee754_rem_pio2f+0x270>
   e61ea:	eef6 7a00 	vmov.f32	s15, #96	; 0x3f000000  0.5
   e61ee:	eee0 7a26 	vfma.f32	s15, s0, s13
   e61f2:	eefd 7ae7 	vcvt.s32.f32	s15, s15
   e61f6:	ee17 3a90 	vmov	r3, s15
   e61fa:	eef8 6ae7 	vcvt.f32.s32	s13, s15
   e61fe:	2b1f      	cmp	r3, #31
   e6200:	eeb1 6a66 	vneg.f32	s12, s13
   e6204:	eea6 0a25 	vfma.f32	s0, s12, s11
   e6208:	ee66 7a87 	vmul.f32	s15, s13, s14
   e620c:	dc1d      	bgt.n	e624a <__ieee754_rem_pio2f+0x186>
   e620e:	4950      	ldr	r1, [pc, #320]	; (e6350 <__ieee754_rem_pio2f+0x28c>)
   e6210:	1e58      	subs	r0, r3, #1
   e6212:	f024 02ff 	bic.w	r2, r4, #255	; 0xff
   e6216:	f851 1020 	ldr.w	r1, [r1, r0, lsl #2]
   e621a:	428a      	cmp	r2, r1
   e621c:	d015      	beq.n	e624a <__ieee754_rem_pio2f+0x186>
   e621e:	ee30 7a67 	vsub.f32	s14, s0, s15
   e6222:	ed85 7a00 	vstr	s14, [r5]
   e6226:	ee30 0a47 	vsub.f32	s0, s0, s14
   e622a:	2e00      	cmp	r6, #0
   e622c:	ee30 0a67 	vsub.f32	s0, s0, s15
   e6230:	ed85 0a01 	vstr	s0, [r5, #4]
   e6234:	dab4      	bge.n	e61a0 <__ieee754_rem_pio2f+0xdc>
   e6236:	eeb1 7a47 	vneg.f32	s14, s14
   e623a:	eeb1 0a40 	vneg.f32	s0, s0
   e623e:	ed85 7a00 	vstr	s14, [r5]
   e6242:	ed85 0a01 	vstr	s0, [r5, #4]
   e6246:	425b      	negs	r3, r3
   e6248:	e7aa      	b.n	e61a0 <__ieee754_rem_pio2f+0xdc>
   e624a:	ee30 7a67 	vsub.f32	s14, s0, s15
   e624e:	15e4      	asrs	r4, r4, #23
   e6250:	ee17 2a10 	vmov	r2, s14
   e6254:	f3c2 52c7 	ubfx	r2, r2, #23, #8
   e6258:	1aa2      	subs	r2, r4, r2
   e625a:	2a08      	cmp	r2, #8
   e625c:	dde1      	ble.n	e6222 <__ieee754_rem_pio2f+0x15e>
   e625e:	eddf 7a39 	vldr	s15, [pc, #228]	; e6344 <__ieee754_rem_pio2f+0x280>
   e6262:	ed9f 7a39 	vldr	s14, [pc, #228]	; e6348 <__ieee754_rem_pio2f+0x284>
   e6266:	eef0 5a40 	vmov.f32	s11, s0
   e626a:	eee6 5a27 	vfma.f32	s11, s12, s15
   e626e:	ee30 0a65 	vsub.f32	s0, s0, s11
   e6272:	eea6 0a27 	vfma.f32	s0, s12, s15
   e6276:	eef0 7a40 	vmov.f32	s15, s0
   e627a:	eed6 7a87 	vfnms.f32	s15, s13, s14
   e627e:	ee35 7ae7 	vsub.f32	s14, s11, s15
   e6282:	ee17 2a10 	vmov	r2, s14
   e6286:	f3c2 52c7 	ubfx	r2, r2, #23, #8
   e628a:	1aa4      	subs	r4, r4, r2
   e628c:	2c19      	cmp	r4, #25
   e628e:	dc3a      	bgt.n	e6306 <__ieee754_rem_pio2f+0x242>
   e6290:	ed85 7a00 	vstr	s14, [r5]
   e6294:	eeb0 0a65 	vmov.f32	s0, s11
   e6298:	e7c5      	b.n	e6226 <__ieee754_rem_pio2f+0x162>
   e629a:	4294      	cmp	r4, r2
   e629c:	ee70 7a27 	vadd.f32	s15, s0, s15
   e62a0:	d01e      	beq.n	e62e0 <__ieee754_rem_pio2f+0x21c>
   e62a2:	ed9f 7a24 	vldr	s14, [pc, #144]	; e6334 <__ieee754_rem_pio2f+0x270>
   e62a6:	ee77 6a87 	vadd.f32	s13, s15, s14
   e62aa:	f04f 33ff 	mov.w	r3, #4294967295	; 0xffffffff
   e62ae:	ee77 7ae6 	vsub.f32	s15, s15, s13
   e62b2:	edc0 6a00 	vstr	s13, [r0]
   e62b6:	ee77 7a87 	vadd.f32	s15, s15, s14
   e62ba:	edc0 7a01 	vstr	s15, [r0, #4]
   e62be:	e76f      	b.n	e61a0 <__ieee754_rem_pio2f+0xdc>
   e62c0:	ed95 7a00 	vldr	s14, [r5]
   e62c4:	edd5 7a01 	vldr	s15, [r5, #4]
   e62c8:	eeb1 7a47 	vneg.f32	s14, s14
   e62cc:	eef1 7a67 	vneg.f32	s15, s15
   e62d0:	4243      	negs	r3, r0
   e62d2:	ed85 7a00 	vstr	s14, [r5]
   e62d6:	edc5 7a01 	vstr	s15, [r5, #4]
   e62da:	e761      	b.n	e61a0 <__ieee754_rem_pio2f+0xdc>
   e62dc:	2303      	movs	r3, #3
   e62de:	e74d      	b.n	e617c <__ieee754_rem_pio2f+0xb8>
   e62e0:	eddf 6a18 	vldr	s13, [pc, #96]	; e6344 <__ieee754_rem_pio2f+0x280>
   e62e4:	ed9f 7a18 	vldr	s14, [pc, #96]	; e6348 <__ieee754_rem_pio2f+0x284>
   e62e8:	ee77 7aa6 	vadd.f32	s15, s15, s13
   e62ec:	f04f 33ff 	mov.w	r3, #4294967295	; 0xffffffff
   e62f0:	ee77 6a87 	vadd.f32	s13, s15, s14
   e62f4:	ee77 7ae6 	vsub.f32	s15, s15, s13
   e62f8:	edc0 6a00 	vstr	s13, [r0]
   e62fc:	ee77 7a87 	vadd.f32	s15, s15, s14
   e6300:	edc0 7a01 	vstr	s15, [r0, #4]
   e6304:	e74c      	b.n	e61a0 <__ieee754_rem_pio2f+0xdc>
   e6306:	ed9f 7a13 	vldr	s14, [pc, #76]	; e6354 <__ieee754_rem_pio2f+0x290>
   e630a:	ed9f 5a13 	vldr	s10, [pc, #76]	; e6358 <__ieee754_rem_pio2f+0x294>
   e630e:	eeb0 0a65 	vmov.f32	s0, s11
   e6312:	eea6 0a07 	vfma.f32	s0, s12, s14
   e6316:	ee75 7ac0 	vsub.f32	s15, s11, s0
   e631a:	eee6 7a07 	vfma.f32	s15, s12, s14
   e631e:	eed6 7a85 	vfnms.f32	s15, s13, s10
   e6322:	e77c      	b.n	e621e <__ieee754_rem_pio2f+0x15a>
   e6324:	3f490fd8 	.word	0x3f490fd8
   e6328:	4016cbe3 	.word	0x4016cbe3
   e632c:	3fc90f80 	.word	0x3fc90f80
   e6330:	3fc90fd0 	.word	0x3fc90fd0
   e6334:	37354443 	.word	0x37354443
   e6338:	43490f80 	.word	0x43490f80
   e633c:	43800000 	.word	0x43800000
   e6340:	000eb5c8 	.word	0x000eb5c8
   e6344:	37354400 	.word	0x37354400
   e6348:	2e85a308 	.word	0x2e85a308
   e634c:	3f22f984 	.word	0x3f22f984
   e6350:	000eb548 	.word	0x000eb548
   e6354:	2e85a300 	.word	0x2e85a300
   e6358:	248d3132 	.word	0x248d3132

000e635c <__ieee754_sqrtf>:
   e635c:	ee10 3a10 	vmov	r3, s0
   e6360:	f023 4200 	bic.w	r2, r3, #2147483648	; 0x80000000
   e6364:	f1b2 4fff 	cmp.w	r2, #2139095040	; 0x7f800000
   e6368:	b470      	push	{r4, r5, r6}
   e636a:	d230      	bcs.n	e63ce <__ieee754_sqrtf+0x72>
   e636c:	b36a      	cbz	r2, e63ca <__ieee754_sqrtf+0x6e>
   e636e:	2b00      	cmp	r3, #0
   e6370:	db3d      	blt.n	e63ee <__ieee754_sqrtf+0x92>
   e6372:	f5b2 0f00 	cmp.w	r2, #8388608	; 0x800000
   e6376:	ea4f 50e3 	mov.w	r0, r3, asr #23
   e637a:	d32c      	bcc.n	e63d6 <__ieee754_sqrtf+0x7a>
   e637c:	f1a0 027f 	sub.w	r2, r0, #127	; 0x7f
   e6380:	f3c3 0316 	ubfx	r3, r3, #0, #23
   e6384:	07d1      	lsls	r1, r2, #31
   e6386:	f443 0300 	orr.w	r3, r3, #8388608	; 0x800000
   e638a:	bf48      	it	mi
   e638c:	005b      	lslmi	r3, r3, #1
   e638e:	2400      	movs	r4, #0
   e6390:	1056      	asrs	r6, r2, #1
   e6392:	005b      	lsls	r3, r3, #1
   e6394:	4625      	mov	r5, r4
   e6396:	2119      	movs	r1, #25
   e6398:	f04f 7280 	mov.w	r2, #16777216	; 0x1000000
   e639c:	18a8      	adds	r0, r5, r2
   e639e:	4298      	cmp	r0, r3
   e63a0:	dc02      	bgt.n	e63a8 <__ieee754_sqrtf+0x4c>
   e63a2:	1a1b      	subs	r3, r3, r0
   e63a4:	1885      	adds	r5, r0, r2
   e63a6:	4414      	add	r4, r2
   e63a8:	3901      	subs	r1, #1
   e63aa:	ea4f 0343 	mov.w	r3, r3, lsl #1
   e63ae:	ea4f 0252 	mov.w	r2, r2, lsr #1
   e63b2:	d1f3      	bne.n	e639c <__ieee754_sqrtf+0x40>
   e63b4:	b113      	cbz	r3, e63bc <__ieee754_sqrtf+0x60>
   e63b6:	f004 0301 	and.w	r3, r4, #1
   e63ba:	441c      	add	r4, r3
   e63bc:	1064      	asrs	r4, r4, #1
   e63be:	f104 547c 	add.w	r4, r4, #1056964608	; 0x3f000000
   e63c2:	eb04 53c6 	add.w	r3, r4, r6, lsl #23
   e63c6:	ee00 3a10 	vmov	s0, r3
   e63ca:	bc70      	pop	{r4, r5, r6}
   e63cc:	4770      	bx	lr
   e63ce:	eea0 0a00 	vfma.f32	s0, s0, s0
   e63d2:	bc70      	pop	{r4, r5, r6}
   e63d4:	4770      	bx	lr
   e63d6:	f413 0200 	ands.w	r2, r3, #8388608	; 0x800000
   e63da:	d001      	beq.n	e63e0 <__ieee754_sqrtf+0x84>
   e63dc:	e00c      	b.n	e63f8 <__ieee754_sqrtf+0x9c>
   e63de:	460a      	mov	r2, r1
   e63e0:	005b      	lsls	r3, r3, #1
   e63e2:	021c      	lsls	r4, r3, #8
   e63e4:	f102 0101 	add.w	r1, r2, #1
   e63e8:	d5f9      	bpl.n	e63de <__ieee754_sqrtf+0x82>
   e63ea:	1a80      	subs	r0, r0, r2
   e63ec:	e7c6      	b.n	e637c <__ieee754_sqrtf+0x20>
   e63ee:	ee70 7a40 	vsub.f32	s15, s0, s0
   e63f2:	ee87 0aa7 	vdiv.f32	s0, s15, s15
   e63f6:	e7e8      	b.n	e63ca <__ieee754_sqrtf+0x6e>
   e63f8:	f04f 32ff 	mov.w	r2, #4294967295	; 0xffffffff
   e63fc:	e7f5      	b.n	e63ea <__ieee754_sqrtf+0x8e>
   e63fe:	bf00      	nop

000e6400 <__kernel_cosf>:
   e6400:	ee10 3a10 	vmov	r3, s0
   e6404:	f023 4300 	bic.w	r3, r3, #2147483648	; 0x80000000
   e6408:	f1b3 5f48 	cmp.w	r3, #838860800	; 0x32000000
   e640c:	da2c      	bge.n	e6468 <__kernel_cosf+0x68>
   e640e:	eefd 7ac0 	vcvt.s32.f32	s15, s0
   e6412:	ee17 3a90 	vmov	r3, s15
   e6416:	2b00      	cmp	r3, #0
   e6418:	d060      	beq.n	e64dc <__kernel_cosf+0xdc>
   e641a:	ee20 7a00 	vmul.f32	s14, s0, s0
   e641e:	eddf 4a31 	vldr	s9, [pc, #196]	; e64e4 <__kernel_cosf+0xe4>
   e6422:	ed9f 5a31 	vldr	s10, [pc, #196]	; e64e8 <__kernel_cosf+0xe8>
   e6426:	eddf 5a31 	vldr	s11, [pc, #196]	; e64ec <__kernel_cosf+0xec>
   e642a:	ed9f 6a31 	vldr	s12, [pc, #196]	; e64f0 <__kernel_cosf+0xf0>
   e642e:	eddf 7a31 	vldr	s15, [pc, #196]	; e64f4 <__kernel_cosf+0xf4>
   e6432:	eddf 6a31 	vldr	s13, [pc, #196]	; e64f8 <__kernel_cosf+0xf8>
   e6436:	eea7 5a24 	vfma.f32	s10, s14, s9
   e643a:	eee7 5a05 	vfma.f32	s11, s14, s10
   e643e:	eea7 6a25 	vfma.f32	s12, s14, s11
   e6442:	eee7 7a06 	vfma.f32	s15, s14, s12
   e6446:	eee7 6a27 	vfma.f32	s13, s14, s15
   e644a:	ee66 6a87 	vmul.f32	s13, s13, s14
   e644e:	ee60 0ac0 	vnmul.f32	s1, s1, s0
   e6452:	eeb6 6a00 	vmov.f32	s12, #96	; 0x3f000000  0.5
   e6456:	eee7 0a26 	vfma.f32	s1, s14, s13
   e645a:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   e645e:	eed7 0a06 	vfnms.f32	s1, s14, s12
   e6462:	ee37 0ae0 	vsub.f32	s0, s15, s1
   e6466:	4770      	bx	lr
   e6468:	ee20 7a00 	vmul.f32	s14, s0, s0
   e646c:	eddf 4a1d 	vldr	s9, [pc, #116]	; e64e4 <__kernel_cosf+0xe4>
   e6470:	ed9f 5a1d 	vldr	s10, [pc, #116]	; e64e8 <__kernel_cosf+0xe8>
   e6474:	eddf 5a1d 	vldr	s11, [pc, #116]	; e64ec <__kernel_cosf+0xec>
   e6478:	ed9f 6a1d 	vldr	s12, [pc, #116]	; e64f0 <__kernel_cosf+0xf0>
   e647c:	eddf 7a1d 	vldr	s15, [pc, #116]	; e64f4 <__kernel_cosf+0xf4>
   e6480:	eddf 6a1d 	vldr	s13, [pc, #116]	; e64f8 <__kernel_cosf+0xf8>
   e6484:	4a1d      	ldr	r2, [pc, #116]	; (e64fc <__kernel_cosf+0xfc>)
   e6486:	eea7 5a24 	vfma.f32	s10, s14, s9
   e648a:	4293      	cmp	r3, r2
   e648c:	eee7 5a05 	vfma.f32	s11, s14, s10
   e6490:	eea7 6a25 	vfma.f32	s12, s14, s11
   e6494:	eee7 7a06 	vfma.f32	s15, s14, s12
   e6498:	eee7 6a27 	vfma.f32	s13, s14, s15
   e649c:	ee66 6a87 	vmul.f32	s13, s13, s14
   e64a0:	ddd5      	ble.n	e644e <__kernel_cosf+0x4e>
   e64a2:	4a17      	ldr	r2, [pc, #92]	; (e6500 <__kernel_cosf+0x100>)
   e64a4:	4293      	cmp	r3, r2
   e64a6:	dc14      	bgt.n	e64d2 <__kernel_cosf+0xd2>
   e64a8:	f103 437f 	add.w	r3, r3, #4278190080	; 0xff000000
   e64ac:	ee07 3a90 	vmov	s15, r3
   e64b0:	eeb7 6a00 	vmov.f32	s12, #112	; 0x3f800000  1.0
   e64b4:	ee36 6a67 	vsub.f32	s12, s12, s15
   e64b8:	ee60 0ac0 	vnmul.f32	s1, s1, s0
   e64bc:	eef6 5a00 	vmov.f32	s11, #96	; 0x3f000000  0.5
   e64c0:	eee7 0a26 	vfma.f32	s1, s14, s13
   e64c4:	eed7 7a25 	vfnms.f32	s15, s14, s11
   e64c8:	ee77 7ae0 	vsub.f32	s15, s15, s1
   e64cc:	ee36 0a67 	vsub.f32	s0, s12, s15
   e64d0:	4770      	bx	lr
   e64d2:	eeb6 6a07 	vmov.f32	s12, #103	; 0x3f380000  0.7187500
   e64d6:	eef5 7a02 	vmov.f32	s15, #82	; 0x3e900000  0.2812500
   e64da:	e7ed      	b.n	e64b8 <__kernel_cosf+0xb8>
   e64dc:	eeb7 0a00 	vmov.f32	s0, #112	; 0x3f800000  1.0
   e64e0:	4770      	bx	lr
   e64e2:	bf00      	nop
   e64e4:	ad47d74e 	.word	0xad47d74e
   e64e8:	310f74f6 	.word	0x310f74f6
   e64ec:	b493f27c 	.word	0xb493f27c
   e64f0:	37d00d01 	.word	0x37d00d01
   e64f4:	bab60b61 	.word	0xbab60b61
   e64f8:	3d2aaaab 	.word	0x3d2aaaab
   e64fc:	3e999999 	.word	0x3e999999
   e6500:	3f480000 	.word	0x3f480000

000e6504 <__kernel_rem_pio2f>:
   e6504:	e92d 4ff0 	stmdb	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, lr}
   e6508:	ed2d 8b04 	vpush	{d8-d9}
   e650c:	b0d7      	sub	sp, #348	; 0x15c
   e650e:	1e5f      	subs	r7, r3, #1
   e6510:	4cda      	ldr	r4, [pc, #872]	; (e687c <__kernel_rem_pio2f+0x378>)
   e6512:	9d64      	ldr	r5, [sp, #400]	; 0x190
   e6514:	9301      	str	r3, [sp, #4]
   e6516:	1ed3      	subs	r3, r2, #3
   e6518:	bf48      	it	mi
   e651a:	1d13      	addmi	r3, r2, #4
   e651c:	f854 6025 	ldr.w	r6, [r4, r5, lsl #2]
   e6520:	10db      	asrs	r3, r3, #3
   e6522:	ea23 73e3 	bic.w	r3, r3, r3, asr #31
   e6526:	f103 0a01 	add.w	sl, r3, #1
   e652a:	468b      	mov	fp, r1
   e652c:	19f1      	adds	r1, r6, r7
   e652e:	9302      	str	r3, [sp, #8]
   e6530:	4681      	mov	r9, r0
   e6532:	eba2 0aca 	sub.w	sl, r2, sl, lsl #3
   e6536:	eba3 0307 	sub.w	r3, r3, r7
   e653a:	d414      	bmi.n	e6566 <__kernel_rem_pio2f+0x62>
   e653c:	4419      	add	r1, r3
   e653e:	9865      	ldr	r0, [sp, #404]	; 0x194
   e6540:	3101      	adds	r1, #1
   e6542:	aa1a      	add	r2, sp, #104	; 0x68
   e6544:	2b00      	cmp	r3, #0
   e6546:	bfaa      	itet	ge
   e6548:	f850 4023 	ldrge.w	r4, [r0, r3, lsl #2]
   e654c:	eddf 7ad0 	vldrlt	s15, [pc, #832]	; e6890 <__kernel_rem_pio2f+0x38c>
   e6550:	ee07 4a90 	vmovge	s15, r4
   e6554:	f103 0301 	add.w	r3, r3, #1
   e6558:	bfa8      	it	ge
   e655a:	eef8 7ae7 	vcvtge.f32.s32	s15, s15
   e655e:	428b      	cmp	r3, r1
   e6560:	ece2 7a01 	vstmia	r2!, {s15}
   e6564:	d1ee      	bne.n	e6544 <__kernel_rem_pio2f+0x40>
   e6566:	2e00      	cmp	r6, #0
   e6568:	f2c0 82d6 	blt.w	e6b18 <__kernel_rem_pio2f+0x614>
   e656c:	9b01      	ldr	r3, [sp, #4]
   e656e:	ad42      	add	r5, sp, #264	; 0x108
   e6570:	009c      	lsls	r4, r3, #2
   e6572:	f106 0e01 	add.w	lr, r6, #1
   e6576:	ab1a      	add	r3, sp, #104	; 0x68
   e6578:	eb05 0e8e 	add.w	lr, r5, lr, lsl #2
   e657c:	1918      	adds	r0, r3, r4
   e657e:	eb09 0104 	add.w	r1, r9, r4
   e6582:	2f00      	cmp	r7, #0
   e6584:	f2c0 81c0 	blt.w	e6908 <__kernel_rem_pio2f+0x404>
   e6588:	eddf 7ac1 	vldr	s15, [pc, #772]	; e6890 <__kernel_rem_pio2f+0x38c>
   e658c:	464b      	mov	r3, r9
   e658e:	4602      	mov	r2, r0
   e6590:	ecf3 6a01 	vldmia	r3!, {s13}
   e6594:	ed32 7a01 	vldmdb	r2!, {s14}
   e6598:	428b      	cmp	r3, r1
   e659a:	eee6 7a87 	vfma.f32	s15, s13, s14
   e659e:	d1f7      	bne.n	e6590 <__kernel_rem_pio2f+0x8c>
   e65a0:	ece5 7a01 	vstmia	r5!, {s15}
   e65a4:	4575      	cmp	r5, lr
   e65a6:	f100 0004 	add.w	r0, r0, #4
   e65aa:	d1ea      	bne.n	e6582 <__kernel_rem_pio2f+0x7e>
   e65ac:	f106 4380 	add.w	r3, r6, #1073741824	; 0x40000000
   e65b0:	3b02      	subs	r3, #2
   e65b2:	009b      	lsls	r3, r3, #2
   e65b4:	aa06      	add	r2, sp, #24
   e65b6:	f103 0804 	add.w	r8, r3, #4
   e65ba:	eddf 8ab1 	vldr	s17, [pc, #708]	; e6880 <__kernel_rem_pio2f+0x37c>
   e65be:	ed9f 8ab1 	vldr	s16, [pc, #708]	; e6884 <__kernel_rem_pio2f+0x380>
   e65c2:	f8cd b010 	str.w	fp, [sp, #16]
   e65c6:	4413      	add	r3, r2
   e65c8:	444c      	add	r4, r9
   e65ca:	4490      	add	r8, r2
   e65cc:	9303      	str	r3, [sp, #12]
   e65ce:	4635      	mov	r5, r6
   e65d0:	ab56      	add	r3, sp, #344	; 0x158
   e65d2:	eb03 0385 	add.w	r3, r3, r5, lsl #2
   e65d6:	2d00      	cmp	r5, #0
   e65d8:	ed13 0a14 	vldr	s0, [r3, #-80]	; 0xffffffb0
   e65dc:	dd19      	ble.n	e6612 <__kernel_rem_pio2f+0x10e>
   e65de:	a942      	add	r1, sp, #264	; 0x108
   e65e0:	eb01 0385 	add.w	r3, r1, r5, lsl #2
   e65e4:	aa05      	add	r2, sp, #20
   e65e6:	ee60 7a28 	vmul.f32	s15, s0, s17
   e65ea:	eeb0 7a40 	vmov.f32	s14, s0
   e65ee:	eefd 7ae7 	vcvt.s32.f32	s15, s15
   e65f2:	ed73 6a01 	vldmdb	r3!, {s13}
   e65f6:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   e65fa:	428b      	cmp	r3, r1
   e65fc:	eea7 7ac8 	vfms.f32	s14, s15, s16
   e6600:	ee37 0aa6 	vadd.f32	s0, s15, s13
   e6604:	eebd 7ac7 	vcvt.s32.f32	s14, s14
   e6608:	ee17 0a10 	vmov	r0, s14
   e660c:	f842 0f04 	str.w	r0, [r2, #4]!
   e6610:	d1e9      	bne.n	e65e6 <__kernel_rem_pio2f+0xe2>
   e6612:	4650      	mov	r0, sl
   e6614:	f000 faf6 	bl	e6c04 <scalbnf>
   e6618:	eeb0 9a40 	vmov.f32	s18, s0
   e661c:	eeb4 0a00 	vmov.f32	s0, #64	; 0x3e000000  0.125
   e6620:	ee29 0a00 	vmul.f32	s0, s18, s0
   e6624:	f7fe fee4 	bl	e53f0 <floorf>
   e6628:	eef2 7a00 	vmov.f32	s15, #32	; 0x41000000  8.0
   e662c:	eea0 9a67 	vfms.f32	s18, s0, s15
   e6630:	f1ba 0f00 	cmp.w	sl, #0
   e6634:	eefd 7ac9 	vcvt.s32.f32	s15, s18
   e6638:	ee17 ba90 	vmov	fp, s15
   e663c:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   e6640:	ee39 9a67 	vsub.f32	s18, s18, s15
   e6644:	f340 8145 	ble.w	e68d2 <__kernel_rem_pio2f+0x3ce>
   e6648:	f105 3eff 	add.w	lr, r5, #4294967295	; 0xffffffff
   e664c:	ab06      	add	r3, sp, #24
   e664e:	f1ca 0208 	rsb	r2, sl, #8
   e6652:	f853 302e 	ldr.w	r3, [r3, lr, lsl #2]
   e6656:	fa43 f002 	asr.w	r0, r3, r2
   e665a:	fa00 f202 	lsl.w	r2, r0, r2
   e665e:	a906      	add	r1, sp, #24
   e6660:	1a9b      	subs	r3, r3, r2
   e6662:	f1ca 0207 	rsb	r2, sl, #7
   e6666:	f841 302e 	str.w	r3, [r1, lr, lsl #2]
   e666a:	4483      	add	fp, r0
   e666c:	fa43 f102 	asr.w	r1, r3, r2
   e6670:	2900      	cmp	r1, #0
   e6672:	dd37      	ble.n	e66e4 <__kernel_rem_pio2f+0x1e0>
   e6674:	2d00      	cmp	r5, #0
   e6676:	f10b 0b01 	add.w	fp, fp, #1
   e667a:	f340 8228 	ble.w	e6ace <__kernel_rem_pio2f+0x5ca>
   e667e:	2200      	movs	r2, #0
   e6680:	4610      	mov	r0, r2
   e6682:	f10d 0e14 	add.w	lr, sp, #20
   e6686:	468c      	mov	ip, r1
   e6688:	e008      	b.n	e669c <__kernel_rem_pio2f+0x198>
   e668a:	f5c3 7180 	rsb	r1, r3, #256	; 0x100
   e668e:	b113      	cbz	r3, e6696 <__kernel_rem_pio2f+0x192>
   e6690:	f8ce 1000 	str.w	r1, [lr]
   e6694:	2001      	movs	r0, #1
   e6696:	3201      	adds	r2, #1
   e6698:	4295      	cmp	r5, r2
   e669a:	dd0c      	ble.n	e66b6 <__kernel_rem_pio2f+0x1b2>
   e669c:	f85e 3f04 	ldr.w	r3, [lr, #4]!
   e66a0:	2800      	cmp	r0, #0
   e66a2:	d0f2      	beq.n	e668a <__kernel_rem_pio2f+0x186>
   e66a4:	3201      	adds	r2, #1
   e66a6:	f1c3 03ff 	rsb	r3, r3, #255	; 0xff
   e66aa:	4295      	cmp	r5, r2
   e66ac:	f8ce 3000 	str.w	r3, [lr]
   e66b0:	f04f 0001 	mov.w	r0, #1
   e66b4:	dcf2      	bgt.n	e669c <__kernel_rem_pio2f+0x198>
   e66b6:	4661      	mov	r1, ip
   e66b8:	f1ba 0f00 	cmp.w	sl, #0
   e66bc:	dd10      	ble.n	e66e0 <__kernel_rem_pio2f+0x1dc>
   e66be:	f1ba 0f01 	cmp.w	sl, #1
   e66c2:	f000 810d 	beq.w	e68e0 <__kernel_rem_pio2f+0x3dc>
   e66c6:	f1ba 0f02 	cmp.w	sl, #2
   e66ca:	d109      	bne.n	e66e0 <__kernel_rem_pio2f+0x1dc>
   e66cc:	1e6a      	subs	r2, r5, #1
   e66ce:	ab06      	add	r3, sp, #24
   e66d0:	f10d 0e18 	add.w	lr, sp, #24
   e66d4:	f853 3022 	ldr.w	r3, [r3, r2, lsl #2]
   e66d8:	f003 033f 	and.w	r3, r3, #63	; 0x3f
   e66dc:	f84e 3022 	str.w	r3, [lr, r2, lsl #2]
   e66e0:	2902      	cmp	r1, #2
   e66e2:	d05c      	beq.n	e679e <__kernel_rem_pio2f+0x29a>
   e66e4:	eeb5 9a40 	vcmp.f32	s18, #0.0
   e66e8:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e66ec:	d169      	bne.n	e67c2 <__kernel_rem_pio2f+0x2be>
   e66ee:	f105 3eff 	add.w	lr, r5, #4294967295	; 0xffffffff
   e66f2:	4576      	cmp	r6, lr
   e66f4:	dc0f      	bgt.n	e6716 <__kernel_rem_pio2f+0x212>
   e66f6:	f105 4280 	add.w	r2, r5, #1073741824	; 0x40000000
   e66fa:	3a01      	subs	r2, #1
   e66fc:	ab06      	add	r3, sp, #24
   e66fe:	eb03 0282 	add.w	r2, r3, r2, lsl #2
   e6702:	2000      	movs	r0, #0
   e6704:	f852 3904 	ldr.w	r3, [r2], #-4
   e6708:	4542      	cmp	r2, r8
   e670a:	ea40 0003 	orr.w	r0, r0, r3
   e670e:	d1f9      	bne.n	e6704 <__kernel_rem_pio2f+0x200>
   e6710:	2800      	cmp	r0, #0
   e6712:	f040 8110 	bne.w	e6936 <__kernel_rem_pio2f+0x432>
   e6716:	1e73      	subs	r3, r6, #1
   e6718:	aa06      	add	r2, sp, #24
   e671a:	f852 3023 	ldr.w	r3, [r2, r3, lsl #2]
   e671e:	2b00      	cmp	r3, #0
   e6720:	f040 81d2 	bne.w	e6ac8 <__kernel_rem_pio2f+0x5c4>
   e6724:	9b03      	ldr	r3, [sp, #12]
   e6726:	f04f 0e01 	mov.w	lr, #1
   e672a:	f853 2904 	ldr.w	r2, [r3], #-4
   e672e:	f10e 0e01 	add.w	lr, lr, #1
   e6732:	2a00      	cmp	r2, #0
   e6734:	d0f9      	beq.n	e672a <__kernel_rem_pio2f+0x226>
   e6736:	44ae      	add	lr, r5
   e6738:	1c6b      	adds	r3, r5, #1
   e673a:	4573      	cmp	r3, lr
   e673c:	dc2d      	bgt.n	e679a <__kernel_rem_pio2f+0x296>
   e673e:	9a02      	ldr	r2, [sp, #8]
   e6740:	1898      	adds	r0, r3, r2
   e6742:	9a01      	ldr	r2, [sp, #4]
   e6744:	f100 4080 	add.w	r0, r0, #1073741824	; 0x40000000
   e6748:	1951      	adds	r1, r2, r5
   e674a:	eb0e 0c02 	add.w	ip, lr, r2
   e674e:	9a65      	ldr	r2, [sp, #404]	; 0x194
   e6750:	3801      	subs	r0, #1
   e6752:	eb02 0080 	add.w	r0, r2, r0, lsl #2
   e6756:	aa1a      	add	r2, sp, #104	; 0x68
   e6758:	eb02 0181 	add.w	r1, r2, r1, lsl #2
   e675c:	eb02 0c8c 	add.w	ip, r2, ip, lsl #2
   e6760:	aa42      	add	r2, sp, #264	; 0x108
   e6762:	eb02 0583 	add.w	r5, r2, r3, lsl #2
   e6766:	f850 3f04 	ldr.w	r3, [r0, #4]!
   e676a:	ee07 3a90 	vmov	s15, r3
   e676e:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   e6772:	2f00      	cmp	r7, #0
   e6774:	ece1 7a01 	vstmia	r1!, {s15}
   e6778:	eddf 7a45 	vldr	s15, [pc, #276]	; e6890 <__kernel_rem_pio2f+0x38c>
   e677c:	db09      	blt.n	e6792 <__kernel_rem_pio2f+0x28e>
   e677e:	464b      	mov	r3, r9
   e6780:	460a      	mov	r2, r1
   e6782:	ecf3 6a01 	vldmia	r3!, {s13}
   e6786:	ed32 7a01 	vldmdb	r2!, {s14}
   e678a:	42a3      	cmp	r3, r4
   e678c:	eee6 7a87 	vfma.f32	s15, s13, s14
   e6790:	d1f7      	bne.n	e6782 <__kernel_rem_pio2f+0x27e>
   e6792:	4561      	cmp	r1, ip
   e6794:	ece5 7a01 	vstmia	r5!, {s15}
   e6798:	d1e5      	bne.n	e6766 <__kernel_rem_pio2f+0x262>
   e679a:	4675      	mov	r5, lr
   e679c:	e718      	b.n	e65d0 <__kernel_rem_pio2f+0xcc>
   e679e:	eeb7 0a00 	vmov.f32	s0, #112	; 0x3f800000  1.0
   e67a2:	ee30 9a49 	vsub.f32	s18, s0, s18
   e67a6:	2800      	cmp	r0, #0
   e67a8:	d09c      	beq.n	e66e4 <__kernel_rem_pio2f+0x1e0>
   e67aa:	4650      	mov	r0, sl
   e67ac:	9105      	str	r1, [sp, #20]
   e67ae:	f000 fa29 	bl	e6c04 <scalbnf>
   e67b2:	ee39 9a40 	vsub.f32	s18, s18, s0
   e67b6:	9905      	ldr	r1, [sp, #20]
   e67b8:	eeb5 9a40 	vcmp.f32	s18, #0.0
   e67bc:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e67c0:	d095      	beq.n	e66ee <__kernel_rem_pio2f+0x1ea>
   e67c2:	eeb0 0a49 	vmov.f32	s0, s18
   e67c6:	f1ca 0000 	rsb	r0, sl, #0
   e67ca:	ee09 ba90 	vmov	s19, fp
   e67ce:	4688      	mov	r8, r1
   e67d0:	f8dd b010 	ldr.w	fp, [sp, #16]
   e67d4:	f000 fa16 	bl	e6c04 <scalbnf>
   e67d8:	ed9f 7a2a 	vldr	s14, [pc, #168]	; e6884 <__kernel_rem_pio2f+0x380>
   e67dc:	eeb4 0ac7 	vcmpe.f32	s0, s14
   e67e0:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e67e4:	f2c0 817e 	blt.w	e6ae4 <__kernel_rem_pio2f+0x5e0>
   e67e8:	eddf 7a25 	vldr	s15, [pc, #148]	; e6880 <__kernel_rem_pio2f+0x37c>
   e67ec:	ee60 7a27 	vmul.f32	s15, s0, s15
   e67f0:	a906      	add	r1, sp, #24
   e67f2:	eefd 7ae7 	vcvt.s32.f32	s15, s15
   e67f6:	1c6b      	adds	r3, r5, #1
   e67f8:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   e67fc:	f10a 0a08 	add.w	sl, sl, #8
   e6800:	eea7 0ac7 	vfms.f32	s0, s15, s14
   e6804:	eefd 7ae7 	vcvt.s32.f32	s15, s15
   e6808:	eebd 0ac0 	vcvt.s32.f32	s0, s0
   e680c:	ee10 2a10 	vmov	r2, s0
   e6810:	f841 2025 	str.w	r2, [r1, r5, lsl #2]
   e6814:	ee17 2a90 	vmov	r2, s15
   e6818:	f841 2023 	str.w	r2, [r1, r3, lsl #2]
   e681c:	4650      	mov	r0, sl
   e681e:	eeb7 0a00 	vmov.f32	s0, #112	; 0x3f800000  1.0
   e6822:	9301      	str	r3, [sp, #4]
   e6824:	f000 f9ee 	bl	e6c04 <scalbnf>
   e6828:	9b01      	ldr	r3, [sp, #4]
   e682a:	2b00      	cmp	r3, #0
   e682c:	f2c0 8166 	blt.w	e6afc <__kernel_rem_pio2f+0x5f8>
   e6830:	009f      	lsls	r7, r3, #2
   e6832:	ac42      	add	r4, sp, #264	; 0x108
   e6834:	aa06      	add	r2, sp, #24
   e6836:	1d38      	adds	r0, r7, #4
   e6838:	eb04 0e07 	add.w	lr, r4, r7
   e683c:	ed9f 7a10 	vldr	s14, [pc, #64]	; e6880 <__kernel_rem_pio2f+0x37c>
   e6840:	4410      	add	r0, r2
   e6842:	f10e 0204 	add.w	r2, lr, #4
   e6846:	ed70 7a01 	vldmdb	r0!, {s15}
   e684a:	eef8 7ae7 	vcvt.f32.s32	s15, s15
   e684e:	ee67 7a80 	vmul.f32	s15, s15, s0
   e6852:	ee20 0a07 	vmul.f32	s0, s0, s14
   e6856:	ed62 7a01 	vstmdb	r2!, {s15}
   e685a:	42a2      	cmp	r2, r4
   e685c:	d1f3      	bne.n	e6846 <__kernel_rem_pio2f+0x342>
   e685e:	f50d 7c82 	add.w	ip, sp, #260	; 0x104
   e6862:	2500      	movs	r5, #0
   e6864:	2e00      	cmp	r6, #0
   e6866:	f2c0 8121 	blt.w	e6aac <__kernel_rem_pio2f+0x5a8>
   e686a:	4807      	ldr	r0, [pc, #28]	; (e6888 <__kernel_rem_pio2f+0x384>)
   e686c:	ed9f 7a07 	vldr	s14, [pc, #28]	; e688c <__kernel_rem_pio2f+0x388>
   e6870:	eddf 7a07 	vldr	s15, [pc, #28]	; e6890 <__kernel_rem_pio2f+0x38c>
   e6874:	4671      	mov	r1, lr
   e6876:	2200      	movs	r2, #0
   e6878:	e011      	b.n	e689e <__kernel_rem_pio2f+0x39a>
   e687a:	bf00      	nop
   e687c:	000eb8e0 	.word	0x000eb8e0
   e6880:	3b800000 	.word	0x3b800000
   e6884:	43800000 	.word	0x43800000
   e6888:	000eb8ec 	.word	0x000eb8ec
   e688c:	3fc90000 	.word	0x3fc90000
   e6890:	00000000 	.word	0x00000000
   e6894:	4295      	cmp	r5, r2
   e6896:	db09      	blt.n	e68ac <__kernel_rem_pio2f+0x3a8>
   e6898:	3004      	adds	r0, #4
   e689a:	ed90 7a00 	vldr	s14, [r0]
   e689e:	ecf1 6a01 	vldmia	r1!, {s13}
   e68a2:	3201      	adds	r2, #1
   e68a4:	4296      	cmp	r6, r2
   e68a6:	eee6 7a87 	vfma.f32	s15, s13, s14
   e68aa:	daf3      	bge.n	e6894 <__kernel_rem_pio2f+0x390>
   e68ac:	f1ae 0e04 	sub.w	lr, lr, #4
   e68b0:	aa56      	add	r2, sp, #344	; 0x158
   e68b2:	eb02 0285 	add.w	r2, r2, r5, lsl #2
   e68b6:	45f4      	cmp	ip, lr
   e68b8:	ed42 7a28 	vstr	s15, [r2, #-160]	; 0xffffff60
   e68bc:	f105 0501 	add.w	r5, r5, #1
   e68c0:	d1d0      	bne.n	e6864 <__kernel_rem_pio2f+0x360>
   e68c2:	9a64      	ldr	r2, [sp, #400]	; 0x190
   e68c4:	2a03      	cmp	r2, #3
   e68c6:	f200 80ae 	bhi.w	e6a26 <__kernel_rem_pio2f+0x522>
   e68ca:	e8df f002 	tbb	[pc, r2]
   e68ce:	b5dc      	.short	0xb5dc
   e68d0:	50b5      	.short	0x50b5
   e68d2:	d110      	bne.n	e68f6 <__kernel_rem_pio2f+0x3f2>
   e68d4:	1e6b      	subs	r3, r5, #1
   e68d6:	aa06      	add	r2, sp, #24
   e68d8:	f852 1023 	ldr.w	r1, [r2, r3, lsl #2]
   e68dc:	1209      	asrs	r1, r1, #8
   e68de:	e6c7      	b.n	e6670 <__kernel_rem_pio2f+0x16c>
   e68e0:	1e6a      	subs	r2, r5, #1
   e68e2:	ab06      	add	r3, sp, #24
   e68e4:	f10d 0e18 	add.w	lr, sp, #24
   e68e8:	f853 3022 	ldr.w	r3, [r3, r2, lsl #2]
   e68ec:	f003 037f 	and.w	r3, r3, #127	; 0x7f
   e68f0:	f84e 3022 	str.w	r3, [lr, r2, lsl #2]
   e68f4:	e6f4      	b.n	e66e0 <__kernel_rem_pio2f+0x1dc>
   e68f6:	eef6 7a00 	vmov.f32	s15, #96	; 0x3f000000  0.5
   e68fa:	eeb4 9ae7 	vcmpe.f32	s18, s15
   e68fe:	eef1 fa10 	vmrs	APSR_nzcv, fpscr
   e6902:	da0b      	bge.n	e691c <__kernel_rem_pio2f+0x418>
   e6904:	2100      	movs	r1, #0
   e6906:	e6ed      	b.n	e66e4 <__kernel_rem_pio2f+0x1e0>
   e6908:	ed5f 7a1f 	vldr	s15, [pc, #-124]	; e6890 <__kernel_rem_pio2f+0x38c>
   e690c:	ece5 7a01 	vstmia	r5!, {s15}
   e6910:	4575      	cmp	r5, lr
   e6912:	f100 0004 	add.w	r0, r0, #4
   e6916:	f47f ae34 	bne.w	e6582 <__kernel_rem_pio2f+0x7e>
   e691a:	e647      	b.n	e65ac <__kernel_rem_pio2f+0xa8>
   e691c:	2d00      	cmp	r5, #0
   e691e:	f10b 0b01 	add.w	fp, fp, #1
   e6922:	bfc8      	it	gt
   e6924:	2102      	movgt	r1, #2
   e6926:	f73f aeaa 	bgt.w	e667e <__kernel_rem_pio2f+0x17a>
   e692a:	eef7 7a00 	vmov.f32	s15, #112	; 0x3f800000  1.0
   e692e:	ee37 9ac9 	vsub.f32	s18, s15, s18
   e6932:	2102      	movs	r1, #2
   e6934:	e6d6      	b.n	e66e4 <__kernel_rem_pio2f+0x1e0>
   e6936:	aa06      	add	r2, sp, #24
   e6938:	ee09 ba90 	vmov	s19, fp
   e693c:	f852 202e 	ldr.w	r2, [r2, lr, lsl #2]
   e6940:	f8dd b010 	ldr.w	fp, [sp, #16]
   e6944:	4673      	mov	r3, lr
   e6946:	4688      	mov	r8, r1
   e6948:	f1aa 0a08 	sub.w	sl, sl, #8
   e694c:	2a00      	cmp	r2, #0
   e694e:	f47f af65 	bne.w	e681c <__kernel_rem_pio2f+0x318>
   e6952:	f10e 4280 	add.w	r2, lr, #1073741824	; 0x40000000
   e6956:	3a01      	subs	r2, #1
   e6958:	a906      	add	r1, sp, #24
   e695a:	eb01 0282 	add.w	r2, r1, r2, lsl #2
   e695e:	f852 1904 	ldr.w	r1, [r2], #-4
   e6962:	3b01      	subs	r3, #1
   e6964:	f1aa 0a08 	sub.w	sl, sl, #8
   e6968:	2900      	cmp	r1, #0
   e696a:	d0f8      	beq.n	e695e <__kernel_rem_pio2f+0x45a>
   e696c:	e756      	b.n	e681c <__kernel_rem_pio2f+0x318>
   e696e:	2b00      	cmp	r3, #0
   e6970:	f340 80c1 	ble.w	e6af6 <__kernel_rem_pio2f+0x5f2>
   e6974:	f103 4280 	add.w	r2, r3, #1073741824	; 0x40000000
   e6978:	3a01      	subs	r2, #1
   e697a:	0090      	lsls	r0, r2, #2
   e697c:	a956      	add	r1, sp, #344	; 0x158
   e697e:	19cd      	adds	r5, r1, r7
   e6980:	1d04      	adds	r4, r0, #4
   e6982:	a92e      	add	r1, sp, #184	; 0xb8
   e6984:	3008      	adds	r0, #8
   e6986:	ed15 7a28 	vldr	s14, [r5, #-160]	; 0xffffff60
   e698a:	440c      	add	r4, r1
   e698c:	4408      	add	r0, r1
   e698e:	ad2f      	add	r5, sp, #188	; 0xbc
   e6990:	ed74 7a01 	vldmdb	r4!, {s15}
   e6994:	ee77 6a87 	vadd.f32	s13, s15, s14
   e6998:	ee77 7ae6 	vsub.f32	s15, s15, s13
   e699c:	ee77 7a87 	vadd.f32	s15, s15, s14
   e69a0:	eeb0 7a66 	vmov.f32	s14, s13
   e69a4:	ed60 7a01 	vstmdb	r0!, {s15}
   e69a8:	42a8      	cmp	r0, r5
   e69aa:	edc4 6a00 	vstr	s13, [r4]
   e69ae:	d1ef      	bne.n	e6990 <__kernel_rem_pio2f+0x48c>
   e69b0:	2b01      	cmp	r3, #1
   e69b2:	f340 80a0 	ble.w	e6af6 <__kernel_rem_pio2f+0x5f2>
   e69b6:	0092      	lsls	r2, r2, #2
   e69b8:	ab56      	add	r3, sp, #344	; 0x158
   e69ba:	441f      	add	r7, r3
   e69bc:	f102 0008 	add.w	r0, r2, #8
   e69c0:	ab2e      	add	r3, sp, #184	; 0xb8
   e69c2:	4418      	add	r0, r3
   e69c4:	3204      	adds	r2, #4
   e69c6:	ed17 7a28 	vldr	s14, [r7, #-160]	; 0xffffff60
   e69ca:	4413      	add	r3, r2
   e69cc:	ac30      	add	r4, sp, #192	; 0xc0
   e69ce:	4602      	mov	r2, r0
   e69d0:	ed73 7a01 	vldmdb	r3!, {s15}
   e69d4:	ee77 6a27 	vadd.f32	s13, s14, s15
   e69d8:	ee77 7ae6 	vsub.f32	s15, s15, s13
   e69dc:	ee77 7a87 	vadd.f32	s15, s15, s14
   e69e0:	eeb0 7a66 	vmov.f32	s14, s13
   e69e4:	ed62 7a01 	vstmdb	r2!, {s15}
   e69e8:	4294      	cmp	r4, r2
   e69ea:	edc3 6a00 	vstr	s13, [r3]
   e69ee:	d1ef      	bne.n	e69d0 <__kernel_rem_pio2f+0x4cc>
   e69f0:	ed5f 7a59 	vldr	s15, [pc, #-356]	; e6890 <__kernel_rem_pio2f+0x38c>
   e69f4:	ed30 7a01 	vldmdb	r0!, {s14}
   e69f8:	4284      	cmp	r4, r0
   e69fa:	ee77 7a87 	vadd.f32	s15, s15, s14
   e69fe:	d1f9      	bne.n	e69f4 <__kernel_rem_pio2f+0x4f0>
   e6a00:	4643      	mov	r3, r8
   e6a02:	2b00      	cmp	r3, #0
   e6a04:	d065      	beq.n	e6ad2 <__kernel_rem_pio2f+0x5ce>
   e6a06:	eddd 6a2e 	vldr	s13, [sp, #184]	; 0xb8
   e6a0a:	ed9d 7a2f 	vldr	s14, [sp, #188]	; 0xbc
   e6a0e:	eef1 7a67 	vneg.f32	s15, s15
   e6a12:	eef1 6a66 	vneg.f32	s13, s13
   e6a16:	eeb1 7a47 	vneg.f32	s14, s14
   e6a1a:	edcb 7a02 	vstr	s15, [fp, #8]
   e6a1e:	edcb 6a00 	vstr	s13, [fp]
   e6a22:	ed8b 7a01 	vstr	s14, [fp, #4]
   e6a26:	ee19 3a90 	vmov	r3, s19
   e6a2a:	f003 0007 	and.w	r0, r3, #7
   e6a2e:	b057      	add	sp, #348	; 0x15c
   e6a30:	ecbd 8b04 	vpop	{d8-d9}
   e6a34:	e8bd 8ff0 	ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, sl, fp, pc}
   e6a38:	1d3a      	adds	r2, r7, #4
   e6a3a:	a82e      	add	r0, sp, #184	; 0xb8
   e6a3c:	ed5f 7a6c 	vldr	s15, [pc, #-432]	; e6890 <__kernel_rem_pio2f+0x38c>
   e6a40:	4402      	add	r2, r0
   e6a42:	ed32 7a01 	vldmdb	r2!, {s14}
   e6a46:	4282      	cmp	r2, r0
   e6a48:	ee77 7a87 	vadd.f32	s15, s15, s14
   e6a4c:	d1f9      	bne.n	e6a42 <__kernel_rem_pio2f+0x53e>
   e6a4e:	4642      	mov	r2, r8
   e6a50:	b37a      	cbz	r2, e6ab2 <__kernel_rem_pio2f+0x5ae>
   e6a52:	eddd 6a2e 	vldr	s13, [sp, #184]	; 0xb8
   e6a56:	eeb1 7a67 	vneg.f32	s14, s15
   e6a5a:	2b00      	cmp	r3, #0
   e6a5c:	ee76 7ae7 	vsub.f32	s15, s13, s15
   e6a60:	ed8b 7a00 	vstr	s14, [fp]
   e6a64:	dd0a      	ble.n	e6a7c <__kernel_rem_pio2f+0x578>
   e6a66:	a82f      	add	r0, sp, #188	; 0xbc
   e6a68:	2201      	movs	r2, #1
   e6a6a:	ecb0 7a01 	vldmia	r0!, {s14}
   e6a6e:	3201      	adds	r2, #1
   e6a70:	4293      	cmp	r3, r2
   e6a72:	ee77 7a87 	vadd.f32	s15, s15, s14
   e6a76:	daf8      	bge.n	e6a6a <__kernel_rem_pio2f+0x566>
   e6a78:	4643      	mov	r3, r8
   e6a7a:	b10b      	cbz	r3, e6a80 <__kernel_rem_pio2f+0x57c>
   e6a7c:	eef1 7a67 	vneg.f32	s15, s15
   e6a80:	edcb 7a01 	vstr	s15, [fp, #4]
   e6a84:	e7cf      	b.n	e6a26 <__kernel_rem_pio2f+0x522>
   e6a86:	aa56      	add	r2, sp, #344	; 0x158
   e6a88:	443a      	add	r2, r7
   e6a8a:	ed5f 7a7f 	vldr	s15, [pc, #-508]	; e6890 <__kernel_rem_pio2f+0x38c>
   e6a8e:	3a9c      	subs	r2, #156	; 0x9c
   e6a90:	ed32 7a01 	vldmdb	r2!, {s14}
   e6a94:	3b01      	subs	r3, #1
   e6a96:	1c59      	adds	r1, r3, #1
   e6a98:	ee77 7a87 	vadd.f32	s15, s15, s14
   e6a9c:	d1f8      	bne.n	e6a90 <__kernel_rem_pio2f+0x58c>
   e6a9e:	4643      	mov	r3, r8
   e6aa0:	b10b      	cbz	r3, e6aa6 <__kernel_rem_pio2f+0x5a2>
   e6aa2:	eef1 7a67 	vneg.f32	s15, s15
   e6aa6:	edcb 7a00 	vstr	s15, [fp]
   e6aaa:	e7bc      	b.n	e6a26 <__kernel_rem_pio2f+0x522>
   e6aac:	ed5f 7a88 	vldr	s15, [pc, #-544]	; e6890 <__kernel_rem_pio2f+0x38c>
   e6ab0:	e6fc      	b.n	e68ac <__kernel_rem_pio2f+0x3a8>
   e6ab2:	ed9d 7a2e 	vldr	s14, [sp, #184]	; 0xb8
   e6ab6:	edcb 7a00 	vstr	s15, [fp]
   e6aba:	2b00      	cmp	r3, #0
   e6abc:	ee77 7a67 	vsub.f32	s15, s14, s15
   e6ac0:	dcd1      	bgt.n	e6a66 <__kernel_rem_pio2f+0x562>
   e6ac2:	edcb 7a01 	vstr	s15, [fp, #4]
   e6ac6:	e7ae      	b.n	e6a26 <__kernel_rem_pio2f+0x522>
   e6ac8:	f04f 0e01 	mov.w	lr, #1
   e6acc:	e633      	b.n	e6736 <__kernel_rem_pio2f+0x232>
   e6ace:	2000      	movs	r0, #0
   e6ad0:	e5f2      	b.n	e66b8 <__kernel_rem_pio2f+0x1b4>
   e6ad2:	9a2e      	ldr	r2, [sp, #184]	; 0xb8
   e6ad4:	9b2f      	ldr	r3, [sp, #188]	; 0xbc
   e6ad6:	edcb 7a02 	vstr	s15, [fp, #8]
   e6ada:	f8cb 2000 	str.w	r2, [fp]
   e6ade:	f8cb 3004 	str.w	r3, [fp, #4]
   e6ae2:	e7a0      	b.n	e6a26 <__kernel_rem_pio2f+0x522>
   e6ae4:	eebd 0ac0 	vcvt.s32.f32	s0, s0
   e6ae8:	a906      	add	r1, sp, #24
   e6aea:	ee10 2a10 	vmov	r2, s0
   e6aee:	462b      	mov	r3, r5
   e6af0:	f841 2025 	str.w	r2, [r1, r5, lsl #2]
   e6af4:	e692      	b.n	e681c <__kernel_rem_pio2f+0x318>
   e6af6:	ed5f 7a9a 	vldr	s15, [pc, #-616]	; e6890 <__kernel_rem_pio2f+0x38c>
   e6afa:	e781      	b.n	e6a00 <__kernel_rem_pio2f+0x4fc>
   e6afc:	9a64      	ldr	r2, [sp, #400]	; 0x190
   e6afe:	2a03      	cmp	r2, #3
   e6b00:	d891      	bhi.n	e6a26 <__kernel_rem_pio2f+0x522>
   e6b02:	a101      	add	r1, pc, #4	; (adr r1, e6b08 <__kernel_rem_pio2f+0x604>)
   e6b04:	f851 f022 	ldr.w	pc, [r1, r2, lsl #2]
   e6b08:	000e6b25 	.word	0x000e6b25
   e6b0c:	000e6b1f 	.word	0x000e6b1f
   e6b10:	000e6b1f 	.word	0x000e6b1f
   e6b14:	000e6af7 	.word	0x000e6af7
   e6b18:	9b01      	ldr	r3, [sp, #4]
   e6b1a:	009c      	lsls	r4, r3, #2
   e6b1c:	e546      	b.n	e65ac <__kernel_rem_pio2f+0xa8>
   e6b1e:	ed5f 7aa4 	vldr	s15, [pc, #-656]	; e6890 <__kernel_rem_pio2f+0x38c>
   e6b22:	e794      	b.n	e6a4e <__kernel_rem_pio2f+0x54a>
   e6b24:	ed5f 7aa6 	vldr	s15, [pc, #-664]	; e6890 <__kernel_rem_pio2f+0x38c>
   e6b28:	e7b9      	b.n	e6a9e <__kernel_rem_pio2f+0x59a>
   e6b2a:	bf00      	nop

000e6b2c <__kernel_sinf>:
   e6b2c:	ee10 3a10 	vmov	r3, s0
   e6b30:	f023 4300 	bic.w	r3, r3, #2147483648	; 0x80000000
   e6b34:	f1b3 5f48 	cmp.w	r3, #838860800	; 0x32000000
   e6b38:	da04      	bge.n	e6b44 <__kernel_sinf+0x18>
   e6b3a:	eefd 7ac0 	vcvt.s32.f32	s15, s0
   e6b3e:	ee17 3a90 	vmov	r3, s15
   e6b42:	b323      	cbz	r3, e6b8e <__kernel_sinf+0x62>
   e6b44:	ee60 7a00 	vmul.f32	s15, s0, s0
   e6b48:	ed9f 5a15 	vldr	s10, [pc, #84]	; e6ba0 <__kernel_sinf+0x74>
   e6b4c:	eddf 5a15 	vldr	s11, [pc, #84]	; e6ba4 <__kernel_sinf+0x78>
   e6b50:	ed9f 6a15 	vldr	s12, [pc, #84]	; e6ba8 <__kernel_sinf+0x7c>
   e6b54:	eddf 6a15 	vldr	s13, [pc, #84]	; e6bac <__kernel_sinf+0x80>
   e6b58:	ed9f 7a15 	vldr	s14, [pc, #84]	; e6bb0 <__kernel_sinf+0x84>
   e6b5c:	eee7 5a85 	vfma.f32	s11, s15, s10
   e6b60:	ee20 5a27 	vmul.f32	s10, s0, s15
   e6b64:	eea7 6aa5 	vfma.f32	s12, s15, s11
   e6b68:	eee7 6a86 	vfma.f32	s13, s15, s12
   e6b6c:	eea7 7aa6 	vfma.f32	s14, s15, s13
   e6b70:	b170      	cbz	r0, e6b90 <__kernel_sinf+0x64>
   e6b72:	ee27 7a45 	vnmul.f32	s14, s14, s10
   e6b76:	eef6 6a00 	vmov.f32	s13, #96	; 0x3f000000  0.5
   e6b7a:	eea0 7aa6 	vfma.f32	s14, s1, s13
   e6b7e:	eddf 6a0d 	vldr	s13, [pc, #52]	; e6bb4 <__kernel_sinf+0x88>
   e6b82:	eed7 0a87 	vfnms.f32	s1, s15, s14
   e6b86:	eee5 0a26 	vfma.f32	s1, s10, s13
   e6b8a:	ee30 0a60 	vsub.f32	s0, s0, s1
   e6b8e:	4770      	bx	lr
   e6b90:	eddf 6a09 	vldr	s13, [pc, #36]	; e6bb8 <__kernel_sinf+0x8c>
   e6b94:	eee7 6a87 	vfma.f32	s13, s15, s14
   e6b98:	eea5 0a26 	vfma.f32	s0, s10, s13
   e6b9c:	4770      	bx	lr
   e6b9e:	bf00      	nop
   e6ba0:	2f2ec9d3 	.word	0x2f2ec9d3
   e6ba4:	b2d72f34 	.word	0xb2d72f34
   e6ba8:	3638ef1b 	.word	0x3638ef1b
   e6bac:	b9500d01 	.word	0xb9500d01
   e6bb0:	3c088889 	.word	0x3c088889
   e6bb4:	3e2aaaab 	.word	0x3e2aaaab
   e6bb8:	be2aaaab 	.word	0xbe2aaaab

000e6bbc <finite>:
   e6bbc:	ee10 3a90 	vmov	r3, s1
   e6bc0:	f043 4000 	orr.w	r0, r3, #2147483648	; 0x80000000
   e6bc4:	f500 1080 	add.w	r0, r0, #1048576	; 0x100000
   e6bc8:	0fc0      	lsrs	r0, r0, #31
   e6bca:	4770      	bx	lr

000e6bcc <matherr>:
   e6bcc:	2000      	movs	r0, #0
   e6bce:	4770      	bx	lr

000e6bd0 <nan>:
   e6bd0:	ed9f 0b01 	vldr	d0, [pc, #4]	; e6bd8 <nan+0x8>
   e6bd4:	4770      	bx	lr
   e6bd6:	bf00      	nop
   e6bd8:	00000000 	.word	0x00000000
   e6bdc:	7ff80000 	.word	0x7ff80000

000e6be0 <fabsf>:
   e6be0:	ee10 3a10 	vmov	r3, s0
   e6be4:	f023 4300 	bic.w	r3, r3, #2147483648	; 0x80000000
   e6be8:	ee00 3a10 	vmov	s0, r3
   e6bec:	4770      	bx	lr
   e6bee:	bf00      	nop

000e6bf0 <finitef>:
   e6bf0:	ee10 3a10 	vmov	r3, s0
   e6bf4:	f023 4000 	bic.w	r0, r3, #2147483648	; 0x80000000
   e6bf8:	f1b0 4fff 	cmp.w	r0, #2139095040	; 0x7f800000
   e6bfc:	bfac      	ite	ge
   e6bfe:	2000      	movge	r0, #0
   e6c00:	2001      	movlt	r0, #1
   e6c02:	4770      	bx	lr

000e6c04 <scalbnf>:
   e6c04:	b508      	push	{r3, lr}
   e6c06:	ee10 3a10 	vmov	r3, s0
   e6c0a:	f033 4200 	bics.w	r2, r3, #2147483648	; 0x80000000
   e6c0e:	ed2d 8b02 	vpush	{d8}
   e6c12:	d011      	beq.n	e6c38 <scalbnf+0x34>
   e6c14:	f1b2 4fff 	cmp.w	r2, #2139095040	; 0x7f800000
   e6c18:	d211      	bcs.n	e6c3e <scalbnf+0x3a>
   e6c1a:	f5b2 0f00 	cmp.w	r2, #8388608	; 0x800000
   e6c1e:	d313      	bcc.n	e6c48 <scalbnf+0x44>
   e6c20:	0dd2      	lsrs	r2, r2, #23
   e6c22:	4402      	add	r2, r0
   e6c24:	2afe      	cmp	r2, #254	; 0xfe
   e6c26:	dc2e      	bgt.n	e6c86 <scalbnf+0x82>
   e6c28:	2a00      	cmp	r2, #0
   e6c2a:	dd1a      	ble.n	e6c62 <scalbnf+0x5e>
   e6c2c:	f023 43ff 	bic.w	r3, r3, #2139095040	; 0x7f800000
   e6c30:	ea43 53c2 	orr.w	r3, r3, r2, lsl #23
   e6c34:	ee00 3a10 	vmov	s0, r3
   e6c38:	ecbd 8b02 	vpop	{d8}
   e6c3c:	bd08      	pop	{r3, pc}
   e6c3e:	ecbd 8b02 	vpop	{d8}
   e6c42:	ee30 0a00 	vadd.f32	s0, s0, s0
   e6c46:	bd08      	pop	{r3, pc}
   e6c48:	4b1d      	ldr	r3, [pc, #116]	; (e6cc0 <scalbnf+0xbc>)
   e6c4a:	eddf 7a1e 	vldr	s15, [pc, #120]	; e6cc4 <scalbnf+0xc0>
   e6c4e:	4298      	cmp	r0, r3
   e6c50:	ee20 0a27 	vmul.f32	s0, s0, s15
   e6c54:	db22      	blt.n	e6c9c <scalbnf+0x98>
   e6c56:	ee10 3a10 	vmov	r3, s0
   e6c5a:	f3c3 52c7 	ubfx	r2, r3, #23, #8
   e6c5e:	3a19      	subs	r2, #25
   e6c60:	e7df      	b.n	e6c22 <scalbnf+0x1e>
   e6c62:	f112 0f16 	cmn.w	r2, #22
   e6c66:	da1e      	bge.n	e6ca6 <scalbnf+0xa2>
   e6c68:	f24c 3350 	movw	r3, #50000	; 0xc350
   e6c6c:	4298      	cmp	r0, r3
   e6c6e:	dc0a      	bgt.n	e6c86 <scalbnf+0x82>
   e6c70:	ed9f 8a15 	vldr	s16, [pc, #84]	; e6cc8 <scalbnf+0xc4>
   e6c74:	eef0 0a40 	vmov.f32	s1, s0
   e6c78:	eeb0 0a48 	vmov.f32	s0, s16
   e6c7c:	f000 f82a 	bl	e6cd4 <copysignf>
   e6c80:	ee20 0a08 	vmul.f32	s0, s0, s16
   e6c84:	e7d8      	b.n	e6c38 <scalbnf+0x34>
   e6c86:	ed9f 8a11 	vldr	s16, [pc, #68]	; e6ccc <scalbnf+0xc8>
   e6c8a:	eef0 0a40 	vmov.f32	s1, s0
   e6c8e:	eeb0 0a48 	vmov.f32	s0, s16
   e6c92:	f000 f81f 	bl	e6cd4 <copysignf>
   e6c96:	ee20 0a08 	vmul.f32	s0, s0, s16
   e6c9a:	e7cd      	b.n	e6c38 <scalbnf+0x34>
   e6c9c:	eddf 0a0a 	vldr	s1, [pc, #40]	; e6cc8 <scalbnf+0xc4>
   e6ca0:	ee20 0a20 	vmul.f32	s0, s0, s1
   e6ca4:	e7c8      	b.n	e6c38 <scalbnf+0x34>
   e6ca6:	3219      	adds	r2, #25
   e6ca8:	f023 43ff 	bic.w	r3, r3, #2139095040	; 0x7f800000
   e6cac:	ea43 53c2 	orr.w	r3, r3, r2, lsl #23
   e6cb0:	eddf 7a07 	vldr	s15, [pc, #28]	; e6cd0 <scalbnf+0xcc>
   e6cb4:	ee00 3a10 	vmov	s0, r3
   e6cb8:	ee20 0a27 	vmul.f32	s0, s0, s15
   e6cbc:	e7bc      	b.n	e6c38 <scalbnf+0x34>
   e6cbe:	bf00      	nop
   e6cc0:	ffff3cb0 	.word	0xffff3cb0
   e6cc4:	4c000000 	.word	0x4c000000
   e6cc8:	0da24260 	.word	0x0da24260
   e6ccc:	7149f2ca 	.word	0x7149f2ca
   e6cd0:	33000000 	.word	0x33000000

000e6cd4 <copysignf>:
   e6cd4:	ee10 3a10 	vmov	r3, s0
   e6cd8:	f023 4200 	bic.w	r2, r3, #2147483648	; 0x80000000
   e6cdc:	ee10 3a90 	vmov	r3, s1
   e6ce0:	f003 4300 	and.w	r3, r3, #2147483648	; 0x80000000
   e6ce4:	4313      	orrs	r3, r2
   e6ce6:	ee00 3a10 	vmov	s0, r3
   e6cea:	4770      	bx	lr

000e6cec <__aeabi_llsl>:
   e6cec:	4091      	lsls	r1, r2
   e6cee:	1c03      	adds	r3, r0, #0
   e6cf0:	4090      	lsls	r0, r2
   e6cf2:	469c      	mov	ip, r3
   e6cf4:	3a20      	subs	r2, #32
   e6cf6:	4093      	lsls	r3, r2
   e6cf8:	4319      	orrs	r1, r3
   e6cfa:	4252      	negs	r2, r2
   e6cfc:	4663      	mov	r3, ip
   e6cfe:	40d3      	lsrs	r3, r2
   e6d00:	4319      	orrs	r1, r3
   e6d02:	4770      	bx	lr

000e6d04 <__aeabi_drsub>:
   e6d04:	f081 4100 	eor.w	r1, r1, #2147483648	; 0x80000000
   e6d08:	e002      	b.n	e6d10 <__adddf3>
   e6d0a:	bf00      	nop

000e6d0c <__aeabi_dsub>:
   e6d0c:	f083 4300 	eor.w	r3, r3, #2147483648	; 0x80000000

000e6d10 <__adddf3>:
   e6d10:	b530      	push	{r4, r5, lr}
   e6d12:	ea4f 0441 	mov.w	r4, r1, lsl #1
   e6d16:	ea4f 0543 	mov.w	r5, r3, lsl #1
   e6d1a:	ea94 0f05 	teq	r4, r5
   e6d1e:	bf08      	it	eq
   e6d20:	ea90 0f02 	teqeq	r0, r2
   e6d24:	bf1f      	itttt	ne
   e6d26:	ea54 0c00 	orrsne.w	ip, r4, r0
   e6d2a:	ea55 0c02 	orrsne.w	ip, r5, r2
   e6d2e:	ea7f 5c64 	mvnsne.w	ip, r4, asr #21
   e6d32:	ea7f 5c65 	mvnsne.w	ip, r5, asr #21
   e6d36:	f000 80e2 	beq.w	e6efe <__adddf3+0x1ee>
   e6d3a:	ea4f 5454 	mov.w	r4, r4, lsr #21
   e6d3e:	ebd4 5555 	rsbs	r5, r4, r5, lsr #21
   e6d42:	bfb8      	it	lt
   e6d44:	426d      	neglt	r5, r5
   e6d46:	dd0c      	ble.n	e6d62 <__adddf3+0x52>
   e6d48:	442c      	add	r4, r5
   e6d4a:	ea80 0202 	eor.w	r2, r0, r2
   e6d4e:	ea81 0303 	eor.w	r3, r1, r3
   e6d52:	ea82 0000 	eor.w	r0, r2, r0
   e6d56:	ea83 0101 	eor.w	r1, r3, r1
   e6d5a:	ea80 0202 	eor.w	r2, r0, r2
   e6d5e:	ea81 0303 	eor.w	r3, r1, r3
   e6d62:	2d36      	cmp	r5, #54	; 0x36
   e6d64:	bf88      	it	hi
   e6d66:	bd30      	pophi	{r4, r5, pc}
   e6d68:	f011 4f00 	tst.w	r1, #2147483648	; 0x80000000
   e6d6c:	ea4f 3101 	mov.w	r1, r1, lsl #12
   e6d70:	f44f 1c80 	mov.w	ip, #1048576	; 0x100000
   e6d74:	ea4c 3111 	orr.w	r1, ip, r1, lsr #12
   e6d78:	d002      	beq.n	e6d80 <__adddf3+0x70>
   e6d7a:	4240      	negs	r0, r0
   e6d7c:	eb61 0141 	sbc.w	r1, r1, r1, lsl #1
   e6d80:	f013 4f00 	tst.w	r3, #2147483648	; 0x80000000
   e6d84:	ea4f 3303 	mov.w	r3, r3, lsl #12
   e6d88:	ea4c 3313 	orr.w	r3, ip, r3, lsr #12
   e6d8c:	d002      	beq.n	e6d94 <__adddf3+0x84>
   e6d8e:	4252      	negs	r2, r2
   e6d90:	eb63 0343 	sbc.w	r3, r3, r3, lsl #1
   e6d94:	ea94 0f05 	teq	r4, r5
   e6d98:	f000 80a7 	beq.w	e6eea <__adddf3+0x1da>
   e6d9c:	f1a4 0401 	sub.w	r4, r4, #1
   e6da0:	f1d5 0e20 	rsbs	lr, r5, #32
   e6da4:	db0d      	blt.n	e6dc2 <__adddf3+0xb2>
   e6da6:	fa02 fc0e 	lsl.w	ip, r2, lr
   e6daa:	fa22 f205 	lsr.w	r2, r2, r5
   e6dae:	1880      	adds	r0, r0, r2
   e6db0:	f141 0100 	adc.w	r1, r1, #0
   e6db4:	fa03 f20e 	lsl.w	r2, r3, lr
   e6db8:	1880      	adds	r0, r0, r2
   e6dba:	fa43 f305 	asr.w	r3, r3, r5
   e6dbe:	4159      	adcs	r1, r3
   e6dc0:	e00e      	b.n	e6de0 <__adddf3+0xd0>
   e6dc2:	f1a5 0520 	sub.w	r5, r5, #32
   e6dc6:	f10e 0e20 	add.w	lr, lr, #32
   e6dca:	2a01      	cmp	r2, #1
   e6dcc:	fa03 fc0e 	lsl.w	ip, r3, lr
   e6dd0:	bf28      	it	cs
   e6dd2:	f04c 0c02 	orrcs.w	ip, ip, #2
   e6dd6:	fa43 f305 	asr.w	r3, r3, r5
   e6dda:	18c0      	adds	r0, r0, r3
   e6ddc:	eb51 71e3 	adcs.w	r1, r1, r3, asr #31
   e6de0:	f001 4500 	and.w	r5, r1, #2147483648	; 0x80000000
   e6de4:	d507      	bpl.n	e6df6 <__adddf3+0xe6>
   e6de6:	f04f 0e00 	mov.w	lr, #0
   e6dea:	f1dc 0c00 	rsbs	ip, ip, #0
   e6dee:	eb7e 0000 	sbcs.w	r0, lr, r0
   e6df2:	eb6e 0101 	sbc.w	r1, lr, r1
   e6df6:	f5b1 1f80 	cmp.w	r1, #1048576	; 0x100000
   e6dfa:	d31b      	bcc.n	e6e34 <__adddf3+0x124>
   e6dfc:	f5b1 1f00 	cmp.w	r1, #2097152	; 0x200000
   e6e00:	d30c      	bcc.n	e6e1c <__adddf3+0x10c>
   e6e02:	0849      	lsrs	r1, r1, #1
   e6e04:	ea5f 0030 	movs.w	r0, r0, rrx
   e6e08:	ea4f 0c3c 	mov.w	ip, ip, rrx
   e6e0c:	f104 0401 	add.w	r4, r4, #1
   e6e10:	ea4f 5244 	mov.w	r2, r4, lsl #21
   e6e14:	f512 0f80 	cmn.w	r2, #4194304	; 0x400000
   e6e18:	f080 809a 	bcs.w	e6f50 <__adddf3+0x240>
   e6e1c:	f1bc 4f00 	cmp.w	ip, #2147483648	; 0x80000000
   e6e20:	bf08      	it	eq
   e6e22:	ea5f 0c50 	movseq.w	ip, r0, lsr #1
   e6e26:	f150 0000 	adcs.w	r0, r0, #0
   e6e2a:	eb41 5104 	adc.w	r1, r1, r4, lsl #20
   e6e2e:	ea41 0105 	orr.w	r1, r1, r5
   e6e32:	bd30      	pop	{r4, r5, pc}
   e6e34:	ea5f 0c4c 	movs.w	ip, ip, lsl #1
   e6e38:	4140      	adcs	r0, r0
   e6e3a:	eb41 0101 	adc.w	r1, r1, r1
   e6e3e:	f411 1f80 	tst.w	r1, #1048576	; 0x100000
   e6e42:	f1a4 0401 	sub.w	r4, r4, #1
   e6e46:	d1e9      	bne.n	e6e1c <__adddf3+0x10c>
   e6e48:	f091 0f00 	teq	r1, #0
   e6e4c:	bf04      	itt	eq
   e6e4e:	4601      	moveq	r1, r0
   e6e50:	2000      	moveq	r0, #0
   e6e52:	fab1 f381 	clz	r3, r1
   e6e56:	bf08      	it	eq
   e6e58:	3320      	addeq	r3, #32
   e6e5a:	f1a3 030b 	sub.w	r3, r3, #11
   e6e5e:	f1b3 0220 	subs.w	r2, r3, #32
   e6e62:	da0c      	bge.n	e6e7e <__adddf3+0x16e>
   e6e64:	320c      	adds	r2, #12
   e6e66:	dd08      	ble.n	e6e7a <__adddf3+0x16a>
   e6e68:	f102 0c14 	add.w	ip, r2, #20
   e6e6c:	f1c2 020c 	rsb	r2, r2, #12
   e6e70:	fa01 f00c 	lsl.w	r0, r1, ip
   e6e74:	fa21 f102 	lsr.w	r1, r1, r2
   e6e78:	e00c      	b.n	e6e94 <__adddf3+0x184>
   e6e7a:	f102 0214 	add.w	r2, r2, #20
   e6e7e:	bfd8      	it	le
   e6e80:	f1c2 0c20 	rsble	ip, r2, #32
   e6e84:	fa01 f102 	lsl.w	r1, r1, r2
   e6e88:	fa20 fc0c 	lsr.w	ip, r0, ip
   e6e8c:	bfdc      	itt	le
   e6e8e:	ea41 010c 	orrle.w	r1, r1, ip
   e6e92:	4090      	lslle	r0, r2
   e6e94:	1ae4      	subs	r4, r4, r3
   e6e96:	bfa2      	ittt	ge
   e6e98:	eb01 5104 	addge.w	r1, r1, r4, lsl #20
   e6e9c:	4329      	orrge	r1, r5
   e6e9e:	bd30      	popge	{r4, r5, pc}
   e6ea0:	ea6f 0404 	mvn.w	r4, r4
   e6ea4:	3c1f      	subs	r4, #31
   e6ea6:	da1c      	bge.n	e6ee2 <__adddf3+0x1d2>
   e6ea8:	340c      	adds	r4, #12
   e6eaa:	dc0e      	bgt.n	e6eca <__adddf3+0x1ba>
   e6eac:	f104 0414 	add.w	r4, r4, #20
   e6eb0:	f1c4 0220 	rsb	r2, r4, #32
   e6eb4:	fa20 f004 	lsr.w	r0, r0, r4
   e6eb8:	fa01 f302 	lsl.w	r3, r1, r2
   e6ebc:	ea40 0003 	orr.w	r0, r0, r3
   e6ec0:	fa21 f304 	lsr.w	r3, r1, r4
   e6ec4:	ea45 0103 	orr.w	r1, r5, r3
   e6ec8:	bd30      	pop	{r4, r5, pc}
   e6eca:	f1c4 040c 	rsb	r4, r4, #12
   e6ece:	f1c4 0220 	rsb	r2, r4, #32
   e6ed2:	fa20 f002 	lsr.w	r0, r0, r2
   e6ed6:	fa01 f304 	lsl.w	r3, r1, r4
   e6eda:	ea40 0003 	orr.w	r0, r0, r3
   e6ede:	4629      	mov	r1, r5
   e6ee0:	bd30      	pop	{r4, r5, pc}
   e6ee2:	fa21 f004 	lsr.w	r0, r1, r4
   e6ee6:	4629      	mov	r1, r5
   e6ee8:	bd30      	pop	{r4, r5, pc}
   e6eea:	f094 0f00 	teq	r4, #0
   e6eee:	f483 1380 	eor.w	r3, r3, #1048576	; 0x100000
   e6ef2:	bf06      	itte	eq
   e6ef4:	f481 1180 	eoreq.w	r1, r1, #1048576	; 0x100000
   e6ef8:	3401      	addeq	r4, #1
   e6efa:	3d01      	subne	r5, #1
   e6efc:	e74e      	b.n	e6d9c <__adddf3+0x8c>
   e6efe:	ea7f 5c64 	mvns.w	ip, r4, asr #21
   e6f02:	bf18      	it	ne
   e6f04:	ea7f 5c65 	mvnsne.w	ip, r5, asr #21
   e6f08:	d029      	beq.n	e6f5e <__adddf3+0x24e>
   e6f0a:	ea94 0f05 	teq	r4, r5
   e6f0e:	bf08      	it	eq
   e6f10:	ea90 0f02 	teqeq	r0, r2
   e6f14:	d005      	beq.n	e6f22 <__adddf3+0x212>
   e6f16:	ea54 0c00 	orrs.w	ip, r4, r0
   e6f1a:	bf04      	itt	eq
   e6f1c:	4619      	moveq	r1, r3
   e6f1e:	4610      	moveq	r0, r2
   e6f20:	bd30      	pop	{r4, r5, pc}
   e6f22:	ea91 0f03 	teq	r1, r3
   e6f26:	bf1e      	ittt	ne
   e6f28:	2100      	movne	r1, #0
   e6f2a:	2000      	movne	r0, #0
   e6f2c:	bd30      	popne	{r4, r5, pc}
   e6f2e:	ea5f 5c54 	movs.w	ip, r4, lsr #21
   e6f32:	d105      	bne.n	e6f40 <__adddf3+0x230>
   e6f34:	0040      	lsls	r0, r0, #1
   e6f36:	4149      	adcs	r1, r1
   e6f38:	bf28      	it	cs
   e6f3a:	f041 4100 	orrcs.w	r1, r1, #2147483648	; 0x80000000
   e6f3e:	bd30      	pop	{r4, r5, pc}
   e6f40:	f514 0480 	adds.w	r4, r4, #4194304	; 0x400000
   e6f44:	bf3c      	itt	cc
   e6f46:	f501 1180 	addcc.w	r1, r1, #1048576	; 0x100000
   e6f4a:	bd30      	popcc	{r4, r5, pc}
   e6f4c:	f001 4500 	and.w	r5, r1, #2147483648	; 0x80000000
   e6f50:	f045 41fe 	orr.w	r1, r5, #2130706432	; 0x7f000000
   e6f54:	f441 0170 	orr.w	r1, r1, #15728640	; 0xf00000
   e6f58:	f04f 0000 	mov.w	r0, #0
   e6f5c:	bd30      	pop	{r4, r5, pc}
   e6f5e:	ea7f 5c64 	mvns.w	ip, r4, asr #21
   e6f62:	bf1a      	itte	ne
   e6f64:	4619      	movne	r1, r3
   e6f66:	4610      	movne	r0, r2
   e6f68:	ea7f 5c65 	mvnseq.w	ip, r5, asr #21
   e6f6c:	bf1c      	itt	ne
   e6f6e:	460b      	movne	r3, r1
   e6f70:	4602      	movne	r2, r0
   e6f72:	ea50 3401 	orrs.w	r4, r0, r1, lsl #12
   e6f76:	bf06      	itte	eq
   e6f78:	ea52 3503 	orrseq.w	r5, r2, r3, lsl #12
   e6f7c:	ea91 0f03 	teqeq	r1, r3
   e6f80:	f441 2100 	orrne.w	r1, r1, #524288	; 0x80000
   e6f84:	bd30      	pop	{r4, r5, pc}
   e6f86:	bf00      	nop

000e6f88 <__aeabi_ui2d>:
   e6f88:	f090 0f00 	teq	r0, #0
   e6f8c:	bf04      	itt	eq
   e6f8e:	2100      	moveq	r1, #0
   e6f90:	4770      	bxeq	lr
   e6f92:	b530      	push	{r4, r5, lr}
   e6f94:	f44f 6480 	mov.w	r4, #1024	; 0x400
   e6f98:	f104 0432 	add.w	r4, r4, #50	; 0x32
   e6f9c:	f04f 0500 	mov.w	r5, #0
   e6fa0:	f04f 0100 	mov.w	r1, #0
   e6fa4:	e750      	b.n	e6e48 <__adddf3+0x138>
   e6fa6:	bf00      	nop

000e6fa8 <__aeabi_i2d>:
   e6fa8:	f090 0f00 	teq	r0, #0
   e6fac:	bf04      	itt	eq
   e6fae:	2100      	moveq	r1, #0
   e6fb0:	4770      	bxeq	lr
   e6fb2:	b530      	push	{r4, r5, lr}
   e6fb4:	f44f 6480 	mov.w	r4, #1024	; 0x400
   e6fb8:	f104 0432 	add.w	r4, r4, #50	; 0x32
   e6fbc:	f010 4500 	ands.w	r5, r0, #2147483648	; 0x80000000
   e6fc0:	bf48      	it	mi
   e6fc2:	4240      	negmi	r0, r0
   e6fc4:	f04f 0100 	mov.w	r1, #0
   e6fc8:	e73e      	b.n	e6e48 <__adddf3+0x138>
   e6fca:	bf00      	nop

000e6fcc <__aeabi_f2d>:
   e6fcc:	0042      	lsls	r2, r0, #1
   e6fce:	ea4f 01e2 	mov.w	r1, r2, asr #3
   e6fd2:	ea4f 0131 	mov.w	r1, r1, rrx
   e6fd6:	ea4f 7002 	mov.w	r0, r2, lsl #28
   e6fda:	bf1f      	itttt	ne
   e6fdc:	f012 437f 	andsne.w	r3, r2, #4278190080	; 0xff000000
   e6fe0:	f093 4f7f 	teqne	r3, #4278190080	; 0xff000000
   e6fe4:	f081 5160 	eorne.w	r1, r1, #939524096	; 0x38000000
   e6fe8:	4770      	bxne	lr
   e6fea:	f092 0f00 	teq	r2, #0
   e6fee:	bf14      	ite	ne
   e6ff0:	f093 4f7f 	teqne	r3, #4278190080	; 0xff000000
   e6ff4:	4770      	bxeq	lr
   e6ff6:	b530      	push	{r4, r5, lr}
   e6ff8:	f44f 7460 	mov.w	r4, #896	; 0x380
   e6ffc:	f001 4500 	and.w	r5, r1, #2147483648	; 0x80000000
   e7000:	f021 4100 	bic.w	r1, r1, #2147483648	; 0x80000000
   e7004:	e720      	b.n	e6e48 <__adddf3+0x138>
   e7006:	bf00      	nop

000e7008 <__aeabi_ul2d>:
   e7008:	ea50 0201 	orrs.w	r2, r0, r1
   e700c:	bf08      	it	eq
   e700e:	4770      	bxeq	lr
   e7010:	b530      	push	{r4, r5, lr}
   e7012:	f04f 0500 	mov.w	r5, #0
   e7016:	e00a      	b.n	e702e <__aeabi_l2d+0x16>

000e7018 <__aeabi_l2d>:
   e7018:	ea50 0201 	orrs.w	r2, r0, r1
   e701c:	bf08      	it	eq
   e701e:	4770      	bxeq	lr
   e7020:	b530      	push	{r4, r5, lr}
   e7022:	f011 4500 	ands.w	r5, r1, #2147483648	; 0x80000000
   e7026:	d502      	bpl.n	e702e <__aeabi_l2d+0x16>
   e7028:	4240      	negs	r0, r0
   e702a:	eb61 0141 	sbc.w	r1, r1, r1, lsl #1
   e702e:	f44f 6480 	mov.w	r4, #1024	; 0x400
   e7032:	f104 0432 	add.w	r4, r4, #50	; 0x32
   e7036:	ea5f 5c91 	movs.w	ip, r1, lsr #22
   e703a:	f43f aedc 	beq.w	e6df6 <__adddf3+0xe6>
   e703e:	f04f 0203 	mov.w	r2, #3
   e7042:	ea5f 0cdc 	movs.w	ip, ip, lsr #3
   e7046:	bf18      	it	ne
   e7048:	3203      	addne	r2, #3
   e704a:	ea5f 0cdc 	movs.w	ip, ip, lsr #3
   e704e:	bf18      	it	ne
   e7050:	3203      	addne	r2, #3
   e7052:	eb02 02dc 	add.w	r2, r2, ip, lsr #3
   e7056:	f1c2 0320 	rsb	r3, r2, #32
   e705a:	fa00 fc03 	lsl.w	ip, r0, r3
   e705e:	fa20 f002 	lsr.w	r0, r0, r2
   e7062:	fa01 fe03 	lsl.w	lr, r1, r3
   e7066:	ea40 000e 	orr.w	r0, r0, lr
   e706a:	fa21 f102 	lsr.w	r1, r1, r2
   e706e:	4414      	add	r4, r2
   e7070:	e6c1      	b.n	e6df6 <__adddf3+0xe6>
   e7072:	bf00      	nop

000e7074 <__aeabi_dmul>:
   e7074:	b570      	push	{r4, r5, r6, lr}
   e7076:	f04f 0cff 	mov.w	ip, #255	; 0xff
   e707a:	f44c 6ce0 	orr.w	ip, ip, #1792	; 0x700
   e707e:	ea1c 5411 	ands.w	r4, ip, r1, lsr #20
   e7082:	bf1d      	ittte	ne
   e7084:	ea1c 5513 	andsne.w	r5, ip, r3, lsr #20
   e7088:	ea94 0f0c 	teqne	r4, ip
   e708c:	ea95 0f0c 	teqne	r5, ip
   e7090:	f000 f8de 	bleq	e7250 <__aeabi_dmul+0x1dc>
   e7094:	442c      	add	r4, r5
   e7096:	ea81 0603 	eor.w	r6, r1, r3
   e709a:	ea21 514c 	bic.w	r1, r1, ip, lsl #21
   e709e:	ea23 534c 	bic.w	r3, r3, ip, lsl #21
   e70a2:	ea50 3501 	orrs.w	r5, r0, r1, lsl #12
   e70a6:	bf18      	it	ne
   e70a8:	ea52 3503 	orrsne.w	r5, r2, r3, lsl #12
   e70ac:	f441 1180 	orr.w	r1, r1, #1048576	; 0x100000
   e70b0:	f443 1380 	orr.w	r3, r3, #1048576	; 0x100000
   e70b4:	d038      	beq.n	e7128 <__aeabi_dmul+0xb4>
   e70b6:	fba0 ce02 	umull	ip, lr, r0, r2
   e70ba:	f04f 0500 	mov.w	r5, #0
   e70be:	fbe1 e502 	umlal	lr, r5, r1, r2
   e70c2:	f006 4200 	and.w	r2, r6, #2147483648	; 0x80000000
   e70c6:	fbe0 e503 	umlal	lr, r5, r0, r3
   e70ca:	f04f 0600 	mov.w	r6, #0
   e70ce:	fbe1 5603 	umlal	r5, r6, r1, r3
   e70d2:	f09c 0f00 	teq	ip, #0
   e70d6:	bf18      	it	ne
   e70d8:	f04e 0e01 	orrne.w	lr, lr, #1
   e70dc:	f1a4 04ff 	sub.w	r4, r4, #255	; 0xff
   e70e0:	f5b6 7f00 	cmp.w	r6, #512	; 0x200
   e70e4:	f564 7440 	sbc.w	r4, r4, #768	; 0x300
   e70e8:	d204      	bcs.n	e70f4 <__aeabi_dmul+0x80>
   e70ea:	ea5f 0e4e 	movs.w	lr, lr, lsl #1
   e70ee:	416d      	adcs	r5, r5
   e70f0:	eb46 0606 	adc.w	r6, r6, r6
   e70f4:	ea42 21c6 	orr.w	r1, r2, r6, lsl #11
   e70f8:	ea41 5155 	orr.w	r1, r1, r5, lsr #21
   e70fc:	ea4f 20c5 	mov.w	r0, r5, lsl #11
   e7100:	ea40 505e 	orr.w	r0, r0, lr, lsr #21
   e7104:	ea4f 2ece 	mov.w	lr, lr, lsl #11
   e7108:	f1b4 0cfd 	subs.w	ip, r4, #253	; 0xfd
   e710c:	bf88      	it	hi
   e710e:	f5bc 6fe0 	cmphi.w	ip, #1792	; 0x700
   e7112:	d81e      	bhi.n	e7152 <__aeabi_dmul+0xde>
   e7114:	f1be 4f00 	cmp.w	lr, #2147483648	; 0x80000000
   e7118:	bf08      	it	eq
   e711a:	ea5f 0e50 	movseq.w	lr, r0, lsr #1
   e711e:	f150 0000 	adcs.w	r0, r0, #0
   e7122:	eb41 5104 	adc.w	r1, r1, r4, lsl #20
   e7126:	bd70      	pop	{r4, r5, r6, pc}
   e7128:	f006 4600 	and.w	r6, r6, #2147483648	; 0x80000000
   e712c:	ea46 0101 	orr.w	r1, r6, r1
   e7130:	ea40 0002 	orr.w	r0, r0, r2
   e7134:	ea81 0103 	eor.w	r1, r1, r3
   e7138:	ebb4 045c 	subs.w	r4, r4, ip, lsr #1
   e713c:	bfc2      	ittt	gt
   e713e:	ebd4 050c 	rsbsgt	r5, r4, ip
   e7142:	ea41 5104 	orrgt.w	r1, r1, r4, lsl #20
   e7146:	bd70      	popgt	{r4, r5, r6, pc}
   e7148:	f441 1180 	orr.w	r1, r1, #1048576	; 0x100000
   e714c:	f04f 0e00 	mov.w	lr, #0
   e7150:	3c01      	subs	r4, #1
   e7152:	f300 80ab 	bgt.w	e72ac <__aeabi_dmul+0x238>
   e7156:	f114 0f36 	cmn.w	r4, #54	; 0x36
   e715a:	bfde      	ittt	le
   e715c:	2000      	movle	r0, #0
   e715e:	f001 4100 	andle.w	r1, r1, #2147483648	; 0x80000000
   e7162:	bd70      	pople	{r4, r5, r6, pc}
   e7164:	f1c4 0400 	rsb	r4, r4, #0
   e7168:	3c20      	subs	r4, #32
   e716a:	da35      	bge.n	e71d8 <__aeabi_dmul+0x164>
   e716c:	340c      	adds	r4, #12
   e716e:	dc1b      	bgt.n	e71a8 <__aeabi_dmul+0x134>
   e7170:	f104 0414 	add.w	r4, r4, #20
   e7174:	f1c4 0520 	rsb	r5, r4, #32
   e7178:	fa00 f305 	lsl.w	r3, r0, r5
   e717c:	fa20 f004 	lsr.w	r0, r0, r4
   e7180:	fa01 f205 	lsl.w	r2, r1, r5
   e7184:	ea40 0002 	orr.w	r0, r0, r2
   e7188:	f001 4200 	and.w	r2, r1, #2147483648	; 0x80000000
   e718c:	f021 4100 	bic.w	r1, r1, #2147483648	; 0x80000000
   e7190:	eb10 70d3 	adds.w	r0, r0, r3, lsr #31
   e7194:	fa21 f604 	lsr.w	r6, r1, r4
   e7198:	eb42 0106 	adc.w	r1, r2, r6
   e719c:	ea5e 0e43 	orrs.w	lr, lr, r3, lsl #1
   e71a0:	bf08      	it	eq
   e71a2:	ea20 70d3 	biceq.w	r0, r0, r3, lsr #31
   e71a6:	bd70      	pop	{r4, r5, r6, pc}
   e71a8:	f1c4 040c 	rsb	r4, r4, #12
   e71ac:	f1c4 0520 	rsb	r5, r4, #32
   e71b0:	fa00 f304 	lsl.w	r3, r0, r4
   e71b4:	fa20 f005 	lsr.w	r0, r0, r5
   e71b8:	fa01 f204 	lsl.w	r2, r1, r4
   e71bc:	ea40 0002 	orr.w	r0, r0, r2
   e71c0:	f001 4100 	and.w	r1, r1, #2147483648	; 0x80000000
   e71c4:	eb10 70d3 	adds.w	r0, r0, r3, lsr #31
   e71c8:	f141 0100 	adc.w	r1, r1, #0
   e71cc:	ea5e 0e43 	orrs.w	lr, lr, r3, lsl #1
   e71d0:	bf08      	it	eq
   e71d2:	ea20 70d3 	biceq.w	r0, r0, r3, lsr #31
   e71d6:	bd70      	pop	{r4, r5, r6, pc}
   e71d8:	f1c4 0520 	rsb	r5, r4, #32
   e71dc:	fa00 f205 	lsl.w	r2, r0, r5
   e71e0:	ea4e 0e02 	orr.w	lr, lr, r2
   e71e4:	fa20 f304 	lsr.w	r3, r0, r4
   e71e8:	fa01 f205 	lsl.w	r2, r1, r5
   e71ec:	ea43 0302 	orr.w	r3, r3, r2
   e71f0:	fa21 f004 	lsr.w	r0, r1, r4
   e71f4:	f001 4100 	and.w	r1, r1, #2147483648	; 0x80000000
   e71f8:	fa21 f204 	lsr.w	r2, r1, r4
   e71fc:	ea20 0002 	bic.w	r0, r0, r2
   e7200:	eb00 70d3 	add.w	r0, r0, r3, lsr #31
   e7204:	ea5e 0e43 	orrs.w	lr, lr, r3, lsl #1
   e7208:	bf08      	it	eq
   e720a:	ea20 70d3 	biceq.w	r0, r0, r3, lsr #31
   e720e:	bd70      	pop	{r4, r5, r6, pc}
   e7210:	f094 0f00 	teq	r4, #0
   e7214:	d10f      	bne.n	e7236 <__aeabi_dmul+0x1c2>
   e7216:	f001 4600 	and.w	r6, r1, #2147483648	; 0x80000000
   e721a:	0040      	lsls	r0, r0, #1
   e721c:	eb41 0101 	adc.w	r1, r1, r1
   e7220:	f411 1f80 	tst.w	r1, #1048576	; 0x100000
   e7224:	bf08      	it	eq
   e7226:	3c01      	subeq	r4, #1
   e7228:	d0f7      	beq.n	e721a <__aeabi_dmul+0x1a6>
   e722a:	ea41 0106 	orr.w	r1, r1, r6
   e722e:	f095 0f00 	teq	r5, #0
   e7232:	bf18      	it	ne
   e7234:	4770      	bxne	lr
   e7236:	f003 4600 	and.w	r6, r3, #2147483648	; 0x80000000
   e723a:	0052      	lsls	r2, r2, #1
   e723c:	eb43 0303 	adc.w	r3, r3, r3
   e7240:	f413 1f80 	tst.w	r3, #1048576	; 0x100000
   e7244:	bf08      	it	eq
   e7246:	3d01      	subeq	r5, #1
   e7248:	d0f7      	beq.n	e723a <__aeabi_dmul+0x1c6>
   e724a:	ea43 0306 	orr.w	r3, r3, r6
   e724e:	4770      	bx	lr
   e7250:	ea94 0f0c 	teq	r4, ip
   e7254:	ea0c 5513 	and.w	r5, ip, r3, lsr #20
   e7258:	bf18      	it	ne
   e725a:	ea95 0f0c 	teqne	r5, ip
   e725e:	d00c      	beq.n	e727a <__aeabi_dmul+0x206>
   e7260:	ea50 0641 	orrs.w	r6, r0, r1, lsl #1
   e7264:	bf18      	it	ne
   e7266:	ea52 0643 	orrsne.w	r6, r2, r3, lsl #1
   e726a:	d1d1      	bne.n	e7210 <__aeabi_dmul+0x19c>
   e726c:	ea81 0103 	eor.w	r1, r1, r3
   e7270:	f001 4100 	and.w	r1, r1, #2147483648	; 0x80000000
   e7274:	f04f 0000 	mov.w	r0, #0
   e7278:	bd70      	pop	{r4, r5, r6, pc}
   e727a:	ea50 0641 	orrs.w	r6, r0, r1, lsl #1
   e727e:	bf06      	itte	eq
   e7280:	4610      	moveq	r0, r2
   e7282:	4619      	moveq	r1, r3
   e7284:	ea52 0643 	orrsne.w	r6, r2, r3, lsl #1
   e7288:	d019      	beq.n	e72be <__aeabi_dmul+0x24a>
   e728a:	ea94 0f0c 	teq	r4, ip
   e728e:	d102      	bne.n	e7296 <__aeabi_dmul+0x222>
   e7290:	ea50 3601 	orrs.w	r6, r0, r1, lsl #12
   e7294:	d113      	bne.n	e72be <__aeabi_dmul+0x24a>
   e7296:	ea95 0f0c 	teq	r5, ip
   e729a:	d105      	bne.n	e72a8 <__aeabi_dmul+0x234>
   e729c:	ea52 3603 	orrs.w	r6, r2, r3, lsl #12
   e72a0:	bf1c      	itt	ne
   e72a2:	4610      	movne	r0, r2
   e72a4:	4619      	movne	r1, r3
   e72a6:	d10a      	bne.n	e72be <__aeabi_dmul+0x24a>
   e72a8:	ea81 0103 	eor.w	r1, r1, r3
   e72ac:	f001 4100 	and.w	r1, r1, #2147483648	; 0x80000000
   e72b0:	f041 41fe 	orr.w	r1, r1, #2130706432	; 0x7f000000
   e72b4:	f441 0170 	orr.w	r1, r1, #15728640	; 0xf00000
   e72b8:	f04f 0000 	mov.w	r0, #0
   e72bc:	bd70      	pop	{r4, r5, r6, pc}
   e72be:	f041 41fe 	orr.w	r1, r1, #2130706432	; 0x7f000000
   e72c2:	f441 0178 	orr.w	r1, r1, #16252928	; 0xf80000
   e72c6:	bd70      	pop	{r4, r5, r6, pc}

000e72c8 <__aeabi_ddiv>:
   e72c8:	b570      	push	{r4, r5, r6, lr}
   e72ca:	f04f 0cff 	mov.w	ip, #255	; 0xff
   e72ce:	f44c 6ce0 	orr.w	ip, ip, #1792	; 0x700
   e72d2:	ea1c 5411 	ands.w	r4, ip, r1, lsr #20
   e72d6:	bf1d      	ittte	ne
   e72d8:	ea1c 5513 	andsne.w	r5, ip, r3, lsr #20
   e72dc:	ea94 0f0c 	teqne	r4, ip
   e72e0:	ea95 0f0c 	teqne	r5, ip
   e72e4:	f000 f8a7 	bleq	e7436 <__aeabi_ddiv+0x16e>
   e72e8:	eba4 0405 	sub.w	r4, r4, r5
   e72ec:	ea81 0e03 	eor.w	lr, r1, r3
   e72f0:	ea52 3503 	orrs.w	r5, r2, r3, lsl #12
   e72f4:	ea4f 3101 	mov.w	r1, r1, lsl #12
   e72f8:	f000 8088 	beq.w	e740c <__aeabi_ddiv+0x144>
   e72fc:	ea4f 3303 	mov.w	r3, r3, lsl #12
   e7300:	f04f 5580 	mov.w	r5, #268435456	; 0x10000000
   e7304:	ea45 1313 	orr.w	r3, r5, r3, lsr #4
   e7308:	ea43 6312 	orr.w	r3, r3, r2, lsr #24
   e730c:	ea4f 2202 	mov.w	r2, r2, lsl #8
   e7310:	ea45 1511 	orr.w	r5, r5, r1, lsr #4
   e7314:	ea45 6510 	orr.w	r5, r5, r0, lsr #24
   e7318:	ea4f 2600 	mov.w	r6, r0, lsl #8
   e731c:	f00e 4100 	and.w	r1, lr, #2147483648	; 0x80000000
   e7320:	429d      	cmp	r5, r3
   e7322:	bf08      	it	eq
   e7324:	4296      	cmpeq	r6, r2
   e7326:	f144 04fd 	adc.w	r4, r4, #253	; 0xfd
   e732a:	f504 7440 	add.w	r4, r4, #768	; 0x300
   e732e:	d202      	bcs.n	e7336 <__aeabi_ddiv+0x6e>
   e7330:	085b      	lsrs	r3, r3, #1
   e7332:	ea4f 0232 	mov.w	r2, r2, rrx
   e7336:	1ab6      	subs	r6, r6, r2
   e7338:	eb65 0503 	sbc.w	r5, r5, r3
   e733c:	085b      	lsrs	r3, r3, #1
   e733e:	ea4f 0232 	mov.w	r2, r2, rrx
   e7342:	f44f 1080 	mov.w	r0, #1048576	; 0x100000
   e7346:	f44f 2c00 	mov.w	ip, #524288	; 0x80000
   e734a:	ebb6 0e02 	subs.w	lr, r6, r2
   e734e:	eb75 0e03 	sbcs.w	lr, r5, r3
   e7352:	bf22      	ittt	cs
   e7354:	1ab6      	subcs	r6, r6, r2
   e7356:	4675      	movcs	r5, lr
   e7358:	ea40 000c 	orrcs.w	r0, r0, ip
   e735c:	085b      	lsrs	r3, r3, #1
   e735e:	ea4f 0232 	mov.w	r2, r2, rrx
   e7362:	ebb6 0e02 	subs.w	lr, r6, r2
   e7366:	eb75 0e03 	sbcs.w	lr, r5, r3
   e736a:	bf22      	ittt	cs
   e736c:	1ab6      	subcs	r6, r6, r2
   e736e:	4675      	movcs	r5, lr
   e7370:	ea40 005c 	orrcs.w	r0, r0, ip, lsr #1
   e7374:	085b      	lsrs	r3, r3, #1
   e7376:	ea4f 0232 	mov.w	r2, r2, rrx
   e737a:	ebb6 0e02 	subs.w	lr, r6, r2
   e737e:	eb75 0e03 	sbcs.w	lr, r5, r3
   e7382:	bf22      	ittt	cs
   e7384:	1ab6      	subcs	r6, r6, r2
   e7386:	4675      	movcs	r5, lr
   e7388:	ea40 009c 	orrcs.w	r0, r0, ip, lsr #2
   e738c:	085b      	lsrs	r3, r3, #1
   e738e:	ea4f 0232 	mov.w	r2, r2, rrx
   e7392:	ebb6 0e02 	subs.w	lr, r6, r2
   e7396:	eb75 0e03 	sbcs.w	lr, r5, r3
   e739a:	bf22      	ittt	cs
   e739c:	1ab6      	subcs	r6, r6, r2
   e739e:	4675      	movcs	r5, lr
   e73a0:	ea40 00dc 	orrcs.w	r0, r0, ip, lsr #3
   e73a4:	ea55 0e06 	orrs.w	lr, r5, r6
   e73a8:	d018      	beq.n	e73dc <__aeabi_ddiv+0x114>
   e73aa:	ea4f 1505 	mov.w	r5, r5, lsl #4
   e73ae:	ea45 7516 	orr.w	r5, r5, r6, lsr #28
   e73b2:	ea4f 1606 	mov.w	r6, r6, lsl #4
   e73b6:	ea4f 03c3 	mov.w	r3, r3, lsl #3
   e73ba:	ea43 7352 	orr.w	r3, r3, r2, lsr #29
   e73be:	ea4f 02c2 	mov.w	r2, r2, lsl #3
   e73c2:	ea5f 1c1c 	movs.w	ip, ip, lsr #4
   e73c6:	d1c0      	bne.n	e734a <__aeabi_ddiv+0x82>
   e73c8:	f411 1f80 	tst.w	r1, #1048576	; 0x100000
   e73cc:	d10b      	bne.n	e73e6 <__aeabi_ddiv+0x11e>
   e73ce:	ea41 0100 	orr.w	r1, r1, r0
   e73d2:	f04f 0000 	mov.w	r0, #0
   e73d6:	f04f 4c00 	mov.w	ip, #2147483648	; 0x80000000
   e73da:	e7b6      	b.n	e734a <__aeabi_ddiv+0x82>
   e73dc:	f411 1f80 	tst.w	r1, #1048576	; 0x100000
   e73e0:	bf04      	itt	eq
   e73e2:	4301      	orreq	r1, r0
   e73e4:	2000      	moveq	r0, #0
   e73e6:	f1b4 0cfd 	subs.w	ip, r4, #253	; 0xfd
   e73ea:	bf88      	it	hi
   e73ec:	f5bc 6fe0 	cmphi.w	ip, #1792	; 0x700
   e73f0:	f63f aeaf 	bhi.w	e7152 <__aeabi_dmul+0xde>
   e73f4:	ebb5 0c03 	subs.w	ip, r5, r3
   e73f8:	bf04      	itt	eq
   e73fa:	ebb6 0c02 	subseq.w	ip, r6, r2
   e73fe:	ea5f 0c50 	movseq.w	ip, r0, lsr #1
   e7402:	f150 0000 	adcs.w	r0, r0, #0
   e7406:	eb41 5104 	adc.w	r1, r1, r4, lsl #20
   e740a:	bd70      	pop	{r4, r5, r6, pc}
   e740c:	f00e 4e00 	and.w	lr, lr, #2147483648	; 0x80000000
   e7410:	ea4e 3111 	orr.w	r1, lr, r1, lsr #12
   e7414:	eb14 045c 	adds.w	r4, r4, ip, lsr #1
   e7418:	bfc2      	ittt	gt
   e741a:	ebd4 050c 	rsbsgt	r5, r4, ip
   e741e:	ea41 5104 	orrgt.w	r1, r1, r4, lsl #20
   e7422:	bd70      	popgt	{r4, r5, r6, pc}
   e7424:	f441 1180 	orr.w	r1, r1, #1048576	; 0x100000
   e7428:	f04f 0e00 	mov.w	lr, #0
   e742c:	3c01      	subs	r4, #1
   e742e:	e690      	b.n	e7152 <__aeabi_dmul+0xde>
   e7430:	ea45 0e06 	orr.w	lr, r5, r6
   e7434:	e68d      	b.n	e7152 <__aeabi_dmul+0xde>
   e7436:	ea0c 5513 	and.w	r5, ip, r3, lsr #20
   e743a:	ea94 0f0c 	teq	r4, ip
   e743e:	bf08      	it	eq
   e7440:	ea95 0f0c 	teqeq	r5, ip
   e7444:	f43f af3b 	beq.w	e72be <__aeabi_dmul+0x24a>
   e7448:	ea94 0f0c 	teq	r4, ip
   e744c:	d10a      	bne.n	e7464 <__aeabi_ddiv+0x19c>
   e744e:	ea50 3401 	orrs.w	r4, r0, r1, lsl #12
   e7452:	f47f af34 	bne.w	e72be <__aeabi_dmul+0x24a>
   e7456:	ea95 0f0c 	teq	r5, ip
   e745a:	f47f af25 	bne.w	e72a8 <__aeabi_dmul+0x234>
   e745e:	4610      	mov	r0, r2
   e7460:	4619      	mov	r1, r3
   e7462:	e72c      	b.n	e72be <__aeabi_dmul+0x24a>
   e7464:	ea95 0f0c 	teq	r5, ip
   e7468:	d106      	bne.n	e7478 <__aeabi_ddiv+0x1b0>
   e746a:	ea52 3503 	orrs.w	r5, r2, r3, lsl #12
   e746e:	f43f aefd 	beq.w	e726c <__aeabi_dmul+0x1f8>
   e7472:	4610      	mov	r0, r2
   e7474:	4619      	mov	r1, r3
   e7476:	e722      	b.n	e72be <__aeabi_dmul+0x24a>
   e7478:	ea50 0641 	orrs.w	r6, r0, r1, lsl #1
   e747c:	bf18      	it	ne
   e747e:	ea52 0643 	orrsne.w	r6, r2, r3, lsl #1
   e7482:	f47f aec5 	bne.w	e7210 <__aeabi_dmul+0x19c>
   e7486:	ea50 0441 	orrs.w	r4, r0, r1, lsl #1
   e748a:	f47f af0d 	bne.w	e72a8 <__aeabi_dmul+0x234>
   e748e:	ea52 0543 	orrs.w	r5, r2, r3, lsl #1
   e7492:	f47f aeeb 	bne.w	e726c <__aeabi_dmul+0x1f8>
   e7496:	e712      	b.n	e72be <__aeabi_dmul+0x24a>

000e7498 <__gedf2>:
   e7498:	f04f 3cff 	mov.w	ip, #4294967295	; 0xffffffff
   e749c:	e006      	b.n	e74ac <__cmpdf2+0x4>
   e749e:	bf00      	nop

000e74a0 <__ledf2>:
   e74a0:	f04f 0c01 	mov.w	ip, #1
   e74a4:	e002      	b.n	e74ac <__cmpdf2+0x4>
   e74a6:	bf00      	nop

000e74a8 <__cmpdf2>:
   e74a8:	f04f 0c01 	mov.w	ip, #1
   e74ac:	f84d cd04 	str.w	ip, [sp, #-4]!
   e74b0:	ea4f 0c41 	mov.w	ip, r1, lsl #1
   e74b4:	ea7f 5c6c 	mvns.w	ip, ip, asr #21
   e74b8:	ea4f 0c43 	mov.w	ip, r3, lsl #1
   e74bc:	bf18      	it	ne
   e74be:	ea7f 5c6c 	mvnsne.w	ip, ip, asr #21
   e74c2:	d01b      	beq.n	e74fc <__cmpdf2+0x54>
   e74c4:	b001      	add	sp, #4
   e74c6:	ea50 0c41 	orrs.w	ip, r0, r1, lsl #1
   e74ca:	bf0c      	ite	eq
   e74cc:	ea52 0c43 	orrseq.w	ip, r2, r3, lsl #1
   e74d0:	ea91 0f03 	teqne	r1, r3
   e74d4:	bf02      	ittt	eq
   e74d6:	ea90 0f02 	teqeq	r0, r2
   e74da:	2000      	moveq	r0, #0
   e74dc:	4770      	bxeq	lr
   e74de:	f110 0f00 	cmn.w	r0, #0
   e74e2:	ea91 0f03 	teq	r1, r3
   e74e6:	bf58      	it	pl
   e74e8:	4299      	cmppl	r1, r3
   e74ea:	bf08      	it	eq
   e74ec:	4290      	cmpeq	r0, r2
   e74ee:	bf2c      	ite	cs
   e74f0:	17d8      	asrcs	r0, r3, #31
   e74f2:	ea6f 70e3 	mvncc.w	r0, r3, asr #31
   e74f6:	f040 0001 	orr.w	r0, r0, #1
   e74fa:	4770      	bx	lr
   e74fc:	ea4f 0c41 	mov.w	ip, r1, lsl #1
   e7500:	ea7f 5c6c 	mvns.w	ip, ip, asr #21
   e7504:	d102      	bne.n	e750c <__cmpdf2+0x64>
   e7506:	ea50 3c01 	orrs.w	ip, r0, r1, lsl #12
   e750a:	d107      	bne.n	e751c <__cmpdf2+0x74>
   e750c:	ea4f 0c43 	mov.w	ip, r3, lsl #1
   e7510:	ea7f 5c6c 	mvns.w	ip, ip, asr #21
   e7514:	d1d6      	bne.n	e74c4 <__cmpdf2+0x1c>
   e7516:	ea52 3c03 	orrs.w	ip, r2, r3, lsl #12
   e751a:	d0d3      	beq.n	e74c4 <__cmpdf2+0x1c>
   e751c:	f85d 0b04 	ldr.w	r0, [sp], #4
   e7520:	4770      	bx	lr
   e7522:	bf00      	nop

000e7524 <__aeabi_cdrcmple>:
   e7524:	4684      	mov	ip, r0
   e7526:	4610      	mov	r0, r2
   e7528:	4662      	mov	r2, ip
   e752a:	468c      	mov	ip, r1
   e752c:	4619      	mov	r1, r3
   e752e:	4663      	mov	r3, ip
   e7530:	e000      	b.n	e7534 <__aeabi_cdcmpeq>
   e7532:	bf00      	nop

000e7534 <__aeabi_cdcmpeq>:
   e7534:	b501      	push	{r0, lr}
   e7536:	f7ff ffb7 	bl	e74a8 <__cmpdf2>
   e753a:	2800      	cmp	r0, #0
   e753c:	bf48      	it	mi
   e753e:	f110 0f00 	cmnmi.w	r0, #0
   e7542:	bd01      	pop	{r0, pc}

000e7544 <__aeabi_dcmpeq>:
   e7544:	f84d ed08 	str.w	lr, [sp, #-8]!
   e7548:	f7ff fff4 	bl	e7534 <__aeabi_cdcmpeq>
   e754c:	bf0c      	ite	eq
   e754e:	2001      	moveq	r0, #1
   e7550:	2000      	movne	r0, #0
   e7552:	f85d fb08 	ldr.w	pc, [sp], #8
   e7556:	bf00      	nop

000e7558 <__aeabi_dcmplt>:
   e7558:	f84d ed08 	str.w	lr, [sp, #-8]!
   e755c:	f7ff ffea 	bl	e7534 <__aeabi_cdcmpeq>
   e7560:	bf34      	ite	cc
   e7562:	2001      	movcc	r0, #1
   e7564:	2000      	movcs	r0, #0
   e7566:	f85d fb08 	ldr.w	pc, [sp], #8
   e756a:	bf00      	nop

000e756c <__aeabi_dcmple>:
   e756c:	f84d ed08 	str.w	lr, [sp, #-8]!
   e7570:	f7ff ffe0 	bl	e7534 <__aeabi_cdcmpeq>
   e7574:	bf94      	ite	ls
   e7576:	2001      	movls	r0, #1
   e7578:	2000      	movhi	r0, #0
   e757a:	f85d fb08 	ldr.w	pc, [sp], #8
   e757e:	bf00      	nop

000e7580 <__aeabi_dcmpge>:
   e7580:	f84d ed08 	str.w	lr, [sp, #-8]!
   e7584:	f7ff ffce 	bl	e7524 <__aeabi_cdrcmple>
   e7588:	bf94      	ite	ls
   e758a:	2001      	movls	r0, #1
   e758c:	2000      	movhi	r0, #0
   e758e:	f85d fb08 	ldr.w	pc, [sp], #8
   e7592:	bf00      	nop

000e7594 <__aeabi_dcmpgt>:
   e7594:	f84d ed08 	str.w	lr, [sp, #-8]!
   e7598:	f7ff ffc4 	bl	e7524 <__aeabi_cdrcmple>
   e759c:	bf34      	ite	cc
   e759e:	2001      	movcc	r0, #1
   e75a0:	2000      	movcs	r0, #0
   e75a2:	f85d fb08 	ldr.w	pc, [sp], #8
   e75a6:	bf00      	nop

000e75a8 <__aeabi_d2iz>:
   e75a8:	ea4f 0241 	mov.w	r2, r1, lsl #1
   e75ac:	f512 1200 	adds.w	r2, r2, #2097152	; 0x200000
   e75b0:	d215      	bcs.n	e75de <__aeabi_d2iz+0x36>
   e75b2:	d511      	bpl.n	e75d8 <__aeabi_d2iz+0x30>
   e75b4:	f46f 7378 	mvn.w	r3, #992	; 0x3e0
   e75b8:	ebb3 5262 	subs.w	r2, r3, r2, asr #21
   e75bc:	d912      	bls.n	e75e4 <__aeabi_d2iz+0x3c>
   e75be:	ea4f 23c1 	mov.w	r3, r1, lsl #11
   e75c2:	f043 4300 	orr.w	r3, r3, #2147483648	; 0x80000000
   e75c6:	ea43 5350 	orr.w	r3, r3, r0, lsr #21
   e75ca:	f011 4f00 	tst.w	r1, #2147483648	; 0x80000000
   e75ce:	fa23 f002 	lsr.w	r0, r3, r2
   e75d2:	bf18      	it	ne
   e75d4:	4240      	negne	r0, r0
   e75d6:	4770      	bx	lr
   e75d8:	f04f 0000 	mov.w	r0, #0
   e75dc:	4770      	bx	lr
   e75de:	ea50 3001 	orrs.w	r0, r0, r1, lsl #12
   e75e2:	d105      	bne.n	e75f0 <__aeabi_d2iz+0x48>
   e75e4:	f011 4000 	ands.w	r0, r1, #2147483648	; 0x80000000
   e75e8:	bf08      	it	eq
   e75ea:	f06f 4000 	mvneq.w	r0, #2147483648	; 0x80000000
   e75ee:	4770      	bx	lr
   e75f0:	f04f 0000 	mov.w	r0, #0
   e75f4:	4770      	bx	lr
   e75f6:	bf00      	nop

000e75f8 <__aeabi_d2uiz>:
   e75f8:	004a      	lsls	r2, r1, #1
   e75fa:	d211      	bcs.n	e7620 <__aeabi_d2uiz+0x28>
   e75fc:	f512 1200 	adds.w	r2, r2, #2097152	; 0x200000
   e7600:	d211      	bcs.n	e7626 <__aeabi_d2uiz+0x2e>
   e7602:	d50d      	bpl.n	e7620 <__aeabi_d2uiz+0x28>
   e7604:	f46f 7378 	mvn.w	r3, #992	; 0x3e0
   e7608:	ebb3 5262 	subs.w	r2, r3, r2, asr #21
   e760c:	d40e      	bmi.n	e762c <__aeabi_d2uiz+0x34>
   e760e:	ea4f 23c1 	mov.w	r3, r1, lsl #11
   e7612:	f043 4300 	orr.w	r3, r3, #2147483648	; 0x80000000
   e7616:	ea43 5350 	orr.w	r3, r3, r0, lsr #21
   e761a:	fa23 f002 	lsr.w	r0, r3, r2
   e761e:	4770      	bx	lr
   e7620:	f04f 0000 	mov.w	r0, #0
   e7624:	4770      	bx	lr
   e7626:	ea50 3001 	orrs.w	r0, r0, r1, lsl #12
   e762a:	d102      	bne.n	e7632 <__aeabi_d2uiz+0x3a>
   e762c:	f04f 30ff 	mov.w	r0, #4294967295	; 0xffffffff
   e7630:	4770      	bx	lr
   e7632:	f04f 0000 	mov.w	r0, #0
   e7636:	4770      	bx	lr

000e7638 <__aeabi_d2f>:
   e7638:	ea4f 0241 	mov.w	r2, r1, lsl #1
   e763c:	f1b2 43e0 	subs.w	r3, r2, #1879048192	; 0x70000000
   e7640:	bf24      	itt	cs
   e7642:	f5b3 1c00 	subscs.w	ip, r3, #2097152	; 0x200000
   e7646:	f1dc 5cfe 	rsbscs	ip, ip, #532676608	; 0x1fc00000
   e764a:	d90d      	bls.n	e7668 <__aeabi_d2f+0x30>
   e764c:	f001 4c00 	and.w	ip, r1, #2147483648	; 0x80000000
   e7650:	ea4f 02c0 	mov.w	r2, r0, lsl #3
   e7654:	ea4c 7050 	orr.w	r0, ip, r0, lsr #29
   e7658:	f1b2 4f00 	cmp.w	r2, #2147483648	; 0x80000000
   e765c:	eb40 0083 	adc.w	r0, r0, r3, lsl #2
   e7660:	bf08      	it	eq
   e7662:	f020 0001 	biceq.w	r0, r0, #1
   e7666:	4770      	bx	lr
   e7668:	f011 4f80 	tst.w	r1, #1073741824	; 0x40000000
   e766c:	d121      	bne.n	e76b2 <__aeabi_d2f+0x7a>
   e766e:	f113 7238 	adds.w	r2, r3, #48234496	; 0x2e00000
   e7672:	bfbc      	itt	lt
   e7674:	f001 4000 	andlt.w	r0, r1, #2147483648	; 0x80000000
   e7678:	4770      	bxlt	lr
   e767a:	f441 1180 	orr.w	r1, r1, #1048576	; 0x100000
   e767e:	ea4f 5252 	mov.w	r2, r2, lsr #21
   e7682:	f1c2 0218 	rsb	r2, r2, #24
   e7686:	f1c2 0c20 	rsb	ip, r2, #32
   e768a:	fa10 f30c 	lsls.w	r3, r0, ip
   e768e:	fa20 f002 	lsr.w	r0, r0, r2
   e7692:	bf18      	it	ne
   e7694:	f040 0001 	orrne.w	r0, r0, #1
   e7698:	ea4f 23c1 	mov.w	r3, r1, lsl #11
   e769c:	ea4f 23d3 	mov.w	r3, r3, lsr #11
   e76a0:	fa03 fc0c 	lsl.w	ip, r3, ip
   e76a4:	ea40 000c 	orr.w	r0, r0, ip
   e76a8:	fa23 f302 	lsr.w	r3, r3, r2
   e76ac:	ea4f 0343 	mov.w	r3, r3, lsl #1
   e76b0:	e7cc      	b.n	e764c <__aeabi_d2f+0x14>
   e76b2:	ea7f 5362 	mvns.w	r3, r2, asr #21
   e76b6:	d107      	bne.n	e76c8 <__aeabi_d2f+0x90>
   e76b8:	ea50 3301 	orrs.w	r3, r0, r1, lsl #12
   e76bc:	bf1e      	ittt	ne
   e76be:	f04f 40fe 	movne.w	r0, #2130706432	; 0x7f000000
   e76c2:	f440 0040 	orrne.w	r0, r0, #12582912	; 0xc00000
   e76c6:	4770      	bxne	lr
   e76c8:	f001 4000 	and.w	r0, r1, #2147483648	; 0x80000000
   e76cc:	f040 40fe 	orr.w	r0, r0, #2130706432	; 0x7f000000
   e76d0:	f440 0000 	orr.w	r0, r0, #8388608	; 0x800000
   e76d4:	4770      	bx	lr
   e76d6:	bf00      	nop

000e76d8 <__aeabi_d2lz>:
   e76d8:	b538      	push	{r3, r4, r5, lr}
   e76da:	2200      	movs	r2, #0
   e76dc:	2300      	movs	r3, #0
   e76de:	4604      	mov	r4, r0
   e76e0:	460d      	mov	r5, r1
   e76e2:	f7ff ff39 	bl	e7558 <__aeabi_dcmplt>
   e76e6:	b928      	cbnz	r0, e76f4 <__aeabi_d2lz+0x1c>
   e76e8:	4620      	mov	r0, r4
   e76ea:	4629      	mov	r1, r5
   e76ec:	e8bd 4038 	ldmia.w	sp!, {r3, r4, r5, lr}
   e76f0:	f000 b80a 	b.w	e7708 <__aeabi_d2ulz>
   e76f4:	4620      	mov	r0, r4
   e76f6:	f105 4100 	add.w	r1, r5, #2147483648	; 0x80000000
   e76fa:	f000 f805 	bl	e7708 <__aeabi_d2ulz>
   e76fe:	4240      	negs	r0, r0
   e7700:	eb61 0141 	sbc.w	r1, r1, r1, lsl #1
   e7704:	bd38      	pop	{r3, r4, r5, pc}
   e7706:	bf00      	nop

000e7708 <__aeabi_d2ulz>:
   e7708:	b5d0      	push	{r4, r6, r7, lr}
   e770a:	2200      	movs	r2, #0
   e770c:	4b0e      	ldr	r3, [pc, #56]	; (e7748 <__aeabi_d2ulz+0x40>)
   e770e:	4606      	mov	r6, r0
   e7710:	460f      	mov	r7, r1
   e7712:	f7ff fcaf 	bl	e7074 <__aeabi_dmul>
   e7716:	f7ff ff6f 	bl	e75f8 <__aeabi_d2uiz>
   e771a:	4604      	mov	r4, r0
   e771c:	f7ff fc34 	bl	e6f88 <__aeabi_ui2d>
   e7720:	2200      	movs	r2, #0
   e7722:	4b0a      	ldr	r3, [pc, #40]	; (e774c <__aeabi_d2ulz+0x44>)
   e7724:	f7ff fca6 	bl	e7074 <__aeabi_dmul>
   e7728:	4602      	mov	r2, r0
   e772a:	460b      	mov	r3, r1
   e772c:	4630      	mov	r0, r6
   e772e:	4639      	mov	r1, r7
   e7730:	f7ff faec 	bl	e6d0c <__aeabi_dsub>
   e7734:	f7ff ff60 	bl	e75f8 <__aeabi_d2uiz>
   e7738:	4623      	mov	r3, r4
   e773a:	2200      	movs	r2, #0
   e773c:	ea42 0200 	orr.w	r2, r2, r0
   e7740:	4610      	mov	r0, r2
   e7742:	4619      	mov	r1, r3
   e7744:	bdd0      	pop	{r4, r6, r7, pc}
   e7746:	bf00      	nop
   e7748:	3df00000 	.word	0x3df00000
   e774c:	41f00000 	.word	0x41f00000

000e7750 <__cxa_atexit>:
   e7750:	b510      	push	{r4, lr}
   e7752:	4c05      	ldr	r4, [pc, #20]	; (e7768 <__cxa_atexit+0x18>)
   e7754:	4613      	mov	r3, r2
   e7756:	b12c      	cbz	r4, e7764 <__cxa_atexit+0x14>
   e7758:	460a      	mov	r2, r1
   e775a:	4601      	mov	r1, r0
   e775c:	2002      	movs	r0, #2
   e775e:	f3af 8000 	nop.w
   e7762:	bd10      	pop	{r4, pc}
   e7764:	4620      	mov	r0, r4
   e7766:	bd10      	pop	{r4, pc}
   e7768:	00000000 	.word	0x00000000

000e776c <exit>:
   e776c:	b508      	push	{r3, lr}
   e776e:	4b07      	ldr	r3, [pc, #28]	; (e778c <exit+0x20>)
   e7770:	4604      	mov	r4, r0
   e7772:	b113      	cbz	r3, e777a <exit+0xe>
   e7774:	2100      	movs	r1, #0
   e7776:	f3af 8000 	nop.w
   e777a:	4b05      	ldr	r3, [pc, #20]	; (e7790 <exit+0x24>)
   e777c:	6818      	ldr	r0, [r3, #0]
   e777e:	6a83      	ldr	r3, [r0, #40]	; 0x28
   e7780:	b103      	cbz	r3, e7784 <exit+0x18>
   e7782:	4798      	blx	r3
   e7784:	4620      	mov	r0, r4
   e7786:	f7ec fc91 	bl	d40ac <_exit>
   e778a:	bf00      	nop
   e778c:	00000000 	.word	0x00000000
   e7790:	000eb978 	.word	0x000eb978

000e7794 <memcmp>:
   e7794:	b510      	push	{r4, lr}
   e7796:	3901      	subs	r1, #1
   e7798:	4402      	add	r2, r0
   e779a:	4290      	cmp	r0, r2
   e779c:	d007      	beq.n	e77ae <memcmp+0x1a>
   e779e:	f810 3b01 	ldrb.w	r3, [r0], #1
   e77a2:	f811 4f01 	ldrb.w	r4, [r1, #1]!
   e77a6:	42a3      	cmp	r3, r4
   e77a8:	d0f7      	beq.n	e779a <memcmp+0x6>
   e77aa:	1b18      	subs	r0, r3, r4
   e77ac:	bd10      	pop	{r4, pc}
   e77ae:	2000      	movs	r0, #0
   e77b0:	bd10      	pop	{r4, pc}

000e77b2 <memcpy>:
   e77b2:	b510      	push	{r4, lr}
   e77b4:	1e43      	subs	r3, r0, #1
   e77b6:	440a      	add	r2, r1
   e77b8:	4291      	cmp	r1, r2
   e77ba:	d004      	beq.n	e77c6 <memcpy+0x14>
   e77bc:	f811 4b01 	ldrb.w	r4, [r1], #1
   e77c0:	f803 4f01 	strb.w	r4, [r3, #1]!
   e77c4:	e7f8      	b.n	e77b8 <memcpy+0x6>
   e77c6:	bd10      	pop	{r4, pc}

000e77c8 <memset>:
   e77c8:	4402      	add	r2, r0
   e77ca:	4603      	mov	r3, r0
   e77cc:	4293      	cmp	r3, r2
   e77ce:	d002      	beq.n	e77d6 <memset+0xe>
   e77d0:	f803 1b01 	strb.w	r1, [r3], #1
   e77d4:	e7fa      	b.n	e77cc <memset+0x4>
   e77d6:	4770      	bx	lr

000e77d8 <srand>:
   e77d8:	b538      	push	{r3, r4, r5, lr}
   e77da:	4b12      	ldr	r3, [pc, #72]	; (e7824 <srand+0x4c>)
   e77dc:	681c      	ldr	r4, [r3, #0]
   e77de:	6ba3      	ldr	r3, [r4, #56]	; 0x38
   e77e0:	4605      	mov	r5, r0
   e77e2:	b9d3      	cbnz	r3, e781a <srand+0x42>
   e77e4:	2018      	movs	r0, #24
   e77e6:	f7fc fda1 	bl	e432c <malloc>
   e77ea:	f243 330e 	movw	r3, #13070	; 0x330e
   e77ee:	63a0      	str	r0, [r4, #56]	; 0x38
   e77f0:	8003      	strh	r3, [r0, #0]
   e77f2:	f64a 33cd 	movw	r3, #43981	; 0xabcd
   e77f6:	8043      	strh	r3, [r0, #2]
   e77f8:	f241 2334 	movw	r3, #4660	; 0x1234
   e77fc:	8083      	strh	r3, [r0, #4]
   e77fe:	f24e 636d 	movw	r3, #58989	; 0xe66d
   e7802:	80c3      	strh	r3, [r0, #6]
   e7804:	f64d 63ec 	movw	r3, #57068	; 0xdeec
   e7808:	8103      	strh	r3, [r0, #8]
   e780a:	2305      	movs	r3, #5
   e780c:	8143      	strh	r3, [r0, #10]
   e780e:	230b      	movs	r3, #11
   e7810:	8183      	strh	r3, [r0, #12]
   e7812:	2201      	movs	r2, #1
   e7814:	2300      	movs	r3, #0
   e7816:	e9c0 2304 	strd	r2, r3, [r0, #16]
   e781a:	6ba3      	ldr	r3, [r4, #56]	; 0x38
   e781c:	2200      	movs	r2, #0
   e781e:	611d      	str	r5, [r3, #16]
   e7820:	615a      	str	r2, [r3, #20]
   e7822:	bd38      	pop	{r3, r4, r5, pc}
   e7824:	2003c404 	.word	0x2003c404

000e7828 <strcmp>:
   e7828:	f810 2b01 	ldrb.w	r2, [r0], #1
   e782c:	f811 3b01 	ldrb.w	r3, [r1], #1
   e7830:	2a01      	cmp	r2, #1
   e7832:	bf28      	it	cs
   e7834:	429a      	cmpcs	r2, r3
   e7836:	d0f7      	beq.n	e7828 <strcmp>
   e7838:	1ad0      	subs	r0, r2, r3
   e783a:	4770      	bx	lr

000e783c <strlen>:
   e783c:	4603      	mov	r3, r0
   e783e:	f813 2b01 	ldrb.w	r2, [r3], #1
   e7842:	2a00      	cmp	r2, #0
   e7844:	d1fb      	bne.n	e783e <strlen+0x2>
   e7846:	1a18      	subs	r0, r3, r0
   e7848:	3801      	subs	r0, #1
   e784a:	4770      	bx	lr

000e784c <dynalib_user>:
   e784c:	4021 000d 405d 000d 4089 000d 408d 000d     !@..]@...@...@..
   e785c:	0000 0000 7325 203a 656c 676e 6874 253d     ....%s: length=%
   e786c:	2064 005b 6e55 6e6b 776f 206e 7974 6570     d [.Unknown type
   e787c:	4e00 544f 5059 0045 4c46 414f 3354 0032     .NOTYPE.FLOAT32.
   e788c:	4e49 3354 0032 4955 544e 0038 4e49 3654     INT32.UINT8.INT6
   e789c:	0034 5453 4952 474e 4200 4f4f 004c 4e49     4.STRING.BOOL.IN
   e78ac:	3154 0036 4f43 504d 454c 3658 0034 4c46     T16.COMPLEX64.FL
   e78bc:	414f 3154 0036 0000                         OAT16...

000e78c4 <CSWTCH.19>:
   e78c4:	787d 000e 7884 000e 788c 000e 7892 000e     }x...x...x...x..
   e78d4:	7898 000e 789e 000e 78a5 000e 78aa 000e     .x...x...x...x..
   e78e4:	78b0 000e 7893 000e 78ba 000e 6f4d 6564     .x...x...x..Mode
   e78f4:	206c 7270 766f 6469 6465 6920 2073 6373     l provided is sc
   e7904:	6568 616d 7620 7265 6973 6e6f 2520 2064     hema version %d 
   e7914:	6f6e 2074 7165 6175 206c 6f74 7320 7075     not equal to sup
   e7924:	6f70 7472 6465 7620 7265 6973 6e6f 2520     ported version %
   e7934:	2e64 4100 6c6c 636f 7461 5465 6e65 6f73     d..AllocateTenso
   e7944:	7372 2928 6620 6961 656c 0064 6e49 6f76     rs() failed.Invo
   e7954:	656b 6620 6961 656c 2064 6e6f 7820 765f     ke failed on x_v
   e7964:	6c61 203a 6625 000a 6c46 7461 7542 6666     al: %f..FlatBuff
   e7974:	7265 2073 2e31 3131 302e 0000               ers 1.11.0..

000e7980 <kInferencesPerCycle>:
   e7980:	03e8 0000                                   ....

000e7984 <g_sine_model_data>:
   e7984:	0018 0000 4654 334c 0000 000e 0018 0004     ....TFL3........
   e7994:	0008 000c 0010 0014 000e 0000 0003 0000     ................
   e79a4:	0a10 0000 05b8 0000 05a0 0000 0004 0000     ................
   e79b4:	000b 0000 0590 0000 057c 0000 0524 0000     ........|...$...
   e79c4:	04d4 0000 00c4 0000 0074 0000 0024 0000     ........t...$...
   e79d4:	001c 0000 0014 0000 000c 0000 0004 0000     ................
   e79e4:	f654 ffff f658 ffff f65c ffff f660 ffff     T...X...\...`...
   e79f4:	fac2 ffff 0004 0000 0040 0000 197c 3ea7     ........@...|..>
   e7a04:	8199 3eb9 8b56 3e9f d888 bf12 1074 3e56     ...>V..>....t.V>
   e7a14:	c6fe bedf 10f2 be5a e2f0 be0a 5a10 be98     ......Z......Z..
   e7a24:	36b9 3dce 7f8f 3e87 b12c bdfd a6e6 be8a     .6.=...>,.......
   e7a34:	3ea5 3eda 3450 bded 9190 be69 fb0e ffff     .>.>P4....i.....
   e7a44:	0004 0000 0040 0000 4167 bf48 cd24 bea0     ....@...gAH.$...
   e7a54:	92b7 bf0c 0000 0000 fe98 3f3c 0000 0000     ..........<?....
	...
   e7a70:	174a be9a cb41 beb6 0000 0000 0000 0000     J...A...........
   e7a80:	d613 3e1e 0000 0000 0000 0000 fb5a ffff     ...>........Z...
   e7a90:	0004 0000 0400 0000 984b bddd 6b40 becb     ........K...@k..
   e7aa0:	0c36 3cd4 44bd 3eb5 7095 3ee3 ace7 3e86     6..<.D.>.p.>...>
   e7ab0:	c400 3d4e a67e 3e1d 87bd 3ebb b8b4 bf09     ..N=~..>...>....
   e7ac0:	1fa1 bef8 908d 3edd fade be6f 75b2 3de4     .......>..o..u.=
   e7ad0:	fe6e 3e36 1820 bec2 c739 befb a4fe be30     n.6> ...9.....0.
   e7ae0:	91f7 bede abde 3e24 bbfb 3ece 23eb be80     ......$>...>.#..
   e7af0:	587b be73 2e9a 3e03 4210 bca9 1210 bd64     {Xs....>.B....d.
   e7b00:	8de3 3d0c 489e be97 5134 bed4 3b02 3e0d     ...=.H..4Q...;.>
   e7b10:	6762 be89 df74 3da2 25f3 beb3 34ef 3d7b     bg..t..=.%...4{=
   e7b20:	7061 3de3 76ba bec0 e97d 3ea7 abc3 bed0     ap.=.v..}..>....
   e7b30:	7ccf bedb 2770 be9a f598 bd3c 4bff 3e4b     .|..p'....<..KK>
   e7b40:	a07e bdf8 6ed4 3d86 4a00 3a07 244c be61     ~....n.=.J.:L$a.
   e7b50:	6854 bdf7 3f02 be77 7923 3eb3 831c bdad     Th...?w.#y.>....
   e7b60:	92c8 3e8d f3a8 bd15 4de6 3d6c e7ac be98     ...>.....Ml=....
   e7b70:	ec81 3ebd 55e2 3e73 77c1 3ec7 1b6e 3d5e     ...>.Us>.w.>n.^=
   e7b80:	7827 3f02 21d4 3d90 dc52 3e1f dabf 3e88     'x.?.!.=R..>...>
   e7b90:	7980 bde3 6f40 be10 4320 bd2e 76f0 bdc5     .y..@o.. C...v..
   e7ba0:	a0cc be04 69f0 bed7 feb1 be64 4120 be84     .....i....d. A..
   e7bb0:	c3b2 be26 f4d8 be09 4464 3dd1 e1d5 bec8     ..&.....dD.=....
   e7bc0:	bc35 be3f 94c0 3d82 2bdc bdb1 db02 bebf     5.?....=.+......
   e7bd0:	7fa5 3e8a b421 3ea2 86cd bf56 3b9c bc76     ...>!..>..V..;v.
   e7be0:	6d85 bf60 0086 be3c 23c1 3e7e cd96 3e3f     .m`...<..#~>..?>
   e7bf0:	9186 3e2d ef55 3e87 977e be03 cd2a 3e01     ..->U..>~...*..>
   e7c00:	c932 be8e 7772 be3b a1e0 bebc b78d 3ea7     2...rw;........>
   e7c10:	051c be95 1ff7 3ebb 3ec9 3ed6 4280 bde9     .......>.>.>.B..
   e7c20:	0c27 bed2 325c be34 cb14 bdca 3add be67     '...\24......:g.
   e7c30:	bb1c be8d ac91 be5c 4052 be6f 71d7 3e94     ......\.R@o..q.>
   e7c40:	7118 be09 299b bed9 667d bed2 d698 beb2     .q...)..}f......
   e7c50:	c900 3a84 dabc bdc2 c21d bf1b ddd4 3e92     ...:...........>
   e7c60:	8707 be6c c240 be3b e2bd 3e9c b50a bea0     ..l.@.;....>....
   e7c70:	d5e2 be9c bb3e 3e7c b417 3ecf 8ed5 bec8     ....>.|>...>....
   e7c80:	f97c 3e5c fc80 3d0d d5c5 3e8b 17f5 3ea2     |.\>...=...>...>
   e7c90:	60c7 be89 95ec 3d87 c27a bf5d 9477 3e98     .`.....=z.].w..>
   e7ca0:	3977 bc07 2942 3e00 d0af 3ea9 2331 bec4     w9..B).>...>1#..
   e7cb0:	3695 be5b dcc7 be83 6b1e 3e47 245b 3e99     .6[......kG>[$.>
   e7cc0:	2799 3e54 20c8 bddd 865a 3e2f f080 be69     .'T>. ..Z./>..i.
   e7cd0:	fc44 bd84 a082 be2a e687 3e2a 34d8 3dae     D.....*...*>.4.=
   e7ce0:	bd50 3eb5 8cc4 be88 bce3 3ea5 daa9 3e9e     P..>.......>...>
   e7cf0:	b83e be23 9080 3d15 3f97 3ec3 5cca 3e9d     >.#....=.?.>.\.>
   e7d00:	e821 3ee1 49c0 bc01 0b00 bd88 f73f 3cca     !..>.I......?..<
   e7d10:	5afb 3eb1 d260 3c0d 23ce bf78 4f8f beb9     .Z.>`..<.#x..O..
   e7d20:	6a69 bf34 5e4b 3ea9 8c64 3ed9 7752 3e36     ij4.K^.>d..>Rw6>
   e7d30:	afeb 3ebe be40 3c36 6508 bd3b e055 bd66     ...>@.6<.e;.U.f.
   e7d40:	e8d2 be9b e386 be09 3d93 3edd 660f 3f18     .........=.>.f.?
   e7d50:	0518 bd33 15de bed7 cfaa be49 a5a2 3e64     ..3.......I...d>
   e7d60:	9ce6 be42 4254 3dcc bda0 be9d 69c2 3e48     ..B.TB.=.....iH>
   e7d70:	8b5b bea2 13c0 3d87 fd36 3e69 8605 be40     [......=6.i>..@.
   e7d80:	7a1e bece 1346 bea7 5268 be86 9e04 bd86     .z..F...hR......
   e7d90:	548c 3dc1 3be0 3cad 6742 bd85 97ea 3e42     .T.=.;.<Bg....B>
   e7da0:	136e bf3b 5b56 3e16 abaa 3edf 41c8 3d36     n.;.V[.>...>.A6=
   e7db0:	2d24 be47 a577 3eae c2c0 3c5b acac 3e4e     $-G.w..>..[<..N>
   e7dc0:	ec99 be13 abf2 3e73 a1aa be48 d3e8 be01     ......s>..H.....
   e7dd0:	b760 bdc7 7264 3dd3 d383 3e99 760c be34     `...dr.=...>.v4.
   e7de0:	da42 3e0d 47fb 3e9a dc8b be92 7f56 3e6b     B..>.G.>....V.k>
   e7df0:	d404 bd88 9e11 3e80 893c 3dff 3eb3 3e88     .......><..=.>.>
   e7e00:	f0f7 3e88 fb28 bec9 3e53 3ecf 75ac bedc     ...>(...S>.>.u..
   e7e10:	cadd 3ed7 5801 3ea7 b829 bf13 8176 bc12     ...>.X.>)...v...
   e7e20:	8b28 bf16 ec0e 3e0e 0a40 bddb ec98 bdbf     (......>@.......
   e7e30:	5532 be0c f9fb 3ec9 4a83 be6d 5976 bee2     2U.....>.Jm.vY..
   e7e40:	7d54 bb9f e89d 3e95 d35c 3dd0 8a19 3eb0     T}.....>\..=...>
   e7e50:	6fde be2e 16d0 3d83 7d9c bf11 cc2b 3c25     .o.....=.}..+.%<
   e7e60:	a52a be27 1422 bec7 7a5e 3eac 414e be94     *.'."...^z.>NA..
   e7e70:	685a 3e7b fd86 3e4e 56a2 be6a feca be81     Zh{>..N>.Vj.....
   e7e80:	c343 bdb1 b8c5 3ea7 2355 3ecd 2eaf 3e76     C......>U#.>..v>
   e7e90:	a869 be90 ba0d 3eb9 ff66 ffff 0004 0000     i......>f.......
   e7ea0:	0040 0000 d653 3de2 b666 3ecc e703 3ef6     @...S..=f..>...>
   e7eb0:	28e0 bf10 0000 0000 3d3e 3eb0 0000 0000     .(......>=.>....
   e7ec0:	f062 3e77 9da6 3ea4 4b3a bef3 9e71 3ea7     b.w>...>:K..q..>
   e7ed0:	0000 0000 3934 3ea2 0000 0000 9ccc 3e4a     ....49.>......J>
   e7ee0:	40ab 3ea3 ffb2 ffff 0004 0000 0040 0000     .@.>........@...
   e7ef0:	71b3 3f67 7a9a bf95 48e1 bee8 728a 3e96     .qg?.z...H...r.>
   e7f00:	d200 bbd3 c51a 3fd7 7eac bec8 a790 be95     .......?.~......
   e7f10:	d73b bedc a841 3f16 5b50 3fcb b952 beed     ;...A..?P[.?R...
   e7f20:	a72e bec6 0faf bf14 dab3 3f59 ec02 bed7     ..........Y?....
   e7f30:	0000 0006 0008 0004 0006 0000 0004 0000     ................
   e7f40:	0004 0000 1166 bf1f fbb8 ffff 000f 0000     ....f...........
   e7f50:	4f54 4f43 4320 6e6f 6576 7472 6465 002e     TOCO Converted..
   e7f60:	0001 0000 0010 0000 000c 0014 0004 0008     ................
   e7f70:	000c 0010 000c 0000 00f0 0000 00e4 0000     ................
   e7f80:	00d8 0000 0004 0000 0003 0000 0090 0000     ................
   e7f90:	0048 0000 0004 0000 ffce ffff 0000 0800     H...............
   e7fa0:	0018 0000 000c 0000 0004 0000 fc1c ffff     ................
   e7fb0:	0001 0000 0000 0000 0003 0000 0007 0000     ................
   e7fc0:	0008 0000 0009 0000 0000 000e 0014 0000     ................
   e7fd0:	0008 000c 0007 0010 000e 0000 0000 0800     ................
   e7fe0:	001c 0000 0010 0000 0004 0000 ffba ffff     ................
   e7ff0:	0000 0100 0001 0000 0007 0000 0003 0000     ................
   e8000:	0004 0000 0005 0000 0006 0000 0000 000e     ................
   e8010:	0016 0000 0008 000c 0007 0010 000e 0000     ................
   e8020:	0000 0800 0024 0000 0018 0000 000c 0000     ....$...........
   e8030:	0000 0006 0008 0007 0006 0000 0000 0100     ................
   e8040:	0001 0000 0004 0000 0003 0000 0001 0000     ................
   e8050:	0002 0000 0003 0000 0001 0000 0000 0000     ................
   e8060:	0001 0000 0001 0000 000a 0000 0310 0000     ................
   e8070:	02a4 0000 0240 0000 01f4 0000 01ac 0000     ....@...........
   e8080:	0148 0000 00fc 0000 00b4 0000 0050 0000     H...........P...
   e8090:	0004 0000 fd26 ffff 003c 0000 0001 0000     ....&...<.......
   e80a0:	000c 0000 0004 0000 fd18 ffff 0020 0000     ............ ...
   e80b0:	6573 7571 6e65 6974 6c61 315f 642f 6e65     sequential_1/den
   e80c0:	6573 345f 4d2f 7461 754d 5f6c 6962 7361     se_4/MatMul_bias
   e80d0:	0000 0000 0001 0000 0001 0000 fd6e ffff     ............n...
   e80e0:	0050 0000 0002 0000 000c 0000 0004 0000     P...............
   e80f0:	fd60 ffff 0034 0000 6573 7571 6e65 6974     `...4...sequenti
   e8100:	6c61 315f 642f 6e65 6573 345f 4d2f 7461     al_1/dense_4/Mat
   e8110:	754d 2f6c 6552 6461 6156 6972 6261 656c     Mul/ReadVariable
   e8120:	704f 742f 6172 736e 6f70 6573 0000 0000     Op/transpose....
   e8130:	0002 0000 0001 0000 0010 0000 fdce ffff     ................
   e8140:	0034 0000 0008 0000 000c 0000 0004 0000     4...............
   e8150:	fdc0 ffff 0019 0000 6573 7571 6e65 6974     ........sequenti
   e8160:	6c61 315f 642f 6e65 6573 335f 522f 6c65     al_1/dense_3/Rel
   e8170:	0075 0000 0002 0000 0001 0000 0010 0000     u...............
   e8180:	fe12 ffff 003c 0000 0003 0000 000c 0000     ....<...........
   e8190:	0004 0000 fe04 ffff 0020 0000 6573 7571     ........ ...sequ
   e81a0:	6e65 6974 6c61 315f 642f 6e65 6573 335f     ential_1/dense_3
   e81b0:	4d2f 7461 754d 5f6c 6962 7361 0000 0000     /MatMul_bias....
   e81c0:	0001 0000 0010 0000 fe5a ffff 0050 0000     ........Z...P...
   e81d0:	0004 0000 000c 0000 0004 0000 fe4c ffff     ............L...
   e81e0:	0034 0000 6573 7571 6e65 6974 6c61 315f     4...sequential_1
   e81f0:	642f 6e65 6573 335f 4d2f 7461 754d 2f6c     /dense_3/MatMul/
   e8200:	6552 6461 6156 6972 6261 656c 704f 742f     ReadVariableOp/t
   e8210:	6172 736e 6f70 6573 0000 0000 0002 0000     ranspose........
   e8220:	0010 0000 0010 0000 feba ffff 0034 0000     ............4...
   e8230:	000a 0000 000c 0000 0004 0000 feac ffff     ................
   e8240:	0019 0000 6573 7571 6e65 6974 6c61 315f     ....sequential_1
   e8250:	642f 6e65 6573 325f 522f 6c65 0075 0000     /dense_2/Relu...
   e8260:	0002 0000 0001 0000 0010 0000 fefe ffff     ................
   e8270:	003c 0000 0005 0000 000c 0000 0004 0000     <...............
   e8280:	fef0 ffff 0020 0000 6573 7571 6e65 6974     .... ...sequenti
   e8290:	6c61 315f 642f 6e65 6573 325f 4d2f 7461     al_1/dense_2/Mat
   e82a0:	754d 5f6c 6962 7361 0000 0000 0001 0000     Mul_bias........
   e82b0:	0010 0000 ff46 ffff 0050 0000 0006 0000     ....F...P.......
   e82c0:	000c 0000 0004 0000 ff38 ffff 0034 0000     ........8...4...
   e82d0:	6573 7571 6e65 6974 6c61 315f 642f 6e65     sequential_1/den
   e82e0:	6573 325f 4d2f 7461 754d 2f6c 6552 6461     se_2/MatMul/Read
   e82f0:	6156 6972 6261 656c 704f 742f 6172 736e     VariableOp/trans
   e8300:	6f70 6573 0000 0000 0002 0000 0010 0000     pose............
   e8310:	0001 0000 ffa6 ffff 0048 0000 0009 0000     ........H.......
   e8320:	002c 0000 000c 0000 0008 000c 0004 0008     ,...............
   e8330:	0008 0000 0010 0000 0004 0000 0001 0000     ................
   e8340:	0000 437f 0001 0000 0000 0000 000d 0000     ...C............
   e8350:	6564 736e 5f65 5f32 6e69 7570 0074 0000     dense_2_input...
   e8360:	0002 0000 0001 0000 0001 0000 0000 000e     ................
   e8370:	0014 0004 0000 0008 000c 0010 000e 0000     ................
   e8380:	0028 0000 0007 0000 0010 0000 0008 0000     (...............
   e8390:	0004 0004 0004 0000 0008 0000 6449 6e65     ............Iden
   e83a0:	6974 7974 0000 0000 0002 0000 0001 0000     tity............
   e83b0:	0001 0000 0001 0000 0010 0000 0000 000a     ................
   e83c0:	000c 0007 0000 0008 000a 0000 0000 0900     ................
   e83d0:	0003 0000                                   ....

000e83d4 <_ZZNK11flatbuffers6VectorIlE3GetEmE19__PRETTY_FUNCTION__>:
   e83d4:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
   e83e4:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
   e83f4:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
   e8404:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
   e8414:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
   e8424:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
   e8434:	7469 2068 2054 203d 6f6c 676e 6920 746e     ith T = long int
   e8444:	203b 6c66 7461 7562 6666 7265 3a73 563a     ; flatbuffers::V
   e8454:	6365 6f74 3c72 3e54 3a3a 6572 7574 6e72     ector<T>::return
   e8464:	745f 7079 2065 203d 6f6c 676e 6920 746e     _type = long int
   e8474:	203b 6c66 7461 7562 6666 7265 3a73 753a     ; flatbuffers::u
   e8484:	666f 7366 7465 745f 3d20 6c20 6e6f 2067     offset_t = long 
   e8494:	6e75 6973 6e67 6465 6920 746e 005d 6e49     unsigned int].In
   e84a4:	7570 2074 7261 6172 2079 6f6e 2074 7270     put array not pr
   e84b4:	766f 6469 6465 6620 726f 6f20 6570 6172     ovided for opera
   e84c4:	6974 6e6f 2720 7325 2e27 000a 6f46 6e75     tion '%s'...Foun
   e84d4:	2064 6f74 206f 616d 796e 6420 6d69 6e65     d too many dimen
   e84e4:	6973 6e6f 2073 6e69 7420 6568 6920 706e     sions in the inp
   e84f4:	7475 6120 7272 7961 6f20 2066 706f 7265     ut array of oper
   e8504:	7461 6f69 206e 2527 2773 0a2e 6900 3c20     ation '%s'...i <
   e8514:	7320 7a69 2865 0029 552f 6573 7372 622f      size()./Users/b
   e8524:	6173 7274 6d6f 442f 7665 6c65 706f 656d     satrom/Developme
   e8534:	746e 702f 7261 6974 6c63 2f65 696c 7262     nt/particle/libr
   e8544:	7261 6569 2f73 6150 7472 6369 656c 545f     aries/Particle_T
   e8554:	6e65 6f73 4672 6f6c 4c77 7469 5f65 7845     ensorFlowLite_Ex
   e8564:	6d61 6c70 7365 682f 6c65 6f6c 775f 726f     amples/hello_wor
   e8574:	646c 6c2f 6269 542f 6e65 6f73 4672 6f6c     ld/lib/TensorFlo
   e8584:	4c77 7469 2f65 7273 2f63 6874 7269 5f64     wLite/src/third_
   e8594:	6170 7472 2f79 6c66 7461 7562 6666 7265     party/flatbuffer
   e85a4:	2f73 6e69 6c63 6475 2f65 6c66 7461 7562     s/include/flatbu
   e85b4:	6666 7265 2f73 6c66 7461 7562 6666 7265     ffers/flatbuffer
   e85c4:	2e73 0068 6e55 7573 7070 726f 6574 2064     s.h.Unsupported 
   e85d4:	6164 6174 7420 7079 2065 6425 6920 206e     data type %d in 
   e85e4:	6574 736e 726f 000a 6e55 6168 646e 656c     tensor..Unhandle
   e85f4:	2064 7566 6c6c 2d79 6f63 6e6e 6365 6574     d fully-connecte
   e8604:	2064 6577 6769 7468 2073 6f66 6d72 7461     d weights format
   e8614:	002e 6e55 6168 646e 656c 2064 534c 4d54     ..Unhandled LSTM
   e8624:	6b20 7265 656e 206c 7974 6570 203a 6425      kernel type: %d
   e8634:	4e00 206f 6176 696c 2064 534c 4d54 6220     .No valid LSTM b
   e8644:	6975 746c 6e69 6f20 7470 6f69 736e 6520     uiltin options e
   e8654:	6978 7473 7200 7365 6168 6570 7300 7571     xist.reshape.squ
   e8664:	6565 657a 4400 4c45 4745 5441 2045 706f     eeze.DELEGATE op
   e8674:	7320 6f68 6c75 6e64 7427 6520 6978 7473      shouldn't exist
   e8684:	6920 206e 6f6d 6564 2e6c 0100                     in model..

000e868f <CSWTCH.73>:
   e868f:	0201 0403 4005                                   .....

000e8694 <_ZZN6tflite24EnumNamesBuiltinOperatorEvE5names>:
   e8694:	8940 000e 8944 000e 8954 000e 896c 000e     @...D...T...l...
   e86a4:	8962 000e 8974 000e 8983 000e 898e 000e     b...t...........
   e86b4:	899f 000e 89a5 000e 89b5 000e 89c6 000e     ................
   e86c4:	89d7 000e 89e2 000e 89ff 000e 8a08 000e     ................
   e86d4:	8b91 000e 8a17 000e 8a23 000e 8b9c 000e     ........#.......
   e86e4:	8a27 000e 8a34 000e 8a3a 000e 8a42 000e     '...4...:...B...
   e86f4:	8b4f 000e 8b69 000e 8a52 000e 8a61 000e     O...i...R...a...
   e8704:	8a66 000e 8a6b 000e 8a7d 000e 8a87 000e     f...k...}.......
   e8714:	8a8c 000e 8a93 000e 8d20 000e 8aab 000e     ........ .......
   e8724:	8ac7 000e 8ace 000e 8ae0 000e 8af2 000e     ................
   e8734:	8afc 000e 8b01 000e 8cad 000e 8b05 000e     ................
   e8744:	8b0d 000e 8b2a 000e 8b38 000e 8b53 000e     ....*...8...S...
   e8754:	8b57 000e 8b5f 000e 8b65 000e 8b71 000e     W..._...e...q...
   e8764:	8b7a 000e 8b96 000e 8b9b 000e 8ba1 000e     z...............
   e8774:	8ba9 000e 8bb1 000e 8bb9 000e 8bbe 000e     ................
   e8784:	8bc2 000e 8bc8 000e 8bd0 000e 8bde 000e     ................
   e8794:	8be9 000e 8b32 000e 8bf0 000e 8bf4 000e     ....2...........
   e87a4:	8c03 000e 8c13 000e 8c18 000e 8bd8 000e     ................
   e87b4:	8c24 000e 8c2e 000e 8c32 000e 8c37 000e     $.......2...7...
   e87c4:	8c36 000e 8a3c 000e 8c3c 000e 8c40 000e     6...<...<...@...
   e87d4:	8c48 000e 8c53 000e 8c5f 000e 8c97 000e     H...S..._.......
   e87e4:	8c6a 000e 8c75 000e 8c7d 000e 8c89 000e     j...u...}.......
   e87f4:	8c95 000e 8c9c 000e 8ca7 000e 8cb1 000e     ................
   e8804:	8cbc 000e 8cc3 000e 8cce 000e 8cd3 000e     ................
   e8814:	8cdd 000e 8ce3 000e 8cfb 000e 8d06 000e     ................
   e8824:	8d19 000e 8d24 000e 8d28 000e 8d30 000e     ....$...(...0...
   e8834:	8d37 000e 8d3c 000e 8d47 000e 8d4d 000e     7...<...G...M...
   e8844:	8d57 000e 8d5b 000e 8d61 000e 8b9d 000e     W...[...a.......
   e8854:	8d66 000e 8d77 000e 8985 000e 8d83 000e     f...w...........
   e8864:	8d93 000e 8d99 000e 8da4 000e 8da7 000e     ................
   e8874:	8dad 000e 8dc4 000e 0000 0000 704f 6220     ............Op b
   e8884:	6975 746c 6e69 635f 646f 2065 756f 2074     uiltin_code out 
   e8894:	666f 7220 6e61 6567 203a 6425 202e 7241     of range: %d. Ar
   e88a4:	2065 6f79 2075 7375 6e69 2067 6c6f 2064     e you using old 
   e88b4:	4654 694c 6574 6220 6e69 7261 2079 6977     TFLite binary wi
   e88c4:	6874 6e20 7765 7265 6d20 646f 6c65 003f     th newer model?.
   e88d4:	6944 6e64 7427 6620 6e69 2064 706f 6620     Didn't find op f
   e88e4:	726f 6220 6975 746c 6e69 6f20 6370 646f     or builtin opcod
   e88f4:	2065 2527 2773 7620 7265 6973 6e6f 2720     e '%s' version '
   e8904:	6425 0a27 4f00 6570 6172 6f74 2072 6977     %d'..Operator wi
   e8914:	6874 4320 5355 4f54 204d 7562 6c69 6974     th CUSTOM builti
   e8924:	5f6e 6f63 6564 6820 7361 6e20 206f 7563     n_code has no cu
   e8934:	7473 6d6f 635f 646f 2e65 000a 4441 0044     stom_code...ADD.
   e8944:	5641 5245 4741 5f45 4f50 4c4f 325f 0044     AVERAGE_POOL_2D.
   e8954:	4f43 434e 5441 4e45 5441 4f49 004e 4544     CONCATENATION.DE
   e8964:	5450 5748 5349 5f45 4f43 564e 325f 0044     PTHWISE_CONV_2D.
   e8974:	4544 5450 5f48 4f54 535f 4150 4543 4400     DEPTH_TO_SPACE.D
   e8984:	5145 4155 544e 5a49 0045 4d45 4542 4444     EQUANTIZE.EMBEDD
   e8994:	4e49 5f47 4f4c 4b4f 5055 4600 4f4c 524f     ING_LOOKUP.FLOOR
   e89a4:	4600 4c55 594c 435f 4e4f 454e 5443 4445     .FULLY_CONNECTED
   e89b4:	4800 5341 5448 4241 454c 4c5f 4f4f 554b     .HASHTABLE_LOOKU
   e89c4:	0050 324c 4e5f 524f 414d 494c 415a 4954     P.L2_NORMALIZATI
   e89d4:	4e4f 4c00 5f32 4f50 4c4f 325f 0044 4f4c     ON.L2_POOL_2D.LO
   e89e4:	4143 5f4c 4552 5053 4e4f 4553 4e5f 524f     CAL_RESPONSE_NOR
   e89f4:	414d 494c 415a 4954 4e4f 4c00 474f 5349     MALIZATION.LOGIS
   e8a04:	4954 0043 534c 5f48 5250 4a4f 4345 4954     TIC.LSH_PROJECTI
   e8a14:	4e4f 4d00 5841 505f 4f4f 5f4c 4432 4d00     ON.MAX_POOL_2D.M
   e8a24:	4c55 5200 4c45 5f55 314e 545f 5f4f 0031     UL.RELU_N1_TO_1.
   e8a34:	4552 554c 0036 4552 4853 5041 0045 4552     RELU6.RESHAPE.RE
   e8a44:	4953 455a 425f 4c49 4e49 4145 0052 5053     SIZE_BILINEAR.SP
   e8a54:	4341 5f45 4f54 445f 5045 4854 5300 4456     ACE_TO_DEPTH.SVD
   e8a64:	0046 4154 484e 4300 4e4f 4143 5f54 4d45     F.TANH.CONCAT_EM
   e8a74:	4542 4444 4e49 5347 5300 494b 5f50 5247     BEDDINGS.SKIP_GR
   e8a84:	4d41 4300 4c41 004c 5543 5453 4d4f 4500     AM.CALL.CUSTOM.E
   e8a94:	424d 4445 4944 474e 4c5f 4f4f 554b 5f50     MBEDDING_LOOKUP_
   e8aa4:	5053 5241 4553 5500 494e 4944 4552 5443     SPARSE.UNIDIRECT
   e8ab4:	4f49 414e 5f4c 4553 5551 4e45 4543 525f     IONAL_SEQUENCE_R
   e8ac4:	4e4e 4700 5441 4548 0052 4142 4354 5f48     NN.GATHER.BATCH_
   e8ad4:	4f54 535f 4150 4543 4e5f 0044 5053 4341     TO_SPACE_ND.SPAC
   e8ae4:	5f45 4f54 425f 5441 4843 4e5f 0044 5254     E_TO_BATCH_ND.TR
   e8af4:	4e41 5053 534f 0045 454d 4e41 5300 4255     ANSPOSE.MEAN.SUB
   e8b04:	5300 5551 4545 455a 5500 494e 4944 4552     .SQUEEZE.UNIDIRE
   e8b14:	5443 4f49 414e 5f4c 4553 5551 4e45 4543     CTIONAL_SEQUENCE
   e8b24:	4c5f 5453 004d 5453 4952 4544 5f44 4c53     _LSTM.STRIDED_SL
   e8b34:	4349 0045 4942 4944 4552 5443 4f49 414e     ICE.BIDIRECTIONA
   e8b44:	5f4c 4553 5551 4e45 4543 525f 4e4e 4500     L_SEQUENCE_RNN.E
   e8b54:	5058 5400 504f 5f4b 3256 5300 4c50 5449     XP.TOPK_V2.SPLIT
   e8b64:	4c00 474f 535f 464f 4d54 5841 4400 4c45     .LOG_SOFTMAX.DEL
   e8b74:	4745 5441 0045 4942 4944 4552 5443 4f49     EGATE.BIDIRECTIO
   e8b84:	414e 5f4c 4553 5551 4e45 4543 4c5f 5453     NAL_SEQUENCE_LST
   e8b94:	004d 4143 5453 5000 4552 554c 4d00 5841     M.CAST.PRELU.MAX
   e8ba4:	4d49 4d55 4100 4752 4d5f 5841 4d00 4e49     IMUM.ARG_MAX.MIN
   e8bb4:	4d49 4d55 4c00 5345 0053 454e 0047 4150     IMUM.LESS.NEG.PA
   e8bc4:	5644 0032 5247 4145 4554 0052 5247 4145     DV2.GREATER.GREA
   e8bd4:	4554 5f52 5145 4155 004c 454c 5353 455f     TER_EQUAL.LESS_E
   e8be4:	5551 4c41 5300 4c45 4345 0054 4953 004e     QUAL.SELECT.SIN.
   e8bf4:	5254 4e41 5053 534f 5f45 4f43 564e 5300     TRANSPOSE_CONV.S
   e8c04:	4150 5352 5f45 4f54 445f 4e45 4553 5400     PARSE_TO_DENSE.T
   e8c14:	4c49 0045 5845 4150 444e 445f 4d49 0053     ILE.EXPAND_DIMS.
   e8c24:	4f4e 5f54 5145 4155 004c 4f4c 0047 5553     NOT_EQUAL.LOG.SU
   e8c34:	004d 5352 5251 0054 4f50 0057 5241 5f47     M.RSQRT.POW.ARG_
   e8c44:	494d 004e 4146 454b 515f 4155 544e 5200     MIN.FAKE_QUANT.R
   e8c54:	4445 4355 5f45 5250 444f 5200 4445 4355     EDUCE_PROD.REDUC
   e8c64:	5f45 414d 0058 4f4c 4947 4143 5f4c 524f     E_MAX.LOGICAL_OR
   e8c74:	4f00 454e 485f 544f 4c00 474f 4349 4c41     .ONE_HOT.LOGICAL
   e8c84:	415f 444e 4c00 474f 4349 4c41 4e5f 544f     _AND.LOGICAL_NOT
   e8c94:	5500 504e 4341 004b 4552 5544 4543 4d5f     .UNPACK.REDUCE_M
   e8ca4:	4e49 4600 4f4c 524f 445f 5649 5200 4445     IN.FLOOR_DIV.RED
   e8cb4:	4355 5f45 4e41 0059 5153 4155 4552 5a00     UCE_ANY.SQUARE.Z
   e8cc4:	5245 534f 4c5f 4b49 0045 4946 4c4c 4600     EROS_LIKE.FILL.F
   e8cd4:	4f4c 524f 4d5f 444f 5200 4e41 4547 5200     LOOR_MOD.RANGE.R
   e8ce4:	5345 5a49 5f45 454e 5241 5345 5f54 454e     ESIZE_NEAREST_NE
   e8cf4:	4749 4248 524f 4c00 4145 594b 525f 4c45     IGHBOR.LEAKY_REL
   e8d04:	0055 5153 4155 4552 5f44 4944 4646 5245     U.SQUARED_DIFFER
   e8d14:	4e45 4543 4d00 5249 4f52 5f52 4150 0044     ENCE.MIRROR_PAD.
   e8d24:	4241 0053 5053 494c 5f54 0056 4e55 5149     ABS.SPLIT_V.UNIQ
   e8d34:	4555 4300 4945 004c 4552 4556 5352 5f45     UE.CEIL.REVERSE_
   e8d44:	3256 4100 4444 4e5f 4700 5441 4548 5f52     V2.ADD_N.GATHER_
   e8d54:	444e 4300 534f 5700 4548 4552 5200 4e41     ND.COS.WHERE.RAN
   e8d64:	004b 4552 4556 5352 5f45 4553 5551 4e45     K.REVERSE_SEQUEN
   e8d74:	4543 4d00 5441 4952 5f58 4944 4741 4d00     CE.MATRIX_DIAG.M
   e8d84:	5441 4952 5f58 4553 5f54 4944 4741 5200     ATRIX_SET_DIAG.R
   e8d94:	554f 444e 4800 5241 5f44 5753 5349 0048     OUND.HARD_SWISH.
   e8da4:	4649 5700 4948 454c 4e00 4e4f 4d5f 5841     IF.WHILE.NON_MAX
   e8db4:	535f 5055 5250 5345 4953 4e4f 565f 0034     _SUPPRESSION_V4.
   e8dc4:	4f4e 5f4e 414d 5f58 5553 5050 4552 5353     NON_MAX_SUPPRESS
   e8dd4:	4f49 5f4e 3556 0300 0804 0d0b 110e 1312     ION_V5..........
   e8de4:	1514 1716 6e49 0066 614e 004e 322a 005e     ....Inf.NaN.*2^.
   e8df4:	7954 6570 2520 2073 2528 2964 6e20 746f     Type %s (%d) not
   e8e04:	6920 2073 6f6e 2074 7573 7070 726f 6574      is not supporte
   e8e14:	0064                                        d.

000e8e16 <_ZZNK11flatbuffers6VectorIlE3GetEmE19__PRETTY_FUNCTION__>:
   e8e16:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
   e8e26:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
   e8e36:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
   e8e46:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
   e8e56:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
   e8e66:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
   e8e76:	7469 2068 2054 203d 6f6c 676e 6920 746e     ith T = long int
   e8e86:	203b 6c66 7461 7562 6666 7265 3a73 563a     ; flatbuffers::V
   e8e96:	6365 6f74 3c72 3e54 3a3a 6572 7574 6e72     ector<T>::return
   e8ea6:	745f 7079 2065 203d 6f6c 676e 6920 746e     _type = long int
   e8eb6:	203b 6c66 7461 7562 6666 7265 3a73 753a     ; flatbuffers::u
   e8ec6:	666f 7366 7465 745f 3d20 6c20 6e6f 2067     offset_t = long 
   e8ed6:	6e75 6973 6e67 6465 6920 746e 005d          unsigned int].

000e8ee4 <_ZZNK11flatbuffers6VectorINS_6OffsetIN6tflite8OperatorEEEE3GetEmE19__PRETTY_FUNCTION__>:
   e8ee4:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
   e8ef4:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
   e8f04:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
   e8f14:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
   e8f24:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
   e8f34:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
   e8f44:	7469 2068 2054 203d 6c66 7461 7562 6666     ith T = flatbuff
   e8f54:	7265 3a73 4f3a 6666 6573 3c74 6674 696c     ers::Offset<tfli
   e8f64:	6574 3a3a 704f 7265 7461 726f 3b3e 6620     te::Operator>; f
   e8f74:	616c 6274 6675 6566 7372 3a3a 6556 7463     latbuffers::Vect
   e8f84:	726f 543c 3a3e 723a 7465 7275 5f6e 7974     or<T>::return_ty
   e8f94:	6570 3d20 6320 6e6f 7473 7420 6c66 7469     pe = const tflit
   e8fa4:	3a65 4f3a 6570 6172 6f74 2a72 203b 6c66     e::Operator*; fl
   e8fb4:	7461 7562 6666 7265 3a73 753a 666f 7366     atbuffers::uoffs
   e8fc4:	7465 745f 3d20 6c20 6e6f 2067 6e75 6973     et_t = long unsi
   e8fd4:	6e67 6465 6920 746e 005d                    gned int].

000e8fde <_ZZNK11flatbuffers6VectorINS_6OffsetIN6tflite6TensorEEEE3GetEmE19__PRETTY_FUNCTION__>:
   e8fde:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
   e8fee:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
   e8ffe:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
   e900e:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
   e901e:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
   e902e:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
   e903e:	7469 2068 2054 203d 6c66 7461 7562 6666     ith T = flatbuff
   e904e:	7265 3a73 4f3a 6666 6573 3c74 6674 696c     ers::Offset<tfli
   e905e:	6574 3a3a 6554 736e 726f 3b3e 6620 616c     te::Tensor>; fla
   e906e:	6274 6675 6566 7372 3a3a 6556 7463 726f     tbuffers::Vector
   e907e:	543c 3a3e 723a 7465 7275 5f6e 7974 6570     <T>::return_type
   e908e:	3d20 6320 6e6f 7473 7420 6c66 7469 3a65      = const tflite:
   e909e:	543a 6e65 6f73 2a72 203b 6c66 7461 7562     :Tensor*; flatbu
   e90ae:	6666 7265 3a73 753a 666f 7366 7465 745f     ffers::uoffset_t
   e90be:	3d20 6c20 6e6f 2067 6e75 6973 6e67 6465      = long unsigned
   e90ce:	6920 746e 005d 6e4f 796c 3120 7320 6275      int].Only 1 sub
   e90de:	7267 7061 2068 7369 6320 7275 6572 746e     graph is current
   e90ee:	796c 7320 7075 6f70 7472 6465 0a2e 3c00     ly supported...<
   e90fe:	6f4e 6e20 6d61 3e65 4900 766e 6c61 6469     No name>.Invalid
   e910e:	7020 6572 612d 6c6c 636f 7461 6465 6920      pre-allocated i
   e911e:	706e 7475 2520 2064 7270 766f 6469 6465     nput %d provided
   e912e:	002e 7241 6e65 2061 6973 657a 6920 2073     ..Arena size is 
   e913e:	6f74 206f 6d73 6c61 206c 6f66 2072 6361     too small for ac
   e914e:	6974 6176 6974 6e6f 6220 6675 6566 7372     tivation buffers
   e915e:	202e 654e 6465 6465 2520 2064 7562 2074     . Needed %d but 
   e916e:	6e6f 796c 2520 2064 6177 2073 7661 6961     only %d was avai
   e917e:	616c 6c62 2e65 5600 7261 6169 6c62 2065     lable..Variable 
   e918e:	7369 6e20 746f 6120 6c6c 636f 7461 6465     is not allocated
   e919e:	4c00 676f 6369 6520 7272 726f 6920 206e     .Logic error in 
   e91ae:	656d 6f6d 7972 7020 616c 6e6e 7265 202c     memory planner, 
   e91be:	6574 736e 726f 2520 2064 6168 2073 6e61     tensor %d has an
   e91ce:	6920 766e 6c61 6469 6c20 6669 7465 6d69      invalid lifetim
   e91de:	0065                                        e.

000e91e0 <_ZZNK11flatbuffers6VectorIfE3GetEmE19__PRETTY_FUNCTION__>:
   e91e0:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
   e91f0:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
   e9200:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
   e9210:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
   e9220:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
   e9230:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
   e9240:	7469 2068 2054 203d 6c66 616f 3b74 6620     ith T = float; f
   e9250:	616c 6274 6675 6566 7372 3a3a 6556 7463     latbuffers::Vect
   e9260:	726f 543c 3a3e 723a 7465 7275 5f6e 7974     or<T>::return_ty
   e9270:	6570 3d20 6620 6f6c 7461 203b 6c66 7461     pe = float; flat
   e9280:	7562 6666 7265 3a73 753a 666f 7366 7465     buffers::uoffset
   e9290:	745f 3d20 6c20 6e6f 2067 6e75 6973 6e67     _t = long unsign
   e92a0:	6465 6920 746e 005d                         ed int].

000e92a8 <_ZZNK11flatbuffers6VectorINS_6OffsetIN6tflite6BufferEEEE3GetEmE19__PRETTY_FUNCTION__>:
   e92a8:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
   e92b8:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
   e92c8:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
   e92d8:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
   e92e8:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
   e92f8:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
   e9308:	7469 2068 2054 203d 6c66 7461 7562 6666     ith T = flatbuff
   e9318:	7265 3a73 4f3a 6666 6573 3c74 6674 696c     ers::Offset<tfli
   e9328:	6574 3a3a 7542 6666 7265 3b3e 6620 616c     te::Buffer>; fla
   e9338:	6274 6675 6566 7372 3a3a 6556 7463 726f     tbuffers::Vector
   e9348:	543c 3a3e 723a 7465 7275 5f6e 7974 6570     <T>::return_type
   e9358:	3d20 6320 6e6f 7473 7420 6c66 7469 3a65      = const tflite:
   e9368:	423a 6675 6566 2a72 203b 6c66 7461 7562     :Buffer*; flatbu
   e9378:	6666 7265 3a73 753a 666f 7366 7465 745f     ffers::uoffset_t
   e9388:	3d20 6c20 6e6f 2067 6e75 6973 6e67 6465      = long unsigned
   e9398:	6920 746e 005d                               int].

000e939e <_ZZNK11flatbuffers6VectorIxE3GetEmE19__PRETTY_FUNCTION__>:
   e939e:	6c66 7461 7562 6666 7265 3a73 563a 6365     flatbuffers::Vec
   e93ae:	6f74 3c72 3e54 3a3a 6572 7574 6e72 745f     tor<T>::return_t
   e93be:	7079 2065 6c66 7461 7562 6666 7265 3a73     ype flatbuffers:
   e93ce:	563a 6365 6f74 3c72 3e54 3a3a 6547 2874     :Vector<T>::Get(
   e93de:	6c66 7461 7562 6666 7265 3a73 753a 666f     flatbuffers::uof
   e93ee:	7366 7465 745f 2029 6f63 736e 2074 775b     fset_t) const [w
   e93fe:	7469 2068 2054 203d 6f6c 676e 6c20 6e6f     ith T = long lon
   e940e:	2067 6e69 3b74 6620 616c 6274 6675 6566     g int; flatbuffe
   e941e:	7372 3a3a 6556 7463 726f 543c 3a3e 723a     rs::Vector<T>::r
   e942e:	7465 7275 5f6e 7974 6570 3d20 6c20 6e6f     eturn_type = lon
   e943e:	2067 6f6c 676e 6920 746e 203b 6c66 7461     g long int; flat
   e944e:	7562 6666 7265 3a73 753a 666f 7366 7465     buffers::uoffset
   e945e:	745f 3d20 6c20 6e6f 2067 6e75 6973 6e67     _t = long unsign
   e946e:	6465 6920 746e 005d 0000                    ed int]...

000e9478 <_ZTVN6tflite18MicroErrorReporterE>:
	...
   e9480:	4135 000d 4149 000d 5d91 000d 0a0d 5400     5A..IA...].....T
   e9490:	6e65 6f73 2072 6e69 6564 2078 6425 6f20     ensor index %d o
   e94a0:	7475 6f20 2066 6172 676e 2065 6c28 6e65     ut of range (len
   e94b0:	7467 2068 7369 2520 2964 4f00 7475 7570     gth is %d).Outpu
   e94c0:	2074 6e69 6564 2078 6425 6f20 7475 6f20     t index %d out o
   e94d0:	2066 6172 676e 2065 6c28 6e65 7467 2068     f range (length 
   e94e0:	7369 2520 2964 4900 706e 7475 6920 646e     is %d).Input ind
   e94f0:	7865 2520 2064 756f 2074 666f 7220 6e61     ex %d out of ran
   e9500:	6567 2820 656c 676e 6874 6920 2073 6425     ge (length is %d
   e9510:	0029 6e49 6f76 656b 2928 6320 6c61 656c     ).Invoke() calle
   e9520:	2064 6661 6574 2072 6e69 7469 6169 696c     d after initiali
   e9530:	617a 6974 6e6f 6620 6961 656c 0a64 4d00     zation failed..M
   e9540:	7369 6973 676e 7220 6765 7369 7274 7461     issing registrat
   e9550:	6f69 206e 6f66 2072 706f 6f63 6564 695f     ion for opcode_i
   e9560:	646e 7865 2520 0a64 5300 696b 7070 6e69     ndex %d..Skippin
   e9570:	2067 706f 6620 726f 6f20 6370 646f 5f65     g op for opcode_
   e9580:	6e69 6564 2078 6425 000a 6e55 7573 7070     index %d..Unsupp
   e9590:	726f 6574 2064 6562 6168 6976 726f 203a     orted behavior: 
   e95a0:	6f66 6e75 2064 7562 6c69 6974 206e 706f     found builtin op
   e95b0:	7265 7461 726f 2520 2073 6977 6874 6320     erator %s with c
   e95c0:	7375 6f74 206d 706f 6974 6e6f 2e73 000a     ustom options...
   e95d0:	6f4e 6564 2520 2073 6e28 6d75 6562 2072     Node %s (number 
   e95e0:	6425 2029 6166 6c69 6465 7420 206f 7270     %d) failed to pr
   e95f0:	7065 7261 2065 6977 6874 7320 6174 7574     epare with statu
   e9600:	2073 6425 4e00 646f 2065 7325 2820 756e     s %d.Node %s (nu
   e9610:	626d 7265 2520 2964 6620 6961 656c 2064     mber %d) failed 
   e9620:	6f74 6920 766e 6b6f 2065 6977 6874 7320     to invoke with s
   e9630:	6174 7574 2073 6425 0000 0000               tatus %d....

000e963c <_ZTVN6tflite12_GLOBAL__N_118StackDataAllocatorE>:
	...
   e9644:	5e49 000d 5e53 000d 5e55 000d 5e77 000d     I^..S^..U^..w^..
   e9654:	7865 6f70 656e 746e 3e20 203d 0030 552f     exponent >= 0./U
   e9664:	6573 7372 622f 6173 7274 6d6f 442f 7665     sers/bsatrom/Dev
   e9674:	6c65 706f 656d 746e 702f 7261 6974 6c63     elopment/particl
   e9684:	2f65 696c 7262 7261 6569 2f73 6150 7472     e/libraries/Part
   e9694:	6369 656c 545f 6e65 6f73 4672 6f6c 4c77     icle_TensorFlowL
   e96a4:	7469 5f65 7845 6d61 6c70 7365 682f 6c65     ite_Examples/hel
   e96b4:	6f6c 775f 726f 646c 6c2f 6269 542f 6e65     lo_world/lib/Ten
   e96c4:	6f73 4672 6f6c 4c77 7469 2f65 7273 2f63     sorFlowLite/src/
   e96d4:	6874 7269 5f64 6170 7472 2f79 6567 6d6d     third_party/gemm
   e96e4:	6f6c 7077 662f 7869 6465 6f70 6e69 2f74     lowp/fixedpoint/
   e96f4:	6966 6578 7064 696f 746e 682e 6500 7078     fixedpoint.h.exp
   e9704:	6e6f 6e65 2074 3d3c 3320 0031 6e49 7570     onent <= 31.Inpu
   e9714:	7374 6120 646e 6f20 7475 7570 7374 6e20     ts and outputs n
   e9724:	746f 6120 6c6c 6620 6f6c 7461 757c 6e69     ot all float|uin
   e9734:	3874 697c 746e 2038 7974 6570 2e73 4900          t8|int8 types..

000e9743 <_ZZN8gemmlowp19RoundingDivideByPOTIlEET_S1_iE19__PRETTY_FUNCTION__>:
   e9743:	6e49 6574 6567 5472 7079 2065 6567 6d6d     IntegerType gemm
   e9753:	6f6c 7077 3a3a 6f52 6e75 6964 676e 6944     lowp::RoundingDi
   e9763:	6976 6564 7942 4f50 2854 6e49 6574 6567     videByPOT(Intege
   e9773:	5472 7079 2c65 6920 746e 2029 775b 7469     rType, int) [wit
   e9783:	2068 6e49 6574 6567 5472 7079 2065 203d     h IntegerType = 
   e9793:	6f6c 676e 6920 746e 005d 0000 0000               long int]....

000e97a0 <_ZTVN6tflite3ops5micro14AllOpsResolverE>:
	...
   e97a8:	61c5 000d 61f3 000d 4137 000d 4139 000d     .a...a..7A..9A..
   e97b8:	6e4f 796c 6620 6f6c 7461 3233 202c 6975     Only float32, ui
   e97c8:	746e 2038 6e61 2064 6e69 3874 6120 6572     nt8 and int8 are
   e97d8:	7320 7075 6f70 7472 6465 6320 7275 6572      supported curre
   e97e8:	746e 796c 202c 6f67 2074 7325 002e 6e4f     ntly, got %s..On
   e97f8:	796c 6920 746e 3233 6120 6572 7320 7075     ly int32 are sup
   e9808:	6f70 7472 6465 6320 7275 6572 746e 796c     ported currently
   e9818:	202c 6f67 2074 7325 002e 552f 6573 7372     , got %s../Users
   e9828:	622f 6173 7274 6d6f 442f 7665 6c65 706f     /bsatrom/Develop
   e9838:	656d 746e 702f 7261 6974 6c63 2f65 696c     ment/particle/li
   e9848:	7262 7261 6569 2f73 6150 7472 6369 656c     braries/Particle
   e9858:	545f 6e65 6f73 4672 6f6c 4c77 7469 5f65     _TensorFlowLite_
   e9868:	7845 6d61 6c70 7365 682f 6c65 6f6c 775f     Examples/hello_w
   e9878:	726f 646c 6c2f 6269 542f 6e65 6f73 4672     orld/lib/TensorF
   e9888:	6f6c 4c77 7469 2f65 7273 2f63 6574 736e     lowLite/src/tens
   e9898:	726f 6c66 776f 6c2f 7469 2f65 7865 6570     orflow/lite/expe
   e98a8:	6972 656d 746e 6c61 6d2f 6369 6f72 6b2f     rimental/micro/k
   e98b8:	7265 656e 736c 632f 6965 2e6c 7063 0070     ernels/ceil.cpp.
   e98c8:	7325 253a 2064 7325 2120 203d 7325 2820     %s:%d %s != %s (
   e98d8:	6425 2120 203d 6425 0029 754e 496d 706e     %d != %d).NumInp
   e98e8:	7475 2873 6f6e 6564 0029 754e 4f6d 7475     uts(node).NumOut
   e98f8:	7570 7374 6e28 646f 2965 6900 706e 7475     puts(node).input
   e9908:	3e2d 7974 6570 6f00 7475 7570 2d74 743e     ->type.output->t
   e9918:	7079 0065 6e69 7570 2d74 623e 7479 7365     ype.input->bytes
   e9928:	6f00 7475 7570 2d74 623e 7479 7365 6900     .output->bytes.i
   e9938:	706e 7475 3e2d 6964 736d 3e2d 6973 657a     nput->dims->size
   e9948:	6f00 7475 7570 2d74 643e 6d69 2d73 733e     .output->dims->s
   e9958:	7a69 0065 6e69 7570 2d74 643e 6d69 2d73     ize.input->dims-
   e9968:	643e 7461 5b61 5d69 6f00 7475 7570 2d74     >data[i].output-
   e9978:	643e 6d69 2d73 643e 7461 5b61 5d69 4400     >dims->data[i].D
   e9988:	656f 2073 6f6e 2074 7573 7070 726f 2074     oes not support 
   e9998:	7974 6570 2520 2c64 7220 7165 6975 6572     type %d, require
   e99a8:	2073 6f62 6c6f 667c 6f6c 7461 697c 746e     s bool|float|int
   e99b8:	757c 6e69 3874 4400 656f 2073 6f6e 2074     |uint8.Does not 
   e99c8:	7573 7070 726f 2074 7974 6570 2520 2c64     support type %d,
   e99d8:	7220 7165 6975 6572 2073 6c66 616f 7c74      requires float|
   e99e8:	6e69 7c74 6975 746e 0038 552f 6573 7372     int|uint8./Users
   e99f8:	622f 6173 7274 6d6f 442f 7665 6c65 706f     /bsatrom/Develop
   e9a08:	656d 746e 702f 7261 6974 6c63 2f65 696c     ment/particle/li
   e9a18:	7262 7261 6569 2f73 6150 7472 6369 656c     braries/Particle
   e9a28:	545f 6e65 6f73 4672 6f6c 4c77 7469 5f65     _TensorFlowLite_
   e9a38:	7845 6d61 6c70 7365 682f 6c65 6f6c 775f     Examples/hello_w
   e9a48:	726f 646c 6c2f 6269 542f 6e65 6f73 4672     orld/lib/TensorF
   e9a58:	6f6c 4c77 7469 2f65 7273 2f63 6574 736e     lowLite/src/tens
   e9a68:	726f 6c66 776f 6c2f 7469 2f65 7865 6570     orflow/lite/expe
   e9a78:	6972 656d 746e 6c61 6d2f 6369 6f72 6b2f     rimental/micro/k
   e9a88:	7265 656e 736c 632f 6e6f 2e76 7063 0070     ernels/conv.cpp.
   e9a98:	7325 253a 2064 7325 7720 7361 6e20 746f     %s:%d %s was not
   e9aa8:	7420 7572 2e65 6800 7361 625f 6169 2073      true..has_bias 
   e9ab8:	7c7c 6e20 646f 2d65 693e 706e 7475 2d73     || node->inputs-
   e9ac8:	733e 7a69 2065 3d3d 3220 6e00 646f 2d65     >size == 2.node-
   e9ad8:	6f3e 7475 7570 7374 3e2d 6973 657a 6b00     >outputs->size.k
   e9ae8:	6654 694c 6574 6641 6966 656e 7551 6e61     TfLiteAffineQuan
   e9af8:	6974 617a 6974 6e6f 6600 6c69 6574 2d72     tization.filter-
   e9b08:	713e 6175 746e 7a69 7461 6f69 2e6e 7974     >quantization.ty
   e9b18:	6570 6100 6666 6e69 5f65 7571 6e61 6974     pe.affine_quanti
   e9b28:	617a 6974 6e6f 6100 6666 6e69 5f65 7571     zation.affine_qu
   e9b38:	6e61 6974 617a 6974 6e6f 3e2d 6373 6c61     antization->scal
   e9b48:	0065 7954 6570 2520 2073 2528 2964 6e20     e.Type %s (%d) n
   e9b58:	746f 7320 7075 6f70 7472 6465 002e          ot supported..

000e9b66 <_ZZN8gemmlowp19RoundingDivideByPOTIlEET_S1_iE19__PRETTY_FUNCTION__>:
   e9b66:	6e49 6574 6567 5472 7079 2065 6567 6d6d     IntegerType gemm
   e9b76:	6f6c 7077 3a3a 6f52 6e75 6964 676e 6944     lowp::RoundingDi
   e9b86:	6976 6564 7942 4f50 2854 6e49 6574 6567     videByPOT(Intege
   e9b96:	5472 7079 2c65 6920 746e 2029 775b 7469     rType, int) [wit
   e9ba6:	2068 6e49 6574 6567 5472 7079 2065 203d     h IntegerType = 
   e9bb6:	6f6c 676e 6920 746e 005d 552f 6573 7372     long int]./Users
   e9bc6:	622f 6173 7274 6d6f 442f 7665 6c65 706f     /bsatrom/Develop
   e9bd6:	656d 746e 702f 7261 6974 6c63 2f65 696c     ment/particle/li
   e9be6:	7262 7261 6569 2f73 6150 7472 6369 656c     braries/Particle
   e9bf6:	545f 6e65 6f73 4672 6f6c 4c77 7469 5f65     _TensorFlowLite_
   e9c06:	7845 6d61 6c70 7365 682f 6c65 6f6c 775f     Examples/hello_w
   e9c16:	726f 646c 6c2f 6269 542f 6e65 6f73 4672     orld/lib/TensorF
   e9c26:	6f6c 4c77 7469 2f65 7273 2f63 6574 736e     lowLite/src/tens
   e9c36:	726f 6c66 776f 6c2f 7469 2f65 7865 6570     orflow/lite/expe
   e9c46:	6972 656d 746e 6c61 6d2f 6369 6f72 6b2f     rimental/micro/k
   e9c56:	7265 656e 736c 642f 7165 6175 746e 7a69     ernels/dequantiz
   e9c66:	2e65 7063 0070 6e69 7570 2d74 743e 7079     e.cpp.input->typ
   e9c76:	2065 3d3d 6b20 6654 694c 6574 4955 746e     e == kTfLiteUInt
   e9c86:	2038 7c7c 6920 706e 7475 3e2d 7974 6570     8 || input->type
   e9c96:	3d20 203d 546b 4c66 7469 4965 746e 0038      == kTfLiteInt8.
   e9ca6:	756f 7074 7475 3e2d 7974 6570 3d20 203d     output->type == 
   e9cb6:	546b 4c66 7469 4665 6f6c 7461 3233 2f00     kTfLiteFloat32./
   e9cc6:	7355 7265 2f73 7362 7461 6f72 2f6d 6544     Users/bsatrom/De
   e9cd6:	6576 6f6c 6d70 6e65 2f74 6170 7472 6369     velopment/partic
   e9ce6:	656c 6c2f 6269 6172 6972 7365 502f 7261     le/libraries/Par
   e9cf6:	6974 6c63 5f65 6554 736e 726f 6c46 776f     ticle_TensorFlow
   e9d06:	694c 6574 455f 6178 706d 656c 2f73 6568     Lite_Examples/he
   e9d16:	6c6c 5f6f 6f77 6c72 2f64 696c 2f62 6554     llo_world/lib/Te
   e9d26:	736e 726f 6c46 776f 694c 6574 732f 6372     nsorFlowLite/src
   e9d36:	742f 6e65 6f73 6672 6f6c 2f77 696c 6574     /tensorflow/lite
   e9d46:	652f 7078 7265 6d69 6e65 6174 2f6c 696d     /experimental/mi
   e9d56:	7263 2f6f 656b 6e72 6c65 2f73 6c65 6d65     cro/kernels/elem
   e9d66:	6e65 7774 7369 2e65 7063 0070 6e49 7570     entwise.cpp.Inpu
   e9d76:	2074 6164 6174 7420 7079 2065 7325 2820     t data type %s (
   e9d86:	6425 2029 7369 6e20 746f 7320 7075 6f70     %d) is not suppo
   e9d96:	7472 6465 002e 7865 6570 7463 6465 745f     rted..expected_t
   e9da6:	7079 0065 552f 6573 7372 622f 6173 7274     ype./Users/bsatr
   e9db6:	6d6f 442f 7665 6c65 706f 656d 746e 702f     om/Development/p
   e9dc6:	7261 6974 6c63 2f65 696c 7262 7261 6569     article/librarie
   e9dd6:	2f73 6150 7472 6369 656c 545f 6e65 6f73     s/Particle_Tenso
   e9de6:	4672 6f6c 4c77 7469 5f65 7845 6d61 6c70     rFlowLite_Exampl
   e9df6:	7365 682f 6c65 6f6c 775f 726f 646c 6c2f     es/hello_world/l
   e9e06:	6269 542f 6e65 6f73 4672 6f6c 4c77 7469     ib/TensorFlowLit
   e9e16:	2f65 7273 2f63 6574 736e 726f 6c66 776f     e/src/tensorflow
   e9e26:	6c2f 7469 2f65 7865 6570 6972 656d 746e     /lite/experiment
   e9e36:	6c61 6d2f 6369 6f72 6b2f 7265 656e 736c     al/micro/kernels
   e9e46:	662f 6f6c 726f 632e 7070 5100 6175 746e     /floor.cpp.Quant
   e9e56:	7a69 6465 4620 6c75 796c 6f43 6e6e 6365     ized FullyConnec
   e9e66:	6574 2064 7865 6570 7463 2073 756f 7074     ted expects outp
   e9e76:	7475 6420 7461 2061 7974 6570 7520 6e69     ut data type uin
   e9e86:	3874 6f20 2072 6e69 3174 0036 7954 6570     t8 or int16.Type
   e9e96:	2520 2064 6f6e 2074 7563 7272 6e65 6c74      %d not currentl
   e9ea6:	2079 7573 7070 726f 6574 2e64 4f00 6c6e     y supported..Onl
   e9eb6:	2079 6c66 616f 3374 2032 7369 7320 7075     y float32 is sup
   e9ec6:	6f70 7472 6465 6320 7275 6572 746e 796c     ported currently
   e9ed6:	202c 6f67 2074 7325 5400 7079 2065 7325     , got %s.Type %s
   e9ee6:	2820 6425 2029 7369 6e20 746f 7320 7075      (%d) is not sup
   e9ef6:	6f70 7472 6465 6220 2079 614d 6978 756d     ported by Maximu
   e9f06:	2f6d 694d 696e 756d 2e6d 4e00 6765 6f20     m/Minimum..Neg o
   e9f16:	6c6e 2079 7563 7272 6e65 6c74 2079 7573     nly currently su
   e9f26:	7070 726f 7374 6620 6f6c 7461 3233 202c     pports float32, 
   e9f36:	6f67 2074 6425 002e 7954 6570 2720 7325     got %d..Type '%s
   e9f46:	2027 7369 6e20 746f 7320 7075 6f70 7472     ' is not support
   e9f56:	6465 6220 2079 6170 6b63 002e 7954 6570     ed by pack..Type
   e9f66:	2520 2073 6f6e 2074 7563 7272 6e65 6c74      %s not currentl
   e9f76:	2079 7573 7070 726f 6574 2e64 4900 706e     y supported..Inp
   e9f86:	7475 7420 7079 2065 7325 6920 2073 6f6e     ut type %s is no
   e9f96:	2074 7563 7272 6e65 6c74 2079 7573 7070     t currently supp
   e9fa6:	726f 6574 0064 6e4f 796c 6620 6f6c 7461     orted.Only float
   e9fb6:	3233 6120 646e 7520 6e69 3874 6120 6572     32 and uint8 are
   e9fc6:	7320 7075 6f70 7472 6465 6320 7275 6572      supported curre
   e9fd6:	746e 796c 202c 6f67 2074 6425 002e          ntly, got %d..

000e9fe4 <_ZZN8gemmlowp19RoundingDivideByPOTIlEET_S1_iE19__PRETTY_FUNCTION__>:
   e9fe4:	6e49 6574 6567 5472 7079 2065 6567 6d6d     IntegerType gemm
   e9ff4:	6f6c 7077 3a3a 6f52 6e75 6964 676e 6944     lowp::RoundingDi
   ea004:	6976 6564 7942 4f50 2854 6e49 6574 6567     videByPOT(Intege
   ea014:	5472 7079 2c65 6920 746e 2029 775b 7469     rType, int) [wit
   ea024:	2068 6e49 6574 6567 5472 7079 2065 203d     h IntegerType = 
   ea034:	6f6c 676e 6920 746e 005d 552f 6573 7372     long int]./Users
   ea044:	622f 6173 7274 6d6f 442f 7665 6c65 706f     /bsatrom/Develop
   ea054:	656d 746e 702f 7261 6974 6c63 2f65 696c     ment/particle/li
   ea064:	7262 7261 6569 2f73 6150 7472 6369 656c     braries/Particle
   ea074:	545f 6e65 6f73 4672 6f6c 4c77 7469 5f65     _TensorFlowLite_
   ea084:	7845 6d61 6c70 7365 682f 6c65 6f6c 775f     Examples/hello_w
   ea094:	726f 646c 6c2f 6269 542f 6e65 6f73 4672     orld/lib/TensorF
   ea0a4:	6f6c 4c77 7469 2f65 7273 2f63 6574 736e     lowLite/src/tens
   ea0b4:	726f 6c66 776f 6c2f 7469 2f65 7865 6570     orflow/lite/expe
   ea0c4:	6972 656d 746e 6c61 6d2f 6369 6f72 6b2f     rimental/micro/k
   ea0d4:	7265 656e 736c 712f 6175 746e 7a69 2e65     ernels/quantize.
   ea0e4:	7063 0070 756f 7074 7475 3e2d 7571 6e61     cpp.output->quan
   ea0f4:	6974 617a 6974 6e6f 742e 7079 0065 6661     tization.type.af
   ea104:	6966 656e 715f 6175 746e 7a69 7461 6f69     fine_quantizatio
   ea114:	2d6e 733e 6163 656c 3e2d 6973 657a 3d20     n->scale->size =
   ea124:	203d 0031 6e69 7570 2d74 743e 7079 2065     = 1.input->type 
   ea134:	3d3d 6b20 6654 694c 6574 6c46 616f 3374     == kTfLiteFloat3
   ea144:	0032 756f 7074 7475 3e2d 7974 6570 3d20     2.output->type =
   ea154:	203d 546b 4c66 7469 5565 6e49 3874 7c20     = kTfLiteUInt8 |
   ea164:	207c 756f 7074 7475 3e2d 7974 6570 3d20     | output->type =
   ea174:	203d 546b 4c66 7469 4965 746e 0038 754f     = kTfLiteInt8.Ou
   ea184:	7074 7475 7420 7079 2065 7325 2820 6425     tput type %s (%d
   ea194:	2029 6f6e 2074 7573 7070 726f 6574 0064     ) not supported.
   ea1a4:	552f 6573 7372 622f 6173 7274 6d6f 442f     /Users/bsatrom/D
   ea1b4:	7665 6c65 706f 656d 746e 702f 7261 6974     evelopment/parti
   ea1c4:	6c63 2f65 696c 7262 7261 6569 2f73 6150     cle/libraries/Pa
   ea1d4:	7472 6369 656c 545f 6e65 6f73 4672 6f6c     rticle_TensorFlo
   ea1e4:	4c77 7469 5f65 7845 6d61 6c70 7365 682f     wLite_Examples/h
   ea1f4:	6c65 6f6c 775f 726f 646c 6c2f 6269 542f     ello_world/lib/T
   ea204:	6e65 6f73 4672 6f6c 4c77 7469 2f65 7273     ensorFlowLite/sr
   ea214:	2f63 6574 736e 726f 6c66 776f 6c2f 7469     c/tensorflow/lit
   ea224:	2f65 7865 6570 6972 656d 746e 6c61 6d2f     e/experimental/m
   ea234:	6369 6f72 6b2f 7265 656e 736c 722f 7365     icro/kernels/res
   ea244:	6168 6570 632e 7070 4e00 6d75 6e49 7570     hape.cpp.NumInpu
   ea254:	7374 6e28 646f 2965 3d20 203d 2031 7c7c     ts(node) == 1 ||
   ea264:	4e20 6d75 6e49 7570 7374 6e28 646f 2965      NumInputs(node)
   ea274:	3d20 203d 0032 312d 7300 7274 7465 6863      == 2.-1.stretch
   ea284:	645f 6d69 6e00 6d75 6f5f 7475 7570 5f74     _dim.num_output_
   ea294:	6c65 6d65 6e65 7374 6e00 6d75 695f 706e     elements.num_inp
   ea2a4:	7475 655f 656c 656d 746e 0073 552f 6573     ut_elements./Use
   ea2b4:	7372 622f 6173 7274 6d6f 442f 7665 6c65     rs/bsatrom/Devel
   ea2c4:	706f 656d 746e 702f 7261 6974 6c63 2f65     opment/particle/
   ea2d4:	696c 7262 7261 6569 2f73 6150 7472 6369     libraries/Partic
   ea2e4:	656c 545f 6e65 6f73 4672 6f6c 4c77 7469     le_TensorFlowLit
   ea2f4:	5f65 7845 6d61 6c70 7365 682f 6c65 6f6c     e_Examples/hello
   ea304:	775f 726f 646c 6c2f 6269 542f 6e65 6f73     _world/lib/Tenso
   ea314:	4672 6f6c 4c77 7469 2f65 7273 2f63 6574     rFlowLite/src/te
   ea324:	736e 726f 6c66 776f 6c2f 7469 2f65 7865     nsorflow/lite/ex
   ea334:	6570 6972 656d 746e 6c61 6d2f 6369 6f72     perimental/micro
   ea344:	6b2f 7265 656e 736c 722f 756f 646e 632e     /kernels/round.c
   ea354:	7070 2f00 7355 7265 2f73 7362 7461 6f72     pp./Users/bsatro
   ea364:	2f6d 6544 6576 6f6c 6d70 6e65 2f74 6170     m/Development/pa
   ea374:	7472 6369 656c 6c2f 6269 6172 6972 7365     rticle/libraries
   ea384:	502f 7261 6974 6c63 5f65 6554 736e 726f     /Particle_Tensor
   ea394:	6c46 776f 694c 6574 455f 6178 706d 656c     FlowLite_Example
   ea3a4:	2f73 6568 6c6c 5f6f 6f77 6c72 2f64 696c     s/hello_world/li
   ea3b4:	2f62 6554 736e 726f 6c46 776f 694c 6574     b/TensorFlowLite
   ea3c4:	732f 6372 742f 6e65 6f73 6672 6f6c 2f77     /src/tensorflow/
   ea3d4:	696c 6574 652f 7078 7265 6d69 6e65 6174     lite/experimenta
   ea3e4:	2f6c 696d 7263 2f6f 656b 6e72 6c65 2f73     l/micro/kernels/
   ea3f4:	6f73 7466 616d 2e78 7063 0070 756f 7074     softmax.cpp.outp
   ea404:	7475 3e2d 6170 6172 736d 7a2e 7265 5f6f     ut->params.zero_
   ea414:	6f70 6e69 0074 756f 7074 7475 3e2d 6170     point.output->pa
   ea424:	6172 736d 732e 6163 656c 3d20 203d 2e31     rams.scale == 1.
   ea434:	2066 202f 3532 0036 6e4f 796c 3120 2c44     f / 256.Only 1D,
   ea444:	3220 2044 6e61 2064 4434 7420 6e65 6f73      2D and 4D tenso
   ea454:	7372 7320 7075 6f70 7472 6465 6320 7275     rs supported cur
   ea464:	6572 746e 796c 202c 6f67 2074 6425 2e44     rently, got %dD.
   ea474:	4f00 6c6e 2079 4432 6120 646e 3420 2044     .Only 2D and 4D 
   ea484:	6574 736e 726f 2073 7573 7070 726f 6574     tensors supporte
   ea494:	2064 7563 7272 6e65 6c74 2c79 6720 746f     d currently, got
   ea4a4:	2520 4464 002e 6e4f 796c 6620 6f6c 7461      %dD..Only float
   ea4b4:	3233 6120 646e 7520 6e69 3874 745f 7320     32 and uint8_t s
   ea4c4:	7075 6f70 7472 6465 6320 7275 6572 746e     upported current
   ea4d4:	796c 202c 6f67 2074 6425 002e               ly, got %d..

000ea4e0 <_ZZN8gemmlowp19RoundingDivideByPOTIlEET_S1_iE19__PRETTY_FUNCTION__>:
   ea4e0:	6e49 6574 6567 5472 7079 2065 6567 6d6d     IntegerType gemm
   ea4f0:	6f6c 7077 3a3a 6f52 6e75 6964 676e 6944     lowp::RoundingDi
   ea500:	6976 6564 7942 4f50 2854 6e49 6574 6567     videByPOT(Intege
   ea510:	5472 7079 2c65 6920 746e 2029 775b 7469     rType, int) [wit
   ea520:	2068 6e49 6574 6567 5472 7079 2065 203d     h IntegerType = 
   ea530:	6f6c 676e 6920 746e 005d 552f 6573 7372     long int]./Users
   ea540:	622f 6173 7274 6d6f 442f 7665 6c65 706f     /bsatrom/Develop
   ea550:	656d 746e 702f 7261 6974 6c63 2f65 696c     ment/particle/li
   ea560:	7262 7261 6569 2f73 6150 7472 6369 656c     braries/Particle
   ea570:	545f 6e65 6f73 4672 6f6c 4c77 7469 5f65     _TensorFlowLite_
   ea580:	7845 6d61 6c70 7365 682f 6c65 6f6c 775f     Examples/hello_w
   ea590:	726f 646c 6c2f 6269 542f 6e65 6f73 4672     orld/lib/TensorF
   ea5a0:	6f6c 4c77 7469 2f65 7273 2f63 6574 736e     lowLite/src/tens
   ea5b0:	726f 6c66 776f 6c2f 7469 2f65 7865 6570     orflow/lite/expe
   ea5c0:	6972 656d 746e 6c61 6d2f 6369 6f72 6b2f     rimental/micro/k
   ea5d0:	7265 656e 736c 732f 6c70 7469 632e 7070     ernels/split.cpp
   ea5e0:	4e20 6e6f 6320 6e6f 7473 6e61 2074 7861      Non constant ax
   ea5f0:	7369 7420 6e65 6f73 2072 6f6e 2074 7573     is tensor not su
   ea600:	7070 726f 6574 0064 552f 6573 7372 622f     pported./Users/b
   ea610:	6173 7274 6d6f 442f 7665 6c65 706f 656d     satrom/Developme
   ea620:	746e 702f 7261 6974 6c63 2f65 696c 7262     nt/particle/libr
   ea630:	7261 6569 2f73 6150 7472 6369 656c 545f     aries/Particle_T
   ea640:	6e65 6f73 4672 6f6c 4c77 7469 5f65 7845     ensorFlowLite_Ex
   ea650:	6d61 6c70 7365 682f 6c65 6f6c 775f 726f     amples/hello_wor
   ea660:	646c 6c2f 6269 542f 6e65 6f73 4672 6f6c     ld/lib/TensorFlo
   ea670:	4c77 7469 2f65 7273 2f63 6574 736e 726f     wLite/src/tensor
   ea680:	6c66 776f 6c2f 7469 2f65 7865 6570 6972     flow/lite/experi
   ea690:	656d 746e 6c61 6d2f 6369 6f72 6b2f 7265     mental/micro/ker
   ea6a0:	656e 736c 732f 6c70 7469 632e 7070 6100     nels/split.cpp.a
   ea6b0:	6978 5f73 6176 756c 2065 3d3e 3020 6100     xis_value >= 0.a
   ea6c0:	6978 5f73 6176 756c 2065 203c 754e 446d     xis_value < NumD
   ea6d0:	6d69 6e65 6973 6e6f 2873 6e69 7570 2974     imensions(input)
   ea6e0:	5400 7079 2065 7325 6320 7275 6572 746e     .Type %s current
   ea6f0:	796c 6e20 746f 7320 7075 6f70 7472 6465     ly not supported
   ea700:	002e 552f 6573 7372 622f 6173 7274 6d6f     ../Users/bsatrom
   ea710:	442f 7665 6c65 706f 656d 746e 702f 7261     /Development/par
   ea720:	6974 6c63 2f65 696c 7262 7261 6569 2f73     ticle/libraries/
   ea730:	6150 7472 6369 656c 545f 6e65 6f73 4672     Particle_TensorF
   ea740:	6f6c 4c77 7469 5f65 7845 6d61 6c70 7365     lowLite_Examples
   ea750:	682f 6c65 6f6c 775f 726f 646c 6c2f 6269     /hello_world/lib
   ea760:	542f 6e65 6f73 4672 6f6c 4c77 7469 2f65     /TensorFlowLite/
   ea770:	7273 2f63 6574 736e 726f 6c66 776f 6c2f     src/tensorflow/l
   ea780:	7469 2f65 7865 6570 6972 656d 746e 6c61     ite/experimental
   ea790:	6d2f 6369 6f72 6b2f 7265 656e 736c 732f     /micro/kernels/s
   ea7a0:	7274 6469 6465 735f 696c 6563 632e 7070     trided_slice.cpp
   ea7b0:	7320 7274 6469 2065 6176 756c 2065 6168      stride value ha
   ea7c0:	2073 6f74 6220 2065 6f6e 2d6e 657a 6f72     s to be non-zero
   ea7d0:	2f00 7355 7265 2f73 7362 7461 6f72 2f6d     ./Users/bsatrom/
   ea7e0:	6544 6576 6f6c 6d70 6e65 2f74 6170 7472     Development/part
   ea7f0:	6369 656c 6c2f 6269 6172 6972 7365 502f     icle/libraries/P
   ea800:	7261 6974 6c63 5f65 6554 736e 726f 6c46     article_TensorFl
   ea810:	776f 694c 6574 455f 6178 706d 656c 2f73     owLite_Examples/
   ea820:	6568 6c6c 5f6f 6f77 6c72 2f64 696c 2f62     hello_world/lib/
   ea830:	6554 736e 726f 6c46 776f 694c 6574 732f     TensorFlowLite/s
   ea840:	6372 742f 6e65 6f73 6672 6f6c 2f77 696c     rc/tensorflow/li
   ea850:	6574 652f 7078 7265 6d69 6e65 6174 2f6c     te/experimental/
   ea860:	696d 7263 2f6f 656b 6e72 6c65 2f73 7473     micro/kernels/st
   ea870:	6972 6564 5f64 6c73 6369 2e65 7063 0070     rided_slice.cpp.
   ea880:	6964 5f6d 6873 7061 0065 756f 7074 7475     dim_shape.output
   ea890:	735f 6168 6570 3e2d 6164 6174 735b 6168     _shape->data[sha
   ea8a0:	6570 735f 7a69 5d65 7300 6168 6570 735f     pe_size].shape_s
   ea8b0:	7a69 0065 756f 7074 7475 735f 6168 6570     ize.output_shape
   ea8c0:	3e2d 6973 657a 2f00 7355 7265 2f73 7362     ->size./Users/bs
   ea8d0:	7461 6f72 2f6d 6544 6576 6f6c 6d70 6e65     atrom/Developmen
   ea8e0:	2f74 6170 7472 6369 656c 6c2f 6269 6172     t/particle/libra
   ea8f0:	6972 7365 502f 7261 6974 6c63 5f65 6554     ries/Particle_Te
   ea900:	736e 726f 6c46 776f 694c 6574 455f 6178     nsorFlowLite_Exa
   ea910:	706d 656c 2f73 6568 6c6c 5f6f 6f77 6c72     mples/hello_worl
   ea920:	2f64 696c 2f62 6554 736e 726f 6c46 776f     d/lib/TensorFlow
   ea930:	694c 6574 732f 6372 742f 6e65 6f73 6672     Lite/src/tensorf
   ea940:	6f6c 2f77 696c 6574 652f 7078 7265 6d69     low/lite/experim
   ea950:	6e65 6174 2f6c 696d 7263 2f6f 656b 6e72     ental/micro/kern
   ea960:	6c65 2f73 7473 6972 6564 5f64 6c73 6369     els/strided_slic
   ea970:	2e65 7063 2070 6e69 7570 2074 6964 206d     e.cpp input dim 
   ea980:	6873 756f 646c 6e20 746f 6520 6378 6565     should not excee
   ea990:	2064 0034 7954 6570 2520 2064 7369 6320     d 4.Type %d is c
   ea9a0:	7275 6572 746e 796c 6e20 746f 7320 7075     urrently not sup
   ea9b0:	6f70 7472 6465 6220 2079 7453 6972 6564     ported by Stride
   ea9c0:	5364 696c 6563 002e 552f 6573 7372 622f     dSlice../Users/b
   ea9d0:	6173 7274 6d6f 442f 7665 6c65 706f 656d     satrom/Developme
   ea9e0:	746e 702f 7261 6974 6c63 2f65 696c 7262     nt/particle/libr
   ea9f0:	7261 6569 2f73 6150 7472 6369 656c 545f     aries/Particle_T
   eaa00:	6e65 6f73 4672 6f6c 4c77 7469 5f65 7845     ensorFlowLite_Ex
   eaa10:	6d61 6c70 7365 682f 6c65 6f6c 775f 726f     amples/hello_wor
   eaa20:	646c 6c2f 6269 542f 6e65 6f73 4672 6f6c     ld/lib/TensorFlo
   eaa30:	4c77 7469 2f65 7273 2f63 6574 736e 726f     wLite/src/tensor
   eaa40:	6c66 776f 6c2f 7469 2f65 7865 6570 6972     flow/lite/experi
   eaa50:	656d 746e 6c61 6d2f 6369 6f72 6b2f 7265     mental/micro/ker
   eaa60:	656e 736c 732f 6476 2e66 7063 0070 6f6e     nels/svdf.cpp.no
   eaa70:	6564 3e2d 6e69 7570 7374 3e2d 6973 657a     de->inputs->size
   eaa80:	6e00 6d75 665f 6c69 6574 7372 2520 7220     .num_filters % r
   eaa90:	6e61 006b 754e 446d 6d69 6e65 6973 6e6f     ank.NumDimension
   eaaa0:	2873 6577 6769 7468 5f73 6566 7461 7275     s(weights_featur
   eaab0:	2965 6900 706e 7475 735f 7a69 0065 6577     e).input_size.we
   eaac0:	6769 7468 5f73 6566 7461 7275 2d65 643e     ights_feature->d
   eaad0:	6d69 2d73 643e 7461 5b61 5d31 4e00 6d75     ims->data[1].Num
   eaae0:	6944 656d 736e 6f69 736e 7728 6965 6867     Dimensions(weigh
   eaaf0:	7374 745f 6d69 2965 6e00 6d75 755f 696e     ts_time).num_uni
   eab00:	7374 6200 6169 2d73 643e 6d69 2d73 643e     ts.bias->dims->d
   eab10:	7461 5b61 5d30 6200 6169 2d73 743e 7079     ata[0].bias->typ
   eab20:	0065 6361 6974 6176 6974 6e6f 735f 6174     e.activation_sta
   eab30:	6574 3e2d 7974 6570 4e00 6d75 6944 656d     te->type.NumDime
   eab40:	736e 6f69 736e 6128 7463 7669 7461 6f69     nsions(activatio
   eab50:	5f6e 7473 7461 2965 6200 7461 6863 735f     n_state).batch_s
   eab60:	7a69 0065 6361 6974 6176 6974 6e6f 735f     ize.activation_s
   eab70:	6174 6574 3e2d 6964 736d 3e2d 6164 6174     tate->dims->data
   eab80:	305b 005d 656d 6f6d 7972 735f 7a69 2065     [0].memory_size 
   eab90:	202a 756e 5f6d 6966 746c 7265 0073 6361     * num_filters.ac
   eaba0:	6974 6176 6974 6e6f 735f 6174 6574 3e2d     tivation_state->
   eabb0:	6964 736d 3e2d 6164 6174 315b 005d 6373     dims->data[1].sc
   eabc0:	6172 6374 5f68 6574 736e 726f 3e2d 7974     ratch_tensor->ty
   eabd0:	6570 4e00 6d75 6944 656d 736e 6f69 736e     pe.NumDimensions
   eabe0:	7328 7263 7461 6863 745f 6e65 6f73 2972     (scratch_tensor)
   eabf0:	7300 7263 7461 6863 745f 6e65 6f73 2d72     .scratch_tensor-
   eac00:	643e 6d69 2d73 643e 7461 5b61 5d30 7300     >dims->data[0].s
   eac10:	7263 7461 6863 745f 6e65 6f73 2d72 643e     cratch_tensor->d
   eac20:	6d69 2d73 643e 7461 5b61 5d31 7700 6965     ims->data[1].wei
   eac30:	6867 7374 745f 6d69 2d65 743e 7079 2065     ghts_time->type 
   eac40:	3d3d 6b20 6654 694c 6574 4955 746e 2038     == kTfLiteUInt8 
   eac50:	7c7c 7720 6965 6867 7374 745f 6d69 2d65     || weights_time-
   eac60:	743e 7079 2065 3d3d 6b20 6654 694c 6574     >type == kTfLite
   eac70:	6e49 3874 6e00 646f 2d65 743e 6d65 6f70     Int8.node->tempo
   eac80:	6172 6972 7365 3e2d 6973 657a 7300 7263     raries->size.scr
   eac90:	7461 6863 695f 706e 7475 715f 6175 746e     atch_input_quant
   eaca0:	7a69 6465 3e2d 7974 6570 3d20 203d 546b     ized->type == kT
   eacb0:	4c66 7469 5565 6e49 3874 7c20 207c 6373     fLiteUInt8 || sc
   eacc0:	6172 6374 5f68 6e69 7570 5f74 7571 6e61     ratch_input_quan
   eacd0:	6974 657a 2d64 743e 7079 2065 3d3d 6b20     tized->type == k
   eace0:	6654 694c 6574 6e49 3874 7300 7263 7461     TfLiteInt8.scrat
   eacf0:	6863 695f 706e 7475 715f 6175 746e 7a69     ch_input_quantiz
   ead00:	6465 3e2d 6964 736d 3e2d 6164 6174 305b     ed->dims->data[0
   ead10:	005d 6373 6172 6374 5f68 6373 6c61 6e69     ].scratch_scalin
   ead20:	5f67 6166 7463 726f 2d73 743e 7079 0065     g_factors->type.
   ead30:	754e 446d 6d69 6e65 6973 6e6f 2873 6373     NumDimensions(sc
   ead40:	6172 6374 5f68 6373 6c61 6e69 5f67 6166     ratch_scaling_fa
   ead50:	7463 726f 2973 7300 7263 7461 6863 735f     ctors).scratch_s
   ead60:	6163 696c 676e 665f 6361 6f74 7372 3e2d     caling_factors->
   ead70:	6964 736d 3e2d 6164 6174 305b 005d 6373     dims->data[0].sc
   ead80:	6172 6374 5f68 6c66 616f 5f74 6577 6769     ratch_float_weig
   ead90:	7468 5f73 6974 656d 3e2d 7974 6570 4e00     hts_time->type.N
   eada0:	6d75 6944 656d 736e 6f69 736e 7328 7263     umDimensions(scr
   eadb0:	7461 6863 665f 6f6c 7461 775f 6965 6867     atch_float_weigh
   eadc0:	7374 745f 6d69 2965 7300 7263 7461 6863     ts_time).scratch
   eadd0:	665f 6f6c 7461 775f 6965 6867 7374 745f     _float_weights_t
   eade0:	6d69 2d65 643e 6d69 2d73 643e 7461 5b61     ime->dims->data[
   eadf0:	5d30 6d00 6d65 726f 5f79 6973 657a 7300     0].memory_size.s
   eae00:	7263 7461 6863 665f 6f6c 7461 775f 6965     cratch_float_wei
   eae10:	6867 7374 745f 6d69 2d65 643e 6d69 2d73     ghts_time->dims-
   eae20:	643e 7461 5b61 5d31 7700 6965 6867 7374     >data[1].weights
   eae30:	665f 6165 7574 6572 3e2d 7974 6570 4e00     _feature->type.N
   eae40:	6d75 6944 656d 736e 6f69 736e 6f28 7475     umDimensions(out
   eae50:	7570 2974 6f00 7475 7570 2d74 643e 6d69     put).output->dim
   eae60:	2d73 643e 7461 5b61 5d30 6f00 7475 7570     s->data[0].outpu
   eae70:	2d74 643e 6d69 2d73 643e 7461 5b61 5d31     t->dims->data[1]
   eae80:	5400 7079 2065 2527 2773 6920 2073 6f6e     .Type '%s' is no
   eae90:	2074 7573 7070 726f 6574 2064 7962 7520     t supported by u
   eaea0:	706e 6361 2e6b 2f00 7355 7265 2f73 7362     npack../Users/bs
   eaeb0:	7461 6f72 2f6d 6544 6576 6f6c 6d70 6e65     atrom/Developmen
   eaec0:	2f74 6170 7472 6369 656c 6c2f 6269 6172     t/particle/libra
   eaed0:	6972 7365 502f 7261 6974 6c63 5f65 6554     ries/Particle_Te
   eaee0:	736e 726f 6c46 776f 694c 6574 455f 6178     nsorFlowLite_Exa
   eaef0:	706d 656c 2f73 6568 6c6c 5f6f 6f77 6c72     mples/hello_worl
   eaf00:	2f64 696c 2f62 6554 736e 726f 6c46 776f     d/lib/TensorFlow
   eaf10:	694c 6574 732f 6372 742f 6e65 6f73 6672     Lite/src/tensorf
   eaf20:	6f6c 2f77 696c 6574 652f 7078 7265 6d69     low/lite/experim
   eaf30:	6e65 6174 2f6c 696d 7263 2f6f 656b 6e72     ental/micro/kern
   eaf40:	6c65 2f73 6f70 7472 6261 656c 6f5f 7470     els/portable_opt
   eaf50:	6d69 7a69 6465 642f 7065 6874 6977 6573     imized/depthwise
   eaf60:	635f 6e6f 2e76 7063 0070 754d 746c 7069     _conv.cpp.Multip
   eaf70:	656c 6420 7065 6874 6977 6573 6320 6e6f     le depthwise con
   eaf80:	2076 706f 2073 616d 6374 2068 706f 6974     v ops match opti
   eaf90:	696d 617a 6974 6e6f 7020 7261 6d61 7465     mization paramet
   eafa0:	7265 2c73 6220 7475 6f20 6c6e 2079 6874     ers, but only th
   eafb0:	2065 6966 7372 2074 6977 6c6c 7520 6573     e first will use
   eafc0:	7420 6568 6620 7361 2074 6170 6874 202c      the fast path, 
   eafd0:	6562 6163 7375 2065 6874 7265 2765 2073     because there's 
   eafe0:	6e6f 796c 6f20 656e 5220 4d41 6320 6361     only one RAM cac
   eaff0:	6568 6120 6176 6c69 6261 656c 5300 7a69     he available.Siz
   eb000:	2065 6f74 206f 616c 6772 2065 6f66 2072     e too large for 
   eb010:	6572 6873 7061 6465 7720 6965 6867 2074     reshaped weight 
   eb020:	7562 6666 7265 2820 6425 6e20 6565 6564     buffer (%d neede
   eb030:	2c64 2520 2064 7661 6961 616c 6c62 2965     d, %d available)
   eb040:	5400 6f6f 6d20 6e61 2079 7562 6666 7265     .Too many buffer
   eb050:	2073 6d28 7861 6920 2073 6425 0029 7562     s (max is %d).bu
   eb060:	6666 7265 6920 646e 7865 2520 2064 7369     ffer index %d is
   eb070:	6f20 7475 6973 6564 7220 6e61 6567 3020      outside range 0
   eb080:	7420 206f 6425 4f00 6576 6c72 7061 203a      to %d.Overlap: 
   eb090:	6425 2820 6425 3e3d 6425 202c 6425 3e2d     %d (%d=>%d, %d->
   eb0a0:	6425 2029 7376 2520 2064 2528 3d64 253e     %d) vs %d (%d=>%
   eb0b0:	2c64 2520 2d64 253e 2964 0000               d, %d->%d)..

000eb0bc <_ZTVN6tflite19GreedyMemoryPlannerE>:
	...
   eb0c4:	3685 000e 368b 000e 3699 000e 38e3 000e     .6...6...6...8..
   eb0d4:	3687 000e 3919 000e 552f 6573 7372 622f     .6...9../Users/b
   eb0e4:	6173 7274 6d6f 442f 7665 6c65 706f 656d     satrom/Developme
   eb0f4:	746e 702f 7261 6974 6c63 2f65 696c 7262     nt/particle/libr
   eb104:	7261 6569 2f73 6150 7472 6369 656c 545f     aries/Particle_T
   eb114:	6e65 6f73 4672 6f6c 4c77 7469 5f65 7845     ensorFlowLite_Ex
   eb124:	6d61 6c70 7365 682f 6c65 6f6c 775f 726f     amples/hello_wor
   eb134:	646c 6c2f 6269 542f 6e65 6f73 4672 6f6c     ld/lib/TensorFlo
   eb144:	4c77 7469 2f65 7273 2f63 6574 736e 726f     wLite/src/tensor
   eb154:	6c66 776f 6c2f 7469 2f65 656b 6e72 6c65     flow/lite/kernel
   eb164:	2f73 656b 6e72 6c65 755f 6974 2e6c 7063     s/kernel_util.cp
   eb174:	0070 6e69 7570 5f74 7270 646f 6375 5f74     p.input_product_
   eb184:	6373 6c61 2065 3d3e 3020 7300 6474 3a3a     scale >= 0.std::
   eb194:	6261 2873 6e69 7570 5f74 7270 646f 6375     abs(input_produc
   eb1a4:	5f74 6373 6c61 2065 202d 6962 7361 735f     t_scale - bias_s
   eb1b4:	6163 656c 2029 3d3c 3120 2d65 2036 202a     cale) <= 1e-6 * 
   eb1c4:	7473 3a64 6d3a 6e69 6928 706e 7475 705f     std::min(input_p
   eb1d4:	6f72 7564 7463 735f 6163 656c 202c 6962     roduct_scale, bi
   eb1e4:	7361 735f 6163 656c 0029 6166 736c 0065     as_scale).false.
   eb1f4:	6e69 7570 2d74 713e 6175 746e 7a69 7461     input->quantizat
   eb204:	6f69 2e6e 7974 6570 6600 6c69 6574 2d72     ion.type.filter-
   eb214:	743e 7079 0065 6966 746c 7265 3e2d 6964     >type.filter->di
   eb224:	736d 3e2d 6164 6174 615b 6666 6e69 5f65     ms->data[affine_
   eb234:	7571 6e61 6974 617a 6974 6e6f 3e2d 7571     quantization->qu
   eb244:	6e61 6974 657a 5f64 6964 656d 736e 6f69     antized_dimensio
   eb254:	5d6e 6100 6666 6e69 5f65 7571 6e61 6974     n].affine_quanti
   eb264:	617a 6974 6e6f 3e2d 6373 6c61 2d65 733e     zation->scale->s
   eb274:	7a69 0065 3164 3d20 203d 3264 7c20 207c     ize.d1 == d2 || 
   eb284:	3164 3d20 203d 2031 7c7c 6420 2032 3d3d     d1 == 1 || d2 ==
   eb294:	3120 0000                                    1..

000eb298 <_ZTVN5spark13EthernetClassE>:
	...
   eb2a0:	43fd 000e 43f3 000e 43e9 000e 43df 000e     .C...C...C...C..
   eb2b0:	43d3 000e 43c7 000e 43bb 000e 43b3 000e     .C...C...C...C..
   eb2c0:	43a9 000e 439f 000e 4761 000e               .C...C..aG..

000eb2cc <_ZTV7TwoWire>:
	...
   eb2d4:	441d 000e 4467 000e 443f 000e 441f 000e     .D..gD..?D...D..
   eb2e4:	4447 000e 444f 000e 4457 000e 445f 000e     GD..OD..WD.._D..

000eb2f4 <_ZTV9IPAddress>:
	...
   eb2fc:	44b1 000e 44a1 000e 44a3 000e 6162 6475     .D...D...D..baud
   eb30c:	5300 7265 6169 006c 6553 6972 6c61 0031     .Serial.Serial1.
   eb31c:	6170 6172 006d 6d63 0064 6469 6800 646e     param.cmd.id.hnd
   eb32c:	7300 7274 006d 6966 746c 6c00 6c76 6100     .strm.filt.lvl.a
   eb33c:	6464 6148 646e 656c 0072 6572 6f6d 6576     ddHandler.remove
   eb34c:	6148 646e 656c 0072 6e65 6d75 6148 646e     Handler.enumHand
   eb35c:	656c 7372 4a00 4f53 534e 7274 6165 4c6d     lers.JSONStreamL
   eb36c:	676f 6148 646e 656c 0072 7061 0070 3025     ogHandler.app.%0
   eb37c:	3031 2075 5d00 0020 202c 2800 3a29 0020     10u .] ., .(): .
   eb38c:	6f63 6564 3d20 0020 6925 6400 7465 6961     code = .%i.detai
   eb39c:	736c 3d20 0020 6e6c 6600 006e 6f63 6564     ls = .ln.fn.code
   eb3ac:	6400 7465 6961 006c 6f6e 656e 7400 6172     .detail.none.tra
   eb3bc:	6563 6900 666e 006f 6177 6e72 6500 7272     ce.info.warn.err
   eb3cc:	726f 7000 6e61 6369 6100 6c6c 0000 0000     or.panic.all....

000eb3dc <_ZTVN5spark9MeshClassE>:
	...
   eb3e4:	45f3 000e 45e9 000e 45df 000e 45d5 000e     .E...E...E...E..
   eb3f4:	45c9 000e 45bd 000e 45b1 000e 45a9 000e     .E...E...E...E..
   eb404:	459f 000e 4595 000e 4761 000e               .E...E..aG..

000eb410 <_ZTVN5spark12NetworkClassE>:
	...
   eb418:	46f9 000e 4703 000e 470d 000e 4717 000e     .F...G...G...G..
   eb428:	4721 000e 472d 000e 4739 000e 4745 000e     !G..-G..9G..EG..
   eb438:	474d 000e 4757 000e 4761 000e               MG..WG..aG..

000eb444 <_ZTV8SPIClass>:
	...
   eb44c:	4921 000e 4923 000e 005a 2b25 3330 3a64     !I..#I..Z.%+03d:
   eb45c:	3025 7532 2500 2d59 6d25 252d 5464 4825     %02u.%Y-%m-%dT%H
   eb46c:	253a 3a4d 5325 7a25 6100 6373 6974 656d     :%M:%S%z.asctime
   eb47c:	0000 0000                                   ....

000eb480 <_ZTV11USARTSerial>:
	...
   eb488:	498d 000e 49dd 000e 49eb 000e 4841 000e     .I...I...I..AH..
   eb498:	49a1 000e 49c3 000e 49af 000e 49d7 000e     .I...I...I...I..
   eb4a8:	498f 000e 4993 000e                         .I...I..

000eb4b0 <_ZTV9USBSerial>:
	...
   eb4b8:	4ac9 000e 4b19 000e 4b27 000e 4841 000e     .J...K..'K..AH..
   eb4c8:	4b05 000e 4acb 000e 4ae1 000e 4b13 000e     .K...J...J...K..
   eb4d8:	4af7 000e 4ac5 000e 7865 0070 7865 6670     .J...J..exp.expf
   eb4e8:	0000 0000 6f6c 6667 0000 0000 7173 7472     ....logf....sqrt
   eb4f8:	0066 0000 0000 0000                         f.......

000eb500 <halF>:
   eb500:	0000 0000 0000 3fe0 0000 0000 0000 bfe0     .......?........

000eb510 <ln2LO>:
   eb510:	3c76 3579 39ef 3dea 3c76 3579 39ef bdea     v<y5.9.=v<y5.9..

000eb520 <ln2HI>:
   eb520:	0000 fee0 2e42 3fe6 0000 fee0 2e42 bfe6     ....B..?....B...

000eb530 <halF>:
   eb530:	0000 3f00 0000 bf00                         ...?....

000eb538 <ln2LO>:
   eb538:	f7d1 3717 f7d1 b717                         ...7....

000eb540 <ln2HI>:
   eb540:	7180 3f31 7180 bf31                         .q1?.q1.

000eb548 <npio2_hw>:
   eb548:	0f00 3fc9 0f00 4049 cb00 4096 0f00 40c9     ...?..I@...@...@
   eb558:	5300 40fb cb00 4116 ed00 412f 0f00 4149     .S.@...A../A..IA
   eb568:	3100 4162 5300 417b 3a00 418a cb00 4196     .1bA.S{A.:.A...A
   eb578:	5c00 41a3 ed00 41af 7e00 41bc 0f00 41c9     .\.A...A.~.A...A
   eb588:	a000 41d5 3100 41e2 c200 41ee 5300 41fb     ...A.1.A...A.S.A
   eb598:	f200 4203 3a00 420a 8300 4210 cb00 4216     ...B.:.B...B...B
   eb5a8:	1400 421d 5c00 4223 a500 4229 ed00 422f     ...B.\#B..)B../B
   eb5b8:	3600 4236 7e00 423c c700 4242 0f00 4249     .66B.~<B..BB..IB

000eb5c8 <two_over_pi>:
   eb5c8:	00a2 0000 00f9 0000 0083 0000 006e 0000     ............n...
   eb5d8:	004e 0000 0044 0000 0015 0000 0029 0000     N...D.......)...
   eb5e8:	00fc 0000 0027 0000 0057 0000 00d1 0000     ....'...W.......
   eb5f8:	00f5 0000 0034 0000 00dd 0000 00c0 0000     ....4...........
   eb608:	00db 0000 0062 0000 0095 0000 0099 0000     ....b...........
   eb618:	003c 0000 0043 0000 0090 0000 0041 0000     <...C.......A...
   eb628:	00fe 0000 0051 0000 0063 0000 00ab 0000     ....Q...c.......
   eb638:	00de 0000 00bb 0000 00c5 0000 0061 0000     ............a...
   eb648:	00b7 0000 0024 0000 006e 0000 003a 0000     ....$...n...:...
   eb658:	0042 0000 004d 0000 00d2 0000 00e0 0000     B...M...........
   eb668:	0006 0000 0049 0000 002e 0000 00ea 0000     ....I...........
   eb678:	0009 0000 00d1 0000 0092 0000 001c 0000     ................
   eb688:	00fe 0000 001d 0000 00eb 0000 001c 0000     ................
   eb698:	00b1 0000 0029 0000 00a7 0000 003e 0000     ....).......>...
   eb6a8:	00e8 0000 0082 0000 0035 0000 00f5 0000     ........5.......
   eb6b8:	002e 0000 00bb 0000 0044 0000 0084 0000     ........D.......
   eb6c8:	00e9 0000 009c 0000 0070 0000 0026 0000     ........p...&...
   eb6d8:	00b4 0000 005f 0000 007e 0000 0041 0000     ...._...~...A...
   eb6e8:	0039 0000 0091 0000 00d6 0000 0039 0000     9...........9...
   eb6f8:	0083 0000 0053 0000 0039 0000 00f4 0000     ....S...9.......
   eb708:	009c 0000 0084 0000 005f 0000 008b 0000     ........_.......
   eb718:	00bd 0000 00f9 0000 0028 0000 003b 0000     ........(...;...
   eb728:	001f 0000 00f8 0000 0097 0000 00ff 0000     ................
   eb738:	00de 0000 0005 0000 0098 0000 000f 0000     ................
   eb748:	00ef 0000 002f 0000 0011 0000 008b 0000     ..../...........
   eb758:	005a 0000 000a 0000 006d 0000 001f 0000     Z.......m.......
   eb768:	006d 0000 0036 0000 007e 0000 00cf 0000     m...6...~.......
   eb778:	0027 0000 00cb 0000 0009 0000 00b7 0000     '...............
   eb788:	004f 0000 0046 0000 003f 0000 0066 0000     O...F...?...f...
   eb798:	009e 0000 005f 0000 00ea 0000 002d 0000     ...._.......-...
   eb7a8:	0075 0000 0027 0000 00ba 0000 00c7 0000     u...'...........
   eb7b8:	00eb 0000 00e5 0000 00f1 0000 007b 0000     ............{...
   eb7c8:	003d 0000 0007 0000 0039 0000 00f7 0000     =.......9.......
   eb7d8:	008a 0000 0052 0000 0092 0000 00ea 0000     ....R...........
   eb7e8:	006b 0000 00fb 0000 005f 0000 00b1 0000     k......._.......
   eb7f8:	001f 0000 008d 0000 005d 0000 0008 0000     ........].......
   eb808:	0056 0000 0003 0000 0030 0000 0046 0000     V.......0...F...
   eb818:	00fc 0000 007b 0000 006b 0000 00ab 0000     ....{...k.......
   eb828:	00f0 0000 00cf 0000 00bc 0000 0020 0000     ............ ...
   eb838:	009a 0000 00f4 0000 0036 0000 001d 0000     ........6.......
   eb848:	00a9 0000 00e3 0000 0091 0000 0061 0000     ............a...
   eb858:	005e 0000 00e6 0000 001b 0000 0008 0000     ^...............
   eb868:	0065 0000 0099 0000 0085 0000 005f 0000     e..........._...
   eb878:	0014 0000 00a0 0000 0068 0000 0040 0000     ........h...@...
   eb888:	008d 0000 00ff 0000 00d8 0000 0080 0000     ................
   eb898:	004d 0000 0073 0000 0027 0000 0031 0000     M...s...'...1...
   eb8a8:	0006 0000 0006 0000 0015 0000 0056 0000     ............V...
   eb8b8:	00ca 0000 0073 0000 00a8 0000 00c9 0000     ....s...........
   eb8c8:	0060 0000 00e2 0000 007b 0000 00c0 0000     `.......{.......
   eb8d8:	008c 0000 006b 0000                         ....k...

000eb8e0 <init_jk>:
   eb8e0:	0004 0000 0007 0000 0009 0000               ............

000eb8ec <PIo2>:
   eb8ec:	0000 3fc9 0000 39f0 0000 37da 0000 33a2     ...?...9...7...3
   eb8fc:	0000 2e84 0000 2b50 0000 27c2 0000 22d0     ......P+...'..."
   eb90c:	0000 1fc4 0000 1bc6 0000 1744               ..........D.

000eb918 <__sf_fake_stdin>:
	...

000eb938 <__sf_fake_stdout>:
	...

000eb958 <__sf_fake_stderr>:
	...

000eb978 <_global_impure_ptr>:
   eb978:	c3a4 2003                                   ... 

000eb97c <link_const_variable_data_end>:
   eb97c:	000d433d 	.word	0x000d433d
   eb980:	000d43d1 	.word	0x000d43d1
   eb984:	000d6445 	.word	0x000d6445
   eb988:	000e437d 	.word	0x000e437d
   eb98c:	000e4409 	.word	0x000e4409
   eb990:	000e4585 	.word	0x000e4585
   eb994:	000e46a9 	.word	0x000e46a9
   eb998:	000e482d 	.word	0x000e482d
   eb99c:	000e4905 	.word	0x000e4905
   eb9a0:	000e4961 	.word	0x000e4961
   eb9a4:	000e4979 	.word	0x000e4979
   eb9a8:	000e4db1 	.word	0x000e4db1
   eb9ac:	000e4e05 	.word	0x000e4e05
   eb9b0:	000e4ec9 	.word	0x000e4ec9
   eb9b4:	000e4f4d 	.word	0x000e4f4d
   eb9b8:	000e4fd1 	.word	0x000e4fd1

000eb9bc <link_constructors_end>:
   eb9bc:	00000000 	.word	0x00000000
